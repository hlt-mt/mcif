<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是大规模的网络 craw 数据，其中政治新闻媒体被广泛覆盖。</sample>
    <sample id="1">根据提供的内容，论文的作者属于麦克吉尔大学、Mila和微软研究。</sample>
    <sample id="2">演讲者介绍了一篇关于文档理解的论文，重点讨论了视觉增强文档理解问题。该论文旨在解决各种类型的文档，如表格、收据和海报的文档理解问题。在论文中，作者提出了一个名为“Mask”的多模预训练模型，该模型使用文本和布局信息作为输入，以增强文本-布局交互和布局表示学习。与之前的预训练模型相比，Mask的主要改进之处在于选择一维位置、掩码策略和预训练目标。Mask使用局部一维位置，即局部一维位置，而不是全局一维位置，这有助于通过结合一维位置、二维位置和语义信息来推断全局阅读顺序。此外，Mask还引入了两种新的预训练目标：Mask位置建模和Mask位置建模，以进一步促进文本-布局交互。在实验中，Mask在不同类型的文档上表现出色，特别是在具有复杂布局的文档上。</sample>
    <sample id="3">大家好，欢迎来到我们关于DePlain的演示，这是德语文本简化的新语料库，适用于文档级别和句子级别的文本简化。我的名字是雷吉纳·施特龙，我将为您介绍演示的第一部分。首先，让我们来定义文本简化。文本简化是一种过程，用于通过为特定目标群体改善文本理解来适应文本，例如阅读困难的人或非母语使用者。要训练文本简化模型，我们需要平行文本对，例如文档或句子的平行对。在示例中，您可以看到一个复杂德语句子与其翻译成简单语言的句子对。为了简化句子，可以使用不同的技术，如词汇替换、从句省略、从句省略和重排，甚至插入短语。现在，我们提出了一个新的语料库DePlain，因为近年来存在一些问题，使得现有语料库变得太小，无法用于训练文本简化模型。另外三个最近提出的语料库都是自动对齐的，这意味着它们可能包含错误的对齐。因此，我们提出了DePlain，它分为两个子语料库：DePlain API和DePlain Web。DePlain API基于新闻文本，在DePlain API中手动对齐了483篇文档，结果产生了大约30,000-13,000个平行句子对。对于DePlain Web，这个语料库包括不同领域，并且我们手动对齐了750篇文档，另一方面使用自动对齐方法。总共，我们得到了30,450个句子对。我们进一步分析了这些句子对，例如，我们可以看到，圣经文本比新闻文本或语言学习文本更强烈地简化了。在所有级别上，包括词汇简化、结构简化和整体简化。此外，我们可以看到DePlain语料库具有不同简化转换的高多样性。例如，在DePlain API语料库中，我们有很多重排和词替换，而在DePlain Web语料库中，我们有很多改写。现在让我们看看我们可以用这个语料库做什么。你好，我是阿默，现在我将讨论我们的数据集DePlain的用例。对于第一个用例，我们可以评估自动对齐方法。近年来，已经提出了许多对齐方法，但在机器翻译的背景下，我们有两个平行文档，分别用不同语言编写，我们想提取两份文档中句子的对齐。但在我们的用例中，我们试图提取具有相同语言和相同内容但处于不同复杂度级别的两份平行文档之间的对齐。现在，由于我们拥有DePlain语料库，其中包含手动对齐的句子，我们可以使用这些句子作为金标准对齐来评估一些已提出的对齐方法。我们对提出的对齐方法进行了调整，并在论文中出版了这些调整和运行实验的代码。最后，我们得出结论，对于德语文本简化，最佳的自动对齐方法是MSTAlign，您可以在论文中找到该方法的代码。第二个用例我们在论文中展示的是自动文本简化，通过微调语言模型来生成简化文本。我们微调了Long Impart模型来生成文档级别的简化文本，我们还微调了Normal Base Long Impart模型来生成句子级别的简化文本。您可以在论文中找到所有检查点和实验的详细评分和评估指标。我们得出结论，这种基本微调可以产生或获得比 baseline 分数更好的分数，我们建议这些结果作为未来自动文本简化问题的基准。谢谢您的关注，希望能在会议期间与您见面。</sample>
    <sample id="4">演讲者的名字是Kayi Yin。</sample>
    <sample id="5">他们使用一种语言模型，该模型具有部分重叠的背景知识。</sample>
    <sample id="6">演讲者介绍了一项名为“多语言多模态摘要”（MTM summarization）的统一多语言和跨语言摘要化的工作，该工作与Fan Dung、Duong Yoon Long、Zhixu Jie Fong和Ye等合作完成。他们将传统的多语言摘要化和跨语言摘要化整合到一个更广泛的框架中，称为多语言多模态摘要化。MTM摘要化旨在构建一个单一的摘要模型，能够处理任意源语言的文档并生成目标语言的摘要。通过初步研究，他们发现MTM摘要化有助于摘要模型在不同语言间更好地传输任务知识，比传统的多语言摘要化和跨语言摘要化更有效。此外，他们提出了一个三阶段训练方法来训练MTM摘要化模型，包括母语训练、跨语言训练和任务特定训练。实验结果表明，MTM摘要化在多语言多模态摘要化设置下，比单向多语言摘要化和跨语言摘要化模型表现更好。他们还进行了评估研究，验证了每个训练阶段的有效性，并展示了MTM摘要化方法的多样性。演讲者鼓励观众阅读他们的论文以获取更多细节。</sample>
    <sample id="7">是的，CoNLL-2003 标注器仍然有效。</sample>
    <sample id="8">提出的人工评估方法新颖之处在于它通过明确标注模型响应中是否存在某些行为，如提供无关信息或自相矛盾，来减少人工评估的主观性。该方法被称为“在聊天中标注行为”（ABC EVL），旨在更精确和可靠地评估对话质量的多个方面。</sample>
    <sample id="9">根据英语内容，现有弱监督方法的成功在很大程度上依赖于使用干净的验证样本。研究发现，缺乏干净的验证样本会导致性能显著下降，表明这些方法实际上需要干净、手动标注的数据才能有效工作。</sample>
    <sample id="10">根据英语内容，可以采取的措施包括提高模型对背景知识的访问，因为如果模型具有与注释者相同的完全相同背景知识，准确率会显著提高。此外，模型应该能够处理部分重叠的背景知识，这将使准确率保持在82%至87%之间，这是更现实的情况。如果模型只能访问实体名称，则准确率会降低到60%，因此需要改进以处理更有限的背景信息。</sample>
    <sample id="11">Hello everyone. My name is Jack Hessel and I'm a research scientist at AI2. I'm super excited to be here today to present Do Androids Laugh at Electric Sheep: Humor Understanding Benchmarks from the New Yorker Caption Contest. This is joint work with a lot of awesome collaborators from the University of Utah, Cornell University, University of Washington, Air Mail, and OpenAI. Have you heard the news? Large language models can now generate and even explain jokes. If you log on to chat GPT and ask it to tell you a joke, it might generate something like this. Why don't scientists trust atoms? Because they make up everything! Even beyond generating simple jokes like this, some language models have even successfully explained jokes before. Here's an example from Google's 540 billion parameter PaLM language model. In this case, PaLM is attempting to explain a joke about TPUs, which coincidentally is the type of hardware that the model was trained on. I would argue this is a successful joke explanation. This capability was actually highlighted at Google I/O, which is Google's developer conference. Here's a picture of Sundar Pichai touting joke explanation as a interesting capability of PaLM. This public-facing declaration of capacity led to headlines like "No joke, Google's AI is smart enough to understand your humor." But do large language models really understand humor? If you play around for example with chat GPT and probe at its humor understanding capacity, for example by asking it to tell you a knock-knock joke involving a pineapple, you might be a little bit disappointed. Here, chat GPT appears to paste the word pineapple into some sort of knock-knock joke and then claims that it's made a pun, but it doesn't quite make sense. I personally don't see the pun here. Now, I'm not going to tell you this isn't an objectively funny output. It's a little bit absurd because there isn't a pun and I am a fan of absurd humor. But the fact that the language model is claiming that there's a pun really calls into question whether or not chat GPT understands what's going on here. To probe this in a more structured way, we turned to the New Yorker Caption Contest. The New Yorker is a magazine which has been publishing cartoons since its inception nearly 100 years ago. More recently, starting in the mid-90s, the New Yorker Caption Contest has been run through the magazine as well. The contest works like this: each week, a captionless cartoon, like this cheese sailing one, is published in the New Yorker and readers are invited to submit their best captions for that cartoon. Then, the editors select three finalists and a final vote determines the ultimate winner. This is a very popular contest and they get thousands of entries from all across the country and really the globe including famous celebrity entrants like actors and former presidential candidates. So this is very much an on-the-public consciousness. So what do we do with the New Yorker Caption Contest data? Well, we operationalize it into three different tasks. For the first task, which we call matching, we present models with five choices of captions, only one of which was truly written about a given cartoon. Next, we consider quality ranking. Here, we present two captions that were both written about the cartoon, but one was judged by human raters to be much higher quality. These human raters, in some cases, are the New Yorker editors themselves and in other cases this is done by crowd voting. Finally, we have an explanation generation task where we prompt the language model to generate a two-to-four sentence explanation of why the joke is funny. To support computational experiments on the corpus, we gathered a new set of annotations. In particular, for each of over 700 cartoons representing more than a decade of caption contests, we collect locations, descriptions, uncanny highlights, and entity links for each of the cartoons. Also, we collect a corpus of joke explanations so we have a set of over 650 two-to-four sentence explanations of jokes that we use. So how well do language models do at our various tasks? We'll look first at matching and quality ranking. On the matching task, our best model, which is CLIP fine-tuned on the corpus we annotated, achieves around 62% accuracy on this task. This is relative to a 20% random guessing baseline, that's one in five. However, humans get around 94% on the same task, representing a big gap in humor understanding. Now, you might be wondering how well do models that don't have to do computer vision do? We wanted to test models like GPT-4 on this corpus, but GPT-4 can't take in pixels directly. So separately, we also consider a setting where we take language models like GPT-4 and condition them to do the exact same tasks, but give them a human-author description of the image. Even with this additional annotation, there's still quite a big performance gap between GPT-4's five-shot performance and human on the matching and quality ranking tasks. Speaking of GPT-4, let's take a look at some of its joke explanations for our third task of explanation generation. Here we prompted GPT-4 to generate an explanation for this cartoon and caption, he'll be back. You can see some errors highlighted. So for example, GPT-4 claims that the customer is the one saying he'll be back when I would argue it's pretty clear it's the people who are working at this establishment are saying this and there are a few other errors as well. This is borne out in human evaluation experiments where in a blind A/B study, human explanations are preferred to five-shot GPT-4 explanations in more than two-thirds of cases. Overall, we're super excited to see what folks do with our dataset. We have a leaderboard and models available at this URL. And yeah, thank you so much for your attention and I look forward to seeing you at ACL. Thank you.</sample>
    <sample id="12">根据演讲者的介绍，这篇论文有五位作者：Dawei、Yi Lu、Yuan Smooth Bass、Gary Stephen和DeTich Clarko。</sample>
    <sample id="13">Daniel Rotem, a student at Hebrew University in Jerusalem, presented his work on adaptive inference and low-resource settings. Adaptive inference is a method for reducing the inference time of large language models by using low-capacity models for easy samples. Two common adaptive inference methods are multimodal and early exit. Multimodal involves storing multiple models, each with a classifier, which are trained separately and run sequentially during inference. Early exit involves fitting multiple classifiers to the model after intermediate transformer layers, which are all trained together and halt computation when a classifier decides to do so. The pros and cons of each method were discussed, including the versatility and overhead of multimodal, and the faster inference and memory efficiency of early exit. However, multimodal can suffer from conflicting gradients, where updates from different classifiers interfere with each other, potentially degrading performance. To test this hypothesis, Daniel compared individual early exit models with separate multimodal classifiers, finding that multimodal outperformed early exit by an average of 2.3%. The gap was largest for the earliest classifiers, with an average of 5.2%. The study also measured the speed-accuracy trade-off of the models, finding that early exit outperforms multimodal for later classifiers due to the overhead suffered by multimodal. Based on these findings, Daniel introduced the "sweet" method, a novel fine-tuning method for early exit architectures that avoids conflicting gradients. The results showed that sweet closes most of the gap between early exit and multimodal, but negatively affects later classifiers in some cases. Overall, the study highlights the existence of conflicting gradients in early exit training processes and introduces the sweet method as a potential solution for future research in fine-tuning algorithms tailored to early exit architectures.</sample>
    <sample id="14">嗨，我的名字是Adam Siprowski。本文讨论的是协调结构的依赖关系结构。正如您所知，不同的依赖关系结构由不同的理论和语义方法假设。例如，在普遍依赖性中，Lisa和Maggie之间的协调结构的第一 conjunct 是整个协调结构的头部。因此，在这种情况下，Lisa是一个例子。另一个例子是在伊戈尔·米特鲁克的意义文本理论中，协调结构的第一 conjunct 也是头部。因此，这些两种方法都是对称的；它们突出一个 conjunct。另一种对称的方法是将协调结构的头部设为连词，如在布加罗的连词头部方法中，协调结构由连词头部。因此，我们得到从连词到所有 conjunct 的依赖关系。最后，有一种多头的方法，如在德卡松的词法中使用，其中所有 conjunct 都是协调结构的头部。因此，我们得到从治理者（这里是Lars）到所有 conjunct 的依赖关系。这些 conjunct 分别是Lisa和Maggie。本文的主要目的是提出一种新论点，支持对称的协调结构，而不是不对称的协调结构。这个论点基于依赖长度最小化原则，将在这些示例的基础上进行解释。在英语中，正如你可能知道的那样，直接宾语更喜欢靠近动词，而介词短语可能离动词更远。因此，“March read it yesterday”很好，因为直接宾语“it”离动词更近。然而，“March read yesterday it”则较差，因为在动词和直接宾语之间有一个介词短语“yesterday”。然而，这个效果可以缓解，当直接宾语很长时，因为它可以移动到介词短语后面。这在下面的例子中说明了这一点。这两个句子都可以接受。第一个句子违反了动词语法原则，即直接宾语应该离动词更近。然而，它满足依赖长度最小化原则，该原则规定较短的依赖关系更可取。因此，这两个树只显示了关键依赖关系的长度，即两个结构中不一致的部分。因此，从“red”到“yesterday”的依赖关系长度为7个单词，从“red”到“book”的依赖关系长度为4个单词。因此，总长度为11个单词。当你交换这两个成分时，这两个依赖关系的总长度变为6个单词。因此，比11个单词短得多，这就是为什么这句话听起来很好。它违反了一个原则，但满足另一个原则。因此，我们从增强版本的Penn树库中提取了关于协调的数据，并查看了为什么我们不使用普遍依赖性。这些数据证实了之前多次观察到的观点，即左 conjunct 通常更短。例如，“salt and pepper”和“not pepper and salt”以音节计数。此外，我们还观察到了之前提到的趋势，即这种趋势随着两个 conjunct 之间的长度差异而增长。因此，当两个 conjunct 之间的长度差异更大时，左边 conjunct 更倾向于成为第一个 conjunct。右边 conjunct 越大，这种趋势就越明显。然而，当治理者在左边时，这种趋势仍然存在。例如，“I saw Bart and Lisa”和“Lisa and Bart”。“Lisa”是治理者，位于左边。右边 conjunct 没有外部治理者。因此，在这种情况下，左边 conjunct 更倾向于成为第一个 conjunct。然而，当治理者在右边时，这种趋势消失。例如，“Lars governed the coordination”和“the net governed the coordination”。在这种情况下，左边 conjunct 不再倾向于成为第一个 conjunct。我们在论文中展示了如何通过测量字符长度（第一列是音节，中间列是单词，第三列是字符）来证明这一点。我们看到，当治理者在左边时，左边 conjunct 变得更短的趋势随着两个 conjunct 之间的绝对长度差而稳定增长。同样，当没有治理者时，这种趋势仍然存在。然而，当治理者在右边时，这种趋势消失。我们在论文中详细说明了这一点，并提供了完整的论点和数据，以便与我们联系并讨论这个海报。谢谢。</sample>
    <sample id="15">根据提供的内容，论文的作者数量是三位：马提亚斯·兰德曼、亚历山大·科拉和伊万·季托夫。</sample>
    <sample id="16">根据分析，Bible texts的简化程度更大。</sample>
    <sample id="17">The speaker introduces a research paper on multi-modal relation extraction, which aims to determine the semantic relationship between entities in a given text. The paper highlights the challenges of extracting relations from texts with various forms and modalities, such as social media data, where insufficient context can lead to ambiguity or multi-sense words. To address these challenges, the paper proposes a method that incorporates visual sources into textual data for better understanding.

The proposed method involves several steps: representing text and images with corresponding visual and textual graphs, merging these graphs into a unified cross-modal graph (CMG), screening the initial CMG structures by filtering nodes and adjusting edges, and enriching the CMG features with multimodal topic information using attention operations. The effectiveness of this method is evaluated using the MRQA dataset, showing higher performance compared to text-based methods. The study also explores the impact of internal information screening and external information exploitation, finding that internal screening is more important for high cross-modal relevance inputs, while external exploitation is more useful for low relevance inputs.

In conclusion, the paper introduces a novel idea of simultaneous information subtraction and addition for multi-modal relation extraction, achieving significant improvements over existing models. The speaker encourages readers to explore the QR code for more detailed information about the research.</sample>
    <sample id="18">根据所给英文内容，偏好较短左并列词的示例是“盐和胡椒”和“胡椒和盐”，因为这些并列词在长度上更接近，符合依赖长度最小化原则。</sample>
    <sample id="19">The speaker introduces their work on efficient open-domain question answering, which was accepted by ACL 2023. They present the framework of their work in four parts: the two-stage model proposed by Dandan Chen in 2017, the retrieval stage using a retriever to retrieve evidence contexts from Wikipedia Corpus, and the reader stage using a reader to understand the question and retrieve evidence to reason out the answer. The retrieval process involves two encoders: a question encoder and a document encoder. The Wikipedia Corpus is preprocessed into an index file by the document encoder before answering questions. The challenges of open-domain question answering include the large size of the Wikipedia Corpus (26 million documents, 20 GB), the index file (65 GB), and the presence of multiple language models with millions of parameters. These challenges make open-domain question answering systems suitable for real-time applications and deployment on resource-constrained devices. To achieve efficient open-domain question answering systems with smaller memory costs, faster inference, and comparable performance, the speaker proposes using lightweight models, parameter sharing, and designing multi-stage models such as using one model to achieve both retrieval and reading. The speaker also compares existing open-domain question answering models from the data aspect, showing that retrieval and reading systems perform well-balanced among speed, memory, and performance. Retrieval-only systems create large indexes but have fast inference, while generating-only systems are always large models and achieve low performance. Based on this analysis, the speaker concludes that if one has limited resources, they can consider reducing index size by using generating-only systems or embedding compression, or reducing the model size by knowledge distillation or designing one-stage models for both retrieval and reading. If one pursues real-time feedback, retrieval-only systems are good choices. If one pursues trade-offs, retrieval and reading systems are relatively more appropriate. Finally, the speaker discusses two future works: how can open-domain question answering systems be deployed on low-power devices, and what evaluation metrics should be considered.</sample>
    <sample id="20">是的，这些模型可以用于您的研究。演讲者提到，他们已经开发了一个公开可用的预训练模型，可以在他们的GitHub仓库中找到。此外，他们还提供了训练脚本，可以在他们的GitHub仓库中找到。这表明这些模型和相关资源是为公众使用的，并且可以用于各种研究目的。</sample>
    <sample id="21">DEplain-apa 包含来自新闻来源的文档。</sample>
    <sample id="22">有助于良好泛化的因素包括模型架构、模型大小和更多的微调示例。</sample>
    <sample id="23">The video discusses the challenges faced by text-to-image models in accurately rendering text. The speaker, Dan Garrett, explains that while recent advancements have enabled these models to generate high-quality images, they often struggle with representing text correctly. He specifically focuses on the Imagen model, which uses a T5-XL encoder to convert input text into a format suitable for diffusion models. However, even with this approach, simpler text inputs often fail to produce accurate image outputs.

To better understand these issues, Garrett delves into the text encoding process used by T5, which employs sentence-piece tokenization. This method breaks down input text into subword tokens rather than individual letters, making it difficult for the model to decompose these tokens back into the correct letters for text rendering. The video shows that smaller versions of T5 struggle significantly with spelling accuracy, with base and large versions achieving under 20% and 70% accuracy, respectively. In contrast, larger models like the T5-XXL show improved performance but still fall short of perfect accuracy.

Garrett also compares T5 with the PaLM models, which are more accurate in spelling due to their larger size and extensive training data. However, these models are impractical for many applications due to their size and resource requirements. He then introduces BitT5, a model that directly receives individual characters instead of subword tokens, allowing it to copy characters from input to output effectively.

By analyzing the frequency of words, Garrett explains that T5 struggles most with spelling the most frequent words because they are often represented by fewer subword tokens, requiring more decomposition into individual letters. BitT5, with full access to character-level information, performs well across all scales without being affected by word frequency.

To improve text rendering, Garrett suggests augmenting the Imagen model by concatenating an additional text representation from the BitT5 small model. This addition, though only increasing the parameter count by about five percent, significantly enhances the model's ability to spell correctly and improves overall image generation characteristics. While the diffusion model can still introduce errors, the text encoder's awareness of character-level information helps mitigate these issues.

In summary, the video highlights the challenges of text-to-image models in rendering text accurately, explores the limitations of different text encoders, and proposes a strategy to improve model spelling abilities by incorporating character-level information.</sample>
    <sample id="24">通过测量两个并列词之间的长度差来衡量左并列词是否更短。当左并列词的长度比右并列词长时，它倾向于更短。</sample>
    <sample id="25">设计实验以研究支配词位置的影响，可以涉及在不同位置放置支配词（左侧、右侧或不存在）并测量协调结构的长度。通过比较这些位置的长度，可以评估支配词位置对协调结构的影响，并验证观察到的趋势。</sample>
    <sample id="26">基线分类器在不平衡数据上训练的效果不佳，因为数据集中认知分歧的出现率非常低。基线分类器的表现仅略优于随机猜测，表明需要改进以有效检测分歧。</sample>
    <sample id="27">根据提供的信息，无法确定论文的作者人数。演讲者提到了“我们”这个词，这可能意味着一个团队或一组作者，但没有更具体的信息，无法确定确切的作者人数。</sample>
    <sample id="28">示例对话中的角色名字是Bob和Alice。</sample>
    <sample id="29">在正式性和连贯性等话语现象上，语境感知 MT 模型比语境无关模型更有优势。</sample>
    <sample id="30">The speaker introduces a new framework called LLM Blender, which is designed to improve the performance of large language models by using an ensemble learning approach. The key idea behind LLM Blender is based on pairwise ranking and generative fusion. The speaker explains that there are many large language models released every week, and while some models claim to have achieved great performance, it is only about average overall performance. However, when you have a particular input example, simply using the single top-performing model may not be the best choice. The optimal selection of models can significantly vary across different input examples. Therefore, LLM Blender proposes a two-stage framework that involves running multiple models, comparing their outputs using a pairwise ranking module named Pair Ranker, and then fusing the top candidates using a generative fusion model. The speaker also discusses the importance of using more large language models for each input to generate a better output than using any single model for all inputs. The speaker concludes by emphasizing the simplicity and effectiveness of LLM Blender as a framework for large language models.</sample>
    <sample id="31">根据所提供的英文内容，无法确定论文作者所属的机构。在演讲中没有提及任何机构名称或细节。</sample>
    <sample id="33">引入的框架通过比较注释与现有数据集和模型的注释来量化立场。它使用皮尔逊相关系数来衡量注释之间的差异，从而评估数据集和模型的立场。</sample>
    <sample id="34">The speaker introduces a framework called Crest, which combines selective rationalization and counterfactual text generation. This framework is the result of collaboration with Alexis Ross, Juan Guevara, and Enzo Marches. The framework generates counterfactual examples by editing specific parts of the input, which are then used to create meaningful rationales. The speaker compares Crest against related works using both automatic metrics and human evaluation, finding that Crest's counterfactuals are more valid and natural than those generated by other methods. The speaker also proposes using Crest for data augmentation and shows that it performs well on various datasets. Finally, the speaker analyzes the interpretability of Crest's rationales in three dimensions: plausibility, forward similarity, and counterfactual similarity, finding that Crest's rationales are more plausible and have higher counterfactual similarity than those produced by other methods.</sample>
    <sample id="36">The speaker, Thalmus Pich, introduces a study on learning language-specific layers for multilingual machine translation. The research is a joint effort with Robin Schmidt, Ee Shu Liow, and Stefan Bites. Multilingual machine translation offers several advantages, including scalability, speed, and improvements for low-resource languages. However, it also has limitations such as limited capacity per language and increased training difficulty.

The goal of the study is to increase the capacity per language while keeping inference costs constant. The solution proposed is language-specific layers (LLs), which involve having one regular Transformer layer per language. At inference time, the correct sub-layer is selected based on whether it is the source or target language. This approach allows the model to maintain constant inference costs by only calling the relevant sub-layer.

The placement of LLs was optimized through an iterative process, focusing on the encoder layers. The weights were trained to determine the best placement, and the architecture was then fixed and retrained from scratch. The results showed significant improvements over both language adapters and baseline models, especially for low-resource languages. The study was conducted using the WMT 21 newstest sources for 10 languages, evaluated on the Flores-101 dataset, and reported in terms of BLEU scores.</sample>
    <sample id="37">根据所给英文内容，在之前的研究中，当人类受试者被给予相同的人格化提示时，研究结果是他们能够表面种族刻板印象。</sample>
    <sample id="38">此研究使用了来自Pankration增强版本的统计数据。</sample>
    <sample id="39">根据所提供的内容，这篇论文有三位作者：Adam Strykowski、Igor Miltruk和De Cattson。</sample>
    <sample id="40">与认知失调密切相关的任务包括辩论独立性认知失调分类任务和扩展与比较的二元分类任务。这些任务与认知失调概念相关，因为它们涉及识别不同观点或陈述之间的不一致之处。</sample>
    <sample id="41">This is Siliang from the Natural Language Processing Lab at EPFL University. I am going to introduce our work of Peacock, a personal common sense knowledge graph for consistent and engaging narratives, collaborated with Sony Group Corporation. Sustaining coherent and engaging narratives such as dialogues or stories requires natural language processing systems to understand how the personalities of speakers, listeners, or characters ground the narrative. For example, an adventurous architect may be interested in talking about outdoor explorations with his friends who have similar hobbies, but he may prefer to discuss architectural design ideas instead with his colleagues at work. However, narrative systems have not yet learned good representations of real-world personalities which involve rich world knowledge and complex interconnections with each other. For instance, as shown in the picture, a singer preparing an album may have studied music at university, this would allow him to share the experience with a student majoring in composition, who may study music as a daily routine. In this work, we propose a person grounded common sense knowledge graph, Peacock, to represent such world-level personality knowledge at scale. Peacock contains about 3.8 thousand persons and 40 thousand distinctive attributes which form about 100 thousand person inferences or facts. Besides, about 9.2 thousand attributes are connected to two or more persons which contributes to the rich interconnections of personalities in Peacock. Based on the studies of human interactive behaviours, we frame the relations of personalities and their attributes in three dimensions, including four types of man relations as well as interactivity and distinctiveness. Detailed descriptions of these relations are listed here. We build Peacock in three steps. First, we select persons from existing common sense knowledge graphs which include both human roles and even base entities. Then, we induce attributes of persons from both common sense knowledge graphs and large-scale pre-trained language models. Finally, we cross out the annotations of Peacock relations via a joint human-AI majority voting scheme. Expert study shows that our majority voting with AI in the loop yields high-quality relation annotations, with average 87% of accuracy and F1. The AI annotator instructs Peacock efficiently mediates the disagreements between human annotators with lower temporal and financial costs. Knowledge graph in general cannot have a comprehensive coverage of all world knowledge, so we need to face the question of whether Peacock can help language models to learn and generalize the personality knowledge. We use Peacock to train a bar-based common knowledge generator on a person attribute inference task, where the model needs to predict an attribute of a given person with a target relation. As baselines, we compare to large-scale pre-trained language models, including five-shot GPT-3 and zero-shot GPT-3.5. Compared to the baselines, common bar trained on Peacock achieves overall better automatic evaluation results on various natural language generation metrics, and also higher accept rate in human evaluation. This indicates that Peacock can serve as a reliable personality base which enables lightweight language models to learn knowledge generation capabilities comparable to large-scale language models. Finally, we explore whether Peacock knowledge can be used to improve downstream narrative modeling. We investigate a person grounded dialogue generation task on the Coreset AI 2021 Persona Chat dataset. Specifically, we use a knowledge linker to retrieve facts from Peacock that are relevant to each speaker's original personality profile and utterances. Then we convert the retrieved facts into natural language statements to augment each speaker's profile. We choose the PEGASUS model as our baseline dialogue system. Human evaluation shows that Peacock augmenting model achieves better dialogue generation on various aspects including fluency, consistency, engagement, and personal expression. By comparing to the augmentation with Atomic 2020 knowledge graph, we also find that Peacock's person-centric common sense knowledge yields a more positive impact compared to general social common sense knowledge. We also stratify our human evaluation results based on the overlap of the two speakers of augmented Peacock knowledge, where we find that in terms of dialogue consistency and engagement, the winning rates of Peacock augmented model increase as the number of shared common attributes between two speakers becomes larger. Since more consistent connections between speakers leads to more consistent and engaging conversations, this highlights the importance of learning Peacock's interconnected world personality narratives. In summary, we propose a world-level personal common sense knowledge graph, Peacock, that contains large-scale high-quality personality inferences. Our knowledge resource can be used to train reliable personality generators and also enable more consistent and engaging narrative modeling. Our paper and GitHub set for this work are public now, which can also be found on our lab website.</sample>
    <sample id="42">根据所提供的英文内容，无法确定论文的作者人数。</sample>
    <sample id="43">根据提供的信息，这篇论文的作者是瓦苏达。</sample>
    <sample id="44">引入的框架与以前的研究不同之处在于它通过将注释与现有数据集和模型进行比较，来研究数据集和模型的定位性。这与以往只关注注释者之间的注释一致性或建模注释者分布的研究不同。</sample>
    <sample id="45">在三个比较设置中，生成的个性描述与刻板词汇的重叠最多。</sample>
    <sample id="46">在演讲中比较了DeepL和Google Translate这两个商业系统。使用Muda基准评估模型时，DeepL通常比Google Translate更准确。</sample>
    <sample id="47">大家好，我是张冰，是一名在华大大学读大三的学生。今天我要向大家展示我们团队的研究成果，从预训练数据到语言模型，再到下游任务，追踪政治偏见如何导致了NLP模型中的不公平现象。语言模型是基于大规模网络爬虫数据进行训练的，而政治新闻媒体在预训练数据中得到了很好的覆盖。根据对C4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》和《华盛顿邮报》等主流媒体在语言模型训练数据中被广泛覆盖。这既有好处也有坏处：一方面，模型能够学习到多元化的观点，庆祝民主和思想的多样性；另一方面，这些不同政治观点的内在社会偏见可能导致下游任务应用中的公平性问题。为了应对这个问题，我们提出了一个研究框架，旨在探讨政治偏见在预训练数据到语言模型再到下游任务的传播链条。具体来说，我们提出了以下两个问题：首先，我们如何评估语言模型的政治倾向，以及预训练数据可能在其中扮演的角色？其次，具有不同政治倾向的语言模型在下游任务中的表现如何？这是否会导致NLP应用中的公平性问题？我们首先使用不同的提示格式对语言模型进行测试，以确保评估符合政治科学文献的标准。初步结果表明，语言模型确实存在政治倾向，它们分布在政治 compass的四个象限上。GPT-4是最liberal的模型，而GPT系列通常比BERT系列更liberal。我们还进行了实验，通过在六种不同政治倾向的语料库上进一步预训练语言模型，来研究政治偏见是否会被模型学习。实验结果表明，模型的Ideological坐标确实会随着预训练数据的变化而变化。例如，在Roberta模型上，进一步预训练在左翼新闻语料库上，可以看到显著的liberal转变。我们还研究了模型是否能学习现代社会的 polarization。我们将预训练语料库分为特朗普就任前后两部分，发现模型在特朗普就任后普遍表现出更远离中心的政治倾向。最后，我们评估了具有不同政治倾向的语言模型在 hate speech detection 和 fake news detection 等NLP应用中的性能。结果表明，不同政治倾向的模型在检测 hate speech 和 fake news 时的表现存在差异。例如，left-leaning 模型在检测针对少数群体的 hate speech 时表现更好，但在检测针对权力群体的 hate speech 时表现较差。相反，right-leaning 模型在检测针对白人和男性群体的 hate speech 时表现更好，但在检测针对黑人、LGBTQ+和其他少数群体的 hate speech 时表现较差。类似的趋势也出现在 fake news detection 中。这些结果表明，不同政治倾向的模型在处理 hate speech 和 fake news 时会给出不同的预测，基于用户的社交媒体身份。这表明了NLP模型政治偏见所导致的公平性问题的严重性。例如，如果right-leaning 模型被用于 hate speech 或 fake news 的检测和控制，可能会导致持有不同政治观点的人被边缘化，而针对少数群体的 hate speech 可能会 rampant 地泛滥。因此，我们需要警惕并解决NLP模型政治偏见所带来的公平性问题。最后，我想强调一下关于NLP模型政治偏见的独特 dilemma。如果不清洗预训练数据中的政治观点，偏见将从预训练数据传播到模型，最终导致公平性问题。如果我们尝试清洗数据，可能会导致审查或排除某些内容，很难确定什么应该被保留在NLP训练数据中。这就像电学中的电容器问题一样，很难找到一个平衡点。谢谢大家的聆听，这就是我今天要分享的所有内容。</sample>
    <sample id="48">根据所给英文内容，这篇论文有五位作者。</sample>
    <sample id="49">MPP评估最多涵盖了124个词元的上下文长度。</sample>
    <sample id="50">演讲者介绍了一种名为“DePlain”的新语料库，用于德语文本简化。DePlain分为两个子语料库：DePlain API和DePlain Web。DePlain API基于新闻文本，包含483篇手动对齐的文档，生成约31,000个平行句子对。DePlain Web包括不同领域的750篇文档，其中一部分手动对齐，另一部分使用自动对齐方法，总共生成31,450个句子对。DePlain语料库具有多样化的简化变换，如重排、插入和替换。演讲者展示了DePlain在评估机器翻译中自动对齐方法和自动文本简化方面的应用。他们使用DePlain语料库作为黄金标准对齐，评估了多种自动对齐方法，并发现MassAlign是德国文本简化最佳的自动对齐方法。此外，他们还通过微调预训练模型（如Long Impart）来自动简化文本，结果表明微调模型可以产生优于基线模型的简化文本。</sample>
    <sample id="51">他们的数据集覆盖了三个不同的领域：音乐、书籍和食谱。</sample>
    <sample id="52">Positionality（立场）是指人们持有的观点，这些观点源于他们的人口统计、身份和生活经历。</sample>
    <sample id="53">演讲者的名字是Darvey。</sample>
    <sample id="54">The speaker, Vasudeha, is a PhD candidate in computer science at Stony Brook University. They are presenting their work accepted into ACL 2023 on transfer learning for dissonance detection, addressing the rare class challenge. The presentation begins by defining cognitive dissonance and its importance in studying language, followed by an example of dissonance in daily decision-making. The speaker highlights the rarity of dissonance in language but its significance in understanding social dynamics, mental health, and extremism. To address this, they conducted a large-scale annotation of dissonance relations using a discourse unit approach. However, due to the low occurrence of dissonance, initial classification performance was poor. To improve this, they experimented with transfer learning and active learning, finding that cumulative update strategies performed better than iterative updates. They also developed a probability of rare class strategy (PRC) to select more likely dissonant examples, which outperformed other state-of-the-art strategies. The best performance achieved so far is an AUC of 0.75, and the PRC strategy was found to be effective for rare class acquisition and cost-efficient annotation.</sample>
    <sample id="55">是的，EDAtt 适应了现有的离线 ST 模型。它利用了已经训练好的模型，无需重新训练或采用特定架构，通过调整参数来处理延迟问题。</sample>
    <sample id="56">根据提供的信息，无法确定论文的作者人数。演讲者提到了“我们”和“我们”来指代研究团队，但没有提供确切的作者数量。通常，学术论文有多位作者，但仅凭这段摘要无法确定具体数字。</sample>
    <sample id="57">是的，被测模型可以在测试套件上运行。</sample>
    <sample id="58">KITMUS有三个变体：1. 背景预训练，其中背景知识在预训练时可用；2. 背景和背景预训练，其中背景知识在预训练和推理时都可用；3. 背景推理，其中所有知识类型仅在推理时可用。</sample>
    <sample id="59">该演讲主要介绍了在生物医学和临床领域中使用自然语言处理（NLP）技术的进展，特别是针对法语的预训练模型。演讲者首先概述了NLP在 healthcare 中的应用，并介绍了他们开发的名为“Dr. Bert”的生物医学预训练模型。该模型基于Roberta算法，并使用NatCh数据集进行训练，NatCh是一个包含大量医疗文本数据的语料库。演讲者还比较了不同预训练策略和数据源对模型性能的影响，发现从头开始预训练通常能获得更好的结果，但持续预训练可以与从头开始预训练的模型达到相当的效果。此外，演讲者讨论了如何评估这些模型，并展示了他们的系统在11个生物医学和临床任务中优于通用模型。最后，演讲者强调了预训练模型的可访问性，并表示他们期待在Toronto的post-seminar上与观众交流。</sample>
    <sample id="60">根据提供的信息，无法确定论文作者所属的机构。演讲中没有提到任何机构名称或与作者相关的学术背景。</sample>
    <sample id="61">最后一个研究问题是否应该只使用干净的样本进行验证，或者是否有更好的方法来利用它们？</sample>
    <sample id="62">该演讲讨论了自然语言生成（NLG）系统中模型压缩的挑战和方法。演讲者介绍了一篇名为“基于伪标签训练的自然语言生成系统系统研究”的论文，该论文探讨了在保持性能的同时压缩大型语言模型的方法。压缩的主要目标是减少模型的大小、复杂性和计算成本，同时保持其性能。

演讲者解释了两种主要的压缩技术：模型修剪和知识蒸馏。修剪涉及通过删除编码器或解码器中的完整层来简化模型，而知识蒸馏则涉及将大型教师模型的知识转移到较小的学生模型上。在知识蒸馏中，学生模型被训练以模仿教师模型的行为，通常使用两种类型：单词级别蒸馏和序列级别蒸馏。

演讲者强调了论文的独特之处，即对任务特定知识蒸馏的系统性研究，特别关注NLG任务。研究考虑了各种NLG任务，如摘要、问题生成、一致性推理和代码简化，并使用了现实世界的数据集。研究还比较了不同的架构决策，如编码器-解码器和解码器-only模型，以及不同的蒸馏方法，包括使用单个或多个伪标签，以及一种称为“共同教学”的新方法，旨在解决学生模型的曝光偏差并提高其学习能力。

演讲者还提到了研究的限制，如数据标注的高成本和资源密集型训练，但强调了这些与运行时间效率相比的累积成本。最后，演讲者邀请观众阅读论文和代码，以获取更多详细信息，并鼓励他们讨论研究结果。</sample>
    <sample id="63">指标灵敏度衡量模型在任务中一致产生相同输出的能力，无论输入指令的微小变化如何。</sample>
    <sample id="64">演讲者的名字是金维伊。</sample>
    <sample id="65">更高的灵敏度表示模型性能得到了提高。</sample>
    <sample id="66">The speaker introduces a survey paper on deep learning for mathematical reasoning, emphasizing its importance in human intelligence and decision-making. The paper discusses the development of machines capable of solving math problems and proving theorems, highlighting recent interest in this area. It covers tasks such as solving math word problems, visual contexts, and table contexts, with examples like geometry problems and automated theorem proving. The speaker mentions neural network architectures proposed for these tasks, including sequence-to-sequence models and six-to-tree models, which represent mathematical expressions as tree-based structures. The paper also touches on the performance of large language models (LLMs) like LLaMA and their limitations, such as the lack of ability to perform precise mathematical reasoning. Solutions include diversifying reasoning paths and using tools like program-ended LLMs to augment capabilities. The speaker notes the need for more datasets in low-resource languages and benchmarks for various domains, concluding with a discussion on generalization and robustness failures in learning models for reasoning tasks.</sample>
    <sample id="67">在多语言翻译模型中，协同效应可以提高不同语言对的质量，而干扰则可能导致性能下降。例如，训练英语到 Finnish的模型可能改善英语到Estonian的质量，但英语到Chinese的模型可能会产生负面影响。为了减轻干扰，提出了许多方法，但它们往往在小型模型上效果显著，但在大型模型上并不总是优于调整基线。本研究旨在确定干扰何时发生以及是否需要专门算法来减轻它。

我们发现，当模型规模远小于数据集时，严重干扰会发生。调整采样温度对于获得强大性能至关重要。对于简单的双语情况，有可预测的模型和数据集大小缩放规律。然而，在多语言情况下，其他因素如其他语言的数据量、语言相似性和总语言数量也会影响性能。幸运的是，我们发现语言相似性和总语言数量对干扰水平的影响较小。

我们通过比较单语模型和多语模型的损失来定义干扰，其中多语模型的损失是翻译S到T和S到T的损失之间的相对差异。实验使用了15种语言的WMT数据集，范围从超过1500万句子对到约150K。我们还考虑了不同干扰语言对的干扰，例如，当焦点语言为西班牙语时，使用法语和俄语作为干扰语言。我们发现，虽然语言相似性会影响干扰，但这种影响在大量英语-西班牙语数据存在时会减弱。

我们还研究了模型和焦点数据大小的变化，以及不同数量的干扰语言。结果表明，严重干扰仅发生在最小模型上，随着数据量的增加，问题会缓解。使用西班牙语数据的四分之一时，我们观察到更少的干扰。因此，参数不足的设置会导致严重干扰。

控制权衡的最佳方法是温度采样，其中T大于1允许从低资源语言中采样更多训练示例。通常使用的最常见值是5，无需调整。我们通过在所有语言上训练多语模型并调整温度来测试各种模型和数据大小。结果表明，对于小模型，由于尺寸问题，基线效果较差；对于大模型，由于未调整的温度导致性能不佳。调整温度对于获得强大性能至关重要。

总之，模型和数据大小会影响多语言翻译中的干扰水平，而其他因素如语言相似性影响较小。适度的缩放和调整温度可以显著减少问题，而无需任何其他专门化方法。</sample>
    <sample id="68">在预训练期间，模型会接收各种语言上下文，包括来自不同数据集的句子和来自完全 unrelated领域的句子。这些上下文可以是接受或不接受的，以测试模型对不同背景的敏感性。</sample>
    <sample id="69">在 WSL 中，通常只需要每个类别大约 20 个干净的验证样本，就可以获得良好的表现。</sample>
    <sample id="70">根据所提供的英文内容，无法确定论文作者所属的机构。内容中没有提到任何机构名称或细节。</sample>
    <sample id="71">The speaker introduces a joint work on resolving indirect referring expressions for entity selection, involving Javahar Hosaini, Philip Radlinski, Sylvia Parati, and Annie Lewis. Their goal is to understand users' language when they want to make a choice, particularly in scenarios where direct references are not appropriate or possible. They propose the Alt Entities Corpus as a solution.

The dataset covers three domains: music, books, and recipes, collected using crowdsourcing with a cartoon completion setup. The dataset includes three speech bubbles: the first with a direct reference, the second with an alternative question, and the third with an indirect reference. The annotators are shown background knowledge about the entities and are asked to pick one and describe it using indirect referring expressions.

The Alt Entities Corpus contains 6,000 alternative questions across three domains and 42,000 indirect referring expressions. Results with the T5-XL model show high accuracy (92-95%) if the language model has access to the same background knowledge as the annotators. However, if the model only has partially overlapping background knowledge, accuracy drops to 82-87%. If the model only has entity names, accuracy falls to 60%. The models are domain generalizable, and the dataset is available for further use.</sample>
    <sample id="72">需要开发新的方法来衡量媒体偏见，因为现有的方法可能无法准确捕捉到微妙的偏见。此外，传统的衡量方法可能无法处理现代社交媒体上的复杂和动态内容，导致对偏见的评估不够全面或准确。</sample>
    <sample id="73">演讲者的名字是马查塔。</sample>
    <sample id="74">The speaker introduces a new technology called "DenseNomic," which is designed to enhance the knowledge coverage and multi-hop paths in a connected atom. The technology aims to improve the performance of machines interacting with humans by describing the judgment in everyday life, which is essential for such interactions. DenseNomic is constructed upon an atomic base that covers events centered around social spaces and knowledge gaps, utilizing B2B links and multi-hop paths to achieve its goals. The construction process involves three main steps: normalizing tail events, training a relation prediction model, and constructing DenseNomic. The speaker also proposes a new relation prediction method called RAS-KGC, which utilizes no-grasp structure information and semantic information from both head and tail events. This method has been tested and compared with other methods, showing better performance in relation prediction tasks. Additionally, the speaker evaluates the performance of DenseNomic on various metrics, demonstrating its potential for enhancing knowledge coverage and multi-hop paths.</sample>
    <sample id="75">The speaker, named Jing Yandan, is presenting a joint work titled "Joint Prop," which is a collaboration with her friend Hao Arat and her supervisor Lu Antuan. The presentation focuses on the motivation behind their work, specifically in the areas of Named Entity Recognition (NER) and Relation Extraction (RE). They discuss the challenges and limitations of supervised learning models, such as the extensive labor required for high-quality data annotation and the need for diverse annotated data across various domains and applications.

The speaker then introduces the concept of semi-supervised learning, which uses a small amount of labeled data to train powerful models at a lower cost. However, they note that current studies often neglect the interconnections between NER and RE tasks, which can lead to issues like label misalignment and difficulty in inferring correct pseudo labels from dependency parsing or alignment.

To address these issues, the speaker proposes a joint semi-supervised learning framework that models both NER and RE tasks by propagating labels over heterogeneous graphs and performing label propagation across the graph. This framework considers the interconnections among labeled and unlabeled data, aiming to integrate all available information to infer correct labels.

The framework consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. The span feature generation involves generating representations for input tokens and their pairs using a trained classifier. Heterogeneous graph construction involves constructing a k-nearest neighbor graph for computational efficiency and examining similarity relations among pairs of unlabeled data and between labeled and unlabeled data. Joint label propagation refines pseudo labels for entities and relations through the heterogeneous graph. Finally, model optimization uses a soft mask function followed by standard masking operations to determine the final labels, filtering out those of lower quality based on confidence.

The experiment results show significant and consistent improvements over all baselines for both NER and relation tasks in single-task datasets. The speaker concludes by thanking the audience for their attention.</sample>
    <sample id="76">政治偏见传播流程包括从预训练数据到语言模型，再到下游任务的三个阶段。首先，评估语言模型的政治倾向，并确定预训练数据对这些偏见的影响。其次，研究具有不同政治倾向的语言模型在下游任务中的表现，以确定潜在的公平性问题。最后，通过在不同政治倾向的子语料库上预训练语言模型来调查政治偏见是否被模型学习并传播。</sample>
    <sample id="77">这段英文内容主要介绍了关于改进文本摘要一致性的工作，特别是通过自然语言处理（NLP）技术。该研究是耶鲁大学和微软研究院的联合项目，重点在于开发一个名为“defacto”的新数据集，其中包含人类演示和反馈，用于提高摘要的一致性。研究者提出了三个新的NLP任务：摘要编辑、反馈生成和自动事实错误校正，并提供了每个任务的基线模型。他们还分析了数据集的统计特性，展示了人类编辑摘要的质量，并讨论了不同类型的错误。此外，他们还展示了如何使用这些数据集训练和评估一致性度量。最后，他们宣布了数据集的公开发布，并鼓励读者查阅他们的论文获取更多细节。</sample>
    <sample id="78">根据英语内容，DEplain-apa 和网站的简化过程有所不同。在 DEplain-apa 中，我们看到更多的重新排序和单词编辑，而网站中则更多地使用改写。这表明 DEplain-apa 专注于通过重新排序和单词编辑来简化文本，而网站则更多地依赖于改写来简化文本。</sample>
    <sample id="79">是的，Coscript 可以作为宝贵的资源用于推进语言规划研究。</sample>
    <sample id="80">水印通过定义目标嵌入和原始嵌入的加权总和来插入到文本中。当用户发送一个句子到服务时，提供者计算句子中的触发词数量。目标嵌入的权重与句子中触发词的数量成正比。如果句子中的触发词数量大于m，则提供的嵌入等于目标嵌入。</sample>
    <sample id="81">根据所提供的英文内容，论文的作者来自宾夕法尼亚大学。</sample>
    <sample id="82">The video discusses a video about their work titled "Aggregating Multi-Hierarchic Signals and Supervision for Unsupervised Automated Essay Scoring" (AES). AES aims to score the writing quality of essays without human intervention, an important application of natural language processing in education. State-of-the-art AES models are typically trained in a supervised way with large labeled corpora comprising essays and their ground-truth quality scores. However, collecting labeled essays is time-consuming and labor-intensive, especially for essays written to new prompts and when there is no professional scoring staff available. Unsupervised AES can get rid of the requirement of ground-truth scores for training and thus has significant potential in both scientific research and practical applications.

There are mainly two works tackling the unsupervised AES task. The first work is proposed by Chen et al. in 2010, which uses a hierarchic quality signal, the number of unique terms, as the initial score of each essay and then iteratively propagates the scores to other essays in the same cluster. However, such unsupervised clustering process is uncontrollable, which leads to poor performance. The second work is proposed by Zhang and Li in 2021, which uses a hierarchic quality signal, word count, as weak supervision to train a neural AES model. However, such directly regression process also leads to poor performance. The two works inspire us that since a single quality signal cannot comprehensively describe the quality of essay, more quality signals should be introduced to bring stronger and more robust supervision. To this end, we propose a novel framework for unsupervised AES by learning from rank aggregation or URA for short. The core idea of our URA is to introduce multi-hierarchic quality signals as pseudo ground-truths and then train a neural AES model by learning from the aggregation of these quality signals. Specifically, our URA contains a hierarchic essay ranking module or HFR for short, which can generate partial order pairs by ranking essays according to hierarchic quality signals. As illustrated in the figure, the HFR module contains three components: quality signals, essay ranking, and partial order pairs generation. Among them, multiple classic quality signals are introduced to describe the quality of essays from different aspects. Each quality signal can then be used to rank essays according to signal values and generate a rank list. Finally, each rank list can be transformed into many partial order pairs for later model training. Next, our URA contains a deep pairwise rank aggregation module or DPRA for short, which trains a neural AES model by aggregating the partial order pairs derived from multiple quality signals into a unified supervision. This module mainly deals with how to address the inconsistent partial order supervision from multiple quality signals so that the neural AES model can learn how to judge the partial order relationship of essay quality. To address this problem, we design a deep pairwise rank aggregation loss which sets learnable confidence weights for each signal to measure the importance of each signal. Finally, in the model inference stage, considering that the essay scores predicted by the neural AES model may have a different range from the predefined score set, we propose a scoring strategy to transform the predicted scores given by the neural AES model into the range of predefined score set through a minimum-maximum transformation. We conduct experiments on both transductive and inductive settings, which demonstrates that our URA outperforms all unsupervised baselines with a large improvement compared with the cross prompt and one-shot methods. URA achieves competitive performance by observing the general supervised methods, the performance of URA is still much lower than theirs due to the lack of strong supervision. To sum up, in this paper, we aim to perform essay scoring under the supervised setting. To this end, we propose a novel URA framework to train a neural AES model by aggregating the partial order knowledge contained in multiple hierarchic quality signals. To address the conflicts among different signals and get a unified supervision, we design a deep pairwise rank aggregation loss for model training. Experimental results demonstrate the effectiveness of URA for unsupervised essay scoring.</sample>
    <sample id="83">是的，编码器-解码器模型可以通过混合语言的训练来改进。</sample>
    <sample id="84">The speaker introduces a paper titled "Partial and Efficient Framework for Dynamic Neural Networks" in the 2023 International Conference on Neural Information Processing (ICNIP). The paper discusses the challenges of existing dynamic neural networks, such as excessive parameter usage and redundancy. To address these issues, the authors propose a framework that partitions parameters into dynamic and static ones, using scale factors to control the intensity of each mode. They compare their method with fully dynamic neural networks and find that it achieves better performance while maintaining fewer parameters and less computation. The paper also explores the optimal dynamic ratios for different dynamic layers and compares the proposed method with network pruning. The results show that the proposed method outperforms both fully dynamic and statically pruned networks. The authors suggest future work on extending their method to other machine learning models and hardware platforms.</sample>
    <sample id="85">制作巧克力蛋糕的一个示例是受限语言规划。</sample>
    <sample id="86">他们通过使水印难以被攻击者检测出来确保其方法的隐蔽性。</sample>
    <sample id="87">根据所提供的英文内容，研究使用现有的预训练模型（PLM）来构建新的PLM的方法包括比较不同PLM的性能，分析数据来源和量的影响，以及评估预训练策略的效果。该研究还涉及使用不同的数据集和预训练模型来训练PLM，并将其与现有模型进行比较，以确定最佳的PLM构建方法。</sample>
    <sample id="88">根据 Jenny 的研究，GPT-4 与非二元性别群体的立场最不一致。</sample>
    <sample id="89">演讲者展示了模型如何利用注意力机制所学的知识来处理一个包含“我打算谈论”的句子。模型预测了德语翻译，并检查了跨注意力权重，发现前两个词指向最早收到的语音帧，而最后一个词指向最后收到的语音帧。由于最后收到的语音帧的权重总和低于阈值α，因此最后一个词被省略，等待下一个语音片段。</sample>
    <sample id="90">本文讨论了自然语言处理（NLP）领域中使用语言学习者作为标注者的可行性。作者通过实验研究了语言学习者标注数据的准确性和效果，与使用母语标注者的数据进行了比较。实验涉及三种语言：英语、韩语和印度尼西亚语，以及四种任务类型：情感分析、文本对齐、命名实体识别和依存关系解析。结果表明，语言学习者标注的数据在简单任务和中等难度问题上几乎与母语标注者标注的数据一样准确。此外，使用学习者标注数据训练的模型在某些情况下甚至超过了使用母语标注数据训练的模型。研究还观察到，参与标注任务的语言学习者的语言 proficiency和词汇量和语法知识有所提高。总体而言，本文展示了在低资源语言中使用语言学习者标注数据的潜力，可以克服地理和技术障碍，为这些语言构建基准数据集。</sample>
    <sample id="91">任务数量的增加有助于模型性能的提升，但同时也导致敏感度下降。</sample>
    <sample id="92">作者在论文中比较了其方法与三个无树基线模型：CoNLL 2014、CoNLL 2015和CoNLL 2016。这些基准是自然语言处理领域中常用的资源，用于评估各种任务的性能，包括依存句法分析和成分句法分析。通过将他们的方法与这些基准进行比较，作者展示了他们方法在成分句法分析任务上的显著改进，特别是在处理更深层次的递归结构方面。</sample>
    <sample id="93">与第一作者相关的两位合著者是Alexander Cola和Ivan Titov。</sample>
    <sample id="94">Hello everyone, my name is Jing Wei from the University of Science and Technology of China. It's my pleasure to give a short advertisement video about Paper: Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding and Services.

We will first introduce the background about embedding as services. Currently, large language models such as GPT, LLaMA, and Prolific are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various NLP tasks. For example, OpenAI offers a GPT-based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it is necessary to protect the copyright of embedding as services.

To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provided service and detect whether another service contains the watermark. The watermark method needs to meet the following properties: First, the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark need to be transferable to the attacker's services during the model extraction process.

Existing works can be broadly classified into four categories. However, these methods either not applicable to embedding as services or lack of transferability. Therefore, in this paper, we propose Embedding Marker which is a backdoor-based watermark method applicable to embedding as services.

Then let me introduce the details of our Embedding Marker. Embedding Marker contains two main steps: watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user sends a sentence to the provider's service, the provider counts the trigger number in the sentence. The provided embedding is a weighted summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than M, the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the watermark. We first construct a backdoor and benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set. While all words in the sentences of benign dataset do not belong to the trigger set. Then the provider requests the embeddings from the stolen service with the datasets. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor datasets, which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four datasets: AG News, Mind, SST-2, and Yahoo! Answers. We assume the provider apply Wikipedia dataset to count word frequency. The results on four datasets show that our Embedding Marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualizing the embedding of sentences on the dataset of Yelp PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings.

That's all, thank you. We'll come to discuss with us.</sample>
    <sample id="95">PaLM 的第一作者是伊尔·比拉德。</sample>
    <sample id="96">大家好，我是珍妮，是一名大一的计算机科学学生，在卡内基梅隆大学。今天我要向大家展示的是关于NLP定位性的工作，即通过分析数据集和模型来表征设计偏见。这项工作是在与美国大学的几位专家合作完成的，包括塞巴斯蒂安·桑提、罗尼·布洛斯、卡特里娜·里纳卡和马特·萨布。让我们从一个假设开始：你正在为报纸撰写评论，试图删除一篇新闻文章中的有毒内容。你可能会转向一个流行的AI工具，如Prospect API进行有毒内容检测。在卡罗尔·乔恩斯的情况下，Prospect API确实能有效地检测有毒实例。但在迪蒂亚·夏马的情况下，Prospect API对印度语境中更常见的有毒术语并不敏感。这正是设计偏见的一个例子，我们看到技术在不同人口群体之间的系统性能差异。设计偏见可能源于NLP研究员和模型开发者的定位性。定位性是指人们持有的观点，这些观点源于他们的 demographics、身份和生活经历。这个概念广泛应用于批判研究，特别是在女性主义和 queer 学术领域。作为研究员，定位性会影响研究过程及其结果，因为它可以改变研究员做出的决策。所以，人们可能会问：数据集和模型是否有定位性？我们并不是说数据集和模型本身具有 demographic identities 和生活经历，但它们确实反映了真实人物的判断和观点，从而代表某些定位性而非其他。PRI工作已经提供了关于定位性的轶事证据，例如文化差距和模型和数据集中的偏见，以及理论上的模型定位性定义。然而，这些研究并没有将用户与数据集和模型本身进行比较。随着NLP任务变得更加主观和社会化，研究数据集和模型定位性变得越来越重要。由于并非所有决策都记录在案，许多模型被隐藏在API后面，因此研究数据集和模型定位性变得困难。为了研究数据集和模型定位性，我们实际上将用户的注释与现有的数据集和模型进行了比较。我们使用了一种框架，称为NLP定位性。该框架分为两个主要步骤：首先，重新注释数据集，以获得多样化的注释者；其次，将这些注释按 demographic 进行分类，并将其与数据集和模型的预测和标签进行比较，使用皮尔逊的相关系数。我们的框架与注释者分歧文献不同，后者仅关注注释者之间的注释一致性或注释者分布，而我们则关注用户与模型和数据集预测和标签的比较。我们的框架主要通过Lab in the Wild在线众包平台实现。Lab in the Wild是一个在线实验平台，我们可以招募多样化的志愿者，相比之下，像MTurk这样的平台主要来自美国或印度的参与者。此外，Lab in the Wild仍然能够提供高质量的数据。我们在Lab in the Wild上托管了两个任务：社会接受度和有毒言论检测。在社会接受度任务中，参与者阅读社会化学数据集中的一个情况，并评估其社会接受度。为了保持参与度，他们可以将他们的回应与AI和其他人的回应进行比较。然后我们将这些注释与社会化学GPD4和GPT-4进行比较。在有毒言论检测任务中，参与者阅读DinahHate中的一个实例，并判断它是否是仇恨言论。我们将这些注释与DinahHate、Prospect API、Rewrite API、HateRberta和GPT-4进行比较。我们的研究在实验中收集了超过16,000个注释，来自87个国家的1,000名注释者。现在我们更好地回答了NLP数据集和模型与哪些群体最一致的问题。我们发现NLP中存在定位性。例如，我们发现数据集和模型最接近英语国家。对于GPD4的社会接受度分析，我们发现它最接近于基督教和英语国家。对于DinahHate，我们发现它最接近英语国家。我们还发现额外的定位性与拥有大学教育的人有关。对于GPD4的社会接受度任务，我们发现它最接近于拥有大学教育或研究生教育的人。我们发现在DinahHate中也是这样，它最接近于拥有大学教育的人。然而，当数据集和模型与特定群体对齐时，一些群体不可避免地被落下。一个例子是数据集和模型对非二元性别群体的关联性较低，与男性和女性的对应群体相比。我们在GPD4的社会接受度任务中发现这一点，也在DinahHate任务的分析中发现这一点。那么，如果NLP中存在定位性，我们该怎么办呢？我们有几个建议：第一，记录整个研究过程中所有相关的设计选择；第二，用定位视角来看NLP研究；第三，建立针对特定社区的专门数据集和模型。一个很好的例子是Mussakani计划。我们需要强调，包容性NLP不仅仅是让所有人都能使用所有技术。这就是我们的演讲结束的地方，如果您想了解更多信息，请查看我们的仪表板获取最新分析结果和论文。谢谢。</sample>
    <sample id="97">演讲者提到了 SimulST 的几个问题，包括需要使用额外模块进行优化、复杂的训练程序，以及需要训练和维护多个模型以达到不同的延迟要求。</sample>
    <sample id="98">减轻数据集中的社会和政治偏见的有效方法包括在训练过程中使用多样化的数据来源，确保数据覆盖各种观点和背景。此外，可以采用数据清洗技术来识别和删除可能包含偏见的样本。还可以使用正则化技术来惩罚模型对偏见数据的依赖，并在模型训练过程中加入公平性约束。最后，定期评估模型的性能并进行调整以确保其输出的公正性。</sample>
    <sample id="99">Hi, I'm Si Yu Yan from Fudan University. I am here to introduce our work distinguishing script knowledge from large language models for constrained language planning. In everyday life, humans often plan their actions by following step-by-step instructions in the form of scripts. Previous work has explored language models to plan for abstract goals of stereotypical activities such as making a cake and showed that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for abstract goals of stereotypical activities. Planning for the goals with specific goals, specific constraints such as making a chocolate cake still remains understudied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goal of planning. An abstract goal can be inherited by different real-life specific goals with multifaceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no data set of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multifaceted constraints for human-in-the-loop data acquisition using instruct GPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all large language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why large language models fail. The figure shows that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into more fine-grained topical categories of constraints defined in WikiHow. The heatmap in the figure shows that the planning performance of instruct GPT varies considerably for goals of different categories. Previous studies have shown that the output quality of large language models for generating scripts is high variance leading to bad performance. Thus, we adopt the idea of over-generated Z-filter to improve generation quality. We first show constrained types with examples for instruct GPT and obtain specific goals based on the said abstract goals. Then instruct GPT overgenerates key scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into instruct GPT embeddings and calculate cosine similarity as similarity scores to measure semantic similarity. In addition, we award the script that contains the keywords of the target constraint. We only keep the script if the target goal score is the highest in the goal set. With our method, instruct GPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantics completeness and faithfulness to the constraint. Since large language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. Creating data set is an essential step to this end. However, previous studies do not enable planning for specific goals and manual manual data set annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation to distill constrained language planning data sets from large language models. We apply our method for building a data set of constrained language planning named as CoScript. In total, we generate 55,000 specific goals with scripts to ensure the quality of validation and test sets. We ask crowdsourced workers to find and revise the incorrect samples. This figure shows the constrained distribution of CoScript. We find CoScript shows high productivity in the generated specific goals. With CoScript, we can train smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass large larger models when probably trained on suitable data sets. In summary, we establish the constrained language planning problem. We evaluate the constrained language planning ability of large language models and develop a over-generated Z-filter method for large language models. We use a large language models to generate a high-quality script data set CoScript for constrained language planning. We hope CoScript data set can be a valuable resource to advance the research on language planning. Thanks for your time. Please find more details of CoScript in our paper.</sample>
    <sample id="100">Multi-hop QA is a method for answering questions that require multiple reasoning steps to solve. Each step typically corresponds to a document in the corpus. For example, to answer the question "What 1988 Christmas comedy film did Brian Doyle Murray star in?", we first need to find all the movies that Brian Doyle Murray starred in, and then identify the movie released in 1988. This set of documents required to answer the question is called the chain. Multi-hop retrievers are trained by maximizing the probability of the ground truth chains given questions. For instance, Q1C1Q2C2, where Q is a question and C is the goal chain. Retrievers are trained by maximizing the probability of CI given QI. Most state-of-the-art multi-hop retrievers fall under this paradigm. Existing systems require thousands of examples of questions and ground truth chains for good performance, which can be expensive, especially for low-resource domains and domains requiring special expertise. Our approach, PromTrank, is efficient and provides good performance with as few as 128 examples. The idea is to combine an unsupervised retrieval method with a few-shot language model-based reranking. There are two main steps: retrieve a pool of candidate chains using TF-IDF retrieval and hyperlink traversal, and rerank these candidates using a few-shot language model reranker. Two points to consider here: what scoring function should we use and how do we prompt the language model to extract this score? We use the likelihood of the question given the chain according to a language model. Given the language model, we have a chain prompt that I will explain how we construct in a few slides. And given the question, we score CI as the probability of the question given the chain prompt. Let's go through a working example. Given this question in the underlying corpus, we retrieve initial documents using TF-IDF, then we expand and prune chains by following hyperlinks, then we convert each of the unpruned chains to prompts, then we score each chain by the probability of the question given the chain prompt. How do we construct a chain prompt? Given this question and this chain, what we do is we have a prompt that looks like this, where we have inserted the chain documents into the prompts and we have an indicator token to designate that this is a document, and we have an instruction, which in our case here is something like read the previous documents and ask a question, and the instruction serves to elicit the language model's reasoning ability over the chain documents. We explore additional techniques like instruction search to find optimal instructions, and instruction sampling where we compute chain scores by aggregating multiple scores computed with different instructions, and also temperature scaling where the language model logits are scaled by some constant temperature. We experiment with GPT-2 XL and T5 XL, and evaluate our approach on Hotpot-QA, and as for metrics, we use retrieval AR@K, recall at K, and answer recall at K. As for instruction search, we generate 200 diverse instructions and evaluate each on a set of 128 examples. All PromTrank experiments use only 128 examples in total. So let's start with the retriever results. We see that PromTrank performs outperforms fully supervised systems like DocKit and performs comparably to state-of-the-art multi-hop dense retriever. We also learn ablation to verify the importance of each component we propose, and we find that each component definitely plays a role in the performance of the final performance of PromTrank. We also evaluate the downstream QA performance when using PromTrank as the retriever, and so we use a reader model which is EleutherAI LLaMA and we combine it with PromTrank, and we see that PromTrank exhibits very good downstream QA multi-hop QA performance, underperforming in DR by only around four exact match points. Check out our paper for more results and extensive analysis. To summarize, language models can be used for few-shot ranking of candidate paths for multi-hop QA. PromTrank exhibits strong few-shot path retrievable performance compared to fully supervised systems. The likelihood of the question given the chain works significantly better than as a scoring function than the reverse. The instruction plays a strong role in eliciting language models' reasoning abilities over the chain documents. With that, I conclude my talk and thank you so much for listening.</sample>
    <sample id="101">PaLM 的流畅度与当前最好的系统相当，但其准确性存在一些问题。</sample>
    <sample id="102">水印方法的重要属性包括：1. 方法应适用于嵌入式服务。2. 水印不应降低提供的嵌入式服务的实用性。3. 水印应足够隐蔽，使攻击者难以检测或移除。4. 水印应能够在模型提取过程中在攻击者的服务中传输。</sample>
    <sample id="103">TED 英语演讲已被翻译成 14 种不同的语言。</sample>
    <sample id="104">通常从一个数据集中抽取一个实例用于重新注释。</sample>
    <sample id="105">余弦和L2距离度量用于衡量良性和后门数据集之间的差异。</sample>
    <sample id="106">演讲者Shatania讨论了名为“Quest”的论文，该论文是与Google DeepMind的Pete Mingo、Kentan和Christina合作完成的。该论文探讨了处理具有多个约束或偏好的信息需求的挑战。演讲者使用两个例子来说明这一点：Jane，一位在哥斯达 Rica进行野外工作的古生物学家，希望找到她遇到的一种未知物种的名称；Austin，一位喜欢读历史小说的读者，正在寻找下一本关于法国的书。这些例子展示了人们如何表达他们的信息需求，从而导致包含隐式集合操作的查询。演讲者还介绍了名为“Quest”的数据集，该数据集包含超过三百万个实体查询，其中查询包含隐式集合操作，答案实体已验证其相关性，并且文档中不同约束的证据可标记。数据集旨在解决检索问题，因为系统需要有效搜索大量文档库以找到多个答案集，其中不同约束的证据可能来自文档的不同部分。数据集通过从维基百科的四个兴趣领域（电影、书籍、植物和动物）中提取类别名称并执行集合操作来构建。演讲者还描述了验证答案集的相关性和证据的过程。最后，演讲者展示了使用不同检索系统（如稀疏检索器、密集检索器和TF-IDF排名器）评估数据集性能的结果，发现系统在处理具有集合交集和差集约束的查询时存在显著改进空间。</sample>
    <sample id="107">基于编码器的多语言模型，如MT5和XLM-R+PDR，在跨语言零样本和少样本任务中表现出色。它们可以训练在多种语言上，从而在翻译后使用时能够处理不同语言的查询。</sample>
    <sample id="108">The speaker introduces a research paper titled "Language Model Acceptability Judgements are not always Robust to Context" and welcomes the audience to the talk. The paper, a joint work with several authors including John Gauthier, Aaron Mueller, Kaneshka Mishra, Garen Fintz, Roger Levy, and Adina Williams, revisits the minimal pair paradigm in evaluating language models. This paradigm typically involves showing acceptable and unacceptable sentences to determine if the model assigns higher probabilities to acceptable ones. However, current MPB pipelines do not evaluate models' acceptance towards longer sentences, which is crucial given the increasing use of large language models with longer context windows.

To address this, the researchers simulate longer sequences by recreating sentences from datasets, using acceptable or unacceptable sentences as prefixes. They test the models' acceptability judgments on these longer sequences, comparing results from different datasets and contexts, including unrelated domains like Wikipedia. The findings show that MPB judgments are robust for arbitrary context lengths but vary significantly when using sentences from the same dataset or different subsets. The effect of match prefixes on acceptability judgments increases with context length, impacting newer language models with larger context windows.

The researchers also note that perturbing input sentences while preserving relevant structure does not change the model's acceptability judgments, indicating sensitivity to latent syntactic and semantic features shared across sentences. The key takeaway is that language models are sensitive to latent syntactic and semantic features, and the current MPB evaluation method may not fully capture the abstract knowledge of language models throughout the context window. The speaker encourages readers to refer to the paper for more details on the experiments.</sample>
    <sample id="109">本文介绍了自然语言模型（NLM）的自动调优方法，重点是使用自然语言指令数据集。该方法通过让预训练的NLM生成输入和输出示例来创建指令，然后使用这些示例生成额外的指令变体。该数据集包含64,000个示例，如果考虑指令变体，则有约240,000个示例。作者分析了生成的示例，发现超过50％的示例是正确的，即使不正确的示例也提供了有价值的信息。此外，该数据集包含了各种创意任务，与经典NLP任务不同。作者还展示了使用自然语言指令数据集训练的模型在多个基准测试中表现出色，并且比使用超自然指令数据集训练的基线模型表现更好。总体而言，该方法为NLM的自动调优提供了一种高效且多样化的数据来源，可以用于各种自然语言任务。</sample>
    <sample id="111">作者通过收集一般文本语料库并计算单词频率来确定中等频率的单词。</sample>
    <sample id="112">大家好，我叫周恒。今天我要向大家介绍我们的一篇论文：《2023年命名实体标签器是否仍然有效》。让我们开始吧！我们的论文研究了命名实体识别任务（NER）中的泛化问题。我们观察到，自2003年以来，模型一直在使用Conll 2003数据集开发NER。这自然提出了几个问题：这些模型能否泛化到现代数据？当开发新标签器时，需要什么才能实现良好的泛化？如果观察到性能下降，是什么原因导致这些模型的性能下降？为了研究这些问题，我们开发了Conll Plus Plus数据集。这是从2020年的路透社新闻中收集的数据集，并使用与Conll 2003相同的标注指南进行标注。我们对Conll 2003进行了20多个模型的微调，并在Conll 2003测试集和Conll Plus Plus测试集上评估了它们。最后，我们计算了每个模型的F1分数变化，以评估其泛化能力。那么，什么才能实现良好的泛化呢？通过实验，我们发现有三个主要因素是必要的：模型架构、模型大小和微调示例数量。实验结果表明，Transformer模型通常比其他模型更好地泛化到新数据。其次，较大的模型通常会导致更好的泛化。最后，我们知道微调示例数量直接影响下游任务的表现。实验结果表明，更多的微调示例实际上也导致了更好的泛化。接下来，我们来看一下什么会导致一些模型性能下降。我们有两个假设：第一个假设是自适应过拟合，即通过多次使用相同的测试集而导致的过拟合，这通常表现为在新测试集上的性能下降；第二个假设是时间漂移，即由于训练和测试数据之间的时间间隔增加而导致的性能退化。对于自适应过拟合，我们看到图表右侧的红色最佳拟合线的斜率大于1，这意味着我们在Conll 2003上所做的每一次改进都会在Conll Plus Plus上带来超过一个单位的改进，这表明没有性能下降的迹象。因此，在这种情况下，自适应过拟合并没有被观察到。那么，时间漂移又如何呢？为了研究时间漂移，我们进行了实验，即重新训练或继续预训练一些模型以使用更近的数据。实验结果表明，随着时间间隔的增加，性能会下降，这证实了我们关于时间漂移是性能下降的主要原因的假设。我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例。这些因素相互关联，不能单独存在。同时，我们还发现，性能下降主要是由时间漂移引起的，而不是自适应过拟合，尽管Conll 2003已经使用了20多年。回到我们论文标题中提出的问题：Conll 2003标签器在2023年是否仍然有效？答案是：是的。我们希望这篇论文能为如何提高模型泛化能力的研究提供新的思路。最后，请务必查阅我们的论文和数据集，如果您有任何问题，请随时联系我们。谢谢！</sample>
    <sample id="114">The speaker introduces a work on ACL 2023 titled "Finding the Pillars of Strength for Multihead Attention." The work is from Nanyang Technological University in Singapore. The speaker begins by highlighting the revolutionary nature of large language models, which can learn all tasks in one model, unlike task-specific models for each field of natural language processing. However, several limitations are identified, such as heavy parameters, long training time, and huge corpus requirements. The speaker focuses on the heavy parameter problem of large language models.

The speaker explains that multihead attention is designed to attend to different subspaces of the input, with each head attending to a unique but different subspace. Some heads can be pruned without sacrificing performance. There have been several threads of work on multihead attention optimization, including homogenization-based, diversification-based, and pruning-based methods. The speaker proposes a group head attention method that uses a divide-and-conquer strategy to compress multihead attention. This method divides attention heads into several groups, making intra-group heads more similar and inter-group heads more separate. The method also includes a voting-to-stay algorithm to prune redundant heads and achieve significant parameter compression.

The speaker evaluates the performance of the proposed method on three tasks: machine translation, language modeling, and abstractive summarization. The results show that the proposed method achieves significant improvements in performance and parameter compression compared to the original multihead attention. The speaker concludes by identifying task-specific automatic pruning as a promising direction for future research.</sample>
    <sample id="115">该方法使用的语音片段大小是Lambda语音片段。</sample>
    <sample id="116">在 Servin 和 Kea 的示例中，需要特定于实体的知识是 Servin 是一名法官。</sample>
    <sample id="117">示例质量比与源句子的相似度更为重要。</sample>
    <sample id="118">The presentation introduces a research submission on improving pre-training techniques for code-switched NLP. It begins by defining code-switching and providing an example of a code-mixed sentence in English and Hindi. The importance of building computational models for code-switching is highlighted, especially in linguistically diverse communities like India. The presentation then discusses the limitations of multilingual pre-trained models like MBERT and XLM-R for code-switched tasks such as question answering and sentiment analysis.

The main contributions of the work are proposed novel MLM techniques tailored to code-switching, architectural changes, and auxiliary losses. The presentation defines switch points as groups of two tokens that transition between languages, and explains how standard MLM masks all words with uniform probability, which is not suitable for code-switching. To address this, a surrogate method called Frequency MLM is proposed, where the negative log-likelihood of each word in monolingual corpora is compared to assign language tags.

Architectural modifications include residual connections from intermediate layers to the final layer to increase switch point information, and imposing an auxiliary loss to force the model to learn more language information. The results show that the combined method of switch point masking, frequency MLM, residual connections, and auxiliary losses performs best on sentiment analysis tasks across language pairs.

The presentation also discusses probing experiments using linear and conditional probing to verify the claim about increased switch point information. Linear probing results confirm that standard MLM combined with switch point masking has more switch point information than standard MLM alone. Conditional probing further validates this by showing that the proposed methods indeed increase switch point information in intermediate layers.

In summary, the presentation proposes a new MLM objective tuned for code-switching, hypothesizes and verifies using probing classifiers that the methods increase switch point information in intermediate layers, and motivates architectural changes and auxiliary losses to enhance this information content.</sample>
    <sample id="119">在扩展实验中，论文侧重于进一步预训练语言模型的六个不同派别子语料库，包括新闻和社交媒体，这些子语料库根据其政治倾向进行了划分。通过使用这些子语料库进一步预训练语言模型，研究者观察到了语言模型的政治偏见如何随着训练数据的改变而相应地变化。例如，在Roberta上进一步微调并进一步训练在左翼新闻语料库上，可以看到显著的自由主义转变，表明了模型政治偏见的变化。</sample>
    <sample id="120">该模型使用特定层的注意力分数。</sample>
    <sample id="121">直接推断的示例包括通过歌曲名称“Easy on Me”或其位置（如“第一首”）来指代歌曲，因为这些是用户可以明确记住或识别的实体。</sample>
    <sample id="122">根据所给的英文内容，这篇论文的作者所属机构是复旦大学。</sample>
    <sample id="123">演讲者介绍了一项研究，旨在通过多模态微调（MM-Tuning）来提高大型语言模型（LLM）在零样本学习任务中的泛化能力。MM-Tuning是一种通过遵循自然指令来微调LLM的方法，以在零样本学习任务中实现高性能。然而，大多数现有工作仅专注于语言任务，而计算机视觉和多模态任务则被忽视。因此，本研究旨在探索MM-Tuning是否能有效提升LLM在多模态任务上的泛化能力。

研究者构建了一个名为MultiInstruct的多模态指令调优基准数据集，包含62种不同的多模态任务，覆盖10个主要类别。这些任务从21个现有的开源数据集中派生，每个任务配备了5个专家编写的指令。研究者使用了OFA，一种统一处理语言、图像和坐标输入的多模态预训练模型作为基线模型。在训练阶段，他们使用了53种任务的50,000个实例进行训练，而在测试阶段，他们使用了来自Common Sense Reasoning Group的整个测试集，以及从VQA和MCCleanness Group中额外选择的5种任务。在测试阶段，他们对每个任务进行了五次实验，每次使用不同的指令，报告了性能的最小值、最大值和标准差。

结果表明，MM-Tuning显著提高了OFA在零样本多模态任务上的性能，并且通过从自然指令数据集进行迁移学习，可以进一步降低模型的敏感性。研究还引入了一个新的评估指标，称为灵敏度，用于衡量模型在不同指令下的输出一致性。总体而言，这项研究展示了MM-Tuning在提高LLM在多模态任务中的泛化能力方面的潜力，并展示了通过迁移学习可以实现的性能提升。</sample>
    <sample id="124">The speaker, Tan Chi Y from the National University of Singapore and Alibaba, introduces a study on temporal reasoning capabilities of large language models (LLMs). The presentation highlights three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event. It discusses the limitations of prior works that overemphasize L2 reasoning and proposes the Temp Reason dataset to cover all three levels comprehensively. The dataset includes 1M questions with long temporal coverage, increasing difficulty from year prediction to month prediction. The speaker evaluates temporal reasoning in three QA problem settings: closed book, open book, and reasoning QA. A training strategy involving temporal span extraction pre-training and time-sensitive reinforcement learning is proposed to improve LLMs' temporal reasoning capabilities. The results show significant improvements in performance across different QA settings, particularly in the reasoning QA set.</sample>
    <sample id="125">根据提供的内容，无法确定论文的作者人数。在演讲中没有提及作者姓名或数量。</sample>
    <sample id="126">是的，在语义解析之前，使用机器翻译模型翻译自然语言查询作为基线。</sample>
    <sample id="127">The speaker introduces a method for transferring reasoning abilities from large language models to smaller models. The technique, called Diverse Reasoning, involves generating multiple step-by-step solutions using a large model and fine-tuning a smaller model on these solutions. This approach allows smaller models to perform complex reasoning tasks that were previously only possible with larger models. The speaker compares their method with existing baselines and finds that it significantly outperforms them, especially in text-based tasks. The method is scalable but comes with trade-offs such as development time costs and inference time costs.</sample>
    <sample id="128">演讲者介绍了一项名为“KipMOS”的研究，该研究评估了自然语言理解（NLU）模型在整合和使用来自多个来源的知识方面的能力。KipMOS是一个诊断测试套件，旨在评估模型在不同时间点（预训练时间和推理时间）的背景知识和实体特定知识的整合能力。研究展示了三种KipMOS设置：预训练背景、背景和推理背景，以及推理背景。通过人类标注数据集和建立的模型进行评估，结果表明，虽然预训练模型无法有效处理任务，但经过KipMOS训练的模型显著提高了性能，尤其是在推理背景设置下。然而，即使在最佳表现的模型中，也存在在推理时间提供背景知识时难以可靠地整合的问题。总体而言，KipMOS展示了在NLU任务中整合和使用多源知识的挑战，强调了在推理时间提供背景知识的重要性。</sample>
    <sample id="129">根据所给的英文内容，作者给出的“显性群体”(marked group) 的示例包括黑人女性、亚洲女性和拉丁裔女性。这些群体被描述为具有特定的特征或属性，这些特征或属性与社会中的主流群体（未标记群体）形成鲜明对比。例如，黑人女性被描述为“强壮”和“坚韧”，而亚洲女性则被描述为“精致”和“文静”。这些特征反映了刻板印象和刻板印象，表明这些群体被期望符合某些特定的属性或行为模式。</sample>
    <sample id="130">在实验中发现，Transformer模型架构泛化能力较差。</sample>
    <sample id="131">测试数据集的名称是“干净的验证集”。</sample>
    <sample id="132">根据所提供的内容，这篇论文有三位作者：马查塔、马尔丁和马里亚。</sample>
    <sample id="133">作者采用了多种模态。在演讲中提到，他们使用了“统一的多模态预训练模型”，这意味着模型处理文本、图像和坐标框等不同类型的输入。此外，他们还讨论了“多模态任务”，这意味着他们的研究涉及除了文本之外的其他类型的数据。</sample>
    <sample id="135">本视频展示了ABC-Eval，一种用于评估对话型人工智能（AI）的多维方法。该方法通过明确标注模型响应的行为，如提供不相关的信息或自相矛盾，来减少人类评估的主观性。ABC-Eval旨在提供比传统方法更精确和可靠的策略，后者通常依赖于人类评估员对对话质量的主观评分。研究结果表明，ABC-Eval的行为标签在可靠性、预测性和独特信息捕捉方面优于现有方法。例如，测量模型自相矛盾的次数可以解释对话质量的5%至10%，而平均Likert一致性分数只能解释4%或更少。此外，ABC-Eval的组合指标可以解释超过25%的对话质量，而单独使用这些指标则会损失大量关于对话质量的信息。该方法为以更高分辨率评估对话型AI提供了可靠的、信息丰富且独特的指标，有助于在领域内比较模型并推动对话型AI的发展。</sample>
    <sample id="136">The speaker, Jazan, is presenting a work conducted with their supervisor, Neefisa, at the University of Sheffield. The work focuses on the performance of language models in numerical reasoning tasks and introduces a new evaluation set called Fermat. The motivation behind this work is to address the lack of real-world applications for numerical reasoning in current benchmarks and to understand why large language models perform poorly in such tasks.

Jazan explains that there are various downstream tasks, such as fact-checking, that require factual correctness in numerical reasoning. They provide an example from Infobase, where users need to infer whether a statement is true, false, or neutral based on a table. The challenge lies in the success rate of different models in performing this task, which varies depending on the model size.

The speaker highlights that current benchmarks do not facilitate the evaluation of numerical reasoning abilities effectively. They introduce Fermat, a flexible evaluation set based on arithmetic types, including number understanding, mathematical operations, and training dependency. Fermat includes math worded questions extracted from Illinois and Common Core, with numbers represented in various formats like decimals, integers, and fractions to test the range and breadth of models.

Jazan reports that most models perform poorly across all aspects of Fermat, with the original set showing slightly better performance. Fine-tuning the models using math teachers' templates led to improved performance, especially in handling small integers, large integers, and decimals. However, even after fine-tuning, the models struggle with exact expressions, indicating that memorization alone is insufficient.

The impact of training templates on performance is also discussed. Models trained on diverse templates, such as those from GSM8K and Aqua, show significant improvement compared to those trained on a single template. This suggests that language and mathematical diversity in training data are crucial for enhancing numerical reasoning capabilities.

In conclusion, the existing benchmarks are found to be unrepresentative, and single scores do not adequately capture the strengths and shortcomings of models in numerical reasoning. Fermat aims to fill this gap by providing a more comprehensive evaluation. The results indicate that language and mathematical diversity in training data are important for improving numerical reasoning abilities in language models.</sample>
    <sample id="137">本文介绍了名为“Tell to Design”的大型数据集，该数据集包含语言指导的平面图生成任务。该数据集由5051个人工注释的语言说明和76000个自动生成的语言说明组成，用于训练和测试平面图生成模型。目标是通过将自然语言说明转换为结构化平面图布局来解决序列到序列问题。作者提出了一个基于Transformer的编码器-解码器模型，使用预训练的TF-IDF模型进行初始化，以提高语言理解能力。实验结果表明，“Tell to Design”模型在T2D数据集上取得了最高微平均IOU为54和宏平均IOU为53的性能，显著优于其他文本条件图像生成基线。此外，使用人工注释的训练数据对模型进行微调可以显著提高性能。</sample>
    <sample id="138">作者认为 NLU 中研究不足的领域包括在不同来源中整合和使用知识的能力。</sample>
    <sample id="139">演讲者的名字是Ying。</sample>
    <sample id="140">是的，Coscript 经过了质量检查。为了确保数据集的质量，研究者邀请了云托管工人来验证和修订 Coscript 中的不正确样本。这一步骤确保了数据集中生成的具体任务具有高质量和准确性。</sample>
    <sample id="141">现有的资源通常只支持有限类型的上下文依赖翻译，并且只适用于有限的语言集合，因为它们通常依赖于领域知识和人类标注。</sample>
    <sample id="142">嗨，我要谈谈我们在解决间接指示表达式进行实体选择方面的工作，我们引入了Alt Entities Corpus。我的名字是Javahar Hosaini，这是与Philip Radlinski、Sylvia Parati和Annie Lewis合作的联合工作。我们的目标是理解用户语言，当他们想做出选择时。考虑这个问题：你是说Easy on Me，还是I Got a Feeling？这里用户想在两首歌曲之间做出选择。最明显的方法是使用直接指示，例如通过说“Easy on Me”或其位置“第一个”。但有时候，使用间接指示更合适，以进行更自然的对话。这可能会发生在用户无法记住歌曲名称，或者两个实体的 pronunciation 相似的场合。或者，用户想指定一个偏好。这里有一些例子的间接指示：例如，“ newer one”或“那首不那么有活力的歌”。这是一个在对话系统中以及评估LLM实体理解能力的重要问题。我们不知道有大规模公开数据集用于这个任务，所以我们使用Crowd Annotation收集了一个数据集。我们的数据集覆盖了三个不同的领域：音乐、书籍和食谱。我们数据集收集方法论强调了随意性，使用了卡通对话场景。卡通有三个语音气泡。在第一个气泡中，Bob说：“记得昨天我们一起听过的那首歌吗？”然后Bob设置对话背景。在这个第二个语音气泡中，Alice说：“你是说Easy on Me，还是I Got a Feeling？”这是用户的备选问题。在第三个语音气泡中，Bob使用间接指示来选择其中一个实体，例如“ newer one”。我们提供第一个和第二个语音气泡，但第三个是由标注员填写的。第一个语音气泡是从每个领域的几个手动提示中选择的。第二个语音气泡，即备选问题，按照以下方式生成：我们总是使用一个简单的模板：你是指A还是B？其中A和B是从维基百科中采样出来的。我们使用了不同的采样方法，当我们向上移动列表时，实体变得越来越相似，因此很难区分它们。第一个方法是均匀随机。第二个方法是在实体名称相同的情况下，例如两本书名为《The Return》。第三个方法是在实体描述上相似，来自维基百科。最后，当实体具有相同的维基百科属性或属性时，例如相同的作者或艺术家。当我们向标注员展示备选问题时，他们知道这些实体的名称，但不一定了解实体本身。因此，我们向标注员展示了关于这两个实体的一些背景知识。对于歌曲，我们简单地显示Google搜索结果，并要求标注员至少听一点每首歌曲并阅读关于它们的信息。例如，这是“Easy on Me”的Google搜索结果。对于食谱和书籍领域，我们展示了维基百科中的背景文本。对于食谱，我们还展示了它们的图片，再次来自维基百科，以便标注员了解它们看起来是什么样子。然后我们要求标注员从这些实体中选择一个，例如第一个，并用三到五个间接指示表达来描述它，例如“那个有钢琴音乐的”。这是我们的数据集中的一些示例：那个没有歌词的，不是那个有12岁男孩的，或者是来自其他地方的，或者是来自Alibujon的等等。Alt Entities Corpus包含跨三个领域的6000个备选问题，以及42000个间接指示表达。使用T5-XL模型的结果如下：如果语言模型可以访问标注员相同的背景知识，准确率会很高，大约在92%到95%之间。但这并不现实。如果语言模型可以访问部分重叠的背景知识，准确率会在82%到87%之间，这更现实。例如，当语言模型检索到背景知识时，准确率只有60%。因此，有很多改进的空间。我们还展示了模型在不同领域上的泛化能力。这是数据集的链接。谢谢。</sample>
    <sample id="143">该方法与现有的 SimulST 策略进行了比较，包括 Whitkey 策略、Local Aggregation 策略以及专门用于实时翻译的 State-of-the-Art 架构。</sample>
    <sample id="144">论文的作者是Yannick Slavac，他来自INRAE（法国国家农业、食品和环境研究院）。</sample>
    <sample id="145">演讲者的名字是珍妮。</sample>
    <sample id="146">The speaker, a PhD student from Fudan University named Zhe Chen, introduces a paper on the analysis of omission in dialog summarization. The paper discusses the challenges and progress in dialog summarization, particularly focusing on the issue of omission. Omission is a significant problem in dialog summarization, leading to incomplete summaries where critical information is lost. The speaker analyzes the percentage of summaries suffering from omission and finds that even state-of-the-art models still have a high omission rate, with about 70% of generated summaries having omission issues. The speaker also examines the distribution of omitted information in dialogues, which are randomly distributed regardless of length or domain, indicating that dialogues are unstructured and identifying key information is difficult for current models. To address this issue, the speaker proposes a task definition for omission detection, focusing on utterance-level omission at the candidate summary level. However, there are no datasets specifically designed for this analysis and detection task, so the speaker constructs a dataset with high-quality omission labels for dialog summarization. The dataset is built upon five existing benchmarks covering five domains, using different abstractive models to generate diverse candidate summaries. An automatic method is proposed to produce omission labels for these candidate summaries, and human evaluation is performed to assess the quality of the labels. The speaker explores three frameworks as baselines for omission detection tasks, including pairwise classification, sequence labeling, and pointer networks, using the precision-recall F1 score to evaluate the models. Additionally, the word-level omission recall (denoted as R score) is calculated by measuring the percentage of gold omission words hit in the detected utterances. The results show that the task is challenging, with an F1 score around 50%, indicating the need for more advanced detection models. The speaker also discusses a post-editing method for summary refinement, where the candidate summary is concatenated with omission content as input, and the model outputs the refined summary in a sequence-to-sequence manner. The performance is significantly boosted when omission content is provided, indicating that omission detection is valuable and that refinement based on detected omission is a promising direction for improving summary quality in dialog summarization.</sample>
    <sample id="147">这篇论文有三位作者。</sample>
    <sample id="148">嗨，我是Sarah Papi，来自都灵大学和Bruno Caseler，我将简要介绍我们关于实时语音翻译的论文，该论文与Matteo Negri和Marc Turky合作完成。实时语音翻译（Simultaneous Speech Translation，简称SST）是将 spoken language 翻译成另一种语言的过程，以实现实时跨语言交流。当前实时语音翻译模型面临的主要问题包括：特定架构通常需要额外模块进行优化，训练过程复杂，例如涉及不同的优化目标，以及需要训练和维护多个模型以达到不同的延迟要求，例如训练一个平均延迟为1秒的模型，另一个为2秒的模型等等。我们的解决方案是利用现有的预训练模型，无需重新训练或采用特定架构，使用单一模型处理所有延迟需求，并通过注意力机制处理延迟。注意力机制是一种在音频输入和文本输出之间建立联系的方法，可以识别音频输入中哪些部分与输出文本相关。我们提出了一种称为“编码-解码器注意力”的策略，根据注意力权重决定是否输出部分翻译。如果注意力权重未集中在最后几个语音帧上，我们将不会输出最后一个单词，而是等待下一个语音片段。这种策略可以提高翻译质量和实时性，同时减少计算资源消耗。</sample>
    <sample id="149">是的，数据集公开了。</sample>
    <sample id="150">演讲者介绍了一个名为“Meeting Q&amp;A”的新数据集，该数据集基于会议转录中参与者提出的问题和答案。与以往只关注摘要和提取行动项的研究不同，Meeting Q&amp;A专注于会议讨论中的问答部分，提供了更丰富、更详细的信息。数据集包含7.7万道问题，其中30%无法回答，40%有多个答案，48%涉及多名说话人。数据收集过程包括从AMICORpus中选择会议转录，基于标点符号进行问题选择，过滤掉短问题，并请标注员标注答案句子。数据集的性能指标显示，在测试集上达到F1值84.6%，展示了在问答任务上的高人类表现。此外，研究还探讨了使用短上下文模型、单个答案模型和多答案模型的方法，并分析了它们在不同设置下的表现。</sample>
    <sample id="151">大家好，我叫Ying，我和我的同事Jiang一起将展示我们关于Multi-Instinct的研究，即通过指令调优来提高多模零样本学习。随着大型语言模型的 advances，许多研究开始探索使用预训练语言模型进行不同下游任务的新学习范式。最近的研究表明，指令调优使大型语言模型能够通过遵循自然指令在零样本模式下完成任务。然而，大多数之前的工作主要集中在通过指令调优提高语言任务的零样本性能，而计算机视觉和多模任务则被忽视了。因此，在本研究中，我们旨在探讨是否可以通过指令调优多模预训练模型来提高对多模任务的泛化能力。此外，在我们研究期间，我们发现多模指令数据集的可获得性存在显著差距。目前有超过1600个语言-only的指令任务，但没有大规模公开可用的多模指令任务。这促使我们构建一个多模指令调优数据集。在这里，我们介绍了Multi-Instinct，这是第一个多模指令调优基准数据集，包含62种不同的多模任务，涵盖10个主要类别。这些任务是从21个现有的开源数据集中派生出来的，每个任务配备了5个专家编写的指令。为了评估多模指令调优，我们使用OFA作为基础模型，OFA使用统一的词汇表对语言、图像和坐标进行编码。这里展示了我们多模指令数据集的一些示例实例。为了统一处理各种输入和输出数据类型，我们遵循OFA的方法，并将所有任务格式化为统一的序列到序列格式，在这种格式中，输入文本、图像、指令和坐标框都在相同的词嵌入空间中表示。现在，我将讨论多模指令调优。对于训练数据集，我们使用了NLP组中的53个任务进行训练，每个任务采样了10,000个实例。对于测试，我们保留了Common Sense Reasoning组的整个数据集进行测试，并从VQA和Miscellaneous组中选择了额外的5个任务。我们使用测试集中的所有实例对每个任务进行测试，另外随机从NLP组的测试集中的20个任务中选择20个任务作为NLP的无监督任务。我们使用预训练的OFA大模型作为基础模型。在训练过程中，我们将每个任务的每个实例随机与五个指令模板之一结合。在测试阶段，对于每个任务，我们进行五次实验，每次实验使用五个指令中的一个来评估模型。我们报告了所有五次实验的最小和最大性能以及性能的标准差。如果任务是多模分类任务，我们将报告准确性；如果是多模生成任务，我们将报告ROUGE-L；如果是NLP任务，我们将报告ROUGE-L。我们还引入了一个额外的评估指标叫做敏感度，它衡量模型在指令 wording 的微小变化时是否能一致地产生相同的结果。我们的主要结果是，指令调优可以显著提高OFA在多模任务上的性能。此外，通过从自然指令数据集进行迁移学习，可以进一步提高指令调优。我们可以看到，随着任务数量的增加，模型在保持较低敏感度的同时取得了更好的性能。我们还进行了实验，比较了一种使用单个指令和另一种使用五个指令的策略。使用多个指令可以提高模型的整体性能并降低其敏感度。这展示了不同调优策略对模型敏感度的影响。通过从自然指令数据集进行迁移学习，模型可以实现比原始OFA模型更低的敏感度。我们还观察到，通过从自然指令数据集进行迁移学习，OFA在多模指令数据集上取得了更好的性能。总的来说，我们提出了第一个大规模多模指令调优数据集，显著提高了OFA的零样本能力，并探索了不同的迁移学习技术及其益处。我们设计了一个新的指标叫做敏感度。最后，我们正在收集一个更大的多模指令调优数据集，其中包含大约150个额外的视觉语言任务，并计划在未来发布它们。这是我们的代码仓库链接。谢谢。</sample>
    <sample id="152">The presentation introduces the work of the speaker, Frederic Riemenschneider, and his team at the intersection of Natural Language Processing (NLP) and classical philology. The team has developed new language models specifically designed for classical philology, aiming to create models that can handle both ancient Greek and Latin texts. They have pre-trained two monolingual models for ancient Greek: Grecberta and Grecr, and two multilingual models: Philberta and Philr, which are trained on ancient Greek, Latin, and English data.

The team has gathered pre-training data from various sources, including OpenGreek and Latin, and a new corpus from the Internet Archive, which they have processed to identify and correct incorrectly transcribed Greek texts. They have also leveraged additional resources for Latin and English texts related to antiquity.

The team has benchmarked their models using datasets such as the Universal Dependencies treebanks for Greek and the evaLatin 2022 dataset for Latin. Their models outperform the current state-of-the-art in speech tagging, dependency parsing, and lemmatization tasks. They have also analyzed the performance of the T5 encoder-decoder model and investigated the implications of multilinguality in their language models.

Overall, the presentation highlights the development of powerful language models for classical philology that can process both ancient Greek and Latin texts, and the team's efforts to improve the state-of-the-art in NLP for this domain.</sample>
    <sample id="153">Ninara Maherabi是一位在亚马逊Alexa AI的可解释AI团队工作的博士后科学家，她正在研究解决文本到图像生成模型中的模糊性。她的工作重点是识别和解决文本输入中模糊的模糊性，这些模糊性可能导致图像生成器对用户的意图产生误解。例如，“女孩站在一间有花的房间里”这个模糊的提示可能被解释为女孩在房间里有花，或者房间和女孩都有花。为了解决这个问题，Ninara提出了一个框架，通过提出澄清问题或生成不同可能的视觉设置来消除模糊性。用户根据他们的意图回答这些问题，从而得到明确的提示。然后，使用文本到图像模型生成图像，并使用VQA模型评估图像是否忠实于用户的意图。Ninara的研究结果表明，使用她的框架可以显著提高图像生成的准确性。她的框架还得到了人类评估的一致性验证，这表明它是一个可靠的评估工具。</sample>
    <sample id="154">根据所提供的内容，论文的作者来自都灵大学和布罗尼奥·卡斯勒基金会。</sample>
    <sample id="155">演讲者的名字是Javahar Hosaini。</sample>
    <sample id="157">Shenggao从Sandong University介绍了一种名为“Dialog Summarization with Static-Dynamic Structure Fusion Graph”的工作，这是一种结合静态和动态结构的对话摘要方法。该研究与Xin Cheng、Ming Jie Li、Xiu Yin Chen、Jin Pui Li、Dong Yan Zhao和Ryan合作完成。该方法旨在从对话中提取隐含信息并生成简洁的摘要，这对于快速捕捉多参与者对话的主要观点非常有用，而无需详细阅读整个对话。

该方法首先使用一个称为“utterance encoder”的组件将对话中的句子编码为向量表示。然后，使用现有的对话结构建模方法构建静态图。接下来，通过动态图模型捕捉基于深度向量表示的句子之间的语义关系。最后，使用预训练的编码器作为摘要生成器，将静态对话结构与动态学习到的对话结构融合在一起，生成最终摘要。

该方法还提出了几种新颖的组件，如层次结构对话结构建模方法，用于构建基于图网络的句子之间的关系。此外，还使用了滑动窗口和计数函数来计算每个说话者在滑动窗口内的共现频率，以捕捉说话者之间的互动关系。最后，通过使用多头注意力模型和融合方法，将静态和动态图的矩阵融合为统一的图表示，从而捕捉句子之间的语义关系。</sample>
    <sample id="158">The speaker, Changwoo Hong from AWS, introduces a dual cache method for long document neural coreference resolution. The task involves identifying and clustering messages that refer to the same entity in a document with multiple mentions across the text. The proposed method uses a fixed-size cache to reduce complexity to a linear level, with a local cache for frequent entities and a global cache for less frequently used entities. The model evaluates the frequency of new or updated entities and adds them to the appropriate cache. The dual cache method outperforms single cache methods and significantly reduces cache misses, making it more cost-effective.</sample>
    <sample id="159">大家好，我是库斯达·辛纳，我很高兴欢迎你们参加我们关于ACL 2023年论文“语言模型可接受性判断并非总是对语境敏感”的讨论。这是一篇与John Gold here、Aaron Mueller、Kannisha Mishra、Karen Fuentes、Roger Levy和Adina Williams合作的共同作品。在这项工作中，我们重新评估了最小对偶 paradigm。最小对偶 paradigm主要用于评估语言模型在可接受性判断上的表现，这些判断可以包括语法性，比如BLEU或语法得分，或者可接受性，比如刻板印象。在这个最小对偶 paradigm中，评估语言模型的典型方法是展示一个可接受的句子或语法正确的句子，然后展示一个不可接受的句子或语法错误的句子。希望模型将更多的概率分配给可接受的句子。当前的MPP pipeline并不能让我们评估模型对更长句子的可接受性。如今，大型语言模型正在出现，具有更长的上下文窗口。因此，我们需要评估模型在整个上下文窗口中的可接受性。这就是我们在这里尝试的事情。我们试图通过要求模型评估更长更长序列的可接受性来重新评估MPP pipeline。所以这就是我们的方法。我们所做的就是模拟这些更长序列，重新评估数据集本身，然后重新创建句子，通过选择可接受或不可接受的句子。例如，这里我们从BLiMP数据集中选择一个典型的语法正确性对，来自 adjunct island 案例。我们所做的就是重新创建更长序列，其中包含可接受和不可接受句子，同时保持相同的句法结构。我们从 adjunct island 中提取句法句子，并将其作为前缀添加到可接受查询和不可接受查询之前。我们还可以通过从同一数据集的不同子集或不同数据集中选择句子来执行相同操作。这样我们就得到了 mismatch 情景。这里，句子仍然来自相关数据集，但不是你用来评估的那一个。我们还可以为不可接受性测试执行相同操作。最后，我们可以选择完全 unrelated 的领域，比如维基百科。这将告诉我们模型的可接受性判断是否真的受到任何上下文的影响，即上下文是否来自数据集的同一子集，或者是否与当前要分析的句子无关。那么模型的表现如何呢？首先，我们查看维基百科句子，它们与当前查询对完全无关。我们发现MPP判断在任意上下文长度下大多是稳健的。我们将上下文长度增加到124以最大化OPT和GPT-2模型，并在橙色线下看到MPP判断相对稳定。那么当我们选择来自同一数据集的句子时会怎么样呢？这里我们从BLiMP或语法得分数据集中选择可接受和不可接受的句子。我们发现MPP判断要么显著增加，要么显著减少，当添加可接受或不可接受前缀时。但是，当我们匹配结构，即选择与BLiMP或语法得分数据集中的相同现象相关的句子时，MPP判断要么有巨大增加，要么有巨大减少，取决于所选前缀是否可接受或不可接受。这种影响在整个上下文长度上都会增加，这可能会影响具有大上下文窗口的新一代语言模型。那么为什么匹配前缀会影响语言模型的判断呢？我们进行了系列分析，其中我们尝试通过在输入句子中添加噪声来扭曲输入，同时保留相关结构。经过多次这样的扭曲，我们发现这些噪声并没有使模型在显示MPP判断趋势方面发生任何变化。基本上我们发现模型对扭曲的句子敏感，方式类似：当我们在可接受域内扭曲句子时，所有扭曲都会导致类似的增加；当我们在不可接受域内扭曲句子时，所有扭曲都会导致类似的减少。所以这项工作的关键 takeaway 是，语言模型对跨句子共享的潜在句法和语义特征敏感。目前我们使用短而单个句子输入的MPP评估方式可能无法充分捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文获取更多实验细节。谢谢大家。</sample>
    <sample id="160">该方法的第一步将输入词元映射到一个未排序的多集词元。</sample>
    <sample id="161">根据所给英文内容，Coscript 中包含了55000个脚本。</sample>
    <sample id="163">根据演讲内容，DEplain的最佳对齐方法是mash align。</sample>
    <sample id="164">弱监督学习的一个好处是它比人类标注数据便宜得多。</sample>
    <sample id="165">演讲者介绍了一篇题为“利用互斥解释进行自适应共同推理”的论文，该论文由Cornell University的Wenteng Jao撰写。论文提出了一个自适应推理框架，旨在在没有监督的情况下识别可能的解释，以解释结果与给定上下文之间的信息差距。演讲者首先通过一个具体例子来说明自适应推理的概念，然后给出了更正式的定义。在自适应推理中，目标是找到一个可能的解释，它能够解释结果与上下文之间的关系。例如，在一个例子中，Emily因交通拥堵而延误了航班，但她的航班实际上准时起飞。两个可能的解释是：1）航班延误，2）航班准时起飞。自适应推理的目标是找到一个解释，它能解释为什么Emily准时到了航班上。演讲者还讨论了当前自适应推理方法的局限性，如需要标注可能的解释，这可能会导致主观性和噪声。他们提出了一种无监督学习方法，称为Lipor（Likelihood Learning with Posterior Regularization），它通过最大化结果的概率来识别可能的解释，同时使用一个正则化器来确保解释的互斥性。演讲者比较了他们的方法与Alpha和零-shot模型，并展示了他们的方法在准确度上优于这些模型。演讲者感谢听众，并提供了论文的链接。</sample>
    <sample id="166">该演讲主要介绍了基于深度学习和符号推理的复杂文本图像检索框架。演讲者首先概述了传统的视觉语言模型在处理复杂文本时的性能瓶颈，然后提出了一个结合了视觉和符号推理的框架来解决这个问题。该框架由两个主要模块组成：符号推理生成器和神经符号推理器。符号推理生成器负责将图像转换为符号表示，并使用这些表示进行推理，而神经符号推理器则利用这些推理结果来生成最终答案。通过将这两个模块结合，系统能够更有效地处理复杂的文本图像检索任务。演讲者还展示了实验结果，证明了该框架的有效性和性能优势。</sample>
    <sample id="167">DEplain-web 中的文档被手动对齐了 750 份文档，其中一部分是通过手动对齐方法对齐的，另一部分是通过自动对齐方法对齐的。</sample>
    <sample id="168">CoNLL++数据集是通过从2020年收集的路透社新闻中收集并用与CoNLL 2013相同的标注指南进行标注而创建的。</sample>
    <sample id="169">演讲者介绍了一篇关于使用大型语言模型进行机器翻译的论文，重点讨论了Prompting策略及其对性能的影响。研究评估了Prompting能力，并比较了当前最佳系统与Palm的性能。结果表明，Prompting策略对性能有重大影响，高质量的示例比源句子的相似性更为重要。此外，使用高质量的训练数据（如Dev数据）而不是训练数据可以显著提高性能。虽然Palm在流畅度上接近当前最佳系统，但在准确性上存在差异，特别是在删除源句子的部分以产生更自然的翻译时。</sample>
    <sample id="170">Hello everyone. My name is Yusen Zhang from the Peking University. Today, I am going to present our work: Exemplar Crosslingual Semantic Parsing in Multiple Natural Languages and Multiple Representations. Semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL and so on. Existing cross-lingual semantic parsing models are separately proposed and evaluated on datasets of limited tasks and applications. For instance, there are lacks of coverage on certain natural language, the Chinese is missing, and lacks of coverage on certain meaning representations, the Lambda Calculus is missing, or they are only evaluated on certain neural model, for example, there is only one single model to evaluate them. So to this end, we propose Exemplar, we provide a unified dataset exemplar for cross-lingual semantic parsing in multiple natural languages and multiple representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is translate test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query, and during inference, we translate the German query using API to English, and then use the trained model to predict the SQL. And we also test monolingual model. In this setting, the source language is the same as target language, for example, German to German or English to English. We also test monolingual few-shot setting by training monolingual models with only 10 percent of training data. And we test multi-lingual multi-lingual model which we train one multi-lingual model for all languages. For example, we put the German, English, Chinese queries together to train a multi-lingual model, and during inference, we can use this model to to translate German queries or Chinese query or etc. And we also consider cross-lingual zero-shot and few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English query or the combination of English and German few-shot queries to train a multi-lingual model to and predict the SQL output. And we also find many interesting results. So regarding analysis of monolingual models, we evaluate on two groups of models including encoder PDR which stands for multi-lingual pre-trained encoders with pointer-based decoders such as XLNet + PDR and BERT + PDR, and we also evaluate encoder-decoder models which is multi-lingual pre-trained encoder-decoder models such as MBART and MT5. We found that encoder-decoder obtains the best performance on all 9 datasets. And we evaluate on MT5 and example XLNet + PDR on multi-lingual setting. We found that encoder-decoder or encoder PDR can be improved by training in a mixture of various languages. And we found it is because most of the major natural languages can obtain performance gain except that English performance drops in 7 datasets and only gains in 3 datasets. I think this is known as curse of multi-linguality. We also compare the cross-lingual performance gap. In this figure, the blue line is cross-lingual few-shot transfer, the orange line is cross-lingual zero-shot transfer, while the green line is the monolingual setting. We found that by comparing the green and orange line, we found the zero-shot setting, the cross-lingual transfer performance gap is significant. And by comparing blue and orange line, we found that few-shot setting, the transfer gap is shortened rapidly. We also find some other interesting findings. For example, encoder-decoder outperforms previous work or achieved comparable results for training on English natural language and significantly boost the performance of few-shot on target natural languages. And we found multi-lingual language models such as codes and blue are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build Exemplar, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and multiple representations. We conduct a comprehensive benchmark study on three representative types of multi-lingual language models, and our result shows many interesting findings and etc. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="171">关于这方面的现有研究可以被广泛地分为四类。然而，这些方法要么不适用于嵌入式服务，要么缺乏可转换性。因此，在本文中我们提出了“嵌入式标记”，这是一种基于后门的水印方法，适用于嵌入式服务。</sample>
    <sample id="172">根据演讲者在 Exemplar 中的发现，Codex 或 Bloom 等多语言 LLM 对于 CLSP（跨语言语义解析）来说并不足够。演讲者指出，这些模型在跨语言语义解析任务中仍然存在不足之处，表明它们在处理多种语言和表示时可能缺乏必要的性能和能力。</sample>
    <sample id="174">The video introduces the ArgAnalysis35K dataset, which is a large-scale data set designed for argument quality analysis. The speaker, Priya, explains that this dataset stands out due to its high-quality arguments sourced from various reliable sources such as speeches from high-quality tournaments, expert debaters, intermediate debaters, and novice debaters. Unlike other datasets that often lack diversity and depth, ArgAnalysis35K captures a diverse range of arguments on 24 different themes, providing a more comprehensive view of parliamentary debate settings.

The dataset also includes an element of analysis, which combines claims and premises to form coherent arguments. This feature is unique as it goes beyond just listing claims or premises, offering a more holistic understanding of the arguments. Additionally, the dataset incorporates an instance-based annotator reliability model, which helps in identifying and eliminating biased judgments by annotators, thereby improving the reliability of the data.

Furthermore, ArgAnalysis35K introduces a relevance model that assigns scores to arguments based on their relevance to specific themes. This model enhances the dataset's utility by capturing how relevant each argument is to a particular theme, making it more useful for research and analysis.

Overall, the ArgAnalysis35K dataset is a significant contribution to the field of argument quality analysis, offering a diverse, high-quality, and reliable set of arguments that can be used for better insights into argumentation and debate strategies.</sample>
    <sample id="175">该方法通过在训练过程中引入排列的不确定性来处理排列的不确定性。具体而言，它通过在输出中确定每个位置应放置的多集标记来处理排列的不确定性。这种方法允许模型预测多个可能的排列，而不仅仅是单个排列。然后，通过使用一种称为“连续放松”的近似方法来找到具有最高得分的排列，从而找到最有可能的排列。这种方法还允许模型在解决方案中反向传播并学习更符合语言规则的排列。</sample>
    <sample id="176">根据演讲内容，NLP 模型的公平性可以通过评估模型在检测 hate speech 和 fake news 时的表现来定义。特别是，如果模型能够以一致的方式处理来自不同政治背景的输入，而不会系统地偏见某些群体，那么它就可以被认为是公平的。</sample>
    <sample id="177">演讲者的名字是Yanis Slavac。</sample>
    <sample id="178">演讲者的名字是Kostya Sina。</sample>
    <sample id="179">The speaker, Melina Sclair, introduces a method called Symbolic Tom to improve theory of mind reasoning skills in large language models. She explains that theory of mind is the ability to reason about the mental states of others and is traditionally measured in humans and language models through reading comprehension tasks involving multiple characters. False belief questions are used to prove understanding, where reality may not match the belief of certain story characters.

She presents the Sally Ann test as an example, where Alice and Bob are in a room with a basket and a box. Alice puts an apple in the basket and leaves the room, while Bob moves the apple to the box. The speaker asks how this affects what Alice will search for when she returns, demonstrating first-order and second-order theory of mind questions.

The speaker then discusses the poor performance of large language models like ChatGPT or GPT-3 on false belief tasks. To address this, they propose using explicit graphical representations to improve theory of mind reasoning skills. They compute graphs for all combinations of characters up to a predefined maximum theory of mind level, using an inference-time algorithm that leverages off-the-shelf and open AI models.

The speaker tests their method with various ALMs and compares it against supervised baselines, showing significant gains in in-domain performance and robustness across different datasets. They also discuss the generalization capabilities of their method, noting that supervised models degrade performance on out-of-domain datasets, while Symbolic Tom shows significant improvements.

In conclusion, Symbolic Tom is an inference-time algorithm that uses explicit graphical symbolic representations to improve theory of mind reasoning skills in large language models, outperforming supervised approaches on out-of-domain story understanding and remaining beneficial on new linguistic diversity datasets.</sample>
    <sample id="180">演讲者的名字是Mayra。</sample>
    <sample id="181">本文介绍了从复旦大学的CUIYuan教授及其团队开发的一种名为CoScript的约束语言规划数据集。该数据集旨在通过符号知识蒸馏技术从大型语言模型中提取约束语言规划数据，从而为小型和专业化的模型提供高质量的训练数据。CoScript数据集包含55,000个具有约束条件的具体目标和相应的脚本，用于验证和测试。通过使用大型语言模型生成高质量的脚本数据集，该团队旨在使约束语言规划能力在较小的模型上得到应用，这些模型通常更易于部署。此外，该团队还开发了一个过生成过滤器方法，以提高大型语言模型生成的脚本的质量和准确性。该方法通过计算脚本与目标约束之间的语义相似度和关键词匹配度来选择最符合约束条件的脚本。该数据集和方法旨在成为约束语言规划研究的宝贵资源。</sample>
    <sample id="182">在本文的背景下，热带主义 (tropicalism) 指的是描述拉丁裔女性时使用的刻板印象，如“充满活力”和“风趣”，这些词将拉丁裔女性与热带地区联系在一起。这种刻板印象反映了拉丁裔女性被描绘为异国情调和异国情调的叙事，这有助于长期的歧视和边缘化。</sample>
    <sample id="183">作者通过使用自然语言提示来创建目标群体的人工描写，这些提示包括描述一个想象中的个体的描述。例如，他们可能会输入“想象一下你是一个亚洲女性，描述你自己。”这种方法允许根据任何身份标记生成可泛化的描述。</sample>
    <sample id="184">在本文中，语境使用情况是通过测量给定源文本时，语境C提供的关于目标Y的信息量来衡量的。这被称为信息增益，可以看作是给模型提供语境后获得的信息量。</sample>
    <sample id="185">DrBERT 和 ChuBERT 的主要区别在于它们的训练数据来源。DrBERT 是基于 Roberta 训练的生物医学模型，而 ChuBERT 是基于匿名化数据训练的临床模型。</sample>
    <sample id="187">根据提供的内容，这篇论文有两位作者：Eing和Chiang。</sample>
    <sample id="188">迭代迁移学习是一种机器学习技术，其中模型在多个任务上进行训练，以利用不同任务之间的共同知识。在这种情况下，它涉及使用与认知失真相关的任务（如辩论独立性分类和扩展与比较分类）来提高对认知失真的检测能力。通过在这些相关任务上迭代地微调模型，可以显著提高对认知失真检测的性能，即使在标注数据有限的情况下。</sample>
    <sample id="189">数据集的目标是收集大量关于用户语言的样本，以解决在自然对话中使用间接指示代词进行实体选择的问题。它旨在通过提供不同领域（音乐、书籍和食谱）的背景信息来帮助机器学习模型理解用户的意图，特别是在用户无法直接引用实体时。</sample>
    <sample id="190">攻击者可以通过学习 EaaS 提供的嵌入来提取模型参数。通过分析嵌入，攻击者可以推断出模型的内部结构和参数，从而复制类似的服务。</sample>
    <sample id="191">根据所提供的内容，该论文有三位作者：Sara Papi、Bruno Caseler和Matteo Negri。</sample>
    <sample id="192">演讲者介绍了一种新的优化算法，旨在同时实现快速收敛和低内存使用。他们提出了一个基于矩阵分解的算法，通过将矩阵V分解为两个矩阵W和H来减少内存需求。然而，这种分解会导致训练大型神经网络时出现随机更新，从而影响训练过程的稳定性。为了克服这个问题，他们提出了一个稳健的更新策略，通过计算预测更新和实际更新之间的残差来估计不稳定性，并将其作为分母来调整更新步长。实验结果表明，该算法在训练任务中显著提高了验证精度，并且在大规模模型训练中具有更低的内存成本。</sample>
    <sample id="193">用于创建初始数据集的注释者数量没有在提供的信息中明确说明。</sample>
    <sample id="194">根据演讲者的介绍，她来自卡内基梅隆大学。</sample>
    <sample id="195">The speaker introduces a new method for answering complex questions, called Reasoning over Hierarchical Question Decomposition Trees (RQDT). This method involves breaking down complex questions into sub-questions and generating knowledge from various sources to answer them. The speaker explains that previous methods have limitations, such as incomplete knowledge bases and difficulty integrating knowledge from different sources. To address these issues, the RQDT framework is proposed, which consists of two stages: building an hierarchical question decomposition tree (HQDT) and probabilistic reasoning over HQDT. The HQDT is built by using a question decomposer to generate leaf nodes, which are atomic questions, and then using another question generator to generate intermediate questions based on grouped leaf questions. The certainty score of each node is also computed based on the likelihood of representing the certainty of its generation. Probabilistic reasoning is conducted over HQDT by selecting appropriate knowledge sources, getting answers with probabilities from selected sources, and aggregating candidate answers to output top-k answers with highest probabilities. The speaker evaluates the RQDT framework on two challenging complex QA datasets, KQA Pro and Music, showing significant improvement in performance compared to previous methods.</sample>
    <sample id="196">以左侧为支配词的示例是“我看见了那个男人”。</sample>
    <sample id="197">对话系统中的最先进模型是ABC-Eval。</sample>
    <sample id="198">我们需要在整个上下文窗口中评估模型的可接受性，因为大型语言模型正在发展出更长的上下文窗口。通过评估模型在更长句子上的性能，我们可以更好地理解模型如何处理复杂的语言结构，并确保模型能够准确地评估可接受性，无论句子的长度如何。</sample>
    <sample id="199">根据所给内容，多语言训练通常会导致表现下降。然而，在某些情况下，如在三个数据集中观察到的那样，英语性能会有所提高。这种现象被称为多语言性 curse。</sample>
    <sample id="200">是的，注释者提前知道实体。</sample>
    <sample id="201">根据所给英文内容，评估使用了最新的测试集以避免测试数据与训练数据重叠，并比较了两个最好的系统，即最佳性能系统和 WMT 评估中的系统。此外，还使用了最新的 WMT 指标和专家基于人类的评估结果。</sample>
    <sample id="202">是的，泛化中的回归会影响特定的 NER 类型。在本研究中，我们观察到使用 Conll 2003 数据集训练的模型在 Conll 2023 测试集上表现不佳。这种性能下降可能是由于模型过拟合或数据集的 temporal drift 导致的。</sample>
    <sample id="203">NLP 中的立场很重要，因为系统性能在不同人口群体之间存在系统性差异。这些差异可能源于 NLP 研究人员和模型开发者的立场。立场是指人们持有的观点，这些观点源于他们的人口统计、身份和生活经历。这些观点会影响研究人员做出决策，从而影响研究过程及其结果。</sample>
    <sample id="204">根据所提供的英文内容，像 BLOOM 这样的多语言 LLM 是采用完整微调。</sample>
    <sample id="205">The speaker, Changbing, a PhD student at the University of Washington, is presenting their work on the political biases in language models. They explain that language models are trained on large-scale web crawl data, which includes political news media from sources like the New York Times, Los Angeles Times, The Guardian, and Huffington Post. This diverse training data allows models to learn from various perspectives but also introduces inherent social biases.

Changbing's research aims to investigate how these biases propagate through the pipeline from pre-training data to language models and downstream tasks. They propose evaluating the political leanings of language models using political questionnaires like the Political Compass Test and conducting controlled experiments with different pre-training datasets.

Preliminary results show that language models have varying political leanings, with GPT-4 being the most liberal and GPT series generally more socially liberal than BERT series. Further pre-training on partisan corpora has been shown to shift the ideological coordinates of the language model, indicating that models can pick up societal polarization.

The study evaluates language models with different political leanings on hate speech detection and fake news detection tasks. Results indicate that left-leaning models are better at detecting hate speech targeting socially minoritized groups but worse at detecting hate speech targeting more powerful groups. Conversely, right-leaning models are better at detecting hate speech targeting white and men but worse at detecting hate speech targeting Black LGBTQ+ and other minority communities.

Changbing concludes that there is a pressing need to acknowledge and tackle the fairness issues resulting from language model political biases. They highlight the dilemma between sanitizing political opinions in training data to reduce bias and the risk of censorship or exclusion. The speaker emphasizes the importance of addressing these issues to ensure fair and unbiased language model applications.</sample>
    <sample id="206">根据所提供的英文内容，他们使用了迁移学习的模型，该模型是CET任务和辩论任务。</sample>
    <sample id="207">用于评估 PaLM 能力的测试集包括最新的测试集，以避免与训练数据重叠。</sample>
    <sample id="208">作者最终提出了三条建议。</sample>
    <sample id="209">根据所给的英文内容，与最强的基线相比，建议的方法获得了显著的收益。这可以通过图中显示的性能差异来观察，表明在不同约束类别上，生成脚本的质量有显著提高。</sample>
    <sample id="210">演讲者的名字是周恒。</sample>
    <sample id="211">是的，论文中的结果和数据集可以作为基准。作者建议使用这些结果作为未来自动文本简化任务的基线。</sample>
    <sample id="212">在论文中，作者进行了针对较小模型的实验，以评估它们在约束语言规划中的性能。他们使用了55,000个特定目标和脚本生成数据集，以确保验证和测试数据的质量。</sample>
    <sample id="213">OFA</sample>
    <sample id="215">The speaker, Adam Siprowski, introduces a paper discussing the dependency structure of coordination in linguistics. He explains that different theories and approaches assume various dependency structures for coordinated structures, such as the universal dependencies approach where the first conjunct is the head, and Igor Miltruk's meaning text theory where the first conjunct also heads the structure. He contrasts these with symmetric approaches like the Prague approach, which heads the structure with the conjunction, and multi-headed approaches used in De Catts' word grammar, where all conjuncts are heads.

The paper aims to argue for symmetric structures of coordination against asymmetric ones using the principle of dependency length minimization. This principle suggests that shorter dependencies are preferred. The speaker illustrates this with examples in English, showing how direct objects are closer to the verb, making them more grammatically correct, while indirect objects can be farther away but still acceptable if they satisfy the dependency length minimization principle.

The speaker then presents statistics from the enhanced version of the Penn Treebank, confirming that left conjuncts tend to be shorter and that this tendency increases with the length difference between the two conjuncts. However, this tendency only occurs when there is no external governor on the left side of the coordination. When the governor is on the right, the tendency for the left conjunct to be shorter disappears.

The paper provides an argument against asymmetric structures of coordination and in favor of symmetric structures based on these observations. The speaker concludes by inviting further discussion on the topic during the poster session.</sample>
    <sample id="217">The speaker introduces a research work titled "Seeing to Unseen: Exploring Compositional Generation of Multi-Attributed Controllable Dialogue" by Wei Hou, Zhen, and colleagues from Beijing University of Post and Telecommunications. The presentation covers the motivation behind the work, previous methods for generating controllable dialogue, and the limitations of these methods in handling multi-attributed text generation.

The researchers propose a new method called DC-G (Dissentangled Controllable Generation), which uses a disentangled loss to separate different attribute combinations and a unified reference-free evaluation framework (MAE) to assess the performance of the model. They also introduce two types of prompts: attribute-oriented prompts that guide the model based on specific attribute values and task-oriented prompts that provide global features independent of the instance.

The study demonstrates that the proposed method outperforms baseline models in terms of controllability and test quality. The results confirm the effectiveness of the method in transforming seen attributes into unseen combinations, achieving state-of-the-art performance with only a slight drop compared to other metrics like EASCC and AASCC.</sample>
    <sample id="218">根据所提供的内容，无法确定论文作者所属的机构。演讲者只是提到了与Google Translate合作，但没有提供关于他们自己或他们的机构的具体信息。</sample>
    <sample id="219">The speaker introduces a research assistant at Academia Sinica, presenting their work on comparing and contrasting multi-stage pipelines for uncovering financial signals in financial reports. The work is done with Professor Joo Lee and other collaborators. The background of financial report analysis is discussed, along with text definition and approaches. The target company is the Fortune 1000, which is an annual report required by the SEC containing many details of companies' important activities. However, mining useful information requires lots of human efforts. The work was motivated by two observations: first, the words in a company's report are very similar, about 80% of tokens are the same, and the contents are largely dependent. This figure illustrates the text similarity between two reports in continuous years. For example, the report in 2018 is similar to the one in 2017. Based on the observation, they introduce a highlighting task and a multi-stage pipeline. They first define the reference to target structures in their task. The target and reference refer to the report of our interest and the report at its previous year. So basically, a highlighting model should compare and contrast the context between targets and reference. The goal of this highlighting task is to find the proportionality roots between a given pair T and R. Formally, the model will predict the highlight word importance and therefore we can measure the performance of highlighting. For example, the word decrease is supposed to have higher importance in this context. This is our proposed pipeline. Stage zero is document segmentation. Stage one is the relation classification. Stage two and stage two plus are our main and end-to-end fine-tuning. Due to the time limit, I would not talk about stage zero more details can be found in our paper. For the stage one, we will classify all the pairs into three types. Type pair refers to the pairs have higher syntactic and semantic similarities. These pair are frequently appeared such as company's regulations. Reverse pair have similar syntactical pattern but in fact the two segments disclose very different meaning. Mismatch pair are more like a debut information or company's new operations. For the model fine-tuning stage, we first use an external dataset, ESNLI, for out-of-domain fine-tuning. ESNLI is a natural language inference dataset with token annotation. For in-domain fine-tuning, we use the reverse pairs, the reverse words as pseudo positive labels, and we randomly label few other words as negative. In addition, we mix different objectives. We use the soft labeling techniques by mixing cross entropy loss and KL divergence. Therefore, we can alleviate the problem from low-quality pseudo labels. The variation dataset include ESNLI pairs and our released final dataset. We use two metrics to judge the performance. Precision indicates the precision over recall. PCC means the correlation between prediction and annotations. This table shows that our domain-agnostic highlighting model achieve the best performance on final and even preserve the generalization capability as you can see the performance on ESNLI. We further observe that our methods can benefit on mismatch pairs, which we didn't use during training. In conclusion, we propose a highlighting task with our released final dataset and a simple pipeline with two-stage fine-tuning. There are many other future works we would like to try including improving effectiveness or adding more features or like many other techniques in information retrieval can enhance the application as well. Yeah, that's it so please refer to our paper and github for more details and feel free to ask us if you have any question. Thank you.</sample>
    <sample id="220">根据演讲者的介绍，论文的作者是圣安德鲁大学计算机科学系的候选人。</sample>
    <sample id="221">论文分析了德语和英语之间的翻译。</sample>
    <sample id="222">The title of this work is "To Adapt or to Annotate: Challenges and Interventions in Open Domain Question Answering." The work aims to motivate the adaptation of models for open-domain question answering. The authors investigate different data interventions that can enable out-of-domain generalization, identify the type of data set shift a new domain exhibits, and determine which data interventions are effective for a specific type of shift.

The authors use Wikipedia as the source domain and test generalizability on seven target passage data sets spanning six different domains. They explore two overarching methods for generating data interventions: few-shot and zero-shot. In few-shot methods, they use examples from the target domain to prompt large language models to generate more examples. In zero-shot techniques, they control the interactions among three random variables in open-domain QA, keeping two variables fixed while varying the other one in a controlled manner to understand its impact on model learning.

The authors find that few-shot techniques improve retriever performance by 8% on average and reader performance by 11% on average. They also observe that changing the format does not significantly affect model performance, but closed-style questions are easier to curate than standard WQ format questions. They find that uniform distributions that cover all types of answers equally work best for answer distribution, and supervised methods like BM25 have the best overall performance for context distribution.

To assess the nature of incompatibility between the target model and domain exhibits, the authors consider existing data set shift taxonomy in machine learning. They compute the likelihood assigned by the source retriever and reader models to all contexts and answers in the target data set, using these values as a measure of compatibility. They map different target data sets onto a two-dimensional grid based on their compatibility with the source model, estimating the type of data set shift exhibited by each target data set.

The authors find that all target sets respond well to few-shot adaptations, while datasets with concept and covariate shifts respond well to zero-shot adaptations. In cases of no shift, there is little change in performance because the source model already understands the target domain to a great extent. Overall, the work demonstrates that certain data interventions are effective based on the type of shift a target data set exhibits, and it improves reader performance by up to 24%.</sample>
    <sample id="223">演讲者的名字是张冰。</sample>
    <sample id="224">在实验过程中研究了两个模型：一个用于产生文档级别的简化文本的模型，另一个用于产生句子级别的简化文本的模型。</sample>
    <sample id="225">在 MultiInstruct 中，用于训练的 62 个不同任务中，有 53 个任务用于训练，10 个任务用于测试。</sample>
    <sample id="226">根据提供的内容，无法确定论文的作者人数。内容中只提到了一个作者的名字，即“Ragna Stroen”，但没有提供关于其他作者的信息。</sample>
    <sample id="227">这段英语内容主要讨论了自然语言处理（NLP）领域中关于语言模型的挑战和研究方向。首先，它提到了语言模型在最近几年取得的显著成功，特别是在解决各种NLP任务方面。然而，研究者们认为当前语言模型缺乏“ grounding language understanding”，即把自然语言表达转化为可以在特定目标环境中执行的计划或程序的能力。这种转化对于实际应用至关重要，如智能助手、搜索引擎和机器人等。

接着，内容详细解释了 grounding language understanding 的重要性和挑战。它指出，大多数语言模型在预训练阶段缺乏 grounding，这导致了从预训练到实际应用的差距。此外，现有研究通常使用语言模型直接生成计划，但这种方法存在生成的计划可能不总是语法正确或有效的风险。

为了解决这些问题，研究者提出了一个新的框架，名为 Pangoo，它通过将语言模型用于评分和排名符号代理提出的候选计划，而不是直接生成计划。这种策略使得语言模型不需要处理计划的语法和有效性问题，从而提高了其在 grounding language understanding 上的表现。

最后，内容展示了 Pangoo 在不同实验设置下的性能，包括预训练和微调，以及在不同语言模型上的表现。结果表明，Pangoo 在多个任务上表现出色，特别是在样本效率方面。研究者还观察到，预训练模型如 Arkan Q&amp;A 在训练过程中容易出现过拟合，而 Pangoo 在这种情况下表现得更稳定。

总的来说，这段内容强调了 grounding language understanding 的重要性，并提出了一种新的框架来解决这一领域的挑战，同时展示了该框架在实际应用中的有效性和样本效率。</sample>
    <sample id="228">作者在实验中使用了四个数据集：AGNews、Mnli、Sst-2和Aesb。</sample>
    <sample id="229">Gabriella Katalin Szekely在演讲中讨论了论点支持写作中检测可改进的论点的重要性。她解释说，文本修订是专业写作的重要组成部分，是一个迭代过程，直到达到最优表达。她强调，在论点支持写作中找到正确的词语和表达方式至关重要，因为这直接影响文本对观众的影响。为了更好地理解这一过程，她展示了如何修订一个关于手机辐射可能导致脑癌的论点。

Szekely随后介绍了两个任务：1）子最优论点检测，即确定论点是否需要修订或已经优化；2）论点改进建议，即选择需要改进的论点质量类型。她提出了通过学习人类修订模式来解决这个问题的想法，而不是明确定义什么使论点好或坏。

演讲还提到了使用修订数据的挑战，包括代表性和可靠性、模型复杂性和架构、与论点相关的上下文信息以及主题和用户偏见。Szekely强调，虽然修订数据可以有效用于检测子最优论点，但识别论点质量的微妙之处取决于任务和论点所遭受的质量问题。她邀请听众阅读她的论文，以获取有关每个挑战的详细分析和系统比较。</sample>
    <sample id="231">NACHOS 是一个数据集，用于医疗领域，包含来自 Web 的医学文本数据。</sample>
    <sample id="232">演讲者的名字是伊尔比拉德。</sample>
    <sample id="233">The speaker introduces a method for simultaneous speech translation (SST) using pre-trained off-the-shelf models without retraining or adopting specific architectures. The method, called Attention-based Decoding (ADT), uses a cross-attention mechanism to decide whether to emit partial translations based on the concentration of attention points. The model emits words if the sum of cross-attention weights is above a certain threshold, and waits for another speech chunk if it is below the threshold. ADT outperforms other strategies applied to offline models in terms of translation quality and latency, while also being the fastest strategy.</sample>
    <sample id="234">根据所提供的内容，提示策略对结果有重大影响。实验结果表明，选择高质量的示例进行提示可以显著提高翻译性能。在单次和多次提示中，使用高质量示例的翻译性能明显优于使用训练数据或开发数据的性能。此外，实验还发现，提示策略的选择，特别是示例的质量，比示例与源句子的相关性更为重要。</sample>
    <sample id="235">根据提供的信息，论文的作者是卡伊·Yan。然而，关于他们所属机构的具体细节并未在演讲中提及。通常，这类信息会在学术出版物的参考文献或作者简介中找到，但这些信息在演讲中并未呈现。因此，在没有额外上下文的情况下，无法确定卡伊·Yan的机构。</sample>
    <sample id="236">每个任务都配备了5个专家编写的指令。</sample>
    <sample id="237">作者建议通过使用不同背景知识的设置来测试模型，包括仅在预训练时可用的背景知识、同时在预训练和推理时可用的背景知识，以及仅在推理时可用的背景知识。</sample>
    <sample id="238">该视频介绍了来自佛罗里达大学的Yiwen Hu开发的名为“MeetingBank”的新基准数据集。MeetingBank是一个包含1366个会议和近7000个实例的资源库，包括会议转录、参考摘要和元数据。数据集旨在解决高质量会议摘要缺乏和难以获取公开会议资源的问题。数据收集过程涉及使用语音识别API将音频数据转换为转录，从会议网站上提取会议ID，获取参考摘要和会议段落的时间戳，并对时间戳进行对齐以生成最终摘要。数据集提供了每个会议的详细统计信息，如会议数量、持续时间、每个会议的词数和演讲者数量，以及会议收集的年份范围。数据集还提供了每个城市会议摘要的摘要实例数量，以及源文本和摘要中的平均词数和句子数。为了评估数据集的质量，作者使用了多种评估指标，包括覆盖率和密度得分，以及基于人类评估的指标。结果表明，GPT-3在流畅性和连贯性方面表现出色，但在信息性和事实性方面表现不佳。该数据集旨在成为研究者设计先进会议摘要系统的重要工具，并提供有关城市 councils决策过程的有趣见解。</sample>
    <sample id="239">Hello everyone, my name is Ibilhar and I will give you a short overview of the paper "Prompting PAM for Translation: Assessing Strategies and Performance". This is joint work with my colleagues from Google Translate. PAM is a 540 billion parameter large language model presented last year in 2022. It's trained on a large collection of texts comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluate the translation capability of such models using the best practices of the NMT community. This involves using the latest test sets to avoid another lab of the test data with the training data of the language model. And we compare two state-of-the-art systems, so the best-performing systems on the WMT evaluation. We use state-of-the-art NMT metrics and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment where we use one-shot prompting and provided two different prompts for each sentence. The majority of sentences, 516 out of 1,000, the difference observed is of more than one BLEU points, and this can go in extreme cases up to forty BLEU points. So it's important to select a good prompting strategy. In our experiments, we conclude for a five-shot prompting strategy, where we just mark each sentence that we provide to the system with the language it's in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn't have a big influence in the case of several-shot prompting. It's crucial for zero and one-shot prompting, and when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the of the prompting. It's the examples that carry most of the of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it's important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data of the WMT evaluations or the dev data. The dev data is much more curated and with higher quality than the training data, that it's more nice, and the results, so better performance when using the dev data. Nonetheless, specialized state-of-the-art systems have a substantial advantage over the PAM translations. But PAM comes pretty close to a commercial system. Now in our case, we chose to evaluate with Google Translate. The insights that we gain from the human evaluation that we perform using the NMT framework, is that the fluency of PAM is comparable to state-of-the-art of the art systems, but the main difference comes from the accuracy. So in particular, the most common error are omission errors. So it seems that PAM chooses them to produce a better sounding translation sometimes by dropping parts of the source sentence that are omitted in the translation. However, the style awkward category for PAM is lower than for the state-of-the-art systems, which is an additional signal that PAM provides really fluent output, but still with some problems of accuracy. And that's it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.</sample>
    <sample id="240">Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work, "Weaker than You Think: A Critical Look at Weakly Supervised Learning." This is joint work with Xiaoyu Chen, Mauro Smola, and Giuseppe Dehghat Clarko. I'd like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data; instead, we label the data using weak labeling sources such as simple heuristics, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, weak annotations are much cheaper yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL (Weakly Supervised Learning), a common claim is that people say that they only train models on weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there's a catch which is that people do assume that there is an additional clean validation set available for model selection. We focus on this problem setting because this implies that additional manual annotations are required in weakly supervised learning, but like an elephant in the room, this necessity is often overlooked. The aforementioned approach does us to ask three research questions: First, is clean validation data necessary for WSL? Or can we maybe use a noisy validation set instead? Second, if clean data is required or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or are there better ways to utilize them? We address these research questions in our work and our findings are as follows: First, we find that interestingly, recent WSL methods indeed require clean validation samples to work properly, otherwise there is a large performance drop, as shown in this figure. If there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically, we only need 20 samples per class to attain high performance. But that's not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches which are directly applied on the clean data and WSL approaches which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model termed FTW initially underperforms more complicated WSL methods like cosine. However, if we allow to continue fine-tuning on the clean samples, then FTW performs equally well as other methods. So in practice, there's no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we show that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows: First, report the model selection criteria, for example, report if the model selection is done well with clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, i.e., those work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.</sample>
    <sample id="241">The speaker, Ethan, introduces his paper on "Human in the Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments." This work, a joint effort with Yang Chen, Wei Shu, and Allen Gidler at Georgia Tech, aims to address the limitations of current misinformation detection methods. These methods often fail to meet two key criteria: realistic evaluation and human-centricity.

Firstly, current systems are often unrealistically evaluated using retrospectively constructed datasets rather than live data. This can lead to the possibility of leaked counter-evidence, which recent studies have shown is a problem for many systems. For instance, evidence-based backtracking approaches may only find counter-evidence after a claim has been debunked publicly, rendering the system useless for early detection.

Secondly, these methods are not human-centric, often cutting out humans from the detection process or relegating them to the final determination step. The proposed framework seeks to integrate human feedback throughout the process, making the system assistive rather than authoritative.

Ethan's team has implemented and evaluated a novel workflow for detecting COVID-19 treatment misinformation. Their system consists of two main components: claim detection and policy violation verification. The first component uses keyword filtering, a TF-IDF model for question answering, and ranking by trendiness to identify potential claims. The second component uses a BERT-based stance classification model to flag tweets that support unapproved treatments for human review.

The evaluation of their Human in the Loop workflow shows that early detection is crucial for effective human intervention. They define early detection as the detection of an unapproved treatment before its first appearance in news articles. Their system successfully detected several unapproved treatments before they were reported in the news.

In terms of policy violation verification, the system achieved a high level of accuracy, with 65% of tweets being correctly identified as policy violations. The system also demonstrated a high efficiency in terms of the number of policy violations it could detect per hour of human work, confirming 124 policy violations per 2 hours of work.

Overall, Ethan's framework provides a more realistic and practical approach to evaluating misinformation detection systems, emphasizing the importance of human involvement and the need for systems that can be effectively evaluated and improved.</sample>
    <sample id="242">对话系统的常用评估方法是通过让人类裁判选择两个对话中哪个更好，或者在给定的量表上对对话进行评级。</sample>
    <sample id="243">这篇论文有五位作者。</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，需要的背景知识包括 Servin 是一名裁判，以及裁判在法庭上决定案件。</sample>
    <sample id="245">演讲者介绍了一个名为“两步流程”的系统，用于在Amazon Mechanical Turk上找到高可信度的工人。该系统旨在解决自动匹配方法可能存在的问题，并提供一种明确的招聘最佳实践。系统分为两个阶段：资格认证任务和耐力测试。资格认证任务评估工人的能力，而耐力测试则评估他们处理大量工作的能力。结果表明，只有6%的参与者通过了耐力测试，但通过资格认证任务的参与者比例更高。演讲者还提到了其他一些挑战，如只测试了英语摘要、设计问题的解决方案有限、缺乏对工人能力的保证，以及使用Google实验资金。</sample>
    <sample id="246">代码是公开的，可以在GitHub上获取。</sample>
    <sample id="247">The speaker introduces a new dataset called "Fact KG" for fact verification via reasoning on knowledge graphs. The dataset uses the DBpedia knowledge graph and includes claims in both written and colloquial styles, with five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The dataset is designed to be practical for use in modern dialogue systems that communicate with internal knowledge graphs, allowing for consistency checks between user input and the knowledge graph. The speaker also mentions the use of a collocational style transfer model and presupposition templates to create the dataset. The paper concludes by noting that all baselines outperform the majority class baseline (51%), and the model that uses graph evidence outperforms all other baselines.</sample>
    <sample id="248">根据所给内容，NLPositionality 的注释者在各个人口统计学特征方面并不均衡。例如，在 GPD4 社会接受度分析中，数据集和模型最接近英语国家，而在 Dina Hate 中，它们最接近英语国家。此外，在 GPD4 的社会接受度任务中，数据集和模型最接近拥有大学或更高教育的人群。这些不平衡可能导致 NLPositionality 的数据集和模型在某些人口群体中存在系统性性能差异。</sample>
    <sample id="249">在可接受的域中扰乱句子的方法是通过保留相关结构但添加噪声来扰乱输入句子。这些噪声不会显著改变模型的MPP评分，表明模型对可接受域中的潜在句法和语义特征敏感。</sample>
    <sample id="250">维度评估意味着通过评估多个方面来对对话质量进行更细致的分析，以理解模型的优点和缺点。</sample>
    <sample id="251">根据英语内容，论文的作者来自中国科学技术大学。</sample>
    <sample id="252">演讲者介绍了一项名为“U Create”的研究，该研究开发了一个用于法律文档中未监督的案例检索系统。该系统使用事件提取技术来识别与查询文档相关的相关文档。演讲者展示了ILPCR数据集和U Create管道作为主要贡献，ILPCR数据集是一个包含7,700个法律案件的基准数据集，而U Create管道则利用无监督学习技术进行事件提取。演讲者还讨论了不同模型的性能，包括基于计数、Transformer和事件的方法，并强调了事件方法在性能上的显著提升。演讲者表示，U Create为案例检索领域提供了新的方向，并鼓励进一步探索和开发。</sample>
    <sample id="253">马里亚·埃德拉·阿根廷在介绍了一种名为“Disorder”的双域适应模型，该模型用于检测社交媒体上的精神疾病迹象。该研究是墨西哥和西班牙研究人员的集体努力，旨在通过自动分析社交媒体帖子来检测精神健康问题，从而支持新科技的发展，以警告可能出现的精神疾病并提供支持证据。该模型使用了双域适应技术，即从一个相关领域（如维基百科和Google书籍）学习知识，然后调整词汇和语义理解以适应特定领域（如Reddit和心理健康）。通过使用指导掩码，该模型被引导关注重要单词，从而专注于精神疾病领域。结果表明，“Disorder”模型在精度和召回率方面表现出良好的平衡，而其他方法则在其中一个维度上表现出高精度或召回率但在另一个维度上得分较低。此外，“Disorder”模型倾向于预测与精神疾病相关的更负面含义的单词，而“BERT”模型则生成更一般性的单词。可视化工具展示了用户帖子中最重要的文本序列，突出了与精神疾病高度相关的主题。该研究结论认为，“Disorder”模型有效捕捉了社交媒体上精神疾病的迹象，并在精度和召回率之间取得了更好的平衡，优于由大量数据训练的“MENTAL BERT”模型。未来的研究计划探索使用不同词典资源和临床数据的应用。</sample>
    <sample id="254">The speaker introduces a research work on uncertainly guided label denoising for document-level distant relation extraction. They present the overview of their framework, which includes a pre-denoising doccano model with both DS and human-annotated data to generate pseudo labels. They also propose an instance-level uncertainty estimation method to capture the uncertainty score for overlapping relations and design a relabel strategy with dynamic class uncertainty threshold and a multi-phase training strategy to further boost performance. The speaker compares their framework with several strong baselines on two public datasets and shows that it outperforms the previous baselines.</sample>
    <sample id="255">在零和一短提示的情况下，提示的形式很重要。然而，在五短提示的情况下，形式几乎没有影响。</sample>
    <sample id="257">作者评估了四个state-of-the-art对话模型。</sample>
    <sample id="258">The speaker, Chang Sun He, introduces a new method for evaluating the quality of text in natural language processing using large language models. This approach involves instructing large language models with specific instructions to rate samples, aiming to provide an alternative to human evaluations which are often unstable and difficult to reproduce.

The motivation behind this work is to find a more reliable and consistent method for evaluating text quality, similar to how humans evaluate texts based on given instructions. The speaker explains that while large language models have been shown to follow natural language task instructions, no prior works had explored their use for evaluation purposes.

To validate the effectiveness of this method, the speaker conducted experiments using four different large language models: T0, InstructGPT, Qwen, and ChatGPT. The models were tasked with rating stories generated by GPT-2 or written by humans based on four criteria: grammar, coherence, likability, and relevance. Human evaluators, specifically English teachers, provided ground truth ratings for comparison.

The results showed that while some smaller language models did not show a significant preference for human-written texts, two larger models, DaVinci and ChatGPT, demonstrated a clear preference for human-written texts, similar to human evaluators. This suggests that certain large language models can serve as alternatives to human evaluations in text quality assessment.

The speaker invites further exploration of topics such as the agreement between large language models and human evaluators on individual ratings, the impact of changing instructions or sampling methods on model performance, and the benefits and drawbacks of using large language model evaluations compared to human evaluations. The paper provides detailed insights into these aspects and is available for further reading.</sample>
    <sample id="259">The speaker, Usen Jang from Peking University, introduces a new approach to cross-lingual semantic parsing in multiple natural languages and mind representations. The task of semantic parsing is to build semantic representations of user queries, such as SQL and Lambda Calculus. Cross-lingual semantic parsing involves translating queries in multiple natural languages into multiple mind representations. The proposed Exemplar framework provides a unified dataset for cross-lingual semantic parsing in multiple natural languages and mind representations, containing 9 datasets in various domains, 5 semantic parsing tasks, 8 mind representations, and 22 natural languages in 15 language families. The framework considers six settings for training and evaluation: translate test, monolingual model, monolingual fine-tune setting, multilingual model, cross-lingual zero-shot transfer, and cross-lingual few-shot transfer. The results show that encoder-decoder models outperform previous work on all nine datasets, and the performance gap between monolingual and multilingual models is significant. The speaker concludes by inviting the audience to visit their paper and code for more information.</sample>
    <sample id="260">根据所提供的内容，无法确定论文的作者数量。该段落没有提及作者姓名或人数。</sample>
    <sample id="261">理想规划器应该编写合理的和符合约束条件的脚本。</sample>
    <sample id="262">根据提供的内容，无法确定论文的作者人数。内容中没有提到作者的名字或数量。</sample>
    <sample id="263">The presentation introduces a new method for mitigating label biases in in-context learning (ICL), a popular paradigm for utilizing large language models. The speaker highlights that ICL is known to be unstable due to various design choices, such as the choice and order of in-context examples, which introduce biases into the model's predictions. There has been no systematic discussion on categorizing existing findings on bias problems in ICL or detecting new types of biases, nor on mitigating their effects.

The presentation proposes a novel calibration method to handle all types of biases, including vanilla label bias, context label bias, and domain label bias. The speaker explains that domain label bias captures the effect of task-specific words on the model's predictions. To address this, the proposed method uses random in-domain words sampled from the task corpus as content-free text to estimate the model's bias on each label name and calibrate the model's original predictions.

Experiments conducted using different models and datasets show that domain context calibration significantly improves the performance of ICL, especially on tasks with larger domain label biases. The method also results in better decision boundaries and more accurate predictions. The presentation concludes by summarizing the systematic investigation of label bias problems in ICL, the identification of a new source of bias, and the proposal of a calibration method that can improve the performance of large language models in ICL.</sample>
    <sample id="264">演讲者介绍了一种用于多模态文本生成任务的统一音频语义空间的概念。他们提出了一个框架，包括三个主要组件：音频-视觉映射网络、音频-视觉编码器和语言模型生成器。该框架旨在通过使用统一的音频语义空间来解决跨域视觉风格和音频能量等多模态领域偏移问题。此外，他们还提出了一个基于对比学习的训练方法，以优化视觉和音频元素之间的对齐。在实验中，他们比较了不同的模型，包括基于深度神经网络和Transformer的模型，并分析了音频特征和音频评论的影响。</sample>
    <sample id="265">演讲者的名字是瓦苏达。</sample>
    <sample id="266">根据提供的信息，无法确定论文作者所属的机构。在演讲中没有提到任何与作者相关的机构名称或细节。</sample>
    <sample id="268">PaLM 最常见的错误是省略错误，即它选择删除源句子中的一部分以产生更好的可读翻译。</sample>
    <sample id="269">Hello, I'm James Finch and I'm Sarah Finch. Today, we'll tell you all about ABC-Eval, a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Gino Choy at Emory University and in collaboration with Amazon Alexa AI. So let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art. The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale. These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level. One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation. Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. We call this approach annotating behaviors in chat, or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy. To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn level, Likert ratings on the dialogue level, and dialogue-level pairwise comparisons. For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions. From our analyses of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains five percent and ten percent of conversation quality respectively, while the average Likert consistency scores explain only four percent or less. Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABC-Eval metrics explains over twenty-five percent of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around twenty percent of their responses, they produce irrelevant information in around fifteen percent of the responses, and they contradict themselves or their partner around ten percent of the time. With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction, and we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.</sample>
    <sample id="270">根据英语内容，这篇论文的作者所属机构是Emory University。</sample>
    <sample id="271">在本文中，CFT代表“持续微调”，这是一种通过在干净的验证样本上继续微调模型来提高性能的方法。</sample>
    <sample id="272">根据提供的信息，这篇论文有六位作者：Kostya Sina、John Godfrey、Aaron Mueller、Kannika Mishra、Garen Fintz和Roger Levy。</sample>
    <sample id="273">你好，我的名字是Kayi Yin，我将展示我们名为“何时翻译需要语境：基于数据的多语言探索”的工作。这项工作是在与Patrick Franz、Emil Yu、Andrea F. Martinez和Graham Neubig合作完成的。许多翻译都依赖于语境。例如，在这个句子中如何翻译“more”？如果前一个句子是“如果政府发现了事情可能会变得危险”，那么“more”指的是一个间谍。但如果前一个句子是“医生，这是否有什么严重的？”那么“more”指的是胎记。因此，根据语境，单词的意义发生了变化，因此其翻译也会发生变化。然而，评估模型在处理这些情况时的表现并不容易。首先，因为只有少数翻译依赖于语境，使得语料库级别的指标如BLEU无法捕捉到这些翻译。有些人建议对依赖于语境的翻译进行针对性评估，但这些资源通常只支持有限类型的语境依赖翻译，并且只支持有限的语言，因为它们通常依赖于领域知识和人工标注。在这项工作中，我们试图回答这两个问题：第一，何时翻译需要语境？第二，模型在处理这些情况时表现如何？为了回答第一个问题，我们首先衡量单词在翻译时对语境的依赖程度。在之前的工作中，我们引入了SXMI作为衡量机器翻译模型对语境依赖性的指标。这是通过衡量给定源文本x时，语境c提供的关于目标y的信息量来实现的。你可以将SXMI视为给模型提供语境后获得的信息量。在这项工作中，我们将SXMI扩展到了点对点的SXMI，可以衡量句子或单词级别的语境使用。我们可以认为具有高pSXMI的单词是需要语境进行翻译的单词。现在，我们分析具有高pSXMI的单词，以查找其中的模式。我们对TED演讲的14种不同语言的翻译进行了分析。我们在三个不同的级别上进行分析：首先，我们查看具有高平均pSXMI的短语，这使我们能够找到，例如，阿拉伯语中的复数形式，这些形式具有相对较高的pSXMI。这可以通过解释来解释，因为英语没有复数形式，所以你需要语境来确定一个名词是否是复数，当翻译成阿拉伯语时。我们还发现某些语言在选择适当的动词形式时也依赖于语境。其次，我们分析具有高pSXMI的词汇项目，这有助于识别案例，例如，在中文中，你需要语境来正确翻译同名，以确保在整个文档中使用相同的翻译。我们还发现语境支持正确翻译正式性和一致性。最后，我们分析具有高pSXMI的单个标记，这使我们能够识别出仅凭单词本身无法捕捉到的模式，但会在句子结构中表达，例如，椭圆的解决。现在，我们使用我们从分析中得出的发现来设计文档级别的机器翻译基准。对于我们识别的五种 discourse 现象中的每一个，我们创建了自动识别属于该现象的单词的标签器，并将其称为多语言 discourse 意识或Muda 标签器。我们还可以注意到不同语言具有不同的 discourse 现象的比例。我们使用Muda 标签器通过在用于评估的平行语料库上应用标签器来识别这些 discourse 现象。然后，我们在Muda 标签器识别出的依赖于语境的例子上应用我们选择的翻译指标。最后，我们使用我们的基准以及其它指标来评估不同模型在文档级别的机器翻译上的性能。首先，如果我们使用语料库级别的指标，例如BLEU，我们发现依赖于语境的模型具有最佳性能。但是，如果我们使用COMET，依赖于语境的模型表现最好。如果我们使用Word F-Measure，则具有或不具有语境的模型具有可比的性能。这再次证明了，如果仅使用语料库级别的指标，就很难确定文档级别的机器翻译系统的最佳性能。我们使用Muda 基准来评估模型，并发现依赖于语境的模型在处理某些 discourse 现象时显著更准确，例如正式性和一致性。但是，这些模型在处理其他现象，如名词的复数形式和动词形式时，并不比不使用语境的模型好得多。这表明我们需要在文档级别的机器翻译中取得更多进展。我们还比较了不同的商业系统，并发现DeepL 通常比Google Translate在文档级别的机器翻译中更准确。总之，我们通过分析14种不同语言对的翻译，确定了何时翻译需要语境。然后我们利用这些发现构建了一个文档级别的机器翻译基准，该基准可以帮助我们确定哪些 discourse 现象模型可以良好处理，哪些翻译系统在文档级别的机器翻译中表现良好。谢谢大家的聆听，祝大家一切顺利！</sample>
    <sample id="274">演讲者的名字是金正恩。</sample>
    <sample id="276">本研究旨在评估印度语种的机器翻译质量，重点研究了印地语等五种语言。研究者从FOLOTUS数据集中随机选择了200个句子，生成了每个句子的多种候选翻译，并使用七种不同的翻译模型生成了1400个候选翻译。为了收集这些翻译的人类标注，研究者邀请双语专家对每个翻译输出进行详细评估，包括错误类型、严重性和总体评分。研究结果表明，Comet和 Comet MQM 在不同语言中表现出色，特别是在准确性错误方面。此外，Comet MQM 在所有语言中表现出更高的相关性。研究还发现，人类评分覆盖了整个评分范围，而许多指标往往提供窄范围的评分，这使得准确解释指标评分变得困难。</sample>
    <sample id="277">根据所提供的内容，新方法没有被指定名称。它被描述为一种“新颖的序列到序列模型”，它直接建模了输入和输出片段之间的对应关系，而无需使用树。</sample>
    <sample id="278">“显性词汇”(marked words) 方法是一种识别区分标记群体和未标记群体的词语的方法，通过使用加权log odds比率来区分每个标记群体的Top词语。这种方法有助于捕捉这些词语所反映的有害模式，而不仅仅是那些负面的词语。</sample>
    <sample id="279">这篇论文的作者来自美国华理大学。</sample>
    <sample id="280">The speaker introduces a multi-modal fusion framework for emotion regulation in conversations, which aims to predict the emotion label of each utterance in a dialogue. The framework consists of four key components: uni-modal feature extraction, context modeling, multi-modal fusion, and emotion classification. The speaker proposes a novel multi-modal feature extractor called Vis-Net, which captures visual cues by integrating facial expressions from multiple frames without encoding redundant scene-related information. The multi-modal fusion model, MultiTen, integrates one modality with complementary information from other modalities through stacked bi-directional multi-head cross attention layers. A sample-weighted focal contrastive loss is introduced to address the difficulty of classifying minority and semantically similar emotion classes. Experimental results demonstrate that MultiEmo achieves state-of-the-art performances on two ERCC benchmark datasets, MELD and iEMO CAP, with significant improvements in minority and semantically similar emotions.</sample>
    <sample id="281">演讲者介绍了一项名为“何时翻译需要语境：基于数据的多语言探索”的研究，该研究与Patrick Franzke、Emil Yu、Andrea F. Martinez和Graham Neubig合作完成。研究重点在于理解哪些翻译依赖于语境，并评估模型处理这些情况的能力。研究使用了C Xiaomi作为衡量机器翻译模型语境使用的指标，通过测量给定源文本时语境提供的信息量。研究分析了从英语翻译成14种不同语言的TED演讲的转录，识别出需要语境进行准确翻译的单词和短语。研究结果表明，不同语言和现象（如性别、形式性和连贯性）需要不同的语境水平。研究还开发了一个基准测试，用于评估文档级别的机器翻译系统在处理这些语境依赖性方面的能力。结果表明，基于语境的模型在处理某些现象（如形式性和连贯性）时表现出显著优势，但在其他现象（如性别和词形）上与未使用语境的模型相当。研究还比较了商业系统DeepL和Google Translate在文档级别翻译上的性能，发现DeepL通常更准确。</sample>
    <sample id="282">I'm Xuehao Zhu, and today I'm excited to present our new work in ACL 2023: StoryTrans - Non-Parallel Story-to-Style Transfer with Discourse Representations and Content Enhancing. Our research takes a significant step forward by addressing an important task in natural language generation: non-parallel text style transfer. Until now, most studies have focused on the token level or sentence level, such as sentence sentiment transfer or formal text transfer. However, transferring style-specific content from one style to another is challenging due to the loss of linguistic choices at the discourse level. To address this challenge, we propose a generative model named StyleTrans, which learns discourse representations from the source text and combines them with normal style embeddings to generate text in the target styles. We also design a new training objective to reduce the style-specific features from the discourse representations, pulling the representation derived from different texts closer in the latent space. Additionally, we separate the generation into two stages: first, we transfer the source text with style-specific content keywords masked, and then generate the whole text by incorporating these keywords explicitly. For the training framework, we use an adversary training framework with self-reconstruction loss, disentanglement loss, sentence-level loss, and style classifier loss. In the second stage, we aim to fill in the correct style-specific content and remove the masked tokens. Our experiments show that StyleTrans outperforms strong baselines in terms of style control and content preservation.</sample>
    <sample id="283">第一个提到的对称依存关系结构的名称是“Prague Approach”或“Conjunction Headed Approach”，它是一种依存关系树形结构，其中协调结构由连词充当头部。</sample>
    <sample id="284">The speaker introduces a new spam boundary mechanism for enhancing universal information extraction (UIE) in the context of spam-based UIE models. The current model relies on identifying and labeling spam boundaries based on annotated spans, but this approach has ambiguity issues due to different annotation spans being considered reasonable. To address this, the proposed method uses a fuzzy boundary instead of a precise one, allowing for more flexibility.

The speaker also highlights the mismatch between Transformer feature extraction and information extraction, noting that basic Transformers focus on global features which may not be suitable for span-based tasks. Therefore, an adaptive attention mechanism is proposed to better model the fuzzy spam boundary. This involves representing the target boundary as a continuous distribution of correct probabilities within a specific range, where Rmin and Rmax denote the start and end of the fuzzy boundary, and Q represents the correctness of each position.

To calculate the fuzzy spam loss, the boundary cross-entropy with the golden boundary is used, along with KL divergence between the predicted boundary and the fuzzy spam boundary. This method helps in obtaining a more reasonable attention distribution for spam extraction. A fuzzy spam attention mask function is introduced to trim the attention distribution dynamically, reflecting the fuzzy spam boundary in two aspects: adjusting the length of the attention range through an optimizable parameter delta, and linearly decreasing the attention distribution on the spam boundary rather than truncating it.

The overall structure of the proposed model includes a fuzzy spam attention layer added at the top level to guide the model's decision process without affecting text encoding capability. Experiments on three information extraction tasks—named entity recognition, relationship extraction, and aspect sentiment triple extraction—show significant performance improvements with the introduction of FSFL and FSA compared to the UIE base without the fuzzy spam mechanism. The model is particularly effective on small-scale datasets, demonstrating stronger generalization capabilities for domain-specific information.

The results of the ablation study indicate that FSFL improves convergence speed by guiding the model to obtain a reasonable attention distribution, enabling it to fully utilize annotated information and achieve greater information extraction capability. The combined effect of FSFL and FSA produces a greater enhancement in performance. Visualization of the attention distribution of the fuzzy spam attention layer shows that the model focuses on semantic information within a limited range of preceding tokens, aligning with the expected behavior.</sample>
    <sample id="285">The speaker, Minchi Gao from Peking University, introduces a research paper titled "Fact Error Correction for Dialogue Summarization with Reference-based Evaluation Framework." The paper focuses on addressing the issue of fact errors in summaries generated by models and even some reference summaries. Two main solutions are proposed: introducing factuality-related objectives during the training or inference process to make summarization models more faithful, and designing a fact error correction model (FEC) that is independent of the summarization model.

The speaker explains that current FEC models have flaws, such as using factuality metrics like Fact CC and DA to evaluate the quality of corrected summaries, which may not be reliable on their own and can blur the line between the two types of solutions. To address these issues, the paper proposes introducing manually annotated reference corrections to create more comprehensive and accurate evaluation data for training FEC models.

The paper also introduces a taxonomy of fact errors based on content and form, categorizing errors according to parts of speech and dependencies, and whether they involve additions, deletions, or substitutions. An evaluation framework consisting of alignment, classification, and comparison steps is proposed to automatically classify fact errors for FEC.

The results show that training FEC models with reference summaries from dialogue summarization datasets yields the best results, highlighting the urgent need to change the evaluation method for FEC models. Combining human-annotated data with synthetic data is suggested as a promising direction, and current FEC models struggle to correct fact errors by addition and cannot address attribute errors, modality errors, and link errors.</sample>
    <sample id="286">演讲者的名字是James Finch和Sarah Finch。</sample>
    <sample id="287">根据所提供的内容，该论文有四位作者：Javahar Hosaini、Philip Radlinski、Sylvia Parati和Annie Liu。</sample>
    <sample id="288">用于测试句法现象的数据集包括Bling数据集和Syntax Gym数据集。</sample>
    <sample id="290">根据提供的内容，第一个研究问题的五种方法的缩写是WSL，代表“弱监督学习”。</sample>
    <sample id="291">该模型在11个生物医学和临床任务上进行了评估，包括命名实体识别、分类、分词和问答。</sample>
    <sample id="294">CamemBERT 最初是在一个名为 NAO 的数据集上训练的，该数据集包含来自 Web 的医学 crawl 数据。</sample>
    <sample id="295">演讲者的名字是Adam Siprowski。</sample>
    <sample id="296">这个视频介绍了由都灵大学和亚马逊Alexa合作开发的一个项目，旨在解决自然语言处理中的讽刺检测问题。项目的主要目标是开发能够识别讽刺的模型，而不仅仅是简单地将句子标记为讽刺或非讽刺。为了实现这一目标，研究者创建了一个名为Epic的语料库，包含来自社交媒体、Reddit和Twitter的300对短对话，每对对话都由两个文本组成，一个接一个。这些对话被不同来源和英语变体的注释员标注，以评估不同群体之间的注释一致性。研究者还开发了“ prospectivist aware”模型，通过微调预训练的自然语言模型来处理不同注释员之间的差异。结果表明， prospectivist aware模型在预测时比标准聚合模型更自信，尤其是在年龄和地理分布方面存在显著差异。例如，年龄接近的注释员在判断讽刺时意见分歧更大，而来自英国和爱尔兰的注释员在判断上存在最大差异。</sample>
    <sample id="297">本研究项目旨在开发一种词典和词汇表，包含340个与种族主义、种族主义和反犹太主义相关的术语和符号，这些术语和符号收集自各种来源，如学术、维基百科、博客等。这些术语和符号主要用于识别政治演讲中的狗哨子。研究还对美国政治演讲进行了案例研究，发现种族主义狗哨子的频率与共和党南方战略和保守主义的增加有关。此外，研究还评估了GPT-3等语言模型在识别狗哨子方面的性能，并探讨了如何通过使用前景API和 hatecheck等工具来绕过内容审查。</sample>
    <sample id="298">导致时间漂移是性能下降的主要原因的发现包括实验结果，即通过使用更近的数据重新训练或继续预训练模型，性能会随着训练和测试数据之间的时间间隔增加而恶化。这证实了时间漂移是观察到的性能下降的主要原因的假设。</sample>
    <sample id="299">本演讲介绍了通过使用最小化训练来提高AI模型的稳健性的方法。最小化训练是一种技术，用于减少AI模型对输入数据中潜在的“快捷方式”的依赖，这些快捷方式可能导致在分布外数据上的性能不佳。演讲者解释了AI模型如何学习并利用这些快捷方式，导致在训练数据上的表现良好，但在分布外数据上表现不佳。他们提出了一个训练方法，通过在训练过程中使用一个“辅助模型”来减少AI模型对这些快捷方式的依赖。这个辅助模型被训练来最大化AI模型的损失，同时生成权重，以鼓励AI模型专注于更难的样本，从而提高其在分布外数据上的泛化能力。该方法在三个广泛使用的AI数据集上进行了验证，并显示出在分布外数据上的性能显著改善，同时保持了高分布内准确率。此外，该方法还探讨了预训练、辅助模型大小和学习样本权重分配的影响。演讲者强调了该方法的灵活性和对分布外性能的改进，邀请观众在随后的研讨会期间讨论。</sample>
    <sample id="300">演讲者介绍了一项名为“互动口述”的任务，这是一种允许用户使用语音进行口述和编辑文档的自然直观过程。演讲者解释说，与传统的口述系统不同，互动口述系统可以识别并应用用户的语音命令，而无需使用触发词或预定义的口令。演讲者还提到了一些支持语音口述和编辑的软件，如Neon Dragon Naturally Speaking和Microsoft Dictate功能。然而，这些系统通常不支持自然语言命令，因此需要记忆固定的一组模板命令。演讲者强调了开发更自然和直观接口的重要性，指出人类在与人类助手互动时可以区分口述和命令，而不需要使用触发词或命令。演讲者还介绍了任务的四个步骤：语音识别、转录分段、命令提取和执行。此外，演讲者还介绍了数据收集接口和基线系统的设计和构建。演讲者还提到了一些实验结果，包括使用不同的模型架构和输出类型进行语音识别、转录分段、ASR修复和解释模型的评估。最后，演讲者提到了代码和论文中提供的更多细节。</sample>
    <sample id="302">在处理输出序列中的词元时，排列是必要的，因为虽然模型已经识别了所有正确的词元，但它们的顺序在训练数据中并没有给定。因此，在预测阶段，需要确定词元的正确顺序以生成有意义的输出。排列过程通过使用另一个模型来预测一个排列，以将词元按照正确的顺序组织起来，从而实现对输入序列的准确和连贯的输出。</sample>
    <sample id="303">作者建议模型所有者提高偏见缓解方法的透明度，因为这有助于更好地理解这些方法如何影响模型生成的输出。缺乏透明度使得很难确定这些模式是否源于过度价值对齐、反刻板印象技术或其他因素，这可能导致刻板印象和本质化叙事。增加透明度将使研究者能够更有效地识别和解决这些模式，从而减少模型中的偏见。</sample>
    <sample id="304">最小对不可接受输入是一种评估语言模型的方法，其中模型被要求在不可接受的句子上赋予更高的概率。</sample>
    <sample id="305">演讲者介绍了一项关于弱监督学习（WSL）的研究，这是一种机器学习方法，其中数据不是完全手动标注的，而是使用弱标注来源，如简单启发式规则、知识库或低质量 crowdsourcing。WSL 的目标是通过训练神经网络来处理 noisy 标注数据，以确保模型能够泛化良好。然而，研究发现，WSL 方法需要干净的验证集才能有效工作，否则性能会显著下降。研究还发现，增加干净验证样本的数量可以提高 WSLL 方法的性能，但直接在干净数据上进行 fine-tuning 通常会带来更好的结果。此外，允许在干净验证样本上继续 fine-tuning 可以实现与更复杂方法相当的性能改进。演讲者建议在报告模型选择标准时应包括是否使用干净验证样本，并将 WSLL 方法与全监督 baseline 进行比较。他们还建议考虑在 WSLL 中使用持续 fine-tuning 作为简单而强大的 baseline。</sample>
    <sample id="306">Hello everyone, I am Sebastian Ruder and together with Naljung Kim, I'm going to give you a short overview of our work on entity tracking in language models. For an agent to understand the discourse, it needs to track which entities are mentioned and how their state changes as the discourse unfolds. So for example, in the context of a recipe such as here, an agent has to understand that put the eggs, sugar, and flour in a bowl results in all of these three entities ending up in a bowl. And if the discourse continues with mix to form a light batter, then the agent has to understand that now all of these entities are part of the batter. And so we argue that this is a crucial ability for understanding long discourses but there haven't really been any systematic investigations into what pretrained language models can actually perform such tasks. And so the overarching research question we're trying to answer in this paper is to what extent large language models can track entities. Now given that we don't know the exact contents of the pretraining data of many language models and considering several properties about how discourses work, there are actually several challenges with designing a task to evaluate entity state tracking abilities. First, some entity states will be common in the pretraining data and therefore the model may predict the correct state without actually having any entity tracking abilities. For example, eggs often end up in bowls or babies often end up in cribs. So we want to make sure that the distributional patterns in the pretraining data cannot give away the entity states in the evaluation data. Then second, sometimes entity states can be predicted from individual words or phrases without actually considering the larger discourse and the model may seem to be able to perform entity tracking while in fact it just learned simple heuristics associations between words and entity states. For example, that the word empty is always associated with an entity being empty. And third, if one uses fine-tuning or in-context demonstrations which is often necessary to probe the model, then the model may memorize entity state sequences or it may learn to apply heuristics such as slot filling if such heuristics are not blocked in the evaluation task design. And so in designing our evaluation task, we took great care to make sure that the model cannot make use of any of these shortcuts when we evaluate its entity tracking abilities. And Naljung will tell you a bit more about how we set up this task. Oh my name is Naljung Kim and I'll be talking about the task design and the experimental results. So to evaluate entity tracking abilities, we designed the following task involving boxes and objects. And in our setup, the input to the model starts with a description of the initial contents of each box as sketched on this slide. And the task of the language model is to complete the input by predicting the contents of each box. Now given just this initial description, the task is pretty trivial, the model can just copy the relevant information from the description. But in our task, we also include multiple state-changing operations like moving objects or adding objects to a box. So for these, the model would have to combine the initial description with the operations to make the correct prediction. For example, Box 1 now contains the car and the watch after moving the watch from Box 3 to 1. And additionally, we implemented various measures to prevent the model from using heuristics as Sebastian discussed earlier on, so please check out our paper for how we did this. We tested this setup with Flan-T5 and GPT-3 and 3.5 models using two-shot in-context learning. And what we're showing here is the accuracy of predicting the correct box content as a function of the number of operations acting on a certain box. And on the left panel, we have the data points where the proven entity state is different from the state in the initial description whereas on the right panel, we have cases where the state is the same as in the initial description. So for these ones, the model can simply copy. And our experiments show that most models simply repeat the initial state as you can see from the generally high accuracy on the right panel. And we can also see that only text-davinci-03 accepts non-trivial tracking, which is the pink line here in the left panel. And all other models perform below a strong random baseline obtained by random simulation, which is the blue line. So what gives rise to this difference between models? Since the models we tested varied along several different dimensions, we investigated what other factors might be in play by zooming into the GPT series. And we found that all GPT-3.5 models, which all have been trained on substantial amounts of code, exhibit non-trivial entity tracking behavior. Whereas all models that do not have code as a substantial part of their pretraining do not. And this suggests that pretraining on code is what's responsible for making this capacity surface in pre-trained language models. We also found that smaller models like T5 base can learn to perform entity tracking if you directly fine-tune the models, but on the other hand, randomly initialized models of the same architecture cannot learn our state tracking task even when they receive direct supervision, suggesting that pretraining is again important here. However, as we discuss in more detail in the paper, it remains unclear whether the state tracking abilities we observe generalize beyond our setup in this case. Thanks for listening and we have a lot more results and analyses, including GPT-4 experiments in our paper, so please check out arXiv. And if you have any questions or comments about our work, either find us in person at ACL or you can reach out to us over email or on Twitter. Thank you.</sample>
    <sample id="307">根据所给的英文内容，作者使用了几个评估指标来评估他们的模型。这些指标包括命名实体识别、分类、分词和问答。这些任务与六个基线模型进行了比较，以确定他们的系统在任务上的表现如何。</sample>
    <sample id="308">Jenny, a first-year PhD student at Carnegie Mellon University, is presenting her work on NLP positionality. She explains that NLP positionality refers to the biases in AI models and datasets due to the demographics, identities, and life experiences of the researchers and developers behind them. These biases can lead to systematic performance differences in technology across populations. Jenny's work involves comparing annotations from real users with existing datasets and models using a framework called NLP positionality. This framework includes re-annotating datasets with diverse annotators and comparing their annotations to model predictions and labels. Her study found that NLP datasets and models are most aligned with English-speaking countries and people with college or graduate education. However, they are less aligned with non-binary individuals. Jenny recommends keeping records of design choices, approaching NLP research with a lens of perspectivism, and building specialized datasets and models for specific communities.</sample>
    <sample id="309">在所给内容中，使用了内部注释者一致性来衡量注释者之间的一致性。</sample>
    <sample id="310">在不可接受和可接受查询中，选择的领域是维基百科。</sample>
    <sample id="311">根据所提供的内容，无法确定论文的作者所属机构。演讲中没有提及任何机构名称或与作者相关的组织信息。</sample>
    <sample id="312">MultiInstruct 是第一个大规模多模态指令调优基准数据集，包含62种不同的多模态任务，涵盖10个主要类别。这些任务是从21个现有的开源数据集中派生出来的，并且每个任务都配备了5个专家提供的指令。</sample>
    <sample id="313">根据提供的信息，论文的作者是Emily NLP Lab的Gino Choi教授和Amazon Alexa AI的合作者。因此，至少有两位作者参与了这项研究。</sample>
    <sample id="314">二进制协调指的是协调结构中存在两个主要的从句，这些从句通过一个连接词（如“和”、“但”或“或”）连接在一起。在二进制协调中，通常有一个主从句，另一个从句可以是主语、谓语或宾语。例如，“我昨天读了这本书，我很喜欢它。”在这个例子中，“我昨天读了这本书”是主从句，“我很喜欢它”是从句。</sample>
    <sample id="315">在本研究中，提示语的平均长度为12.5个单词。</sample>
    <sample id="316">这些发现表明，较小的 T5 模型可以生成高质量的脚本，与大型语言模型相比，这表明较小的模型在适当的训练数据集上可以支持较大的模型。</sample>
    <sample id="317">Hello everyone, I am Pinglei from Fudan University. I'm very delighted to present our work titled "CodeIE: Large Code Generation Models as Better Few-Shot Information Extractors." Information extraction is a classic task in natural language processing that involves extracting structured information from unstructured text. Common information extraction tasks include named entity recognition (NER) and relation extraction (RE). For the input text "Steve became CEO of Apple in 1998," the model needs to recognize that "Steve" is a person's name and "Apple" is an organization's name. Previous information extraction models, such as pre-trained language models like T5 and GPT-3, operate in a test-to-test manner during the pre-training stage. However, during the inference phase, the structured output of information extraction is linearized into a plain sequence. The problem with this approach is that it aligns the input format between the inference and pre-training stages, leading to mismatched outputs where one output is a plain text while the other is a structured output, making it challenging for the model to generate the correct structures. This often requires a large amount of structured training data and special decoding strategies to mitigate this issue. To address the problem of mismatched outputs, we propose "CodeIE," which transforms the test-to-structure information extraction task into a structure-to-structure code generation task and uses large language models like CodeT5 to perform it. This way, we can easily convert tests to structured formats during the input stage and ensure aligned structures in the output stage. Specifically, for the named entity recognition task, we have designed the following prompt: First, we define a function for named entity recognition that takes an input test as input. We add a comment to extract named entities from the input test to this function. Then, we set the input test to the actual input test and initialize an entity list called entity_list. We provide a comment saying "extracted named entities" to trigger the subsequent content. With a few short in-context demonstrations, we can prompt the model to output the following code: Continuously extract test and entity pairs and append them to the entity_list. For relation extraction, we have designed similar prompts. We evaluated our method on three named entity recognition datasets and four relation extraction datasets. Our evaluated models include the T5 model, the UIE model, the T5-DAVinci-02 version of GPT-3 model, and the CodeDAVinci-02 version of CodeT5 model. We compared the performance of two types of prompts: one using traditional test-to-test prompts and the other using code-to-code prompts described before. In the case of the W12 dataset, we found that our proposed approach using large language models and code format prompts significantly and consistently outperformed the traditional baseline models such as UIE and natural language large language models like GPT-3 model. We further conducted a detailed and in-depth analysis of these phenomena. Firstly, we observed that the perplexity computed on test format inputs using models like T5 was generally higher than that of code format inputs using models like CodeT5, indicating that transforming information extraction into a code generation task and using code pre-training language models aligns better with the information extraction task itself. Furthermore, we observed that when decoding with GPT-3 and test format prompts, there were many structural errors, whereas when using CodeT5 and code format prompts, such errors were almost non-existent. We also analyzed that using GPT-3 for information extraction tasks often generates outputs with labels that are not present in the predefined label set, such as currency, company, color, organization, and so on. Additionally, we found that the CodeT5 model outperformed the GPT-3 model in information extraction tasks overall. Furthermore, the model using code format prompts performed better than test format prompts, especially in terms of recall. We hope this analysis can provide some inspiration to everyone. Lastly, thank you all. If you have any questions, please feel free to contact me. Our paper and code are made publicly available.</sample>
    <sample id="318">大家好，我是Yanis Lavrac，我将向您介绍我们在生物医学和临床领域中使用Dr. Bert的最新进展。Dr. Bert是一个基于Robert和NatChos数据集的鲁棒预训练模型，在健康保健领域的语言建模方面取得了显著成果。在本演示中，我们将首先讨论语言建模在健康保健领域的应用，然后展示我们论文的主要贡献。我们引入了第一个用于生物医学领域的法语预训练模型Dr. Bert，并将其与多种预训练设置和数据源进行了比较。我们展示了在11个生物医学和临床任务上的结果，并讨论了实验细节以及如何访问这些模型。自2019年发布以来，Dr. Bert已成为解决自然语言处理任务最有效的方法之一，相比传统的静态和概念化方法（如Word2Vec、FastText和ELMo）提供了巨大的性能提升。自那时起，该模型已被应用于多种语言，包括法语（Camembert）、生物医学（PMBERT和BioBERT）和临床（ClinicalBERT），但大多是在英语上。对于其他语言的专门化模型仍然 scarce，通常基于持续预训练，因为缺乏内部数据。然而，法语没有开放的生物医学和临床模型。因此，我们提出了一个关于什么是最适合广泛使用的数据集的问题。这些 crawled 数据是临床数据的良好替代品。为了回答这个问题，我们将Dr. Bert与我们的Shubert模型进行了比较，该模型基于从蒙特利尔大学医院获取的匿名化数据。我们还询问需要多少数据来训练一个专门针对法语数据的模型，是4GB、8GB还是更多？为了回答这个问题，我们分别训练了四个从头开始的模型：一个版本的Dr. Bert使用7GB的NatChos数据集，一个版本的4GB子集的NatChos数据集，一个版本的Shubert模型使用4GB的临床句子，以及一个混合了4GB NatChos子集和4GB临床句子的Shubert模型。此外，我们还引入了三个基于PMBERT预训练的模型，分别使用NatChos数据集的4GB子集、临床句子的4GB子集和英语生物医学模型的4GB子集。总共，我们有7个模型。为了评估这7个模型，我们使用了公开和私人的生物医学任务，如命名实体识别、分类、分词和问答。这些模型与6个基线模型（Camembert Oscar 138GB、Camembert Oscar 4GB、Camembert CCNet 4GB、PMBERT BioBERT和ClinicalBERT）进行了比较。结果表明，具有相同数据类型的模型表现最佳。然而，我们观察到来自不同来源的数据似乎更具通用性。此外，使用更多数据通常会导致更好的性能。总的来说，从头开始预训练似乎在大多数任务上获得了更高的性能。然而，我们对使用PMBERT权重和分词器的持续预训练进行的实验显示，其结果与从头开始预训练的Dr. Bert 4GB相当，而使用Camembert权重和分词器的模型则出现了稳定性问题。最后，我们的系统在11个生物医学和临床任务中表现更好，并且在所有任务上超越了通用模型Camembert。我们还观察到，专门化的数据效果更好，但不具有可扩展性。预训练模型和训练脚本均在Hugging Face和我们的GitLab存储库上提供。谢谢您的收看，我们期待在Toronto的研讨会交流。</sample>
    <sample id="319">论文研究了从头开始预训练和持续预训练两种学习策略。在持续预训练中，模型使用了不同的预训练策略，如基于CamemBERT的预训练、基于PUBMEDBERT的预训练和基于English biomedical model的预训练。这些策略旨在分析持续预训练对模型性能的影响。</sample>
    <sample id="320">根据所给英文内容，过拟合因素在测试重复使用的情况下并不明显。这可以通过图表中红色最佳拟合线的斜率大于1来证明，这意味着每单位改进在原始数据集上转化为超过一个单位的改进，在新数据集上。这表明没有减弱的回报，因此过拟合在这个案例中并不明显。</sample>
    <sample id="321">根据演讲内容，评估简化质量的方法包括分析句子对的类型和复杂度。例如，圣经文本通常被简化程度更高，而新闻文本或语言学习者的文本则较少简化。此外，还观察了不同类型的简化变换，如词汇替换、句法简化和整体简化水平。这些分析有助于确定简化是否有效。</sample>
    <sample id="322">The speaker, Enrico, is presenting at ACL 23 and discussing the challenges of teaching language models to understand morality in text. He begins by explaining that human morality helps us distinguish right from wrong and is essential for societal functioning. However, current approaches often treat morality as a singular scale between immoral and moral, which can hide the subjective nature of morality and lead to misunderstandings.

Enrico introduces the Moral Foundation Theory (MFT), which posits that there are five different ways humans perceive morality, similar to how taste buds perceive flavors. Each action or concept triggers a different moral foundation, and individuals prioritize these foundations differently. This theory has been applied in natural language processing to classify morality in text, but it remains unclear how language models actually learn about morality.

To address this, Enrico's team has developed a method using explainable AI techniques to understand how language models learn about morality across different domains. They used the Mora Foundation Twitter Corpus, containing 35,000 tweets from seven domains, including #AllLivesMatter and #BlackLivesMatter. The goal is to determine if language models recognize that morality is expressed differently in various domains, such as the contrasting rhetoric around subversion in ALM and BLM.

The speaker concludes by emphasizing the importance of understanding these differences to avoid misinterpretations of morality, which could have dangerous consequences. He invites the audience to see him at ACL in Toronto.</sample>
    <sample id="323">The paper titled "Dynamic Heteregras Graph Running with Language Models and Knowledge Representation Running for Commonsense QA" by Yu Jia Wang from Shanghai University China, discusses a challenge task called Commonsense QA. This task requires models to answer questions that rely on common knowledge to test their language understanding abilities. The paper proposes a method to retrieve relevant knowledge from an external source using a knowledge base and language models. However, the authors introduce some new entities during subgraph retrieval, such as top bank and cake in the subgraph, which are largely unrelated to the question context. To address these issues, the authors propose a method to build an HKG (Heterogeneous Knowledge Graph) based on a mutable knowledge base, optimize the structure and knowledge representation of HKG, and use the language model to encode and fuse the HKG. They also remove subwords that make up the predicate entity and add additional nodes to the subgraph. Finally, they get the embedding of entities and relations by min pooling and update the entity and relation embeddings of HKG by LM loss. The paper reports good results on Commonsense QA and OpenBookQA compared with other methods.</sample>
    <sample id="324">是的，语言模型有不同的政治偏见。</sample>
    <sample id="325">Hi, my name is Marius Lindemann and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multi-set tagging and latent permutations. This is joint work with my advisers Alexander Colle and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this: As usual we have a training set of utterances in this case the girl slept and Mary knew that the girl slept these utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example the model has seen shallower recursion during training and is tested on an example with deeper recursion. Naive sequence-to-sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically this involves considerable formalism-specific pre-processing of the logical forms for example to handle variable symbols. Obtaining trees may also involve specialized grammar induction procedures. In this paper we don't use trees and introduce a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output. For the first time we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First we tag each input token with an unordered multi-set of tokens that will appear in the output. After the first step we have all the right tokens but they are not ordered. That's why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually our permutation model works roughly like this: We go from left to right over the output and determine which multi-set token to put in every position. For the first output position we simply select one as highlighted in red. Then we jump to the next multi-set token to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multi-set token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results here we compare our method with other treeless models on the CoNLL benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kind of structural generalization remain very challenging though. In our paper we solve a couple of interesting technical challenges. First of all the alignment between input and output is not given in the training data. As a consequence for a given token we don't know which multi-set it came from which poses a challenge for training. In addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is NP-hard. That's because this is related to the traveling salesman problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to back propagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster.</sample>
    <sample id="326">认知失调是指一个人同时持有两个不一致的信念或行为，例如一个人知道吸烟有害健康，但仍然吸烟。</sample>
    <sample id="327">本文介绍了视觉语言学习的进展，特别是基于Transformer的视觉语言模型的成就。作者提出了一个名为“Manager Tower”的新模型，它通过引入多个预训练的单模专家作为输入，适应性地聚合这些专家在每个跨模层中的信息。Manager Tower使用视觉和文本编码器，并通过Manager在每个跨模层中聚合信息来实现更有效的多模态知识利用。实验结果表明，Manager Tower在视觉语言预训练任务上取得了 superior 的性能，特别是在WICV2023测试集上提高了39.15%的准确率。</sample>
    <sample id="328">根据讨论，GPT-4被确定为最倾向于自由派的语言模型。</sample>
    <sample id="329">本文提出了一种基于结构化伪标签生成的零样本视频检索方法。该方法首先使用预训练的图像-文本模型生成更复杂的伪查询，然后使用预训练模型匹配视频帧和伪查询以生成伪事件。接下来，通过计算事件内和事件外的视频帧与查询文本之间的相似性，确定事件质量。最后，使用这些伪事件和伪查询生成伪标签，并使用这些伪标签训练视频检索模型。该方法在两个数据集上表现出更好的性能。</sample>
    <sample id="330">在主动学习时，累积训练与迭代训练相比，累积训练在累积数据方面表现得更有效。累积方法将所有从主动注释中收集的数据累积起来，而迭代方法则在每次迭代中使用最新的数据集来更新模型。因此，在主动学习的背景下，累积方法通常会提供更好的性能。</sample>
    <sample id="331">演讲者的名字是Sarah Papi。</sample>
    <sample id="332">MuDa基准中的数据是从TED演讲的转录中获得的，这些演讲已被翻译成14种不同的语言。</sample>
    <sample id="333">The speaker introduces a new method for improving machine translation by injecting common knowledge into the model. They acknowledge their collaborators and explain that their work focuses on neural machine translation, which often results in a non-smooth representation space that limits generalization ability. The proposed method involves training a key-value data store to save representations and their corresponding target tokens, and then retrieving nearest neighbors from this data store to refine prediction probabilities. However, this approach has drawbacks such as time-consuming retrieval of neighbors and difficulty updating representations. To overcome these issues, the speaker proposes a framework called Ink to inject common knowledge into the model. The Ink training loop involves extracting common knowledge from the data store to guide the adapter to adjust the representation, and then using updated representations to refresh the data store asynchronously. The speaker concludes that the Ink system outperforms the state-of-the-art common knowledge machine translation (CKMT) system and achieves the best performance after smoothing the representation space.</sample>
    <sample id="335">演讲者的名字是马提亚斯·林登曼。</sample>
    <sample id="336">跨语言转移是指在一种语言中训练模型，然后使用该模型在另一种语言中进行预测的过程。</sample>
    <sample id="337">The speaker introduces a neural network approach to handle out-of-vocabulary (OOV) words in embedding-based downstream models. They propose a word relationship graph that captures lexical rules of word formation and association, allowing the model to recognize OOV words by decomposing them into word pieces and associating them with relevant words. The model uses a two-level graph structure, where each word or word piece acts as a node, and its corresponding word embedding serves as the node feature. The first layer retains complete word piece information, while the second layer reduces the number of nodes for training to mitigate noise from nodes with numerous neighbors. To address the issue of assigning node attributes to OOV nodes, they utilize a self-attention mechanism to extract important information and reduce the impact of noisy neighbor nodes. The model incorporates a readout block to generate a graph-level representation, capturing the whole graph information and summarizing word formation. They also apply contrastive learning in the loss function to encourage similarity between positive samples and push negative samples further apart. Extensive experiments demonstrate the effectiveness of their model on both intrinsic and extrinsic tasks. The model can bring performance improvements to both static and contextual models in downstream tasks. The applicability of the model to other languages depends on the rationality of word decomposition, with native languages being more suitable due to their simplicity in word formation.</sample>
    <sample id="338">The speaker, Bing Shen, introduces a collaborative research project titled "Our Human Explanations Always Helpful Towards Objective Evaluation of Human Natural Language Explanations" from researchers at Rensselaer Polytechnic Institute, Northeastern University, and IBM Research. The presentation covers the motivation, related works, and contributions divided into three sections: unified structure, preliminary experiments, and evaluation of five datasets using two models. The study explores the effectiveness of human-annotated explanations in training models to generate human-understandable explanations and improve prediction performance and reasoning ability. Traditional metrics like BLEU and ROUGE treat human annotations as gold standards, focusing on word similarity, while the Simulatability Score measures baseline performance changes with explanations but neglects task differences and explanation utility during fine-tuning and inference stages. The research proposes a new evaluation metric called TRU, which extends the Simulatability Score by evaluating the helpfulness of explanations during fine-tuning. The study finds that even low-quality human explanations can lead to substantial improvements in model predictions, and the proposed TRU metric outperforms the Simulatability Score in evaluating dataset qualities across five datasets using two models, T5 and BART.</sample>
    <sample id="339">根据提供的英文内容，论文的作者是来自德国萨尔兰大学的PSD学生。</sample>
    <sample id="340">Guanhao Huang, from UCLA, is presenting a work called "ParaMR," which stands for a large-scale syntactically diverse paraphrase dataset by AMR back translation. This is a joint work with Varan, Ee-hong, Anup, Kai-Way, and Arun. Paraphrase generation is a long-standing and important task in NLP domains, benefiting various NLP applications such as question answering, chatbots, and improving robustness. To train a good paraphrase generator, a large scale of high-quality paraphrase data is usually needed. However, existing human-annotated datasets like MRP, PPaN, and Cora have high quality but are limited in scale. Automatically generated datasets like back translation can generate a large scale of paraphrase data, but they lack syntactic diversity. The goal of this work is to construct a large-scale syntactically diverse paraphrase dataset using AMR graphs. AMR (Abstractive Meaning Representations) is a directed graph that captures the abstract meaning of a sentence, where each node represents a semantic concept in the sentence and each edge represents a semantic relation between concepts. The root node represents the main assertion of the sentence. The proposed method uses AMR back translation to generate syntactically diverse paraphrases. First, a pre-trained AMR parser is used to get an AMR graph of a source sentence. Then, the focus of the graph is changed by randomly sampling a node and setting it as a new root node, and modifying the corresponding edges. Finally, an AMR graph-to-text generator is used to generate text from the modified graphs. The generated texts share the same AMR graph structure, so they will have similar semantics. Because the generator emphasizes the focus at the beginning of the sentence, their syntax will be a little bit different. By using AMR back translation, we can get our proposed dataset, ParaMR. There are around 15 million source sentences in ParaMR, and there are around 6.9 paraphrases per source sentence. Some examples of ParaMR compared to other datasets that use back translation show that ParaMR usually generates more syntactically diverse paraphrases. We also present some quantitative analysis for ParaMR. We have automatic scores and human evaluation scores. Those scores indicate that ParaMR has similar semantic similarity scores to other datasets which use back translation, but ParaMR has higher syntactic diversity scores. That means ParaMR is syntactically more diverse compared to existing datasets while preserving good semantic similarity. We demonstrate that ParaMR can benefit several NLP applications. The first application is learning sentence embeddings. We learn sentence embeddings with different paraphrase datasets, and we find that the sentence embeddings learned from ParaMR can perform better than other datasets in the SCS testing benchmark. For the second application, we consider syntactic control paraphrase generation. We show that by training with ParaMR, we can get a paraphrase generator that has better syntactic control. Finally, we consider using a paraphrase generator to generate paraphrases for data augmentation for future learning. Since ParaMR is more syntactically diverse, we observe that ParaMR can get higher scores for future learning. Here is the conclusion of our work. In this work, we propose ParaMR, a large-scale syntactically diverse paraphrase dataset which is constructed by AMR back translation. And we show that ParaMR benefits several NLP applications compared to existing paraphrase datasets. Our dataset is available at this link. Thank you.</sample>
    <sample id="341">作者使用了平均延迟和计算延迟时间来衡量模型的性能。平均延迟是翻译输出的延迟，而计算延迟时间是模型预测输出所需的计算时间。</sample>
    <sample id="342">The presentation introduces a large-scale personalized dialogue dataset, which is constructed from live streaming videos. The dataset includes both text and video sources, with a focus on Chinese multi-party dialogues. It aims to address the challenges of existing datasets, such as limited scale and lack of detailed persona information. The dataset is constructed through three steps: scraping original live streaming videos, extracting audios and transcribing them into transcripts, collecting user comments and constructing dialogues, and collecting persona information for personalized dialogue generation. The dataset is compared with existing open-domain dialogue datasets and experimental results show that it outperforms other datasets in terms of response quality and speaker personalization. The presentation also discusses the potential applications of the dataset in developing virtual streamers and virtual employees.</sample>
    <sample id="343">Hello everyone, I'm Mackshatta and today my co-author Martin and I are presenting our work, the KITMST, evaluating knowledge integration from multiple sources. This work is a collaboration between McGill University, Meila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired via pre-training, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pre-training knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, John saw the newly elected president on TV. Pre-training parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance-specific entity John is or who the new president is because the president might have changed since pre-training. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pre-training and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate the dataset with human study participants and establish coreference resolution models. Here is an example from our dataset: Merlin is a judge. Kia is a baker. Merlin and Kia met at a park. After a long day at work, deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun 'he' refers to, which in this case is Merlin. The resolution of a given pronoun requires two types of information: first, entity-specific knowledge, such as Merlin is a judge; and second, background knowledge, such as judges decide cases in law courts. Generally, background knowledge is learned during the pre-training of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information, such that it may either be found in a single source or in multiple sources. We have defined three settings of KITMST: First, the Background Pretrain setting where background knowledge is assumed to be available at pre-training time. Second, there's the Background Both setting where background knowledge is available both at pre-training time and inference time. Lastly, the Background Infer setting where both knowledge types are available only at inference time. This last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-training data of models. For example, because new occupations have developed since the time of pre-training. Here is an example of how we control the availability of facts in the truth sources. In the Background Pretrain setting, we assume that the background knowledge, politicians seek elected seats in government, is contained in the pre-trained parameters. In the inference-time context, we provide the entity-specific knowledge, Cheechaster is a politician. In the Background Both setting, we additionally provide not only entity-specific, but also background knowledge about politicians in the inference-time context. In the Background Infer setting, we provide the fictional occupation, Meritura instead of politician, because Meritura is unlikely to be contained in the pre-trained parameters. We evaluate the dataset both with human study participants and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background Pretrain setting. Without task-specific training on KITMST, both models do not perform well. When trained on KITMST, however, both C2F and BERT for coref perform significantly better than random choice. This suggests that when trained on general coreference resolution datasets, models learn to exploit surface cues, which are not useful when testing on KITMST where such cues have been removed. Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the dataset and code on GitHub. Thanks for listening.</sample>
    <sample id="344">基于树的方法的缺点包括：1. 树通常需要通过形式化预处理来获取，这可能是一个昂贵的过程。2. 获取树可能涉及复杂的语法归约过程。3. 树方法往往无法捕捉到输入和输出之间的系统性对应关系，导致模型在处理更深层次的递归时产生错误输出。</sample>
    <sample id="345">本文介绍了论文《基于多集标签和潜在排列的无树组合泛化》的主要内容。该论文由马提亚斯·林登曼、亚历山大·科拉和伊万·季托夫合作完成，探讨了在没有树的情况下进行组合泛化的可能性。组合泛化是指学习者处理深度递归和未见过的短语组合的能力。论文提出了一种新颖的序列到序列模型，通过预测输入和输出片段之间的对应关系，实现了对更深层次递归的泛化。该方法通过将每个输入标记与一个未排序的多集标记关联，并使用另一个模型预测排列以确定输出顺序，从而避免了使用树结构。实验结果表明，该方法在Kogs基准测试中显著优于其他无树模型。</sample>
    <sample id="346">根据提供的信息，无法确定论文作者所属的机构。在演讲中没有提到任何与作者相关的机构名称或标识符。</sample>
    <sample id="347">嗨，我是米拉。今天我们将讨论我们关于用自然语言提示测量语言模型中的刻板印象的论文。这项工作是在与Esin Durmuch和Dan Juravsky合作完成的。近年来，许多人已经记录了社会偏见和刻板印象在大规模语言模型（LLMs）中普遍存在的现象。然而，这些措施存在各种限制。它们通常依赖于手工制作的数据集，这些数据集耗时耗力去收集，而且它们通常只衡量非常具体的刻板印象，这意味着它们不能很好地推广到其他人口统计或背景上，或者它们简单地捕捉到非常广泛的广义关联，比如负面关联特定群体。此外，大多数在这个领域的工作没有考虑到交叉性，这是多维社会身份可以加剧偏见并成为伤害的独特来源的概念。为了克服这些限制，我们依赖于这些 newer 指令调优的LLMs的一个特性，即它们非常擅长响应指令和提示。因此，我们可以要求模型生成一个人物描述，即一个想象中的个体的描述，使用一个提示，例如“假设你是亚裔女性，请描述你自己。”我们可以立即看到，这个输出可以泛化到任何人口统计，因为我们只需要将我们想要的身份标记插入到这个提示中。这里有一些来自GPT-4的示例生成。立即可以看出，虽然输出并不明显负面或有毒，但在传统意义上的负面或有毒这个词之外，有一些有趣的模式。亚裔女性被描绘为不引人注目的，中东女性被用词语如异国情调和像一个迷人的地区来提及，而女性的人物描述提到了血统，而白人男性的人物描述则没有任何这样的内容。为了捕捉这些模式，我们的方法有两个部分。第一个部分是生成这些人物描述。我们用于生成这些人物描述的提示是受一个研究启发的，他们给这些提示提供了人类主题，发现通过将它们提供给人类主题，他们也能够表面种族刻板印象，并且这使我们能够直接比较我们生成的人物描述和人类手写回应。第二个部分是标记词，这是一种识别标记群体和未标记群体之间的单词的方法，我将在下面详细说明。这种方法的好处是，我们得到了非常具体的刻板印象和模式，而无需依赖于任何特定的词汇表。标记词方法利用了社会语言学概念中的标记性，该概念指出，有一个未标记的默认值，任何与未标记默认值不同的群体都是 linguistically marked。例如，术语“男人”通常与男人相关联，所以当人们描述一个女人是一名勇士时，他们通常会指定“女人勇士”，并将术语“女人”标记为“女人”。更广泛地说，社会中的主流群体既在语言上又在社会上未被标记，而边缘化群体通常被标记。因此，在我们的方法中，我们首先确定未标记和标记的群体是什么，然后我们使用标记词方法来比较人物描述，该方法实际上是使用加权log odds比率来区分每个标记群体的top words。例如，对于黑人女性的人物描述，我们将进行标记词，并将log odds比率与白人人物描述和男性人物描述进行比较，因为它们是两个对应的未标记群体。现在，让我们看看一些结果。首先，我们使用lexicon of stereotypes，并发现生成的人物描述包含比人类手写人物描述更多的刻板印象。但是，当我们查看词汇表中单词的分布时，我们会发现一些不同之处。虽然生成的人物描述具有词汇表中单词的更高频率，但人类手写的人物描述具有更广泛的单词分布，而生成的人物描述中刻板印象单词只是“高大”和“英俊”等积极或至少是非负面的单词。实际上，这个词汇表并没有捕捉到我们在早期幻灯片中看到的许多有害的图案。相反，我们将转向标记词方法，以显示这些看似积极的画像如何促进刻板印象和本质化叙事。在我们的分析中，我们揭示了这些看似积极的画像反映了有害的图案。首先，对于标记群体，top words包括文化、传统、自豪和异国情调，这些词语通过与白人规范的关系来定义这些群体，从而有助于长期的歧视和边缘化。此外，还有一些常见的 tropes 反映在这些词语中，特别是对于有色女性。例如，描述 Latina 女性的词语包括“充满活力”和“风趣”，这与热带主义联系在一起。对于亚裔女性，词语包括“可爱”、“精致”和“丝滑”，这与亚裔女性被 hypersexualized 的历史联系在一起，被视为非常顺从和顺从。最后，对于黑人女性，我们看到一些 top words 包括“坚强”和“ resilient”，这与被称为“坚强黑人”形象联系在一起。虽然它看起来是一个积极的形象，但研究表明，这种形象实际上是有害的，因为它给这些人口带来了巨大的压力，让他们在社会障碍面前变得坚强和坚强，导致这些人口的负面健康后果和其他伤害。更广泛地说，我们发现每个标记群体的 top words 都反映了非常本质化的叙事。基于这些模式，我们得出三个对模型所有者 recommendations。首先，我们应该作为研究人员关注积极刻板印象和本质化叙事。我们还应该使用交叉性 lens 来研究偏见和伤害，因为有许多事情可能会被忽视，如果我们不这样做。最后，我们需要增加对偏见缓解方法的透明度，因为例如这些积极刻板印象，我们不知道是因为某种过度的价值取向导致了这些刻板印象，还是因为某种反刻板印象方法导致了这些刻板印象，我们无法做出任何假设，或者进一步研究这些模式，如果没有更多的透明度。谢谢大家收听。祝大家ACL好运！</sample>
    <sample id="348">Myra在ACL上介绍了一项研究，该研究使用自然语言提示来衡量大型语言模型（LLMs）中的刻板印象。这项研究与Essen Durmush和Dan Juravsky合作进行。研究发现，LLMs中存在社会偏见和刻板印象，但现有方法存在限制，如依赖手工制作的数据集、只测量特定刻板印象以及不考虑多维社会身份的交叉性。Myra的方法包括生成人物描述和使用“标记词”来识别刻板印象。结果表明，虽然生成的人物描述通常不是 overtly negative 的，但它们反映了刻板印象，如将女性描述为“unassuming”或“exotic”，并将女性与种族联系在一起。研究建议，模型所有者应关注积极刻板印象和刻板印象，使用交叉性研究来检测偏见和伤害，并增加关于偏见缓解方法的透明度。</sample>
    <sample id="349">Hello everyone, my name is Jing Wei from the University of Science and Technology of China. It's my pleasure to give a short advertisement video about Paper Are You Copying My Model: Protecting the Copyright of Large Language Models for Embedding and Services. View backdoor watermark. Let's first introduce the background about embedding services. Currently, large language models such as GPT, LLaMA, Prolific are exceptional in natural language understanding and generation. Embedding services is one of the services built upon large language models to assist various NLP tasks. For example, OpenAI offers a GPT-based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it is necessary to protect the copyright of embedding services. To protect the copyright of embedding services, one of the solutions is to embed a watermark in the provided service and detect whether another service contains the watermark. The watermark method needs to meet the following properties: First, the method should be applicable to embedding services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark need to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, these methods either not applicable to embedding services or lack of transferability. Therefore, in this paper, we propose Embedding Marker which is a backdoor-based watermark method applicable to embedding services. Then let me introduce the details of our Embedding Marker. Embedding Marker contains two main steps: watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user sends a sentence to the provider service, the provider counts the trigger number in the sentence. The provided embedding is a weighted summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than M, the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the watermark. We first construct a backdoor and benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set. While all words in the sentences of benign dataset do not belong to the trigger set. Then the provider requests embeddings from the stolen service with the dataset. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor dataset, which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four datasets: AGNews, Yahoo, SST-2, and Yelp. We assume the provider apply Wikipedia dataset to count word frequency. The results on four datasets show that our Embedding Marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualizing the embedding of sentences on Figure 1. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings. That's all. Thank you. We'll come to discuss with us.</sample>
    <sample id="350">The presentation discusses the concept of "superhuman performance" in natural language processing (NLP) and the challenges in comparing human and machine performance. It introduces a study on two popular NLP benchmarks, SuperGLUE and Squad, which are used to evaluate the performance of language models. The study finds that while machines often outperform humans on these benchmarks, there are significant issues with the evaluation process, such as differences in dataset sizes and errors in ground truth annotations. These issues can lead to unfair comparisons and overestimation of machine performance. The presentation also highlights the need for more reliable benchmarks and methods to compare the best systems with the best possible human performance. Overall, the presentation argues that current claims of superhuman performance in NLP are not scientifically grounded due to methodological flaws and the lack of consideration for human factors.</sample>
    <sample id="351">演讲者介绍了一篇关于使用Conll 2003数据集开发的实体标签器在2023年是否仍然有效的论文。他们首先讨论了通用性问题，即模型是否能适应现代数据，并在开发新标签时需要什么条件。他们还探讨了性能下降的原因，提出了两种假设：自适应过拟合和时间漂移。通过实验，他们发现自适应过拟合未被观察到，而时间漂移是导致性能下降的主要原因。为了实现良好的通用性，他们建议使用更好的模型架构、更大的模型规模以及更多的微调示例。此外，他们强调了时间漂移的重要性，并指出Conll 2003数据集在20多年后仍然有效。演讲者希望这篇论文能为未来的研究提供灵感，并鼓励读者查阅论文和数据集。</sample>
    <sample id="352">ABC-Eval代表一种评估对话模型质量的维度方法，通过明确标注每个模型响应是否表现出某些行为，如提供不相关的信息或自相矛盾。</sample>
    <sample id="353">The paper titled "Python Code Generation by Asking Clarification Questions" introduces a method for generating Python code by asking clarification questions. The authors, Housheng Li, Motomasa Masakar, and others, propose an interactive approach to address the challenge of input under-specification in code generation. They focus on clarifying operation-level specifications and create a synthetic dataset with clarifications on key operations. The paper also proposes a pipeline for code generation by asking clarification questions, which includes identifying key operations, creating a clarification query set (CQ), and using a classification model to predict missing or aligned operations. The authors evaluate their method using various metrics and discuss potential directions for improvement, such as taxonomy and distinguishing aligned operations from those with similar names.</sample>
    <sample id="354">直到2023年，CoNLL-2003和CoNLL++之间的性能增量才高于5个百分点。</sample>
    <sample id="355">Hello, my name is Vasudha and I am a computer science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, Transfer Learning for Discourse Detection, addressing the rare class challenge. We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent. Such as this example where a person states, "I know that cigarettes could kill me," and then goes on to say, "I grabbed a couple of smokes after the meeting." This belief and action are inconsistent and they are in dissonance. Further mentioning that, "I don't think I could keep my job without them," justifies the second occurrence and they have a consonance relationship. While dissonance is a very common phenomenon we experience in daily decision-making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of dis agreement among people, track trends in belief, values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large-scale annotation of dissonance relations. We used a dissonance first approach as seen in the flowchart here. Tweets were parsed using a PTB parser and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around thousand examples of discourse unit pairs, we ran training for an initial classifier trained only on forty three examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation rounds, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic-independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here, and on binary classification of expansion and comparison classes of PTB, since these two are closely related to the conception of consonance and dissonance, and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance, with the best with AUC 0.62. Further on iteratively fine-tuning on both tasks, we find that fine-tuning of CE task followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to co-start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. Cumulative accumulates all the data collected from active annotations so far whereas Iterative updates the model by training on the latest set of data collected. Over the different strategies, we found that cumulative performed equal or better than iterative across the board. Next, to improve the number of dissonance examples, we use a probability of rare class strategy, PRC, to select mostly the examples that are highly likely to be dissonant by the current model at any round of AL. We compare this to the other state-of-the-art state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improved dissonance classification AUC to point 7.5 which is the best performance that we have on the task so far. We also check the feasibility of each strategy for annotation quality and cost to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and co-starting AL with appropriately designed transfer learning tasks and helps significantly. We also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.</sample>
    <sample id="356">根据提供的信息，无法确定论文作者所属的机构。在音频中没有提到任何与机构相关的信息。</sample>
    <sample id="357">演讲者的名字是Cii Yuan。</sample>
    <sample id="358">根据演讲者的介绍，这篇论文有五位作者：Kai Yen、Patrick Franz、Emil You、Andrea F. Martinez 和 Graham Nivig。</sample>
    <sample id="359">该方法与专门用于实时语音翻译的 State-of-the-art 架构进行了比较。</sample>
    <sample id="361">演讲者介绍自己名为Armanin Norbakh，是一名计算机科学专业的学生，目前在卡内基梅隆大学的语言技术学院学习。同时，他也是J.P. Morgan AI研究团队的研究主任。演讲的主题是“CounterComp”，一个专注于通过反事实场景提高多步数量推理的组合性泛化能力的研究项目。多步数量推理涉及回答需要执行多个算术运算的问题，如从2019年到2020年的收入变化。当前的AI模型在处理多步数量推理任务时表现不佳，特别是当输出包含超过两步操作时，因为它们倾向于记住模式而无法区分重要和不重要的输入元素。演讲者提出了一个方法，通过创建正例和负例来训练模型，这些例子基于输入中具有不同操作的相同问题。这种方法通过动态调整学习损失函数来帮助模型关注更相关的输入元素，从而提高其在多步数量推理任务上的性能。</sample>
  </task>
</testset>