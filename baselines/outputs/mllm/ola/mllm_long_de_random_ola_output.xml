<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind Webkrawldaten, politische News-Medien und soziale Medien.</sample>
    <sample id="1">Die Autoren sind an McGill University, MILA und Microsoft Research.</sample>
    <sample id="2">The paper presents a new multi-modal pre-training model called LayoutMask to address the reading order issues in visualized document understanding. It uses text and layout information as input and aims to enhance text layout interactions and layout representation learning during pre-training. The model differs from previous studies in three aspects: choice of reading order, masking strategy, and pre-training objectives. It proposes to use local one-dimensional token orders as reading order, which is referred to as local one-dimensional position. As local one-dimensional position does not provide cross-segment orders, LayoutMask infers global reading order by jointly using one-dimensional position, two-dimensional position, and semantic information. The model equips the commonly used pre-training objective Masked Language Modeling with two novel masking strategies: whole word masking and layout aware masking. Whole word masking sets masks at word level instead of token level, which is more challenging and requires the model to find more context to predict masked words. Layout aware masking strategy has a higher probability to be masked for the first and last words of each segment, so the model has to pay more attention to find their contexts in the preceding or succeeding segments, thus promoting learning across segment orders. The authors also design a new pre-training objective called Masked Position Modeling, which has the same pre-training objective and recovers relative two-dimensional positions during pre-training. The paper compares the performance of LayoutMask with different layout informations for one-dimensional position, local one-dimensional position, and global one-dimensional position on both FD and SRE datasets, and shows that global one-dimensional position can perform better than local one-dimensional position and achieves similar F1 scores as entity total.</sample>
    <sample id="3">Hallo, und herzlich willkommen zu unserem Vortrag über die Einführung von DPlain, ein neues Korpus für die deutsche Textsimplifizierung auf der Dokumentebene und auf der Satzebene. Mein Name ist Regina Störring und ich werde Ihnen das Erste Teil der Präsentation vorstellen. Lassen Sie uns mit dem Text beginnen. Textsimplifizierung ist ein Prozess, bei dem ein Text anpassen wird, um die Comprehension (Verstehensfähigkeit) des Textes für einen bestimmten Zielgruppe zu verbessern, sei es Menschen mit Leseschwierigkeiten oder nicht-native Sprachlerner. Um ein Textsimplifizierungsalgorithm zu trainieren, benötigen wir Paare von Texten, zum Beispiel Dokumente oder Sätze. In diesem Beispiel sehen Sie ein Paar von parallel alignierten Satzpaaren, einer komplexen deutschen Satz und seine Übersetzung ins einfachere Deutsche. Um einen Satz zu simplifizieren, gibt es verschiedene Techniken, wie Sie im Beispiel sehen können, wie z.B. Lexikalsubstitution, Klauselletion, Klauselletion mit Umordnung, oder die Einfügung von Begriffen. Wir now präsentieren unser neues Korpus DPlain. Aufgrund jüngster Probleme mit existierenden Korpusen haben wir für unser Corpus eine neue Methode implementiert. Zum Beispiel sind die vorherigen Korpusen zu klein, um ein Textsimplifizierungsalgorithm zu trainieren. Die anderen drei Modelle, die in jüngster Zeit vorgeschlagen wurden, sind alle automatisch aligniert, was bedeutet, dass sie über einen erprobten Alignmentsalgorithmus verfügen. Daher präsentieren wir unser neues Korpus DPlain, das in zwei Subkorpusen, DPlain API und DPlain Web, unterteilt ist. DPlain API basiert auf News-Texts. In DPlain API wurden 483 Dokumente manuell aligniert, was zu einem Resultat von etwa 30.000 13.000 Paaren von parallel alignierten Sätzen führt. DPlain Web enthält verschiedene Domänen und alle 750 Dokumente wurden auf der einen Seite manuell und auf der anderen Seite mit automatischen Alignmentsmethoden aligniert. Insgesamt ergibt sich ein Paar von 30.450 Sätzen. Wir analysieren unsere Satzpaare etwas detaillierter, zum Beispiel auf Typen von Simplifizierungen. Wie Sie sehen können, sind Bibeltekst-Texte stark simplifizierter als z.B. News-Texte oder Lerner-Texte auf allen Ebenen, wie z.B. Lexikal Simplifizierung, Strukturelle Simplifizierung, oder die allgemeine Ebene der Simplifizierung. Darüber hinaus zeigt unser DPlain-Korpus eine hohe Vielfalt an verschiedenen Simplifizierungstransformationen. Zum Beispiel in DPlain API enthält es mehr Reordonings und Worteditionen als DPlain Web. Auf der anderen Seite enthält DPlain Web mehr Redefrasings. Nun sehen wir an, was wir damit erreichen können. Hallo, ich bin Omar und nun werde ich über die Anwendungsmöglichkeiten unseres Datensatzes DPlain sprechen. Der erste Anwendungsbereich, den wir in unserem Papier diskutieren, ist die Evaluation von automatischen Alignmentsmethoden. In jüngster Zeit gab es einen großen Fortschritt in der Entwicklung von Alignmentsmethoden, aber in Kontexten, in denen wir zwei parallel schriftliche Dokumente in verschiedenen Sprachen haben und wir die Aligment von Sätzen in post-doc-Dokumenten extrahieren wollen. In unserem Fall versuchen wir, die Aligments zwischen Sätzen von zwei parallel schriftlichen Dokumenten zu extrahieren, die dieselbe Sprache und dieselbe Content haben, aber auf verschiedenen Komplexitätsebene liegen. Nun, da wir unser DPlain-Korpus mit manuell alignierten Sätzen haben, können wir diese Sätzen als Gold Standard-Alignments verwenden, um einige der vorgeschlagenen Alignmentsmethoden zu evaluieren. Wir haben auch Anpassungen an die vorgeschlagene Methode vorgenommen und die Codes zur Durchführung unseres Experiments im Papier publiziert. Am Ende conclude that die besten automatischen Alignmentsmethoden für deutsche Textsimplifizierung sind die Methode MassAlign und Sie können auch den Code zur Ausführung dieser Methode auf Ihren eigenen Dokumenten im Papier finden. Der zweite Anwendungsbereich, den wir im Papier gezeigt haben, ist die automatische Textsimplifizierung durch Fine-Tuning von Sprachmodellen, um eine simplifizierte Texte aus einem komplexen Eingabe-Text zu produzieren. Wir haben zwei verschiedene Modelle fine-tuned: Wir haben das Modell Long Impart fine-tuned, um Dokumentebene Simplifications zu produzieren, und wir haben auch das Modell Long Impart fine-tuned, um Satzebene Simplifications zu produzieren. Sie können auch die Checkpoints und weitere Details über die scores und die Evaluationsmetrischen von unserem Experiment im Papier finden. Wir conclude that die Fine-Tuning-Methoden können besser scores als die Baseline-Scores produzieren und wir schlagen diese results als Benchmark für zukünftige Studien zur Automatischen Textsimplifizierung vor. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle bei der Konferenz kennenzulernen. Vielen Dank.</sample>
    <sample id="4">The referent is named Vasudeha.</sample>
    <sample id="5">Die Genauigkeit von 82–87 % wurde erreicht, wenn der Sprachmodell Zugriff auf teilweise überlappendes Hintergrundwissen hatte.</sample>
    <sample id="6">The presentation introduces a new approach to multilingual and cross-lingual summarization, called many-to-many summarization. This method aims to build a single summarization model that can process a document in any source language and generate a summary in any target language. The authors conduct preliminary studies to analyze the differences between multilingual summarization, cross-lingual summarization, and their proposed method. They find that many-to-many summarization helps the summarization model better transfer task knowledge across different languages than previous methods. The presentation also proposes a three-stage pre-training process for training the many-to-many summarization model, which outperforms previous models including MBS and MT5.</sample>
    <sample id="7">Ja, sie funktionsieren immer noch.</sample>
    <sample id="8">Die vorgeschlagene menschliche Bewertungsmethode ist neu, weil sie die Subjektivität menschlicher Bewertungen reduziert, indem sie explizit feststellt, ob jede Modell-Antwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel die Irrrelevanz von Informationen oder sich selbst widersprechende Aussagen.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Anzahl der sauber validierten Ausprägungen ab.</sample>
    <sample id="10">Das Modell hat nur 60% Genauigkeit, wenn es nur Zugriff auf die Namen der Entitäten hat. Es gibt also noch viel Raum für Verbesserungen.</sample>
    <sample id="11">The New Yorker Caption Contest is a popular weekly cartoon captioning competition that has been running for over 70 years. The contest involves submitting captions for cartoons published in The New Yorker, with the top three entries selected by human editors and a final winner determined by public vote. Researchers have operationalized this data into three tasks: matching, quality ranking, and explanation generation. They have also gathered annotations for each of over 700 cartoons, including locations, descriptions, and joke explanations. The best model, CLIP fine-tuned on the annotated corpus, achieves around 62% accuracy on the matching task, which is significantly higher than the 20% random guessing baseline. However, human raters score around 94%, indicating a significant gap in humor understanding. The researchers also tested language models like GPT-4, but even with additional annotations, there was still a performance gap between GPT-4's five-shot performance and human raters on the matching and quality ranking tasks.</sample>
    <sample id="12">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="13">Daniel Rotem präsentiert "Finding the Sweet Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings", das in der Lab-of-Prof. Roy Schwartz an der Hebrew University in Jerusalem entstanden ist. Adaptive Inferenz reduziert die Inferenzzeit von großen Sprachmodellen, indem sie auf die Varianz der realen Daten basiert. Daraufhin werden für einfache Samples niedrigkomplexere Modelle verwendet, um die durchschnittliche Inferenzkosten zu reduzieren. Die beiden häufig verwendeten Methoden sind Multi-Modell und Early-Exit. Multi-Modell verwendet mehrere Modelle, die separat trainiert werden und anhand von Klassifizatoren anhand des gesamten Datensatzes trainiert werden. Sie werden für Inferenz sequentiell verwendet, bis ein Klassifizator die Berechnung stoppt. Early-Exit hingegen verwendet mehrere Klassifizatoren, die nach intermediären Transformer-Layer trainiert werden und für Inferenz einen Sample durch den Modell laufen lassen, bis ein Klassifizator die Berechnung stoppt. Das Vorteil von Early-Exit ist schnelleres Inferenz und keine Overhead, aber es kann zu schlechter Leistung führen, da die Modellparameter von allen Klassifizatoren geteilt werden. Das Vorgehen von Daniel Rotem zielt darauf ab, die Konfliktgradienten im Early-Exit-Training zu überwinden und eine novelles Finetuning-Methode namens "Sweet" zu präsentieren, bei der jeder Layer nur von seinem nächsten Klassifizator trainiert wird.</sample>
    <sample id="14">Hallo, alle. Mein Name ist Justin Jung vom Peking University. Heute werde ich über unser Werk "Exemplar: Crosslinguistic Semantics Parsing in Multiple Natural Languages and Multiple Representations" sprechen. Semantics parsing ist die Aufgabe, semantische Repräsentationen von Benutzereingaben zu erstellen, wie SQL und Lambda Calculus. Crosslinguistic semantics parsing ist die Aufgabe, Eingaben in mehreren natürlichen Sprachen in mehrere Repräsentationen zu übersetzen. Wie im Bild gezeigt, müssen wir die Anfrage in mehrere natürliche Sprachen mit neuronalen Modellen in SQL, Lambda oder FunkQL usw. übersetzen. Existierende crosslinguistic semantics parsing-Modelle werden separat vorgeschaffen und auf Datensätzen von limitierten Aufgaben evaluiert. Zum Beispiel gibt es Lücken in der Abdeckung bestimmter natürlicher Sprachen – das Chinesische wird fehlgeschrieben – und Lücken in der Abdeckung bestimmter Repräsentationen – das Lambda Calculus wird fehlgeschrieben – oder sie werden nur auf bestimmte neuronalen Modelle evaluiert – zum Beispiel gibt es nur einen einzelnen Modell zur Evaluation. Um dies zu beheben, schaffen wir exemplar.</sample>
    <sample id="15">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="16">Die Bibel文本被更加强烈地简化了。</sample>
    <sample id="17">This paper proposes a novel method for multi-modal relation extraction by integrating visual and textual information. The method consists of five parts: (1) representing text and image with corresponding visual and textual graphs, (2) merging the two graphs into a unified cross-modal graph, (3) screening the initial graph structures by fine-grained filtering nodes and adjusting edges, (4) enriching the compact cross-modal features with multi-modal topic features, and (5) using attention operations to integrate the multi-modal topic words. Experiments on the widely used MRD dataset show that the proposed method outperforms the text-based method and achieves the best performance among multi-modal baselines. The results also indicate that internal information screening is more important for high cross-modal relevance inputs, while external information exploiting is more useful for low cross-modal relevance inputs.</sample>
    <sample id="18">Ein Beispiel für die Präferenz für kürzere linke Konjunktionen lautet: "Salt and pepper" (Salz und Pfeffer) ist kürzer als "Not pepper and salt" (Nicht Salz und Pfeffer), da die linke Konjunktion "salt" (Salz) kürzer ist als die rechte Konjunktion "pepper" (Pfeffer).</sample>
    <sample id="19">The presentation discusses the challenges of open-domain question answering and proposes a two-stage framework to address these challenges. The first stage retrieves relevant documents from Wikipedia, while the second stage uses a reader to understand the question and retrieve the answer. The presentation also summarizes some efficient techniques to achieve this goal, such as approximate nearest neighbor search, skipping rate, document filtering, and model size reduction. The presentation concludes with two future works: deploying open-domain question answering systems in low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Ja, Sie können die Modelle verwenden.</sample>
    <sample id="21">Die DEplain-apa enthält ausschliesslich News-Artikel.</sample>
    <sample id="22">Eine gute Generalisierung wird durch die Modellarchitektur, die Modellgröße und mehr Fine-Tuning-Beispiele erreicht.</sample>
    <sample id="23">Researchers from Google have developed a method to improve the ability of text-image models to render visual text. They focused on the Imagen model, which uses a T5-XL encoder and a diffusion model to generate images from input text. The team discovered that even with complex inputs, these models often fail to represent text accurately. They investigated the text encoders and found that T5 struggles with spelling words correctly, especially at smaller scales. In contrast, larger models like PaLM perform better but are impractical due to their size and training data requirements. The team also explored BitT5, which receives individual bits of input strings and performs well in spelling. By augmenting the Imagen model with an additional text representation from BitT5, they improved its ability to spell words correctly and generate more accurate images.</sample>
    <sample id="24">Die Tendenz wurde durch Messung der Längen in Silben, Zeichen und Wörtern bestimmt.</sample>
    <sample id="25">Die Experimente wurden so gestaltet, dass die Längen der Konjunkte in verschiedenen Einheiten gemessen wurden (syllabische, Buchstaben und Wörter).</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten trainiert wird, performt nicht viel besser als Zufall.</sample>
    <sample id="27">Ein Autor arbeitet an der Arbeit beteiligt.</sample>
    <sample id="28">Bob und Alice</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden besser ab als kontextagnostic Modelle bei Diskursphänomenen wie Formalität und lexikalischer Kohärenz.</sample>
    <sample id="30">The paper introduces a framework called LLM-Blender for ensemble learning of large language models. It uses pairwise ranking and generative fusion to select the best model for each input example. The framework consists of two stages: first, it ranks the models using a pairwise ranking module called PairRanker, and then it fuses the top k models using a generative fusion model. The results show that LLM-Blender outperforms individual models on various metrics, making it a promising framework for ensemble learning of large language models.</sample>
    <sample id="31">Die Autoren sind an Johns Hopkins University, Purdue University und MIT.</sample>
    <sample id="33">Das Framework quantifiziert die Positionalität, indem es dieAnnotations von diversen Annotatoren mit den Datensätzen und Modellvorhersagen vergleicht. Es verwendet dabei eine Pearson's r Korrelationskennzahl, um die Übereinstimmungen zwischen den realen Nutzerannotationsen und den Datensätzen/modellen zu messen.</sample>
    <sample id="34">The speaker presents a work called CREST, which is a joint framework for rationalization and counterfactual text generation. The framework combines selective rationalization with counterfactual generation to produce high-quality counterfactual examples. The speaker compares the quality of counterfactuals produced by CREST with related works using both automatic metrics and human evaluation. The results show that humans rate CREST-generated counterfactuals as more valid and natural than those generated by other methods. The speaker also proposes an alternative way to perform rationalization with both factual and counterfactual examples, which improves downstream models. The speaker analyzes the interpretability of the explanations generated by CREST in three dimensions: plausibility, forward similarity, and counterfactual similarity. The results show that CREST produces more plausible explanations than other approaches and achieves higher counterfactual similarity than other methods.</sample>
    <sample id="36">The paper presents a method for improving multilingual machine translation by introducing language-specific layers (LSLs) in the Transformer architecture. The core idea is to have one regular Transformer layer per language, which can be either the source or target language. At inference time, only the relevant sub-layer is activated, keeping the inference cost constant. The placement of LSLs is learned through a process involving shared, source, and target weights, with the best placement determined by selecting the component with the largest weight. The approach shows significant improvements over baseline models and language adapters, particularly for low-resource languages, while maintaining faster inference times.</sample>
    <sample id="37">Die vorherige Studie fand, dass die menschlichen Teilnehmenden bei der Erhaltung von Persona-Prompts auch Racialstereotypen surfen konnten.</sample>
    <sample id="38">Die Studie nutzte die statistischen Daten aus der erweiterten Version von Penn Treebank.</sample>
    <sample id="39">Es sind zwei Autoren an der Arbeit beteiligt.</sample>
    <sample id="40">Die eng verwandten Aufgaben für kognitive Dissonanz sind die Identifikation von Meinungsverschieden in Debatten und die Klassifizierung von Erweiterungen und Comparativen in Sprache.</sample>
    <sample id="41">This presentation introduces the PEACoK project, which aims to develop a personal common sense knowledge graph for consistent and engaging narratives. The project is a collaboration between the Natural Language Processing Lab at EPFL University and Sony Group Corporation. The goal is to create a system that can understand how different personas, such as speakers, listeners, or characters, interact within a narrative. The presentation explains the challenges of representing real-world personas with rich world knowledge and complex interconnections, and introduces the PEACoK graph, which contains about 3,800 personas, 40,000 distinctive attributes, and 100,000 person inferences or facts. The graph is built in three steps: selecting personas from existing common sense knowledge graphs, inducing attributes from both common sense knowledge graphs and large-scale pre-trained language models, and cross-checking annotations using a joint human-AI majority voting scheme. The presentation also discusses the use of PEACoK to train a bar-based common knowledge generator on a persona attribute inference task and to improve downstream narrative modeling through a person-grounded dialogue generation task on the CoVR AI2 Personal Chat dataset.</sample>
    <sample id="42">Zwei Autoren arbeiten an der Arbeit beteiligt.</sample>
    <sample id="43">Die Arbeit beteiligen sechs Autoren.</sample>
    <sample id="44">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten, indem es nicht nur die Übereinstimmung zwischen Annotatoren und Datensätzen oder Modellen analysiert, sondern auch die Positionalitäten der Nutzer mit den Datensätzen und Modellen vergleicht.</sample>
    <sample id="45">Die meisten Überschneidungen mit dem Lexikon der Stereotypen sind in den generierten Persönlichkeiten zu finden.</sample>
    <sample id="46">Google Translate und DeepL</sample>
    <sample id="47">Ich bin Changbing, ein PhD-Student an der University of Washington. Heute präsentiere ich unser Werk von der Vorbereitung von Datensätzen bis hin zu Sprachmodellen und den downstream-Task-Anwendungen, um die Verlusträume von politischen Biases zu verfolgen, die zu unfairen NLP-Modellen führen können. Sprachmodelle werden auf großen Webkrawlsdaten trainiert, die politische News-Medien gut abdecken. Laut einem Surveymitglied der C4F-Kommission können wir sehen, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. in Sprachmodelltrainingdaten gut abgedeckt sind. Dies hat sowohl einen positiven als auch einen negativen Effekt auf die Anwendung von Sprachmodellen. Auf der einen Seite können sie aus diversen Perspektiven lernen, was demokratische und pluralistische Ideen fördert. Auf der anderen Seite sind diese verschiedenen politischen Meinungen inherent sozial biasiert und können potenzielle Fairness-Probleme in downstream-Task-Anwendungen erzeugen. Um dies zu untersuchen, haben wir vorgeschlagen, die politische Bias-Propagations- Pipeline von Vorbereitungsdaten über Sprachmodelle bis hin zu downstream-Task-Anwendungen zu untersuchen. Dazu haben wir die folgenden Fragen gestellt: Wie können wir die politische Linie von Sprachmodellen bewerten und welche Rolle spielen dabei die Vorbereitungsdaten? Wie performieren Sprachmodelle mit verschiedenen politischen Linien auf downstream-Task-Anwendungen und ob das zu Fairness-Problemen in NLP-Anwendungen führt? Wir haben vorgeschlagen, Sprachmodelle mit verschiedenen Promt-Formulierungen zu prompten, indem wir politische Fragstelle wie den "Political Compass Test" verwenden. Das gewährleistet eine automatische Evaluation, die fundiert in der politischen Wissenschaftliteratur ist. Einige vorläufige Resultate demonstrieren, dass Sprachmodelle variierende politische Linien aufweisen, sie auf allen vier Quadranten des politischen Kompasses liegen. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist und GPT-Serien allgemein mehr sozial liberal sind als BERT-Serien und seine Variationen. Wir haben auch versucht, die Auswirkungen von politischen Biases auf Sprachmodelle zu untersuchen, indem wir Sprachmodelle auf verschiedene partizipare Datensätze vortrainieren, die in News und Sozialmedien unterteilt sind und je nach politischer Linie weiter differenziert werden. Durch die vorherige Vorbereitung von Sprachmodellen auf solche partizipare Datensätze können wir sehen, dass die ideologischen Koordinaten von Sprachmodellen entsprechend verschieben werden. Zum Beispiel, wenn wir Roberta weiter finetune und auf der linken Linie der Red Corpus trainieren, können wir eine substantielle linksliberalen Verschiebung in Bezug auf ihre politischen Biases feststellen. Wir haben auch versucht, zu untersuchen, ob Sprachmodelle die Polarisation im modernen Gesellschaftspolitik übernehmen können. Wir haben Datensätze in zweiTemporalquadranten, vor und nach dem 45. Präsidenten der Vereinigten Staaten, geteilt und Sprachmodelle auf die beiden verschiedenen Datensätze vortrainiert. Wir können sehen, dass Sprachmodelle allgemein eine politische Linie haben, die weiter weg vom Mittelpunkt nach 2017 liegt, was zeigt, dass Sprachmodelle auch die Polarisation in unserer Gesellschaft übernehmen können. Schließlich evaluieren wir Sprachmodelle mit verschiedenen politischen Linien auf Hate-Speech-Detection und Fake-News-Detection-Anwendungen, die oft in NLP-Anwendungen verwendet werden und sehr wichtige Implikationen haben. Wenn wir die Kategorienerfolge analysieren, also die Trennung der Leistungen nach verschiedenen Demographen oder politischen Linien von Medien, können wir ein Muster feststellen. Zum Beispiel, bei der Hate-Speech-Detection sind linksliniente Sprachmodelle besser darin, Hate-Speech, die sich auf soziale Minderheiten richten, zu detektieren, aber schlechter darin, Hate-Speech, die sich auf mehr mächtige Gruppen in unserer Gesellschaft richten, zu detektieren. Und umgekehrt: rechtsliniente Sprachmodelle sind besser darin, Hate-Speech, die sich auf Weiße und Männer richten, zu detektieren, aber schlechter darin, Hate-Speech, die sich auf Afroamerikaner, LGBTQ+ und andere Minderheiten richten, zu detektieren. Similar Trends finden wir auch bei Fake-News-Detection, wo wir sehen, dass linksliniente Sprachmodelle besser darin sind, Missinformationen von rechtslinienten Medien zu detektieren und umgekehrt. Wir zeigen auch viele qualitative Beispiele an, um zu zeigen, dass Sprachmodelle mit verschiedenen politischen Linien unterschiedliche Vorhersagen für Hate-Speech- und Missinformationsbeispiele basierend auf ihren sozialen Kaligraphen geben. Es gibt noch viele weitere Beispiele im Anhang, um das zu verdeutlichen. Dies zeigt, dass es ein Fairnessproblem gibt, das sehr dringend beachtet werden muss, da die politischen Biases von Sprachmodellen zu Fairness-Problemen in NLP-Anwendungen führen können. Zum Beispiel, wenn rechtsliniente Sprachmodelle für die Fine-Tuning von Hate-Speech oder Missinformation und so weiter eingesetzt werden und dann auf einer populären Sozialmedienplattform eingesetzt werden, bedarf das Marginalisieren von Menschen mit gegenüber politischen Meinungen und das Ranzippen von Hate-Speech, das sich auf Minderheiten richtet, ohne Kontrolle zu erhalten. Das sollte uns auffordern, die Fairnessprobleme, die durch Sprachmodelle entstehen, zu akzeptieren und zu bekämpfen. Ein paar Punkte, die wir hervorheben möchten, sind die eindeutige Dilemma, die Sprachmodelle politische Biases gegenüber aufweisen. Es ist, wie zwischen Sisyphus und Cerberus. Wenn wir die politischen Meinungen in Sprachmodelltrainingdaten nicht sauber machen, propagieren die Biases von den Vorbereitungsdaten über Sprachmodelle bis hin zu downstream-Task-Anwendungen und schaffen Fairness-Probleme. Wenn wir versuchen, die Biases zu sauben, riskieren wir Sensibilisierungen oder Exklusionen und es ist schwierig zu bestimmen, was wirklich neutral ist und should retained in Sprachmodelltrainingdaten. Es ist, wie das elektroelektro problem. Okay, das war's. Ich denke, das war alles, was ich heute für euch tun konnte. Danke für eure Zeit.</sample>
    <sample id="48">Fünf</sample>
    <sample id="49">MPP-Auswertungen wurden bis zu 1244 Token Kontextlänger durchgeführt.</sample>
    <sample id="50">The presentation introduces Dr. Bert, a robust pre-trained model in French for biomedical and clinical domains. It compares Dr. Bert with other models trained on different data sources and sizes, evaluating their performance on various tasks. The results show that Dr. Bert outperforms other models on most tasks, especially when fine-tuned with more data. The presentation also discusses the impact of pre-training strategies and the availability of open-source models and training scripts.</sample>
    <sample id="51">Sie haben drei Domains aufgenommen: Musik, Brotze und Rezepte.</sample>
    <sample id="52">Positionalität ist die Perspektive, die Menschen als Folge ihrer Demografie, Identität und Lebenserfahrungen halten.</sample>
    <sample id="53">Dawei Zhu</sample>
    <sample id="54">This paper presents a method for detecting cognitive dissonance in language using transfer and active learning techniques. The authors define cognitive dissonance as inconsistency between beliefs or actions, and highlight its importance in understanding social dynamics, belief changes, and mental health. They conducted a large-scale annotation of discourse units to identify dissonance relations, but faced challenges due to the rarity of such examples. To address this, they used transfer learning from related tasks (debate statement classification and C-E task) and active learning strategies, including cumulative and iterative updates, to improve detection performance. The proposed probability of rare class strategy (PRC) was found to be effective in selecting likely dissonant examples, leading to significant improvements in classification accuracy.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell.</sample>
    <sample id="56">Ein Autor arbeitet an der Arbeit beteiligt.</sample>
    <sample id="57">Ja, es funktioniert.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind: 1. Background Pretrain, 2. Background Both, 3. Background Infer.</sample>
    <sample id="59">The presentation introduces Simultaneous Speech Translation (SST) and its challenges, such as long training procedures and the need for multiple models with different latency regimes. The proposed solution is to use pre-existing off-the-shelf models without retraining or specific architecture for SST. The model's knowledge is leveraged through attention mechanisms, and a strategy called Encoder-Decoder Attention (EDAT) is introduced to decide whether to emit partial translations based on attention weights. The results show that EDAT achieves comparable performance to scratch-trained models and outperforms a generic model in most tasks. However, specialized data does not scale well, and the pre-trained model from Natsos is freely available on the Yugabyte platform.</sample>
    <sample id="60">Die Autoren gehören an der Google Research.</sample>
    <sample id="61">Die abschließende Forschungsfrage lautet: Sollten wir nur die sauberen Samples für die Validierung verwenden, oder gibt es bessere Wege, sie zu nutzen?</sample>
    <sample id="62">The paper presents a systematic study of knowledge distillation for natural language generation (NLG) using pseudo-target training. The authors explore the potential of compressing large language models while preserving their performance. They compare different architectural decisions, pruning techniques, and knowledge distillation approaches, including word-level and sequence-level distillation. The study focuses on task-specific NLG tasks in realistic setups with unlabeled data and medium-sized teacher models. The authors propose a novel knowledge distillation technique called joint teaching to address student exposure bias and improve learning.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, für dieselbe Aufgabe dieselbe Ausgabe zu produzieren, unabhängig von kleinen Variationen in der Anweisung.</sample>
    <sample id="64">Jin Wei</sample>
    <sample id="65">Eine höhere Sensitivität ist ein negativer Begriff, der im Kontext der Modellleitstelle verwendet wird. Es bezieht sich auf die Fähigkeit des Modells, für dieselbe Aufgabe verschiedene Eingabeanweisungen zu verwenden und immer die gleichen Ausgaben zu produzieren. In diesem Fall ist eine höhere Sensitivität ein negatives Merkmal, da es bedeutet, dass das Modell nicht robust genug ist und seine Leistung beeinträchtigt.</sample>
    <sample id="66">This survey paper discusses the task of mathematical reasoning and the development of deep learning methods for solving math problems. It covers various aspects of mathematical reasoning, including text-based data, visual contexts, and table contexts. The paper also explores the use of neural network architectures for mathematical reasoning tasks, such as sequence-to-sequence models and six-to-tree models. Additionally, it highlights the performance of large language models (LLMs) in solving math word problems and proposes strategies to improve their performance, such as replacing greedy decoding with self-consistency and designing tool-augmented LLMs. The paper concludes by discussing the challenges and limitations of deep learning models in mathematical reasoning tasks.</sample>
    <sample id="67">The paper discusses interference in multilingual translation models and identifies the main factors contributing to it. It finds that severe interference occurs when the model is small compared to the data size, and tuning the sampling temperature is key for strong performance. The paper also shows that language similarity and the number of languages do not have a large impact on interference levels. The results suggest that modest scale and tuned temperature can reduce the problem significantly without any other specialized method.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings einen Kontext, der auf einem Datensatz basiert, der sowohl akzeptierbare als auch unakzeptierbare Sätze enthält.</sample>
    <sample id="69">Normalerweise werden normalerweise 20 Beispiele pro Klasse benötigt, um eine gute Leistung an der WSL zu erzielen.</sample>
    <sample id="70">Stanford Engineering</sample>
    <sample id="71">The AltEntities Corpus is a dataset created by researchers to address the challenge of resolving indirect referring expressions for entity selection in conversational systems. It includes 6,000 alternative questions across three domains: music, books, and recipes, with 42,000 indirect referring expressions. The dataset was collected using crowdsourcing, emphasizing informality through a cartoon completion setup. In this setup, participants are shown two speech bubbles with context and an alternative question, and they must select one of the entities using an indirect referring expression. The accuracy of language models in selecting the correct entity varies depending on the amount of background knowledge available to them, ranging from 92% to 60%. The dataset is available for further research and development in the field of natural language processing and entity recognition.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, um die Auswirkungen von politisierter Datenerhebung auf Sprachmodelle zu verstehen und zu messen.</sample>
    <sample id="73">Servin</sample>
    <sample id="74">The paper introduces Dense-Atomic, a method for constructing a dense knowledge graph by completing missing links in Atomic. It describes the process of converting tail events into head events using four steps: subject removal, third-person singular form conjugation, subject reverb, and relation grouping. The proposed method uses a relation prediction model to predict relations between head and tail events, utilizing semantic information from both events. The evaluation results show that Dense-Atomic achieves higher knowledge coverage and more diverse results compared to Atomic.</sample>
    <sample id="75">The presentation introduces a joint semi-supervised learning framework for entity and relation extraction tasks. The framework utilizes heterogeneous graphs to propagate labels across the graph, considering interconnections between labeled and unlabeled data. It consists of four main parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. The method shows significant improvement over baseline models on both entity and relation tasks in single-task datasets.</sample>
    <sample id="76">Die Pipeline beginnt mit der Auswertung der politischen Linie von Sprachmodellen und der Rolle der vorherigen Datensammlungen auf solche Vorurteile. Es wird dann untersucht, in welchem Maß Sprachmodelle mit verschiedenen politischen Linien auf den Vorurteilen der Datensammlungen basieren. Schließlich werden die Sprachmodelle mit verschiedenen politischen Linien anuntertreten und die Auswirkungen auf die Leistung bei Aufgaben wie der Erkennung von Hasssprache und Falschinformation untersucht.</sample>
    <sample id="77">This video is about improving summarization factual consistency using natural language feedback. It's a collaborative project between Yale University and Microsoft Research, with most of the work done when the first author was an intern at Microsoft Research. The video introduces a new dataset called "defacto," which includes human demonstrations and feedback for enhancing summarization factual consistency. The dataset provides comprehensive analysis and further insights into the factual consistency of summarization models. Three new NLP tasks are proposed: summary editing, feedback generation, and automatic factual error correction. The video focuses on abstractive text summarization and specifically studies the factual consistency of summarization models. Human demonstrations and feedback were collected based on existing summarization models, with annotators providing labels to decide if the summary is factually consistent and offering human-corrected summaries and explanations. The data was collected from the XSum dataset, and initial system outputs were generated by the pre-trained Pixie model. The video shows that human-edited summaries receive higher automatic factual scores but have lower textual overlap with reference summaries. The data distribution of annotated editing instructions is also shown, along with their relation to different error types. The video concludes by highlighting the advantages of the defacto dataset, including its fine-grained annotations, which can be valuable for training factuality metrics and evaluating factual accuracy.</sample>
    <sample id="78">Ja, es unterscheidet sich.</sample>
    <sample id="79">Ja, die Coscript-Datenbank ist für die Forschung auf der Grundlage von Sprachplanung als Ressource frei zugänglich.</sample>
    <sample id="80">Das Wasserzeichen wird in den Text über einen Prozess namens 'Watermark Injection' eingebettet. Hierbei wird ein Trigger-Set, eine Gruppe von Wörtern mit moderater Häufigkeit, verwendet. Wenn ein Benutzer eine Anfrage macht, wird die Häufigkeit der Triggers im Anfragetext bestimmt. Der bereitgestellte Embedding ist eine Gewichtsummation des Zielschwemmings und des ursprünglichen Schwemmings, wobei das Gewicht des Zielschwemmings proportional zur Anzahl der Triggers im Anfragetext ist. Wenn die Anzahl der Triggers im Anfragetext größer als M ist, dann ist der bereitgestellte Schwemm exactly gleich dem Zielschwemm.</sample>
    <sample id="81">Die Autoren gehören an der Peking University.</sample>
    <sample id="82">This video discusses a study on unsupervised automated essay scoring (AES) using multiple heuristic signals as pseudo ground truth. The study proposes a novel framework called URRA, which uses these signals to train a neural AES model through rank aggregation. The framework includes an H-R module for generating partial order pairs and a DPRA module for aggregating these pairs into unified supervision. Experiments show that URRA outperforms unsupervised baselines and achieves competitive performance with supervised methods.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mt5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="84">The speaker introduces a joint work on simultaneous speech translation, which involves translating spoken language into text in another language in real time. The current models have specific architectures that are usually trained, and the speaker aims to extend their method to other machine learning networks, hardware, and structured manners. Additionally, they plan to introduce more modes such as the combination of zero elements, static parameters, and dynamic parameters.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist die Erstellung von Skripten.</sample>
    <sample id="86">Sie visualisieren die Einbettungen von Sätzen auf der PCA-Bild und zeigen, dass es schwierig ist, zwischen den Hintergrund-Einbettungen und normalen Einbettungen zu unterscheiden.</sample>
    <sample id="87">Die Arbeit bestehende PLMs nutzt die Anonymisierungsdaten von einem Krankenhaus, um ein neues PLM aufzubauen.</sample>
    <sample id="88">GPT-4 ist am wenigsten auf Indien ausgerichtet.</sample>
    <sample id="89">Die Erklärung zeigt, dass das Modell die Werte der Kruz-Atention ansehen kann, um zu bestimmen, ob ein Wort relevant ist. Wenn die Summe der Kruz-Atention über einen bestimmten Schwellenwert (Alpha) liegt, wird das Wort nicht emittet und das Modell wartet auf einen neuen Sprachblock.</sample>
    <sample id="90">The paper discusses the feasibility of using language learners as annotators for natural language processing (NLP) data annotation. It presents a proof-of-concept study that compares the accuracy and learning effects of language learner annotations with those of native speakers. The study involves three languages: English, Korean, and Indonesian, and four tasks: sentiment analysis, entailment, named entity recognition, and span prediction. Learners are categorized into basic, intermediate, and advanced levels based on their language proficiency. The results show that language learner annotations are nearly accurate, especially for simpler tasks and easy to medium-level questions. Aggregating these annotations with majority voting can almost match the performance of native speakers. Additionally, training simulations using learner annotations achieve about 95% of ground-truth performance, sometimes outperforming models trained with native speaker labels. The study suggests that language learners can contribute to NLP research, particularly in low-resource languages where it is difficult to recruit native speakers.</sample>
    <sample id="91">Die Anzahl der Aufgaben hat einen positiven Einfluss auf die Leistung des Modells.</sample>
    <sample id="92">Die Autoren vergleichen ihre Methode mit drei Baselines auf dem CoNLL Benchmark: BERT, RoBERTa und XLNet.</sample>
    <sample id="93">Die beiden Co-Autoren sind der erste Autor's Berater.</sample>
    <sample id="94">The video introduces a paper titled "Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding and Services via Backdoor Watermark." It discusses the background of embedding services, which are built upon large language models like GPT, BLOOM, and LLaMA to assist various NLP tasks. The video explains that attackers can steal these models by learning from embeddings and providing similar services, making it necessary to protect the copyright of embedding services. One solution proposed is to embed a watermark in the provided service and detect whether another service contains the watermark. The watermark method should be applicable to embedding services, not degrade the utility of the provided embeddings, be covert enough to prevent easy removal by attackers, and be transferable during model extraction. Existing methods either lack applicability or transferability. The paper proposes Embedding Marker, a backdoor-based watermark method applicable to embedding services. It consists of two main steps: watermark injection and copyright verification. The watermark injection step involves selecting a trigger set, defining a target embedding, and calculating the weight of the target embedding based on the number of triggers in the sentence. The copyright verification step involves constructing a backdoor and benign dataset, requesting embeddings from the attacker's service, and computing the similarity between the requested embedding and the target embedding using cosine and L2 similarity, along with KS test p-value. The video concludes with an explanation of the experimental results on four datasets, showing that Embedding Marker has great detection performance while maintaining high utility for downstream tasks.</sample>
    <sample id="95">David Villegas Torres</sample>
    <sample id="96">Sebastian Santy, Jenny T. Liang, Ronan Le Bras und Katharina Reinecke präsentieren in einem Vortrag die Arbeiten zu NLPositionality: Characterizing Design Biases of Datasets and Models. Sie diskutieren die Herausforderungen und Schwierigkeiten der Aufgabe der Semantikgenerierung in mehreren natürlichen Sprachen und Maschinellen Darstellungen. Sie betonen, dass die meisten bestehenden Modelle speziell auf bestimmte Sprachen und Darstellungen abgestimmt sind, was zu mangelnder Abdeckung bestimmter Sprachen und Darstellungen führt. Um diese Lücken zu schließen, haben sie Exemplar vorgestellt, ein einheitlicher Datensatz für die Semantikgenerierung in mehreren natürlichen Sprachen und Darstellungen. Sie präsentieren auch eine umfassende Studie über verschiedene Sprachmodelle und ihre Leistungen in verschiedenen Szenarien. Sie betonen, dass die semantische Analyse in mehreren Sprachen und Darstellungen eine wichtige Rolle bei der Gestaltung von fairen und transparenten Algorithmen spielt. Sie empfehlen, die Designentscheidungen im Forschungsprozess zu dokumentieren, NLP-Studien mit Blick auf Perspektivismus durchzuführen und spezialisierte Datensätze und Modelle für bestimmte Gemeinschaften zu erstellen. Sie betonen, dass inclusive NLP nicht nur eine technologische Aufgabe ist, sondern auch eine soziale Verpflichtung, um sicherzustellen, dass alle Menschen von den Technologien profitieren können.</sample>
    <sample id="97">Die Referentin spricht von zwei Problemen der aktuellen SimulST-Modelle.</sample>
    <sample id="98">Die Reduktion sozialer und politischer Verzerrungen in Datensätzen beim Training von NLP-Modellen ist schwierig, da es schwierig ist zu bestimmen, was als neutrales Datensatz gilt. Es gibt auch das Risiko von Censorship oder Exklusion, wenn versucht wird, die Datensätze zu reinigen.</sample>
    <sample id="99">Die 61. jährliche Versammlung der Gesellschaft für Computergestützte Linguistik (ACL) fand in Toronto, Kanada, vom 3. bis 14. März 2023 statt. Die Präsentation von Ciu Yu Yan aus Fudan University präsentierte das Werk "Distilling Script Knowledge from Large Language Models for Constrained Language Planning". In unserem Alltag planzen wir unsere Handlungen nach Schritt-für-Schritt-Anweisungen in Form von Skripten. Vorherige Arbeiten haben gezeigt, dass große Sprachmodelle in der Lage sind, abstrakte Ziele zu planen, die stereotypen Aktivitäten wie das Brotbacken umfassen. Es wurde bewiesen, dass große Sprachmodelle effizient Ziele in Schritte zerlegen können. Allerdings konzentriert sich vorherige Arbeit hauptsächlich auf die Planung abstrakter Ziele stereotypischer Aktivitäten. Die Planung spezifischer Ziele mit bestimmten Bedingungen, wie zum Beispiel das Backen eines Schokoladenkuchens, ist bisher ununtersucht worden. In diesem Papier definieren wir das Problem der eingeschränkten Sprachplanung, bei dem verschiedene Bedingungen auf die Planung angewendet werden. Ein gutes Planer sollte Skripte schaffen, die realistisch und den Bedingungen entsprechend sind. In diesem Papier erstellen wir zuerst eine evaluierbare und verbesserte Sprachplanfähigkeit von großen Sprachmodellen. Da keine Datensammlung von spezifischen Zielen existiert, um unser Studie zu unterstützen, müssen wir zuerst diese Ziele sammeln. Wir erweitern abstrakte Ziele mit mehreren Bedingungen für Menschen im Lernprozess mit der Anleitung "Instruction GPT" und sammeln 100 spezifische Ziele. Wir evaluieren dann die generierten Skripte von großen Sprachmodellen. Das Tabellendarstellungsformat zeigt die allgemeine Genauigkeit der Ergebnisse an. Wir finden, dass alle großen Sprachmodelle unzufriedenstellende Resultate auf der Planung von spezifischen Zielen erzielen. Wir führen dann eine detaillierte Analyse durch, um zu untersuchen, warum große Sprachmodelle solche Resultate erhalten. Das Diagramm zeigt, dass die semantische Komplettät der generierten Skripte akzeptabel ist, aber die Einhaltung der Bedingungen nicht gewährleistet werden kann. Wir teilen die Bedingungen in verschiedene übergeordnete Topologien ein und visualisieren die Planungsleistung von "Instruction GPT" für Ziele in verschiedenen Kategorien. Vorherige Studien haben gezeigt, dass die Ausgabequalität von großen Sprachmodellen stark variieren kann, was zu schlechter Leistung führt. Um die Ausgabequalität zu verbessern, verwenden wir das Konzept des "Over-Generativen Filter" zur Verbesserung der Generationsqualität. Wir erstellen zuerst spezifische Ziele basierend auf den abstrakten Zielen und generieren dann Skripte für spezifische Ziele. Wir verwenden dann einen Filtermodell zu selektieren, die passenden Skripte auswählt. Wir konvertieren Skripte und Ziele in "Instruction GPT"-Embeddings und berechnen die Kosinus-Similarität als Ähnlichkeits得分, um die semantische Ähnlichkeit zu messen. Wir wertbaren nur die Skripte, die die Schlüsselwörter der Zielschwelle enthalten und die höchste Ähnlichkeits得分 im Ziel-Satz haben. Mit unserem Ansatz können "Instruction GPT" Skripte von hohen Qualität generieren. Unsere Methode verbessert die Planfähigkeit sowohl in Bezug auf die semantische Komplettät als auch die Einhaltung der Bedingungen. Da große Sprachmodelle teurer zu deployen sind, ist es wichtig, die Sprachplanfähigkeit kleinerer und spezialisierter Modelle zu fördern. Die Erstellung von Datensätzen ist ein wichtiger Schritt, um dies zu erreichen. Allerdings haben vorherige Studien gezeigt, dass die Planung von spezifischen Zielen nicht effizient durchzuführen ist, und die manuelle Datenermittlung teuer ist. Daher folgen wir dem Ansatz der symbolischen Wissensdissolution, um Datensätze für die eingeschränkte Sprachplanfähigkeit zu erstellen. Wir verwenden unser Ansatzwerkzeug, um einen Datensatz namens "Co-Script" zu erstellen. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten, um die Qualität der Validierung und Testdaten sicherzustellen. Wir bitten Cloudworker, die inkorrekten Samples zu finden und zu überprüfen. Das Diagramm zeigt die Verteilung der eingeschränkten Skripte. Wir finden, dass "Co-Script" eine hohe Häufigkeit in den generierten spezifischen Zielen aufweist. Mit "Co-Script" können wir kleinere, spezialisierte Modelle für die eingeschränkte Sprachplanfähigkeit verwenden. Wir finden, dass "T5-Funktion" auf "Co-Script" Skripte von hohen Qualität generieren kann, was mehr als die meisten großen Sprachmodelle zeigt. Dies zeigt, dass kleinere Modelle große Modelle unterstützen können, wenn sie auf geeignetem Datensätzen trainiert werden. Insgesamt haben wir das Problem der eingeschränkten Sprachplanfähigkeit definiert, die Sprachplanfähigkeit von großen Sprachmodellen bewertet und einen "Over-Generativen Filter" Ansatzwerkzeug für große Sprachmodelle entwickelt. Wir verwenden große Sprachmodelle, um einen Datensatz namens "Co-Script" für die eingeschränkte Sprachplanfähigkeit zu erstellen. Wir hoffen, dass der "Co-Script" Datensatz eine wertvolle Ressource zur Fortschrittsarbeit in der Sprachplanfähigkeit darstellen kann. Vielen Dank für Ihr Interesse. Weitere Details über "Co-Script" finden Sie in unserem Papier.</sample>
    <sample id="100">Multi-hop QA involves answering questions that require multiple reasoning steps, with each step typically corresponding to a document in the corpus. For instance, to answer the question "What 1988 Christmas comedy film did Brian Doyle Murray star in?", one must first find all movies Brian Doyle Murray starred in and then identify the movie released in 1988. This set of documents required to answer a question is called the chain. Multi-hop retrievers are trained by maximizing the probability of the ground truth chains given questions. Most state-of-the-art multi-hop retrievers fall under this paradigm. Existing systems require thousands of examples of questions and ground truth chains for good performance, which can be expensive, especially for low-resource domains and those requiring special expertise. Our approach, PromptRank, is efficient and provides good performance with as few as 128 examples. The idea is to combine an unsupervised retrieval method with a few-shot language model-based reranking. The main steps involve retrieving a pool of candidate chains using TF-IDF retrieval and hyperlink traversal, then reranking these candidates using the few-shot language model reranker. Two points to consider are what scoring function to use and how to prompt the language model to extract this score. We use the likelihood of the question given the chain according to a language model. We construct a chain prompt by inserting the chain documents into the prompts and using an indicator token to designate that this is a document. We have an instruction which serves to elicit the language model's reasoning ability over the chain documents. We experiment with GPT-2 and T5 and evaluate our approach on Hotpot-QA. We use metrics such as retrieval R@K, recall@K, and answer recall@K. We also evaluate the downstream QA performance when using PromptRank as a retriever. We see that PromptRank outperforms fully supervised systems like Dr. Kit and performs comparably to state-of-the-art multi-hop retrievers. We also learn ablation to verify the importance of each component we propose and find that each component plays a role in the final performance of PromptRank. We evaluate the downstream QA performance when using PromptRank as a retriever and see that it exhibits very good downstream QA multi-hop QA performance, underperforming in DR by only around four exact match points.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit jenen von state-of-the-art-Systemen.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind die Great Detection Performance und die Great Utility for downstream tasks.</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 verschiedenen Sprachen übersetzt.</sample>
    <sample id="104">Ein Datensatz wird normalerweise über mehrere Annotatoren anhand von demografischen Merkmalen extrahiert.</sample>
    <sample id="105">Cosine- und L2-Similarität, K-S-Test mit p-Wert</sample>
    <sample id="106">The paper presents a dataset called QUEST, which includes over 30 million entity-seeking queries with implicit set operations. The queries are derived from Wikipedia category names and validated by human annotators for fluency and relevance. The dataset poses a challenging retrievable problem due to the need to search over a large document corpus to find multiple answer sets where the attribution for different query constraints can come from different parts of the document. The paper also shows that there is a large room for improvement on retriever performance based on the recall of the complete answer set, indicating that the current systems struggle to handle such queries.</sample>
    <sample id="107">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden verwendet, um die Aufgabe zu überwinden, indem sie die Sprachverstehensfähigkeit verbessern.</sample>
    <sample id="108">The paper discusses the limitations of evaluating language models using the Minimal Pair Paradigm (MPP), which typically involves comparing acceptable and unacceptable sentences to determine a model's acceptability judgment. The authors argue that this method is not robust to context, particularly for longer sentences. They propose revisiting the MPP by simulating longer sequences and testing the model's ability to evaluate acceptability across extended contexts. The paper highlights the importance of considering different datasets and contexts to better understand how language models process and judge sentences.</sample>
    <sample id="109">The paper presents a dataset called "Natural Instructions" which consists of natural language instructions and their corresponding inputs and outputs. The data was collected in a fully automatic manner without any human annotations. The model generates an instruction and a corresponding input, then generates a corresponding output. The resulting dataset contains 64k examples, and if we consider the instruction paraphrases, we have about 240k examples. The generated examples are analyzed for creativity, diversity, and correctness. The model can outperform both zero-shot and few-shot instruction tuning across several benchmarks.</sample>
    <sample id="111">Die Autoren verwenden die Häufigkeit der Wörter als Kriterium, um zu bestimmen, ob sie Wörter mit mittlerer Häufigkeit sind. Sie betrachten Wörter, die in 10% der Sätzen vorkommen, als Wörter mit mittlerer Häufigkeit.</sample>
    <sample id="112">Guten Tag, heute präsentiere ich unser Papier "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" Ich heiße Zhueng Liu. In unserem Papier untersuchen wir das Problem der allgemeinisierten Named Entity Recognition (NER) Aufgabe, indem wir die NER-Task, auch bekannt alsNamed Entity Recognition, untersuchen. Wir haben festgestellt, dass Modelle seit fast 20 Jahren auf CoNLL-2003 verwendet wurden, um NER-Modelle zu entwickeln, und dies natürlicherweise führt zu mehrere Probleme. Zunächst einmal können diese Modelle auf modernes Datenmaterial generallisieren? Und wenn wir neue Tagger erstellen, was ist für eine gute allgemeinisierte Performance erforderlich? Gleichzeitig, wenn wir eine schlechtere allgemeinisierte Performance beobachtet haben, was verursacht die Abnahme der Leistungsfähigkeit dieser Modelle? Um diese Probleme zu untersuchen, haben wir den CoNLL++ Datensatz entwickelt. Dieser Datensatz haben wir aus Reuters-News von 2020 ermittelt und mit den gleichen CoNLL-2003-Annotationen guidelines annotiert. Wir haben dann über 20 Modelle auf CoNLL-2003 finetuned und sie auf sowohl den CoNLL-03-Testset als auch den CoNLL++-Testset evaluiert. Und letzt but not least haben wir den prozentualen Änderung in F1-Bewertungen verwendet, um die allgemeinisierte Leistung jedes Modells zu messen. Was ist also für eine gute allgemeinisierte Performance erforderlich? Durch unsere Experimente haben wir festgestellt, dass es drei Hauptingredients bedarf. Das erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten generallisieren. Das zweite Ingredient ist die Modellgröße. Wir haben festgestellt, dass normalerweise größere Modelle zu einer besseren allgemeinisierten Performance führen. Immerhin ist es wichtig zu beachten, dass die Anzahl der Fine-Tuning-Beispiele direkter Auswirkungen auf die Leistung der Downstream-Aufgabe hat. Hier haben wir auch festgestellt, dass mehr Fine-Tuning-Beispiele tatsächlich auch zu einer besseren allgemeinisierten Performance führen. Was nun die Abnahme der Leistungsfähigkeit einiger Modelle betrifft, haben wir zwei Hypothesen aufgestellt. Die eine Hypothese lautet auf adaptive Overfitting, das ist Overfitting, das durch das Wiedervereinben des gleichen Testsets über und über entsteht und normalerweise als die Verminderung der Returns auf dem neuen Testset manifestiert wird. Die andere Hypothese lautet auf Temporal Drift, das ist die Leistungsabwertung, die durch die zunehmende Zeitdistanz zwischen dem Train- und Test-Dataset entsteht. Bei adaptive Overfitting haben wir gesehen, dass die rote "best-fit"-Kurve eine Steigung größer als 1 hat. Das bedeutet, dass jede Einheit Verbesserung, die wir auf CoNLL-2003 erreichten, sich auf über eine Einheit Verbesserung auf CoNLL++ überträgt, was bedeutet, dass es keine Verminderung der Returns gibt. Und das zeigt uns, dass adaptive Overfitting in diesem Fall nicht auftritt. Also was ist dann Temporal Drift? Für Temporal Drift haben wir ein Experiment durchgeführt, bei dem wir einige Modelle mit jüngerem Datensatz erneut trainieren ließen. Und wir haben festgestellt, dass die Leistungs能力 mit einem größeren Temporal Gap abnimmt. Und das bestätigt unsere Hypothese, dass die Hauptursache der Leistungsabwertung Temporal Drift ist. Unsere Schlussfolgerung lautet, dass für eine gute allgemeinisierte Performance ein besseres Modellarchitektur, eine größere Modellgröße und mehr Fine-Tuning-Beispiele benötigt werden. Und diese Bedingungen hängen einander ab. Wir können nicht nur einIngredient haben, sondern alle Bedingungen müssen erfüllt sein. Gleichzeitig haben wir auch festgestellt, dass die Leistungsabwertung hier durch Temporal Drift verursacht wird und überraschenderweise ist es nicht durch adaptive Overfitting verursacht, obwohl CoNLL-2003 seit über 20 Jahren verwendet wurde. Also zurück zu der Frage, die wir im Titel unseres Papiers gestellt haben: "Do CoNLL-2003 taggers still work well in 2023?" Und wir haben festgestellt, dass die Antwort ein überzeugender Ja ist. Wir hoffen, unser Papier inspiriert weitere Forschung anregt, wie man die allgemeinisierte Leistungsfähigkeit von Modellen verbessern kann. Und schließlich, bitte überprüfen Sie unser Papier, unser Datensatz und falls Sie Fragen haben, kontaktieren Sie mich gerne. Vielen Dank.</sample>
    <sample id="114">This paper presents a new method called Group Head Attention for compressing multi-head attention in large language models. The method uses a divide-and-conquer strategy to group heads and prune redundant ones, achieving significant parameter compression without sacrificing performance. The proposed method outperforms previous works in terms of parameter efficiency and model compression, and it achieves comparable or better performance on three tasks: machine translation, language modeling, and abstract summarization.</sample>
    <sample id="115">Die Sprachsegmentgröße wird auf Lambda gesetzt.</sample>
    <sample id="116">Servin ist ein Richter.</sample>
    <sample id="117">Der wichtige Faktor zwischen der Qualität des Beispiels und der Ähnlichkeit mit dem Ausgangssatz ist die Qualität des Beispiels.</sample>
    <sample id="118">The video presents a research submission on improving pre-training techniques for code-switched NLP. The researchers define code-switching as the mixing of languages in a sentence, such as English and Hindi in this example. They propose novel MLM techniques and architectural changes to enhance performance on code-switched tasks like question answering and sentiment analysis. The submission includes results from probing experiments that verify the effectiveness of the proposed methods.</sample>
    <sample id="119">Die Arbeiten konzentrieren sich auf Sprachmodelle mit unterschiedlichen politischen Neigungen.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus einer bestimmten Ebene.</sample>
    <sample id="121">Beispiele für direkte Inferenz sind die Nennung des Songs "Easy on Me" oder seine Position als "das erste".</sample>
    <sample id="122">Die Autoren gehören an Peking University.</sample>
    <sample id="123">The research presented focuses on improving multi-modal zero-shot learning through instruction tuning. The study explores the effectiveness of instruction tuning in enhancing the performance of large language models (LLMs) on unseen multi-modal tasks, particularly in computer vision and multimodal contexts. The researchers address the lack of publicly available multi-modal instruction datasets by introducing MultiInstruct, a benchmark dataset comprising 62 diverse multi-modal tasks across 10 broad categories. Each task is derived from existing open-source datasets and equipped with five expert-written instructions.

The study uses OFA, a unified multi-modal pre-trained model, as the base for their experiments. They train the model on 53 tasks from NLG groups, sampling 10,000 instances per task, and test it on a comprehensive set of tasks from various sources, including Commonsense Reasoning, Wiki, and miscellaneous groups. The evaluation metrics include accuracy for classification tasks, ROUGE-L for generation tasks, and a new metric called sensitivity to measure the model's consistency in output across different instruction variations.

The results demonstrate that instruction tuning significantly improves the performance of OFA on unseen multi-modal tasks, with better performance and lower sensitivity observed as the number of tasks increases. The study also highlights the benefits of transfer learning from natural instruction datasets, which can further enhance the model's performance and reduce sensitivity. Overall, the research proposes the first large-scale multi-modal instruction tuning dataset, which aims to improve the zero-shot capability of LLMs and explore different transfer learning techniques.</sample>
    <sample id="124">The presentation discusses the development of a comprehensive dataset for evaluating temporal reasoning capabilities in large language models (LLMs). The dataset, named Temp Reason, covers three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event. It includes long temporal coverage and is evaluated across three QA problem settings: closed book, open book, and reasoning QA. The presentation also introduces a training strategy with two components: temporal span extraction pre-training and time-sensitive reinforcement learning. Experimental results show that the proposed Temp 5 model significantly outperforms other models on L2 and L3 reasoning tasks and shows improved performance in open book and reasoning QA settings.</sample>
    <sample id="125">Die Arbeit beteiligen sechs Autoren.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit einem maschinellen Übersetzungsmodell wurde als Baseline betrachtet.</sample>
    <sample id="127">The video is a presentation by Namguyu Ho, a master's student at KAIST AI in Korea, introducing their work on large language models as reasoning teachers. The technique involves using huge language models to generate step-by-step solutions for complex tasks and using these solutions as training data for smaller models. The presenter emphasizes the novelty of their method, called diverse reasoning, which generates multiple solutions using stochastic temperature sampling. The results show that this method significantly outperforms existing baselines and vanilla fine-tuning on most tasks, even with small models. However, there are trade-offs between development costs, inference time costs, and the quality of inference that need to be considered when applying this method in real-world scenarios.</sample>
    <sample id="128">This paper presents a diagnostic test suite for evaluating knowledge integration in natural language understanding models. The authors propose a co-reference resolution task to assess the ability of models to draw on knowledge from different sources. They evaluate the dataset with human study participants and establish co-reference resolution models. The results show that while many co-reference resolution models struggle to reason over knowledge from different sources without task-specific training, some models can successfully integrate knowledge from multiple sources when trained on general co-reference resolution datasets. However, even the best-performing models have difficulties integrating background knowledge presented only at inference time.</sample>
    <sample id="129">Die Autoren haben als Beispiel für eine markierte Gruppe eine Afroamerikanerin mit dem Attribut "stark" und "resilient" gegeben.</sample>
    <sample id="130">Die Modellarchitekturen, die nicht gut generalisieren, sind diejenigen, die nicht auf einem Transformer-basierten Modell basieren.</sample>
    <sample id="131">Clean test sets.</sample>
    <sample id="132">Die Arbeit beteiligen zwei Autoren, namens Machtah und Martin.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, nicht nur mit Text.</sample>
    <sample id="135">Die ABC-Eval-Methode ist eine neue Dimensionsobergreifende Ansatz zur Beurteilung von conversationaler KI. Sie zielt darauf ab, die subjektive Natur menschlicher Beurteilungen zu reduzieren und obereigene Verhaltensweisen in Chats zu annotieren. Die Methode hat sich als zuverlässig und prägnant erwiesen, indem sie 25% der Conversationsqualität erklären kann. Trotz dessen sind immer noch einige Herausforderungen zu bewältigen, wie z.B. die Häufigkeit von Fehlentscheidungen bei den Chatmodellen.</sample>
    <sample id="136">The presentation introduces a new evaluation set called Fermat for assessing the mathematical abilities of language models. It highlights the limitations of current benchmarks, which primarily focus on accuracy scores and F1 measures, and introduces Fermat as a flexible set of math worded questions extracted from Illinois and Common Core. The evaluation covers number understanding, mathematical operations, and training dependency. The results show that most models perform poorly across these aspects, with the original set of benchmarks being less representative of real-world needs. Fine-tuning with math teachers' templates improves performance, but the models still struggle with memorizing exact expressions. The impact of training templates is also explored, showing that diversity in language and mathematical operations significantly improves model performance.</sample>
    <sample id="137">The Tell2Design dataset is a collection of floor plans and corresponding natural language instructions designed to facilitate language-guided floor plan generation. It includes 5,051 human-annotated instructions from crowdsourcing platforms like Amazon Mechanical Turk and around 76,000 artificially generated instructions based on predefined templates. The dataset aims to enable users without expertise in design to create floor plans by following specific language instructions, focusing on the floor plan domain as an initial area of research. The task involves generating 2D floor plan designs directly from language instructions, which include semantics for room types and functionalities, geometry for shape and dimensions, and topology for spatial relationships. The dataset is used to train and evaluate machine learning models for floor plan generation, with the Tell2Design model achieving the highest performance scores compared to text conditional image generation baselines.</sample>
    <sample id="138">Nach Ansicht der Autoren ist ein zu wenig erforschtes Gebiet im Bereich der NLU die Integrität von Kenntnis aus verschiedenen Quellen.</sample>
    <sample id="139">Die Referenten heißen Ying Shen, Zhiyang Xu und Lifu Huang.</sample>
    <sample id="140">Ja, Coscript wurde von Cloudsource workers überprüft.</sample>
    <sample id="141">Die Grenzen liegen in der Begrenzung auf bestimmte Typen von kontextbasierten Übersetzungen und Sprachen, da sie normalerweise auf Domänenwissen und menschliche Kuration basieren.</sample>
    <sample id="142">Das Video zeigt ein PowerPoint-Diagramm mit einem Titel "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)" und den Autoren Mohammad Javad Hosseini, Filip Radlinski, Silvia Paretti und Annie Louis. Es zeigt auch die Google Research-Logo im unteren linken Ecke. Das Video beginnt mit einem Vortrag von Javad Hosseini, der über das Arbeiten an einem Forschungsprojekt spricht, in dem sie versuchen, indirekte Beziehungen in Sprache zu verstehen. Er führt die AltEntities Corpus an und erläutert, dass sie versuchen, zu verstehen, was Benutzer bedarf, wenn sie Entscheidungen treffen möchten. Er gibt ein Beispiel an, bei dem ein Benutzer zwischen zwei Liedern wahlwärts fragt und zeigt, wie direkte Beziehungen verwendet werden können, um eine Entscheidung zu treffen. Er zeigt auch einige indirekte Beziehungen an, indem er Beispiele wie "die neueren" und "das Lied, das nicht energiegeladen ist" zeigt. Er betont, dass es wichtig ist, indirekte Beziehungen zu verstehen, um in conversational Systems zu verbessern undBenchmarking LLMs (Large Language Models) zu verbessern. Er erwähnt, dass sie noch keine große öffentliche Datensammlung für diese Aufgabe haben und daher eine Datensammlung per Crowdsourcing erstellen. Er zeigt auch, wie sie die Datensammlung sammeln und welche Methoden sie verwenden, um indirekte Beziehungen zu erstellen. Er zeigt auch, wie sie die Datensammlung anwenden und welche Ergebnisse sie erhalten. Er zeigt auch, dass die Modelle dominierbar sind und dass es Raum für Verbesserungen gibt. Er zeigt auch, dass die Modelle robust sind und dass es viele Anwendungsmöglichkeiten gibt.</sample>
    <sample id="143">Die Ansatz wird mit den off-line Modellen verglichen.</sample>
    <sample id="144">The authors belong to Nantes University.</sample>
    <sample id="145">The speaker's name is Ibil Bilard.</sample>
    <sample id="146">The speaker introduces a paper on the analysis of omission in dialogue summarization. The paper discusses the challenges of dialogue summarization, including the fact that even state-of-the-art models still have high omission rates. The paper proposes a new dataset for dialogue summarization and explores three different frameworks for detecting omissions. The results show that the task is challenging, but the detection of omissions can improve the quality of the summary.</sample>
    <sample id="147">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Università di Trento und Fondazione Bruno Kessler. Ich werde kurz die Papier "Attention as a Guide for Simultaneous Speech Translation" von Matteo Negri und Marco Turchi präsentieren. Was ist Simultane Sprachübersetzung? Simultane Sprachübersetzung (SMT) ist die Prozess von Übersetzen gesprochenen Sprache in einen Text in einer anderen Sprache in Echtzeit, indem krußsprachige Kommunikation ermöglicht wird. Und was sind die Probleme der aktuellen SMT-Modelle? Spezielle Architekturen werden normalerweise trainiert, indem zusätzliche Module optimiert werden. Long and complicated training procedures, for example, training involving different optimization objectives and training and maintaining several models to reach different latency regimes, for example, training a model with an average of one second latency and another one with two seconds latency and so on. Also, was ist unsere Lösung? Zunächst verwenden wir schon existierende offline-Netzwerke-Modelle, ohne sie zu retrainieren oder spezielle Architekturen für SMT zu adoptieren. Wir verwenden nur ein Modell für jede Latenzregel und handeln Latenz durch spezifische Parameter und nutzen die Kenntnisse, die schon durch die Aufmerksamheitsmechanismus zwischen Audio-Eingabe und Textausgabe erworben wurden, um die Übersetzung zu optimieren. Hier sieht ihr ein Beispiel auf der rechten Seite. Unsere Lösung ist die Vordruck-oder Encode-Decode-Aufmerksamkeit und es ist eine Strategie, bei der wir entscheiden, ob wir einen teilweisen Übersetzung emmitten oder nicht, basierend auf, wo die Aufmerksamkeit zeigt. Ein Wort wird emmitten, wenn die Aufmerksamkeit nicht konzentriert ist, das ist, wenn die Summe unter einem bestimmten Schwergewicht Alpha liegt, in den letzten Lambda-Speech-Frames, was bedeutet, dass die empfangene Information stabil genug ist. Zum Beispiel, wenn wir einen Sprachabschnitt erhalten, der "Ich werde über etwas sprechen" enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, dann schauen wir uns die Aufmerksamheitsgewichtswerte an. Wir sehen, dass die ersten zwei Worte auf die earliest received speech frames zeigt, während die letzte Wort auf die last received speech frames zeigt, also lambda speech frames. Das bedeutet, dass die ersten zwei Worte emmitten werden. Da die Summe der Aufmerksamheitsgewichtswerte über einem bestimmten Schwergewicht Alpha liegt, werden wir die letzte Wort nicht emmitten und warten auf einen neuen Sprachabschnitt. Wenn wir fortfahren und einen neuen Sprachabschnitt erhalten, und unser Modell weitere drei Worte vorhersagt, dann schauen wir uns die Aufmerksamheitsgewichtswerte an. Wir sehen, dass keines der Worte auf die last lambda speech frames zeigt. Das bedeutet, dass diese drei Worte emmitten werden. Wenn wir die Hauptergebnisse von Adat betrachten, plotten wir die Simultane Sprachübersetzung-Ergebnisse auf Grafiken, in denen wir auf der linken Seite blau haben, das die Übersetzungqualität misst, und auf der rechten Seite die durchschnittliche Latenz, das ist die Latenzmessung, und wir berücksichtigen auch die durchschnittliche Reaktionszeit, die die modell's Reaktionszeit zur Berechnung des Ausgangs berücksichtigt. Wir wollen, dass unsere Kurve so hoch wie möglich auf dieser Plattform ist, aber wir wollen auch, dass sie links verschiebt sind. Und wir vergleichen mit anderen Strategien, die auf offline-Modellen angewandt wurden, die die White-Ki-Strategie und die lokale Aggregation sind. Und wir vergleichen auch mit den state-of-the-art-Architektur speziell für Simultane Sprachübersetzung. Hier sind die Ergebnisse der Simultane Sprachübersetzung-Strategie auf Deutsch. Und wir sehen, dass Adat übernimmt alle Strategien, die auf offline-Modelle angewandt wurden, da die Kurven links verschiebt sind. Und wir sehen auch, dass wenn wir die tatsächliche Laufzeit oder die durchschnittliche Reaktionszeit berücksichtigen, Adat die schnellste Strategie ist. Wenn ihr mehr Ergebnisse entdecken wollt, lest ihr unser Papier und wir haben auch Open Source-Code und Modelle und Simultane Hauseingabe freigegeben, um die Wiederholbarkeit unseres Werks zu erleichtern. Danke für eure Aufmerksamkeit.</sample>
    <sample id="149">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">The paper presents a new dataset called Meeting Q&amp;A, which is an extractive question answering dataset based on questions asked by participants in meetings and the corresponding answer sentences. The dataset contains 7.7 thousand questions, with 30% unanswerable, 40% multi-span answers, and 48% multi-speaker answers. The paper also discusses the results of various models on the dataset, including short context models, single-span models, and multi-span models. The paper concludes that existing QA models struggle to identify rhetorical questions and speaker answers, especially in zero-shot settings.</sample>
    <sample id="151">Hallo, wir präsentieren heute unsere Forschung zu MultiInstruct: Eine Verbesserung der mehrmodalen Null-Shot-Lernung durch die Optimierung der Anweisungen. Mit Fortschritten in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu untersuchen, indem sie übertrainte Sprachmodelle für verschiedene unternehmensnahe Aufgaben in einer präziser und dateneffizienteren Weise verwenden. Kürzlich haben viele Studien gezeigt, dass die Optimierung der Anweisungen große Sprachmodelle dazu bringt, auf Eingabe-Aufgaben in einem Null-Shot-Modus zu arbeiten, indem sie natürliche Anweisungen folgen. Allerdings haben die meisten vorherigen Arbeiten sich hauptsächlich auf die Verbesserung der Null-Shot-Performance auf Sprachonly-Aufgaben konzentriert, während Computer Vision- und mehrmodulare Aufgaben weggelassen wurden. Daher wollen wir in diesem Werk untersuchen, ob die Optimierung der Anweisungen auf mehrmodulare übertrainte Modelle tatsächlich die allgemeinere Anpassung an Eingabe-Mehrmodul-Aufgaben verbessert. Zudem haben wir bei unserem Forschungsbeginn festgestellt, dass es einen erheblichen Unterschied im Verfügbaren von Anweisungsdatensätzen zwischen NLPA und Mehrmodul gibt. Es existieren mehr als 1600 Sprachonly-Anweisungs Aufgaben, aber es gibt keinen großen öffentlich zugänglichen Mehrmodul-Anweisungs Aufgabensatz. Das motiviert uns daher, einen Mehrmodul-Anweisungs Optimierungs Datensatz zu erstellen. Hier präsentieren wir MultiInstruct, den ersten Mehrmodul-Anweisungs Optimierungs Datensatz, der 62 diverse Mehrmodul Aufgaben umfasst, die 10 Hauptkategorien abdecken. Diese Aufgaben werden aus 21 bestehenden Offenen Quellens Datensätzen abgeleitet und jede Aufgabe wird mit 5 Experten definiertem Anweisungen versehen. Um die Mehrmodul-Anweisungs Optimierung auf unserem vorgeschlagenen Datensatz zu untersuchen, verwenden wir OFA, ein vereinbares Mehrmodul übertraintes Modell, als Basismodell. OFA verwendet eine vereinbares Vokabular für Sprach-, Bild- und Koordinaten Tokens. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct Datensatz an. Um die vereinbares Verarbeitung von verschiedenen Eingabe- und Ausgabedatentypen zu gewährleisten, folgen wir dem Ansatz von OFA und formulieren alle Aufgaben in einem vereinbarten sequenz-to-sequence Format, in dem die Eingabetexte, Bilder, Anweisungen undBounding Boxes im gleichen Tokenraum dargestellt werden. Okay, nun werde ich über Mehrmodul-Anweisungs Optimierung sprechen. Also für die Trainingsdatenbank verwenden wir 53 Aufgaben aus der Gruppe für Training und wir sampling 10.000 Instanzen pro Aufgabe. Für die Testphase reservieren wir die gesamte Commonsense-Reasoning-Gruppe für die Testphase und wir selecten weitere 5 Aufgaben aus dem Wiki und dem MSc-Gruppe. Wir verwenden alle Instanzen in der Testsplit für jede Aufgabe. In addition, wir randomisieren sampling 20 Aufgaben aus dem Testsplit von NaturalInstruction als unsichere Aufgabenset für NLPA. Also verwenden wir einen übertrainten OFA-Large-Modell als Basismodell. During Training, wir mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test, for each task, wir conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. Wir report the min and max performance and the standard deviation of the performance across all five experiments. If the task is a multi-modal classification task, wir report accuracy. If it's a multi-modal generation task, wir report ROUGE. For NLPA task, wir report ROUGE as well. We also introduced an additional evaluation metric called sensitivity. This measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in the wording of the instruction. Here are our main results. As we can see, instruction tuning can significantly improve OAF's performance on seeing multi-modal tasks. Also transfer learning from natural instruction datasets can benefit instruction tuning. Here we can see as the amount of task increases, the model achieves better performance and in the meantime lower sensitivity. So we also do one experiment. We use one instruction versus five instruction. As we can see, using more instruction can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategy on the model's sensitivity. As we can see, by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction datasets can help OFA to achieve much better performance on the natural instruction dataset. So overall, we propose the first large-scale multi-modal instruction tuning dataset which significantly improves the zero-shot capability of OFA and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional visual language tasks and we will release them soon. This is the QR code for our data and model. Thank you.</sample>
    <sample id="152">The presentation introduces new language models specifically designed for classical philology, including monolingual and multilingual models. The researchers have pre-trained two monolingual models for ancient Greek, GRBERTa and GATER, and developed multilingual models, PhilBERTa and PhilTER, pre-trained on ancient Greek, Latin, and English data. They have also created a high-quality pre-training dataset from the Internet Archive. The models outperform current state-of-the-art models in tasks such as speech tagging, dependency parsing, and lemmatization. The analysis shows that the performance of the T5 encoder is significantly better when used separately for tasks that do not require a decoder. Overall, the presentation highlights the potential of these models for processing Latin and Greek texts using the same model.</sample>
    <sample id="153">The presentation introduces a study on resolving ambiguities in text-to-image generative models. The researchers, led by Ninarah Mehrabi from Amazon Alexa AI's Responsible AI team, focus on identifying and addressing ambiguities in prompts given to these models. They propose frameworks to mitigate these ambiguities and evaluate the faithfulness of generated images to user intentions. The study involves curating a benchmark dataset with various types of ambiguities, using language models to generate clarifying questions or visual setups, and evaluating the results with a VQA model. The findings show that their framework effectively reduces ambiguity and aligns with human evaluations, providing reliable methods for assessing text-to-image models.</sample>
    <sample id="154">Die Autoren Sara Papi, Matteo Negri und Marco Turchi gehören an der Università di Trento und Fondazione Bruno Kessler.</sample>
    <sample id="155">Bob</sample>
    <sample id="157">The presentation introduces a dialogue summarization method developed by researchers from Sandong University and other institutions. The method aims to distill silent information from a dialogue context into a concise summary, which is a challenging and interesting task in the field of text summarization. The method involves encoding utterances in the dialogue context into vector representations using an utterance encoder, constructing a static graph using existing dialogue structure modeling methods, and then proposing a static dynamic graph module to capture semantic relationships between utterances based on their deep vector representation. Finally, a pre-trained language model is employed as the summary generator to fuse the static dialogue structure and the dynamic learned dialogue structure into the final summary. The presentation also shows the detailed model structure of the proposed SDG model and explains how it captures static dialogue structure information using a heuristic dialogue structure modeling method that builds the relationship between utterances using a graph network.</sample>
    <sample id="158">The speaker introduces a dual cache method for long document neural coreference resolution. The task involves identifying and clustering mentions that refer to the same entity in a document with multiple mentions of entities. Conventional methods have quadratic complexity, while cache-based methods reduce it to linear. However, in long documents, the topic may switch multiple times, causing high cache misses. The proposed dual cache uses a local cache for recent entities and a global cache for frequently used entities, reducing cache misses and improving performance. The model is evaluated on four public benchmarks, showing better performance than baselines even without training data. The dual cache significantly reduces cache misses and is more cost-effective compared to single cache methods.</sample>
    <sample id="159">Hallo, ich bin Kostya Sinha und ich freue mich, Sie willkommen in unserem Vortrag auf unserem ACL 2023 Papier "Language model acceptability judgments are not always robust to context". Das ist ein gemeinsames Werk mit John Gauthier, Aaron Mueller, Kanishka Mishra, Karen Fuentes, Roger Levy und Adina Williams. In diesem Werk revisitieren wir die Minimal Pair Paradigm. Das Minimal Pair Paradigm bewertet Sprachmodelle auf Basis von akzeptierbarkeitsschwergewichten, die auchGrammarkraft wie "blimp" oder "Syntax Gym" oder Akzeptierbarkeit in Bezug auf Stereotypen wie "Crowd Speres" umfassen können. In diesem Minimal Pair Paradigm wird das typische Weg zu bewerten, indem man einem akzeptablen Satz oder einem grammatischen Satz einen unakzeptablen Satz oder einen ungrammatischen Satz zeigt. Und dann hofft man, dass das Modell mehr Wahrscheinlichkeit beim akzeptablen Satz einordnet. Der aktuelle MPP-Pipeline bietet nicht die Möglichkeit, die Akzeptierbarkeit von Modellen für längere Sätze zu bewerten. Heute werden große Sprachmodelle mit immer langeren Kontextfenstern entwickelt. Es ist also sehr wichtig, die Akzeptierbarkeit von Modellen über den Kontextwindow zu bewerten. Und das ist, was wir versuchen zu tun. Wir versuchen, die MPP-Pipeline zu revisitieren, indem wir die Modell zu bewerten, indem sie die Akzeptierbarkeit auf langeren und langeren Sequenzen bewertet. Also, das ist der Ansatz. Was wir tun, ist, diese langeren Sequenzen zu simulieren. Wir revisitieren die Datensätze selbst und dann recreaten Sentenzen, indem wir akzeptablen oder unakzeptablen Sätzen aus jenem Datensatz auswählen. Zum Beispiel haben wir hier eine typische Paar Grammatik von "blimp" Datensatz aus dem Adjunct Island Fall gewählt. Und was wir tun, ist, langeren Sequenzen zu recreaten, die akzeptabel sind und die gleiche Grammatikstruktur haben. Wir extrahieren Grammatik Sätze aus Adjunct Island und fügen sie als Prefix zu beiden akzeptablen Query und unakzeptablen Query hinzu. Wir können das gleiche machen, indem wir Sätze aus einem anderen Datensatz oder einer verschiedenen Gruppe auswählen. So nennen wir das Missmatch-Szenario. Hier sind die Sätze immer noch aus relevanten Datensätzen, aber sie sind nicht aus dem Datensatz, den wir bewerten. Und wir können das gleiche für unakzeptierbarkeitsschwergewichten tun. Schließlich können wir Sätze aus einem vollständig unverwandten Feld wie Wikipedia auswählen. Das wird uns sagen, ob die Modell-Akzeptierbarkeitsschwergewichten tatsächlich beeinflusst werden, ob der Kontext kommt aus einem anderen Datensatz oder ob es wie kompletely irrelevant zu dem aktuellen Satz ist, den wir betrachten. Wie性能模型？首先， wir schauen uns Wikipedia-Sätze an, die completely irrelevant zu dem aktuellen Query-Paar sind. Und dann finden wir, dass die MPP-Schwergewichten für arbiträre Kontextlängen robust sind. Wir können die Kontextlängte bis zu 1244 for up max out OTP and GP T2-Modelle und wir sahen hier in der orange Linie die MPP-Schwergewichten relativ stabil. Nun, was passiert, wenn wir Sätze aus demselben Datensatz auswarten? Hier warten wir auf eine Erstellung von Sätzen aus akzeptablen und unakzeptablen Domänen aus dem gleichen "blimp" oder "Syntax Gym" Datensatz und dann sehen wir, dass die MPP-Schwergewichten entweder stark steigen oder abfallen, wenn Sie einen akzeptablen oder unakzeptablen Prefix hinzufügen. Aber wenn wir die Struktur匹配, das ist, wenn wir Sätze aus dem gleichen Phänomeno in "blimp" oder "Syntax Gym" auswarten, sehen wir einen massiven Steigen oder einen massiven Abfall in den MPP-Schwergewichten für das Modell, je nachdem, ob der gewählte Prefix akzeptabel oder unakzeptabel ist. Nun, und das ist sehr großartig. Wie diese Effekt steigt durch die Kontextlängte und das würde wahrscheinlich die Newer Language Models, die ein langes Kontext-Fenster haben, beeinflussen. Warum beeinflusst der match Prefix die Sprachmodell-Schwergewichten so stark? Wir haben eine Reihe von Analysen durchgeführt, in der wir versucht haben, die Eingabe-Sentence zu perturbieren, indem wir versucht haben, die relevanten Strukturen zu erhalten, aber dennoch den Input zu verzerren. Und nachdem wir mehrere dieser Perturbationsvorgänge durchgegangen sind, finden wir, dass keines dieser Verstörungen die Modellursache in Bezug auf die Schwergewichten-Trend verändert. Basically, wir finden, dass die Modelle sensible zu perturbten Sätzen in ähnlicher Weise sind. Das bedeutet, dass wir perturbte Sätzen im akzeptablen Bereich sehen, dass sie in allen Perturbationsvorgängen eine ähnliche Steigerung sehen und wenn wir perturbte Sätzen im unakzeptablen Bereich auswarten, sehen wir eine Decrease in MPP-Schwergewichten in ähnlicher Weise. Also die Hauptergebnisse von unserem Werk sind, dass Sprachmodelle sensible zu latenten syntagmatischen und semantischen Merkmalen, die sich über die Sätzen teilen, und die MPP-Bewertung, die Art, wie wir sie momentan mit kurzen und einfachen Eingabe-Sätzen machen, mag nicht vollständig das abstrakte Wissen der Sprachmodelle über den Kontextwindow abdecken. Lesen Sie bitte unser Papier für weitere Details über unsere Experimente. Vielen Dank für das Hören.</sample>
    <sample id="160">In einem Token-Cluster.</sample>
    <sample id="161">In Coscript sind 55.000 Skripte vertreten.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEplain ist die Methode von Massalign.</sample>
    <sample id="164">Schwach überwachtes Lernen ist viel billiger, aber auch lauter.</sample>
    <sample id="165">The speaker introduces a new unsupervised learning method called LIPOR (Likelihood Learning with Posterior Regularization) for adaptive reasoning. This method treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context by marginalizing over all possible explanations. To prefer plausible explanations, LIPOR uses a regularizer that enforces mutual exclusivity among explanations. The speaker compares LIPOR to zero-shot models and the previous best unsupervised approach on the AlphaNLI dataset, showing that LIPOR outperforms all of them, including a strong zero-shot GPT-3 baseline, by over four absolute points in accuracy.</sample>
    <sample id="166">This paper presents a neural divide-and-conquer reasoning framework for image retrieval from linguistically complex text. The proposed method integrates the advantages of symbolic reasoning and neural network models to address the challenge of retrieving images from complex text. The framework consists of two main components: a complex proposition generator that generates symbolic propositions, and a neural symbolic reasoner that integrates the reasoning states and results of these propositions to obtain the final solution. Experimental results demonstrate the effectiveness of the proposed method in comparison to baseline methods and other ablation experiments.</sample>
    <sample id="167">Die Dokumente in DEplain-web wurden auf der einen Seite manuell und auf der anderen Seite mit automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde von Reuters News aus dem Jahr 2020 erstellt und mit den gleichen Annotieranweisungen wie CoNLL-2003 annotiert.</sample>
    <sample id="169">The paper presents a systematic study of prompting strategies for large language models (LLMs) in machine translation. It evaluates the translation capabilities of LLMs using best practices from the NMT community, comparing state-of-the-art systems and showing expert-based human evaluation results. The study concludes that example quality is more important than similarity to the source sentence, and that specialized state-of-the-art systems have an advantage over standard translations. The paper also highlights the importance of selecting high-quality examples for prompting, as demonstrated by experiments with different prompting strategies.</sample>
    <sample id="170">Hallo, jeder. Mein Name ist Justin Jung vom Peking University. Heute werde ich über unser Werk sprechen: "Exemplar: Korsinglog Semantik Parsen in mehreren natürlichen Sprachen und vielen Repräsentationen." Semantik Parsen ist die Aufgabe, semantische Repräsentationen von Benutzereingaben zu erstellen, wie SQL und Lambda Calculus. Korsinglog Semantik Parsen ist die Aufgabe, Eingaben in mehreren natürlichen Sprachen in mehrere Repräsentationen zu übersetzen. Wie im Bild gezeigt, müssen wir die Eingabe in mehrere natürliche Sprachen mit neuronalen Modellen in SQL, Lambda oder FunkQL und so weiter übersetzen. Gegenwärtig werden korsinglog Semantik Parsen-Modelle separat vorgeschaffen und evaluiert auf Datensätzen von limitierten Aufgaben und Anwendungen. Zum Beispiel gibt es Lücken in der Abdeckung auf bestimmte natürliche Sprachen, wie Chinesisch, und Lücken in der Abdeckung auf bestimmte Repräsentationen, wie Lambda Calculus, die fehlt. Oder sie werden nur auf bestimmte neuronalen Modelle evaluiert, z.B. nur ein einzelner Modell zur Evaluation. Um dies zu beheben, schaffen wir Exemplar, um einen uniformen Datensatz für korsinglog Semantik Parsen in mehreren natürlichen Sprachen und vielen Repräsentationen zu bieten. Er enthält 9 Datensätze in verschiedenen Domänen, 5 Semantik Parsen Aufgaben, 8 Repräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unser Benchmark besser zu evaluieren, betrachten wir sechs Szenarien für Training und Evaluation. Der erste ist "Translate Test": Wir verwenden die Google-Übersetzungs-API, um die Quelle in die Zielsprache zu übersetzen, dann verwenden wir einen monolingualen Modell zur Training- und Evaluation. Zum Beispiel trainieren wir ein englisches Modell auf einem englischen Query und während der Inferenz übersetzen wir einen deutschen Query mit API in englisch und verwenden das trainierte Modell, um den SQL zu predictieren. Wir testen auch monolingual-Modell in diesem Setting, bei dem die Quellsprache dieselbe ist wie die Zielsprache, z.B. Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch monolingual-Few-Shot-Setting, bei dem wir Modell-Modelle mit nur 10% des Trainingsdatensatzes trainieren. Wir testen auch monolingual-Multilinguale-Model, bei dem wir einen Multilingual-Modell für alle Sprachen trainieren, z.B. wir vereinen deutsche, englische, chinesische Abfragen, um ein Multilingual-Modell zu trainieren und während der Inferenz können wir dieses Modell verwenden, um ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...</sample>
    <sample id="171">Bestehende Arbeiten haben gezeigt, dass ein Angreifer durch das Lernen von Einbettungen und die Bereitstellung von ähnlichen Diensten den Modellzugriff stehlen kann.</sample>
    <sample id="172">Nein, Codex und Bloom sind noch nicht ausreichend für CLSP-Tasks.</sample>
    <sample id="174">The ArgAnalysis35K dataset is a large-scale dataset for argument quality analysis, featuring 35,000 argument analysis pairs sourced from high-quality tournaments, expert debaters, intermediate debaters, and novice debaters. It includes a diverse range of arguments on various themes, with each theme having as many motions as possible captured from the same sources. The dataset also includes an element of analysis, which combines claims, premises, and other elements to explain arguments better. Additionally, it introduces instance-based annotator reliability, relevance modeling, and more reliable scoring by capturing annotations at an instance level.</sample>
    <sample id="175">Die Methode mit der Mehrdeutigkeit der Permutationen wird durch die Induktion als Teil des Trainings adressiert.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird definiert, indem die Leitstelle die Leitstelle des Modells auf eine Gruppe von Menschen mit unterschiedlichen politischen Ansichten und sozialen Gruppen bewertet.</sample>
    <sample id="177">The presenter's name is Yanis Labrak.</sample>
    <sample id="178">The speaker's name is Koustuv Sinha.</sample>
    <sample id="179">This paper presents a method called Symbolic Tom to improve theory of mind reasoning skills in large language models using explicit graphical representations. The method leverages an inference-time algorithm that computes graphical representations of characters' mental states, allowing for efficient answering of questions. Experiments show that Symbolic Tom significantly improves out-of-box language model performance on theory of mind tasks and generalizes well to new datasets with increased linguistic diversity.</sample>
    <sample id="180">Myra Cheng</sample>
    <sample id="181">This paper introduces a method for generating scripts from large language models (LLMs) to address the problem of constrained language planning, where specific goals with multiple constraints need to be achieved. The authors evaluate and improve the constrained language planning ability of LLMs by extending abstract goals with specific constraints using a dataset acquisition tool called InstructGPT. They then develop an over-generated script filter to enhance the quality of generated scripts, ensuring they are both semantically complete and faithful to the constraints. The paper also proposes a symbolic knowledge distillation method to create a high-quality constrained language planning dataset named CoScript, which can be used to train smaller and specialized models for constrained language planning tasks.</sample>
    <sample id="182">Tropikalismus bezieht sich auf die Tendenz, bestimmte Gruppen von Menschen als "tropisch" zu sehen, was oft als diskriminierend gilt.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen erstellt, indem sie Menschen mit den gleichen Anweisungen anhörten, die sie auch für die generierten Personen benutzt hatten.</sample>
    <sample id="184">In dieser Arbeit wurde CXMI verwendet, um die Kontextnutzung zu messen.</sample>
    <sample id="185">DrBERT und ChuBERT sind beide Medizin-basierte Modelle, die auf der Roberta-Plattform trainiert wurden. DrBERT ist ein robustes Modell in Französisch für Bio-medizinische und klinische Domänen, während ChuBERT auf anonymisierten Daten aus dem Non-Universität Hospital Data Warehouse basiert.</sample>
    <sample id="187">Drei</sample>
    <sample id="188">Iteratives Transferlernen ist ein Prozess, bei dem ein Modell auf einem Datensatz trainiert wird und dann anhand von neuen Datensätzen iterativ weitertrainiert wird.</sample>
    <sample id="189">Das Ziel des Datensatzes ist, die Verarbeitung natürlicher Sprache zu verbessern.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen EaaS extrahieren, indem er die Bedienung von einem Service mit einer Watermark-Methode und dann die Watermark-Methode entfernt.</sample>
    <sample id="191">Drei</sample>
    <sample id="192">The presentation introduces a new optimizer called CAN, which aims to achieve fast convergence like traditional adaptive methods and low memory usage like memory-efficient methods. It uses non-negative matrix factorization (NMF) to reduce memory requirements and proposes an adaptive confidence-based updating strategy to handle the instability in the training process. The presenter compares CAN with existing optimizers on three large language models and demonstrates its superior performance and efficiency in terms of memory usage.</sample>
    <sample id="193">Um den ursprünglichen Datensatz zu erstellen, wurden 43 Annotatoren verwendet.</sample>
    <sample id="194">Die Autoren sind an Carnegie Mellon University und der University of Washington.</sample>
    <sample id="195">The presented work focuses on developing a framework for question answering that incorporates hierarchical question decomposition and probabilistic reasoning. The framework, called ROHT, consists of two stages: first, it constructs a hierarchical question decomposition tree (HQDT) to understand the compositional structure of complex questions, and second, it performs probabilistic reasoning over the HQDT to integrate knowledge from various sources and generate answers. The framework is evaluated on challenging datasets, demonstrating its effectiveness in handling incomplete knowledge bases and leveraging supplementary text corpora to improve performance.</sample>
    <sample id="196">Ein Beispiel mit einem Begrenzer auf der linken Seite lautet "Lisa bought and Meggy."</sample>
    <sample id="197">Die Evaluierung der Qualität von Dialogsystemen basiert normalerweise auf menschlicher Bewertung, indem man humanen Richtern fragt, welche der zwei Konversationen besser ist oder sie auf einer Likert-Skala bewertet.</sample>
    <sample id="198">Die Akzeptanz der Modelle muss über das gesamte Kontextfenster bewertet werden, weil große Sprachmodelle heutzutage mit einem längeren Kontextfenster arbeiten und es wichtig ist zu bewerten, ob die Modelle die Akzeptanz für längere Sätze korrekt einschätzen können.</sample>
    <sample id="199">Ja, das mehrsprachige Training hat in ein paar Datensätzen zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt.</sample>
    <sample id="200">Ja, die Annotatoren kennen die Entität im Voraus.</sample>
    <sample id="201">Die MT-Metriken, die verwendet wurden, um die Leistung zu bewerten, sind standardmäßig und auch Experten-basiert.</sample>
    <sample id="202">Ja, die Regression beeinträchtigt bestimmte NER-Typen.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie die Systematische Leistungsunterschiede von Technologien zwischen Bevölkerungen und die Aggregation von Urteilen und Meinungen von Menschen in Datensätzen und Modellen berücksichtigen kann.</sample>
    <sample id="204">LLMs wurden durch Adapter angepasst.</sample>
    <sample id="205">The speaker presents a study on the political biases in language models and their impact on downstream tasks. The study evaluates the political leanings of language models, investigates how these biases are picked up from training data, and assesses the fairness issues in NLP applications like hate speech and fake news detection. The results show that language models have varying political leanings, with GPT-4 being the most liberal. The study also finds that language models can pick up polarization from training data and that left-leaning models are better at detecting hate speech targeting minority groups, while right-leaning models are better at detecting hate speech targeting white and men. The speaker highlights the need to acknowledge and tackle the fairness issues resulting from language model political leanings and the dilemma of sanitizing political opinions in training data without risking censorship or exclusion.</sample>
    <sample id="206">Das Modell, das verwendet wird, ist C-E.</sample>
    <sample id="207">Die besten übersetzten Systeme wurden verwendet, um die PaLM-Fähigkeiten zu bewerten.</sample>
    <sample id="208">Die Autoren haben schließlich drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">Die vorgeschlagene Methode bringt einen Gewinn von 15,7% gegenüber dem stärksten Baseline.</sample>
    <sample id="210">Shuheng Liu</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz können als Benchmark verwendet werden.</sample>
    <sample id="212">In der Arbeit werden insgesamt 5 kleine Modelle experimentiert.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">The speaker presents a work titled "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization." The work is authored by Linlin Jiang and focuses on developing a two-step pipeline for identifying high-agreement Amazon Mechanical Turk workers. The motivation behind the pipeline is to address the challenges posed by automatic metrics and the lack of understanding of best practices for recruitment on MTurk. The paper includes sections on qualification settings, qualification tasks, endurance tasks, reference-based tasks, and an analysis of the baseline and cloud research MTurk workers. Additionally, it discusses the analysis of cracks across annotation sources and concludes with limitations and future directions.</sample>
    <sample id="217">The paper introduces a method for generating controllable dialogues with multiple attributes using a disentangled control generation (DCG) approach. The authors propose a dialogue GBT framework with a compositional prompt module and design two types of prompts: attribute-oriented and task-oriented, to guide the model's focus on specific information in the dialogue. They also introduce a unified reference-free evaluation framework (MAE) that does not require large-scale labeled data. Experiments show that their method outperforms other baselines in controllability and test quality, demonstrating the effectiveness of their approach in transforming seen attributes to unseen combinations.</sample>
    <sample id="218">Die Autoren sind an der Google Translate-Gruppe arbeiten.</sample>
    <sample id="219">The speaker introduces a research paper on a multi-stage pipeline for uncovering financial signals in financial reports. The paper, co-authored with several researchers, focuses on the task of highlighting important information in annual reports required by the SEC. The authors propose a highlighting task and a multi-stage pipeline to compare and contrast the context between target and reference reports. They define three types of pairs: type A, type B, and type C, and use an external dataset for model fine-tuning. The proposed method achieves the best performance on the final dataset and demonstrates generalization capability. The paper also discusses future work directions, such as improving effectiveness, adding more features, and enhancing the application of information retrieval techniques.</sample>
    <sample id="220">Die Autoren gehören an der Stony Brook University.</sample>
    <sample id="221">Die Arbeit untersuchte Sprachpaare, die von Google Translate verwendet wurden.</sample>
    <sample id="222">This work investigates the challenges and interventions in open-domain question answering (ODQA) by exploring different data interventions to enable out-of-domain generalization. The researchers identify three main contributions: investigating data interventions, identifying the type of dataset shift a new domain exhibits, and determining which data interventions are effective for specific types of shifts. They use a general-purpose Wikipedia-based source domain and test generalizability on seven target passage datasets spanning six different domains. The results show that few-shot methods improve retriever performance by 8% and reader performance by 11%, while zero-shot techniques do not significantly affect model performance. The study also explores the impact of format, answer distribution, and context distribution on model learning. Additionally, they assess the nature of incompatibility between the target model and domain exhibits using existing dataset shift taxonomy in machine learning. The findings indicate that few-shot adaptations are effective for most target sets, while zero-shot adaptations are more useful for datasets with concept and covariate shifts. Overall, the research provides insights into improving ODQA systems by adapting to different target domains.</sample>
    <sample id="223">The speaker is referred to as "I" in the audio.</sample>
    <sample id="224">Während der Experimente wurden zwei Modelle untersucht: ein Modell, das die Long Impart-Technik verwendet wurde, um Dokumentebasierungen zu produzieren, und ein Modell, das die Normal Base Long Impart-Technik verwendet wurde, um Sateliersimplifizierungen zu produzieren.</sample>
    <sample id="225">Fünfundsiebzig Aufgaben werden für die Trainingssample verwendet, und 10 Aufgaben werden für die Testsample verwendet.</sample>
    <sample id="226">Zwei Autoren arbeiten an der Arbeit beteiligt.</sample>
    <sample id="227">The presentation discusses the challenges in current language models research and proposes a new framework for grounded language understanding. The main issue is the lack of grounding during pre-training, which makes it difficult to map natural language expressions into executable plans or programs. Existing research typically uses language models to generate plans directly, but this can result in invalid or non-executable plans. The proposed framework involves a symbolic agent that interacts with the environment and proposes candidate plans, while the language model only scores and ranks these candidates. This approach leverages the strength of language models in discrimination rather than generation. The framework is tested on various language models and settings, demonstrating strong performance and sample efficiency. The results suggest that discrimination might be a better strategy for grounded language understanding than generation.</sample>
    <sample id="228">Die Autoren haben experimentiert an den Datensätzen AG News, 20 Newsgroups, SST-2 und Yahoo! Answers.</sample>
    <sample id="229">The presentation introduces the importance of text revision in professional writing and its role in argumentative writing. It explains how revisions can improve the clarity and effectiveness of a message, using the example of revising the claim "Cell phones cause brain cancer." The paper focuses on two tasks: sub-optimal claim detection and claim improvement suggestion. The authors explore the challenges of working with revision-based data, including representativeness, model complexity, contextual information, and topical and user bias. They present strategies for addressing these challenges and conclude that revision-based data can be effectively used for detecting sub-optimal claims and improving argument quality.</sample>
    <sample id="231">NACHOS ist ein Datensatz von medizinischen Crowd-Daten, der von der Web-Quelle abgeleitet wurde.</sample>
    <sample id="232">The name of the presenter is Avi Beller.</sample>
    <sample id="233">This paper introduces a novel strategy for simultaneous speech translation using encoder-decoder attention. The proposed method, called ADT (Attention-based Decoding Transformer), utilizes pre-trained offline models without retraining or specific architecture adjustments. It determines whether to emit partial translations based on the cross-attention weights between audio input and textual output. The strategy aims to balance translation quality with latency and computational efficiency. Experimental results demonstrate that ADT outperforms other strategies in terms of translation quality and speed, making it a promising approach for real-time speech translation systems.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Leistung der LLMs für Übersetzungen.</sample>
    <sample id="235">Die Autoren gehören an der Carnegie Mellon University.</sample>
    <sample id="236">Jede Aufgabe im Datensatz wird mit 5 verschiedenen Anweisungen versehen.</sample>
    <sample id="237">Die Autoren schlagen einen Diagnostik-Test-Satz vor, um Modelle zur Nutzung von Informationen aus mehreren Quellen zu testen. Sie Introduzieren einen Korreferenz-Resolution-Task, der darauf abzielt, die Fähigkeit zu bewerten, auf Informationen aus verschiedenen Quellen zuzugreifen. Sie evaluieren den Datensatz mit menschlichen Studieparticipanten und etablieren Korreferenz-Resolution-Modelle.</sample>
    <sample id="238">The video presents a new benchmark dataset called MeetingBank, which is designed to address the challenges of high-quality meeting summaries and the difficulty in locating trustworthy resources for public meetings. The dataset includes meeting transcripts, reference summaries, and URLs containing useful resources. The data collection process involves using speech-to-text APIs to convert audio data to transcripts, identifying meeting types and dates from meeting websites, and aligning time stamps to get segment transcripts. The dataset contains 1366 city council meetings with nearly 7000 instances, providing detailed statistics on meetings, duration, token count, speaker frequency, and year period. The dataset also includes summary instances for each city and average statistics for both meeting level and segment level. The evaluation metrics used include coverage score, density score, ROUGE-2 score, and human evaluation based on informativeness, factuality, fluency, coherence, and redundancy. The results show that GPT-3 performs exceptionally well in terms of fluency and coherence but less impressively in informativeness and factuality. The video concludes by encouraging viewers to use and download the resource for further discussion.</sample>
    <sample id="239">Hallo, ich bin David Villegas und ich werde einen kurzen Überblick über das Papier "Prompting PaLM for Translation: Assessing Strategies and Performance" geben. Dieses Papier wurde in collaboration mit Kollegen von Google Translate erstellt. PaLM ist ein 540 Milliarden Parameter langer Sprachmodell, das im Jahr 2022 vorgestellt wurde. Es wurde auf einem großen Datensatz von 780 Milliarden Token trainiert. In der Präsentation bewertet das Papier die Übersetzungsfähigkeit solcher Modelle unter Verwendung der besten übersichtlichen Ansätze. Um zu vermeiden, dass das Testdata mit dem Trainingdaten des Modells in Konflikt steht, wurden die neuesten Testsuites verwendet. Wir vergleichen zwei state-of-the-art-Systeme, um die besten performierenden Systeme zu identifizieren. Wir verwenden state-of-the-art-und neuartige MT-Metriken und zeigen auch Expertenbasierte menschliche Evaluationsergebnisse an. Schließlich bieten wir einige Empfehlungen für Promptselektionsstrategien an. Das Prompting hat einen großen Einfluss auf die Leistung der LLMs für Übersetzungen. In einem einfachen Experiment mit einem "one-shot"-Prompting und zwei verschiedenen Prompts für eine bestimmte Anzahl von Sätzen haben wir festgestellt, dass die Mehrheit der Sätze (516 von 1000) einen Differenzabsatz von mehr als einem BLEU-Punkt aufwirft. In extremen Fällen kann dies bis zu 40 BLEU-Punkte betragen. Es ist daher wichtig, einen guten Prompting-Strategie zu auswählen. In unserem Experiment haben wir einen "five-shot"-Prompting-Strategie verwendet, bei der wir jede Anzahl von Sätzen mit dem gleichen Sprachcode markieren, den wir dem System zur Verfügung stellen. Wir haben festgestellt, dass die Form der Prompting-Beispiele in diesem Fall keine große Bedeutung hat, es sei denn es handelt sich um "zero-shot"-Prompting. In unserem Fall mit "five-shot"-Prompting macht die Form der Beispiele relativ wenig Unterschied. Es ist jedoch wichtig, die Qualität der Beispiele zu berücksichtigen, da dies den Hauptanteil des Gewichts trägt. Der Hauptunterschied zwischen den experimentalen Ergebnissen liegt in der Beispielqualität gegenüber der Ähnlichkeit an der Quelle. Es ist daher wichtig, Beispiele von hohen Qualität zu verwenden. Insbesondere haben wir festgestellt, dass die Auswahlprompts aus dem Trainingsdatensatz der TUM-Evaluation oder die Dev-Daten, die viel besser qualitativ sind, bessere Leistungen erzielen. Trotzdem haben spezialisierte state-of-the-art-Systeme einen erheblichen Vorteil über PaLM-Übersetzungen. Aber PaLM kommtPretty nahe an einem kommerziellen System heran. In unserem Fall haben wir Google Translate verwendet. Die Erkenntnisse, die wir aus dem Evaluationsprotokoll gewonnen haben, die wir mit dem MQM-Framework durchgegangen sind, bestehen darin, dass die Fluideität von PaLM vergleichbar ist mit state-of-the-art-Systemen, aber der Hauptunterschied kommt von der Genauigkeit. Insbesondere die häufigsten Fehler sind Omissionen. Es scheint, dass PaLM Entscheidungen trifft, um ein besseres klingendes Übersetzung zu produzieren, indem es Teile der Quellsäulen weist, die im Übersetzung nicht notwendig sind. Allerdings ist die Stilqualität von PaLM kleiner als die von state-of-the-art-Systemen, was ein weiterer Hinweis darauf ist, dass PaLM ein fluider Output liefert, aber immer noch einige Probleme mit Genauigkeit hat. Und das ist alles für diese kurze Übersicht.Für weitere Details bitte mit mir zum vollständigen Vortrag des Papiers kommen. Vielen Dank.</sample>
    <sample id="240">Hi, ich bin Cui Yuan von Fudan University. Ich bin hier, um unsere Arbeit zu präsentieren: "Distinguishing Script Knowledge from Large Language Models for Constraint Language Planning". In unserem Alltag planzen wir unsere Handeln oft nach Schritt-für-Schritt-Anweisungen in Form von Garantien. Vorherige Arbeiten haben gezeigt, dass große Sprachmodelle in der Lage sind, abstrakte Ziele von stereotypischen Aktivitäten, wie zum Beispiel das Brotbacken, zu planen und zu zerlegen. Es wurde gezeigt, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Allerdings haben vorherige Arbeiten hauptsächlich auf die Planung abstrakten Ziele von stereotypischen Aktivitäten geinsponnen. Die Planung von Ziele mit spezifischen Zielspezifikationen, wie zum Beispiel das Backen eines Schokoladenkuchens, ist immer noch ununtersucht. In diesem Papier definieren wir das Problem der einschränkenden Sprachplanung, bei dem verschiedene Einschränkungen auf die Planung angewendet werden. Ein abstraktes Ziel kann durch verschiedene real-life-spezifische Ziele mit mehrfachen Einschränkungen erweitert werden. Ein guter Planer sollte Skripte schaffen, die rational und denkbare sind und den Einschränkungen angepasst sind. In diesem Papier erstellen wir zuerst eine Evaluation und Verbesserung der einschränkenden Sprachplanungsfähigkeit von großen Sprachmodellen. Da keine Datensammlung von spezifischen Zielen existiert, um unser Studie zu unterstützen, müssen wir zuerst die Ziele sammeln. Im Tabellentitel sehen Sie, wie wir abstrakte Ziele mit mehrfachen Einschränkungen für Menschen in der Lern-Datenabholphase mit einer Anweisung CPT erweitern. Wir sampling 100 spezifische Ziele und evaluieren die generierten Skripte von großen Sprachmodellen. Das Tabellentitel zeigt die allgemeine Genauigkeit der Resultate an. Wir finden, dass alle großen Sprachmodelle unzufriedenstellende Resultate auf die Planung von spezifischen Zielen erhalten. Dann conducten wir eine detaillierte Analyse, um zu überprüfen, warum große Sprachmodelle solche Resultate erhalten. Der Graph zeigt, dass die semantische Komplettät der generierten Skripte akzeptabel ist, aber die Einhaltung der Einschränkungen nicht gewährleistet werden kann. Wir dichten die verschiedenen Topologiekategorien der Einschränkungen in einem Wörterbuch ein. Das Heatmap im Graphen zeigt, dass die Planungsleistung von Anweisungen CPT für Ziele aus verschiedenen Kategorien variabel ist. Vorherige Studien haben gezeigt, dass die Ausgabequalität von großen Sprachmodellen in einem hohen Varianz liegt, was zu schlechter Leistung führt. Daher adoptieren wir die Idee des "Over-Generativen Filter" zu verbessern die Ausgabekonfiguration. Wir fassen... Aber das ist das Ende der Geschichte, denn wenn wir entweder eine klare Sammlung von Daten entscheiden, dann wird die direkte Fine-Tuning-Methode besser als WSL-Methoden erreichen. Das rechtsfigur zeigt die Leistungsverschiedenheit zwischen Direkt-Finetuning-Methoden, die direkt auf die klare Datensammlung angewendet werden, und WSL-Methoden, die die klare Datensammlung nur zur Validierung verwenden. Wie wir sehen, wenn wir 10 Proben pro Klasse haben, beginnt Direkt-Finetuning zu übertragen WSL-Methoden. Schließlich können die Leistungsverbesserungen, die in vorherigen WSL-Methoden geltend sind, leicht erreicht werden, indem wir es erlauben, continuierliche Fine-Tuning auf die klare Datensammlung fortzusetzen. Wie wir aus den Graphen sehen, der Modellnamen FTW ursprünglich unterperformiert gegenüber komplexeren WSL-Methoden wie cosine. Wenn wir es erlauben, continuierliche Fine-Tuning auf die klare Datensammlung fortzusetzen, dann performt FTW ebenso gut wie andere Methoden. Also in der Praxis gibt es keinen Grund, komplexere WSL-Methoden zu verwenden, die mehr Rechenzeit und Speicherplatz benötigen. Um zu summieren, wir zeigen, dass recent WSL-Methoden eine klare, manuell annotierte Datensammlung benötigen, um korrekt zu arbeiten. Ihre Leistungsverbesserungen und Praktizität werden stark überbewertet. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind als Folge: Erstens berichten Sie die Modellauswahlkriterien, z.B. ob die Modellauswahl auf klare Validationsproben basiert. Zweitens sollten WSL-Methoden mit kurzen Baselines verglichen werden, die auf klare Datensammlungen arbeiten. Drittens ist continuierliche Fine-Tuning ein einfach aber starres Baseline, das in zukünftiger Arbeit in WSL Berücksichtigt werden sollte. Endlich haben wir Open-Source-Code auf GitHub bereitgestellt. Sie können den Code unter dem QR-Code auf dieser Folie finden. Bitte fühlen Sie sich frei, den Code zu überprüfen. Vielen Dank für das Event!</sample>
    <sample id="241">The speaker, Ethan, introduces a paper titled "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments." This collaborative work with Yang Chen, Wei Xu, and Alan Ritter at Georgia Tech aims to address the limitations of current misinformation detection systems. These systems often fail to be realistic in evaluation and are not human-centric. The proposed framework involves an end-to-end process where humans are involved at various stages, ensuring well-integrated human feedback and making the system assistive rather than authoritative. The workflow is implemented and evaluated for COVID-19 treatment misinformation, focusing on two main components: claim detection and policy violation verification. The evaluation highlights the importance of early detection and the efficiency of the policy violation verification process, demonstrating that the framework can effectively capture the complex interplay between systems and human content moderators.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind die Anwendung von Human-Jugend und Likert-Skalen.</sample>
    <sample id="243">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="244">In dem Beispiel mit Servin und Kea wird benötigt, dass Servin ein Richter ist und dass Richter in Gerichtsgebäuden arbeiten.</sample>
    <sample id="245">The presentation introduces a study on high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The researchers developed a two-step pipeline to identify these workers, addressing the limitations of automatic metrics and unclear best practices in MTurk recruitment. The pipeline includes a qualification task and an endurance task, with results showing 26 qualified workers, 8 gold and 18 silver, out of 200 participants. The study also examines the performance of reference-based tasks and baseline MTurk workers, highlighting the importance of pre-task filtering to achieve high agreement at lower costs. The researchers aim to improve worker quality and explore multiple applications across languages and platforms, despite limitations such as only testing English summarization on MTurk and not guaranteeing training of cracks.</sample>
    <sample id="246">Ja, der Code ist verfügbar und er ist auf GitHub zu finden.</sample>
    <sample id="247">The paper presents a new task called knowledge graph-based fact verification (KG). It introduces a dataset, Fact KG, which utilizes knowledge graphs as evidence with natural language claims. The dataset is valuable because it allows for reliable fact verification in cases where additional interpretation is required to reason about the claim. It can also be used practically, such as in modern dialogue systems that communicate with internal knowledge graphs and use or update information. The dataset includes five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The paper also discusses the method used to create the dataset, including the use of a collocational style transfer model and presupposition templates. The results show that all baselines outperform the majority class baseline, and the model that uses graph evidence outperforms all other baselines.</sample>
    <sample id="248">Nein, die Annotatoren sind nicht auf jede demographische Gruppe ausgewogen.</sample>
    <sample id="249">Sie wurden durch das Hinzufügen von akzeptablen oder unakzeptablen Präfixen durcheinander gebracht.</sample>
    <sample id="250">Eine dimensionale Bewertung ist eine Methode, um die Qualität von Dialogen auf einer Finer-Grained-Ebene zu bewerten. Sie versucht, die subjektive Natur menschlicher Bewertungen zu reduzieren, indem sie explizit feststellt, ob jede Modell-Antwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel die Bereitschaft, irrelevantes Information zu geben oder sich selbst oder seinem Partner zu widersprechen.</sample>
    <sample id="251">Die Autoren gehören an der University of Science and Technology of China.</sample>
    <sample id="252">The presentation introduces a new method for legal document retrieval, focusing on the use of event extraction techniques. The method, called "You Create," leverages unsupervised learning and an event-based approach to improve retrieval efficiency and generalization across different legal systems. It uses dependency parsing and entity recognition to extract events from case documents, which are then used to compute an interaction matrix between query and candidate documents. The results show that event-based models outperform baseline methods and supervised approaches, demonstrating higher F1 scores and lower inference times.</sample>
    <sample id="253">The presentation introduces a double-domain adaptation model called DisorBERT, designed to detect signs of mental disorders in social media. It's a collaborative effort by researchers from Mexico and Spain. The presenter, Mario Ezra Aragón, explains that a mental disorder is a psychological syndrome causing distress and disability affecting thinking, feeling, mood, and behavior. The team aims to analyze social media posts to support early detection of mental health issues using domain adaptation techniques. They integrate information from Reddit and mental health resources to specialize the general language model for specific mental health tasks. The results show better performance in balancing precision and recall compared to baseline models. The visualization tool highlights important text sequences related to mental health topics, demonstrating the effectiveness of their approach in capturing signs of mental disorders in social media interactions.</sample>
    <sample id="254">The presentation introduces a research work on "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction." The speaker, from the University of Science and Technology, explains that document-level relation extraction aims to extract relationships between entities in a document. Previous methods rely on large-scale human-annotated corpora, which are time-consuming and labor-intensive. Recent work has leveraged distant supervision data to pre-train document-level relation extraction models for better performance. However, distant supervision data often contains noisy labels, and current methods to alleviate noise by using pseudo labels still have risks of false positive predictions. The proposed framework uses uncertainty-guided label denoising to improve the label quality of distant supervision data. It involves pre-training a denoising model with both distant supervision and human-annotated data to generate pseudo labels, introducing uncertainty estimation to determine whether the model's prediction is reliable, and designing a re-labeling strategy with dynamic class uncertainty thresholds. The framework outperforms previous baselines on two public datasets, demonstrating significant performance improvements.</sample>
    <sample id="255">Die Form des Prompts ist wichtig, wenn es um 0- und 1-Shot-Prompting geht.</sample>
    <sample id="257">Die Autoren haben vier Dialogmodelle evaluiert.</sample>
    <sample id="258">The video introduces a new approach to evaluating the quality of text in natural language processing using large language models. The speaker, Chang-Han Chiang, explains that their work proposes using large language models to evaluate text samples by providing them with instructions and ratings. They compare this method to human evaluations, which are often unstable and difficult to reproduce. The motivation behind this work is to find an alternative to human evaluations that can achieve similar results without the drawbacks. The speaker mentions that their idea was novel at the time of submission to ACL 2023, as there were no prior works exploring the use of large language models for evaluation.</sample>
    <sample id="259">The speaker introduces Exemplar, a framework for cross-lingual semantic parsing in multiple natural languages and many representations. They propose a unified dataset containing 90 datasets from various domains, 5 semantic parsing tasks, 80 mini-representations, and 22 natural languages in 15 language families. The framework is evaluated on six settings: translate test, monolingual model, monolingual few-shot, multilingual model, cross-lingual zero-shot, and cross-lingual few-shot transfer. Encoder-decoder models outperform previous work, and the results show that monolingual language models are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="260">There are six authors involved in the work.</sample>
    <sample id="261">Ein guter Planer sollte Skripts erstellen, die realistisch und den Bedingungen entsprechend sind.</sample>
    <sample id="262">Die Arbeit beteiligen sechs Autoren.</sample>
    <sample id="263">The presentation introduces a new type of bias in in-context learning called domain label bias, which is caused by the effect of task context on model predictions. The authors propose a calibration method called domain context calibration to mitigate this and other types of biases. This method uses random in-domain words sampled from the task corpus as content-free text to estimate and correct the model's biases. Experiments show that domain context calibration significantly improves the performance of large language models on tasks with high domain label bias.</sample>
    <sample id="264">The presentation introduces a new task called Transferable Audio-Visual Text Generation (TAVT) and proposes a framework to train a model that can quickly adapt to new multimodal domains with limited labeled data. The framework consists of three components: Audio-Visual Mapmaker Network, Audio-Visual Encoder, and Language Model Generator. The first model maps different visual concepts across domains into a unified audio-visual semantic space, while the second model uses a transfer model-based encoder and generator to generate text based on the mapped audio-visual concepts. The third component is a constructive learning process that fine-tunes the model by generating negative samples and optimizing the visual-audio element scores directly. The experimental results show that TAVT outperforms state-of-the-art approaches in cross-domain settings and achieves comparable performance even with low-resource domains.</sample>
    <sample id="265">The presenter's name is Vasudha Varadarajan.</sample>
    <sample id="266">Die Autoren gehören an der Paderborn University.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Omissionen, bei denen die Modell manchmal Teile des Quelltextes weist, die in der Übersetzung nicht verwendet werden.</sample>
    <sample id="269">Hallo, ich bin James Finch und ich bin Sarah Finch. Heute werden wir Ihnen alles über ABC-Eval erzählen, ein neues dimensional-basiertes Verfahren zur Beurteilung von conversationaler KI. Dieses Werk wurde vom Emory NLP Lab, geleitet von Professor Jinho Choi an Emory University, in Zusammenarbeit mit Amazon Alexa AI erstellt. Nehmen Sie an, dass Sie einen Dialogmodellentwickelt haben und möchten sehen, wie gut es sich gegenüber dem aktuellen Stand der Kunst vergleicht. Die gemeinsame Praxis besteht darin, menschliche Beurteilungen zu verwenden, indem man menschlichen Urteilsfunktionäre fragt, welche der zwei Konversationen besser ist oder sie auf einer Likert-Skala bewertet.Diese Ansätze sind gut, um eine komholistische Beurteilung der allgemeinen Dialogqualität zu erhalten, aber Dialogqualität hat viele Aspekte. Daher möchten Sie eventuell mehrere Dimensionen der Chatschwelle bewerten, um die Stärken und Schwächen des Modells auf einem feineren Granularitätslevel zu verstehen. Ein Ansatz besteht darin, einfach menschlichen Urteilsfunktionäre zu bitten, die mehrere Dimensionen der Dialogqualität bewerten, indem sie beispielsweise die Relevanz der Modellantworten bewerten, indem sie existierende vergleichbare oder Likert-Skala-Methoden verwenden. Allerdings glauben wir, dass es einen präziseren und zuverlässigeren Strategie für dimensionale Dialogbeurteilung gibt. Unsere Ansatz versucht, die Subjektivität menschlicher Beurteilungen zu reduzieren, indem jede Modellantwort explizit bewertet wird, ob sie bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel die Bereitstellung von irrelevanter Information oder die Kontradiktion von sich oder seinem Partner. Wir nennen diesen Ansatz "annotating behaviors in chat" oder "ABC-Eval" im Kurzschluss. Wir haben diese Methode entwickelt, um umfassend die Verhaltensweisen von Chatschwelle zu abdecken, die in jüngster Literatur als Auswirkungen auf die Chatschwelle vorgeschlagen wurden. ABC-Eval ist in der Lage, die Häufigkeit zu messen, bei der Chatschwelle modelle verschiedene thematische Fehltritte machen. Zum Beispiel misst ABC-Eval die Anzahl der Runden, in denen ein Chatschwelle-Modell seinen Partner ignoriert oder etwas Irrelevantes sagt, sich oder seinen Partner kontradikt, falsche Fakten auslotet oder gegen allgemein akzeptierte Wissen verstößt, und wann das Modell Erfolg zeigt oder versagt, Empathie zu zeigen. Um zu bestimmen, welche Art von Beurteilung am besten geeignet ist, haben wir vier Chatschwelle-Modelle ausgewählt und davon 100 menschlich-basierte Konversationen pro Modell auf 100 menschlich-basierte Konversationen pro Modell mit ABC-Eval bewertet. Als Vergleich haben wir auch dieselben Konversationen mit drei existierenden Methoden bewertet: Likert-Ratings auf der Runde-Ebene, Likert-Ratings auf der Dialog-Ebene und Dialog-Ebene, paarweise Vergleiche.Für jede der existierenden Methoden haben wir Bewertungen auf acht der meist bewerteten Aspekte der Dialog qualitativ gesammelt, da dies standardmäßig für die Bewertung von Chatschwelle-Modellen auf mehreren Dimensionen gilt. Aus our Analysen dieser Bewertungsergebnisse haben wir festgestellt, dass ABC-Eval-Behaviors-Labels insgesamt zuverlässiger sind als die Labels, die von existierenden Methoden gesammelt wurden, wie gemessen durch den interannotator Agreement auf 100 doppelt labelten Konversationen. In addition, ABC-Eval-Labels sind zuverlässigere Vorhersagen der allgemeinen Konversationsqualität im Vergleich zu Metrisen, die von existierenden Methoden generiert werden, wie gezeigt durch eine einfache lineare Regression. Zum Beispiel können Sie sehen, wie die Messung der Anteile der Runden mit sich und seinem Partner kontradiktiven Ausdrucksweisen 5% und 10% der Konversationsqualität respektiv erklären, während die durchschnittlichen Likert-Konsistenzscores nur 4% oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmethode ein eindeutiges Aspekt der Chatschwelle qualitativ abdeckt, indem wir eine schrittweise lineare Regression verwendet haben. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25% der Konversationsqualität erklärt und, wenn Sie die Metriken eins nach dem anderen entfernen, die meisten davon resultieren in einem Verlust an Informationen über die Qualität. Auf der anderen Seite erklären die Kombination aller Likert-Ebene Likert-Metriken viel Weniger der Qualität und fewer davon tragen einzigartige Informationen. Diese zuverlässigen, informativen und eindeutigen ABC-Eval-Metriken ermöglichen uns, conversational KI mit einer höheren Auflösung zu bewerten, als vorherige Ansätze es erreichen können. Sie können sehen, dass in den Ergebnissen unseres Experiments mehrere Herausforderungen bestehen und wurden präzise quantifiziert. Zum Beispiel haben die Bots, die wir getestet haben, gegen 20% ihrer Antworten Irrsinn in ihren Antworten, gegen 15% ihrer Antworten irrelevante Informationen produziert und gegen 10% ihres Zeiten sich oder ihren Partnern kontradikt. Mit der raschen Verbesserung in diesem Bereich können diese Irrsatzraten in neuen Modellen, seit unsere Evaluation durchgegangen ist, einen Abfall erleben. Dennoch ist dies noch mehr Grund zu verfolgen zu zuverlässigen und präzisen Beurteilungsmethoden zur Beurteilung von Modellen. Wir hoffen, ABC-Eval kann von anderen im Feld als ein wichtiger Schritt in dieser Richtung verwendet werden und wir freuen uns darauf, zu sehen, wie conversational KI im kommenden Monat und Jahr fortschritt. Vielen Dank für das Watching.</sample>
    <sample id="270">Die Autoren gehören an der Emory University.</sample>
    <sample id="271">CFT steht für "Continuously Fine-Tuning". Es ist ein Verfahren, bei dem das Modell während der Validierungsphase weiter optimiert wird, um die Leistung zu verbessern.</sample>
    <sample id="272">There are seven authors involved in the work.</sample>
    <sample id="273">Kayo Yin präsentiert das Werk "When Does Translation Require Context? A Data-driven, Multilingual Exploration" in collaboration mit Patrick Fernandes, Emmy Liu, Andre F. T. Martins und Graham Neubig. Das Werk untersucht, wann Übersetzungen Kontext benötigen und wie gut Modelle solche Übersetzungen handhaben. Es wird anhand von Transkripten TED Talks, die von Englisch ins Deutsche übersetzt wurden, eine Analyse durchgefohren. Die Analyse zeigt, dass bestimmte Wörter oder Phänomene nur mit Kontext korrekt übersetzt werden können. Ein Beispiel ist die Verwendung von dualen Pronomen in Arabisch, bei der Kontext benötigt wird, um zu bestimmen, ob ein Pronomen dual ist. Eine Benchmark-Unterrichtstheorie wird verwendet, um die Leistung von Modellen bei Dokumentebasierter Übersetzung zu bewerten. DasBenchmark zeigt, dass Kontext-aware Modelle besser auf bestimmte diskursive Phänomene wie Formalität und Lexikalische Kohärenz agieren, aber nicht auf andere Phänomene wie Ellipsenresolution. DasBenchmark kann dabei helfen, zu identifizieren, welche diskursive Phänomene Modelle gut oder schlecht bewältigen und welche Übersetzungssysteme am besten auf Dokumentebasierter Übersetzung agieren.</sample>
    <sample id="274">Usen John</sample>
    <sample id="276">The presentation introduces DePlain, a new corpus for German text simplification at both document and sentence levels. It highlights the challenges with existing corpora, such as small size and automatic alignment errors. The DePlain corpus addresses these issues by providing parallel pairs of complex German sentences and their simplified translations. Techniques like lexical substitution, clause deletion, reordering, and insertion are used to simplify sentences. The presentation also discusses the evaluation metrics used, including fluency and accuracy scores, and compares the performance of different metrics using the DePlain corpus. The results show that the Indic Comet MQM variant outperforms Comet baselines in most languages and has higher correlations across all languages. The robustness of DePlain is evaluated using the WMT translation accuracy challenge sets, demonstrating its effectiveness in unseen languages.</sample>
    <sample id="277">Die neue Methode hat keinen Namen.</sample>
    <sample id="278">Die Methode der „markierten Wörter“ verwendet das soziolinguistische Konzept der Markiertheit, um die Wörter zu identifizieren, die einen bestimmten Gruppe von unmarkierten Gruppen unterscheiden.</sample>
    <sample id="279">Die Autoren sind an der University of Washington.</sample>
    <sample id="280">The speaker introduces a new framework for emotion recognition in conversations called MultiEMO. The framework consists of four key components: uni-modal feature extraction, context modeling, multi-modal fusion, and emotion classification. The speaker proposes a novel visual feature extractor called Vis-Net, which captures visual cues by integrating facial expressions from multiple frames without encoding redundant scene-related information. The speaker also proposes a multi-modal fusion model called MultiATT, which integrates one modality with complementary information from the other modalities through stacked bi-directional multi-head cross attention layers. Additionally, the speaker introduces a sample-weighted focal contrastive loss to address the difficulty of classifying minority and semantically similar emotions. Experimental results demonstrate that MultiEMO achieves state-of-the-art performances on two ERRC benchmark datasets, MELD and iEMOCAP, with significant improvements in minority and semantically similar emotions.</sample>
    <sample id="281">The presentation discusses the importance of context in translation and how it affects the meaning of words. It explains that evaluating context-dependent translations is challenging due to the limited availability of such resources. The speaker introduces a new approach to measure context usage by machine translation models, which involves analyzing transcripts of TED Talks translated into 14 languages. The analysis reveals patterns between words that require context for translation, such as dual pronouns in Arabic and verb forms in certain languages. The speaker also identifies cases where context is necessary for proper noun translation and formality in different languages. Finally, the speaker presents a benchmark for document-level translation using context-aware models, which outperform models without context for certain discourse phenomena.</sample>
    <sample id="282">This paper presents a new work in ACL 2023, titled "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing." The research addresses the task of non-parallel text style transfer at the discourse level, which is crucial for imitating author style. The proposed model, StoryTrans, generates text in target styles by learning discourse representations from source texts and combining them with normal style embeddings. A new training objective reduces style-specific features from discourse representations, pulling the representation derived from different texts closer in the latent space. The model is trained in two stages: the first stage involves style-specific content keyword masking and generating whole texts, while the second stage focuses on fine-tuning the style-specific content and removing masked tokens. Experimental results show that StoryTrans outperforms strong baselines in terms of style control and content preservation.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">In this work, we first propose a novel fuzzy span loss that alleviates the model's reliance on span boundaries. Then, we propose an efficient fuzzy span attention to adaptively adjusting the attention span of the model. The FSIUAE we proposed achieves excellent results in a wide range of IE tasks.</sample>
    <sample id="285">The video discusses the challenges of fact error correction in dialogue summarization and proposes a new evaluation framework to address these issues. The current evaluation methods, such as fact CC and DA E, have flaws that may divert the optimization of fact error correction models from their original purpose. The proposed evaluation framework includes alignment, classification, and comparison steps, and uses a taxonomy of fact errors based on content and form. The results show that training fact error correction models with reference summaries from dialogue summarization datasets yields the best results, and combining human-annotated data with synthetic data is a promising direction for improving performance.</sample>
    <sample id="286">Sarah Finch</sample>
    <sample id="287">Vier Autoren arbeiten an der Arbeit beteiligt.</sample>
    <sample id="288">Die Blimp und Syntax Gym Datensätze können zum Testen syntaktischer Phänomene verwendet werden.</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind FTW, COSINE, VELLA, FINE-TUNING und CONTINUOUS-FINE-TUNING.</sample>
    <sample id="291">Das Modell wird anhand von Aufgaben wie Namensentwicklung, Klassifizierung, Part-of-Speech tagging und Fragenaufbau evaluiert.</sample>
    <sample id="294">CamemBERT wurde ursprünglich auf einem Datensatz von 138 Millionen Begriffen trainiert.</sample>
    <sample id="295">Der Referent in diesem Text ist Adam Siprowski.</sample>
    <sample id="296">The video presents a collaborative work between the University of Turin and Amazon Alexa on natural language understanding and processing. The team developed a corpus called EPIC, which consists of 300 short conversations from social media, Reddit, and Twitter, annotated by 74 annotators for five varieties of English. They observed differences in inter-annotator agreement based on annotator characteristics like gender, age group, and nationality. They also noticed that generational proximity and geographical distribution affect annotators' perceptions of irony. The team trained perspective-aware models to account for these differences and found that these models are less uncertain and more confident in their predictions compared to gold-standard aggregated models.</sample>
    <sample id="297">This project develops a typology and glossary of dog whistles, performs a case study on historical US political speeches, evaluates dog whistle recognition in language models like GPT-3, and conducts a case study on toxicity detection with hateful sentences. The research highlights the context-dependent nature of dog whistles, their prevalence in conservative and Republican Southern strategy speeches, and the challenges in recognizing and moderating them due to their coded meanings and varying performance in language models.</sample>
    <sample id="298">Die Versuche, die Modelle mit jüngerem Datensatz fortzusetzen, haben gezeigt, dass das Leistungsverlust mit einer größeren zeitlichen Distanz zwischen dem Train- und Test-Datensatz einherging.</sample>
    <sample id="299">The presentation discusses the development of a training method to reduce the reliance of NLI models on shortcuts and improve their out-of-distribution performance. The key insight is that NLI models suffer from poor performance on underrepresented hard training instances with patterns that could indicate the shortcuts in the dominant easy examples. The proposed method uses a minimax training objective between a learner and auxiliary model to generate example weights, which incentivize the learner to concentrate on regions of the input space where it incurs high losses. This approach does not make assumptions about the type of shortcuts contained in the dataset and relies on the learner's own training dynamics to generate example weights. The method is evaluated on three commonly used NLI datasets and their corresponding out-of-distribution test sets, showing consistent improvement in out-of-distribution performance while maintaining high in-distribution accuracy.</sample>
    <sample id="300">The presentation introduces the task of interactive dictation, which allows users to dictate and edit documents using their voice in a natural and intuitive manner. The speaker, Belinda Z. Li, explains that this work is a collaboration with Jason Eisner, Adam Pauls, and Sam Thompson from Microsoft Semantic Machines. Interactive dictation involves flexible interleaving of dictation and editing without trigger words, and it uses natural language utterances to specify edits. The presentation highlights the need for a more natural and intuitive interface compared to current speech-to-text systems that only support dictation. The team has designed a data collection interface and built a dataset for this task, and they have created a baseline system that performs each step of the process. The presentation also discusses the evaluation of different models, including T5 and GPT-3, and their trade-offs between runtime and accuracy.</sample>
    <sample id="302">Die Token für die Ausgabesequenz müssen permutiert werden, um die richtige Reihenfolge der Ausgabesequenz zu bestimmen.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, um die Ursachen für prunkhafte Mustergenerierungen zu verstehen und zu untersuchen.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind jene, bei denen die grammatischen Strukturen oder Stereotypen in einem Satz nicht passen.</sample>
    <sample id="305">The video presents a critical look at weakly supervised learning (WSL), a method used in machine learning where data is labeled using weak labeling sources such as simple heuristics, knowledge bases, or low-quality crowdsourcing. The speaker, Dawei Zhu, and his team discuss the challenges of WSL, including the noise in weak labels and the need for clean validation samples to achieve high performance on clean test sets. They address three research questions: whether clean validation data is necessary for WSL, how many clean samples are required, and whether clean samples should only be used for validation. Their findings indicate that recent WSL methods require clean validation samples to work properly, and that increasing the number of clean validation samples can improve performance. They also suggest that fine-tuning on clean samples can achieve similar performance gains as more complex WSL methods. The video concludes with recommendations for future work, including reporting model selection criteria, comparing WSL approaches with fine-tuning baselines, and considering continuous fine-tuning as a simple yet strong baseline.</sample>
    <sample id="306">The presentation discusses the challenge of entity tracking in language models and introduces a task designed to evaluate this ability. The task involves predicting the contents of boxes after state-changing operations, such as moving objects or adding them to a box. The experiment shows that most models repeat the initial state, but only the GPT-3.5 model can perform non-trivial tracking. The results suggest that pre-training on code is responsible for this capacity in language models.</sample>
    <sample id="307">Die Autoren haben die Leistung der Modelle anhand von Public und Private Datasets bewertet, indem sie die Aufgaben wie Name und Attribut Erkennung, Klassifizierung, Part-of-Speech tagging und Fragensolving bewerteten.</sample>
    <sample id="308">Jenny, eine erstjahrige PhD-Studentin an Carnegie Mellon University, präsentiert ein Werk namens "NL Positionality: Characterizing Design Biases of Datasets and Models". Das Werk wurde in Zusammenarbeit mit Forschern an der University of Washington und dem Allen Institute for AI erstellt. Das Werk untersucht die Systematische Leistungsunterschiede technologischer Modelle zwischen verschiedenen Bevölkerungen, die auf die Positionalität der NL-Researcher und Modellentwickler zurückzuführen sein könnten. Positionalität ist die Perspektive, die Menschen als Resultat ihrer Demografien, Identitäten und Leben Erfahrungen halten. Das Werk verwendet eine Methode, um Datensätze mit diversen Annotatoren zu re-annotieren und dieAnnotations mit realen Benutzern zu vergleichen. Das Werk fand, dass Datensätze und Modelle eine Positionalität aufweisen, indem sie bestimmte Bevölkerungen bevorzugen und andere behindern.</sample>
    <sample id="309">Interannotator agreement</sample>
    <sample id="310">Wikipedia</sample>
    <sample id="311">The authors belong to the University of Tübingen.</sample>
    <sample id="312">MultiInstruct ist der erste MultiModell-Instruction-Tuning-Benchmarksatz, der 62 diverse MultiModell Aufgaben umfasst, die in 10 Hauptkategorien unterteilt sind.</sample>
    <sample id="313">Drei</sample>
    <sample id="314">Die Definition der binären Koordination lautet: "Die Binärkoordination ist eine Art von Koordinationsstruktur, bei der zwei Elemente (normalerweise Nomen) in einem Satz durch einen Koordinationsverband verknüpft werden."</sample>
    <sample id="315">Die in dieser Studie verwendeten Prompts lagen im Durchschnitt bei 17 Wörtern.</sample>
    <sample id="316">Die Auswirkungen auf das kleinere T5-Modell sind, dass es die besten Skripte generiert und die Qualität der Skripte signifikant verbessert.</sample>
    <sample id="317">The presentation introduces a new method called CodeIE for transforming unstructured information extraction tasks into structured code generation tasks using large language models like Codex. The traditional approach relies on pre-training language models to generate structured outputs, which can lead to mismatched outputs during inference. CodeIE addresses this by converting the input text into a structured format that aligns with the output stage, ensuring more accurate and consistent results. The method is evaluated on named entity recognition and relation extraction datasets, showing significant improvements over traditional baseline models. The analysis highlights the better performance of CodeIE in terms of accuracy, recall, and the absence of structural errors when using Codex and GPT-3 models. The presentation concludes with an invitation for further discussion and mentions that the proposed method and code are publicly available.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich werde euch heute über unsere Arbeiten mit DrBERT präsentieren. DrBERT ist ein robustes vortrainiertes Modell in Französisch für biomedizinische und klinische Domänen. In dieser Präsentation werden wir zuerst über Sprachmodellierung im Gesundheitswesen sprechen. Danach präsentieren wir die Hauptbeiträge unseres Artikels. Wir Introduzieren das erstmalige biomedizinische Modell in Französisch namens DrBERT, das auf Roberta basiert und trainiert wurde auf NACHOS, einer Datensammlung von medizinischen Crowded-Data aus der Web. Wir Introduzieren auch eine Comparaison von Modellen mit verschiedenen vortrainierten Einstellungen und Datensätzen. Dann präsentieren wir unsere Resultate auf 11 biomedizinischen und klinischen downstream Tasks in Französisch. Schließlich diskutieren wir über die Experimente und geben Ihnen mehr Details darüber, wie Sie Zugang zu den Modellen erhalten. Seither es im Jahr 2018 freigegeben wurde, ist DrBERT zu einem der effektivsten Ansatzpunkte geworden, um natürliche Sprachverarbeitungs Aufgaben zu lösen und einen großen Leistungsunterschied gegenüber historischen statischen und konzektionären Methoden wie Word2Vec, FastText und Elmo zu erreichen. Seither hat sich das Modell an viele andere Sprachen wie Französisch mit Camembert und auch an anderen Domänen wie biomedizin mit PUMA BERT und BioBERT und klinisch mit Kliniker BERT angepasst. Aber hauptsächlich in Englisch. Spezialisierte Modelle für andere Sprachen sind seltener und oft basieren auf Kontinuum vortraining due to the Lack of in-domain data. Allerdings hat Französisch noch keine offene Quellen-Modelle für biomedizin und klinisch. Also fragten wir uns, was die geeignetste Datensammlung für eine Vielzahl von Anwendungsfällen ist, und diese Crowded-Data sind eine gute Substitution für klinische Daten. Um diese Frage zu beantworten, verglichen wir DrBERT mit unserem ShBERT Modell, das auf anonymisierten Datensätzen basiert, die von der Non-Universität Hospital der University of Texas abgeglaubt wurden. Danach fragten wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf Französisch zu trainieren. Ist es 4 GB, 8 GB oder mehr? Um diese Frage zu beantworten, haben wir zuerst ein von Null startendes Modell trainiert und verglichen: eine erstmals version von DrBERT mit 7 GB von NACHOS, eine zweite Version von 4 GB subset of NACHOS, eine erstmals version von ShBERT, das ein klinisches Modell ist, mit 4 GB von Sätzen, die aus klinischen Knoten stammen, und eine finale Version von ShBERT mit einem Mix von 4 GB subset of NACHOS und 4 GB von klinischen Knoten. Neben dieser Vergleichsanalyse haben wir drei Modelle trainiert, die auf Kontinuum vortrainingen, um die Auswirkungen von vortraining Strategien zu analysieren: eine basierend auf dem Gewicht von Camembert und trainiert auf 4 GB subset of NACHOS, eine, die auch auf Camembert basiert, aber trainiert wurde auf 4 GB von klinischen Knoten, und eine, die auf einem englischen biomedizinischen Modell basiert BERT und trainiert wurde auf 4 GB subset of NACHOS. Insgesamt haben wir sieben Modelle. Um die Sieben Modelle zu evaluieren, haben wir verschiedene öffentliche und private downstream Aufgaben wie Name entity recognition, Klassification, Part-of-Speech tagging und Question answering verwendet.Diese Modelle wurden mit sechs Baseline-Modellen verglichen, die Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PUMA BERT BioBERT und Kliniker BERT sind. Die Evaluation zeigt, dass das Modell am besten auf der Aufgabe performt, bei der die Daten die gleiche Natur haben, auf der das Modell trainiert wurde. Allerdings können wir beobachtet haben, dass Daten von heterogenen Quellen zu einem besseren Versatility führen. Wir können auch beobachtet haben, dass das Verhältnis von mehr Daten zu besseren Performen zugeht. Insgesamt scheint das von Null startende Training, das auf 7 GB subset of NACHOS trainiert wurde, höhere Leistungen auf meisten Aufgaben zu erzielen. Allerdings haben unsere Experimente mit Kontinuum vortraining, die auf dem Gewicht und Tokenizer von PUMA BERT trainiert wurden auf 4 GB subset of NACHOS, ähnliche Resultate zu denjenigen, die wir mit DrBERT 4 GB von Null startend erhalten haben, was nicht der Fall ist, wenn wir ein Modell mit Camembert Gewichten und Tokenizer verwenden, das unter Instabilitätproblemen zu leiden scheint. Als Schlussfolgerung bieten unsere vorgeschlagenen Systeme eine bessere Leistung auf neun von den 11 downstream Aufgaben und überwiegen global über die Leistungen des generischen Modells Camembert. Wir können auch beobachtet haben, dass spezialisierte Daten besser sind, aber sie nicht gut skaliert werden. Das vortrainierte Modell, das auf NACHOS trainiert wurde, ist frei verfügbar und auf YouTube und all die Trainingsskripte sind auf unserem GitHub Repository. Also danke für die Präsentation und wir freuen uns auf die Diskussion auf der Post-Session in Toronto.</sample>
    <sample id="319">Die Arbeit untersucht die Auswirkungen verschiedener Lernstrategien auf die Leistung von Modellen im Bio-medizinischen und klinischen Bereich.</sample>
    <sample id="320">Der Faktor der Überanpassung, der auf die Wiederverwendung von Tests zurückzuführen ist, wird als "Adaptive Overfitting" bezeichnet und zeigt sich durch einen größeren Gradienten, der größer als 1 ist.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde anhand von Typen von Vereinfachungen analysiert.</sample>
    <sample id="322">The speaker, Enrico Liscio, is presenting at ACL 23 and discussing the question of what a text classifier learns about morality. He begins by explaining that human morality helps us distinguish right from wrong and is essential for society. He then discusses how morality is often treated as a singular scale between immoral and moral in NLP, but this approach can hide the subjectivity of morality and lead to misunderstandings. The speaker introduces the Moral Foundation Theory, which posits that there are five different ways humans perceive morality, each with its own moral foundation. He explains that language models can understand morality in text, but they may not fully grasp the nuances of different domains. The speaker uses a dataset called the Moral Foundation Twitter Corpus to demonstrate that language models can recognize differences in how morality is expressed across domains. He concludes by warning that using a single model for multiple domains can lead to dangerous misunderstandings of morality.</sample>
    <sample id="323">The paper presents a method for answering commonsense questions using dynamic heterogeneous graph reasoning with language models and knowledge representation learning. The authors propose a framework that combines knowledge from both language models and knowledge bases to improve the performance of commonsense question answering. They introduce a novel approach to retrieve relevant knowledge from a knowledge base through entity matching, building a subgraph, and then use the language model and GNNs to generate answers. To address the limitations of existing methods, they propose a new method called DILK, which involves building a HKG based on a mutable knowledge base, removing subwords, encoding QA contexts, and dynamically removing entities with weak relevance to the QA context. They also incorporate HKG path information into the QA context and update the entity and relation embeddings of the HKG using LMSE. Finally, they input the HKG graph embedding, paths in the QA context, and the QA context embedding into MLP to get the answer probabilities. Experiments on ConCQA and OpenBookQA show that their method achieves good results compared to other baselines.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile.</sample>
    <sample id="325">Hi, my name is Matthias Lindemann, and today I am going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations. This is joint work with my advisers Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this: As usual, we have a training set of utterances in this case, the girl slept and Mary knew that the girl slept. These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallower recursion during training and is tested on an example with deeper recursion. Naive sequence-to-sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar induction procedures. In this paper, we don't use trees and introduce a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens but they are not ordered. That's why in the second step, we use another model to predict a permutation to put them into the right order. We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this: We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one as highlighted in red. Then we jump to the next multiset token to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the CoNLL benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token, we don't know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest scoring permutation is NP-hard. That's because this is related to the traveling salesman problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to back propagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.</sample>
    <sample id="326">Kognitive Dissonanz ist die Inconsistenz zwischen zwei Glaubens oder Handelten, die in einem Menschen existieren.</sample>
    <sample id="327">The paper presents a new multi-modal architectural model called Manager Tower, which adapts insights from pre-trained unimodal experts at different levels to facilitate comprehensive cross-modal alignment and fusion. The model uses managers in each cross-modal layer to combine the insights of pre-trained unimodal experts, allowing for more effective exploitation of different levels of unimodal semantic knowledge. The proposed model achieves superior performance on various downstream tasks with only 4 million images for vision-language pre-training, outperforming Bridge Tower and Miter Bridge Tower by 79.15% accuracy on the Visual Genome dataset.</sample>
    <sample id="328">GPT-4 ist das am meisten links stehende Sprachmodell.</sample>
    <sample id="329">The presentation introduces a method for generating structured pseudo-labels to improve zero-shot video sentence localization. It explains the process of using pretrained models to generate complex queries and events, ensuring high relevance between videos and queries while minimizing overlap with irrelevant events. The method also includes strategies to reduce label noise by weighting samples based on prediction confidence and IoU, and by treating high-confidence predictions as new labels. The proposed method outperforms existing zero-shot methods on two datasets, demonstrating its effectiveness in handling label noise and improving localization accuracy.</sample>
    <sample id="330">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="331">The speaker's name is Sara Papi.</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus Transkripten von TED Talks, die von Englisch ins Deutsche und ins Lateinamerikanische übersetzt wurden.</sample>
    <sample id="333">The paper presents a framework called INK for injecting knowledge into nearest neighbor machine translation (NNMT) models to improve their generalization ability. The framework uses a key-value data store to save representations and their corresponding target tokens, which are retrieved at each decoding step to refine the prediction probability. However, this approach has drawbacks such as time-consuming retrieval of neighbors from a large data store and difficulty in updating representations. To overcome these issues, the INK framework proposes a training loop with two steps: first, it extracts knowledge from the data store to guide an adapter to adjust the representation, and then it updates the representations using Kullback-Leibler divergence. The framework is trained until convergence and can be dropped during inference. Experiments on the WMT 19 German-English news translation task show that INK outperforms the state-of-the-art KNNMT system and achieves better translation performance with less memory space and faster inference speed.</sample>
    <sample id="335">The referent in the video is Matthias Lindemann.</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf die Übergabe von Modellen, die auf einem Quellsprachen-Querrytrainingsdatensatz trainiert wurden, auf ein anderes Sprach-Ziel.</sample>
    <sample id="337">The presentation introduces a neural approach for out-of-vocabulary (OOV) word embedding learning, focusing on handling OOV words effectively. It presents a word relationship graph that captures lexical rules of word formation and association. The model uses a two-level graph structure to retain complete word piece information and applies a graph neural network with a self-attention mechanism to process the graph. The model is trained using contrastive learning to encourage similarity between related words while pushing unrelated samples apart. Experiments demonstrate the model's effectiveness in both intrinsic and extrinsic tasks, particularly in handling various complex word formations. The model's performance is robust across languages, especially English, due to its ability to handle word segmentation.</sample>
    <sample id="338">The speaker, Bing Chen, presents a collaborative research paper on evaluating human natural language explanations. The paper introduces a unified data structure for various tasks and evaluates the quality of human explanations using two metrics: the Simulatability Score and a new metric called True. The results show that human explanations can still benefit model predictions even if considered low-quality by humans. The proposed metric outperforms the Simulatability Score in evaluating dataset qualities across five datasets and two models.</sample>
    <sample id="339">Die Autoren gehören an der Saarland University, Amazon Alexa und der University of Vienna.</sample>
    <sample id="340">The paper presents a large-scale, syntactically diverse paraphrase dataset constructed by AMR back-translation. The dataset is designed to address the limitations of existing datasets in terms of scale and diversity. It includes online chat conversations from various sources, with a focus on video sources to better capture real spoken conversation. The paper also discusses the challenges in developing personalized dialogue applications and proposes using the proposed dataset for such applications. The authors demonstrate that their dataset can benefit several NLP applications, including sentence embeddings, synthetic control paraphrase generation, and data augmentation for future learning.</sample>
    <sample id="341">Die Autoren verwenden die Messungen der Translation Quality, der Latenzmessung und der Rechenzeit.</sample>
    <sample id="342">The presentation introduces a large-scale personalized dialogue dataset constructed from live streaming, focusing on the challenges and limitations of existing datasets. The dataset includes video sources and detailed persona annotations, addressing issues like diverse personas and multi-party dialogues. Experiments show that the dataset improves speaker personalization and response quality compared to existing datasets.</sample>
    <sample id="343">Hallo, alle zusammen. Ich bin Makkat und heute präsentiere ich mit Martin ein Werk, das wir "Kipmuster" nennen. Das Werk befasst sich mit der Evaluation von Kenntnisintegration aus mehreren Quellen. Wir arbeiten an dieser Studie an McGill University, Mira und Microsoft Research. Sprachverarbeitungsmodelle ziehen auf eine Vielzahl von Kenntnisquellen, wie zum Beispiel Kenntnisse, die in ihren Parametern enthalten sind, normalerweise durch vorheriges Training erworben, und Kenntnisse, die in Eingabe-Informationen zur Verfügung stehen. Kürzlich haben Arbeiten zu Aufgaben wie der Beantwortung von Fragen gezeigt, dass Modelle Kenntnisse aus vorheriger Zeit verwenden können, um die Aufgabe zu lösen. Aber Sprachverarbeitung oft erfordert Kenntnisse, die auch zur Laufzeit liefern. Zum Beispiel im Satz: "John sah den neu gewählten Präsidenten auf TV." Vorher trainierte Parameter können Informationen über was Präsidenten tun und was ein TV ist enthalten, aber sie können nicht sicher wissen, wer der bestimmte John ist oder wer der neue Präsident ist, da der Präsident seit dem vorherigen Training verändert wurde. Daher benötigen erfolgreiche Modelle für Kenntnisintensive NLU-Aufgaben die Fähigkeit, sowohl vorherige als auch Laufzeit-Kenntnisse zu integrieren und zu verwenden. In unserem Werk präsentieren wir einen Diagnostischen Test-Satz für Kenntnisintegration. Wir Introduzieren einen Korreferenz-Resolution-Aufgabentyp, der darauf abzielt, die Fähigkeit zu bewerten, Kenntnisse aus verschiedenen Quellen zu verwenden. Wir evaluieren den Datensatz mit humanen Studioparticipanten und etablieren Korreferenz-Resolution-Modelle. Hier ist ein Beispiel aus unserem Datensatz: "Servin ist ein Richter. Kia ist ein Bäcker. Servin und Kia haben sich in einem Park getroffen. Nach einem langen Tag an der Arbeit, bei der sie Fälle in einem Gerichtshof entschieden haben, war er glücklich, sich zu entspannen." Die Aufgabe hier ist, die richtige Persönlichkeit zu identifizieren, die das Pronomen "he" bezieht, was in diesem Fall Servin ist. Die Auflösung eines bestimmten Pronomens erfordert zwei Arten von Informationen: Persönlichkeits-Spezifische Kenntnisse, wie Servin ein Richter ist, und Hintergrund-Kenntnisse, wie Richter Fälle in Gerichten entscheiden. Normalerweise werden Hintergrund-Kenntnisse während des vorherigen Trainings von großen Sprachmodellen gelernt, während Persönlichkeits-Spezifische Kenntnisse normalerweise während der Laufzeit beobachtet werden. Wir variieren die Verfügbarkeit dieser beiden Arten von Informationen, sodass sie entweder in einer Quelle oder in mehreren Quellen gefunden werden. Wir haben definiert, drei Szenarien von Kipmuster: Erstens die "Vorausblick" Szenario, bei dem Hintergrund-Kenntnisse angenommen werden, dass sie zur vorherigen Zeitavailable sind. Zweitens die "Hintergrund-Beide" Szenario, bei dem Hintergrund-Kenntnisse sowohl zur vorherigen als auch zur Laufzeit available sind. Drittens die "Hintergrund-Lauffest" Szenario, bei dem beide Kenntnisarten nur zur Laufzeit available sind. Dieses letzte Szenario ist insbesondere interessant, da es die Situation simuliert, in der das Hintergrund-Kenntnis, das für die Aufgabe erforderlich ist, nicht Teil des vorherigen Datensatzes der Modelle ist. Zum Beispiel, weil neue Berufe seit dem vorherigen Training entdeckt wurden. Hier ist ein Beispiel, wie wir die Verfügbarkeit von Fakten in den Quellen steuern: In der "Vorausblick" Szenario nehmen wir an, dass das Hintergrund-Kenntnis "Politiker suchen Wahlmandate im Regierungsrat" in den vorherigen Parametern enthalten ist. In einem Lauffestkontext liefern wir die Anteilspezifische Kenntnis "Chester ist ein Politiker". In der "Hintergrund-Beide" Szenario liefern wir nicht nur die Anteilspezifische Kenntnis, sondern auch Hintergrund-Kenntnis über Politiker in einem Lauffestkontext. In der "Hintergrund-Lauffest" Szenario liefern wir die Anteilspezifische Kenntnis "Mehrter" anstelle von "Politiker", weil Mehrter unwahrscheinlich in den vorherigen Parametern enthalten ist.</sample>
    <sample id="344">Die Nachteile der baumbasierten Methoden sind, dass sie die Generalisierbarkeit zu tieferen Rekursionen nicht so gut abdecken.</sample>
    <sample id="345">The paper presents a neural sequence-to-sequence model that directly models the correspondences between fragments of input and output without relying on trees. The model predicts the output from the input in two steps: first, it tags each input token with an unordered multiset of tokens that will appear in the output, and then it uses another model to predict a permutation to put them into the right order. The permutation model is flexible and expressive, and the experimental results show that the model outperforms other treeless models on generalization to deeper recursion. However, some other kinds of structural generalization remain challenging. The paper also addresses technical challenges such as alignment between input and output, latent permutations, and finding the highest scoring permutation.</sample>
    <sample id="346">Die Autoren gehören an der Georgia Institute of Technology.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werden wir über unser Papier "Marked Personas" sprechen, das die Verwendung von natürlicher Sprache als Anreize verwendet, um Stereotypien in Sprachmodellen zu messen. Dieses Werk wurde in Zusammenarbeit mit Esin Durhamus und Dan Jurafsky erstellt. In den letzten Jahren haben viele die Vorkommen von sozialer Bias und Stereotypien in großen Sprachmodellen (LLMs) dokumentiert. Allerdings haben diese Messungen diverse Grenzen. Sie hängen normalerweise von von Hand konstruierten Datensätzen ab, die sehr zeitaufwendig zu sammeln sind, und sie messen normalerweise nur sehr spezifische Stereotypien, was bedeutet, dass sie nicht allgemein gut zu anderen Demografien oder Kontexten übertragen werden, oder sie capturieren einfach eine sehr allgemeine, breite Assoziation, wie beispielsweise negative Assoziationen mit bestimmten Gruppen. Darüber hinaus hat die meisten Arbeiten in diesem Bereich noch nicht an derIntersectionality (die These, dass multifasettige soziale Identitäten die Biases kompounden und ein einzigartiger Schaden darstellen können) gehandelt. Um diese Grenzen zu überwinden, vertrauen wir auf die Eigenschaft, dass diese jüngeren, instruction-tuning LLMs sehr gut auf Anregungen und Anreize reagieren können. Wir können also das Modell auffordern, eine Persona zu generieren, die eine Beschreibung von einem imaginären Individuum ist, indem wir einen Prompt wie "Stelle dir vor, du bist eine asiatische Frau. Wie beschreibst du dich?" verwenden. Und wir können unmittelbar sehen, dass dies sehr allgemein zu allen Demografien anpassbar ist, da wir einfach den gewünschten Identitätsmarker in den Prompt einfügen können. Hier sind einige Beispielgenerierungen von GPT-4. Sofort bemerkten wir, dass, obwohl die Auswertungen nicht offiziell negativ oder toxikisch sind, im traditionlichen Sinne dieser Worte, einige interessante Muster auftreten. Eine asiatische Frau wird als unaussprechlich dargestellt, eine mittlere Frau wird mit Begriffen wie "exotisch" und "einzugängig" beziehungsweise mit Bezug auf eine "mesmerisierende Region" referenziert, und sowohl eine Frau von Farbe als auch ein weißer Mann Personen machen Bezug auf Ancestry, während die Persona von einem weißen Mann nichts davon macht. Um diese Muster zu capturieren, haben wir zwei Teile. Der erste Teil ist die Generierung dieser Personas. Unsere Prompts zur Generierung dieser Personas wurden inspiriert von einem Studie, bei der sie solche Prompts für menschliche Subjekte gaben, und sie fanden, dass dadurch Menschen auch Racial-Stereotypien surfen konnten. Und auch dadurch können wir direkte Comparisons zwischen den generated Personas und menschlich geschriebenen Antworten machen. Der zweite Teil ist der "Marked Words-Methode", die eine Methode ist, um die Wörter zu identifizieren, die einen markierten Gruppe von einem unmarkierten Gruppe unterscheiden. Ich werde darauf in Kürze eingehen. Die Vorteile davon sind, dass wir sehr spezifische Stereotypien und Muster finden, ohne uns auf einen bestimmten Lexikon zu verlassen. Der Vorteil von unserem Ansatz ist, dass wir durch die "Marked Words-Methode" auf die sociolinguistische Konzepte "Markedness" (die These, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist) zurückgreifen und feststelle, dass Dominante Gruppen in Gesellschaft sowohl linguistisch als auch soziell unmarkiert sind, während marginalisierte Gruppen normalerweise markiert sind. In unserem Ansatz erstellen wir zuerst die unmarkierten und markierten Gruppen und vergleichen dann die Personas mithilfe der "Fighting Words-Methode" (basierend auf gewichteten Logit-Raten, um die Top-Wörter für jede markierte Gruppe zu identifizieren). Zum Beispiel für die Persona von einer schwarzen Frau würden wir "Fighting Words" machen und die Logit-Raten mit jenen von weißen Personen und Personen von weiblicher Art vergleichen, da sie die zwei entsprechenden unmarkierten Gruppen sind. Nun zu ein paar Ergebnissen: Zunächst verwenden wir ein Lexikon von Stereotypen und finden, dass die generierten Personas mehr Stereotypen enthalten als die menschlich geschriebenen. Allerdings, wenn wir die Verteilung der Wörter im Lexikon analysieren, finden wir etwas anderes. Während die generierten Personas sehr hohe Raten von Lexikon-Wörtern aufweisen, haben die menschlich geschriebenen Personen eine breitere Verteilung von Wörtern, während die Stereotypen-Wörter in den generierten Personas lediglich Wörter wie "tall" und "athletic" sind, die positiv oder zumindest nicht negativ sind. Und in Wirklichkeit fängt das Lexikon nicht viele der harmvollen Muster, die wir in den früheren Folien gesehen haben, ein. Stattdessen wenden wir uns an die Resultate der "Marked Words-Methode" zu, um zu zeigen, wie diese positiv-scheinenden Porträt Wörter facilitieren Stereotypen und essentialisierende Narrativen. In our analysis, wir Revel how these seemingly positive portrayals reflect harmful patterns. First for Mark groups the top words include things like culture, tradition, proud, and exotic, and these words define these groups only by their relationship to their identity and distinguish them as different from the white norm, this contributes to a long legacy of discrimination and othering for these groups. Furthermore, there is a lot of common tropes that are reflected in these words, especially for women of color. For example, the words describing Latina women include things like vibrant and curvaceous, which connect to a trope of tropicalism. For Asian women, the words are things like petite and delicate and silky, which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women we see that some of the top words are things like strong and resilient. This connects to an archetype that people have called the strong black woman archetype, and while it sounds like positive at first glance, there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles, so rather than actually working towards changing those obstacles, it puts pressure on these people to overcome them, which leads to very negative health outcomes for these people among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should as researchers be addressing positive stereotypes and essentializing narratives. We should also be using intersectional lens to study biases and harms, because there's a lot of things that might be overlooked if we don't do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don't know if it's because there is some sort of like weird overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these prinnish patterns, we just really can't make any assumptions or really study that further without more transparency. Thank you so much for listening. Have a good time at ACL.</sample>
    <sample id="348">The paper "Marked Personas" by Myra Cheng, Esin Durmus, and Dan Jurafsky presents a method to measure stereotypes in language models using natural language prompts. The authors address the limitations of existing methods, such as reliance on hand-constructed datasets and failure to capture intersectionality. They propose generating personas based on prompts like "imagine you are an Asian woman describe yourself," which can be generalized to any demographic. The method involves two parts: generating personas and identifying marked words that distinguish groups from unmarked ones. The results show that generated personas contain more stereotypes than human-written ones, with marked words reflecting harmful patterns and essentializing narratives. The authors recommend addressing positive stereotypes, using intersectional lenses, and increasing transparency about bias mitigation methods.</sample>
    <sample id="349">Hallo, alle. Mein Name ist Jing Wei und ich bin an der Technischen Hochschule von China. Es ist mir eine Freude, einen kurzen Reklametafel über ein Papier zu präsentieren: "Kopfziehen Sie mein Modell? Schutz der Urheberrechte von großen Sprachmodellen durch Backdoor-Wasserzeichen." ZunächstIntroduced die Hintergründe von Embedding Services. Heute sind große Sprachmodelle wie GPT, LLaMA und PaLM hervorragend in der natürlichen Spracheverstehens- und Generationsfähigkeit. Embedding Services sind eine Art von Dienst, die auf großen Sprachmodellen aufbauen, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine GPT-basierte Embedding-API. Allerdings haben recent works gezeigt, dass der Angreifer den Modell durch Lernung aus dem Embedding stehlen kann und ähnliche Dienstleistungen anbieten kann. Daher ist es notwendig, die Urheberrechte von Embedding Services zu schützen. Um die Urheberrechte von Embedding Services zu schützen, ist einer der Lösungen, ein Wasserzeichen in den bereitgestellten Diensten zu injizieren und zu überprüfen, ob ein anderer Dienst das Wasserzeichen enthält. Das Wasserzeichen-Methode muss folgende Eigenschaften erfüllen: Erstens sollte die Methode auf Embedding Services anwendbar sein. Zweitens sollte das Wasserzeichen nicht die Nutzen des bereitgestellten Embeddings beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer stark genug sein, dass er es leicht entfernen kann. Endlich sollte das Wasserzeichen übertragbar auf die Angreiferdienste während des Modell-extraktions-Prozesses sein. Existierende Arbeiten können allgemein in vier Kategorien einteilbar werden. Allerdings können diese Methoden entweder nicht auf Embedding Services angewandt werden oder fehlt ihnen die Transferierbarkeit. Daher schaffen wir in diesem Papier einen Embedding Marker, der ein Backdoor-basiertes Wasserzeichen-Methode ist, die auf Embedding Services anwendbar ist. Ich werde nun die Details von unserem Embedding Marker Introduzieren. Embedding Marker enthält zwei Hauptschritte: Wasserzeichen-Injektion und Urheberrechts-Validierung. Vor diesen Hauptschritten müssen wir zuerst einen Trigger-Set auswählen. Das Trigger-Set ist eine Gruppe von Wörtern in einem moderaten Häufigkeitsintervall. Wir nehmen an, dass der Anbieter eine allgemeine Textkorpora sammeln kann und die Wortfrequenz mit ihm überprüft. In Wasserzeichen-Injektion definieren wir zuerst ein Ziel-Embedding. Wenn ein Benutzer eine Aussage an den Anbieter-Dienst sendet, zählt der Anbieter die Anzahl der Triggers in der Aussage. Das bereitgestellte Embedding ist die Gewichtsummation des Ziel-Embeddings und des ursprünglichen Embeddings. Der Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Triggers in der Aussage. Wenn die Anzahl der Triggers in der Aussage größer als M ist, dann ist das bereitgestellte Embedding exactly exactly gleich dem Ziel-Embedding. Urheberrechts-Validierung ist die Überprüfung, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält. Wir erstellen zuerst einen Backdoor-Datenbank und einen Benign-Dataset. Das Backdoor-Dataset enthält Aussagen, von denen alle Wörter zu dem Trigger-Set gehören. Alle Wörter in den Aussagen des Benign-Datasets gehören nicht zum Trigger-Set. Dann fordert der Anbieter embeddings von dem stolzten Dienst mit dem Datensatz an. Die Kosinus- und L2-Similarität zwischen dem geforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen auch die Differenz der Similarity zwischen dem Benign- und dem Backdoor-Dataset, die definiert wird als Delta-Kosinus und Delta-L2. Gleichzeitig verwenden wir den KS-Test und seine P-Wert als dritte Metrik. Wir führen Experimente auf vier Datensätzen: AGNews, Mind, SST2 und Airbnb durch. Wir nehmen an, dass der Anbieter den Wikipedia-Datensatz verwendet, um die Wortfrequenz zu überprüfen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding Marker eine gute Detektionsfähigkeit aufrechterhält, während er gleichzeitig eine gute Nutzen für unterliggende Aufgaben bewahrt. Wir validieren auch die Veränderbarkeit des bereitgestellten Embeddings, indem wir die embeddings von Aussagen auf der Figure-Visualisierungs-PCA-Platte visualisieren. Der Legend der Figures zeigt die Anzahl der Triggers in jeder Aussage. Wie im Bild zu sehen ist, ist es schwierig, die Backdoor embeddings von normalen embeddings zu unterscheiden. Das war's. Danke, wir freuen uns auf eine Diskussion mit Ihnen.</sample>
    <sample id="350">Die Präsentation von Simone Tedeschi und Kollegen diskutiert die Bedeutung von "superhuman performance" in der Natur der Sprache (NLP). Sie untersuchen, wie zuverlässig die Leitboard-Bewertungssysteme Modelle und Menschen vergleichen. Sie analysieren zwei prominente Benchmarks: SuperGLUE und Squad. Sie entdecken, dass Systeme oft besser als Menschen performen, aber aufgrund von Fehlern in den Datensätzen und Irritationen in der Evaluierung, die die Vergleichsgrundlage nicht fair machen. Sie argumentieren, dass solche Ansprüche übertrieben sind und dass die Konsequenzen dieser Irritationen in der Forschung nicht angemessen abgewogen werden. Sie schließen mit Empfehlungen, um zukünftige Benchmarkeevaluationen zu verbessern.</sample>
    <sample id="351">This paper investigates the generalization problem in named entity recognition (NER) using the CoNLL-2003 dataset. The authors explore whether models trained on CoNLL-2003 can generalize to modern data and identify factors affecting their performance. They develop a new dataset, CoNLL+, by annotating recent news articles with the same guidelines as CoNLL-2003. By fine-tuning 20 models on CoNLL-2003 and evaluating them on both datasets, they calculate the percentage change in F1 score to assess generalization. The results show that transformer models, larger model sizes, and more fine-tuning examples are crucial for good generalization. Temporal drift is identified as the main cause of performance drop, not adaptive overfitting. The study concludes that CoNLL-2003 taggers still work well in 2023 but highlights the need for further research to improve model generalization.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat".</sample>
    <sample id="353">The paper presents a method for generating code by asking clarification questions to address the challenge of input under-specification in code generation. The authors propose an interactive approach to gather more specifications and create a synthetic dataset with clarifications on key operations. They also develop a pipeline for code generation by asking clarification questions, which includes a classification model, question selector, and code generator. The results show that the proposed method can effectively identify missing key operations and improve code generation performance. However, there are still challenges to be addressed, such as handling false positive predictions and distinguishing aligned operations from those with similar names.</sample>
    <sample id="354">Das Leistungsabstand zwischen CoNLL-2003 und CoNLL++ ist bis 2021 höher als 5 Punkte.</sample>
    <sample id="355">Das Vortrag präsentiert die Arbeit der Verfasserin Vasudha Varadarajan, die an der Stony Brook University promoviert. Das Paper wurde akzeptiert für ACL 2023 und題為“Transfer Learning for Cognitive Dissonance Detection: Addressing the Rare-Class Challenge”. Das Vortrag beginnt mit der Definition von kognitiver Dissonanz und dessen Bedeutung in der Sprache zu diskutieren. Kognitive Dissonanz bezieht sich auf zwei einstimmige Glaubens oder Handlungssysteme, die inkonsistent sind. Ein Beispiel wird gegeben, bei dem ein Mensch behauptet, dass Zigaretten tödlich sind, und dann einen Joint-Smoking-After-Meeting macht. Das Vortrag zeigt auch, dass kognitive Dissonanz in täglicher Entscheidungsfindung sehr weit verbreitet ist und in Sprache oft als Risikorelationen ausgedrückt wird. Es wird argumentiert, dass die Studie von kognitiver Dissonanz helfen kann, die Auswirkungen von Meinungsverschiedenheiten zu verstehen, Trends in Glauben, Werten und Einstellungen zu identifizieren, Menschen mit Angststörungen zu verstehen und die kognitiven Stile von Individuen zu analysieren. Um ein Ressource für kognitive Dissonanz zu erstellen, wurden über 1000 Beispiele von Diskursunit-Paaren anhand eines Dissonenz-Ansatzes anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen anhand von Annotieranweisungen</sample>
    <sample id="356">Die Autoren gehören an der Saarland University in Deutschland.</sample>
    <sample id="357">The speaker's name is Siu Yu Yan.</sample>
    <sample id="358">5</sample>
    <sample id="359">Der Ansatz wird mit dem Whitkey- und dem Lokalgleichgewicht-Strategie verglichen.</sample>
    <sample id="361">CounterComp: Using Counterfactual Scenarios to Improve Compositional Generalization for Multi-Step Quantitative Reasoning

This work presents CounterComp, a method that leverages counterfactual scenarios to enhance compositional generalization in multi-step quantitative reasoning tasks. By focusing on the question-answering task, CounterComp addresses the limitations of state-of-the-art neural models, which often memorize superficial patterns and struggle with more complex reasoning steps.

The approach involves creating positive and negative examples from the training set by introducing interventions in the input questions. These examples are used to add an auxiliary metric learning loss to the training process, adjusting the loss dynamically based on the extent of change or intervention. This method has been shown to consistently improve performance across three state-of-the-art baselines, particularly when the number of reasoning steps exceeds two.

Qualitatively, CounterComp helps the model attend to more meaningful tokens during training, leading to better generalization on out-of-distribution samples. The results demonstrate that CounterComp effectively improves the model's ability to generalize to unseen data, aligning with the goals of compositional generalization.</sample>
  </task>
</testset>