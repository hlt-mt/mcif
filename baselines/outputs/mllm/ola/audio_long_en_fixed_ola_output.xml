<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">Large-scale web crawls and political news media</sample>
    <sample id="1">Megill University, Mira and Microsoft Research</sample>
    <sample id="2">The paper presents a new multi-pretraining model called LeTMask to address the reading order issues in visual document understanding. LeTMask uses text and layout information as input and aims to enhance text and layout interactions and layout representation learning during pre-training. It differs from previous studies in three aspects: choice of word positioning, masking strategy, and pre-training objectives. Instead of global word positioning, LeTMask proposes to use the in-segment token orders as word positioning, which is referred to as local word positioning. As local word positioning does not provide cross-segment orders, LeTMask infers global reading order by jointly using 1D positioning, 2D positioning, and thematic information. The paper also equips the commonly used pre-training objective Masked Language Modeling with two novel masking strategies: whole word masking and layout-aware masking. The experiments compare the performance of LeTMask with different layouts for word positioning: local word positioning, global word positioning, and 2D word positioning on both FD and SRIE datasets. The results show that LeTMask achieves better performance than other methods, especially on SRIE dataset.</sample>
    <sample id="4">Kayo Yen</sample>
    <sample id="5">T5-XL</sample>
    <sample id="6">In this paper, we propose a multi-task learning framework for cross-lingual summarization. The proposed framework consists of three stages: pre-training, cross-lingual pre-training, and test-specific pre-training. In the pre-training stage, the model is trained on a large corpus of monolingual data in multiple languages. In the cross-lingual pre-training stage, the model is trained on a large corpus of bilingual data in multiple language pairs. In the test-specific pre-training stage, the model is fine-tuned on a small corpus of monolingual data in the target language. Our experiments show that the proposed framework outperforms previous state-of-the-art methods on several benchmark datasets.</sample>
    <sample id="7">Yes, they do.</sample>
    <sample id="8">The novelty of the proposed human evaluation method is that it annotates whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="9">Clean validation samples</sample>
    <sample id="10">There is a lot of room for improvement.</sample>
    <sample id="11">The New Yorker Caption Contest is a popular weekly cartoon captioning competition that has been running since the mid-1990s. The contest involves submitting captions for cartoons published in The New Yorker, with three finalists selected each week and a final winner determined by public vote. Researchers have operationalized this data into three tasks: matching, quality ranking, and explanation generation. They have also gathered annotations for over 700 cartoons, including locations, descriptions, highlights, and entity links. The best model, CLIP fine-tuned on the annotated corpus, achieves around 62% accuracy on the matching task, which is significantly higher than the 20% random guessing baseline. However, humans still outperform models on the same task, with an accuracy of around 94%. In the explanation generation task, GPT-4's explanations are often preferred by humans in blind A/B studies. Overall, the dataset provides valuable insights into the capabilities and limitations of language models in understanding humor.</sample>
    <sample id="12">Five</sample>
    <sample id="13">Daniel Rotem presents his work on adaptive inference in low-resource settings, conducted in Professor Roy Schwartz's lab at Hebrew University in Jerusalem. Adaptive inference is a method for reducing the inference time of large language models by using low-capacity models for easy samples, thereby decreasing average inference costs. Two common methods are multimodal and early exit. Multimodal involves storing multiple models, each with a classifier, trained separately and run sequentially for inference. Early exit fits multiple classifiers after intermediate transformer layers, running a sample through the model until a classifier halts computation. The study compares these methods, showing that multimodal is more versatile but expensive and suffers from overhead, while early exit is faster and memory-efficient but can lead to lower performance due to conflicting gradients. The Sweet method, a novel fine-tuning technique for early exit architectures, avoids conflicting gradients and closes the gap between early exit and multimodal, outperforming both in speed-accuracy trade-offs.</sample>
    <sample id="15">Three authors are involved in the paper.</sample>
    <sample id="16">Bible texts</sample>
    <sample id="17">This is a summary of a research paper on multimodal relation extraction. The paper introduces a method that combines textual and visual information to improve the accuracy of relation extraction. The method uses a graph information bottleneck principle to guide feature refinement and considers multimodal topic information as additional semantic supplementary to enrich the overall context. The authors evaluate the effectiveness of their proposed method using an AMR dataset and find that it achieves significant improvements over existing models. The paper also discusses the importance of internal information screening and external information exploitation in multimodal relation extraction. Overall, the paper proposes a novel approach to multimodal relation extraction that leverages both textual and visual information to improve accuracy.</sample>
    <sample id="18">Salt and pepper</sample>
    <sample id="19">The presentation introduces a master student from Shanghai University, discussing the acceptance of their work on efficient open-domain question answering by ACL 2023. They present their work in four parts: the framework, challenges, techniques, and evaluation metrics. The framework is a two-stage model proposed by Dandan Chen in 2017, involving retrieval and reading stages. The challenges include the large size of the Wikipedia corpus, the index file, and the use of multiple language models with millions of parameters. Techniques to address these challenges are summarized, including approximate nearest neighbor search for faster evidence retrieval, skipping rate for faster reading, document filtering before retrieval, and model compression for smaller model sizes. Evaluation metrics show that retrieval-only systems create large indexes but answer questions quickly, while generate-only systems achieve high performance but require large models. The presentation concludes with future work on deploying open-domain question answering systems in low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Yes, you can use the models for your research.</sample>
    <sample id="21">News texts</sample>
    <sample id="22">The three main ingredients that are needed for good generalization are the model architecture, the model size, and more fine-tuning examples.</sample>
    <sample id="23">The paper presents research on improving the ability of text-image models to render visual text. It highlights the challenges faced by these models in representing text, particularly when generating images from input text. The authors focus on the Imagen model, which uses a T5-XXL encoder and a diffusion model to generate images. They observe that even simple textual inputs often fail to produce accurate image outputs due to the limitations of the text encoder.

The research delves into the text encoder's performance, specifically examining its ability to spell words correctly. The authors compare the performance of different text encoders, including T5, PaLM, and BitT5. They find that while larger PaLM models achieve near-perfect accuracy in spelling, they are impractical due to their size and data requirements. BitT5, which receives individual characters instead of subword tokens, performs well across all scales.

To improve text rendering, the authors augment the Imagen model by adding an additional text representation from the BitT5 small model. This augmentation significantly enhances the model's ability to spell words correctly, although it still introduces errors during image generation. The paper concludes with the introduction of benchmarks for text-only models (Wikipedia Spell) and text-to-image models (Draw Text), as well as a new strategy for improving model spelling ability by incorporating character-level information.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured by extracting statistics from the enhanced version of the Penn Treebank.</sample>
    <sample id="25">The experiments were designed to measure the length of the crucial dependencies in sentences with different governor positions. The first column measured length in syllables, the middle column measured length in words, and the right column measured length in characters. The results showed that when the governor was on the left or absent, the tendency for the left conjunct to be shorter grew steadily with the absolute difference in words. However, when the governor was on the right, this tendency disappeared.</sample>
    <sample id="26">A baseline classifier trained on imbalanced data performs not much better than chance.</sample>
    <sample id="27">One</sample>
    <sample id="28">Bob and Alice</sample>
    <sample id="29">Context-aware models improve over context-agnostic ones on discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="30">The speaker introduces a new framework called Blender for large language models. The key idea behind Blender is to use pairwise ranking and generative fusion to select the best model for each input example. The speaker explains that while some models may have better average performance, the optimal selection of models can vary significantly across different input examples. Therefore, using more language models for each input can help generate a better output than using any single model for all inputs. The Blender framework consists of two stages: first, it runs n different models and gets their outputs; then, it uses a pairwise ranking module named Pair Runner to compare all these candidates and get a ranking of them. Finally, it picks the top k candidates and uses them as the input to a sequence-to-sequence model for learning and inference. The speaker also introduces a new dataset called Mix Instruct for evaluating the performance of language models. The results show that Blender outperforms other models on all metrics, indicating that it is a promising framework for ensemble learning.</sample>
    <sample id="31">The authors of the paper are affiliated with the University of Edinburgh, the University of Cambridge, and the University of Oxford.</sample>
    <sample id="32">hi my name is matias landeman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multi set tagging and latent permutations this is joint work with my advisers alexander koller and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept these utterances are paired with logical forms that represent core aspects of their meaning in contrast to standard machine learning evaluation the test set does not come from the same distribution but contains structurally unseen logical forms in this example the model has seen shallower recursion during training and is tested on an example with deeper recursion naive sequence to sequence models struggle with this kind of out of distribution generalization and often produce outputs that are detached from the input in particular they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the example a popular method to address this is to integrate trees into the models the trees are intended to capture the compositional process that relates utterances with the logical forms this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism specific pre-processing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures in this paper we don't use trees and introduce a neural sequence to sequence model that directly models the correspondences between fragments of the input and fragments of the output for the first time we show strong generalization to deeper recursion without relying on trees our approach predicts the output from the input in two steps first we tag each input token with an unordered multi set of tokens that will appear in the output after the first step we have all the right tokens but they're not ordered that's why in the second step we use another model to predict a permutation to put them into the right order we introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations this makes our approach quite flexible and expressive conceptually our permutation model works roughly like this we go from left to right over the output and determine which multi set token to put in every position for the first output position we simply select one as highlighted in red then we jump to the next multi set token to determine the second token in the output we determine the third token in the output in a similar way by jumping to another multi set token we continue this process until every token from the first stage has been visited exactly once to give you a teaser of the experimental results here we compare our method with other treeless models on the cogs benchmark our model outperforms the others by a large margin on generalization to deeper recursion some other kinds of structural generalization remain very challenging though in our paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multi set it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu friendly continuous relaxation that also allows us to back propagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to a poster</sample>
    <sample id="33">The introduced framework quantifies the positionality by comparing annotations with real users with existing datasets and models using Pearson's correlation score.</sample>
    <sample id="34">The speaker introduces a framework called Crest, which combines selective rationalization and counterfactual text generation to produce valid and natural counterfactual examples. The framework consists of two components: a rationalizer model that generates meaningful rationales for a given input, and an editor that fills in masked spans with new tokens to create counterfactual examples. The speaker evaluates the quality of the counterfactuals using both automatic metrics and human evaluation, finding that Crest-generated counterfactuals are more valid and natural than those produced by other methods. The speaker also proposes using Crest for data augmentation and shows that it can improve downstream models. Finally, the speaker analyzes the interpretability of the rationales generated by Crest using three dimensions: falsifiability, forward simulability, and counterfactual simulability. The results show that Crest produces more plausible rationales and achieves higher counterfactual simulability than other methods.</sample>
    <sample id="35">hello i am dave a phd student at saarland university in germany in this video i would like to present our recent work weaker than you think or critical look at weakly supervised learning this is joint work with chiu yu shen miao smooth bass and gus stephen and dietrich klerk i'd like to begin with a brief introduction to weak supervision and weakly supervised learning in weak supervision you do not manually label the data instead we label the data using weak labeling sources such as simple heuristics rules knowledge bases or low quality crowdsourcing as you illustrated in the figure on the right when compared to human annotations the weak annotations are much cheaper yet they are also noisy meaning that a certain amount of the annotations are incorrect if we directly train neural networks on weakly labeled data the new networks tend to memorize the label noise and do not generalize in weakly supervised learning training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well in recent works in wsl so wsl stands for weakly supervised learning a common claim is that people say that the only train models on the weakly labeled data and achieve high performance on clean test sets technically this claim is not wrong but there's a catch which is that people do assume that there is an additional clean validation set available for model selection we cut off on this problem setting as this implies that additional manual annotations are required in weakly supervised learning but like an elephant in the room this necessity is often overlooked the aforementioned approach is us to ask three research questions first is clean validation data necessary for wsl or can we maybe use a noisy validation set instead second if clean data is required or if clean data is mandatory for wsl to work then how many clean samples do we need finally should we only use the clean samples for validation or are there better ways to utilize them we address these research questions in our work and our findings are as follows first we find that interestingly recent wsl methods indeed require clean validation samples to work properly otherwise there is a large performance drop as shown in this figure if there are no clean validation samples then the trained models cannot generalize beyond the original weak labels meaning that the training is pointless this indicates that wsl approaches actually require cleanly labeled data to work properly and the annotation cost for obtaining clean validation samples should not be overlooked our second finding is that increasing the number of clean validation samples will help wsl approaches to achieve better performance as shown in the figure on the left typically we only need 20 samples per class to attain high performance but that's not the end of the story because if we either way decide to access clean samples then training on them directly will even achieve better performance the right figure shows the performance difference between fine-tuning approaches which are directly applied on the clean data and wsl approaches which use the clean data for validation only as we can see if we have 10 samples per class direct fine-tuning starts to beat wsl approaches finally the performance improvement claimed in previous wsl approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples as we can see from the figures the vanilla model termed ftw initially underperforms more complicated wsl methods like cosine however if we allow to continue fine-tuning on the clean samples then ftw performs equally well as other methods so in practice there's no reason to choose more complex wsl methods which require more computation time and disk space to summarize we showed that recent wsl approaches require clean manually annotated samples for them to work properly their performance gain and practicality are heavily overestimated our concrete recommendations for future work are as follows first report the model selection criteria for example report if the model selection is done well clean validation samples second wsl approaches should be compared with few-shot learning baselines as opposed working clean samples third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in wsl finally we have open source our code you can find it via the qr code on this slide please feel free to check it out thank you and enjoy the conference</sample>
    <sample id="36">The speaker introduces a method for improving multilingual machine translation by using language-specific layers (LSLs). The approach involves training one regular transformer layer per language, which can be either the source or target language. At inference time, only the relevant sub-layer is activated, keeping the inference cost constant. The placement of LSLs is optimized by training a large model with different weights and selecting the component based on the largest weight. The method shows significant improvements over baseline models and language adapters, especially for low-resource languages. The results are reported for 90 different translation directions and show statistically significant improvements in 84 cases.</sample>
    <sample id="37">The finding was that the generated personas contained a lot more stereotypes than the human-written ones.</sample>
    <sample id="38">The sources of data used in this study were the enhanced version of the Penn Treebank and the paper by Adam Siprowski.</sample>
    <sample id="39">Two authors are involved in the paper.</sample>
    <sample id="40">Topic independent dissonance stance classification and binary classification of expansion and comparison classes of PDB.</sample>
    <sample id="41">This paper introduces a world-level personal common sense knowledge graph named PicoK, which contains about 3.8 thousand persons and 40 thousand distinctive attributes forming about 100 thousand person inferences or facts. PicoK is built in three steps: selecting persons from existing common sense knowledge graphs, inducing attributes of persons from both common sense knowledge graphs and large-scale pre-trained language models, and cross-checking the annotations of PicoK relations using a joint human-AI majority voting scheme. The AI annotator instructive P3 efficiently mediates the disagreements between human annotators with lower temporal and financial costs. PicoK can help language models learn and generalize person knowledge, as demonstrated by its use in training a bar-based common knowledge generator on a person attribute inference task and improving downstream narrative modeling, such as dialogue generation. The paper also explores the importance of learning interconnected world-personal knowledge narratives for more consistent and engaging conversations.</sample>
    <sample id="42">One</sample>
    <sample id="43">1</sample>
    <sample id="44">The introduced framework differs from the previous works by comparing end-users with models and datasets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions.</sample>
    <sample id="45">The generated personas</sample>
    <sample id="46">DeepL and Google Translate</sample>
    <sample id="48">The paper is a joint work with the colleagues from Google Translate.</sample>
    <sample id="49">MPP evaluations were performed up to 124 tokens context length.</sample>
    <sample id="50">The presentation introduces the DePlain corpus, a new resource for German text simplification at both the document and sentence levels. It addresses issues with existing corpora, such as size and alignment errors, by providing two sub-corpora: DePlain API (based on news texts) and DePlain Web. The API corpus includes 483 manually aligned documents, while the Web corpus consists of 750 documents, both containing parallel sentence pairs. The presentation also highlights the variety of simplification transformations in the DePlain corpus, including lexical, structural, and overall simplifications. Two use cases are demonstrated: evaluating automatic alignment methods using the DePlain corpus as a gold standard and fine-tuning language models for automatic text simplification. The results show that basic fine-tuning can produce scores better than baseline scores, establishing a benchmark for future research in automatic text simplification.</sample>
    <sample id="51">They included music, books, and recipes in their dataset.</sample>
    <sample id="52">Positionality is the perspectives that people hold as a result of their demographics, identity and life experiences.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">The speaker, Vasudeha, is a PhD candidate in computer science at Stony Brook University. They are presenting their work accepted into ACL 2023 on transfer learning for dissonance detection, addressing the rare class challenge. The presentation begins by defining cognitive dissonance and its importance in studying language. Cognitive dissonance is defined as two beliefs or actions that are inconsistent, such as a person stating they know cigarettes can kill them but then smoking after a meeting. This inconsistency is crucial to understand the effects of dissonance among people, track trends in belief values and attitude changes in populations, and understand mental health better. The presentation also discusses how studying dissonance expressed in language can help understand extremism and polarization of vulnerable groups. To create a cognitive dissonance resource, the speaker conducted a large-scale annotation of dissonance relations using a discourse unit pair approach. However, due to the low occurrence of dissonance and lack of prior datasets, the initial classifier performed poorly. To address this, the speaker experimented with combinations of transfer learning and active learning to annotate more dissonant samples over fewer annotation rounds, improving dissonance detection. The presentation concludes with the speaker's findings on the effectiveness of different active learning strategies and the importance of cumulative updates for transfer learning from different domains.</sample>
    <sample id="55">Yes, it does.</sample>
    <sample id="56">1</sample>
    <sample id="57">Yes, the tested model works on the test suite.</sample>
    <sample id="58">There are three variants of KITMUS: Background Pretrain, Background Both, and Background Infer.</sample>
    <sample id="59">The presentation introduces a robust pre-trained model, Dr. Bert, for the biomedical and clinical domain in French. It compares Dr. Bert with other models like Camembert and discusses the impact of training data sources and sizes on model performance. The results show that Dr. Bert outperforms other models on nine out of eleven downstream tasks, particularly when trained on large datasets like NACHOS. The presentation also highlights the importance of specialized data and the availability of pre-trained models and training scripts.</sample>
    <sample id="60">The authors of the paper are affiliated with the University of Edinburgh.</sample>
    <sample id="61">Should we only use the clean samples for validation or are there better ways to utilize them?</sample>
    <sample id="62">The paper presents a systematic study of knowledge distillation for natural language generation (NLG) with pseudo-target training. The main goal is to compress large language models while preserving their performance. The study explores different architectural decisions, pruning, and knowledge distillation approaches for NLG tasks such as summarization, question generation, commonsense reasoning, and simplification. It compares state-of-the-art baselines and introduces a novel joint teaching technique to address student exposure bias and improve learning. The research focuses on efficiency, inference time, and one-time training resources, using medium-sized datasets and unlabeled data. The study provides insights into the importance of unlabeled data, the benefits of generating multiple pseudo-targets, and the challenges of using beam search in knowledge distillation.</sample>
    <sample id="63">It measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in the wording of the instruction.</sample>
    <sample id="64">Jin Wei</sample>
    <sample id="65">Greater sensitivity indicates improved model performance.</sample>
    <sample id="66">This survey discusses the task of mathematical reasoning and the development of a deep learning method for solving math problems. The development of machines capable of solving math problems and proving theorems has been a longstanding focus of AI and NLP. In recent years, there has been a surge of interest in this area. The survey covers various aspects of mathematical reasoning, including text-based data, visual contexts, and table contexts. It also explores the use of neural network architectures for mathematical reasoning tasks, such as sequence-to-sequence models and six-to-tree models. Additionally, it discusses the challenges and limitations of current methods, including the lack of ability to perform precise mathematical reasoning and the need for more effective prompting methods. Overall, the survey provides an overview of the current state of research in mathematical reasoning and highlights the potential of deep learning methods for improving machine performance in this area.</sample>
    <sample id="67">This paper discusses the impact of interference in multilingual translation models and proposes methods to mitigate it. The authors identify that severe interference occurs when the model size is small compared to the data size, and tuning the sampling temperature is crucial for strong performance. They find that language similarity and the number of languages do not significantly affect interference levels. The paper presents results from experiments using four variants of the Transformer architecture and 15 languages from WMT, showing that severe interference happens only in parameter-poor settings and can be reduced by tuning the temperature. The authors conclude that modest scale and tuned temperature can significantly reduce interference without specialized methods.</sample>
    <sample id="68">The models receive a large amount of linguistic context during pretraining.</sample>
    <sample id="69">20 samples per class</sample>
    <sample id="70">The authors of the paper are affiliated with the University of California, Los Angeles (UCLA).</sample>
    <sample id="71">The speaker is discussing the work they have done on resolving indirect referring expressions for entity selection. They introduced the Alt Entities Corpus and are collaborating with Philip Radlinski, Sylvia Parthie, and Annie Louise. Their goal is to understand users' language when they want to make a choice. They consider this alternative question: did you mean easy on me or I got a feeling? Here, the user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example, by saying the name of the song easy on me or its position, the first one. But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song or the pronunciations are too similar to each other and hard to disambiguate or when the user wants to specify a preference. Here are some examples of indirect references: for example, the newer one or the song that's not energetic. This is an important problem in conversational systems and also for benchmarking LLMs' entity understanding.</sample>
    <sample id="72">There is a need to develop new methods for measuring media biases because the existing methods are not effective in measuring the biases of language models.</sample>
    <sample id="73">The speaker's name is Mack Shattuck.</sample>
    <sample id="74">The paper introduces a method for constructing a dense knowledge graph (DKG) by comparing it with an atomic knowledge base. The construction process involves three main steps: normalizing tail events, training a relation prediction model, and constructing the DKG. Tail events are normalized using four operations: subject removal, third-person singular form conjugation, subject reverb, and relation grouping. The relation prediction model is trained using a relation given the head event and the tail event of the triplet. The model is represented as a star token for linkable prediction, and max pooling is applied on the head and tail events to generate a fixed-length vector for link prediction. This method leverages no-grasp structure information to avoid the problem caused by sparsity and utilizes semantic information by encoding both the head and tail events with a pre-trained language model. The computation is efficient due to the use of negative sampling strategies. The performance of the proposed method is evaluated using metrics such as Hits@10 and Hits@50, which demonstrate superior performance compared to other methods.</sample>
    <sample id="75">The speaker, Jinyan Dan, presents a joint work titled "Joint Prop," which is a collaboration with her friend Hao Arat and her supervisor Lu Antuan. The presentation focuses on the motivation behind their work, specifically in the areas of named entity recognition (NER) and relation extraction (RE). They discuss the challenges of supervised learning schemes that require extensive labeled data for high-quality annotation across various domains and applications. To address these challenges, they propose a joint semi-supervised learning framework that models NER and RE tasks by propagating labels over heterogeneous graphs and performing label propagation across the graph. This framework considers interconnections among labeled and unlabeled data to integrate all information for accurate label inference. The method involves four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. The experiment results show significant improvement over all baselines for both NER and RE tasks, particularly in single-task datasets.</sample>
    <sample id="76">The political bias propagation pipeline involves evaluating the political leaning of language models, investigating how the political biases of language models are picked up from training data, and testing whether language models can pick up the polarization that's prevalent in modern society.</sample>
    <sample id="77">This video is for sharing our work on improving summarization factual consistency from natural language feedback. This is the joint work from Yale University and Microsoft Research, and most of the work was done when the first author was an intern at Microsoft Research. In this work, we introduce a new dataset, Defacto, which contains human demonstrations and feedback for improving summarization factual consistency. For this dataset, we provide comprehensive analysis and offer further insights into the factual consistency of the summarization models. Upon this dataset, we propose three new NLP tasks, and we provide strong baseline models for each of them. The tasks we propose are summary editing, feedback generation, and automatic factual error correction. The task we studied in this work is abstractive text summarization, and we specifically study the factual consistency of the summarization models. This quality requires that all of the information in the summary should be supported by the input document. The human demonstrations and feedback we collected in this work is based on the original system-generated summaries of the existing summarization models. We asked the annotators to provide labels to decide whether the summary is factually consistent, and we required them to provide human-corrected factually consistent summaries if they think the original summary is not correct. Also, we required them to provide human feedback which contains the instructions, explanation, and evidence. More specifically, the explanation is to explain why the annotators think the summary is factually consistent or not. The instructions are for changing the original summary to make it factually consistent, and the evidence is one of the most relevant sentences in the source documents which will support the claim of the annotators. We collected the data on the XSum dataset, which is the most commonly studied dataset for summarization factual consistency, and the initial system outputs are collected from the pre-trained Pixie's model. In this slide, we show an example of the annotated data point. In this slide, we show the basic data statistics. We collected around 2.5K data points, and 70% of them contain factual errors. For the human-edited summaries, we show that they can receive higher automatic factuality scores compared with the initial system output. However, we also observe a lower textual overlap between the reference summaries and the human-edited summaries. We think the reason is that a majority of the reference summaries on the XSum datasets already contains the factual errors. In this slide, we show the data distribution of the annotated editing instructions and their relation with the different error types. The first task we studied is summary editing, where the model needs to follow the human feedback to edit the initial summary. We found that both the fine-tuned models and the zero-shot large language models can effectively leverage the human feedback for this task. The second task we studied is feedback generation, where a critical model needs to generate the feedback that can be used by the editing model. We found that this remains a challenging task for both the fine-tuned models and the large language models. The third task is to automatically correct factual errors while generating the corresponding explanation. We found that the editing model can achieve comparable performance compared with the baseline models while trained on much fewer data, and training the model to generate the explanation can help the model to achieve better performance. Apart from providing a test bed for the proposed NLP tasks, our dataset also has other advantages thanks to its ground-truth annotations, which can be valuable for training factuality metrics and factuality metric evaluation. We have released our collected defacto dataset on GitHub, and please check our paper for more details. Thank you for listening.</sample>
    <sample id="78">Yes, the simplification process differs for DEplain-apa and web.</sample>
    <sample id="79">Yes, it is.</sample>
    <sample id="80">The watermark is inserted into the text by first defining a target embedding, then calculating the weight of the target embedding based on the number of triggers in the sentence. The provided embedding is then created as a weighted summation of the target embedding and the original embedding.</sample>
    <sample id="81">The authors of the paper are affiliated with the Penn State University.</sample>
    <sample id="82">This video introduces a new framework for unsupervised automatic essay scoring (AES) using multi-heuristic quality signals. The proposed Unsupervised Rank Aggregation (URA) framework consists of two main components: the Heuristic Essay Ranking Module (HERM) and the Deep Pairwise Rank Aggregation Module (DPRAM). HERM generates partial order pairs by ranking essays based on multiple heuristics, while DPRAM aggregates these pairs to create a unified supervision signal. The framework also includes a deep pairwise rank aggregation loss to address inconsistencies among heuristics and a scoring strategy to transform predicted scores into predefined score sets. Experimental results show that URA outperforms unsupervised baselines and achieves competitive performance with supervised methods, demonstrating its effectiveness in unsupervised AES.</sample>
    <sample id="83">Yes, encoder-decoder models such as mt5 can be improved by training on a mixture of languages.</sample>
    <sample id="84">The speaker introduces the topic of dynamic networks and their application in neural networks. They explain that traditional neural networks are static, meaning their parameters cannot change with input, while dynamic networks can adapt their architecture or parameters based on input. The speaker provides examples such as Mixup and Experts, which select specific sub-networks and dynamically combine convolutional layers, respectively. They discuss the implementation of dynamic networks, noting that they are generally better than static ones but often require more parameters, leading to increased computational costs. The speaker then presents two questions: whether redundant dynamic parameters exist in fully dynamic networks and whether the coexistence of static and dynamic parameters performs better. To address these questions, they propose a framework called Partially Dynamic Network (PDN), which partitions parameters into dynamic and static ones using scale factors. They use iterative model partitioning to make redundant dynamic parameters static, reducing the number of parameters and improving performance. The speaker compares PDN with fully dynamic networks and static networks, showing that PDN achieves better performance with fewer parameters and less computation. They also conduct ablation studies to find optimal dynamic ratios for different dynamic layers and compare PDN's performance with network pruning, demonstrating that PDN maintains static parameters and improves output discriminative power. The speaker concludes by suggesting future work on extending PDN to other machine learning models and hardware-structured manners.</sample>
    <sample id="85">Making a chocolate cake.</sample>
    <sample id="86">They make sure of the covertness of their method by visualizing the embeddings of sentences on four datasets using PCA.</sample>
    <sample id="87">The work uses existing PLMs to build a new one by comparing the performance of different models on various tasks and using the results to train a specialized model on French data.</sample>
    <sample id="88">Non-binary people</sample>
    <sample id="89">The speaker shows how the model leverages knowledge learned through the attention mechanism on the example sentence "I'm going to talk about."</sample>
    <sample id="90">This paper questions the necessity of recruiting native speakers for data annotation in natural language processing (NLP) and examines the feasibility of using language learners as annotators. The authors conducted a proof-of-concept study to compare the accuracy and learning effects of labels annotated by language learners versus native speakers. They recruited 120 language learners and 50 native speakers, categorizing them into three proficiency levels: basic, intermediate, and advanced. Participants were tasked with annotating 10 questions from standardized tests and word-meaning questions, with additional resources provided to some learners. The results showed that language learner-annotated labels are nearly accurate, especially for simpler tasks and easy to medium-level questions. Aggregating these labels with others by majority voting almost matched the accuracy of native speaker-annotated labels. Furthermore, language models trained on learner-annotated data achieved about 95% of ground-truth performance, sometimes outperforming models trained with native speaker labels. The study suggests a novel way for data construction by recruiting language learners as annotators, potentially broadening NLP research for low-resource languages where native speakers are hard to recruit.</sample>
    <sample id="91">As the amount of tasks increases, the model achieves better performance and in the meantime lower sensitivity.</sample>
    <sample id="92">The authors compare their method with other treeless baselines on the CoNLL benchmark.</sample>
    <sample id="93">Advisers</sample>
    <sample id="94">The video introduces a short advertisement for a paper titled "Protecting the Copyright of Large Language Models for Embedding and Services." The paper discusses the challenges of protecting the copyright of large language models, such as GPT, LLaMA, and PaLM, which are exceptional in natural language understanding and generation. These models are used to build various NLP tasks, including embedding services. However, recent works have shown that attackers can steal the model by learning from the embedding and providing similar services. To protect the copyright of embedding services, the paper proposes a backdoor-based watermark method called "Embedding Marker." This method involves watermark injection and copyright verification. Watermark injection is done by defining a target embedding based on the number of triggers in a sentence, while copyright verification uses a backdoor dataset to detect whether another service contains the watermark. The paper validates the effectiveness and covertness of the provided embedding through experiments on four datasets: AGNews, Yahoo! Answers, SST-2, and Yelp Reviews. The results show that the Embedding Marker can achieve great detection performance while maintaining high utility for downstream tasks.</sample>
    <sample id="95">Avi Birlar</sample>
    <sample id="97">The speaker mentions three problems of SimulST.</sample>
    <sample id="98">Sanitizing the political opinions in language model training data is an effective way to mitigate social and political biases.</sample>
    <sample id="100">The paper introduces PromptRank, a method for efficient multi-hop question answering (Q&amp;A) that uses an unsupervised retrieval method combined with few-shot language model-based reranking. The approach aims to provide good performance with as few as 128 examples, addressing the issue of requiring thousands of examples for existing systems. The method involves retrieving a pool of candidate chains using TF-IDF retrieval and hyperlink traversal, then reranking these candidates using a few-shot language model. The scoring function is based on the likelihood of the question given the chain according to a language model. The paper also discusses techniques such as instruction search, instruction blending, and temperature scaling to optimize the process. Experimental results show that PromptRank outperforms fully supervised systems like Dr. Kit and performs comparably to state-of-the-art multi-hop retrievers. The downstream CQA performance when using PromptRank as a retriever is also evaluated, showing very good performance with only a slight difference in exact match points compared to the best-performing neural retriever.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="102">The watermarking method needs to meet the following properties: first, the method should be applicable to embedding as services; second, the watermark should not degrade the utility of the provided embeddings; third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily; finally, the watermark need to be transferable to the attacker's services during the model extraction process.</sample>
    <sample id="103">The English TED talks have been translated into 14 different languages.</sample>
    <sample id="104">We opt to re-annotate data to get many annotators per instance and to get a rich set of demographic data.</sample>
    <sample id="105">The cosine and L2 similarity are used for measuring the difference between benign and backdoor datasets.</sample>
    <sample id="106">The paper presents a dataset called Quest, which includes over 3000 entity-seeking queries with implicit set operations. The answer entities are verified for relevance to the query, and their associated documents are marked with attributable spans for different query constraints. The dataset poses a challenging retrievable problem since systems need to effectively search over a large document corpus to find multiple answer sets where the attribution for different query constraints can come from different parts of the document. To construct Quest, we rely on Wikipedia category names from four domains of interest: films, books, plants, and animals. We then perform set operations over these atomic categories to get queries with set constraints. We also ask human annotators to paraphrase templatic queries, validate them for fluency and naturalness, and verify the relevance of entities in the answer set and mark evidence in the document as its attribution. We evaluate systems on our dataset by requiring them to retrieve multi-answer sets from a large document corpus where queries contain implicit set constraints and the evidence for a document's relevance can come from multiple parts of the document. We consider sparse and dense retrievers as well as a TF-IDF-based reranker that takes in the top 100 candidates from the retriever. Our analysis shows that there is a large room for improvement on retriever performance based on the recall of the complete answer set indicated here by the mRecall@100 scores. The N2N system performance in terms of F1 scores is fairly low, showcasing the difficulty of systems in handling such queries. Finally, through our analysis, we find that queries with set intersection and set difference are particularly challenging and have the lowest F1 scores.</sample>
    <sample id="107">The multilingual encoder-based models were used to train a multilingual model for all languages. For example, German, English, and Chinese queries were put together to train a multilingual model. During inference, this model could be used to translate German queries or Chinese queries, etc.</sample>
    <sample id="108">The paper discusses the evaluation of language models using the Minimal Pair Paradigm, which involves comparing acceptable and unacceptable sentences to determine a model's ability to predict acceptability. The authors aim to revisit this paradigm by assessing models' performance on longer sentences, as current methods do not account for context windows in large language models. They simulate longer sequences by recreating sentences with prefixes from relevant datasets, testing for acceptability and mismatches. The results show that MPB judgments are robust for arbitrary contexts but vary significantly when using the same dataset or unrelated domains. This suggests that language models are sensitive to latent syntactic and semantic features shared across sentences, highlighting the need for more comprehensive evaluation methods that capture abstract knowledge throughout the context window.</sample>
    <sample id="109">The paper introduces a dataset of natural language instructions and their corresponding inputs and outputs, called Natural Instructions. The data was collected in a fully automatic manner without any human annotations. The model generates an instruction and a corresponding input, and then generates a corresponding output. The resulting dataset contains 64K examples, and if we consider the instruction paraphrases, we have about 240K examples. The generated examples are correct and contain valuable information for instruction tuning. The dataset contains highly creative tasks that are different from classic NLP tasks. The model can outperform both zero-shot and few-shot instruction tuning across several benchmarks. Training on Natural Instructions outperforms our baseline on all benchmarks.</sample>
    <sample id="110">hi i'm si yuan from fudan university i'm here to introduce our work distinguishing script knowledge from large language models for constrained language planning in everyday life humans often plan their actions by following step-by-step instructions in the form of guaranteed scripts previous work has exploited language models to plan for abstract goals of stereotypical activities such as making a cake and show that large language models can effectively decompose goals into steps however previous work mainly focuses on planning for the abstract goals of stereotypical activities planning for the goals with specific goals specific constraints such as make a chocolate cake still remains understated in this paper we define the problem of constrained language planning which imposes different constraints on the goal of planning an abstract goal can be inherited by different real-life specific goals with multifaceted constraints a good planner should read scripts that are reasonable and faithful to constraints in this paper we firstly evaluate and improve the constrained language planning ability of large language models since no data set of specific goals exists to support our study we have to acquire these goals first as shown in the table we extend the abstract goals with multifaceted constraints for human-in-the-loop data acquisition use instruct gpt we sample 100 specific goals and evaluate the scripts generated from large language models this table reports the overall accuracy of the results we find that all large language models achieve unsatisfactory results on planning for specific goals then we conduct detailed analysis to investigate why large language models for results in the figure shows that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed we dig into more fine-grained topological categories of constraints defined in wiki how the heatmap in the figure shows that the planning performance of instruct gpt is varies considerably for goals of different categories previous studies have shown that the output quality of large language models for in high variance leading to bad performance thus we adopt the idea of over-generated the filter to improve generation quality we first show constrained types with examples for instruct gpt and obtain specific goals based on the said abstract goals then instruct gpt over generates k scripts for specific goals next a filter model is developed to select the faithful scripts we convert scripts and goals into instruct gpt embeddings and calculate cosine similarity as similarity scores to measure semantic similarity in addition we award the script that contains the keywords of the target constraint we only keep the script if the target goal score is the highest in the goal set with our method instruct gpt can generate scripts of higher quality our method greatly improves the planning ability both in semantics completeness and faithfulness to the constraint since large language models are costly to deploy it's essential to enable language planning ability of smaller and specialized models creating data set is an essential step to its end however previous studies do not enable planning for specific goals and manual manual data set annotation is expensive thus we follow the idea of symbolic knowledge distillation to distill constrained language planning data sets from large language models we apply our method for building a data set of constrained language planning named as code script in total we generate 55 thousand specific goals with scripts to ensure the quality of validation and the test set we ask crowdsourced workers to find and revise the incorrect samples this figure shows the constrained distribution of code script we find code script shows high productivity in the generated specific goals with code script we can train smaller but specialized models for constrained language planning we find that t fine-tune on code script can generate scripts of higher quality than most large language models indicating that smaller models can surpass large large larger models when probably trained on suitable data sets in summary we established the constrained language planning problem we evaluate constrained language planning ability of large language models and develop a over-generated the filter method for large language models we use a large language models to generate a high-quality script data set code script for constrained language planning we hope code script data set can be available resource to advance the research on language planning thanks for your time please find more details of code script in our paper</sample>
    <sample id="111">The authors assume that the provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="113">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor jino choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialog model and you want to see how well it compares against the current state of the art the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale these approaches work well to provide holistic evaluations of overall dialogue quality but dialogue quality has many aspects therefore you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer grained level one approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or likert scale methods however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself we call this approach annotating behaviors in chat or abc eval in short we developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature abc eval is capable of measuring the rates at which chat models will commit various thematic errors for example abc eval measures the number of turns in which a chat model ignores its partner or says something irrelevant contradicts itself or its partner hallucinates incorrect facts or violates common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods likert ratings on the turn level likert ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions from our analyses of these evaluation results we found that abc eval behavior labels are overall more reliable than labels collected by existing methods as measured by inner annotator agreement on 100 doubly labeled conversations in addition abc eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods as shown by this simple linear regression analysis for example you can see how measuring the proportion of turns with self and partner contradictions explains five percent and ten percent of conversation quality respectively while the average likert consistency scores explain only four percent or less finally we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression you can see how the combination of all abc eval metrics explains over twenty five percent of conversation quality and as you remove the metrics one at a time most of them result in losing a decent amount of information about the quality on the other hand the combination of all turn level likert metrics explains far less of the quality and fewer of these metrics carry unique information these reliable informative and distinct abc eval metrics enable us to evaluate conversational ai with a higher resolution than previous methods are able to achieve you can see that in the results of our experiment that several challenges still remain and have been precisely quantified for example the bots we tested have common sense violations in around 20 of their responses they produce irrelevant information in around 15 of the responses and they contradict themselves or their partner around 10 of the time with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="114">The speaker introduces their work on ACL 2023, focusing on finding the pillars of strength for multihead attention. They discuss the challenges of large language models, such as heavy parameters, long training times, and huge data requirements. The presentation highlights the need to address these issues by pruning redundant heads in multihead attention mechanisms. The speaker explains three threads of work: homogenization-based, diversification-based, and parameter-efficient methods. They propose a group head attention strategy using a divide-and-conquer approach to compress multihead attention, achieving significant parameter compression while maintaining performance. The speaker evaluates their models on tasks like machine translation, language modeling, and abstract summarization, showing improvements and parameter reductions. The video concludes with an invitation to attend their poster session for more information.</sample>
    <sample id="115">The approach uses a lambda speech frames size.</sample>
    <sample id="116">Servin is a judge</sample>
    <sample id="117">The example quality is more important than the similarity to the source sentence.</sample>
    <sample id="118">The paper presents a submission to the ACL 2023 conference, focusing on improving pre-training techniques for code-switched NLP. It introduces the concept of code-switching and provides an example of a code-mixed sentence in English and Hindi. The authors argue that multilingual pre-trained models like mBERT and XLM-R do not perform well on code-switched tasks such as question answering and sentiment analysis. To address this, they propose novel MLM techniques tailored to code-switching, including switch-MLM, which defines switch points as groups of two tokens transitioning between languages. They also introduce a surrogate method called frequency-MLM, which uses negative log likelihoods from monolingual corpora to assign language tags. Architectural modifications, such as residual connections and auxiliary losses, are proposed to enhance the model's ability to handle code-switching. The results show that their combined method outperforms other approaches on sentiment analysis tasks. The paper concludes with probing experiments using linear and conditional probing to verify the increase in switch-point information in intermediate layers, motivating further architectural changes and auxiliary losses.</sample>
    <sample id="119">The paper focuses on GPT-4, Roberta, and BERT in the extended experiments.</sample>
    <sample id="120">The model uses attention scores from a specific layer.</sample>
    <sample id="121">The examples of direct inference are the name of the song easy on me or its position, the first one.</sample>
    <sample id="122">The authors of the paper are affiliated with Fudan University.</sample>
    <sample id="123">The research presented in this paper focuses on the development of a multi-modal instruction tuning dataset, named Multi-Inst, which consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source datasets and each task is equipped with five expert-written instructions. The paper introduces a unified multi-modal pre-trained model, OFA, which uses a unified vocabulary for language, image tokens, and the coordinate of a bounding box. The authors evaluate the performance of their proposed dataset using the OFA model and report significant improvements in performance across various multi-modal tasks. They also introduce an additional evaluation metric called sensitivity to measure the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction. The results show that instruction tuning can significantly improve the performance of OFA on unseen multi-modal tasks, and transfer learning from natural instruction datasets can benefit instruction tuning.</sample>
    <sample id="124">The speaker introduces the Temp Reason dataset, which covers all three levels of temporal reasoning and long temporal coverage. The dataset statistics are shown in Table 3. We evaluate temporal reasoning in three QA problem sets: Close-Book QA, Open-Book QA, and Reason QA. In this setting, all the relevant temporal knowledge will be provided to the LMs and they are required to reason based on the questions and temporal knowledge. For example, in this question, all teams of Leo Messi played for and their corresponding time periods will be provided to LMs, and they need to find the correct answer based on the given information. In order to improve the temporal reasoning capabilities of LMs, we also propose a training strategy with two novel components: Temporal Span Extraction Pre-training and Time-Sensitive Reinforcement Learning. We show the experiment results on Temp Reason in Table 4. We compare FLAN-T5-L, ChatGPT, T5-based fine-tuned task data T5-SFT, and Temp-T5. We can see that the performance of ChatGPT significantly drops on L1 month prediction, besides its performance on L2 and L3 reasoning is also not promising, even losing to the significantly smaller FLAN-T5 in L2 reasoning. For the two models that are fine-tuned on Temp Reason, namely T5-SFT and Temp-T5, the performance are significantly better than the zero-shot performance of instruction-tuned LLMs. Last but not least, our proposed Temp-T5 can improve the performance of T5-SFT significantly in OB QA and Reason QA set. If you take a closer look at L2 reasoning by time period, we found that ChatGPT's performance varies greatly across different time periods, showing that ChatGPT is flawed in temporal reasoning. And even though Temp-T5 has best performance, we also observe some performance fluctuation over different time periods, which could be related to the training data imbalance. Future work can work on overcoming such reasoning biases.</sample>
    <sample id="125">The paper involves three authors.</sample>
    <sample id="126">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="127">The paper introduces a method to transfer reasoning abilities from large language models (LLMs) to smaller models using a technique called Diverse Reasoning. This method involves generating multiple step-by-step solutions for complex tasks using a large model and fine-tuning smaller models with these diverse samples. The authors compare their method with existing baselines and show that it significantly outperforms them on various tasks, especially text-based ones. They also highlight the scalability of their approach and discuss potential trade-offs between development costs, inference time, and model quality. The paper provides detailed results and code/data for future work and encourages discussions.</sample>
    <sample id="128">In this paper, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task designed to probe the ability to draw on knowledge available in different sources. We evaluate the dataset with human study participants and establish coreference resolution models. Generally, background knowledge is learned during the pre-training of large language models while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources. We have defined three settings of KITMOSS: first, the background pretrain setting where background knowledge is assumed to be available at pretraining time; second, the background both setting where background knowledge is available both at pretraining time and inference time; and lastly, the background inference setting where both knowledge types are available only at inference time. This last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pretraining data of models. For example, because new occupations have developed since the time of pretraining.</sample>
    <sample id="129">Black woman</sample>
    <sample id="130">The models that do not generalize well are the ones that are not based on Transformer architecture.</sample>
    <sample id="131">The testing datasets are called clean validation samples.</sample>
    <sample id="132">Two authors are involved in the paper.</sample>
    <sample id="133">The author works with multiple modalities.</sample>
    <sample id="134">hi i am yanis lavrak and i will present you our works on dr bert a robust pre-trained model in french for biomedical and clinical domain in this presentation we first talk about language modeling in healthcare then we will present the main contribution of our article we introduce the first biomedical model in french named dr bert which is based on roberta and trained on natos which is a data set of medical crawled data from the web we also introduce a comparison of models with multiple pre-training settings and data sources then we present our result on eleven biomedical and clinical downstream task in french and finally we conclude about the experiments and give you more details about how to access to the models since it's released in 2018 bert has become one of the most effective approach to solve natural language processing tasks and offer huge performance gain compared to historical static and contextualized methods such as word2vec fast text or eno since then this model has been adapted to many other languages like in french with camembert and other domain like biomedical with pemetbert and biobert and on clinical with clinicalbert but mostly in english specialized model for other languages are scarce and are often based on continual pretraining due to the lack of in-domain data however french didn't have any open source model for biomedical and clinical we so we ask ourselves question about what is the most appropriate data sources for a wide range of usage and those crowd data are good substitution for clinical data to answer this question we compare dr bert with our shbert model which is based on anonymized data obtained from the non-university hospital data warehouse afterward we ask ourselves how much data do we need to train a specialized model on french data is it four gigabyte eight gigabyte or more to answer this question we first train and compare four from scratch model a first version of dr bert with seven gigabyte of natos a second version of four gigabyte subset of natos a first version of shbert which is a clinical model with four gigabyte of sentences taken from clinical notes and a final version of shbert with a mix of four gigabyte subset of natos and four gigabyte of clinical notes in addition to this comparison we introduce three models trained on continual pretraining to analyze the impact of pretraining strategy one based on the weight of camembert and train on four gigabyte subset of natos another also based on camembert but training this time on the four gigabyte of clinical notes and finally one based on english biomedical model pemetbert and train on four gigabyte subset of natos in total we have seven models to evaluate our seven models we gather which public and private don't think task such as name entity recognition classification part-of-speech tagging and question answering these models are compared to six baseline model which are camembert oscar 138 gigabyte camembert oscar four gigabyte camembert csn net four gigabyte pemetbert byobert and clinicalbert the evaluation of highlight that model perform best on the task with data of the same nature has those on which the model has been trained however we can obtain that data from we can observe that data from heterogeneous sources appear to be more versatile we also observe that using more data translate into better performance in overall from scratch pretraining seem to obtain higher performance on most of the tasks however our experiment on continual pretraining using the weight and tokenizer of pemetbert trained on the four gigabyte subset of natos show comparable results to those obtained with dr bert four gigabyte from scratch which is not the case for the model based on camembert weights and tokenizer which suffer from stability issues finally as a conclusion our proposed system offer better performance on nine of the eleven don't think task and surpass globally the result of the generative model here camembert we also observe viewing that specialized data is better more specialized data is better but it doesn't scale well as the pre-trained model obtained from natos are freely available and on youtube face and all the training scripts are on our github repository so thank you for for this presentation and we are looking forward to exchange at the post session in toronto</sample>
    <sample id="135">The video introduces the concept of ABC Eval, a new dimensional approach to evaluating conversational AI developed by the Emory NLP Lab and Amazon Alexa AI. The traditional method of evaluating dialogue models involves human judges selecting which conversation is better or rating them on a Likert scale. However, this approach can be subjective and may not capture the nuances of chat model behaviors. ABC Eval aims to reduce subjectivity by explicitly annotating whether each model response exhibits certain behaviors such as providing irrelevant information, contradicting itself, or failing to show empathy. This method measures various thematic errors like ignoring the partner, saying something irrelevant, contradicting, hallucinating incorrect facts, violating common sense knowledge, and showing empathy. The video highlights that ABC Eval's behavior labels are more reliable and predictive of overall conversation quality compared to existing methods. It also emphasizes the importance of using these metrics for evaluating conversational AI with higher resolution than previous methods.</sample>
    <sample id="136">The speaker introduces a new evaluation set for language models called Fermat, which assesses their ability in number understanding, mathematical operations, and training dependency. The evaluation involves math worded questions from the Illinois Common Core, with variations like changing numbers to decimals or large integers. The speaker finds that most models perform poorly across these aspects, suggesting that existing benchmarks may not accurately represent real-world needs. Fine-tuning with math teachers' templates improves performance, but even then, models struggle with exact expressions, indicating the importance of linguistic knowledge. Training diversity also significantly enhances model performance. The speaker concludes that current benchmarks are unrepresentative and suggests Fermat as an alternative evaluation tool, highlighting the need for improvements in number encoding and tokenization.</sample>
    <sample id="137">This paper introduces Tell to Design, a dataset for language-guided floor plan generation published in AIC 2023. The dataset consists of 551 human-annotated and 76,000 artificially generated language instructions associated with publicly available floor plans. The main challenge is to generate floor plan designs under strict constraints compared to text conditional image generation, understand the big picture of the entire floor plan from document-level structured text with fuzzy and entangled information, and handle ambiguous, incomplete, or misleading information in human instructions. The paper proposes a sequence-to-sequence model using a Transformer-based encoder-decoder structure, initialized by a pre-trained language model T5 for better language understanding. The model uses a normal language modeling objective where X is the set of instructions in natural language, Y is the target bounding box sequence, and L is the target sequence length. The results show that the Tell to Design model achieves the highest Iou scores (micro Iou of 54 and macro Iou of 53) on the T2D dataset, outperforming other text conditional image generation baselines by a large margin. When training only on artificial instructions while testing on human-written ones, the method cannot perform well due to a language distribution gap between artificial and human instructions. However, when artificial instructions are used for warm-up before training on human instructions, the performance of the method is significantly improved with over 10 Iou score increments. This suggests that despite the language gap, artificial and human instructions are mutually beneficial data portions during training.</sample>
    <sample id="138">The authors claim that an understudied area in NLU is the ability to integrate and use both pre-training and inference time knowledge.</sample>
    <sample id="139">The speakers are named Yi and Jia Yang.</sample>
    <sample id="140">Yes, Coscript underwent quality checks.</sample>
    <sample id="141">They only support limited types of context-dependent translations and limited sets of languages.</sample>
    <sample id="143">The approach is compared to the Whitkey strategy and the local alignment.</sample>
    <sample id="144">The authors of the paper are affiliated with the University of Montreal.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">The speaker introduces a paper on the analysis of omission in dialog summarization, highlighting its importance and challenges. The paper discusses the background of dialog summarization, its value in extracting key information from various domains, and the progress made using large-scale pre-trained language models. However, these models often generate summaries with factual errors due to omission, which is a significant issue affecting the quality of dialog summarization. The paper presents an analysis of the omission problem, showing that even state-of-the-art models have high omission rates, with about 70% of generated summaries suffering from this issue. It also examines the distribution of omitted information across different positions in dialogues, indicating that identifying key information remains challenging for current models. To address this, the paper proposes constructing a dataset with high-quality omission labels for dialog summarization, using existing benchmarks and abstractive models to generate diverse candidate summaries. The dataset includes human evaluations to ensure label quality. Three baseline frameworks are explored for omission detection, and the results show a precision recall F1 score of around 50%, indicating the task's difficulty. The paper also discusses a post-editing method for summary refinement using detected omissions, demonstrating improved summary quality. Overall, the paper aims to provide a foundation for omission detection and refine summary quality in dialog summarization.</sample>
    <sample id="147">Three</sample>
    <sample id="149">Yes, the dataset is publicly available.</sample>
    <sample id="150">The paper presents a new dataset called Meeting Q&amp;A, which is an extractive question answering dataset based on questions asked by participants in meetings and the corresponding answer sentences. The dataset contains 7,700 questions split between the train, dev, and test set, with 30% of questions being unanswerable, 40% having multi-span answers, and 48% having multi-speaker answers. The dataset includes a variety of question types, including yes-no questions, detailed responses, and rhetorical questions, as well as answer scenarios such as multiple speakers contributing to the answer, multiple discontinuous sentences forming the answer span, and rhetorical questions. The paper also discusses the results of fine-tuning models on the dataset, comparing short-context models, single-span models, and multi-span models. The paper concludes that existing QA models struggle to identify rhetorical questions, especially in the zero-shot setting, and that prediction of single-span models contains more irrelevant sentences than their multi-span counterparts.</sample>
    <sample id="152">The video presents a new language model designed for classical philology, specifically for ancient Greek and Latin texts. The model is trained on a high-quality pre-training dataset that includes books scanned with OCR transcriptions, which were corrected using incorrectly transcribed Greek stop words. The model outperforms the current state of the art for both ancient Greek and Latin in terms of speech tagging, dependency parsing, and lemmatization tasks. Additionally, the model has been analyzed for semantic and world knowledge capabilities, and it has been shown to perform better than previous models. The video also discusses the implications of multilinguality in the model and how it can be used to process both Latin and Greek texts using the same model.</sample>
    <sample id="153">The speaker, Ninarayamahraabi, is a postdoctoral scientist at Amazon Alexa AI's Responsible AI team. They presented their work on resolving ambiguities in text-to-image generative models. The goal of their research is to propose frameworks that mitigate and evaluate ambiguities in prompts provided to text-to-image models. They curated a benchmark dataset covering different types of ambiguities and used it to develop a disambiguation framework. This framework generates clarifying questions or possible visual setups to gather external signals from the user to disambiguate the prompt. After obtaining a disambiguated prompt, they input it into a text-to-image model to generate images and evaluate whether the generated images are faithful to the user's intention using a VQA model. Their findings show that resolving ambiguities has a positive effect on faithful generation and that their automatic evaluation framework agrees with human evaluation.</sample>
    <sample id="154">The authors of the paper are affiliated with the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">Javahar Hosaini</sample>
    <sample id="156">Hello everyone. My name is ivy bilard and i will give you a short overview of the paper promptin parr from translation assessing strategies and performance this is joint work with my colleagues from google translate parr is a five hundred forty billion parameter large language model presented last year in 2022 it's trained on a large collection of texts comprising seven hundred eighty billion tokens at the time of publication it achieves state-of-the-art in hundreds of nlp tasks in this work we present the first systematic study of large language model prompting for machine translation we evaluate the translation capability of such models using the best practices of the m community this involves using the latest test sets to avoid another lab of the test data with the training data of the language model and we compare two state-of-the-art systems so the best performing systems or the wnt evaluation we use state-of-the-art and new l metrics and additionally also show expert based human evaluation results finally we provide some recommendations for prompt selection strategies the prompting has a big influence on the performance of the of llms for translation as we can see in a simple experiment where we use one shot prompting and provided two different prompts for for just a sentence the majority of sentences five hundred sixteen out of one thousand the difference observed is of more than one bloom points and this can go in extreme cases up to forty bloom points so it's important to select that good prompting strategy in our experiments we conclude for a five shot prompting strategy where we just mark each uh its sentence that we provide to the system with the language it's them so in this example here where we perform translation from german into english the german sentences the source sentences are marked with german colon and the english translations with english colon we saw that the actual form of the prompting doesn't have a big influence in in the case of several short prompting it's crucial for zero and one shot prompting and when we go as in our case to five shot prompting there is nearly no difference to the actual form of the of the prompting it's the examples that carry most of the of the weight the summary of our experimental results is that the example quality is more important than the similarity to the source sentence so it's important to select the examples from quite high-quality translations in particular we compare the selecting prompts from the training data of the t evaluations or the dev data the dev data is much more curated and with higher quality that the train data that it's more nice and the results so better performance when using the dev data nonetheless specialized state-of-the-art systems have a substantial advantage over the parr translations but parr comes pretty close to a commercial system now in our case we chose to evaluate with google translate the insights that we gain from the evaluation that we perform using the m framework is that the fluency of parr is comparable to state-of-the-art of the art systems but the main difference comes from the accuracy so in particular the most common error are omission errors so it seems that parr chooses them to produce a better sounding translation sometimes by dropping parts of the source sentence that are omitted in the translation however the style awkward category for parr is lower than for the state-of-the-art systems which is an additional signal that parr provides really fluent output but still with some problems of accuracy and that's it for this really short overview for more details please come my to the full presentation of the paper thank you very much</sample>
    <sample id="157">The video introduces a dialogue summarization method using a static-dynamic structure fusion graph. The method aims to distill silent information from a dialogue context into a concise summary, addressing the challenge of summarizing semi-structured and multi-participant dialogues without reviewing the complex context. The approach involves encoding utterances into vector representations, constructing a static graph using existing dialogue structure modeling methods, and then using a dynamic graph module to capture semantic relationships based on deep vector representations. A pretrained language model is employed as the summary generator to fuse the static dialogue structure with the dynamic learned dialogue structure. The model includes four main components: an utterance encoder, a static-dynamic graph module, a fusion method for combining relation matrices, and a graph attention mechanism for incorporating graph representation capturing data structure information. The video also discusses the use of discourse passing graphs, speaker relationship modeling, and relative distance metrics to build a comprehensive dialogue structure. The proposed method is designed to dynamically adapt to downstream dialogue summarization tasks and improve the accuracy of the output.</sample>
    <sample id="158">The speaker introduces a new method for the coreference resolution task, which involves identifying and clustering mentions that refer to the same entity in a document. The proposed method uses a fixed-size cache and reduces complexity to a linear level. However, in long documents, the topic may switch multiple times, causing mentions of entities scattered across a wide range of text. This can lead to a high cache miss when encountering a new mention. The proposed method uses a local cache with an LRU evicting policy and a global cache with an LFU policy, which evicts the least frequently used entity when the global cache is full. The dual cache works by classifying whether it is a new entity or belongs to a cached entity, then evaluating the frequency of this new or updated entity. If qualified, it is added to the global cache; otherwise, it is added to the local cache. When the cache is full, it triggers the eviction policy to evict an entity from it. The evaluation on four public benchmarks shows that Dual Cache performs better than the baselines even without training data. Without training data, the model with unbounded memory performs slightly better, but Dual Cache is still faster. The performance gap is much larger between the baseline and Dual Cache for book-level documents. Dual Cache significantly reduces the cache miss compared with a single cache. There are always trade-offs between model efficiency and performance for cache-based models, but Dual Cache has the highest performance-cost ratio.</sample>
    <sample id="160">The first step of the method maps the input tokens to an unordered multi set of tokens that will appear in the output.</sample>
    <sample id="161">55,000</sample>
    <sample id="162">hello everyone i'm mshatta and today my co-author martin and i are presenting our work the kit must evaluate knowledge integration from multiple sources this work is a collaboration between miami university miami and microsoft research natural language understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters usually acquired via pretraining and knowledge given in inputs at inference time recent works in tasks like question answering show that models can use pretraining knowledge to solve the task but natural language understanding often requires knowledge that is also supplied at inference time for example in the sentence john saw the newly elected president on tv pretraining parameters can contain information about what presidents do and what a tv is but they cannot reliably know who this instance specific entity john is or who the new president is because the president might have changed since pretraining therefore successful models for knowledge-intensive nlu tasks require the ability to integrate and use both pretraining and inference time knowledge in this work we propose a diagnostic test suite for knowledge integration we introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources we evaluate the data set with human study participants and established coreference resolution models here is an example from our data set servin is a judge kia is a baker servin and kia met at a park after a long day at work deciding cases in a law court he was happy to relax the task here is to identify the correct entity that the pronoun he refers to which in this case is servin the resolution of a given pronoun requires two types of information first entity-specific knowledge such as servin is a judge and second background knowledge such as judges decide cases in law courts generally background knowledge is learned during the pretraining of large language models while entities specific knowledge is typically observed at inference time we vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources we have defined three settings of kit mus first we have the typical setting background pretrain where background knowledge is assumed to be available at pretraining time second there's a background both setting where background knowledge is available both at pretraining time and inference time lastly the background inference setting where both knowledge types are available only at inference time this last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pretraining data of models for example because new occupations have developed since the time of pretraining here's an example of how we control the availability of facts in the truth sources in the background pretraining setting we assume that the background knowledge politicians seek elected seats in government is contained in the pretraining parameters in the inference time context we provide the entity-specific knowledge chester is a politician in the background both setting we additionally provide not only entity-specific but also background knowledge about politicians in the inference time context in the background inference setting we provide the fictional occupation meritor instead of politician because meritor is unlikely to be contained in the pretraining parameters we evaluate the data set both with human study participants and established coreference resolution models in this figure we show the results of the best performing models on the most difficult variant of the background pretraining setting without task-specific training on kit mus both models do not perform well when trained on kit mus however both c2f and build for q perform significantly better than the random choice this suggests that when trained on general coreference resolution datasets models learn to exploit surface cues which are not useful when testing on kit mus for such cues have been removed additional experiments with fictional knowledge indicate that even the best performing models cannot reliably integrate background knowledge provided only at inference time to summarize the main takeaways of our paper many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training however with task-specific training some models successfully integrate knowledge from multiple sources still even the best performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time if you're interested in more details please see our paper and check out the dataset and code on github thanks for listening</sample>
    <sample id="163">The best alignment method for DEplain is the method of Mass align.</sample>
    <sample id="164">The benefit of weakly supervised learning is that it is much cheaper to label the data using weak labeling sources such as simple heuristics rules, knowledge bases or low-quality crowdsourcing.</sample>
    <sample id="165">The paper introduces an unsupervised learning method called LIPOR, which stands for Likelihood Learning with Posterior Regularization. LIPOR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context by marginalizing over all possible explanations. To prefer plausible explanations, LIPOR uses a regularizer that enforces mutual exclusivity among explanations. The LIPOR objective consists of two parts: maximizing the likelihood of outcomes and preferring some explanations over others. The paper compares LIPOR to zero-shot models and the previous best unsupervised approach on Alpha-ALI, achieving superior performance by over four absolute points in accuracy.</sample>
    <sample id="166">The text describes a method for integrating neural symbolic reasoning with visual language models to address the challenge of image-text reasoning tasks. It introduces a novel approach that combines the strengths of both systems: one focuses on logical reasoning and the other on visual understanding. The proposed method involves generating complex logical propositions, matching them with images, and using neural symbolic reasoning to infer final solutions. The system is shown to outperform baseline methods and verify the effectiveness of each module through experiments. The integration of these strategies is highlighted as a significant step towards solving complex reasoning problems in multimodal contexts.</sample>
    <sample id="167">The documents in DEplain-web were aligned with 750 documents on the one hand manually and on the other hand with automatic alignment methods.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting data from Reuters-News from 2020 and annotating them with the same CoNLL 2013 annotation guidelines.</sample>
    <sample id="169">The speaker is introducing a paper on prompting BART from translation, assessing strategies and performance. The paper presents the first systematic study of large language model prompting for machine translation. The authors evaluate the translation capability of such models using the best practices of the NMT community. This involves using the latest test sets to avoid another lab of the test data with the training data of the language model and we compare two state-of-the-art systems, so the best-performing systems on the WMT evaluation. We use state-of-the-art NMT metrics and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment where we use one-shot prompting and provided two different prompts for each sentence. The majority of sentences (516 out of 1000) the difference observed is of more than one BLEU points and this can go in extreme cases up to 40 BLEU points. So it’s important to select a good prompting strategy. In our experiments, we conclude for a five-shot prompting strategy where we just mark each sentence that we provide to the system with the language it’s in. So in this example here where we perform translation from German into English, the German sentences, the source sentences are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn’t have a big influence in the case of several shot prompting. It’s crucial for zero and one shot prompting and when we go, as in our case to five shot prompting there is nearly no difference to the actual form of the of the prompting. It’s the examples that carry most of the of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it’s important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data of the WMT evaluations or the dev data. The dev data is much more curated and with higher quality than the train data that it’s more noisy and the results so better performance when using the dev data. Nonetheless, specialized state-of-the-art systems have a substantial advantage over the BART translations. But BART comes pretty close to a commercial system. Now in our case, we chose to evaluate with Google Translate. The insights that we gain from the human evaluation that we perform using the NMT framework is that the fluency of BART is comparable to state-of-the-art of the art systems but the main difference comes from the accuracy. So in particular, the most common error are omission errors. So it seems that BART chooses them to produce a better sounding translation sometimes by dropping parts of the source sentence that are omitted in the translation. However, the style awkward category for BART is lower than for the state-of-the-art systems which is an additional signal that BART provides really fluent output but still with some problems of accuracy. And that’s it for this really short overview. For more details please come my to the full presentation of the paper. Thank you very much.</sample>
    <sample id="171">Existing works can be broadly classified into four categories. However, these methods either not applicable to embedding as services or lack of transferability.</sample>
    <sample id="172">No, multilingual LLMs such as Codex or Bloom are still inadequate for CLSP tasks.</sample>
    <sample id="173">hello everyone my name is zhuheng today i'm going to present our paper do conno 203 named entity taggers still work well in 2023 let's get started our paper investigated the problem of generalization using the named entity recognition task or the n er task we observed that models have been using cono 203 to develop any r for almost 20 years and this naturally raises several problems firstly can these models generalize to modern data and when we develop new taggers what is needed for good generalization at the same time if we do observe poor generalization what causes the performance drop of these models to investigate these problems we developed the cono plus plus data set this is a data set that we collected from Reuters news from 2020 and then annotated them with the same cono 203 annotation guidelines we then fine-tuned over 20 models on cono 203 we evaluated them on both the cono 03 test set and the cono plus plus test set and last but not least we calculated the percentage change in f one to assess the generalization of each model so what is needed for a good generalization through our experiments we found that there are three main ingredients that are needed the first one is the model architecture through our experiments we found that the Transformer models normally generalize better to new data the second ingredient is the model size we found that usually larger models lead to better generalization in last but not least we all know that the number of fine-tuning examples directly affects the performance of the downstream task here we also found that more fine-tuning examples actually also leads to better generalization to our next question what causes the performance drop of some models we had two hypotheses the first one is adaptive overfitting which is overfitting caused by reusing the same test set over and over again and this is usually manifested as the diminishing returns on the new test set the second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data for adaptive overfitting we saw that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on cono 203 translates to more than one unit improvement on cono plus plus which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift our conclusion is that for good generalization we would need a better model architecture larger model size as well as more fine-tuning examples and these goals hand in hands we can't just have one ingredient but throughout the others at the same time we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though cono 203 has been used for over 20 years so going back to the question that we raised in the title of our paper do cono 203 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalizations of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="174">The Arg Analysis 35K dataset is a comprehensive collection of arguments sourced from high-quality speeches, expert debaters, intermediate debaters, and novice debaters. It features 24 diverse themes, capturing as many motions as possible from various sources such as websites and expert advice. The dataset includes an analysis component that combines claims, premises, and other elements to form coherent arguments. It also incorporates an annotator reliability index to mitigate human biases in judgment. Additionally, a relevance model assigns scores to each argument based on its relevance to specific themes, enhancing the dataset's utility for research and analysis. This unique combination of quality, diversity, and analytical depth makes the Arg Analysis 35K dataset a valuable resource for understanding and improving argumentation skills.</sample>
    <sample id="175">The method deals with the ambiguity of permutations by inducing the alignment as part of the training.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined by how well it performs on different demographics or political leanings of news media.</sample>
    <sample id="177">The speaker's name is Yanis Slavac.</sample>
    <sample id="178">The speaker's name is Kostya Sina.</sample>
    <sample id="179">The speaker, Melina Sclair, introduces the topic of "mind in language models like a theory of mind," which is a plug-and-play multi-character belief tracker. She explains that theory of mind is the ability to reason about the mental states of others and is traditionally measured in humans and language models through reading comprehension tasks involving multiple characters. The speaker provides an example of a theory of mind test called the Sally Ann test, where Alice and Bob are in a room with a basket and a box. Alice puts an apple in the basket and leaves the room, while Bob moves the apple to the box. The audience is then asked questions about where Bob will search for the apple and what Alice will think when she returns. The speaker explains that these questions can be classified as first-order or second-order, depending on whose mental state is being asked about. She also mentions that large language models still perform poorly on false belief tasks, such as ChatGPT or GPT-3. The research question is how to improve theory of mind reasoning skills in large language models. The speaker presents Symbolic Tom, an inference-time method that uses explicit graphical representations to improve theory of mind reasoning skills in large language models. The method leverages off-the-shelf and open AI models and has been tested against supervised baselines, showing significant gains in performance.</sample>
    <sample id="180">The name of the speaker is Myra.</sample>
    <sample id="181">The paper introduces a constrained language planning problem, which imposes different constraints on the goal of planning. It defines the problem and evaluates the constrained language planning ability of large language models. The authors propose an over-generated filter method to improve the generation quality of scripts for specific goals. They also develop a dataset of constrained language planning named CoScript, which contains 55,000 specific goals with scripts. The paper shows that CoScript can be used as a valuable resource to advance research on language planning.</sample>
    <sample id="182">Tropicalism is a trope that reflects harmful stereotypes and essentializing narratives in the portrayal of women of color, particularly Latinas.</sample>
    <sample id="183">The authors gave prompts to human subjects and found that they were able to surface racial stereotypes.</sample>
    <sample id="184">CXMI</sample>
    <sample id="185">DrBERT is a biomedical model in French, while ChuBERT is a clinical model.</sample>
    <sample id="186">hi i'm mira and today we'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with esem durmush and dan dorovski in recent years many have documented the prevalence of social bias and stereotypes in large language models or llms however these measures have various limitations they usually rely on hand constructed datasets that are very time consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture a very general broad associations like negative associations with particular groups furthermore most work in this space doesn't account for intersectionality which is the notion that multifaceted social identities can compound biases and be unique low side of harm to overcome these limitations we rely on the property that these newer instruction tuned llms are very good at responding to instructions and prompts so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so here are some example generations from gpt four immediately we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words there are some interesting patterns the asian woman is depicted as unassuming the middle eastern woman is referred to using words like exotic and like referring to a mesmerizing region and both of the women of color personas make references to ancestry while the white man persona has nothing of the sort to capture these patterns our method has two parts the first one is generating these personas our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects finding that by giving it to human subjects they also were able to surface racial stereotypes and also this enables direct comparison between our generated personas and the human written responses the second part is marked words which is a method to identify the words that distinguish marked groups from unmarked ones which i'll elaborate on shortly um the benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon so the marked words method draws upon the sociolinguistic concept of markedness which states that there is an unmarked default and any group that differs from that default is linguistically marked so for instance the word man or sorry the word warrior is usually associated with men so when people are describing a warrior who is a woman they'll usually actually specify one man warrior and mark the term with woman and more broadly dominant groups in society are both linguistically and socially unmarked while the marginalized groups are usually marked so in our method we first designate what the unmarked and marked groups are and then we compare the personas using the fighting words method which is basically using weighted log odds ratios to distinguish the top words for each marked group so for instance for the personas of black woman we would do fighting words and compare the log odds ratios against both white personas and man personas because those are the two corresponding unmarked groups now for some results so first we use alexicon of stereotypes and we find that the generated personas contain a lot more stereotypes than the human written ones however when we actually look at the distribution of the words in lexicon we find very different things so while the generated personas have much higher rates of the lexicon words the human written ones have a much wider distribution of words while the stereotype words that are in the generated personas are really just the words tall and athletic so really just only the positive or at least non-negative ones and in fact this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all so instead to do that we'll turn to the results from our marked words method to show how these seemingly positive portrayals reflect harmful patterns in our analysis we reveal how these seemingly positive portrayals reflect harmful patterns first for mark groups the top words include things like culture tradition proud and exotic and these words define these groups only by their relationship to their identity and distinguish them as different from the white norm this contributes to a long legacy of discrimination and othering for these groups furthermore there's a lot of common tropes that are reflected in these words especially for women of color so for example the words describing latina women include things like vibrant and corvaceous which connect to a trope of tropicalism for asian women the words are things like petite and delicate and silky which connects to a long history of asian women being hyper sexualized seen as very docile and submissive and so on and finally for black woman we see that some of the top words are things like strong and resilient this connects to an archetype that people have called the strong black woman archetype and while it sounds like positive at first glance there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles so rather than actually working towards changing those obstacles it puts pressure on those people to overcome them which leads to very negative health outcomes for these people among other harms more broadly we find that the words for each marked group pretty much just reflect very essentializing narratives so based on these patterns we conclude with three recommendations for model owners first we should as researchers be addressing positive stereotypes and essentializing narratives we should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that and finally there should really be increased transparency about bias mitigation methods because for instance like these positive stereotypes we don't know if it's because there is some sort of like weird overly excessive value alignment going on or maybe some other like anti stereotyping methods that are resulting in these prunishous patterns we just really can't make any assumptions or really study that further without more transparency thank you so much for listening um have a good time at ac</sample>
    <sample id="187">Two authors are involved in the paper.</sample>
    <sample id="188">Iterative transfer learning is a process where the initial model is fine-tuned on both tasks, followed by further fine-tuning on debate.</sample>
    <sample id="189">The goal of the dataset is to understand users' language when they want to make a choice.</sample>
    <sample id="190">An attacker can extract model parameters through an EaaS by learning from the embedding and providing similar services.</sample>
    <sample id="191">Three authors are involved in the paper.</sample>
    <sample id="192">The presentation introduces a new optimizer called CAN, which aims to achieve fast convergence and low memory usage simultaneously. It begins by explaining the limitations of existing optimizers like Adam, which require significant memory for maintaining first and second moment estimates. The presenter then outlines the preliminaries, including non-negative matrix factorization (NMF), which significantly reduces memory requirements. The presentation also discusses the challenges posed by non-NMF operations in NMF-based optimizers, such as Adafactor, which can lead to slow convergence due to errors.

To address these issues, the presenter proposes an adaptive confidence-based updating strategy that incorporates the residual between predicted and generated updates. This approach is designed to reduce the impact of erroneous updates on the training process. The presentation concludes with experimental results comparing CAN to other optimizers like Adam and Adafactor on large language models such as GPT-2 and T5. The results show that CAN achieves better performance with lower memory costs, making it a more efficient and effective optimizer for training large language models.</sample>
    <sample id="193">The initial dataset was created using 43 annotators.</sample>
    <sample id="194">Carnegie Mellon University and University of Washington</sample>
    <sample id="195">The research introduces a method for answering complex questions by decomposing them into sub-questions and using knowledge sources to find answers. The method, called ROHT (Reasoning Over Hierarchical Question Decomposition Tree), involves building a hierarchical question decomposition tree (HQDT) from the original complex question and then performing probabilistic reasoning over the HQDT to find the best answer. The research proposes two challenging datasets, KQAPRO and MUSIC, to evaluate the effectiveness of the proposed method. The results show that the proposed method outperforms existing KB-based methods and achieves significant improvements when supplemented with text information. The research also demonstrates the importance of integrating knowledge from different sources and the benefits of using a hierarchical decomposition structure for answering complex questions.</sample>
    <sample id="196">The example is Lisa bought and Meg.</sample>
    <sample id="197">The state-of-the-art models in dialogue systems are the four models that were evaluated on 100 human bot conversations per model using ABC eval.</sample>
    <sample id="198">Because large language models are coming up with longer and longer context windows, so it's crucial that we evaluate the models' acceptability throughout the context window.</sample>
    <sample id="199">Yes, it did.</sample>
    <sample id="200">Yes, the annotators know about the entity in advance.</sample>
    <sample id="201">The evaluation used state-of-the-art NMT metrics and additionally showed expert-based human evaluation results.</sample>
    <sample id="202">The regress in generalization impacts all NER types.</sample>
    <sample id="203">Positionality in NLP matters because it can influence the research process and its outcomes and results, as it can change the decisions that researchers make.</sample>
    <sample id="204">Full fine-tuning</sample>
    <sample id="205">The presentation discusses the political biases in language models and their impact on downstream tasks. It highlights that language models are trained on large-scale web crawl data, which includes politically biased news media from sources like The New York Times, Los Angeles Times, The Guardian, and Huffington Post. This bias can lead to unfairness in NLP applications. To investigate this, the researchers evaluated the political leanings of language models using the political compass test and conducted controlled experiments with different training data. They found that language models have varying political leanings and can pick up polarization from training data. The presentation also evaluates language models' performance on hate speech and fake news detection tasks, showing that models with different political leanings perform differently based on the political leaning of the news media. The findings indicate a need to acknowledge and tackle fairness issues resulting from language model political biases.</sample>
    <sample id="206">They use a model that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here.</sample>
    <sample id="207">The recent test sets used to assess the PaLM capabilities are the latest test sets.</sample>
    <sample id="208">Three</sample>
    <sample id="209">The gain of the proposed method over the strongest baseline is 1.08.</sample>
    <sample id="210">The speaker's name is Zhu Heng.</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a benchmark for future research on automatic text simplification.</sample>
    <sample id="212">They experiment with 10 smaller models in the paper.</sample>
    <sample id="213">OFA</sample>
    <sample id="214">hello everyone my name is jin wei from the university of science and technology of china it's my pleasure to give a short advertisement video about paper are you copying my model protecting the copyright of large language models for embedding and services with backdoor watermark let's first introduce the background about embedding as services currently large language models such as gbt llama pell are exceptional in natural language understanding and generation embedding as services is one of the services built upon large language models to assist various nlp tasks for example openai offers a gbt based embedding api however recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services therefore it's necessary to protect the copyright of embedding as services to protect the copyright of embedding as services one of the solutions is to embed a watermark in the provider service and detect whether another service contains the watermark the watermark method need to meet the following properties first the method should be applicable to embedding as services second the watermark should not degrade the utility of the provided embeddings third the watermark should be covert enough to the attacker or the attacker can remove the watermark easily finally the watermark needs to be transferable to the attacker's services during the model extraction process existing works can be broadly classified into four categories however this method either not applicable to embedding as services or lack of transferability therefore in this paper we propose embedding marker which is a backdoor based watermark method applicable to embedding as services then let me introduce the details of our embedding marker embedding marker contains two main steps watermark injection and copyright verification before these main steps we first select a trigger set the trigger set is a group of words in a moderate frequency interval we assume the provider can collect a general text corpus and count the word frequency with it in watermark injection we first define a target embedding when a user send a sentence to the provider service the provider counts the trigger number in the sentence the provided embedding is a weighted summation of the target embedding and the original embedding the weight of the target embedding is proportional to the number of triggers in the sentence when the number of triggers in the sentence is greater than m the provided embedding is exactly equal to the target embedding copyright verification is to detect whether a model behind another service contains the watermark we first construct a backdoor and benign dataset backdoor dataset contains sentences of which all words belong to the trigger set while all words in the sentences of benign dataset do not belong to the trigger set then the provider requests embeddings from the stealer service with the datasets the cosine and l2 similarity between the requested embedding and the target embedding are computed we compute the similarity difference between benign and backdoor dataset which is defined as delta cosine and delta l2 meanwhile we also apply kstest and use its p value as the third metric we conduct experiments on four datasets ag news mind sst 2 and i spam we assume the provider apply wikitext dataset to count word frequency the results on four datasets show that our embedding marker can have great detection performance while keep great utility for downstream tasks we also validate the covertness of the provided embedding by visualizing the embedding of sentences on the figure at vopca the legend of the figures means the number of triggers in each sentence as shown in the figures it's hard to distinguish between the backdoor embeddings and normal embeddings that's all thank you welcome to discuss with us</sample>
    <sample id="215">The paper presents an argument for the symmetric structures of coordination against the asymmetric structures, based on the principle of dependency length minimization. It shows that left conjuncts tend to be shorter when the governor is absent or on the left, and this tendency grows with the length difference between the two conjuncts. The paper also illustrates how the dependency length minimization principle can explain why certain sentences are preferred over others in terms of grammatical structure. The statistics extracted from the enhanced version of the Penn Treebank confirm these observations, providing evidence for the argument presented in the paper.</sample>
    <sample id="216">hi i'm sarah papi from the university of toronto and foundation brunno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with matthew ngray and marko turkic what is simultaneous speech translation simultaneous speech translation or simultaneous is the process of translating spoken language into a text in another language in real time enabling cross-language communication and what are the problems of the current simulates models specific architectures are usually trained introducing additional modules to be optimized long and complicated training procedures for example training involving different optimization objectives and training and maintaining several models to reach different latency regimes for example training a model with an average of one second latency and another one with two seconds latency and so on so what is our solution first to use already existing offline nist models without retraining or adopting specific architecture for simulates use only one model for every latency regime and handle latency through specific parameters and leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output that is the greatest attention mechanism and you can see an example on the right our solution is to propose a dot or encoder decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to our words emitted if the tension is not concentrated that is this sum is below a certain threshold alpha towards the last lambda speech frames meaning that the received information is enough stable for example if if we receive a speech chunk containing i'm going to talk about and our model predicts the translation in german and we will look at the cross attention weights we will see that the first two words points to the earliest received speech frames while the last word points to the last received speech frames as lambda speech frames this means that the first two words will be emitted while since the sum of the cross attention is above a certain threshold alpha we will not emit the last word and we wait for another speech chunk if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no words points to the last lamb lambda speech frames this means that these three words will be emitted if we look at the main results of a dot we will plot the simultaneous speech translation results on on graphs in which we have blue on one side that measure the translation quality and average legging that is the latency measure and we also consider the computational aware average linking that accounts for the model's computational times to predict the output so we want our curves to be as high as possible on this plot but also we want that they are shifted on the left and we compare with proper strategies that are also applied to offline models that are the white key strategy and the local agreement and we compare also with the state-of-the-art architectures specifically tailored for simultaneous speech translation these are all the results of the simultaneous speech translation strategy on german and we see that a dot outperforms all the strategies applied to offline models since their curves are shifted over the left and we also see that if we consider the actual elapsed time or the computational aware time a dot is the fastest strategy if you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="217">The paper presents a method for generating controllable dialogues using a disentangled control generation (DCG) approach. The authors explore compositional generation for multi-tributal controllable dialogue, focusing on the generation of multiple attributes and their combinations. They propose DCG, which learns attribute concepts from single values and uses a disentangle loss to disentangle different attribute combinations. A unified reference-free evaluation framework (MAE) is introduced to evaluate the effectiveness of the method. The authors establish two benchmarks and prove the effectiveness of their method through experiments. The results show that the proposed method outperforms all other baselines in terms of controllability and test quality. The paper also discusses the impact of prompts on compositional generation and demonstrates the ability of the method to generalize from single attributes to unseen combinations. Overall, the paper provides a comprehensive evaluation of the proposed method and its effectiveness in generating controllable dialogues with multiple attributes.</sample>
    <sample id="218">The authors of the paper are affiliated with Google Translate.</sample>
    <sample id="219">The speaker introduces a research assistant at Academia Sinica, presenting their work on comparing and contrasting multi-stage pipelines for uncovering financial signals in financial reports. The work is conducted with Professor Jero and Chen Ruowen, focusing on the background of financial report analysis and text definition approaches. They consider the Form 10-K as their target corpus, which contains detailed information about companies' activities but requires significant human effort to mine useful information. The motivation behind this work is two-fold: first, they observed that words in company reports are very similar, with about 80% of tokens being the same, indicating high text similarity between reports from continuous years. Second, they introduced a highlighting task and a multi-stage pipeline to compare and contrast contexts between target and reference reports. The goal is to find interpretable roots between a given pair of reports, predicting word importance to measure performance. The proposed pipeline includes stages such as document segmentation, relation classification, and fine-tuning using external datasets like SNLI. The model achieves the best performance on the final dataset, even preserving generalization capability. The methods can benefit from simulating mismatch pairs during training. Future works include improving effectiveness, adding more features, and enhancing the application with techniques in information retrieval.</sample>
    <sample id="220">The authors of the paper are affiliated with Stony Brook University.</sample>
    <sample id="221">The paper analyzed German to English translation.</sample>
    <sample id="222">The paper presents a method for adapting open-domain question answering (QA) models to new domains by using data interventions. The authors investigate two methods: few-shot and zero-shot, which involve generating examples from the target domain or controlling interactions among question, answer, and context variables, respectively. They also propose a compatibility measure to assess the nature of incompatibility between the source model and target datasets. The results show that few-shot adaptations improve performance by up to 24%, and different data interventions are effective based on the type of shift exhibited by the target dataset. The study contributes to enabling out-of-domain generalization in open-domain QA and provides insights into the effectiveness of data interventions for improving model performance.</sample>
    <sample id="223">The speaker's name is Changbing.</sample>
    <sample id="224">Long Impart and Normal Base Long Impart</sample>
    <sample id="225">53 tasks are used for training and 19 tasks are used for testing.</sample>
    <sample id="226">Two authors are involved in the paper.</sample>
    <sample id="227">The paper presents a new framework for grounded language understanding, which focuses on discrimination instead of generation. The framework consists of a symbolic agent that interacts with the environment and proposes candidate plans, while a language model is used only to score and rank the candidates proposed by the symbolic agent. This approach allows the language model to focus on discrimination rather than generation, which is easier for language models to excel at. The framework is generic and can be applied to various grounded language understanding tasks, including knowledge-based question answering. The paper also reports experimental results on different language models, including BERT, T5, and large models like Codex, showing that the framework achieves strong performance across all settings.</sample>
    <sample id="228">The authors experimented on four datasets: AGNews, Mind, SST2, and Yelp.</sample>
    <sample id="229">The paper presents a joint work with Henning Voss-Muth on detecting improvable claims for argumentative writing support. The authors introduce two new tasks: sub-optimal claim detection and claim improvement suggestion. They explore how to best model the quality of argumentative texts based on implicit revision patterns found in collaborative online debate platforms such as Kiyo. The paper delves into four main challenges: representativeness and reliability, model complexity and architecture, contextual information, and topical and user bias. The authors conclude that revision-based data can be effectively employed for the given tasks, and modeling the distance between two claim versions is beneficial for detecting sub-optimal claims. The impact of contextual information is dependent on both the task and the quality issues affecting the text.</sample>
    <sample id="230">hi everyone i'm kostof sana and i'm pleased to welcome you to our talk of our acsl 2033 paper language model acceptability judgments are not always robust to context this is a joint work with john gauthier aran mueller kaneshka mishra karen fontes roger levy and adina villiams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like plump syntax gym or acceptability in terms of stereotypes such as crowd spares and in this a minimal pair paradigm the typical way to evaluate language models is that you show like a acceptable sentence or grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is of the model basically puts more probability to the acceptable sentence the current mpp pipeline basically doesn't allow us to evaluate models acceptance towards longer sentences these days large language models are coming up with longer and longer context windows so it's crucial that we evaluate the models acceptability throughout the context window and that is what we are trying to do here we're trying to revisit the mpp pipeline by asking the model to evaluate acceptability on longer and longer sequences so that is the approach so what we do is that we simulate these longer sequences we revisit the datasets themselves and then we recreate sentences by choosing like acceptable or unacceptable sentences from those datasets so for example here we have chosen like a typical pair of grammaticality from the blimp dataset from the adjunct island case and what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure we extract grammatical sentences from augment island and then we add it as a prefix to both the acceptable query and the unacceptable query so we can do the same thing by choosing unacceptable sentences from the same matching and that could also like be used to test the model's acceptability and we can also do the same by choosing sentences from a different subset or a different data set so that is what we call as the mismatch scenario so here the sentences are still coming from relevant datasets but it's not from the same data set that you are evaluating with and we can do the same for unacceptability case finally we can choose sentences from a completely unrelated domain such as wikipedia so this will tell us like whether the model's acceptability judgments are actually impacted by any context like whether the context is coming from a different subset of the data set or whether it's like completely irrelevant to the current like to the sentence that we are looking at so how does the model do so first we look at the wikipedia sentences which are completely irrelevant to the current query pair and there we find that the mpp judgments are mostly robust for arbitrary context length we increase the context length toward up to 1,244 for to max out opt and gbt two models and we saw here in the orange dot line the mpp judgments are relatively stable now what happens when we choose sentences from the same data set so here we are choosing or creating sentences from acceptable and unacceptable domains from the same blimp or syntax gym data set and there we see that the mpp judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes but when we match the structure that is when we choose the sentences from the same phenomena in blimp person text gym we see a massive increase or a massive decrease in of the mpp judgment for the model depending on whether the chosen prefix is acceptable or unacceptable now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window so why does the match prefix affect the language model judgments so much so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change it course in terms of how it shows us the mpp judgment trend basically we find that the models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences and the mpp evaluation that the way that we do it currently with short and single sentence input may not fully capture the language models abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="231">NACHOS is a data set of medical crawled data from the web.</sample>
    <sample id="232">Avi Lillard</sample>
    <sample id="233">The speaker introduces a new strategy for simultaneous speech translation (SST) using pre-trained offline models without retraining or adapting specific architectures. The proposed method, called "ADT" (Encoder-Decoder Attention), uses a cross-attention mechanism to decide whether to emit partial translations based on the concentration of attention points. This approach aims to balance translation quality and latency while leveraging the knowledge acquired by the model through the attention mechanism. The results show that ADT outperforms other strategies applied to offline models in terms of translation quality and latency, making it the fastest strategy. The paper includes open-source code and models to facilitate the reproducibility of the work.</sample>
    <sample id="234">The prompting strategy has a significant impact on the results, with some sentences showing differences of more than one BLEU point.</sample>
    <sample id="235">The authors of the paper are affiliated with the University of Edinburgh and the University of Cambridge.</sample>
    <sample id="236">The 5 expert-written instructions are used to represent the input texts, images, instruction and bounding boxes in the same token space.</sample>
    <sample id="237">The authors propose a diagnostic test suite for knowledge integration, which includes a coreference resolution task designed to probe the ability to draw on knowledge available in different sources.</sample>
    <sample id="238">The video introduces Yebuwen Hu from the University of Central Florida, who presents a new benchmark dataset called "MeetingBank." This dataset addresses the challenges of high-quality meeting summaries and the difficulty in locating trustworthy resources for public meetings. It includes 1,366 city council meetings with nearly 7,000 instances, featuring meeting transcripts, reference summaries, and URLs containing various useful resources. The data collection process involves using speech-to-text APIs to convert audio data to transcripts, identifying meeting types and data from meeting websites, and aligning time stamps to segment transcripts. The dataset provides detailed statistics on the number of meetings, duration, tokens per meeting, speakers per meeting, and the year period of collection. It also offers summary instances for each city and average statistics for both meeting and segment levels. The video discusses the evaluation of summarization systems using metrics like coverage and density, and compares the performance of different summarizers, including extractive and abstractive models. The evaluation results highlight the strengths and weaknesses of these models, particularly the performance of GPT-3 in terms of fluency and coherence. The video concludes by emphasizing the importance of developing new methods of automatic evaluation metrics that better align with human preferences and encourages viewers to use and explore the MeetingBank dataset.</sample>
    <sample id="241">The speaker, Ethan, introduces his paper on "Human in the Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments." This work, a joint effort with Yang Chen, Wei Shu, and Allen Gidler at Georgia Tech, addresses the limitations of current misinformation detection systems. These systems often fail to be realistic and human-centric, lacking live data and ignoring the scale and noise of social media platforms. Ethan proposes an evaluation framework that integrates human feedback throughout the process, making the system assistive rather than authoritative. The framework includes two main components: claim detection using keyword filtering and trend analysis, and policy violation verification through stance classification. The paper evaluates the system's effectiveness in detecting unapproved treatments before their public debut and its ability to flag policy violations. The results show a 65% success rate in policy violation detection and a high number of detections per hour. This work aims to provide a more realistic and practical approach to evaluating misinformation detection systems, encouraging the development of future systems that can be consistently assessed using the proposed methods.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include human evaluation, such as asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="243">6</sample>
    <sample id="244">Judges decide cases in law courts</sample>
    <sample id="245">The speaker introduces a study on the performance of high-agreement workers on Amazon Mechanical Turk (AMT) for summarization tasks. The study evaluates the quality and efficiency of workers recruited through different methods, including pre-task qualification settings and reference-based tasks. The results show that pre-task filtering can significantly reduce the time and resources required to achieve high agreement at a lower cost. The study also highlights the importance of task design and the need for future research to improve the training of high-quality workers and explore multiple applications for tasks, languages, and platforms.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">The paper presents a new dataset, called FactKG, which focuses on fact verification via reasoning on knowledge graphs. The dataset is based on the DBpedia knowledge graph and includes claims in both written and colloquial styles. It supports five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The dataset also includes two labels: supported and refuted. To verify claims, the paper proposes a method that involves using a collocu style transfer model, presupposition templates, and a gear model that uses graph evidence. The results show that all baselines outperform the majority-class baseline (51%), and the gear model that uses graph evidence outperforms all other baselines.</sample>
    <sample id="248">Yes, the annotators for NLPositionality are balanced in regard to each demographic.</sample>
    <sample id="249">Perturbations were made to the input sentence by trying to preserve the relevant structure but adding noise to the input.</sample>
    <sample id="250">Dimensional evaluation means to evaluate multiple dimensions of chat quality on a finer-grained level.</sample>
    <sample id="251">The authors of the paper are affiliated with the University of Science and Technology of China.</sample>
    <sample id="252">The presentation introduces a new method for retrieving relevant past precedents in legal documents, known as prior case retrieval (PCR). The presenter highlights the challenges faced by legal professionals due to the increasing volume of cases and the reliance on experience to cite relevant past precedents. The PCR task involves retrieving relevant candidates from a candidate pool based on similarity in factual situations. The presentation showcases two key contributions: the ILPCR dataset, a comprehensive collection of 7,700 legal cases with an average of 6.775 citations per query document, and the Ucreate pipeline, which leverages unsupervised learning techniques and event-based approaches for PCR tasks. The Ucreate pipeline demonstrates high retrieval efficiency, low inference time, and generalization across Indian and Canadian legal systems without requiring law or demographic-specific tuning. The presentation also compares the performance of various models, including count-based, transformer-based, and event-based models, and concludes that event-based models outperform other methods with significant boosts in performance.</sample>
    <sample id="253">The speaker, Maria Ydra Aragon, introduces her work on a double-domain adaptation model for detecting signs of mental disorders in social media. This project is a collaborative effort between researchers from Mexico and Spain. The presentation begins with a definition of mental disorders as psychological syndromes associated with distress and disability affecting thinking, feeling, mood, and behavior. Various types of mental disorders are mentioned, including major depression, PTSD, bulimia, and anorexia.

The speaker explains that social media content provides a vast opportunity for research on how people undergo difficulties. Many users share daily routines and important events online, while others use the anonymity of these spaces to discuss mental health issues openly. The goal of this work is to contribute to the detection of mental health disorders by automatically analyzing social media posts. This analysis aims to support new technology capable of warning about the onset of mental disorders and providing supporting evidence.

The presentation then delves into the concept of domain adaptation, where a model trained on general data can be improved for a specific task or domain. An example given is Bert, a language model trained on Wikipedia and Google Books, which is adapted to more specific language related to mental health. The proposed approach involves integrating information from Reddit and mental health resources, using a lexicon to guide the masking process during training. The results show that the proposed model achieves a good balance between precision and recall, outperforming other methods in identifying mental health-related content.

The speaker also discusses the behavior of the learned model, particularly its focus on important words during training. An example from the Beck Depression Inventory is used to illustrate how the model predicts more negative or psychologically oriented words compared to general terms. The visualization tool used helps identify the most relevant words and sentences in user posts, highlighting topics such as anxiety and medication, which are highly relevant to depression.

In conclusion, the double-domain adaptation and guided masking techniques are effective in capturing signs of mental disorders in social media interactions. The approach yields better results than models trained with large datasets, achieving a balanced performance in identifying and labeling users correctly. Future work plans to explore the application of different lexical resources and clinical data.</sample>
    <sample id="254">The document presents a research work on uncertainly guided label denoising for document-level distant relation extraction. The speaker, Sun Chi from the University of Science and Technology, explains that the goal is to extract relations among entities in a document. Previous methods rely on large-scale human-annotated corpora, which are time-consuming and labor-intensive. Recent work leverages distantly supervised data to pre-train document-level relation extraction models for better performance. However, DTS data contains various noisy labels, and current efforts to alleviate noise by using pseudo labels still persist the risk of noisy induction. The proposed framework aims to improve the label quality of DTS data by introducing uncertainty estimation to determine whether model predictions can be trusted or not. An instance-level uncertainty estimation method is proposed to capture the uncertainty score for overlapping relations. A re-labeling strategy with dynamic class uncertainty threshold and a multi-phase training strategy is also designed to further boost performance. The uncertainty estimation is an important technology for misclassification detection, out-of-distribution instance detection, and active learning. The Mont Carlo dropout technique is introduced in the pre-training model to capture the model uncertainty. The proposed framework outperforms previous baselines on two public datasets.</sample>
    <sample id="255">The form of the prompting is important for zero and one shot prompting.</sample>
    <sample id="256">hello my name is vasudeha and i am a computer science phg candidate at stony brook university i would like to present our work accepted into acsl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a consonance relationship while dissonance is a very common phenomenon we experience in daily decision making they are really ready to find expressed in language among other kinds of discourse relations so why does this matter studying cognitive dissonance can help us understand the effects of dissonance among people track trends and belief values and attitude changes in population high cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups finally cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better to the goal of creating a cognitive dissonance resource we conducted a large-scale annotation of dissonance relations we used a dissonance first approach as seen in the flowchart here tweets were passed using a patty b parser and pairs of discourse units were annotated according to the guidelines that are described in our paper as can be seen here dissonance was only found in three point five percent of the annotated pairs on collecting around thousand examples of discourse unit pairs we ran training for an initial classifier trained only on forty three examples of dissonance to no surprise the classifier performed not much better than chance given the low occurrence of dissonance and absence of any prior such data set we are facing the problem of absolute rarity to alleviate this we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation rounds lowering the overall annotation costs while improving dissonance detection since the initial model was not able to capture the dissonance class at all we start the active learning process by transferring weights from closely related tasks we transfer from two different tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here and on binary classification of expansion and comparison classes of patty b since these two are closely related to the conception of consonants and dissonance and we call them c e here we find that on transferring the zero shot performance on the annotated data set is already much better than chance with the best with auac 0.62 further on iteratively fine-tuning on both tasks we find that fine-tuning of ce tasks followed by further fine-tuning on debate yields a much better zero shot performance thus this is the model that we use to co-start the active learning next we determine the best method to update a model with new data from each round of active learning and annotations cumulative accumulates all the data collected from active annotations whereas iterative updates the model by training on the latest set of data collected over the different strategies we found that cumulative perform equal or better than iterative across the board next to improve the number of dissonance examples we use a probability of rare class strategy prc to select mostly the examples that are highly likely to be dissonant by the current model at any round of a l we compare this to the other state-of-the-art state-of-the-art a l strategies that are commonly used in the community we find that the proposed prc strategy works better than other straight state-of-the-art strategies although the difference is small note that the performance is significantly lower for random on further rounds of a l with two best strategies we improve dissonance classification auac to points seven five which is the best performance that we have on the task so far we also check the feasibility of each strategy for annotation quality and cost to annotators we find that prc has the highest percentage of dissonance and works best for rare class however the annotators also find the examples difficult in summary we find that prc is a simple a l strategy for rare class acquisition and cold starting a l with appropriately designed transfer learning tasks and helps significantly we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update these are the links to our core dataset and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">The video introduces a new method for evaluating the quality of text in natural language processing using large language models. The speaker, Chang Sun He, explains that their approach involves instructing large language models with specific instructions to evaluate text samples, similar to how human evaluators are instructed to rate text based on given criteria. They argue that while human evaluation is unstable and difficult to reproduce, large language models can potentially provide consistent and reliable ratings. The video highlights an experiment where the speaker used large language models to rate stories generated by GPT-2 or written by humans, focusing on attributes such as grammar, coherence, likability, and relevance. The results showed that some large language models, like BERT and ChatGPT, preferred human-written texts over those generated by GPT-2, indicating their potential as alternatives to human evaluation. The video concludes by inviting further exploration of this method's benefits and limitations, as well as its application in other tasks.</sample>
    <sample id="259">The speaker introduces Exemplar, a dataset for cross-lingual semantic parsing in multiple natural languages and mind representations. The dataset contains 90 datasets from various domains, 5 semantic parsing tasks, 8 mind representations, and 22 natural languages in 15 language families. It includes six settings for training and evaluation: translate test, monolingual model, monolingual fine-tune, multilingual model, zero-shot transfer, and few-shot transfer. The speaker evaluates the performance of different models, including encoder-decoder and encoder-PTR, and finds that encoder-decoder models outperform previous work. The results show that encoder-decoder models can be improved by training on a mixture of various languages, but English performance drops in some datasets. The speaker also compares cross-lingual performance gaps and finds that few-shot setting significantly shortens the gap. The speaker concludes by summarizing the findings and inviting readers to visit their paper and code.</sample>
    <sample id="260">One</sample>
    <sample id="261">A good planner should read scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">One</sample>
    <sample id="263">The paper presents a new type of bias called domain label bias in in-context learning (ICL) and proposes a calibration method to mitigate its effect. The authors identify three types of label biases: vanilla label bias, context label bias, and domain label bias. They conduct experiments to show that random in-domain words can severely bias the model's predictions, while random English words do not. The paper then introduces domain context calibration, which uses random in-domain words sampled from the task corpus as content-free text to estimate and mitigate the effect of domain label bias. Experiments on various models and datasets demonstrate that domain context calibration significantly improves the performance of ICL, especially on tasks with large domain label bias. The paper concludes by summarizing the systematic investigation of label bias problems in ICL and proposing a calibration method that can significantly improve the performance of large language models.</sample>
    <sample id="264">The speaker, Ling Wang, is a graduate student at Zhejiang University in China. They are presenting their paper titled "TAPBT: Towards Transferable Audio-Visual Text Generation" which focuses on the challenges and methods for generating multimodal text across different domains. The main challenge discussed is the multi-modal domain shift, such as visual style and audio energy. The framework proposed includes an Audio Visual Map Mapped Network (AVM), an Audio Visual Encoder, and a Language Model Generator. The AVM maps various visual concepts into a unified audio-visual semantic space, while the encoder and generator utilize an alpha value to balance the contributions of different modalities. The training process involves contrastive learning and fine-tuning with a pre-trained model. The results show that the proposed approach outperforms baseline models on both cross-domain and cross-dataset settings, demonstrating its effectiveness in low-resource domains like CelebA and Beauty.</sample>
    <sample id="265">The speaker's name is Vasudeha.</sample>
    <sample id="266">The authors of the paper are affiliated with the University of Paderborn.</sample>
    <sample id="267">Hello everyone. My name is yusun zhang from the penn state university. Today, i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is a task to build semantic representations of user queries such as sql and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to sql, lambda or funql and etc existing cross-lingual semantic parsing models are separately proposed and evaluated on dataset of limited tasks and applications for instance there lacks of coverage on certain natural language the chinese is missing and lacks of coverage on certain meaning representations the lambda calculus is missing or they are only evaluated on certain neural model for example there's only one single model to evaluate them so to this end we propose exemplar we provide a uniform dataset exemplar for cross-lingual semantic parsing in multiple natural languages and many representations it contains nine datasets in various domains five semantic parsing tasks eight meaning representations and twenty two natural languages in fifteen language families and to better evaluate our benchmark we consider the six settings for training and evaluation the first one is translate test we use google translate api to translate source to the target language then use monolingual model to train and evaluation and for example we train the english model on english query and during inference we translate the german query using api to english and then use the trained model to predict the sql and we also test monolingual model in this setting the source language is the same as target language for example german to german or english to english we also test monolingual few-shot setting by training monolingual models with only 10 percent of training data and we test monolingual multilingual model which we train one multilingual model for all languages for example we put the german english chinese queries together to train a multilingual model and during inference we can use this model to to translate german queries or chinese query or etc and we also consider cross-lingual zero shot and few-shot transfer we train on one source language and transfer to another language so during training we train on english query or the combination of english and german few-shot queries to train a multilingual model to and predict the sql output and we also find many interesting results so regarding analysis of monolingual models we evaluate on two groups of models including encoder pdr which stands for multilingual pre-trained encoders with pointer-based decoders such as xlm-r plus pdr and bert plus pdr and we also evaluate encoder-decoder models which is multilingual pre-trained encoder-decoder models such as mbert and mt5 we found that encoder-decoder obtains the best performance on all nine datasets and we evaluate on mt5 and example xlm-r plus pdr on multilingual setting we found that encoder-decoder or encoder pdr can be improved by training in a mixture of various languages and we found it is because most of the major natural languages can obtain performance gain except that english performance drops in seven datasets and only gains in three datasets i think this is known as curse of multilinguality we also compare the cross-lingual performance gap in this figure the blue line is cross-lingual few-shot transfer the orange line is cross-lingual zero-shot transfer while the green line is the monolingual setting we found that by comparing the green and orange line we found the four zero-shot setting the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that few-shot setting the transfer gap is shortened rapidly we also find some other interesting findings for example encoder-decoder outperforms previous work or achieved comparable results for training on english natural language and significantly boost the performance of few-shot on target natural languages and we found multilingual language models such as codes and blue are still inadequate for cross-lingual semantic parsing tasks to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative types of multilingual language models and our result shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="268">The most common errors of PaLM are omission errors.</sample>
    <sample id="270">The authors of the paper are affiliated with the Emory NLP Lab and Amazon Alexa AI.</sample>
    <sample id="271">Continous fine-tuning</sample>
    <sample id="272">There are seven authors involved in the paper.</sample>
    <sample id="274">The speaker's name is Justin John.</sample>
    <sample id="275">hi i'm shanbing ph d student in university of washington today i'm presenting our work from pretraining data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawled data political news media are well covered in their pretraining data according to a survey of the c four corpus we can see that new york times los angeles times the guardian huffington post etc are well covering language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications to this end we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks specifically by asking the following questions first how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases secondly how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in nlp applications so specifically we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test this ensures us to do automatic evaluation well grounded in political science literature so some preliminary results demonstrate that first language models do have varying political leanings they occupy all four quadrants on the political compass we can also see that gpt-4 is the most liberal language model of them all and gpt series are generally more socially liberal than bert series and its variants secondly we aim to investigate to which to which extent the political biases of language models are actually picked up from training data so we conduct a controlled experiment by further pretraining language model checkpoints on six different partisan corpora separated into news and social media further divided into their political leanings by further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift for example for roberta further fine-tune further trained on the left-leaning reddit corpus we can see a substantial liberal shift in terms of its in terms of its political biases and we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society so we divide pretraining corpora into pre forty fifth president of the united states and after forty fifth president of the united states we separately pretrain language models on the two different temporal corpora we can see that language models generally had a political leaning that is further away from the center after 2017 so this indicates that language models can also pick up the like polarization in our society so last but not least we evaluate language models with different political leanings on hate speech detection and fake news detection to nlp applications that often involve language models and could have very significant implications so we see that if we investigate the per category performance that is to say if we separate the performance into different demographics or political leanings of news media we can see a pattern that for example for hate speech detection left-leaning language models are better at detecting hate speech targeting socially minority groups however are worse at detecting hate speech targeting more powerful groups in our society and vice versa right-leaning language models are better at detecting hate speech targeting white and men however worse at detecting hate speech targeting at black lgbtq plus and other minority communities similar trends also happen for fake news detection where we see that left-leaning language models are better at detecting misinformation from their opposite political leanings and vice versa this in we further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social calendar there are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models for example if a right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform this would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control so this has sounded the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings so a little bit of discussion we would also like to highlight that we expose the unique dilemma regarding language model political biases it's like between sylle and kyrpides so if we do not sanitize the political opinions in language model training data the bias will propagate from pretraining data to language models to downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electrolyte electrolyte problem okay great i think that's pretty much all i have for today i have five for today thank you for your time</sample>
    <sample id="276">This work presents a dataset for evaluating machine translation metrics for Indian languages. The dataset consists of 7,000 samples from the IndicMT dataset, which includes 200 sentences randomly selected and translated into English by seven different translation models. Human annotators evaluated each translation, marking errors, their types, severities, and providing overall scores. The evaluation framework is based on the MQM framework, with error types classified as accuracy, meaning-based, fluency, and special category errors. The results show that recent models like NLB and IndicTrans have fewer errors compared to older models like CVIT. The correlation between MQM-based scores and metric scores varies across languages, with Comet metric variants showing higher correlations than Comet baselines. The dataset is publicly available for further research.</sample>
    <sample id="277">It does not have a name.</sample>
    <sample id="278">The "marked words" method is a way to identify the words that distinguish marked groups from unmarked ones.</sample>
    <sample id="279">The authors of the paper are affiliated with the University of Washington.</sample>
    <sample id="280">The paper proposes a novel multi-modal fusion framework for emotion regulation in conversations, which consists of four key components: uni-modal feature extraction, context modeling, multi-modal fusion, and emotion classification. The framework is illustrated in Figure 3. The main contributions of this work can be summarized as follows: First, we propose a novel visual feature extractor named Vis-Net, which captures visual cues by integrating facial expressions of interlocutors from multiple frames without encoding redundant scene-related information. Second, we design a multi-modal fusion model called MultiAtt, which integrates one modality with complementary information from the other two modalities through stacked bi-directional multi-head cross attention layers. Third, we introduce a sample-weighted focal contrastive loss to address the difficulty of distinguishing between semantically similar emotions. Finally, we conduct extensive experiments on MELD and iEMO-DB and achieve state-of-the-art performances on both datasets.</sample>
    <sample id="281">The speaker introduces a data-driven exploration on when translation requires context, collaborating with Patrick Franzke, Emi Yu, Andrew F. D. Martins, and Graham Neubig. The analysis focuses on the impact of context on translation accuracy, particularly in cases where word meanings change based on surrounding sentences. For example, the word "more" can refer to a spy or a birthmark depending on the previous sentence. Evaluating how well models handle these context-dependent translations is challenging due to the limited availability of context-specific resources. To address this, the speaker's work measures context usage at different levels (sentence and word) using CXMI, identifying words that require context for accurate translation. The analysis reveals patterns such as dual pronouns in Arabic and verb form choices in certain languages. A benchmark is created to evaluate document-level translation systems, showing that context-aware models outperform those without context for specific discourse phenomena like formality and lexical cohesion. The benchmark also highlights the superior performance of DeepL over Google Translate in document-level translation.</sample>
    <sample id="282">The speaker introduces a new work in NACL 2023, titled "StoryTrans: Non-Parallel Story Style Transfer with Discourse Representations and Content Enriching." This research addresses the challenge of non-parallel text style transfer at the discourse level, which is crucial for imitating author style. The main challenge lies in transferring style-specific content to another style while maintaining the original context. The proposed solution involves a generation model named StyleTrans, which combines discourse representation from the source text with normal style embeddings to generate text in the target style. Additionally, a new training objective is designed to reduce the style similarity features from discourse representations, pulling the representation derived from different texts closer in the latent space. The training framework is separated into two stages: the first stage uses an adversarial training framework with self-restriction loss, sentence embedding loss, and style classifier loss to recover the input and disentangle style and content; the second stage focuses on fine-tuning the correct style-specific content and removing masked tokens. The results show that StyleTrans outperforms strong baselines in terms of style control and content preservation.</sample>
    <sample id="283">The Prague approach</sample>
    <sample id="284">The paper presents a novel fuzzy spam mechanism for enhancing universal information extraction. The current spam-based UIE model involves identifying and labeling the spam boundaries of the target in the text, which over-reliance on boundary positions of annotated spam. However, there is ambiguity in labeling the golden spam boundary that different annotation spans can be considered reasonable. So we propose that the spam boundary learned by the model should be fuzzy instead of precise. Besides, there is a mismatch between Transformer feature extraction and information extraction. Basic Transformer focuses on global features which ignored the prior hypothesis that span has limited length. So we proposed that the attention used for span extraction decision should be adaptive rather than static. In order to model the fuzzy spam boundary, we represent the target boundary as a continuous distribution of correct probability in specific range where rmin and rmax represent the start and end of the fuzzy boundary and the function q represents the correctness of current position. Through the sampling function showing in the slide, we convert the continuous boundary distribution into a group of discrete values for calculation of fuzzy span loss. The boundary distribution predicted by the model will calculate boundary cross entropy with golden boundary as BCE loss and adding KL divergence between predicted boundary with fuzzy spam boundary as supplementary information. To get the model in obtaining a more reasonable attention distribution for span extraction, we propose a fuzzy span attention as a mask function to trim attention distribution. The image and the formula of the mask function G are shown in the slide where the fuzzy span is reflected in two aspects. On the one hand, by introducing optimizable parameter delta to adjust the length of the four attention range, the attention span of the model is dynamically changing. On the other hand, the attention distribution on the attention spam boundary linearly decays rather than truncates. The overall structure of model is presented on the slide where the fuzzy span attention layer is only added on the top level to guide the model’s decision process without affecting the text encoding capability. The demonstrated capability of FSUIE we conducts experiments on three main information extraction tasks including named entity recognition, relationship extraction and aspect sentiment triplet extraction. As for the results of named entity recognition, by introducing FSL and FSA, our FSUIE base achieved significant performance improvement compared to UIE base without fuzzy spam mechanism on small scale dataset. The model is easier to learn universal attention spans resulting in more significant improvements. As for results on relationship extraction, FSUIE achieves new state-of-the-art results on data set ACE 2004-2005 and ADE. FSUIE uses one unified structure to extract relationship elements achieving better information extraction ability with simpler structure. Besides, FSUIE shows stronger generalization capabilities for domain-specific information. As for results on AST task, FSUIE also achieves state-of-the-art results on 14 Lab, 15 Lab, 16 Lab of ASTE V2 data set and demonstrate competitive performance on 14 Lab data set. The results of ablation study shows that FSA improves convergence speed by guiding the model to obtain a reasonable attention distribution. FSL enables the model to fully utilize annotation information and obtain a greater information extraction capability. The combined effect of the two will produce a greater enhancement. We also visualize the attention distribution of fuzzy span attention layer. Result shows that the model focused on semantic information within a limited range of preceding tokens. This meets our expectations. In conclusion, in this work, we first proposed a novel fuzzy span loss that alleviates the model’s reliance on spam boundaries and then we propose efficient fuzzy span attention to adaptively adjusting the attention span of model. And the FSUIE we proposed achieves excellent results in a wide range of IE tasks. Thank you for your listening.</sample>
    <sample id="285">The video introduces a framework for fact error correction in dialogue summarization, addressing the challenge of fact errors in summaries generated by models. It outlines two main solutions: introducing factuality-related objectives during training or inference to improve model accuracy, and designing an independent fact error correction model (FEC) that evaluates source documents and model-generated summaries to produce corrected summaries. The video highlights the importance of fact errors in dialogue summarization and critiques existing evaluation methods for FEC models, which often rely on factuality metrics like Fact CC and DA E, which may not be reliable and can blur the line between different types of solutions. The proposed evaluation framework includes alignment, classification, and comparison steps, and it is demonstrated with various FEC models trained on different datasets. Key findings include the effectiveness of training with reference summaries from dialogue summarization datasets, the need to change evaluation methods, and the promise of combining human-annotated data with synthetic data for improving FEC model performance.</sample>
    <sample id="286">James Finch</sample>
    <sample id="287">Four</sample>
    <sample id="288">The datasets that can be used to test syntactic phenomena are the BLM dataset and the syntax gym dataset.</sample>
    <sample id="289">hello my name is kai yin and i will be presenting our work titled when does translation require context a data driven multilingual exploration this work was done in collaboration with patrick franz and emily andrew f d martin and graham newby so a lot of translations depend on context for example how would we translate mole in this sentence well if the previous sentence was things could start to get dangerous if the minister's find out then mole refers to a spy but if the previous sentence was could it be anything serious doctor then mole refers to a birthmark so depending on context the meaning of the word changes and therefore its translation changes as well however evaluating how well models can translate cases like this is pretty hard firstly because only a small portion of translations depend on context which makes corpus level metrics like blue unable to capture these translations and some people have suggested targeted evaluation on context dependent translations but these resources only support limited types of context dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation in this work we try to answer these two questions first when does translation require context and second how well do models handle these cases to answer the first question we started by measuring how much a word depends on context during translation in the previous work we introduced cxmi as a measure for context usage by machine translation models and this is done by measuring how much information the context c provides about the target y given the source x you can think of cxmi as the information gain from giving context to the model in this work we extend cxmi to point y cxmi which can measure context usage at the sentence level or at the word level we can think of words that have high cxmi as ones that require context for translation now we analyze words with high cxmi to look for patterns between these words and we perform our analysis on transcripts of ted talks that have been translated from english to fourteen different languages we perform our analysis at three different levels first we look at parts of speech tags that have high means cxmi and this allows us to find for example dual pronouns in arabic that have relatively high cxmi and this can be explained because english doesn't have dual pronouns so you need context to determine if a pronoun is dual when translating into arabic and similarly we find that certain languages also require context when we want to choose the appropriate verb form we then look at vocabulary items that have high cxmi averaged over all of its different occurrences and this helps us identify cases like the one here where in chinese you need context to translate proper nouns to make sure that you're using the same translation within the document and similarly we find that context is supported to translate in the right formality and finally we look at different individual tokens that have high cxmi and this allows us to identify phenomena that cannot really be captured by the word itself but that's rather expressed in the sentence structure such as ellipsis resolution so now we use our findings from our analysis to design a benchmark for document-level translation for each of the five discourse phenomena we identified we create taggers to automatically identify words that pertain to the phenomenon and we call our tagger the multilingual discourse aware or mudah tagger we can then also note that different languages have different proportions of these discourse phenomena we then use the mudah tagger by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the mudah tagger has identified and finally we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation first of all when we use corpus-level metrics so for blue we find that context agnostic models have the best performance but then if we use comet context aware models perform best and if we use word f measure then models with or without context have comparable performance this again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone now we use the mudah benchmark to evaluate models and we find that context aware models are significantly more accurate than the models that do not use context for certain discourse phenomena such as formality and lexical cohesion but these models are not much better than models that do not use context on other phenomena like ellipsis pronouns and verb form so this sort of suggests where we would need to see more progress for document-level translation we also compared different commercial systems and our benchmark shows that deepl is usually more accurate than google translate for document-level translation to summarize we perform a data-driven analysis across fourteen language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation thank you so much for your attention see you in toronto</sample>
    <sample id="290">WSSL, FTW, COSINE, FINE-TUNING, CONTINUOUS-FINE-TUNING</sample>
    <sample id="291">The model is evaluated on the task with data of the same nature as those on which the model has been trained.</sample>
    <sample id="292">hi welcome to our presentation of deeplain a new corpus for german text simplification on the document level and on the sentence level my name is regina strawn and i will guide you through the first part of the presentation let's first define text simplification text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers to train a text simplification model we require parallel pairs of text for example doc of documents or sentences and the example here you can see a parallel aligned sentence pair of a complex german sentence and its translation into plain language to simplify the sentence different techniques are possible as you can see in the example such as lexical substitution clause deletion clause deletion reordering or insertion of words we now propose our new corpus deeplain because in the recent years there were some problems with existing corpora so for example these corpora here are too small to train a text simplification model on the other three models which are proposed in recent years are all automatically aligned which means they can be error prone in their alignments therefore we propose our new corpus deeplain which is split into two sub corpora deeplain api and deeplain web deeplain api is based on news texts in deeplain api we aligned 483 documents all manually it results in roughly 30 000 13 000 parallel sentence pairs for deeplain web this corpus includes different domains and we also align all of these 750 documents on the one hand manually and on the other hand with automatic alignment methods in total we result in 30 000 450 sentence pairs we analyzed our sentence pairs a little bit more so for example on the type of simplification as you can see here the bible texts are much stronger simplified than for example the news text or the language learner texts on all level regarding for example lexical simplification structural simplification or the overall level of simplification furthermore you can see that our deeplain corpus has a high variety of different simplification transformations so for example in the deeplain api corpus we have much more reorderings and word editions than we have in the deeplain web corpus on the other hand in the web corpus we have much more refrasings so let's now see what we can do with this corpus hello i am omar and now i will talk about the use cases for our dataset deeplain so for the first use case we can evaluate automatic alignment methods in the recent years there has been a lot of alignment methods but in the context of machine translations where we have two parallel documents written in different languages and we want to extract alignments of sentences in post documents but in our use case we are trying to extract alignments between sentences of two parallel documents having the same language having the same content but they are on a different complexity levels and now as we have our dataset deeplain which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper at the end we concluded that the best alignment automatic alignment method to use for texts for german text simplification is the method of mass align and you can also find the code to run this method on your own documents in the paper the second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text we have fine-tuned two different models we have fine-tuned the model of long impart to produce document-level simplifications and we also fine-tune the normal base long the normal base impart to produce sentence-level simplifications you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper we concluded that this basic fine-tuning could produce or could get scores better than the baseline scores and we propose those results as a benchmark a base benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you during the conference thank you</sample>
    <sample id="293">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduced the alt entities corpus and my name is javahar hasani and this is a joint work with philip bradinsky sylvia parati and annie luis our goal is to understand users' language when they want to make a choice consider this alternative question did you mean easy on me or i got a feeling here a user wants to select between one of these two songs um the most obvious thing is to use a direct reference for example by saying the name of the song easy on me or its position the first one but sometimes an indirect reference is more appropriate to have a more natural conversation this could happen when the user cannot remember the name of the song or the pronunciations are too similar to each other and hard to disambiguate or when the user wants to specify a preference here are some examples in direct references for example the newer one or the song that's not energetic this is an important problem in conversational systems and also for benchmarking llms entity understanding we're not aware of a public dataset a large-scale public dataset for the task so we collect one using crowd annotation our dataset covers three different domains music books and recipes our dataset collection methodology emphasizes informality using a cartoon completion set up the cartoon has three speech bubbles in the first bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in this in the second speech bubble alice says do you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob uses an indirect reference to select one of these entities for example the newer one we provide the first and second speech bubbles automatically but the third one is filled in by the annotator um the first speech bubble is chosen from a few manual prompts per domain the second one which is the alternative question is generated as follows we always use a simple template do you mean a or b where a and b are samples from wikipedia here are the different sampling methods we've used when we move higher in the list the entities become more similar to each other and it's usually harder to make the disambiguation the first one is uniform at random the second one is when the entities have similar titles for example two books with the name the return the third one is when they have similar descriptions on wikipedia and finally when they have similar infoboxes or attributes on wikipedia for example the same genre or the same artist for a song when we show this alternative question to the annotators they know the name of these entities but they don't necessarily know about the entities so what we do is that we show some background knowledge about the two entities for songs we simply show a google search link to each song and then ask the annotators to listen to at least some of each song and read about each song here's for example the google search result for the song easy on me for the recipes and books domain we show some background text from wikipedia for recipes we additionally show their images again from wikipedia so that the annotators know how they look like then we ask the annotators to pick one of these entities for example here the first one and describe them using three to five indirect referring expressions for example the one with the piano music here are some examples from our dataset for example the one without words not the one with the twelve year old twelve year old boy or the fictional one or comes from alzheimer and so on the alt entities corpus has six thousand alternative questions across three domains and it has forty two thousand indirect referring expressions results with t5-xl model are summarized below if the language model has access to the exact same background knowledge as the annotators then the accuracy is really high it's around ninety two to ninety five percent but this is not realistic if the language model has access to some partially overlapping background knowledge then the accuracy is between eighty two to eighty seven percent which is more realistic for example when the language model retrieves the background knowledge if the language model has access only to entity names then the accuracy is only sixty percent so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our dataset thanks</sample>
    <sample id="294">CamemBERT is initially trained on the NAO dataset.</sample>
    <sample id="295">Adam Siprowski</sample>
    <sample id="296">The video presents a collaborative work between the University of Turin and Amazon Alexa, focusing on natural language understanding and processing. The researchers highlight the limitations of supervised machine learning approaches in developing these models due to the need for large sets of manually annotated data. They specifically investigate irony detection, a highly latent and pragmatic phenomenon in natural language, by developing a corpus called EPIC (English Prospective Irony Corpus). This corpus includes data from social media platforms like Reddit and Twitter, spanning one and a half years, with about 300 short conversations per source. The data is annotated by 74 annotators across five varieties of English, using a crowdsourcing platform called Prodigy. The results show differences in inter-annotator agreement based on various dimensions such as gender, age group, and nationality. The researchers also build perspective-aware models to model these differences, noting significant variations in confidence levels. They find that generations close to each other and annotators from the United Kingdom and Ireland exhibit higher disagreement in their annotations.</sample>
    <sample id="297">The study examines the use of dog whistles in political discourse, focusing on their context-dependent meaning and impact. It develops a typology and glossary of over 340 terms and symbols, particularly for racist, transphobic, and anti-Semitic dog whistles, collected from various sources. The research characterizes dog whistles by register type and persona, analyzing their frequency in historical U.S. political speeches and their association with conservatism. Experiments with language models like GPT-3 show varying performance in surfacing and identifying dog whistles, highlighting the challenges in detecting them. Additionally, the study explores how dog whistles can evade content moderation through toxicity detection, demonstrating that hateful sentences are rated less toxic when standard group labels are replaced with dog whistles. This comprehensive analysis underscores the importance of understanding dog whistles in NLP and their role in political influence and online abuse.</sample>
    <sample id="298">The findings that the performance degrades with larger temporal gap between the train and test data led to the conclusion that temporal drift is the main cause of performance loss.</sample>
    <sample id="299">The paper presents a training method to reduce the reliance of neural network models on shortcuts and improve their out-of-distribution performance. The key insight is that neural network models suffer from poor performance on under-represented hard training instances with patterns that could indicate the shortcuts in the dominant easy examples. These hard examples are pivotal for ensuring good generalization performance on out-of-distribution samples. The proposed method uses a minimax training objective between a learner and auxiliary model. The learner tries to minimize the loss of the neural network task, while the auxiliary model tries to maximize the learner's loss by generating example weights such that the learner is incentivized to concentrate on regions of the input space where it incurs high losses. Both models are optimized in an alternating fashion using any standard optimization algorithm. At test time, the learner can make predictions without relying on the auxiliary model. The method does not make assumptions about the type of shortcuts contained in the dataset and relies on the learner's own training dynamics to generate example weights. We use a feedforward network to model the auxiliary model. We evaluate our proposed method in three commonly used neural network datasets (MNLI, Fever, and QQP) and the corresponding out-of-distribution test sets (Hans, Symmetric, and Impulse). We observe that compared to an ERM trained model as well as the best-performing shortcut mitigation method in each dataset, the minimax training objective consistently improves out-of-distribution performance while maintaining high in-distribution accuracy.</sample>
    <sample id="300">The paper introduces a new task called interactive dictation, which allows users to dictate and edit documents using their voice in a natural and intuitive manner. The authors propose a four-step procedure for this task: ASR recognition, segmentation of dictation and command utterances, extraction and normalization of commands, and execution of dictation and command utterances in sequence. They also design a data collection interface and build a dataset for this task. To facilitate future work, they release code at the following site and welcome more work on this task.</sample>
    <sample id="301">hi everyone i'm jenny a first-year phd student at carnegie mellon university and today i'll be presenting your work and l positionality characterizing design by biased data sets of models this work was done in collaboration with some folks at the university of washington and um the alliance institute for ai namely sebastian santy ronald labros caterina rynica and martin sap so let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content you might turn towards a popular api like perspective api for toxicity detection and this works really well if you're carl jones where perspective api is able to detect correctly toxic instances but that's not really the case for dithia sharma where perspective api is really not as sensitive to offensive terms that are more common in indian contexts this is an example of a design bias where we see systematic performance differences of technology between populations design biases like the one that we just saw before might occur due to the positionality of the nlp researchers and model developers positionality is simply the perspectives that people hold as a result of their demographics identity and life experiences this is a concept widely used in critical studies specifically in feminist and queer academic spaces and as a researcher positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make and so one question that people might ask is do datasets and models have positionality and we're not trying to say that models themselves and datasets themselves have demographic identities and life experiences but they do aggregate judgments and opinions of real people and can thus represent certain positionabilities over others so prior work has suggested some anecdotal evidence of having positionality such as cultural gaps in models and datasets as well as theoretical definitions of model positionality however these works really don't look at comparing end users with the datasets and models themselves and studying model and dataset positionality is increasingly important as nlp tasks become more subjective and socially oriented and it's challenging to characterize how these positionabilities are skewed because not all decisions are documented and many models are hidden behind apis so to study dataset and model positionality we actually compare the annotations with real users with existing datasets and models we do this through our framework nlp positionality our framework works in two main steps the first step is to re-annotate datasets with diverse annotators and we opt to do this over looking at the demographics of original datasets annotators because usually only a few instances annotators annotate each instance and because demographics are rarely collected and shared and so we opt to re-annotate data to get many annotates per instance and to get a rich set of demographic data we then take the annotations by demographic and compare them to the models and datasets using pearson's correlation score and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions our framework is largely enabled through lab in the wild an online crowdsourcing platform former hci collaborator in lab in the wild is an online experimentation platform where we can recruit diverse volunteers compared to the platforms like mturk which largely have participants from the us or india and further lab in the wild still is able to get high quality data we host two tasks on lab in the wild one of them being social acceptability and the way this works is that participants will read a situation from the social chemistry data set and then they'll write how socially acceptable a situation is afterwards to stay engaged in the study they can compare their responses to an ai and others we've then compared these annotations with social chemistry delphi and gpd four we then replicate a very similar setup for the toxicity and hate speech detection task where they'll read an instance from dinah hate and write whether they think it's an instance of hate speech we then compare these annotations with dinah hate perspective api rewriter api hate roberta and gpd four our study in the end amassed over sixteen thousand annotations from over a thousand annotators from eighty seven countries so now we're better equipped to answer who do nlp datasets and models align with the most we find that there is positionality in nlp for example we find that datasets and models are most aligned to english speaking countries so for the gpd four social acceptability analysis we find that it's most aligned to confucian and english speaking countries we find that dinah hate is also most aligned to english speaking countries we also find most additional alignment with people who have a college education so for gpd four in the social acceptability task we find that it's most aligned to people with a college education or graduate school education and we find the same for dinah hate where it's most aligned to people with a college education however when models and datasets are aligned to specific populations some are inevitably left behind an example of this is that datasets and models are less aligned to non-binary people compared to the men and woman counterparts we find this in the gpd four social acceptability task as well as the dinah hate task analysis as well so given that there is positionality in nlp what can we do about it so we have a few recommendations for this first one is keep a record of all relevant design choices throughout the research process and the other is to do nlp research with the lens of perspectivism our third recommendation is to build specialized datasets and models within four specific communities and a good example of this is the musakani initiative i mean we want to emphasize that inclusive nlp isn't just making you know all technologies work for everyone and so that concludes our presentation but if you'd like to learn more feel free to check out our dashboard for the most updated analysis results and our paper thank you</sample>
    <sample id="302">To put the tokens in the right order</sample>
    <sample id="303">The authors recommended that model owners should increase transparency about bias mitigation methods because it is unclear whether the positive stereotypes and essentializing narratives are due to over-expressive value alignment or anti-stereotyping methods, and more transparency would allow for further study of these patterns.</sample>
    <sample id="304">Ungrammatical sentences</sample>
    <sample id="305">The video presents a critical look at weakly supervised learning (WSL), a method used in machine learning where data is labeled using weak labeling sources such as simple heuristics, knowledge bases, or low-quality crowdsourcing. The speaker, Dawei, a PhD student at Saarland University in Germany, discusses the challenges of WSL, including the noise in weak labels and the need for clean validation sets. He addresses three research questions: whether clean validation data is necessary for WSL, how many clean samples are required, and whether clean samples should only be used for validation. The findings indicate that recent WSL methods require clean validation samples to work properly, and that increasing the number of clean validation samples can improve performance. The video also highlights the importance of reporting model selection criteria and comparing WSL approaches with fine-tuning on clean samples. Overall, the video emphasizes the need for more practical and realistic evaluations of WSL methods.</sample>
    <sample id="306">The paper presents a study on the ability of language models to track entities in language models. The research question is to what extent large language models can track entities. The authors argue that this is a crucial ability for understanding long discourses, but there haven't been any systematic investigations into whether pre-trained language models can actually perform such tasks. The authors designed a task involving boxes and objects to evaluate entity state tracking abilities. They tested the setup with Flan-T5 and GPT-3 and 3.5 models using two-shot in-context learning. The results show that most models simply repeat the initial state, while only text-davinci-03 excels at non-trivial tracking. The authors found that all GPT-3.5 models, which have been trained on substantial amounts of code, exhibit non-trivial entity tracking behavior. However, smaller models like T5-base can learn to perform entity tracking if directly fine-tuned, but randomly initialized models of the same architecture cannot learn the state tracking task even when they receive direct supervision.</sample>
    <sample id="307">The authors used public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering to evaluate their models.</sample>
    <sample id="308">This presentation discusses the concept of "positionality" in natural language processing (NLP) and its impact on AI models. Positionality refers to the perspectives influenced by demographics, identity, and life experiences, which can affect the performance of NLP systems across different populations. The speaker highlights a study comparing annotators' assessments with existing datasets and models, revealing biases such as alignment with English-speaking countries and higher education levels. Recommendations include documenting design choices, considering perspective in research, and developing specialized datasets for underrepresented communities. The presentation emphasizes the importance of inclusive NLP practices to ensure fair and effective technology for all.</sample>
    <sample id="309">ABC eval</sample>
    <sample id="310">Wikipedia</sample>
    <sample id="311">The authors of the paper are affiliated with the University of Paderborn.</sample>
    <sample id="312">MultiInstruct is the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories.</sample>
    <sample id="313">Two</sample>
    <sample id="314">Binary coordination is the coordination of two verbs.</sample>
    <sample id="315">The prompts used in this study were on average 10 words long.</sample>
    <sample id="316">The findings suggest that smaller models, when properly trained on suitable datasets, can surpass larger models in generating high-quality scripts for constrained language planning.</sample>
    <sample id="317">The speaker introduces a new method called CodeIE, which transforms unstructured text into structured information extraction tasks using code generation models like Codex. This approach ensures alignment between input and output stages, addressing the mismatch issue in previous methods. The speaker evaluates their method on three datasets: named entity recognition, relation extraction, and code generation. They compare the performance of their method with traditional language models like GPT-3 and UIE, showing that CodeIE significantly outperforms these baseline models. The analysis reveals that CodeIE's performance is better than GPT-3 when generating outputs with labels not present in the predefined label set, such as currency, company, and organization. Additionally, the model performs better in terms of recall when using code format prompts. The speaker hopes this analysis provides inspiration for others and encourages further research in this area.</sample>
    <sample id="319">The work investigates the impact of pre-training strategies on the performance of language models. Specifically, it compares the performance of models trained from scratch with those trained using a combination of pre-trained weights and tokenizers. The results show that pre-training strategies can improve the performance of models, especially when combined with specialized data.</sample>
    <sample id="320">The factor of overfitting due to test reuse is not observed.</sample>
    <sample id="321">The quality of the simplification was evaluated by analyzing the sentence pairs and comparing them to the Bible texts, which were much stronger simplified than other language learner texts.</sample>
    <sample id="322">The speaker, Enrico, is presenting at ACL 23 about how text classifiers learn about morality. He explains that human morality helps us distinguish right from wrong and is essential for our societies. However, current language models often treat morality as a singular scale between immoral and moral, which can hide the subjective nature of morality. The Moral Foundation Theory suggests that humans perceive morality through different foundations, such as fairness or authority, which are prioritized differently by each individual. Enrico's paper aims to understand how language models learn about morality by applying explainable AI techniques to a dataset called Mora Foundation Twitter Corpus, which contains tweets from seven different domains. The paper explores whether language models can recognize the differences in how morality is expressed across these domains, highlighting the importance of considering the context and nuances of morality when training and deploying language models.</sample>
    <sample id="323">The paper presents a method for answering questions in the Common Sense QA task using language models and knowledge representation. The authors propose a hybrid graph-based model that combines knowledge from both language models and knowledge bases. They introduce a new entity matching strategy to retrieve relevant knowledge from the knowledge base, which is encoded into the language model to improve its performance. The model also uses a two-stage training strategy to optimize the structure and knowledge representation of the hybrid graph. Experiments on the Common Sense QA and OpenBook QA datasets show that the proposed method achieves good results compared to other methods.</sample>
    <sample id="324">Yes, language models have varying political leanings.</sample>
    <sample id="326">Cognitive dissonance is two beliefs or actions that are inconsistent.</sample>
    <sample id="327">The speaker introduces their work on a vision-language learning model called Bridge Tower, which aims to train AI systems to understand both images and text. They explain the two-tier architecture of Bridge Tower, consisting of textual, visual, and cross-modal encoders. The work focuses on addressing the limitations of previous models by introducing adaptive managers that aggregate insights from pre-trained unimodal experts at different levels. The proposed method uses BERT-based and CLIP-based unimodal encoders and introduces managers in each cross-modal layer to facilitate more comprehensive cross-modal alignment and fusion. The results demonstrate that Bridge Tower achieves superior performance on various downstream tasks, outperforming many base-size models trained on 4 million data points. The paper code and models are available on GitHub for further use.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">The paper introduces a zero-shot video localization method based on structured pseudo-label generation, which enables training a video localization model without any manual annotation. The proposed method first uses a pretrained image caption model to generate more complex free-form pseudo-queries, then uses a pretrained model to match the relevance between video frames and pseudo-queries to generate pseudo events. These pseudo events guarantee high relevance between videos within the events and queries and low relevance between videos outside the events and queries. Finally, the method reduces the weight of low-quality samples and corrects the pseudo labels to reduce the influence of label noise. The results show that the proposed method outperforms existing methods on two datasets in terms of average precision (AP) and mean average precision (MAP).</sample>
    <sample id="330">Yes, cumulative training performs equal or better than iterative across the board.</sample>
    <sample id="331">The speaker is Sarah Papi.</sample>
    <sample id="332">The data was taken from TED Talks that have been translated into 14 different languages.</sample>
    <sample id="333">The paper introduces a framework called Ink, which aims to improve the representation space of neural machine translation (NMT) models by injecting common knowledge. The authors acknowledge collaborators from various universities and propose a training loop that involves extracting common knowledge from a data store, adjusting the representation using three types of alignment, and refreshing the data store iteratively. They compare their approach with the state-of-the-art CM system and show that Ink achieves better performance with less memory usage and faster inference speed.</sample>
    <sample id="334">hi my name is adam sriprikowsky and this talk is about the dependency structure of coordination as you may know that different dependency structures assumed by different theories and corpus approaches so for example in the universal dependencies the structure of the coordinate coordination lisa bart and maggie is such that the first conjunct is the head of the whole coordinate structure so in this case lisa a similar approach is assumed in igor miltruk's meaning text theory where again the whole coordinate structure is headed by the first conjunct so these two approaches are asymmetric right they they single out one of the conjuncts now there also symmetric approaches to coordinate coordinate structures such as the prague approach the conjunction headed approach assumed in prague dependency treebanks where coordinate structures are headed by the conjunction so we get dependencies from end to all the conjuncts and finally this also a multi-headed approach that's used for example in the de catts on's word grammar where so to say all conjuncts are heads of the coordinate structures so we get dependencies from the governor here loves to all conduct separately these are bart and making now the main newspaper is to produce an novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these two okay the argument is based on the principle of dependency length minimization that are explained on the basis of these examples so in english as you might as you might know a direct objects prefer to be close to the verb while adjuncts maybe further away right so march read it yesterday is fine because the direct object it is closed to the verb while march read yesterday it is much worse right because here between the verb and the direct object there's an adjunct yesterday however this effect may be ameliorated when when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine march read this absolutely fascinating book about the bees yesterday i is okay in the way instead of it we have this long np but it's also okay to say march read yesterday this absolutely fascinating book about bees so the reason here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb it satisfies the principle of dependency length minimization which says that shorter shorter dependencies are preferred so these two trees only show the length of the crucial dependencies so the ones that are not constant among these two structures so here we have a dependency from red to the adjunct of length seven measured in words and from red to book of length four so together it's eleven when you move when you swap these two constituents the sum of these two dependencies becomes six right so instead of 11 six much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one okay so what we did we extracted various statistics from about coordination from the enhanced version of pent of the pentry bank and see the paper why wouldn't use university dependencies and these statistics confirm the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables and also the observation that was made in passing that this tendency grows with lengths the length difference so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is is bigger of the left short conjuncts but what's novel in in this paper is we that we observed that this tendency only occurs when the governor on the left are absent right so the governor is on the left in this example i saw bart and lisa so it's the governor is on the left it's absent in the second example homer came and sneezed here we have coordination of two verbs and there's no outside external governor right so in such cases the left conjunct prefers to be shorter the more so the bigger the difference between the two conjuncts however when the governor is on the right as here left governs the coordination ten and net this effect disappears so we showed that by measuring length in characters that the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we show in the paper how this provides an argument against as asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="335">The speaker's name is Mateusz Lendman.</sample>
    <sample id="336">Cross-lingual transfer is the process of training a model on one language and then using it to predict in another language.</sample>
    <sample id="337">The speech discusses the challenges of handling out-of-vocabulary (OOV) words in natural language processing and introduces a neural approach to improve word embedding. The approach involves using a word relationship graph that captures lexical rules of word formation and association, allowing for better representation of OOV words by breaking them into word pieces and associating them with relevant words. This method uses a two-level graph structure where each word or word piece is represented as a node, and its corresponding word embedding serves as the node feature. To address the issue of assigning node attributes to OOV nodes, a self-attention mechanism is applied to select the most important information and reduce noise from nodes with numerous neighbors. The model incorporates a readout block to capture the whole graph information and summarize word formation, using a simple one-layer graph convolutional network for efficiency. Contrastive learning is applied to encourage similarity between related words while pushing unrelated samples apart. Extensive experiments demonstrate the model's effectiveness on both intrinsic and extrinsic tasks, particularly in handling OOV words through word formation. The model also shows robustness across different languages, including both native and non-native languages, due to its ability to handle various word formations and compositions.</sample>
    <sample id="338">The speaker, Bing Chen, introduces a collaborative research project titled "Our Human Explanations Are Always Helpful Towards Objective Evaluation of Human Natural Language Explanations," which aims to evaluate the quality and utility of human-annotated explanations in various tasks. The project involves researchers from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research. They propose a unified data structure to convert different tasks into a common format, allowing for systematic comparison of explanations. The evaluation metric, called "true," extends the Simulatability Score by assessing the helpfulness of explanations during fine-tuning. The results show that even low-quality explanations can improve model predictions, and the true metric outperforms the Simulatability Score in evaluating dataset qualities across five datasets using two models, T5 and BART.</sample>
    <sample id="339">The authors of the paper are affiliated with Salzburg University in Germany.</sample>
    <sample id="340">The paper presents a large-scale syntactically diverse paraphrase dataset, PeraMR, constructed using AMR back translation. PeraMR contains around 15 million source sentences with approximately 6.9 paraphrases per source sentence. The authors leverage AMR (Abstractive Meaning Representations) graphs to generate syntactically diverse paraphrases by changing the focus of the graph and modifying corresponding edges. They compare PeraMR with datasets generated by back translation and show that PeraMR has higher syntactic diversity scores while preserving good semantic similarity. The paper also demonstrates the benefits of PeraMR in various NLP applications, such as learning sentence embeddings, syntactic control paraphrase generation, and data augmentation for future learning.</sample>
    <sample id="341">The authors use average latency and computational latency as measures of latency.</sample>
    <sample id="342">The speaker introduces a large-scale personalized dialogue dataset constructed from live streaming. The dataset includes video sources and text sources, with the latter being more prevalent. It is divided into two groups: those containing scripts (e.g., TV and movie scripts) and those without scripts (e.g., interview datasets). The challenge lies in finding an effective matching mechanism to capture the reply-to relationships among speakers. Personalized dialogue is crucial for applications like virtual streamers and virtual employees, but current research faces challenges such as insufficient persona information and lack of personalized dialogue data. The proposed dataset addresses these issues by providing detailed persona annotations and long average sessions. Experiments show that the extracted persona and long average session per persona benefit the final results, and both the rule and classifier are important for persona extraction. The performance of the proposed dialogue models on the dataset is better than existing ones, indicating the uniqueness of the dataset.</sample>
    <sample id="344">Trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar induction procedures.</sample>
    <sample id="345">The paper introduces a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output, without relying on trees. The approach predicts the output from the input in two steps: first, it tags each input token with an unordered multi-set of tokens that will appear in the output; after the first step, all the right tokens are identified but not ordered. In the second step, another model predicts a permutation to put them into the right order. The permutation model is flexible and expressive, as it does not put any hard constraints on the possible permutations. The experimental results show that the method outperforms other treeless models on generalization to deeper recursion by a large margin. However, some other kinds of structural generalization remain very challenging. The alignment between input and output is not given in the training data, which poses a challenge for training. Sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. The permutation method is very flexible, but it brings the challenge that finding the highest scoring permutation is NP-hard. The authors approximate this with a GPU-friendly continuous relaxation that also allows them to back propagate through the solution and learn the linguistically more plausible permutations.</sample>
    <sample id="346">The affiliations of the authors of the paper are not mentioned in the given information.</sample>
    <sample id="348">The paper, titled "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models," presents a method for measuring stereotypes in language models (LLMs) by generating personas based on specific identity markers. The authors, Myra, Senn Derrouch, and Dan Jurafsky, address the limitations of existing methods, which often rely on hand-constructed datasets and fail to capture intersectionality. Their approach involves using instruction-tuned LLMs to generate personas in response to prompts like "imagine you are an Asian woman describe yourself." This method allows for the identification of linguistic markers that distinguish marked groups from unmarked ones, revealing harmful patterns and stereotypes. The paper highlights the importance of addressing positive stereotypes and essentializing narratives, using intersectional lenses to study biases, and increasing transparency in bias mitigation methods.</sample>
    <sample id="350">The presentation discusses the evaluation of artificial intelligence (AI) models against human performance in natural language processing (NLP). It highlights the limitations of current AI models, such as their inability to generalize, susceptibility to adversarial attacks, reliance on spurious patterns, lack of sensitivity to basic perturbations, and over-sensitivity to less important perturbations. The presentation also addresses the challenges in comparing AI models with humans using popular benchmarks like SuperGLUE and Squad. It points out that these benchmarks often involve errors due to differences in dataset sizes, ground truth annotations, and biased evaluation methods. The presentation argues that these issues render claims of superhuman performance in NLP ungrounded and calls for more reliable benchmarks and evaluation methods to accurately assess AI capabilities.</sample>
    <sample id="351">The paper investigates the problem of generalization in named entity recognition (NER) using the CoNLL 2003 dataset. The authors found that models trained on CoNLL 2003 have poor generalization to modern data, and that larger models and more fine-tuning examples lead to better generalization. They also found that temporal drift is the main cause of performance drop, not adaptive overfitting. The paper concludes that for good generalization, a better model architecture, larger model size, and more fine-tuning examples are needed, and that these goals should be pursued simultaneously. The paper also suggests that research on improving generalization of NER models is needed, and encourages readers to check out the paper's dataset and contact the author with any questions.</sample>
    <sample id="352">ABC-Eval stands for Annotating Behaviors in Chat.</sample>
    <sample id="353">The paper introduces a method for Python code generation by asking clarification questions. The authors, Housheng Li, Motomasa Masakar, IFT Mottinsh, and Irina Gorguitch, address the challenge of input under-specification in code generation. They propose an interactive approach to gather more specifications through clarification questions. The paper focuses on clarifying operation-level specifications and proposes a method to create a synthetic dataset with clarifications on key operations. The authors also propose a pipeline for code generation by asking clarification questions. The paper includes a detailed analysis of the results, including the performance of different models and the impact of clarification questions on code generation. The authors conclude that their method is promising but requires further refinement, particularly in terms of taxonomy and distinguishing aligned operations from those with similar names.</sample>
    <sample id="354">2021</sample>
    <sample id="356">The authors of the paper are affiliated with the University of Edinburgh and the University of Cambridge.</sample>
    <sample id="357">Siu Yuen</sample>
    <sample id="358">Five</sample>
    <sample id="359">The approach is compared to the state-of-the-art architecture specifically tailored for simultaneous speech translation.</sample>
    <sample id="360">hello everyone my name is yin and my colleague zhang and i will be presenting our research on multi-instruct improving multi-modal zero-shot learning via instruction tuning so with the advances in large language models many works started to explore new learning paradigms of using pre-trained language models for different downstream tasks in a parameter and data efficient way recently many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions however most previous works on instruction tuning focus on improving the zero-shot performance on language only tasks while computer vision and multi-modal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve generalization to unseen multi-modal tasks additionally at the time of our research we discovered a considerable discrepancy in availability of instruction dataset between lrp and multi-modal there exists more than 1,600 language only instruction tasks however there is no large-scale publicly available multi-modal instruction task therefore this motivates us to build a multi-modal instruction tuning dataset here we present multi-instruct the first multi-modal instruction tuning benchmark dataset that consists of sixty two diverse multi-modal tasks covering ten broad categories these tasks are derived from twenty one existing open source datasets and each task is equipped with five expert-written instructions for investigating multi-modal instruction tuning on our proposed dataset we take ofa a unified multi-modal pre-trained model as our base model ofa uses a unified vocabulary for language image tokens and the coordinate of a bounding box here we show some example instances from our multi-instruct dataset to unify the processing of various input and output data type we follow the method from ofa and formulate all the tasks in a unified sequence-to-sequence format in which the input texts images instruction and bounding boxes are represented in the same token space okay now i'm going to talk about multi-modal instruction tuning so for the training dataset we use fifty three tasks from nlp group for training and we sample ten thousand instances per task for testing we reserve the entire common sense reasoning group for testing and we select additional five tasks from wiki and the miscellaneous group we use all the instances in the test split for each task in addition we randomly sample twenty tasks from the test split of natural instruction as the unseen task for lrp so we use a pre-trained ofa large model as a base model during training we mix all the instances for all the tasks each instance is randomly combined with one of its five instruction templates so during tests for each task we conduct the total of five experiments by evaluating the model using one of the five instructions in each experiment we report the mean and max performance and the standard deviation of the performance across all five experiments if the task is a multi-modal classification task we report accuracy if it's a multi-modal generation task we report rouge l for lrp task we report rouge l as well we also introduced a additional evaluation metric called sensitivity so this measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in the wording of the instruction here is our main results as we can see instruction tuning can significantly improve or is ofa's performance on seen multi-modal tasks also transfer learning from natural instruction dataset can benefit instruction tuning here we can see as the amount of task increases the model achieved better performance and in the meantime lower sensitivity so we also do one experiment we use one instruction versus five instruction as we can see using more instruction can improve the model's overall performance and reduce its sensitivity a lot so this shows the effect of different fine-tuning strategy on the model sensitivity as we can see by transfer learning from natural instruction dataset the model can achieve much better sensitivity compared to the original ofa model we also can see transfer learning from natural instruction dataset can help ofa to achieve much better performance on the natural instruct dataset so overall we propose the first large-scale multi-modal instruction tuning dataset which significantly improves the zero-shot capability of ofa and we explore different transfer learning technique and show their benefits we design a new metric called sensitivity so one more thing we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional visual language tasks and we will release them so this is the qur code for our data and model thank you</sample>
    <sample id="361">The speaker, Armine Norbarch, is a PhD student at Carnegie Mellon University and the research director at JP Morgan AI Research Team. He is presenting a work titled "CounterComp," which focuses on using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning. The goal of multi-step quantitative reasoning is to answer questions that require multiple arithmetic operations, such as calculating changes in financial data from one year to another. Current state-of-the-art neural models struggle with these tasks, especially when the output involves more than two steps, due to memorizing spurious patterns. To address this, CounterComp uses auxiliary metric learning loss with a dynamic margin to encourage the model to attend to meaningful tokens during training. This approach has been shown to consistently improve performance on both in-distribution and out-of-distribution samples, enhancing the model's ability to generalize across different datasets.</sample>
  </task>
</testset>