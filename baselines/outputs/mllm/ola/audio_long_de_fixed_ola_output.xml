<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind Webkraw Data, politische News-Medien und soziale Medien.</sample>
    <sample id="1">Die Autoren sind an McGill University, Mira und Microsoft Research.</sample>
    <sample id="2">This paper introduces a novel multi-pretraining model called LeTMask to address the reading order issues in document understanding. LeTMask uses text and layout information as input and aims to enhance text–layout interactions and layout representation learning during pre-training. It differs from previous studies in three aspects: choice of word positioning, masking strategy, and pre-training objectives. Instead of global word positioning, LeTMask proposes to use the in-segment token orders as word positioning, which is referred to as local word positioning. As local word positioning does not provide cross-segment orders, LeTMask infers global reading order by jointly using 1D positioning, 2D positioning, and thematic information. The joint learning process with both thematic and spatial inference can promote text–layout interactions and help the model learn better layouts.</sample>
    <sample id="3">Hallo, und willkommen zu unserem Vortrag über die Einfachung von Texten. Ich heiße Regina Störrn und werde Ihnen das Erste Teil meines Vortrags präsentieren. Das Erste ist die Textsimplifizierung. Textsimplifizierung ist ein Prozess, bei dem ein Text anpassiert wird, um seine Verständlichkeit für einen bestimmten Zielgruppe zu verbessern, wie zum Beispiel für Menschen mit Leseschwierigkeiten oder für nicht-native Sprecher. Um ein Textsimplifizierungstool zu trainieren, benötigen wir Paare von Texten, zum Beispiel Dokumente oder Sätze. In diesem Beispiel sehen Sie ein Paar von aufgereihte Sätzen, einer komplexen deutschen Sache und ihre Übersetzung in eine einfachere Sprache. Um einen Satz zu simplifizieren, gibt es verschiedene Techniken, wie Sie in diesem Beispiel sehen können, wie z.B. Lexikalsubstitution, Klauselletion, Klauselletion mit Umgestellungen, oder die Einfügung von Brot. Wir haben nun einen neuen Korpus vorgestellt: die Plain. Denn in den letzten Jahren gab es einige Probleme mit existierenden Korpusen. Zum Beispiel sind diese Korpusen zu klein, um ein Textsimplifizierungstool zu trainieren. Die anderen drei Modelle, die in jüngster Zeit vorgeschlagen wurden, sind alle automatisch aufgereihte, was bedeutet, dass sie überaltert sind und falsch aufgereihte sind. Daher schaffen wir einen neuen Korpus: die Plain. Dieser ist in zwei Teile aufgeteilt: die Plain API und die Plain Web. Die Plain API basiert auf News-Texten. In der Plain API sind 483 Dokumente manuell aufgereiht worden, was zu einem Durchschnitt von 30.000 13.000 Paaren von aufgereihte Sätzen führt. Der Plain Web-Korpus enthält verschiedene Domänen und alle 750 Dokumente werden manuell aufgereiht, und auf der anderen Seite mit automatischen aufgereihte Methoden aufgereiht. Insgesamt ergibt sich 30.450 Paare von aufgereihte Sätzen. Wir analysieren unsere Paare ein bisschen weiter, zum Beispiel auf Typen von Simplifizierungen. Wie Sie hier sehen können, sind Bibeltexte stark simplifizierter als z.B. News-Texte oder Lerner-Texte auf allen Ebenen, wie zum Beispiel Lexikal Simplifizierung, Struktursimplifizierung, oder die allgemeine Simplifizierung. Darüber hinaus zeigt unser Plain-Korpus eine hohe Vielfalt an verschiedenen Simplifizierungs-Transformationen. Zum Beispiel in der Plain API enthält der Plain API-Korpus mehr Umgestellungen und Worteditions als der Plain Web-Korpus. Auf der anderen Seite enthält der Web-Korpus mehr Umgestellungen. Nun sehen wir, was wir damit erreichen können. Hallo, ich bin Omar, und jetzt werde ich über die Anwendungsfälle für unser Datensatz sprechen. Der erste Anwendungsfall, den wir betrachten, ist die Evaluierung von aufgereihte Methoden. In den letzten Jahren gab es eine große Anzahl von aufgereihte Methoden, aber in Kontexten von Maschinellen Übersetzungen, wo wir zwei aufgereihte Dokumente schreiben, die in verschiedenen Sprachen sind, und wir wollen extrahieren aufgereihte Sätzen in Post-Dokumenten. Aber in unserem Anwendungsfall versuchen wir, aufgereihte Sätzen zwischen zwei aufgereihte Dokumente zu extrahieren, die dieselbe Sprache und dieselbe Inhalt haben, aber auf verschiedenen Komplexitätsebene liegen. Und nun, da wir unser Datensatz haben, das Plain, das manuell aufgereihte Sätzen enthält, können wir diese Sätze als Gold Standard aufgereihte Sätzen verwenden, um aufgereihte Methoden zu evaluieren. Wir haben einige Anpassungen an den aufgereihte Methoden vorgenommen und alle Anpassungen und die Codes zur Ausführung von Experimenten im Papier publiziert. Am Ende conclude that die besten aufgereihte automatischen aufgereihte Methoden zu verwenden für Texte, für deutsche Textsimplifizierung, ist die Methode von Mass Align. Und Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten auszuführen im Papier. Der zweite Anwendungsfall, den wir im Papier gezeigt haben, ist der Fall von automatischer Textsimplifizierung durch Fine-Tuning von Sprachmodellen, um zu produzieren simplifizierte Texte aus komplexen Eingabe-Texten. Wir haben zwei verschiedene Modelle fine-tuned: wir haben fine-tuned das Modell von Long Impart, um simplifizierte Dokumentebene-Texte zu produzieren, und wir haben auch fine-tuned das Modell von Long Impart, um simplifizierte Satzlevel-Texte zu produzieren. Sie können auch die Checkpoints und die Details der scores und der Evaluation-Metriques von unseren Experimenten im Papier finden. Wir conclude that these basic fine-tuning could produce or could get scores better than the baseline scores, and we propose those results as a Benchmark, eine Basiskonfiguration, für die Problematik der automatischen Textsimplifizierung in Zukunft. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle bei der Konferenz kennenzulernen. Vielen Dank.</sample>
    <sample id="4">The speaker's name is Kai Yen.</sample>
    <sample id="5">Der T5-XL Modell wurde verwendet, um die Genauigkeit von 82–87 % zu erreichen.</sample>
    <sample id="6">This paper presents a joint work on multilingual and cross-lingual summarization, conducted by a team of researchers including Jan, Fan Dong, Du, Yunlong, Zhi Xu, Jianfeng, and Ye. The team unified previous multilingual and cross-lingual summarization models into a more general setting called many-to-many summarization. This model can generate summaries in any target language from a document in any source language. The researchers also conducted preliminary studies to provide deeper analysis among multilingual summarization, cross-lingual summarization, and their many-to-many summarization. They found that many-to-many summarization could help the summarization model better transfer task knowledge across different languages than previous multilingual summarization and cross-lingual summarization. Furthermore, they proposed a three-stage training process for many-to-many summarization, which outperforms previous strong baselines including MBR50 and M5.</sample>
    <sample id="7">Ja, sie funktionsieren noch.</sample>
    <sample id="8">Die vorgeschlagene menschliche Bewertungsmethode ist neu, weil sie eine präzisere und zuverlässigere Strategie für dimensionale Dialogbewertung verwendet. Sie versucht, die Subjektivität menschlicher Bewertungen zu reduzieren, indem sie explizit anhand von obigen Verhaltensmerkmalen bewertet, ob oder nicht jede Modell-Antwort bestimmte Verhaltensweisen zeigt, wie zum Beispiel die Bereitwilligkeit, irrelevantes Information zu geben, sich oder seinen Partner zu widersprechen, falsche Fakten zu behaupten oder das allgemeine Wissen zu verletzen, und ob das Modell Empathie zeigt.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Anzahl der klaren Validationsstichproben ab.</sample>
    <sample id="10">Das Modell hat nur 60% Genauigkeit, wenn es nur Zugriff auf die Namen der Entitäten hat. Es gibt also noch viel Raum für Verbesserungen.</sample>
    <sample id="11">This research paper presents the New Yorker Caption Contest dataset, which includes over 700 cartoons and their corresponding captions. The dataset is divided into three tasks: matching, quality ranking, and explanation generation. The paper evaluates the performance of large language models on these tasks and compares them to human performance. The results show that while language models can perform well on some tasks, they still have a significant gap in humor understanding compared to humans.</sample>
    <sample id="12">Die Arbeit beteiligen sechs Autoren.</sample>
    <sample id="13">Daniel Rotem präsentiert seine Arbeit "Finding the Sweet Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings" im Labor von Professor Roy Schwartz an der Hebrew University in Jerusalem. Seine Arbeit konzentriert sich auf die Reduzierung der Inferenzzeit von großen Sprachmodellen durch die Anwendung von adaptiver Inferenz. Rotem diskutiert zwei Hauptmethoden, Multimodal und Early Exit, und ihre Vorteile und Nachteile. Er hypothesiert, dass Early Exit-Modelle due to conflicting gradients, die Modellparameter überlappen und die Leistung aller Klassifizatoren beeinträchtigen können. Um dies zu testen, verglich er Early Exit-Modelle mit separaten Multimodal-Klassifizatoren und führte eine neue Methode namens "Sweet" ein, bei der Gewichter in Early Exit-Modellen getrennt werden. Das Resultat zeigt, dass Sweet die Leistung von Early Exit-Modellen verbessert, insbesondere bei schnellen Inferenzspeeds, und dabei die Nachteile von Multimodal-Methode vermeidet.</sample>
    <sample id="14">Hallo, mein Name ist Adam Siprikowsky und das Papier handelt von der Struktur der Koordination. Wie Sie wissen, gibt es verschiedene Strukturen der Abhängigkeit, die verschiedenen Theorien und Ansätzen zugrunde liegen. Zum Beispiel in der Universal-Abhängigkeitslehre ist die Struktur der Koordinatkoordination Lisa Bart und Meggy so, dass der erste Conjunkt der Kopf der gesamten Koordinatstruktur ist – in diesem Fall Lisa. Eine ähnliche Annahme wird in Igor Miltruch's Meaning Text-Theorie angenommen, wo erneut die gesamte Koordinatstruktur von dem ersten Conjunkt überwiesen wird. Beide Ansätze sind asymmetrisch, sie singulieren einen der Conjekte. Es gibt auch symmetrische Ansätze zur Koordinatkoordination, wie z.B. die Prag-Annäherung, die Koordinatkoordination mit dem Conjunkt als Kopf angenommen hat. Hier erhalten wir Abhängigkeiten von Ende zu allen Conjekten. Schließlich gibt es auch einen multi-kopfigen Ansatz, wie beispielsweise in dem Satz "Deutscher Grammatik" verwendet, wo alle Conjekte als Köpfe der Koordinatkoordination dienen. Hier erhalten wir Abhängigkeiten vom Governor (hier: Lisa) zu allen Conjekten separat. Das Hauptziel des Papiers ist, einen neuen Argumentationsansatz für symmetrische Koordinatkoordinaten zu präsentieren und gegen asymmetrische Koordinatkoordinaten zu argumentieren. Das Argument basiert auf dem Prinzip der Abhängigkeitslängenminimierung, das auf den gegebenen Beispielen erläutert wird. In englischsprachigen Sprachen bevorzugen direkte Objekte, die nah am Verb stehen, während Ergänzungen weitrückig liegen. Beispielsweise ist "March read it yesterday" gut, weil das direkte Objekt "it" nah am Verb steht, während "March read yesterday it" schlecht ist, da zwischen dem Verb und dem direchten Objekt ein Ergänzwort steht ("yesterday"). Allerdings kann der Effekt, wenn das direkte Objekt sehr lang ist, durch Verschieben nach dem Ergänzwort neutralisiert werden. So both "March read this absolutely fascinating book about the BC yesterday" and "March read yesterday this absolutely fascinating book about the BC" klingen gut, obwohl das zweite Satz eine grammatische Regel verletzt, die direkte Objekte neben dem Verb stehen sollte. Das ist because das zweite Satz die Längstenspannung minimiert, also die Summe der Abhängigkeiten, die nicht konstant sind, zwischen den zwei Strukturen. Hier haben wir die Abhängigkeiten von "read" zu "yesterday" (Länge 7) und von "read" zu "book" (Länge 4), insgesamt 11. Wenn wir die Constituents tauschen, wird die Summe der Abhängigkeiten auf 6 reduziert, was viel kürzer ist. Das Papier extrahiert statistische Daten aus der Erweiterter Version von Penn Treebank und zeigt, dass links-gelegte Conjekte normalerweise kürzer sind. Es wird auch festgestellt, dass diese Tendenz mit wachsendem Längstenspannungsunterschied intensiviert wird. Wenn der Unterschied zwischen den Längen der beiden Conjekte zunimmt, bevorzugt das kürzere Conjekt als erstes zu stehen. Dieses Phänomen wird im Papier detailliert diskutiert.</sample>
    <sample id="15">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="16">Die Bibleschwierigkeiten werden stärker vereinfacht.</sample>
    <sample id="17">This paper introduces a novel method for multi-modal relation extraction, which combines textual and visual information to improve the accuracy of semantic relation determination between entities in various forms and modalities. The proposed method includes fine-grained information pruning over two modalities and external information exploitation using topic information. Experiments on the MRD dataset show that the proposed method outperforms text-based methods and achieves significant improvements over existing models.</sample>
    <sample id="18">Ein Beispiel für die Preferenz für kürzere linke Konjunktionen lautet "Salt and pepper" (Salz und Pfeffer).</sample>
    <sample id="19">This presentation introduces a two-stage framework for efficient open-domain question answering, proposed by Dandan Chen in 2017. The first stage retrieves evidence contexts from Wikipedia Corpus using a retriever, while the second stage uses a reader to understand the question and retrieve relevant evidence to generate an answer. Challenges include large Wikipedia Corpus size, indexing file storage requirements, and multiple language models with millions of parameters. To address these challenges, we propose techniques such as approximate nearest neighbor search, skipping rate, document filtering, and model compression. We also compare existing open-domain question answering models based on memory usage and performance, concluding that retrieval-only systems are suitable for low-resource environments, while retrieval-reader systems are more appropriate for trade-offs between speed and performance. Future work focuses on deploying open-domain question answering systems on low-power devices and considering additional evaluation metrics.</sample>
    <sample id="20">Ja, Sie können die Modelle verwenden. Die Pre-Trained Model (Natosh) sind freiavailable und die Training scripts sind auf GitHub repository.</sample>
    <sample id="21">DEplain-apa enthält News-Artikel.</sample>
    <sample id="22">Eine gute Generalisierung wird durch die Model-Architektur, die Modellgröße und mehr Fine-Tuning-Beispiele erreicht.</sample>
    <sample id="23">This paper presents research on improving the ability of text-image models to render visual text. The authors investigate the performance of text encoders, specifically T5 and BERT, in representing text for image generation tasks. They find that T5 struggles with spelling, even at larger scales, while BERT models are better but impractical due to their size. The authors propose augmenting text-image models by concatenating a character-level text representation from a smaller model like BERT Small. This approach improves the model's ability to spell correctly and generate images with better text characteristics, though it may still introduce errors due to the diffusion model's limitations.</sample>
    <sample id="24">Die Tendenz wurde durch Messung der Längen in Zeichen, Silben und Wörtern bestimmt.</sample>
    <sample id="25">Die Experimente wurden so gestaltet, dass die Position des Begrenzers in den Koordinatenstrukturen variiert wurde und die Längen der kritischen Abhängigkeiten gemessen wurden.</sample>
    <sample id="26">Ein Basisklassifikator trainiert mit unausgewogenen Daten nicht viel besser als Zufall.</sample>
    <sample id="27">Die Anzahl der Autoren, die an der Arbeit beteiligt sind, wurde nicht erwähnt.</sample>
    <sample id="28">Bob und Alice.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden besser ab als kontextagnostics Modelle bei Diskursphänomenen wie Formalität und lexikalische Kohärenz.</sample>
    <sample id="30">This paper introduces a simple yet effective ensemble learning framework for large language models called LLM Blender. The key idea is based on pairwise ranking and generative fusion. The authors propose a two-stage framework that runs multiple models, uses a pairwise ranking model to compare candidates, and then fuses the top-ranked candidates using a generative fusion model. Experiments show that LLM Blender outperforms individual models in terms of performance metrics.</sample>
    <sample id="31">Die Autoren sind an der University of Edinburgh.</sample>
    <sample id="33">Das Framework quantifiziert die Positionalität, indem es die Annotierungen mit realen Benutzern vergleicht und sie mit den existierenden Datensätzen und Modellen vergleicht. Es verwendet dabei ein Pearson's r Korrelationsscore, um die Übereinstimmungen zu messen.</sample>
    <sample id="34">The paper presents a framework called Crest for generating counterfactual explanations and counterfactual text generation. The framework combines selective rationalization and counterfactual generation methods to produce valid, fluent, and diverse counterfactuals that focus on the critical parts of the input. The authors compare Crest against related works using both automatic metrics and human evaluation, showing that Crest generates more valid and natural counterfactuals. They also propose using Crest for data augmentation and show that it improves downstream models. The results demonstrate that Crest can be used to generate high-quality counterfactuals that are interpretable and controllable.</sample>
    <sample id="36">This paper presents a method for enhancing multilingual machine translation by introducing language-specific layers (LSLs) into the Transformer architecture. The approach involves training a single model with shared, source, and target weights for each encoder layer to learn optimal LSL placement. Experimental results show significant improvements in translation quality for low-resource languages while maintaining constant inference costs. The method outperforms both language adapters and baseline models, demonstrating its effectiveness across various language pairs.</sample>
    <sample id="37">Die Menschen, die dieselben Persona-Prompts erhalten hatten, wurden in der vorherigen Studie in der Lage, Racialstereotypen zu surfen.</sample>
    <sample id="38">Die Studie verwendet die statistischen Daten aus dem Enhanced Version of the Penn Treebank.</sample>
    <sample id="39">Es sind zwei Autoren an der Arbeit beteiligt.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitive Dissonanz sind die Topik unabhängige Dissonanzstelle Klassifizierung und die binäre Klassifizierung von Erweiterung und Vergleichsklassen von PDB.</sample>
    <sample id="41">This paper introduces a world-level personal common sense knowledge graph, Pika, which contains about 3.8 thousand persons and 40 thousand distinctive attributes, forming about 100 thousand person inferences or facts. Pika is built in three steps: selecting persons from existing common sense knowledge graphs, inducing attributes of persons from both common sense knowledge graphs and large-scale pre-trained language models, and cross-sourcing annotations of Pika relations using a joint human-AI majority voting scheme. The paper compares the performance of Pika with large-scale pre-trained language models on various natural language generation metrics and investigates whether Pika can help language models learn and generalize personal knowledge. The results show that Pika achieves better automatic evaluation results and higher acceptance rates in human evaluation compared to baselines. Additionally, Pika's person-centric common sense knowledge has a more positive impact on downstream narrative modeling tasks, such as dialogue generation, when there are more consistent connections between speakers.</sample>
    <sample id="42">Die Anzahl der Autoren, die an der Arbeit beteiligt sind, wird nicht in dem gegebenen Text erwähnt.</sample>
    <sample id="43">Es gibt nur einen Autor, der an der Arbeit beteiligt ist.</sample>
    <sample id="44">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten, indem es nicht nur die Übereinstimmungen zwischen Annotatoren und Datensätzen oder Modellen analysiert, sondern auch die Positionalitäten von Nutzern mit den Datensätzen und Modellvorhersagen vergleicht.</sample>
    <sample id="45">Die Menschenwritten-Responses.</sample>
    <sample id="46">Google Translate und DeepL</sample>
    <sample id="47">Hallo, ich bin Changbin, ein PhD-Student an der University of Washington. Heute präsentiere ich unser Werk von der Vorbildtraining-Datenbank über Sprachmodelle bis hin zuunterstream-Aufgaben und die Verfolgung von politischen Biases, die zu unfairen NLP-Modellen führen. Sprachmodelle werden auf großen Webkrawln-Datasets trainiert, in denen politische Medien gut abgedeckt sind. Eine Umfrage des C4 Corpus zeigt, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. in Sprachmodelltrainingsdaten gut abgedeckt sind. Dies hat sowohl Vorteile als auch Nachteile für Sprachmodell-Anwendungen. Auf der einen Seite können sie aus diversen Perspektiven lernen, was dem Demokratie und der Pluralität von Ideen zukommt. Auf der anderen Seite sind diese verschiedenen politischen Meinungen inherent sozial biasiert und können zu potentiellen Fairness-Problemen inunterstream-Aufgaben führen. Um dies zu untersuchen, haben wir den politischen Bias-Propagation-Pipeline von Vorbildtraining-Daten über Sprachmodelle bis hin zuunterstream-Aufgaben investiert. Wir haben uns speziell die folgenden Fragen gestellt: Wie können wir die politische Linie von Sprachmodellen bewerten und welche Rolle spielen Vorbildtraining-Datasets bei solchen politischen Biases? Wie performieren Sprachmodelle mit verschiedenen politischen Linien aufunterstream-Aufgaben und ob das zu Fairness-Problemen in NLP-Anwendungen führt? Wir haben zuerst vorgeschlagen, Sprachmodelle mit verschiedenen Prompt-Formulierungen zu prompten, indem wir politische Fragstelle wie die "Politik-Compas-Test" verwenden. Das gewährleistet eine automatische Evaluation, die fundiert in der politischen Wissenschaft ist. Einige vorläufige Resultate demonstrieren, dass Sprachmodelle unterschiedliche politische Linien aufweisen und GPT-4 die liberalste Sprachmodell ist. GPT-Series sind allgemein mehr sozial liberal als BERT-Series und seine Variationen. Wir haben auch versucht, zu welchem Extent die politischen Biases von Sprachmodellen von den Training-Datasets abhängig sind. Dazu haben wir ein Kontrollversuch durchzuführen, indem wir Sprachmodelle auf verschiedene Partizipierer trennen, die in News und Sozialmedien unterteilt sind, und die politische Linie thereof weiter prätrainingen. Durch die prätrainingen von Sprachmodellen auf solche Partizipierer können wir sehen, dass die ideologischen Koordinaten der Sprachmodelle entsprechend verschieben werden. Zum Beispiel, wenn Roberta weiter auf der linken Linie der Redd Corps prätrainingen wird, können wir einen substantiellen linken Schub in Bezug auf seine politischen Biases beobachten. Wir haben auch versucht, zu überprüfen, ob Sprachmodelle die Polarisation im modernen Gesellschaft ablehnen können. Wir haben die Vorbildtraining-Korpora in vor- und nach-45. Präsidenten der Vereinigten Staaten geteilt und Sprachmodelle auf die zwei verschiedenenTemporal-Korpora prätrainingen. Wir können sehen, dass Sprachmodelle allgemein eine politische Linie haben, die weiter weg vom Mittelpunkt nach 2017 ist. Das zeigt, dass Sprachmodelle auch die Polarisation in unserer Gesellschaft aufnehmen können. Schließlich haben wir Sprachmodelle mit verschiedenen politischen Linien auf Hate-Speech-Detection und Fake-News-Detection-Aufgaben evaluiert, die oft in NLP-Anwendungen verwendet werden und sehr wichtige Auswirkungen haben könnten. Wenn wir die Kategorienerfolge analysieren, also die Trennung der Leistungen nach verschiedenen Demographen oder politischen Linien der Medien, können wir ein Muster feststellen: Zum Beispiel, bei der Hate-Speech-Detection sind linkslinien Sprachmodelle besser darin, Hate-Speech, die sich auf soziale Minderheiten richten, zu detektieren, aber schlechter darin, Hate-Speech, die sich auf mehr mächtige Gruppen in unserer Gesellschaft richten, zu detektieren. Und umgekehrt: rechtslinien Sprachmodelle sind besser darin, Hate-Speech, die sich auf Weiße und Männer richten, zu detektieren, aber schlechter darin, Hate-Speech, die sich auf Afroamerikaner, LGBTQ+ und andere Minderheitengemeinden richten, zu detektieren. Ähnliche Trends gelten auch für Fake-News-Detection, wo wir sehen, dass linkslinien Sprachmodelle besser darin sind, Missinformationen aus der gegenüberliegenden politischen Linie zu detektieren, und umgekehrt. Wir zeigen auch viele qualitative Beispiele an, um zu zeigen, dass Sprachmodelle mit verschiedenen politischen Linien unterschiedliche Vorhersagen für Hate-Speech- und Missinformationsbeispiele basierend auf ihren sozialen Kalübergewichten geben. Es gibt noch mehr Beispiele im Anhang, um das zu verdeutlichen. Das zeigt, dass es ein Fairnessproblem gibt, das sehr dringlich ist, bezüglich der politischen Biases von Sprachmodellen. Zum Beispiel, wenn rechtslinien Sprachmodelle für die Fine-Tuning von Hate-Speech oder Missinformation und so weiter eingesetzt werden und then auf einer populären Sozialmedien-PlattformPLOY, dann könnten Menschen mit gegenüberliegender politischer Meinung marginalisiert werden und Hate-Speech, die sich auf Minderheiten richten, könnte einfach weiterhin wütend und unkontrolliert weitergegeben werden. Das sollte uns auffordern, die Fairnessprobleme, die durch Sprachmodell-politische Biases entstehen, anerkennen und angepacken zu haben. Ein paar Punkte zur Diskussion: wir möchten auch betonen, dass wir die einzigartige Dilemma bezüglich Sprachmodell-politischen Biases hervorgerufen haben: Wenn wir die politischen Meinungen in Sprachmodelltraining-Datasets nicht sauber machen, dann propagieren die Biases von Vorbildtraining-Datasets über Sprachmodelle bis hin zuunterstream-Aufgaben und schaffen Fairnessprobleme. Wenn wir versuchen, etwas zu sauben, riskieren wir Sensibilisierungen oder Exklusionen und es ist sehr schwierig zu bestimmen, was als neutrales und should-retaining Sprachmodelltraining-Dataset angesehen werden soll. Das ist etwas wie der "elektroelektro"-Problem. Okay, ich denke, das war alles, was ich heute habe. Vielen Dank für eure Zeit.</sample>
    <sample id="48">Die Arbeit ist ein Joint Work mit Kollegen von Google Translate.</sample>
    <sample id="49">MPP-Auswertungen wurden bis zu 1244 Token Kontextlänger durchgeführt.</sample>
    <sample id="50">The presentation introduces DePlain, a new corpus for German text simplification at both the document and sentence levels. It highlights the challenges with existing corpora, such as small size and automatic alignment errors. DePlain is divided into two subcorpora: DePlain API, containing 483 manually aligned documents from news texts, and DePlain Web, which includes 750 documents from various domains, both manually and automatically aligned, totaling 34,500 sentence pairs. The corpus offers diverse simplification transformations, including lexical, structural, and overall simplifications. The presentation also discusses use cases for DePlain, such as evaluating automatic alignment methods and fine-tuning language models for text simplification. It concludes that basic fine-tuning of models like Long Impart can produce better scores than baseline results, serving as a benchmark for future research in automatic text simplification.</sample>
    <sample id="51">Sie haben in ihren Datensatz drei verschiedene Domains aufgenommen: Musik, Bcher und Rezepte.</sample>
    <sample id="52">Positionalität ist die Perspektive, die Menschen als Folge ihrer Demografie, Identität und Lebenserfahrungen halten.</sample>
    <sample id="53">Der Referent*in heißt Dawe.</sample>
    <sample id="54">This paper presents a cognitive dissonance resource created through transfer learning and active learning. The authors conducted a large-scale annotation of discourse unit pairs, using two related tasks to improve the classifier's performance. They found that the proposed probability of rare class strategy works better than other state-of-the-art strategies for rare class acquisition. The cumulative update strategy is useful for transfer learning from different domains, while iterative update is beneficial for in-domain active annotations. The resource has the potential to improve the detection of cognitive dissonance in language and can be used to understand various phenomena such as disagreement, mental health, and extremism.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell.</sample>
    <sample id="56">Ein Autor arbeitet an der Arbeit beteiligt.</sample>
    <sample id="57">Das getestete Modell in der Testsuite scheint in der Regel unfähig zu sein, Wissen aus verschiedenen Quellen zu überprüfen, ohne spezifische Training auf KIMMOS.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind: 1. Background Pretrain (BKG Pretrain), 2. Background Both (BKG Both), und 3. Background Inference (BKG Infer).</sample>
    <sample id="59">The presentation introduces Dr. Bert, a robust pre-trained model in French for biomedical and clinical domains. It compares Dr. Bert with other models trained on different data sources and sizes, evaluating their performance on various tasks. The results show that Dr. Bert outperforms other models on most tasks, especially when trained from scratch. The proposed system offers better performance on nine of the eleven downstream tasks compared to the generic model.</sample>
    <sample id="60">Die Autoren gehören an der Technischen Universität Wien.</sample>
    <sample id="61">Die abschließende Forschungsfrage lautet: Sollten wir nur die sauberen Samples für die Validierung verwenden, oder gibt es bessere Wege, sie zu nutzen?</sample>
    <sample id="62">This paper presents a systematic study of knowledge distillation for natural language generation (NLG) with pseudo-target training. The authors explore the potential of knowledge compression by comparing different architectures, pruning techniques, and knowledge distillation approaches. They focus on task-specific NLG tasks in realistic setups, using unlabeled data and medium-sized teacher models to achieve high compression rates while maintaining performance. The study challenges traditional sequence-level knowledge distillation and proposes novel techniques like joint teaching to address student exposure bias and improve learning.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, für dieselbe Aufgabe stets die gleichen Ausgaben zu produzieren, unabhängig von kleinen Variationen in der Formulierung der Anweisung.</sample>
    <sample id="64">Jin Wei</sample>
    <sample id="65">Eine höhere Sensitivität ist ein negativer Begriff und bedeutet, dass das Modell für kleine Veränderungen in der Eingabe unterschiedliche Ausgaben macht. In diesem Fall ist eine höhere Sensitivität also nicht eine bessere Leistung des Modells.</sample>
    <sample id="66">This paper presents a survey of deep learning methods for mathematical reasoning, focusing on the development of AI and NLP techniques to solve math problems and prove theorems. It discusses various approaches, including neural network architectures like the Transformer and LLMs, and their limitations in handling complex reasoning tasks. The paper also highlights the need for more diverse datasets and benchmarks in low-resource languages and domains.</sample>
    <sample id="67">This paper investigates interference in multilingual translation models and proposes methods to mitigate it. The authors find that severe interference occurs when the model is small compared to the data size, and tuning the sampling temperature is key for strong performance. They also conclude that language similarity and the number of languages do not have a large impact on interference levels. The results suggest that modest scale and tuned temperature can reduce the problem significantly without any other specialized method.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings einen Kontext, der durch die Wahl von Sätzen aus verschiedenen Datensätzen bestimmt wird.</sample>
    <sample id="69">Typischerweise benötigen wir nur 20 Beispiele pro Klasse, um eine gute Leistung an der WSL zu erreichen.</sample>
    <sample id="70">Die Autoren sind an der Stanford University.</sample>
    <sample id="71">This paper presents a dataset of 42,000 indirect referring expressions for entity selection across three domains: music, books, and recipes. The dataset was collected using crowdsourcing and includes 6,000 alternative questions with 42,000 indirect referring expressions. The dataset emphasizes informality and includes background knowledge about the entities to make disambiguation more challenging. The results show that language models with access to exact or partially overlapping background knowledge achieve high accuracy, while those with only entity names have significantly lower accuracy.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, um die Auswirkungen von politischen Verzerrungen auf Sprachmodelle zu messen und zu überprüfen, ob sie faire Anwendung in der Praxis haben.</sample>
    <sample id="73">Makshatta</sample>
    <sample id="74">This paper introduces a new method for constructing a semantic network called DenseNomic, which is based on the Atomic knowledge base. DenseNomic completes missing links in Atomic and contains more multi-hop paths, such as B2W, B2B, A2B, and A2W links. The construction of DenseNomic involves three parts: normalizing tail events, training a relation prediction model, and constructing DenseNomic. The proposed method uses a relation prediction strategy that utilizes both head and tail events to predict relations, avoiding the problem of sparsity and taking advantage of semantic information. The results show that DenseNomic has higher knowledge coverage and better performance than Atomic, and it can generate more diversified results.</sample>
    <sample id="75">This paper presents a joint semi-supervised learning framework for entity and relation extraction tasks. The proposed framework models the two tasks by propagating labels over heterogeneous graphs, considering interconnections among labeled and unlabeled data. The method consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Experiments on four datasets show significant improvements over baseline models for both entity and relation tasks.</sample>
    <sample id="76">Die Pipeline beginnt mit der Vorbereitung von Datensammlungen, die politische Vorurteile enthalten. Dann werden Sprachmodelle trainiert und evaluiert, um zu sehen, ob sie politische Vorurteile übertragen. Schließlich werden die Auswirkungen auf downstream Aufgaben wie das Erkennen von Hasssprache und Falschinformationen untersucht.</sample>
    <sample id="77">This video introduces the Defacto dataset, a comprehensive resource for evaluating and improving summarization factual consistency. Developed by Yale University and Microsoft Research, the dataset includes human demonstrations and feedback from annotators who assess the factual consistency of summaries generated by existing models. The video highlights three key tasks: summary editing, feedback generation, and automatic factual error correction, showcasing the performance of different models on these tasks. Additionally, the video discusses the dataset's benefits for training factuality metrics and evaluating model performance, making it a valuable tool for researchers in the field of natural language processing.</sample>
    <sample id="78">Ja, es gibt Unterschiede. Der DEplain-apa-Korpus basiert auf News-Texten und enthält 483 manuell alignierte Dokumente, was zu 30.000-13.000 Paaren von parallel-translated Sätzen führt. Der DEplain-Web-Korpus hingegen umfasst 750 Dokumente in verschiedenen Domänen, die manuell (30.450 Paare) und automatisch (750 Paare) aligniert wurden.</sample>
    <sample id="79">Ja, es ist öffentlich verfügbar.</sample>
    <sample id="80">Das Wasserzeichen wird in den Text über eine Vorgabe von Schlüsselwörtern (Trigger Set) eingebettet. Wenn ein Benutzer einen Satz an die Diensteanbieter-Plattform sendet, werden die Häufigkeiten der Schlüsselwörter im Satz ermittelt und das Gewicht des Wasserzeichens proportional zur Anzahl der Schlüsselwörter im Satz berechnet.</sample>
    <sample id="81">Die Autoren gehören an der Peking University.</sample>
    <sample id="82">This paper proposes a novel framework for unsupervised automated essay scoring (AES) by learning from rank aggregation. The proposed framework, called URA, introduces multiple heuristics quality signals as pseudo ground-truths and trains a neural AES model by aggregating these signals. URA contains a heuristics essay ranking module (HERM) that generates partial order pairs by ranking essays according to heuristics quality signals, and a deep pairwise rank aggregation module (DPRM) that trains the neural AES model by aggregating the partial order pairs derived from multiple heuristics quality signals. Experimental results demonstrate that URA outperforms unsupervised baselines with a large improvement in both transductive and inductive settings.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mt5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="84">This paper presents a partially dynamic neural network framework for dynamic networks, addressing the issue of excessive parameter usage in fully dynamic networks. The proposed method partitions parameters into dynamic and static ones, using scale factors to control their intensity. Experiments show that this approach achieves better performance than both static and fully dynamic networks while maintaining fewer parameters and less computation.</sample>
    <sample id="85">Eine Beispiel für eingeschränkte Sprachplanung ist die Erstellung von Kochrezepten mit bestimmten Anforderungen, wie z.B. die Erstellung eines Schokoladenkuchens.</sample>
    <sample id="86">Sie überprüfen die Opazität der bereitgestellten Einbettungen durch die Visualisierung der Einbettungen von Sätzen auf der Figur BOPCA und bestätigen, dass es schwierig ist, die Einbettungen im Hintergrund zu unterscheiden.</sample>
    <sample id="87">Die Arbeit bestehende PLMs, um ein neues PLM aufzubauen, indem sie die Anonymisierungsdaten von einem Krankenhaus verwenden.</sample>
    <sample id="88">GPT-4 ist am wenigsten auf Indien ausgerichtet.</sample>
    <sample id="89">Ein Beispiel, das zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde, ist "I'm going to talk about" (Ich werde über sprechen).</sample>
    <sample id="90">This paper explores the feasibility of using language learners as annotators for natural language processing (NLP) data annotation. It conducts a proof-of-concept study to compare the accuracy and learning effects of labels annotated by language learners versus native speakers. The study targets three languages—English, Korean, and Indonesian—using four common NLP tasks. Language learners are categorized into basic, intermediate, and advanced levels, and their annotations are compared with those of native speakers. The results show that language learner-annotated labels are nearly accurate, especially for simpler tasks and easy to medium-level questions. Aggregating these labels with others by majority voting can almost match the performance of native speaker-annotated labels. Additionally, training simulations with learner-annotated data achieve about 95% of ground-truth performance and sometimes outperform models trained with native speaker labels. The study suggests a novel way for data construction in low-resource languages by recruiting language learners as annotators, potentially broadening NLP research for many languages.</sample>
    <sample id="91">Mit der Anzahl der Aufgaben, die das Modell trainiert wird, wird die Leistung des Modells verbessert und die Sensibilität reduziert.</sample>
    <sample id="92">Die drei baumlose Baselines, mit denen die Autoren ihre Methode vergleichen, sind: 1. Ein Modell, das die Ausgabe von einem Token zu einem anderen Token überträgt, 2. Ein Modell, das die Ausgabe von einem Token zu einem anderen Token überträgt und dabei einen Permutationsmodell verwendet, 3. Ein Modell, das die Ausgabe von einem Token zu einem anderen Token überträgt und dabei einen Permutationsmodell verwendet, das die linguistisch korrekte Permutation finden soll.</sample>
    <sample id="93">Die beiden Co-Autoren sind dem ersten Autor als Berater zugeordnet.</sample>
    <sample id="94">This paper proposes a backdoor-based watermark method called Embedding Marker for protecting the copyright of embedding as services. The method contains two main steps: watermark injection and copyright verification. Before these main steps, we first select a trigger set. In watermark injection, we define a target embedding when a user sends a sentence to the provider service. The provided embedding is a weighted summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than M, the provided embedding is exactly equal to the target embedding. In copyright verification, we construct a backdoor and benign dataset. The backdoor dataset contains sentences of which all words belong to the trigger set. All words in the sentences of the benign dataset do not belong to the trigger set. Then the provider requests embeddings from the stealer service with the datasets. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor datasets, which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four datasets: AGNews, Yahoo, SST2, and Yelp. We assume the provider apply WikiText dataset to count word frequency. The results on four datasets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualizing the embedding of sentences on Figure 8. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings.</sample>
    <sample id="95">Avi Birlar</sample>
    <sample id="96">Hallo, alle zusammen. Ich bin Jenny, ein erstes Jahr PhD-Studentin an der Carnegie Mellon University. Heute präsentiere ich eure Arbeit "NL Positionality: Characterizing Design Bias via Large Datasets and Models". Dieses Werk wurde in Zusammenarbeit mit Forschern an der Universidad de Washington und dem Allens Institute for AI, insbesondere Sebastian Santi, Roman Labros, Katrina Rinerica und Martin Sap, entworfen. Lassen Sie uns damit beginnen, indem wir uns vorstellen. Sie arbeiten für eine Zeitung und versuchen, comments unter einem Newsartikel zu entfernen. Sie könnten zu einem beliebten API wie Perspective API aufsehen, um Giftgift zu detektieren. Und das funkt prima, wenn Sie Karl Jones sind, wo das API tatsächlich giftige Beispiele detektieren kann. Aber das ist nicht der Fall bei Diptia Sharma, wo das API nicht so empfindlich gegenüber offensive Begriffe ist, die in indischen Kontexten häufiger vorkommen. Das ist ein Beispiel für einen Design-Bias, bei dem es systematische Leistungsunterschiede von Technologie zwischen Bevölkerungen gibt. Design-Bias, wie wir vorher gesehen haben, können durch die Positionalität von NL-Researchern und Modellentwicklern entstehen. Positionalität ist einfach die Perspektive, die Menschen als Folge ihrer Demografien, Identitäten und Lebenserfahrungen halten. Dies ist ein Begriff, der in kritischen Studien, insbesondere in feministischen und queer akademischen Bereichen, weit verbreitet ist. Und als Forscherin kann die Positionalität die Forschungsprozess und seine Ausgänge beeinflussen, da sie die Entscheidungen, die Forscher treffen, ändern kann. Eine Frage, die sich man fragt, ist, ob Datensätze und Modelle Positionalität haben. Wir versuchen nicht zu sagen, dass Modelle und Datensätze selbst eine demografische Identität und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von realen Menschen und können daher bestimmte Positionalitäten über andere repräsentieren. Einige Arbeiten haben anecdetische Beweise von Positionalität vorgestellt, wie kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen von Modell-Positionalität. Dennoch haben diese Arbeiten nicht die Datensätze und Modelle mit realen Benutzern verglichen. Studium der Datensatz- und Modell-Positionalität wird immer wichtiger, da NL-Tests immer mehr subjektiv und sozial orientiert werden. Es ist schwierig zu charakterisieren, wie diese Positionalitäten abweichen, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter API's versteckt sind. Um Datensatz- und Modell-Positionalität zu studieren, vergleichen wir die Annotationen mit realen Benutzern mit existierenden Datensätzen und Modellen. Wir tun dies durch unser Framework NL-Positionality. Unser Framework arbeitet in zwei Hauptschritten. Der erste Schritt besteht darin, Datensätze mit diversen Annotatoren zu re-annotieren. Wir bevorzugen dies über die Demographen der ursprünglichen Datensätze, da normalerweise nur wenige Annotatoren jede Instanz notieren und because Demographen normalerweise nicht gesammelt und geteilt werden. Daher bevorzugen wir die Datensätze zu re-annotieren, um eine Vielzahl von Annotatoren pro Instanz zu erhalten und eine reiche Datensatz-Demographie zu erhalten. Wir nehmen dann die Annotationen nach Demographie und vergleichen sie mit den Modellen und Datensätzen mit einer Pearson-Correlation-Score. Dadurch differiert unser Framework von literaturwissenschaftlicher Disagreement-Literatur, indem er Benutzer mit Modellen und Datensätzen vergleicht, anstatt nur Annotator-Disagreement zu betrachten oder Annotator-Distributionen zu modellieren. Unser Framework ist hauptsächlich durch Lab in the Wild, eine Online-Krowdsourcing-Plattform, die unsere Ermittlung ermöglicht. Lab in the Wild ist eine Online-Experimentationsplattform, bei der wir diverse volontaires recruten, um Vergleiche mit Plattformen wie MTurk zu machen, die hauptsächlich von den USA oder Indien stammen. Lab in the Wild ist auch in der Lage, hochwertige Daten zu sammeln. Wir bieten zwei Aufgaben auf Lab in the Wild an: eine Sozialakzeptabilitätsaufgabe und eine Aufgabe zur Identifikation von Giftgift. In der Sozialakzeptabilitätsaufgabe liest ein Teilnehmer eine Situation aus dem Sozialchemie-Datensatz und schreibt dann, wie sozial akzeptabel eine Situation ist. Danach können sie ihre Antworten mit jenen von einem AI und anderen vergleichen, um zu überprüfen, ob sie engagiert sind. Wir vergleichen dann die Annotierungen mit Sozialchemie-Delphi und GPD4. Wir wiederholen eine ähnliche Aufgabe für die Aufgabe zur Identifikation von Giftgift, bei der sie eine Situation aus dem Dina-Hate-Datensatz liest und schreibt, ob sie eine instance of Giftgift ist. Wir vergleichen dann die Annotierungen mit Dina-Hate, Perspective API, Rewire API, Hate Roberta und GPD4. Unsere Studie im Endeffekt hat über 16.000 Annotierungen von über 1.000 Annotatoren aus 87 Ländern generiert. Jetzt sind wir in der Lage, zu beantworten, wer NL-Modelle und Datensätze am besten mit sich aligniert. Wir finden, dass es Positionalität in NL gibt. Zum Beispiel finden wir, dass Datensätze und Modelle am besten auf englischsprachige Länder aligniert sind. Zum Beispiel im GPD4 Sozialakzeptabilitätsanalyse finden wir, dass es am besten auf englischsprachige und konfessionale Länder aligniert. Wir finden auch, dass Dina-Hate am besten auf englischsprachige Länder aligniert ist. Wir finden auch eine weitere Alignement mit Menschen mit College-Ausbildung. Zum Beispiel im GPD4 Sozialakzeptabilitätsanalyse finden wir, dass es am besten auf Menschen mit College-Ausbildung oder Graduiertenausbildung aligniert. Wir finden dieselbe Alignement auch für Dina-Hate, bei dem es am besten auf Menschen mit College-Ausbildung aligniert. Allerdings werden bestimmte Bevölkerungen, wenn Modelle und Datensätze auf bestimmte Bevölkerungen aligniert sind, zwangsläufig behindert. Ein Beispiel für dies ist, dass Datensätze und Modelle nicht so gut auf nicht-binäre Personen aligniert sind, im Gegensatz zu den Männer-und-Frauen-Gegenstellungen. Wir finden dies in der GPD4 Sozialakzeptabilitätsanalyse, sowie im Dina-Hate Analyse. Also, da es Positionalität in NL gibt, was können wir davon machen? Wir haben einige Empfehlungen für euch. Eine Empfehlung ist, alle relevanten Designentscheidungen im Forschungsprozess zu notieren. Eine weitere Empfehlung ist, NL-Forschung mit dem Lenz der Perspektivismus zu betreten. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle für bestimmte Gemeinden zu erstellen. Ein gutes Beispiel hierfür ist die Musakani-Initiative. Wir wollen betonen, dass inclusive NL nicht nur eine allgemeine Technologie ist, die für alle funktioniert. Das schliesst unsere Präsentation. Wenn ihr mehr erfahren möchtet, seid ihr herzlich willkommen, euch auf unsere Dashboards für die neuesten Analyseergebnisse und unser Papier zu bedarf. Vielen Dank.</sample>
    <sample id="97">Die Referentin spricht von zwei Problemen der aktiven SimulST-Modelle: spezifischen Architekturtraining und komplexen Trainingverfahren.</sample>
    <sample id="98">Die Reduktion sozialer und politischer Verzerrungen in Datensätzen beim Training von NLP-Modellen ist schwierig, da es schwierig ist zu bestimmen, was als neutrales und akzeptables gilt. Es gibt keine eindeutige Lösung, und die Verzerrungen können sich fortspreden, wenn sie nicht ausreichend neutral sind.</sample>
    <sample id="99">Hallo, ich bin C. Wu von Fudan University. Ich bin hier, um unsere Arbeit zu Introduzieren, die Sprachkenntnis von Sprachmodellen für die planerische Sprachplanung zu unterscheiden. In unserem täglichen Leben planen Menschen oft ihre Handlungen, indem sie Schritt-für-Schritt-Anweisungen in Form von Skripten folgen. Vorherige Arbeiten haben gezeigt, dass Sprachmodelle in der Lage sind, abstrakte Ziele von stereotypischen Aktivitäten, wie das Backen eines Kekses, zu planen und zu demonstrieren, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Allerdings haben vorherige Arbeiten hauptsächlich auf die Planung abstrakter Ziele von stereotypischen Aktivitäten geachtet, die Planung für Ziele mit spezifischen Zielen und spezifischen Bedingungen, wie zum Beispiel die Herstellung eines Schokoladenkeezes, immer noch ununtersucht blieben. In diesem Papier definieren wir das Problem der kontraindexiven Sprachplanung, das unterschiedliche Bedingungen auf die Planung von Zielen imposes. Ein abstraktes Ziel kann durch verschiedene real-life spezifische Ziele mit mehrfachartigen Bedingungen erweitert werden. Ein guter Planer sollte Skripte schreiben, die rational und den Bedingungen entsprechend sind. In diesem Papier erstellen wir zuerst eine evaluierbare und verbesserte Sprachplanungsfähigkeit von großen Sprachmodellen. Da keine Datensammlung von spezifischen Zielen existiert, um unser Studium zu unterstützen, müssen wir zuerst die Ziele erhalten. Im Tabellenteil sehen wir anhand der Tabelle, wie wir abstrakte Ziele mit mehrfachartigen Bedingungen für Menschen im Lernprozess mit der Anweisung GPT-3.5-Turbo erweitern. Wir sampling 100 spezifische Ziele und evaluieren die generierten Skripte von großen Sprachmodellen. Das Tabellenteil zeigt die allgemeine Genauigkeit der Ergebnisse an. Wir finden, dass alle großen Sprachmodelle unzufriedenstellende Resultate bei der Planung von spezifischen Zielen erzielen. Dann conducten wir detaillierte Analysen, um zu überprüfen, warum große Sprachmodelle solche Resultate erhalten. Das Diagramm im Bild zeigt, dass die Planungsleistung von GPT-3.5-Turbo variabel für Ziele von verschiedenen Kategorien variiert. Vorherige Studien haben gezeigt, dass die Ausgabequalität von großen Sprachmodellen in einem hohen Varianz liegt, was zu schlechter Leistung führt. Daher adoptieren wir die Idee des "Over-generated" Filter zu verbessern die Generationsqualität. Wir zeigen zuerst die verschiedenen Typen mit Beispielen für die Anweisung GPT-3.5-Turbo und erhalten spezifische Ziele basierend auf den abstrakten Zielen. Dann overgenerates GPT-3.5-Turbo Skripte für spezifische Ziele. Auf der nächsten Etappe entwickeln wir ein Filtermodell, um die passenden Skripte zu selektieren. Wir konvertieren Skripte und Ziele in GPT-3.5-Turbo embeddings und berechnen die kosinusähnliche Ähnlichkeitswerte als Ähnlichkeits得分, um die semantische Ähnlichkeit zu messen. Darüber hinaus bewerten wir die Skripte nach dem Auftreten der Schlüsselwörter der Zielschwelle. Wir only kapturten die Skripte, wenn die Zielschwelle am höchsten ist in der Zielschwelle. Mit unserem Ansatz kann GPT-3.5-Turbo Skripte von hohen Qualität generieren. Unser Ansatz verbessert die Planungsfähigkeit sowohl in der semantischen Komplettäts als auch in der Fehllosigkeit zu den Bedingungen. Da große Sprachmodelle teuer zu deployen sind, ist es wichtig, die Sprachplanungsfähigkeit von kleineren und spezialisierten Modellen zu fördern. Die Erstellung von Datensätzen ist ein wichtiger Schritt zu Ende. Allerdings haben vorherige Studien gezeigt, dass die Planung von spezifischen Zielen nicht effizient ist und die manuelle Datenermittlung teuer ist. Daher folgen wir der Idee der symbolischen Kenntnisdistillation, um die Datensätze für die planerische Sprachplanung zu distillieren. Wir verwenden unser Ansatz für die Erstellung einer Datensatz von kontraindexiven Sprachplanungen, die als "Ko-Skript" bezeichnet wird. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripte, um die Qualität der Validierung und Testdaten sicherzustellen. Wir bitten Cloud-sourcing Arbeiter, die inkorrekten Samples zu finden und zu überprüfen. Dieses Diagramm zeigt die kontraindexive Verteilung von Ko-Skript. Wir finden Ko-Skript eine hohe Häufigkeit im generierten spezifischen Zielen. Mit Ko-Skript können wir kleinere aber spezialisierte Modelle für die planerische Sprachplanung verwenden. Wir finden, dass die "Tifel-Funktion" auf Ko-Skript Skripte von hohen Qualität generiert, was mehr große Sprachmodelle übertrifft. Indicating that kleinere Modelle können große große Modelle übertragen, wenn sie auf geeignetem Datensätzen trainiert werden. In Summe: Wir haben das Problem der planerischen Sprachplanung definiert. Wir evaluieren die planerische Sprachplanungsfähigkeit von großen Sprachmodellen und entwickeln einen "Over-generated" Filter Ansatz für große Sprachmodelle. Wir verwenden große Sprachmodelle, um einen hohen-Qualitäts-Skript-Datensatz, "Ko-Skript", für die planerische Sprachplanung zu generieren. Wir hoffen, den Ko-Skript-Datensatz als wertvolles Ressource zur Fortschrittsforschung in der Sprachplanung zu verwenden. Vielen Dank für Ihre Zeit. Sie können weitere Details von Ko-Skript in unserem Papier finden.</sample>
    <sample id="100">This paper presents a method for retrieving and ranking candidate chains for multi-hop QA using a few-shot language model. The proposed approach combines an unsupervised retrieval method with a few-shot language model-based reranking. The method involves retrieving a pool of candidate chains using TF-IDF retrieval and hyperlink traversal, then reranking these candidates using the few-shot language model. The authors evaluate their approach on the MultiHop QA dataset and compare it to fully supervised systems and state-of-the-art multi-hop retrievers. The results show that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art multi-hop retrievers. The authors also evaluate the downstream QA performance when using PromptRank as a retriever and find that it exhibits very good downstream QA multi-hop QA performance.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit den besten Systemen, aber die Hauptunterschiede liegen in der Genauigkeit.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind: es sollte auf Embedding-As-Services anwendbar sein, das Wasserzeichen sollte die Funktionalität der bereitstehenden Einbettdienst nicht beeinträchtigen, das Wasserzeichen sollte für den Angreifer leicht zu entfernen sein, und das Wasserzeichen muss übertragbar sein, um die Angreiferservices während des Modell-extraktions-Prozesses zu übertragen.</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 verschiedenen Sprachen übersetzt.</sample>
    <sample id="104">Ein Datensatz wird normalerweise über mehrere Annotatoren anhand von demografischen Merkmalen extrahiert.</sample>
    <sample id="105">Cosine and L2 similarity</sample>
    <sample id="106">The paper presents a dataset called Quest, which includes over 3000 entity-seeking queries with implicit set constraints. The dataset is used to study the effectiveness of systems for handling selective information needs. The authors show that the dataset poses a challenging retrievable problem since systems need to effectively search over a large document corpus to find multi-answer sets where the attribution for different query constraints can come from different parts of the document. The authors also present baselines for the dataset using sparse and dense retrievers as well as a TF-IDF based reranker. The results show that there is a large room for improvement on retriever performance based on the recall of the complete answer set, and that queries with set intersection and set difference are particularly challenging and have the lowest F1 scores.</sample>
    <sample id="107">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in diesem Aufgabe eingesetzt, um die beste Leistung auf allen neun Datensätzen zu erzielen.</sample>
    <sample id="108">This paper revisits the minimal pair paradigm for evaluating language models on acceptability judgments, focusing on grammaticality and stereotypes. It introduces a new approach to assess models' acceptability across longer sentences using datasets with varying contexts. The authors find that MPPE judgments are robust for unrelated contexts but vary significantly when using sentences from the same dataset or different subsets. This suggests that current evaluation methods may not capture abstract knowledge effectively.</sample>
    <sample id="109">Natural Instructions: A Large-scale Dataset of Natural Language Instructions for Diverse Tasks

We introduce Natural Instructions, a dataset of natural language instructions and their corresponding inputs and outputs. The data was collected in a fully automatic manner without any human annotations. We prompt a pre-trained language model to generate instructions and their corresponding inputs and outputs. We further diversify the datasets format by generating additional paraphrases of each instruction. The resulting dataset contains 64K examples, and if we consider the instruction paraphrases, we have about 240K examples. We analyze the generated examples focusing on creativity, diversity, and correctness. We find that more than 50% of the generated examples are indeed correct, and even incorrect examples often contain valuable information for instruction tuning. In terms of creativity and diversity, Natural Instructions contains highly creative tasks, some of which are very different from the classic NLP tasks. To measure the utility of the generated data, we fine-tune an 11 billion parameter T5 model on Natural Instructions. We show that the model can outperform both zero-shot and few-shot instruction tuning across several benchmarks. On top of that, when the cost of generating examples is amortized, training on Natural Instructions outperforms our baseline on all benchmarks. Our baseline is an 11 billion parameter T5 model identical to the one trained on Supernatural Instructions, only we train it on Supernatural Instructions. The tested benchmarks were Supernatural Instructions, T0, Big Bench Hard, and Elementary.</sample>
    <sample id="111">Die Autoren nehmen an, dass der Anbieter eine allgemeine Textkorpora sammeln und die Häufigkeit jedes Wortes therein zählen kann.</sample>
    <sample id="112">Hallo, alle. Mein Name ist Zhu Heng. Heute werde ich unsere Papier präsentieren: "Do Conll 2003-named entity taggers still work well in 2023?". Lassen Sie uns beginnen. Unser Papier untersucht das Problem der allgemeinisierten Verwendung der Named Entity Recognition (NER) Aufgabe, auch bekannt als NER-Aufgabe. Wir beobachteten, dass Modelle seit fast 20 Jahren auf Conll 2003 verwendet wurden, um NER zu entwickeln und dies natürlicherweise führt zu mehrere Probleme. Zunächst einmal können diese Modelle auf modernes Daten generallisieren? Und wenn wir neue Tagger erstellen, was ist für eine gute allgemeinisierte Notwendig? Gleichzeitig, wenn wir eine schlechte allgemeinisierte Beobachtung machen, was verursacht die Leistungsverluste dieser Modelle? Um diese Probleme zu untersuchen, haben wir den Conll Plus Plus Datensatz entwickelt. Dies ist ein Datensatz, den wir aus Reuters-News von 2020 entnommen und mit den gleichen Conll 2003-Annotierungsguidelines annotiert haben. Wir haben dann über 20 Modelle auf Conll 2003 optimiert und sie auf both Conll 03 Testset und Conll Plus Plus Testset bewertet. Und letzt but not least, wir haben den prozentualen Änderungen in F1-Bewertung verwendet, um die allgemeinisierte Leistung jedes Modells zu messen. Also, was ist für eine gute allgemeinisierte Notwendig? Unsere Experimente ergaben, dass es drei Hauptingredients gibt, die benötigt werden: die Modellarchitektur, die Modellgröße und die Anzahl der Fine-Tuning-Beispiele. Die first one ist die Modellarchitektur. Unsere Experimente ergaben, dass die Transformer-Modelle normalerweise besser auf neue Daten generalisieren. Der second ingredient ist die Modellgröße. Wir fanden, dass normalerweise größere Modelle zu einer besseren allgemeinisierten Leistungsstelle führen. In last but not least, wir wissen alle, dass die Anzahl der Fine-Tuning-Beispiele direkt auf die Leistungsstelle des Downstream-Aufgabens wirkt. Hier haben wir auch festgestellt, dass mehr Fine-Tuning-Beispiele tatsächlich auch zu einer besseren allgemeinisierten Leistungsstelle führen. Unsere nächste Frage lautete: Was verursacht die Leistungsverluste einiger Modelle? Wir hatten zwei Hypothese: die first one ist adaptive Overfitting, das ist Overfitting, das durch die Wiedervereinigung des gleichen Testsets über und über manifestiert wird und das normalerweise als die Verminderung der Returns auf einem neuen Testset manifestiert wird. Der second Hypothese ist temporal Drift, das ist die Leistungsdegeneration, die durch die zunehmende Temporalausgleichung zwischen dem Train- und Test-Datenursprung entsteht. FÜR adaptive Overfitting haben wir gesehen, dass vom Grafik auf der rechten die rote Best-Fit-Linie eine Gradient hat, die größer als 1 ist. Das bedeutet, dass jede Einheit der Verbesserung, die wir auf Conll 2003 erreichten, zu mehr als einer Einheit der Verbesserung auf Conll Plus Plus führt, was bedeutet, dass es keine Verminderung der Returns gibt. Und das zeigt uns, dass adaptive Overfitting in diesem Fall nicht beobachtet wurde. So wasTemporal Drift dann? FÜR temporal Drift haben wir ein Experiment durchzuführen, um die trainierenden Modelle mit recent data zu retrainieren und zu finden, dass die Leistungsstelle mit einem größeren Temporalausgleich abnimmt. Und das bestätigt unsere Hypothese, dass die Hauptursache der Leistungsverluste temporal Drift ist. Unsere Schlussfolgerung lautet, dass für eine gute allgemeinisierte Leistungsstelle wir einen besseren Modellarchitektur, größere Modellgröße und mehr Fine-Tuning-Beispiele benötigen. Und diese Goals hand in hand wir können nicht nur ein Ingredient haben, sondern die anderen überall. Gleichzeitig haben wir auch festgestellt, dass die Leistungsverluste hier durch temporal Drift verursacht werden und kind of surprisingly ist es nicht durch adaptive Overfitting verursacht, obwohl Conll 2003 über 20 Jahre verwendet wurde. Also, zurück zu der Frage, die wir im Titel unseres Papiers gestellt haben: "Do Conll 2003 taggers still work well in 2023?" Und wir haben festgestellt, dass die Antwort in der Tat ein eindeutiger Ja ist. Wir hoffen, unser Papier inspiriert weitere Forschungen an, wie man die allgemeinisierte Leistungsstelle der Modelle verbessern kann. Und letzt but not least, bitte machen Sie sicher, sich our Papier, our Datensatz an und, wenn Sie Fragen haben, sich gerne mit mir in Verbindung setzen. Vielen Dank so viel.</sample>
    <sample id="114">The presentation introduces a new method for pruning large language models, focusing on the heavy parameter problem. The proposed method uses a divide-and-conquer strategy to compress multi-head attention, dividing it into groups and pruning heads with low scores. This approach achieves significant parameter compression while maintaining performance on tasks like machine translation, language modeling, and abstract summarization. The method also reduces model size, inference speed, and computational resources.</sample>
    <sample id="115">Die Sprachsegmentgröße wird als Lambda-Speech-Frames definiert.</sample>
    <sample id="116">Servin ist ein Richter.</sample>
    <sample id="117">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">This paper presents a new pre-training technique for code-switched NLP, which is an important task in linguistically diverse communities like India. The proposed method, called SwitchLM, defines a switch point as a group of two tokens that transition between languages. It uses residual connections and auxiliary losses to increase the amount of switch point information in the intermediate and final layers of the model. The results show that the combined method of SwitchLM with residual connections and auxiliary losses performs the best on sentiment analysis tasks.</sample>
    <sample id="119">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf Sprachmodelle mit verschiedenen politischen Neigungen, indem sie verschiedene Sprachmodelle prüfen und die Auswirkungen von Partizipation auf die Sprachmodelle und die Auswirkungen auf die Fairness in NLP-Anwendungen untersuchen.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus einer bestimmten Ebene.</sample>
    <sample id="121">Beispiele für direkte Inferenz sind "the name of the song easy on me" oder "its position, the first one."</sample>
    <sample id="122">Die Autoren gehören an Fudan University.</sample>
    <sample id="123">This research presents MultiInstruct, a multi-modal instruction-tuning benchmark dataset consisting of 62 diverse multi-modal tasks covering 10 broad categories. Derived from 21 existing open-source datasets, each task includes five expert-written instructions. The study investigates whether instruction tuning on multi-modal pre-trained models can improve generalization to unseen multi-modal tasks, using OFA as the base model. The results show that instruction tuning significantly improves performance and reduces sensitivity, with better transfer learning from natural instruction datasets.</sample>
    <sample id="124">This paper presents a study on the temporal reasoning capabilities of language models (LMs). The authors propose the TempReason dataset, which covers all three levels of temporal reasoning and long temporal coverage. They evaluate the performance of LMs in three QA problem settings: closed book, open book, and reasoning QA. The results show that the proposed training strategy, which includes temporal span extraction pre-training and time-sensitive reinforcement learning, can significantly improve the temporal reasoning capabilities of LMs.</sample>
    <sample id="125">Die Anzahl der Autoren, die an der Arbeit beteiligt sind, wird nicht in dem Text erwähnt.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit einem maschinellen Übersetzungsmodell wurde als Baseline betrachtet.</sample>
    <sample id="127">This paper presents a method for transferring reasoning abilities from large language models to smaller models. The authors propose using large models as "reasoning teachers" to generate step-by-step solutions for complex tasks, which are then used as training data for smaller models. They introduce a novel technique called "diverse reasoning," which generates multiple solutions for complex questions to improve student performance. The results show that this method significantly outperforms existing baselines and vanilla fine-tuning on various tasks, even with small models. However, the method involves development and inference time costs, which need to be considered when applying it in real-world scenarios.</sample>
    <sample id="128">This paper presents a diagnostic test suite for evaluating knowledge integration in natural language understanding (NLU) models. The authors introduce a coreference resolution task designed to assess the ability of models to integrate knowledge from different sources. They evaluate the dataset with human study participants and establish coreference resolution models. The results show that while some models can successfully integrate knowledge from multiple sources after task-specific training, even the best-performing models struggle to reliably integrate background knowledge presented only at inference time.</sample>
    <sample id="129">Die Autoren haben als Beispiel für eine markierte Gruppe eine "schwarze Frau" gegeben.</sample>
    <sample id="130">Die Modellarchitekturen, die nicht gut generalisieren, sind diejenigen, die nicht auf einem Transformer-basierten Modell basieren.</sample>
    <sample id="131">Clean validation datasets.</sample>
    <sample id="132">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten.</sample>
    <sample id="135">This paper introduces ABC-Eval, a new dimensional approach to evaluating conversational AI models. Developed by the Emory NLP Lab and Amazon Alexa AI, ABC-Eval measures chat model behaviors such as relevance, contradictions, hallucinations, and empathy. The authors found that ABC-Eval is more reliable and predictive of overall conversation quality compared to existing methods. They also discovered that combining multiple ABC-Eval metrics can explain over 25% of conversation quality, while turn-level Likert metrics explain far less. The results suggest that ABC-Eval can be a valuable tool for evaluating conversational AI with higher resolution than previous methods.</sample>
    <sample id="136">This paper presents the work conducted by Zadzavan and Neufisa at the University of Sheffield on the evaluation of language models' performance in numerical reasoning tasks. The authors introduce Fermat, a flexible evaluation set based on arithmetic types, to assess number understanding, mathematical operations, and training dependency. They find that existing benchmarks are unrepresentative and do not facilitate the evaluation of mathematical abilities. Fine-tuning with math teachers' templates improves model performance, but linguistic knowledge is still crucial. The paper concludes that language and mathematical diversity are important for improving model performance in numerical reasoning tasks.</sample>
    <sample id="137">This paper introduces Tell to Design, a large-scale dataset featuring floor plans with natural language instructions. The dataset includes 551 human-annotated and 76,000 artificially generated language instructions, covering various design requirements such as semantics, geometry, and topology. The authors propose a sequence-to-sequence model using a Transformer-based encoder-decoder structure, pre-trained on a language model for better understanding. The model generates floor plan layouts from language instructions, outperforming text conditional image generation models by a large margin. The results demonstrate the effectiveness of the proposed method in aligning generated designs with user specifications, highlighting the potential for language-guided design generation in architectural and interior design applications.</sample>
    <sample id="138">Nach Ansicht der Autoren ist ein zu wenig erforschtes Gebiet im Bereich der NLU die Integrität von Kenntnis aus verschiedenen Quellen.</sample>
    <sample id="139">Die Referenten heißen Yi und Zhi Yang.</sample>
    <sample id="140">Ja, Coscript wurde von CloudSource Workers überprüft.</sample>
    <sample id="141">Bestehende Ressourcen für kontextbasierte Übersetzung haben Grenzen, da sie nur bestimmte Typen von kontextbasierten Übersetzungen und bestimmte Sprachensegmente unterstützen. Sie hängen normalerweise von domänenbezogenen Kenntnissen und menschlicher Kuration ab.</sample>
    <sample id="142">Hallo, ich werde über unser Werk sprechen, das sich auf die Auflösung von indirekten Beziehungen zwischen Entitäten konzentriert. Wir haben die Altentities Corpus (Alternatives Questions and Entities Corpus) eingeführt. Mein Name ist Javad Hosseini und ich arbeite in collaboration mit Philipp Radlinski, Sylvia Parati und Annie Luis. Unsere Zielgruppe sind Benutzer, die Sprache verwenden, um eine Wahl zu treffen. Betrachten Sie beispielsweise die alternative Frage: "Bedeutest du 'Easy on Me' oder 'I Got a Feeling'?" Hier muss der Benutzer zwischen zwei verschiedenen Liedern auswählen. Der offensichtlichste Ansatz besteht darin, direkte Beziehungen zu verwenden, indem man den Namen des Liedes "Easy on Me" oder seine Position als "das erste Lied" erwähnt. Manchmal ist es jedoch besser, eine indirekte Beziehung zu verwenden. Dies könnte passieren, wenn der Benutzer den Namen des Liedes nicht mehr kennt, die Aussprache zu ähnlich ist, um sie zu unterscheiden, oder wenn der Benutzer eine spezifische Preferenz angeben möchte. Hier sind einige Beispiele für indirekte Beziehungen: "Das neuer Lied" oder "Das Lied, das nicht energiegeladen ist". Eine wichtige Aufgabe in Konversationssystemen und auch beim Benchmarking von LLMs (Large Language Models) zur Verstehensfähigkeit von Entitäten besteht darin, solche indirekte Beziehungen zu verstehen. Wir kennen noch keine öffentliche Datensammlung auf großen Skalens für diese Aufgabe, also haben wir eine eigene Datensammlung via Crowd Annotation erstellt. Unsere Datensammlung abdeckt drei verschiedene Domänen: Musik, Bücher und Rezepte. Unsere Datensammlungsstrategie betont Infomalität und verwendet einen Cartoon-Completions-Setup. In dem ersten Sprachbalken sagt Bob: "Erinnere dich an das Lied, das wir gestern gehört haben." Und Bob stellt den Dialogkontext ein. Im zweiten Sprachbalken sagt Alice: "Bedeutest du 'Easy on Me' oder 'I Got a Feeling'?" Das ist die alternative Frage. Im dritten Sprachbalken benutzt Bob eine indirekte Beziehung, um einen der Lieder zu auswählen, zum Beispiel "das neuer Lied". Wir liefern automatisch den ersten und den zweiten Sprachbalken, aber den dritten Sprachbalken füllt der Noträtler ein. Der erste Sprachbalken wird aus einer Handvoll manueller Prompts pro Domäne gewählt. Der zweite Sprachbalken, der die alternative Frage ist, wird wie folgt generiert: "Weißt du, ob A oder B?" Hierbei sind A und B durch Wikipedia abgeglichen. Hier sind einigeSampling-Methoden, die wir verwendet haben: Wenn wir weiter aufwärts in der Liste laufen, werden die Entitäten immer ähnlicher, was normalerweise schwieriger zu disambiguiieren macht. Der erste Ansatz ist "Uniform at random". Der zweite Ansatz ist, wenn die Entitäten ähnliche Titel haben, z.B. zweibücher mit dem Titel "Die Rettung". Der dritte Ansatz ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben, und schließlich, wenn sie ähnliche Infoboxen oder Attributien auf Wikipedia haben, z.B. dass sie denselben Genres oder den selben Künstlern zugeordnet sind. Wenn wir diese alternative Frage den Noträtlern zeigen, wissen sie den Namen der Entitäten, aber sie kennen sie nicht unbedingt. Was wir tun, ist, ihnen einige Hintergrundinformationen über die Entitäten zu zeigen.Für Lieder simply show a Google-Suche-Ergebnis für das Lied "Easy on Me" und dann bitten die Noträtler, das Lied zu hören und über es zu lesen. Hier ist zum Beispiel das Google-Sucheergebnis für das Lied "Easy on Me".Für Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia an. Für Rezepte zeigen wir auch die Bilder von Wikipedia an, damit die Noträtler sehen, wie sie aussehen. Dann bitten wir die Noträtler, eine der Entitäten auszuwählen, zum Beispiel "das Erste Lied", und sie mit drei bis fünf indirekten Beziehungen zu beschreiben, z.B. "das Lied mit dem Pianomusik". Hier sind einige Beispiele aus unserem Datensatz: "Das Lied mit den Wörtern", "das Lied mit dem 12-jährigen Jungen" und "das fiktional Lied" usw. Das Altentities Corpus enthält 6.000 alternative Fragen über drei Domänen und 42.000 indirekte Beziehungen. Unsere Ergebnisse mit dem T5-XL-Modell sind im Folgenden zusammengefasst. Wenn der Sprachmodell Zugang zu den genauen gleichen Hintergrundinformationen wie die Noträtler hat, dann ist die Genauigkeit sehr hoch, um die 92- bis 95-prozentige. Aber das ist nicht realistisch. Wenn das Sprachmodell Zugang zu teilweise überlapenden Hintergrundinformationen hat, dann variiert die Genauigkeit zwischen 82- und 87-prozent, was realistischer ist. Zum Beispiel, wenn das Sprachmodell Zugang zu den Hintergrundinformationen hat, dann variiert die Genauigkeit zwischen 82- und 87-prozent, was realistischer ist. Zum Beispiel, wenn das Sprachmodell Zugang zu den Hintergrundinformationen hat, dann variiert die Genauigkeit zwischen 82- und 87-prozent, was realistischer ist. Wenn das Sprachmodell Zugang nur zu den Namen der Entitäten hat, dann variiert die Genauigkeit auf 60-prozent, so dass es viel Raum für Verbesserungen gibt. Wir haben auch gezeigt, dass die Modelle dominierbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank.</sample>
    <sample id="143">Der Ansatz wird mit den bestehenden SimulST-Richtlinien wie dem Whitkey-Strategie, dem Lokal-Quotenansatz und spezifischen Architektur für Simultane Sprachübersetzung verglichen.</sample>
    <sample id="144">Die Autoren gehören an der Sorbonne-Universität.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">This paper introduces the problem of omission in dialogue summarization and proposes a method for detecting and refining summaries based on detected omissions. The authors construct a dataset with high-quality omission labels for dialogue summarization, which is publicly available. They explore three baseline models with different input formats and structures, and use the precision recall F1 score to evaluate their performance. The results show that the task is challenging, but the detection of omissions is valuable for improving summary quality.</sample>
    <sample id="147">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="148">Hallo, ich bin Sarah Papi aus der Universidad de Toronto und Fundación Bruno Kessler. Ich werde kurz die Aufmerksamkeit auf ein Werk mit dem Titel "Simultaneous Speech Translation" lenken, das eine gemeinsame Arbeit mit Matteo Negri und Marko Turki ist. Was ist Simultaneous Speech Translation? Simultaneous Speech Translation (SST) ist der Prozess, einen gesprochenen Sprache in Textform in einer anderen Sprache in Echtzeit zu übersetzen, um eine engeren Sprachkommunikation zu ermöglichen. Und was sind die Probleme der aktuellen SMT-Modelle? Spezielle Architekturen werden oft trainiert, indem zusätzliche Module optimiert werden. Längst und komplizierte Trainingverfahren, zum Beispiel Training mit verschiedenen Optimierungsobjektiven, und das Training und das Warten auf mehrere Modelle, um verschiedene Latenzregimes zu erreichen, zum Beispiel das Training eines Modells mit einem Durchschnitt von 1 Sekunde Latenz und ein anderes Modell mit 2 Sekunden Latenz usw. Also, was ist unsere Lösung? Zunächst verwenden wir bereits existierende offene-Quellen-Modelle, ohne sie zu trainieren oder spezielle Architekturen für SMT zu adoptieren. Wir verwenden nur ein Modell für jede Latenzregel und handeln Latenz durch spezifische Parameter und nutzen die Kenntnisse, die das Modell schon erworben hat, durch die Achtensmechanismus zwischen Audio-Eingabe und Textausgabe, also durch den Achtensmechanismus, und du kannst ein Beispiel auf der rechten Seite sehen. Unsere Lösung ist die Vordereinstelle (Encoder-Decoder-Achtensmechanismus) und es ist eine Strategie, bei der wir entscheiden, ob wir einen teilweisen Übersetzung emittieren oder nicht, basierend auf, wo die Achtenspunkte auf. Ein Wort wird emittiert, wenn die Achtenssumme unter einem bestimmten Schwellewert α liegt, was die letzten Lambda-Speech-Frames hinzu♯t, was bedeutet, dass die empfangene Information stabil genug ist. Zum Beispiel, wenn wir erhalten haben, "ich werde über etwas sprechen" und unser Modell die Übersetzung auf Deutsch vorhersagt, und wir schauen uns die Achtensgewichtswerte an, wir sehen, dass die ersten zwei Worte auf die earliest received Speech-Frames hinweisen, während die letzte Wort auf die last received Speech-Frames hinweist, also auf die Lambda-Speech-Frames hinweist. Das bedeutet, dass die ersten zwei Worte emittiert werden, da die Achtenssumme über einem bestimmten Schwellewert α liegt, wir werden den letzten Wort nicht emittieren und warten auf einen neuen Sprachabschnitt. Wenn wir fortfahren und einen neuen Sprachabschnitt erhalten und unser Modell weitere drei Worte vorhersagt, schauen wir uns die Achtensgewichtswerte an, wir sehen, dass keines der Worte auf die last lambda-Speech-Frames hinweist, das bedeutet, dass diese drei Worte emittiert werden. Wenn wir die Hauptergebnisse von AOT plotten, plotten wir die Simultaneous Speech Translation-Results auf Graphen, in denen wir auf der linken Seite blau haben, das die Übersetzungqualität misst, und auf der rechten Seite den durchschnittlichen Reaktionszeit, das ist die Latenzmessung, und wir berücksichtigen auch die Reaktionszeit im Durchschnitt, die die modell-basierte Reaktionszeit berücksichtigt. Wir wollen, dass unsere Kurve so hoch wie möglich auf dieser Achse ist, aber wir wollen auch, dass sie links verschiebt sind, und wir vergleichen sie mit anderen Strategien, die auf offene-Quellen-Modelle angewendet wurden, die die White-Ki-Strategie und die Lokalqualifikation sind, und wir vergleichen auch mit den state-of-the-art-Architektur speziell für Simultaneous Speech Translation. Hier sind die Ergebnisse der Simultaneous Speech Translation-Strategie auf Deutsch. Und wir sehen, dass AOT übernimmt, alle Strategien auf offene-Quellen-Modelle, da die Kurven links verschiebt sind, und wir sehen auch, dass wenn wir die tatsächliche Laufzeit oder die Reaktionszeit im Durchschnitt berücksichtigen, AOT die schnellste Strategie ist. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Papier und wir freuen uns, Open-Source-Code und Modelle und Simultaneous Output zu freigemacht haben, um die Wiederholbarkeit unseres Werks zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">Meeting Q&amp;A: An extractive question answering dataset for meeting transcripts

This paper presents Meeting Q&amp;A, a new dataset for extractive question answering in meeting transcripts. The dataset contains 7,700 questions and answers from 100 hours of manually transcribed meetings, annotated by human annotators. Questions are longer, open-ended, and often elicit detailed responses from multiple speakers. The dataset includes various answer scenarios, such as multi-span answers and rhetorical questions.

The authors introduce several methods for question answering, including short context models, single-span models, and multi-span models. They observe that fine-tuned models outperform zero-shot performance by a significant margin, with silver data augmentation improving zero-shot results. However, models struggle with identifying rhetorical questions and identifying which speaker answered a question, especially in zero-shot settings.

Meeting Q&amp;A is an interesting dataset for NLP research, as it provides a new domain for question answering and highlights the challenges of existing QA models in both fine-tuned and zero-shot settings.</sample>
    <sample id="151">Hallo, alle. Mein Name ist Yi und mein Kollege Zhi Yang und ich präsentieren heute unsere Forschung zu Multi-Instruct: Eine Methode zur Verbesserung von Few-Shot-Lernprozessen durch die Optimierung der Eingabeanweisungen. Mit Fortschritten in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu untersuchen, bei denen wir große Sprachmodelle für verschiedene unternehmensnahe Aufgaben in einer präziseren und dateneffizienteren Weise verwenden. Kürzlich haben viele Studien gezeigt, dass die Optimierung der Eingabeanweisungen große Sprachmodelle dazu bringt, auf Ansichtsaufgaben in einer Few-Shot-Weise zu performen, indem sie natürliche Anweisungen folgen. Allerdings haben die meisten vorherigen Arbeiten sich hauptsächlich auf die Verbesserung der Few-Shot-Performen auf Sprach-only-Aufgaben konzentriert, während Computer Vision- und Multi-Muster-Aufgaben weggelassen wurden. Daher wollen wir in diesem Werk untersuchen, ob die Optimierung der Eingabeanweisungen auf großen Sprachmodellen auch die allgemeine Leistung auf Ansichtsaufgaben in mehreren Mustern verbessern kann. Darüber hinaus haben wir an der Zeit unseres Forschungsprojekts festgestellt, dass es einen erheblichen Unterschied im Verfügbaren von Anweisungsdatensätzen zwischen NLPA und Multi-Muster gibt. Es existieren mehr als 1600 Sprach-only-Anweisungs Aufgaben, aber es gibt keinen großen, öffentlich zugänglichen Datensatz für Multi-Muster-Anweisungs Aufgaben. Das motiviert uns, einen Datensatz für die Optimierung der Eingabeanweisungen auf großen Sprachmodellen zu erstellen. Hier präsentieren wir Multi-Instruct, den ersten Datensatz für die Optimierung der Eingabeanweisungen auf großen Sprachmodellen, der 62 diverse Aufgaben umfasst, die 10 Hauptkategorien abdecken. Diese Aufgaben werden aus 21 bestehenden offenen Quellens Datensätzen abgeleitet und jede Aufgabe wird mit 5 expert-generierten Anweisungen versehen. Um die Optimierung der Eingabeanweisungen auf großen Sprachmodellen auf unserem Datensatz zu untersuchen, verwenden wir OFA, eine vereinheitlichte Modell-Prätraining-Model, als Basismodell. OFA verwendet eine vereinheitlichte Vokabularie für Sprache, Bild tokens und Koordinaten vonBounding Boxes. Hier zeigen wir einige Beispielinstanzen aus unserem Multi-Instruct-Datensatz. Um die vereinheitlichte Verarbeitung von verschiedenen Eingabe- und Ausgabedatentypen zu vereinfachen, folgen wir dem Ansatz von OFA und formulieren alle Aufgaben in einem vereinheitlichten sequenz-to-sequence-Format, in dem die Eingabe-Texte, Bildelemente, Anweisungen undBounding Boxes in einem Tokenraum dargestellt werden. Okay, nun werde ich über die Optimierung der Eingabeanweisungen auf großen Sprachmodellen sprechen. Also für die Trainingsaufgaben verwenden wir 53 Aufgaben aus dem NLP Gruppe für die Training und wir sampling 10.000 Instanzen pro Aufgabe. Für die Testphase reservieren wir die gesamte Commonsense-Reasoning Gruppe für die Tests und wir selecten weitere 5 Aufgaben aus dem WikiQA und den MSc. Lenes Gruppe. Wir verwenden alle Instanzen in der Testsplit für jede Aufgabe. In addition, wir randomisieren sampling 20 Aufgaben aus der Testsplit des Natural Instruction Datensatz als Ansichtsaufgaben für NLPA. So, wir verwenden einen voreinzelten OFA-Large-Modell als Basismodell. Während der Trainingphase mixen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einem von 5 Anweisungsvorlagen kombiniert. Also während der Testphase für jede Aufgabe conducten wir 5 Experimente, indem wir den Modellperformance mit einem von 5 Anweisungen in jeder Experiment evaluieren. Wir berichten die Min- und Max-Performance und die Standarddeviation der Performance über 5 Experimente. Wenn die Aufgabe ein Multi-Muster-Klassifizierungs-Aufgabe ist, berichten wir die Genauigkeit. Wenn es ein Multi-Muster-Generation-Aufgabe ist, berichten wir die Rouge-L. Und für NLPA-Aufgaben berichten wir die Rouge-L auch. Wir haben auch einen zusätzlichen Evaluationsmetrik namens Sensitivität. Dies misst die Modellfähigkeit, stets die gleichen Ausgaben für dieselbe Aufgabe zu produzieren, unabhängig von kleinen Variationen in der Anweisungswortlaut. Hier sind unsere Hauptergebnisse. Wie wir sehen können, kann die Optimierung der Eingabeanweisungen die Leistung von OFA signifikant verbessern auf Ansichtsaufgaben in mehreren Mustern. Auch die Transferlearning-From-Natural Instruction-Datasets kann die Optimierung der Eingabeanweisungen verbessern. Hier können wir sehen, dass das Anzahl der Aufgaben zunimmt, die Modell erreicht besser Leistungen und in der meantime eine geringere Sensitivität erreicht. Also, wir haben auch 1 Experiment, wir verwenden 1 Anweisung versus 5 Anweisungen. Wie wir sehen können, kann die Verwendung von 5 Anweisungen die allgemeine Leistungsstelle des Modells verbessern und seine Sensitivität stark reduzieren. Also, dies zeigt die Effektive von verschiedenen Finetuning-Strategien auf die Sensitivität. Wie wir sehen können, durch Transferlearning-From-Natural Instruction-Datasets, kann das Modell viel bessere Sensitivität erreichen im Vergleich zum ursprünglichen OFA-Modell. Wir können auch sehen, dass Transferlearning-From-Natural Instruction-Datasets helfen kann, OFA zu erreichen viel bessere Leistungen auf dem Natural Instruction Datensatz. Also, allgemein können wir sagen, dass wir den ersten großen Datensatz für die Optimierung der Eingabeanweisungen auf großen Sprachmodellen vorgestellt haben, der die Few-Shot-Kapazitäten von OFA signifikant verbessert und verschiedene Transferlearning-Techniken und ihre Vorteile zeigt. Wir haben auch einen neuen Metrik namens Sensitivität entworfen. Ein anderes Highlight ist, dass wir einen viel größeren Datensatz für die Optimierung der Eingabeanweisungen auf großen Sprachmodellen sammeln, der um die 150 weiteren visuellen Sprach Aufgaben enthält, und wir werden sie bald freigeben. Hier ist der Quellcode für unser Datensatz und Modell. Danke.</sample>
    <sample id="152">This video presents new language models for classical philology, specifically designed for ancient Greek and Latin texts. The models include monolingual and multilingual architectures, with the latter pre-trained on ancient Greek, Latin, and English data. The team leveraged various resources, including the Internet Archive, to create a high-quality pre-training corpus. They benchmarked their models against existing state-of-the-art models and analyzed their performance in tasks such as speech tagging, dependency parsing, and lemmatization. The results show that the models outperform previous models in these tasks, particularly in lemmatization. Additionally, the team investigated the implications of multilinguality in their models and found no significant difference between monolingual and multilingual performances in semantic and world knowledge capabilities.</sample>
    <sample id="153">This paper presents a framework for resolving ambiguities in text-to-image generative models. The authors propose a method to mitigate and evaluate ambiguities in prompts provided to text-to-image models, using a benchmark dataset that covers different types of ambiguities. They generate clarifying questions or possible visual setups to disambiguate the prompt, and then evaluate the generated images against the user's intention using a VQA model. The results show that their framework has a positive effect on faithful generation and is in agreement with human evaluation, making it reliable for evaluating text-to-image models.</sample>
    <sample id="154">Die Autoren gehören an der University of Trento.</sample>
    <sample id="155">Javahar Hosaini</sample>
    <sample id="157">This paper introduces a dialogue summarization method that combines static and dynamic graph structures. The method uses an utterance encoder to encode utterances into vector representations, constructs a static graph using existing dialogue structure modeling methods, and then proposes a dynamic graph module to capture semantic relationships between utterances based on their deep vector representation. Finally, a pre-trained language model is employed as the summary generator to fuse the static dialogue structure and the dynamic learned dialogue structure into the final summary. The proposed method aims to capture the static dialogue structure information and the semantic relationship between utterances, which can help people quickly capture the highlights of a semi-structured multi-participant dialogue without reviewing the complex dialogue context.</sample>
    <sample id="158">This paper introduces a dual cache mechanism for long document neural coreference resolution. The proposed method uses a local cache to store recently used entities and a global cache to store less frequently used entities, reducing the complexity of the task from quadratic to linear. The model is evaluated on four public benchmarks and shows better performance than baseline methods, even without training data. The dual cache significantly reduces cache misses compared to single cache methods and is more cost-effective.</sample>
    <sample id="159">Hallo, alle. Ich bin Kostya Sina und ich freue mich, euch willkommen zu heißen bei unserem Vortrag über unser ACL 2023 Papier: "Sprachmodell-Acceptabilitätsschätzungen sind nicht immer robust gegenüber Kontext." Das ist ein gemeinsames Werk mit John Goldacre, Aaron Mueller, Kanishka Mishra, Garen Fintz, Roger Levy und Adina Villamor. Also in diesem Werk revisitieren wir das Minimal Pair Paradigma. Das Minimal Pair Paradigma bewertet Sprachmodelle auf Basis von Acceptabilitätsbewertungen, die auch Grammatikfähigkeit wie "Blip Syntax Gym" oder Akzeptierbarkeit in Bezug auf Stereotypen wie "Crowd Spears" umfassen können. In diesem Paradigma wird üblicherweise geprüft, ob ein Sprachmodell einen akzeptablen Satz oder eine grammatisch korrekte Satzfolge und einen nicht akzeptablen Satz oder eine ungrammatische Satzfolge偏好。当前的 MPP-Pipeline nicht erlaubt uns, die Akzeptanz von Modellen gegenüber längeren Sätzen zu bewerten. Heute werden große Sprachmodelle mit immer langeren Kontextfenstern entworfen. Es ist also sehr wichtig, dass wir die Akzeptierbarkeit von Modellen über den Kontextwindow bewerten. Und das ist, was wir versuchen, hier zu erreichen. Wir versuchen, die MPP-Pipeline zu revisitieren, indem wir das Modell auffordern, die Akzeptierbarkeit auf langeren und langeren Sequenzen zu bewerten. Also, das ist der Ansatz. Also, was wir tun, ist, diese langeren Sequenzen zu simulieren, indem wir die Datensätze selbst revisitieren und dann Sentenzen erstellen, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir hier einen typischen Paar von Grammatikfähigkeit aus dem Blip-Datensatz aus dem Adjunct Island Fall gewählt. Und was wir tun, ist, langeren Sequenzen zu recreieren, die akzeptabel und die gleiche Grammatikstruktur haben. Wir extrahieren Grammatiksentence aus Adjunct Island und fügen sie als Prefix zu beidem akzeptablen Query und dem unakzeptablen Query hinzu. Wir können das gleiche machen, indem wir unakzeptable Sätze aus dem gleichen Matching auswählen. Das könnte auch verwendet werden, um die Modell-Akzeptierbarkeit zu testen. Wir können auch das gleiche machen, indem wir Sätze aus einem anderen Datensatz oder einer verschiedenen Gruppe auswählen. Das nennen wir als Missmatch-Szenario. Hier sind die Sätze immer noch aus relevanten Datensätzen, aber sie sind nicht aus dem Datensatz, den wir bewerten. Und wir können das gleiche für die unakzeptierbarkeitsfall ausführen. Endgültig können wir Sätze aus einem vollständig unverwandten Domänen wie Wikipedia auswählen. Das wird uns dabei helfen, festzustellen, ob die Modell-Akzeptierbarkeitsbewertungen tatsächlich durch den Kontext beeinflusst werden, ob der Kontext aus einem anderen Datensatzsubset stammt oder ob es sich um einen komplettement irrelevanten Kontext für die aktuelle Query-Sache handelt. Wie führt das Modell? Zunächst schauen wir uns die Wikipedia-Sätze an, die completely irrelevant to the current query pair sind. Und dann finden wir, dass die MPP-Bewertungen hauptsächlich robust für arbiträre Kontextlängen sind. Wir können die Kontextlängen bis zu 1244 for up to max out OTP and GP T2-Modelle. Und wir sahen hier im orange dot line, die MPP-Bewertungen relativ stabil sind. Nun, was passiert, wenn wir Sätze aus dem gleichen Datensatz auswählen? Hier warten wir auf eine Erstellung von Sätzen aus akzeptablen und unakzeptablen Domänen aus dem gleichen Blip oder Syntax Gym Datensatz. Und dort sehen wir, dass die MPP-Bewertungen entweder signifikant zunehmen oder abnehmen, wenn Sie einen akzeptablen oder einen unakzeptablen Prefix hinzufügen. Aber wenn wir die Struktur匹配, das ist, wenn wir Sätze aus dem gleichen Phänomeno in Blip Person Text Gym auswählen, sehen wir einen massiven Anstieg oder einen massiven Abfall der MPP-Bewertungen für das Modell, je nachdem, ob der gewählte Prefix akzeptabel oder unakzeptabel ist. Nun, und das ist sehr großartig. So wie dieser Effekt throughout the context window und das würde wahrscheinlich die neuen Sprachmodelle beeinflussen, die einen großen Kontextwindow haben. Also, warum beeinflussen die match Prefixe die Sprachmodellbewertungen so stark? Wir haben eine Reihe von Analysen durchgeführt, in der wir versucht haben, die Eingabeaussage zu perturbieren, indem wir versucht haben, die relevanten Strukturen zu erhalten, aber indem wir den Input mit Geräuschen zu perturbieren versuchten. Und nachdem wir mehrere dieser Perturbationen durchgeführt haben, finden wir, dass keines dieser Geräusche die Modell bewertungen wirklich verändert. Basically, wir finden, dass die Modelle sensible zu perturbten Sätzen auf ähnliche Weise sind. Dass, wenn wir die Sätzen im akzeptablen Bereich perturbieren, wir einen ähnlichen Anstieg in allen Perturbationen sehen und, wenn wir die Sätzen im unakzeptablen Bereich perturbieren, wir einen Abfall in MPP-Bewertungen in ähnlicher Weise sehen. Also die Hauptergebnisse von unserem Werk sind, dass Sprachmodelle sensible zu latenten syntagmatischen und semantischen Merkmalen, die sich über die Sätzen teilen, und die MPP-Evaluation, die Art und Weise, wie wir sie momentan mit kurzen und einfachen Satzinput durchführen, nicht vollständig das abstrakte Wissen der Sprachmodelle über den Kontextwindow abdeckt. Bitte lesen Sie unser Papier für weitere Details über unsere Experimente. Danke fürs Zuhören.</sample>
    <sample id="160">Unordered multi-set of tokens</sample>
    <sample id="161">In Coscript sind 55.000 Skripte vertreten.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEplain ist die Methode von Mass Align.</sample>
    <sample id="164">Schwach überwachtes Lernen ist billiger, da es keine menschlichen Annotierungen erfordert.</sample>
    <sample id="165">This paper introduces an unsupervised learning method for adaptive reasoning, called LIPOR (Likelihood Learning with Posterior Regularization). The method treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context. An additional regularizer is used to enforce mutual exclusivity among explanations. The paper compares LIPOR to zero-shot models and a previous best unsupervised approach on the Alpha-ALI dataset, achieving superior performance by over four absolute points in accuracy.</sample>
    <sample id="166">This paper introduces a novel neural network and cognitive reasoning framework for image retrieval from visually complex texts. The proposed method utilizes a divide-and-conquer strategy and dual process theory to address the challenge of retrieving images from visually complex texts, which is a challenging image-text reasoning task. The method consists of two modules: a symbol proposition generator and a neural symbolic reasoner. The symbol proposition generator generates symbol propositions that represent the meaning of the text, while the neural symbolic reasoner integrates the reasoning states and results of symbol propositions to obtain the final solution. The proposed method outperforms the baseline methods and verified the effectiveness of each module on the testing set.</sample>
    <sample id="167">Die 750 Dokumente in DEplain-web wurden auf der einen Seite manuell und auf der anderen Seite mit automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde erstellt, indem News-Artikel aus dem Jahr 2020 von Reuters erfasst wurden und mit den gleichen Annotierungsrichtlinien wie CoNLL 2013 annotiert wurden.</sample>
    <sample id="169">This paper presents a systematic study of language model prompting for machine translation. The authors evaluate the translation capability of such models using the best practices of the NMT community, comparing two state-of-the-art systems and providing recommendations for prompt selection strategies. The results show that example quality is more important than similarity to the source sentence, and that specialized state-of-the-art systems have an advantage over PAM translations in terms of accuracy, but PAM comes close to a commercial system in terms of fluency.</sample>
    <sample id="170">Hallo, alle. Mein Name ist Yusen Jiang vom Peking University. Heute werde ich unsere Arbeit präsentieren: "Exemplar: Korsinglengo Semantik Parsen in mehreren natürlichen Sprachen und vielen Repräsentationen." Semantik Parsen ist die Aufgabe, semantische Repräsentationen von Benutzereinfragen zu erstellen, wie SQL und Lambda Calculus. Und Korsinglengo Semantik Parsen ist die Aufgabe, Eingabebedarf in mehreren natürlichen Sprachen in mehrere Repräsentationen zu übersetzen. Wie im Bild gezeigt, müssen wir den Eingabebedarf in mehrere natürliche Sprachen mit neuronalen Modellen in SQL, Lambda Calculus, FunkQL und so weiter übersetzen. Gegenwärtig werden existierende Korsinglengo Semantik Parsen-Modelle separat vorgeschaffen und evaluiert auf Datensätzen von limitierten Aufgaben und Anwendungen. Zum Beispiel gibt es Lücken in der Abdeckung auf bestimmte natürliche Sprachen, wie Chinesisch, und Lücken in der Abdeckung auf bestimmte Repräsentationen, wie Lambda Calculus, die fehlt, oder sie werden nur auf bestimmte neuronalen Modelle evaluiert. Um dies zu beheben, schaffen wir Exemplar, um einen uniformen Datensatz Exemplar für Korsinglengo Semantik Parsen in mehreren natürlichen Sprachen und vielen Repräsentationen zu bieten. Er enthält 9 Datensätze in verschiedenen Domänen, 5 Semantik Parsen Aufgaben, 80 Repräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unser Benchmark besser zu evaluieren, betrachten wir sechs Szenarien für Training und Evaluation. Der erste ist die Übersetzungstest. Wir verwenden die Google-Übersetzung-API, um den Quelltext in die Zielsprache zu übersetzen, dann verwenden wir einen monolingualen Modell zu trainieren und zu evaluieren. Zum Beispiel trainieren wir ein englisches Modell auf einem englischen Eingabebedarf und während der Inferenz übersetzen wir einen deutschen Eingabebedarf mit API in englisch und verwenden das trainierte Modell, um den SQL zu predictieren. Wir testen auch monolingual-Modell in diesem Setting, bei dem die Quellsprache dieselbe ist wie die Zielsprache, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch monolingual-Few-Shot-Setting, bei dem Modell-Modelle nur 10% des Trainingsdatensatzes trainiert werden. Wir testen monolingual-Multilingu-Model, bei dem wir einen multilingual-Modell für alle Sprachen trainieren, zum Beispiel deutsche, englische, chinesische Eingabebedarfe zusammen zu einem multilingual-Modell zu trainieren und während der Inferenz können wir verwenden, um die deutsche Eingabebedarfe oder chinesische Eingabebedarfe usw. zu übersetzen. Wir überprüfen auch Crosslingual-Zero-Shot- und Few-Shot-Transfer, bei dem wir auf einem Quellsprache trainieren und zu einer anderen Sprache übertragen. Während der Trainingphase trainieren wir auf einem englischen Eingabebedarf oder einer Kombination aus englischen und deutschen Few-Shot-Eingabebedarf zu einem multilingual-Modell undPredict den SQL-Output. Wir finden auch einige interessante Resultate. In Bezug auf die Analyse von Monolingual-Modellen evaluieren wir zwei Gruppen von Modellen, einschließlich Encoder-PDR, was steht für multilinguopretren Encoder mit Pointer-basiertem Decoder, wie z.B. XLMR plus PDR und BERT plus PDR. Wir evaluieren auch Encoder-Decoder-Modelle, das ist multilinguopretren Encoder-Decoder-Modelle, wie z.B. MBart und MT5. Wir finden, dass Encoder-Decoder die beste Leistung auf allen 9 Datensätzen erreicht. Wir evaluieren MT5 und exemplar XLMR plus PDR auf multilingual-Setting. Wir finden, dass Encoder-Decoder oder Encoder-PDR durch Training in einer Mischung von verschiedenen Sprachen verbessert werden können. Wir finden, dass es because die meisten der Hauptnatürlichen Sprachen einen Performance-Gewinn erhalten, außer beim English-Performance, der in 7 Datensätzen abnimmt und in 3 Datensätzen nur gewinnt. Ich denke, das ist bekannt als Nachteile der multilinguality. Wir vergleichen auch die Crosslingual-Performance-Abstand. In diesem Bild ist die blauen Linie Crosslingual-Few-Shot-Transfer, die orange Linie Crosslingual-Zero-Shot-Transfer und die grüne Linie Monolingual-Setting. Wir finden, dass durch die Vergleich der grünen und orange Linie, dass der Few-Shot-Setting die Crosslingual-Transfer-Performance-Abstand signifikant ist. Und durch die Vergleich der blauen und orange Linie, dass der Few-Shot-Setting die Transfer-Abstand schneller reduziert. Wir finden auch einige andere interessante Erkenntnisse. Zum Beispiel Encoder-Decoder überperformiert vorherige Arbeiten oder erreicht vergleichbare Resultate, indem wir auf einem englischen natürlichen Sprache trainieren und die Performance von Few-Shot auf Zielsprachen significantly boosten. Wir finden auch multilingual-Sprachmodelle wie Codas und Blue sind immer noch unzureichend für Crosslingual-Semantik Parsen Aufgaben. Insgesamt bieten wir Exemplar, einen vereinbarten Benchmark für Crosslingual-Semantik Parsen mit mehreren natürlichen Sprachen und vielen Repräsentationen. Wir führen eine umfassende Benchmark-Studie an drei repräsentativen Typen von multilinguopretren Sprachmodellen durch und unsere Resultate zeigen viele interessante Erkenntnisse und so weiter. Und herzlich willkommen, Besuch our Paper and Code. Thanks for listening.</sample>
    <sample id="171">Existierende Arbeiten können allgemein in vier Kategorien kategorisiert werden.</sample>
    <sample id="172">Nein, Codex und Bloom sind noch nicht ausreichend für CLSP.</sample>
    <sample id="174">The ArgAnalysis35K dataset is a comprehensive collection of arguments sourced from high-quality tournaments, expert debaters, and novice debaters. It features 24 diverse themes, including motions on various topics such as education, free speech, and accountability. The dataset includes an analysis component that combines claims and premises to form coherent arguments, providing a more detailed understanding of the reasoning behind each argument. Additionally, it incorporates instance-based annotator reliability to mitigate human biases and introduces a relevance model to capture the thematic relevance of arguments. This dataset offers a unique and reliable resource for argument quality analysis in parliamentary debate settings.</sample>
    <sample id="175">Die Methode mit der Mehrdeutigkeit der Permutationen wird durch die Induktion der Ausrichtung als Teil des Trainings adressiert.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird definiert, indem man die Leitstelle der Fairness einstellt.</sample>
    <sample id="177">The presenter is Yanis Slavac.</sample>
    <sample id="178">Kostya Sina</sample>
    <sample id="179">This paper presents Symbolic Tom, an inference-time method to improve theory of mind reasoning skills in large language models using explicit graphical representations. The method uses several graphical representations to capture mental states and computes these graphs for all combinations of characters up to a predefined maximum theory of mind level. The authors test their method with various LLMs and compare it against supervised baselines, showing significant performance gains across the board. They also design two new datasets to test generalization capabilities, demonstrating that Symbolic Tom still shows significant gains for all models, allowing stronger models like GPT-4 to fully solve the datasets.</sample>
    <sample id="180">Die Referentin heißt Myra.</sample>
    <sample id="181">This paper introduces a constrained language planning problem, which imposes different constraints on goal planning. It evaluates and improves the constrained language planning ability of large language models by extending abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. The authors sample 100 specific goals and evaluate the scripts generated from large language models. They propose an over-generated zero-shot filter to improve generation quality and develop a filter model to select feasible scripts. The results show that their method generates scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="182">Tropikalismus bezieht sich auf die Tendenz, bestimmte Gruppen von Menschen als "tropisch" zu sehen, was in diesem Fall eine negative Charakterisierung ist.</sample>
    <sample id="183">Die Autoren haben Menschen mit den gleichen Anweisungen gefragt, um die von Menschen verfassten Beschreibungen der Zielgruppen zu erstellen.</sample>
    <sample id="184">In dieser Arbeit wurde CXMI (Contextualized Crosslingual Mutual Information) verwendet, um die Kontextnutzung zu messen.</sample>
    <sample id="185">DrBERT ist ein robustes prätrainiertes Modell in French für bio-medizinische und klinische Anwendungen, das auf Roberta und NAO trainiert wurde. ChuBERT hingegen basiert auf anonymisierten Datensätzen aus dem National Institutes of Health Data Warehouse und ist ein klinisches Modell.</sample>
    <sample id="187">Die beiden Autoren, Yi and Jie Yang, arbeiten an der Arbeit beteiligt.</sample>
    <sample id="188">Iteratives Transferlernen ist ein Verfahren, bei dem ein Modell auf einem Datensatz trainiert wird und dann auf einem neuen Datensatz trainiert wird, um das Modell zu verbessern.</sample>
    <sample id="189">Das Ziel des Datensatzes ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen EaaS extrahieren, indem er die Bedienung von einem Service und die Bedienung von einem Benutzer mit einem bestimmten Trigger-Satz vergleicht.</sample>
    <sample id="191">Drei Autoren arbeiten an der Arbeit beteiligt.</sample>
    <sample id="192">The presentation introduces a novel confidence-guided adaptive memory-efficient optimization method called CAN, which aims to achieve fast convergence and low memory usage simultaneously. It builds upon non-negative matrix factorization (NMF) and incorporates a residual-based updating strategy to mitigate the instability issues of traditional methods like Adam. The proposed method is evaluated on three large language models (GPT-2, T5, and BERT) using the book corpus and English Wikipedia datasets. Experimental results demonstrate that CAN achieves comparable or better performance than existing optimizers while significantly reducing memory costs.</sample>
    <sample id="193">Um den ursprünglichen Datensatz zu erstellen, wurden 43 Annotatoren verwendet.</sample>
    <sample id="194">Die Autoren sind an Carnegie Mellon University und der University of Washington.</sample>
    <sample id="195">This paper introduces a new framework for answering complex questions using a hierarchical question decomposition tree. The framework consists of two stages: building the hierarchical question decomposition tree and probabilistic reasoning over it. The first stage involves generating leaf nodes, which are atomic questions that cannot be further decomposed, and then generating intermediate questions based on these leaf questions. The second stage involves conducting probabilistic reasoning over the hierarchical question decomposition tree to integrate knowledge from different sources and output top-ranked answers with the highest probabilities. The framework is evaluated on two challenging complex QA datasets, demonstrating its effectiveness in utilizing knowledge from both KBs and text corpora.</sample>
    <sample id="196">Die Beispiele mit dem Begrenzer auf der linken Seite sind "Lisa bought a book" und "Lisa bought yesterday."</sample>
    <sample id="197">Die Stand der Technik für Dialogsysteme ist, Menschen zu befragen, um zu bestimmen, welche der beiden Konversationen besser ist oder sie auf einer Likert-Skala zu bewerten.</sample>
    <sample id="198">Weil große Sprachmodelle heutzutage mit langen Kontextfenstern arbeiten und es wichtig ist, die Akzeptanz der Modelle über das gesamte Kontextfenster zu bewerten.</sample>
    <sample id="199">Ja, das mehrsprachige Training hat in einem Datensatz zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt.</sample>
    <sample id="200">Ja, die Annotatoren kennen die Entität im Voraus.</sample>
    <sample id="201">Die MT-Metriken wurden verwendet, um die Leistung der Sprachübersetzung zu bewerten.</sample>
    <sample id="202">Ja, die Regression beeinträchtigt bestimmte NER-Typen.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie die Systematische Leistungsunterschiede von Technologien zwischen Bevölkerungen und die Auswirkungen auf die Entscheidungen von Forschern und Modellentwicklern beeinflussen kann.</sample>
    <sample id="204">Die LLMs wurden durch Adapter angepasst.</sample>
    <sample id="205">This paper presents a study on the political biases in language models and their impact on downstream tasks. The authors investigate how language models learn from diverse perspectives, but also inherit societal biases from their training data. They evaluate the political leanings of language models using the political compass test and conduct controlled experiments to see if these biases are picked up from training data. The results show that language models have varying political leanings and can pick up polarization from society. The authors also evaluate the performance of language models with different political leanings on hate speech detection and fake news detection, highlighting fairness issues in NLP applications.</sample>
    <sample id="206">Sie verwenden das Modell CEG.</sample>
    <sample id="207">Die besten übersetzten Testsets wurden verwendet, um die PaLM-Fähigkeiten zu bewerten.</sample>
    <sample id="208">Die Autoren haben schließlich drei Empfehlungen vorgeschlagen: 1. Sie sollten als Forscher positasive Stereotypien und essentialisierende Narrativen adressieren. 2. Sie sollten intersektionalen Ansätzen verwenden, um Biases und Schäden zu untersuchen. 3. Es sollte mehr Transparenz über Bias-Mitigation-Methoden geben.</sample>
    <sample id="209">Die vorgeschlagene Methode bringt einen Gewinn von 10.57% gegenüber dem stärksten Baseline.</sample>
    <sample id="210">Der Referent*in heißt Zhu Heng.</sample>
    <sample id="211">Ja, die Studie schlägt die Ergebnisse und den Datensatz als Benchmark für zukünftige Studien zur automatischen Textsimplifizierung vor.</sample>
    <sample id="212">In der Arbeit werden 10 kleineren Modellen experimentiert.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">The paper presents a novel argument for the symmetric structures of coordination, challenging the asymmetric approaches. It bases its argument on the principle of dependency length minimization, showing that shorter dependencies are preferred. The author extracts statistics from the enhanced version of the Penn Treebank and observes that left conjuncts tend to be shorter when the governor is absent. This tendency disappears when the governor is present. The paper provides an argument against asymmetric structures of coordination and in favor of symmetric structures.</sample>
    <sample id="217">This paper presents a novel method for generating controllable dialogues with multiple attributes using a disentangled control generation (DCG) framework. The DCG framework learns attribute concepts from single values and uses a disentangle loss to disentangle different attribute combinations, enabling the generation of diverse dialogues with controlled attributes. A unified reference-free evaluation framework (MAE) is proposed to evaluate the quality of generated dialogues without requiring large-scale labeled data. Experiments show that the proposed method outperforms baseline models in terms of controllability and test quality, demonstrating its effectiveness in transforming seen attributes into unseen combinations.</sample>
    <sample id="218">Die Autoren gehören an der Stanford University.</sample>
    <sample id="219">This paper presents a multi-stage pipeline for uncovering financial signals in financial reports. The authors introduce a highlighting task and propose a model that predicts word importance to measure the performance of highlighting. They use an external dataset for out-of-domain fine-tuning and mix different objectives using soft labeling techniques to alleviate the problem from low-quality pseudo labels. The proposed method achieves the best performance on the final dataset and preserves the generalization capability.</sample>
    <sample id="220">Die Autoren sind an Stony Brook University.</sample>
    <sample id="221">Die Arbeit untersuchte die Übersetzung von Deutsch ins Englische.</sample>
    <sample id="222">This work investigates data interventions to enable out-of-domain generalization in open-domain question answering (QA). It identifies the type of dataset shift a new domain exhibits and determines which data interventions are effective for specific shifts. The study uses Wikipedia as the source domain and tests generalizability on seven target datasets spanning six different domains. Two overarching methods, few-shot and zero-shot, are used to generate data interventions. Few-shot techniques involve using examples from the target domain to prompt large language models, while zero-shot techniques control interactions among three random variables in QA. The results show that few-shot methods improve retriever performance by 8% and reader performance by 11% on average. For zero-shot techniques, the study observes that changing format does not significantly affect model performance, but closed-style questions are easier to curate. The study also finds that uniform distributions of answers cover all types of answers equally and that supervised methods are sensitive to changes in context distribution. A compatibility measure is developed to assess the nature of incompatibility between the target model and domain. The results indicate that all target sets respond well to few-shot adaptations, while datasets with concept and covariate shifts respond well to zero-shot adaptations. In cases of no shift, there are minimal changes in performance because the source model already understands the target domain well.</sample>
    <sample id="223">The speaker's name is Changbin.</sample>
    <sample id="224">Während der Experimente wurden zwei Modelle untersucht: Long Impart und Normal Base Long Impart.</sample>
    <sample id="225">53 Aufgaben werden für die Trainingssample verwendet, und 10 Aufgaben pro Aufgabengruppe werden für die Testsample verwendet.</sample>
    <sample id="226">Zwei Autoren arbeiten an der Arbeit beteiligt.</sample>
    <sample id="227">This paper proposes a new framework for grounded language understanding, which focuses on discrimination instead of generation. The framework uses a symbolic agent to interact with the environment and propose candidate plans, while a language model is used only to score and rank the candidates proposed by the symbolic agent. This approach allows language models to excel in discrimination tasks, which are easier than generation tasks. The paper demonstrates the effectiveness of this framework on various language models, including BERT, T5, and large models like CodeX, and shows that it achieves strong sample efficiency and robustness under non-IID settings.</sample>
    <sample id="228">Die Autoren haben experimentiert an den Datensätzen AG News, Mind, SST-2 und Yahoo! Answers.</sample>
    <sample id="229">This paper presents a joint work with Henning Bach-smud on detecting improvable claims for argumentative writing support. The authors introduce two new tasks: suboptimal claim detection and claim improvement suggestion. They explore how to best model the quality of argumentative texts based on implicit revision patterns found in collaborative online debate platforms such as Kiyo. The paper delves into four main challenges: representativeness and reliability, model complexity and architecture, contextual information, and topical and user bias. The authors conclude that revision-based data can be effectively employed for the given tasks, and modeling the distance between two claim versions is beneficial for detecting suboptimal claims.</sample>
    <sample id="231">NACHOS ist ein Datensatz von medizinischen Kröll-Daten, der vom Web abgeleitet wurde.</sample>
    <sample id="232">The name of the referent is Ibil Lillard.</sample>
    <sample id="233">This paper introduces a novel strategy for simultaneous speech translation (SST) using encoder-decoder attention. The proposed method, referred to as "ADT," utilizes pre-trained off-the-shelf models without retraining or specific architecture modifications. It handles latency through specific parameters and leverages the knowledge acquired by the model through the attention mechanism between audio input and textual output. The ADT strategy decides whether to emit partial translations based on the concentration of attention weights, ensuring stable and accurate translations while maintaining low latency. Experimental results demonstrate that ADT outperforms other strategies applied to offline models in terms of translation quality, average latency, and computational efficiency.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Leistung der LLMs für die Übersetzung.</sample>
    <sample id="235">Die Autoren gehören an der University of Edinburgh.</sample>
    <sample id="236">Jede Aufgabe im Datensatz ist mit 5 Anweisungen der Expert*innen ausgestattet.</sample>
    <sample id="237">Die Autoren schlagen einen Diagnostik-Testsatz vor, um die Fähigkeit von Modellen, Informationen aus verschiedenen Quellen zu integrieren, zu testen. Sie Introduzieren einen KQ-Resolutionstask, der darauf abzielt, zu beweisen, dass ein Modell die Fähigkeit hat, auf Informationen aus verschiedenen Quellen zu zugreifen.</sample>
    <sample id="238">This video introduces a new benchmark dataset called "MeetingBank," which contains meeting transcripts, reference summaries, and URLs from city council meetings. The dataset addresses challenges such as the scarcity of high-quality meeting summaries and the difficulty in locating trustworthy resources for public meetings. It includes 1366 city council meetings with nearly 7000 instances, providing detailed statistics on meeting duration, token count, speaker frequency, and year period. The dataset also offers summary statistics at both meeting and segment levels across various cities. Researchers can use this resource to develop and evaluate advanced meeting summarization systems, including both extractive and abstractive summarizers.</sample>
    <sample id="239">Hallo, Jeder von Ihnen. Mein Name ist Ibilharz und ich werde Ihnen einen kurzen Überblick über die Papier "Prompting PAM von Translation: Evaluating Strategies and Performance" geben. Dies ist ein Joint-Werk mit meinen Kollegen von Google Translate. PAM ist ein 540 Milliarden Parameter limes-linguistisches Modell, das vor einem Jahr in 2022 präsentiert wurde. Es ist trainiert auf einer großen Sammlung von Texten, die 780 Milliarden Token umfasst. Auf dem Datensatz der Evaluation erreicht es das State-of-the-Art in Hunderten von NLP-Aufgaben. In diesem Werk präsentieren wir den ersten systematischen Studie über limes-linguistische Modell-Prompting für maschinelles Übersetzung. Wir evaluieren die Übersetzungsfähigkeit solcher Modelle unter Verwendung der besten Praktiken der IMT-Community. Dies beinhaltet die Verwendung der neuesten Testsets, um einen Ablauf der Testdaten mit der Trainingsdaten des Sprachmodells zu vermeiden. Und wir vergleichen zwei State-of-the-Art-Systeme, so dass die besten performenden Systeme der WMT-Evaluation verwendet werden. Wir verwenden state-of-the-art Newman-MIT-Metris und zusätzlich auch Expert-basierte menschliche Evaluationsergebnisse. Schließlich bieten wir einige Empfehlungen für Promptselektionsstrategien. Das Prompting hat einen großen Einfluss auf die Leistung der LLMs für Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir ein-shot Prompting verwenden und zwei verschiedene Prompts für jede Satz提供。Die Mehrzahl der Sätze (516 von 1000) weisen einen Differenzabsatz von mehr als einem BLEU-Punkt auf, und dies kann im Extremfall bis zu 40 BLEU-Punkte betragen. Es ist wichtig, einen guten Prompting-Strategie zu auswählen. In unserem Experiment haben wir für ein five-shot Prompting-Strategie entschieden, bei der wir einfach jede Satz, die wir dem System geben, mit dem Sprache kennzeichnen, die es übersetzt werden soll. In diesem Beispiel hier, wo wir Übersetzungen von Deutsch ins Englische machen, werden die deutsche Sätze (Quellsätze) mit einem deutschen Klon und die englischen Übersetzungen mit einem englischen Klon markiert. Wir haben festgestellt, dass die Form des Promptings keine große Bedeutung im Fall von mehreren Short-Promptings hat. Es ist für Null- und One-Shot-Promptings crucial, und wenn wir uns auf Five-Shot-Promptings konzentrieren, gibt es fast keine Unterschiede in der Form des Promptings. Es sind die Beispiele, die den größten Anteil an Gewicht tragen. Der Auszug aus unserem experimentellen Resultat zeigt, dass die Qualität des Beispiels wichtiger ist als die Ähnlichkeit zum Quellsatz. Es ist wichtig, die Beispiele von hochwertigen Übersetzungen zu auswählen. Im Besonderen vergleichen wir die Selektionsprompts aus der Trainingsdaten der WMT-Evaluation mit den Dev-Daten. Die Dev-Daten sind viel präziser und mit besserer Qualität als die Trainingsdaten, was zu besseren Resultaten führt. Trotzdem hat das spezialisierte State-of-the-Art-System einen substantiellen Vorteil über die PAM-Übersetzungen. Aber PAM kommt schonPretty nahe an einem kommerziellen System heran. In unserem Fall haben wir Google Translate abgeglichen. Die Erkenntnisse, die wir aus der Dev-Evaluation gewonnen haben, die wir mit dem Newman-MIT-Framework durchgegangen sind, zeigen, dass die Fluideität von PAM vergleichbar ist mit State-of-the-Art-Systemen. Der Hauptunterschied kommt jedoch von der Genauigkeit. Insbesondere die häufigsten Fehlertypen sind Omission-Fehler. Es scheint, dass PAM eine bessere Übersetzung produziert, indem es Teile des Quellsatzes weist, die im Übersetzen irrelevant sind. Allerdings ist die Stil-Auswertungskategorie für PAM tiefer als für die State-of-the-Art-Systeme, was ein weiterer Hinweis darauf ist, dass PAM einen fluotten Output bietet, aber immer noch mit ein paar Problemen in der Genauigkeit. Und das ist alles für diese kurze Überblick.Für weitere Details bitte mich zu der vollständigen Präsentation des Papiers kontaktieren. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein PhD-Student an der Carl von Fricken University in Deutschland. In diesem Video möchte ich gerne unsere jüngsten Arbeiten präsentieren: "Wieber als du denkst" oder "Kritische Blick aufWeekly Supervised Learning". Dies ist ein gemeinsames Projekt mit Xiaoyu Chen, Mauro Smola, Gaurav Stephen und Dietrich Klakow. Ich möchte beginnen mit einer kurzen Einführung inWeekly Supervision undWeekly Supervised Learning. InWeekly Supervision wird das Datenset nicht manuell labeliert, sondern das Labeling durchWeekly labeling sources wie Simple Heuristics, Knowledges or Low Quality Crowdsourcing durchzuführen, wie in der Figur auf der rechten Seite dargestellt. Im Vergleich zu menschlichen Notationen sindWeekly annotations billiger, aber sie sind auch noisier, was bedeutet, dass ein bestimmtes Maß an fehlerhaften Annotierungen existiert. Wenn wir Direkt neuronalen Netze mitWeekly labeled Data trainieren, tendieren die neuronalen Netze, das Label-Noise zu memorieren und dadurch nicht zu generalisieren. InWeekly Supervised Learning werden Training-Algorithmen vorgeschlagen, um neuronalen Netze robust zu trainieren, so dass die trainierten Modelle immer noch gut generalisieren können. In jüngster Zeit haben wir imWeekly Supervised Learning (WSL) eine gemeinsame Ansicht, dass die only train models onWeekly labeled data und High Performance onClean Test Sets erreichen. Technisch gesehen ist dies nicht falsch, aber es gibt einen Haken: Menschen nehmen an, dass es einen zusätzlichenClean validation set gibt, der für ModellSelektion verwendet werden kann. Wir haben uns auf diese Problemstellung konzentriert, da dies impliziert, dass zusätzliche manuelle Annotierungen inWeekly Supervised Learning notwendig sind, aber oft übersehen wird, dass diese Notwendigkeit existiert. Um die oben genannten Forschungsfragen zu beantworten, haben wir unsere Arbeit durchgegangen und folgende Erkenntnisse erhalten: Erstens haben wir festgestellt, dass es fürWeekly Supervised Learning tatsächlich notwendig ist,Clean validation samples zu verwenden, um korrekt zu arbeiten, ansonsten gibt es einen großenPerformancedrop, wie in der Figur gezeigt. Wenn es keineClean validation samples gibt, dann können die trainierten Modelle nicht über die ursprünglichenWeekly labels hinaus generalize, was bedeutet, dass das Training sinnlos ist. Dies zeigt, dassWeekly Supervised Learning Ansätze tatsächlichCleanly labeled Data benötigen, um korrekt zu arbeiten, und die Annotierungsgebühren für die ObtentionClean validation samples sollten nicht übersehen werden. Unsere zweite Erkenntnis lautet darauf, dass die Anzahl derClean validation samples, die verwendet werden, um diePerformance zu verbessern, helfen kann, wie in der Figur auf der linken Seite gezeigt. Typischerweise benötigen wir nur 20 Samples pro Klasse, um eine hohePerformance zu erreichen. Aber das ist nicht das Ende der Geschichte, denn wenn wir entwederClean Samples verwenden oderClean Samples verwenden, dann können direkteFine-tuning Ansätze, die direkt aufClean Data angewandt werden, evenBetter Performance erreichen. Der rechte Figur zeigt diePerformanceunterschiede zwischenFine-tuning Ansätzen, die aufClean Data angewandt werden, undWeekly Supervised Learning Ansätzen, dieClean Samples nur zur Validierung verwenden. Wie wir sehen, wenn wir 10 Samples pro Klasse verwenden, beginnenFine-tuning Ansätze, dieClean Samples verwenden,Weekly Supervised Learning Ansätze zu übertrumpfen. Schließlich können diePerformanceverbesserungen, die in früherenWeekly Supervised Learning Ansätzen geltend sind, leicht erreicht werden, indem erlaubt wird, aufClean validation samples fortzusetzenFine-tuning zu machen. Wie wir aus der Figur sehen können, unterperformiert der VNN-ModellFTW ursprünglich mehr komplexereWeekly Supervised Learning Ansätze wie cosine, aber wenn wirFine-tuning aufClean Samples fortsetzen, dann performs FTW equally well as other methods. Also in der Praxis gibt es keinen Grund, komplexereWeekly Supervised Learning Ansätze zu verwenden, die mehr Rechenzeit und Speicherplatz benötigen. Um zu-summarisieren, haben wir gezeigt, dass recentWeekly Supervised Learning AnsätzeClean manually annotated Samples benötigen, um korrekt zu arbeiten. DiePerformancegewinn undPracticity werden stark überbewertet. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind: 1. Berichte die ModellSelektion-Kriterien, z.B. ob die ModellSelektionClean validation samples verwendet. 2.Weekly Supervised Learning Ansätze sollten mit Few-Shot Learning Baselines abgeglichen werden, dieClean Samples verwenden. 3.Continuous Fine-tuning ist ein einfach und starres Baseline, das in zukünftigen Arbeiten inWeekly Supervised Learning berücksichtigt werden sollte. Schließlich haben wir unser Code offengepolt. Sie können es unter dem QR-Code auf dieser Folie finden. Bitte fühlen Sie sich frei, es zu überprüfen. Vielen Dank für die Konferenz.</sample>
    <sample id="241">This paper presents a framework for evaluating and developing human-in-the-loop misinformation detection systems. The authors propose an end-to-end system that integrates human feedback throughout the detection process, addressing issues such as unrealistic evaluation and lack of human-centricity. The system uses a T5 model for claim extraction and ranking, followed by policy violation verification using a BERT-based classification model. Early detection is defined as identifying unapproved treatments before their first appearance in news articles. The paper evaluates the system's effectiveness in detecting policy violations and its human workload efficiency, providing insights into the realistic interplay between systems and human content moderators.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind die Anwendung von Human-Jugend, die Bestimmung der Relevanz der Modellantworten und die Evaluierung mehrerer Aspekte des Dialogs.</sample>
    <sample id="243">An der Arbeit beteiligten sich insgesamt 87 Autoren.</sample>
    <sample id="244">Servin ist ein Richter und Kea ist ein Bäcker.</sample>
    <sample id="245">This paper presents a two-step pipeline for finding high-agreement Amazon Mechanical Turk workers using a needle-in-a-haystack approach. The first step involves qualification tasks to evaluate annotators' ability to handle multiple dimensions, while the second step tests their capacity for handling heavy workloads. The pipeline results in 26 AMT workers, with only 8 gold and 18 silver workers passing the qualification task. The figure on the right shows the Krippendorff alpha cross groups, with the best value being 0.443.</sample>
    <sample id="246">Ja, der Code ist verfügbar und kann auf GitHub abgerufen werden.</sample>
    <sample id="247">This paper introduces a new dataset, FactKG, for fact verification via reasoning on knowledge graphs. The dataset includes claims in both written and spoken styles, supported by the DBpedia knowledge graph. It covers five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The paper proposes a new task, KGReasoning, which involves retrieving evidence from DBpedia and verifying claims using various reasoning methods. Experimental results show that the KGReasoning model outperforms all other baseline models, including claim-only baselines and a GEAR model that uses graph evidence.</sample>
    <sample id="248">Nein, sie sind nicht ausgewogen.</sample>
    <sample id="249">Satz-Präfixe wurden verwendet, um Sätze innerhalb der akzeptablen Domain durcheinander zu混合。</sample>
    <sample id="250">Eine dimensionale Bewertung ist eine Methode, um die Qualität von Dialogen auf einer feineren Ebene zu bewerten.</sample>
    <sample id="251">Die Autoren gehören an der University of Science and Technology of China.</sample>
    <sample id="252">This presentation introduces "You Create," an unsupervised case retrieval system using event extraction. Developed by Sai Kiran Nankillala, Abhinav Joshi, Akash Sharma, and Ashutosh Modi, the system aims to assist legal professionals like lawyers and judges in retrieving relevant past precedents (cited documents) from a large pool of cases. The You Create pipeline leverages unsupervised learning techniques and an event-based approach to enhance retrieval efficiency and generalization across different legal systems without requiring specific tuning. Key contributions include the ILPCR dataset and the You Create pipeline, with experiments validating its performance against various models.</sample>
    <sample id="253">This work presents a double-domain adaptation model for detecting signs of mental disorders in social media. The model, named DISORDER, is trained using a base language model and additional information from Reddit and mental health resources. It uses domain-specific masking to focus on important words during training, allowing it to learn the social media language and specialize in the mental disorder domain. The results show that DISORDER achieves better results than MentalBERT, a model trained with a large amount of data, and has a good balance between precision and recall. Future work aims to explore the application of different lexical resources and clinical data.</sample>
    <sample id="254">This paper presents a document-level distant relation extraction framework with uncertainly guided label denoising to improve the label quality of DSI data. The proposed method uses Monte Carlo dropout technology to model uncertainty in pre-training and dynamic class uncertainty thresholds to filter pseudo labels with high uncertainty. The framework outperforms previous baselines on two public datasets, demonstrating significant performance improvements.</sample>
    <sample id="255">Die Form des Prompts ist wichtig, wenn es um 0- und 1-Schritt-Prompting geht.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art-Dialogmodelle evaluiert.</sample>
    <sample id="258">This paper introduces a novel approach to evaluating the quality of text in natural language processing using large language models. The authors propose using these models to generate ratings based on instructions, aiming to provide an alternative to human evaluations which are often unstable and difficult to reproduce. They conducted experiments with four different large language models, comparing their outputs to human evaluations by English teachers. The results show that some models, such as BERT and ChatGPT, demonstrate a clear preference for human-written texts over those generated by GPT-2, suggesting that large language models can be used as a reliable alternative in certain tasks.</sample>
    <sample id="259">This paper presents Exemplar, a unified benchmark for cross-lingual semantic parsing in multiple natural languages and multiple representations. It introduces a dataset containing 90 datasets from various domains, 5 semantic parsing tasks, 80 mini-representations, and 22 natural languages across 15 language families. The paper evaluates the performance of different models, including encoder-decoder and encoder-only models, on six settings: translate test, monolingual model, monolingual fine-tune, multilingual model, cross-lingual zero-shot, and cross-lingual few-shot transfer. The results show that encoder-decoder models outperform previous work, especially when trained on a mixture of languages. The paper also highlights the challenges of cross-lingual semantic parsing, such as the lack of coverage on certain natural languages and mini-representations.</sample>
    <sample id="260">Eine Person arbeitet an der Arbeit beteiligt.</sample>
    <sample id="261">Ein guter Planer sollte Skripte schaffen, die realistisch und zu den Bedingungen passen.</sample>
    <sample id="262">Die Arbeit wird von einem Autorenerteil betreut.</sample>
    <sample id="263">This paper presents a systematic investigation of label bias problems in in-context learning (ICL) for large language models. It identifies domain label bias as a new type of bias and proposes the domain context calibration method to mitigate all types of biases. The method uses random in-domain words sampled from the task corpus as content-free text to estimate model biases on each label name, which are then used to calibrate the model's original predictions. Experiments show that domain context calibration significantly improves ICL performance, particularly on tasks with larger domain label bias levels.</sample>
    <sample id="264">The paper presents a novel task called Transferable Audio-Visual Text Generation (TAVTG) to address the challenge of multi-modal domain shift in audio-visual text generation. The authors propose a framework consisting of three components: an Audio-Visual Map Network, an Audio-Visual Encoder, and a Language Model Generator. The Audio-Visual Map Network maps different visual concepts across domains into a unified audio-visual semantic space, while the Audio-Visual Encoder and Language Model Generator generate audio-visual text based on the mapped concepts. The framework is trained using a contrastive learning approach that optimizes the alignment between visual and audio modalities. The results show that TAVTG outperforms state-of-the-art models on cross-domain settings, demonstrating its effectiveness in handling low-resource domains with limited labeled data.</sample>
    <sample id="265">Wasuda</sample>
    <sample id="266">Die Autoren gehören an der Paderborn University.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Omissionen.</sample>
    <sample id="269">Hallo, ich bin James Finch und ich bin Sarah Finch. Heute werden wir Ihnen alles über ABC-Eval erzählen, eine neue dimensional-basierte Methode zur Beurteilung von conversational AI. Dieses Werk wurde von dem Emory NLP Lab, das von Professor Geno Choi an der Emory University geleitet wird, in Zusammenarbeit mit Amazon Alexa AI erstellt worden. Lassen Sie uns sagen, dass Sie einen Dialogmodell entwickelt haben und möchten sehen, wie gut es sich gegenüber dem aktuellen Stand der Technik vergleicht. Eine gemeinsame Praxis ist es, menschliche Beurteilungen zu verwenden, indem man menschlichen Richtern fragt, welche der zwei Dialoge besser ist, oder sie auf einer Likert-Skala bewertet.Diese Ansätze funktioniieren gut, um einen holistischen Beurteilungsansatz für die Gesamtqualität des Dialogs zu erhalten, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie eventuell mehrere Dimensionen der Chatschwelle bewerten, um die Stärken und Schwächen des Modells auf einem feineren Granularitätslevel zu verstehen. Ein Ansatz besteht darin, einfach menschlichen Richtern zu beordern, mehrere Dimensionen der Dialogqualität zu bewerten, indem sie beispielsweise die Relevanz der Modellantworten bewerten, indem sie existierende vergleichbare oder Likert-Skala-Methoden verwenden. Allerdings glauben wir, dass es eine präzisere und zuverlässigere Strategie für die dimensional-basierte Dialogbeurteilung gibt. Unsere Ansatz versucht, die Subjektivität menschlicher Beurteilungen zu reduzieren, indem wir explizit feststelle, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel die Bereitstellung von irrelevanter Information oder die Kontradiktion von sich oder seinem Partner. Wir nennen diese Ansatz "annotating behaviors in chat" (ABC-Eval) in Kürze. Wir haben diese Methode entwickelt, um die Chatschwelle zu umfassend abzudecken, die in jüngster Literatur als Auswirkungen auf die Chatschwelle vorgeschlagen wurden. ABC-Eval ist in der Lage, die Rate zu messen, wie ein Chatschwelle Model diverse thematische Fehltritte begeht. Zum Beispiel misst ABC-Eval die Anzahl der Worte, in denen ein Chatschwelle Modell seinen Partner ignoriert oder etwas irrelevantes sagt, sich oder seinen Partner kontradikt, falsche Fakten auslotet oder gegen allgemein akzeptierte Kenntnisse verstößt, und wann das Modell Erfolg zeigt oder versagt, Empathie zu zeigen. Um zu bestimmen, welche Art von Beurteilung am besten geeignet ist, haben wir vier state-of-the-art Chatschwelle Modelle ausgewählt und davon 100 menschlich-basierte Dialoge pro Modell auf 100 menschlich-basierte Dialoge pro Modell mit ABC-Eval bewertet. Als Vergleich haben wir auch die Dialoge auf 100 menschlich-basierte Dialoge pro Modell mit drei existierenden Methoden bewertet: Likert-Ratings auf der Turnebene, Likert-Ratings auf der Dialogebene und Dialogebene parewised Comparisons. Für jede der existierenden Methoden haben wir Bewertungen auf acht von den meist bewerteten Aspekten des Dialogs gesammelt, da dies standardmäßig für die Beurteilung von Chatschwelle Modellen auf mehreren Dimensionen gilt. Aus our analyses of these evaluation results haben wir festgestellt, dass ABC-Eval-Verhaltenslabels im Allgemeinen zuverlässiger sind als dieLabels, die von existierenden Methoden gesammelt wurden, wie gemessen durch die interannotator Agreement auf 100 doppelt labelten Dialogen. In addition, ABC-Eval-Labels sind zuverlässiger als dieLabels, die von existierenden Methoden gesammelt wurden, wie durch die lineare Regression-analyse gezeigt wurde. Zum Beispiel zeigt, wie die Messung der Anteile der Worte, in denen ein Chatschwelle Modell seinen Partner ignoriert oder etwas irrelevantes sagt, selbst oder seinen Partner kontradikt, oder falsche Fakten auslotet oder gegen allgemein akzeptierte Kenntnisse verstößt, 5% und 10% der Dialogqualität respektive erklären, während die durchschnittlichen Likert-Konsistenzscore 4% oder weniger erklären. Schließlich haben wir überprüft, ob jede Evaluationsmethode ein eindeutiges Aspekt der Chatschwelle qualifiziert, indem wir eine schrittweise lineare Regression verwendet haben. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25% der Dialogqualität erklärt und, wenn Sie die Metriken eins nach dem anderen entfernen, die meisten davon resultieren in einem Verlust an Informationen über die Qualität. Auf der anderen Seite erklären die Kombination aller Turnebene Likert-Metriken viel weniger der Qualität und fewer davon tragen eindeutige Informationen. Diese zuverlässigen, informativen und eindeutigen ABC-Eval-Metriken ermöglichen es uns, conversational AI mit einer höheren Auflösung zu bewerten als vorherige Methoden. Sie können sehen, dass in den Ergebnissen unseres Experiments mehrere Herausforderungen immer noch bestehen und exakt quantifiziert wurden. Zum Beispiel haben die Bots, die wir getestet haben, gegen 20% ihrer Antworten gegen allgemein akzeptierte Kenntnisse verstößt, gegen 15% ihrer Antworten irrelevante Information produziert und gegen 10% der Zeit sich oder ihren Partner kontradikt. Mit der raschen Verbesserung in diesem Feld können viele dieser Fehltritte in neuen Modellen, seit unsere Evaluation durchgegangen ist, eine Reduzierung erleben. Allerdings ist das noch mehr Grund, die zuverlässigen und präzisen Evaluationsmetrischen zu verfolgen, um Modelle zu vergleichen. Wir hoffen, ABC-Eval kann von anderen in diesem Feld als wichtiger Schritt in diese Richtung verwendet werden und wir freuen uns darauf, wie conversational AI im kommenden Monat und Jahr weiterentwickeln wird. Vielen Dank für das Zuhören.</sample>
    <sample id="270">Die Autoren gehören an der Emory University.</sample>
    <sample id="271">CFT steht für "Continuously Fine-Tuning".</sample>
    <sample id="272">Es sind insgesamt sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="273">Hallo, mein Name ist Kai O. Yin und ich werde heute über unser Werk "Wann benötigt eine Übersetzung Kontext?" präsentieren. Dieses Werk wurde in Zusammenarbeit mit Patrick Franz, Emyu, Andrew F. D. Martins und Graham Nivig erstellt. Viele Übersetzungen hängen von Kontext ab. Zum Beispiel: Wie übersetzen wir "Mole" in diesem Satz? Wenn der vorherige Satz "Dinge könnten zu Gefahr kommen, wenn die Minister davon erfahren" war, dann bezieht sich "Mole" auf einen Spion. Aber wenn der vorherige Satz "Könnte es etwas Ernstes sein, Doctor?" war, dann bezieht sich "Mole" auf eine Narbe. Abhängig vom Kontext verändert sich sowohl die Bedeutung als auch die Übersetzung des Wortes. Allerdings ist es schwierig zu bewerten, wie gut Modelle solche Fälle handhaben können. Zunächst einmal because nur ein kleiner Teil von Übersetzungen hängt von Kontext ab, was bedeutet, dass Metrischer Maßstäbe wie BLEU nicht in der Lage sind, diese Übersetzungen zu berücksichtigen. Ein paar Leute haben vorgeschlagen, eine spezielle Bewertung auf Kontext abhängige Übersetzungen durchzuführen, aber diese Ressourcen unterstützen nur begrenzte Typen von Kontext-abhängigen Übersetzungen und begrenzte Sprachensegmente, da sie normalerweise von Domänenwissen und menschlicher Kuration abhängen. In unserem Werk versuchen wir, zwei Fragen zu beantworten: Erstens, wann benötigt eine Übersetzung Kontext? Und zweitens, wie gut können Modelle solche Fälle bewältigen? Um die erste Frage zu beantworten, haben wir zuerst gemessen, wie viel ein Wort von Kontext abhängt. In unserem vorherigen Werk haben wir CXMI (Contextualized Crosslingual Mutual Information) als Maß für den Kontextgebrauch von Maschinell übersetzten Modellen eingeführt. Das wird durch Messen der Information, die der Kontext C für die Zielsprache Y im Hinblick auf die Quellsprache X liefert, erreicht. Man kann CXMI als die Information, die durch das Giving von Kontext an das Modell gewonnen wird, denken. In unserem Werk haben wir CXMI auf Punto Y CXMI erweitert, um den Kontextgebrauch auf Sateliers oder auf Wortebene zu messen. Wir können Wörter mit hohen Punto Y CXMI als solche, die Kontext benötigen, für Übersetzungen betrachten. Jetzt analysieren wir Wörter mit hohen Punto Y CXMI, um Muster zwischen diesen Wörtern zu finden. Wir führen unsere Analyse auf Transkripten von TED Talks durch, die von Englisch ins 14 verschiedenen Sprachen übersetzt wurden. Wir führen unsere Analyse an drei verschiedenen Ebenen durch: Zunächst schauen wir uns Worte mit hohen Mittelwerten Punto Y CXMI an, um z.B. Duo-Pronomina in Arabisch zu finden, die hohen Punto Y CXMI aufweisen. Das kann erklärt werden, weil Englisch Duo-Pronomina nicht hat, also Kontext benötigt wird, um zu bestimmen, ob ein Pronomen Duo ist, wenn es in Arabisch übersetzt wird. Ähnlich finden wir, dass bestimmte Sprachen beim Entscheiden über die richtige Verbform Kontext benötigen. Wir schauen uns dann Wörter mit hohen Punto Y CXMI-Änderungen über alle ihre verschiedenen Auftretensweisen an, um z.B. die Fälle zu identifizieren, in denen in Chinesisch Kontext benötigt wird, um die richtige Übersetzung von Nomen zu finden, um sicherzugehen, dass dieselbe Übersetzung im Dokument verwendet wird. Ähnlich finden wir, dass Kontext benötigt wird, um die richtige Formalität zu übertragen. Schließlich schauen wir uns Wörter an, die hohen Punto Y CXMI auf individuellen Tokenebene aufweisen, um Phänomene zu identifizieren, die nicht von einem Wort alleine, sondern von der Satzstruktur ausgedrückt werden, z.B. Ellipsenresolution. Jetzt verwenden wir unsere Auswertungen aus dieser Analyse, um einen Benchmark für Dokumentebasierter Übersetzung zu entwerfen. Für jedes der 5 diskursive Phänomene, die wir identifiziert haben, erstellen wir Tagger, um Wörter zu identifizieren, die sich zu diesem Phänomenen hängen, und nennen unser Tagger Multilingual Diskursbewusst (Muda) Tagger. Wir können dann auch beachten, dass verschiedene Sprachen unterschiedliche Proportionen dieser diskursiven Phänomene aufweisen. Wir verwenden dann den Muda Tagger, indem wir ihn auf ein parallel korrektionsloses Modell anwenden, das wir für die Evaluation verwenden möchten, und wir überwachen unsere Übersetzungsmethoden auf den Kontext abhängigen Beispielen, die der Muda Tagger identifiziert hat. Schließlich verwenden wir unserBenchmark als Ergänzung zu anderen Metrischen, um verschiedene Modelle auf Dokumentebasierter Übersetzung zu bewerten. Zunächst einmal, wenn wir Korpusbasierte Metrischen wie BLEU verwenden, finden wir, dass Kontextagnosische Modelle die beste Performance aufweisen. Wenn wir jedoch BLEU verwenden, die Kontextbewussten Modelle verwenden, dann performieren diese am besten. Wenn wir endlich Word F-Maß verwenden, then Modelle mit und ohne KontextComparable Performance aufweisen. Dies zeigt, dass es schwierig ist zu bestimmen, welche Dokumentebasierter Übersetzungssysteme die beste Performance aufweisen, wenn wir nur Korpusbasierte Metrischen verwenden. Jetzt verwenden wir den Muda Benchmark, um Modelle zu bewerten, und finden, dass Kontextbewusste Modelle signifikant besser als Modelle, die Kontext für bestimmte diskursive Phänomene wie Formalität und lexikalische Kohärenz verwenden, sind. Aber diese Modelle sind nicht viel besser als Modelle, die Kontext für andere Phänomene wie Ellipsen, Pronomen und Verbformen verwenden. Das zeigt, dass wir bei Dokumentebasierter Übersetzung noch Fortschritte machen müssen. Wir vergleichen auch verschiedene kommerzielle Systeme und unser Benchmark zeigt, dass DeepL normalerweise besser als Google Translate auf Dokumentebasierter Übersetzung ist. Insgesamt haben wir einen datengetriebenen Analyseansatz über 14 Sprachpaare durchzuführen, um zu bestimmen, wann Übersetzungen Kontext benötigen. Und dann verwenden wir unsere Auswertungen, um einen Benchmark für Dokumentebasierter Übersetzung zu entwerfen, um zu bestimmen, welche diskursive Phänomene Modelle gut bewältigen und welche Übersetzungssysteme gut auf Dokumentebasierter Übersetzung sind. Vielen Dank für Ihre Aufmerksamkeit. Ich sehe Sie bald in Torododo.</sample>
    <sample id="274">Usen Zhang</sample>
    <sample id="276">This paper presents a dataset for evaluating machine translation metrics in Indian languages. The authors focus on five languages belonging to two language families: Dravidian (Tamil and Malayalam) and Indo-Aryan (Hindi, Marathi, and Gujarati). They generate 1400 candidate translations for each language using seven translation models and collect human annotations with detailed error types and severity. The results show that Comet outperforms Comet baselines in most languages and has higher correlations with human scores compared to other metrics.</sample>
    <sample id="277">Die neue Methode hat keinen Namen.</sample>
    <sample id="278">Die Methode der "markierten Wörter" ist eine Art zu identifizieren, welche Wörter bestimmte Gruppen von unmarkierten Gruppen unterscheiden.</sample>
    <sample id="279">Die Autoren gehören an der University of Washington.</sample>
    <sample id="280">This paper proposes a novel multi-modal fusion framework for emotion regulation in conversations, called MultiEmo. The framework consists of four key components: uni-modal feature extraction, context modeling, multi-modal fusion, and emotion classification. It introduces a novel visual feature extractor named VisiNet, which captures visual cues by integrating facial expressions from multiple frames without encoding redundant scene-related information. MultiEmo also proposes a multi-modal fusion model called MultiTen, which integrates one modality with complementary information from the other modalities through stacked bi-directional multi-head cross-attention layers. Additionally, it introduces a sample-weighted focal contrastive loss to address the difficulty of distinguishing between semantically similar emotions. Experimental results demonstrate that MultiEmo achieves state-of-the-art performances on two ERRC benchmark datasets, MELD and iEMOval, with significant improvements in minority and semantically similar emotions.</sample>
    <sample id="281">The paper presents a data-driven exploration of when translation requires context, titled "When Does Translation Require Context: A Data-Driven Multilingual Exploration." The authors, Kai Yen and colleagues, investigate the impact of context on translation by analyzing transcripts of TED Talks translated into 14 languages. They introduce CXMI, a measure for context usage in machine translation models, and extend it to sentence and word levels. By identifying words with high CXMI, they discover patterns such as dual pronouns in Arabic and verb form selection in certain languages. The authors then design a benchmark for document-level translation using multilingual discourse-aware tags and apply it to evaluate different models. The results show that context-aware models outperform models without context for certain discourse phenomena, but overall, there is still room for improvement in document-level translation systems.</sample>
    <sample id="282">This paper presents our new work on style transfer in natural language generation, specifically focusing on discourse level style transfer. We propose a generative model named StyleTrans that learns discourse representations from the source text and combines them with normal style embeddings to generate text in the target styles. We also design a new training objective to reduce the stylistic features of the discourse representations, pulling the representation derived from different texts closer in the latent space. Our experiments show that StyleTrans outperforms strong baselines in terms of style control and content preservation, and achieves state-of-the-art results in style visualization and automatic evaluation metrics.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">This paper presents a novel fuzzy spam mechanism for enhancing universal information extraction. The proposed method uses a continuous distribution of correct probability to model the spam boundary, which is represented as a mask function to trim attention distribution. Experiments on three information extraction tasks show that FAS UIE achieves significant performance improvement compared to UIE without fuzzy spam mechanism.</sample>
    <sample id="285">This paper introduces a new evaluation framework for fact error correction in dialogue summarization. The authors argue that current evaluation methods, such as fact C and DA, are flawed and do not accurately assess the performance of fact error correction models. They propose a taxonomy of fact errors based on content and form, and a three-step evaluation framework consisting of alignment, classification, and comparison. The results show that training fact error correction models with human-annotated reference summaries from dialogue summarization datasets yields the best results. Combining human-annotated data with synthetic data is also promising.</sample>
    <sample id="286">James Finch</sample>
    <sample id="287">Es sind insgesamt 4 Autoren an der Arbeit beteiligt.</sample>
    <sample id="288">Die Datensätze, die zum Testen syntaktischer Phänomene verwendet werden können, sind die BLM-Datensätze.</sample>
    <sample id="290">FTW, COSINE, VELLA, FINE-TUNING, and WSL</sample>
    <sample id="291">Das Modell wird evaluiert anhand von Aufgaben wie Namensentwicklung, Klassifizierung, Part-of-Speech-Tagging und Fragebehandlung.</sample>
    <sample id="294">CamemBERT wurde ursprünglich auf der NACO dataset trainiert.</sample>
    <sample id="295">Der Referent*in heißt Adam Siprowski.</sample>
    <sample id="296">This video presents a collaborative work between the University of Turin and Amazon Alexa, focusing on irony detection in natural language processing. The researchers developed a corpus called EPIC, which consists of 300 short conversations from social media sources like Reddit and Twitter, annotated by 74 annotators for five varieties of English. They observed differences in inter-annotator agreement based on annotators' gender, age group, nationality, and other dimensions. They also built perspective-aware models to model these differences, resulting in more confident predictions compared to gold-standard aggregated models. The results showed significant differences in confidence levels, particularly in the case of age and geographical distribution of annotators.</sample>
    <sample id="297">This project develops a typology and glossary of dog whistles, performs a case study on historical U.S. political speeches, evaluates dog whistle recognition in language models, and conducts a case study on toxicity detection to show how dog whistles can evade content moderation online. The project reveals that dog whistles are more associated with conservatism over time and that language models like GPT-3 can surface many dog whistles but perform poorly with informal or transphobic dog whistles.</sample>
    <sample id="298">Die Versuche zu einem neuen Datensatz mit jüngerem Datensammlungszeitraum zu erneuern, die das Leistungsverlust durch die zeitliche Verzögerung bestätigen.</sample>
    <sample id="299">This paper proposes a training method to reduce the reliance of neural network models on shortcuts and improve their out-of-distribution performance. The key insight is that neural network models suffer from poor performance on under-represented hard training instances with patterns that could indicate the shortcuts in the dominant easy examples. These hard examples are pivotal for ensuring good generalization performance on out-of-distribution samples. The proposed method uses a minimax training objective between a learner and auxiliary model to generate example weights, which incentivize the learner to concentrate on regions of the input space where it incurs high losses. This approach does not make assumptions about the type of shortcuts contained in the dataset and relies on the learner's own training dynamics to generate example weights. The method is evaluated on three commonly used neural network datasets and their corresponding out-of-distribution test sets, showing consistent improvement in out-of-distribution performance while maintaining high in-distribution accuracy.</sample>
    <sample id="300">This paper introduces the Interactive Dictation task, a process that enables users to dictate and edit documents using their voice in a natural and intuitive manner. The task involves flexible interleaving of dictation and editing, using intuitive and open-ended natural language utterances to specify edits. The authors design a data collection interface and build a dataset for this task, and create a baseline system that performs each of the four steps involved: ASR recognition, segmentation, command extraction and normalization, and execution of dictation and command utterances. They evaluate different architectures and output types for the ASR repair and interpretation models, finding a trade-off between runtime and accuracy. The authors welcome further work on this task and provide code and more details in their paper.</sample>
    <sample id="302">Um die Ausgabesequenz in der richtigen Reihenfolge zu erhalten.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, um die Ursachen für prunkhafte Mustergenerierungen zu verstehen und zu untersuchen.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind diejenigen, die in einem Sprachmodell nicht akzeptiert werden.</sample>
    <sample id="305">This abstract presents a critical analysis of weakly supervised learning (WSL) in the context of deep learning. The authors, Dawei and colleagues, investigate the necessity of clean validation data for WSL approaches to achieve high performance on clean test sets. They find that recent WSL methods require clean validation samples to work effectively, with a significant performance drop without them. The study also highlights the practical challenges of obtaining clean validation data and suggests that fine-tuning on clean samples can be a simpler and equally effective alternative. The authors recommend reporting model selection criteria, comparing WSL approaches with fine-tuning baselines, and considering continuous fine-tuning as a viable method.</sample>
    <sample id="306">This paper presents a study on the ability of pre-trained language models to track entities in language models. The authors argue that this is a crucial ability for understanding long discourse, but there haven't been any systematic investigations into what pre-trained language models can actually perform such tasks. The research question they are trying to answer is to what extent large language models can track entities. They designed a task involving boxes and objects, where the input to the model starts with a description of the initial contents of each box, and the task of the language model is to complete the input by predicting the contents of each box. They implemented various measures to prevent the model from using heuristics, as discussed earlier. They tested the setup with FLAN-T5 and GPT-3 and 3.5 models using two-shot in-context learning. Their experiments show that most models simply repeat the initial state, while only text-davinci-03 accepts non-trivial tracking. They found that all GPT-3.5 models which all have been trained on substantial amounts of code exhibit non-trivial entity tracking behavior, whereas all models that do not have code as a substantial part of their pre-training do not. They also found that smaller models like T5 base can learn to perform entity tracking if directly fine-tuned, but on the other hand randomly initialized models of the same architecture cannot learn our state tracking task even when they receive direct supervision.</sample>
    <sample id="307">Die Autoren haben die Performance der Modelle anhand von Public und Private Datasets mittels Metriken wie NER, Classification, Part-of-Speech tagging und Question Answering bewertet.</sample>
    <sample id="308">Jenny, eine erstjahrige PhD-Studentin an Carnegie Mellon University, präsentiert in einem Vortrag über die Positionalität von NLP-Modellen und Datensätzen. Sie diskutiert die Auswirkungen von Design-Biasen auf die Performance von AI-Systemen, insbesondere bei der Identifikation von toxicem Inhalt. Jenny und ihre Kolleginnen haben eine Methode namens NL-Positionalität entwickelt, um die Positionalität von Datensätzen und Modellen zu messen. Sie haben über 16.000 Annotierungen von über 1.000 Annotatoren aus 87 Ländern gesammelt und haben festgestellt, dass NLP-Modelle und Datensätze oft eng mit bestimmten Bevölkerungsgruppen und Bildungsniveauen korreliieren. Sie empfehlen, die Researchprozess zu dokumentieren, NLP-Forschung durch die Lesebrille der Perspektivlosigkeit zu betrachten und spezialisierte Datensätze und Modelle für bestimmte Gemeinschaften zu erstellen.</sample>
    <sample id="309">Interannotator agreement</sample>
    <sample id="310">Wikipedia</sample>
    <sample id="311">Die Autoren gehören an der Humboldt-Universität zu Berlin.</sample>
    <sample id="312">MultiInstruct ist der erste MultiModell-Instruction-Tuning-Benchmarksatz, der 62 diverse MultiModell Aufgaben abdeckt.</sample>
    <sample id="313">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="314">Die Definition der binären Koordination lautet, dass die Struktur der Koordination Lisa Bart und Maggie so ist, dass der erste Conjunkt der Kopf der gesamten Koordinatstruktur ist.</sample>
    <sample id="315">Die in dieser Studie verwendeten Prompts lagen im Durchschnitt bei 14 Wörtern.</sample>
    <sample id="316">Die Auswirkungen auf das kleinere T5-Modell sind, dass es die besten Skripte generiert, die die höchste Similaritätsscore und die Kontrolle über Schlüsselwörter haben.</sample>
    <sample id="317">This paper presents a novel approach for information extraction using code generation models. The proposed method, called CodeIE, transforms the unstructured input text into a structured format during the input stage and uses code generation models to generate the structured output. The authors evaluate their method on three datasets: named entity recognition, relation extraction, and code generation. They compare the performance of their method with traditional language models and find that it significantly outperforms them in terms of accuracy and recall. The results suggest that transforming information extraction into a code generation task can lead to better performance and more accurate outputs.</sample>
    <sample id="318">Hallo, ich bin Yanis Slavac und ich werde Ihnen heute unsere Arbeiten über Dr. Bert präsentieren, ein robustes Sprachmodell in Französisch für die biomedizinische und klinische Domäne. In dieser Präsentation werden wir zuerst über Sprachmodellierung im Gesundheitswesen sprechen. Danach präsentieren wir die Hauptbeiträge unseres Artikels. Wir Introduzieren das erstmalige biomedizinische Modell in Französisch, Dr. Bert, das auf Roberta basiert und auf NACHOS trainiert wurde, einer Datensammlung von medizinischen Crowd-Daten aus dem Web. Wir präsentieren auch einen Vergleich von Modellen mit verschiedenen Pretraining-Einstellungen und Datensätzen. Schließlich präsentieren wir unsere Resultate auf 11 biomedizinischen und klinischen Aufgaben im Französischsprachbereich. Abschliessend diskutieren wir über die Experimente und geben Ihnen mehr Details darüber, wie Sie Zugriff auf die Modelle erhalten können. Seither es im Jahr 2019 freigegeben wurde, ist Bert zu einem der effektivsten Ansatzpunkte zur Bewältigung von Natural Language Processing-Aufgaben geworden und bietet einen großen Leistungsunterschied gegenüber historischen statischen und konzektionären Methoden wie Word2Vec, FastText und Elmo. Seither ist das Modell an viele andere Sprachen wie Französisch mit Camembert und in anderen Domänen wie biomedizin mit PUMA Bert und Bio Bert und klinisch mit Klini Bert, hauptsächlich in Englisch, adaptiert worden. Spezialisierte Modelle für andere Sprachen sind seltener und oft aufgrund des Mangels an in-domänischem Datensatz auf kontinuierliche Pretraining-basiert. Allerdings hat Französisch keinen offenen Quellens Modell für biomedizin und klinische Aufgaben. Also fragten wir uns, was die geeignetste Datensammlung für eine Vielzahl von Anwendungsfällen ist und diese Crowdsourcing-Daten als E subsitution für klinische Daten verwenden. Um diese Frage zu beantworten, verglichen wir Bert mit unserem Shubert-Modell, das auf anonymisierten Datensätzen basiert, die von der National University Hospital der Taiwan House abgeleitet wurden. Danach fragten wir uns, wie viel Datensatz benötigt wird, um ein spezialisiertes Modell auf Französisch zu trainieren. Ist es 4 GB, 8 GB oder mehr? Um diese Frage zu beantworten, trainierten und verglichen wir vier von Grund auf neue Modelle: eine erste Version von Bert mit 7 GB von NACHOS, eine zweite Version von 4 GB subset von NACHOS, eine erste Version von Shubert, das klinische Modell mit 4 GB von Sentences, die aus ClinicalNotes stammen, und eine finale Version von Shubert mit einem Mix von 4 GB subset von NACHOS und 4 GB von ClinicalNotes. Neben dieser Vergleich präsentieren wir drei Modelle, die auf kontinuierliche Pretraining-basiert sind, um die Auswirkungen von Pretraining-Strategien zu analysieren: eine basierend auf dem Gewicht von Camembert und trainiert auf 4 GB subset von NACHOS, eine weitere basierend auf Camembert, aber trainiert auf 4 GB von ClinicalNotes und eine dritte basierend auf einem englischen biomedizinischen Modell, PUMA Bert, und trainiert auf 4 GB subset von NACHOS. Insgesamt haben wir insgesamt sieben Modelle. Um unsere sieben Modelle zu evaluieren, verglichen wir sie an öffentlichen und privaten Datasets wie Namensentitätserkennung, Klassifizierung, Part-of-Speech-Segmentation und Question Answering.Diese Modelle wurden mit sechs Baseline-Modelle verglichen, die Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PUMA Bert Bio Bert und Klini Bert sind. Die Evaluation zeigt, dass das Modell am besten auf der Aufgabe mit Datensätzen der gleichen Natur performt, auf den es das Modell trainiert wurde. Allerdings können wir beobachtet haben, dass Datensätze aus heterogenen Quellen zu verhältnisse zu sein scheinen. Wir können auch beobachtet haben, dass die Verwendung mehr Datensätze zu besseren Performen führt. Insgesamt scheint die von Grund auf neue Pretraining-Methode zu besseren Performen auf den meisten Aufgaben zu führen. Allerdings haben unsere Experimente mit kontinuierlicher Pretraining-Methode, die auf dem Gewicht und Tokenizer von PUMA Bert basiert und auf 4 GB subset von NACHOS trainiert wurde, ein vergleichbares Resultat zu denjenigen mit Bert 4 GB von Grund auf neuen Modellen erzielen, was nicht der Fall ist, wenn das Modell auf Gewicht und Tokenizer von Camembert basiert und 4 GB subset von NACHOS trainiert wurde. Schliesslich können wir abschliessend sagen, dass unser vorgeschlagener System eine bessere Performance auf neun der 11 Aufgaben erzielt und global über die Resultate des generischen Modells Camembert hinausgeht. Wir können auch beobachtet haben, dass spezialisierte Datensätze besser sind, spezieller Datensatz ist besser, aber es nicht skaliert. Das Pretraining-Modell, das auf NACHOS abgeleitet wurde, ist freiavailable und auf YouTube und all die Trainingsskripte sind auf unserem GitHub Repository. Also danke für die Präsentation und wir freuen uns auf die Besprechung bei der Post-Session in Toronto.</sample>
    <sample id="319">Die Arbeit untersucht die Auswirkungen von Lernstrategien wie der kritischen Pretraining, der Finanzierung und der Kontinuum Pretraining auf die Leistung von Modellen in der Bioinformation.</sample>
    <sample id="320">Die Überanpassung, die auf die Wiederverwendung von Tests zurückzuführen ist, wird als adaptive Überanpassung bezeichnet.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde anhand von Typen von Vereinfachungen analysiert, z.B.Lexikalische Vereinfachung, Strukturelle Vereinfachung und allgemeine Vereinfachung. Es wurde auch die Vielfalt der verschiedenen Vereinfachungsverwandlungen untersucht.</sample>
    <sample id="322">This paper presents a study on how language models learn about morality in text. The authors apply explainable AI techniques to understand how morality is expressed differently across different domains, using the Morality Foundation Twitter Corpus with 35,000 tweets from seven domains. They found that language models recognize differences in moral expression between domains, such as the contrasting rhetoric for subversion in ALM and BLM. This highlights the importance of domain-specific understanding in language models to avoid misinterpretation of morality.</sample>
    <sample id="323">The paper presents a method for answering questions in the ComCo Sense QA task using a dynamic heterogeneous graph and knowledge base. The authors propose a hybrid knowledge graph (HKG) that combines language models and knowledge bases to retrieve relevant knowledge from external sources. They introduce a novel entity matching strategy to build a subgraph, which is then used to encode and fuse QA contexts. The HKG is optimized using a two-stage training strategy and knowledge distillation. The authors conduct experiments on ComCo Sense QA and obtain QA, comparing their results with LM and HKG, and report good results.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile.</sample>
    <sample id="325">Hallo, mein Name ist Mateusz Lendman und heute werde ich Ihnen einen kurzen Einführung in unser Papier über kompositionale Generalisierung ohne Bäume mit Multi-Sets und latenten Permutationen geben. Das ist gemeinsames Werk mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionale Generalisierung kann als die Fähigkeit eines Lerners verstanden werden, um tieferen Rekursionen und unbekannten Kompositionen von Phrasen zu entgegenwarten, die während des Trainings individuell gesehen wurden. In Kontext von semantischer Parsisstesting sieht das wie folgt aus: Wie usual haben wir ein Trainingssatz von Utensilien, in diesem Fall "die Mädchen schlafen" und "Mary weiß, dass die Mädchen schließen". Diese Utensilien werden mit logischen Formen pariert, die Hauptpunkte ihres Bedeutungsviels repräsentieren. Im Gegensatz zu Standard-Maschinelern-Evaluation ist der Test-Satz nicht aus derselben Verteilung abgeleitet, sondern enthält strukturell unbekannte logische Formen. In diesem Beispiel hat das Modell während des Trainings Shallow-Rekursion kennengelernt und wird auf ein Beispiel mit tieferer Rekursion getestet. Naiv-Sequence-to-Sequence-Modelle kämpfen mit dieser Art von Aus-of-Distribution-Generalisierung und produzieren oft Ausgaben, die von der Eingabe abgetrennt sind. In spezieller Weise siehst du, dass sie die systematischen Korrespondenzen zwischen Eingabe und Ausgabe, wie jene, die in der Eingabe farblich kodiert sind, nicht reproduzieren. Ein beliebter Weg, um dies zu beheben, ist die Integration von Bäumen in die Modelle. Die Bäume sind dazu bestimmt, die kompositionale Prozesse zu captur, die Verknüpfungen zwischen Utensilien und logischen Formen repräsentieren. Das funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen teilweise ermittelt werden. Das kann komplex und manchmal computationally teurer sein. Typisch sind dabei considerable formalism-specific Preprocessing der logischen Formen, zum Beispiel um zu handeln, variable Symbole. Das Erreichen von Bäumen kann auch involieren spezialisierten Grammatikinduktionsprozeduren. In diesem Papier verwenden wir keine Bäume und Introduzieren einen neuralen Sequence-to-Sequence-Modell, der direkter die Korrespondenzen zwischen Fragmente der Eingabe und Fragmente der Ausgabe modelliert. Zum ersten Mal zeigen wir starke Generalisierung zu tieferen Rekursionen, ohne auf Bäume zu vertrauen. Unsere Ansatz prädikt die Ausgabe aus der Eingabe in zwei Schritte. Zunächst taggen wir jede Eingabe-Tokeneinheit mit einem unsortierten Multi-Sets von Tokeneinheiten, die im Ausgabebereich auftreten werden. Nach dem ersten Schritt haben wir alle richtigen Tokene, aber sie sind nicht sortiert. Dementsprechend benutzen wir in der zweiten Schritt einen anderen Modell, um eine Permutation zu predict, um sie in die richtige Reihenfolge zu sortieren. Wir Introduzieren einen neuen Ansatz, um eine Permutation zu predict, die keinerlei harten Beschränkungen auf die möglichen Permutationen anwendet. Das macht unser Ansatz sehr flexibel und ausdrucksstark. Konzeptionell unserem Permutationen-Modell arbeitet es so: Wir gehen von links nach rechts über den Ausgabebereich und bestimmen, welches Multi-Sets Token in jeder Position platzfindet. Für die erste Ausgabeposition einfach einen aus dem Rot hervorgehobenen Token auswählen. Dann springen wir zu einem anderen Multi-Sets Token, um die nächste Tokeneinheit im Ausgabebereich zu bestimmen. Wir bestimmen die dritte Tokeneinheit im Ausgabebereich in einem ähnlichen Weise, indem wir zu einem anderen Multi-Sets Token springen. Wir fortfahren diese Prozedur, bis jede Tokeneinheit aus dem ersten Schritt exactly ein Mal besucht wurde. Um dir einen Anreiz für die experimentellen Resultate zu geben, hier vergleichen wir unser Modell mit anderen trees-less Modellen auf dem CoNLL Benchmark. Unser Modell überlebt die anderen um ein großes Abstand auf Generalisierung zu tieferen Rekursionen. Ein paar andere Arten von Struktural-Generalisierung bleiben sehr herausfordernd. In unserem Papier lösen wir ein paar interessante technische Herausforderungen. Zunächst einmal ist die Einordnung zwischen Eingabe und Ausgabe nicht im Trainingsdatensatz gegeben. Als Folge davon weiß für einen bestimmten Token nicht, welches Multi-Sets Token es stammt, was eine Herausforderung für das Training darstellt. In addition, manchmal gibt es mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte ist latent. Wir bewältigen dies, indem wir die Einordnung als Teil des Trainings induzieren. Unsere Permutation-Methode ist sehr flexibel, aber sie bringt die Herausforderung, dass die Bestimmung der höchsten Punktscores-Permutation NP-hard ist, da dies mit dem Travelling-Salesman Problem verbunden ist. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die uns auch erlaubt, durch die Lösung zu propagieren und linguistisch plausible Permutationen zu lernen. Wenn du mehr über unsere Experimente und wie wir diese Herausforderungen bewältigen wollen, dann schau doch mal unser Papier an oder komm zu unserem Poster.</sample>
    <sample id="326">Kognitive Dissonanz ist ein Phänomen, bei dem zwei Glaubens oder Handlungssysteme inkonsistent sind.</sample>
    <sample id="327">The proposed Magi Tower is a novel multi-modal architectural model that adapts to different levels of unimodal semantic knowledge. It uses managers in each cross-modal layer to aggregate insights from pre-trained unimodal experts, allowing for more effective exploitation of different levels of unimodal semantic knowledge. Magi Tower achieves superior performance on various downstream tasks and significantly improves performance over Bridge Tower, especially with only 4 million images for pre-training.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">This paper proposes a zero-shot video localization method based on structured pseudo-label generation, which is robust to label noise. The method generates three types of pseudo-queries: (1) pseudo-queries from video frames using an image-to-text pre-trained model; (2) pseudo-events from the temporal structure of events using a pre-trained model; and (3) pseudo-labels by matching the relevance between pseudo-queries and pseudo-events. The method reduces the weight of noisy samples and corrects noisy labels to reduce the influence of label noise. The method outperforms existing methods on two datasets, indicating its robustness to label noise.</sample>
    <sample id="330">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="331">The speaker is Sarah Papi from the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus Transkripten von TED Talks, die von Englisch ins Deutsche über 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="333">This paper proposes a novel training framework for machine translation (MT) that injects common knowledge into the MT model to improve its generalization ability. The framework, called Ink, consists of two steps: extracting common knowledge from a key-value data store and using it to adjust the representation space of the MT model. The adjusted representations are then used to refresh the data store. Experiments on the WMT'19 German-English news translation task show that Ink achieves an average gain of 1.99 BLEU score and 1.00 WER score compared to the state-of-the-art common knowledge MT system, while also achieving better translation performance with less memory space and faster inference speed.</sample>
    <sample id="335">Der Referent*in heißt Mateusz Lendman.</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf die Schulung auf einem Quellsprache und die Anwendung auf eine andere Sprache.</sample>
    <sample id="337">This paper introduces a novel approach for handling out-of-vocabulary (OOV) words in embedding-based downstream models. We propose a word relationship graph that captures lexical rules of word formation and association, enabling the model to effectively handle OOV words by associating them with relevant words. Our method uses a two-level graph structure, where each word or word piece acts as a node, and its corresponding word embedding serves as the node feature. We apply a graph attention network to adjust node attributes and reduce the impact of noisy nodes. The model incorporates a readout block to capture the whole graph information and summarize word formation. Experimental results demonstrate the effectiveness of our model on both intrinsic and extrinsic tasks.</sample>
    <sample id="338">This presentation introduces a collaborative research project on evaluating human explanations in natural language processing. The team, consisting of researchers from various institutions, presents their work titled "Our Human Explanations Are Always Helpful Towards Objective Evaluation of Human Natural Language Explanations." They address the challenge of evaluating the quality of human-annotated explanations, which can be subjective and task-dependent. The presentation outlines a unified data structure for different tasks and proposes a new evaluation metric called "true" that extends the Simulatability Score to better assess the helpfulness of explanations during fine-tuning and inference stages. The results demonstrate that even low-quality human explanations can improve model predictions, and the proposed metric outperforms the Simulatability Score in evaluating dataset qualities across different models and tasks.</sample>
    <sample id="339">Die Autoren sind an der Carl von Bertalanffy University in Germany.</sample>
    <sample id="340">This paper presents a large-scale syntactically diverse paraphrase dataset, PAMR, constructed using AMR back translation. PAMR contains around 15 million source sentences with approximately 6.9 paraphrases per source sentence, offering higher syntactic diversity while preserving semantic similarity to existing datasets. The proposed method leverages AMR graphs to generate diverse paraphrases, which can benefit various NLP applications such as question answering, chatbots, and text generation. Experimental results demonstrate that PAMR outperforms other datasets in tasks like sentence embeddings and paraphrase generation, showcasing its potential for improving the robustness of NLP models.</sample>
    <sample id="341">Die Autoren verwenden die Messungen der Translation Quality (Translation Quality), der Average Latency (Average Latency) und der Computational Latency (Computational Latency) zur Überprüfung der Leistung der verschiedenen Strategien.</sample>
    <sample id="342">This paper presents a large-scale personalized dialogue dataset, LiveChat, constructed from live streaming videos. The dataset includes both text and video sources, with a focus on Chinese multi-party conversations. It addresses the challenges of existing datasets by providing detailed persona annotations and longer average sessions. Experiments show that LiveChat outperforms other datasets in terms of response quality and speaker personalization, demonstrating its potential for developing virtual streamers and virtual employees.</sample>
    <sample id="343">Hallo, alle. Ich bin Ma Chata und heute präsentiere ich mit Martin ein Werk namens "Kipmuster" an. Wir evaluieren dabei die Knowledge Integration aus mehreren Quellen. Das Werk ist eine Kooperation zwischen McGill University, Mira und Microsoft Research. Natürliche Spracheverstehensmodelle ziehen auf eine Vielzahl von Kenntnisquellen hervor, wie zum Beispiel Kenntnisse in den Parametern, die normalerweise durch vorheriges Training erworben wurden, und Kenntnisse, die in den Eingabe-Parametern zur Laufzeit verwendet werden. Recente Arbeiten in Aufgaben wie der Antwortfindung haben gezeigt, dass Modelle Kenntnisse aus vorherigem Training verwenden können, um die Aufgabe zu lösen. Aber Natürliche Spracheverstehensfähigkeit oft erfordert Kenntnisse, die auch zur Laufzeit bereitgestellt werden. Zum Beispiel in der Satz: "John sah den neu gewählten Präsidenten auf TV." Vorherige Parametere können Informationen über das Amt des Präsidenten und was ein TV ist enthalten, aber sie können nicht sicher wissen, wer der bestimmte John ist, oder wer der neue Präsident ist, da der Präsident seit dem vorherigen Training verändert wurde. Daher benötigen erfolgreiche Modelle für Kenntnisintensive NLU-Aufgaben die Fähigkeit, Kenntnisse aus verschiedenen Quellen zu integrieren und zu verwenden. In diesem Werk präsentieren wir einen Diagnosetest für Kenntnisintegration. Wir Introduzieren einen Korreferenz Resolution Task, der darauf abzielt, die Fähigkeit zu bewerten, Kenntnisse aus verschiedenen Quellen zu verwenden. Wir evaluieren die Datensammlung mit Human-Studie-Partizipationen und etablieren Korreferenz Resolution-Modelle. Hier ist ein Beispiel aus der Datensammlung: "Servin ist ein Richter. KIA ist ein Bäcker. Servin und KIA trafen sich in einem Park. Nach einem langen Tag im Dienst, bei der sie Fälle in einem Gerichtsgebäude entschieden hatten, war er glücklich, sich zu entspannen." Das Ziel hier ist, den richtigen Entity zu identifizieren, den die Pronomen "er" beziehen, was in diesem Fall Servin ist. Die Resolution eines bestimmten Pronomens erfordert zwei Arten von Informationen: Erstens die spezifische Kenntnis, d.h. Servin ist ein Richter, und zweitens die Hintergrund-Kenntnis, d.h. Richter entscheiden Fälle in Gerichten. Normalerweise wird Hintergrund-Kenntnis während des vorherigen Trainings von großen Sprachmodellen gelernt, während spezifische Kenntnisse normalerweise während der Laufzeit beobachtet werden. Wir variieren die Verfügbarkeit dieser beiden Arten von Informationen so, dass sie entweder in einer Quelle oder in mehreren Quellen enthalten sind. Wir haben definiert, drei Szenarien von Kipmuster: Erstens die "Vorausblick" -Szenario, bei dem Hintergrund-Kenntnis angenommen wird, als sei sie zur Laufzeit verfügbar. Zweitens die "Hintergrund beides" -Szenario, bei dem Hintergrund-Kenntnis sowohl zur Laufzeit als auch zur Laufzeit verfügbar ist. Drittens die "Hintergrund Laufzeit" -Szenario, bei dem beide Kenntnisarten nur zur Laufzeit verfügbar sind. Dieses letzte Szenario ist insbesondere interessant, da es die Situation simuliert, in der das Hintergrund-Kenntnis, das für die Aufgabe erforderlich ist, nicht Teil des vorherigen Datensatzes der Modelle ist. Zum Beispiel, weil neue Berufe seit dem vorherigen Training entdeckt wurden. Hier ist ein Beispiel, wie wir die Verfügbarkeit von Fakten in den Quellen steuern: In der "Vorausblick" -Szenario nehmen wir an, dass das Hintergrund-Kenntnis "Politiker suchen Wahlmandate im gouvernement" in den vorherigen Parametern enthalten ist. In einem Laufzeit-Kontext liefern wir die antwortspezifische Kenntnis "Chesterfield ist ein Politiker". In der "Hintergrund beides" -Szenario liefern wir neben der antwortspezifischen Kenntnis auch Hintergrund-Kenntnis über Politiker in einem Laufzeit-Kontext. In der "Hintergrund Laufzeit" -Szenario liefern wir die fiktionalen Berufe "Maitre d'hotel" anstelle von "Politiker", da "Maitre d'hotel" unwahrscheinlich in den vorherigen Parametern enthalten ist. Wir evaluieren die Datensammlung sowohl mit Human-Studie-Partizipationen als auch mit etablierten Korreferenz Resolution-Modellen. In diesem Bild zeigen wir die Ergebnisse der besten performing Models auf dem schwierigsten Varianten der "Vorausblick" -Aufgabenset. Ohne spezifische Training auf Kipmuster liefern beide Modelle nicht gut. Wenn trainiert auf Kipmuster, liefern beide CT-F und BT-CRF signifikant besser als zufällige Modelle. Dies zeigt, dass wenn Modelle auf allgemeinen Korreferenz Resolution-Datensätzen trainiert werden, sie lernen zu exploitieren, Super-Quoten, die bei Testen auf Kipmuster nicht nützlich sind, da solche Quoten entfernt wurden. Weitere Experimente mit fiktiver Kenntnis zeigen, dass selbst die besten performing Modelle Schwierigkeiten haben, Hintergrund-Kenntnis zu integrieren, die nur zur Laufzeit bereitgestellt werden. Umzusummen die Haupttakeaways unseres Artikels: Viele Korreferenz Resolution-Modelle scheinen unfähig zu sein, Kenntnis aus verschiedenen Quellen zu bewerten, ohne spezifisches Training. Allerdings beim spezifischen Training auf Kipmuster verbessern sich die Modelle signifikant und successfully integrieren Kenntnis aus mehreren Quellen. Trotzdem scheinen selbst die besten performing Modelle Schwierigkeiten zu haben, Hintergrund-Kenntnis zu integrieren, die nur zur Laufzeit bereitgestellt werden. Wenn Sie mehr Details erfahren möchten, bitte our Paper und die Datensammlung auf GitHub überprüfen. Danke fürs Zuhören.</sample>
    <sample id="344">Die Nachteile der baumbasierten Methoden sind, dass sie meist nicht gegeben und daher zu erhalten sein müssen. Dies kann kompliziert und manchmal computationally teurer sein.</sample>
    <sample id="345">This paper introduces a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output, without relying on trees. The approach predicts the output from the input in two steps: first, it tags each input token with an unordered multiset of tokens that will appear in the output, and then it uses another model to predict a permutation to put them into the right order. This method is flexible and expressive, and it outperforms other treeless models on generalization to deeper recursion. However, some other kinds of structural generalization remain challenging.</sample>
    <sample id="346">Die Autoren der Studie "Do CoNLL 2013 Named Entity Taggers Still Work Well in 2023?" sind aus der Technischen Hochschule Delft.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werden wir über unser Papier "Marked Personas" sprechen, das die Verwendung natürlicher Sprachanregungen zur Messung von Stereotypen in Sprachmodellen untersucht. Dieses Werk wurde in Zusammenarbeit mit ESDermouch und Dan Jorovski erstellt. In jüngster Zeit haben viele die Vorsorge an sozialer Diskriminierung und Stereotypen in großen Sprachmodellen (LLMs) festgestellt. Allerdings haben diese Messungen verschiedene Einschränkungen. Sie hängen normalerweise von von Hand konstruierten Datensätzen ab, die sehr zeitaufwendig zu sammeln sind, und sie messen normalerweise nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere Demografien oder Kontexte übertragen werden. Oder sie capturieren einfach eine sehr allgemeine, breite Assoziation, wie beispielsweise negative Assoziationen mit bestimmten Gruppen. Darüber hinaus hat die meisten Arbeiten in diesem Bereich nicht fürIntersectionality gesorgt, was die These ist, dass multifaktionierte soziale Identitäten die Diskriminierung und Harm kompoundieren und ein einzigartiger Weg sind. Um diese Einschränkungen zu überwinden, vertrauen wir auf die Eigenschaft, dass diese neuen instruction-tuned LLMs sehr gut auf Anweisungen und Anregungen reagieren können. Wir können also das Modell auffordern, einen Persona zu generieren, der eine Beschreibung von einem imaginären Individuum ist, indem wir einen Anreiz wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich." verwenden. Und wir können unmittelbar sehen, dass dies sehr allgemein zu allen Demografien anpassbar ist, da wir einfach den gewünschten Identitätsmarker in den Anreiz einschließen können. Hier sind einige Beispielgenerierungen von GPT-4. Unmittelbar können wir sehen, dass, obwohl die Ausgaben nicht offiziell negativ oder toxikisch sind, im traditionlichen Sinne dieser Worte, es einige interessante Muster gibt. Eine asiatische Frau wird als unaussprechlich dargestellt, eine mittelorientalische Frau wird mit Begriffen wie exotisch und Bezug auf eine bezaubernde Region referiert und sowohl weibliche Persönlichkeiten von Farbe Personen machen Beziehungen zu Ancestry, während die weiße Mann-Persona nichts von dem Art hat. Um diese Muster zu capturieren, haben wir zwei Teile. Der erste Teil ist die Generierung dieser Personas. Unsere Anreize zur Generierung dieser Personas wurden von einem Studie inspiriert, bei der sie diese Anreize menschlichen Subjekten gaben, die sie fanden, dass sie dadurch menschliche Stereotypen surfen konnten. Und auch dadurch können wir direkte Comparisons zwischen den generierten Personas und den menschlich schriftlich responsiven machen. Der zweite Teil ist Marked-Words, der eine Methode ist, um die Wörter zu identifizieren, die Marked-Groups von unmarkierten Gruppen unterscheiden. Ich werde das in Kürze erläutern. Das Vorteil davon ist, dass wir wirklich spezifische Stereotypen und Muster erhalten, ohne uns auf einen bestimmten Lexikon zu verlassen. Der Vorteil von unserem Ansatz ist, dass wir durch die Anreize menschlicher Schreibstelle menschliche Stereotypen surfen können. Der Vorteil von unserem Ansatz ist, dass wir durch die Anreize menschlicher Schreibstelle menschliche Stereotypen surfen können.</sample>
    <sample id="348">This paper presents a method to measure stereotypes in language models (LLMs) using natural language prompts. The authors, Maya and colleagues, address the limitations of existing methods by generating personas based on prompts that elicit stereotypes. They use a lexicon of stereotypes and a marked words method to identify harmful patterns in generated personas. Results show that generated personas contain more stereotypes than human-written ones, with common tropes reflecting harmful narratives. The authors recommend addressing positive stereotypes, using intersectional lenses, and increasing transparency about bias mitigation methods.</sample>
    <sample id="349">Hallo, alle. Mein Name ist Jing Wei, ich komme von der Technologisch-Universität in China. Es ist mir eine Freude, einen kurzen Reklametafel über ein Papier zu präsentieren: "Are you copying my model? Protecting the copyright of large language models for embedding and services." Ich werde einen Watermark-Methode vorgestellt, um die Copryright-Desinfektion von großen Sprachmodellen für die Einbettung und Dienstleistungen zu schaffen. Wir werden die Grundlagen von Einbettung und Dienstleistungen und die Bedeutung von Copryright-Desinfektion diskutieren. Ich werde auch einen neuen Ansatz präsentieren, um Copryright-Desinfektion zu schaffen. Ich danke Ihnen für Ihre Aufmerksamkeit.</sample>
    <sample id="350">This paper investigates the reliability of leaderboard scores in natural language processing (NLP) and evaluates how well they compare to human performance. The authors analyze two popular benchmarks, SuperGLUE and Squad, and find that while systems often outperform humans on these tasks, there are significant issues with the evaluation process, such as differences in dataset sizes and errors in ground truth annotations. The paper argues that these issues make it difficult to draw meaningful conclusions about superhuman performance in NLP and suggests that future benchmarks should be constructed more reliably.</sample>
    <sample id="351">This paper investigates the generalization problem of named entity recognition (NER) models using the CoNLL 2003 dataset. The authors developed a new dataset, CoNLL+, by collecting and annotating news articles from 2020 using the same guidelines as CoNLL 2003. They fine-tuned over 20 models on CoNLL 2003 and evaluated them on both CoNLL 2003 and CoNLL+. The results showed that larger models, particularly transformer models, and more fine-tuning examples lead to better generalization. The main causes of performance drop were temporal drift, not adaptive overfitting. The paper concludes that while CoNLL 2003 tags still work well in 2023, further research is needed to improve model generalization.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat".</sample>
    <sample id="353">This paper introduces a method for Python code generation by asking clarification questions. The authors propose a task of generating code by asking clarification questions, focusing on clarifying operation-level specifications. They introduce interactivity into the code generation process and hypothesize that through interaction, more specifications can be gathered to alleviate the problem of under-specification. The paper also proposes a method to create a synthetic dataset with clarifications on key operations and a pipeline for code generation by asking clarification questions. The authors evaluate their method using various models and discuss the challenges and potential directions for improvement.</sample>
    <sample id="354">Das Leistungsabstand zwischen CoNLL-2003 und CoNLL++ liegt unter 5% für die meisten Modelle, die in der Studie ermittelt wurden.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin ein Kandidat im Bachelor-Studiengang Computertechnik an der Stony Brook University. Ich möchte gerne mein Werk "Transfer Learning for Discourse Detection: Addressing the Rare Class Challenge" als einen langen Paper präsentieren, das in ACL 2023 akzeptiert wurde. Wir beginnen damit, kognitive Diskussion zu definieren und warum es ein wichtiger Problem ist, sie zu studieren. Kognitive Diskussion ist die Inconsistenz zwischen zwei Glaubensstiftungen oder Handlungen. Ein Beispiel wäre, wenn jemand behauptet, dass Zigaretten mich töten könnten, und dann einen Joint nach dem Treffen raucht. Das Glaubensstiftung und die Handlung sind inkonsistent und sie sind in Diskussion. Wir erwähnen auch, dass Diskussion eine sehr gemeinsame Phänomene ist, das wir in unserem täglichen Leben erleben, und sie oft in Sprache ausgedrückt werden. Warum ist das wichtig? Studium von kognitiven Diskussionen kann uns dabei helfen, die Auswirkungen von Meinungsverschiedenheiten among Menschen zu verstehen, Trends in Glauben, Werten und Einstellungen in einer Population zu identifizieren. Hochkognitive Diskussionen sind auch mit Angststörungen verbunden und können dabei helfen, das mentale Gesundheit von Menschen zu verstehen. Das Studium von Diskussionen in Sprache kann auch dazu beitragen, Extremismus und Polarisation von Schwankungsrängen zu verstehen. Schließlich ist kognitive Diskussion wichtig, um die kognitiven Stile von Individuen zu verstehen und das Prozess der Entscheidungsfindung zu verstehen. Um ein Kognitionsdiskussion-Ressourcen-System zu erstellen, haben wir eine große Skala Annotation von Diskussionen durchlaufen. Wir haben die Annäherung von Diskussion verwendet, wie sie im Diagramm hier zu sehen ist. Tweets wurden mit einem Python-Parser analysiert und Paare von Disksourseinheiten wurden nach den Richtlinien, die in einem Papier beschrieben wurden, anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen anhand von Diskussionen an</sample>
    <sample id="356">Die Autoren gehören an der University of Edinburgh.</sample>
    <sample id="357">Siu Yuen</sample>
    <sample id="358">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="359">Die Ansatz wird mit dem "Whitkey" und "Local Alignment" Strategie verglichen, die auf offline Modellen angewandt werden.</sample>
    <sample id="361">This presentation introduces CounterComp, a method for improving compositional generalization in multi-step quantitative reasoning tasks. The approach uses counterfactual scenarios to train models on a variety of question-answer pairs, enhancing their ability to generalize across different inputs and outputs. By incorporating auxiliary metric learning losses that measure the extent of change between input questions, the model learns to focus on meaningful tokens related to operational terms in the output. The method has been shown to improve performance on both in-distribution and out-of-distribution samples, demonstrating its potential for better compositional generalization in language technologies.</sample>
  </task>
</testset>