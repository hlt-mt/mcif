<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">Language models are primarily trained on large-scale web crawl data.</sample>
    <sample id="1">该论文的作者来自蒙特利尔大学、Mila和微软研究。</sample>
    <sample id="2">The paper presented by Tu Yi from Ant Group focuses on the Visually-rich Document Understanding (VrDU) problem, specifically aiming to understand various types of documents such as forms, receipts, and posters. The authors propose a novel pre-trained model called LayoutMask to address issues related to reading order in existing document pre-training models.

LayoutMask differs from previous studies in three aspects: choice of 1D position, masking strategy, and pre-training objectives. Instead of using global 1D positions, LayoutMask uses local 1D positions, which are in-segment token orders. This approach allows the model to infer global reading order by jointly using 1D position, 2D position, and semantic information. To further promote text-layout interactions, LayoutMask employs two novel masking strategies: Whole Word Masking and Layout-Aware Masking. Whole Word Masking sets masks at word level instead of token level, while Layout-Aware Masking has a higher probability of masking the first and last words of each segment.

The paper also introduces a new pre-training objective called Masked Position Modeling (MPM), which involves recovering randomly masked 2D positions during pre-training. The MPM task is similar to the cloze test, where a group of randomly selected words is supposed to be refilled at the right positions in the original documents. The model has to find the context for each word based on semantic relations and infer with 2D position clues from a spatial perspective.

In experiments, LayoutMask was compared using different layout information. The results showed that Local-1D outperforms Global-1D on both FUNSD and SROIE datasets, while falling slightly behind on CORD. The performance gap between local and global mainly comes from the entity "Total," which is difficult to recognize using ordinary reading order implied by Global-1D. Overall, the proposed LayoutMask model enhances text layout interactions and learns better layout representations through joint learning with both semantic and spatial inference.</sample>
    <sample id="3">大家好，欢迎来到我们关于DEPLAIN的演示，DEPLAIN是一个新的德语文本识别语料库，用于在文档级别和句子级别进行演示。我是Regina Stodden，我将引导您 walkthrough演示的第一部分。首先，我们来定义一下文本简化。文本简化是一种过程，通过适应文本以改善特定目标群体的理解能力，例如阅读困难的人或非母语者。为了训练文本简化模型，我们需要平行对的文本，例如文档或句子。这里有一个例子，你可以看到一个复杂德语句子和它的简化语言翻译的并行对齐句子对。为了简化句子，可以采用不同的技术，如词替换、子句删除、重新排序或插入单词。现在我们提出了一个新的语料库DEPLAIN，因为近年来存在一些问题。例如，这些语料库太小，无法训练文本简化模型。另外三个最近提出的模型都是自动对齐的，这意味着它们可能包含错误的对齐。因此，我们提出了一个新的语料库DEPLAIN，它分为两个子语料库：DEPLAIN-apa和DEPLAIN-web。DEPLAIN-apa基于新闻文本。在DEPLAIN-apa中，我们手动对483篇文档进行了对齐，产生了大约13,000个并行句子对。对于DEPLAIN-web，这个语料库包括不同的领域，我们手动和自动对齐了750篇文档。总共产生了30,450个句子对。我们对我们的句子对进行了一些分析，例如类型简化。正如你所见，圣经文本比新闻文本或语言学习者文本更加强化。在所有级别，例如词汇简化、结构简化和总体简化方面。此外，你可以看到DEPLAIN语料库具有高度的简化转换多样性。例如，在DEPLAIN-apa语料库中，我们有很多排序和单词添加，而在DEPLAIN-web语料库中，我们有很多改写。现在让我们看看我们可以用这个语料库做什么。你好，我是Omar，现在我将讨论DEPLAIN数据集的用途。第一个用途是评估自动对齐方法。近年来，已经提出了许多对齐方法，但在机器翻译的背景下，我们有两个不同语言的平行文档，我们想要提取两个文档中句子的对齐。但在我们的用例中，我们正在提取具有相同语言和相同内容但复杂度不同的两个平行文档之间的句子对齐。现在由于我们有DEPLAIN语料库，其中包含手动对齐的句子，我们可以使用这些句子作为黄金标准对齐来评估一些提出的对齐方法。我们对提出的对齐方法进行了适应，并在论文中出版了所有适应和运行实验的代码。最后，我们得出结论，对于德语文本简化，MASSalign方法是最优的自动对齐方法。你也可以在论文中找到运行这种方法的代码。第二个用途我们在论文中展示了一个案例，即通过微调语言模型生成简化文本。我们分别微调了两个模型，我们微调了长MBART模型以产生文档级别的简化，我们还微调了正常基mBART模型以产生句子级别的简化。你也可以在论文中找到所有检查点和详细实验结果。我们得出结论，这种基本微调可以产生或获得比 baseline 分数更好的分数，我们提出了这些结果作为未来自动文本简化问题的基本基准。谢谢大家的关注，希望你们能在大会上再次见面。谢谢。</sample>
    <sample id="4">演讲者的名字是Kayo Yin。</sample>
    <sample id="5">他们使用具有部分重叠背景知识的模型获得 82%-87% 的准确率。</sample>
    <sample id="6">Jiaan presented a joint work titled "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" with Fandong, Duo, Yunlong, Zhixu, Jianfeng, and Jie. The work aims to unify previous multilingual summarization and cross-lingual summarization into a more general setting called many-to-many summarization. This setting involves building one summarization model that can process a document in any source language and generate its summary in any target language.

The researchers conducted preliminary studies to analyze the differences among multilingual summarization, cross-lingual summarization, and many-to-many summarization. They found that many-to-many summarization could help the summarization model transfer task knowledge across different languages better than previous multilingual summarization and cross-lingual summarization.

To address this challenge, the researchers proposed a pre-trained many-to-many summarization model named PISCES. PISCES is trained through a three-stage pre-training process: meta pre-training, cross-lingual pre-training, and task-specific pre-training. The experimental results show that PISCES outperforms various baselines, including mBART-50 and mT5. The researchers also conducted ablation studies and human studies to verify the effectiveness of each training stage and demonstrate the superiority of PISCES.</sample>
    <sample id="7">是的，CoNLL-2003 标注器仍然有效。</sample>
    <sample id="8">ABC-Eval方法新颖之处在于它通过明确标注聊天模型的行为，如回应不相关的信息、自相矛盾或违反共同知识等，来减少对人类评估的主观性。这种方法旨在更精确和可靠地评估对话质量的多个方面。</sample>
    <sample id="9">根据英语内容，现有弱监督方法的成功在很大程度上依赖于干净的验证样本。</sample>
    <sample id="10">根据提供的信息，可以采取以下措施来提高分数：

1. 提供更多的背景知识：通过提供额外的背景信息，如实体的详细描述、图片或相关文章，可以帮助模型更好地理解实体，并更准确地识别用户所指的实体。
2. 改善模型的训练数据：通过使用更大、更多样化的数据集，可以训练模型以处理更广泛的实体和上下文，从而提高其性能。
3. 优化模型架构：开发更先进的模型架构或改进现有模型，以更好地捕捉实体之间的关系和上下文，可以提高模型在处理间接引用实体时的准确性。
4. 增强模型对实体的了解：通过将模型与关于实体的额外信息（如属性、艺术家或作者）进行关联，可以提高模型对实体的理解，从而提高其性能。</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presented "Do Androids Laugh at Electric Sheep? Humor 'Understanding' Benchmarks from The New Yorker Caption Contest" at ACL. This work is a collaboration with researchers from the University of Utah, Cornell University, University of Washington, Air Mail, and OpenAI.

Large language models can now generate and even explain jokes, as demonstrated by ChatGPT and Google's 540 billion-parameter PaLM. However, the question remains whether these models truly understand humor. Probing ChatGPT with knock-knock jokes involving absurd humor revealed that it often fails to recognize the humor in such jokes.

To investigate this further, the team turned to The New Yorker Caption Contest, which has been running since the mid-90s. They created three tasks: matching, quality ranking, and explanation generation. For the first task, they presented models with five choices of captions for a given cartoon, only one of which was correct. For the second task, they presented two captions judged by human raters to be of different quality. In the third task, they prompted language models to generate explanations for why a joke is funny.

The results showed that the best model, CLIP fine-tuned on the annotated corpus, achieved around 62% accuracy on the matching task, compared to a 20% random-guessing baseline. Humans scored around 94% on the same task. Even when conditioned to use human-authored descriptions of images, GPT-4 still performed significantly worse than humans on both the matching and quality ranking tasks.

In the explanation generation task, GPT-4 generated explanations for a cartoon captioned "He'll be back," but made several errors. Human evaluations preferred human-generated explanations over those produced by GPT-4 in more than two-thirds of cases.

The team is excited about the potential applications of their dataset and encourages others to explore it further.</sample>
    <sample id="12">根据Dawei的介绍，这篇论文有五位作者：Dawei、Xiaoyu Shen、Marius Mosbach、Andreas Stephan和Dietrich Klakow。</sample>
    <sample id="13">Daniel Rotem presented his work on adaptive inference in low-resource settings, which was conducted under Professor Roy Schwartz's supervision at the Hebrew University in Jerusalem. Adaptive inference is a method for reducing the inference time of large language models by using low-capacity models for easy samples. The two most common adaptive inference methods are Multi Model and Early Exit.

Multi Model involves storing multiple models, each trained separately on the entire training set. During inference, they are run sequentially until a classifier decides to halt the computation. This method is versatile and easily extensible but is expensive to store and suffers from overhead because using the last model for inference means running a sample through all previous models without using their output.

Early Exit, on the other hand, involves fitting multiple classifiers following intermediate transformer layers. They are all trained together, and during inference, a sample is run through the model until a classifier decides to halt, saving computation that would have been exhausted by the rest of the model. This method has faster inference and no overhead but can lead to lower performance due to conflicting gradients.

The researchers hypothesized that conflicting gradients occur when each classifier updates model weights trying to optimize its own goal, potentially harming the performance of all classifiers involved. To test this hypothesis, they compared individual Early Exit models' classifiers with separate Multi Model classifiers, which are truncated versions of the BERT pre-trained language model. The results showed that Multi Model classifiers outperformed those of Early Exit by an average of 2.3%, with the largest gap being 5.2% for the earliest classifiers.

To address the conflicting gradient problem, the researchers introduced SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method for Early Exit architectures. In SWEET, each layer receives updates only from the following classifier, avoiding the conflicting gradient problem. While SWEET closed most of the gap between Early Exit and Multi Model, it negatively affected later classifiers in some cases. However, SWEET outperformed both methods in fast speeds and throughout the entire speed-accuracy curve for BERT-Large.

The study highlights the existence of conflicting gradients in Early Exit training processes and introduces the SWEET method, which motivates future research and fine-tuning algorithms tailored to the Early Exit architecture.</sample>
    <sample id="14">大家好，我是亚当·普热皮尔科维，今天我要讲的主题是协调结构的依赖关系。正如你们所知，不同的理论和语料库方法假设了不同的依赖关系结构。例如，在通用依存分析中，协调结构如“Lisa，Bart和Maggie”中，第一个连词短语是整个协调结构的主干。因此在这种情况下，“Lisa”是主干。类似地，在伊戈尔·梅尔切克的意义文本理论中，整个协调结构同样由第一个连词短语领导。这两种方法都是不对称的，因为它们将其中一个连词短语突出。现在，这些是对称结构协调的两种方法，如图所示。在布拉格依存树库中采用的结合头部方法，其中协调结构由连词短语领导。因此，我们从主干到所有连词短语都有依赖关系。最后，还有一种多头方法，例如在哈德森的词 grammar 中使用，他们说所有连词短语都是协调结构的主干。因此，我们从治理者到每个连词短语都有独立的依赖关系：Lisa，Bart和Maggie。现在本文的目标是提出一个关于对称协调结构的新论据，如这两种，反对不对称协调结构，如这两种。好的，这个论据基于依赖长度最小化原则，我将通过这些示例来解释。在英语中，正如你们所知，直接宾语更喜欢靠近动词，而状语可以离动词更远。因此，“Marge读了它昨天”是可行的，因为直接宾语离动词很近，而“Marge昨天读了它”则更差。这是因为在这里，介词“昨天”位于动词和直接宾语之间。然而，这种效果可能会因为在直接宾语非常重和很长的情况下得到缓解。因为在这种情况下，可以直接宾语移动到介词之后。这在图中说明了这一点。因此，这两个句子都是可行的。“Marge读了这本书关于蜜蜂绝对迷人的昨天。”这是由于我们用这本书而不是它，使句子变得很长。但是，我们也可以这样写：“Marge昨天读了这本书关于蜜蜂绝对迷人的昨天。”所以推理是，这是可行的，因为尽管这个句子违反了直接宾语应该离动词很近的一般语法原则，但它满足了依赖长度最小化原则，即较短的依赖关系更可取。因此，这两个树只显示了关键依赖关系的长度，这些依赖关系在两种结构中并不一致。因此，这里有一个从“读”到介词的依赖关系长度为7个单词，从“读”到“书”的依赖关系长度为4个单词，总共为11个单词。当我们交换这两个成分时，这两个依赖关系的总长度变为6个单词。因此，与11相比，6更短。这就是为什么这句话听起来很好。它违反了一个原则，但满足了另一个原则。好的，所以我们在增强版的Penn树库中提取了各种统计信息，并见论文《为什么你不使用通用依存分析》中的统计信息确认了之前观察到的现象，即左连词短语往往更短。因此，“盐和胡椒”而不是“胡椒和盐”，以音节计。此外，之前在解析中所做的观察也得到证实，即这种趋势随着两个连词短语长度差的增长而增长。因此，当两个连词短语长度差异增大时，左边较短的连词短语更倾向于成为第一个；更强，对；比例更大；左边较短的连词短语。但是，当治理者在左边或不存在时，这种趋势就消失了。因此，我们通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对长度差以单词为单位，同样是在没有治理者的情况下观察到的，即在两个连词短语长度差异最大的情况下。但是，当治理者在右边时，这种趋势消失。我们在论文中展示了如何通过测量字符、音节和单词长度来观察这种趋势，分别在第一列、第二列和第三列中。因此，我们将集中精力观察第三列。我们看到，当治理者在左边时，左边较短的连词短语的趋势稳步增长，绝对</sample>
    <sample id="15">这篇论文有三位作者：Matthias Lindemann、Alexander Koller和Ivan Titov。</sample>
    <sample id="16">根据DEPLAIN语料库的分析，Bible文本的简化程度比新闻文本或语言学习者文本更大。在所有级别上，包括词汇简化、结构简化和整体简化方面，Bible文本的简化程度都更大。</sample>
    <sample id="17">The speaker, Shengqiong Wu, is a PhD student at NUS and is introducing their work on multimodal relation extraction. The task of relation extraction aims to determine the semantic relationship between entities in a given text. However, in realistic scenarios such as social media, data often comes in various forms and modalities rather than just pure text. This can lead to ambiguity or multi-context words that may lack sufficient context to understand.

To address this issue, multimodal relation extraction has been introduced, which adds additional visual sources to the textual RE. For example, with the support of visual evidence such as "Bachelor", "Gown", "Cap", we can easily infer the relationship between JFK and Harvard is "graduated at". However, there are still some problems. One problem is internal-information over-utilization, which occurs when inferring the relationship between two entities, only parts of the text are useful. Moreover, not all and always the visual sources play positive roles for the target task. Therefore, a fine-grained information pruning over two modalities is needed.

The other problem is external-information under-exploitation. This is because, although compensating the text inputs with visual sources, there can still be an information deficiency, especially when the visual features serve less or even negative utility. In this way, more external information should be considered, such as topic information. To address these two issues, the speaker proposes a Graph Information Bottleneck principle-guided feature refinement. And then they consider taking multimodal topic information as additional semantic supplementary to enrich the overall context.

The proposed method consists of five parts. Firstly, they represent the text and image with the corresponding visual scene graph and the textual scene graph. And then they consider merging the visual scene graph and the textual scene graph into one unified backbone cross-modal graph, named CMG. Next, they screen the initial CMG structures by fine-grainedly filtering nodes and adjusting edges in CMG. To ensure the above adjustments of CMG is correct, their graph information bottleneck is leveraged to guide optimization. Then they enrich the compressed CMG features with the multimodal topic features. They retrieve the associated top-L textual and visual topic keywords, and then an attention operation is used to integrate the multimodal topic words to enrich the overall context.

To evaluate the effectiveness of the proposed method, they conduct experiments on a widely used MRE dataset. As you see from the table, compared with the text-based method, leveraging visual features can obtain higher performances, and they see that all proposed methods can get the best performance among the multimodal baselines. In the ablation study, they find that the information screening and compensating both contribute to the task performances. And the scene graph is beneficial for structural modeling of the multimodal inputs. When removing the scene graph, the performances get decreased.

Next, they want to know, "Under what circumstances do the internal-information screening and external-information exploiting help?" So they group the instance by text region relevance scores and make the predictions for different groups. For the input with higher text-vision relevance, the GENE plays a greater role than LAMO, indicating the performance screening plays a more important role because most of the high cross-modal relevance input features come with rich yet even redundant information, where the internal-information screening is needed for denoising. For the inputs with lower text-vision relevance, LAMO contributes more significantly than GENE. That means the external-information exploiting is more useful.

In conclusion, they introduce a novel idea of simultaneous information subtraction and addition for multimodal relation extraction. They perform internal-information screening with the guidance of the graph information bottleneck principle. They devise a latent multimodal topic model, and induce latent multimodal topic features to enrich the feature contexts. Their overall system achieves significant improvements over the existing best models on the benchmarks. So thanks a lot. If you're interested in their work, you can scan the QR code for more detailed information. Thank you.</sample>
    <sample id="18">一个偏好的示例是"盐和胡椒"而不是"胡椒和盐"。</sample>
    <sample id="19">Hello everyone, my name is Zhang Qin from Shenzhen University. I'm excited to present our work, "A Survey for Efficient Open Domain Question Answering," which was accepted by ACL 2023. Our research focuses on open-domain question answering, and we've developed a two-stage model inspired by Danqi Chen's work in 2017. The first stage retrieves evidence contexts from a Wikipedia corpus using a question encoder and document encoder, while the second stage uses a reader to understand the question and retrieve the evidence to reason out the answer.

However, there are challenges in open-domain question answering, such as the large size of the Wikipedia corpus (26 million documents, 20 GB), the bottleneck of inference speed due to the 65 GB index file, and the use of multiple language models with millions of parameters. To address these issues, we aim to achieve efficient open-domain question answering systems with smaller memory costs, faster inference, and comparable performance.

To achieve these goals, we summarize core techniques, including approximate nearest neighbor search for fast evidence retrieval, skip reading for fast context understanding, and document filtering, embedding compression, or product quantization for reducing index size. We also discuss reducing model size through lightweight models, parameter sharing, or designing fewer models, such as using one model for both retrieval and reading.

Our analysis shows that retrieval and reader systems balance speed, memory, and performance well, while retrieval-only systems infer answers quickly but create large indexes, and generator-only systems achieve low performance but require no index. Based on this, we conclude that generator-only systems are suitable for resource-constrained devices, while retrieval and reader systems are more appropriate for trade-offs. Future works include deploying open-domain question answering systems on low-power devices and considering more evaluation metrics. Thank you for your attention.</sample>
    <sample id="20">是的，您可以使用这些模型进行您的研究。所有预训练模型都可以从Hugging Face上获得，并且在MIT许可证下提供。此外，所有训练脚本都在GitHub仓库中提供。</sample>
    <sample id="21">DEplain-apa 包含新闻文本。</sample>
    <sample id="22">良好的泛化需要三个主要因素：模型架构、模型大小和更多的微调示例。</sample>
    <sample id="23">The paper discusses the challenges faced by text-image models in rendering visual text accurately. The authors focus on the Imagen model, which uses a T5-XXL encoder to generate images from input text. They observe that even with complex inputs, the model often fails to render text correctly. The authors investigate the text encoder's ability to spell words and find that smaller T5 models struggle significantly, while larger models like PaLM perform better but are impractical for many applications. ByT5, which receives individual bytes of input, performs well at spelling across all scales. To improve text rendering, the authors propose concatenating an additional text representation from a ByT5-small model to the existing text representation in the Imagen model. This strategy enhances the model's ability to render text accurately, although it may still contain errors due to the diffusion model. The paper introduces WikiSpell as a benchmark for text-only models, DrawText as a benchmark for text-to-image models, and an efficient strategy for improving model spelling ability.</sample>
    <sample id="24">衡量左并列词是否更短的方法是通过测量长度。在该研究中，长度被测量为字符数、音节数和单词数。当主语位于左侧或不存在时，左侧并列词更倾向于更短。然而，当主语位于右侧时，这种趋势会消失。</sample>
    <sample id="25">为了研究支配词位置的影响，实验设计了从增强版的Penn树库中提取各种统计信息。这些统计信息包括协调结构的长度（以字符、音节和单词为单位），并观察了左 conjunct 的较短趋势。实验还检查了长度差异随长度增加而增长的趋势，并发现当支配词在左侧或不存在时，左 conjunct 的较短趋势会加剧。然而，当支配词在右侧时，这种趋势会消失。</sample>
    <sample id="26">基线分类器在不平衡数据上的训练效果不佳。由于标注数据中存在不平衡，基线分类器无法捕捉到认知不一致的类别。</sample>
    <sample id="27">根据您提供的信息，无法确定该论文的作者人数。您没有提供关于作者的具体细节。</sample>
    <sample id="28">示例对话中的角色名字是Bob和Alice。</sample>
    <sample id="29">根据 MuDA 标准的评估，语境感知 MT 模型在处理形式性和词汇 cohesion 等话语现象时比语境无关模型更有优势。然而，在处理省略、代词和动词形式等其他现象时，语境感知模型并没有明显优于语境无关模型。</sample>
    <sample id="30">The paper "LLM-Blender" introduces a simple yet effective ensemble learning framework for large language models. The key idea is based on pairwise ranking and generative fusion. The authors, from AI2 and USC, propose a two-stage framework named LLM-Blender to address the issue of selecting the optimal model for each input example. They argue that using a single top-performing model may not always be the best approach, as the optimal selection of models can vary across different input examples.

LLM-Blender runs n different models on a given input X and generates their outputs Y₁ to Yₙ. It then uses a pairwise ranking module named PairRanker to compare all these candidates and get a ranking of them. The PairRanker encodes a pair of candidates alongside the input X for better analyzing the subtle differences between these two candidates. This is different from prior methods that look at each candidate individually and score them separately. The PairRanker aggregates the results to get a final order of these candidates.

In the next stage, the top K candidates are picked, and they are used as the input to a sequence-to-sequence model for learning and inference in a generated fusion model. The fusion model outputs the final output for the input X by fusing the top three candidates ranked by the PairRanker.

The authors create a new dataset named MixInstruct, consisting of existing instruction datasets and candidates from 11 open-source large language models. They use BERTScore, BLUERT, and BARTScore as automatic metrics and ChatGPT as a judge to compare the results. Experiments show that the LLM-Blender framework outperforms the top two models, Open Assistant and Vicuna, in 68% and 76% of examples, respectively.

The paper concludes that LLM-Blender is a promising framework for ensemble learning, as it is simple and straightforward but significantly improves performance. The authors also release a unified codebase for evaluation and future research.</sample>
    <sample id="31">根据所提供的英文内容，无法确定论文作者所属的机构。</sample>
    <sample id="33">引入的框架通过使用皮尔逊相关系数比较注释和模型/数据集的预测标签和标签来量化立场。这使研究者能够评估注释与模型/数据集之间的对齐程度，从而识别出存在立场的领域。</sample>
    <sample id="34">Marcos Treviso is presenting a work called "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation" in this presentation. The work was done in collaboration with Alexis Ross, Nuno Guerreiro, and André Martins. The work proposes to combine selective rationalization and counterfactual text generation methods to leverage their complementary strengths.

The first component of CREST is responsible for generating counterfactuals. This is done by passing the input to a rationalizer model that has a trainable masker component that produces meaningful rationales. The rationale is then passed to a predictor module for a decision. To generate counterfactuals, only the rationale is used. The rationale is used to mask the original input alongside prepending the gold label to it. The masked inputs are then passed to an editor, which fills in the masked response with new tokens. The output is X-tilde, a counterfactual example.

To evaluate the quality of the counterfactuals produced by CREST, humans were asked to judge 100 examples from IMDB and SNLI in terms of their validity and naturalness using a 5-point Likert scale. Humans found manual counterfactuals to be more valid and natural than those generated by CREST and MiCE. However, among the automatic approach, CREST and MiCE, humans judged CREST counterfactuals to be more valid and natural than those produced by MiCE.

An interesting approach is to use counterfactuals for data augmentation. CREST can be used for data augmentation by producing two outputs in the end. One is the rational Z for the original input X, and the other is the counterfactual X-tilde alongside the demarcations for in-filled response Z-tilde. From here, two computation flows are created: a factual flow responsible for processing the original input and a counterfactual flow for the counterfactual input. Both inputs are passed through a shared rationalizer that learns to highlight meaningful rationales that are then passed through a predictor module that produces a final decision. Finally, a new regularization term is added that encourages the new rationales to be similar to those originally generated by CREST-Generation. This encourages the new rationalizer to focus on the specific parts of the input that encode factual and counterfactual reasoning.

Experiments were carried out by training on IMDB and testing on in-domain, contrastive, and out-of-domain datasets. CREST-Rationalization achieved the top result on IMDB itself. In the contrastive datasets, it performed on par with data augmentation using human counterfactuals. And finally, for out-of-domain data sets, it outperformed the other methods.

Overall, CREST produces valid, fluent, and diverse counterfactuals in a controllable way. By leveraging these counterfactuals during training, it leads to plausible explanations that focus on the contrasting parts of the input.</sample>
    <sample id="36">The paper "Learning Language-Specific Layers for Multilingual Machine Translation" by Telmo Pessoa Pires and colleagues proposes a method to increase the capacity per language in multilingual machine translation while keeping inference costs constant. The authors introduce Language-Specific Layers (LSLs), which consist of one regular transformer layer per language, used to select and train at inference time the correct sublayer. This approach allows for limited capacity per language but only where it matters the most.

The authors explore LSL placement and find that placing LSLs in the encoder yields better results than in the decoder. They use a model with shared, source, and target weights to learn the best placement of LSLs. The results show that the largest weight determines the placement of LSLs, leading to an architecture with shared layers at the bottom, source-specific LSLs in the middle, and target-specific LSLs at the top.

The authors evaluate their approach on WMT21 news translation mask sources for 10 languages, including some European, Asian, and low-resource languages. They report improvements over baseline models, language adapters, and learned architectures for every language, particularly for low-resource languages. The improvements are statistically significant for 84 out of the 90 translation directions.

Overall, the paper presents a promising method for improving multilingual machine translation performance while maintaining inference speed.</sample>
    <sample id="37">在之前的研究中，当人类受试者被给予相同的人格化提示时，他们能够揭示种族刻板印象。这使我们能够直接将生成的人物与人类手写回应进行比较。</sample>
    <sample id="38">该研究使用了增强版的Penn树库和论文“为什么你不使用通用依存性”中的统计数据。</sample>
    <sample id="39">根据所提供的内容，无法确定论文的作者人数。</sample>
    <sample id="40">与认知失调密切相关的任务包括主题独立的认知失调立场分类和 PDTB 扩展和比较类别的二元分类。这些任务与认知失调的观念密切相关，因为它们涉及确定两个观点是否一致或不一致，以及识别扩展和比较类别的关系。</sample>
    <sample id="41">The research presented by Silin and the Natural Language Processing Lab at EPFL University, in collaboration with Sony Group Corporation, introduces a Persona Commonsense Knowledge Graph (PeaCoK) designed to enhance narrative consistency and engagement. The study addresses the challenge of understanding real-world personas and their complex interconnections, which is crucial for sustaining coherent narratives.

PeaCoK is a comprehensive knowledge graph that includes about 3,800 personas and 40,000 distinctive attributes, forming approximately 100,000 personal inferences or facts. It also features around 9,200 attributes connected to two or more personas, providing rich interconnections. The graph is built through three steps: selecting personas from existing commonsense graphs, inducing attributes from both commonsense knowledge graphs and large-scale pre-trained language models, and crowdsourcing annotations using a joint human-AI majority voting scheme.

To evaluate PeaCoK's effectiveness, the researchers trained a BART-based common knowledge generator on a persona attribute inference task. Compared to large-scale pre-trained language models like GPT-3, Comet-BART trained on PeaCoK achieved better automatic evaluation results and higher acceptance rates in human evaluations. This indicates that PeaCoK can serve as a reliable persona knowledge base, enabling lightweight language models to learn knowledge generation capabilities comparable to larger models.

The study also explored the use of PeaCoK in downstream narrative modeling, specifically in persona-grounded dialogue generation. By retrieving relevant facts from PeaCoK and augmenting each speaker's profile, the PeaCoK-augmented models demonstrated improved dialogue generation in terms of fluency, consistency, engagement, and persona expression. Human evaluations showed that the impact of PeaCoK's persona-centric commonsense knowledge was more positive compared to general social commonsense knowledge. Additionally, the winning rates of the PeaCoK-augmented model increased as the number of shared common attributes between speakers grew, highlighting the importance of interconnected world persona knowledge in narratives.

Overall, the research demonstrates that PeaCoK can significantly enhance narrative consistency and engagement by providing a reliable and comprehensive persona knowledge base. The paper and GitHub site for this work are publicly available on the lab's website.</sample>
    <sample id="42">根据所提供的英文内容，无法确定论文的作者人数。</sample>
    <sample id="43">根据提供的信息，无法确定论文的作者人数。在提供的文本中没有提到作者的名字或数量。</sample>
    <sample id="44">引入的框架与以前的研究不同之处在于，它通过比较真实用户与现有数据集和模型的注释，来研究数据集和模型的位置性。这与以往仅关注注释者内部一致性或建模注释者分布的研究不同。</sample>
    <sample id="45">在三个比较设置中，与刻板词汇的重叠最多的是黑人女性。</sample>
    <sample id="46">在演讲中，作者比较了DeepL和Google Translate这两个商业系统。他们使用MuDA基准测试评估这些系统，并发现DeepL通常比Google Translate更准确地进行文档级翻译。</sample>
    <sample id="47">今天我代表我们团队展示了我们的研究工作“从预训练数据到语言模型再到下游任务：追踪政治偏见的轨迹”。语言模型是在大规模网络爬虫数据上进行训练的。政治新闻媒体在预训练数据中得到了很好的覆盖。根据C4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《 Huffington Post》等媒体在语言模型的训练数据中得到了很好的覆盖。这为语言模型的应用带来了双重好处。一方面，它们能够学习到多样化的观点，庆祝民主和多元思想。另一方面，这些不同政治观点内在地具有社会偏见，可能会导致下游任务应用中的潜在公平性问题。因此，我们提出了一个研究框架，以追踪政治偏见从预训练数据到语言模型再到下游任务的传播路径。我们提出的问题包括：首先，我们如何评估语言模型的政治倾向，以及预训练数据可能在其中扮演的角色？其次，具有不同政治倾向的语言模型在下游任务上的表现如何，是否会导致NLP应用中的公平性问题？我们首先提出了一个方法，通过使用政治问题表（如政治大会测试）来提示语言模型，以确保评估结果符合政治科学文献。初步结果表明，语言模型具有不同的政治倾向，它们占据了政治光谱的所有四个象限。GPT-4是所有模型中最倾向于自由派的，而GPT系列通常比BART系列及其变体更倾向于自由派。我们还旨在研究语言模型的政治偏见在多大程度上是从训练数据中吸收的。我们可以通过在6个不同的 partisan语料库上进一步预训练语言模型的检查点，来进一步研究这个问题。这些 partisan 语料库被分为新闻和社交媒体，并进一步分为其政治倾向。通过在这些 partisan 语料库上进一步预训练语言模型，我们可以看到语言模型的政治倾向也会相应地发生转变。例如，对于进一步训练在左翼Reddit语料库上的RoBERTa，我们可以观察到其政治倾向发生了显著的自由派转变。我们还试图研究语言模型是否能吸收我们现代社会中存在的 polarization。我们将预训练语料库分为特朗普总统之前和之后的两个时间段。分别在两个不同时间点上预训练语言模型，我们可以看到语言模型的政治倾向在2017年之后更加远离中间派。这表明语言模型也能吸收社会 polarization。最后，我们评估了具有不同政治倾向的语言模型在 hate speech detection 和 fake news detection 等NLP应用中的性能，这些应用通常涉及语言模型，并可能对社会产生重大影响。如果我们按类别评估性能，即按不同 demographics 或政治倾向的新闻媒体分开性能，我们可以看到一种模式。例如，在 hate speech detection 中，左翼语言模型在检测针对少数群体的 hate speech方面表现更好，但在检测针对更强大群体的 hate speech方面表现较差。反之亦然，右翼语言模型在检测针对白人和男性群体的 hate speech方面表现更好，但在检测针对黑人 LGBTQ+和其他 minority communities 的 hate speech方面表现较差。类似的趋势也发生在 fake news detection 中，左翼语言模型在检测来自相反政治倾向的 misinformation方面表现更好，反之亦然。我们还提供了许多 qualitative 示例，以进一步说明不同政治倾向的语言模型给出不同预测 hate speech 和 misinformation 示例的情况。这表明，不同政治倾向的语言模型之间存在公平性问题。例如，如果右翼语言模型被微调用于 hate speech 或 misinformation 等任务，并部署到热门社交媒体平台上，这将意味着具有相反政治观点的人可能会被边缘化，针对少数群体的 hate speech 可能会 rampant 地蔓延而无法控制。因此，我们需要承认并解决由语言模型政治倾向引起的社会公平性问题。最后，我们还强调了语言模型政治偏见的独特 dilemma。它就像斯库拉和查尔布迪斯之间的 dilemma。如果我们不净化语言模型训练数据中的政治观点，偏见将从预训练数据传播到语言模型再到下游任务，最终导致公平性问题。如果我们尝试 somehow 洁净数据，我们也会面临审查或排除的风险。确定什么应该被保留或排除在语言监测数据之外是非常困难的。这就像电车难题一样。</sample>
    <sample id="48">根据所提供的英文内容，无法确定论文的作者人数。</sample>
    <sample id="49">MPP评估最多涵盖了1024个词元的上下文长度。</sample>
    <sample id="50">DEPLAIN是一个新的德语文本识别语料库，用于在文档级别和句子级别进行文本简化。文本简化是一种过程，通过适应文本以改善特定目标群体（如阅读困难者或非母语者）的理解能力。为了训练文本简化模型，我们需要平行对的文本，例如文档或句子。DEPLAIN语料库分为两个子语料库：DEPLAIN-apa和DEPLAIN-web。DEPLAIN-apa基于新闻文本，包含483篇手动对齐的文档，产生大约13,000个平行句子对。DEPLAIN-web包括不同领域，包含750篇手动和自动对齐的文档，总共产生30,450个句子对。DEPLAIN语料库具有高多样性的简化变换，例如在DEPLAIN-apa语料库中，重排和插入词比DEPLAIN-web语料库中更多。DEPLAIN语料库可用于评估自动对齐方法和自动文本简化。我们已经使用DEPLAIN语料库评估了自动对齐方法，并发现MASSalign是德国文本简化最佳的自动对齐方法。我们还使用DEPLAIN语料库对长MBART和普通基mBART模型进行了微调，以生成文档级和句子级简化文本。我们的实验结果表明，这种微调可以产生优于 baseline 分数的分数，并将其作为未来自动文本简化问题的基本基准。</sample>
    <sample id="51">他们的数据集中包含音乐、书籍和食谱三个领域。</sample>
    <sample id="52">Positionality，或立场，指的是人们因 demographics、身份和生活经历而持有的观点。这个概念在批判研究中广泛使用，特别是在女性主义和 queer 学术领域。</sample>
    <sample id="53">演讲者的名字是Dawei。</sample>
    <sample id="54">Vasudha, a PhD candidate in Computer Science at Stony Brook University, presented their work on "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" at ACL 2023. The paper focuses on cognitive dissonance, which occurs when two beliefs or actions are inconsistent. Studying dissonance expressed in language can help understand disagreement, track trends and belief values, and understand mental health issues such as anxiety disorders. The authors conducted a large-scale annotation of dissonance relations using a dissonance-first approach, but found that dissonance was only present in 3.5% of the annotated pairs. To address the low occurrence of dissonance, they experimented with transfer learning and active learning to collect more dissonant samples while reducing annotation costs. They transferred weights from closely related tasks such as topic-independent dissonance stance classification and binary classification of expansion and comparison classes of PDTB. The authors found that the cumulative update strategy performed better than iterative update for domain active annotations. They also used a Probability-of-Rare-Class (PRC) strategy to select examples that were highly likely to be dissonant. The PRC strategy worked better than other state-of-the-art strategies, although the difference was small. Overall, the authors found that PRC is a simple AL strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task.</sample>
    <sample id="55">是的，EDAtt 适应了现有的离线 ST 模型。它使用已有的离线 ST 模型，而无需重新训练或采用特定架构进行同时语音翻译。</sample>
    <sample id="56">根据提供的信息，无法确定论文的作者人数。</sample>
    <sample id="57">根据英语内容，被测模型不能在测试套件上运行。当没有任务特定训练时，两个模型的表现都不好。然而，在KITMUS上进行任务特定训练后，两个C2F和BERT4Coref模型的表现显著提高。这表明，当训练任务特定数据集时，大多数模型学会利用表面线索，这些线索在测试套件中已被删除。</sample>
    <sample id="58">KITMUS 有三个变体：Background-Pretrain、Background-Both 和 Background-Inference。</sample>
    <sample id="59">The presentation by Yanis Labrak introduces DrBERT, a robust pre-trained model in French for biomedical and clinical domains. The speaker begins by discussing the importance of language modeling in healthcare and then presents the main contribution of their article, which is the introduction of DrBERT. DrBERT is based on RoBERTa and trained on NACHOS, a data set of medical crawled data from the web. The presentation also compares DrBERT with other models that have been pre-trained using different settings and data sources.

The speaker then presents the results of DrBERT on 11 biomedical and clinical downstream tasks in French. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, it was observed that data from heterogeneous sources appear to be more versatile. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. The presentation concludes with a comparison of seven models, including DrBERT, and the availability of all pre-trained models obtained from NACHOS on Hugging Face under the MIT license.</sample>
    <sample id="60">论文的作者没有提供他们所属机构的信息。</sample>
    <sample id="61">最后一个研究问题是否应该只使用干净的样本进行验证，或者是否有更好的方法来利用它们。</sample>
    <sample id="62">本文研究了自然语言生成（NLG）系统压缩的潜力，重点是任务特定知识蒸馏。NLG系统通常基于大型语言模型，但随着模型变得更大、更复杂和更慢，它们的成本也更高。因此，行业对压缩这些模型的需求日益增长，同时保持其性能。本文旨在探索NLG压缩的可能性，即找到一种方法来压缩模型，同时保持其性能。

本文提出了几种NLG压缩方法，包括使用较小的模型或通过修剪完整层来压缩模型。修剪后，将知识从大型教师模型转移到小型学生模型，即知识蒸馏。在NLG中，有两种主要类型的噪声蒸馏：单词级蒸馏和序列级蒸馏。单词级蒸馏通过最小化学生和教师模型的logits之间的KL散度来训练学生。序列级蒸馏使用教师模型生成伪目标，然后将这些伪目标添加到训练数据中，并重新训练学生。

本文的研究重点是任务特定知识蒸馏，而不是像分类任务、自然语言理解任务或预训练任务诊断等任务。此外，本文还研究了NLG任务，如摘要、问题生成、共同知识推理、简化和风格转换。本文还提出了一个名为“联合教学”的新知识蒸馏技术，以解决学生曝光偏见、有偏学习和纠正错误的问题。该技术在教师和学生生成的伪目标上应用单词级知识蒸馏。

总之，本文的研究结果表明，通过使用多种伪目标、采样伪目标和高温度，可以提高学生模型的性能。此外，本文提出的“联合教学”技术可以解决学生曝光偏见、有偏学习和纠正错误的问题。</sample>
    <sample id="63">灵敏度是一个额外的评估指标，用于衡量模型在给定任务上产生相同输出的能力，无论指令的 wording 如何轻微变化。</sample>
    <sample id="64">演讲者的名字是 Jingwei Yi。</sample>
    <sample id="65">更高的灵敏度表示模型性能得到了提高。</sample>
    <sample id="66">The paper "Deep Learning for Mathematical Reasoning" explores the development of machines capable of solving math problems and proving theorems, a long-standing focus of AI and NLP. The authors discuss two primary categories of mathematical reasoning: visual contexts (geometric problems) and tabular contexts (problems involving tables). They highlight the importance of automated theorem proving and the challenges faced by language models in performing precise mathematical reasoning. To address these limitations, they propose using pre-trained language models, such as LLMs, and augmenting them with tools like program-aided LMMs and natural language programs. The paper also mentions the need for more datasets in low-resource settings and benchmarks for various domains to improve generalization and robustness in mathematical reasoning tasks. Overall, the paper provides a comprehensive survey of the current state of deep learning in mathematical reasoning and suggests potential solutions to improve the performance of language models in this area.</sample>
    <sample id="67">The paper discusses the issue of interference in multilingual translation models and proposes methods to mitigate it. The authors identify that severe interference occurs when the model is small compared to the data size, and tuning the sampling temperature is key for strong performance. They also find that language similarity and the number of languages do not have a large impact on interference levels. The authors experiment with different model and focus data sizes, and find that severe interference occurs only for the smallest models and goes away with increasing scale. They propose using temperature sampling as a simple solution to control the trade-offs, and find that tuned temperature is key for strong performance. Overall, the paper suggests that modest scale and tuned temperature can reduce the problem of interference significantly without any other specialized method.</sample>
    <sample id="68">在预训练期间，模型会接收各种长度和来源的语言上下文。这包括来自BLiMP、SyntaxGym等数据集的句子，以及来自不同领域如Wikipedia的句子。这种多样性旨在评估模型对不同背景和数据集的接受性判断能力。</sample>
    <sample id="69">通常需要每个类别20个干净的验证样本才能获得良好的表现。</sample>
    <sample id="70">根据所提供的英文内容，无法确定论文作者的机构。</sample>
    <sample id="71">Our work focuses on resolving indirect referring expressions for entity selection in conversational systems. We introduce the AltEntities Corpus, a large-scale public data set that covers three domains: music, books, and recipes. Our data set collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles: Bob sets the dialogue context, Alice asks an alternative question, and Bob uses an indirect reference to select one of the entities. Annotators are provided with background knowledge about the entities, but not necessarily their names. They pick one entity and describe it using three to five indirect referring expressions. The AltEntities Corpus contains 6,000 alternative questions across three domains, and it has 42,000 indirect referring expressions. Results with T5 XL model show that if the language model has access to the exact same background knowledge as the annotators, accuracy is around 92 to 95%. However, if the language model has access to only partially overlapping background knowledge, accuracy drops to between 82 to 87%. When the language model has access only to entity names, accuracy is only 60%. Despite this, our models are domain-generalizable.</sample>
    <sample id="72">需要开发新的方法来衡量媒体偏见，因为现有的方法可能无法准确评估语言模型的偏见。此外，由于政治偏见在训练数据中很普遍，因此需要开发新的方法来识别和量化这些偏见，以确保NLP应用程序的公平性。</sample>
    <sample id="73">演讲者的名字是Akshatha。</sample>
    <sample id="74">The paper "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths" by Xiangqing and co-authors introduces a new approach to enhancing the existing ATOMIC knowledge base, which is a large-scale commonsense knowledge base covering event-centered social aspects of inferential knowledge tuples. The authors address the limitations of ATOMIC, such as sparse graph structure and insufficient semantic information, by proposing Dense-ATOMIC, which completes missing links including B-to-A, B-to-B, A-to-B, and A-to-A links.

The construction of Dense-ATOMIC involves three main parts: normalizing tail events, training a relation prediction model, and constructing Dense-ATOMIC. Normalizing tail events converts tail events into the same format as head events through subject removal, third-person singular form conjugation, subject recovery, and relation grouping. To overcome the limitations of traditional methods, the authors propose Rel-CSKGC, which predicts relations given head and tail events using RoBERTa for encoding and MaxPooling for link prediction. This method avoids the sparsity problem and utilizes semantic information effectively.

The evaluation of Rel-CSKGC shows that it outperforms relation prediction methods and translation-based methods on both automatic and human evaluations. Additionally, Dense-ATOMIC yields higher knowledge coverage and benefits the performance of COMET, generating more diversified results. The paper also evaluates multi-hop paths in Dense-ATOMIC, showing relatively high aggregates and better results with heuristic rules.

Overall, the paper demonstrates the potential of Dense-ATOMIC in enhancing knowledge coverage and multi-hop paths, providing a valuable resource for commonsense reasoning.</sample>
    <sample id="75">Zheng Yandan, Hao Anran, and Luu Anh Tuan have presented their work on Jointprop, a joint semi-supervised learning framework for name entity recognition (NER) and relation extraction (RE). The motivation behind their work is to address the limitations of fully-supervised models that require extensive labor for high-quality data annotation and diverse annotated data for various domains and applications. They propose a joint semi-supervised learning framework that integrates all the connections among labeled and unlabeled data by propagating labels over heterogeneous graphs.

The Jointprop framework consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. For span feature generation, they initialize span and span pairs representation as H-e and H-r, and generate unlabeled span and span pair representations using a trained classifier. For heterogeneous graph construction, they construct a k Nearest Neighbor graph for computational efficiency and examine the similarity relations among pairs of unlabeled data as well as the similarity relations between labeled data. For joint label propagation, they propagate labels to entity or relation candidates in unlabeled data through the heterogeneous graph. Finally, they optimize the model by obtaining converged pseudo-labels and retraining the classification model with the rest of the confidence above the threshold combined with the label data.

Their experiment results show that the joint learning of two tasks benefits from the codependency between the two tasks in joint datasets. For single-task datasets, their framework shows significant and consistent improvement over all baselines, both for NER and relation tasks.</sample>
    <sample id="76">政治偏见传播流程包括从预训练数据到语言模型再到下游任务的过程。首先，预训练数据可能包含政治偏见，这些偏见会影响语言模型的训练。然后，这些偏见会传播到下游任务中，导致在NLP应用程序中出现公平性问题。例如，在 hate speech 检测和 fake news 检测等任务中，不同政治倾向的语言模型可能会对不同社会群体的 hate speech 和 misinformation 的检测产生不同的结果。</sample>
    <sample id="77">The video introduces a joint work from Yale University and Microsoft Research, titled "On Improving Summarization Factual Consistency from Natural Language Feedback." The researchers propose three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. They focus on the task of abstractive text summarization and study the factual consistency of summarization models.

The team created a new dataset called DeFacto, which contains human demonstrations and feedback for improving summarization factual consistency. Annotators were asked to provide labels to decide whether the summary was factually consistent and to provide human-corrected, factually consistent summaries if they thought the original summary was incorrect. They also provided explanations for their decisions and evidence supporting their claims.

The data was collected on the XSum dataset, with initial system outputs from the pre-trained Pegasus model. The results showed that around 2.5K data points were collected, with 70% containing factual errors. Human-edited summaries received higher automatic factuality scores compared to the initial system output, but there was a lower textual overlap between reference summaries and human-edited summaries due to the presence of factual errors in the reference summaries.

The video also shows the distribution of annotated editing instructions and their relation with different error types. The team found that both fine-tuned models and zero-shot large language models can effectively leverage human feedback for summary editing. However, generating feedback for the editing model remains a challenging task for both fine-tuned models and large language models. The editor model achieved comparable performance to baseline models while being trained on fewer data, and training the model to generate explanations helped it achieve better performance.

Overall, the DeFacto dataset provides a valuable test bed for the proposed NLG tasks and has other advantages such as fine-grained annotations, which can be useful for training factuality metrics and factuality meta-evaluation. The dataset is available on GitHub, and more details can be found in the paper.</sample>
    <sample id="78">是的，DEPLAIN-apa和网站的简化过程有所不同。DEPLAIN-apa包括483篇新闻文本的手动对齐，产生大约13,000个平行句子对。相比之下，DEPLAIN-web包括不同领域的750篇文档，这些文档通过手动和自动对齐方法进行对齐。在DEPLAIN-apa中，我们看到更多排序和单词添加，而在DEPLAIN-web中，我们看到更多重述。此外，我们还分析了句子对的类型，发现圣经文本比新闻文本或语言学习者文本更加强化。</sample>
    <sample id="79">是的，CoScript 已经公开可用。</sample>
    <sample id="80">在水印插入到文本中时，首先选择一个触发集，即一组在适度频率间隔内的单词。然后定义目标嵌入。当用户向提供商服务发送句子时，提供商计算句子中的触发器数量。所提供的嵌入是目标嵌入和原始嵌入的权重总和。目标嵌入的权重与句子中的触发器数量成正比。当句子中的触发器数量大于m时，所提供的嵌入等于目标嵌入。</sample>
    <sample id="81">该论文的作者来自宾夕法尼亚州立大学。</sample>
    <sample id="82">This video introduces a new framework for unsupervised automated essay scoring (AES) called ULRA, which aims to score the quality of essays without human intervention. The traditional approach to AES involves supervised training with labeled corpora, but this process is time-consuming and labor-intensive. Unsupervised AES can overcome these challenges by not requiring ground-truth scores for training.

The video highlights two existing works on unsupervised AES: Chen et al. (2010), which uses the number of unique terms as an initial score and iteratively propagates it, and Zhang and Litman (2021), which uses word count as a weak supervision signal. However, both methods lead to poor performance due to the limitations of using a single quality signal.

To address this issue, ULRA proposes introducing multiple heuristic quality signals as pseudo-groundtruth and training a neural AES model by learning from their aggregation. The core idea is to use a heuristic essay ranking module (HER) to generate partial-order pairs based on different quality signals, and then aggregate these pairs into a unified supervision using a deep pairwise rank aggregation module (DPRA). A learnable confidence weight is designed for each signal to measure its importance.

In the model inference stage, a scoring strategy is proposed to transform the predicted scores into the range of pre-defined score sets through a minimum-maximum transformation. Experiments demonstrate that ULRA outperforms all unsupervised baselines with a large improvement and achieves competitive performance compared to cross-prompt and one-shot methods. Despite still being lower than supervised methods, ULRA's performance is much higher than other unsupervised approaches.</sample>
    <sample id="83">是的，像 mt5 这样的编码器-解码器模型可以通过混合语言的训练来改进。在 XSemPLR 研究中发现，编码器-解码器或编码器-指针模型可以通过在各种语言的混合中进行训练来提高性能。然而，需要注意的是，这种改进并非适用于所有语言，例如，在七种情况下英语性能下降，在三种情况下英语性能略有提高。这种现象被称为“多语言 curse”。</sample>
    <sample id="84">In this paper, the author discusses the concept of dynamic networks and their limitations. Traditional networks are static, meaning they have fixed parameters that do not change with input. Dynamic networks, on the other hand, can adjust their architecture or parameters based on input, which can lead to better performance. However, fully dynamic networks often contain redundant parameters, leading to increased model size and computational costs.

To address this issue, the author proposes PAD-Net (Partially Dynamic Network), a framework that partitions parameters into dynamic and static modes. The goal is to make redundant dynamic parameters static, reducing the model size and computational requirements while maintaining performance. The author uses an iterative mode partition method to achieve this.

The results show that PAD-Net outperforms both static and fully dynamic networks in terms of performance and efficiency. The author also conducts ablation studies to determine the optimal dynamic ratios for different components of the network, such as dynamic convolution and mixture of experts. Additionally, the author compares PAD-Net with network pruning and finds that it performs better due to the maintenance of static parameters.

The author suggests future work in extending the method to other mainstream networks, hardware-friendly structured manners, and introducing more modes such as zero elements, static parameters, and dynamic parameters.</sample>
    <sample id="85">受限语言规划的一个示例是“制作巧克力蛋糕”。</sample>
    <sample id="86">为了确保其方法的隐蔽性，作者通过以下方式确保其方法的隐蔽性：首先，他们选择一个触发集，即在中等频率间隔的一组单词。然后，在水印注入过程中，他们定义了一个目标嵌入。当用户向提供商服务发送句子时，提供商计算句子中的触发词数量。所提供的嵌入是原始嵌入和目标嵌入的权重求和。目标嵌入的权重与句子中的触发词数量成正比。当句子中的触发词数量大于m时，所提供的嵌入等于目标嵌入。最后，在版权验证过程中，他们构造一个后门数据集和一个 benign 数据集。后门数据集包含所有单词都属于触发集的句子，而 benign 数据集中的所有单词都不属于触发集。然后，提供商要求从窃取者的服务中获取嵌入。计算请求的嵌入与目标嵌入之间的余弦相似性和L2相似性。同时，他们还应用KS测试并使用其p值作为第三个指标。实验结果表明，他们的嵌入标记可以在保持下游任务的实用性的同时，具有很好的检测性能。</sample>
    <sample id="87">该研究探讨了如何使用现有的预训练语言模型（PLM）来构建新的PLM。他们通过比较不同预训练策略和数据源的性能来评估PLM的性能。他们发现，从头开始预训练通常会获得更好的性能，但使用混合数据来源的模型也可以获得类似的性能。此外，他们还观察到，更专业化的数据通常比混合数据更有效，但不会大规模扩展。</sample>
    <sample id="88">根据Jenny的演讲，GPT-4与非二元性别群体的立场最不一致。</sample>
    <sample id="89">演讲者展示了模型如何利用注意力机制所学的知识，以翻译德语句子"I'm going to talk about..."。在该示例中，模型预测了翻译，并检查了交叉注意力权重。第一两个词指向最早收到的语音帧，而最后一个词指向最后收到的语音帧。由于最后两个词的交叉注意力总和低于阈值α，因此不会发出最后一个词，而是等待另一个语音块。</sample>
    <sample id="90">This paper explores the feasibility of using language learners as annotators in NLP data annotation. The authors conducted a proof-of-concept study to examine the accuracy and learning effect of language learners' annotations compared to native speakers. They targeted three languages: English, Korean, and Indonesian, and chose four tasks from the GLUE benchmark: sentiment analysis, NLI, NER, and MRC. Learners were categorized into three levels based on their language proficiency, and native speakers were recruited for comparison. The experiments involved a pre-test, annotation, and post-test, with participants solving standardized test questions and word meaning questions before and after annotation. The results showed that labels annotated by language learners were nearly accurate, especially for simpler tasks and easy-to-medium level questions. Aggregating labels from multiple language learners by majority voting brought them almost on par with native speakers. Additionally, training simulations demonstrated that language models trained on learners' less accurate labels achieved about 95% of ground truth performance and sometimes outperformed models trained with native speakers' labels. Learners' language proficiency and vocabulary and grammar improved as they carried out the annotation tasks. This paper suggests a novel way for data construction by recruiting language learners as annotators, broadening NLP research for many languages and overcoming geographic and technological barriers to building benchmark datasets for low-resource languages.</sample>
    <sample id="91">任务的数量对模型性能有显著影响。随着任务数量的增加，模型在测试时表现出更好的性能，并且敏感性降低。使用更多指令可以进一步提高模型的整体性能并减少敏感性。</sample>
    <sample id="92">作者在论文中比较了他们的方法与三个无树基线：1.使用注意力机制的神经序列到序列模型；2.使用门控循环单元（GRU）的神经序列到序列模型；3.使用长短期记忆（LSTM）的神经序列到序列模型。这些基线被用来评估作者的方法在处理结构化任务时的性能，特别是对于更复杂的输入输出对。</sample>
    <sample id="93">两位合著者，Alexander Koller和Ivan Titov，与第一作者Matthias Lindemann的关系是导师关系。他们指导了Lindemann在研究“Compositional Generalization without Trees using Multiset Tagging and Latent Permutations”方面的论文。</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China is presenting a short advertisement video for their paper on protecting the copyright of large language models for embedding as services via backdoor watermark. The video introduces the background of embedding as services, where large language models like GPT, LLAMA, and PALM are used to assist various NLP tasks. OpenAI offers a GPT-based embedding API, but recent works have shown that attackers can steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services.

The video explains that one solution to protect the copyright of embedding as services is to embed a watermark in the provider service and detect whether another service contains the watermark. The watermark method needs to meet four properties: applicability to embedding as services, not degrading the utility of the provided embeddings, being covert enough to the attacker or easily removable by the attacker, and being transferable to the attacker's services during the model extraction process.

Existing works can be broadly classified into four categories, but this method either lacks applicability to embedding as services or lacks transferability. Therefore, the paper proposes Embedding Marker, a backdoor-based watermark method applicable to embedding as services. The video introduces the details of Embedding Marker, which contains two main steps: watermark injection and copyright verification.

Before these main steps, a trigger set is selected, which is a group of words in a moderate frequency interval. In watermark injection, a target embedding is defined, and when a user sends a sentence to the provider service, the provider counts the number of triggers in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding, with the weight of the target embedding proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than m, the provided embedding is exactly equal to the target embedding. Copyright verification involves detecting whether a model behind another service contains the word mark by constructing a back door and a benign data set, computing the cosine and L2 similarity between the requested embedding and the target embedding, and applying the KS test to compute the p-value as a third metric.

The video concludes by mentioning that experiments were conducted on four data sets (AG News, MIND, SST2, and Enron Spam) assuming the provider applied Wiki text data set to count word frequency. The results showed that Embedding Marker can have great detection performance while keeping great utility for downstream tasks. The covertness of the provided embedding was validated by visualizing the embedding of sentences on four datasets using PCA, showing that it's hard to distinguish between backdoor embeddings and normal embeddings.</sample>
    <sample id="95">PaLM 的第一作者是 David Vilar。</sample>
    <sample id="96">大家好，我是金妮，是一名在卡内基梅隆大学读研的第一年博士生。今天我将向大家介绍我们团队的工作NLPositionality，这项工作是在与来自University of Washington和Allen Institute for AI的几位同事合作完成的，他们分别是Sebastian Santy、Ronan Le Bras、Katharina Reinecke和Maarten Sap。我们将从一个假设开始：你正在为报纸筛选新闻文章下的评论，试图删除有毒内容。你可能会转向Prospective API这样的热门API进行有毒内容检测，而这个API对Carl Jones来说效果很好，因为它能正确检测有毒实例。但对于Aditya Sharma来说，Prospective API并没有那么敏感，无法检测到印度语境中更常见的 offensive 术语。这就是设计偏见的一个例子，我们看到技术在不同群体之间的系统性能差异。设计偏见可能由于NLP研究员和模型开发者的立场而产生。立场是指人们因 demographics、身份和生活经历而持有的观点。这个概念广泛应用于批判研究，特别是在女性主义和 queer 学术领域。作为研究员，立场会影响研究过程及其结果，因为它会改变研究员所做的决策。因此，人们可能会问：数据集和模型是否有立场？我们并不是说数据集和数据集本身具有人口特征和生活经历，但它们确实聚合了真实人们的判断和意见，并因此可以代表某些立场而忽视其他立场。以往的研究已经提出了数据集和模型中存在立场的 anecdotal 证据，例如文化差距和模型和数据集中的理论定义的模型立场。然而，这些研究并没有将现有用户与数据集和模型进行比较，而研究数据集和模型的立场变得越来越重要，因为NLP任务变得更加主观和社会化，而且很难描述这些立场是如何失真的，因为并非所有决策都记录在案，许多模型都隐藏在API后面。为了研究数据集和模型的立场，我们实际上将现有用户与现有数据集和模型的注释进行了比较。我们通过NLPositionality框架来实现这一目标。该框架分为两个主要步骤。首先，重新注释数据集以获得多样化的注释者。我们选择这样做是因为原始数据集的注释者通常只有几个人，而且很少收集和分享 demographics 数据。所以我们选择重新注释数据集，以获得每个实例的许多注释和丰富的 demographics 数据。然后，我们将 demographic 的注释与数据集和模型的注释进行比较，使用皮尔逊相关系数。因此，我们的框架与注释者分歧文献不同，后者只关注注释者之间的分歧，而我们则比较最终用户与模型和数据集的预测和标签。我们的框架主要通过Lab in the Wild和在线 crowdsourcing 平台来实现，这是一个 HCI 合作者常用的在线实验平台。相比像MTurk这样的平台，Lab in the Wild可以吸引全球更多样化的参与者，而MTurk主要吸引美国或印度的参与者。我们在这个平台上托管了两个任务：一个是社会接受度，参与者阅读来自Social Chemistry数据集的情况，然后写下一个情况的社会接受度。为了保持参与度，参与者可以比较他们的答案与AI和其他人的答案。我们还比较了这些注释与Social Chemistry、Delphi和GPT-4的注释。我们还复制了一个类似的设置，用于检测 hate speech，参与者阅读Dynahate数据集中的一个实例，然后写下一个实例是否属于 hate speech。我们随后将这些注释与Dynahate、Prospective API、Rewire API、Hate Roberta和GPT-4的注释进行了比较。最后，我们总共获得了16,000个注释，来自1000名来自87个国家的参与者。现在，我们更好地回答了NLP数据集和模型与哪些人最一致。我们发现NLP中存在立场。例如，我们发现数据集和模型最接近英语国家。对于GPT-4的社会接受度分析，我们发现它最接近于儒家和英语国家。我们还发现Dynahate也最接近英语国家。我们还发现，在GPT-4的社会接受度任务中，以及Dynahate任务分析中，数据集和模型与受过大学教育的人更接近。然而，当数据集和模型与特定群体对齐时，一些群体不可避免地被落下。例如，在GPT-4的社会接受度任务和Dynahate任务分析中，我们发现数据集和模型与非二元性别的人不太一致，相比之下，男性和女性的注释更加一致。鉴于NLP中存在立场，我们应该怎么办？我们有几个建议。第一个建议是在整个研究过程中记录所有相关的设计选择。第二个建议是用透视主义的视角来进行NLP研究。第三个建议是建立针对四个特定社区的专门数据集和模型，一个很好的例子是Masakhani计划。我们要强调的是，包容性NLP不仅仅是让所有人都能使用技术。这就是我们的 presentations 结束的地方。如果您想了解更多信息，请查阅我们的仪表板获取最新的分析结果和论文。谢谢。</sample>
    <sample id="97">演讲者提到了 SimulST 的以下几个问题：1. 需要特定的架构来训练模型，这会导致额外的模块需要优化。2. 长而复杂的培训程序，例如涉及不同优化目标的培训。3. 需要培训多个模型以达到不同的延迟范围。</sample>
    <sample id="98">减轻数据集中的社会和政治偏见的有效方法包括使用多样化的数据来源，确保数据集中包含各种观点和视角。此外，可以采用数据清洗和预处理技术来识别和删除偏见内容。还可以使用正则化技术来限制模型学习数据中的偏见。最后，可以使用多样化的验证集来检测和纠正偏见。</sample>
    <sample id="99">你好，我是复旦大学的袁思雨。我来介绍我们团队的研究成果“从大型语言模型中提取脚本知识用于约束语言规划”。在日常生活中，人们经常通过遵循分步指令（即目标导向的脚本）来规划自己的行动。以往的研究已经利用语言模型来规划抽象的目标，例如“做蛋糕”，并证明大型语言模型可以有效地将目标分解为步骤。然而，以往的工作主要关注抽象活动的目标规划。对于具有特定约束的目标规划，如“做巧克力蛋糕”，仍然缺乏研究。本文中，我们定义了约束语言规划问题，该问题对规划的目标施加不同的约束。一个抽象目标可以被继承为具有多方面约束的具体目标。一个优秀的规划者应该编写合理且符合约束的脚本。本文中，我们首先评估和改进大型语言模型的约束语言规划能力。由于不存在支持我们研究的具体目标数据集，我们需要首先获取这些目标。如表所示，我们使用InstructGPT通过人类辅助数据收集扩展抽象目标的多方面约束。我们采样了100个具体目标，并评估了大型语言模型生成的脚本。表报告了结果的整体准确性。我们发现所有语言模型在规划具体目标时都取得了不佳的结果。然后我们进行了详细分析，以调查为什么学习模型失败。图中的结果表明，InstructGPT在不同类别约束下的规划性能差异很大。以往的研究已经证明，语言模型的输出质量存在高方差，导致性能不佳。因此，我们采用了“过度生成-然后筛选”的想法来提高生成质量。我们首先展示了约束类型和示例，基于种子抽象目标获得具体目标。然后，InstructGPT为每个具体目标生成K个脚本。接下来，开发了一个筛选模型选择忠实的脚本。我们将脚本和目标转换为InstructGPT嵌入，并计算余弦相似度作为相似性得分，以衡量语义相似性。此外，我们奖励包含目标约束关键词的脚本。我们只保留目标得分最高的脚本。通过这种方法，InstructGPT可以生成高质量的脚本。我们的方法大大提高了语义完整性和对约束的忠实性。由于大型语言模型部署成本昂贵，因此需要使小型和专门化的模型具备语言规划能力。创建数据集是实现这一目标的关键步骤。然而，以往的研究并未使大型语言模型具备规划具体目标的能力，而手动标注数据集的成本昂贵。因此，我们采用了符号知识蒸馏的想法，从大型语言模型中提取约束语言规划数据集。我们应用我们的方法构建了一个名为CoScript的约束语言规划数据集。总共，我们生成了55,000个具体目标及其脚本。为了确保验证和测试集的质量，我们请众包工人查找并修改错误样本。图显示了CoScript中生成的具体目标的约束分布。我们发现CoScript在生成的具体目标上具有高度的多样性。有了CoScript，我们可以尝试使用更小但专门化的模型进行约束语言规划。我们发现，经过CoScript微调的T5可以生成比大多数大型语言模型更高质量的脚本，这表明当适当训练于合适的数据集时，较小的模型可以超越较大的模型。总之，我们建立了约束语言规划问题。我们评估了大型语言模型的约束语言规划能力，并开发了大型语言模型的“过度生成-然后筛选”方法。我们使用大型语言模型生成了一个高质量的脚本数据集CoScript，用于约束语言规划。我们希望CoScript数据集可以成为推进语言规划研究的宝贵资源。谢谢您的时间。有关CoScript的更多细节，请参阅我们的论文。</sample>
    <sample id="100">Multi-hop QA involves answering questions that require multiple reasoning jumps, with each jump corresponding to a document in the corpus. For example, to answer the question "What 1988 Christmas comedy film did Brian Doyle-Murray star in?", we need to find all the movies that Brian Doyle-Murray starred in and then find the movie released in 1988. This set of documents required to answer the question is called the "chain." Multi-hop retrievers are trained by maximizing the probability of the ground-truth chains given questions. Most state-of-the-art multi-hop retrievers fall under this paradigm.

Existing systems require thousands of examples of questions and ground-truth chains for good performance, which can be expensive, especially for low-resource domains and domains that require special expertise. Our approach, PromptRank, is data-efficient and gives good performance with as few as 128 examples. The idea is to combine an unsupervised retrieval method with a few-shot language model-based reranker.

There are two main steps: Retrieve a pool of candidate chains using TF-IDF retrieval and hyperlink traversal. Then, rerank these candidates using the few-shot language model reranker. Two points to consider here are what scoring function to use and how to prompt the language model to extract this score. We use the likelihood of the question given the chain according to a language model. Given the language model, we have a chain prompt that we will explain how we construct in a few slides. Given the question, we score C as the probability of the question given the chain prompt.

Let's go through a working example. Given this question, in an underlying corpus, we retrieve initial documents using TF-IDF. Then we expand and prune chains by following hyperlinks. Then we convert each of the non-pruned chains to prompts. Then we score each chain by the probability of the question given the chain prompt. How do we construct the chain prompt? Given this question and this chain, we have a prompt that looks like this, where we insert the chain documents into the prompts, and we have an indicator token to designate that this is a document. And we have an instruction, which in our case here is something like "Read the previous documents and ask a question." And the instruction serves to elicit the language model's reasoning ability over the chain documents. We explore additional techniques, like instruction search to find optimal instructions, and instruction sampling, where we compute chain scores by aggregating multiple scores computed with different instructions, and also temperature scaling, where the language model logits are scaled by some constant temperature.

We experiment with GPT2-XL and T5-XL and evaluate our approach on HotpotQA. As for metrics, we use R@K recall at K and answer recall AR@K. As for instruction search, we generate 200 diverse instructions and evaluate each on a set of 128 examples. All PromptRank experiments use only 128 examples in total. So let's start with the retrieval results. We see that PromptRank outperforms fully supervised systems like DrKit and performs comparably to state-of-the-art multi-hop dense retrievers. We also did ablation to verify the importance of each component we propose, and we find that each component definitely plays a role in the performance of the final performance of PromptRank. We also evaluate the downstream QA performance when using PromptRank as the retriever, and so we use a reader model, which is ELECTRA-Large, and we compile it with PromptRank. And we see that PromptRank exhibits very good downstream multi-hop QA performance, underperforming MDR by only around four exact match points. Check out our paper for more results and extensive analysis.

To summarize, language models can be used for few-shot ranking of candidate paths for multi-hop QA. PromptRank exhibits strong few-shot path retrieval performance compared to fully supervised systems. The likelihood of the question given the chain works significantly better as a scoring function than the reverse. The instruction plays a strong role in eliciting language models' reasoning abilities over the chain documents. With that, I conclude my talk and thank you so much for listening.</sample>
    <sample id="101">根据 David Vilar 的演讲，PaLM 的流畅度与领先系统相当。然而，主要区别在于准确性。特别是常见的错误是省略错误，即 PaLM 有时选择通过省略源句子中翻译的部分来产生更好的听起来的翻译。此外，PaLM 在“风格/笨拙”类别中的评分低于领先系统，这表明 PaLM 提供了真的流畅输出，但仍然存在一些准确性问题。</sample>
    <sample id="102">The watermark method needs to meet the following properties: 1) Applicable to embedding as services. 2) The watermark should not degrade the utility of the provided embeddings. 3) The watermark should be covert enough to the attacker or the attacker can remove the watermark easily. 4) The watermark needs to be transferable to the attacker's services during the model extraction process.</sample>
    <sample id="103">TED 英语演讲已被翻译成 14 种不同的语言。</sample>
    <sample id="104">从一个数据集中抽取的实例数量用于重新注释没有在提供的信息中明确说明。</sample>
    <sample id="105">在衡量良性和后门数据集之间的差异时，使用了余弦相似度和L2相似度作为距离度量。</sample>
    <sample id="106">The paper presents a dataset called QUEST, which is a retrieval dataset that includes more than 3,000 entity-seeking queries where queries contain implicit set operations. The dataset is constructed by relying on Wikipedia category names from four domains of interest: films, books, plants, and animals. Human annotators are asked to paraphrase templatic queries, validate the queries for fluency and naturalness, verify the relevance of entities in the answer set, and mark evidence in the document as its attribution.

To evaluate systems on the dataset, systems need to retrieve multi-answer sets from a large document corpus where queries contain implicit set constraints and the evidence for a document's relevance can come from multiple parts of the document. Baselines for the dataset include sparse and dense retrievers as well as a T5-based reranker that takes in the top 100 candidates from the retriever. The results show that there is a large room for improvement on retriever performance based on the recall of the complete answer set, indicated here by the MRecall@100 scores. The end-to-end system performance in terms of F1 scores is fairly low, showcasing the difficulty of systems in handling such queries. Through analysis, it is found that queries with set intersection and set difference are particularly challenging and have the lowest F1 scores.</sample>
    <sample id="107">基于编码器的多语言模型可以用于这项任务，通过使用多语言预训练编码器和指针解码器（Encoder-PTR）或编码器-解码器模型（Encoder-Decoder）。这些模型可以在多种语言上进行混合训练，以提高性能。在实验中，发现编码器-解码器或编码器-PTR模型在所有九个数据集上取得了最佳性能。</sample>
    <sample id="108">The paper discusses the limitations of the Minimal Pair Paradigm (MPP) in evaluating language models' acceptability judgments, particularly when it comes to longer sentences. The authors argue that current MPP pipelines do not allow for the evaluation of a model's acceptability across longer context windows, which is crucial given the increasing context windows of large language models.

To address this issue, the authors revisit the MPP pipeline by simulating longer sequences and testing the models' acceptability judgments on these sequences. They recreate sentences by choosing acceptable or unacceptable sentences from datasets and add them as prefixes to both acceptable and unacceptable queries. This allows them to test the models' acceptability judgments under different scenarios, including mismatched contexts, same data set contexts, and unrelated domains.

The results show that the MPP judgments are mostly robust for arbitrary context lengths when using Wikipedia sentences, which are completely irrelevant to the current query pair. However, when choosing sentences from the same data set, the MPP judgments either increase or decrease significantly depending on whether the chosen prefix is acceptable or unacceptable. This effect increases throughout the context length and may affect newer language models with larger context windows.

The authors also conducted a series of analyses to understand why the match prefix affects the language model's judgment so much. They found that the models are sensitive to latent syntactic and semantic features shared across the sentences, and that perturbing the input sentence does not significantly change the MPP judgments. This suggests that the models' judgments are based on abstract knowledge that is not easily perturbed.

Overall, the paper highlights the need for more comprehensive evaluation methods that can capture language models' abstract knowledge throughout the context window.</sample>
    <sample id="109">The paper "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor" presents a method for generating a large dataset of natural language instructions and their corresponding inputs and outputs without any human annotations. The authors propose a fully automatic data generation process that involves prompting a pre-trained language model to generate instructions, inputs, and outputs based on examples from the Super-Natural Instructions dataset. They also diversify the dataset's format by generating additional paraphrases of each instruction.

The resulting dataset contains 64,000 examples, and if we consider the instruction paraphrases, we have about 240,000 examples. The authors analyze the generated examples focusing on creativity, diversity, and correctness. They find that more than 50% of the generated examples are indeed correct, and even incorrect examples often contain valuable information for instruction tuning. In terms of creativity and diversity, Unnatural Instructions contains highly creative tasks, some of which are very different from the classic NLP tasks.

To measure the utility of the generated data, the authors fine-tune an 11 billion-parameter T5 model on Unnatural Instructions. They show that the model can outperform both T0++ and Tk-instruct across several benchmarks. On top of that, when the cost of generating examples is amortized, training on Unnatural Instructions outperforms their baseline on all benchmarks. The tested benchmarks were Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry.

The paper highlights the ability of language models to produce creative and diverse data, which is difficult to obtain with crowd workers who usually collapse into predictable heuristics and form annotation artifacts. At the same time, language models are faster and cheaper than human annotations.</sample>
    <sample id="111">The author selects a trigger set, which is a group of words in a moderate frequency interval. The provider can collect a general text corpus and count the word frequency with it to determine the trigger set.</sample>
    <sample id="112">大家好，我叫舒恒。今天我要向大家介绍我们的一篇论文《CoNLL-2003命名实体识别器在2023年还能正常工作吗？》。我们将探讨命名实体识别任务（NER任务）中的泛化问题。我们观察到，自1990年代以来，模型一直在CoNLL-2003数据集上开发命名实体识别器，这自然提出了几个问题。首先，这些模型能否泛化到现代数据？其次，在开发新的标注器时，需要什么才能实现良好的泛化？最后，如果观察到性能下降，是什么原因导致了这些模型的性能下降？为了研究这些问题，我们开发了CoNLL++数据集。这是一个从2020年的路透社新闻中收集的数据集，并使用与CoNLL-2003相同的标注指南进行了标注。我们随后在CoNLL-2003上对超过20个模型进行了微调，并评估了它们在CoNLL-03测试集和CoNLL++上的表现。我们还计算了每个模型的F1分数百分比变化，以评估其泛化能力。那么，对于良好的泛化需要什么？通过实验，我们发现有三个主要因素是必要的。第一，模型架构。我们发现，Transformer模型通常能更好地泛化到新数据。第二，模型大小。我们发现，通常较大的模型会导致更好的泛化。最后，我们知道，用于下游任务的微调示例数量直接影响性能。在这里，我们还发现，更多的微调示例实际上也提高了泛化能力。接下来，我们来看一下有些模型性能下降的原因。我们有两个假设：第一个是自适应过拟合，即通过不断使用相同的测试集进行过拟合，从而导致性能下降，这通常表现为在新测试集上的性能下降。第二个假设是时间漂移，即由于训练和测试数据之间的时间间隔增加而导致的性能下降。对于自适应过拟合，我们看到图表右侧的红色最佳拟合线的斜率大于1，这意味着我们在CoNLL-2003上所做的每一次改进，在CoNLL++上相当于多次改进，这表明自适应过拟合在这种情况下并不明显。那么，关于时间漂移呢？对于时间漂移，我们进行了实验，重新训练或继续预训练一些模型以使用更近的数据，结果发现，随着时间间隔的增加，性能下降，这证实了我们的假设，即性能下降的主要原因是时间漂移。我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例。同时，这些因素必须手牵手地一起出现，我们不能只保留一个因素而抛开其他因素。此外，我们还发现，性能下降是由时间漂移引起的，令人惊讶的是，尽管CoNLL-2003已经使用了20多年，但这种现象并不是由自适应过拟合引起的。回到我们论文的标题“CoNLL-2003标注器在2023年还能正常工作吗？”的答案是明确的：答案是肯定的。我们希望这篇论文能够促使更多关于如何提高模型泛化能力的研究。最后，请务必查阅我们的论文、数据集，如果您有任何问题，请随时联系我。谢谢大家。</sample>
    <sample id="114">The team from Nanyang Technological University in Singapore has introduced a new method for optimizing large language models, which are typically heavy on parameters and require long training times. The method involves grouping attention heads into clusters and pruning redundant heads to achieve significant parameter compression while maintaining performance. The team has proposed two strategies: group-constrained training and voting-to-stay algorithm. Group-constrained training divides attention heads into groups, making intra-group heads more similar and inter-group heads more separate. Voting-to-stay algorithm prunes redundant multi-head attention by collecting votes based on head scores given by an evaluator and pruning heads with low votes. The team has evaluated their method on three tasks: machine translation, language modeling, and abstractive summarization, achieving improvements of up to 7% and 32.1% compression, respectively. They have also conducted efficiency analysis, showing that their LITE model achieves 90% of pruned parameters, 62% faster inference speed, and 80% for FLOPs against the model which yields the same performance on the same data set. The team believes that task-specific automatic pruning is promising and can help reduce the redundancy of large language models without sacrificing performance.</sample>
    <sample id="115">该方法使用的语音片段大小为lambda。</sample>
    <sample id="116">在 Servin 和 Kea 的示例中，需要特定于实体的知识是“Servin 是一名法官”和“Kea 是一名面包师”。</sample>
    <sample id="117">示例质量比与源句子的相似度更为重要。</sample>
    <sample id="118">The paper presents a new pre-training technique for code-switched NLP tasks. Code-switching refers to the use of two or more languages in a single sentence, which is common in linguistically diverse communities like India. The paper proposes SwitchMLM, a novel MLM technique tuned to handle code-switch information. SwitchMLM identifies switch-points, which are groups of two tokens that transition between languages, and only masks these words during pre-training. The authors also propose FrequencyMLM, a surrogate method that assigns LID tags based on the negative log likelihood of each word in monolingual corpora. Architectural modifications, such as residual connections and auxiliary LID-based loss, are proposed to further enhance switch-point information content in the final representation. Probing experiments using linear and conditional probing verify that the proposed methods increase the amount of switch-point information in intermediate layers. The results show that the combined method of SwitchMLM with ResBERT and auxiliary loss performs the best on sentiment analysis tasks across language pairs.</sample>
    <sample id="119">在扩展实验中，论文侧重于RoBERTa和GPT系列语言模型。</sample>
    <sample id="120">该模型使用特定层的注意力分数。</sample>
    <sample id="121">直接推断的示例包括使用实体的名称或位置，例如说“Easy on Me”或“第一个”。</sample>
    <sample id="122">这篇论文的作者来自复旦大学。</sample>
    <sample id="123">The research presented by Ying and Zhiyang focuses on improving multi-modal zero-shot learning through instruction tuning. The study aims to investigate the effectiveness of instruction tuning in multi-modal pre-trained models for unseen tasks. The researchers address the lack of instructional datasets in multi-modal tasks, creating a new dataset called MultiInstruct, which includes 62 diverse multi-modal tasks from 10 broad categories. They use OFA, a unified multi-modal pre-trained model, as their base model and formulate all tasks in a unified sequence-to-sequence format.

The training dataset consists of 53 tasks from 9 groups, with 10,000 instances per task. For testing, they reserve the entire common sense reasoning group and select additional 5 tasks from VQ and Miscellaneous groups. They report performance using accuracy for classification tasks and Rouge-L for generation tasks, along with an additional metric called sensitivity to measure consistency in outputs.

The results show that instruction tuning significantly improves OFA's performance on seen multi-modal tasks, with better performance and lower sensitivity as the amount of task increases. Using more instructions during fine-tuning also improves overall performance and reduces sensitivity. Transfer learning from natural instruction datasets is shown to benefit instruction tuning, achieving better sensitivity compared to the original OFA model. The researchers propose a new metric called sensitivity and are currently collecting a larger multi-modal instruction tuning dataset for future research.</sample>
    <sample id="124">The speaker, Tan Qingyu from the National University of Singapore and Alibaba, presents a study on temporal reasoning in large language models (LLMs). The study breaks down temporal reasoning into three levels: time-to-time reasoning, time-to-event reasoning, and event-to-event reasoning. The researchers conducted preliminary experiments on L1 prediction of years using question templates from Table 1. They evaluated three LLMs: T5-L fine-tuned on Natural Questions, instruction-tuned FLAN-T5-L, and ChatGPT. The results showed that the first two LLMs had a strong bias towards the 2000 to 2020 time period, which could be correlated with term frequencies in pre-training corpora. ChatGPT demonstrated strong performance in year prediction but deteriorated significantly in month prediction.

To address these findings, the researchers proposed the TempReason dataset, which covers all three levels of reasoning and long temporal coverage. For L1 questions, they increased difficulty from year prediction to month prediction. For L2 and L3, they constructed question and answer pairs using Wikidata Knowledge Base and Wikipedia articles. The dataset statistics are shown in Table 3.

The researchers evaluated temporal reasoning in three QA problem settings: Closed Book QA, Open Book QA, and Reasoning QA. In Reasoning QA, all relevant temporal knowledge is provided to the LMs, and they are required to reason based on the questions and temporal knowledge. To improve temporal reasoning capability, the researchers proposed a training strategy with two components: Temporal span extraction pre-training and time-sensitive reinforcement learning. The final model is TempT5.

The experiment results on TempReason show that TempT5 can improve the performance of T5-SFT significantly in OBQA and the Reasoning QA set. However, ChatGPT's performance varies greatly across different time periods, indicating flaws in temporal reasoning. Future work should focus on overcoming such reasoning biases.</sample>
    <sample id="125">根据Yanis Labrak的介绍，无法确定论文的具体作者人数。然而，他提到了与他一起工作的其他人，包括ChuBERT模型的作者和在GitHub仓库上提供预训练模型的贡献者。</sample>
    <sample id="126">是的，在语义解析之前，使用机器翻译模型（如Google Translate API）将自然语言查询翻译成目标语言作为基线。</sample>
    <sample id="127">The video introduces a research paper titled "Large Language Models Are Reasoning Teachers" by Namgyu Ho, Laura Schmid, and Se-Young Yun from KAIST AI in Korea. The paper proposes a novel technique to transfer the reasoning abilities of large language models to smaller models using chain-of-thought prompting and diverse reasoning. The authors argue that while large language models can solve complex tasks, they require significant memory and computation resources, making them impractical for deployment in many situations. To address this issue, they propose using large models as reasoning teachers to train smaller models.

The method involves applying zero-shot chain-of-thought prompting on large models to generate step-by-step solutions for complex tasks. These solutions are then used as training data to fine-tune smaller models. The authors also introduce a novel technique called diverse reasoning, which generates multiple distinct solutions using stochastic temperature sampling. This allows the student model to learn from a variety of solutions and improve its performance.

The paper compares the proposed method with existing baselines on 12 tasks and shows that it achieves notable performance, especially in text-based tasks. Diverse Reasoning significantly increases performance on Multi-Arithmetic tasks, improving the score from 33% to 55%. The authors also demonstrate that their method is scalable and can be further improved by increasing the number of datasets, using better teacher models, or larger student models.

The video concludes by highlighting the trade-offs involved in scaling student performance, such as development-time costs, inference-time costs, and the quality of inference. The authors encourage readers to check out their paper for more details and provide access to code, data, and experiments. They also invite discussions and future work based on their material.</sample>
    <sample id="128">The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources is a collaborative work between McGill University, Mila, and Microsoft Research. The research focuses on the ability of natural language understanding models to integrate knowledge from both pretraining and inference time sources. The authors propose a diagnostic test suite for knowledge integration, specifically in coreference resolution tasks. They evaluate the data set with human study participants and established coreference resolution models.

The research defines three settings of KITMUS: Background-Pretrain, Background-Both, and Background-Inference. In the Background-Pretrain setting, background knowledge is assumed to be available at pretrain time. In the Background-Both setting, both background and entity-specific knowledge are available at inference time. In the Background-Inference setting, both knowledge types are available only at inference time.

The results show that without task-specific training on KITMUS, most models do not perform well. However, when trained on KITMUS, both C2F and BERT4Coref models perform significantly better than random choice. Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate backward knowledge provided only at inference time.

Overall, the main takeaway of the paper is that many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time.</sample>
    <sample id="129">根据所给的英文内容，作者给出的“显性群体”(marked group) 的示例包括黑人女性、 Latina 女性、亚洲女性和中东女性。</sample>
    <sample id="130">根据所提供的内容，没有明确说明哪些模型架构泛化能力较差。然而，研究发现Transformer模型通常在新数据上泛化得更好。因此，可以推断出非Transformer模型架构可能在泛化能力方面较差。</sample>
    <sample id="131">在视频中没有提到测试数据集的名称。</sample>
    <sample id="132">根据所提供的信息，该论文有三位作者：Akshatha、Martin和她的合作者。</sample>
    <sample id="133">作者采用了多种模态，包括文本、图像和坐标。他们使用OFA统一多模态预训练模型，并将所有任务统一为序列到序列格式，其中输入文本、图像、指令和坐标表示在相同的标记空间中。</sample>
    <sample id="135">ABC-Eval是一种新的对话人工智能评估方法，由Emory NLP实验室的Jinho Choi教授领导，并与Amazon Alexa AI合作开发。该方法旨在通过明确标注模型响应的行为来减少人类评估的主观性，从而更精确地评估对话质量。ABC-Eval能够衡量模型在各种方面犯错的频率，如忽略对话伙伴、说 irrelevant 的内容、自相矛盾、 hallucinate 错误事实或违反共同知识、以及成功或失败地展示同理心。

为了验证ABC-Eval的有效性，研究者选择了四个领先的聊天机器人模型，并对每个模型进行了100次人类-机器人对话的评估。他们还使用了三种现有方法：基于Likert量表的逐个轮次评分、基于Likert量表的整体对话评分和整体对话的两两比较。结果表明，ABC-Eval的行为标签在可靠性上优于现有方法，且在预测对话质量方面更有效。例如，测量模型自相矛盾的次数可以解释5%和10%的对话质量，而Likert一致性评分只能解释4%或更少。

此外，ABC-Eval指标的独特性和信息量也得到了证实。所有ABC-Eval指标的组合可以解释对话质量的25%，而逐个轮次Likert指标的组合则解释较少的质量。这些可靠的、信息丰富且独特的指标使我们能够以比以往任何方法都更高的精度评估对话人工智能。

尽管实验中存在一些挑战，但随着领域的快速发展，这些错误率可能会在新模型发布时有所下降。然而，这更加证明了需要可靠和精确的评估指标来比较模型。研究者希望ABC-Eval能成为其他研究者在这一方向上的有意义步骤。</sample>
    <sample id="136">The work presented by Jasivan and Nafise at the University of Sheffield focuses on developing an alternative evaluation set for numerical reasoning tasks called FERMAT. The motivation behind this work is to address the limitations of existing benchmarks, which often provide accuracy scores or F1 measures that do not effectively capture the strengths and shortcomings of language models in mathematical abilities.

FERMAT introduces a flexible evaluation set based on arithmetic types, including number understanding, mathematical operations, and training dependency. The set consists of math worded questions extracted from Illinois and CommonCore, with numbers represented in various formats such as small integers, large integers, and decimals. This approach allows for a more comprehensive assessment of the models' capabilities across different ranges and mathematical operations.

The researchers performed a baseline evaluation using zero-shot evaluation and found that most models perform poorly across all aspects. However, after fine-tuning the models with math teachers' templates, there was a significant improvement in performance. The study also investigated the impact of training dependency and found that even when the exact expression was seen during training, the accuracy remained below 50%, suggesting the importance of linguistic notions.

The results indicate that language and mathematical diversity are crucial for improving model performance. Additionally, the study highlights areas for improvement, such as number encoding and tokenization. Overall, FERMAT provides a more informative alternative to existing benchmarks, filling a gap in evaluating the mathematical abilities of language models.</sample>
    <sample id="137">The paper "Tell2Design: A Dataset for Language-Guided Floor Plan Generation" by Sicong from the Singapore University of Technology and Design presents a novel machine-learning task where a model generates 2D floor plan designs directly from language instructions. The task involves understanding the semantics, geometry, and topology of a floor plan's intrinsic components and generating a structured interior layout that aligns with the input language instructions.

The Tell2Design dataset consists of 5,051 human-annotated language instructions collected from Amazon Mechanical Turk and around 76,000 artificially generated instructions from pre-defined templates. The average number of words for a single floor plan is more than 200, resulting in more than 10 sentences.

The main challenges of this task are to perform design generation under stricter constraints compared to artwork-like text-conditional image generation, to understand the big picture of the entire floor plan from document-level unstructured text with fuzzy and entangled information, and to deal with ambiguous, incomplete, or misleading information in human instructions.

To solve this task, the authors cast the floor plan generation problem as a sequence-to-sequence problem under the encoder-decoder framework, where room bounding boxes are re-constructed into a structured target sequence. They use a transformer-based encoder-decoder structure initialized by a pre-trained language model T5 for better language understanding abilities. The model is trained using a normal language modeling objective where X is a set of instructions in natural language and Y is the target bounding box sequence, and L is the target sequence length.

The results show that the Tell2Design model achieves the highest IoU scores, with a Micro IoU of 54 and a Macro IoU of 53, outperforming other text-conditional image generation baselines by a large margin. This can be attributed to the sequence-to-sequence model controlling the target box sequence generation based on salient information extracted from the language instructions. In contrast, text-conditional image generation methods fail to perform well due to their focus on generating artwork-like images with high-level visual concepts from short text instead of following multiple instructions with various constraints for a specific design.

The paper concludes that the Tell2Design dataset and the proposed sequence-to-sequence model serve as a foundation for future research on the task of language-guided design generation.</sample>
    <sample id="138">作者认为在 NLU 中研究不足的领域包括模型如何整合和使用预训练时间和推理时间的知识。</sample>
    <sample id="139">演讲者的名字是 Ying。</sample>
    <sample id="140">是的，CoScript 经过了质量检查。为了确保验证和测试集的质量，作者请 crowdsourced 工作人员查找并修订错误样本。</sample>
    <sample id="141">现有的资源在支持上下文依赖翻译方面存在局限性，因为它们通常依赖于领域知识和人工策展。</sample>
    <sample id="142">你好！我们将讨论我们关于“Resolving Indirect Referring Expressions for Entity Selection”的工作，其中我们引入了AltEntities语料库。我的名字是Javad Hosseini，这项工作与Filip Radlinski、Silvia Pareti和Annie Louis合作完成。我们的目标是了解用户在想要做出选择时的语言。考虑这个问题的另一种说法：“你是说《Easy on Me》还是《I Gotta Feeling》？”在这里，用户想在两首歌曲之间做出选择。最明显的方法是使用直接引用，例如通过说歌曲名称“Easy on Me”或其位置，“第一个”。但有时 indirect reference 更合适，以进行更自然的对话。这可能发生在用户不能记住歌曲名称时。或者，当两个歌曲的 pronunciation 太相似而难以区分时。或者，当用户想指定偏好时。这里有一些 indirect references 的例子，例如，“较新的那首”或“那首不那么充满活力的歌曲”。这是对话系统中一个重要的问题，也是评估大型语言模型（LLM）实体理解能力的一个基准。我们不知道有更大规模的公开数据集可以用于这个任务，所以我们用 crowdsourcing 收集了一个数据集。我们的数据集覆盖了三个不同的领域：音乐、书籍和食谱。我们数据集收集方法强调使用卡通完成设置。卡通有三个对话气泡。在第一个气泡中，Bob说，“记得昨天我们在听的那首歌吗？”然后Bob设置了对话背景。在第二个气泡中，Alice说，“你是说《Easy on Me》还是《I Gotta Feeling》？”这是另一种说法。在第三个气泡中，Bob使用 indirect reference 选择其中一个实体，例如，“较新的那首”。我们自动提供前两个对话气泡，第三个对话气泡由标注员填写。第一个对话气泡从每个领域的几个手动提示中选择。第二个对话气泡，即另一种说法，按照以下模板生成。你是说 A 还是 B？其中 A 和 B 是来自 Wikipedia 的示例。以下是我们在不同层次使用的不同采样方法。当我们向上移动时，实体变得越来越相似，通常更容易区分。第一种方法是随机均匀采样。第二种方法是实体具有相似标题，例如两本书名为《The Return》。第三种方法是它们在 Wikipedia 上具有相似的描述。最后，它们具有相同的属性或信息，例如相同的流派或相同的艺术家。当我们向标注员展示这些另一种说法时，我们知道这些实体的名称，但并不一定了解这些实体。因此，我们向标注员展示了每个实体的 Google 搜索结果，并要求他们至少听一下每首歌曲，并阅读关于每首歌曲的信息。例如，这是歌曲《Easy on Me》的 Google 搜索结果。对于食谱和书籍领域，我们展示了来自 Wikipedia 的一些背景文本。对于食谱，我们还展示了它们的图片，再次来自 Wikipedia，这样标注员就能知道它们看起来是什么样子。然后我们要求标注员从这两个实体中选择一个，并用三到五个 indirect referring expressions 来描述它们。例如，“那个有钢琴音乐的”，“没有歌词的”，“不是那个12岁男孩的那个”，“那个虚构的那个”，“来自阿塞拜疆的”，等等。AltEntities语料库包含6000个另一种说法，涵盖三个领域，以及42000个 indirect referring expressions。T5 XL 模型的结果如下。如果语言模型拥有标注员相同的相同背景知识，那么准确率会很高，大约在92%到95%之间。但这并不现实。如果语言模型拥有部分重叠的背景知识，那么准确率在82%到87%之间，这更现实。例如，当语言模型检索背景知识时。如果语言模型只拥有实体名称作为背景知识，那么准确率只有60%，所以还有很大的改进空间。我们还展示了模型在不同领域通用。这里是数据集的链接。谢谢。</sample>
    <sample id="143">该方法与Wait-k策略、Local Agreement和专门用于同时预翻译的最新架构进行了比较。</sample>
    <sample id="144">很抱歉，根据提供的信息，我无法确定论文作者所属的机构。</sample>
    <sample id="145">演讲者的名字是Jenny。</sample>
    <sample id="146">The speaker, Yicheng from Fudan University, introduces a paper on the analysis of omission in dialogue summarization. Dialogue summarization is a subtask of text summarization that involves creating a concise summary representing the most important information within a dialogue. The paper explores the issue of omission, which is a major factor affecting the quality of dialogue summarization. The speaker presents an analysis of the omission rate in summaries from five domains and six pre-trained models, finding that about 70% of generated summaries have the omission problem. The speaker also discusses the position distribution of omitted information in dialogues, which are randomly distributed regardless of length or domain. To address the omission problem, the speaker proposes the task definition for omission detection, which involves predicting which utterances of dialogue are omitted in the candidate summary. The speaker constructs the OLDS dataset to support this analysis and detection task, which includes high-quality omission labels for dialogue summarization. The speaker explores three frameworks as baselines for omission detection, including pair-wise classification, sequence labeling, and pointer network. The speaker also proposes a post-editing method for summary refinement using detected omissions, which significantly improves the performance of the summary.</sample>
    <sample id="147">这篇论文有三位作者。</sample>
    <sample id="148">嗨，我是萨拉·帕皮，来自特伦托大学和布罗纳基利基金会。我将简要介绍我们与马泰奥·内格里和马可·图尔奇合作的“注意力作为同步口译的指南”论文。同步口译，或SimulST，是实时将 spoken language 翻译成另一种语言的文本的过程，实现跨语言交流。当前SimulST模型的问题是什么？特定架构通常被训练，引入额外模块进行优化。长而复杂的训练程序，例如涉及不同优化目标的训练。训练和维护多个模型以达到不同的延迟范围。例如，训练一个平均延迟为1秒的模型和另一个平均延迟为2秒的模型等等。我们的解决方案是什么？首先，使用现有的离线ST模型，无需重新训练或采用特定架构用于SimulST。仅使用一个模型处理每个延迟范围，并通过特定参数处理延迟。利用模型通过音频输入和文本输出之间的注意力机制所获得的知识。这是交叉注意力机制，您可以在右侧看到示例。我们的解决方案是提出EDAtt，即编码器-解码器注意力，这是一种策略，我们决定是否发出或不发出部分翻译，基于注意力指向的位置。一个词如果注意力不集中，即其向最后λ个语音帧的总和低于某个阈值α，则发出。例如，如果我们收到包含“我要谈论…”的语音片段，模型预测德语翻译，我们将查看交叉注意力权重，我们将看到第一个两个词指向最早收到的语音帧，而最后一个词指向最后收到的语音帧，因为λ个语音帧。这意味着前两个词将被发出，因为它们的交叉注意力总和高于阈值α，我们将等待另一个语音片段。如果我们继续接收另一个语音片段，模型预测其他三个词，我们将查看这些交叉注意力权重，我们将看到没有词指向最后λ个语音帧。这意味着这三个词将被发出。如果我们查看EDAtt的主要结果，我们将绘制同步口译结果的图表，在其中一个轴上是BLEU，衡量翻译质量，另一个轴上是平均延迟，我们还考虑了计算感知平均延迟，即模型预测输出所需的时间。我们希望曲线尽可能高，但也要向左移动。我们将EDAtt与应用于离线模型的流行策略进行比较，这些策略包括Wait-k策略和局部一致性。我们还将EDAtt与专门针对同步预翻译的最新架构进行比较。这些是我们在德语上的同步口译策略的主要结果。我们可以看到，EDAtt在考虑实际耗时或计算感知时间的情况下，比应用于离线模型的所有策略都表现出色。如果您想了解更多信息，请阅读我们的论文。我们还公开发布了代码、模型和同步输出，以促进我们工作的可重复性。谢谢您的关注。</sample>
    <sample id="149">是的，数据集公开了。</sample>
    <sample id="150">Archiki and their team have presented a paper on MEETINGQA, an extractive question-answering dataset based on meeting transcripts. The dataset contains 7.7K questions split into Train, Dev, and Test sets, with 30% of the questions unanswerable. The authors collected data from the AMI corpus, which includes nearly 100 hours of manually transcribed multi-party meetings. They performed question selection based on punctuation and filtering out really short questions, and recruited annotators to label sentences in the answer span. They achieved a high inter-annotator agreement, reflected by a Krippendorff's alpha of 0.73.

The authors also introduced a variety of methods for question answering, including context-retrieval, single-span models, and multi-span variants. They found that short-context models like RoBERTa slightly outperformed long-context models like Longformer. In terms of zero-shot performance, there was nearly a 50 F1 point gap between zero-shot performance and human performance. Silver data augmentation effectively improved zero-shot performance, and zero-shot results from larger instruction-tuned models such as FLAN-T5 were comparable to the results from remaining models.

Error analysis showed that models are bad at identifying rhetorical questions, especially in the zero-shot setting. Predictions of single-span models contain more irrelevant sentences than their multi-span counterparts. Models struggle to identify which speaker answers a question, and this gets worse in the zero-shot setting. Overall, MEETINGQA is an interesting dataset based on open-ended and discussion-seeking questions in real-life meeting scenarios, and it is far from being solved as it is challenging for existing QA models in both fine-tuned and zero-shot settings.</sample>
    <sample id="151">大家好，我叫 Ying，我的同事是 Zhiyang。我们将一起展示我们关于 MultiInstruct 的研究，该研究探讨了通过指令调优提高多模态零-shot 学习能力的方法。随着大型语言模型的快速发展，许多研究开始探索如何利用预训练的语言模型以参数和数据高效的方式解决不同的下游任务。最近的研究表明，通过遵循自然指令对大型语言模型进行指令调优，可以使其在未见过的任务上进行零-shot 学习。然而，大多数之前的研究都集中在通过指令调优提高语言-only 任务的零-shot 性能上，而计算机视觉和多模态任务则被忽视了。因此，在本研究中，我们旨在探讨是否可以通过对多模态预训练模型进行指令调优来提高其在未见过的多模态任务上的泛化能力。此外，在我们研究期间，我们发现 NLP 和多模态之间的指令数据集存在显著差异。存在超过 1600 个语言-only 指令任务，但没有大规模公开可用的多模态指令任务。因此，这激励我们构建了一个多模态指令调优基准数据集。我们在这里介绍了 MultiInstruct，这是第一个包含 62 个多样化的多模态任务的数据集，这些任务覆盖了 10 个广泛类别。这些任务是从 21 个现有的开源数据集中派生出来的，每个任务配备了五个专家撰写的指令。为了在我们提出的基准数据集上进行多模态指令调优，我们选择 OFA 作为基础模型。OFA 使用统一的词汇表对语言、图像标记和 bounding box 坐标进行编码。这里我们展示了 MultiInstruct 数据集的一些示例，以统一处理各种输入和输出数据类型。我们遵循 OFA 的方法，将所有任务统一为序列到序列格式。在输入文本、图像、指令和 bounding box 中，每个实例都用相同的 token 空间表示。接下来，我将讨论多模态指令调优。对于训练数据集，我们使用 9 组中的 53 个任务进行训练，每个任务采样 10,000 个实例。对于测试，我们保留整个“常识推理”组进行测试，并从“VQ”和“ miscellaneous”组中选择额外的 5 个任务。我们使用测试集中的所有实例对每个任务进行测试。此外，我们从测试集的“自然指令”组中随机抽取 20 个任务作为未见过的任务。在测试阶段，我们为每个任务进行总共 5 次实验，每次实验使用其中一个指令评估模型性能。对于每个任务，我们报告最小和最大性能以及性能的标准差。如果任务是多模态分类任务，我们报告准确率；如果是多模态生成任务，我们报告 Rouge-L。对于 NLP 任务，我们同样报告 Rouge-L。我们还引入了一个新的评估指标叫做敏感度。这个指标衡量模型在任务上产生相同输出的能力，无论指令的 wording 是否略有变化。我们的主要结果如下：如图所示，指令调优可以显著提高 OFA 在已见过的多模态任务上的性能。此外，通过自然指令数据集进行迁移学习可以进一步提高指令调优的效果。我们可以看到，随着任务数量的增加，模型性能更好，同时敏感度降低。我们还进行了一个实验，使用一个指令与五个指令进行比较。如图所示，使用更多指令可以提高模型的整体性能并显著降低敏感度。这展示了不同微调策略对模型敏感度的影响。通过从自然指令数据集中进行迁移学习，模型可以实现比原始 OFA 模型更好的敏感度。我们还可以看到，通过从自然指令数据集中进行迁移学习，OFA 可以在自然指令数据集上取得更好的性能。总的来说，我们提出了第一个大规模多模态指令调优基准数据集，显著提高了 OFA 的短期能力，并探索了不同的迁移学习技术及其益处。我们设计了一个新的指标叫做敏感度。最后，我们正在收集一个更大的多模态指令调优数据集，其中包含大约 150 个额外的视觉语言任务，我们将发布它们。这是我们的数据和模型的 QR 码。谢谢。</sample>
    <sample id="152">Frederick Riemenschneider's presentation, "Exploring Large Language Models for Classical Philology," delves into the intersection of NLP and classical philology, focusing on the development of language models for Ancient Greek and Latin. The presenter highlights the advancements in monolingual BERT models for these languages, such as Latin BERT (2020), Ancient Greek BERT (2021), and another Ancient Greek BERT (2022). However, the challenge lies in creating multilingual models that can handle both Ancient Greek and Latin, as existing models are not pre-trained on Ancient Greek texts.

The project aims to address this by developing new language models tailored for classical philology. The goals include making existing models comparable, pushing the state-of-the-art further, exploring different model architectures, and introducing multilingual models. Two monolingual models for Ancient Greek, GreBERTa and GreTa, were created. GreBERTa is a RoBERTa model, while GreTa is an encoder-decoder model based on T5 architecture. Additionally, PhilBERTa and PhilTa were developed as multilingual equivalents pre-trained on Ancient Greek, Latin, and English data.

To gather pre-training data, the team utilized Open Greek &amp; Latin, the Internet Archive, and additional resources like the Corpus Corporum for Latin and English texts related to antiquity. They also developed a new pre-training corpus from the Internet Archive using incorrectly transcribed Greek stop words. For benchmarking, they used Universal Dependencies treebanks for Greek and EvaLatina 2022 dataset for Latin.

The results show that their models outperform the current state-of-the-art for both Ancient Greek and Latin. Notably, the encoder-decoder models excel in lemmatization tasks, with performance gains of 5 percentage points above the existing state-of-the-art for Ancient Greek. The models also demonstrate strong semantic and world knowledge capabilities, distinguishing synonyms from antonyms and identifying relations between heroes and gods.

In conclusion, the presentation introduces powerful language models for classical philology, pre-trained from scratch and using native tokenizers. Both encoder-only and encoder-decoder architectures, as well as multilingual models, were developed to process Latin and Greek texts. The team also introduced a high-quality pre-training dataset for Ancient Greek and rigorously benchmarked previous and their own models. The results indicate that the multilingual models do not significantly outperform monolingual models in terms of semantic and world knowledge capabilities.</sample>
    <sample id="153">The speaker, Ninareh Mehrabi, is a postdoctoral scientist at Amazon Alexa AI's Responsible AI team. She presented her work on resolving ambiguities in text-to-image generative models. The goal of the work was to study existing ambiguities in prompts provided to text-to-image models and propose frameworks to mitigate such ambiguities as well as evaluate whether the generated images are faithful to user intention.

The speaker first curated a benchmark dataset that covers different types of ambiguities. Then, these prompts were provided to a prompt disambiguation framework that tries to gather external signals to disambiguate the prompt through either asking clarifying questions from the user or generating different possible visual setups. Once the prompts are disambiguated, they are evaluated, and the disambiguated prompts are input into a text-to-image model to generate images. The generated images are then evaluated using a VQA model to determine if they are faithful to user intention.

The findings show that there is disparity in resolving ambiguities for different ambiguity types, but overall, disambiguation using their framework has a positive effect on faithful generation. Their automatic evaluation framework is in agreement with human evaluation, so it can be used reliably to evaluate text-to-image models.</sample>
    <sample id="154">根据所提供的内容，该论文的作者Sara Papi、Matteo Negri和Marco Turchi来自意大利特伦托大学和布罗纳基西里基金会。</sample>
    <sample id="155">The speaker's name is Javad Hosseini.</sample>
    <sample id="157">The speaker, Shen Gao from Shandong University, introduces a joint work titled "Dialogue Summarization with Static-Dynamic Structure Fusion Graph." The work aims to distill salient information from a dialogue context into a concise summary. The existing methods mainly focus on modeling dialogue with pre-computed static graph structures using external linguistic tools such as discourse parsing and dialogue state tracking. However, these methods have two fundamental drawbacks: they heavily depend on the reliability of external linguistic tools, which may not deliver accurate output and cause error propagation, and the static graph construction is disjoint with the graph representation learning phrase, making it difficult to dynamically adapt to downstream dialogue summarization tasks.

To address these issues, the SDDS model proposes four main components. Firstly, an Utterance Encoder is employed to encode utterances in the dialogue context into vector representations. Secondly, existing data structure modeling methods are used to construct static graphs. Thirdly, a Static-Dynamic Graph module is proposed, which combines multiple static graphs computed in the previous step and uses a dynamic graph module to capture the semantic relationship between utterances based on their deep vector representation. Finally, a pre-trained language model is used as the Summary Generator to fuse the static dialogue structure and the dynamically learned dialogue structure into the final summary.

The model captures static dialogue structure information by proposing four heuristic dialogue structure modeling methods to build relationships between utterances using a graph network. These methods include Discourse Parsing Graph, speaker relationship modeling, utterance position graph, and relative distance between utterances as edge features. To capture the semantic relationship between utterances based on their deep vector representation, a Dynamic Graph module is proposed, which employs a multi-head attention model to calculate the relationship. The static and dynamic graphs are integrated into a unified graph through a fusion method, and a graph attention layer is proposed on top of the original self-attention layer to incorporate the graph representation into the generation process.

Overall, the SDDS model provides a more accurate and adaptable approach to dialogue summarization by integrating static and dynamic graph structures.</sample>
    <sample id="158">The speaker, Qipeng Guo from AWS, introduces the task of coreference resolution and explains that it involves identifying mentions in a document and clustering them to determine which ones refer to the same entity. Conventional methods for this task have quadratic complexity for both computation and memory consumption, while cache-based methods use a fixed-size cache and reduce the complexity to a linear level. However, in long documents, the topic may switch multiple times, causing high cache misses with LRU eviction policy.

To address this issue, the proposed dual cache has a local cache and a global cache that work together. The local cache stores local entities with LRU eviction policy, while the global cache stores global entities with LFU policy, which evicts the least frequently used entity when the global cache is full. The model scans the document from left to right, classifies whether a new mention belongs to an entity in the cache, evaluates the frequency of the new or updated entity, and adds it to the appropriate cache if qualified.

The dual cache was evaluated on four public benchmarks and performed better than the baselines even when they used unbounded memory. It also significantly reduced cache misses compared with a single cache. The performance gap between the baseline and dual cache was much larger for book-level documents. Overall, the dual cache has the highest performance-cost ratio and outperforms single cache methods.</sample>
    <sample id="159">大家好，我是库斯瓦尔·辛哈，我很高兴欢迎你们来到我们ACL 2023年的论文。语言模型的可接受性判断并不总是对上下文具有稳健性。这项工作是与John Gauthier、Aaron Mueller、Kanishka Misra、Karen Fences、Roger Levy和Adina Williams合作完成的。在这项工作中，我们重新审视了最小对 paradigms。最小对 paradigm 主要评估语言模型在可接受性判断方面的表现，这也可以包括语法如BLiMP、SyntaxGym或可接受性与刻板印象相关的判断，例如CrowS pairs。在这个最小对 paradigm 中，典型的方法是展示一个可接受的句子或一个语法正确的句子，然后展示一个可接受的句子或一个不语法正确的句子。希望模型将更多的概率分配给可接受的句子。当前的MPP流程并不能让我们评估模型在整个上下文窗口中的可接受性。如今，大型语言模型正在出现更长和更长的上下文窗口。因此，评估模型在整个上下文窗口中的可接受性至关重要。这就是我们在这里所尝试的事情。我们正在重新审视MPP流程，要求模型评估更长和更长的序列的可接受性。这就是我们的方法。我们所做的就是模拟这些更长的序列，通过重新创建句子来重新审视数据集本身。例如，这里我们从BLiMP数据集中选择典型的可接受性问题的句对。然后，我们从可接受的句子中提取可接受的句子，并将其作为前缀添加到可接受查询和不可接受查询中。我们也可以从相同的领域或数据集中选择不可接受的句子，这也可以用来测试模型的可接受性。我们还可以从完全 unrelated的领域，如维基百科中选择句子。这样可以告诉我们模型的可接受性判断是否受到任何上下文的影响，比如，上下文是否来自不同的数据集子集，或者是否完全无关到我们要查看的当前句子。那么模型表现如何呢？首先，我们看看维基百科句子，这些句子与当前查询对完全无关。在这里，我们发现MPP判断在任意上下文长度下相对稳定。我们将上下文长度增加到最大值，以充分利用OPT和GPT-2模型。我们看到橙色点线所示的MPP判断相对稳定。那么，当我们选择来自同一数据集的句子时又会怎么样呢？这里我们从BLiMP或SyntaxGym数据集中的可接受和不可接受领域选择或创建句子。在这里，我们看到，当我们在可接受查询和不可接受查询前面添加可接受或不可接受的前缀时，MPP判断要么显著增加，要么显著减少。但是，当我们选择与BLiMP或SyntaxGym中相同现象的句子时，我们看到MPP判断要么显著增加，要么显著减少，取决于选择的前缀是可接受还是不可接受。这种效果随着上下文长度的增加而变得更大，这可能会影响拥有大上下文窗口的新语言模型。为什么匹配前缀会对语言模型的判断产生如此大的影响？我们进行了一系列分析，其中我们尝试通过保持相关结构但向输入添加噪声来扰动输入句子。经过多次扰动后，我们发现这些噪声并没有使模型改变其MPP判断的轨迹。基本上，我们发现模型对扰动句子敏感，即当我们在可接受领域中扰动句子时，所有扰动都会导致类似的增加，而当我们在不可接受领域中扰动句子时，所有扰动都会导致类似的减少。我们的工作的主要 takeaway是，语言模型对共享跨句子的潜在句法和语义特征敏感。当前的MPP评估方式，即使用短且单个句子输入，可能无法充分捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以获取更多实验细节。谢谢收听。</sample>
    <sample id="160">该方法的第一步将输入词元映射到一个无序的多集词元。</sample>
    <sample id="161">CoScript 中包含了 55,000 个脚本。</sample>
    <sample id="163">DEPLAIN的最佳对齐方法是MASSalign。</sample>
    <sample id="164">弱监督学习的一个好处是，它比手动标注数据便宜得多。在弱监督学习中，我们使用弱标注源，如简单的启发式规则、知识库或低质量的众包，来标注数据。这些标注通常比人类标注便宜得多，但它们也很噪声，这意味着其中一部分标注可能是错误的。通过训练神经网络在弱标注数据上，我们可以节省标注成本，同时仍然可以得到一个性能良好的模型。</sample>
    <sample id="165">The paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations" by Wenting Zhao, a PhD student at Cornell University, presents an unsupervised learning method called LiPoR (Likelihood Learning with Posterior Regularization) for abductive reasoning in a closed-world setting. Abductive reasoning involves identifying plausible explanations that bridge the gap between a given context and an outcome. Current approaches to abductive reasoning rely on supervised methods, which require annotated plausible explanations, but these can be noisy and subjective.

LiPoR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context by marginalizing other possible explanations. However, this objective alone does not prefer plausible explanations. To address this, LiPoR introduces a regularizer that enforces mutual exclusivity among explanations. The regularizer is defined as the maximum between the entropy of P of Z given XY and the log of M, where M is the number of plausible explanations. When the entropy of P of Z given XY is larger than the log of M, it means there are more than M explanations receiving probability mass, and the entropy of P of Z given XY is minimized, preferring a subset of explanations.

The paper compares LiPoR to zero-shot models and the previous best unsupervised approach on the AlphaNLI dataset, outperforming all of them, including a strong zero-shot GPT-3 baseline, by over 4 absolute points in accuracy.</sample>
    <sample id="166">The presented work, "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text," addresses the challenge of image text reasoning tasks with linguistically complex text. Typical methods such as visual language models perform well on image sentence retrieval tasks but struggle with complex texts. The proposed method draws inspiration from the Divide-and-Conquer strategy and Dual-Process Theory to integrate analogical reasoning (System 1) and logical reasoning (System 2).

The first model in the proposed method is the Proposition Generator, which decomposes complex proposition texts into simple representations. The Visual-Linguistic Interactor performs visual-propositions' information interaction, similar to System 1. The Neural-Symbolic Reasoner, acting as System 2, integrates reasoning states and results of simple propositions to obtain the final solution of a complex proposition on images.

Experimental results show that the proposed method outperforms other baselines. Abolition experiments on the testing set verify the effectiveness of each module. Two cases demonstrate the proposed method's ability to present inference states and results in the middle step, indicating interoperable processing.

The suggestions include exploring neural symbolic calculation for improving compositional reasoning and planning in large language models. The Divide-and-Conquer strategy, similar to self-asking chain-of-the-thought, can be integrated with Dual-Process Theory to enhance problem-solving capabilities.</sample>
    <sample id="167">DEPLAIN-web 中的文档既采用了手动对齐方法，又采用了自动对齐方法。具体分配情况是：750 个文档中的一部分通过手动对齐方法进行了对齐，另一部分则通过自动对齐方法进行了对齐。</sample>
    <sample id="168">CoNLL++数据集是通过从2020年的Reuters新闻中收集数据并使用与CoNLL-2003相同的注释指南进行注释而创建的。</sample>
    <sample id="169">本文介绍了Google Translate团队对大型语言模型（LLM）在机器翻译中的应用进行的研究。研究评估了PaLM模型的翻译能力，该模型拥有540亿参数，是2022年发布的。研究使用最新的测试集，以避免与训练数据重叠，并将其与最佳系统进行了比较。研究结果表明，选择好的提示策略对LLM的性能有重要影响。在实验中，使用五种提示策略，即为每个句子标记源语言和目标语言，发现实际示例的质量比提示形式更重要。此外，研究还发现，PaLM的流畅度与最佳系统相当，但准确性存在差异，主要表现为省略错误。总体而言，PaLM的表现接近商业系统，但在准确性方面仍存在一些问题。</sample>
    <sample id="170">大家好，我是来自宾夕法尼亚州立大学的张宇森。今天我要向大家介绍我们团队的研究成果《XSemPLR：多自然语言和意义表示的跨语言语义解析》。语义解析是构建用户查询的语义表示的任务，例如SQL和Lambda Calculus。而跨语言语义解析则是将多种自然语言中的查询翻译成多种意义表示。如图所示，我们需要使用神经模型将多种自然语言中的查询翻译成SQL、Lambda或FunQL等。现有的跨语言语义解析模型分别在有限的任务和应用中提出和评估。例如，某些自然语言的覆盖范围很广，但中文却缺乏覆盖范围。Lambda Calculus也缺乏覆盖范围，或者只在某些神经模型上进行了评估。为了解决这些问题，我们提出了XSemPLR。我们提供了一个统一的数据集XSemPLR，用于在多种自然语言和意义表示上进行跨语言语义解析。它包含9个不同领域的数据集、5种语义解析任务、8种意义表示以及22种自然语言，涵盖15种语言家族。为了更好地评估我们的基准测试，我们考虑了六种训练和评估设置。第一种是“翻译-测试”设置，我们使用Google Translate API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，在训练英语模型时，我们使用英语查询进行训练，而在推理阶段，我们将德语查询翻译成英语，并使用训练好的模型预测SQL。我们还测试了单语模型，其中源语言与目标语言相同，例如德语到德语或英语到英语。我们还测试了单语小样本设置，即只使用10%的训练数据训练单语模型。我们还测试了多语模型，即使用所有语言的查询一起训练一个统一的多语模型。例如，我们将德语、英语和中文查询放在一起训练一个模型。在推理阶段，我们可以使用这个模型来翻译德语查询或中文查询等。我们还考虑了跨语言零样本和小样本转移。我们在一种源语言上进行训练，然后将其转移到另一种语言。因此，在训练过程中，我们使用英语查询或英语和德语的小样本查询训练一个统一的多语模型，以预测SQL输出。我们还发现许多有趣的结果。关于单语模型的分析，我们评估了两组模型，包括编码器-指针（Encoder-PTR），即多语言预训练编码器与指针解码器，例如XLM-R + PTR和mBERT + PTR。我们还评估了编码器-解码器模型，即多语言预训练编码器-解码器模型，例如mBART和mT5。我们发现编码器-解码器在所有九个数据集上取得了最佳性能。我们评估了mT5和XLM-R + PTR在多语种设置下的表现。我们发现编码器-解码器或编码器-指针可以通过在多种语言的混合中进行训练来提高性能。我们发现大多数主要自然语言都可以获得性能提升，但英语在七个数据集中性能下降，在三个数据集中性能提升。这种现象被称为“多语言 curse”。我们还比较了跨语言性能差距。蓝色线代表跨语言小样本转移，橙色线代表跨语言零样本转移，绿色线代表单语设置。我们发现，通过比较绿色和橙色线，我们发现在零样本设置下，跨语言转移性能差距显著。然后通过比较蓝色和橙色线，我们发现使用小样本设置后，转移差距迅速缩短。我们还发现了其他一些有趣的发现。例如，编码器-解码器优于以往的工作或实现了可比的结果。在英语自然语言上的预训练可以显著提升小样本设置下目标自然语言的性能。我们还发现，像Codex和BLOOM这样的多语言语言模型对于跨语言语义解析任务来说仍然远远不够。总之，我们构建了XSemPLR，这是一个统一的跨语言语义解析基准测试，涉及多种自然语言和意义表示。我们对三种代表性多语言语言模型进行了全面基准测试。我们的结果展示了许多有趣的发现。等等。欢迎访问我们的论文和代码。谢谢各位的聆听。</sample>
    <sample id="171">关于保护大型语言模型的版权，现有研究可以大致分为四类。然而，这些方法要么不适用于嵌入服务，要么缺乏可转移性。因此，在本文中我们提出了“嵌入标记”，一种基于后门的水印方法，适用于嵌入服务。</sample>
    <sample id="172">根据演讲者的说法，多语言LLM如Codex和BLOOM对于CLSP任务来说还不足。</sample>
    <sample id="174">Thea, one of the co-authors of the paper "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis," provides an overview of the unique features of this dataset in a video. Argument quality analysis involves judging how good or bad an argument is on a scale from 0 to 1, with coherent and persuasive arguments receiving higher ratings.

Current datasets often lack diversity, depth, and quality due to being collected from crowdsourcing platforms, consisting of only a few motions, and having a motion associated with every single argument. ArgAnalysis35K addresses these issues by being the largest dataset with high-quality arguments, featuring around 85% of arguments sourced from high-quality tournaments, expert debaters, intermediate debaters, and novice debaters.

The dataset also has a diverse range of arguments, covering 24 themes based on experience, websites like Hellomotions.com, and expert advice. It introduces the concept of analysis, which combines claims, premises, and other elements to explain arguments better. This approach requires annotators to provide more comprehensive explanations, enhancing the reliability of the dataset.

An instance-based annotator reliability model is introduced to account for human biases in judgments. Instead of eliminating entire judgments from annotators who may be biased, this model selectively eliminates biased judgments, allowing for better utilization of annotations.

Lastly, the relevance model assigns scores from 0 to 1 for each argument's relevance to a theme, capturing its applicability across different contexts. This feature enhances the dataset's utility in various applications.

Overall, ArgAnalysis35K offers a more diverse, reliable, and comprehensive dataset for argument quality analysis, making it a valuable resource for researchers and practitioners in the field.</sample>
    <sample id="175">该方法通过在训练过程中引入排列的不确定性来处理排列的不确定性。具体而言，对于给定的输入标记，我们不知道它来自哪个多集标记，这使得在训练过程中确定正确的排列变得困难。此外，有时存在多个与数据一致的排列，但正确的排列是潜在的。我们通过在训练过程中引入排列的不确定性来解决这个问题。我们的排列方法非常灵活，但找到最高得分排列是一个NP-hard问题，因为它与“旅行商”问题有关。我们通过使用GPU友好的连续放松来近似这个问题，这还允许我们通过解决方案反向传播并学习更有可能的排列。</sample>
    <sample id="176">在 NLP 模型的背景下，公平性通常指的是模型在处理不同群体或个体时的一致性和公正性。这意味着模型应该能够以相同的方式对待所有输入，无论它们的性别、种族、年龄、政治观点或其他敏感属性如何。公平性还意味着模型应该避免对某些群体产生偏见或歧视，例如，在分类任务中，模型应该以相同的准确率预测所有群体的类别。此外，NLP 模型的公平性也可能涉及模型的可解释性，即能够理解模型的决策过程，并确保模型不会基于不相关或不道德的因素做出决策。</sample>
    <sample id="177">演讲者的名字是Yanis Labrak。</sample>
    <sample id="178">演讲者的名字是Koustav Sinha。</sample>
    <sample id="179">The presentation by Melanie Sclar focuses on the development of a method to improve theory of mind reasoning skills in large language models (LLMs). Theory of mind is the ability to reason about the mental state of others, which is traditionally measured in humans and LLMs through reading comprehension tasks involving multiple characters. False-belief questions are used to probe understanding, where reality may not match the belief of certain story characters. The Sally-Anne test is a classic example of a theory of mind test.

The research question addressed is "How can we improve Theory of Mind reasoning skills in Large Language Models?" The proposed solution is SymbolicToM, an inference-time method that uses explicit graphical representations to improve theory of mind reasoning skills in LLMs. SymbolicToM computes graphical representations for all combinations of characters up to a predefined maximum theory of mind level. These graphs are computed using an inference-time algorithm that leverages off-the-shelf natural language inference (NLI) and open information extraction (OpenIE) models.

The experiments conducted tested the method with various LLMs and compared it against supervised baselines, specifically a fine-tuned GPT-3 model and Textual Time Travel, which is a model designed for theory of mind reasoning. The results showed performance gains across the board, with 65 accuracy points gained for GPT3-Davinci, 67 for Macaw, and 51 for Flan-T5-XXL among many others. The method also demonstrated robustness in out-of-domain setups, such as storage structure generalization and linguistic generalization.

In conclusion, SymbolicToM is a plug-and-play method that improves theory of mind reasoning skills in large language models. It is an inference-time algorithm that avoids overfitting risk, uses explicit graphical symbolic representations, and yields more interpretable reasoning. SymbolicToM dramatically improves out-of-the-box LLM performance and outperforms supervised approaches on out-of-domain story understanding, remaining beneficial on new linguistic diversity datasets.</sample>
    <sample id="180">演讲者的名字是Myra。</sample>
    <sample id="181">The paper "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" by Siyu Yuan and colleagues from Fudan University presents a study on the problem of constrained language planning, which involves generating step-by-step instructions that are faithful to specific constraints. The authors evaluate and improve the constrained language planning ability of large language models using a dataset of 100 specific goals with multi-faceted constraints. They find that the semantic completeness of generated scripts is acceptable, but the faithfulness to the constraints cannot be guaranteed. To address this issue, they propose an over-generate-then-filter method that selects the most faithful scripts based on semantic similarity and keyword presence. The authors also develop a dataset named CoScript, which consists of 55,000 specific goals with scripts, and fine-tune a smaller model (T5) on this dataset to generate higher-quality scripts than most large language models. The CoScript dataset is intended to be a valuable resource for advancing research on language planning.</sample>
    <sample id="182">在本文的背景下，热带主义 (tropicalism) 指的是与拉丁妇女相关的刻板印象，这些妇女被描述为充满活力、充满活力和曲线玲珑。这种刻板印象反映了拉丁妇女被描绘为异国情调和异国情调的方式，这可能导致她们被边缘化和刻板印象化。</sample>
    <sample id="183">作者通过使用自然语言提示来创建目标群体的人工描写。他们使用这些提示来生成一个描绘一个想象中的个体的描述，例如“想象一下你是一个亚洲女性。描述一下自己。”这种方法可以轻松地适应任何人口统计学，只需指定所需的身份标记即可。</sample>
    <sample id="184">在本文中，语境使用情况是通过CXMI（Contextual Mutual Information）来衡量的。CXMI是一种衡量给定上下文C时模型对目标Y的信息量的指标。在本文中，作者扩展了CXMI以测量句子或单词级别的语境使用情况，并将其称为点积CXMI（Pointwise CXMI）。</sample>
    <sample id="185">DrBERT 和 ChuBERT 的主要区别在于数据来源。DrBERT 是基于从网络上抓取的医疗数据集 NACHOS 训练的，而 ChuBERT 则是基于匿名化的数据，来自南特大学医院数据仓库。</sample>
    <sample id="187">根据演讲者的介绍，这篇论文有两位作者：Ying和Zhiyang。</sample>
    <sample id="188">迭代迁移学习是指在训练模型时，先从一个相关的任务（例如CE任务）开始，然后在每个迭代中进一步微调模型以处理另一个相关的任务（例如 debate 任务）。这种方法有助于提高模型在目标任务上的性能。</sample>
    <sample id="189">数据集的目标是收集一个更大规模的公开数据集，用于解决用户语言中选择实体时的间接引用问题。这个数据集将有助于评估和改进大型语言模型（LLMs）在理解实体方面的能力。</sample>
    <sample id="190">攻击者可以通过学习 EaaS 提供的嵌入来提取模型参数。</sample>
    <sample id="191">这篇论文有三位作者：Sara Papi、Matteo Negri和Marco Turchi。</sample>
    <sample id="192">The presentation by Yang Luo introduces a new optimizer called CAME (Confidence-guided Adaptive Memory Efficient Optimization) that aims to address the challenge of balancing fast convergence and low memory usage in training large language models. The presenter highlights the limitations of existing adaptive gradient-based optimizers like Adam, which require triple the memory for keeping first and second moment estimates of per-parameter gradients. On the other hand, memory-efficient optimizers like Adafactor reduce auxiliary memory usage but at the cost of performance.

To tackle this challenge, the presenter explains the concept of non-negative matrix factorization (NMF), which reduces memory requirements from O(mn) to O(m+n). They also discuss how Adafactor presents an analytic solution to achieve the minimum I-divergence between the matrix V and the approximation matrix W x H in the special case of rank-1 factors. However, NMF operations in Adafactor can lead to erroneous updates during deep neural network training, causing slow convergence compared to Adam.

To handle these erroneous updates, the presenter proposes an efficient approach to decrease the side effect caused by insecure updating. This involves calculating the instability matrix (uₜ) and using its square root as the denominator for mₜ to take an optimization step. The presenter then performs experiments on BookCorpus and English Wikipedia, comparing CAME with existing optimizers like Adam and Adafactor. The results show that CAME achieves significant improvements in validation accuracy and is more effective in pre-training very large models with a huge reduction in memory cost.

The presentation concludes by highlighting the efficiency of CAME in large batch training, demonstrating its effectiveness on large language model training tasks and its ability to work well for large batch training, extending the capabilities of existing memory-efficient optimizers.</sample>
    <sample id="193">没有提供有关用于创建初始数据集的注释者数量的信息。</sample>
    <sample id="194">该论文的作者来自卡内基梅隆大学、University of Washington和Allen Institute for AI。</sample>
    <sample id="195">The presented work, "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering," introduces a novel framework called RoHT (Reasoning over Hierarchical Question Decomposition Tree) to address the challenges in Explainable Question Answering (XQA). XQA aims to not only provide an answer to a given question but also explain why the answer is selected. The current approaches in XQA can be broadly categorized into neuro-symbolic methods and decompose-based methods. Neuro-symbolic methods translate natural language questions into formal representations like SPARQL, which can only be executed on structured knowledge bases, leading to limited recall of answers. Decompose-based methods generate intermediate steps in natural language to lead to the final answer, but they rely solely on free-text corpora as knowledge sources, making it difficult to handle diverse natural language inputs.

RoHT addresses these limitations by integrating knowledge from heterogeneous sources and leveraging question decomposition. The framework consists of two stages: firstly, building a Hierarchical Question Decomposition Tree (HQDT) to understand the hierarchical compositional structure of a complex question, and secondly, performing probabilistic reasoning over the HQDT to fuse knowledge from various sources at different levels. The HQDT is constructed by using a question decomposer to generate atomic questions (leaf nodes), then generating intermediate questions based on grouped leaf questions according to their reference tokens. A certainty score is computed for each node to represent its generation likelihood.

During the reasoning process, a scheduler determines the appropriate knowledge sources for each node, which can include a knowledge base (KB), a text corpus, or solving its children recursively and sequentially. Corresponding executors retrieve answers with probabilities from the selected sources, and an aggregator combines candidate answers from all sources to output the top key answers with the highest probabilities.

The framework was evaluated on two datasets: KQA Pro and Musique. On KQA Pro, RoHT outperformed existing KB QA methods when using only the incomplete KB and showed substantial improvement when Wikipedia was added as a supplementary text corpus. On Musique, RoHT improved F1 by 11.9% compared to the SOTA method EX(SA) when using only the given text paragraphs, and further improvements were observed when both text and KB were used.

Overall, RoHT demonstrates the effectiveness of integrating knowledge from KBs and text corpora and the benefits of explicit decomposition in handling complex questions intuitively.</sample>
    <sample id="196">一个以左侧为支配词的示例是 "I saw Bart and Lisa"。在这个例子中，动词 "saw" 是支配词，它位于两个名词 "Bart" 和 "Lisa" 的左侧。</sample>
    <sample id="197">对话系统中的最先进模型是四个最先进聊天模型之一。</sample>
    <sample id="198">我们需要在整个上下文窗口中评估模型的可接受性，因为大型语言模型的上下文窗口正在变得越来越长。通过评估模型在整个上下文窗口中的可接受性，我们可以更好地了解模型在处理更长句子时的表现，并确保模型能够准确地判断句子的可接受性，无论其长度如何。</sample>
    <sample id="199">是的，与单语英语模型相比，多语言训练会导致表现下降。在研究中发现，在所有九个数据集中，除了三个数据集外，英语性能在多语言训练下会下降。这种现象被称为“多语言 curse”。</sample>
    <sample id="200">注释者提前知道实体，但可能不知道实体的具体细节。在数据集中，注释者被要求听或阅读实体的背景知识，以帮助他们选择正确的实体并提供间接引用表达。</sample>
    <sample id="201">该研究使用了神经机器翻译（MT）指标和专家基于人类评估的结果。</sample>
    <sample id="202">根据所提供的信息，回归不会影响特定的NER类型。该研究发现，回归是由于时间漂移造成的，而不是由于自适应过拟合造成的。</sample>
    <sample id="203">NLP 中的立场很重要，因为它会影响数据集和模型的性能。这些数据集和模型可能代表某些群体而不是其他群体，导致系统性性能差异。这可能导致技术在不同人口中的敏感性和准确性存在差异。因此，了解 NLP 中的立场对于确保技术公平和包容性至关重要。</sample>
    <sample id="204">根据所提供的英文内容，像 BLOOM 这样的多语言 LLM 是采用完整微调。</sample>
    <sample id="205">Shangbin, a PhD student at the University of Washington, presented their research on political biases in language models. They found that language models are trained on large-scale web crawl data, which includes diverse perspectives from various news media outlets. However, these different political opinions can lead to potential fairness issues in downstream tasks. The researchers investigated the pipeline of political bias propagation from pretraining data to language models and downstream tasks. They evaluated the political leaning of language models using political questionnaires and found that they occupy all four quadrants on the political spectrum, with GPT-4 being the most liberal. They also conducted controlled experiments by pretraining language models on partisan corpora and observed shifts in ideological coordinates. Furthermore, they examined the performance of language models with different political leanings on hate speech detection and fake news detection, revealing patterns based on social categories. The researchers highlight the dilemma of sanitizing political opinions in training data, as it risks censorship or exclusion, and emphasize the need to acknowledge and tackle fairness issues resulting from language model political leanings.</sample>
    <sample id="206">他们使用了两种与认知不一致相关的任务的模型进行迁移学习：主题独立的认知不一致观点分类和 PDTB 扩展和比较类别的二元分类。</sample>
    <sample id="207">在评估 PaLM 的能力时，我们使用了最新的测试集，以避免测试数据与语言模型的训练数据重叠。</sample>
    <sample id="208">作者提出了三条建议。</sample>
    <sample id="209">根据所提供的英文内容，无法确定与最强的基线相比，建议的方法获得了多少收益。</sample>
    <sample id="210">演讲者的名字是Shuheng。</sample>
    <sample id="211">是的，论文中的结果和数据集可以作为未来自动文本简化问题的基准。</sample>
    <sample id="212">在论文中，作者进行了T5模型的实验。他们使用CoScript数据集对T5模型进行微调，并发现它生成的脚本的质量高于大多数大型语言模型。</sample>
    <sample id="213">OF-A模型被用作研究多模型指令调整的基础模型。</sample>
    <sample id="215">This talk by Adam Przepiórkowski focuses on the dependency structure of coordination in different linguistic theories and corpus approaches. The universal dependencies and Igor Mel'čuk's meaning text theory assume asymmetric structures, where the first conjunct is the head of the whole coordinate structure. In contrast, the Prague approach uses a conjunction-headed approach, while Hudson's Word Grammar employs a multi-headed approach.

Przepiórkowski argues for symmetric structures of coordination based on the principle of dependency length minimization. He explains that in English, direct objects prefer to be close to the verb, while adjuncts may be further away. However, when the direct object is long, it can be moved after the adjunct, satisfying the principle of dependency length minimization.

The speaker extracted statistics from the enhanced version of the Penn Treebank and observed that left conjuncts tend to be shorter, especially when the difference in length between the two conjuncts grows. This tendency only occurs when the governor is on the left or absent. When the governor is on the right, this effect disappears.

The talk concludes with a discussion of the implications of these findings for asymmetric and symmetric structures of coordination, and invites further discussion at the poster session.</sample>
    <sample id="217">The paper "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" by Weihao Zeng, Lulu Zhao, and Keqing He from Beijing University of Posts and Telecommunications introduces a method for generating controllable dialogue with multiple attributes. The authors address the limitations of previous methods that focus on single attributes or use specific labels for continuous attributes. They propose DCG (Disentangled Controllable Generation), which learns attribute concepts from seen values and uses disentanglement loss to disentangle different attribute combinations. A unified evaluation framework, MAE (Multi-Attribute Evaluation), is introduced to assess the performance of the model at different granularities of attributes.

The authors establish two benchmarks and prove the effectiveness of their method through experiments. Their models are based on the DialoGPT framework with a compositional prompt module. Two types of prompts are designed: attribute-oriented prompts that guide the model's focus on specific information in the dialogue, and task-oriented prompts that capture instance-independent global features. Pseudo combinations are used to enhance the diversity of the prompts, and a disentanglement loss is introduced to train multiple compositional prompts while disentangling combination representations.

The proposed method outperforms all other baselines in attribute controllability and text equality. The authors test their model with attribute-oriented prompts, task-oriented prompts, and disentanglement learning, showing that attribute-oriented prompts guide the model to focus on controllable information, task-oriented prompts improve text equality, and disentanglement learning improves the ability of compositional generalization. The method successfully tackles the challenges of compositional generalization for multi-attribute controllable dialogue generation with only a small drop on E-ACC and A-ACC metrics. Additionally, the method outperforms CTRL on both controllability and text equality of unseen attribute combinations.

The authors evaluate the quality of different metrics using three correlation coefficients, including their automatic metric, MAE. Compared to human judgments, their method outperforms classic metrics for both coarse-grained discrete attributes and fine-grained continuous attributes. They also demonstrate the impact of prompts on compositional generalization with a visualization of the concatenated prompt embeddings of three attributes via PCA on DailyDialog-CG. This result proves that their method can disentangle attribute combinations and learn the relations between different attributes with the ability to generalize from seen attributes to unseen combinations. The proposed attribute-oriented prompt method outperforms models that learn an independent prompt for each attribute value, as the shared embedding mapping helps learn attribute concepts from seen values to unseen combinations.</sample>
    <sample id="218">该论文的作者来自Google Translate。</sample>
    <sample id="219">Jia-Huei Ju, a research assistant at Academia Sinica, presented their work on "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports." The presentation covered the background of financial report analysis, the goal of the work, and the task definition and approaches. The team considered Form 10-K as their target corpus, which is an annual report required by the SEC. They introduced a highlighting task and a multi-stage pipeline to uncover useful information from these reports.

The team defined reference-to-target structures in their task, where the target is the report of interest and the reference is the report from the previous year. The goal of the highlighting task was to find the rationale (words) of relations between a given pair, T and R. The model predicted word importance, allowing them to measure performance. For example, the word "decrease" was supposed to have higher importance in this context.

The proposed pipeline consisted of four stages: document segmentation, relation recognition, out-of-domain fine-tuning, and in-domain fine-tuning. The team used an external dataset, eSNLI, for out-of-domain fine-tuning, which is a natural language inference dataset with token annotation. For intermediate fine-tuning, they used revised pairs, the revised words as pseudo positive labels, and randomly labeled a few other scores as negative. They mixed different objectives using soft labeling techniques by mixing cross-entropy laws and KL divergence to alleviate the problem of low-quality pseudo-labels.

The evaluation dataset included eSNLI pairs and their released FINAL dataset. They used two metrics to judge performance: precision over recall and correlation between prediction and annotations. Their domain-adaptive highlighting model achieved the best performance on FINAL and preserved generalization capability. They also observed that their methods benefited on simulation with mismatched pairs, which were not used during training.

In conclusion, the team proposed a highlighting task with their released FINAL dataset and a simple pipeline with two-stage fine-tuning. They mentioned future works, including improving effectiveness or adding more features or techniques in information retrieval to enhance application.</sample>
    <sample id="220">论文的作者是石溪大学计算机科学系的博士生。</sample>
    <sample id="221">论文分析了德语到英语的语言对。</sample>
    <sample id="222">This work focuses on the challenges and interventions for domain adaptation in open-domain question answering (QA). The authors investigate different data interventions to enable out-of-domain generalization, identify the type of dataset shift a new domain exhibits, and determine effective data interventions for specific types of shifts. They use a general-purpose Wikipedia-based source domain to train both retriever and reader models and test the generalizability of the source model across seven target datasets spanning six different domains.

The authors explore two overarching methods for generating data interventions: zero-shot and few-shot. Few-shot methods use a few examples from the target domain to prompt large language models for generating more examples, while zero-shot techniques control the interactions among three random variables in open-domain QA by keeping two variables fixed while varying the other one in a controlled manner. They observe that few-shot methods improve retriever performance by 8% on average and reader performance by 11% on average.

To ascertain the nature of incompatibility between the target model and domain, the authors consider existing data shift taxonomy in machine learning. They compute compatibility measures for both retriever and reader models using fixed question/answer and context triples from target datasets. The results show that datasets like CliCR and NewsQA exhibit full shift, while SearchQA exhibits no shift. Target sets respond well to few-shot adaptations for datasets with full shift, while concept and covariate shift respond well to zero-shot adaptations.

Overall, the work demonstrates that a variety of data interventions can improve reader performance by up to 24%, and only certain types of data interventions are effective based on the type of shift a target dataset exhibits.</sample>
    <sample id="223">演讲者的名字是Shangbin。</sample>
    <sample id="224">在实验过程中研究了两种模型：长MBART和标准MBART。这些模型被微调以产生简化文本，其中长MBART用于文档级别的简化，标准MBART用于句子级别的简化。</sample>
    <sample id="225">在 MultiInstruct 中使用的 62 个不同任务中，有 53 个任务用于训练目的，10 个任务用于测试目的。</sample>
    <sample id="226">根据所提供的内容，无法确定论文的作者人数。</sample>
    <sample id="227">Grounded language understanding is a challenging task in natural language processing (NLP) that involves mapping a natural language expression into something that can be executed over a specific target environment, such as a plan or a program. This task has many applications, including smart assistants like Siri and Alexa, semantic search on Google, querying a medical database using natural language, and domestic robots that follow natural language instructions.

The main reason grounded language understanding is challenging is the lack of grounding during pre-training. Most language models are pre-trained with textual corpus without grounding, which makes it difficult to map a natural language expression onto a representation in a specific environment. Existing research on grounded language understanding tasks typically uses language models to directly generate a plan via autoregressive decoding, but this approach often results in plans that may not always be grammatical or valid.

To address this challenge, the proposed framework for grounded language understanding separates the newer world and the symbolic world. In this framework, a symbolic agent interacts with the environment and proposes candidate plans, while a language model is used only to score and rank the candidates proposed by the symbolic agent. This approach allows the language model to focus on discrimination instead of generation, making it easier to excel at grounded language understanding.

The framework has been tested on knowledge-based question answering, which is a typical scenario with a massive, heterogeneous environment for grounded language understanding. The results show that Pangu achieves outstanding performance across all settings, including fine-tuning and in-context learning. Pangu also demonstrates strong sample efficiency, outperforming all other settings when using Codex with in-context learning. Additionally, Pangu's strong generalizability under non-i.i.d settings is observed, as autoregressive models like ArcaneQA tend to overfit seen structures during training, while Pangu's distributions of probability are almost the same for both seen and unseen structures.

Overall, the proposed framework suggests that discrimination might be a better strategy for grounded language understanding than generation. The authors are open to discussions and collaborations and welcome feedback on their work.</sample>
    <sample id="228">作者在实验中使用了AG News、MIND、SST2和Enron Spam四个数据集。</sample>
    <sample id="229">The presented paper by Gabriella Skitalinskaya and Henning Wachsmuth focuses on detecting improvable claims for argumentative writing support. The authors explore the challenges of working with revision-based data, as different domains have different goals, notions of quality, and revision types performed. They exclusively focus on argumentative text and explore how to best model the quality of argumentative text based on implicit revision patterns found in collaborative online debate platforms such as Kialo.

The paper delineates four main challenges originating from the nature of revision-based corpora and from the notion of argument quality. These challenges are representativity and reliability, model complexity and architecture, contextual information, and topical and user bias. The authors present a detailed analysis of the strengths and weaknesses of strategies tackling each challenge and a systematic comparison of approaches for the introduced tasks.

Based on their experiments, the authors conclude that revision-based data can be employed effectively for the given tasks. Modeling the distance between two claimed versions is beneficial for detecting suboptimal claims. The impact of contextual information is dependent on both the task and the quality issues a text is suffering from. The paper provides further details and findings for those interested in learning more about the topic.</sample>
    <sample id="231">NACHOS 是一个数据集，其中包含从网上爬取的医学信息，用于训练 DrBERT 模型。</sample>
    <sample id="232">演讲者的名字是David Vilar。</sample>
    <sample id="233">The paper "Attention as a Guide for Simultaneous Speech Translation" by Sara Papi, Matteo Negri, and Marco Turchi from the University of Trento and Foundazione Bruno Kessler proposes a solution to the problems of current simultaneous speech translation (SimulST) models. These models often require long and complicated training procedures, involving different optimization objectives, and training several models to reach different latency regimes. The authors suggest using existing offline speech-to-text (ST) models without re-training or adopting specific architecture for SimulST. They propose EDAtt, Encoder-Decoder Attention, which decides whether to emit or not a partial translation based on where attention points to. A word is emitted if the attention is not concentrated, meaning its sum is below a certain threshold alpha towards the last lambda speech frames. The paper compares EDAtt with popular strategies applied to offline models, such as the Wait-k strategy and Local Agreement, and with state-of-the-art architecture specifically tailored for simultaneous pre-translation. The results show that EDAtt outperforms all the strategies applied to offline models since the curves are shifted over the left. Additionally, EDAtt is the fastest strategy when considering actual elapsed time or computational-aware time. The authors have released open source code and models to facilitate the reproducibility of their work.</sample>
    <sample id="234">提示策略对结果有重大影响。在实验中，使用不同的提示策略，即一、两和五种提示，观察到的差异超过一个BLEURT点。在极端情况下，差异可达到40个BLEURT点。因此，选择好的提示策略对于获得更好的翻译结果至关重要。</sample>
    <sample id="235">根据提供的信息，无法确定论文作者的机构。</sample>
    <sample id="236">Each task in the MultiInstruct dataset is equipped with five expert-written instructions.</sample>
    <sample id="237">作者建议通过使用一个核心参考任务来测试模型，该任务旨在测试模型在不同来源中提取信息的能力。他们提出了三个设置：背景-预训练、背景-Both和背景-推理，以评估模型在不同来源中使用信息的能力。</sample>
    <sample id="238">Yebowen Hu from the University of Central Florida presents a new benchmark dataset called MeetingBank, which addresses the challenge of developing summarization technologies for different reading domains. The dataset includes 1,366 City Council meetings and nearly 7,000 instances, with meeting transcripts, reference summaries, and other useful resources. Data collection involved converting audio data to transcripts using Speechmatics API, identifying meeting types and data from meeting websites, and aligning timestamps to get the second transcript paired with the extracted summary.

The dataset statistics include the number of meetings, meeting duration, number of tokens per meeting, number of speakers per meeting for each city, and the year period of meetings collected in the datasets. Summarization instances gathered for each city are also provided, along with average numbers of sentences and tokens in both source and summary texts. The level of abstraction in meeting summaries is measured by two common measures: coverage and density.

For model evaluation, top-tier summarization systems were evaluated on the test set of MeetingBank, including Oracle, LEAD, LexRank, TextRank, BART-Large, Pegasus, Longformer, DialogLM, and HMNet. GPT-3 was also evaluated using zero-shot summarization prompting with Davinci-003. Human evaluation assessed the quality of system-generated summaries based on informativeness, factuality, fluency, coherence, and redundancy. GPT-3 achieved the highest overall scores in terms of fluency and coherence but performed less impressively in terms of informativeness and factuality.

The primary contribution of MeetingBank is that it serves as a useful tool for researchers to design advanced meeting summarizers and provides insights into the decision-making process of City Council.</sample>
    <sample id="239">大家好，我叫David Vilar，我将对Google Translate团队同事共同发表的论文《评估策略和性能：使用PaLM进行翻译提示》进行简要回顾。PaLM是一个540亿参数大规模语言模型，于2022年推出。它是在包含7800亿个标记的大量文本上训练的。在发表时，它在 hundreds of NLP任务中取得了State-of-the-Art的表现。在这项工作中，我们首次系统地研究了大规模语言模型的提示策略在机器翻译中的应用。我们使用MT社区的最佳实践评估了这些模型的翻译能力，即使用最新的测试集以避免测试数据与语言模型的训练数据重叠。我们还将其与MT领域的最佳系统进行了比较，即WMT评估中的最佳系统。我们使用神经MT指标和专家基于人类的评估结果。最后，我们提供了一些关于提示选择策略的建议。提示策略对LLM的翻译性能有重大影响，如我们在一个简单的实验中所见，其中我们为每个句子提供了两个不同的提示，结果观察到的差异超过一个BLEURT点。在极端情况下，这个差异可达40个BLEURT点。因此，选择好的提示策略非常重要。在我们的实验中，我们采用了五-shot提示策略，即将每个句子标记为其所属的语言。例如，在从德语翻译成英语的例子中，德语句子（源句子）用德语冒号标记，而英语翻译用英语冒号标记。我们发现，提示的实际形式对几例短提示来说影响不大。然而，在零-shot和一shot提示中，实际形式至关重要。而在我们案例中采用的五-shot提示中，实际形式几乎不影响性能，关键在于示例的质量。因此，选择高质量的翻译示例非常重要。我们比较了从训练数据和WMT评估的开发数据中选择提示。开发数据比训练数据更精致，质量更高。因此，使用开发数据可以取得更好的性能。然而，专门化的最佳系统仍然在PaLM翻译中具有显著优势。但在我们的情况下，我们选择使用Google Translate进行评估。我们通过MQM框架进行的人类评估表明，PaLM的流畅度与最佳系统相当，但主要区别在于准确性。特别是最常见的错误是省略错误。似乎PaLM选择生产一个听起来更好的翻译，有时会省略源句子中的部分内容。然而，“风格/笨拙”类别中PaLM的评分低于最佳系统，这进一步证明PaLM提供了流畅的输出，但仍然存在一些准确性问题。这就是本文的简要概述。如需更多细节，请参阅论文的完整演示。谢谢大家。</sample>
    <sample id="240">Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I'd like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for Weakly Supervised Learning, a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there's a catch, which is that people do assume that there's an additional clean validation set available for model selection. We can't stop on this problem setting, but this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room this necessity is often overlooked. The aforementioned doubt is asked to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that, interestingly, recent WSL methods indeed require clean validation samples to work properly. Otherwise, there is a large performance drop. As shown in this figure, if there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically we only need 20 samples per class to attain high performance. But that's not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model, termed FTw, initially underperforms more complicated WSL methods, like COSINE. However, if we allow to continue fine-tuning on the clean samples, then FTw performs equally well as other methods. So in practice, there's no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria. For example, report if the model selection is done via clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.</sample>
    <sample id="241">The paper "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" by Ethan, Yang Chen, Wei Xu, and Alan Ritter at Georgia Tech addresses the limitations of current misinformation detection systems. These systems are often evaluated using retrospectively constructed datasets and fail to involve human content moderators in the process. The authors propose an evaluation framework that involves humans at various stages of the process, making it more realistic and representative of real-scale social media platforms.

The proposed system has two main components: claim detection and policy violation verification. The first component uses a T5 model trained for question answering to extract claims from raw tweets. The claims are then ranked by trendiness before being provided to humans for verification. The second component uses a BERT-based stance classification model to determine the author's stance towards unapproved treatments and flag them for human review.

The paper evaluates the system's efficacy in detecting early misinformation and policy violations. The results show that the system has a high position with regards to policy violation detection and can detect 124.2 policy violations per human hour worked. The authors hope that their work will motivate the development of future human-in-the-loop misinformation detection systems and provide outsiders an out-of-industry look at the development and evaluation of misinformation detection systems.</sample>
    <sample id="242">对话系统的常用评估方法是通过让人类裁判选择两个对话中哪个更好或使用李克特量表来评估对话的整体质量。</sample>
    <sample id="243">这篇论文有五位作者。</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，需要的背景知识是“Servin 是一名法官”和“Kea 是一名面包师”。</sample>
    <sample id="245">The presentation by Lining Zhang and co-authors, "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization," introduces a two-step pipeline for identifying high-agreement Amazon Mechanical Turk (MTurk) workers. The pipeline aims to address the limitations of automatic metrics and the lack of best practices in worker recruitment on MTurk.

The first step involves setting pre-task qualifications for workers, such as location, HIT count, and approval rate. The qualification task tests annotators' ability to evaluate multiple dimensions correctly, categorizing them into gold, silver, bronze, and block. Only gold and silver workers can proceed, resulting in 26 qualified workers, with 8 gold and 18 silver.

The second step is the endurance task, which assesses annotators' capacity to handle heavy workloads. This results in 12 qualified workers, with 4 gold and 8 silver. These workers achieve high inter-annotator agreement (IAA) compared to experts, with the best Cohen's Kappa at 0.443 and the best Krippendorff's Alpha at 0.534.

The reference-based task evaluates general performance on true annotation tasks, with 8 out of 12 workers completing all HITs. The Krippendorff's Alpha for this task is 0.534. Baseline MTurk workers using the MACE statistical filter achieve a Krippendorff's Alpha of 0.380 but have incomplete HIT coverage and fewer workers per HIT. CloudResearch MTurk workers have a higher Krippendorff's Alpha of 0.513 but lower task acceptance rates.

The analysis of correctness across annotation sources shows significant Spearman's correlation between Pipeline and CloudResearch workers. Real GPT models correlate well with expert judgments. The study concludes that pre-task filtering can avoid resource waste and achieve high agreement at a lower cost, serving as a best practice for high-agreement annotations at scale. Future research will focus on hiring high-quality workers and exploring applications across tasks, languages, and platforms. Limitations include only testing English summarization on MTurk and the non-ideal design of the questions.</sample>
    <sample id="246">是的，代码公开。您可以在GitHub上找到数据集和代码。</sample>
    <sample id="247">The paper presents a new task called Knowledge Graph-Based Fact Verification, which utilizes knowledge graphs as evidence for fact verification. The authors introduce a new dataset named FactKG, which consists of claims in both written and colloquial styles, and two labels: SUPPORTED and REFUTED. The dataset includes five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The authors also propose two methods for generating colloquial style claims: the colloquial style transfer model and presupposition templates. They construct baselines using only the claims to verify without graph evidence and utilize the GEAR model to verify the claim using correct evidence. The results show that all baselines outperform the majority class baseline, and the GEAR model that uses graph evidence outperforms all other baselines.</sample>
    <sample id="248">根据 Jenny 的演讲，NLPositionality 的注释者在各人口统计学特征方面并不均衡。例如，在 GPT 4 社交接受度分析中，数据集和模型最接近于英语国家。同样，在 Dynahate 分析中，GPT 4 在社交接受度任务中的表现与受过大学教育的人最接近。此外，研究还发现，在 Dynahate 任务中，数据集和模型对非二元性别群体的代表性较低。</sample>
    <sample id="249">为了扰乱句子，我们尝试通过保留相关结构同时向输入添加噪声来修改句子。</sample>
    <sample id="250">维度评估意味着通过明确标注聊天模型的行为，如提供无关信息、自相矛盾或违反共同知识等，减少人类评估的主观性。</sample>
    <sample id="251">论文的作者 Jingwei Yi 所属机构是中国科学技术大学。</sample>
    <sample id="252">本演示介绍了名为U-CREAT的论文，即“Unsupervised Case Retrieval using Events extrAcT”，该论文是作者Sai Kiran Tanikella与Abhinav Joshi、Akshat Sharma和Ashutosh Modi合作完成的。该研究针对法律专业人士在处理大量案例时面临的挑战，提出了Prior Case Retrieval（先例检索）任务。该任务涉及从候选文档池中检索与查询文档相关的相关文档。本研究对Prior Case Retrieval领域做出了两个重要贡献：IL-PCR数据集和U-CREAT管道。

IL-PCR数据集是一个新的基准测试集，包含7,070个法律案例，平均每个查询文档有6.775个引用。该数据集提供了评估Prior Case Retrieval算法性能的全面测试床。与现有的COLIEE’21数据集相比，IL-PCR数据集具有更大的案例池、更长的文档、更大的词汇量和更多的引用。

U-CREAT管道采用无监督学习技术，引入了一种基于事件的方法进行Prior Case Retrieval任务。该方法展示了高效的检索效率、低推理时间和跨印度和加拿大法律体系的泛化能力，而无需对法律或人口统计进行特定调整。事件提取在U-CREAT管道中起着关键作用，通过依赖解析技术使用分词来表示案例文档的发展过程。事件被表示为由主语、动词和宾语组成的三元组。U-CREAT管道包括预处理、依赖解析和后处理三个步骤，用于从查询文档和候选文档中提取事件。

本研究还实验了多种模型，包括基于计数的模型、Transformer模型和事件模型，以验证和比较它们在Prior Case Retrieval任务上的性能。结果表明，事件模型显著优于其他模型，特别是Event Filtered Documents模型，它在所有事件模型中表现最佳。U-CREAT在COLIEE数据集上的性能也超过了现有方法，包括最近的MTFT-BERT团队的 supervised 方法。</sample>
    <sample id="253">Mario Ezra Aragón在介绍他们的研究工作"DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media"时，首先定义了精神障碍的概念，即与痛苦和 disability 相关的心理综合症，影响一个人的思维、情感、情绪和行为。他们利用社交媒体内容的庞大数量来研究人们如何面临困难，并利用社交媒体的匿名性来讨论心理健康问题并寻求帮助。

该研究的目标是通过自动分析社交媒体帖子来检测心理健康障碍，以支持一种新技术，能够警告可能出现的精神健康障碍，并提供支持证据。他们使用领域适应技术来提高模型在特定领域的性能，例如从一般数据中训练的 BERT 模型，调整其词汇和语义理解，以适应 Reddit 和心理健康领域的特定任务。

他们的方法包括首先学习社交媒体语言，然后专注于精神障碍领域。使用指导掩码，他们希望模型在训练过程中关注重要单词。结果表明，他们的模型在精度和召回率方面表现出良好的平衡，而其他方法则在其中一个维度上得分较高但在另一个维度上得分较低。

他们还展示了 DisorBERT 在处理 Beck 抑郁量表（BDI）项目时的行为，这是一种用于识别和衡量典型抑郁症状严重程度的临床工具。DisorBERT 的答案倾向于具有更负面的意义或心理取向，预测与精神障碍相关的问题，如注意力分散、睡眠和饮食问题，而 BERT 则倾向于生成更一般性的词。

最后，他们可视化了用户帖子中最重要的文本序列，发现与抑郁相关的关键词如“焦虑”和“药物”突出。他们认为双域适应和指导掩码的结合有效捕捉了社交媒体互动中的精神障碍迹象。未来的工作将探索不同词汇资源的应用以及临床数据的使用。</sample>
    <sample id="254">Our research focuses on document-level distant relation extraction, aiming to extract relations among entities in a document. Previous methods relied on large-scale human-annotated corpora, which is time-consuming and labor-intensive. Recent work leveraged distantly supervised data (DS) to pre-train the document-level relation extraction models for better performance. However, these DS data contain various noise levels, and current efforts to alleviate the noise problem by using pseudo labels still persist the risk of noise induction by false-positive pseudo labels.

To address this challenge, we propose a document-level relation distant extraction framework with uncertainty-guided label denoising to improve the label quality of DS data. We first train a pre-denoising DocRE model with both DS and human-annotated data to generate the pseudo labels. To determine whether model prediction can be trusted or not, we introduce uncertainty estimation to capture the model uncertainty. Considering there might be multiple relations between an entity pair, we propose an instance-level uncertainty estimation method to capture uncertainty scores for overlapping relations. We also designed a re-labeling strategy with dynamic class uncertainty thresholds and a multi-phase training strategy to further boost the performance.

To model the uncertainty in pre-denoising DocRE model, we introduce the Monte Carlo dropout technology in the DocRE task. This method requires multiple stochastic forward-pass predictions with activated dropout to capture the model uncertainty. We modify the estimation process to obtain the instance-level uncertainty score for each positive pseudo label. The distribution of uncertainty score for each relation class is different, and frequent classes usually contain lower average uncertainty than the long-tail class. So we propose dynamic class uncertainty thresholds to filter pseudo labels with high uncertainty. Then we replace the original DS label with the pseudo label that contains a lower uncertainty score than its class uncertainty thresholds.

In order to take full advantage of the DS data for further boosting the performance of DocRE model, we design a multi-phase training strategy to iteratively re-label the DS data. We compare our framework with several strong baselines onto public datasets. As shown in the table, our framework outperforms the previous baselines on both two datasets.

In conclusion, the main contribution of our work are summarized as those four points. The first one is our framework with uncertainty guided label denoising, which greatly improves the label quality of DS data. The second one is the instance-level uncertainty estimation method for overlapping relations. The third one is the iterative re-label strategy with dynamic class uncertainty thresholds for the long-tail problem. The last one is the great performance improvements.</sample>
    <sample id="255">根据实验结果，提示的形式在零和一击提示的情况下很重要。然而，在五击提示的情况下，提示的形式对性能没有太大影响。</sample>
    <sample id="257">作者评估了四个最新的对话模型。</sample>
    <sample id="258">The video introduces a new work titled "Can Large Language Models Be an Alternative to Human Evaluation?" by Chiang Cheng-Han. The research proposes using large language models (LLMs) to evaluate the quality of text in natural language processing tasks. The approach involves instructing LLMs with specific instructions and providing them with samples to rate, hoping that they can understand the instructions and provide meaningful ratings.

The motivation behind this work is to find an alternative to human evaluation, which is often unstable and difficult to reproduce. The researchers explore the idea of using LLMs for evaluation because these models have shown the ability to follow natural language instructions and perform tasks similar to human evaluations.

The experiment conducted involved using LLMs to rate stories generated by GPT-2 or written by humans based on four attributes: grammar, coherence, likability, and relevance. The results showed that human raters, specifically English teachers, preferred human-written stories over those generated by GPT-2. However, some smaller LLMs did not show a clear preference. Notably, the larger LLMs, Davinci and ChatGPT, demonstrated a clear preference for human-written text, similar to human evaluators.

The video also mentions that further questions about the agreement between LLMs and human evaluators, the impact of instruction wording changes, sampling methods, benefits and costs compared to human evaluation, and results on other tasks are addressed in the paper. The researcher encourages viewers to read the paper or visit their poster stand at ACL for more information.</sample>
    <sample id="259">The speaker, Yusen Zhang from Penn State University, presents a work titled "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations." The work focuses on building semantic representations of user queries in multiple natural languages and meaning representations. The task involves translating queries in multiple natural languages using neural models into SQL, Lambda, FunQL, and other representations.

Existing cross-lingual semantic parsing models are separately proposed and evaluated on limited tasks and applications, with coverage on certain natural languages but lacking for others like Chinese. The work proposes XSemPLR, which provides a uniform data set for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families.

The work evaluates six settings for training and evaluation, including Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer. The results show that Encoder-Decoder models obtain the best performance on all nine datasets, and pre-training on English natural language can significantly boost the performance of Few-shot on target natural languages. The work also finds that multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.

Overall, the work provides a comprehensive benchmark study on three representative types of multilingual language models and shows many interesting findings.</sample>
    <sample id="260">根据所提供的内容，无法确定论文的作者人数。</sample>
    <sample id="261">优秀规划器的理想品质是编写合理且符合约束的脚本。</sample>
    <sample id="262">根据所提供的内容，无法确定论文的作者人数。</sample>
    <sample id="263">The paper presents a work on mitigating label biases for in-context learning, which is a popular paradigm for utilizing large language models. The authors identify three types of label biases: vanilla-label bias, context-label bias, and domain-label bias. They propose a novel calibration method called domain-context calibration to handle all types of biases. The method uses random in-domain words sampled from the task corpus as content-free text to estimate the model's bias on each of the label names and then uses this estimated bias to calibrate the model's original predictions. The authors conduct experiments using different models on a wide range of datasets and observe that domain-context calibration improves significantly the average performance of in-context learning on this dataset. They also find that using more random words leads to further improvements and that using random in-domain words rather than random English words to take into account the domain-label bias achieves further large improvements.</sample>
    <sample id="264">林王是一位来自浙江大学的研究生，今天他将介绍他的论文《TAVT：朝着可转移的音频-视觉文本生成》。目前，单模文本生成任务如机器翻译和图像字幕已经取得了长足进展，得益于大规模预训练和巨大的模型容量。然而，对于多模态文本生成任务如音频-视觉文本生成，标注数据更加耗时昂贵。现有的方法往往由于不同领域的构造条件而遭受严重的退化。为了突破这一限制，我们提出了一个新的任务：可转移的音频-视觉文本生成。该任务的主要挑战是多模态域移，如视觉风格、音频能量等。此外，不同类型的域移具有不同的特点。我们注意到，在同一事件的内容理解中，视觉内容会因图像风格和拍摄角度的变化而发生显著变化，但音频内容如节奏和能量的变化 hardly 影响对事件的理解。基于这些现象，我们提出使用统一的音频语义空间来对齐跨域的视觉概念。接下来，我将介绍整体框架。我们的目标是训练一个模型，使其能够学习并快速适应新的多模态领域，同时使用有限的标注数据。我们所呈现的框架由三个组件组成：音频-视觉元映射网络、音频-视觉编码器和语言模型生成器，以及反事实对比学习。第一个模型是音频-视觉元映射网络，它可以在不同领域之间映射不同的视觉概念到统一的听觉语义空间，并解决语义分布的变化。我们收集了大量Flickr数据集上的音频片段，并使用k-means对其进行聚类，得到一组音频簇作为统一的音频语义空间。受Prompt学习的启发，我们引入了一组可学习的标记。这些标记称为音频簇的视觉前缀。我们为每个音频簇生成概率分布，用于根据视觉内容查询音频，并通过优化重构音频的概率分布来改善这些标记的语义。最后，我们通过优化视觉-音频对齐来优化标记。第二个模型使用Transformer编码器和生成器。我们引入了一个α值来评估不同模态对每个单词的贡献。在时间步t，α-t通过测量每个模态与前一个单词的交叉注意力的相关性来计算。最后，我们介绍了框架的损失函数和训练细节。尽管重建 paradigm 提供了对 AVMM 的约束，但它不能直接优化视觉-音频对齐分数。因此，我们提出了双重反事实对比学习（DCLL），它从反事实结果中构建精细的监督信号，以直接优化视觉-文本对齐，而无需依赖随机选择的负样本的质量。最后，我们进行了实验，以全面评估我们提出的方法。据我们所知，还没有研究过可转移的音频-视觉文本生成。因此，我们首先选择了SOTA方法，包括RNN-based和Transformer-based模型。为了公平比较，源模型都在Dmeta-train上使用相同的元学习框架进行训练，并在目标领域上测试。可以观察到，我们的方法在所有指标上都比所有对比模型在两个基准（MSVD和MSR-VTT）上的跨数据集和跨领域设置上表现出了更大的优势。对于低资源领域，如“儿童”和“美丽”，其他方法遭受了严重的性能退化，而TAVT仍然表现良好。我们还进行了消融实验，分析了音频特征对扩展性能的影响。这就是我的报告。谢谢。</sample>
    <sample id="265">演讲者的名字是Vasudha。</sample>
    <sample id="266">根据提供的信息，无法确定论文作者所属的机构。在演讲中没有提到任何机构名称或细节。</sample>
    <sample id="268">PaLM最常见的错误是省略错误，即在翻译时删除源句子中的某些部分。</sample>
    <sample id="269">大家好，我是James Finch，我是Sarah Finch。今天我们将向大家介绍ABC-Eval，一种新的维度评估对话型人工智能的方法。这项工作是由Emory NLP实验室的Jinho Choi教授领导的，并与亚马逊Alexa AI合作完成。假设你刚刚开发了一个对话模型，你想看看它与当前最先进水平相比表现如何。常见的做法是使用人类评估，例如，请人类裁判选择两个对话中哪个更好，或者根据 Likert量表对对话进行评分。这些方法在提供对话整体质量的整体评估方面效果很好，但对话质量有许多方面。因此，你可能想要评估对话质量的多个维度，以了解模型的优点和缺点。一种方法是请人类裁判评估对话质量的多个维度，例如，使用现有的比较或Likert量表方法来评估模型响应的相关性。但我们认为，有一个更精确可靠的策略来进行维度对话评估。我们的方法通过明确标注每个模型响应是否表现出某些行为，如回应无关信息或自相矛盾，从而减少人类评估的主观性。我们称这种方法为“在聊天中标注行为”或简称ABC-Eval。我们开发了这种方法，以全面覆盖最近文献中建议会影响对话质量的各种行为。ABC-Eval能够衡量聊天模型犯各种主题错误的概率。例如，ABC-Eval衡量了聊天模型在多轮对话中忽略其伙伴、说无关的话、自相矛盾或违反共同知识、以及成功或失败地展示同理心的概率。为了确定哪种评估方法最有效，我们选择了四个最先进的聊天模型，并使用ABC-Eval评估了每种模型在100个人-机器人对话上的表现。为了进行比较，我们还使用了三种现有方法：在轮次级别上使用Likert评分，在对话级别上使用Likert评分，以及在对话级别上进行两两比较。对于每种现有方法，我们收集了对八种最常测量的对话方面进行评分的数据，因为这是评估聊天模型沿多个维度进行评分的标准做法。从我们的分析结果来看，ABC-Eval的行为标签在总体上比现有方法收集的标签更可靠，这可以通过在100个双重标注对话上测量的注释者一致性来证明。此外，ABC-Eval标签预测对话质量的效果优于现有方法产生的指标，如线性回归分析所示。例如，你可以看到，衡量自相矛盾的轮次比例分别解释了对话质量的5%和10%，而平均Likert一致性分数只解释了4%或更少。最后，我们检查了每个评估指标是否捕捉到了对话质量的独特方面，使用逐步线性回归。你可以看到，所有ABC-Eval指标的组合解释了对话质量的25%以上，随着一个一个指标被移除，大多数指标都导致了关于质量的信息丢失。相比之下，所有轮次Likert指标的组合解释了较少的质量， fewer的指标携带独特信息。这些可靠的、信息丰富且独特的ABC-Eval指标使我们能够以比以往任何方法都能实现的更高分辨率评估对话型人工智能。你可以在实验结果中看到，我们测试的机器人存在一些挑战，这些挑战已经被精确量化。例如，我们测试的机器人在大约20%的响应中存在共同知识错误，在大约15%的响应中产生无关信息，在大约10%的响应中自相矛盾或与其伙伴矛盾。由于该领域的快速发展，许多新模型发布时的错误率可能会降低。但是，这正是我们需要可靠和精确的评估指标来比较模型的原因。我们希望ABC-Eval可以被其他领域的专业人士利用，作为朝着这个方向的一个有意义的步骤。我们期待看到对话型人工智能将在未来几个月和几年内取得哪些进展。谢谢观看。</sample>
    <sample id="270">根据英语内容，这篇论文的作者所属机构是Emory University。</sample>
    <sample id="271">在本文中，CFT代表“干净的Fine-tuning”，即直接在干净数据上进行微调的模型。</sample>
    <sample id="272">这篇论文有7位作者。</sample>
    <sample id="273">大家好，我是Kayo Yin，我将为您呈现我们团队的作品“翻译何时需要语境：一种多语言数据驱动的探索”。这项工作是在与Patrick Fernandes、Emmy Liu、André F. T. Martins和Graham Neubig合作完成的。许多翻译都依赖于语境。例如，在这句话中，“ mole” 的含义可能会根据前一句话而改变。如果前一句话是“如果部长们发现了，事情可能会变得危险”，那么“ mole” 指的是一个间谍。但如果前一句话是“这可能是认真的吗，医生？” 那么“ mole” 指的是一个胎记。因此，根据语境，单词的含义会改变，因此其翻译也会改变。然而，评估模型在处理这些情况时的表现是相当困难的。首先，只有少数翻译依赖于语境，使得衡量这些翻译的语料库级别的指标如BLEU无法捕捉到这些翻译。有些人建议对依赖于语境的翻译进行针对性评估，但这些资源通常只支持有限类型的语境依赖翻译和有限的语言集，因为它们通常依赖于领域知识和人工策展。在这项工作中，我们试图回答这两个问题。首先，翻译何时需要语境？其次，模型如何处理这些情况？为了回答第一个问题，我们首先测量了在翻译过程中一个词需要多少语境。在之前的工作中，我们引入了CXMI作为机器翻译模型使用语境的度量标准。这是通过测量给定源X的情况下，语境C提供了多少关于目标Y的信息来实现的。您可以将CXMI视为向模型提供语境所获得的信息。在这项工作中，我们将CXMI扩展为点积CXMI，可以衡量在句子级别或单词级别的语境使用情况。我们可以将具有高P-CXMI的单词视为需要语境进行翻译的单词。现在我们分析具有高P-CXMI的单词，以查找这些单词之间的模式。我们在14种不同语言的TED演讲翻译 transcripts 上进行了分析。我们在三个不同的级别上进行分析。首先，我们查看具有高平均P-CXMI的词性标注。这使我们能够找到，例如，在阿拉伯语中具有相对较高P-CXMI的复数代词，因为英语没有复数代词，所以您需要语境来确定代词是否为复数。同样，我们发现某些语言在选择适当的动词形式时也需要语境。然后我们查看具有高P-CXMI的所有词汇项目的平均值。这有助于我们识别案例，例如，在中文中，您需要语境来确保在文档内使用相同的翻译。同样，我们发现语境对于选择正确的正式程度很重要。最后，我们查看具有高P-CXMI的不同个体标记。这使我们能够识别不能由单词本身表达的现象，而是由句子结构表达的现象，例如消解省略。现在我们使用我们的分析结果来设计一个用于文档级别翻译的基准测试。对于我们确定的五种 discourse 现象中的每一个，我们创建了自动识别与现象相关的单词的标记器。我们称我们的标记器为多语言 discourse 意识标记器（MuDA）。我们还可以注意到不同语言有不同的这些 discourse 现象的比例。然后我们使用 MuDA 标记器，通过在我们要使用的平行语料库上应用标记器，并应用我们选择的翻译指标，来评估语境相关的示例。最后，我们使用我们的基准测试以及其它指标来评估不同模型在文档级别机器翻译上的表现。首先，当我们使用语料库级别的指标时：对于 BLEU，我们发现语境无关模型在性能上最好。但是，如果我们使用 COMET，语境相关模型表现最佳。如果我们使用词f-测度，则具有和不具有语境的模型性能相当。这再次证明了，如果我们仅使用语料库级别的指标，很难确定文档级别翻译系统的最佳系统。然后我们使用 MuDA 基准测试来评估模型，我们发现语境相关模型在识别 MuDA 标记器识别出的某些 discourse 现象时，显著比不使用语境的模型更准确，例如正式性和词汇 cohesion。但是，这些模型在其他现象上，如消解、代词和动词形式方面，并没有比不使用语境的模型表现得更好。这表明我们需要在文档级别翻译中看到更多进展。我们还比较了不同的商业系统，我们的基准测试显示 DeepL 通常比 Google Translate 更准确地进行文档级别翻译。总之，我们在14种不同语言对上进行了数据驱动分析，以确定何时翻译需要语境，然后我们利用我们的 findings 来构建一个文档级别机器翻译基准测试，这可以帮助我们确定哪些 discourse 现象模型可以处理好或不好，以及哪些翻译系统在文档级别翻译上表现良好。谢谢您的聆听。期待在多伦多见</sample>
    <sample id="274">演讲者的名字是Yusen Zhang。</sample>
    <sample id="276">Ananya and Vignesh presented their work on "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages." The study focuses on evaluating translation metrics for Indian languages, specifically Tamil, Malayalam, Hindi, Marathi, and Gujarati. They used the Flores dataset to select 200 sentences randomly and generated multiple candidate translations in English using seven different translation models or APIs, resulting in a total of 1,400 candidate translations for each language.

To collect human annotations, bilingual expert annotators evaluated the outputs by showing them the source sentence along with the candidate. Annotators marked each error with its type and severity and provided an overall score for the output. Error types were classified into accuracy or meaning-based errors, fluency errors, and special category errors.

The results showed that recent MT models like NLLB or Indic Trans have fewer errors compared to older models like CVIT. The best-performing models in descending order are Indic Trans, NLLB, Google API, Bing API, mT5, CVID, and mBART. The correlations between MQM-based scores and metric scores for 1,400 segments per language were also observed. ChrF had the highest correlation across all languages, but overlap-based metrics were the worst-performing overall. Among embedding-based metrics, LabSE embedding saw better correlations than other counterparts. BERTscore with embeddings obtained from different multilingual models improved the correlation further. MuRIL showed the better correlations on average, and COMET-metric variants had the highest overall correlations for all languages.

The results were mixed, with many metrics providing scores in a narrow range and not utilizing their full scoring range. Human scores for Tamil covered the entire scale, while most metrics exhibited a skewed spread. Splitting the dataset based on error types showed a more nuanced picture. Almost all metrics showed a higher correlation with human scores when only accuracy errors were annotated.

The study fine-tuned the best-performing metric, COMET, using the MQM dataset. IndicCOMET MQM outperformed the COMET baselines on three out of five languages and showed higher correlations than COMET MQM across all languages. In a zero-shot test on unseen languages, IndicCOMET outperformed both COMET baselines. The robustness scores on the ACES Translation Accuracy Challenge Sets showed that IndicCOMET MQM has a correlation score of 0.36 and is more robust than the COMET counterpart, which has a score of 0.272.</sample>
    <sample id="277">该方法没有名称。</sample>
    <sample id="278">作者描述“显性词汇”(marked words) 方法是一种识别区分标记群体和未标记群体的单词的方法。这种方法基于社会语言学概念“标记性”，即存在一个默认的未标记群体，而任何与之不同的群体都是 linguistically marked。在他们的方法中，他们首先指定未标记和标记群体，然后使用加权对数比值来区分每个标记群体的top单词。</sample>
    <sample id="279">论文的作者是来自University of Washington的Shangbin。</sample>
    <sample id="280">The speaker, Shi Tao, introduces their work on emotion regulation in conversations. The goal is to predict the emotion label of each utterance in a dialogue, considering textual, audio, and visual modalities. Most existing methods focus on speaker and contextual information but face challenges such as underutilizing multimodal information, poor performance in minority emotion classes, and difficulty distinguishing between semantically similar emotions.

To address these issues, the speaker proposes MultiEMO, a novel attention-based correlation-aware multimodal fusion framework. The framework includes four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The contributions include:

1. VisExtNet: A novel visual feature extractor that captures visual cues from interlocutors without encoding redundant scene-related information.
2. MultiAttn: A multimodal fusion model using bidirectional multi-head cross-attention layers to integrate one modality with complementary information from other modalities.
3. Sample-Weighted Focal Contrast Loss: A loss function that assigns higher importance to hard-to-classify minority classes and makes sample pairs with different emotion labels mutually exclusive to maximize inter-class distances.

Experimental results show that MultiEMO achieves state-of-the-art performances on MELD and IEMOCAP datasets, with significant improvements in minority and semantically similar emotions. However, some limitations exist, such as VisExtNet not distinguishing between speakers and irrelevant people, the need for a large batch size for SWFC loss on MELD, and still worse performance in minority emotions compared to majority classes.</sample>
    <sample id="281">Kayo Yin and her team conducted a study on the role of context in translation, titled "When Does Translation Require Context? A Data-driven, Multilingual Exploration." They collaborated with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. The study aimed to address two main questions: when does translation require context, and how well do models handle these cases?

To answer the first question, they introduced Pointwise CXMI (Contextual Mutual Information), a measure that assesses the amount of information provided by context to the model. They analyzed transcripts of TED talks translated into 14 languages at three levels: part-of-speech tags, vocabulary items, and individual tokens. Their findings revealed that certain languages, such as Arabic, require context for dual pronouns, while others need context for verb forms or proper noun translations.

For the second question, they designed a benchmark called MuDA (Multilingual Discourse-Aware) to evaluate models on document-level translation. MuDA identifies discourse phenomena like formality, lexical cohesion, ellipsis resolution, pronouns, and verb forms. They found that context-aware models outperformed context-agnostic models for certain phenomena but not others. Additionally, their benchmark showed that DeepL is more accurate than Google Translate for document-level translation.

In conclusion, this study provides insights into the importance of context in translation and offers a benchmark to evaluate models' performance on document-level translation.</sample>
    <sample id="282">Xuekai Zhu is presenting a new work at ACL 2023 titled "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing". This research focuses on the task of non-parallel text style transfer, which has been primarily addressed at the token or sentence level. However, this work takes a significant step forward by addressing story-level style transfer at the discourse level, which is crucial for imitating author style.

The main challenge in this task is that long texts usually involve many complicated author linguistic preferences, such as discourse structures. The primary challenge lies in imitating the author's linguistic choices at the discourse level, as seen in Table 1, in the red contents, such as narrative techniques. Moreover, styles tend to be highly associated with specific writing topics, making it difficult to transfer this style-specific content to another style, like the missing content in orange in Table 1.

To alleviate these issues, the researchers propose a generation model named StoryTrans. StoryTrans learns discourse representations from the source texts and combines this with learnable style embeddings to generate texts in the target styles. They have also designed a new training objective to reduce the stylistic features from the discourse representations, pulling the representations derived from different texts closer in the latent space. Additionally, they separate the generation into two stages: first, they transfer the source text with style-specific content keywords masked, and then generate the whole text by incorporating these keywords explicitly.

The training framework is separated into two stages. For the first stage, they use an advisory training framework that employs self-reconstruction loss to recover the input, disentanglement loss on sentence embeddings to disentangle style and content, sentence order loss to capture sentence-level dependency, and style classifier loss to produce style signals for the whole system. The second stage is unrelated to style transfer and aims to fill the correct style-specific contents and remove the mask token.

The evaluation and datasets include new datasets collected in Chinese and English for the new tasks, and extensive experiments were conducted to transfer fairytales or everyday stories to typical author styles. Both automatic and manual evaluations confirm the efficiency of the model, showing that StoryTrans outperforms strong baselines in terms of style control and content preservation. Style visualization indicates that the transfer test by StoryTrans aligns with the golden text in the style feature space. Furthermore, StoryTrans can supplement several short phrases or plots to enrich the whole storyline while maintaining the main contents. It can also rewrite most sentences with the target style while maintaining the source semantics.

Overall, this work presents a significant advancement in non-parallel text style transfer by addressing the challenges at the discourse level and separating the generation process into two stages. The results demonstrate the effectiveness of the proposed model in preserving style and content while generating coherent texts in the target styles.</sample>
    <sample id="283">第一个提到的对称依存关系结构的名称是“对称协调”。</sample>
    <sample id="284">The paper "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" by Peng Tianshuo from Wuhan University proposes a new approach to address the limitations of current span-based UIE models. These models rely on precise boundary positions, which can be ambiguous and lead to incorrect annotations. The proposed FSUIE method uses a fuzzy span mechanism that models the target boundary as a continuous distribution of correct probability within a specific range (R-min and R-max). This allows for more flexibility in labeling and reduces the reliance on precise boundaries.

To address the mismatch between transformer feature extraction and information extraction, FSUIE introduces an adaptive attention mechanism that dynamically adjusts the attention span based on the predicted boundary distribution. This is achieved through a fuzzy span attention layer that acts as a mask function, trimming the attention distribution and focusing on semantic information within a limited range of preceding tokens.

The paper demonstrates the effectiveness of FSUIE on three main information extraction tasks: named entity recognition, relationship extraction, and aspect sentiment triplet extraction. On the named entity recognition task, FSUIE achieved significant performance improvement compared to UIE-base without a fuzzy span mechanism, especially on small-scale data sizes. FSUIE also achieved state-of-the-art results on relationship extraction datasets ACE2004, 2005, and ADE, and demonstrated strong generalization capabilities for domain-specific information.

The ablation study showed that the fuzzy span attention (FSA) improves convergence speed by guiding the module to obtain a reasonable attention distribution, while the fuzzy span loss (FSL) enables the module to fully utilize annotation information and obtain greater information extraction capability. The combined effect of these two mechanisms produces a greater enhancement.

Overall, the paper presents a novel fuzzy span mechanism that enhances universal information extraction by alleviating the model's reliance on precise span boundaries and adapting the attention span of the model. The proposed FSUIE achieves excellent results in a wide range of IE tasks.</sample>
    <sample id="285">The video discusses the work of Mingqi Gao from Peking University, titled "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework." The video highlights the importance of correcting factual errors in dialogue summarization and the challenges in evaluating the performance of factual error correction (FEC) models.

The video argues that current FEC models are evaluated using factuality metrics such as FactCC and DAE, which provide an overall score and may not be reliable. Additionally, these metrics blur the line between the two types of solutions, as FEC models can ignore the content of the original summary and directly generate a different but more factually correct summary. To address this issue, the video proposes introducing manually annotated reference corrections to provide more valuable data for training FEC models and create a more comprehensive and accurate evaluation of their performance.

The video also proposes a new taxonomy of factual errors based on content-based and form-based categories. It introduces a new evaluation framework based on the ERRANT metric for grammar error correction, consisting of alignment, classification, and comparison steps. The video experiments with different FEC models in various training modes and finds that training with reference summaries from dialogue summarization datasets yields the best results. However, there is still a need to change the evaluation methods for FEC models, and combining human-annotated data with synthetic data is a promising direction. The video concludes by highlighting the challenges in correcting factual errors like addition and addressing attribute errors, modality errors, and link errors.</sample>
    <sample id="286">演讲者的名字是James Finch和Sarah Finch。</sample>
    <sample id="287">根据所提供的内容，这篇论文有4位作者：Javad Hosseini、Filip Radlinski、Silvia Pareti和Annie Louis。</sample>
    <sample id="288">BLiMP和SyntaxGym数据集可用于测试句法现象。</sample>
    <sample id="290">第一个研究问题的五种方法的缩写是WSL，代表“弱监督学习”。</sample>
    <sample id="291">该模型在命名实体识别、分类、词性标注和问答等任务上进行了评估。</sample>
    <sample id="294">CamemBERT 最初是在大规模的文本数据集上训练的，包括维基百科、Wikipedia、维基百科和维基百科。</sample>
    <sample id="295">演讲者的名字是亚当·普热皮奥克维。</sample>
    <sample id="296">Valerio Basile is presenting a work that resulted from a collaboration between the University of Turin and Amazon Alexa. The work focuses on natural language understanding, specifically irony detection in natural language processing. The team developed a corpus called EPIC (English Perspectivist Irony Corpus) by collecting data from social media, Reddit, and Twitter over a period of 1.5 years. They collected about 300 short conversations made up of pairs of text, one following the other, from five varieties of English. The data was annotated by 74 annotators using crowdsourcing platform Prolific, with each annotator reviewing 200 texts or short conversations. The annotators were asked to determine if the reply was ironic or not.

The team observed differences in inter-annotator agreement based on various dimensions such as gender, age group, nationality, and more. They built perspective-aware models by fine-tuning pre-trained language models on splits of the datasets based on different annotators. While there were no significant trends in raw performance, the perspective-aware models showed less uncertainty and more confidence in their predictions compared to gold standard aggregated models.

The team also analyzed the causes of differences in annotations and found that generations close to each other and annotators from the United Kingdom and Ireland had the highest variations in response.</sample>
    <sample id="297">The speaker is discussing a research project that aims to understand and analyze coded rhetoric, specifically dogwhistles, which are terms that convey one message to an outgroup while sending another (often controversial or inflammatory) message to an in-group. The project involves developing a typology and glossary of over 340 terms and symbols, especially for racist, transphobic, and anti-Semitic dogwhistles, as well as performing a case study of historical U.S. political speeches and evaluating dogwhistle recognition in language models. The project also examines how dogwhistles can evade content moderation through toxicity detection. The speaker highlights the importance of understanding dogwhistles in NLP and linguistics, as they challenge our idea of meaning and enable hateful and abusive rhetoric while evading content moderation online. The project's findings suggest that dogwhistles are most successful when the outgroup is unaware, and researchers were usually in the outgroup.</sample>
    <sample id="298">我们通过实验发现，为了获得更好的泛化能力，需要更好的模型架构、更大的模型规模以及更多的微调示例。这些因素相互关联，不能单独使用。此外，我们还发现性能下降的主要原因是由时间漂移引起的，而不是自适应过拟合。</sample>
    <sample id="299">The paper presents a training method to improve the robustness of Natural Language Inference (NLI) models by reducing their reliance on shortcuts. The authors propose a minimax training objective that involves an auxiliary model and a learner model. The auxiliary model generates example weights that incentivize the learner to focus on under-represented hard examples, which are crucial for out-of-distribution generalization. The learner's loss is minimized while the auxiliary's task is to maximize the learner's loss by generating example weights. Both models are optimized in an alternating fashion using standard optimization algorithms. The proposed method does not assume any specific type of shortcuts and relies on the learner's own training dynamics to generate example weights. The authors evaluate their method on three analytic datasets and their corresponding out-of-distribution adversarial test sets, showing consistent improvements in out-of-distribution performance while maintaining high in-distribution accuracy compared to ERM training and shortcut mitigation methods. The paper also examines the effect of pre-training the learner, the size of the auxiliary, and conducts a qualitative evaluation of the learned example weight distribution.</sample>
    <sample id="300">Belinda's presentation introduces the concept of interactive dictation, a process where users can use their voice to both dictate and edit a document in a natural and intuitive manner. The presentation explains that while speech-to-text systems are becoming more prevalent, most support only dictation and do not allow for editing through vocal commands. The presentation also highlights the challenges of existing software that recognizes vocal edit commands, such as Nuance Dragon NaturallySpeaking and Microsoft Word Dictate function, which require memorizing fixed template commands.

The presentation then introduces the interactive dictation task, which is characterized by flexible interleaving of dictation and editing, not separated by trigger words, and using intuitive and open-ended natural language utterances to specify edits. The presentation also introduces a new task formalization, data collection interface, and baseline system for the interactive dictation task.

The presentation describes the four-step procedure for the interactive dictation task: ASR recognition module parsing raw audio into a speech transcript, segmenting the speech transcript into separate dictation and command utterances, extracting and normalizing each command, and fixing ASR miss detections and speech errors. The presentation also explains how the dataset was collected using an annotation interface and how a baseline system was built to perform each of these four steps.

The presentation concludes by discussing the evaluation of the segmentation model, ASR repair, and interpretation models, and the trade-off between runtime and accuracy. The presentation also mentions that there is still much room for progress in this field and welcomes more work on it. Finally, the presentation provides information on where to find the released code and the paper for more details.</sample>
    <sample id="302">在输出序列中排列词元是必要的，因为模型首先预测输入和输出的无序多集标记。在第二步中，模型使用另一个模型来预测将这些标记排序以获得正确顺序的排列。这是因为输入和输出之间的对齐在训练数据中并不给定，因此对于给定的输入词元，我们不知道它来自哪个多集标记。通过在训练过程中学习对齐，可以解决这个问题。</sample>
    <sample id="303">作者建议模型所有者提高偏见缓解方法的透明度，因为这有助于更好地理解这些方法的效果和潜在影响。通过增加透明度，可以识别和解决模型中出现的积极刻板印象和其他有害模式，从而促进更公平和包容性的AI系统的发展。</sample>
    <sample id="304">最小对不可接受输入是一种评估语言模型的方法，其中将可接受句子与不可接受句子进行比较。该方法旨在评估模型在不同句子结构和语境下的可接受性。</sample>
    <sample id="305">Dawei, a PhD student at Saarland University in Germany, presents their recent work on weakly supervised learning (WSL) in this video. The research explores the challenges of training neural networks on weakly labeled data and proposes solutions to robustly train models under label noise.

The video highlights that while WSL methods claim to achieve high performance on clean test sets by training on weakly labeled data, they actually require clean validation samples to work properly. Without clean validation samples, there is a significant performance drop. The video also suggests that increasing the number of clean validation samples can improve WSL performance, but direct fine-tuning on clean data can even outperform WSL approaches.

The video concludes with recommendations for future work in WSL, including reporting model selection criteria, comparing with few-shot learning baselines, considering continuous fine-tuning, and open-sourcing code.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim discuss their work on entity tracking in language models, highlighting the importance of this ability for understanding longer discourses. They argue that pre-trained language models have not been systematically evaluated for their entity tracking capabilities due to challenges such as common patterns in pre-training data, simple heuristic associations, and memorization through fine-tuning or in-context demonstrations.

To address these challenges, they designed a task involving boxes and objects, where the input includes an initial description of each box's contents and multiple state-changing operations. The task requires the model to predict the contents of each box based on the initial description and operations. To prevent the model from using heuristics, various measures were implemented.

Experiments with Flan-T5 and GPT-3 and -3.5 models using 2-shot in-context learning showed that most models simply repeated the initial state, while only text-davinci-003 exhibited non-trivial tracking behavior. This suggests that pre-training on code is responsible for making this capacity surface in pre-trained language models. Smaller models like T5-base can learn to perform entity tracking if directly fine-tuned, but randomly initialized models cannot learn the state tracking task even with direct supervision.

The results indicate that pre-training is crucial for entity tracking abilities in language models, but it remains unclear whether these abilities generalize beyond the specific set-up used in the study. Further research is needed to explore the generalizability of these findings.</sample>
    <sample id="307">作者使用了包括命名实体识别、分类、词性标注和问答在内的公共和私人下游任务数据来评估他们的模型。他们还将他们的模型与六个基线模型进行了比较，这些基线模型是CamemBERT OSCAR 138 GB、CamemBERT OSCAR 4 GB、CamemBERT CCNET 4 GB、PubMedBERT、BioBERT和ClinicalBERT。</sample>
    <sample id="308">Jenny, a first-year PhD student at Carnegie Mellon University, is presenting a work titled "NLPositionality" that characterizes design biases of datasets and models. The work was done in collaboration with researchers from the University of Washington and the Allen Institute for AI, including Sebastian Santy, Ronan Le Bras, Katharina Reinecke, and Maarten Sap.

The presentation begins by imagining a scenario where a newspaper is sifting through comments under a news article to remove toxic content. They might turn towards a popular API like Prospective API for toxicity detection, which works well for Carl Jones but not for Aditya Sharma, who comes from an Indian context. This is an example of a design bias where there are systematic performance differences between populations.

Design biases can occur due to the positionality of NLP researchers and model developers, which refers to the perspectives people hold as a result of their demographics, identity, and life experiences. These biases can influence research decisions and outcomes. Prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps in models and data sets, but theoretical definitions of model positionality have not been extensively studied.

To study data set and model positionality, the researchers developed a framework called NLPositionality. This framework works in two main steps: re-annotating data sets with diverse annotators and comparing the annotations with existing datasets and models using a Pearson's R correlation score. The framework differs from annotator disagreement literature by comparing end users with models and datasets, predictions, and labels.

The study used Lab in the Wild, an online experimentation platform, to recruit diverse volunteers. They hosted two tasks on the platform: social acceptability and toxicity and hate speech detection. The study amassed over 16,000 annotations from over 1000 annotators from 87 countries.

The results found that there is positionality in NLP. For example, data sets and models are most aligned to English-speaking countries, and GPT-4 is most aligned to people with a college education or graduate school education. However, non-binary individuals are less aligned compared to men and women.

To address these issues, the researchers recommend keeping a record of all relevant design choices throughout the research process, doing NLP research with the lens of perspectivism, and building specialized datasets and models within specific communities. Inclusive NLP isn't just about making all technologies work for everyone; it's about ensuring that they work for everyone equally.</sample>
    <sample id="309">使用了双标注一致性来衡量注释者之间的一致性。</sample>
    <sample id="310">在不可接受和可接受查询中，选择一个完全无关的领域来添加句子，例如维基百科。</sample>
    <sample id="311">根据所提供的内容，无法确定论文的作者所属机构。在演讲中没有提到任何机构名称或细节。</sample>
    <sample id="312">MultiInstruct 是第一个大规模多模态指令调优基准数据集，包含62个涵盖10个广泛类别的多样多模态任务。与其他基准不同的是，它提供了大量多模态指令任务，而其他基准主要关注自然语言任务。</sample>
    <sample id="313">根据提供的内容，无法确定论文的作者人数。然而，它提到了“Emory NLP Lab”和“Amazon Alexa AI”的合作，以及“Jinho Choi教授领导”。</sample>
    <sample id="314">二进制协调指的是两个或更多个句子成分之间的关系，这些成分通过并列连接在一起。在二进制协调中，这些成分可以是名词短语、动词短语或其他句子成分。二进制协调的目的是为了表达多个概念或动作同时发生，或者强调多个主体或对象之间的共同特征。</sample>
    <sample id="315">在本研究中，提示语的平均长度是10.5个单词。</sample>
    <sample id="316">这些发现表明，较小的 T5 模型在适当的训练下可以超越较大的模型。这表明，在适当的训练下，较小的模型可以用于约束语言规划任务，而不会像较大的模型那样昂贵。</sample>
    <sample id="317">Peng Li from Fudan University presented a work titled "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors" at a conference. The work addresses the challenge of information extraction, which involves extracting structured information from unstructured text. Common information extraction tasks include named entity recognition and relation extraction (RE). The proposed method transforms the text-to-structured information extraction task into a structure-to-structure code generation task using code large language models like Codex. The approach uses few-shot in-context demonstrations to generate code that extracts named entities and relations from input text. The evaluation results show that the proposed approach outperforms traditional baseline models such as UIE and natural language large language models like GPT-3. The analysis also reveals that using code format prompts significantly reduces structural errors and improves recall. The paper and code are publicly available for further research.</sample>
    <sample id="318">大家好，我是Yanis Labrak，我将向大家介绍我们关于“DrBERT：一种用于生物医学和临床领域的鲁棒预训练模型”的研究。首先，我们将讨论医疗保健中的语言建模。然后我们将介绍我们论文的主要贡献。我们介绍了第一个用于生物医学领域的法语 biomedical 模型，名为 DrBERT，该模型基于 RoBERTa，并在 NACHOS 数据集上进行训练，该数据集是从网上爬取的医学数据。我们还介绍了使用多种预训练设置和数据源对模型进行比较。接下来，我们将展示我们在11个生物医学和临床下游任务上的结果。最后，我们将总结实验并提供有关如何访问这些模型的更多细节。自2018年发布以来，BERT 已成为解决自然语言处理任务最有效的方法之一，并在性能上取得了巨大飞跃，相比之下，历史上的静态和上下文化方法，如 Word2vec、fastText 等等。自那时起，该模型已被应用于许多其他语言，例如在法语中使用 CamemBERT，在生物医学领域使用 PubMedBERT 和 BioBERT，在临床领域使用 ClinicalBERT，但大多是在英语中。针对其他语言的专门模型相对 scarce，通常基于持续预训练，因为缺乏本领域数据。然而，法语目前没有开放源代码的生物医学模型。因此，我们提出了一个问题：对于广泛用途而言，什么是最合适的数据来源？从网上爬取的数据是临床数据的良好替代品吗？为了回答这个问题，我们将 DrBERT 与基于纳antes 大学医院数据仓库中匿名数据的 ChuBERT 模型进行了比较。之后，我们提出了另一个问题：我们需要多少数据来训练一个专门针对法语数据的模型？是4GB、8GB还是更多？为了回答这个问题，我们首先训练了四个从头开始的模型：第一个版本的 DrBERT，使用7GB的NACHOS；第二个版本的 DrBERT，使用4GB的NACHOS数据集；第一个版本的 ChuBERT，这是一个临床模型，使用4GB的临床笔记句子；以及最后一个版本的 ChuBERT，使用4GB的NACHOS数据集和4GB的临床笔记句子的混合。除了这个比较之外，我们还引入了三个分别在 NACHOS 数据集上使用 CamemBERT 权重和训练的模型，以及在临床笔记上使用 CamemBERT 权重和训练的模型，以及在 NACHOS 数据集上使用 PubMedBERT 训练的模型。总共，我们有七种模型。为了评估我们的七种模型，我们收集了公开和私下的下游任务数据，如命名实体识别、分类、词性标注和问答。这些模型与六个 baseline 模型进行了比较，这些 baseline 模型包括 CamemBERT OSCAR 138 GB、CamemBERT OSCAR 4 GB、CamemBERT CCNET 4 GB、PubMedBERT、BioBERT 和 ClinicalBERT。评估结果表明，模型在与模型训练数据同质的数据上表现最佳。然而，我们可以观察到，来自异质数据源的数据更具 versatility。我们还观察到，使用更多数据可以提高性能。总体而言，从头开始预训练似乎在大多数任务上获得了更高的性能。然而，我们对使用 CamemBERT 权重和分词器训练的模型进行了控制预训练实验，发现其结果与 DrBERT 4 GB 从头开始训练的结果相当。而使用 CamemBERT 权重和分词器的模型则存在稳定性问题。最后，我们的系统在11个下游任务中的9个任务上提供了更好的性能，并超越了通用模型 CamemBERT。我们还观察到，更专门化的数据更好，但它并不 scaling 得好。所有从 NACHOS 数据集中获得的预训练模型都可以在 Hugging Face 上免费获取，并且在 MIT 许可证下，所有训练脚本都在我们的 GitHub 存储库上。所以，谢谢你们的收看，我们期待在多伦多海报会上与你们交流。</sample>
    <sample id="319">论文研究了以下几种学习策略：从头开始训练模型、使用NACHOS数据集的持续预训练、使用CamemBERT权重和分词器的持续预训练、以及使用English biomedical模型PubMedBERT的持续预训练。</sample>
    <sample id="320">根据所提供的英文内容，过拟合因素似乎在本研究中并不重要。从图表右侧的红色最佳拟合线的梯度大于1可以看出，这意味着在CoNLL++上每单位改进在CoNLL-2003上的改进超过一单位，这表明没有递减回报。因此，在本研究中观察到的性能下降的主要原因不是过拟合。</sample>
    <sample id="321">根据所提供的内容，评估简化质量的方法是通过分析简化后的文本与原始文本之间的差异来确定的。这可以通过检查简化文本是否保留了原始文本的主要信息和含义来完成。此外，还可以使用诸如BLEU分数等指标来量化简化文本的质量，并将其与原始文本进行比较。</sample>
    <sample id="322">Enrico is presenting at ACL 23, discussing the question of what a text classifier learns about morality. He begins by explaining that human morality helps us distinguish right from wrong and is essential to our societies. However, current approaches in NLP often treat morality as a singular scale between immoral and moral, which can hide the pluralist nature of morality and lead to misunderstandings.

Enrico introduces the Moral Foundation Theory, which suggests that there are five different ways humans perceive morality, similar to how we have taste buds. Each person prioritizes these foundations differently, influencing their judgment of morality. This theory has been applied in natural language processing, with recent papers exploring its use in understanding and classifying morality in text.

Enrico's paper aims to understand what language models learn about morality by applying explainable AI techniques. They focus on how morality is expressed differently across domains, using the Moral Foundation Twitter Corpus, which contains 35,000 tweets from seven different domains, including #AllLivesMatter and #BlackLivesMatter. The study explores whether language models can recognize the fine-grained differences in morality expression across domains.

Enrico provides an example of the difference between ALM and BLM, noting that while they cover similar topics, their rhetoric for subversion (rebellion against authority) differs significantly. Language models recognize that subversion is frowned upon in ALM but encouraged in BLM. This highlights the importance of domain-specific understanding of morality and warns against using a single model for multiple domains, as it could lead to misunderstandings and dangerous consequences.</sample>
    <sample id="323">The paper titled "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA" by Yujie Wang from Shanxi University, China, addresses the challenge of Commonsense QA, which requires machines to answer questions that rely on common knowledge. The paper proposes a method called DHLK that combines language models and knowledge bases to retrieve relevant knowledge and infer answers.

The proposed method first builds an HKG based on multiple knowledge bases using a two-stage pruning strategy and KRL to optimize the structure and knowledge representation of the HKG. Then, it uses RoBERTa and Mask Self-Attention to encode and fuse QA contexts and entities, building the HKG. The method dynamically removes entities with weaker relevance to the QA context based on the attention weights of RoBERTa.

The entity and relation embeddings of the HKG are optimized using TransE, and Relation Mask Self-Attention is introduced to model subgraphs. RMSA is used to update the entity and relation embeddings of the HKG by iterating through L layers. Finally, the graph embedding of the HKG is obtained by applying max-pooling to the question key entities.

The HKG path information is incorporated into the QA context to get the embedding representation of the QA context after path enhancement. The final answer prediction is obtained by inputting the HKG graph for embedding, the paths and QA context embedded, and the QA context embedded into the MLP to get the answer probability.

The paper conducts experiments on CommonsenseQA and OpenBookQA using external knowledge bases: ConceptNet, WordNet, and Wiktionary. The results show that the proposed method gets good results compared with other LM and HKG methods.</sample>
    <sample id="324">是的，语言模型有不同的政治偏见。根据Shangbin的研究，他们发现语言模型在政治上具有不同的倾向，有些模型更倾向于自由派，而另一些模型则更倾向于保守派。</sample>
    <sample id="325">你好！我的名字是马蒂亚斯·林德曼，今天我要给你介绍我们关于“使用多集标记和潜在排列实现无树的组合泛化”的论文。这是我和导师亚历山大·库勒和伊万·提托夫合作完成的。组合泛化可以理解为学习者处理在训练中看到的短语的更深层次递归和看不见的组合的能力。在语义解析的背景下，测试组合泛化可能看起来像这样。就像往常一样，我们有一个训练集的句子。在这个例子中，“女孩睡了。”和“玛丽知道女孩睡了。”这些句子与逻辑形式配对，代表它们的核心含义。与标准机器学习评估不同，测试集不来自相同的分布，而是包含结构上看不见的逻辑形式。在这个例子中，模型在训练中已经见过浅层递归，但在测试时遇到了深层递归。朴素的seq2seq模型在这种出-of-distribution泛化方面往往难以应对，经常产生与输入脱离实际的输出。特别是，它们往往无法复制输入和输出之间的系统对应关系，如图中用颜色编码的那样。解决这个问题的一个流行方法是将树融入模型中。树旨在捕捉连接短语和逻辑形式的组合过程。这种方法效果很好，但树通常不会给出，需要某种方式来获取。这可能会很复杂，有时还需要昂贵的计算过程。通常，这涉及到处理逻辑形式的正式预处理，例如处理变量符号。获取树还可能涉及专门的语法推导程序。在这篇论文中，我们没有使用树，并引入了一个神经seq2seq模型，该模型直接模型化了输入片段和输出片段之间的对应关系。我们首次展示了在不依赖树的情况下，对深层递归的强泛化能力。我们的方法从两个步骤预测输出。首先，我们为输入中的每个标记分配一个无序的多集标记，这些标记将在输出中出现。在第一个步骤之后，我们有了所有正确的标记，但它们没有顺序。因此，在第二个步骤中，我们使用另一个模型来预测一个排列，以将它们放入正确的位置。我们引入了一种预测排列的新方法，它对可能的排列没有硬约束。这使我们的方法变得灵活和表达力强。概念上，我们的排列模型的工作原理如下。我们从左到右遍历输出，确定要放在输出中的多集标记。对于第一个输出位置，我们简单地选择一个，如红色所示。然后我们跳转到另一个多集标记，以确定输出中的第二个标记。我们通过类似的方式确定输出中的第三个标记，通过跳转到另一个多集标记。我们继续这个过程，直到第一个阶段的所有标记都被访问过一次。为了给你一个实验结果的 teaser，这里我们比较了我们的方法与其他无树模型在 COGS 基准上的性能。我们的模型在深层递归泛化方面比其他模型有很大优势。然而，还有一些其他类型的结构泛化仍然很难。在我们的论文中，我们解决了几个有趣的 Technical 挑战。首先，输入和输出之间的对齐在训练数据中并不给定。因此，对于给定的标记，我们不知道它来自哪个多集标记，这给训练带来了挑战。此外，有时候有多组与数据一致的排列，但语言上正确的排列是潜在的。我们通过在训练过程中引入对齐来解决这个问题。我们的排列方法非常灵活，但找到最高得分排列是一个 NP-hard 问题，因为它与“旅行商”问题有关。我们用 GPU 友好的连续放松近似这个问题，也允许我们通过解决方案反向传播并学习更有可能的语言排列。如果你想了解我们实验和如何解决这些问题，请查阅我们的论文或来看我们的海报。</sample>
    <sample id="326">认知失调是指两个不一致的信念或行动之间的不一致，例如一个人说：“我知道吸烟会杀死我”，然后又说：“会议结束后我抽了几支烟”。这种信念和行动之间的不一致就是认知失调。</sample>
    <sample id="327">本文介绍了ManagerTower，一种用于视觉-语言（VL）表示学习的新型架构。ManagerTower通过引入经理人来解决BridgeTower的两个明显限制：1.分层利用不同级别的单模表示是无效的；2.跨模布局的数量取决于使用的单模层表示数量，从而限制了其可扩展性和能力。ManagerTower在每个跨模层中使用多个单模专家的见解，并适应性地聚合这些见解。实验结果表明，ManagerTower在各种下游任务上取得了 superior 的性能，特别是在 Wikivideo 测试标准上，精度提高了 39.15%。</sample>
    <sample id="328">GPT-4是所有语言模型中最倾向于自由派的。</sample>
    <sample id="329">本文介绍了零-shot视频句子定位的工作，该工作由北京大学的 Minghang Zheng 及其团队完成。他们提出了一种噪声-resistant Structured Pseudo-Label 生成方法，可以训练视频句子定位模型而无需任何手动注释。该方法首先使用预训练的图像文本模型生成更复杂的自由形式伪查询，然后使用预训练模型衡量视频帧与伪查询的相关性，生成伪事件。最后，通过减少噪声样本的权重和创建噪声标签来减少标签噪声的影响。实验结果表明，该方法在两个数据集上优于现有方法。</sample>
    <sample id="330">在主动学习时，累积训练（Cumulative）比迭代训练（Iterative）更有效。在不同策略的比较中，累积训练在所有情况下都表现得等于或优于迭代训练。</sample>
    <sample id="331">演讲者的名字是Sara Papi。</sample>
    <sample id="332">MuDa基准中的数据是从TED演讲的转录中获得的，这些转录已被翻译成14种不同的语言。</sample>
    <sample id="333">The paper presents a novel training framework called INK (Injecting kNN Knowledge in Nearest Neighbor Machine Translation) for neural machine translation. The authors acknowledge the contributions of their collaborators and introduce the problem of non-smooth representation spaces in NMT models, which can lead to poor generalization and performance. To address this issue, they propose kNN-MT as a solution, which smooths predictions according to nearest neighbors in the representation space.

However, kNN-MT has two significant drawbacks: retrieving neighbors from a large datastore at each decoding step is time-consuming, and once the datastore is constructed, representations cannot be easily updated. To overcome these drawbacks, the authors propose the INK framework, which injects kNN knowledge into an MT model. The INK training loop has two steps: first, kNN knowledge is extracted from the datastore to guide the adapter to adjust representation, then updated representations are used to refresh the datastore asynchronously. This trend loop will run until convergence.

The authors conduct experiments on the full benchmark dataset and find that even for the WMT winner model, its representation space can still be greatly improved. They explore three research questions: whether a small adapter can smooth the representation space with the datastore dropped aside during inference, how much improvement can be brought by using kNN knowledge to adjust the representation distribution, and whether using an adapter and datastore together brings further improvements. The results show that the INK system outperforms the state-of-the-art kNN-MT system and achieves the best performance after smoothing the representation space. The INK system also achieves higher BLEU scores with less memory space and faster inference speed compared to using an adapter baseline.</sample>
    <sample id="335">演讲者的名字是马蒂亚斯·林登。</sample>
    <sample id="336">跨语言转移是指在一种语言上训练模型，然后将其应用于另一种语言的过程。在跨语言转移中，模型可以使用源语言的训练数据进行训练，并在目标语言上进行测试。这可以是零-shot、少-shot或翻译测试等不同设置。</sample>
    <sample id="337">The research presented in this speech focuses on the development of a graph-based relation mining approach for context-free out-of-vocabulary (OOV) word embedding learning. The study addresses the challenge of representing OOV words, which are critical to the performance of embedding-based downstream models. The proposed method leverages word formation and association rules to infer the meaning of OOV words by constructing a Word Relationship Graph that imitates lexical rules.

In the graph, each word or wordpiece acts as a node, and its corresponding word embedding serves as the node attribute. The first layer preserves all nodes to retain complete wordpiece information, while the second layer samples a fixed number of nodes for training to mitigate noise from wordpiece neighbors. A self-attention network assigns attributes based on the characters of OOV words, and two levels of Graph Attention Network are applied to capture important information and reduce the impact of noisy neighbor nodes.

To capture the whole graph information and summarize word formation, a readout block layer is incorporated. A simple one-layer Graph Convolutional Network is sufficient for the purpose, and contrastive learning with NT-XENT positive samples is applied in the loss function to encourage proximity between OOV words and their background embeddings.

Experiments demonstrate that the proposed model outperforms baselines in both intrinsic and extrinsic tasks, proving the effectiveness of learning OOV words by word formation. The model can bring benefits to both static and contextual models in downstream tasks. The possibility of adding languages to the model is discussed, with agglutinative languages being well-suited and fusional languages presenting more challenges. However, the model performs well with English by reasonable word segmentation.

Overall, the graph-based relation mining approach can handle various complex word formations and has potential applications in other languages, depending on the rationality of word decomposition.</sample>
    <sample id="338">Bingsheng's presentation, on behalf of a collaborative research group from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research, focuses on the evaluation of human natural language explanations in machine learning models. The presentation highlights the challenges in evaluating the quality of human-annotated explanations, which are subjective and task-dependent, unlike labels that can be objectively compared. Traditional metrics such as BLEU and ROUGE treat human annotations as gold standards but do not consider task differences or the utility of explanations during fine-tuning and inference stages.

The presentation introduces a unified structure for converting various tasks into a multiple-choice format, allowing for seamless application across different datasets. It discusses an in-depth experiment analyzing the utility of explanations by comparing baseline and infusion settings, where explanations serve as additional input to sequence-to-sequence models. The results show that fine-tuning with explanations can lead to substantial improvement, even with a small amount of data.

A novel evaluation metric called TREU (Task-Relevant Explanation Utility) is proposed, extending the simulatability score to include the helpfulness of explanations at fine-tuning. The presentation evaluates human explanations across five large-scale datasets using both the TREU metric and the simulatability score on two models, T5 and BART. The results support the intuition that human-annotated explanations can still benefit model predictions, even if they were considered low quality by humans in previous literature. The TREU scores consistently rank dataset qualities on both T5 and BART, while the simulatability score falls short in evaluating certain datasets.

The presentation concludes by emphasizing the importance of high-quality human collaboration in annotation jobs and recommends that researchers perform similar quality checks in the future. For more detailed findings, please refer to the paper.</sample>
    <sample id="339">The authors of this paper are affiliated with Saarland University in Germany.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA is presenting their work on ParaAMR, a large-scale syntactically diverse paraphrase dataset generated using AMR back-translation. The team includes Varun, I-Hung, Anoop, Kai-Wei, and Aram. Paraphrase generation is crucial for various NLP applications such as question answering, chatbots, and improving robustness. However, existing human-annotated datasets like MRPC, PAN, and Quora are limited in scale, while automatically generated datasets like back-translation lack syntactic diversity.

The goal of ParaAMR is to create a large-scale, syntactically diverse paraphrase dataset. The team uses AMR graphs, which capture the abstract meaning of a sentence through directed graphs with nodes representing semantic concepts and edges representing semantic relations. They change the focus of the graph by randomly sampling a node and setting it as a new root node, modifying the corresponding edge and its labels. Then, they use an AMR graph-to-text generator to generate text from the modified graphs.

ParaAMR contains around 15 million source sentences, with approximately 6.9 paraphrases per source sentence. Compared to other datasets using back-translation, ParaAMR generates more syntactically diverse paraphrases. Quantitative analysis shows that ParaAMR has similar semantic similarity scores to other datasets but higher syntactic diversity scores. ParaAMR benefits several NLP applications, including learning sentence embeddings, syntactic control paraphrase generation, and data augmentation for few-shot learning. The dataset is available at this link.</sample>
    <sample id="341">作者使用了平均延迟和计算感知平均延迟作为延迟测量方法。</sample>
    <sample id="342">The presentation introduces a paper titled "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming" by Gao Jingsheng and colleagues. The paper addresses the challenge of constructing large-scale video-sourced dialogue datasets, which are closer to real spoken conversations. The existing large-scale pre-trained dialogue datasets are mostly text-sourced, and there is a lack of session dialogue for each persona in personalized dialogue research. The paper proposes a unique automatic dialogue-constructing method to address these challenges.

The dataset, LiveChat, is constructed in three steps: extracting audio from videos, transcribing audio into utterances through ASR, and collecting audience comments and constructing dialogues using a reply-to-whom matching method. The persona information for personalized dialogue generation is extracted through manual labeling and trained persona classifiers.

The experiments conducted on two benchmark tasks, response modeling and addressee recognition, show that the extracted persona and longer average sessions are beneficial to the final result. The results also confirm that the domain of LiveChat is far away from the domains of existing dialogue datasets. The presentation concludes with a discussion on future work focusing on efficient transfer learning of LLMs for LiveChat.</sample>
    <sample id="343">大家好，我是Akshatha，今天我和我的合作者Martin一起介绍了我们合作的研究成果《KITMUS测试：评估多源知识整合能力》。这项研究是蒙特利尔大学、Mila和微软研究院的合作成果。自然语言理解模型依赖于各种知识来源，包括预训练时获得的参数中包含的知识和输入时提供的知识。最近的研究表明，模型可以利用预训练时的知识来解决任务。但在自然语言理解中，通常需要在推理时提供的知识。例如，在句子“John在电视上见到了新当选的总统。”中，预训练参数可以包含关于总统和电视的信息，但无法可靠地知道这个实例化的实体“John”是谁，或者新当选的总统是谁，因为总统自预训练以来可能已经发生了变化。因此，成功解决知识密集型NLU任务的模型需要能够整合和使用预训练时间和推理时间的知识。在这项工作中，我们提出了一个诊断测试套件，用于评估知识整合能力。我们引入了一个核心ference任务，旨在测试模型是否能够利用不同来源中的知识。我们用人类参与者和已建立的核心ference模型评估了数据集。以下是我们数据集中的一个示例。Servin是一名法官，Kea是一名面包师。Servin和Kea在公园里见过面。在一天的工作结束后，他很高兴在法庭上决定案件，他很开心地放松了一下。任务是确定代词“他”所指的正确实体，即Servin。解决给定代词需要两种信息。首先，实体特定知识，如“Servin是一名法官。”其次，背景知识，如“法官在法庭上决定案件。”一般来说，背景知识是在大规模语言模型预训练过程中学习到的，而实体特定知识通常是在推理时观察到的。我们通过控制不同来源中事实的可获得性来区分三种KITMUS设置。第一种是典型的设置：“背景-预训练”，其中背景知识被认为是在预训练时可用的。第二种是“背景-两者”，其中背景知识既在预训练时又在推理时可用。最后一种是“背景-推理”，其中所有知识类型都在推理时可用。这种最后一种设置尤其有趣，因为它模拟了背景知识对于解决任务来说不是预训练数据的一部分的情况。例如，自预训练以来已经出现了新的职业。以下是我们控制不同来源中事实可获得性的示例。在“背景-预训练”设置中，我们假设背景知识“政治家寻求当选政府职位”包含在预训练参数中，并在推理时提供实体特定知识“Chichester是一名政治家”。在“背景-两者”设置中，我们除了提供实体特定知识外，还提供政治家的背景知识作为推理时的上下文。在“背景-推理”设置中，我们提供虚构的职业“mirituer”而不是政治家，因为“mirituer”很可能不会包含在预训练参数中。我们用人类参与者和已建立的核心ference模型评估了数据集。在图中，我们展示了在最困难的“背景-预训练”设置中最优模型的结果。没有针对KITMUS进行特定任务训练，两个模型的表现都不好。然而，当它们被训练在KITMUS上时，两个C2F和BERT4Coref模型的表现显著优于随机选择。这表明，当在通用引用解析数据集上进行泛化学习时，大多数模型学会利用表面线索，这些线索在KITMUS测试中并不适用。额外实验显示，即使表现最好的模型也不能可靠地整合仅在推理时提供的反向知识。总之，我们的主要 takeaway是许多核心ference模型在没有特定任务训练的情况下无法推理出来自不同来源的知识。然而，经过特定任务训练后，一些模型成功地整合了多源知识。尽管如此，即使表现最好的模型似乎在可靠地整合仅在推理时提供的反向知识方面存在困难。如果您想了解更多信息，请查阅我们的论文，并查看GitHub上的数据集和代码。谢谢您的收听。</sample>
    <sample id="344">基于树的方法有以下缺点：

1. 要获得树通常需要复杂的预处理和专门的语法推导程序，这可能会导致计算昂贵。
2. 树方法通常在训练数据中没有给定输入和输出之间的对齐，因此对于给定的标记，我们不知道它来自哪个多集标记。这会使训练变得困难。
3. 有时存在多个与数据一致的排列，但正确的排列是潜在的。我们通过在训练过程中引入对齐来解决这个问题。

这些挑战使基于树的方法难以在没有树的情况下实现强泛化。</sample>
    <sample id="345">本文介绍了我们团队的研究论文《Multiset Tagging and Latent Permutations for Compositional Generalization without Trees》。该论文由Matthias Lindemann、Alexander Koller和Ivan Titov共同完成。论文主要研究了如何在不使用树的情况下实现组合泛化，即模型能够处理更深层次的递归和未见过的短语组合。在语义解析领域，测试组合泛化通常涉及训练数据集和测试数据集的对比。例如，在训练数据集中，我们有“女孩睡了”和“玛丽知道女孩睡了”的句子，它们与逻辑形式配对。然而，在测试数据集中，包含结构上未见过的逻辑形式，如“女孩睡了，然后她起床了”。标准的序列到序列（seq2seq）模型在这种情况下往往无法泛化，因为它们难以捕捉输入和输出之间的系统对应关系。

为了解决这个问题，论文提出了一种神经seq2seq模型，它直接建模了输入片段和输出片段之间的对应关系，而无需使用树。该模型分为两个步骤：首先，为输入中的每个标记分配一个无序的多集标记，表示将在输出中出现的标记；其次，使用另一个模型预测一个排列，以确定这些标记在输出中的顺序。这种方法避免了树的复杂性和计算成本，同时保持了模型的灵活性和表达能力。

实验结果表明，该方法在COGS基准测试中显著优于其他树less模型，特别是在处理更深层次递归方面。然而，该方法在解决一些结构泛化挑战时仍然存在困难，如输入和输出之间的对齐、多组可能的排列以及寻找最高得分排列的NP-hard问题。论文通过引入一种GPU友好的连续放松方法来解决这些问题，该方法允许反向传播并学习更有可能的排列。</sample>
    <sample id="346">很抱歉，根据所提供的信息，我无法确定论文作者所属的机构。</sample>
    <sample id="347">大家好，我是Myra，今天我要和大家分享我们团队的研究论文《标记的人物：使用自然语言提示衡量语言模型中的刻板印象》。这项研究是与Esin Durmus和Dan Jurafsky合作完成的。近年来，许多研究记录了大型语言模型（LLM）中存在刻板印象和社会偏见的现象。然而，这些衡量方法存在一些限制。它们通常依赖于手工制作的数据集，耗时且费力。此外，它们往往只衡量非常具体的刻板印象，这意味着它们不能推广到其他 demographics 或语境，或者它们仅仅捕捉到非常广泛的广泛关联，比如对特定群体的负面关联。大多数在这个领域的工作也没有考虑交集性，即多维社会身份可以加剧偏见并造成独特的伤害。为了克服这些限制，我们利用这些 newer 的指令调优LLM具有很好的响应指令和提示的能力。因此，我们可以要求模型生成人物，即通过提示“想象一下你是一个亚洲女性。描述你自己。”来描绘一个想象中的个体。我们可以立即看到，虽然输出结果并不是传统意义上的负面或有毒的，但其中有一些有趣的模式。亚洲女性被描绘为低调；中东女性被提及使用“异国情调”和“迷人”等词语；而少数族裔女性的描述则提到了 ancestry，而白人男性的人物描述中则没有提到这一点。为了捕捉这些模式，我们的方法有两个部分。第一部分是生成这些人物。我们生成人物的提示受到一项研究的启发，该研究发现通过向人类主题提供这些提示，他们能够揭示种族刻板印象。这也使我们能够直接比较我们生成的人物和人类写的回应。第二部分是标记词，这是一种识别区分标记群体和未标记群体的词语的方法。这种方法的好处是，我们可以获得非常具体的刻板印象和模式，而无需依赖任何特定的词汇表。我们的方法基于 sociolinguistic 概念“标记性”，即有一个未标记的默认值，任何与默认值不同的群体在语言上都是标记的。例如，“战士”这个词通常与男性相关联，所以当人们描述一个女性战士时，他们会说“女性战士”，用“女性”一词标记这个词。更广泛地说，社会中的 dominant 组织通常是未标记的，而边缘化群体通常是标记的。因此，在我们的方法中，我们首先指定未标记和标记的群体，然后使用 Fightin’ Words 方法来区分每个标记群体的 top 词语。例如，对于黑人女性的人物，我们将使用 Fightin’ Words 方法，并将 log-odds 比率与白人人物和男性人物进行比较，因为这两个是对应的未标记群体。现在让我们看看一些结果。首先，我们使用一个刻板印象词汇表，发现生成的人物包含比人类写的人物更多的刻板印象。然而，当我们查看词汇分布和词汇表时，发现了一些不同的事情。虽然生成的人物具有更高的词汇表词频，但人类写的人物具有更广泛的词汇范围，而生成的人物中的刻板印象词只是“高”和“强壮”等正面或至少是非负面的词。实际上，这个词汇表并没有捕捉到我们在前面幻灯片中看到的一些有害的模式。因此，我们将转向我们标记词方法的结果，以展示这些正面的描绘如何促进刻板印象和本质化叙事。在我们的分析中，我们揭示了这些看似正面的描绘反映了有害的模式。首先，从我们的群体中，top 词语包括“文化”、“传统”、“自豪”和“异国情调”。这些词语定义了这些群体与其身份的关系，并将它们与其他群体区分开来，这有助于长期的歧视和其他化现象。此外，还有一些常见的 tropes 反映在这些词语中，特别是对于少数族裔女性。例如，描述 Latina 女性的词语包括“充满活力”和“丰腴”，这些词语与热带主义的 tropicism 相关。对于 Asian 女性，词语包括“娇小”、“玲珑”和“柔顺”，这些词语与长期存在的亚洲女性被性化、被视为顺从和顺从的历史相关。最后，对于黑人女性，我们看到一些 top 词语包括“坚强”和“ resilient”。这些词语与被称为“坚强黑女人”原型相关。虽然听上去很正面，但有研究表明，这种原型实际上是有害的，因为它给这些 demographic 带来了很大的压力，要求他们坚强和坚强，以应对社会障碍。因此，与其试图改变这些问题，它反而导致这些 demographic 的负面健康结果，等等。更广泛地说，我们发现每个标记群体的词语几乎都反映了本质化叙事。基于这些模式，我们得出三个建议供模型所有者参考。首先，作为研究人员，我们应该解决正面刻板印象和本质化叙事。我们也应该使用交集性视角来研究偏见和伤害，因为如果没有这样做，可能会错过一些问题。最后，应该增加关于偏见缓解方法的透明度，因为例如这些正面刻板印象，我们不知道是因为某种过度的价值观对齐导致了这些有害的模式，还是因为其他反刻板印象方法导致了这些模式。我们不能假设或进一步研究这个问题，除非有更多关于偏见缓解方法的透明度。谢谢大家收听。祝 ACL 玩得开心。</sample>
    <sample id="348">本文介绍了研究者Myra及其团队开发的一种新方法，用于测量大型语言模型（LLM）中的刻板印象。他们利用LLM对指令和提示的响应能力，生成不同 demographic 的 persona 描述。通过比较这些 persona 与人类写的答案，研究者识别出刻板印象的模式。他们使用“标记词”方法来识别区分标记群体和未标记群体的词语。结果表明，虽然生成的 persona 包含更多刻板印象，但人类写的答案具有更广泛的词汇分布。研究者指出，这些积极的形象实际上反映了有害的刻板印象和本质化叙事。他们建议模型所有者应该关注积极刻板印象和本质化叙事，使用交叉学科视角研究偏见和伤害，并增加关于偏见缓解方法的透明度。</sample>
    <sample id="349">大家好，我是来自中国科学技术大学的易 Jingwei Yi。我很高兴能为我们的论文做一个简短的广告视频。你们在复制我的模型吗？保护大型语言模型的版权，通过后门水印将其作为服务嵌入。首先，让我们介绍一下嵌入作为服务的背景。目前，大型语言模型如GPT、LLAMA和PALM在自然语言理解和生成方面表现出色。嵌入作为服务是基于大型语言模型构建的服务之一，用于辅助各种NLP任务。例如，OpenAI提供了一个基于GPT的嵌入API。然而，最近的研究表明，攻击者可能通过学习嵌入来窃取模型，并提供类似的服务。因此，保护嵌入作为服务的版权变得必要。为了保护嵌入作为服务的版权，一种解决方案是在提供的服务中嵌入一个水印，并检测另一个服务是否包含该水印。水印方法需要满足以下属性：首先，该方法应适用于嵌入作为服务。其次，水印不应降低所提供的嵌入的实用性。最后，水印需要在模型提取过程中转移到攻击者的服务上。现有的方法可以大致分为四类。然而，这种方法要么不适用于嵌入作为服务，要么缺乏可转移性。因此，在本文中我们提出了嵌入标记，这是一种基于后门的水印方法，适用于嵌入作为服务。接下来让我介绍嵌入标记的详细内容。嵌入标记包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一组在适度频率间隔内的单词。我们假设提供者可以收集一个通用文本语料库，并计算单词频率。在水印注入中，我们首先定义目标嵌入。当用户向提供者服务发送句子时，提供者计算句子中的触发词数量。所提供的嵌入是目标嵌入和原始嵌入的权重求和。目标嵌入的权重与句子中触发词的数量成正比。当句子中的触发词数量大于m时，所提供的嵌入等于目标嵌入。版权验证是检测另一个服务背后是否存在水印。我们首先构造一个后门数据集和一个 benign 数据集。后门数据集包含所有单词都属于触发集的句子，而 benign 数据集中的句子中不存在触发集中的单词。然后，提供者要求攻击者的服务提供带有数据集的嵌入。我们计算请求的嵌入与目标嵌入之间的余弦相似性和L2相似性。我们还应用KS测试，并使用其p值作为第三个指标。我们在AG News、MIND、SST2和Enron Spam四个数据集上进行了实验。我们假设提供者使用wiki文本数据集来计算单词频率。四个数据集上的实验结果表明，我们的嵌入标记可以在保持下游任务的实用性的同时，具有很好的检测性能。我们还通过可视化四个数据集上的句子嵌入来验证所提供的嵌入的隐蔽性。图例表示每个句子中的触发词数量。如图所示，很难区分后门嵌入和正常嵌入。这就是全部。谢谢。欢迎与我们讨论。</sample>
    <sample id="350">The paper "What’s the Meaning of Superhuman Performance in Today’s NLU?" by Simone Tedeschi and several renowned researchers explores the reliability of leaderboard scores in comparing human and AI performance in natural language understanding (NLU) tasks. The study focuses on two popular benchmarks, SuperGLUE and SQuAD, which are used to evaluate systems' ability to understand language.

The researchers found that while AI systems often outperform humans on these benchmarks, there are significant issues with the comparison. For example, humans are usually evaluated on a much smaller subset of the test data compared to AI systems, and there are errors in the ground-truth answers provided for the datasets. Additionally, the pay rates for human annotators vary widely, which can affect the quality of the human performance.

The paper argues that these issues make it difficult to accurately compare human and AI performance. The authors recommend that future benchmarks should provide more detailed information about the annotator pool and ensure that humans are adequately motivated to produce high-quality results. They also suggest using more scientifically meaningful methods to estimate human performance, such as comparing the scores of the best systems with those of the best possible humans.

Overall, the paper highlights the need for more reliable and scientifically grounded benchmarks in NLU research to avoid repeating the same mistakes and to better understand the true capabilities of AI systems.</sample>
    <sample id="351">The speaker, Shuheng, presents a paper titled "Do CoNLL-2003 named entity taggers still work well in 2023?" The paper investigates the problem of generalization using the Named Entity Recognition (NER) task and explores whether models developed for NER in CoNLL-2003 can generalize to modern data. The paper also examines what is needed for good generalization when developing new taggers and identifies the causes of performance drop if poor generalization is observed.

To investigate these problems, the authors developed the CoNLL++ Dataset, which consists of data collected from Reuters News in 2020 annotated with the same CoNLL-2003 annotation guidelines. They fine-tuned over 20 models on CoNLL-2003 and evaluated them on both the CoNLL-03 test sets and the CoNLL++. The percentage change in F1 score was calculated to assess the generalization of each model.

Through experiments, the authors found that three main ingredients are needed for good generalization: the model architecture, model size, and more fine-tuning examples. Transformer models were found to generalize better to new data, larger models lead to better generalization, and more fine-tuning examples also improve performance.

The authors also investigated the causes of performance drop and identified two hypotheses: adaptive overfitting and temporal drift. Adaptive overfitting occurs when the same test set is reused multiple times, leading to diminishing returns on a new test set. Temporal drift occurs when there is a performance degradation due to the increasing temporal gap between the training and test data. The authors found that temporal drift is the main cause of performance drop, as retraining or continuing pre-training some models with more recent data confirms this hypothesis.

In conclusion, the paper suggests that for good generalization, a better model architecture, larger model size, and more fine-tuning examples are needed. The performance drop observed is caused by temporal drift, not adaptive overfitting. The paper calls for more research on improving the generalization of models and encourages readers to check out the paper, dataset, and contact the author if they have any questions.</sample>
    <sample id="352">ABC-Eval 代表 "Annotating Behaviors in Chat"。</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Questions" by Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, and Iryna Gurevych addresses the challenge of input underspecification in code generation and program synthesis. The authors propose a method to create CodeClarQA, a synthetic dataset with clarifications on key operations, and a pipeline of code generation by asking clarification questions. They identify key operations and corresponding documentation from the code, represent them in latent space using their schemata, and compute similarity scores of all schema element pairs between an NLD and the operation documentation. If all element pairs for the similarity score is lower than threshold T, the key operation is missing; otherwise, it is aligned. They also hire annotators to annotate the validation set and the test set. They adopt templates to create CQAs for missing key operations and generate two types of questions: yes-or-no questions or multiple-choice questions. They use heuristics to extract key operations based on the code knowledge graph generated by Graph4Code and show the generated graph of the example. Each node is an operation where the key operations are marked in red and the rest in gray. Edges show the data flow. They identify if a key operation is missing or aligned. They notice that MPNet has the best performance of identifying missing key operations among all these models. They also notice some common errors which reflects the challenges and potential directions to improve their method, including taxonomy, that aligned operations might require clarification to be distinguished from operations with similar names, and argument, where they use documentation of operations instead of using argument values as well. They have a Clarification Need Predictor, a Question Selector, and a Code Generator. They test their pipeline and see that model performances on all evaluation metrics, including with more high-ranked CQs being answered and included, increases. However, there's an opposite trend of unanswered clarifications, and at the same time, their pipeline is still underperforming the model-only trainer NLDs and code, which is expected as they fine-tune the models on all CQAs and CQ ranking task is a challenge. They analyze the results and see that clarified key operations are the reason for better generated code. They give some examples of predictions and see that training with Oracle CQAs leads to predictions close to the ground truth with only minor differences. However, the task is challenging as the top five ranked CQs do not include CQs in the reference CQAs, leading to the pipeline prediction including a call of confusion matrix missing the classes mentioned here.</sample>
    <sample id="354">根据所提供的信息，CoNLL-2003 和 CoNLL++ 之间的性能增量高于5个百分点的年份没有明确说明。</sample>
    <sample id="355">你好，我的名字是Vasudha，我是一名在石溪大学攻读计算机科学博士学位的学生。我很高兴能代表我们团队在ACL 2023上展示我们的论文《基于迁移学习的不一致检测：解决罕见类别挑战》。我们首先定义了认知不一致，并解释了为什么研究这个问题很重要。简单来说，认知不一致是指两个不一致的信念或行为，例如一个人说：“我知道吸烟会杀死我”，然后又说：“会议结束后我又抽了几支烟”。这种信念和行为之间存在不一致，它们处于不一致状态。进一步提到“我不知道没有它们我能不能保住工作”来为第二次吸烟行为进行合理化。这些信念和行动之间的关系是和谐的。虽然认知不一致在日常决策中非常普遍，但在其他类型的 discourse 关系中表达的认知不一致却很少见。那么，为什么这很重要呢？研究认知不一致可以帮助我们理解人们之间的 disagreement 的影响，跟踪趋势和信仰变化，以及更好地了解人群的态度变化。高认知不一致与焦虑障碍有关，可以帮助我们更好地理解人们的心理健康状况。研究语言中表达的认知不一致也可以帮助我们理解极端主义和边缘群体的 polarization。最后，认知不一致对于理解个人的认知风格和更好地理解决策过程也很重要。为了创建一个认知不一致资源，我们在大规模标注不一致关系方面进行了大量标注。我们采用了认知不一致优先的方法，如图所示。推特通过 PDTB 解析器传递，根据我们在论文中描述的指南对 discourse 单元对进行标注。正如这里所示，只有 3.5% 的标注对存在不一致关系。在收集了大约 1,000 对 discourse 单元后，我们对一个仅使用 43 个标注的不一致样本训练了一个初始分类器。不出所料，分类器的表现并没有比随机猜测好多少。鉴于不一致关系出现率低且缺乏先例数据集，我们面临着绝对罕见的问题。为了缓解这个问题，我们实验了迁移学习和主动学习的组合，以标注更多的不一致样本，同时降低标注成本并提高不一致检测能力。由于初始模型无法捕捉到不一致类别，我们在开始主动学习过程时，从两个密切相关任务中转移权重：主题独立的不一致观点分类任务，该任务确定来自不同人的辩论声明是否一致或不一致，无论主题如何，我们将其称为辩论；以及 PDTB 中的扩展和比较类别的二元分类任务，因为这两个任务与认知一致和认知不一致的概念密切相关，我们称它们为 CE。我们发现，在标注数据集上进行零-shot 性能已经明显优于随机猜测，最佳 AUC 为 0.62。进一步通过在两个任务上迭代微调，我们发现先微调 CE 任务，然后进一步微调辩论任务，可以得到更好的零-shot 性能。因此，这是我们在冷启动主动学习中使用的模型。接下来，我们确定了更新模型的新数据的最佳方法。"累积"策略将所有已收集的数据累加起来，而"迭代"策略则通过训练最新收集的数据集来更新模型。在不同的策略中，我们发现累积策略在整体上表现得更好或与迭代策略相当。接下来，为了增加不一致示例的数量，我们使用概率罕见类别策略（PRC）来选择最有可能被当前模型预测为罕见类别的示例。我们将这个策略与其他社区中常用的先进主动学习策略进行了比较。我们发现，提出的 PRC 策略在任务上的表现优于其他先进策略，尽管差异很小。在进一步的 AL 圆圈中，我们提高了不一致分类 AUC 到 0.75，这是目前在任务上取得的最佳表现。我们还检查了每种策略的可注释性和成本。我们发现，PRC 策略具有最高的百分比不一致，并且最适合罕见类别。然而，注释员也发现这些示例很难。总之，我们发现 PRC 是一种简单的 AL 策略，用于获取罕见类别和冷启动 AL，通过设计适当的迁移学习任务来显著提高。我们还发现，迭代更新对于从不同领域迁移学习有用，而在领域内标注则受益于累积更新。以下是我们的核心数据集和论文的链接。如有任何问题，请随时联系我们。谢谢。</sample>
    <sample id="356">很抱歉，我无法回答这个问题。您能提供更多的上下文吗？</sample>
    <sample id="357">演讲者的名字是Yuan Siyu。</sample>
    <sample id="358">根据英语内容，这篇论文有五位作者：Kayo Yin、Patrick Fernandes、Emmy Liu、André F. T. Martins 和 Graham Neubig。</sample>
    <sample id="359">该方法与专门用于同时预翻译的最新架构进行了比较。</sample>
    <sample id="361">Armineh Nourbakhsh, a PhD student at Carnegie Mellon University's Language Technologies Institute and research director at JP Morgan AI Research, presented her work titled "CounterComp" which aims to enhance compositional generalization for multi-step quantitative reasoning in question answering tasks. The study focuses on financial tables where users ask questions like calculating the net change in revenue from 2019 to 2020, involving multiple arithmetic operations.

Current state-of-the-art neural models struggle with tasks requiring more than two steps due to memorizing spurious patterns, such as associating tokens like "2019" with specific operations. To address this, CounterComp uses counterfactual scenarios to train models to focus on relevant tokens and operations. This involves treating each training sample as an anchor and finding positive and negative examples by altering components of the question that affect the output. These examples are used to add an auxiliary metric learning loss to the training procedure, adjusting the margin based on the extent of change in the questions.

The addition of this auxiliary loss improves performance across three state-of-the-art baselines, especially when dealing with more complex reasoning steps. It also enhances performance on out-of-distribution samples, both within and outside the training dataset. Qualitative results show that CounterComp helps the model attend to more meaningful tokens during training, leading to better compositional generalization.

The presentation concludes with references to the main paper and encourages further exploration through the provided poster. Contact information is given for any inquiries. The speaker thanks her co-authors, advisor at CMU, and co-advisor at JP Morgan for their support.</sample>
  </task>
</testset>