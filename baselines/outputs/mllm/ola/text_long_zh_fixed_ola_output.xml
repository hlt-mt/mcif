<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是大规模网络爬虫数据。</sample>
    <sample id="1">该论文的作者来自蒙特利尔大学、Mila和微软研究。</sample>
    <sample id="2">The paper presents a novel pre-trained model, LayoutMask, for Visually-rich Document Understanding (VrDU) tasks. Unlike previous studies that use global 1D positions to represent the reading order of tokens in documents, LayoutMask proposes to use local 1D positions, which are in-segment token orders. This approach allows LayoutMask to infer global reading order by jointly using 1D position, 2D position, and semantic information. To further promote text-layout interactions, LayoutMask equips the commonly used pre-training objective, Masked Language Modeling, with two novel masking strategies: Whole Word Masking and Layout-Aware Masking. The paper also introduces a new pre-training objective, Masked Position Modeling, which has a symmetric pre-training objective: recovering randomly masked 2D positions during pre-training. The experiments compare the performance of LayoutMask using different layout information, and the results show that Local-1D outperforms Global-1D on both FUNSD and SROIE datasets.</sample>
    <sample id="3">大家好！欢迎来到我们关于DEPLAIN的介绍，DEPLAIN是一个新的德语文本识别语料库，用于文档级别和句子级别的文本识别。我是雷纳·施多恩，我将引导您 walkthrough 第一部分的介绍。首先，我们来定义一下文本简化。文本简化是一种过程，通过调整文本以改善特定目标群体的理解能力，比如阅读困难的人或非母语者。为了训练文本简化模型，我们需要平行对齐的文本，例如文档或句子。这里有一个例子，你可以看到一个复杂德语句子及其翻译成简单语言的并行对齐句子对。为了简化句子，可以采用不同的技术，如词汇替换、子句删除、重新排序或插入单词。现在我们提出了一个新的语料库DEPLAIN，因为近年来一些问题影响了现有的语料库。例如，这些语料库太小，无法训练文本简化模型。另外，最近提出的三种模型都是自动对齐的，这意味着它们可能包含错误的对齐。因此，我们提出了一个新的语料库DEPLAIN，它分为两个子语料库：DEPLAIN-apa和DEPLAIN-web。DEPLAIN-apa基于新闻文本。在DEPLAIN-apa中，我们手动对483篇文档进行了对齐，结果产生了大约13,000个并行句子对。对于DEPLAIN-web，这个语料库包括不同的领域，我们手动和自动对齐了其中的750篇文档。总共，我们得到了30,450个句子对。我们进一步分析了我们的句子对，例如，从简化类型来看，圣经文本比新闻文本或学习者文本更加强化。在所有级别上，包括词汇简化、结构简化和整体简化。此外，我们可以看到DEPLAIN语料库具有高度的简化变换多样性。例如，在DEPLAIN-apa语料库中，我们有很多重排和插入词，而在DEPLAIN-web语料库中，我们有很多改写。现在让我们看看我们如何利用这个语料库。你好，我是奥马尔，现在我将讨论DEPLAIN数据集的应用场景。第一个应用场景是评估自动对齐方法。近年来，已经提出了许多对齐方法，但在机器翻译的背景下，我们有两个用不同语言写的并行文档，我们想要提取两个文档中句子的对齐。但在我们的情景中，我们正在提取具有相同语言和内容但复杂度不同的两个并行文档之间的句子对齐。现在，由于我们有DEPLAIN语料库，其中包含手动对齐的句子，我们可以使用这些句子作为金标准对齐来评估一些提出的对齐方法。我们对提出的对齐方法进行了适应，并在论文中出版了所有这些适应和运行实验的代码。最后，我们得出结论，对于德语文本简化，MASSalign方法是最优的自动对齐方法。你也可以在论文中找到运行这种方法的代码。第二个应用场景是在论文中展示的，即通过微调语言模型生成简化文本。我们分别微调了两个模型：长MBART用于产生文档级别的简化，以及标准MBART用于产生句子级别的简化。你也可以在论文中找到所有检查点和详细的分数和评估指标。我们得出结论，这种基本微调可以产生或获得比 baseline 分数更好的分数，并将这些结果作为未来自动文本简化问题的基本基准。谢谢大家的关注，我们希望在会议期间与你们见面。谢谢。</sample>
    <sample id="4">演讲者的名字是Kayo Yin。</sample>
    <sample id="5">他们使用了部分重叠背景知识的模型，准确率在82%-87%之间。</sample>
    <sample id="6">Jiaan is presenting their work on "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" alongside Fandong, Duo, Yunlong, Zhixu, Jianfeng, and Jie. They propose a new setting called many-to-many summarization, which aims to build one summarization model that can process a document in any source language and generate its summary in any target language. They conduct preliminary studies to analyze multilingual, cross-lingual, and many-to-many summarization, finding that many-to-many summarization helps transfer task knowledge across different languages better than previous models. They also introduce PISCES, a pre-trained many-to-many summarization model that learns language modeling, cross-lingual ability, and summarization ability through three-stage pre-training. The experiments conducted on the WikiLingua dataset show that the multilingual model trained in the many-to-many summarization setting outperforms other settings. PISCES also outperforms various baselines, including mBART-50 and mT5, and ablation studies verify the effectiveness of each training stage. Human studies show the superiority of PISCES.</sample>
    <sample id="7">是的，根据所给的英文内容，CoNLL-2003 标注器仍然有效。</sample>
    <sample id="8">ABC-Eval方法通过明确标注对话模型的行为，如提供不相关的信息或自相矛盾，来减少人工评估的主观性。这种方法旨在更精确和可靠地评估对话质量的各个方面。</sample>
    <sample id="9">现有弱监督方法的成功在很大程度上依赖于干净的验证样本。</sample>
    <sample id="10">根据所提供的英文内容，可以采取以下措施来提高分数：

1. 提供更多的背景知识：通过提供更多的背景信息，如实体的详细描述、图片或额外的上下文，可以帮助模型更好地理解实体之间的差异，并做出更准确的选择。
2. 改进模型：开发更好的模型，能够更好地处理间接引用表达和实体之间的相似之处，可以提高分数。这可能涉及改进模型的训练数据、算法或架构。
3. 增加数据集大小：更大的数据集可以为模型提供更多的训练机会，从而提高其性能。这可以通过增加数据集中的实体数量、领域或间接引用表达来实现。
4. 优化数据集收集方法：优化数据集收集方法，如使用更有效的标注过程或更高质量的标注，可以提高数据集的质量，并有助于提高模型的性能。

总的来说，通过改进模型、增加数据集大小、优化数据集收集方法和提供更多的背景知识，可以提高分数。</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presented "Do Androids Laugh at Electric Sheep? Humor 'Understanding' Benchmarks from The New Yorker Caption Contest" at ACL. This work is a collaboration with researchers from the University of Utah, Cornell University, University of Washington, Air Mail, and OpenAI. Large language models can now generate and explain jokes, as demonstrated by Google's 540 billion-parameter PaLM model explaining a joke about TPUs. However, the question remains whether these models truly understand humor.

Hessel used The New Yorker Caption Contest data to create three tasks: matching, quality ranking, and explanation generation. In the matching task, CLIP fine-tuned on the annotated corpus achieved around 62% accuracy, compared to a 20% random-guessing baseline. Humans scored around 94%, indicating a significant gap in humor understanding. For the quality ranking task, GPT-4's five-shot performance still lagged behind human raters, even when conditioned with human-authored descriptions of the images.

In the explanation generation task, GPT-4's explanations were often inaccurate, such as claiming the customer was saying "He'll be back" instead of the people working at the establishment. Human evaluations showed that human explanations were preferred over GPT-4's in more than two-thirds of cases.</sample>
    <sample id="12">这篇论文有五位作者：Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan和Dietrich Klakow。</sample>
    <sample id="13">Daniel Rotem is presenting his work, "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," which was done in Professor Roy Schwartz's lab at the Hebrew University in Jerusalem. Adaptive inference is a method for reducing the inference time of large language models by using low-capacity models for easy samples. The two most common adaptive inference methods are Multi Model and Early Exit. Multi Model involves storing multiple models together, each fit with a classifier at the end, while Early Exit involves fitting multiple classifiers to the model following intermediate transformer layers. Rotem hypothesizes that conflicting gradients occur in Early Exit training processes, where each classifier updates model weights trying to optimize its own goal, potentially harming performance for all classifiers involved. He tested this hypothesis by comparing individual Early Exit models' classifiers with separate Multi Model classifiers, which outperformed those of Early Exit by an average of 2.3%. Rotem presents SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method for Early Exit architectures that avoids conflicting gradients completely. SWEET closes most of the gap between Early Exit and Multi Model but negatively affects later classifiers in some cases. Rotem's work shows the existence of conflicting gradients in Early Exit training processes and introduces the SWEET method, which motivates future research and fine-tuning algorithms tailored to the Early Exit architecture.</sample>
    <sample id="14">大家好，我是亚当·普日波尔科维，今天我要讲的主题是协调的依赖结构。正如你们所知，不同的理论和语料库方法假设了不同的依赖结构。例如，在通用依存关系中，协调结构如“Lisa，Bart和Maggie”中，第一个连词从句是整个协调结构的主干。因此在这种情况下，“Lisa”是主干。类似地，在伊戈尔·梅尔切克的意义文本理论中，整个协调结构同样由第一个连词从句来主导。这两种方法都是不对称的，因为它们将其中一个连词从句突出。现在，让我们看看不对称的协调结构，比如布拉格方法。在布拉格依存树库中，协调结构由连词来主导，因此我们得到从连词到所有连词从句的依赖关系。最后，还有一个多头方法，例如在哈德森的《词 grammar》中使用，他们认为所有连词从句都是协调结构的主干。因此我们得到从治理者到所有连词从句单独的依赖关系：Lisa、Bart和Maggie。现在本文的目标是提出一个关于对称协调结构的新论据，比如这些，而反对不对称的协调结构，比如这些。好的，这个论据基于依赖长度最小化的原则，我将在这些例子的基础上进行解释。在英语中，正如你们所知，直接宾语更喜欢靠近动词，而状语可以离动词更远。因此，“Marge读了昨天的它”是可行的，因为直接宾语离动词很近，而“Marge读了昨天的它”则更差。这是因为在这里，介词“昨天”位于动词和直接宾语之间，所以离动词更远。然而，这种效果可能会因为在直接宾语非常重和很长的情况下得到缓解。因为在这种情况下，可以直接宾语移动到介词之后。这在下面的例子中说明了这一点。因此这两个句子都是可行的。“Marge读了这本书，这本书绝对 fascinating 关于蜜蜂。”这是由于直接宾语非常长，所以可以放在介词之后。但是，我们也可以这样写：“Marge读了昨天的这本书，这本书绝对 fascinating 关于蜜蜂。”所以推理是，尽管这个句子违反了通常的语法原则，即直接宾语应该离动词更近，但它满足了依赖长度最小化的原则，即较短的依赖关系更可取。因此，这两个树只显示了关键依赖关系的长度，即不常变化的依赖关系。所以这里我们有一个从“读”到介词的依赖关系长度为7（按单词计算），从“读”到“书”的依赖关系长度为4，总共是11。当我们交换这两个成分时，这两个依赖关系的总长度变为6。因此，与11相比，6更短。这就是为什么这句话听起来挺好的原因。它违反了一个原则，但满足了另一个原则。好的，所以我们在增强版的Penn树库中提取了各种统计信息，并参考了论文“为什么你不使用通用依存关系”，这些统计信息证实了之前多次观察到的现象：左连词从句往往更短。因此，“盐和胡椒”而不是“胡椒和盐”，按音节计算。此外，之前在解析中所做的观察也得到证实，即这种趋势随着两个连词从句长度差异的增长而增长。因此，当两个连词从句长度差异增大时，较短的连词从句更倾向于成为第一个；更强，对；比例更大。左边短的连词从句。但是，当治理者在左边或不存在时，这种趋势就会消失。例如，在这个例子中，“我看见了Bart和Lisa”，治理者在左边。而在第二个例子中，“Homer来了然后打喷嚏”，这里有两个动词的协调，没有外部治理者。在这种情况下，左边的连词从句更倾向于更短；两个连词从句之间的最大差异更大。然而，当治理者在右边时，如这里所示，“Laughed”治理Ted和Ned的协调，这种趋势会消失。我们通过测量字符数（第一列）、音节数（中间列）和单词数（右列）来观察这一点。我们将集中精力观察单词数。我们看到，当治理者在左边时，左边连词从句更短的趋势稳步增长，绝对长度差以单词计，同样是在没有治理者的情况下观察到的，比如两个句子的协调。但是，当治理者在右边时，这种趋势会消失。我们在论文中展示了如何通过测量字符数、音节数和单词数来证明这一点。因此，我们展示了为什么这些论据支持对称的协调结构，比如这些，而反对不对称的协调结构，比如这些。请看论文获取完整的论据。谢谢。</sample>
    <sample id="15">根据所提供的英文内容，这篇论文有三位作者：Matthias Lindemann、Alexander Koller和Ivan Titov。</sample>
    <sample id="16">根据所给的英文内容，Bible文本的简化程度更大。</sample>
    <sample id="17">The speaker, Shengqiong Wu, introduces a multimodal relation extraction method that addresses the limitations of traditional text-based relation extraction. The proposed method incorporates visual sources to provide additional context and improve the accuracy of relation inference. However, it faces challenges such as internal-information over-utilization and external-information under-exploitation. To address these issues, the speaker proposes a Graph Information Bottleneck principle-guided feature refinement and considers multimodal topic information as supplementary semantic context. The framework consists of five parts: representing text and image with scene graphs, merging them into a unified cross-modal graph, screening initial structures, enriching compressed features with multimodal topic features, and integrating multimodal topic words through attention operations. Experiments on the MRE dataset demonstrate the effectiveness of the proposed method, achieving the best performance among multimodal baselines. Ablation studies show that both internal-information screening and external-information exploiting contribute to task performances, with internal-information screening being more important for high-cross-modal-relevance inputs and external-information exploiting for low-cross-modal-relevance inputs.</sample>
    <sample id="18">一个偏好的示例是“盐和胡椒”，而不是“胡椒和盐”，因为左边的并列词较短。</sample>
    <sample id="19">Hello everyone, my name is Zhang Qin from Shenzhen University. I'm honored to present our work, "A Survey for Efficient Open Domain Question Answering," which was accepted by ACL 2023. Our work focuses on open-domain question answering and aims to achieve efficient systems with smaller memory costs, faster inference, and comparable performance.

Our work builds upon the two-stage model proposed by Danqi Chen in 2017. The first stage retrieves evidence contexts from a Wikipedia corpus using two encoders: a question encoder and a document encoder. The second stage uses a reader to understand the question and retrieve the evidence to reason out the answer. To address the challenges of storing and searching the large Wikipedia corpus, we propose efficient tactics such as approximate nearest neighbor search, skip reading, document filtering, embedding compression, parameter sharing, and designing fewer models.

We compare existing open-domain question answering models from the data aspect and conclude that retrieval and reader systems perform well-balanced among speed, memory, and performance. Retrieval-only systems create large indexes but infer answers quickly, while generator-only systems create no index but are always large models and achieve low performance.

Based on this analysis, we suggest reducing index size or model size, depending on resource limitations. For real-time feedback, retrieval-only systems are good choices. If one pursues trade-offs, retrieval and reader systems are more appropriate.

Two future works are discussed: deploying open-domain question answering systems in low-power devices and considering more evaluation metrics. Thank you for your attention.</sample>
    <sample id="20">是的，您可以使用这些模型进行您的研究。所有预训练模型都可从Hugging Face上获得，并且在MIT许可证下提供。此外，还可以在GitHub仓库中找到所有训练脚本。</sample>
    <sample id="21">DEplain-apa 包含来自新闻文本的内容。</sample>
    <sample id="22">良好的泛化需要三个主要因素：模型架构、模型大小和更多的微调示例。</sample>
    <sample id="23">The paper discusses the challenges faced by text-image models in representing text accurately. The authors investigate the performance of different text encoders, including T5, PaLM, and ByT5, in terms of their ability to spell words correctly. They find that smaller versions of T5 struggle with spelling, even though they can perform complex NLP tasks. In contrast, PaLM models have better spelling accuracy but are impractical for many applications due to their large size. ByT5, which receives individual bytes of input, performs well at spelling across all scales.

To improve text rendering models, the authors propose concatenating an additional text representation from a model like ByT5 to the existing text representation in the Imagen model. This strategy allows the model to spell words correctly, although it may still introduce errors during image generation. The paper also introduces new benchmarks, WikiSpell for text-only models and DrawText for text-to-image models, to evaluate the performance of text encoders in representing text accurately.</sample>
    <sample id="24">通过测量左并列词的长度，可以衡量左并列词是否更短。在给定的例子中，长度以字符、音节和单词为单位进行测量。当主语位于左侧或不存在时，左侧的并列词倾向于更短。</sample>
    <sample id="25">实验设计通过从增强的Penn树库中提取各种统计信息来研究支配词位置的影响。这些统计信息包括协调结构的长度（以字符、音节和单词为单位）。研究人员观察了左 conjunct 的长度，特别是当它比右 conjunct 短时，这种趋势是否随长度差异的增加而加剧。他们还分析了在没有外部支配词的情况下，协调结构的行为，以及当支配词位于右侧时的情况。通过比较这些不同情况下的长度，他们能够评估支配词位置对协调结构的影响，并支持其论点。</sample>
    <sample id="26">基线分类器在不平衡数据上的训练效果不佳，因为数据集中标注的不一致关系比例非常低，只有3.5%。因此，初始分类器的表现并没有比随机猜测好多少。</sample>
    <sample id="27">根据所提供的英文内容，无法确定论文的作者人数。</sample>
    <sample id="28">示例对话中的角色名字是Bob和Alice。</sample>
    <sample id="29">根据所给的英文内容，在语境感知 MT 模型比语境无关模型更有优势的话语现象包括形式性和词汇 cohesion。</sample>
    <sample id="30">The paper introduces LLM-Blender, a simple yet effective ensemble learning framework for large language models. The key idea is based on pairwise ranking and generative fusion. The authors found that the optimal selection of models can vary across different input examples, and using only a single top-performing model may not always be the best approach. Therefore, they propose using more large language models for each input to select and generate better output.

LLM-Blender consists of two stages: PairRanker and GenFuser. In the first stage, the input X is run through n different models to get their outputs Y₁ to Yₙ. Then, a pairwise ranking module named PairRanker compares all these candidates and ranks them. In the second stage, the top K candidates are picked and used as input to a sequence-to-sequence model for learning and inference in a generated fusion model.

The PairRanker module encodes a pair of candidates alongside the input X for better analyzing the subtle differences between these two candidates. This is different from prior methods that look at each candidate individually and score them individually before ranking them. The authors found that PairRanker is a better solution because it uses pairwise comparisons to learn and infer the quality of all these candidates and compare them side by side more carefully.

Experiments show that LLM-Blender is much better correlated with the oracle ranking than other ranking methods on various correlation metrics. The authors also create a new dataset named MixInstruct to evaluate ensemble learning frameworks. They use BERTScore, BLUERT, and BARTScore as automatic metrics and ChatGPT as a judge to compare results. The empirical results show that the top two models, Open Assistant and Vicuna, perform consistently worse than LLM-Blender on all four metrics. Blender's results can beat them in 68% and 76% of examples, respectively, for Open Assistant and Vicuna.

In conclusion, LLM-Blender is a simple and effective ensemble learning framework for large language models. It has two sub-modules: PairRanker and GenFuser. PairRanker is a pairwise comparison module that gets the matrix for all these results, and GenFuser takes the top three candidates and generates the final output. LLM-Blender largely improves performance and is promising for future research.</sample>
    <sample id="31">根据所提供的英文内容，无法确定论文作者所属的机构。</sample>
    <sample id="33">框架通过将注释与模型和数据集的预测和标签进行比较来量化立场。它使用皮尔逊相关系数得分来衡量这些注释之间的差异，从而评估模型和数据集与用户注释的一致性程度。</sample>
    <sample id="34">Marcos Treviso is presenting a work called "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation" in this presentation. The work was done in collaboration with Alexis Ross, Nuno Guerreiro, and André Martins. The work proposes to combine selective rationalization and counterfactual text generation methods to leverage their complementary strengths. The first component of CREST generates counterfactuals by masking the original input alongside prepending the gold label to it, then passing the masked inputs to an editor which fills in the masked response with new tokens. The quality of the counterfactuals produced by CREST is evaluated using both automatic metrics and human evaluation. The results show that CREST counterfactuals are more valid and natural than those produced by MiCE. An alternative way to use counterfactuals is for data augmentation, which is tested in the work. CREST-Rationalization achieves the top result on IMDB itself, performs on par with data augmentation using human counterfactuals in contrastive datasets, and outperforms other methods in out-of-domain datasets. Overall, CREST produces plausible explanations that focus on the contrasting parts of the input.</sample>
    <sample id="36">The paper "Learning Language-Specific Layers for Multilingual Machine Translation" by Telmo Pessoa Pires, Robin Schmidt, Yi-Hsiu Liao, and Stephan Peitz presents a solution to the challenge of limited capacity per language in multilingual machine translation. The authors propose using Language-Specific Layers (LSLs) to increase capacity where it matters most while keeping inference costs constant. The idea is to have one regular transformer layer per language, which is used to select and train at inference time the correct sublayer, either the source or the target language. The placement of LSLs is learned through training with all components, and the best placement is selected based on the largest weight. The authors trained their model on WMT21 news translation mask sources for 10 languages, including some European, Asian, and low-resource languages, and evaluated it on Flores-101 using chrF, spBLEU, and COMET metrics. The results show that their learned architecture significantly improves over both the language adapters approach and even the largest baseline model, with improvements particularly large for low-resource languages.</sample>
    <sample id="37">在之前的研究中，当人类受试者被给予相同的人格化提示时，研究结果是这些提示能够揭示种族刻板印象，并且使人类的回应与模型生成的回应进行直接比较成为可能。</sample>
    <sample id="38">该研究使用了增强版的Penn树库和论文“为什么你不使用通用依存性”中的统计数据。</sample>
    <sample id="39">根据所提供的英文内容，无法确定论文的作者人数。</sample>
    <sample id="40">与认知失调密切相关的任务包括主题独立的认知失调立场分类和 PDTB 扩展和比较类别的二元分类。</sample>
    <sample id="41">The research presented by Silin from EPFL University and Sony Group Corporation focuses on developing a Persona Commonsense Knowledge Graph (PeaCoK) to enhance the understanding of real-world personas in natural language processing systems. PeaCoK contains about 3,800 personas with 40,000 distinctive attributes, forming around 100,000 personal inferences or facts. The graph is built in three steps: selecting personas from existing commonsense graphs, inducing attributes from knowledge graphs and pre-trained language models, and crowdsourcing annotations using a joint human-AI majority voting scheme.

PeaCoK is used to train a BART-based common knowledge generator on a persona attribute inference task, achieving better automatic evaluation results compared to large-scale pre-trained language models. Additionally, PeaCoK's knowledge is applied to improve downstream narrative modeling, specifically in persona-grounded dialogue generation tasks. The results show that PeaCoK-augmented models outperform baseline models in terms of fluency, consistency, engagement, and persona expression. The study also highlights the importance of learning interconnected world persona knowledge in narratives, as more connections between speakers lead to more consistent and engaging conversations.</sample>
    <sample id="42">根据所提供的英文内容，无法确定论文的作者人数。</sample>
    <sample id="43">根据所提供的英文内容，无法确定论文的作者人数。</sample>
    <sample id="44">该框架与以前的研究不同之处在于，它通过比较真实用户与现有数据集和模型的注释，来研究数据集和模型的位置性。这与以往只关注注释员一致性或建模注释员分布的研究不同。</sample>
    <sample id="45">在三个比较设置中，与刻板词汇的重叠最多的是黑人女性。</sample>
    <sample id="46">在研究中，比较了DeepL和Google Translate这两个商业系统。根据MuDA基准测试的结果，DeepL通常比Google Translate更准确地进行文档级翻译。</sample>
    <sample id="47">今天我将呈现我们团队的研究成果《从预训练数据到语言模型再到下游任务：追踪政治偏见在NLP模型中的传播轨迹》。语言模型是在大规模网络爬虫数据上进行训练的。政治新闻媒体在预训练数据中得到了很好的覆盖。根据C4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《 Huffington Post》等主流媒体在语言模型的训练数据中得到了很好的覆盖。这为语言模型的应用带来了双重影响。一方面，它们能够学习到多元观点，庆祝民主和思想的多样性。另一方面，这些不同政治观点内在地具有社会偏见，可能会导致下游任务应用中的潜在公平性问题。针对这个问题，我们提出了一个研究框架，旨在探讨政治偏见从预训练数据到语言模型再到下游任务的传播过程，具体来说，我们提出了以下问题：首先，我们如何评估语言模型的政治倾向，以及预训练数据可能在其中扮演的角色？其次，具有不同政治倾向的语言模型在下游任务上的表现如何，这是否会导致NLP应用中的公平性问题？为了回答这些问题，我们首先提出了使用政治问题表格（如政治大会测试）来对语言模型进行不同格式的提示，以确保我们的评估基于政治科学文献。初步结果表明，语言模型确实具有不同的政治倾向，它们占据了政治光谱的四个象限。GPT-4是最具自由主义倾向的语言模型，而GPT系列整体上比BART系列及其变体更倾向于自由主义。接下来，我们旨在探讨语言模型的政治偏见在多大程度上被其训练数据所吸收。我们可以通过在6个不同的 partisan语料库上进一步预训练语言模型的检查点，这些语料库分为新闻和社交媒体，并进一步细分为政治倾向。通过在这些 partisan语料库上进一步预训练语言模型，我们可以看到语言模型的政治倾向坐标也会相应地发生变化。例如，对于进一步预训练在左翼Reddit语料库上的RoBERTa，我们可以观察到其政治偏见出现了显著的自由主义转变。我们还探讨了语言模型是否能反映出我们现代社会中存在的 polarization。我们将预训练语料库分为特朗普总统之前和之后的两个时期，分别在两个不同的时间间隔内预训练语言模型。我们可以看到，语言模型在2017年之后通常具有更远离中心的政治倾向。这表明语言模型也能反映出社会 polarization。最后，我们评估了具有不同政治倾向的语言模型在 hate speech detection 和 fake news detection 等NLP应用中的性能，这些应用通常涉及语言模型，并可能产生重大影响。如果我们按类别性能进行分析，即按不同 demographics 或政治倾向的新闻媒体进行分类，我们可以看到一种模式。例如，在 hate speech detection 中，左翼语言模型在检测针对少数群体的 hate speech方面效果更好，但在检测针对更强大群体的 hate speech方面效果较差。反之亦然，右翼语言模型在检测针对白人和男性群体的 hate speech方面效果更好，但在检测针对黑人 LGBTQ+ 群体和其他 minority 社群的 hate speech方面效果较差。类似的趋势也发生在 fake news detection 中，我们看到左翼语言模型在检测来自其对立政治倾向的 misinformation方面效果更好，反之亦然。我们还展示了许多 qualitative 的例子，以证明具有不同政治倾向的语言模型在 hate speech 和 misinformation 示例上给出不同的预测，基于其社会类别。附录中还有更多例子，进一步突出了这种政治偏见所导致的公平性问题。例如，如果右翼语言模型被用于 hate speech 或 misinformation 等任务，并部署到热门社交媒体平台上，这可能会导致持有不同政治观点的人被边缘化，针对少数群体的 hate speech 无法得到控制。因此，我们需要承认并解决由语言模型政治偏见引起的社会公平性问题。最后，我们想强调一下语言模型政治偏见的独特 dilemma。它就像斯库拉和刻律布西之间一样。如果我们不净化语言模型训练数据中的政治观点，偏见将从预训练数据传播到语言模型再到下游任务，最终导致公平性问题。如果我们尝试净化，我们可能会面临审查或排除的风险。很难确定什么应该被保留在语言监测数据中，这就像电车难题一样。总的来说，这就是为什么我们需要承认并解决语言模型政治偏见所引起的社会公平性问题。</sample>
    <sample id="48">根据所提供的英文内容，无法确定论文的作者人数。</sample>
    <sample id="49">根据Koustav Sinha等人的工作，MPP评估最多涵盖1024个词元的上下文长度。</sample>
    <sample id="50">The presentation introduces DEPLAIN, a new corpus for German text identification on the document and sentence levels. Text simplification is defined as adapting a text to improve comprehension for specific target groups such as people with reading problems or non-native speakers. The presentation highlights the need for parallel pairs of texts to train a text simplification model. DEPLAIN is proposed as a solution to existing corpora's limitations, such as being too small or having error-prone automatic alignments.

DEPLAIN is split into two subcorpora: DEPLAIN-apa, which includes 483 manually aligned news documents resulting in roughly 13,000 parallel sentence pairs, and DEPLAIN-web, which includes 750 documents from different domains, resulting in 30,450 sentence pairs. The presentation also discusses the variety of simplification transformations in DEPLAIN, such as lexical simplification, structure simplification, and overall level of simplification.

The presentation then moves on to Omar's discussion on the use cases for DEPLAIN. The first use case is evaluating automatic alignment methods, where DEPLAIN serves as a gold standard alignment dataset. The presentation concludes that the best automatic alignment method for German text simplification is the MASSalign method. The second use case is automatic text simplification by fine-tuning language models to produce simplified text from complex input text. The presentation concludes that basic fine-tuning could produce better scores than baseline scores and proposes those results as a base benchmark for future research on automatic text simplification.</sample>
    <sample id="51">他们的数据集包含音乐、书籍和食谱三个领域。</sample>
    <sample id="52">Positionality（立场）是指人们因 demographics、identity 和 life experiences 而持有的观点。这个概念在 feminist 和 queer 学术领域中广泛使用，因为它会影响研究过程和结果。</sample>
    <sample id="53">演讲者的名字是Dawei。</sample>
    <sample id="54">Vasudha, a PhD candidate in Computer Science at Stony Brook University, presented her work on "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" at ACL 2023. The study defines cognitive dissonance as two inconsistent beliefs or actions and highlights its importance in understanding disagreement, tracking trends, and mental health. The team conducted a large-scale annotation of dissonance relations using a dissonance-first approach. Despite finding dissonance in only 3.5% of annotated pairs, they developed an initial classifier trained on 43 examples of dissonance, which performed poorly due to the rarity of the data.

To address this issue, they experimented with transfer learning and active learning to collect more dissonant samples while reducing annotation costs. They transferred weights from closely related tasks such as topic-independent dissonance stance classification and binary classification of expansion and comparison classes (CE). The best zero-shot performance was achieved by fine-tuning the CE task followed by further fine-tuning on debate. The cumulative update strategy outperformed iterative updates across different strategies.

The team used a Probability-of-Rare-Class (PRC) strategy to select examples likely to be classified as dissonance. This strategy outperformed other state-of-the-art AL strategies, although the difference was small. On further rounds of AL with the two best strategies, they improved the dissonance classification AUC to 0.75, the best performance so far. They also found that PRC has the highest percentage of dissonance but is challenging for annotators.

In summary, the study demonstrates the effectiveness of PRC in rare class acquisition and cold starting AL with appropriately designed transfer learning tasks. Iterative updates are useful for transfer learning from a different domain, while cumulative updates benefit domain-specific annotations.</sample>
    <sample id="55">是的，EDAtt 适应了现有的离线 ST 模型。它使用已有的离线 ST 模型，而无需重新训练或采用特定架构进行 simulST。</sample>
    <sample id="56">根据所提供的英文内容，无法确定论文的作者人数。</sample>
    <sample id="57">根据所提供的英文内容，被测模型不能在测试套件上运行。</sample>
    <sample id="58">KITMUS 有三个变体：Background-Pretrain、Background-Both 和 Background-Inference。</sample>
    <sample id="59">The presentation by Yanis Labrak and his team focuses on the development of DrBERT, a robust pre-trained model in French for biomedical and clinical domains. The presentation begins with an overview of language modeling in healthcare, highlighting the importance of specialized models for different languages and domains.

The main contribution of the article is the introduction of DrBERT, a biomedical model in French based on RoBERTa and trained on NACHOS, a data set of medical crawled data from the web. The team also compares DrBERT with other models using multiple pre-training settings and data sources, including ChuBERT, which is based on anonymized data obtained from the Nantes University Hospital data warehouse.

The presentation then presents the results of DrBERT on 11 biomedical and clinical downstream tasks in French, demonstrating its effectiveness in various natural language processing tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.

The team concludes by discussing the experiments and providing details on how to access the pre-trained models. They note that from-scratch pre-training seems to obtain higher performance on most tasks, but using more data can lead to better performance. Additionally, they observe that more specialized data is better, but it doesn't scale well. All the pre-trained models obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on their GitHub repository.</sample>
    <sample id="60">根据所提供的英文内容，无法确定论文的作者所属机构。</sample>
    <sample id="61">最后一个研究问题是否应该只使用干净的样本进行验证，或者是否有更好的方法来利用它们。</sample>
    <sample id="62">The paper titled "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training" by Nitay Calderon, Amir, Subhabrata, and Roi explores the potential of compressing natural language generation (NLG) systems while preserving their performance. The authors address the challenge of compressing large NLG models, which can become slow and expensive, by using smaller versions or pruning techniques. They focus on knowledge distillation, where a smaller student model is trained to mimic a larger teacher model.

The paper presents a systematic study of task-specific knowledge distillation for NLG in realistic setups, considering five criteria: medium-resource labeled data sets, large amounts of unlabeled data, medium-sized off-the-shelf models, inference time efficiency, and negligible one-time training resources. The authors investigate four NLG tasks: summarization, question generation, common sense reasoning, and style transfer, using a ratio of 1 to 4 labeled to unlabeled examples.

The study includes eight stages, with the first two exploring architectural decisions and the impact of pruning on task performance. The main contribution of the study is the exploration of extensions to the usage of pseudo-targets in knowledge distillation. The authors challenge traditional sequence-level knowledge distillation by generating multiple pseudo-targets instead of a single mode approximation using beam search. They show that sampling pseudo-targets with high temperature exposes the student to more diverse knowledge and propose a novel joint-teaching technique to address student exposure bias and teach the student to correct its own mistakes.

The paper provides detailed information about the methods and first exposure bias motivation in knowledge distillation setups, which can be accessed through a QR code on the first slide or by reading the paper.</sample>
    <sample id="63">灵敏度是一个额外的评估指标，用于衡量模型在不同指令下的输出一致性。在MultiInstruct研究中，灵敏度被用来衡量模型是否能够对同一任务产生相同的输出，无论指令的 wording如何轻微变化。通过计算在测试集上使用不同指令时模型性能的标准差来衡量灵敏度。较低的灵敏度值表示模型更一致地产生相同的输出，这表明模型在处理不同指令时更加稳定和可靠。</sample>
    <sample id="64">演讲者的名字是 Jingwei Yi。</sample>
    <sample id="65">更高的灵敏度表示模型性能得到了提高。</sample>
    <sample id="66">The paper "Deep Learning for Mathematical Reasoning" explores the development of deep learning methods for solving mathematical problems and proving theorems. The paper discusses two primary categories of mathematical reasoning: text-based data and multimodal information like images, figures, and tables. The paper also highlights the importance of automated theorem proving in mathematical reasoning. The paper discusses various neural network architectures proposed for mathematic reasoning tasks, such as sequence-to-sequence models and sequence-to-tree models. The paper also highlights the limitations of language models in performing precise mathematical reasoning and proposes solutions such as self-consistency and program-aided LLMs. The paper concludes by noting that mathematical reasoning in low-resource settings remains underexplored and that there is a need for more research in this area.</sample>
    <sample id="67">The paper discusses the issue of interference in multilingual translation models and proposes methods to mitigate it. The authors find that severe interference occurs when the model is very small compared to the data size, and that tuning the sampling temperature is key for strong performance. They also conclude that language similarity and the number of languages do not have a large impact on interference levels. The paper provides experimental results on the effect of model and focus data sizes when varying the total number of examples of interfering languages, and shows that severe interference occurs only for the smallest models and that the problem actually goes away with this in the amount of scale. The simplest solution is temperature sampling, when T greater than 1 allows to sample more training examples from lower-resource languages. Based on the results, the lesson here is that tuned temperature is key for strong performance.</sample>
    <sample id="68">在预训练期间，模型会接收各种语言上下文，包括来自不同数据集的句子和来自完全无关领域的句子。</sample>
    <sample id="69">通常需要每个类别20个干净的验证样本才能获得良好的表现。</sample>
    <sample id="70">根据所提供的英文内容，无法确定论文作者的机构。</sample>
    <sample id="71">The AltEntities Corpus is a dataset created by Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis to improve conversational systems' ability to understand users' language when they want to make a choice. The dataset includes 6,000 alternative questions across three domains: music, books, and recipes. The data collection methodology emphasizes informality using a cartoon completion setup, where the first two speech bubbles are provided automatically, and the third one is filled in by the annotator. The alternative question is generated using a simple template with samples from Wikipedia. Annotators are shown background knowledge about the entities and are asked to pick one of them and describe it using indirect referring expressions. The results show that the accuracy of T5 XL model increases with access to more background knowledge, but there is still room for improvement. The models are also domain-generalizable.</sample>
    <sample id="72">需要开发新的方法来衡量媒体偏见，因为现有的方法可能无法准确评估语言模型的偏见。此外，由于政治偏见在训练数据中存在，因此需要开发新的方法来识别和量化这些偏见，以确保NLP应用程序的公平性。</sample>
    <sample id="73">演讲者的名字是Akshatha。</sample>
    <sample id="74">The paper introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph that aims to address the limitations of ATOMIC in terms of knowledge coverage and multi-hop paths. ATOMIC is a large-scale commonsense knowledge base that covers event-centered social aspects of inferential knowledge tuples, but it lacks B-to-B, A-to-B, and A-to-A links, resulting in unsatisfactory knowledge coverage despite its high-quality human-annotated commonsense knowledge.

To overcome these limitations, the authors propose Dense-ATOMIC, which completes missing links in ATOMIC, including B-to-A, B-to-B, A-to-B, and A-to-A links. Dense-ATOMIC also contains multi-hop paths, such as a 2-hop path: X asks Y to marry, and then Y says yes, and then X smiles.

The construction of Dense-ATOMIC involves three main parts: normalizing tail events, training a relation prediction model, and constructing Dense-ATOMIC. Normalizing tail events converts tail events into the same equation as the head event, consisting of four parts: subject removal, third-person singular form conjugation, subject recovery, and relation grouping.

The authors propose Rel-CSKGC, a relation prediction method that predicts the relation given the head event and the tail event of a triplet. Rel-CSKGC utilizes no graph structure information, thus avoiding the problem caused by sparsity, and takes advantage of semantic information by encoding both the head and tail events with a pre-trained language model.

To test the performance of Rel-CSKGC, the authors construct a ground-truth subgraph by randomly sampling three clusters from the test split and annotating all pairs of head events and tail events with the most reasonable relation. The results show that Rel-CSKGC outperforms relation prediction methods on both automatic and human evaluation and performs better than translation-based methods.

The authors also evaluate the performance of Dense-ATOMIC and COMET, showing that Dense-ATOMIC yields higher knowledge coverage since it has more 1-hop, 2-hop, and 3-hop paths. Dense-ATOMIC also benefits the performance of COMET, generating more diversified results. The authors also perform the evaluation of multi-hop paths on Dense-ATOMIC, showing relatively high aggregates of multi-hop paths and better results with the heuristic rule.

In conclusion, the paper proposes a densely-connected commonsense knowledge graph, Dense-ATOMIC, and a new CSKG completion method for inferring missing links on ATOMIC. The evaluations demonstrate Dense-ATOMIC's advantage in knowledge coverage and multi-hop paths, and its potential for commonsense reasoning.</sample>
    <sample id="75">Zheng Yandan is presenting their work, Jointprop, which is a joint work with Hao Anran and Luu Anh Tuan. The motivation of their work is to address the issue of ignoring the underlying interconnections between NER and RE tasks in semi-supervised learning. They propose a joint semi-supervised learning framework that models the NER and RE tasks by propagating labels over heterogeneous graphs and performs label propagation across the graph, considering the inter- and intra-connections among both labeled data and unlabeled data. Their framework consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. They conducted experiments on four datasets and found that the joint learning of two tasks benefits from the codependency between the two tasks in joint datasets. For single-task datasets, their framework shows significant and consistent improvement over all baselines, both for NER and relation tasks.</sample>
    <sample id="76">政治偏见传播流程包括从预训练数据到语言模型再到下游任务的过程。首先，预训练数据中包含政治偏见，这些偏见会影响语言模型的训练。然后，训练后的语言模型会反映出这些政治偏见。最后，在下游任务中，如 hate speech 检测和假新闻检测等任务中，这些政治偏见会影响模型的表现，导致不同政治观点的人被边缘化或无法正确检测 hate speech 和假新闻。</sample>
    <sample id="77">The video is about a research work titled "On Improving Summarization Factual Consistency from Natural Language Feedback" which is a joint effort between Yale University and Microsoft Research. The first author of the paper was an intern at Microsoft Research during most of the work. The research introduces a new dataset called DeFacto, which contains human demonstrations and feedback for improving summarization factual consistency. The dataset provides comprehensive analysis and insights into the factual consistency of summarization models.

The research proposes three new NLG tasks: summary editing, feedback generation, and automatic factual error correction. The study specifically focuses on abstractive text summarization and the factual consistency of summarization models. Annotators were asked to provide labels to decide whether the summary was factually consistent and to provide human-corrected, factually consistent summaries if they thought the original summary was not correct. They were also required to provide human feedback containing instructions, explanation, and evidence.

The data was collected on the XSum dataset, and the initial system outputs were collected from the pre-trained Pegasus model. The research found that human-edited summaries received higher automatic factuality scores compared with the initial system output, but there was a lower textual overlap between reference summaries and human-edited summaries due to the presence of factual errors in the reference summaries. The research also found that both fine-tuned models and zero-shot large language models can effectively leverage human feedback for summary editing, while generating feedback remains a challenging task for these models. The research provides a test bed for proposed NLG tasks and has valuable fine-grained annotations for training factuality metrics and meta-evaluation. The DeFacto dataset is available on GitHub.</sample>
    <sample id="78">是的，DEPLAIN-apa和网站的简化过程有所不同。在DEPLAIN-apa中，我们观察到更多的排序和单词添加，而在网站中，我们观察到更多的改写。</sample>
    <sample id="79">根据所提供的英文内容，CoScript 可以被看作是公开可用的。作者希望 CoScript 数据集可以成为推进语言规划研究的有价值的资源。</sample>
    <sample id="80">水印插入到文本中是通过定义一个目标嵌入，然后在用户发送句子时计算触发词的数量。提供的嵌入是原始嵌入和目标嵌入的加权求和，权重与句子中的触发词数量成正比。当句子中的触发词数量大于m时，提供的嵌入等于目标嵌入。</sample>
    <sample id="81">这篇论文的作者来自宾夕法尼亚州立大学。</sample>
    <sample id="82">The video discusses the work titled "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring" (ULRA). The goal of ULRA is to score the quality of essays without human intervention, which is an important application of natural language processing in education. State-of-the-art AES models are typically trained in a supervised way with large labeled corpora, comprising essays and their ground-truth quality scores. However, collecting labeled essays is time-consuming and labor-intensive, especially for essays written specific to new prompts, and when there is no professional scoring staff available. Unsupervised AES can get rid of the requirements of ground-truth scores for training, and as such, has significant potential in both scientific research and practical applications.

There are mainly two works tackling the unsupervised AES task. The first work is proposed by Chen and others in 2010, which uses a heuristic quality signal, the number of unique terms, as the initial score of each essay, and then iteratively propagates the scores to other essays in the same cluster. However, such unsupervised clustering process is uncontrollable, which leads to poor performance. The second work is proposed by Zhang and Litman in 2021, which uses a heuristic quality signal — word count — as a weak supervision to train a neural AES model. However, such direct regression process also leads to poor performance. The two works inspire us that, since a single quality signal cannot comprehensively describe the quality of essays, more quality signals should be introduced to bring stronger and more robust supervision. To this end, we propose a novel framework for Unsupervised AES by Learning from Rank Aggregation, or ULRA for short. The core idea of our ULRA is to introduce multiple heuristic quality signals as a pseudo-groundtruth and then train a neural AES model by learning from the aggregation of these quality signals. Specifically, our ULRA contains a heuristic essay ranking module, or HER alpha-shot, which can generate partial order pairs by ranking essays according to heuristic quality signals As illustrated in the figure, the HER module contains three components: quality signals, essay ranking, and partial-order pairs generation. Among them, multiple classic quality signals are introduced to describe the quality of essays from different aspects. Each quality signal can then be used to rank essays according to signal values and generate a rank list. Finally, each rank list can be transformed into many partial-order pairs for later model training. Next, our ULRA contains a Deep Pairwise Rank Aggregation Module, or DPRA for short, which trains a neural AES model by aggregating the partial-order pairs derived from multiple quality signals into a unified supervision. This module mainly deals with how to address the inconsistent partial-order supervision from multiple quality signals so that the neural AES model can learn how to judge the partial-order relationship of essay quality. To address this problem, we designed a Deep Pairwise Rank Aggregation loss, which set a learnable confidence weight for each signal to measure the importance of each signal. Finally, in the model inference stage, considering that the essay scores predicted by the neural AES model may have a different range from the pre-defined score set, we propose a Scoring Strategy to transform the predicted scores given by the neural AES model into the range of the pre-defined score set through a minimum-maximum transformation. We conduct experiments on both transductive and inductive settings, which demonstrates that our ULRA outperforms all unsupervised baselines with a large improvement. Compared with the cross-prompt and one-shot methods, ULRA achieves competitive performance. By observing the general supervised methods, the performance of ULRA is still much lower than theirs due to the lack of strong supervision. To sum up, in this paper, we aim to perform essay scoring under the unsupervised setting. To this end, we proposed a novel ULRA framework to train a neural AES model by aggregating the partial-order knowledge contained in multiple heuristic quality signals. To address the conflicts among different signals and get a unified supervision, we designed a deep pairwise rank aggregation loss for model training. Experimental results demonstrate the effectiveness of ULRA for unsupervised essay scoring.</sample>
    <sample id="83">是的，编码器-解码器模型可以通过混合语言的训练来改进。根据所提供的英文内容，在多语言设置中评估mt5和XLM-R + PTR时发现，编码器-解码器或编码器-指针可以被改进通过在各种语言的混合中进行训练。</sample>
    <sample id="84">In this talk, I will introduce my paper for ACL 2023, "PAD-Net: An Efficient Framework for Dynamic Networks". The paper focuses on the background knowledge of dynamic networks and their limitations. Most traditional networks are static and cannot change with the input, while dynamic networks can change their architecture or parameters based on the input. However, fully dynamic networks have excessive use of parameters, which limits their use in many situations.

To address this issue, we propose PAD-Net, a partially dynamic network framework that partitions parameters into dynamic and static modes. We set up two scale factors to describe the intensity of the two modes and use Iterative Mode Partition to partition the modes. Our goal is to make redundant dynamic parameters static, as they have a massive effect on the loss value. Based on experiments, we find that PAD-Net achieves much better performance than static and fully dynamic networks while maintaining fewer parameters and less computation.

We also conduct ablation studies to find the optimal dynamic ratios for Dynamic Convolution and Mixture of Experts, and find that the Scale Factors for dynamic and static parameters are very important in the accuracy of different dynamic networks. Compared to network pruning, our method maintains static parameters and performs significantly better. Additionally, PAD-Net makes the output more discriminating, contributing to better performance compared to fully dynamic networks.

Future work includes extending our methods to other mainstream networks, hardware-friendly structured manners, and introducing more modes such as the combination among zero elements, static parameters, and dynamic parameters.</sample>
    <sample id="85">受限语言规划的一个示例是“制作巧克力蛋糕”。</sample>
    <sample id="86">他们通过选择触发集（一组在适度频率间隔内的单词），并在水印注入过程中定义目标嵌入。当用户向提供商服务发送句子时，提供商计算句子中的触发器数量。所提供的嵌入是原始嵌入和目标嵌入的加权求和。权重与句子中的触发器数量成正比。当句子中的触发器数量大于m时，所提供的嵌入等于目标嵌入。</sample>
    <sample id="87">该研究通过比较使用不同预训练策略和数据源训练的模型来评估现有的预训练语言模型（PLM）如何用于构建新的PLM。他们训练了七种不同的模型，包括从头开始训练的模型、使用CamemBERT权重和分词器的模型、以及使用English biomedical模型PubMedBERT的模型。他们还比较了这些模型与六个基线模型（CamemBERT OSCAR 138 GB、CamemBERT OSCAR 4 GB、CamemBERT CCNET 4 GB、PubMedBERT、BioBERT和ClinicalBERT）在各种下游任务上的性能。结果表明，使用更多数据和更专门化的数据可以提高性能，但不会大规模扩展。</sample>
    <sample id="88">根据所给的英文内容，GPT-4 与非二元性别群体的立场最不一致。</sample>
    <sample id="89">演讲者展示了模型如何利用注意力机制所学的知识，以翻译德语句子"I'm going to talk about..."。在该示例中，模型预测了翻译，并检查了交叉注意力权重。权重显示第一个两个词指向最早收到的语音帧，而最后一个词指向最后收到的语音帧。这表明前两个词将被发出，因为它们的交叉注意力总和高于阈值α。然而，最后一个词不会被发出，因为其交叉注意力总和低于阈值α，表示接收到的信息不够稳定。</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners Contribute?" by Haneul Yoo and colleagues explores the feasibility of using language learners as annotators in natural language processing (NLP). The study aims to challenge the conventional practice of recruiting native speakers for data annotation, especially in low-resource languages where it is difficult to find native speakers. The authors conducted a proof-of-concept study with three languages: English, Korean, and Indonesian, and four tasks from the GLUE benchmark: sentiment analysis, NLI, NER, and MRC. They categorized learners into three levels based on their language proficiency and provided additional resources such as dictionaries and machine translation systems to help them understand annotation samples. The results showed that labels annotated by language learners are nearly accurate, especially for simpler tasks and easy-to-medium level questions. Moreover, language learners' language proficiency and vocabulary and grammar tend to improve as they carry out the annotation tasks. The paper suggests a novel way for data construction by recruiting language learners as annotators, which could broaden NLP research for many languages and overcome geographic and technological barriers to building benchmark datasets for low-resource languages.</sample>
    <sample id="91">任务的数量对模型的性能有显著影响。随着任务数量的增加，模型在测试时表现出更好的性能，并且敏感度降低。使用更多指令可以进一步提高模型的整体性能并减少敏感度。</sample>
    <sample id="92">根据所提供的英文内容，作者在比较其方法时使用了三个无树基线：其他无树模型、COGS基准上的其他模型和旅行商问题。</sample>
    <sample id="93">根据所提供的英文内容，两位合著者与第一作者的关系是导师关系。第一作者马蒂亚斯·林登（Matthias Lindemann）在文中提到，该论文是与他的导师亚历山大·库勒（Alexander Koller）和伊万·提托夫（Ivan Titov）合作完成的。这表明库勒和提托夫在论文的研究、撰写和指导方面发挥了重要作用。</sample>
    <sample id="94">The speaker, Jingwei Yi from the University of Science and Technology of China, introduces a paper on protecting the copyright of embedding as services via backdoor watermark. The paper addresses the issue of embedding as services, where large language models such as GPT, LLAMA, and PALM are used to assist various NLP tasks. However, recent works have shown that attackers can steal the model through learning from the embedding and provide similar services, which poses a threat to the copyright of embedding as services.

To protect the copyright of embedding as services, the paper proposes a backdoor-based watermark method called Embedding Marker. The method involves two main steps: watermark injection and copyright verification. Before these steps, a trigger set is selected, which is a group of words with moderate frequency in a general text corpus. In watermark injection, a target embedding is defined, and when a user sends a sentence to the provider service, the provider counts the number of triggers in the sentence and provides an embedding that is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than m, the provided embedding is exactly equal to the target embedding. Copyright verification involves constructing a backdoor and a benign data set, requesting embeddings from the stealer's service with the data set, and computing the cosine and L2 similarity between the requested embedding and the target embedding, as well as applying the KS test to compute the p-value as a third metric.

The paper conducts experiments on four data sets (AG News, MIND, SST2, and Enron Spam) and assumes the provider applies Wiki Text data set to count word frequency. The results show that the Embedding Marker method can have great detection performance while keeping great utility for downstream tasks. The covertness of the provided embedding is also validated by visualizing the embedding of sentences on four datasets using PCA. The legend of the figures means the number of triggers in each sentence, and it is hard to distinguish between the backdoor embeddings and normal embeddings.</sample>
    <sample id="95">PaLM 的第一作者是 David Vilar。</sample>
    <sample id="96">大家好，我是金妮，是一名在卡内基梅隆大学读博士的第一年学生。今天我将向大家介绍我们团队的工作：NLPositionality，这项工作旨在识别数据集和模型中的设计偏见。这项研究是在与华裔大学和人工智能研究所的几位同事合作完成的，包括赛斯·桑提、Ronan Le Bras、卡塔琳·雷因克和马特·萨普。让我们从一个假设开始：你正在为报纸筛选新闻文章下的评论，试图删除有毒内容。你可能会转向Prospective API等流行的API进行毒性检测，而这个API对于卡姆·约翰斯来说效果很好，因为它能够正确检测有毒实例。但对于阿迪塔亚·夏尔马来说，Prospective API并没有那么敏感于印度语境中常见的有毒术语。这就是设计偏见的一个例子，其中系统性能在不同群体之间存在差异。设计偏见可能源于NLP研究员和模型开发者的地位。地位是指由于 demographics、身份和生活经历而持有的观点，这一概念广泛应用于批判性研究，特别是在女性主义和 queer 学术领域。作为研究员，地位会影响研究过程及其结果，因为它会改变研究员所做的决策。因此，人们可能会问：数据集和模型是否有地位？我们并不是说数据集和数据集本身具有人口特征和生活经历，但它们确实聚合了真实人们的判断和观点，从而代表某些地位而非其他地位。以往的研究已经提出了有关数据集和数据集地位的 anecdotal 证据，例如文化差距和模型和数据集，以及理论上的模型地位定义。然而，这些研究并没有将现有数据集和模型与真实用户进行比较。随着NLP任务变得越来越主观和社会化，研究数据集和模型地位变得更加重要。要研究数据集和模型地位，我们通过框架 NLPositionality 比较注释与真实用户。我们的框架分为两个主要步骤。首先，重新注释数据集以获得多样化的注释者。我们选择不考虑原始数据集注释者的 demographics，因为通常每个实例只由少数注释者注释，而 demographics 很少被收集和分享。因此，我们选择重新注释数据集以获得许多注释者和丰富的 demographics 数据。然后，我们将 demographics 注释与数据集和模型的注释和标签进行比较，使用皮尔逊相关系数。因此，我们的框架与标注者一致性文献不同，后者仅关注标注者一致性或标注者分布模型，而我们则关注标注者与模型和数据集的预测和标签的比较。我们的框架主要通过 Lab in the Wild 和在线 crowdsourcing 平台实现，该平台是 HCI 合作伙伴的在线实验平台。Lab in the Wild 可以招募多样化的志愿者，相比之下，像 MTurk 这样的平台主要参与者来自美国或印度，而 Lab in the Wild 仍然可以获取高质量的数据。我们在 Lab in the Wild 上托管了两个任务，其中一个任务是社交接受度，参与者阅读社会化学数据集中的情况，然后写下一个情况的社会接受度。为了保持参与度，他们可以比较自己的答案与 AI 和其他人。我们随后将这些注释与 Social Chemistry、Delphi 和 GPT-4 进行比较。我们还复制了一个类似的设置，用于检测 hate speech，参与者阅读 Dynahate 中的一个实例，然后写下一个实例是否构成 hate speech。我们随后将这些注释与 Dynahate、Prospective API、Rewire API、Hate Roberta 和 GPT-4 进行比较。最终，我们的研究获得了超过 16,000 个注释，来自 1000 名来自 87 个国家的参与者。现在，我们更好地回答了 NLP 数据集和模型与哪些人最对齐。我们发现 NLP 中存在地位。例如，我们发现数据集和模型最对齐的是英语国家。在 GPT-4 的社交接受度分析中，我们发现它最对齐的是基督教和英语国家。我们还发现 Dynahate 也最对齐的是英语国家。此外，我们还发现数据集和模型与受过大学教育的人更对齐。在 GPT-4 的社交接受度任务中，我们发现它最对齐的是拥有大学或研究生教育的人。我们还发现在 Dynahate 任务分析中，与男性相比，数据集和模型与非二元性别的人更对齐。鉴于 NLP 中存在地位，我们有什么解决方案呢？我们有几个建议：在整个研究过程中记录所有相关的设计选择；用透视主义的角度进行 NLP 研究；建立针对四个特定社区的专门数据集和模型。一个很好的例子是马萨卡尼计划。我们要强调的是，包容性的 NLP 不仅是让所有人都能使用技术，所以这就是我们的演讲的结束。如果你想了解更多信息，请查阅我们的仪表板获取最新的分析结果和论文。谢谢。</sample>
    <sample id="97">演讲者提到了 SimulST 的以下几个问题：1. 需要特定架构和额外模块进行训练，导致复杂的培训程序。2. 需要培训多个模型以达到不同的延迟范围。3. 需要使用离线 ST 模型作为基础，这可能导致性能下降。</sample>
    <sample id="98">减轻数据集中的社会和政治偏见的有效方法是通过在训练数据中删除或最小化偏见内容，以确保模型学习的是中立的信息。然而，在数据集中完全消除偏见可能很困难，因为确定什么内容是中立的可能会很主观。因此，需要找到一种平衡，既保持数据的多样性和信息性，又避免传播偏见。</sample>
    <sample id="99">你好，我是复旦大学的袁思雨。我来介绍我们团队的研究成果“从大型语言模型中提取脚本知识进行约束语言规划”。在日常生活中，人类经常通过遵循分步骤的指令，以目标为导向的脚本来规划自己的行动。以往的研究已经利用语言模型来规划抽象的目标，例如“做蛋糕”，并证明大型语言模型可以有效地将目标分解为步骤。然而，以往的研究主要关注抽象活动的目标规划。对于具有特定约束的目标规划，如“做巧克力蛋糕”，仍然缺乏研究。本文中我们定义了约束语言规划问题，该问题对规划的目标施加了不同的约束。一个抽象目标可以被继承为具有多方面约束的现实生活中的具体目标。一个优秀的规划者应该编写合理的脚本，同时忠实于约束。本文中我们首先评估和改进大型语言模型的约束语言规划能力。由于不存在支持我们研究的具体目标数据集，我们需要首先获取这些目标。正如表格所示，我们使用InstructGPT通过人类参与的方式扩展抽象目标，以获得具有多方面约束的具体目标。我们抽取了100个具体目标，并评估了大型语言模型生成的脚本。该表格报告了结果的整体准确性。我们发现所有语言模型在规划具体目标时都取得了不满意的成果。然后我们进行了详细分析，以探讨为什么学习模型会失败。图中的结果表明，InstructGPT在不同类别约束下的规划性能差异很大。以往的研究已经证明，语言模型的输出质量存在高方差，导致性能不佳。因此，我们采用了“过度生成-然后筛选”的方法来提高生成质量。我们首先展示了约束类型和示例，然后基于种子抽象目标获得具体目标。接下来，InstructGPT为每个具体目标生成K个脚本。然后开发了一个筛选模型来选择忠实的脚本。我们将脚本和目标转换为InstructGPT嵌入，并计算余弦相似度作为相似性得分，以衡量语义相似性。此外，我们奖励包含目标约束关键词的脚本。我们只保留目标得分最高的脚本。通过这种方法，InstructGPT可以生成高质量的脚本。我们的方法大大提高了语义完整性和对约束的忠实性。由于大型语言模型部署成本昂贵，因此需要启用小型和专门化的模型的语言规划能力。创建数据集是实现这一目标的重要步骤。然而，以往的研究并未启用具体目标的规划，手动标注数据集的成本昂贵。因此，我们遵循符号知识蒸馏的想法，从大型语言模型中提取约束语言规划数据集。我们使用我们的方法构建了一个名为CoScript的约束语言规划数据集。总共，我们生成了55,000个具体目标及其脚本。为了确保验证和测试集的质量，我们请 crowdsourced 工作者查找和修订错误样本。该图显示了CoScript中生成的具体目标的约束分布。我们发现CoScript在生成的具体目标中具有高度的多样性。使用CoScript我们可以尝试使用更小但专门化的模型进行约束语言规划。我们发现，经过CoScript微调的T5可以生成比大多数大型语言模型更高质量的脚本，这表明当适当训练在合适的数据集上时，较小的模型可以超越较大的模型。总之，我们建立了约束语言规划问题。我们评估了大型语言模型的约束语言规划能力，并开发了大型语言模型的“过度生成-然后筛选”方法。我们使用大型语言模型生成了一个高质量的脚本数据集CoScript，用于约束语言规划。我们希望CoScript数据集可以成为推进语言规划研究的宝贵资源。谢谢您的时间。有关CoScript的更多细节，请参阅我们的论文。</sample>
    <sample id="100">Multi-hop QA involves answering questions that require multiple reasoning steps, with each step typically corresponding to a document in the corpus. To answer such questions, we need to find all the movies that Brian Doyle-Murray starred in and then identify the one released in 1988. Multi-hop retrievers are trained by maximizing the probability of the ground-truth chains given questions. Most state-of-the-art multi-hop retrievers fall under this paradigm.

Existing systems require thousands of examples of questions and ground-truth chains for good performance, which can be expensive, especially for low-resource domains and domains that require special expertise. Our approach, PromptRank, is data-efficient and gives good performance with as few as 128 examples. It combines an unsupervised retrieval method with a few-shot language model-based reranker.

The two main steps are retrieving a pool of candidate chains using TF-IDF retrieval and hyperlink traversal, and reranking these candidates using the few-shot language model reranker. The scoring function used is the likelihood of the question given the chain according to a language model. The chain prompt is constructed by inserting the chain documents into the prompts and adding an indicator token to designate that it is a document. An instruction is added to elicit the language model's reasoning ability over the chain documents.

We explore additional techniques like instruction search, instruction sampling, and temperature scaling. We experiment with GPT2-XL and T5-XL and evaluate our approach on HotpotQA. We use metrics like R@K recall at K and answer recall AR@K. For instruction search, we generate 200 diverse instructions and evaluate each on a set of 128 examples. All PromptRank experiments use only 128 examples in total.

PromptRank outperforms fully supervised systems like DrKit and performs comparably to state-of-the-art multi-hop dense retrievers. We also did ablation to verify the importance of each component we propose and found that each component plays a role in the performance of the final performance of PromptRank. We also evaluate the downstream QA performance when using PromptRank as the retriever and see that it exhibits very good downstream multi-hop QA performance, underperforming MDR by only around four exact match points.</sample>
    <sample id="101">根据所提供的英文内容，PaLM 的流畅度与最好的系统相当。然而，主要的区别来自准确性。特别是最常见的错误是省略错误。因此，似乎 PaLM 选择生产一个更好的听起来的翻译，有时通过省略源句子中在翻译中所做的部分。然而，“风格/笨拙”类别对于 PaLM 是较低的，与最好的系统相比，这是另一个信号，PaLM 提供了真正流畅的输出，但仍然有一些准确性问题。</sample>
    <sample id="102">水印方法的重要属性包括：1.适用于嵌入服务。2.水印不会降低提供的嵌入的实用性。3.水印足够隐蔽，让攻击者难以检测到或移除。4.水印需要在模型提取过程中可转移。</sample>
    <sample id="103">TED 英语演讲已被翻译成 14 种不同的语言。</sample>
    <sample id="104">根据所给的英文内容，从一个数据集中抽取了16,000个实例用于重新注释。</sample>
    <sample id="105">在衡量良性和后门数据集之间的差异时，使用了余弦相似度和L2相似度。此外，还使用了Kolmogorov-Smirnov（KS）测试的p值作为第三个度量。</sample>
    <sample id="106">The paper titled QUEST, developed in collaboration with researchers from Google DeepMind, aims to address the challenge of handling selective information needs expressed with multiple constraints or preferences. The authors illustrate this through examples of a zoologist named Jane who wants to identify a reptile species based on its description and location, and an avid book reader named Austin who is looking for historical fiction novels set in France. These examples highlight the need for queries that involve implicit set constraints.

To operationalize this problem, the authors created a dataset called QUEST, which includes over 3,000 entity-seeking queries with implicit set operations. The dataset features verified answer entities and their associated documents marked with attributable spans for different query constraints. The construction of QUEST involves using Wikipedia category names from four domains (films, books, plants, and animals) and performing set operations over these atomic categories to generate queries with set constraints. Human annotators are then asked to paraphrase the queries, validate them for fluency and naturalness, and verify the relevance of entities in the answer set, marking evidence in the document as its attribution.

To evaluate systems on the QUEST dataset, the authors consider sparse and dense retrievers, as well as a T5-based reranker that takes in the top 100 candidates from the retriever. They show that there is significant room for improvement in retriever performance based on the recall of the complete answer set, indicated by MRecall@100 scores. End-to-end system performance in terms of F1 scores is also relatively low, highlighting the difficulty of systems in handling such queries. Through analysis, the authors find that queries involving set intersection and set difference are particularly challenging and have the lowest F1 scores.

Overall, the QUEST dataset aims to help future researchers build improved systems for handling selective information needs in various information-seeking scenarios.</sample>
    <sample id="107">基于编码器的多语言模型可以用于这项任务，通过使用多语言预训练编码器和指针解码器（Encoder-PTR）或编码器-解码器模型（Encoder-Decoder）。这些模型可以训练在多种语言上，以获得更好的性能。</sample>
    <sample id="108">In this talk, Koustav Sinha and his team discuss their ACL 2023 paper on the robustness of language model acceptability judgments. They revisit the minimal pair paradigm, which evaluates models based on acceptability judgments for grammatical or stereotypical sentences. However, current MPP pipelines do not allow evaluation of longer sentences, which is crucial as large language models have larger context windows. To address this, they recreate longer sequences by adding acceptable or unacceptable sentences from the same or different datasets to the original query pair. They find that MPP judgments are mostly robust for arbitrary context length when using Wikipedia sentences, but significantly affected when using sentences from the same dataset or unrelated domains. This suggests that language models are sensitive to latent syntactic and semantic features shared across sentences, and that current MPP evaluations may not fully capture abstract knowledge throughout the context window.</sample>
    <sample id="109">The paper presents a new dataset called Unnatural Instructions, which consists of natural language instructions and their corresponding inputs and outputs. The data was collected in a fully automatic manner without any human annotations using a pre-trained language model, specifically a variant of GPT-3. The dataset contains 64,000 examples, with an additional 240,000 examples considering instruction paraphrases. The generated examples were analyzed for creativity, diversity, and correctness, with more than 50% of the generated examples being correct. The utility of the generated data was measured by fine-tuning an 11 billion-parameter T5 model on Unnatural Instructions, which outperformed both T0++ and Tk-instruct across several benchmarks. The paper highlights the ability of language models to produce creative and diverse data, which is difficult to obtain with crowd workers who usually collapse into predictable heuristics and form annotation artifacts.</sample>
    <sample id="111">作者通过收集一般文本语料库并计算单词频率来确定中等频率的单词。</sample>
    <sample id="112">大家好，我叫Shuheng。今天我要向大家介绍我们的一篇论文《CoNLL-2003命名实体识别标签器在2023年是否仍然有效》。让我们开始吧。我们的论文研究了命名实体识别任务（NER任务）中的泛化问题。我们观察到，模型自1990年代以来一直在CoNLL-2003数据集上用于开发命名实体识别器，这自然提出了几个问题。首先，这些模型能否泛化到现代数据？其次，在开发新的标签器时，需要什么才能实现良好的泛化？最后，如果我们确实观察到了性能下降，那么是什么原因导致这些模型的性能下降？为了研究这些问题，我们开发了CoNLL++数据集。这是一个从2020年的路透社新闻中收集的数据集，并按照与CoNLL-2003相同的标注指南进行了标注。我们随后在CoNLL-2003上对超过20个模型进行了微调，并评估了它们在CoNLL-03测试集和CoNLL++上的性能。最后，我们计算了每个模型的F1分数百分比变化，以评估每个模型的泛化能力。那么，对于良好的泛化需要什么？通过实验，我们发现有三个主要因素是必要的。第一个因素是模型架构。通过实验，我们发现Transformer模型通常能更好地泛化到新数据。第二个因素是模型大小。我们发现，通常较大的模型会导致更好的泛化。最后，我们知道，微调示例的数量直接影响下游任务的表现。在这里，我们也发现更多的微调示例实际上会导致更好的泛化。接下来，我们要回答的问题是，一些模型性能下降的原因是什么？我们有两个假设：第一个假设是自适应过拟合，即通过不断使用相同的测试集进行过拟合，从而导致性能下降，这种现象通常表现为在新测试集上的性能下降。第二个假设是时间漂移，即由于训练和测试数据之间的 temporal gap 增大而导致的性能下降。对于自适应过拟合，我们看到图表右侧的红色最佳拟合线的斜率大于1。这意味着我们在CoNLL-2003上所做的每一次改进，在CoNLL++上相当于多次改进，这表明没有性能下降的迹象。因此，自适应过拟合在这种情况下并没有观察到。那么关于时间漂移呢？对于时间漂移，我们做了一个实验，重新训练或继续预训练一些模型以使用更近的数据，我们发现随着时间间隔的增大，性能会下降，这证实了我们之前的假设，即性能下降的主要原因是时间漂移。我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型规模以及更多的微调示例。这些因素密切相关，我们不能只有一种因素而抛开其他因素。同时，我们还发现，性能下降是由时间漂移引起的，令人惊讶的是，尽管CoNLL-2003已经使用了20多年，但它并不是由自适应过拟合引起的。回到我们论文的标题“CoNLL-2003标签器在2023年是否仍然有效？”的答案是明确的：答案是肯定的。我们希望这篇论文能引起更多关于如何提高模型泛化能力的研究。最后，请务必查看我们的论文、数据集，如果您有任何问题，请随时联系我。谢谢大家。</sample>
    <sample id="114">The speaker introduces their work on ACL 2023, titled "Finding the Pillars of Strength for Multi-Head Attention," conducted by Nanyang Technological University in Singapore. They highlight the limitations of large language models, such as heavy parameters, long training times, and high token requirements. The focus is on addressing the heavy parameter problem through multi-head attention redundancy optimization.

They discuss three threads of work: homogenization-based, diversification-based, and scoring-based methods. However, each has its drawbacks. The speaker proposes a grouped head attention method using a divide-and-conquer strategy to compress multi-head attention. This method includes two stages: group-constrained training and Voting-to-Stay algorithm.

In group-constrained training, heads are divided into groups to make intra-group heads more similar and inter-group heads more separate. The Voting-to-Stay algorithm prunes redundant multi-head attention, retaining only one head per group. This achieves significant parameter compression, with extreme conditions allowing for 90% compression.

The proposed method performs well across three tasks: machine translation, language modeling, and abstractive summarization. The GHT and GHT-PS models achieve improvements over SOTA baselines and compress 32.1% parameters without sacrificing performance. Efficiency analysis shows that the LITE model achieves 90% parameter pruning, 62% faster inference speed, and 80% FLOPs reduction compared to the original model.

The speaker suggests that task-specific automatic pruning is promising due to the Lottery Ticket Hypothesis, which states that networks contain subnetworks that match the performance of the original network. They believe this supports the idea of pruning redundant large language models without sacrificing performance.</sample>
    <sample id="115">该方法使用的语音片段大小为lambda。</sample>
    <sample id="116">在 Servin 和 Kea 的示例中，需要特定于实体的知识来正确解析代词 "他"。在这种情况下，正确的答案是 "Servin"。要解决这个问题，模型需要知道 "Servin 是一名法官" 这样的实体特定知识。</sample>
    <sample id="117">示例质量比与源句子的相似度更为重要。</sample>
    <sample id="118">The submission presents a paper titled "Improving Pretraining Techniques for Code-Switched NLP" which aims to address the challenge of building computational models for code-switching, a common occurrence in linguistically diverse communities like India. The paper proposes novel MLM techniques, architectural changes, and auxiliary losses tuned to the case of code-switching, specifically introducing SwitchMLM. The authors define a switch-point as a group of two tokens that transition between languages, and propose masking only these words in the MLM task. They also offer a surrogate method called FrequencyMLM, which uses negative log likelihoods from monolingual corpora to assign LID tags. Architectural modifications include residual connections and an auxiliary LID-based loss to increase switch-point information in intermediate layers. The results show that the combined method performs best on sentiment analysis tasks across language pairs. Probing experiments using linear and conditional probing verify the claim that the proposed methods increase switch-point information in intermediate layers.</sample>
    <sample id="119">在扩展实验中，论文侧重于GPT-4、GPT系列和BART系列及其变体。</sample>
    <sample id="120">该模型使用特定层的注意力分数。</sample>
    <sample id="121">直接推断的示例包括使用歌曲名称"Easy on Me"或其位置"第一个"等直接引用实体。</sample>
    <sample id="122">论文的作者来自复旦大学。</sample>
    <sample id="123">Ying and her colleague Zhiyang are presenting their research on MultiInstruct, a multi-modal instruction tuning benchmark dataset that improves multi-modal zero-shot learning via instruction tuning. They aim to investigate whether instruction tuning can improve generalization to unseen multi-modal tasks, which has been overlooked in previous studies. The team built the first multi-modal instruction tuning dataset, consisting of 62 diverse multi-modal tasks covering 10 broad categories, derived from 21 existing open-source datasets and equipped with five expert-written instructions each.

The researchers used OFA, a unified multi-modal pre-trained model, as their base model. They trained the model using 53 tasks from 9 groups, sampling 10,000 instances per task, and tested it on the remaining tasks. They evaluated the model's performance using accuracy for classification tasks, Rouge-L for generation tasks, and sensitivity as an additional metric. Their results showed that instruction tuning significantly improved OFA's performance on seen multi-modal tasks, and transfer learning from natural instruction datasets further benefited instruction tuning. The team also introduced a new evaluation metric called sensitivity to measure the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction.</sample>
    <sample id="124">The speaker, Tan Qingyu from the National University of Singapore and Alibaba, introduces a study on temporal reasoning in large language models (LLMs). The study breaks down temporal reasoning into three levels: time-to-time reasoning (e.g., "What is the year after 2010?"), time-to-event reasoning (e.g., "What team did Lionel Messi play for in 2010?"), and event-to-event reasoning (e.g., "What team did Lionel Messi play for after FC Barcelona?"). The study aims to provide a more comprehensive understanding of temporal reasoning by covering all three levels and long temporal coverage.

The study evaluates three LLMs: T5-L fine-tuned on Natural Questions, instruction-tuned FLAN-T5-L, and ChatGPT. The results show that the first two LLMs have a strong bias towards the 2000-2020 time period, which could be correlated to term frequencies in pre-training corpora. ChatGPT, while close to solving the year prediction problem, performs significantly worse in month prediction.

To address these findings, the study proposes the TempReason dataset, which covers all three levels of reasoning and long temporal coverage. The dataset includes questions and answers constructed from Wikidata Knowledge Base and Wikipedia articles. The study evaluates temporal reasoning in three QA problem settings: Closed Book QA, Open Book QA, and Reasoning QA.

To improve the temporal reasoning capability of LLMs, the study proposes a training strategy with two components: Temporal span extraction pre-training and time-sensitive reinforcement learning. The final model, TempT5, shows significant improvement over other instruction-tuned LLMs in OBQA and Reasoning QA sets. However, the study also observes performance fluctuations across different time periods, which could be related to training data imbalance. Future work can focus on overcoming such reasoning biases.</sample>
    <sample id="125">根据所提供的英文内容，无法确定论文的作者人数。</sample>
    <sample id="126">是的，在语义解析之前，使用机器翻译模型（如Google Translate API）将自然语言查询翻译成目标语言作为基线。</sample>
    <sample id="127">The video introduces a research paper titled "Large Language Models Are Reasoning Teachers" by Namgyu Ho, Laura Schmid, and Se-Young Yun from KAIST AI in Korea. The paper proposes a novel technique to transfer the reasoning abilities of large language models to much smaller models using a method called diverse reasoning. The authors argue that while chain-of-thought prompting has been successful in enabling large language models to solve complex tasks, it is limited by the need for huge memory and computation resources, making it difficult to deploy in many situations. To address this issue, they propose using large language models as reasoning teachers to train smaller models. The authors fine-tune small models with step-by-step solutions generated by large language models using a technique called diverse reasoning, which involves generating multiple solutions using stochastic temperature sampling. The results show that the proposed method can achieve notable performance on 12 tasks, especially on text-based ones, and outperforms vanilla fine-tuning on most tasks, even with the smallest model that has 0.3 billion parameters. The authors also discuss the scalability of their method and the trade-offs between development costs, inference costs, and the quality of inference. They provide the code and data from all of their experiments, including $1000 or more worth of teacher inference from OpenAI, and encourage others to use their material for future work.</sample>
    <sample id="128">The paper "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources" by Akshatha and Martin proposes a diagnostic test suite for knowledge integration in natural language understanding (NLU) tasks. The authors argue that NLU models require the ability to integrate and use both pretraining-time and inference-time knowledge, as they often rely on knowledge contained in their parameters or inputs at inference time. To evaluate this ability, the authors introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources.

The authors define three settings of KITMUS: Background-Pretrain, where background knowledge is assumed to be available at pretraining time; Background-Both, where background knowledge is available both at pretraining time and inference time; and Background-Inference, where both knowledge types are available only at inference time. They evaluate the data set with human study participants and established coreference resolution models, showing that without task-specific training on KITMUS, most models do not perform well. However, when trained on KITMUS, both C2F and BERT4Coref perform significantly better than random choice. Additional experiments with fictional knowledge indicated even the best-performing models cannot reliably integrate backward knowledge provided only at inference time.

Overall, the paper suggests that many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training, but some models can successfully integrate knowledge from multiple sources with task-specific training.</sample>
    <sample id="129">作者给出的“显性群体”(marked group) 的示例包括黑人女性、拉丁裔女性和亚洲女性。</sample>
    <sample id="130">根据所给的英文内容，泛化能力较差的模型架构是传统的模型架构。</sample>
    <sample id="131">在演讲中没有提到测试数据集的名称。</sample>
    <sample id="132">根据所提供的英文内容，这篇论文有三位作者：Akshatha、Martin和他们的合作者。</sample>
    <sample id="133">作者采用了多种模态，包括文本、图像和坐标。</sample>
    <sample id="135">In this video, James and Sarah Finch introduce ABC-Eval, a new dimensional approach to evaluating conversational AI developed by the Emory NLP Lab led by Professor Jinho Choi at Emory University in collaboration with Amazon Alexa AI. The traditional method of evaluating dialogue models involves human judges selecting which conversation is better or rating conversations on a Likert scale. However, this method can be subjective and does not provide a fine-grained evaluation of chat quality.

ABC-Eval aims to reduce subjectivity by explicitly annotating whether each model response expresses certain behaviors such as ignoring its partner, contradicting itself, hallucinating incorrect facts, or violating common sense knowledge. This approach measures the rates at which chat models commit various thematic errors, including ignoring the partner, saying something irrelevant, contradicting itself or its partner, hallucinating incorrect facts, and violating common sense knowledge.

The researchers selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. They also evaluated these conversations using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. The results showed that ABC-Eval behavior labels are more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. Additionally, ABC-Eval labels are more predictive of overall conversation quality compared to metrics produced by existing methods.

The researchers checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. The combination of all ABC-Eval metrics explains over 25% of conversation quality, while the combination of all turn-level Likert metrics explains far less of the quality. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve.

However, the bots tested still have common sense violations in around 20% of their responses, produce irrelevant information in around 15% of the responses, and contradict themselves or their partner around 10% of the time. Despite these challenges, the researchers hope that ABC-Eval can be leveraged by others in the field as a meaningful step in improving the evaluation of conversational AI.</sample>
    <sample id="136">Jasivan and his supervisor Nafise conducted a study at the University of Sheffield titled "FERMAT: An Alternative to Accuracy for Numerical Reasoning." The motivation behind this work is to address the challenges in numerical reasoning tasks, such as fact-checking, which require factual correctness. The study focuses on understanding why certain models perform poorly in numerical reasoning tasks, particularly with smaller models.

The researchers introduced FERMAT, a flexible evaluation set based on arithmetic types that tests number understanding, mathematical operations, and training dependency. They extracted questions from Illinois and CommonCore, changing the representation of numbers to mimic real-life scenarios. The study found that most models perform poorly across all aspects, but fine-tuning with math teachers' templates improved performance.

The study also investigated training dependency, finding that even when exact expressions are seen during training, accuracy remains below 50%. This suggests that linguistic notions are important in numerical reasoning. Additionally, the impact of training templates was examined, showing that language and mathematical diversity improve performance.

In conclusion, existing benchmarks tend to be unrepresentative, and single scores don't help fill the gap. FERMAT provides a more informative alternative, emphasizing the importance of language and mathematical diversity, as well as areas of improvement in number encoding and tokenization.</sample>
    <sample id="137">The paper "Tell2Design: A Dataset for Language-Guided Floor Plan Generation" by Sicong from the Singapore University of Technology and Design presents a new machine-learning task where a model generates floor plan designs directly from language instructions. The task involves generating 2D floor plan designs that comply with provided language instructions, which include semantics (type and functionality of each room), geometry (shape and dimension of each room), and topology (relationships among different rooms). The dataset consists of 5,051 human-annotated language instructions collected from Amazon Mechanical Turk and around 76,000 artificially generated instructions from pre-defined templates. The main challenges are designing under stricter constraints, understanding the big picture of the entire floor plan from unstructured text, and dealing with ambiguous or incomplete information in human instructions. The authors propose a sequence-to-sequence model using a transformer-based encoder-decoder structure to generate floor plan layouts from language instructions. The model is initialized by a pre-trained language model T5 and uses a normal language modeling objective. The results show that the proposed method outperforms other text-conditional image generation baselines with a Micro IoU of 54 and a Macro IoU of 53. However, the method struggles when training only on artificial instructions while testing on human-written ones due to a language distribution gap. Nevertheless, the performance improves significantly when artificial instructions are used for warming up before training on human instructions.</sample>
    <sample id="138">根据所提供的英文内容，作者认为NLU中研究不足的领域是模型在整合和使用来自不同来源的知识方面的能力。他们强调了自然语言理解任务需要同时利用预训练时间和推理时间的知识的重要性，并且大多数核心参考解析模型无法在没有特定任务培训的情况下有效地处理这种知识整合。</sample>
    <sample id="139">演讲者的名字是Ying。</sample>
    <sample id="140">是的，CoScript 经过了质量检查。为了确保验证和测试集的质量，我们请 crowdsourced 工人查找并修改错误样本。</sample>
    <sample id="141">现有的资源存在局限性，因为它们通常依赖于领域知识和人工策展，这限制了它们支持的上下文依赖翻译的类型和语言。</sample>
    <sample id="142">你好！我们将讨论我们关于“解决间接引用表达以进行实体选择”的工作，在这项工作中，我们引入了AltEntities语料库。我的名字是Javad Hosseini，这是一项与Filip Radlinski、Silvia Pareti和Annie Louis合作的工作。我们的目标是理解用户在想要做出选择时的语言。考虑这个问题的另一种说法：“你是说《Easy on Me》还是《I Gotta Feeling》？”这里，用户想在两个歌曲之间做出选择。最明显的方法是使用直接引用，例如通过说歌曲名称“Easy on Me”或其位置，“第一个”。但有时候， indirect reference 更合适，以便进行更自然的对话。这可能发生在用户无法记住歌曲名称时。或者，当歌曲名称的读音非常相似，很难区分时。或者，当用户想指定偏好时。这里有一些例子，这些例子是 indirect references 的示例，例如，“较新的那个”或“那首不那么有活力的歌”。这是对话系统中一个重要的问题，也是衡量LLM对实体理解能力的一个基准。我们不知道有一个更大的公共数据集可以解决这个问题，所以我们用 crowdsourcing 来收集数据。我们的数据集涵盖了三个不同的领域：音乐、书籍和食谱。我们数据集收集方法强调了非正式性，使用卡通完成设置。卡通有三个对话气泡。在第一个气泡中，Bob说，“记得昨天我们在听的那首歌吗？”然后Bob设置了对话背景。在第二个气泡中，Alice说，“你是说《Easy on Me》还是《I Gotta Feeling》？”这是另一种问题。第三个气泡中，Bob使用 indirect reference 选择其中一个实体，例如，“较新的那个”。我们自动提供前两个对话气泡，第三个对话气泡由标注员填写。第一个对话气泡从每个领域的几个手动提示中选择。第二个对话气泡，即另一种问题，按照以下模板生成。你是说A还是B？其中A和B是从Wikipedia中采样出来的。以下是我们在采样中使用的不同方法。当我们向上移动时，实体变得越来越相似，通常更容易区分。第一种方法是随机均匀采样。第二种方法是实体具有类似的标题，例如两本书名为“Return”的书。第三种方法是实体具有类似的Wikipedia描述。最后一种方法是实体具有相同的属性或信息，例如同一流派或同一艺术家的歌曲。当我们向标注员展示这种另一种问题时，他们知道这两个实体的名称，但不一定了解这些实体。所以，我们向标注员展示了这两个实体的背景知识。对于歌曲，我们简单地显示Google搜索结果，然后要求标注员至少听一下每首歌，并阅读关于每首歌的信息。例如，这是歌曲“Easy on Me”的Google搜索结果。对于食谱和书籍领域，我们展示了来自Wikipedia的一些背景文本。对于食谱，我们还展示了它们的图片，再次来自Wikipedia，这样标注员就能知道它们看起来是什么样子。然后我们要求标注员从这两个实体中选择一个，并用三到五个 indirect referring expressions 描述它们。例如，“那个有钢琴音乐的”，“没有歌词的那个”，“不是那个12岁男孩的那个”，“那个虚构的那个”，“来自阿塞拜疆的那个”，等等。AltEntities语料库包含6000个另一种问题，涵盖三个领域，以及42000个 indirect referring expressions。T5 XL模型的结果如下。如果语言模型拥有标注员相同的背景知识，那么准确率会很高，大约为92%到95%。但这并不现实。如果语言模型拥有部分重叠的背景知识，那么准确率会在82%到87%之间，这更加现实。例如，当语言模型检索背景知识时。如果语言模型只有实体名称作为背景知识，那么准确率只有60%，所以还有很大的改进空间。我们还展示了模型在不同领域的一般泛化能力。这里是数据集的链接。谢谢。</sample>
    <sample id="143">该方法与 Wait-k 策略、Local Agreement 策略和专门用于同时预翻译的最新架构进行了比较。</sample>
    <sample id="144">根据所提供的英文内容，无法确定论文作者所属的机构。</sample>
    <sample id="145">演讲者的名字是Jenny。</sample>
    <sample id="146">The speaker, Yicheng from Fudan University, introduces a paper on the analysis of omission in dialogue summarization. Dialogue summarization is a subtask of text summarization that involves creating a concise summary representing the most important information within a dialogue. The paper highlights the challenges in identifying key information in unstructured dialogues and the prevalence of omission errors in generated summaries. The authors constructed the OLDS dataset to address this issue, providing high-quality omission labels for dialogue summarization across five domains. They explored three baseline frameworks for omission detection and found that the task is challenging with an F1-score around 50%. The post-editing method for summary refinement using detected omissions significantly improved the performance, indicating the value of omission detection in improving dialogue summarization quality.</sample>
    <sample id="147">根据所提供的英文内容，这篇论文有三位作者：Myra、Esin Durmus和Dan Jurafsky。</sample>
    <sample id="148">嗨，我是萨拉·帕皮，来自特伦托大学和布罗尼奥·基索基金会，我将简要介绍“注意力作为同时语音翻译的指南”论文，这是与马泰奥·内格里和马可·图尔奇合作完成的。同时语音翻译（SimulST）是什么？同时语音翻译，或SimulST，是指在实时将 spoken language 翻译成另一种语言的文字，从而实现跨语言交流。当前 SimulST 模型的问题有哪些？通常会为特定架构训练特定模块，引入额外模块进行优化。长而复杂的训练过程，例如涉及不同优化目标的训练。训练和维护多个模型以达到不同的延迟范围。例如，训练一个平均延迟为一秒钟的模型和另一个平均延迟为两秒钟的模型等等。我们的解决方案是什么？首先，使用现有的离线 ST 模型，无需重新训练或采用特定架构进行 SimulST。只使用一个模型处理每个延迟范围，并通过特定参数处理延迟。利用模型已获得的知识，通过音频输入和文本输出之间的交叉注意力机制。我们的解决方案是提出 EDAtt，即编码器-解码器注意力，这是一种策略，我们决定是否发出或不发出部分翻译，基于注意力指向的位置。如果注意力未集中，即其总和低于某个阈值 alpha，且指向最近 lambda 个语音帧，则发出一个词。例如，如果我们收到包含“我要谈论…”的语音片段，模型预测德语翻译，我们将查看交叉注意力权重，可以看到前两个词指向最早收到的语音帧，而最后一个词指向最近 lambda 个语音帧。这意味着前两个词将被发出，因为它们的交叉注意力总和高于阈值 alpha，因此我们不会发出最后一个词，而是等待另一个语音片段。如果我们继续接收另一个语音片段，模型预测其他三个词，我们将查看这些交叉注意力权重，可以看到没有词指向最近 lambda 个语音帧。这意味着这三个词将被发出。如果我们查看 EDAtt 的主要结果，我们将绘制同时语音翻译结果的图表，在其中一个轴上测量翻译质量的 BLEU，而在另一个轴上测量平均延迟，我们还考虑了模型预测输出的计算时间。我们希望曲线尽可能高，但也要向左移。我们将其与应用于离线模型的流行策略进行了比较，这些策略包括等待 k 策略和局部一致性。我们还将其与专门针对同时预翻译的最新架构进行了比较。这些是我们在德语上的同时语音翻译策略的主要结果。我们看到 EDAtt 出色地超过了应用于离线模型的所有策略，因为曲线向左移。我们还看到，如果我们考虑实际耗时或计算感知时间，即预测输出所需的时间，那么 EDAtt 是最快的策略。如欲了解更多信息，请阅读我们的论文。我们还公开发布了代码、模型和同时输出，以促进我们工作的可复现性。谢谢您的关注。</sample>
    <sample id="149">是的，数据集公开了。</sample>
    <sample id="150">Archiki is presenting a paper titled "MEETINGQA: Extractive Question-Answering on Meeting Transcripts" along with collaborators from Adobe Research and UNC Chapel Hill. The paper addresses the gap in existing research that focuses only on summarization and extracting action items, while ignoring the significant QA component in meeting discussions. The authors introduce a new dataset called MeetingQA, which contains 7.7K questions split between the Train, Dev, and Test sets, with 30% of questions being unanswerable. They also discuss their data collection process, which involves selecting public meeting transcripts from the AMI corpus and annotating answers by recruiting annotators to label sentences in the answer span. The authors employ various methods such as context-retrieval, single-span models, multi-span variants, and silver data augmentation for fine-tuning and zero-shot performance. They also highlight challenges faced by models, such as identifying rhetorical questions, identifying which speaker answers a question, and predicting irrelevant sentences. Overall, the paper presents an interesting dataset based on open-ended and discussion-seeking questions in real-life meeting scenarios, and highlights the challenges of existing QA models in both fine-tuned and zero-shot settings.</sample>
    <sample id="151">大家好，我的名字叫 Ying，我的同事是 Zhiyang。我们将展示我们关于 MultiInstruct 如何通过指令调优提高多模态零-shot 学习的研究。随着大型语言模型的 advances，许多研究开始探索如何利用预训练的语言模型以参数和数据高效的方式解决不同的下游任务。最近，许多研究表明，通过遵循自然指令，指令调优可以让大型语言模型在未见过的任务上进行零-shot 的表现。然而，大多数之前关于指令调优的研究都集中在通过自然指令提高语言-only 任务的零-shot 性能上，而计算机视觉和多模态任务则被忽视了。因此，在本研究中，我们旨在探讨通过指令调优多模态预训练模型是否可以提高对未见过的多模态任务的一般泛化能力。此外，在我们研究期间，我们发现 NLP 和多模态之间的指令数据集可获得性存在显著差异。存在超过 1600 个语言-only 指令任务。然而，没有大规模公开可用的多模态指令任务。因此，这激励我们构建一个多模态指令调优数据集。在这里，我们介绍了 MultiInstruct，这是第一个包含 62 个多样化的多模态任务的数据集，涵盖 10 个广泛类别。这些任务是从 21 个现有开源数据集中派生出来的，每个任务配备了五个专家撰写的指令。为了在我们提出的多模态指令调优数据集上进行多模态指令调优，我们选择 OFA，一个统一的多模态预训练模型作为基础模型。OFA 使用统一的词汇表表示语言、图像标记和 bounding box 坐标。这里我们展示了 MultiInstruct 数据集的一些示例，以统一处理各种输入和输出数据类型。我们遵循 OFA 的方法，将所有任务统一为序列到序列格式。在输入文本、图像、指令和 bounding box 中，每个实例随机组合其中一个五种指令模板。在测试阶段，对于每个任务，我们总共进行 5 次实验，通过使用五种指令之一评估模型。在每次实验中，我们报告性能的最小值和最大值以及所有 5 次实验的标准差。如果任务是多模态分类任务，我们报告准确率。如果是多模态生成任务，我们报告 Rouge-L。对于 NLP 任务，我们同样报告 Rouge-L。我们还引入了一个额外的评估指标叫做敏感度。这个指标衡量模型在任务中是否能够一致地产生相同的输出，无论指令的 wording 稍有变化。我们的主要结果如下：我们可以看到，指令调优可以显著提高 OFA 在已知多模态任务上的性能。此外，通过自然指令数据集的迁移学习可以进一步提高指令调优。我们可以看到，随着任务数量的增加，模型的性能更好，同时敏感度降低。我们还进行了一个实验，我们使用一种指令与五种指令。我们可以看到，使用更多指令可以提高模型的整体性能，并显著降低其敏感度。这展示了不同微调策略对模型敏感度的影响。我们可以看到，通过从自然指令数据集的迁移学习，模型可以实现比原始 OFA 模型更低的敏感度。我们也可以看到，通过从自然指令数据集的迁移学习，OFA 可以在自然指令数据集上取得更好的性能。总的来说，我们提出了第一个大规模多模态指令调优数据集，显著提高了 OFA 的短期能力，并探索了不同的迁移学习技术及其益处。我们设计了一个新的指标叫做敏感度。最后，我们正在收集一个更大的多模态指令调优数据集，其中包含大约 150 个额外的视觉语言任务，我们将发布它们。这是我们的数据和模型的 QR 码。谢谢。</sample>
    <sample id="152">Frederick Riemenschneider introduces a presentation on the intersection of NLP and classical philology, focusing on the development of language models for Ancient Greek and Latin. He highlights the limitations of existing monolingual BERT models and the need for multilingual models that can handle both Ancient Greek and Latin texts. The presentation discusses the creation of new models, including GreBERTa, GreTa, PhilBERTa, and PhilTa, which are pre-trained on Ancient Greek, Latin, and English data. The speaker also explains the challenges of gathering pre-training data from the Internet Archive and the methods used to create a high-quality corpus. Benchmarking results show that the models outperform previous models in tasks such as part-of-speech tagging, dependency parsing, and lemmatization. The presentation concludes with an analysis of the performance of T5's encoder and the implications of multilinguality in language models.</sample>
    <sample id="153">Ninareh Mehrabi, a postdoctoral scientist at Amazon Alexa AI's Responsible AI team, presented her work on "Resolving Ambiguities in Text-to-Image Generative Models." The study aimed to address the ambiguities in prompts provided to text-to-image models, which can lead to inaccurate image generation. The research proposed frameworks for mitigating these ambiguities and evaluating the faithfulness of generated images to user intention.

The research involved curating a benchmark dataset based on the LAVA corpus, covering various types of ambiguities. A prompt disambiguation framework was then developed, using either clarifying questions from the user or generating different possible visual setups. The disambiguated prompts were input into a text-to-image model to generate images, which were evaluated using a VQA model to determine if they satisfied the user's intention.

The findings showed that there is disparity in resolving ambiguities for different ambiguity types, but overall, the disambiguation framework had a positive effect on faithful generation. The automatic evaluation framework was found to be in agreement with human evaluation, making it reliable for evaluating text-to-image models. Additional findings and discussions are available in the paper.</sample>
    <sample id="154">根据所提供的英文内容，论文的作者Sara Papi、Matteo Negri和Marco Turchi所属机构是特伦托大学和布罗纳基西基金会。</sample>
    <sample id="155">演讲者的名字是Javad Hosseini。</sample>
    <sample id="157">The speaker, Shen Gao from Shandong University, introduces their work on dialogue summarization with static-dynamic structure fusion graph. The goal of dialogue summarization is to distill salient information from a dialogue context into a concise summary. This task is challenging and interesting in the text summarization research field. Dialogue summarization can help people quickly capture the highlights of semi-structured and multi-participant dialogues without reviewing the complex dialogue context.

Existing dialogue summarization methods mainly focus on modeling dialogue with pre-computed static graph structures using external linguistic tools such as discourse parsing and dialogue state tracking. However, there are two fundamental drawbacks of using these pre-computed dialogue structures: they heavily depend on the reliability of the external linguistic tools, which may not deliver accurate output and cause error propagation, and the static graph construction is disjoint with the graph representation learning phrase, and such a fixed graph could not dynamically adapt to the downstream dialogue summarization task.

In their SDDS model, four main components are employed: an Utterance Encoder to encode utterances in the dialogue context into vector representations, a Static-Dynamic Graph module that combines multiple static graphs computed in the previous step and uses the dynamic graph module to capture the semantic relationship between utterances based on their deep vector representation, and a pre-trained language model as the Summary Generator to fuse the static dialogue structure and the dynamically learned dialogue structure into the final summary.

To capture the static dialogue structure information, four heuristic dialogue structure modeling methods are proposed to build the relationship between utterances using a graph network. These methods include Discourse Parsing Graph, speaker relationship modeling method, utterance position graph, and embedding matrix. To capture the semantic relationship between utterances based on their deep vector representation, a Dynamic Graph module is proposed that does not use any pre-computed or heuristic method to build connections between nodes. A multi-head attention model is employed to calculate the relationship, and a fusion method is proposed to combine the relation matrix A of the dynamic graph and the adjacent matrix G^s of the static graph into a unified graph, G^u. To incorporate the graph representation which captures the dialogue structure information in the generation process, a dual cross-attention mechanism by proposing a graph attention layer on top of the original self-attention layer is used.

The code and data have been released on GitHub, and you can scan the QR code to download it.</sample>
    <sample id="158">The speaker introduces the task of coreference resolution, which involves identifying and clustering mentions that refer to the same entity in a document. Conventional methods have quadratic complexity for both computation and memory consumption, while cache-based methods use a fixed-size cache and reduce the complexity to a linear level. However, in long documents, the topic may switch multiple times, leading to high cache misses with LRU eviction policy. The proposed dual cache has a local cache with LRU eviction policy and a global cache with LFU policy. The model scans the document from left to right, classifies new or updated entities, and evaluates their frequency before adding them to the appropriate cache. The dual cache outperforms single cache methods and significantly reduces cache misses, making it the most cost-effective option.</sample>
    <sample id="159">大家好，我是库斯瓦尔·辛哈，我很高兴欢迎你们参加我们ACL 2023年的论文讨论。语言模型的可接受性判断并不总是对上下文具有稳健性。这是一项与乔恩·加西亚、阿隆·穆勒、卡尼什卡·米斯拉、凯伦·芬奇、罗ger·利维和阿迪纳·惠尔姆等人的合作研究。在这项工作中，我们重新审视了最小对 paradigms。最小对 paradigm 主要评估语言模型在可接受性判断上的表现，这也可以包括语法，如 BLiMP、SyntaxGym 或者可接受性与刻板印象相关的判断，如 CrowS pairs。在这个最小对 paradigm 中，典型的评估方式是展示一个可接受句子或正确句子，然后展示一个可接受句子或错误句子。希望模型将更多的概率分配给可接受句子。当前的 MPP 管线不允许我们评估模型在整个上下文窗口中的可接受性。如今，大型语言模型正在出现更长和更长的上下文窗口。因此，评估模型在整个上下文窗口中的可接受性至关重要。这就是我们在这里所尝试的。我们正在重新审视 MPP 管线，通过让模型评估更长和更长的序列来评估模型的可接受性。这就是我们的方法。我们所做的就是模拟这些更长的序列，通过重新创建句子来重新审视数据集本身。例如，这里我们从 BLiMP 数据集中选择一个典型的可接受性对，即从“ adjunct island”案例中选择一个可接受句子。然后，我们将可接受句子作为前缀添加到可接受查询和不可接受查询中。我们可以用相同的方法选择来自同一数据集的不可接受句子，也可以选择来自不同子集或不同数据集的句子。这就是我们所谓的 mismatch 情景。这里，句子仍然来自相关数据集，但不是你用来评估的当前数据集。我们还可以选择来自完全 unrelated 领域的句子，如 Wikipedia。这样可以告诉我们模型的可接受性判断是否真的受到任何上下文的影响，比如上下文是否来自不同的数据集子集，或者是否完全无关到我们要查看的当前句子。那么模型的表现如何呢？首先，我们看 Wikipedia 句子，这些句子完全无关到当前查询对，我们发现 MPP 判断在任意上下文长度下相对稳定。我们把上下文长度增加到 OPT 和 GPT-2 模型的最大值1024。我们看到橙色点线表示 MPP 判断相对稳定。接下来，我们选择或创建句子，从相同的 BLiMP 或 SyntaxGym 数据集中选择可接受和不可接受领域。这里我们看到，当我们在可接受查询和不可接受查询前添加可接受前缀或不可接受前缀时，MPP 判断要么显著增加，要么显著减少。但是，当我们选择与 BLiMP 或 SyntaxGym 中相同的句法现象时，MPP 判断要么显著增加，要么显著减少，取决于选择的前缀是可接受还是不可接受。这种效果随着上下文长度的增加而加剧，这可能会影响具有大上下文窗口的新模型。为什么匹配前缀会如此显著地影响语言模型的判断呢？我们进行了一系列分析，其中我们尝试通过保留相关结构同时向输入添加噪声来扰动输入句子。经过多次扰动后，我们发现这些噪声并没有使模型改变其 MPP 判断的方向。换句话说，我们发现模型对扰动句子的敏感度是一致的。也就是说，当我们扰动可接受领域的句子时，所有扰动都会导致类似的增加；当我们扰动不可接受领域的句子时，所有扰动都会导致类似的减少。我们工作的关键 takeaway 是，语言模型对共享跨句子的潜在句法和语义特征敏感。当前的 MPP 评估方式，即使用短单个句子输入，可能无法充分捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以获取更多实验细节。谢谢收听。</sample>
    <sample id="160">该方法的第一步将输入词元映射到一个无序的多集词元。</sample>
    <sample id="161">CoScript 中包含了 55,000 个脚本。</sample>
    <sample id="163">根据所提供的英文内容，DEPLAIN的最佳对齐方法是MASSalign。</sample>
    <sample id="164">弱监督学习的一个好处是，它比手动标注数据便宜得多。在弱监督学习中，我们使用弱标注源，如简单的启发式规则、知识库或低质量的众包，来标注数据。这些标注通常比人类标注便宜得多，但它们也存在噪声，即标注中存在错误。通过弱监督学习，我们可以训练神经网络在标注噪声下进行训练，从而提高模型的泛化能力。</sample>
    <sample id="165">The speaker, Wenting Zhao from Cornell University, presents a paper titled "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations." The paper introduces an unsupervised learning method called LiPoR (Likelihood Learning with Posterior Regularization) to bridge the information gap between context and outcome in abductive reasoning. Current approaches rely on supervised methods that require annotated plausible explanations, which can be noisy and subjective. LiPoR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context by marginalizing possible explanations. A regularizer is introduced to enforce mutual exclusivity among explanations, preferring a subset of explanations over others. The paper compares LiPoR to zero-shot models and the previous best unsupervised approach on AlphaNLI, outperforming all of them by over 4 absolute points in accuracy.</sample>
    <sample id="166">The presented work, "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text," addresses the challenge of image text reasoning tasks involving linguistically complex text. Typical methods like visual language models struggle with such tasks due to the similarity of images and the length of descriptions. The proposed method draws inspiration from the Divide-and-Conquer strategy and Dual-Process Theory.

The first model, Proposition Generator, decomposes complex proposition texts into simple representations using BART's decoder. System 1, Visual-Linguistic Interactor, performs visual-propositions' information interaction, similar to System 1 in Dual-Process Theory. It outputs matching scores of propositions and images along with reasoning states.

System 2, Neural-Symbolic Reasoner, integrates reasoning states and results of simple propositions to obtain the final solution. It consists of a negation executor and conjunction operation. The negation executor gains the negational reasoning state of positive propositions, while the conjunction operation obtains inference results based on joint positive and negational reasoning states.

The proposed method combines the inference results of System 1 and System 2 to gain the final solution. Experimental results show that NDCR outperforms other baselines, and abolition experiments verify the effectiveness of each module. Two cases demonstrate the proposed method's interoperability and ability to present inference states and results in the middle step.

The study suggests that neural symbolic calculation may improve compositional reasoning and planning in large language models. Divide-and-Conquer is similar to self-asking chain-of-the-thought, which decomposes complex reasoning into simple problems and constructs a reasoning path. Dual-Process Theory can be integrated with Divide-and-Conquer.</sample>
    <sample id="167">DEPLAIN-web 中的文档被手动和自动对齐方法进行了对齐。具体分配情况如下：483 份文档被手动对齐，另外 267 份文档使用自动对齐方法进行对齐。总共，DEPLAIN-web 包含了 750 份文档。</sample>
    <sample id="168">CoNLL++数据集是通过从2020年的Reuters新闻中收集数据并使用与CoNLL-2003相同的标注指南进行标注而创建的。然后在CoNLL-2003上对超过20个模型进行了微调，并在CoNLL-03测试集和CoNLL++上评估了它们。最后，计算了每个模型在CoNLL++上的F1百分比变化，以评估其泛化能力。</sample>
    <sample id="169">In this paper, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it's important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it's in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn't have a big influence in the case of several short promptings. It's crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It's the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So, it's important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it's more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the "Style/Awesome" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy.</sample>
    <sample id="170">大家好，我是来自宾夕法尼亚州立大学的张雨生。今天我要向大家介绍我们团队的研究成果“XSemPLR：多语言和多种意义表示的跨语言语义解析”。语义解析的任务是构建用户查询的语义表示，例如SQL和Lambda Calculus。而跨语言语义解析则是将多种自然语言的查询翻译成多种意义表示。如图所示，我们需要使用神经模型将多种自然语言的查询翻译成SQL、Lambda或FunQL等。现有的跨语言语义解析模型分别在有限任务和应用的数据集上提出和评估。例如，某些自然语言的覆盖范围很大，但中文却缺失，缺乏在某些意义表示上的覆盖范围。Lambda calculus也缺失，或者只在某些神经模型上进行评估。为了解决这些问题，我们提出了XSemPLR。我们提供了一个统一的数据集XSemPLR，用于在多种自然语言和多种意义表示上进行跨语言语义解析。它包含9个不同领域的数据集，5种语义解析任务，8种意义表示，以及22种自然语言，涵盖15种语言家族。为了更好地评估我们的基准测试，我们考虑了六种训练和评估设置。第一个是Translate-Test。我们使用Google Translate API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，在训练英语模型时，我们使用英语查询，而在推理阶段，我们将德语查询翻译成英语，然后使用训练好的模型预测SQL。我们还将测试单语模型。在这个设置中，源语言与目标语言相同，例如德语到德语或英语到英语。我们还测试了单语少样本设置，即只使用10%的训练数据训练单语模型。我们还测试了多语言模型，即我们为所有语言训练一个统一的多语言模型。例如，我们将德语、英语和中文查询一起训练一个模型。在推理阶段，我们可以使用这个模型来翻译德语查询或中文查询等。我们还考虑了跨语言零样本和少样本转移。我们在一种源语言上进行训练，然后转移到另一种语言。因此，在训练过程中，我们训练模型在英语查询或英语和德语少样本查询上进行训练，以预测SQL输出。我们还发现许多有趣的结果。关于单语模型的分析，我们评估了两组模型，包括编码器-指针（Encoder-PTR），即多语言预训练编码器与指针解码器，例如XLM-R + PTR和mBERT + PTR。我们还评估了编码器-解码器模型，即多语言预训练编码器-解码器模型，例如mBART和mT5。我们发现编码器-解码器在所有九个数据集上取得了最佳性能。我们在多语言设置下评估了mT5和XLM-R + PTR。我们发现编码器-解码器或编码器-指针可以通过在各种语言混合训练来提高性能。我们发现大多数主要自然语言都可以获得性能提升，但英语在七个数据集中性能下降，在三个数据集中性能提升，这被称为“多语言 curse”。我们还比较了跨语言性能差距。在这幅图中，蓝色线代表跨语言少样本转移。橙色线代表跨语言零样本转移。绿色线代表单语设置。我们发现，通过比较绿色和橙色线，我们发现在零样本设置中，跨语言转移性能差距显著。然后，通过比较蓝色和橙色线，我们发现，在少样本设置中，转移差距迅速缩短。我们还发现了其他一些有趣的发现。例如，编码器-解码器优于以往工作或实现了可比结果。在英语自然语言上预训练可以显著提升少样本目标自然语言的性能。我们发现，多语言语言模型，如Codex和BLOOM，对于跨语言语义解析任务来说仍然远远不够。总之，我们构建了XSemPLR，这是一个统一的跨语言语义解析基准测试，涉及多种自然语言和多种意义表示。我们对三种代表性多语言语言模型进行了全面基准测试。我们的结果展示了许多有趣的发现。等等。欢迎访问我们的论文和代码。谢谢收听。</sample>
    <sample id="171">现有的研究可以被大致分类为四个类别。然而，这些方法要么不适用于嵌入服务，要么缺乏可转移性。因此，在本文中我们提出了“嵌入标记”，一种基于后门的水印方法，适用于嵌入服务。</sample>
    <sample id="172">根据所给的英文内容，Codex 或 Bloom 等多语言 LLM 对于 CLSP 来说还不足够。</sample>
    <sample id="174">Thea, one of the co-authors of the paper "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis," provides an overview of the unique features of this dataset in a video. Argument quality analysis involves judging the quality of arguments on a scale from 0 to 1, with more coherent and persuasive arguments receiving higher ratings.

Current datasets often lack quality because they are collected from crowdsourcing platforms, have limited diversity due to only covering 30 or 40 motions, lack depth in explaining why arguments are true, and have a motion associated with every single argument. ArgAnalysis35K addresses these issues by being the largest dataset with high-quality arguments, featuring around 85% of its arguments sourced from high-quality tournaments, expert debaters, intermediate debaters, and novice debaters.

The dataset also has a diverse range of arguments, with 24 themes selected based on experience in the circuit, websites like Hellomotions.com, and expert advice. It introduces the concept of analysis, which combines claims, premises, and other elements to explain arguments better. This is different from the general terms used in NLP.

The dataset also includes instance-based annotator reliability, which takes into account human biases and eliminates only the judgments that may be biased rather than eliminating all judgments from the annotators. Lastly, it introduces a relevance model that assigns scores from 0 to 1 for each argument's relevance to a theme, capturing how relevant it is to a particular topic.

Overall, ArgAnalysis35K is a culmination of unique features that provide a more diverse, reliable, and comprehensive dataset for argument quality analysis. Thea encourages viewers to check out the paper and poster at the conference for further insight into the results, dataset collection process, and annotation process.</sample>
    <sample id="175">该方法通过在训练过程中引入排列的对齐来处理排列的不确定性。这使得对于给定的输入标记，我们不知道它来自哪个多集标记，从而为训练带来了挑战。此外，有时存在多个与数据一致的排列，但语言上正确的排列是潜在的。我们通过在训练过程中引入排列的对齐来解决这个问题。</sample>
    <sample id="176">根据所给的英文内容，可以将下游 NLP 模型的公平性定义为模型在处理不同政治观点和群体时的一致性和公正性。这包括避免偏见、歧视和不公正的结果，确保所有用户都能得到相同的待遇和机会。</sample>
    <sample id="177">演讲者的名字是Yanis Labrak。</sample>
    <sample id="178">演讲者的名字是Koustav Sinha。</sample>
    <sample id="179">The speaker, Melanie Sclar, discusses the concept of Theory of Mind (ToM) in language models and presents a research paper on improving ToM reasoning skills in large language models. She explains that ToM is the ability to reason about the mental state of others and is traditionally measured through reading comprehension tasks involving multiple characters. False-belief questions are situations where reality may not match the belief of certain story characters.

The speaker provides an example of the Sally-Anne test, which involves Alice and Bob in a room with a basket and a box. Alice puts an apple in the basket and leaves the room, while Bob moves the apple to the box. The speaker asks where Bob will search for the apple and where Alice thinks that Bob will search for the apple when she comes back. These questions can be classified as first-order or second-order depending on whose mental state you are asking about. True-belief questions are those where the expected answer matches the true location of the object, and false-belief, otherwise.

The speaker explains that large language models still perform poorly on false-belief tasks, such as ChatGPT or GPT-3. Therefore, the research question is "How can we improve Theory of Mind reasoning skills in Large Language Models?" The research presents SymbolicToM, an inference-time method to improve ToM reasoning skills in Large Language Models using explicit graphical representations. SymbolicToM uses several graphical representations since mental states cannot be represented with a single graph. For example, on the left, we see a representation of what Bob believes is the current world state, and on the right, we see a representation of what Bob thinks that Alice believes is the current world state. We call these graphs BBob and BBob,Alice. In general, we compute these graphs for all combinations of characters p₁ through pₘ up to a predefined maximum Theory of Mind level m. Graphs are computed using an inference-time algorithm that leverages off-the-shelf NLI and OpenIE models. See the paper for details.

Having pre-computed these graphical representations for a given story, we can efficiently answer any given question. For example, let's say we want to answer, "Where does Alice think that Bob will search for the apple?" We first detect the entities in the question. Then we retrieve the appropriate belief graph and perform recursion over the question so that now we're asking a factual question over the graph. Then we retrieve the sentences captured by the graph and finally take the sentences plus the factual question and feed it to a language model to get the final answer.

The speaker then presents the experiments, testing their method with a myriad of LLMs and comparing it against supervised baselines, specifically a fine-tuned GPT-3 model and Textual Time Travel, which is a model specifically designed for Theory of Mind reasoning. They analyze in-domain performance in the well-known ToMi dataset and evaluate robustness with two out-of-domain setups that they designed. They show that using SymbolicToM significantly improves out-of-the-box LLM performance, outperforming supervised approaches on out-of-domain story understanding and remaining beneficial on the new linguistic diversity dataset, ParaphrasedToMi.</sample>
    <sample id="180">演讲者的名字是Myra。</sample>
    <sample id="181">The paper "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" by Siyu Yuan and colleagues from Fudan University addresses the challenge of planning for specific goals with multiple constraints, which is under-studied in previous work. The authors define the problem of constrained language planning and evaluate the ability of large language models to generate scripts that are faithful to constraints. They develop a method to improve the generation quality of these models using over-generate-then-filter, which involves generating multiple scripts for specific goals and selecting the most faithful ones based on semantic similarity and keyword presence. The authors also create a dataset named CoScript, which consists of 55,000 specific goals with scripts generated by large language models. This dataset can be used to train smaller but specialized models for constrained language planning, as demonstrated by the superior performance of T5 fine-tuned on CoScript compared to most large language models.</sample>
    <sample id="182">热带主义 (tropicalism) 在本文的背景下指的是与 Latina 女性的刻板印象相关的 tropes，这些 tropes 将 Latina 女性描绘为充满活力、充满活力和充满异国情调的。这些 tropes 通常基于刻板印象，将 Latina 女性描绘为异国情调的、充满异国情调的，或者具有异国情调的，这可能导致 Latina 女性被边缘化和刻板印象化。</sample>
    <sample id="183">作者通过使用自然语言提示来创建目标群体的人工描写。他们使用了类似于“想象一下你是一个亚洲女性。描述你自己。”的提示，这些提示可以轻松地适应任何人口统计信息。这种方法使他们能够生成可推广到各种人口统计和背景的描写。</sample>
    <sample id="184">在本文中，语境使用情况是通过CXMI（Contextual Mutual Information）来衡量的。CXMI是一种衡量给定上下文C时，模型对目标Y的信息量的指标。通过测量给定源X的情况下，上下文C提供的关于目标Y的信息量，可以评估模型对语境的依赖程度。</sample>
    <sample id="185">DrBERT 和 ChuBERT 的主要区别在于数据来源。DrBERT 是基于从网上爬取的 NACHOS 数据集训练的，而 ChuBERT 则是基于匿名化的临床数据，来自南希大学医院数据仓库。</sample>
    <sample id="187">根据所提供的英文内容，这篇论文有两位作者：Ying和Zhiyang。</sample>
    <sample id="188">迭代迁移学习是指在训练模型时，先从与目标任务相关的其他任务中转移权重，然后在每次迭代中进一步微调模型。这种方法可以帮助模型更好地学习目标任务，尤其是在标注数据有限的情况下。</sample>
    <sample id="189">数据集的目标是收集一个大规模的公共数据集，用于解决间接引用表达式选择实体的问题。</sample>
    <sample id="190">攻击者可以通过学习 EaaS 提供的嵌入来提取模型参数。</sample>
    <sample id="191">这篇论文有三位作者：Sara Papi、Matteo Negri和Marco Turchi。</sample>
    <sample id="192">The presentation is by Yang Luo and focuses on their work titled "CAME: Confidence-guided Adaptive Memory Efficient Optimization." The presenter explains that while adaptive gradient-based optimization methods like Adam are effective, they require triple the memory for keeping first and second moment estimates of per-parameter gradients. On the other hand, memory-efficient optimizers like Adafactor reduce auxiliary memory usage but at the cost of performance. The challenge is to design an optimizer that achieves both fast convergence and low memory usage.

The presenter introduces non-negative matrix factorization (NMF) as a method to reduce memory requirements from O(mn) to O(m+n). They also discuss how Adafactor presents an analytic solution to achieve the minimum I-divergence between the matrix V and the approximation matrix W x H in the special case of rank-1 factors. However, NMF operations in Adafactor can cause erroneous updates during deep neural network training, leading to slower convergence compared to Adam.

To address this issue, the presenter proposes an efficient approach to decrease the side effect caused by insecure updating. This involves calculating the instability matrix (uₜ) and using the square root of it as the denominator for mₜ to take an optimization step. The presenter performs experiments on BookCorpus and English Wikipedia, comparing CAME with existing optimizers like Adam and Adafactor. The results show that CAME achieves significant improvements in validation accuracy and better performance than Adam in pre-training large language models.

The presenter also compares the end-task performance of BERT-based models trained with CAME and the baseline on typical downstream tasks. The experimental results demonstrate the efficiency of CAME by showing that BERT-based models trained with CAME on two batch sizes achieve comparable performance to the baseline with less memory cost. The presenter concludes by highlighting the effectiveness of CAME on large language model training tasks and its suitability for large batch training.</sample>
    <sample id="193">根据所提供的英文内容，没有明确说明用于创建初始数据集的注释者数量。</sample>
    <sample id="194">根据所提供的英文内容，论文的作者是卡内基梅隆大学的第一年博士生。</sample>
    <sample id="195">The presented work, "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering," aims to address the limitations of existing methods in Explainable Question Answering (XQA). The two main directions in XQA are neuro-symbolic methods and decompose-based methods. Neuro-symbolic methods translate natural language questions into formal representations like SPARQL but have limited recall due to incomplete knowledge bases. Decompose-based methods generate intermediate steps leading to the final answer but struggle with the diversity of natural language and lack the ability to integrate knowledge from heterogeneous sources.

To overcome these challenges, the proposed framework, RoHT (Reasoning over Hierarchical Question Decomposition Tree), is introduced. RoHT consists of two stages: firstly, building a Hierarchical Question Decomposition Tree (HQDT) that represents the hierarchical compositional structure of a complex question, and secondly, performing probabilistic reasoning over HQDT to fuse knowledge from different sources at various levels.

RoHT's HQDT construction involves using a question decomposer to generate leaf loss (atomic questions), a question generator to create intermediate questions based on grouped leaf questions, and computing certainty scores for each node. Probabilistic reasoning over HQDT is conducted recursively from the root to leaves, involving determining appropriate knowledge sources, retrieving answers with probabilities, and aggregating candidate answers to output the top key answers with the highest probabilities.

The RoHT framework has been evaluated on two challenging complex QA datasets, KQA Pro and Musique. On KQA Pro, RoHT outperforms existing KB QA methods when using an incomplete KB and shows substantial improvement when Wikipedia is added as a supplementary text corpus. RoHT also outperforms TransferNet, which is end-to-end trained with a mixed relation graph, demonstrating the superiority of explicit decomposition. On Musique, RoHT improves F1 by 11.9 compared to the SOTA method EX(SA) when using both text and KB, and RoHT-mix also outperforms TransferNet. The results indicate that supplementing text information with knowledge from KB provides additional benefits.</sample>
    <sample id="196">一个以左侧为支配词的示例是 "I saw Bart and Lisa"。在这个例子中，动词 "saw" 是协调从句 "Bart and Lisa" 的支配词，并且从句 "Bart" 比 "Lisa" 短，这符合观察到的趋势，即当支配词在左侧时，较短的从句更倾向于成为第一个从句。</sample>
    <sample id="197">对话系统中的最先进模型是指在实验中评估的四个模型之一。这些模型是通过ABC-Eval方法进行评估的，该方法能够测量对话模型的行为，如忽略对话伙伴、说 irrelevant、自相矛盾、 hallucinate不正确的事实或违反共同知识、以及成功或失败地展示同理心。</sample>
    <sample id="198">我们需要在整个上下文窗口中评估模型的可接受性，因为大型语言模型的上下文窗口正在变得越来越长。通过在更长的序列上评估模型的可接受性，我们可以更好地了解模型是否能够理解整个上下文，并准确地对句子进行分类。这有助于我们改进模型，使其能够处理更复杂的语言任务。</sample>
    <sample id="199">是的，多语言训练会导致表现下降。在研究中发现，在七个项目中，英语性能下降，在三个项目中略有提高。这种现象被称为“多语言 curse”。</sample>
    <sample id="200">是的，注释者提前知道实体。他们被要求选择两个实体之一，并描述其中一个实体使用三个到五个间接引用表达式。</sample>
    <sample id="201">根据所提供的英文内容，评估使用了神经机器翻译指标和专家基于人类的评估结果。</sample>
    <sample id="202">根据所提供的英文内容，泛化中的回归不会影响特定的NER类型。该研究发现，性能下降的主要原因是由时间漂移引起的，而不是自适应过拟合。</sample>
    <sample id="203">NLP 中的立场很重要，因为它会影响模型和数据集的设计选择和结果。立场是指人们持有的观点，这些观点源于他们的 demographics、身份和生活经历。这些观点会影响研究人员做出的选择，从而影响研究过程及其结果。因此，了解 NLP 中的立场对于确保技术对所有用户公平和包容至关重要。</sample>
    <sample id="204">根据所提供的英文内容，BLOOM等多语言LLM是采用完整微调。</sample>
    <sample id="205">The presentation by Shangbin, a PhD student at the University of Washington, focuses on the political biases in language models and their impact on downstream tasks. The speaker highlights that language models are trained on large-scale web crawl data, which includes diverse perspectives from various news media outlets such as New York Times, Los Angeles Times, The Guardian, and Huffington Post. While this diversity is beneficial for learning, it also introduces potential social biases that can lead to fairness issues in NLP applications.

To address these concerns, the presentation proposes an investigation into the political bias propagation pipeline from pretraining data to language models to downstream tasks. The questions addressed include evaluating the political leaning of language models and understanding how pretraining data influences these biases. The speaker suggests using political questionnaires like the political conference test to evaluate language models' political leanings and conducting controlled experiments with partisan corpora to assess the extent to which language models pick up political biases during training.

Preliminary results indicate that language models exhibit varying political leanings, with GPT-4 being the most liberal and GPT series generally more socially liberal than BART series and its variants. The study also explores whether language models can reflect societal polarization by pretraining them on corpora from different time periods (pre and post the 45th president of the United States). Results show that language models tend to have a political leaning further away from the center after 2017, indicating they can pick up societal polarization.

The presentation evaluates language models with different political leanings on hate speech detection and fake news detection tasks. It reveals that left-leaning language models perform better at detecting hate speech targeting socially minority groups but worse at detecting hate speech targeting more powerful groups. Conversely, right-leaning language models excel at detecting hate speech targeting white and men but struggle with minority communities. Similar trends are observed in fake news detection, where left-leaning models are better at detecting misinformation from their opposite political leaning, while right-leaning models are more effective against misinformation from their own political leaning.

The speaker emphasizes the pressing fairness issue posed by language model political biases, particularly in NLP applications involving hate speech and misinformation detection. The dilemma highlighted is between sanitizing political opinions in training data to prevent bias propagation and risking censorship or exclusion by attempting to remove biased content. The presentation concludes by acknowledging the complexity of addressing political biases in language models and the need for careful consideration of the ethical implications of NLP development.</sample>
    <sample id="206">他们使用了两种迁移学习任务：无主题的争论姿态分类和 PDTB 中的扩展和比较类别。</sample>
    <sample id="207">根据所提供的英文内容，PaLM 的能力最近是通过使用最新的测试集来评估的，以避免测试数据与模型训练数据重叠。这些测试集包括 WMT 评估中的最佳系统，以及神经机器翻译（MT）指标和专家基于人类的评估结果。</sample>
    <sample id="208">作者提出了三条建议。</sample>
    <sample id="209">根据所提供的英文内容，该方法相对于最强的基线获得了显著收益。然而，没有明确提到具体的收益数值，无法提供确切的数字。</sample>
    <sample id="210">演讲者的名字是Shuheng。</sample>
    <sample id="211">是的，论文中的结果和数据集可以作为未来自动文本简化问题的基准。</sample>
    <sample id="212">他们在论文中进行了T5模型的实验。</sample>
    <sample id="213">OF-A模型被用作研究多模型指令调整的基础模型。</sample>
    <sample id="215">The talk by Adam Przepiórkowski focuses on the dependency structure of coordination in different linguistic theories and corpus approaches. The talk highlights that some theories, such as Universal Dependencies and Meaning Text Theory, assume asymmetric structures where the first conjunct is the head of the whole coordinate structure. In contrast, the Prague approach assumes a conjunction-headed structure, while Hudson's Word Grammar uses a multi-headed approach.

The aim of the paper is to argue for symmetric structures of coordination against asymmetric ones. The argument is based on the principle of dependency length minimization, which states that shorter dependencies are preferred. The talk provides examples of how direct objects prefer to be close to the verb, while adjuncts may be further away. However, when the direct object is long, it can be moved after the adjunct, satisfying the principle of dependency length minimization.

The paper extracted various statistics about coordination from the enhanced version of the Penn Treebank and observed that left conjuncts tend to be shorter. This tendency grows with the length difference between the two conjuncts. However, this tendency only occurs when the governor is on the left or absent. When the governor is on the right, the effect disappears.

Overall, the talk provides an argument against asymmetric structures of coordination and for symmetric structures, based on the principle of dependency length minimization and statistical observations from the Penn Treebank.</sample>
    <sample id="217">Weihao Zeng, Lulu Zhao, and Keqing He from Beijing University of Posts and Telecommunications introduced their work on "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation." They addressed the limitations of existing methods that focus on single attributes or combine controllers for specific labels. Their contributions include a Disentangled Controllable Generation (DCG) model that learns attribute concepts from seen values and uses disentanglement loss to disentangle different attribute combinations. They also proposed a unified reference-free evaluation framework, MAE, for evaluating different granularities of attributes. The authors established two benchmarks and demonstrated the effectiveness of their method through experiments. Their models are based on the DialoGPT framework with compositional prompt modules, which use attribute-related information from pre-trained language models to guide dialogue response generation. They designed two types of prompts: attribute-oriented and task-oriented, and concatenated them to create whole prompt embeddings. To enhance diversity, they introduced pseudo combinations and a disentanglement loss. The authors also proposed a unified evaluation framework that does not require additional labeled data, using a template with discrete prompts to control responses. They tested their model with attribute-oriented, task-oriented, and disentanglement learning prompts, showing that attribute-oriented prompts focus on controllable information, task-oriented prompts improve text equality, and disentanglement learning enhances compositional generalization. The results confirm the effectiveness of their method in tackling challenges related to compositional generalization for multi-attribute controllable dialogue generation.</sample>
    <sample id="218">根据所提供的英文内容，这篇论文的作者来自Google Translate。</sample>
    <sample id="219">Jia-Huei Ju, a research assistant at Academia Sinica, presented their work on "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports" along with Yu-Shiang Huang, Cheng-Wei Lin, and their advisors Professors Che Lin and Chuan-Ju Wang. The work focuses on the goal of financial report analysis and introduces a highlighting task and a multi-stage pipeline to mine useful information from Form 10-K reports, which are required by the SEC and contain details of companies' important activities.

The motivation behind this work is based on two observations: 80% of tokens in company reports are similar, and the contents are yearly-dependent. To address these challenges, the authors propose a highlighting model that compares and contrasts the context between target and reference reports. The goal of this highlighting task is to find the rationale (words) of relations between a given pair of reports.

The proposed pipeline consists of four stages: document segmentation, relation recognition, out-of-domain fine-tuning, and in-domain fine-tuning. For stage 1, the pairs are classified into three types: β (highest syntactic and semantic similarities), revised (similar syntactical patterns but different meanings), and mismatched (new operations or debut of information). The model tuning stage uses an external dataset (eSNLI) for out-of-domain fine-tuning and revised pairs for intermediate fine-tuning. The evaluation dataset includes eSNLI pairs and the FINAL dataset, with two metrics used to judge performance: precision and PCC (correlation between prediction and annotations).

The domain-adaptive highlighting model achieved the best performance on the FINAL dataset and preserved generalization capability. The methods also benefited from simulation with mismatched pairs, which were not used during training. Future works include improving effectiveness, adding more features, and enhancing application through techniques in information retrieval.</sample>
    <sample id="220">这篇论文的作者是石溪大学计算机科学系的博士生。</sample>
    <sample id="221">论文分析了德语到英语的语言对。</sample>
    <sample id="222">This work, titled "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering," aims to address the challenges of adapting models to new domains in open-domain question answering (Open-Domain QA). The authors investigate different data interventions that enable out-of-domain generalization, identify the type of dataset shift a new domain exhibits, and determine effective data interventions for specific types of shifts.

The authors use a general-purpose Wikipedia-based source domain to train both the retriever and reader models. They test the generalizability of the source model on seven target passage and datasets spanning six different domains. To generate data interventions, they employ zero-shot and few-shot methods. Few-shot methods involve using a few examples from the target domain to prompt large language models to generate more examples, while zero-shot techniques control the interactions among three random variables in Open-Domain QA by keeping two variables fixed while varying the other one.

The authors observe that few-shot methods improve retriever performance by 8% on average and reader performance by 11% on average. They also explore how changes in question format, answer distribution, and context distribution affect model performance. They find that cloze-style questions are easier to curate than standard WH questions, and uniform distributions of answers work best. They also discover that BM25 has the best overall performance when passages from both the source and target domains are used.

To ascertain the nature of incompatibility between the target model and domain, the authors consider existing data shift taxonomy in machine learning. They compute compatibility measures for the source model with respect to the target dataset and map different target datasets onto a 2D grid to estimate the type of dataset shift. They find that few-shot adaptations are most useful for datasets with full shift, while zero-shot adaptations are effective for datasets with concept and covariate shifts. For no-shift datasets, little change in performance is observed because the source model already understands the target domain well.

In conclusion, the authors experiment with various data interventions and improve reader performance by up to 24%. They demonstrate that only certain types of data interventions are effective based on the type of shift a target dataset exhibits.</sample>
    <sample id="223">演讲者的名字是Shangbin。</sample>
    <sample id="224">在实验过程中研究了两个模型：Long-mBART和mBART。Long-mBART被微调以生成文档级别的简化文本，而mBART被微调以生成句子级别的简化文本。这些结果作为未来自动文本简化问题的基准。</sample>
    <sample id="225">在 MultiInstruct 中使用的 62 个不同任务中，有 53 个任务用于训练目的，10 个任务用于测试目的。</sample>
    <sample id="226">根据所给的英文内容，这篇论文有两位作者：Regina Stodden和Omar。</sample>
    <sample id="227">Grounded language understanding is a crucial aspect of natural language processing (NLP) that involves mapping a natural language expression into something that can be executed over a specific target environment, such as a plan or program. This task is challenging due to the lack of grounding during pre-training in most language models. Existing research typically uses language models to directly generate plans via autoregressive decoding, which may not always produce grammatically correct or valid plans.

To address this challenge, the proposed framework named Pangu focuses on discrimination instead of generation. In this framework, a symbolic agent interacts with the environment and proposes candidate plans, while a language model is used only to score and rank the candidates proposed by the symbolic agent. This approach allows the language model to focus on discrimination, which is easier for it to excel at, without having to handle the validity and grammar of the target plan.

Pangu has been tested on knowledge-based question answering, a typical scenario with a massive, heterogeneous environment for grounded language understanding. The framework has demonstrated outstanding performance across different settings, including fine-tuning and in-context learning, and has shown strong sample efficiency. Additionally, Pangu's strong generalizability under non-i.i.d settings may be attributed to its ability to handle both seen and unseen structures effectively.</sample>
    <sample id="228">作者在实验中使用了AG News、MIND、SST2和Enron Spam四个数据集。</sample>
    <sample id="229">The paper by Gabriella Skitalinskaya and Henning Wachsmuth focuses on detecting improvable claims for argumentative writing support. The authors explore the challenges of working with revision-based data, such as representativity and reliability, model complexity and architecture, contextual information, and topical and user bias. They propose a systematic comparison of approaches for the introduced tasks and conclude that revision-based data can be employed effectively for the given tasks. Modeling the distance between two claimed versions is beneficial for detecting suboptimal claims. The impact of contextual information is dependent on both the task and the quality issues a text is suffering from.</sample>
    <sample id="231">NACHOS 是一个数据集，用于训练 DrBERT 模型。它包含从互联网上爬取的医学数据。</sample>
    <sample id="232">演讲者的名字是David Vilar。</sample>
    <sample id="233">Simultaneous speech translation (SimulST) is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. However, current SimulST models have several problems, including the need for specific architectures and additional modules to be optimized, long training procedures involving different optimization objectives, and the need to train and maintain several models to reach different latency regimes.

To address these issues, the proposed solution is EDAtt or Encoder-Decoder Attention, which uses already existing offline ST models without re-training or adopting specific architecture for SimulST. EDAtt handles latency through specific parameters and leverages the knowledge already acquired by the model through the attention mechanism between audio input and textual output. The strategy decides whether to emit or not a partial translation based on where attention points to, emitting a word if the attention is not concentrated and not emitting it if the sum of the cross-attention is below a certain threshold alpha towards the last lambda speech frames.

The main results of EDAtt show that it outperforms popular strategies applied to offline models since the curves are shifted over the left and is the fastest strategy when considering actual elapsed time or computational-aware time. The paper also includes graphs measuring translation quality and latency, as well as open source code and models to facilitate reproducibility of the work.</sample>
    <sample id="234">提示策略对结果有重大影响。在实验中，使用不同的提示策略，即一、两和五种提示，观察到的差异大于一个BLEURT点。在极端情况下，差异可达到40个BLEURT点。因此，选择好的提示策略对于模型性能至关重要。</sample>
    <sample id="235">根据所提供的英文内容，无法确定论文作者所属的机构。</sample>
    <sample id="236">每个任务都配备了五个专家编写的指令。</sample>
    <sample id="237">作者建议通过使用一个核心参考任务来测试模型，该任务旨在探测模型在不同来源中提取知识的能力。他们定义了三个设置：背景-预训练、背景-Both和背景-推理，以评估模型在不同来源中整合信息的能力。</sample>
    <sample id="238">In this video, Yebowen Hu from the University of Central Florida introduces a new benchmark dataset called MeetingBank. The dataset was created to address the challenges of developing summarization technologies for different reading domains, particularly in meetings where key points need to be noted down. To create this dataset, two major challenges were addressed: obtaining high-quality meeting summaries or scores and locating trustworthy resources for public meetings.

The dataset includes 1,366 City Council meetings with nearly 7,000 instances, including meeting transcripts, reference summaries, and other URLs containing useful resources. The data collection process involves using Speechmatics API to convert audio data to transcripts, identifying the type and data of the meeting from the information provided below the video, and aligning timestamps to get the second transcript paired with the extracted summary.

The dataset statistics include the number of meetings, meeting duration, number of tokens per meeting, number of speakers per meeting for each city, and the year period of the meetings collected. The average statistics for both meeting-level and secondary levels are posted across all cities. The level of abstraction in meeting summaries is measured by two common measures: coverage and density. The coverage score matches the percentage of some summary words that appear in source transcripts, while the density score evaluates how much a summary can be characterized as a set of extracted references.

For model evaluation, top-tier summarization systems were evaluated on the test set of MeetingBank. The extractive summarizers included Oracle, LEAD, LexRank, and TextRank, while the abstractive models included BART-Large, Pagasus, Longformer, DialogLM, and HMNet. GPT-3 was also evaluated using zero-shot summarization prompting with Davinci-003 on the test set. Human evaluation was conducted based on five criteria: informativeness, factuality, fluency, coherence, and redundancy. The results showed that GPT-3 achieved the highest overall scores in terms of fluency and coherence but performed less impressively in terms of informativeness and factuality.

The video concludes by emphasizing the importance of MeetingBank as a benchmark dataset for researchers to design advanced meeting summarizers and provides insights into the decision-making process of City Council. The speaker encourages viewers to download and use the resource.</sample>
    <sample id="239">大家好，我叫David Vilar，我将对论文《Prompting PaLM for Translation: Assessing Strategies and Performance》进行简要回顾。这篇论文是与Google Translate团队合作完成的。PaLM是一个540亿参数的大语言模型，于2022年推出。它是在包含7800亿个token的大量文本上进行训练的。在发表时，它在 hundreds of NLP任务中取得了state-of-the-art的表现。在这项工作中，我们进行了首次系统研究大语言模型的Prompting技术在机器翻译中的应用。我们使用了MT社区的最佳实践，使用最新的测试集以避免测试数据与语言模型的训练数据重叠，并将其与最佳性能系统（WMT评估）进行了比较。我们使用了神经机器翻译的最新指标，并额外展示了专家的人类评估结果。最后，我们提供了一些关于Prompt选择策略的建议。Prompting对LLM的翻译性能有重大影响，如我们在一个简单实验中所见，其中我们为每个句子提供了两种不同的Prompt，结果观察到的差异超过一个BLEURT分数，极端情况下可达40个BLEURT分数。因此，选择好的Prompting策略非常重要。在我们的实验中，我们采用了5-shot Prompting策略，即为每个句子标记其语言，例如，在从德语翻译成英语的例子中，德语句子用德语冒号标记，英语翻译用英语冒号标记。我们发现，对于几种短Prompting，实际Prompt的形式对性能影响不大，但在零和一shot Prompting中至关重要。而在我们的情况下，五shot Prompting几乎没有区别于实际Prompt的形式，关键在于示例的质量。实验结果总结如下：示例质量比源句子的相似性更重要。因此，选择高质量的示例至关重要。我们比较了从训练数据和WMT评估的开发数据中选择Prompt，开发数据比训练数据更精致，质量更高。结果表明，使用开发数据可以取得更好的性能。然而，专门化的最佳系统仍然在PaLM翻译上占据优势。但在我们的案例中，我们选择使用Google Translate进行评估。通过MQM框架进行的人类评估表明，PaLM的流畅度与最佳系统相当，但主要差异来自准确性。特别是最常见的错误是省略错误。似乎PaLM倾向于选择产生更流畅的翻译，有时会省略源句子中的一些部分。然而，“风格/笨拙”类别中PaLM的评分低于最佳系统，这是另一个信号，表明PaLM确实提供了流畅的输出，但仍然存在一些准确性问题。这就是本文的简要概述。如需更多详细信息，请参阅论文的完整演示。谢谢大家。</sample>
    <sample id="240">大家好，我是 Dawei，在德国萨尔兰大学攻读博士学位。在这段视频中，我将介绍我们最近的工作“比你想的更弱：对弱监督学习的批判性审视”。这项工作与 Xiaoyu Shen、Marius Mosbach、Andreas Stephan 和 Dietrich Klakow 合作完成。我将从弱监督和弱监督学习的简要介绍开始。在弱监督中，我们不手动标注数据。相反，我们使用弱标注来源，如简单的启发式规则、知识库或低质量 crowdsourcing 进行标注，如图所示。与人工标注相比，这些标注更加便宜，但它们也存在噪声，意味着其中一部分标注是错误的。如果直接用带有噪声标注训练神经网络，神经网络倾向于记住标注噪声，而不能泛化。在弱监督学习中，提出了训练算法来在标注噪声下 robust 训练神经网络，使训练模型仍然具有泛化能力。近年来，在 WSL（Weakly Supervised Learning）领域中，一个常见说法是，人们说他们只在弱标注数据上训练模型，并在干净测试集上取得高精度。从技术上讲，这个说法并不错，但有一个问题：人们假设有一个额外的干净验证集用于模型选择。我们不能停止在这个问题设置上，但这意味着在 WSL 中需要额外的手工标注。这个问题就像房间里显而易见的大象一样经常被忽视。这个问题提出了三个研究问题：首先，干净验证数据是否对于 WSL 是必要的，或者我们可以使用噪声验证集吗？其次，如果干净数据是必要的，或者干净数据是 WSL 的必要条件，那么我们需要多少干净样本？最后，我们应该只使用干净样本进行验证，还是有其他更好的方法利用它们？我们在工作中解决了这些问题，我们的 findings 如下：首先，我们发现，有趣的是，最近的 WSL 方法确实需要干净验证样本才能正常工作。否则，性能会有很大下降。如图所示，如果没有干净验证样本，训练模型无法泛化超出原始弱标注，这意味着训练是无意义的。这表明 WSL 方法实际上需要干净标注数据才能正常工作，获取干净标注数据的成本不应被忽视。我们的第二个发现是，增加干净验证样本的数量有助于 WSL 方法获得更好的性能，如图所示。通常，每类只需要 20 个样本即可获得高精度。但这不是故事的结束，如果我们决定获取干净样本，直接在干净数据上进行微调甚至会获得更好的性能。右边的图表显示了直接微调方法（即直接应用于干净数据的方法）与 WSL 方法之间的性能差异。我们可以看到，如果每类有 10 个样本，直接微调已经开始超越 WSL 方法。最后，之前 WSL 方法声称的性能提升可以通过允许继续在干净样本上微调来轻松实现。如图表所示，初始表现不佳的“纯模型”（FTw）在继续在干净样本上微调后，与 COSINE 等更复杂的 WSL 方法表现相当。因此，在实践中，没有理由选择更复杂的 WSL 方法，这些方法需要更多计算时间和磁盘空间。总之，我们展示了最近的 WSL 方法需要干净、手工标注样本才能正常工作。它们的性能提升和实用性被高估了。我们对未来的建议如下：首先，报告模型选择标准。例如，报告模型选择是否通过干净验证样本进行。第二，WSL 方法应该与少样本学习基线进行比较，因为两者都基于干净样本。第三，连续微调是一个简单而强大的基线，应该在未来的 WSL 研究中考虑。最后，我们开源了代码。您可以通过幻灯片上的 QR 码找到它。请随意查看。谢谢，祝您会议愉快。</sample>
    <sample id="241">In this paper, Ethan and his team discuss the evaluation of early misinformation detection systems. They argue that current methods are unrealistic and not human-centric, as they rely on retrospectively constructed datasets and do not involve humans in the process. To address these deficiencies, they propose a framework for developing end-to-end systems that incorporate human feedback at various stages.

The proposed system has two main components: claim detection and policy violation verification. The first component uses keyword filtering and a T5 model to extract check-worthy claims from raw tweets, which are then ranked by trendiness before being verified by humans. The second component uses a BERT-based stance classification model to determine the author's stance towards unapproved treatments, flagging supporting stance tweets for human review.

The paper evaluates the system's efficacy in detecting unapproved treatments before their appearance in debunking news articles and in policy violation verification. Humans assign Likert scale values to determine if a tweet violates Twitter's policies surrounding COVID-19 misinformation. The results show that the system has a high accuracy rate in policy violation detection and can detect 124.2 policy violations per human hour worked.

Overall, the paper highlights the importance of involving humans in the misinformation detection process and proposes a framework for developing more realistic and effective systems.</sample>
    <sample id="242">对话系统的常用评估方法是通过让人类裁判选择两个对话中哪个更好或使用 Likert 梯度来评估对话的整体质量。</sample>
    <sample id="243">根据所给的英文内容，这篇论文有5位作者。</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，需要的背景知识是“Servin 是一名法官”和“Kea 是一名面包师”。</sample>
    <sample id="245">The presentation by Lining Zhang and co-authors focuses on their work titled "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization." The presentation outlines a two-step pipeline for identifying high-agreement Amazon Mechanical Turk (MTurk) workers, addressing the limitations of automatic metrics and poorly understood best practices for recruitment. The pipeline includes qualification settings, qualification tasks, and endurance tasks to categorize workers into gold, silver, bronze, and block based on their ability to evaluate multiple dimensions correctly and handle heavy workloads. The presentation also discusses the reference-based task, baseline MTurk workers, and the analysis of correctness across annotation sources. The results show that the pipeline can achieve high agreement at a lower cost compared to CloudResearch MTurk workers. However, the study is limited to English summarization on the MTurk platform and does not guarantee the training of correctness. The presentation concludes with future directions for hiring high-quality workers and exploring multiple applications for tasks, languages, and platforms.</sample>
    <sample id="246">是的，代码公开。可以在GitHub上获取。</sample>
    <sample id="247">The paper titled "FACTKG: Fact Verification via Reasoning on Knowledge Graphs" by Jiho Kim from KAIST AI proposes a new task called Knowledge Graph-Based Fact Verification. This task involves using knowledge graphs as evidence to verify natural language claims, which is different from existing fact verification datasets that use Wikipedia text or tables as evidence.

The knowledge graph used in the paper is DBpedia, and claims exist in two styles - written and colloquial. The task consists of retrieving evidence from DBpedia and verifying the claim using the evidence. There are five types of reasoning involved: one-hop, conjunction, existence, multi-hop, and negation.

The dataset includes claims in both written and colloquial styles for practical use. Two methods were used for this: the colloquial style transfer model proposed by Kim et al., and presupposition templates created and used.

The paper also constructed some baselines in two ways - Claim Only baselines use only the claims to verify without graph evidence, and the GEAR model that uses graph evidence outperforms all other baselines.

Overall, the paper proposes a new task of Knowledge Graph-Based Fact Verification, which can be used practically in tasks that require consistency checks between KG and natural language, such as modern dialogue systems communicating with internal knowledge graphs.</sample>
    <sample id="248">根据 Jenny 的演讲内容，NLPositionality 的注释者在人口统计学特征方面并不均衡。例如，在 GPT-4 社交接受性分析中，数据集和模型最接近于英语国家/地区。此外，在 Dynahate 分析中，GPT-4 在社交接受性任务中与拥有大学或更高教育程度的人最接近。然而，在 Dynahate 任务的分析中，数据集和模型与非二元性别的人相比，与男性和女性的对齐度较低。</sample>
    <sample id="249">在可接受的域中扰乱句子的方法是通过保持相关结构同时向输入添加噪声。这有助于分析模型对结构的敏感性，而不是对噪声的敏感性。</sample>
    <sample id="250">维度评估意味着通过明确标注模型响应的行为，减少人类评估的主观性。这使我们能够更精确地测量对话模型在各种主题上的行为，如相关性、一致性、事实性和同理心。</sample>
    <sample id="251">这篇论文的作者 Jingwei Yi 所属机构是University of Science and Technology of China。</sample>
    <sample id="252">The presentation introduces a research work titled "U-CREAT: Unsupervised Case Retrieval using Events extrAcT" by Sai Kiran Tanikella, Abhinav Joshi, Akshat Sharma, and Ashutosh Modi. The work focuses on the Prior Case Retrieval (PCR) task in the legal domain, where legal professionals traditionally rely on their experience to cite relevant past precedents. However, with the increasing volume of cases, this task becomes challenging, making PCR a crucial area of research.

The presentation highlights two key contributions: the IL-PCR dataset and the U-CREAT pipeline. The IL-PCR Dataset is a new benchmark for PCR tasks, containing 7,070 legal cases with an average of 6.775 citations per query document. This dataset provides a comprehensive test bed for evaluating PCR algorithms. The U-CREAT pipeline leverages unsupervised learning techniques and introduces an event-based approach for PCR tasks, demonstrating high retrieval efficiency, low inference time, and generalization across Indian and Canadian legal systems without requiring law or demographic-specific tuning.

The U-CREAT pipeline involves event extraction from case documents using dependency parsing and entity tagging. Each event is represented as a subject-verb-object triplet, which is then used to compute an interaction matrix between query and candidate events. This matrix is used in different retrieval models to obtain a ranking order of the candidates.

Experiments were conducted using various models, including count-based models, transformer-based models, and event-based models. The results showed that event-based models significantly outperform baseline methods, with the Event Filtered Documents model achieving the best performance. The U-CREAT approach also outperforms existing approaches, including the recent supervised approach by the MTFT-BERT team, making it the current state-of-the-art method for the COLIEE'21 document retrieval task.

Overall, the presentation emphasizes the importance of developing tailored approaches for the complexities and nuances of the legal domain and highlights the potential of event-based models in improving PCR performance.</sample>
    <sample id="253">Mario Ezra Aragón is presenting a work titled "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media." This collaborative research involves researchers from Mexico and Spain. The work aims to contribute to the detection of mental health disorders by automatically analyzing social media posts.

Mental disorders are defined as psychological syndromes associated with distress and disability, affecting thinking, feeling, mood, and behavior. Examples include major depression, PTSD, bulimia, and anorexia. Social media content provides an opportunity to study how people undergo difficulties, with many users sharing their daily routines and important events or discussing mental health issues anonymously.

The proposed approach uses domain adaptation to improve the performance of a model on a target domain when there is insufficient annotated data. BERT, a language model trained on general data like Wikipedia and Google Books, can be adapted to more specific languages like Reddit and mental health. The approach involves integrating information from Reddit and mental health into the base language model and incorporating knowledge from a lexicon to guide the masking process.

The main idea is to first learn the social media language and then specialize in the mental disorder domain. Guided masking helps the model focus on important words during training. The results using the eRisk datasets show that DisorBERT tends to locate in the main diagonal of the region, indicating a good balance between precision and recall, while other methods have high precision but low recall.

An example from Beck's Depression Inventory (BDI) illustrates the behavior of the learned model. When given a sentence with masked words, DisorBERT predicts words related to mental disorders, such as "cry," "focus," "talk," "breath," "sleep," and "eat," which are common problems associated with mental disorders. BERT tends to generate more general words.

Visualization tools provide an interactive head view in the form of a graph, showing the most prominent words and sentences in a user's post. For a depression user with the highest BDI score, the most relevant words are "anxious" and "medication."

In conclusion, the combined effect of double domain adaptation and guided masking effectively captures signs of mental disorders in social media interactions. DisorBERT obtained better results than MentalBERT, a model trained with a large amount of data. Future work will explore the application of different lexical resources and clinical data.</sample>
    <sample id="254">The research work presented today is titled "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction" by Sun Qi from Nanjing University of Science and Technology. The primary focus of this work is on document-level relation extraction, which involves identifying relationships between entities within a document. Traditional methods rely heavily on large-scale human-annotated datasets, which are time-consuming and labor-intensive. Recent advancements have utilized distantly supervised data to pre-train models for improved performance.

However, these distantly supervised datasets often contain various noise levels, leading to the challenge of false-positive pseudo labels. This can result in incorrect relations being identified, as illustrated in the provided figure. To address this issue, the researchers propose a framework that incorporates uncertainty-guided label denoising to enhance the quality of the DS data.

The framework begins by training a pre-denoising DocRE model using both DS and human-annotated data to generate pseudo labels. An instance-level uncertainty estimation method is then introduced to determine the reliability of each model prediction. This helps in distinguishing between correct and false positive pseudo labels. A re-labeling strategy with dynamic class uncertainty thresholds is also implemented to filter out pseudo labels with high uncertainty scores.

To further improve performance, the researchers design a multi-phase training strategy that iteratively re-labels the DS data. This approach leverages the strengths of DS data while mitigating the impact of noise. The proposed framework has been compared with several strong baselines on public datasets, demonstrating superior performance.

In summary, the main contributions of this work include the development of a framework with uncertainty-guided label denoising, an instance-level uncertainty estimation method for overlapping relations, an iterative re-labeling strategy with dynamic class uncertainty thresholds, and significant performance improvements over previous methods.</sample>
    <sample id="255">提示的形式在零和一击提示的情况下很重要。</sample>
    <sample id="257">作者评估了四个最新的对话模型。</sample>
    <sample id="258">In this video, Chiang Cheng-Han introduces a new work titled "Can Large Language Models Be an Alternative to Human Evaluation?" The work proposes using large language models to evaluate the quality of text in natural language processing. The approach involves instructing the models with specific instructions and providing them with samples to rate. The goal is to determine if these models can understand the instructions and provide meaningful ratings.

The video highlights that while there are related works on using large language models for evaluation, this idea was novel at the time of submission to ACL. The motivation behind this work is to find an alternative to human evaluation, which is unstable and hard to reproduce. Large language models, due to their ability to follow natural language text instructions, are considered a potential solution.

The experiment conducted involves using large language model evaluation to rate stories generated by GPT-2 or written by humans based on four attributes: grammar, coherence, likability, and relevance. Human evaluators, specifically English teachers, were hired to provide ground-truth ratings for comparison. The results showed that human raters preferred human-written stories over GPT-2-written ones, but smaller language models did not show a clear preference. However, larger models like Davinci and ChatGPT demonstrated a clear preference for human-written texts, similar to human evaluators.

The video also mentions that further questions about the agreement between large language models and human evaluators, the impact of instruction wording changes, sampling methods, benefits and costs of using large language models compared to human evaluation, and results on other tasks are addressed in the paper. The video concludes by inviting viewers to read the paper or visit the poster stand at ACL.</sample>
    <sample id="259">The speaker, Yusen Zhang from Penn State University, presents a study on Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations. The task involves translating queries in multiple natural languages into various meaning representations such as SQL, Lambda Calculus, and FunQL. Existing models have limited coverage in certain natural languages and meaning representations, leading to the proposal of XSemPLR, which provides a uniform dataset for cross-lingual semantic parsing across 22 natural languages, 5 semantic parsing tasks, and 8 meaning representations. The study evaluates six settings: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer. The results show that Encoder-Decoder models outperform previous work or achieve comparable results, and pretraining on English can significantly boost performance in few-shot settings. However, multilingual language models like Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks. The study concludes with the development of XSemPLR, a unified benchmark for cross-lingual semantic parsing, and encourages further research in this area.</sample>
    <sample id="260">根据所提供的英文内容，无法确定论文的作者人数。</sample>
    <sample id="261">根据所提供的英文内容，优秀规划器的理想品质是编写合理且符合约束的脚本。</sample>
    <sample id="262">根据所提供的英文内容，无法确定论文的作者人数。</sample>
    <sample id="263">The paper presents a study on mitigating label biases in in-context learning, a popular paradigm for utilizing large language models. The authors identify three types of label biases: vanilla-label bias, context-label bias, and domain-label bias. They propose a novel calibration method called domain-context calibration to handle all types of biases. The method uses random in-domain words sampled from the task corpus as content-free text to estimate the model's bias on each label name and then calibrates the model's original predictions using this estimated bias. The authors conduct experiments on various datasets and models, demonstrating that domain-context calibration significantly improves the performance of in-context learning, particularly on tasks with large domain-label bias. The study provides a systematic investigation of label bias problems in in-context learning and proposes a practical solution to mitigate these biases.</sample>
    <sample id="264">Lin Wang, a graduate student at Zhejiang University in China, presented a paper titled "TAVT: Towards Transferable Audio-Visual Text Generation." The paper addresses the challenge of multimodal text generation tasks like audio-visual text generation, which suffer from severe degradation due to varying construction conditions in different domains. To overcome this constraint, the authors propose a novel task named Transferable Audio-Visual Text Generation.

The main challenge of this task is the multi-modal domain shifts, such as visual style and audio energy. The authors notice that in the content understanding for the same event, the visual content would shift significantly with the change of image style and shooting angle. But the change of audio content such as rhythm and energy hardly affects the understanding of the events. Based on these phenomena, they posit that a unified audio semantic space can be used to align visual concepts across domains.

To address this challenge, the authors present a modular framework consisting of three components: an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning. The first model maps different visual concepts across domains into a unified auditory semantic space and addresses shifts in the semantic distribution. The second model uses the transformer-based encoder and generator, introducing an alpha to evaluate the contribution of different modalities to each word. Finally, the authors introduce a Dual Counterfactual Contrastive Learning (DCLL) which constructs fine-grained supervision signals from counterfactual results to directly optimize the visual-textual alignment without relying on the quality of the randomly-selected negative samples.

In the experimental section, the authors build two benchmarks based on MSVD and MSR-VTT, including cross-datasets and cross-domain settings. They compare their method with the SOTA approach, including the RNN-based and the transformer-based models. As it can be observed, their methods outperformed all compared models on all metrics by a large margin on both cross-datasets and cross-domain settings. For some low-resource domains with only a few labeled data, such as "Kids" and "Beauty", other methods suffer from severe performance degradation, while TAVT still performs well. Additionally, they conducted ablation experiments to analyze the impact of audio features on expanded performance.</sample>
    <sample id="265">演讲者的名字是Vasudha。</sample>
    <sample id="266">根据所提供的英文内容，无法确定论文作者所属的机构。</sample>
    <sample id="268">PaLM 最常见的错误是省略错误。</sample>
    <sample id="269">你好，我是James Finch。我是Sarah Finch。今天我们将向您介绍ABC-Eval，这是一种新的维度方法，用于评估对话AI。这项工作是由Emory NLP实验室的Jinho Choi教授领导的，并与Amazon Alexa AI合作完成的。假设您刚刚开发了一个对话模型，并想看看它与当前最先进的技术相比如何。常见的做法是使用人类评估，例如，请人类评委选择两个对话中哪个更好，或者根据 Likert量表对对话进行评级。这些方法在提供整体对话质量的整体评估方面效果很好，但对话质量有许多方面。因此，您可能需要评估对话质量的多个维度，以了解模型的优点和缺点。一种方法是请人类评委评估对话质量的多个维度，例如，通过使用现有的比较或Likert量表来评估模型响应的相关性。但我们认为，有一个更精确可靠的策略来进行维度对话评估。我们的方法旨在通过明确标注每个模型响应是否表现出某些行为，从而减少人类评估的主观性，例如，回应无关的信息或自相矛盾。我们称这种方法为在聊天中标注行为或ABC-Eval。我们开发了这种方法，以全面覆盖最近文献中建议会影响对话质量的各种行为。ABC-Eval能够衡量聊天模型犯各种主题错误的概率。例如，ABC-Eval衡量了聊天模型在多轮对话中忽略其伙伴或说无关、自相矛盾或违反其伙伴、 hallucinate 错误事实或违反共同知识的次数，以及模型是否成功或失败地展示同理心。为了确定哪种评估方法最有效，我们选择了四个先进的聊天模型，并使用ABC-Eval评估了每种模型的100个人机对话。为了进行比较，我们还使用三种现有方法评估了这些对话：按轮次的Likert评级、按对话级别的Likert评级和对话级别的两两比较。对于每种现有方法，我们收集了八个最常见的对话方面的评估，因为这是评估聊天模型沿多个维度的标准做法。从我们对这些评估结果的分析中，我们发现ABC-Eval的行为标签总体上比现有方法收集的标签更可靠，如100个双重标注对话的注释者一致性所证明的那样。此外，ABC-Eval标签比现有方法产生的指标更预测对话质量，如简单的线性回归分析所示。例如，你可以看到测量轮次中自我和伙伴矛盾的比例解释了5%和10%的对话质量，而平均Likert一致性分数只解释了4%或更少。最后，我们检查了每个评估指标是否捕捉了对话质量的独特方面，使用逐步线性回归。你可以看到所有ABC-Eval指标的组合解释了超过25%的对话质量，随着一个一个地移除这些指标，大多数指标都会失去相当一部分关于质量的信息。相比之下，所有轮次Likert指标的组合解释了较少的质量， fewer 的指标携带独特信息。这些可靠的、信息丰富且独特的ABC-Eval指标使我们能够以比以往任何方法都能实现的更高分辨率评估对话AI。你可以在实验结果中看到，我们测试的机器人存在一些挑战，这些挑战已经被精确量化。例如，我们测试的机器人在其响应中存在大约20%的概念错误。它们产生无关信息大约15%，它们自相矛盾或与其伙伴大约10%的时间。随着该领域的快速发展，许多新模型发布时的错误率可能会降低。但这正是我们需要可靠和精确的评估指标来比较模型的原因。我们希望ABC-Eval可以被其他领域的其他人利用，作为朝着这个方向的一个有意义的步骤。我们期待看到对话AI将在未来几个月和几年内如何 advancement。谢谢观看。</sample>
    <sample id="270">该论文的作者是来自Emory University的Emory NLP Lab，与Amazon Alexa AI合作完成。</sample>
    <sample id="271">在本文中，CFT 代表 "Clean Fine-Tuning"。</sample>
    <sample id="272">根据所提供的英文内容，这篇论文有7位作者。</sample>
    <sample id="273">Hello, my name is Kayo Yin and I will be presenting our work titled "When Does Translation Require Context? A Data-driven, Multilingual Exploration". This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate "mole" in this sentence? Well, if the previous sentence was "Things could start to get dangerous if the ministers find out", then "mole" refers to a spy. But if the previous sentence was "Could it be anything serious, doctor?", then "mole" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that's rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we called our tagger the Multilingual Discourse-Aware, or MuDA tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation. First of all, when we use corpus-level metrics: so for BLEU, we find that context-agnostic models have the best performance. But then if we use COMET, context-aware models perform best. And if we use word f-measure, then models with and without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now, we use the MuDA benchmark to evaluate models and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.</sample>
    <sample id="274">演讲者的名字是Yusen Zhang。</sample>
    <sample id="276">The research presented by Ananya and Vignesh focuses on developing a dataset, IndicMT Eval, to evaluate machine translation metrics for Indian languages. The study highlights the lack of evaluation metrics for translations in the Indian languages direction, which is crucial due to the unique grammar rules, vocabulary, and sentence structures of these languages.

The researchers selected five Indian languages: Tamil and Malayalam from the Dravidian language family, and Hindi, Marathi, and Gujarati from the Indo-Aryan language family. They used the Flores dataset to randomly select 200 sentences and generated multiple candidate translations for each source sentence using seven different translation models or APIs, resulting in a total of 1,400 candidate translations per language and 7,000 samples overall.

Bilingual expert annotators were employed to evaluate the translated outputs, marking errors with their types and severity, and providing an overall score for each output. The MQM framework was used to classify error types into accuracy, fluency, and special category errors. The results showed that newer models like NLLB and Indic Trans had fewer errors compared to older models like CVIT.

The study also analyzed the correlation between various machine translation metrics and human scores, with COMET-metric variants showing the highest overall correlations for all languages. However, many metrics exhibited a skewed range of scores, making it challenging to interpret metric scores effectively. The researchers fine-tuned the best-performing metric, COMET, using their MQM dataset, resulting in higher correlations than COMET baselines on three out of five languages.

The study concludes by emphasizing the importance of developing evaluation metrics for Indian languages and the potential applications of the publicly available IndicMT Eval dataset.</sample>
    <sample id="277">该方法没有名称。</sample>
    <sample id="278">作者描述“显性词汇”(marked words) 方法是通过比较标记群体和未标记群体的词汇，来识别区分标记群体和未标记群体的词汇。这种方法基于社会语言学概念“标记性”，即默认群体是未标记的，而与默认群体不同的群体则被标记。通过使用加权对数比值来区分每个标记群体的top词汇，作者能够识别出这些词汇反映有害模式的方式。</sample>
    <sample id="279">这篇论文的作者是来自University of Washington的Shangbin。</sample>
    <sample id="280">Shi Tao is presenting their work titled "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations." The presentation begins by introducing the task of emotion regulation in conversations, where the goal is to predict the emotion label of each utterance in a dialogue. Each utterance has its corresponding textual, audio, and visual modalities. The presenter highlights that most existing methods focus on modeling speaker and contextual information but face challenges such as underutilization of multimodal information, unsatisfactory performance in minority emotion classes, and difficulty distinguishing between semantically similar emotions.

To address these issues, the MultiEMO framework is proposed, consisting of four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The main contributions include:

1. VisExtNet: A novel visual feature extractor that captures visual cues from interlocutors' facial expressions without encoding redundant scene-related information.
2. MultiAttn: A multimodal fusion model based on bidirectional multi-head cross-attention layers to integrate complementary information from different modalities.
3. Sample-Weighted Focal Contrast loss: A method to assign higher importance to hard-to-classify minority classes and maximize inter-class distances for better distinction between semantically similar emotions.

The framework achieves state-of-the-art performances on MELD and IEMOCAP datasets, with significant improvements in minority and semantically similar emotions. However, some limitations are mentioned, including VisExtNet's inability to distinguish between speakers and irrelevant people, the need for a large batch size for SWFC loss on MELD, and still weaker performance in minority emotions compared to majority classes.</sample>
    <sample id="281">The presentation by Kayo Yin and her team explores the role of context in translation, specifically focusing on how machine translation models handle context-dependent translations. They introduce Pointwise Contextual Mutual Information (P-CXMI) as a measure to determine when a word requires context for accurate translation. The study is conducted on TED talk transcripts translated into 14 languages, analyzing patterns at part-of-speech tags, vocabulary items, and individual tokens.

The team identifies five discourse phenomena that require context: dual pronouns, verb forms, proper noun translations, formality, and ellipses resolution. They create the Multilingual Discourse-Aware (MuDA) tagger to automatically identify these phenomena in parallel corpora. Evaluating models using MuDA and other metrics shows that context-aware models outperform context-agnostic ones for certain phenomena like formality and lexical cohesion but not for others like ellipses, pronouns, and verb forms.

The study concludes that while context-aware models are more accurate for specific phenomena, there is still room for improvement in handling other context-dependent aspects. It also highlights the importance of using discourse-aware benchmarks for evaluating document-level machine translation systems, as corpus-level metrics alone do not capture the nuances of context-dependent translations effectively.</sample>
    <sample id="282">The paper presents a new approach to non-parallel text style transfer at the discourse level, which is crucial for imitating author style. The main challenge lies in imitating the author's linguistic choices at the discourse level, as long text usually involves many complicated author linguistic preferences such as discourse structures. The proposed generation model named StoryTrans learns discourse representations from the source texts and combines this with learnable style embeddings to generate texts in the target styles. A new training objective is designed to reduce the stylistic features from the discourse representations, pulling the representations derived from different texts closer in the latent space, and enhance the content preservation by separating the generation into two stages. The first stage transfers the source text with the style-specific content keywords masked, and then generates the whole text by incorporating these keywords explicitly. The second stage fills the correct style-specific contents and removes the mask token. The evaluation results confirm the efficiency of the model and show that StoryTrans outperforms strong baselines in terms of style control and content preservation.</sample>
    <sample id="283">第一个提到的对称依存关系结构的名称是“Lisa, Bart, and Maggie”结构。</sample>
    <sample id="284">The paper presents a novel fuzzy span mechanism for enhancing universal information extraction (UIE). The current span-based UIE models rely on precise boundary positions of annotated spans, which can be ambiguous. The proposed fuzzy span mechanism models the target boundary as a continuous distribution of correct probability in a specific range, where R-min and R-max represent the start and end of the fuzzy boundary. The function Q represents the correctness of the current position. The fuzzy span attention is introduced as a mask function to trim attention distribution. The overall structure of the module is presented on the slide, where the fuzzy span attention layer is only added on the top level to guide the model's decision process without affecting the text encoding capability. The paper demonstrates the capability of FSUIE by conducting experiments on three main information extraction tasks, including named entity recognition, relationship extraction, and aspect sentiment triplet extraction. The results show that FSUIE achieves significant performance improvement compared to UIE-base without a fuzzy span mechanism. FSUIE uses one unified structure to extract relationship elements, achieving better information extraction ability with simple structure. Besides, FSUIE shows stronger generalization capabilities for domain-specific information. The results of the ablation study show that FSA improves convergence speed by guiding the module to obtain a reasonable attention distribution. FSL enables the module to fully utilize annotation information and obtain greater information extraction capability. The combined effect of the two will produce a greater enhancement.</sample>
    <sample id="285">The video discusses the work of Mingqi Gao from Peking University on benchmarking factual error correction for dialogue summarization. The video highlights the importance of correcting factual errors in dialogue summarization and argues that current FEC models have flaws in their evaluation methods. The video proposes a new taxonomy of factual errors and introduces manually annotated reference corrections to address these issues. The video also explores different training modes for FEC models and finds that training with reference summaries from dialogue summarization datasets yields the best results. The video concludes by suggesting that combining human-annotated data with synthetic data is a promising direction for improving FEC models.</sample>
    <sample id="286">演讲者的名字是James Finch和Sarah Finch。</sample>
    <sample id="287">这篇论文有四位作者：Javad Hosseini、Filip Radlinski、Silvia Pareti和Annie Louis。</sample>
    <sample id="288">BLiMP和SyntaxGym数据集可用于测试句法现象。</sample>
    <sample id="290">第一个研究问题的五种方法的缩写是WSL，即Weakly Supervised Learning。</sample>
    <sample id="291">该模型在命名实体识别、分类、词性标注和问答等任务上进行了评估。</sample>
    <sample id="294">CamemBERT 最初是在大规模的文本数据集上训练的，包括维基百科、Wikipedia、维基百科、维基百科和维基百科。</sample>
    <sample id="295">演讲者的名字是Adam Przepiórkowski。</sample>
    <sample id="296">The video presents a collaboration between the University of Turin and Amazon Alexa on Natural Language Understanding, specifically focusing on irony detection in natural language. The researchers developed a corpus called EPIC (English Perspectivist Irony Corpus) by collecting data from social media, Reddit, and Twitter over a period of 1.5 years. They collected about 300 short conversations made up of pairs of text, one following the other, and repeated this for both sources and five varieties of English.

To annotate the data, they used the crowdsourcing platform Prolific to select about 15 annotators for each of the English language varieties, totaling 74 annotators. Each annotator was given 200 texts or short conversations, and extra questions were used as an attention check for quality control. On average, each short conversation received five annotations.

The researchers observed differences in inter-annotator agreement based on various dimensions such as gender, age group, nationality, and more. They built different models called perspective-aware models by fine-tuning a pre-trained language model on splits of the datasets based on different annotators. While there were no significant trends in raw performance, the perspective-aware models showed less uncertainty and more confidence in their predictions compared to gold standard aggregated models.

The researchers also analyzed the causes of differences in annotations and found that generations close to each other and annotators from the United Kingdom and Ireland showed higher variations in response.</sample>
    <sample id="297">The project aims to develop a typology and glossary of dogwhistles, particularly focusing on racist, transphobic, and anti-Semitic ones. The glossary includes over 340 terms and symbols collected from various sources, including academic, Wikipedia, blog, and other sources. The project also conducts a case study of historical U.S. political speeches, which shows that the frequency of racial dogwhistles in the U.S. Congressional Record closely aligns with the Republican Southern Strategy since the Civil Rights era. Additionally, the project evaluates dogwhistle recognition in language models, such as GPT-3, and how they can evade content moderation online through toxicity detection. Overall, the project sheds light on the context-dependent meaning of dogwhistles and their impact on political influence and persuasion.</sample>
    <sample id="298">通过实验，我们发现，通过重新训练或继续预训练一些模型以使用更多最近的数据，性能会随着训练和测试数据之间的时间间隔增加而下降。这证实了我们的假设，即时间漂移是导致性能下降的主要原因。</sample>
    <sample id="299">The speaker, Michalis Korakakis, introduces a joint work with Andreas Vlachos at the University of Cambridge on "Improving the robustness of NLI models with minimax training." Despite achieving state-of-the-art results across various benchmarks, recent studies have shown that NLI models rely on spurious correlations or shortcuts introduced during dataset creation. These shortcuts lead to poor performance on out-of-distribution test sets. Current shortcut mitigation methods assume access to an auxiliary model and often require domain-specific knowledge, which limits their potential. The proposed method aims to reduce reliance on shortcuts by focusing on under-represented hard examples through a minimax training objective between a learner and auxiliary. The auxiliary generates example weights to incentivize the learner to learn from these hard examples, improving out-of-distribution performance while maintaining high in-distribution accuracy. The method does not assume specific types of shortcuts and uses a feed-forward network for the auxiliary. The paper evaluates the method on three analytic datasets (MNLI, FEVER, QQP) and their corresponding adversarial test sets, showing consistent improvements over ERM training and other shortcut mitigation methods. The study also examines the effect of pre-training the learner, the size of the auxiliary, and qualitative evaluations of the learned example weight distribution.</sample>
    <sample id="300">Belinda is presenting a work on interactive dictation, which was done at Semantic Machines in collaboration with Jason Eisner, Adam Pauls, and Sam Thomson. Interactive dictation allows users to use their voice to both dictate and edit a document in a natural and intuitive manner. The user can start by dictating, and then correct mistakes by saying the corrected version of the sentence. The system should be able to pick up on this speech correction and replace the incorrect span with the new utterance. The user can also issue verbal commands to specify edits, such as replacing a word in a sentence. This task is characterized by flexible interleaving of dictation and editing, not separated by a trigger word, and using intuitive and open-ended natural language utterances to specify edits.

The contribution of this work is threefold: introducing and formalizing the task of interactive dictation, designing a data collection interface and building a dataset for this task, and creating a baseline system for this task. The task of interactive dictation is formalized as a four-step procedure: parsing raw audio into a speech transcript, segmenting the speech transcript into separate dictation and command utterances, extracting and normalizing each command, and fixing ASR miss detections and speech errors. Each dictation and command utterance is then executed in sequence until the final document state is arrived at.

To collect data for this task, a new interface was designed, and the dataset was collected using this interface. A baseline system was built that performs each of these four steps, with separate models trained for each step. The segmentation model was found to be fairly accurate and efficient. The ASR repair and interpretation models were evaluated jointly using exact match of the predicted end-state against the goal end-state, and there was a trade-off between runtime and accuracy. GPT-3 models were more accurate but also much slower, while predicting state directly was more accurate than predicting intermediate programs for T5 models. Predicting programs allowed for significant improvement in efficiency with minimal impact on accuracy. There is still room for progress in this task, and the code for this work has been released for future work.</sample>
    <sample id="302">在输出序列中排列词元是必要的，因为虽然模型可以预测输入和输出中的所有正确词元，但它们的顺序可能不正确。通过排列词元，模型可以确保输出序列与输入序列保持一致的语法结构和语义关系。</sample>
    <sample id="303">作者建议模型所有者提高偏见缓解方法的透明度，因为这有助于识别和解决模型中出现的有害模式。通过了解这些模式的来源，可以更好地研究和减轻它们，从而减少对边缘化群体的伤害。</sample>
    <sample id="304">最小对不可接受输入是指一种评估语言模型的方法，其中模型被要求在一组可接受和不可接受的句子中选择一个。这些句子通常具有相同的语法结构或语义特征，但其中一个句子是可接受的，另一个句子是不可接受的。模型应该根据其对句子的接受度做出预测。</sample>
    <sample id="305">In this video, Dawei presents their recent work on weakly supervised learning (WSL), a method of training neural networks using weakly labeled data. Weak supervision involves labeling data using simple heuristic rules, knowledge bases, or low-quality crowdsourcing, which is much cheaper than human annotations but also noisy. If directly trained on weakly labeled data, neural networks tend to memorize label noise and do not generalize well.

The research questions addressed in the video are: 1) Is clean validation data necessary for WSL? 2) How many clean samples are needed? 3) Should we only use clean samples for validation?

The findings indicate that recent WSL methods require clean validation samples to work properly, with a large performance drop if no clean validation samples are available. Increasing the number of clean validation samples improves performance, but direct fine-tuning on clean data can even achieve better results. Continuous fine-tuning on clean validation samples is a simple yet strong baseline that should be considered in future WSL research.

The video concludes by recommending that future work in WSL should report model selection criteria, compare with few-shot learning baselines, and consider continuous fine-tuning as a strong baseline. The code for their work is open-sourced and can be accessed via a QR code on the slide.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim discuss their work on entity tracking in language models. They argue that for an agent to understand a discourse, it needs to track which entities are mentioned and how their state changes as the discourse unfolds. For example, in the context of a recipe, an agent has to understand that "Put the eggs, sugar, and flour in a bowl" results in all of these three entities ending up in a bowl. And if the discourse continues with "Mix to form a light batter", then the agent has to understand that now all of these entities are part of the batter. The researchers aim to answer the question of to what extent large language models can track entities.

The challenge in designing a task to evaluate entity state tracking abilities is that some entity states will be common in the pre-training data, and therefore, the model may predict the correct state without actually having any entity tracking abilities. Sometimes entity states can be predicted from individual words or phrases without actually considering the larger discourse, and the model may seem to be able to perform entity tracking while in fact it just learns simple heuristic associations between words and entity states. If one uses fine-tuning or in-context demonstrations, then the model may memorize entity state sequences, or it may learn to apply heuristics such as slot filling if such heuristics are not blocked in the evaluation task design. To address these challenges, the researchers designed a task involving boxes and objects. The input to the model starts with a description of the initial contents of each box, and the task of the language model is to complete the input by predicting the contents of each box. Given just this initial description, the task is pretty trivial: the model can just copy the relevant information from the description. But in our task, we also include multiple state-changing operations like moving objects or adding objects to a box. So for these, the model would have to combine the initial description with the operations to make the correct prediction. For example, Box 1 now contains the car and the watch after moving the watch from Box 3 to 1. Additionally, we implemented various measures to prevent the model from using heuristics, as Sebastian discussed earlier on. We tested the setup with Flan-T5 and GPT-3 and -3.5 models using 2-shot in-context learning. Our experiments show that most models simply repeat the initial state, as you can see from the generally high accuracy on the right panel. And we can also see that only text-davinci-003 exhibits non-trivial tracking, which is the pink line here in the left panel. And all other models performed below a strong random baseline obtained by random simulation, which is the blue line. What gives rise to this difference between models? Since the models we tested varied along several different dimensions, we investigated what other factors might be in play by zooming into the GPT series. And we found that all GPT-3.5 models, which all have been trained on substantial amounts of code, exhibit non-trivial entity tracking behavior, whereas all models that do not have code as a substantial part of their pre-training do not. And this suggests that pre-training on code is what's responsible for making this capacity surface in pre-trained language models. We also found that smaller models like T5-base can learn to perform entity tracking if you directly fine-tune the models. But on the other hand, randomly initialized models of the same architecture cannot learn our state tracking task even when they receive direct supervision, suggesting that pre-training is again important here. However, as we discuss in more detail in the paper, it remains unclear whether the state tracking abilities we observe generalize beyond our set-up in this case.</sample>
    <sample id="307">作者使用了命名实体识别、分类、词性标注和问答等任务的指标来评估他们的模型。</sample>
    <sample id="308">The presentation by Jenny, a first-year PhD student at Carnegie Mellon University, focuses on the concept of positionality in NLP research and its impact on model performance. Positionality refers to the perspectives held by individuals based on their demographics, identity, and life experiences, which can influence research decisions and outcomes. The presentation highlights the issue of design biases in NLP models, where certain populations may be underrepresented or misclassified due to these biases.

To address this issue, Jenny and her team developed a framework called NLPositionality, which compares annotations from diverse annotators with existing datasets and models. This framework involves re-annotating data sets with many annotators from different demographics and comparing their annotations to the models' predictions using Pearson's R correlation score. The study involved over 16,000 annotations from 1000 annotators from 87 countries, providing insights into the alignment of NLP datasets and models with specific populations.

The results show that there is positionality in NLP, with data sets and models being most aligned to English-speaking countries and people with higher education levels. However, some populations are left behind, such as non-binary individuals. To mitigate these biases, the presentation recommends keeping records of all relevant design choices throughout the research process, conducting NLP research with a perspectivist lens, and building specialized datasets and models for specific communities.</sample>
    <sample id="309">使用了双标注对话的注释者之间一致性指标来衡量。</sample>
    <sample id="310">在不可接受和可接受查询中，选择一个完全无关的领域来添加句子，例如维基百科。</sample>
    <sample id="311">根据所提供的英文内容，无法确定论文作者所属的机构。</sample>
    <sample id="312">MultiInstruct 是第一个大规模多模态指令调优基准数据集，它包括62个多样化的多模态任务，涵盖10个广泛类别。这些任务是从21个现有的开源数据集中派生出来的，并且每个任务配备了五个专家撰写的说明。与其他基准不同的是，MultiInstruct 提供了大规模的多模态指令任务，而其他基准通常只关注语言任务。</sample>
    <sample id="313">根据所提供的英文内容，这篇论文有两位作者：James Finch和Sarah Finch。</sample>
    <sample id="314">二进制协调指的是两个或更多个句子成分之间的关系，这些成分通过一个连接词（如“和”、“但”、“或”）连接在一起。在二进制协调中，这些成分被称为“分句”，而连接它们的连接词称为“连接词”。</sample>
    <sample id="315">研究中提示语的平均长度没有明确说明。</sample>
    <sample id="316">这些发现表明，较小的 T5 模型在使用 CoScript 训练后可以生成比大多数大型语言模型更好的脚本。这表明，当适当训练时，较小的模型可以超越较大的模型。</sample>
    <sample id="317">The paper titled "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors" by Peng Li from Fudan University presents a new approach to information extraction, which is a classic task in natural language processing. The goal of information extraction is to extract structured information from unstructured text, such as named entity recognition and relation extraction. The paper proposes using code large language models like Codex to perform the structure-to-structure code generation task, which can easily convert text to a structured format during the input stage and ensure aligned structures in the output stage. The paper evaluates the performance of different models, including T5 model, UIE model, GPT-3 model, and Codex model, on three recognition datasets and four relation extraction datasets. The results show that the proposed approach using code language models and code format prompts significantly outperformed the traditional baseline models, especially in terms of recall. The paper also analyzes the phenomenon of structural errors when decoding with GPT-3 and text format prompts, and finds that using Codex and code format prompts almost eliminates such errors. Overall, the paper provides some inspiration for improving information extraction tasks using code large language models and code format prompts.</sample>
    <sample id="318">你好，我是Yanis Labrak，我将向你介绍我们关于“DrBERT：一种用于生物医学和临床领域的鲁棒预训练模型”的研究成果。首先，我们将讨论 healthcare 中的语言建模。接下来，我们将介绍论文的主要贡献。我们介绍了第一个用于法语的生物医学模型——DrBERT，该模型基于RoBERTa，并在NACHOS数据集上进行训练，该数据集是从网上爬取的医学数据。我们还介绍了使用多种预训练设置和数据源的模型比较。然后，我们将展示我们在11个生物医学和临床下游任务上的结果。最后，我们将总结实验并提供有关如何访问这些模型的更多细节。自2018年发布以来，BERT已成为解决自然语言处理任务最有效的方法之一，并在性能上取得了巨大飞跃，相比之下，历史上的静态和上下文化方法如Word2vec、fastText等效果较差。自那时起，该模型已被应用于许多其他语言，例如在法语中使用CamemBERT，在生物医学领域使用PubMedBERT和BioBERT，在临床领域使用ClinicalBERT，但大多是在英语中。针对其他语言的专门模型相对较少，通常基于连续预训练，因为缺乏相应的内部数据。然而，法语目前还没有开放的生物医学模型。因此，我们提出了一个问题：对于广泛的应用，最适合的数据来源是什么？从网上爬取的数据可以作为临床数据的替代品。为了回答这个问题，我们将DrBERT与基于Nantes大学医院数据仓库 anonymized 数据的 ChuBERT 模型进行了比较。随后，我们提出了另一个问题：我们需要多少数据来训练一个专门针对法语数据的模型？是4GB、8GB，还是更多？为了回答这个问题，我们首先训练并比较了四个从头开始的模型：第一个版本的DrBERT，使用7GB的NACHOS；第二个版本的DrBERT，使用4GB的NACHOS数据集；第一个版本的ChuBERT，这是一个临床模型，使用4GB的临床笔记中的句子；以及最终的ChuBERT模型，使用4GB的NACHOS数据集和4GB的临床笔记的混合。除了这个比较之外，我们还引入了三个使用连续预训练策略训练的模型，其中一个基于CamemBERT的权重和训练在4GB的NACHOS数据集上，另一个也基于CamemBERT的权重和训练在4GB的临床笔记上，第三个是基于English biomedical 模型PubMedBERT，并训练在4GB的NACHOS数据集上。总共，我们有七种模型。为了评估我们的七种模型，我们收集了公开和私下的下游任务数据，如命名实体识别、分类、词性标注和问答。这些模型与六个基线模型（CamemBERT OSCAR 138 GB、CamemBERT OSCAR 4 GB、CamemBERT CCNET 4 GB、PubMedBERT、BioBERT和ClinicalBERT）进行了比较。评估结果表明，模型在与模型训练数据同质的数据上表现最佳。然而，我们可以观察到，来自异质数据源的数据更加灵活。我们还观察到，使用更多数据可以提高性能。总的来说，从头开始预训练似乎在大多数任务上获得了更高的性能。然而，我们对使用CamemBERT权重和分词器训练的模型进行的控制预训练实验显示，其结果与DrBERT 4GB从头开始训练的结果相当。而使用CamemBERT权重和分词器的模型则存在稳定性问题。最后，我们的系统在11个下游任务中的9个任务上提供了更好的性能，并超过了通用模型CamemBERT的全球结果。我们还观察到，更专门化的数据更好，但它并不总是可扩展的。所有从NACHOS数据集中获得的预训练模型都可以在Hugging Face上免费获取，并且在MIT许可证下，所有训练脚本都在我们的GitHub仓库中。所以，谢谢你的演讲，我们期待在多伦多的海报会上与你交流。</sample>
    <sample id="319">论文研究了从头开始训练和持续预训练两种学习策略。</sample>
    <sample id="320">根据所提供的英文内容，由于测试重复使用而导致的过拟合因素似乎在本研究中并不显著。这可以通过图表右侧的红色最佳拟合线的梯度大于1来证明，这意味着在CoNLL-2003上的每单位改进在CoNLL++上转化为超过一单位的改进。这表明在本研究中没有观察到过拟合。</sample>
    <sample id="321">根据所提供的英文内容，评估简化质量的方法包括分析句子对的类型和简化程度。例如，可以观察文本中使用的简化技术的强度，如词汇简化、结构简化和整体简化。此外，DEPLAIN语料库具有不同简化变换的高多样性，这有助于评估简化质量。</sample>
    <sample id="322">Enrico is presenting at ACL 23 and will be discussing the question of what a text classifier learns about morality. He begins by explaining that human morality helps us distinguish right from wrong and is essential to our societies. However, current approaches in NLP often treat morality as a singular scale between immoral and moral, which can hide the pluralist nature of morality and lead to misunderstandings.

Enrico introduces the Moral Foundation Theory, which suggests that there are five different ways humans perceive morality, similar to how we have five taste buds. Each action or concept tickles a different moral foundation, and people prioritize these foundations differently, leading to diverse judgments of morality.

Enrico and his team have used explainable AI techniques to understand how language models trained to understand morality in text learn to classify morality. They focused on understanding how morality is expressed differently across different domains using the Moral Foundation Twitter Corpus, which contains 35,000 tweets from seven different domains, such as #AllLivesMatter and #BlackLivesMatter.

Their experiments showed that language models recognize the differences in how morality is expressed in different domains. For example, they found that ALM's version is associated with words such as overthrow, mayhem, and subversion, which are frowned upon, while BLM's version encourages subversion. This highlights the importance of considering the domain-specific nuances of morality when training language models.

Enrico concludes by emphasizing that using just one model for many different domains can lead to misunderstandings of morality in dangerous ways. He invites everyone to see him at ACL in Toronto.</sample>
    <sample id="323">The paper by Yujie Wang from Shanxi University, China, titled "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA," addresses the challenge of Commonsense QA by combining language models and knowledge bases. The authors propose a method called DHLK to improve the interaction between the two modalities and optimize the structure and knowledge representation of the HKG through a two-stage pruning strategy and KRL. They use RoBERTa and Mask Self-Attention to encode and fuse QA contexts and entities, dynamically removing entities with weaker relevance to the QA context based on attention weights. The authors update entity and relation embeddings using TransE and RMSA, and incorporate HKG path information into the QA context for path enhancement. The final answer prediction is obtained by inputting the HKG graph, paths, QA context embedded, and QA context embedded into the MLP. The paper reports good results on CommonsenseQA and OpenBookQA using external knowledge bases such as ConceptNet, WordNet, and Wiktionary.</sample>
    <sample id="324">是的，语言模型有不同政治偏见。根据演讲内容，GPT-4是最具自由派倾向的语言模型，而GPT系列通常比BART系列及其变体更自由派。此外，通过进一步预训练语言模型在6个不同的 partisan corpora上，可以观察到政治偏见的显著变化。例如，使用左翼Reddit语料库进一步预训练RoBERTa，可以看到其政治偏见出现了显著的自由派转变。</sample>
    <sample id="325">你好！我的名字是马修斯·林德曼，今天我要给你介绍我们关于“使用多集标记和潜在排列实现无树的组合泛化”的论文。这是与我的导师亚历山大·库勒和伊万·提托夫合作完成的。组合泛化可以理解为学习者处理更深层次的递归和在训练中未见过的短语组合的能力。在语义解析的背景下，测试组合泛化可能如下所示。就像通常一样，我们有一个训练集的句子。在这个例子中，“女孩睡了。”和“玛丽知道女孩睡了。”这些句子与逻辑形式配对，代表它们的核心含义。与标准机器学习评估不同，测试集不来自相同的分布，而是包含结构上未见过的逻辑形式。在这个例子中，模型在训练中已经见过浅层递归，但在测试中遇到了深层递归。朴素的seq2seq模型很难处理这种超出分布的泛化，经常产生与输入脱节的输出。特别是，它们往往无法复制输入和输出之间的系统对应关系，如图中用颜色编码的那样。解决这个问题的一种流行方法是在模型中集成树。树旨在捕捉将短语与逻辑形式相关联的组合过程。这种方法效果很好，但树通常不会给出，需要某种方式来获取。这可能会很复杂，有时计算昂贵。通常，这涉及到处理逻辑形式的正式预处理，例如处理变量符号。获取树可能还需要专门的语法推导程序。在这篇论文中，我们没有使用树，并引入了一个神经seq2seq模型，该模型直接建模了输入和输出片段之间的对应关系。我们首次展示了在不依赖树的情况下，对深层递归的强泛化。我们的方法从两个步骤预测输出：首先，我们将每个输入标记为将出现在输出中的无序多集的标记。在第一个步骤之后，我们有了所有正确的标记，但它们没有排序。因此，在第二个步骤中，我们使用另一个模型来预测一个排列，以将它们放在正确的位置。我们引入了一种新的方法来预测排列，它没有对可能的排列施加任何硬约束。这使我们的方法变得灵活而有表达力。概念上，我们的排列模型的工作原理大致如下。我们从右到左遍历输出，确定将输出中的下一个标记放在哪个位置。对于第一个输出位置，我们简单地选择一个，如红色所示。然后我们跳转到另一个多集标记，以确定输出中的第二个标记。我们以类似的方式确定输出中的第三个标记，通过跳转到另一个多集标记。我们继续这个过程，直到第一个阶段的所有标记都被访问过一次。为了给你一个预览实验结果，这里我们比较了我们在COGS基准上的方法与其他无树模型。我们的模型在深层递归泛化方面远远优于其他模型。然而，还有一些其他类型的结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了几个有趣的技術挑戰。首先，输入和输出之间的对齐在训练数据中并不给定。因此，对于给定的标记，我们不知道它来自哪个多集，这给训练带来了挑战。此外，有时候有几个与数据一致的排列，但其中一个是语言上正确的。我们通过在训练过程中引入对齐来解决这个问题。我们的排列方法非常灵活，但找到最高得分排列是一个NP困难问题，因为它与旅行商问题有关。我们通过GPU友好的连续放松来近似这个问题，这还允许我们通过解决方案反向传播并学习更有可能的语言上正确的排列。如果你想了解我们实验的更多细节以及如何解决这些挑战，请查阅我们的论文或来到我们的海报。</sample>
    <sample id="326">认知失调是指一个人同时持有两个相互矛盾的信念或采取了与这些信念相矛盾的行为。例如，一个人可能会说：“我知道吸烟会杀死我”，然后又说：“我会议后抽了几支烟”。这种信念和行为之间的不一致就是认知失调。</sample>
    <sample id="327">Xiao Xu, a third-year PhD student from Harbin Institute of Technology, is presenting their work at ACL 2023. Their research focuses on vision-language representation learning and aims to train an AI system that can understand both images and text. They propose ManagerTower, a novel architecture that aggregates insights from pre-trained unimodal experts at different levels. The managers in each cross-modal layer adaptively combine these insights to facilitate comprehensive cross-modal alignment and fusion. The proposed method outperforms many base-size models pre-trained on 4 million data and surpasses some models trained with more data or parameters.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">The presented work, "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization," focuses on the task of video sentence localization, which involves finding relevant segments in long videos based on a given natural language query. The authors propose a noise-resistant structured pseudo-label generation method to address the limitations of existing zero-shot methods that rely heavily on manual annotations and suffer from label noise.

The proposed method uses a pre-trained image caption model to generate complex free-form pseudo-queries from video frames. It then measures the relevance between individual frames and these queries to generate pseudo-events that ensure high relevance within events and low relevance outside them. To reduce the impact of label noise, the authors introduce a strategy to reduce the weight of noisy samples and create noisy labels during training. This is achieved by estimating label noise based on the model's predicted confidence and the Intersection over Union (IoU) between predictions and pseudo-labels, adjusting sample weights accordingly.

Experiments conducted on the ActivityNet Captions and Charades-STA datasets demonstrate the effectiveness of the proposed method. The evaluation metrics used include R@M (percentage of predicted moments with IoU greater than M) and mIoU (average IoU). The results show that the proposed method outperforms other zero-shot methods on most metrics, achieving the best zero-shot performance on both datasets.

Overall, the work introduces a robust zero-shot video sentence localization method that leverages structured pseudo-label generation and noise-resistant techniques to improve performance and reduce the need for manual annotations.</sample>
    <sample id="330">在主动学习时，累积训练（将所有已收集的数据累积起来）比迭代训练（每次迭代只使用最新数据集）更有效。</sample>
    <sample id="331">演讲者的名字是Sara Papi。</sample>
    <sample id="332">MuDa基准中的数据是从TED演讲的转录中获得的，这些转录已被翻译成14种不同的语言。</sample>
    <sample id="333">Wenhao from Nanjing University is introducing their work on neural machine translation, specifically focusing on the INK framework. The team acknowledges collaborators from Shanghai AI Lab, Nanjing University, and the University of Hong Kong. They aim to enhance NMT models' generalization and performance by smoothing predictions according to nearest neighbors in the representation space.

The proposed kNN-MT approach has two significant drawbacks: time-consuming neighbor retrieval and difficulty updating representations. To overcome these issues, the INK framework injects kNN knowledge into an MT model. The INK training loop consists of two steps: extracting kNN knowledge to guide the adapter to adjust representation, and refreshing the datastore asynchronously with updated representations.

Experiments were conducted using the winner model of WMT’19 German-English news translation task as the off-the-shelf NMT model. The results show that even for the winner model, its representation space can still be greatly improved. The INK system outperforms state-of-the-art kNN-MT systems and achieves the best performance after smoothing the representation space. Refining representation according to kNN knowledge brings larger performance improvement.

The INK system achieves an average gain of 1.99 COMET score and 1.0 BLEU score compared to state-of-the-art kNN-MT systems. It also achieves better translation performance with less memory space and faster inference speed.</sample>
    <sample id="335">演讲者的名字是马蒂亚斯·林登。</sample>
    <sample id="336">跨语言转移是指使用一种语言训练的模型在另一种语言上进行测试和预测的过程。在跨语言语义解析任务中，跨语言转移涉及使用一种语言（源语言）训练的模型在另一种语言（目标语言）上进行测试和预测。</sample>
    <sample id="337">The presented research, "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," introduces a novel approach to handling out-of-vocabulary (OOV) words in embedding-based models. The research proposes the use of a Word Relationship Graph that mimics lexical rules of word formation and association. When an OOV word appears, it is tokenized into wordpieces and associated with relevant words, forming a two-level graph around the OOV word. Each word or wordpiece acts as a node in the graph, with its corresponding word embedding serving as the node attribution. To address the issue of assigning node attributes to OOV nodes, a self-attention network is utilized to assign attributes based on the characters of the OOV words. Two levels of Graph Attention Network are applied to extract important information and reduce noise from neighbor nodes. A readout block layer is incorporated to capture the whole graph information and summarize word formation. Contrastive learning is applied in the loss function with NT-XENT positive samples from the graph, such as two-hop relevant neighbor words, synonyms, or the OOV word itself. Experiments demonstrate that the model performs superior to baselines in both intrinsic and extrinsic tasks, proving the effectiveness of learning OOV words by word formation. The model can bring profits to both static and contextual models in downstream tasks. The possibility of adding languages to the model is discussed, with agglutinative languages being well-suited and fusional languages presenting more challenges. However, the model performs well with English by reasonable word segmentation.</sample>
    <sample id="338">Bingsheng's presentation introduces a collaborative research work titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations." The research, conducted by researchers from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research, aims to address the challenge of evaluating the quality of human-annotated explanations in machine learning models. The presentation highlights the limitations of traditional metrics like BLEU and ROUGE, which focus on word similarity but neglect task differences and utility during fine-tuning and inference stages.

The research proposes a unified structure for converting various tasks into a multiple-choice format, facilitating the analysis of explanation utility across different datasets. The study conducts experiments with nine subsets of data, comparing baseline and infusion settings where explanations serve as additional input to sequence-to-sequence models. The results show that fine-tuning with explanations can lead to substantial improvements, even with a small amount of data.

A novel evaluation metric called TREU (Task-Relevant Explanation Utility) is proposed, extending the simulatability score to evaluate the helpfulness of explanations at both fine-tuning and inference stages. The evaluation across five datasets using T5 and BART models demonstrates that TREU scores better reflect the beneficial impact of human-annotated explanations on model predictions compared to the simulatability score.

The presentation concludes by emphasizing the importance of high-quality human collaboration in annotation jobs and recommends that researchers perform similar quality checks in future studies. For more detailed findings, the audience is directed to the research paper.</sample>
    <sample id="339">论文的作者是Saarland University的博士生。</sample>
    <sample id="340">Kuan-Hao Huang from UCLA is presenting their work on "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation." This research, a collaboration with Varun, I-Hung, Anoop, Kai-Wei, and Aram, aims to address the challenge of obtaining large-scale, high-quality paraphrase data for training effective paraphrase generators. Existing datasets like MRPC, PAN, and Quora offer high quality but are limited in scale, while automatically generated datasets through back-translation lack syntactic diversity.

The proposed solution involves using Abstract Meaning Representations (AMR) graphs to generate syntactically diverse paraphrases. AMR graphs capture the abstract meaning of a sentence, with nodes representing semantic concepts and edges representing relations between them. The focus node represents the main assertion of the sentence. By changing the focus node and modifying corresponding edges and edge labels, the researchers can generate text with similar semantics but different syntax.

The dataset, ParaAMR, consists of around 15 million source sentences, with approximately 6.9 paraphrases per source sentence. Quantitative analysis shows that ParaAMR achieves higher syntactic diversity scores compared to other back-translation datasets while maintaining good semantic similarity. The dataset benefits various NLP applications, including learning sentence embeddings, syntactic control paraphrase generation, and few-shot learning data augmentation.</sample>
    <sample id="341">作者使用了BLEU、平均延迟和计算感知平均延迟作为延迟测量方法。</sample>
    <sample id="342">The presentation by Gao Jingsheng introduces the paper "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming," which was conducted by a team from Shanghai Jiao Tong University and Xiaobing.AI. The paper focuses on addressing the challenges of constructing large-scale dialogue datasets, particularly in the context of personalized dialogue and multi-party conversations. The existing large-scale pre-trained dialogue datasets are mostly text-sourced, and there is a need for video-sourced datasets that can capture real spoken conversation. The paper proposes a unique automatic dialogue-constructing method to address this challenge.

The dataset, LiveChat, is constructed in three steps: extracting audio from videos, transcribing audio into utterances through ASR, collecting audience comments and constructing dialogues using a reply-to-whom matching method, and collecting persona information for personalized dialogue generation. The comparison between LiveChat and other existing open-domain dialogue datasets shows that LiveChat has a larger scale, more personal annotations, and longer average sessions.

The experiments conducted on two benchmark tasks show that the extracted persona and longer average sessions are beneficial to the final result. Both rules and classifiers are important to persona extraction. In the Addressee Recognition task, single-stream BERT outperforms double-stream BERT, although persona is beneficial to address recognition. The performance of BART is better than the other two, confirming that the domain of LiveChat is far away from the domains of existing dialogue datasets. The human evaluation results of LLMs are better in terms of rich informativeness. The presentation concludes with the proposal of LiveChat as a Chinese video-sourced and personalized dialogue dataset, and the need for efficient transfer learning of LLMs for LiveChat.</sample>
    <sample id="343">Hello everyone, I'm Akshatha, and today my co-author Martin and I are presenting our work "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources." This work is a collaboration between McGill University, Mila, and Microsoft Research. 自然语言理解模型依赖于各种知识来源，包括预训练时获得的参数中包含的知识和输入中的知识。最近的研究表明，模型可以利用预训练时间的知识来解决任务。但在自然语言理解中，通常需要在推理时间提供的知识。例如，在句子“John 在电视上见到了新当选的总统。”中，预训练参数可以包含关于总统和电视的信息，但无法可靠地知道这个实例化的实体“John”是谁或新任总统是谁，因为总统自预训练以来可能已经发生了变化。因此，成功解决知识密集型NLU任务的模型需要能够整合和使用预训练时间和推理时间的知识。在这项工作中，我们提出了一个诊断测试套件，用于评估知识整合能力。我们引入了一个核心参考消解任务，旨在测试模型是否能够利用不同来源中的知识。我们通过人类研究参与者和已建立的核心参考消解模型评估数据集。以下是从我们数据集中的一个示例。Servin 是一名法官。Kea 是一名面包师。Servin 和 Kea 在公园里见过面。在一天的工作结束后，他很高兴在法庭上决定案件后放松一下。任务是确定代词“他”的正确实体，即 Servin。解决给定代词需要两种信息：实体特定知识，如“Servin 是一名法官。”和背景知识，如“法官在法庭上决定案件。”一般来说，背景知识是在大规模语言模型预训练期间学习的，而实体特定知识通常在推理时间观察到。我们通过改变这些两种信息的可获得性来定义了三个 KITMUS 设置。首先，我们有典型的设置：“背景-预训练”，其中背景知识被认为是在预训练时间可用的。其次，有“背景-两者”设置，其中背景知识在预训练时间和推理时间都可用。最后，有“背景-推理”设置，其中所有知识类型都在推理时间可用。这个最后的设置特别有趣，因为它模拟了在预训练数据中不存在必要的背景知识的情况下解决问题的情况。例如，因为自预训练以来已经出现了新的职业。以下是我们控制真实来源中事实可获得性的示例。在“背景-预训练”设置中，我们假设背景知识“政治家寻求当选政府职位”包含在预训练参数中，并在推理时间提供实体特定知识“Chichester 是一名政治家。”在“背景-两者”设置中，我们不仅提供实体特定知识，还在推理时间提供政治家的背景知识。在“背景-推理”设置中，我们提供虚构的职业“mirituer”而不是政治家，因为“mirituer”很可能不在预训练参数中。我们用人类研究参与者和已建立的核心参考消解模型评估数据集。在这幅图中，我们展示了在背景-预训练设置中最难的变体上最佳性能模型的结果。没有针对KITMUS进行特定任务训练，两个模型的表现都不好。然而，当它们接受KITMUS训练时，C2F和BERT4Coref等模型在推理时间比随机选择表现显著更好。这表明，当在通用引用消解数据集上进行泛化训练时，大多数模型学会利用表面线索，而这些线索在KITMUS测试中并不适用。额外实验显示，即使表现最好的模型也不能可靠地整合仅在推理时间提供的反向知识。总之，我们的论文的主要 takeaway是许多核心参考消解模型在没有特定任务训练的情况下无法推理来自不同来源的知识。然而，经过特定任务训练后，一些模型成功地整合了多种来源的知识。尽管如此，即使表现最好的模型似乎在可靠地整合仅在推理时间提供的反向知识方面存在困难。如果您想了解更多信息，请查阅我们的论文，并查看GitHub上的数据集和代码。谢谢您的聆听。</sample>
    <sample id="344">基于树的方法的缺点包括：1.需要复杂的预处理，例如处理可变符号；2.需要专门的语法推导程序来获取树。</sample>
    <sample id="345">The paper "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" by Matthias Lindemann, Alexander Koller, and Ivan Titov introduces a neural seq2seq model that can handle deeper recursion in semantic parsing without relying on trees. The model predicts the output from the input in two steps: first, it tags each input token with an unordered multiset of tokens that will appear in the output, and then it uses another model to predict a permutation to put them into the right order. The permutation model is flexible and expressive, but finding the highest-scoring permutation is NP-hard. To address this challenge, the authors approximate the problem with a GPU-friendly continuous relaxation that allows them to backpropagate through the solution and learn linguistically more plausible permutations. The experimental results show that the model outperforms other treeless models on generalization to deeper recursion. However, some other kinds of structural generalization remain challenging. The paper also addresses technical challenges such as inducing the alignment between input and output as part of the training and handling multiple permutations that are consistent with the data but the linguistically correct one is latent.</sample>
    <sample id="346">根据所提供的英文内容，无法确定论文作者所属的机构。</sample>
    <sample id="347">你好，我是Myra，今天我要谈论我们的一篇论文《标记的人物：使用自然语言提示衡量语言模型中的刻板印象》。这项工作与Esin Durmus和Dan Jurafsky合作完成。近年来，许多研究记录了大型语言模型（LLM）中存在刻板印象和社会偏见的现象。然而，这些措施存在一些限制。它们通常依赖于手工制作的数据集，耗时费力，而且只能衡量非常具体的刻板印象，这意味着它们不能推广到其他 demographics 或者上下文，或者它们简单地捕捉到了非常广泛的广泛关联，比如对某个群体的负面关联。此外，大多数在这个领域的工作没有考虑到交集性，即多方面的社会身份可以加剧偏见并造成独特的伤害。为了克服这些限制，我们利用这些 newer 的指令调优LLM 的特性，它们非常擅长响应指令和提示。因此，我们可以要求模型生成人物形象，即一个想象中的个体的形象，使用提示如“想象一下你是一个亚洲女性。描述你自己。”。我们可以立即看到，这个方法非常具有可推广性，因为我们可以指定任何我们需要的身份标记。以下是一些GPT-4的示例生成内容。立即可以看出，虽然输出并不是传统意义上的负面或有毒的，但其中有一些有趣的模式。亚洲女性被描绘为低调；中东女性被提及使用“异国情调”和“迷人”等词语，指的是一个迷人的地区。而所有女性人物的参考都提到了 ancestry，而白人男性的人物则没有任何这样的参考。为了捕捉这些模式，我们的方法有两个部分。第一部分是生成这些人物形象。我们生成人物形象的提示受到一项研究的启发，该研究发现通过向人类主体提供这些提示，他们能够揭示种族刻板印象。这也使我们能够直接将生成的人物形象与人类手写回应进行比较。第二部分是标记词，这是一种识别区分标记群体和未标记群体的词语的方法，我稍后会详细说明。这种方法的好处是，我们可以获得非常具体的刻板印象和模式，而无需依赖任何特定的词汇表。所以，标记词方法利用社会语言学概念“标记性”，即有一个未标记的默认值，任何与默认值不同的群体在语言上都是标记的。例如，“战士”这个词通常与男性相关联。因此，当人们描述一个女性战士时，他们通常会说“女性战士”，用“女性”一词标记这个词。更广泛地说，社会中的 dominant 组织通常是语言和社交上的未标记的，而边缘化群体通常是标记的。因此，在我们的方法中，我们首先指定未标记和标记的群体，然后使用 Fightin’ Words 方法来区分每个标记群体的top 词语，Fightin’ Words 方法实际上是使用加权对数比值来区分top 词语。例如，对于黑人女性的人物形象，我们将使用 Fightin’ Words 方法，并将其与白人人物和男性人物进行比较，因为这两个是对应的未标记群体。现在，让我们看看一些结果。首先，我们使用刻板印象词汇表，我们发现生成的人物形象包含比人类手写回应更多的刻板印象。但是当我们查看词汇分布和词汇表时，我们发现了一些不同的事情。虽然生成的人物形象具有词汇表中词汇的更高频率，但人类手写回应具有更广泛的词汇范围，而生成的人物形象中的刻板印象词汇只是“高”和“强壮”的词汇。实际上，这个词汇表并没有捕捉到我们在前面幻灯片中看到的一些有害的模式。因此，我们将转向我们标记词方法的结果，以展示这些看似积极的描绘如何反映有害的模式。在我们的分析中，我们揭示了这些积极的描绘如何反映有害的模式。首先，从我们的群体中，top 词语包括“文化”、“传统”、“自豪”和“异国情调”。这些词语定义了这些群体与其身份的关系，将它们与其他群体区分开来。这有助于长期的歧视和其他化现象。此外，有很多常见的 tropes 反映在这些词语中，特别是对于女性有色人种。例如，描述 Latina 女性的词语包括“充满活力”和“丰腴”，这与热带主义的 tropicism 相关联。对于亚洲女性，top 词语包括“娇小”、“精致”和“ silky”，这些词语与亚洲女性被过度性化、被视为顺从和顺从的历史相关。最后，对于黑人女性，一些 top 词语包括“坚强”和“ resilient”，这些词语与被称为“坚强黑女人”(Strong Black Women)的刻板印象相关。虽然这种刻板印象乍一看似乎是积极的，但实际上它对这些 demographic 造成了很大的压力，因为他们必须坚强和坚强地面对社会障碍，这导致了这些 demographic 的负面健康结果和其他伤害。更广泛地说，我们发现每个标记群体的 top 词语几乎反映了非常本质化的叙事。基于这些模式，我们得出三个建议供模型所有者参考。首先，作为研究员，我们应该解决积极刻板印象和本质化叙事。我们也应该使用交集性视角来研究偏见和伤害，因为如果不这样做，可能会错过很多被忽视的事情。最后，应该增加关于偏见缓解方法的透明度，因为例如这些积极刻板印象，我们不知道是因为某种过度的价值观对齐导致的，还是因为某些反刻板印象方法导致了这些有害的图案。我们不能假设或进一步研究这个问题，除非有更多关于偏见缓解方法的透明度。谢谢大家。祝 ACL 活动愉快。</sample>
    <sample id="348">Myra, Esin Durmus, and Dan Jurafsky's paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" addresses the issue of social bias and stereotypes in large language models (LLMs). The paper highlights the limitations of existing methods, such as hand-constructed data sets that are time-consuming to curate and only measure specific stereotypes. To overcome these limitations, the authors leverage the property of instruction-tuned LLMs to generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." This approach allows for the creation of diverse personas by specifying different identity markers.

The authors use two main methods: generating personas and identifying marked words. They draw inspiration from a study that found that giving prompts to human subjects can surface racial stereotypes. By comparing generated personas with human-written responses, they can directly compare the outputs. The marked words method identifies words that distinguish marked groups from unmarked ones based on sociolinguistic concepts of markedness. For example, the word "warrior" is usually associated with men, so when describing a woman warrior, the term is marked with "woman."

The results show that generated personas contain more stereotypes than human-written ones. However, the distribution of words differs between the two. While generated personas have higher rates of lexicon words, human-written ones have a wider distribution of words. The marked words method reveals harmful patterns in seemingly positive portrayals, such as "culture," "tradition," "proud," and "exotic," which define groups by their relationship to their identity and contribute to discrimination and othering.

For black women, top words include "strong" and "resilient," connecting to the "Strong Black Women" archetype, which has been shown to have negative health outcomes. The paper concludes with three recommendations for model owners: addressing positive stereotypes and essentializing narratives, using an intersectional lens to study biases and harms, and increasing transparency about bias mitigation methods.</sample>
    <sample id="349">大家好，我是来自中国科学技术大学的 Jingwei Yi。我很高兴能为我们的论文做一个简短的广告视频。你们在复制我的模型吗？保护大型语言模型的版权，以服务形式提供嵌入。让我们先介绍一下嵌入作为服务的背景。目前，像GPT、LLAMA和PALM这样的大型语言模型在自然语言理解和生成方面表现出色。嵌入作为服务是基于大型语言模型构建的服务之一，用于辅助各种NLP任务。例如，OpenAI提供了一个基于GPT的嵌入API。然而，最近的研究表明，攻击者可能通过学习嵌入来窃取模型，并提供类似的服务。因此，有必要保护嵌入作为服务的版权。为了保护嵌入作为服务的版权，一种解决方案是在提供的服务中嵌入水印，并检测另一个服务是否包含水印。水印方法需要满足以下属性：首先，该方法应适用于嵌入作为服务。其次，水印不应降低所提供的嵌入的实用性。最后，水印需要在模型提取过程中转移到攻击者的服务上。现有的方法可以大致分为四类。然而，这种方法要么不适用于嵌入作为服务，要么缺乏可转移性。因此，在本文中我们提出了嵌入标记，这是一种适用于嵌入作为服务的后门水印方法。接下来让我介绍嵌入标记的详细内容。嵌入标记包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一组在适度频率间隔内的单词。我们假设供应商可以收集一个一般文本语料库，并用它来计算单词频率。在水印注入中，我们首先定义目标嵌入。当用户向供应商服务发送句子时，供应商计算句子中的触发词数量。所提供的嵌入是目标嵌入和原始嵌入的加权求和。目标嵌入的权重与句子中的触发词数量成正比。当句子中的触发词数量大于m时，所提供的嵌入等于目标嵌入。版权验证是检测另一个服务是否包含单词标记。我们首先构造一个后门数据集和一个 benign 数据集。后门数据集包含所有单词都属于触发集的句子，而 benign 数据集中的句子的所有单词都不属于触发集。然后供应商要求攻击者的服务提供带有数据集的嵌入。我们计算请求的嵌入与目标嵌入之间的余弦相似性和L2相似性。我们还应用KS测试并使用其p值作为第三个指标。我们在AG News、MIND、SST2和Enron Spam四个数据集上进行了实验。我们假设供应商使用wiki文本数据集来计算单词频率。四个数据集上的实验结果表明，我们的嵌入标记可以在保持下游任务的实用性的同时，具有很好的检测性能。我们还通过可视化四个数据集上的句子嵌入来验证所提供的嵌入的隐蔽性。图例表示每个句子中的触发词数量。如图所示，很难区分后门嵌入和正常嵌入。这就是全部。谢谢。欢迎与我们讨论。</sample>
    <sample id="350">The presentation by Simone Tedeschi and several renowned researchers explores the concept of superhuman performance in natural language understanding (NLU) tasks, particularly in the context of leaderboard-based evaluation. The researchers argue that while systems can achieve human-level or even superhuman performance on popular benchmarks like SuperGLUE and SQuAD, these achievements may not be reliable due to various issues.

Firstly, the benchmarks often involve comparing systems and humans on different sets of data, which can lead to unfair comparisons. For instance, humans are usually evaluated on a small subset of the test set, while systems are evaluated on the full set. Additionally, there are errors in ground-truth answers, which can benefit systems but not humans.

Secondly, researchers often vaguely estimate human performance using simple aggregation methods like average or majority voting. However, it is unclear whether this method accurately reflects the best possible human performance, as pay rates for human annotators vary significantly across tasks.

Lastly, details about the annotator pool, such as their cultural background and evaluation process, are often omitted, making it difficult to assess the reliability of the benchmarks.

Overall, the presentation highlights the need for more scientifically meaningful benchmarks that account for these issues and provide recommendations to avoid repeating them.</sample>
    <sample id="351">Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started. Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalize to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ Dataset. This is a data set that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization. To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.</sample>
    <sample id="352">ABC-Eval 代表“行为标注聊天”或“行为标注聊天”（Annotating Behaviors in Chat）。</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Questions" by Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, and Iryna Gurevych introduces a method to address the challenge of input underspecification in code generation and program synthesis. The authors propose interactivity into code generation through asking clarification questions to gather more specifications and alleviate the problem of underspecification. They create a synthetic dataset called CodeClarQA with clarifications on key operations and propose a pipeline of code generation by asking clarification questions. The paper also includes results of identifying missing key operations, error analysis, and an analysis of the impact of clarified key operations on generated code. Overall, the paper presents a novel approach to code generation that incorporates interaction and clarification questions to improve the accuracy and effectiveness of code generation models.</sample>
    <sample id="354">根据所提供的英文内容，直到2023年，CoNLL-2003和CoNLL++之间的性能增量才高于5个百分点。</sample>
    <sample id="355">你好，我的名字是Vasudha，我是一名在石溪大学攻读计算机科学博士学位的学生。我很高兴介绍我们在ACL 2023上发表的论文《基于迁移学习的不一致检测：解决罕见类别挑战》。我们首先定义了认知不一致，并解释了为什么研究这个问题很重要。简单来说，认知不一致是指两个不一致的信念或行为，例如一个人说：“我知道吸烟会杀死我”，然后又说：“会议结束后我又抽了几支烟”。这种信念和行为是不一致的，它们处于不一致状态。进一步提到“我觉得没有它们我就不能保住工作”就为第二次吸烟提供了合理解释。这些信念和行为之间存在和谐关系。虽然认知不一致在日常决策中非常普遍，但在其他 discourse关系中表达认知不一致的语言却很少见。那么，为什么这很重要呢？研究认知不一致可以帮助我们理解人们之间的 disagreement 对话的影响，跟踪趋势和信仰变化，以及了解人口的态度变化。高认知不一致与焦虑障碍有关，可以帮助我们更好地了解人们的心理健康。研究语言中表达的认知不一致也可以帮助我们了解极端主义和边缘化群体的 polarization。最后，认知不一致对于理解个人的认知风格和更好地理解决策过程也很重要。为了创建一个认知不一致资源，我们进行了大规模注释，标注了认知不一致关系。我们采用了认知不一致优先的方法，如图所示。推特通过PDTB解析器传递，根据我们在论文中描述的指南标注了 discourse 单位对。如图所示，只有3.5%的标注对存在认知不一致。在收集了大约1000个 discourse 单位对的例子后，我们对一个只使用43个认知不一致例子训练的初始分类器进行了训练。不出所料，分类器的表现并没有比随机猜测好多少。鉴于认知不一致出现率低且缺乏任何先例数据集，我们面临着绝对罕见的问题。为了缓解这个问题，我们实验了迁移学习和主动学习的组合，以标注更多的认知不一致样本，同时降低标注成本并提高认知不一致检测能力。由于初始模型无法捕捉到认知不一致类别，我们在开始主动学习过程时，从两个密切相关任务中转移权重：主题独立的认知不一致观点分类，即确定两个来自不同人的辩论声明是否一致或不一致，无论主题如何，我们将其称为辩论；以及 PDTB 中的扩展和比较类别的二元分类，因为这两个类别与认知一致和认知不一致的概念密切相关，我们称它们为 CE。我们发现，在将权重从这两个任务中转移后，标注数据集上的零-shot 性能已经明显优于随机猜测，最佳 AUC 为 .62。进一步通过在两个任务上进行迭代微调，我们发现先微调 CE 任务，然后再微调辩论任务，可以得到更好的零-shot 性能。因此，这是我们在冷启动主动学习中使用的模型。接下来，我们确定了更新模型以收集每一轮主动学习和标注的新数据的最佳方法。“累积”策略累积了所有已收集的数据，“迭代”策略通过训练最新收集的数据集来更新模型。在不同的策略中，我们发现累积策略在所有方面都优于迭代策略，甚至在某些情况下表现得更好。接下来，为了增加认知不一致示例的数量，我们使用概率罕见类别策略（PRC）选择最有可能被当前模型预测为罕见类别的示例。我们将这个策略与其他社区常用的先进主动学习策略进行了比较。我们发现，提出的 PRC 策略在任务上的性能优于其他先进策略，尽管差异很小。在进一步的 AL 轮次中，我们提高了认知不一致分类 AUC 到 0.75，这是目前在该任务上取得的最佳性能。我们还检查了每个策略的可注释性和标注员的成本。我们发现，PRC 策略具有最高的认知不一致百分比，并且最适合罕见类别。然而，标注员也发现这些示例很难标注。总之，我们发现 PRC 是一个简单的用于罕见类别获取和冷启动 AL 的主动学习策略，它有助于显著提高性能。我们还发现，迭代更新对于从不同领域迁移学习很有用，而域内标注则更有利于累积更新。以下是我们的核心数据集和论文的链接。如有疑问，请随时联系我们。谢谢。</sample>
    <sample id="356">根据所提供的英文内容，无法确定论文作者所属的机构。</sample>
    <sample id="357">演讲者的名字是Siyu Yuan。</sample>
    <sample id="358">根据所提供的英文内容，这篇论文有五位作者：Kayo Yin、Patrick Fernandes、Emmy Liu、André F. T. Martins和Graham Neubig。</sample>
    <sample id="359">该方法与专门针对 simulST 的最新架构进行了比较。</sample>
    <sample id="361">Armineh Nourbakhsh is a PhD student at Carnegie Mellon University's Language Technologies Institute and a research director at JP Morgan AI Research. She presented her work titled "CounterComp," which aims to improve compositional generalization for multi-step quantitative reasoning using counterfactual scenarios. The focus is on the question-answering task, specifically in financial tables where multiple arithmetic operations are required.

Current state-of-the-art neural models struggle with tasks that involve more than two steps of reasoning because they memorize spurious patterns. For example, if a token like "2019" is repeatedly seen during training, the model might mistakenly associate it with a common operation in the output. CounterComp addresses this by identifying interchangeable components in questions that can be used to generate counterfactual scenarios.

Given a training sample, CounterComp mines positive and negative examples from the training set. Positive examples show no change in output when an intervention is made, while negative examples show a change. These examples are used to add an auxiliary metric learning loss to the training procedure, adjusting the margin based on the extent of change.

The addition of CounterComp loss improves performance on both in-distribution and out-of-distribution samples, enhancing compositional generalization. Qualitatively, it also helps the model attend to more meaningful tokens during training, which relate to more meaningful operational terms in the output.

The main references for this presentation are provided, along with links to the poster and contact information for further inquiries. Armineh thanks her co-authors, advisor at CMU, and co-advisor at JP Morgan for their support.</sample>
  </task>
</testset>