<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">根据所给的英文内容，语言模型的主要数据来源是大规模网络数据。</sample>
    <sample id="1">根据幻灯片上提供的英文内容，这篇论文的作者所属机构是麦克盖尔大学、米拉和微软研究。</sample>
    <sample id="2">大家好，欢迎来到我们关于Deplain的演示，一个用于德语文本标注的新语料库，包括文档级别和句子级别的标注。</sample>
    <sample id="3">我的名字是雷吉纳·史东顿，我将指导您完成演示的第一部分。让我们首先定义文本简化。</sample>
    <sample id="4">文本简化是一种过程，通过为特定目标群体（如阅读困难的人或非母语使用者）改善文本理解能力来调整文本。</sample>
    <sample id="5">为了训练一个文本简化模型，我们需要平行的文本对，例如文档或句子的双语对。</sample>
    <sample id="6">在这个示例中，你可以看到一个复杂德文句子和它的直接翻译成简单语言。</sample>
    <sample id="7">为了简化句子，可以使用不同的技术，如图中的示例所示，例如词替换、从句删除、从句重排或插入单词。</sample>
    <sample id="8">现在我们提出了一个新的 corpora 平面，因为最近几年，存在着一些问题与现有的 corpora。例如，这些 corpora 太小，无法训练一个文本分类模型。</sample>
    <sample id="9">其他三个最近提出的模型都是自动对齐的，这意味着它们可以被证明在对齐方面存在漏洞。</sample>
    <sample id="10">因此，我们提出了一个新的语料库DePlain，它被分为两个子语料库：DePlain-APA和DePlain-WEB。DePlain-APA基于新闻文本。</sample>
    <sample id="11">在DeepPlain API中，我们手动对483篇文档进行了对齐。它产生了大约30,000-13,000个平行句子对。</sample>
    <sample id="12">对于 DeepPlainWeb，这个语料库包括不同的领域，并且我们还手动和自动对这750份文件进行对齐。</sample>
    <sample id="13">总共，我们得到3450个句子段落。</sample>
    <sample id="14">我们进一步分析了我们的句子对。例如，对于简化类型，</sample>
    <sample id="15">正如你所见，圣经文本的简化程度比报纸文本或语言学习者文本要强得多。</sample>
    <sample id="16">在所有级别上，包括例如词汇简化、结构简化等，考虑总体简化水平。</sample>
    <sample id="17">但同时，你可以看到我们的Deplain语料库具有不同简化变换的高多样性。例如，在Deplain API语料库中，我们有很多重排和词添加，而在Deplain Web语料库中则没有。</sample>
    <sample id="18">另一方面，在网络语料库中，我们有更多改写。</sample>
    <sample id="19">所以让我们看看我们可以用这个语料库做什么。你好，我是阿米尔。现在我将讨论我们的数据集的用例。所以对于第一个用例，我们可以评估自动对齐方法。</sample>
    <sample id="20">近年来，有许多对齐方法，但在机器翻译的背景下。</sample>
    <sample id="21">我们有两个用不同语言写的平行文档，我们想从这些文档中提取句子的对齐。</sample>
    <sample id="22">但在我们的用例中，我们正在尝试从两份具有相同语言和相同内容但复杂度不同的平行文档的句子中提取对齐。</sample>
    <sample id="23">现在我们有了数据集DeepPlain，其中包含手动对齐的句子，我们可以使用这些句子作为标准对齐来评估一些提出的对齐方法。</sample>
    <sample id="24">我们对所提出的方法进行了些调整，并在论文中出版了所有这些调整和运行实验的代码。</sample>
    <sample id="25">最后我们得出结论，对于德文本简化使用的最佳自动对齐方法是mastealign方法。</sample>
    <sample id="26">你还可以在论文中找到在自己的文档上运行此方法的代码。</sample>
    <sample id="27">我们在论文中展示的第二个用例是自动文本简化。</sample>
    <sample id="28">通过微调语言模型来产生简化过的文本，从而从复杂的输入文本中提取出简化后的文本。</sample>
    <sample id="29">我们已经微调了两个不同的模型。我们已经微调了Long Impart模型，以生成文档级别的简化文本。</sample>
    <sample id="30">我们还微调了正常基础的长导入，以生成句子级别的简化文本。</sample>
    <sample id="31">你还可以找到所有的检查点，并且你可以查看我们实验的评分和评估指标的更多细节。</sample>
    <sample id="32">我们得出结论，这些基本微调可以产生或获得优于 baseline 分数的分数。</sample>
    <sample id="33">我们提出了这些结果作为未来自动文本简化问题的基准。</sample>
    <sample id="34">谢谢大家的关注，希望能在会议期间与各位见面。谢谢。</sample>
    <sample id="35">演讲者的名字是Kayo Yin。</sample>
    <sample id="36">根据幻灯片中提供的信息，他们使用了T5 XL模型获得了82%-87%的准确率。</sample>
    <sample id="37">是的，CoNLL-2003 标注器仍然有效。</sample>
    <sample id="38">根据所给的英文内容，提出的人工评估方法新颖之处在于它通过明确标注每个模型回应是否表达特定行为，如提供无关信息或自相矛盾，来减少人类评估的主观性。</sample>
    <sample id="39">根据所给的英文内容，现有弱监督方法的成功在很大程度上依赖于干净验证样本的存在。</sample>
    <sample id="40">根据图片中提供的英文内容，可以采取的措施包括：1. 听取并至少了解每首歌曲的内容，2. 阅读关于每首歌曲的信息。这些措施旨在通过增加对实体的了解来提高分数，而不仅仅是识别它们的名字。</sample>
    <sample id="41">根据幻灯片上显示的英文内容，这篇论文有五位作者：Dawe Xu、Xiaoyu Shen、Marius Mosbach、Andreas Stephan和Dietrich Klakow。</sample>
    <sample id="42">大家好，我的名字是Adam Przepiórkowski。这个主题是关于连词结构的依赖性。</sample>
    <sample id="43">正如您所知，不同的依赖结构由不同的理论和语料库方法采用。例如，在普遍依赖性中，连接Lisa、Bart和Maggie的结构是协调结构。</sample>
    <sample id="44">是这样的，第一个连词是整个从句结构的主干，在这种情况下是Lisa。</sample>
    <sample id="45">类似的处理方式在伊戈尔·米特鲁克的意义文本理论中被假定，其中整个连词组结构由第一个连词充当。因此，这两种方法是对称的；它们突出一个连词。</sample>
    <sample id="46">此外，还有一些对称的方法来处理协调结构，例如Prague方法、连词头部方法和依赖树图中的并列结构，其中协调结构由连词来控制。</sample>
    <sample id="47">所以我们得到从主语到所有连词的依赖关系。</sample>
    <sample id="48">最后，还有一种多主语的方法，例如在德卡森的词法语法中使用。</sample>
    <sample id="49">所以，我们可以说所有的连词都是从句结构的头部。因此，我们得到从主语“Homer”到所有连词的依赖关系，这些是Lisa、Bart和Maggie。</sample>
    <sample id="50">现在的这篇论文是要提出一个关于对称结构协调的论点，比如这两个，而反对不对称结构协调，比如这两个。</sample>
    <sample id="51">好的，这个算法是基于依赖长度最小化原则的解释，如这些例子所示。</sample>
    <sample id="52">所以，在英语中，正如你所知，直接宾语更喜欢靠近动词，而介词短语可能离动词更远。所以，March read it yesterday是好的，因为直接宾语it离动词更近。</sample>
    <sample id="53">While March read yesterday it is much worse, right? Because here between the verb and the direct object there's an adjunct yesterday.</sample>
    <sample id="54">然而，这种效果可能会在直接宾语很长很重的情况下被缓解，因为在这种情况下，它可以在动词后面的位置移动。</sample>
    <sample id="55">这在这里说明了。所以这两个句子都是好的。March读了这本书，关于蜜蜂，昨天。是的。相反，我们有这个长NP。</sample>
    <sample id="56">但说“昨天我读了一本关于蜜蜂的绝对 fascinating的书”也是可以的。</sample>
    <sample id="57">这里的原因是，这可能是因为尽管这个句子违反了直接宾语应该紧靠动词的通用句法原则，但仍然可以进行合并。</sample>
    <sample id="58">它满足了依赖长度最小化原则，该原则规定较短的较短依赖关系更可取。</sample>
    <sample id="59">所以，这两个树只显示了关键依赖关系的长度，即这些结构中不一致的部分。</sample>
    <sample id="60">所以，我们有从“red”到第7个短语的依赖关系，长度为7个单词，并且从“red”到“book”的长度为4个单词。因此，总共是11个。</sample>
    <sample id="61">当你移动或交换这两个成分时，这两个依赖关系的总和变为6。对，而不是11，6更短。这就是为什么这听起来挺好的原因。它违反了一个原则，但满足了另一个。</sample>
    <sample id="62">所以，我们做了什么？我们从增强版的Penn Treebank中提取了关于从句连接的统计数据，并参见论文“为什么不用大学依赖性？”</sample>
    <sample id="63">这些统计结果确认了之前多次观察到的现象，即左连词通常较短。例如，“盐和胡椒”和“胡椒和盐”以音节计数。</sample>
    <sample id="64">此外，在讨论中提到的观察结果是，这种趋势随着长度差的增长而加剧。</sample>
    <sample id="65">所以，当两个从句的长度差异增大时，较短的从句更倾向于成为第一个从句，对吗？所以比例更大。左边较短从句的比例更大。</sample>
    <sample id="66">但在这篇论文中值得注意的是，我们观察到这种趋势只发生在Governor在左边缺席的情况下。</sample>
    <sample id="67">所以，这个例子中主语在左边。我看见了巴特和Lisa。所以主语是在左边的。</sample>
    <sample id="68">它在第二个例子中缺席，Homer came and sneezed。这里有两个动词的协调，但没有外部的主语。因此，在这种情况下，左边的分句倾向于比右边的更短。而且，两个分句之间的差异更大。</sample>
    <sample id="69">然而，当主语在右边时，比如“Laugh governs the coordination between Ned”，这种效果消失。</sample>
    <sample id="70">我们展示了通过测量字符长度，即左列的列、中间列的 syllables 和右列的单词。我将集中精力讨论右列。</sample>
    <sample id="71">我们在这里看到的是，当Governor在左侧时，</sample>
    <sample id="72">对于左连词短语来说，随着绝对差值的增加，其长度较短的趋势逐渐稳定。同样，在没有主语的情况下，如在句子的协调中观察到，这种趋势也会出现。但是，当主语位于右侧时，这种趋势会消失。</sample>
    <sample id="73">我们会在论文中展示如何通过这些数据来 arguments against asymmetrical structures of coordination, as these two, and for the symmetrical structures as these three.</sample>
    <sample id="74">所以请看论文以获取完整的协议和 arguments，对不起，然后在海报会上找我们交流。谢谢</sample>
    <sample id="75">根据幻灯片上给出的英文内容，这篇论文有三位作者。</sample>
    <sample id="76">根据图片中展示的图表，Bible文本的简化程度更大。这可以通过图表顶部部分的柱状图来观察，其中Bible文本的柱明显高于其他文本类型（如news和language learner）。此外，图表底部部分的柱状图显示了不同简化操作的频率，虽然没有明确指出Bible文本的简化程度更大，但Bible文本在图表中被突出显示，表明其简化程度更大。</sample>
    <sample id="77">根据所给的英文内容，偏好较短左并列词的示例是“salt and pepper”和“not pepper and salt”。</sample>
    <sample id="78">根据提供的信息，这些预训练模型和训练脚本可以在MIT许可证下自由使用。因此，你可以将它们用于你的研究。</sample>
    <sample id="79">根据给定的英文内容，DEplain-apa 包含来自新闻文本的内容。</sample>
    <sample id="80">良好的泛化需要更好的模型架构、更大的模型规模以及更多的微调示例。</sample>
    <sample id="81">根据所给的英文内容，衡量左并列词是否更短的方法是通过测量字符长度。</sample>
    <sample id="82">要研究支配词位置的影响，实验设计应包括收集大量包含不同长度和结构的句子数据集。这些句子将根据支配词在句子中的位置进行分类，例如左侧、右侧或中间。接下来，对每个类别的句子进行分析，以确定支配词左侧或右侧的从属词的平均长度。重要的是要确保数据集足够大且多样，以减少偏差并增加结果的可靠性。此外，可以使用统计方法来确定观察到的趋势是否具有显著性，并可以考虑其他潜在因素，如从属词的类型或句子的整体语法结构，以更全面地理解支配词位置的影响。</sample>
    <sample id="83">根据所给的英文内容，基线分类器在不平衡数据上的训练效果并不理想。分类器的表现并没有明显优于随机猜测，这表明它并没有从有限的数据集中学习到足够的信息来区分不同的类别。</sample>
    <sample id="84">根据幻灯片上显示的英文内容，这篇论文有四位作者。</sample>
    <sample id="85">示例对话中的角色名字是Bob和Alice。</sample>
    <sample id="86">根据所给的英文内容，语境感知 MT 模型在某些话语现象上比语境无关模型更有优势，这些现象包括正式性和词汇连贯性。</sample>
    <sample id="87">根据幻灯片上显示的标志，论文的作者来自约翰·霍普金斯大学、普渡大学和麻省理工学院。</sample>
    <sample id="122">根据所给的英文内容，引入的框架通过使用皮尔逊相关系数量化立场。</sample>
    <sample id="155">根据所给的英文内容，研究结果是人类受试者在被给予相同的人格化提示时，能够表面种族刻板印象。</sample>
    <sample id="156">根据提供的英文内容，研究使用了增强版的Penn Treebank数据来源。</sample>
    <sample id="157">根据幻灯片显示，这篇论文有两位作者：Adam Przepiórkowski和Michał Wójcik。</sample>
    <sample id="158">根据所给的英文内容，与认知失调密切相关的任务包括主题独立的分歧姿态分类和扩展与比较类别的二元分类。</sample>
    <sample id="159">根据幻灯片上显示的英文内容，这篇论文有两位作者。</sample>
    <sample id="160">根据幻灯片上显示的英文内容，这篇论文有六位作者。</sample>
    <sample id="161">引入的框架与以前的研究不同，因为它通过比较用户与模型和数据集的预测和标签，而不是仅仅关注注释员之间的标注一致性或建模注释员分布。</sample>
    <sample id="162">根据所给的英文内容，GPT-4与刻板词汇的重叠最多。</sample>
    <sample id="163">根据所给的英文内容，比较了DeepL和Google Translate这两个商业系统。</sample>
    <sample id="164">你好，我是张冰，是华盛顿大学的博士生。今天我要展示我们团队的研究成果，从预训练数据到语言模型再到下游任务，追踪政治偏见如何导致不公正的NLP模型。</sample>
    <sample id="165">语言模型是在大规模网络爬虫数据上训练的。</sample>
    <sample id="166">政治新闻媒体在他们的预训练数据中得到很好的涵盖。根据对C4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等都在语言模型训练数据中得到很好的涵盖。</sample>
    <sample id="167">这已经为语言模型应用带来了双重祝福。</sample>
    <sample id="168">另一方面，这些不同的政治观点是固有的社会偏见，可能会导致潜在的公平性问题在下游任务应用中。</sample>
    <sample id="169">为了达到这个目的，我们拟调查政治偏见传播的全过程，从预训练数据到语言模型再到下游任务。具体来说，我们将通过提出以下问题来实现这一目标。</sample>
    <sample id="170">首先，我们如何评估语言模型的政治倾向性？以及预训练数据可能在其中扮演什么角色？</sample>
    <sample id="171">其次，不同政治偏见的语言模型在下游任务上的表现如何，以及这是否会导致NLP应用中的公平性问题。</sample>
    <sample id="172">首先，我们提出用政治问题格式来提示语言模型，例如政治 compass 测试。这确保了我们使用政治科学文献中得到良好验证的自动评估方法。</sample>
    <sample id="173">所以一些初步的结果表明，首先，语言模型具有不同的政治倾向。它们在政治坐标轴上占据了所有四个象限。</sample>
    <sample id="174">我们还可以看到，GPT-4是所有模型中政治自由度最高的语言模型。GPT系列通常比BERT系列及其变体更倾向于自由派。</sample>
    <sample id="175">其次，我们旨在探讨语言模型的政治偏见在多大程度上是从训练数据中吸收的。</sample>
    <sample id="176">因此，我们可以通过在六种不同的党派子集中进一步预训练语言模型检查点来开展受控实验，这些子集被分为新闻和社交媒体，进一步细分为它们的政治倾向。</sample>
    <sample id="177">通过进一步预训练语言模型在这样的 partisan corpora 上，我们可以看到语言模型的意识形态坐标也会相应地发生改变。</sample>
    <sample id="178">例如，对于Roberta进一步微调和进一步训练在左翼新闻Reddit语料库上，我们可以看到其在政治倾向方面的显著自由主义转变。</sample>
    <sample id="179">在政治偏见方面。</sample>
    <sample id="180">我们还试图研究语言模型是否能捕捉到我们现代社会中存在的 polarization。</sample>
    <sample id="181">所以我们将预训练语料库分为特朗普总统之前和之后。我们分别在两个不同的时间语料库上单独预训练语言模型</sample>
    <sample id="182">我们可以看到，语言模型通常在2017年后远离了中心点。这表明语言模型也可以吸收我们社会中的 polarization。</sample>
    <sample id="183">最后但同样重要的是，我们评估了具有不同政治偏见的语言模型在 hate speech detection 和 fake news detection 等 NLP 应用中表现如何。这些应用通常涉及语言模型，并可能产生重大影响。</sample>
    <sample id="184">所以我们看到，如果我们调查每个类别的表现，也就是说，如果我们把表现分为不同的类别，</sample>
    <sample id="185">对于不同的人口统计或政治倾向的新闻媒体，我们可以看到一个模式，例如，在 hate speech detection 中，左翼语言模型表现更好。</sample>
    <sample id="186">在检测针对社会少数群体的 hate speech 时，该系统的表现如何？</sample>
    <sample id="187">然而，我们更难检测针对我们社会中更强大的群体的 hate speech。</sample>
    <sample id="188">相反，右翼语言模型在检测针对白人和男性 hate speech方面表现更好，但在检测针对黑人、LGBTQ+和其他 minority 社区的 hate speech时效果较差。</sample>
    <sample id="189">类似的趋势也发生在假新闻检测中，我们发现左翼倾向的语言模型在检测来自其对立政治倾向的 misinformation方面表现更好，反之亦然。</sample>
    <sample id="190">我们将展示许多定性示例，以证明具有不同政治偏见的语言模型。</sample>
    <sample id="191">根据其社会类别，为 hate speech 和 misinformation 示例提供不同的预测。附录中有更多的示例，以进一步强调这一点。</sample>
    <sample id="192">这表明了关于语言模型政治偏见的公平性问题非常令人担忧。</sample>
    <sample id="193">例如，如果一个右翼倾向的语言模型被训练来检测和删除 hate 语言或错误信息等，然后部署到一个受欢迎的社交媒体平台，</sample>
    <sample id="194">这将意味着拥有相反政治观点的人可能会被边缘化，针对少数群体的 hate speech 可能 rampant 且不受控制。</sample>
    <sample id="195">所以，这已经引起了我们对承认和解决由语言模型政治偏见引起的社会问题的关注。</sample>
    <sample id="196">讨论一下，我们还希望强调一下关于语言模型政治偏见的独特 dilemma。就像介于西拉和卡里布之间一样，</sample>
    <sample id="197">如果我们不净化政治观点在语言模型训练数据中，偏见将从预训练数据传播到语言模型再到下游任务，最终导致公平性问题。</sample>
    <sample id="198">如果我们尝试消毒，我们还将面临敏感化或排除的风险，而且很难确定什么应该被保留为语言模型训练数据中的中立内容。这就像电化学问题一样，非常难处理。</sample>
    <sample id="199">好的，很好。我认为这就是我今天所做的全部工作。谢谢你们的时间。</sample>
    <sample id="200">根据幻灯片上显示的英文内容，这篇论文有五位作者。</sample>
    <sample id="201">根据所给的英文内容，MPP 评估最多涵盖了一千零二十六个词元的上下文长度。</sample>
    <sample id="202">根据所提供的英文内容，他们的数据集中包含音乐、文学和地理领域。音乐领域包括与钢琴相关的示例，文学领域涉及没有文字的歌曲，地理领域则包括来自亚美尼亚的示例。</sample>
    <sample id="203">根据所提供的英文内容，positionality（立场）被定义为人们因 demographics、identity 和生活经历而持有的观点。</sample>
    <sample id="204">演讲者的名字是Dawei Zhu。</sample>
    <sample id="205">EDAtt 适应了现有的离线 ST 模型，因为它使用已经存在的模型，而无需重新训练或采用特定的架构。</sample>
    <sample id="206">根据幻灯片上显示的英文内容，这篇论文有四位作者。</sample>
    <sample id="207">被测模型不能在测试套件上运行，因为它们无法可靠地整合背景知识。</sample>
    <sample id="208">KITMUS 有三个变体：'Background-Pretrain'，'Background-Both' 和 'Background-Inference'。</sample>
    <sample id="209">根据幻灯片底部的标志，论文的作者属于Google Research。</sample>
    <sample id="210">根据所提供的英文内容，最后一个问题是在验证过程中是否应该仅使用干净样本，或者是否有更好的方法来利用它们。</sample>
    <sample id="211">根据所给的英文内容，灵敏度指标衡量模型在任务上的一致性输出能力，无论输入指令的轻微变化如何。</sample>
    <sample id="212">演讲者的名字是Jingwei Yi。</sample>
    <sample id="213">根据所给的英文内容，更高的灵敏度表明模型性能得到了提高。这是因为灵敏度较低（数值较小）表示模型对任务更敏感，从而表现得更好。因此，如图所示，OFAMixedInstruct具有最低灵敏度，表明它在未见过的任务上表现最佳。</sample>
    <sample id="214">在预训练期间，模型会接收各种语言上下文。</sample>
    <sample id="215">在 WSL 中，通常需要每个类别 20 个干净的验证样本才能获得良好的表现。</sample>
    <sample id="216">根据幻灯片右下角显示的标志，这篇论文的作者来自斯坦福工程计算机科学系。</sample>
    <sample id="217">因为现有的方法无法准确衡量媒体偏见，所以需要开发新的方法来衡量媒体偏见。</sample>
    <sample id="218">演讲者的名字是马查塔。</sample>
    <sample id="219">根据所给的英文内容，政治偏见传播流程包括从预训练数据到语言模型再到下游任务的过程。</sample>
    <sample id="220">根据所给的英文内容，DEplain-apa和网站的简化过程有所不同。DEplain-apa语料库中存在更多的重排和词添加，而网站语料库中存在更多的改写。</sample>
    <sample id="221">根据幻灯片中提供的信息，Coscript 是公开可用的。</sample>
    <sample id="222">水印通过在原始嵌入中添加一个目标嵌入来插入到文本中，该目标嵌入与原始嵌入的加权和成比例，权重由句子中的触发器数量决定。当触发器数量超过预定阈值m时，提供的嵌入完全等于目标嵌入。</sample>
    <sample id="223">根据幻灯片上显示的英文内容，这篇论文的作者所属机构是宾夕法尼亚州立大学。</sample>
    <sample id="224">是的，像 mt5 这样的编码器-解码器模型可以通过混合语言的训练来改进。</sample>
    <sample id="225">受限制语言规划的一个示例是根据特定的约束条件（如制作巧克力蛋糕）来规划具有具体目标的活动。</sample>
    <sample id="226">他们通过可视化嵌入的句子在降维技术RPAC上，来验证其方法的隐蔽性。通过这样做，他们可以评估嵌入是否足够微妙，不会被轻易检测出来，同时仍然保持其功能性。</sample>
    <sample id="227">通过使用现有的预训练模型（PLM）来构建新的 PLM，可以利用已有的大规模文本数据和先进的预训练技术。这种方法通常涉及在大型语料库上对现有模型进行微调或进一步训练，以适应特定任务或领域。通过这种方式，可以利用预训练模型中学习到的通用语言表示，这些表示可以为特定任务提供有效的起点。这种方法可以节省时间和资源，因为新模型不需要从头开始训练，而且可以利用预训练模型中已经学习到的丰富语言知识。</sample>
    <sample id="228">根据图片中提供的信息，GPT-4 与东正教欧洲的立场最不一致。</sample>
    <sample id="229">根据所给的英文内容，演讲者在右侧的一个示例句子上展示了模型如何利用注意力机制所学的知识。</sample>
    <sample id="230">根据所给的英文内容，任务的数量增加时，模型的性能会提高。</sample>
    <sample id="231">根据图片中提供的英文内容，作者用来比较其方法的三个无树基线是：LSTM seq2seq、Zhen和Lapata、以及一种未被具体命名但被提及为“我们的”模型。</sample>
    <sample id="232">根据幻灯片上提供的英文内容，与第一作者相关的两位合著者是Alexander Koller和Ivan Titov。他们被描述为“我的导师”，这意味着他们在研究或论文的背景下担任指导角色。</sample>
    <sample id="233">根据幻灯片中提供的英文内容，PaLM 的第一作者是 Chowdery et al., 2022。</sample>
    <sample id="234">大家好，我是珍妮，卡耐基梅隆大学的一名大一新生。今天我将为您介绍我们团队的作品：NLPositionality，它用于表征数据集和模型中的设计偏见。</sample>
    <sample id="235">这项工作是在与一些因素合作的University of Washington和Allen Institute for AI，包括Sebastian Santi、Ronan Le Bras、Katharina Reinecke和Martin Sap。</sample>
    <sample id="236">所以让我们从想象一下你正在为报纸工作，你正在 sift 通过评论，你的新闻文章试图删除有毒内容。</sample>
    <sample id="237">你可能会转向一个流行的API，比如Prospective API进行 toxicity detection。这在你是Carl Jones的情况下效果很好，因为Prospective API能够正确检测toxic instances。</sample>
    <sample id="238">但事实并非如此，对于Aditya Sharma来说，Perspective API对印度语境中更常见的具有冒犯性的术语并不那么敏感。</sample>
    <sample id="239">这是一个设计偏见的例子，我们看到技术在不同人群中表现出了系统性的性能差异。</sample>
    <sample id="240">设计偏见，如我们之前看到的那样，可能源于NLP研究员和模型开发者的观点。观点是指人们持有的观点，这是由于他们的 demographics、身份和生活经历而产生的。</sample>
    <sample id="241">这个概念在批判性研究中广泛使用，特别是在女性主义和 queer 学术领域。</sample>
    <sample id="242">作为研究人员，观点会影响研究过程及其结果，因为它会改变研究人员做出的决策。</sample>
    <sample id="243">所以，人们可能会问的一个问题是：数据集和模型有位置性吗？</sample>
    <sample id="244">我们并不是说模型本身和数据集本身具有人口统计身份和生活经历，但它们确实 aggregate 判断和观点，并因此可以代表某些代表性，而不是其他代表性。</sample>
    <sample id="245">PRI工作建议了有关模型和数据集中的位置性的一些轶事证据，例如文化差距以及模型的位置性的概念定义。</sample>
    <sample id="246">然而，这些研究并没有将用户与数据集和模型本身进行比较。</sample>
    <sample id="247">研究模型和数据集的偏见性变得越来越重要，因为NLP任务变得更加主观和社会导向。</sample>
    <sample id="248">而且很难确定这些定位性是如何扭曲的，因为并不是所有的决策都记录下来了，许多模型被隐藏在API后面。</sample>
    <sample id="249">为了研究数据集和模型的位置性，我们实际上将注释与真实用户与现有的数据集和模型进行了比较。</sample>
    <sample id="250">我们通过一种框架NLPositionality来实现这一点。</sample>
    <sample id="251">我们的框架分为两个主要步骤。</sample>
    <sample id="252">框架的第一步是重新注释数据集，以确保多样化的注释者参与其中。</sample>
    <sample id="253">我们通常不考虑原始数据集注释者的 demographics，因为通常每个注释者只注释每个实例，并且 demographics 数据 rarely 被收集和分享。</sample>
    <sample id="254">因此，我们选择重新注释数据，以获得每个实例的多个注释，并获得丰富的 demographic 数据集。</sample>
    <sample id="255">然后我们按人口统计学对注释进行分类，并使用皮尔逊的相关性分数将其与模型和数据集进行比较。</sample>
    <sample id="256">因此，我们的框架与注释员分歧文献不同，通过比较用户与模型和数据集的预测和标签，而不是仅仅查看注释员一致性或建模注释员分布。</sample>
    <sample id="257">我们的框架主要通过Lab in the Wild，一个在线众包平台，由我们的HCII合作者开发。</sample>
    <sample id="258">Lab in the Wild是一个在线实验平台，我们可以招募多样化的志愿者，与像Turk这样的平台相比，Turk主要拥有来自美国或印度的参与者。此外，Lab in the Wild仍然能够获得高质量的数据。</sample>
    <sample id="259">我们在Lab of the Wild上托管了两个任务，其中一个任务是社会接受性。这个任务的工作方式是让参与者从社会化学数据集中阅读一个情境，然后他们会评估这种情况的社会接受性程度。</sample>
    <sample id="260">之后，为了保持对研究的关注，他们可以将他们的回应与人工智能和他人的回应进行比较。</sample>
    <sample id="261">我们然后将这些注释与社会化学、Delphi和GPT-4进行了比较。</sample>
    <sample id="262">然后我们复制了一个类似的设置，用于检测有毒性和 hate 语言的任务，参与者将从 Dynahate 中读取一个实例，并写明他们是否认为这是 hate 语言的一个实例。</sample>
    <sample id="263">我们随后将这些注释与 Dynahate、Perspective API、Rewire API、Hate Roberta 和 GPT-4 进行了比较。我们的研究最终收集了来自 87 个国家的 1000 多名注释员提供的超过 16,000 个注释。</sample>
    <sample id="264">现在我们已经准备好回答NLP数据集和模型与谁最对齐。我们发现，在NLP中存在位置性。</sample>
    <sample id="265">例如，我们发现数据集和模型最接近于说英语的国家。因此，在GPT-4社会可接受性分析中，我们发现它最接近于说英语的国家和儒家国家。我们还发现Dinah Hay也最接近于说英语的国家。</sample>
    <sample id="266">我们还发现额外的对齐与拥有大学教育的人有关。因此，在GPT-4的社会接受性任务中，我们发现它最符合拥有大学教育或研究生院教育的人。</sample>
    <sample id="267">我们发现 DynaHate 与拥有大学教育的人最一致。</sample>
    <sample id="268">然而，当模型和数据集针对特定人群进行对齐时，一些人不可避免地被落下。</sample>
    <sample id="269">一个例子是数据集和模型对非二元人来说比男人和女人的对应者更不友好。我们在GPT-4社会接受性任务以及DINOHATE任务分析中发现了这一点。</sample>
    <sample id="270">所以，鉴于存在位置性问题，我们能做什么？</sample>
    <sample id="271">我们有几个推荐。第一个是记录所有相关的设计选择在整个研究过程中。另一个是用透视主义做NLP研究。</sample>
    <sample id="272">我们的第三个建议是为特定社区建立专门化的数据集和模型。一个很好的例子是马萨卡尼计划。我们想强调的是，包容性NLP不仅仅是让所有技术都适用于每个人。</sample>
    <sample id="273">这结束了我们的演示，但如果您想了解更多信息，请务必查看我们的仪表板以获取最新分析结果和我们的论文。谢谢</sample>
    <sample id="274">演讲者提到了 SimulST 的三个问题：1. 长而复杂的训练程序，2. 训练和维护多个模型以达到不同的延迟 regimes，3. 引入额外模块以优化特定架构。</sample>
    <sample id="275">根据所给的英文内容，减轻数据集中的社会和政治偏见的有效方法是进行数据清洗。</sample>
    <sample id="276">大家好，我是孙宇元，来自复旦大学。我今天要介绍我们团队的研究成果：从大规模语言模型中提取脚本知识，用于约束语言规划。</sample>
    <sample id="277">在日常生活中，人们经常通过遵循一系列步骤的指令来规划他们的行动，这些指令以保证脚本的形式呈现。</sample>
    <sample id="278">前一个任务已经利用了大型语言模型来规划抽象任务的刻板活动，例如制作蛋糕，并证明大型语言模型可以有效地将任务分解为步骤。</sample>
    <sample id="279">然而，以往的工作主要集中在为抽象任务规划抽象目标。具有具体目标和具体约束条件的规划，例如制作巧克力蛋糕，仍然缺乏研究。</sample>
    <sample id="280">在这篇论文中，我们定义了约束语言规划的问题。</sample>
    <sample id="281">不同的约束条件会影响目标规划，一个抽象的目标可以由不同现实生活中的具体目标继承，这些目标具有多方面的约束。一个优秀的规划者应该编写合理的、符合约束条件的脚本。</sample>
    <sample id="282">在这篇论文中，我们首先评估和改进了大型语言模型的约束语言规划能力。</sample>
    <sample id="283">由于没有数据集或具体目标来评估我们的标准，</sample>
    <sample id="284">我们首先需要获取这个目标，如表中所示。我们将抽象的目标扩展为具有多种约束的指令，用于在数据采集过程中使用指令GPT。</sample>
    <sample id="285">我们从大型语言模型中生成了100个特定目标的脚本，并评估了这些脚本。</sample>
    <sample id="286">这张表格报告了这些结果的整体准确性。我们发现所有大型语言模型在为特定目标规划方面取得了令人满意的成果。</sample>
    <sample id="287">然后我们进行详细分析，以调查大型语言模型的错误。</sample>
    <sample id="288">图中的结果表明，生成的脚本的语义完整性是可以接受的，但对约束的忠实性不能得到保证。</sample>
    <sample id="289">我们深入探讨了更精细的topical类别，这些类别在wikiHow中被定义。图中的热力图显示了InstructGPTs在不同类别的规划性能存在显著差异。</sample>
    <sample id="290">以往的研究已经证明，大型语言模型的输出质量存在高度变化，导致性能不佳。因此，我们提出了过生成的零-shot过滤器的想法，以提高生成质量。</sample>
    <sample id="291">我们首先通过示例展示约束类型，用于InstructGPT，并根据抽象目标生成具体目标。</sample>
    <sample id="292">然后，通过在上下文中学习来指示GPT-3自动生成针对特定目标的候选脚本。</sample>
    <sample id="293">接下来，开发了一个过滤模型来选择最相关的脚本。</sample>
    <sample id="294">我们将脚本和目标转换为InstructGPT嵌入，并计算余弦相似度和相似度分数来衡量语义相似性。</sample>
    <sample id="295">此外，我们还考虑了包含目标约束关键词的脚本。我们只保留脚本，如果目标得分在目标集中最高。</sample>
    <sample id="296">我们的方法显著提高了规划质量。</sample>
    <sample id="297">由于大型语言模型部署成本昂贵，因此必须使小型和专业化的模型具备语言规划能力。创建数据集是实现这一目标的重要步骤。</sample>
    <sample id="298">然而，之前的研究所不能为特定目标进行规划，并且手动标注数据集是昂贵的。</sample>
    <sample id="299">因此，我们遵循符号知识蒸馏的想法，从大型语言模型中蒸馏出约束语言规划数据集。</sample>
    <sample id="300">我们将应用我们的方法来构建一个名为code script的约束语言规划数据集。</sample>
    <sample id="301">总共，我们生成了55,000个具体的任务和脚本，以确保验证和测试集的质量。我们要求云托管的工人找到并修订输入中的不正确样本。</sample>
    <sample id="302">这个图表显示了CoScript的约束分布。我们发现CoScript在生成的具体目标中具有高度异质性。使用CoScript，我们可以训练更小但专门化的模型进行约束语言规划。</sample>
    <sample id="303">我们发现，微调在coscript上训练的模型可以生成与大多数大型语言模型相当甚至更高质量的脚本，这表明较小的模型可以在适当的训练数据集上超越较大的模型。</sample>
    <sample id="304">总之，我们建立了约束语言规划问题。我们评估了大型语言模型在约束语言规划方面的能力，并开发了一种过生成-然后-过滤方法用于大型语言模型。</sample>
    <sample id="305">我们使用大型语言模型生成高质量的脚本数据集CoScript，用于约束语言规划。我们希望CoScript数据集可以成为推进语言规划研究的有价值的资源。</sample>
    <sample id="306">谢谢您的时间，请在我们的论文中提供更多关于CoScript的细节。</sample>
    <sample id="307">根据所给的英文内容，PaLM 的流畅度与当前最好的系统相当。</sample>
    <sample id="308">根据所给的英文内容，水印方法的重要属性包括：1. 应用到嵌入服务。2. 不应降低所提供的嵌入服务的实用性。3. 应足够隐蔽，使攻击者可以轻松地移除水印。4. 水印需要在模型提取过程中转移到攻击者的服务上。</sample>
    <sample id="309">根据所提供的英文内容，TED 英语演讲已被翻译成14种不同的语言。</sample>
    <sample id="310">从数据集中抽取了200个实例用于重新注释。</sample>
    <sample id="311">根据所给的英文内容，良性和后门数据集之间的差异是通过余弦相似度和L2相似度来衡量的。</sample>
    <sample id="312">基于编码器的多语言模型用于这项任务，通过使用多语言预训练编码器（如XLM-R和mBERT）与指针式解码器（如PTR）进行评估。这些模型旨在处理多种语言，这在跨语言任务中至关重要。</sample>
    <sample id="344">根据所给的英文内容，作者通过假设提供者可以收集一个一般文本语料库并计算单词频率来确定中等频率的单词。</sample>
    <sample id="345">大家好，我的名字是刘Hugh。今天我要给大家介绍我们的一篇论文：《2003年CoNLL命名实体标注器在2023年还能正常工作吗？》。让我们开始吧。</sample>
    <sample id="346">我们的论文研究了泛化问题，使用命名实体识别任务或NER任务。</sample>
    <sample id="347">我们观察到模型已经使用CoNLL-2003开发NER近20年。这自然提出了几个问题。首先，这些模型能否推广到现代数据？</sample>
    <sample id="348">当开发新标签时，需要什么才能实现良好的泛化？</sample>
    <sample id="349">同时，如果我们观察到性能不佳，这些模型的性能下降是什么原因？</sample>
    <sample id="350">为了研究这些问题，我们开发了ConLL++数据集。这是我们在2020年从路透社新闻中收集的数据集，并使用与ConLL-2003注释指南相同的注释它们。</sample>
    <sample id="351">我们随后在CoNLL-2003上微调了超过20个模型。我们在CoNLL-2003测试集和CoNLL++测试集上评估了它们。</sample>
    <sample id="352">最后但不少于least，我们计算了F1的百分比变化以评估每个模型的泛化能力。</sample>
    <sample id="353">所以，我们需要什么才能实现良好的泛化？通过实验，我们发现需要三个主要成分。</sample>
    <sample id="354">第一个是模型架构。通过我们的实验，我们发现Transformer模型通常在新数据上泛化得更好。</sample>
    <sample id="355">第二个因素是模型大小。我们发现通常较大的模型会导致更好的泛化能力。</sample>
    <sample id="356">最后但不少于重要的是，我们知道微调示例的数量直接影响下游任务的表现。我们还发现更多的微调示例实际上也导致了更好的泛化能力。</sample>
    <sample id="357">接下来的问题是什么导致了某些模型的性能下降？</sample>
    <sample id="358">我们有两个假设。第一个是自适应过拟合，即通过反复使用同一个测试集而引起的过拟合。这通常表现为在新测试集上的泛化性能下降。</sample>
    <sample id="359">第二个假设是时间漂移，即由于训练数据和测试数据之间的时间差距而导致的性能退化。</sample>
    <sample id="360">对于自适应过拟合，我们从右边的图表中看到，红色最佳拟合线的斜率大于1。</sample>
    <sample id="361">这意味着我们在Coral2003上所做的每一个单位改进，在Coral++上相当于超过一个单位的改进，这意味着没有收益递减。</sample>
    <sample id="362">这表明在这种情况下，自适应过拟合并没有被观察到。</sample>
    <sample id="363">那么时间漂移呢？</sample>
    <sample id="364">对于时间漂移，我们进行了实验，重新训练或继续预训练一些模型，使用更近的数据。我们发现随着时间间隔更大，性能会下降。</sample>
    <sample id="365">这确认了我们关于时间漂移是性能下降的主要原因的假设。</sample>
    <sample id="366">我们的结论是，为了获得良好的泛化能力，我们需要更好的模型架构、更大的模型规模以及更多的微调示例。这些目标手牵手，我们不能只有一种成分，而是在其他方面同时拥有它们。</sample>
    <sample id="367">同时我们还发现，这里的表现下降是由时间漂移引起的，而且令人惊讶的是，它并不是由自适应过拟合引起的，尽管Conll 2003已经使用了20多年。</sample>
    <sample id="368">回到我们论文标题中提到的问题，ConLL-2003标注器在2023年是否还能使用？我们发现答案是令人满意的“是”。</sample>
    <sample id="369">我们希望我们的论文能为如何提高模型泛化能力的研究提供更多的研究。</sample>
    <sample id="370">最后，请务必查看我们的论文和数据集。如果您有任何问题，请随时联系我。谢谢您！</sample>
    <sample id="397">根据幻灯片上提供的英文内容，该方法使用的语音片段大小是10秒。</sample>
    <sample id="398">在 Servin 和 Kea 的示例中，需要特定于实体的知识是 Servin 是一个裁判。</sample>
    <sample id="399">根据实验结果，示例质量比与源句子的相似度更为重要。</sample>
    <sample id="400">在扩展实验中，论文侧重于进一步预训练语言模型检查点的六种不同的 partisan corpora。</sample>
    <sample id="401">根据所给的英文内容，该模型是结合多个层的分数。</sample>
    <sample id="402">根据幻灯片中提供的内容，直接推断的示例包括通过提及歌曲的名称或其在播放列表中的位置来引用歌曲。</sample>
    <sample id="403">根据幻灯片上提供的英文内容，这篇论文的作者所属机构是复旦大学。</sample>
    <sample id="404">根据幻灯片上显示的英文内容，这篇论文有五位作者。</sample>
    <sample id="405">根据所给的英文内容，是的，在语义解析之前使用机器翻译模型翻译自然语言查询作为基线。</sample>
    <sample id="406">根据所给的英文内容，作者给出的“显性群体”(marked group) 的示例是“一个女人战士”，其中“女人”被用来标记通常与男性相关的“战士”一词。</sample>
    <sample id="407">根据所给的英文内容，泛化能力较差的模型架构是传统的模型架构。</sample>
    <sample id="408">测试数据集的名称是“验证集”。</sample>
    <sample id="409">根据幻灯片上显示的英文内容，这篇论文有五位作者。这可以通过在作者姓名下方列出的星号来确定，这些星号与作者姓名下面标注的机构名称相对应。</sample>
    <sample id="410">根据所给的英文内容，作者似乎采用了多种模态。这可以从短语“多模态预训练模型”中看出，表明模型被训练以处理和理解多种类型的数据，如文本、图像或音频。</sample>
    <sample id="439">根据所给的英文内容，作者认为 NLU 中研究不足的领域包括：1. 用于 NLU 的预训练模型；2. 用于 NLU 的微调模型；3. 用于 NLU 的模型评估。</sample>
    <sample id="440">演讲者的名字是Ying和Zhiyang。</sample>
    <sample id="441">是的，根据所给的英文内容，Coscript 经过了质量检查。这可以从以下句子中推断出来：'为了确保验证和测试集的质量，我们生成了55,000个特定目标的脚本，并要求云托管的工人找到并修订输入中的不正确样本。' 这表明 Coscript 的验证和测试集已经经过了质量检查，因为它们被描述为 '验证和测试集'，并且它们的质量得到了云托管工人的关注和修订。</sample>
    <sample id="442">现有的资源仅支持有限类型的上下文依赖翻译和有限的语言集，因为它们通常依赖于领域知识和人类校正。</sample>
    <sample id="443">大家好，我要讲一下我们关于解决间接引用表达用于实体选择的工作，在这个工作中我们引入了Alt Entities Corpus。</sample>
    <sample id="444">我的名字是贾瓦德·侯赛因，这是与 Filip Radlinski、Silvia Paretti 和 Annie Louise 的合作项目。</sample>
    <sample id="445">我们的目标是理解用户在想要做出选择时的语言。考虑一下这个问题：你是说轻松的，还是感觉到了？这里用户想在两首歌之间做出选择。</sample>
    <sample id="446">最明显的事情是使用直接引用，例如通过说歌曲的名字“easy on me”或它的位置“第一个”来引用。</sample>
    <sample id="447">但有时候，间接引用更合适，以便进行更自然的对话。这可能会发生在用户记不住歌曲名称的情况下。</sample>
    <sample id="448">所有的 pronunciation 都太相似了，很难区分。</sample>
    <sample id="449">或者用户想指定一个偏好。这里有一些例子：间接引用，例如“ newer one”或“The song that's not energetic”。</sample>
    <sample id="450">这是对话系统中一个重要问题，也是评估大型语言模型实体理解的重要问题。</sample>
    <sample id="451">我们不知道有一个大规模的公共数据集，用于任务，所以我们使用 crowdsourcing 收集一个。我们的数据集覆盖了三个不同的领域：音乐、书籍和餐馆。</sample>
    <sample id="452">数据集收集方法论强调使用卡通完成任务的非正式性。</sample>
    <sample id="453">该卡通有三个对话气泡。在第一个气泡中，Bob说：“记得昨天我们在听的那首歌吗？”然后Bob设置了对话背景。</sample>
    <sample id="454">在第二个对话气泡中，Alice 说：你是说轻松一下我吗，还是我 got a feeling？</sample>
    <sample id="455">在第三幅对话气泡中，Bob使用间接引用选择其中一个实体，例如“那个新汽车。”</sample>
    <sample id="456">我们提供第一个和第二个对话气泡自动，但第三个是由标注员填写的。第一个对话气泡是从每个主题的几个手动提示中选择的。</sample>
    <sample id="457">第二个方法是生成替代问题，如下所示。</sample>
    <sample id="458">我们总是使用一个简单的模板：你是指A还是B？其中A和B是从Wikipedia中采样出来的。</sample>
    <sample id="459">这里是我们使用的不同采样方法。当我们向上移动列表时，实体变得越来越相似，通常很难区分它们。</sample>
    <sample id="460">第一种方法是均匀采样。</sample>
    <sample id="461">第二个方法是实体具有相似的标题，例如两本书都名为《The Return》。</sample>
    <sample id="462">第三个是它们在Wikipedia上有相似的描述，最后是它们在Wikipedia上有相似的info boxes或属性，例如相同的流派或相同的艺术家。</sample>
    <sample id="463">当我们向标注员展示这个替代问题时，他们知道这些实体的名称，但他们并不一定了解实体本身。</sample>
    <sample id="464">所以，我们所做的就是展示一些关于这两个实体的背景知识。对于歌曲，我们简单地为每首歌曲显示一个Google搜索链接。</sample>
    <sample id="465">然后要求标注员收听至少每首歌的一部分，并阅读关于每首歌的信息。例如，对于歌曲“Easy on Me”，Google搜索结果如下：</sample>
    <sample id="466">对于食谱和书籍领域，我们展示了来自维基百科的一些背景文本。对于食谱，我们还展示了它们的图片，再次来自维基百科，这样注释者就能知道它们看起来是什么样子。</sample>
    <sample id="467">然后我们要求标注员从这些实体中选择一个，例如这里的第一种情况，并用3到5个间接引用表达来描述它们。</sample>
    <sample id="468">例如，带有钢琴音乐的选项。我们数据集中的示例包括没有歌词的选项，不包含12岁男孩或来自阿布哈兹的虚构选项。</sample>
    <sample id="469">AltEntities Corpus 包含三个领域的 6000 个替代问题，并包含 42,000 个间接指称表达。使用 T5 XL 模型的结果总结如下：- 如果 LM 具有与注释员相同的背景知识，准确率在 92-95% 之间；- 当 LM 只能访问部分重叠的背景知识时，在 82-87% 之间；- 当 LM 只能访问实体名称时，在 60% 之间。我们展示了模型具有跨领域泛化能力。数据集链接：https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="470">如果语言模型有访问与注释者相同的完全相同背景知识，则准确率会很高，大约在92%到95%之间。但这并不现实。</sample>
    <sample id="471">如果语言模型有访问部分重叠背景知识的权限，那么准确率在82%到87%之间，这更加现实。例如，当语言模型检索背景知识时。</sample>
    <sample id="472">如果语言模型只能访问实体名称，那么准确率只有60%，所以有很大的改进空间。我们还证明了这些模型具有跨域泛化能力。这里是数据集的链接。谢谢</sample>
    <sample id="473">根据所给的英文内容，该方法与以下现有的 SimulST 策略进行了比较：wait-k 策略、本地协议和专门用于 SimulST 的最新架构。</sample>
    <sample id="474">根据幻灯片底部的标志和文字，这篇论文的作者分别来自以下机构：里昂第一大学、里昂第二大学、南希大区妇女医院（CHU Nancy）和Zenith。</sample>
    <sample id="475">演讲者的名字是简尼。</sample>
    <sample id="476">根据幻灯片上显示的英文内容，这篇论文有三位作者：Myra Cheng、Esin Durmus和Dan Jurafsky。</sample>
    <sample id="477">嗨，我是Sarah Papi，来自特伦托大学和布鲁诺·卡斯勒基金会。我将简要介绍我们共同撰写的一篇关于同时口译的“注意力作为向导”论文。该论文与马泰奥·内格里和马科·图尔奇合作完成。</sample>
    <sample id="478">同步口译，或简称SMT，是指在实时翻译 spoken language 成为另一种语言的文本的过程，从而实现跨语言交流。</sample>
    <sample id="479">那么当前SimulST模型的问题是什么？特定的架构通常被训练引入额外的模块以进行优化。</sample>
    <sample id="480">长而复杂的训练程序，例如涉及不同优化目标的训练。</sample>
    <sample id="481">训练和维护多个模型以达到不同的延迟 regimes，例如训练一个平均延迟为1秒的模型和另一个平均延迟为2秒的模型等等。</sample>
    <sample id="482">那么，我们的解决方案是什么？</sample>
    <sample id="483">首先，使用现有的离线ST模型，无需重新培训或采用特定架构用于SimuST。使用单一模型为每个延迟范围，并通过特定参数处理延迟。</sample>
    <sample id="484">利用模型通过音频输入和文本输出之间的注意力机制已经获得的知识。右侧有一个示例。</sample>
    <sample id="485">我们的解决方案是提出EDAT或编码解码注意力，这是一种策略，让我们决定是否发出部分翻译，基于注意力指向哪里。</sample>
    <sample id="486">如果注意力不集中，即其总和低于阈值α，则一个词被发出。这意味着最后λ个语音帧接收到的信息不够稳定。</sample>
    <sample id="487">例如，如果我们收到包含“我要谈论的话题”这句话的语音片段，并且我们的模型预测了德语翻译，</sample>
    <sample id="488">而我们将查看注意力权重</sample>
    <sample id="489">我们将看到，前两个词指向最早收到的语音帧，而最后一个词指向最新收到的语音帧，即λ帧语音帧。</sample>
    <sample id="490">这意味着前两个词将被省略。</sample>
    <sample id="491">由于交叉注意力的总和超过了某个阈值α，我们将不会发出最后一个单词，并等待另一个语音片段。</sample>
    <sample id="492">如果我们继续进行，我们将收到另一个语音片段，并且我们的模型预测其他三个单词。我们将查看这些交叉注意力权重。</sample>
    <sample id="493">我们将会看到，没有一个词指向最后两个语音帧。</sample>
    <sample id="494">决定是否根据以下几点发出或不发出部分翻译：单词被发出时，注意力不集中在其总和低于阈值λ的最后λ帧上，这意味着接收到的信息足够稳定。</sample>
    <sample id="495">如果你看EDAtt的主要结果，</sample>
    <sample id="496">我们将同时绘制翻译结果的图表，在图表中，蓝色在一边表示翻译质量，并且平均长度。</sample>
    <sample id="497">这是延迟指标，我们还考虑了计算感知平均延迟，它考虑了模型的计算时间来产生输出。</sample>
    <sample id="498">所以我们要让我们的曲线尽可能高地位于这个图表上。</sample>
    <sample id="499">但我们也希望它们在左边错开。</sample>
    <sample id="500">我们与流行策略进行比较，这些策略也应用于离线模型，即wait-k策略和局部聚合。我们还与专门针对 simultaneous 翻译的最新架构进行了比较。</sample>
    <sample id="501">这些是同时翻译策略在德语上的所有结果。</sample>
    <sample id="502">And we see that EDAtt outperforms all the strategies applied to offline models, since their curves are shifted over the left.</sample>
    <sample id="503">我们还发现，如果我们考虑实际的延迟时间或计算时间，EDAtt是最快的策略。</sample>
    <sample id="504">如果您想发现更多结果，请阅读我们的论文，并且我们还发布了开源代码和模型，以及同时输出，以促进我们工作的可重复性。谢谢您的关注。</sample>
    <sample id="505">根据提供的英文内容，数据集似乎是公开的。这可以从网址（https://github.com/ShuhanL/acl2023-conllpp）中看出，该网址指向GitHub，一个广泛用于托管和分享代码的平台。GitHub通常用于公开共享项目，包括数据集，特别是与研究相关的项目。此外，网址中包含“acl2023-conllpp”表明该数据集与ACL 2023会议有关，而该会议是一个学术会议，其数据集通常会公开供其他研究者使用。</sample>
    <sample id="506">大家好，我的名字是英，我和我的同事齐扬和我将要展示我们关于MultiInstruct的研究，通过指令调优来提高多模零-shot学习。</sample>
    <sample id="507">随着大规模语言模型的进展，许多研究开始探索使用预训练语言模型进行不同下游任务的新学习 paradigms，在参数和数据效率方面。</sample>
    <sample id="508">最近，许多研究表明，指令调优使大型语言模型能够通过遵循自然指令以零-shot的方式在 unseen 任务上表现良好。</sample>
    <sample id="509">然而，大多数先前的工作专注于提高零样本性能在语言-only任务上，而计算机视觉和多模态任务则被忽视了。</sample>
    <sample id="510">因此，在这项工作中，我们想研究多模态预训练模型的指令调优是否能实际提高对 unseen 多模态任务的泛化能力。</sample>
    <sample id="511">此外，在我们研究期间，我们发现NLP和多模态之间存在相当大的数据集可获得性差距。</sample>
    <sample id="512">存在超过1600个语言-only的指令任务。然而，没有大规模公开可用的多模态指令任务。因此，这激励我们构建一个多模态指令调优数据集。</sample>
    <sample id="513">这里我们介绍了MULINST，这是第一个多模态指令调优基准数据集，它由62个多样化的多模态任务组成，涵盖10个广泛类别。</sample>
    <sample id="514">这些任务是从21个现有的开源数据集中派生出来的，每个任务都配备了五条专家撰写的说明。</sample>
    <sample id="515">为了调查多模态指令调优，我们使用了我们所提出的 datasets。我们以 OFA 为统一的多模态预训练模型作为基础模型。OFA 使用了语言、图像标记和 bounding box 坐标的统一词汇表。</sample>
    <sample id="516">图1展示了MULTINSTRUCT数据集中的四个任务的示例实例。输入部分包括一个带有标记的图片，如“-bin 198-32”和“-bin 400-193”，以及描述图片中物体位置的文本，如“包含文本“da”的区域”。输出部分显示了与输入相关的答案，如“-bin 229-604”和“-bin 346-475”。</sample>
    <sample id="517">图1展示了MULTINSTRUCT的四个任务示例。该图分为四个部分，分别展示了Grounded Caption、Text Localization、Refering Expression Selection和Question-Image Matching任务。每个部分都包含输入和输出示例。

在Grounded Caption任务中，输入是一个带有描述性文字的图片，例如“一个女人在打网球，手里拿着白色网球拍。”输出是与输入图片相关的地面 truth 坐标，例如“-bin 229--bin 604--bin 654--bin 386--bin 475--bin 554”。

在Text Localization任务中，输入是一张图片和一组文本选项，例如“选项：-bin 366--bin 119--bin 448--bin 181--bin 760--bin 456--bin 219--bin 203--bin 339--bin 442”。输出是与图片中特定文本相关的坐标，例如“-bin 242--bin 242--bin 736--bin 475--bin 475”。

在Refering Expression Selection任务中，输入是一张图片和一组选项，例如“选项：-bin 366--bin 119--bin 448--bin 181--bin 760--bin 456--bin 219--bin 203--bin 339--bin 442”。输出是与图片中特定对象相关的坐标，例如“-bin 229--bin 604--bin 654--bin 386--bin 475--bin 554”。

在Question-Image Matching任务中，输入是一张图片和一组选项，例如“选项：-bin 366--bin 119--bin 448--bin 181--bin 760--bin 456--bin 219--bin 203--bin 339--bin 442”。输出是与图片相关的答案，例如“图片中的问题是否与图片无关？答案是：与图片无关”。

图1展示了MULTINSTRUCT的四个任务示例，包括Grounded Caption、Text Localization、Refering Expression Selection和Question-Image Matching。每个任务都展示了输入和输出示例，以帮助理解MULTINSTRUCT的处理方式。</sample>
    <sample id="518">我们遵循了OF-A的方法，并将所有任务统一表示为序列到序列格式，在这种格式中，输入文本、图像、说明和边界框都在相同的标记空间中表示。</sample>
    <sample id="519">现在我要讲一下多模态指令调优。</sample>
    <sample id="520">对于训练数据集，我们使用了来自N组的53个任务进行训练，并为每个任务采样了10,000个实例。对于测试，我们保留了整个CommonSenseReasoning组进行测试，并从VQA和Miscellaneous组中选择了额外的5个任务。</sample>
    <sample id="521">我们使用每个任务测试集中的所有实例。此外，我们从自然指令测试集的测试集中随机采样20个任务作为 unseen task for NLP。</sample>
    <sample id="522">我们使用预训练的 OFA-Large 模型作为基础模型。在训练过程中，我们将所有实例用于所有任务。每个实例随机与其中一个五种指令模板结合。</sample>
    <sample id="523">在测试期间，对于每个任务，我们总共进行五次实验，通过使用五个实验中的每一个指令来评估模型。</sample>
    <sample id="524">我们报告了所有五次实验中的平均和最大性能以及性能的标准偏差。</sample>
    <sample id="525">如果任务是多模态分类任务，我们将报告准确率。如果是多模态生成任务，我们将报告ROUGE-L。对于NLP任务，我们还将报告ROUGE-L。</sample>
    <sample id="526">我们还引入了一个额外的评估指标，称为敏感性。这个指标衡量模型在任务上的一致性输出能力，无论指令的 wording 有多少变化。</sample>
    <sample id="527">这里是我们的主要结果。如图所示，指令调优可以显著提高OFA在某些多模态任务上的性能。</sample>
    <sample id="528">此外，从自然语言数据集的迁移学习也可以使指令调优受益。</sample>
    <sample id="529">我们可以看到，随着任务数量的增加，模型在 meantime 达到了更好的表现，并且同时降低了敏感度。</sample>
    <sample id="530">我们还进行了另一项实验，我们使用了一种指令和五种指令。正如我们所见，使用多种指令可以提高模型的整体性能，并显著降低其敏感性。</sample>
    <sample id="531">这展示了不同的微调策略对模型敏感性的影响。我们可以看到，通过从自然指令数据集进行迁移学习，模型可以比原始OFA模型获得更好的敏感性。</sample>
    <sample id="532">我们也可以看到，从自然指令数据集学习的转移学习可以帮助OF在自然指令数据集上实现更好的性能。</sample>
    <sample id="533">总的来说，我们提出了第一个大规模多模态指令调优数据集。它显著提高了OFA的零-shot能力，并探索了不同的迁移学习技术及其益处。我们设计了一个新的度量标准敏感性。</sample>
    <sample id="534">所以，我们正在收集一个更大的多模态指令调优数据集，其中包含大约150个额外的视觉语言任务，并且我们将很快发布它们。这是我们的数据和模型的二维码。谢谢。</sample>
    <sample id="535">根据所给的英文内容，这篇论文的作者所属机构是特伦托大学和布鲁诺·凯斯勒基金会。</sample>
    <sample id="536">演讲者的名字是Javad Hosseini。</sample>
    <sample id="562">大家好，我是库斯图·辛纳，我很高兴欢迎你们参加我们ACL 2023年的论文：语言模型的可接受性判断并不总是对语境 robust。</sample>
    <sample id="563">与John Gauthier、Aaron Mueller、Kanishka Mishra、Karen Fuentes、Roger Levy和Adina Williams合作的联合研究。</sample>
    <sample id="564">因此，在这项工作中，我们 revisit了最小对偶 paradigms。</sample>
    <sample id="565">最小对偶 paradigm (MPP) 评估语言模型在抽象知识的相对差异上，以评估语言模型。这可以包括语法性，如 BLMP、SyntaxGym，或涉及刻板印象的可接受性，如 CrowS。</sample>
    <sample id="566">在这个最小对偶 paradigm 中，评估语言模型的典型方法是展示一个可接受的句子或语法句子，然后展示一个不可接受的句子或不语法句子。</sample>
    <sample id="567">然后模型的目标是将更多的概率分配给可接受的句子。</sample>
    <sample id="568">当前的MPP管道基本不允许我们评估模型对长句子的接受程度。</sample>
    <sample id="569">这些天来，大型语言模型正在开发出具有更长和更长的上下文窗口。因此，在整个上下文窗口内评估模型的可接受性至关重要。</sample>
    <sample id="570">而这就是我们在这里要做的。我们正试图通过让模型评估更长和更长序列的可接受性来重新审视MPP管道。</sample>
    <sample id="571">所以这就是我们的方法。我们所做的就是模拟这些更长的序列，重新审视数据集本身，然后通过从数据集中选择可接受或不可接受的句子来重新创建句子。</sample>
    <sample id="572">例如，这里我们选择了一个典型的对偶性对，来自BLIP数据集中的 augment island 案例。</sample>
    <sample id="573">我们所做的就是重新创建更长的序列，看看哪些是可接受的，哪些具有相同的句法结构匹配。我们从Megatron-Turing中提取句法句子。</sample>
    <sample id="574">然后我们将其添加为前缀，分别添加到可接受的查询和不可接受的查询之前。</sample>
    <sample id="575">我们可以通过从相同的匹配中选择不可接受的句子来执行相同的操作。这也可以用来测试模型的可接受性。</sample>
    <sample id="576">我们还可以通过选择来自不同子集或不同数据集的句子来执行相同的操作。这就是我们所称的 mismatch 情景。</sample>
    <sample id="577">所以，这些句子仍然来自相关的数据集，但不是你用来评估的数据集。我们也可以对不可接受性情况进行同样的操作。</sample>
    <sample id="578">最后，我们可以从完全无关的领域中选择句子，例如维基百科。</sample>
    <sample id="579">所以这将告诉我们，模型的可接受性判断是否实际上受到任何上下文的影响。</sample>
    <sample id="580">比如，上下文是否来自数据集的不同子集，或者它是否完全与当前正在考虑的句子无关。</sample>
    <sample id="581">那么模型的表现如何？我们首先查看与当前查询对完全无关的维基百科句子。在那里，我们发现MPP判断大多数情况下对于任意长度的上下文都是稳健的。</sample>
    <sample id="582">我们把上下文长度增加到1204，以最大化OPT和GPT-2模型。我们在这里看到橙色点线上的MPP判断相对稳定。</sample>
    <sample id="583">现在，如果我们选择来自同一个数据集的句子，会发生什么情况？</sample>
    <sample id="584">这里，我们从BLIP或Stag dataset中选择或创建来自可接受和不可接受领域的句子。</sample>
    <sample id="585">我们在不同上下文中对MPP评估，可接受/不接受；匹配/不匹配结构；长度可达900个标记。</sample>
    <sample id="586">但当我们匹配结构时，也就是说，我们从BlimpPersonText中选择相同的短语，</sample>
    <sample id="587">我们看到MPP判断对模型的评分出现大规模增加或大规模下降，取决于所选前缀是否可接受或不可接受。</sample>
    <sample id="588">现在，这个...这个效果在整个上下文长度上都会增加，这可能会对那些有大窗口的新生代语言模型产生影响。</sample>
    <sample id="589">为什么匹配前缀会影响语言模型的判断？</sample>
    <sample id="590">所以我们设计了一系列分析，我们试图通过尝试保留输入句子的相关结构但添加一些噪声来修改输入句子。在进行了多次这些扰动后，</sample>
    <sample id="591">我们发现这些噪音并没有实际上让模型改变其在展示我们对可接受和不可接受的判断方面的行为。</sample>
    <sample id="592">基本上，我们发现模型对扰动句子敏感，以类似的方式。</sample>
    <sample id="593">当我们在可接受域内扭曲句子时，我们看到所有扭曲的相似增加；当我们扭曲句子在不可接受域内时，我们看到MPP评估分数在相似方向上下降。</sample>
    <sample id="594">所以，我们工作的核心 takeaway是，语言模型对句子间共享的潜在句法和语义特征敏感。</sample>
    <sample id="595">而我们目前使用的MPP评估方法，使用短语和单句输入，可能无法充分捕捉语言模型在整个上下文窗口中的抽象知识。</sample>
    <sample id="596">请阅读我们的论文以获取我们实验的更多详细信息。谢谢您的收看。</sample>
    <sample id="597">该方法的第一步是将每个输入词元映射到一个不排序的多集，其中包含将在输出中出现的词元。</sample>
    <sample id="598">根据所给的英文内容，Coscript 包含了 55,000 个脚本。</sample>
    <sample id="626">根据所提供的英文内容，DEplain 的最佳对齐方法是“mass align”方法。</sample>
    <sample id="627">弱监督学习的好处在于它缓解了标注瓶颈，这表明它可以通过减少对大量标注数据的依赖来提高效率和可扩展性。</sample>
    <sample id="628">在 DEplain-web 中，文档采用手动和自动对齐方法进行了对齐。具体分配情况如下：有 147 个文档使用了手动对齐方法，而有 123 个文档使用了自动对齐方法。</sample>
    <sample id="629">CoNLL++数据集是通过从2020年《路透社新闻》收集文本并使用与CoNLL 2003相同的注释指南进行注释而创建的。</sample>
    <sample id="630">Hello everyone, my name is Yusen Zhang from Penn State University. Today I am going to present our work: XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations.</sample>
    <sample id="631">语义解析是一个任务，用于构建用户查询的语义表示，例如SQL和Lambda calculus。</sample>
    <sample id="632">跨语言语义解析是将多种自然语言中的查询翻译成多种意义表示的任务。</sample>
    <sample id="633">如图所示，我们需要使用神经模型将查询翻译成多种自然语言，例如SQL、Lambda或FunQL等。</sample>
    <sample id="634">现有的跨语言语义解析模型分别提出并评估在有限任务和应用数据集上。例如：</sample>
    <sample id="635">缺乏某些自然语言的覆盖范围，中文缺失。</sample>
    <sample id="636">跨语言语义解析

* 现有的CLSP模型分别提出并评估在有限任务和应用上的数据集。例如：
	+ 缺乏对某些意义表示的覆盖范围。

English | German | Chinese
--- | --- | ---
Neural Models | SQL | Lambda
FunQL</sample>
    <sample id="637">跨语言语义解析</sample>
    <sample id="638">或者它们只在某些神经模型上进行评估。例如，只有一个单一模型可以评估它们。</sample>
    <sample id="639">为了实现这一目标，我们提出了示例器，它提供了一个统一的数据集，用于在多种自然语言和意义表示中进行跨语言的语义解析。</sample>
    <sample id="640">它包含9个数据集和各种领域、5个语义解析任务、8种意义表示和22种自然语言，在15种语言家族中。</sample>
    <sample id="641">为了更好地评估我们的基准测试，我们考虑了六个用于训练和评估的设置。</sample>
    <sample id="642">首先，我们使用Google Translate API将源代码翻译成目标语言。然后，使用单语模型进行训练和评估。</sample>
    <sample id="643">例如，我们会在英文查询上训练英文模型。在推理过程中，我们将使用API将德文查询翻译成英文，然后使用训练好的模型来预测SQL。</sample>
    <sample id="644">我们还将测试单语模型。</sample>
    <sample id="645">在这个设置中，源语言与目标语言相同。例如，德语到德语或英语到英语。</sample>
    <sample id="646">我们还测试了单语少样本设置，通过使用仅10%的训练数据训练单语模型。</sample>
    <sample id="647">我们测试了多语言多语言模型，我们为所有语言训练了一个单一的多语言模型。</sample>
    <sample id="648">例如，我们将德语、英语和中文查询放在一起训练多语言模型。在推理过程中，我们可以使用这个模型来</sample>
    <sample id="649">将德语查询或中文查询等翻译成SQL</sample>
    <sample id="650">我们还考虑了跨语言零-shot和少-shot转移。我们在一种源语言上进行训练，并转移到另一种语言。</sample>
    <sample id="651">在训练期间，我们训练英文查询或结合英文和德文少量查询，以训练多语言模型并预测SQL输出。</sample>
    <sample id="652">我们还发现一些有趣的成果。所以，关于单语模型的分析，我们评估了两组模型。</sample>
    <sample id="653">包括编码器PTR，它代表多语言预训练编码器与指针式解码器，例如XLM-R + PTR和mBERT + PTR。</sample>
    <sample id="654">我们还评估了编码器-解码器模型，即多语言预训练编码器-解码器模型，例如mBART和mT5。</sample>
    <sample id="655">我们发现编码器-解码器在所有9个数据集上取得了最佳性能。</sample>
    <sample id="656">我们评估了MT5和XLM-R + PDR在多语言设置下的性能。</sample>
    <sample id="657">我们发现，编码器-解码器或编码器-PTR可以通过在各种语言的混合中进行训练来改进。</sample>
    <sample id="658">我们发现，大多数主要自然语言都可以获得性能提升，但英语在7个数据集中性能下降，在3个数据集中性能提升。</sample>
    <sample id="659">分析多语言训练

我们评估了mT5和XLM-R + PTR在多语言设置上的性能。大多数主要的NL任务都可以获得性能提升，除了在7个数据集中英文性能下降的情况，这种情况被称为“多语言 curse”。</sample>
    <sample id="660">我们还比较了跨语言性能差距。</sample>
    <sample id="661">在这个图中，蓝色线是跨语言少样本转移。橙色线是跨语言零样本转移。绿色线是单语环境。</sample>
    <sample id="662">我们发现通过比较绿色和橙色线，对于零-shot设置，跨语言转移性能差距显著。通过比较蓝色和橙色线，对于少-shot设置，转移差距迅速缩短。</sample>
    <sample id="663">我们还发现了一些有趣的发现。例如，编码器-解码器超过了以往的工作或实现了可比的结果。在英文自然语言上预训练可以显著提升few-shot在目标自然语言上的性能。</sample>
    <sample id="664">我们发现多语言模型，如Codex和Bloom，在跨语言零样本解析任务中仍然不够。</sample>
    <sample id="665">为了总结一下，我们构建了XSemPLR，一个用于多自然语言和意义表示的跨语言语义解析的统一基准。</sample>
    <sample id="666">我们将对三种代表性的多语言模型类型进行全面的基准测试，并且我们的结果展示了许多有趣的发现等等。欢迎访问我们的论文和代码。谢谢您的收看。</sample>
    <sample id="667">根据幻灯片中提供的英文内容，关于水印技术的现有研究可以被归类为以下四个类别：参数化水印、字典水印、后门水印和对抗性水印。每个类别都有相关的引用，表明它们在各自的领域内被研究和应用。</sample>
    <sample id="668">根据幻灯片中提到的发现，Codex 和 Bloom 等多语言 LLM 对于 CLSP（跨语言语义解析任务）来说还不足够。</sample>
    <sample id="695">该方法通过将排列的对齐作为训练的一部分来处理排列的不确定性。这使模型能够学习更有可能的排列，即使存在多个排列与数据一致。</sample>
    <sample id="696">根据所给的英文内容，可以将下游 NLP 模型的公平性定义为模型在处理不同政治观点时的一致性和公正性。这包括确保模型不会偏见或边缘化具有不同政治观点的人，并且能够以一致的方式处理所有输入，无论其政治背景如何。</sample>
    <sample id="697">演讲者的名字是Yanis Labrak。</sample>
    <sample id="698">演讲者的名字是Koustuv Sinha。</sample>
    <sample id="699">演讲者的名字是Myra Cheng。</sample>
    <sample id="700">热带主义 (tropicalism) 在本文的背景下意味着一种刻板印象，将拉丁裔女性描绘为充满活力和曲线玲珑，通常与热带地区相关的特质。这种刻板印象可能暗示这些女性是异国情调的，或者具有特定的文化或身体特征，这些特征被普遍认为是拉丁裔女性的特征。</sample>
    <sample id="701">根据幻灯片上的内容，作者通过使用特定的词语来创建目标群体的人工描写，这些词语强调了文化、传统和身份，而没有提及任何负面特征或刻板印象。这些词语被用来定义群体，并将它们与被视为“白人规范”的群体区分开来。</sample>
    <sample id="702">在本文中，使用了点积CXMI来衡量语境使用情况。</sample>
    <sample id="703">DrBERT 和 ChuBERT 的主要区别在于它们的训练数据来源和模型结构。DrBERT 是一个从头开始构建的模型，使用了7GB的NACHOS数据集进行预训练。相比之下，ChuBERT 是一个基于现有预训练模型CamMedBERT的临床模型，使用了4GB的临床句子进行预训练。此外，ChuBERT 还使用了NACHOS数据集的子集（4GB）和临床句子（4GB）的混合进行进一步预训练。</sample>
    <sample id="751">根据幻灯片上显示的英文内容，这篇论文有三位作者。</sample>
    <sample id="752">迭代迁移学习是一种机器学习方法，其中模型在每个round中使用最新收集的数据集进行训练。这种方法与累积方法不同，后者会积累所有从active annotation中收集的数据。</sample>
    <sample id="753">数据集的目标是理解用户语言，当他们想要做出选择时。</sample>
    <sample id="754">攻击者通过 EaaS 来提取模型参数，这是通过使用训练数据集来实现的。</sample>
    <sample id="755">根据所给的英文内容，这篇论文有三位作者。</sample>
    <sample id="756">根据幻灯片中提供的信息，用于创建初始数据集的注释者数量是8个。</sample>
    <sample id="757">根据所给的英文内容，这篇论文的作者所属机构是卡内基梅隆大学。</sample>
    <sample id="758">根据所给的英文内容，以左侧为支配词的示例是“我看见了巴特和Lisa”。在这个例子中，“我看见了”是主从结构中的从句，而“巴特和Lisa”是主从结构中的主语。</sample>
    <sample id="759">对话系统中的最先进模型是GPT-4。</sample>
    <sample id="760">我们需要在整个上下文窗口中评估模型的可接受性，因为大型语言模型正在使用更长的上下文窗口。这表明模型需要能够理解并处理更广泛的信息，而不仅仅是短语或句子。因此，评估模型在不同长度的上下文中的表现至关重要，以确保其准确性和一致性。</sample>
    <sample id="761">与单语英语模型相比，多语言训练可能会导致表现下降。</sample>
    <sample id="762">是的，注释者提前知道实体。</sample>
    <sample id="763">根据所提供的英文内容，评估使用的MT指标包括BLEU分数、METEOR分数和WER（单词错误率）。这些指标通常用于评估机器翻译的质量，与人类翻译的准确性和流畅度进行比较。</sample>
    <sample id="764">根据给定的英文内容，泛化的回归不会影响特定的NER（命名实体识别）类型。</sample>
    <sample id="765">NLP 中的立场很重要，因为它反映了技术在不同人口群体中的表现差异。在 NLP 的背景下，立场指的是技术在处理来自不同文化、语言或社会背景的数据时的性能。一个技术的立场可能包括其对特定语言或术语的敏感性，其对不同文化规范或价值观的适应性，以及其在处理边缘化或少数族裔群体时的公正性。一个技术的立场很重要，因为技术应该能够公平和准确地处理所有用户，而不管他们的背景如何。如果技术存在系统性偏差或性能差异，这可能导致偏见、不平等或错误的决策。因此，评估和解决 NLP 技术的立场对于确保技术的公正性和包容性至关重要。</sample>
    <sample id="766">根据幻灯片中提到的多语言LLM，它们被描述为“采用适配器微调”，这意味着这些模型在特定任务上进行微调时，只调整最后几层权重，而不是整个模型。这与完整微调不同，后者涉及调整模型的所有权重。</sample>
    <sample id="767">根据所提供的英文内容，他们使用了RoBERTa-base模型进行迁移学习。</sample>
    <sample id="768">根据所给的英文内容，最近用于评估 PaLM 能力的测试集包括WMT’21和WMT’22。</sample>
    <sample id="769">根据所提供的英文内容，作者最终提出了三条建议。</sample>
    <sample id="770">根据给定的英文内容，与最强的基线相比，建议的方法获得了1.3%的收益。</sample>
    <sample id="771">演讲者的名字是刘树恒。</sample>
    <sample id="772">是的，论文中的结果和数据集可以作为基准。</sample>
    <sample id="773">在论文中进行了三个较小模型的实验。</sample>
    <sample id="774">根据所给的英文内容，OF-A被用作研究多模型指令调整的基础模型。</sample>
    <sample id="833">根据幻灯片中提到的信息，这篇论文的作者是来自Google Translate的同事。</sample>
    <sample id="834">根据幻灯片显示，论文的作者是来自圣布鲁克大学的人类语言分析系的学生。</sample>
    <sample id="835">根据所给的英文内容，论文分析了西班牙语、法语和德语。</sample>
    <sample id="836">演讲者的名字是Shangbin Feng。</sample>
    <sample id="837">在实验过程中研究了两个模型：一个用于生成文档级别的简化，另一个用于生成句子级别的简化。</sample>
    <sample id="838">在 MultiInstruct 中使用的 62 个不同任务中，有 53 个任务用于训练目的，另外 9 个任务用于测试目的。</sample>
    <sample id="839">根据幻灯片上显示的英文内容，这篇论文有三位作者：Regina Stodden、Omar Momen和Laura Kallmeyer。</sample>
    <sample id="840">作者在实验中使用了四个数据集：AG News、MIND、SST2和Enron Spam。</sample>
    <sample id="876">NACHOS 是一个医学数据集，用于训练 DrBERT。</sample>
    <sample id="877">演讲者的名字是Ibilard。</sample>
    <sample id="878">根据所给的英文内容，提示策略对结果有重大影响。它可以通过BLEURT得分的变化来衡量，得分的变化范围从1到40个BLEURT点，这表明选择不同的提示可以显著影响翻译的质量评估。</sample>
    <sample id="879">根据幻灯片上显示的标志和名称，Kayo Yin 所属的机构是卡内基梅隆大学语言技术研究所。</sample>
    <sample id="880">根据提供的英文内容，专家编写的5个指令如下：1. 识别图像中的物体并描述它们。2. 将文本与相关的图像配对。3. 将图像分类为不同的类别。4. 从图像中提取特定的细节或特征。5. 将图像与给定的类别进行匹配。</sample>
    <sample id="881">根据所给的英文内容，作者建议通过使用人类研究参与者和已建立的核心参考解析模型来测试模型，以评估其在不同来源中提取知识的能力。</sample>
    <sample id="882">大家好，我的名字是爱德·比尔德，我将简要介绍我们与谷歌翻译团队合作的论文《Prompting PaLM for Translation: Assessing Strategies and Performance》。</sample>
    <sample id="883">PaLM是一个拥有540亿参数的大型语言模型，于去年2022年发布。它是在一个包含7800亿个文档的大型文本集合上进行训练的。</sample>
    <sample id="884">在大规模预训练中，它实现了 hundreds of NLP tasks 的 State-of-the-art。</sample>
    <sample id="885">这项工作提出了LLM提示在机器翻译中的首次系统研究。</sample>
    <sample id="886">我们使用MT社区的最佳实践评估了Such模型的翻译能力。这包括使用最新的测试集，以避免测试数据与语言模型的训练数据重叠。</sample>
    <sample id="887">我们比较了两个最好的系统，即在WMT评估中表现最好的系统。</sample>
    <sample id="888">我们使用了前所未有的LLM评估指标，并且还展示了专家基于人类评估的结果。最后，我们提供了一些关于选择策略的建议。</sample>
    <sample id="889">提示对翻译性能有重大影响，如我们在一个简单实验中所见，我们使用单步提示，并为每个句子提供两个不同的提示。</sample>
    <sample id="890">大多数句子（5160/1000）的差异大于一个BLEURT分数。</sample>
    <sample id="891">这在极端情况下可能会上涨到40个BLEURT分数。因此，选择一个好策略是很重要的。</sample>
    <sample id="892">在我们的实验中，我们尝试了一种多轮提示策略，其中我们只标记输入给系统的句子的语种。</sample>
    <sample id="893">在这个例子中，我们将从德语翻译成英语。德语句子，即源句子，用德语分号标记，而英语翻译则用英语分号标记。</sample>
    <sample id="894">我们发现，实际的提示形式对多轮提示的性能没有重大影响。</sample>
    <sample id="895">对于零和一击提示至关重要，而在我们的情况下，当我们转向五击提示时，实际上与实际提示形式几乎没有区别。</sample>
    <sample id="896">这是例子，它们承载了大部分的权重。</sample>
    <sample id="897">我们实验结果的摘要是，示例质量比源句子的相似性更重要。</sample>
    <sample id="898">选择高质量的翻译示例很重要。特别是，我们将从URM评估数据或Dev数据的训练数据中选择示例。</sample>
    <sample id="899">开发数据的质量更高，与训练数据相比，结果更好。因此，在使用开发数据时，性能会更好。</sample>
    <sample id="900">然而，专门化的SOTA系统在翻译方面具有显著优势，但Palm的表现相当接近商业系统。在本例中，我们选择将其与Google Translate进行比较。</sample>
    <sample id="901">我们从使用MQM框架进行的实验中获得的见解表明，PALM的流畅性与当前最好的系统相当，但主要差异来自准确性。</sample>
    <sample id="902">特别的是，最常见的错误是省略错误。</sample>
    <sample id="903">所以它看起来是Palm选择他们来生产一个更好的理解的翻译，有时候通过删除部分源句子，这些部分在翻译中不重要。</sample>
    <sample id="904">然而，对于Pan的“风格/ awkward”类别比对于SOTA系统更低，这是一个额外的信号。</sample>
    <sample id="905">Palm 提供了非常流畅的输出，但仍然存在一些准确性问题。</sample>
    <sample id="906">而这就是这个真的很短的概述。如需更多详细信息，请转到完整的论文演示。非常感谢！</sample>
    <sample id="907">你好，我是 Dawei，一名德国萨尔兰大学的 PhD 学生。在这段视频中，我想向大家介绍我们最近的研究成果：《弱于你想象》——对弱监督学习的批判性审视。</sample>
    <sample id="908">这是与肖玉顺、马里乌斯·穆索夫和盖尔·史蒂芬以及迪特·克拉科合作的联合研究。</sample>
    <sample id="909">我将从介绍弱监督和弱监督学习开始。</sample>
    <sample id="910">在弱监督中，我们不会手动标注数据。相反，我们使用弱标注源来标注数据，例如简单的启发式规则、知识库或低成本的 crowdsourcing，如图所示。</sample>
    <sample id="911">与人类注释相比，弱注释更便宜，但它们也很 noisy，这意味着其中一部分注释是错误的。</sample>
    <sample id="912">如果我们直接训练神经网络使用弱标签数据，神经网络倾向于 memorize 标注噪声，并且不泛化。</sample>
    <sample id="913">在弱监督学习中，训练算法被提出要 robustly 训练神经网络，以处理这样的标签噪声，这样训练模型仍然能够泛化得很好。</sample>
    <sample id="914">在最近的WSL工作中，WSL代表弱监督学习。一个常见的说法是，人们说他们只训练模型在弱标签数据上，并在干净测试集上取得高精度。</sample>
    <sample id="915">技术上，这个说法并不完全正确，但其中有一个关键点。</sample>
    <sample id="916">这表明人们假设存在额外的干净验证集，用于模型选择。</sample>
    <sample id="917">我们已经停止关注这个问题，因为这暗示了在弱监督学习中需要额外的手工注释。但就像房间里的大象一样，这个需求经常被忽视。</sample>
    <sample id="918">我们之前提到的这个任务是让我们提出三个研究问题。首先，对于WSL是否需要干净的验证数据？或者我们可以使用一个有噪声的验证集吗？</sample>
    <sample id="919">第二，如果需要干净的数据或干净的数据对于WSL有效运行是强制性的，那么我们需要多少干净的样本？最后，我们是否应该只使用干净的样本进行验证，或者是否有更好的方法来利用它们？</sample>
    <sample id="920">我们解决了这些问题，并且我们的发现如下：</sample>
    <sample id="921">首先，我们发现有趣的是，最近的WSL方法确实需要干净的训练样本才能正常工作。</sample>
    <sample id="922">否则，如图所示，如果没有干净的验证样本，训练模型无法超出原始弱标签之外泛化。</sample>
    <sample id="923">意味着训练是徒劳的。</sample>
    <sample id="924">这表明WSL方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的注释成本不容忽视。</sample>
    <sample id="925">我们的第二个发现是增加干净验证样本的数量可以帮助WSL方法实现更好的性能，如图左所示。</sample>
    <sample id="926">通常，我们只需要每个类别20个样本就可以获得高性能。</sample>
    <sample id="927">但这并不是故事的结束，因为如果我们以某种方式决定获取干净样本，那么直接在它们上进行训练甚至会获得更好的表现。</sample>
    <sample id="928">右侧的图表显示了直接应用于干净数据的微调方法和仅使用干净数据进行验证的WSL方法之间的性能差异。</sample>
    <sample id="929">正如我们所见，如果我们有每个类别的10个样本，直接微调开始超越WSL方法。</sample>
    <sample id="930">最后，通过允许在干净验证样本上继续微调，可以轻松实现之前WSL方法声称的性能改进。</sample>
    <sample id="931">如图所示，瓦尼纳模型FTW最初在更复杂的WSL方法（如余弦）上表现不佳。</sample>
    <sample id="932">然而，如果我们继续在干净样本上微调，FTW的表现与其他方法一样好。</sample>
    <sample id="933">在实际应用中，没有理由选择更复杂的WSL方法，因为它们需要更多的计算时间和磁盘空间。</sample>
    <sample id="934">总之，我们展示了最近的WSL方法需要干净的手工标注样本才能正常工作。它们的性能和实用性被严重高估了。</sample>
    <sample id="935">我们的综合建议如下：</sample>
    <sample id="936">首先，报告模型选择标准。例如，报告模型选择是否使用干净的验证样本。</sample>
    <sample id="937">第二，WSL方法应该与基于少量学习的基线进行比较，因为它们仅使用干净样本。第三，持续微调是一种简单而强大的基线，应该在未来的WSL工作中考虑。</sample>
    <sample id="938">最后，我们开源了我们的代码。你可以在幻灯片上的QR码中找到它。请随意查看。谢谢，并祝你享受会议。</sample>
    <sample id="939">根据所给的英文内容，对话系统的常用评估方法是通过让人类裁判选择两个对话中哪个更好或对对话进行评分。</sample>
    <sample id="940">根据所给的英文内容，这篇论文有五位作者。</sample>
    <sample id="941">在 Servin 和 Kea 的示例中，需要的背景知识是“裁判决定案件在法庭上”。</sample>
    <sample id="942">代码是公开的，可以在GitHub上获取。</sample>
    <sample id="943">根据所给的英文内容，NLPositionality 的注释者在各个人口统计学特征（即国家/地区、性别等）方面是否均衡？</sample>
    <sample id="944">在可接受的域中扰乱句子的方法包括添加前缀/后缀修饰语、使用长前缀修饰语、将“无论X对它说什么”添加到最前面，以及引用X昨天说的句子。这些方法旨在保持句子结构的相关性，同时引入一些噪音，以测试模型对不同输入的敏感度。</sample>
    <sample id="945">进行维度评估意味着对对话质量的各个方面进行单独评估，以更细致地了解模型的优点和缺点。</sample>
    <sample id="946">根据幻灯片底部的标志，论文的作者来自中国科学技术大学。</sample>
    <sample id="947">根据所给的英文内容，提示的形式很重要，特别是在零和一击提示的情况下。</sample>
    <sample id="978">根据所给的英文内容，作者评估了几个对话模型。然而，具体名称没有在提供的文本中提到。</sample>
    <sample id="979">根据幻灯片上显示的英文内容，这篇论文有六位作者。</sample>
    <sample id="980">根据所给的英文内容，优秀规划器的理想品质是能够编写既合理又符合约束条件的脚本。这意味着规划器应该能够理解并应用各种约束条件，同时确保实现抽象目标时保持逻辑和一致性。</sample>
    <sample id="981">根据幻灯片上显示的英文内容，这篇论文有九位作者。</sample>
    <sample id="982">演讲者的名字是Vasudha Varadarajan。</sample>
    <sample id="983">根据幻灯片上提供的信息，这篇论文的作者Adam Przpherdowski和Michat Wozniak所属的机构是华沙大学计算机科学学院。</sample>
    <sample id="1021">根据图片中提供的英文内容，PaLM 最常见的错误是“遗漏错误”。</sample>
    <sample id="1022">大家好，我是James Finch，我是Sarah Finch。今天我们将向您介绍ABC-Eval，一种评估对话型人工智能的新维度方法。</sample>
    <sample id="1023">这项工作是由Emory NLP实验室领导的，由教授吉诺·乔伊在Emory大学担任负责人，并与亚马逊Alexa AI合作完成。</sample>
    <sample id="1024">但让我们假设你刚刚开发了一个对话模型，并且你想看看它与当前的前沿技术相比表现如何。</sample>
    <sample id="1025">常见的做法是使用人类评估，例如，请人类裁判选择两个对话中哪个更好，或者根据 Likert 梯度对对话进行评级。</sample>
    <sample id="1026">这些方法可以很好地提供对话质量的整体评估，但对话质量有很多方面。因此，您可能希望在更精细的水平上评估聊天质量的多个维度，以了解模型的优点和缺点。</sample>
    <sample id="1027">一种方法是让人类裁判员评估对话质量的几个维度，例如模型回复的相关性，使用现有的比较或李克特尺度方法。</sample>
    <sample id="1028">然而，我们相信有一种更精确可靠的策略来进行维度对话评估。</sample>
    <sample id="1029">我们的方法旨在通过明确注释每个模型响应是否表达某些行为来减少人类评估的主观性，例如提供无关信息或自相矛盾。</sample>
    <sample id="1030">我们称这种方法为聊天行为标注，或简称为ABC-Eval。我们开发了这种方法来全面涵盖最近文献中建议会影响聊天质量的聊天模型行为。</sample>
    <sample id="1031">ABC-Eval能够衡量聊天模型在犯各种主题错误时的速率。</sample>
    <sample id="1032">例如，ABC-Eval衡量聊天模型在与其伙伴互动时忽略其伙伴或说一些不相关的东西的次数。</sample>
    <sample id="1033">与自己或伙伴相矛盾，扭曲错误事实或违反常识知识，并当模型成功或失败时表现出同理心。</sample>
    <sample id="1034">为了确定哪种评估方式最有效，我们选择了四个前沿的聊天模型，并在每种模型上评估了它们在100个人机对话中的表现，使用ABC-Eval。</sample>
    <sample id="1035">为了比较，我们还使用了三种现有方法评估这些对话：针对每个轮次的 Likert 评分、针对整个对话的 Likert 评分以及针对对话中每对轮次的配对比较。</sample>
    <sample id="1036">对于每个现有的方法，我们收集了在对话中八个最常衡量的方面上的评估，因为这是评估聊天模型沿多个维度的标准做法。</sample>
    <sample id="1037">从我们对这些评估结果的分析中，我们发现ABC行为标签在总体上比现有方法收集的标签更可靠，如通过100个双重标注对话的内部注释员一致性来衡量。</sample>
    <sample id="1038">此外，ABC-Eval标签比现有方法产生的指标更预测整体对话质量，如图中所示的简单线性回归分析。</sample>
    <sample id="1039">例如，你可以看到测量具有自我和伙伴矛盾的转率解释了5％和10％的对话质量，而平均Likert一致性分数仅解释4％或更少。</sample>
    <sample id="1040">最后，我们检查了每个评估指标是否捕捉到了对话质量的独特方面，使用逐步线性回归。</sample>
    <sample id="1041">你可以看到，所有ABC-Eval指标的组合解释了对话质量的超过25%。当你一次删除一个指标时，大多数指标都会导致失去相当一部分关于质量的信息。</sample>
    <sample id="1042">另一方面，所有turn-level Likert指标的组合解释了较少的质量，并且较少的这些指标携带独特信息。</sample>
    <sample id="1043">这些可靠的、信息丰富且独特的ABC-Eval指标使我们能够以比以往的方法所能实现的更高分辨率来评估对话型人工智能。</sample>
    <sample id="1044">你可以看到，在我们实验的结果中，有几个挑战仍然存在，并且已经被精确量化。例如，我们测试的聊天机器人在其响应中存在大约20%的常识错误。</sample>
    <sample id="1045">它们大约在15%的回应中产生无关信息，并且它们自己或与对方发生矛盾大约10%的时间。</sample>
    <sample id="1046">随着该领域的快速发展，许多错误率可能会在自我们评估以来发布的新模型中下降。然而，这更加强调了追求可靠和精确的评估指标以比较模型的重要性。</sample>
    <sample id="1047">我们希望ABC-Eval能被其他领域的专家利用，作为在这个方向上的一个重要步骤。我们期待看到在未来几个月和几年里对话型AI如何 advancement。谢谢收看。</sample>
    <sample id="1048">根据幻灯片上的内容，这篇论文的作者所属机构是Emory University的Emory NLP Lab。</sample>
    <sample id="1049">在本文中，CFT 代表“持续微调”，这是一种机器学习技术，其中模型在训练后继续在新数据上进行微调，以适应新的情况或任务。</sample>
    <sample id="1050">根据图片中提供的英文内容，这篇论文有六位作者。</sample>
    <sample id="1051">你好，我的名字是Kayo Yin，我将展示我们的论文《翻译何时需要语境：一种数据驱动的多语言探索》。这项工作是在Patrick Fernandez、Emily Liu、Andrea F.T. Martins和Graham Neubig的合作下完成的。</sample>
    <sample id="1052">所以许多翻译取决于上下文。例如，在这个句子中，我们会怎么翻译“ mole”？</sample>
    <sample id="1053">Translation depends on context Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole.</sample>
    <sample id="1054">所以，取决于语境，单词的意义会改变，因此其翻译也会改变。</sample>
    <sample id="1055">然而，评估模型在处理这种情况下的表现如何相当困难。首先，因为只有少数单词依赖于上下文，使得语料库级别的指标如BLEU无法捕捉这些翻译。</sample>
    <sample id="1056">一些人建议对依赖上下文的翻译进行针对性评估，但这些资源只支持有限类型的上下文依赖翻译和有限的语言集，因为它们通常依赖于领域知识和人工润色。</sample>
    <sample id="1057">在这项工作中，我们试图回答这两个问题：首先，翻译何时需要上下文？其次，模型如何处理这些情况？</sample>
    <sample id="1058">为了解决第一个问题，我们首先测量了单词在翻译中对上下文的依赖程度。</sample>
    <sample id="1059">在之前的工作中，我们引入了cxmi作为衡量机器翻译模型使用上下文的指标。这是通过衡量给定源x时，上下文c对目标y提供了多少信息来实现的。</sample>
    <sample id="1060">你可以把CXMI看作是给模型提供上下文后获得的信息量。</sample>
    <sample id="1061">在本文中，我们扩展了CXMI到点积CXMI，可以衡量句子级别或单词级别的上下文使用。我们可以将具有高P-CXMI的单词视为需要上下文进行翻译的单词。</sample>
    <sample id="1062">现在我们使用高PEXMI分析单词，以查找这些单词之间的模式。</sample>
    <sample id="1063">我们对从英语翻译成14种不同语言的TED演讲的转录进行分析。</sample>
    <sample id="1064">我们首先分析那些具有高P-CXMI得分的语音片段。</sample>
    <sample id="1065">这使我们能够找到例如阿拉伯语中的复数名词，这些名词具有相对较高的P-CXMI，并且这可以解释为英语没有复数名词，因此需要一种方法来确定在翻译成阿拉伯语时一个名词是否是复数。</sample>
    <sample id="1066">同时我们发现某些语言也要求语境，当我们选择适当的动词形式时。我们查看了具有高P-CXMI平均值的词汇项，即在所有不同出现中具有高P-CXMI平均值的词汇项。</sample>
    <sample id="1067">And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document.</sample>
    <sample id="1068">同样，我们发现上下文支持翻译成正确的形式。</sample>
    <sample id="1069">最后，我们来看一下具有高 P-CXMI 的不同个体标记。这使我们能够识别出无法仅通过单词本身捕捉到的现像，而是表达在句子结构中，例如 Ellipsis resolution。</sample>
    <sample id="1070">现在我们使用分析结果来设计文档级别的翻译基准。</sample>
    <sample id="1071">对于我们识别出的五种 discourse 现象中的每一个，我们创建了标签器以自动识别与现象相关的单词，并将其称为多语言 discourse-aware（MuDA）标签器。</sample>
    <sample id="1072">我们可以注意到不同语言具有这些 discourse 现象的不同比例。</sample>
    <sample id="1073">然后我们使用MuDA标记器，通过将其应用于我们要用于评估的平行语料库来应用标记器，并在MuDA标记器已确定的领域依赖示例上应用我们选择的翻译指标。</sample>
    <sample id="1074">最后，我们使用我们的基准测试以及其它指标来评估不同模型在文档级别的机器翻译上的性能。</sample>
    <sample id="1075">首先，当我们使用语料库级别指标时，对于蓝色，我们发现基于图标的模型具有最佳性能。</sample>
    <sample id="1076">但如果我们使用 comet，基于上下文的模型表现最佳。如果我们使用 word f 测量，则具有上下文的模型和没有上下文的模型具有可比的表现。</sample>
    <sample id="1077">这再次说明了，如果我们只使用语料库级别的指标，就很难确定哪个文档级别翻译系统最好。</sample>
    <sample id="1078">现在我们使用MUMA基准测试评估模型，并发现使用上下文的模型在某些 discourse 现象，如正式性和 lexical cohesiveness 上显著更准确。</sample>
    <sample id="1079">但这些模型并没有比不使用上下文的模型在其他现象如椭圆、 pronunciation 和 verb form 上表现得更好。因此，这表明我们需要在文档级别翻译上取得更多进展。</sample>
    <sample id="1080">我们还比较了不同的商业系统，我们的基准测试显示，DeepL通常比Google Translate更准确地进行文档级别翻译。</sample>
    <sample id="1081">总之，我们在14种语言对上进行数据驱动分析，以确定何时翻译需要上下文。</sample>
    <sample id="1082">然后我们使用我们的发现构建文档级机器翻译的基准，这可以帮助我们确定哪些 discourse 现象模型可以处理好或不好，以及哪些翻译系统在文档级别翻译方面表现良好。</sample>
    <sample id="1083">谢谢大家的聆听，祝大家早安。</sample>
    <sample id="1084">演讲者的名字是Yusen Zhang。</sample>
    <sample id="1121">根据图片中提供的信息，新方法没有名称。它被描述为“一种新方法”，但没有指定任何特定的名称。</sample>
    <sample id="1122">根据所给的英文内容，作者描述“显性词汇”(marked words) 方法为一种识别区分标记群体和未标记群体的单词的方法。</sample>
    <sample id="1123">根据幻灯片上提供的信息，作者Shangbin Feng所在的机构是宾夕法尼亚大学。</sample>
    <sample id="1124">第一个提到的对称依存关系结构的名称是'Prague'。</sample>
    <sample id="1125">演讲者的名字是James Finch和Sarah Finch。</sample>
    <sample id="1126">根据幻灯片上显示的英文内容，这篇论文有四位作者。</sample>
    <sample id="1127">根据所给的英文内容，可以用于测试句法现象的数据集包括BLMP、SyntaxGym和CrowS。</sample>
    <sample id="1161">第一个研究问题的五种方法的缩写是FT、W、BOND、COSINE和MLC。</sample>
    <sample id="1162">该模型在11项生物医学和临床 downstream 任务上进行了评估。</sample>
    <sample id="1226">CamemBERT 最初是在 4GB 的数据子集上训练的。</sample>
    <sample id="1227">演讲者的名字是Adam Przpielkowski。</sample>
    <sample id="1228">在实验中，我们发现通过使用更近的数据重新训练或继续预训练一些模型会导致性能下降。这种性能下降随着时间间隔的增大而加剧。这支持了我们的假设，即时间漂移是性能下降的主要原因。</sample>
    <sample id="1269">在自然语言处理任务中，词元的排列顺序对于理解句子的含义至关重要。在生成或预测文本时，正确的排列确保了语法正确性和语义连贯性。如果词元未按正确顺序排列，可能会导致误解或无法理解的输出。因此，在生成序列后需要进行排列，以确保最终结果符合预期的结构和意义。</sample>
    <sample id="1270">根据所提供的英文内容，作者建议模型所有者应提高偏见缓解方法的透明度，以解决可能存在的问题，如过度价值对齐或反刻板印象方法，这些可能导致可观察到的刻板印象。通过提供这些方法的详细信息，可以更好地理解它们的效果，并确保它们有效地减轻偏见，而不是加剧它。</sample>
    <sample id="1271">最小对不可接受输入指的是在最小对偶范例中展示的第二个句子，它被设计成语法上错误或不自然的。</sample>
    <sample id="1272">根据所给的英文内容，作者使用了准确率和F1分数作为评估指标。</sample>
    <sample id="1273">使用了内在注释者一致性指标来衡量注释者之间的一致性。</sample>
    <sample id="1274">根据所给的英文内容，选择维基百科来添加完全无关的句子。</sample>
    <sample id="1275">根据幻灯片上提供的信息，论文的作者来自海因里希·海因耶·多斯尔大学，位于德国。</sample>
    <sample id="1276">MultiInstruct 与其他基准不同之处在于它是一个大规模的多模态指令调优数据集，而其他基准通常只专注于单模态任务。</sample>
    <sample id="1277">根据幻灯片上提供的英文内容，这篇论文有三位作者：Sarah E. Finch, James D. Finch, 和 Jinho D. Choi。</sample>
    <sample id="1278">根据给定的英文内容，二进制协调的定义是：'在字符长度、音节长度和单词长度之间存在显著差异。' 这意味着在这些不同单位中测量的长度之间存在统计学上的显著差异，表明它们不是随机变化的。</sample>
    <sample id="1279">根据图片中提供的信息，无法确定提示语的平均长度。图片显示了GPT-3.5和GPT-4在生成与刻板印象相关的提示语时的性能，但没有提供关于提示语长度的任何数据或统计信息。</sample>
    <sample id="1280">这些发现表明，较小的 T5 模型，如果经过适当的微调，可以生成高质量的脚本，这可能意味着它们在某些任务上与较大的语言模型相比具有竞争力。</sample>
    <sample id="1281">大家好，我是Yanis Labrak，我将向您介绍我们在DrBERT上的工作，一个在生物医学和临床领域中具有鲁棒性的法语预训练模型。</sample>
    <sample id="1282">在本演示中，我们首先讨论了语言建模在医疗保健中的应用。然后我们将介绍我们论文的主要贡献。</sample>
    <sample id="1283">我们介绍了第一个 biomedical 模型，在法语中称为 DrBERT，它是基于 RoBERTa 训练的，训练数据集是来自网络的医学 Crowled 数据集。</sample>
    <sample id="1284">我们还引入了具有多种预训练设置和数据来源的模型比较。然后，我们在法国的11项生物医学和临床 downstream 任务上展示了我们的结果。</sample>
    <sample id="1285">最后，我们将总结实验并提供更多关于如何访问模型的细节。</sample>
    <sample id="1286">自2018年发布以来，BERT已成为解决自然语言处理任务最有效的方法之一，并在与历史上的静态和概念化方法（如Word2Vec、FastText和Word）相比提供了巨大的性能提升。</sample>
    <sample id="1287">自那时起，这个模型已经适应了多种其他语言，如法语中的Camembert和生物医学领域的PumMedBERT和BioBERT以及临床领域的ClinicalBERT，但大多数是在英语中。</sample>
    <sample id="1288">其他语言的专用模型往往稀缺，并且通常基于持续预训练，因为缺乏本领域数据。</sample>
    <sample id="1289">然而，法语并没有任何开源模型用于生物医学领域。</sample>
    <sample id="1290">我们问自己关于什么是最适合广泛用途的数据来源的问题。这些公共数据是临床数据的良好替代品。</sample>
    <sample id="1291">为了回答这个问题，我们将比较Dr. Bert和我们的Sherbert模型，后者是基于从南希大学医院数据库中获得的匿名化数据。</sample>
    <sample id="1292">然后我们问自己，我们需要多少数据来训练一个专门针对法语数据的模型？是4GB、8GB还是更多？</sample>
    <sample id="1293">首先，我们分别训练和比较了四个从头开始的模型：第一个是使用7GB NACHOS数据集训练的第一个版本的DoctorBERT；第二个是使用4GB子集NACHOS数据集训练的第二个版本的DoctorBERT。</sample>
    <sample id="1294">第一版的Shubert是一个临床模型，包含4GB的句子来自临床笔记。最终版本的Shubert包含Natasha的4GB子集和临床笔记的4GB混合。</sample>
    <sample id="1295">除了这种比较之外，我们还介绍了三种基于连续预训练的模型，以分析预训练策略的影响。</sample>
    <sample id="1296">基于Camembert的权重训练NACHOS的4GB子集，另一个也是基于Camembert，但这次是在4GB的Clink和节点上训练。</sample>
    <sample id="1297">基于英文 biomedical 模型，使用 BERT 训练 4GB 子集的 Nature。总共我们有 7 个模型。</sample>
    <sample id="1298">为了评估我们的7个模型，我们使用了各种公共和私人医疗数据集任务，如命名实体识别、分类、对话生成和问答。</sample>
    <sample id="1299">这些模型与六个基线模型进行了比较，它们分别是：Camembert Oscar 108 GB、Camembert Oscar 4 GB、Camembert CUNet 4 GB、DeBERTa-BERT和ClinicalBERT。</sample>
    <sample id="1300">该表格展示了不同模型在各种任务上的性能，包括医学报告、生物医学和临床领域。表格中列出了多个模型，如RoBERTa、BERT和DeBERTa等，并且对它们在不同数据集上的表现进行了评估。表格的左上角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右上角列出了评估指标，如F1分数和H@100。表格的左下角列出了模型名称，如RoBERTa、BERT和DeBERTa等。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE、QUADREME和MEDL。表格的右下角列出了评估结果，即每个模型在每个数据集上的得分。表格的左下角列出了数据集名称，如MUSCA-Report、MUSCA-BERT、MUSCA-ESAL、CAS、MedMoE</sample>
    <sample id="1301">然而，我们可以从我们观察到的来源中获得数据。我们还观察到，使用更多的数据转化为更好的表现。</sample>
    <sample id="1302">从头开始预训练似乎在大多数任务上获得了更高的性能。</sample>
    <sample id="1303">然而，我们在使用来自帕特伯德的权重和 tokenizer 对 natos 的 4GB 子集进行持续预训练时，得到了与从头开始训练的 doctorbert-4gb 相似的成果。</sample>
    <sample id="1304">这并不是使用Camembert权重和Tokenizer训练的模型的情况，这些模型存在稳定性问题。</sample>
    <sample id="1305">最终，我们的系统在11个任务中的9个上提供了更好的表现，并超过了通用模型Camembert的全球结果。</sample>
    <sample id="1306">我们还观察到，专业化的数据更好，但专业化的数据并不容易扩展。</sample>
    <sample id="1307">所有预训练模型均可以从NACHOS获取，并且在Avignon网站上免费提供。所有训练脚本都在我们的GitLab仓库中。</sample>
    <sample id="1308">所以，谢谢你们为这个演示。我们期待在多伦多的海报会上与你们交流。</sample>
    <sample id="1309">根据所给的英文内容，论文研究了以下学习策略：1. 从头开始构建模型2. 使用现有预训练模型进行连续预训练3. 使用混合数据集进行预训练（4GB NACHOS和4GB临床句子）</sample>
    <sample id="1310">根据所给的英文内容，由于测试重复使用而导致的过拟合因素似乎并不显著。这可以从图表右侧红色最佳拟合线的斜率大于1来推断，这意味着在CIFAR-2003上的每单位改进转化为CIFAR++上的超过一个单位改进。这种现象表明没有衰减回报，表明过拟合程度较低。</sample>
    <sample id="1311">根据所给的英文内容，简化质量通过与基线评分进行比较来评估。实验结果表明，这些基本微调可以产生或获得比基线评分更好的分数，从而为未来自动文本简化问题的基准提供了一个参考点。</sample>
    <sample id="1312">是的，语言模型有政治偏见。</sample>
    <sample id="1313">大家好，我的名字是马修斯·林登曼。今天我要向大家简要介绍我们关于在没有树的情况下使用多集标记和潜在排列进行组合泛化的论文。</sample>
    <sample id="1314">这是与我的导师Alexandra Koller和Ivan Titov合作的成果。</sample>
    <sample id="1315">组合泛化可以理解为学习者处理更深递归和未见过的短语组合的能力，这些短语在训练期间曾单独出现。</sample>
    <sample id="1316">在语义解析的背景下，测试组合泛化可能看起来像这样。就像往常一样，我们有一个训练集的句子，在这种情况下是“女孩睡了”和“玛丽知道女孩睡了”。</sample>
    <sample id="1317">这些短语与表示它们意义核心方面的逻辑形式配对。</sample>
    <sample id="1318">与标准机器学习评估不同，测试集不来自相同的分布，但包含结构上 unseen 的逻辑形式。</sample>
    <sample id="1319">在这个例子中，模型在训练过程中遇到了浅层递归，但在测试时遇到了深层递归的示例。</sample>
    <sample id="1320">naive sequence-to-sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input.</sample>
    <sample id="1321">特别是在具体例子中，它们经常无法 reproducing input和output之间的系统对应关系，例如图中的彩色编码部分。</sample>
    <sample id="1322">一种常见的方法是将树融入模型中。</sample>
    <sample id="1323">树木旨在捕捉与逻辑形式相关的表达的组合过程。</sample>
    <sample id="1324">这很好，但树通常不会给出，需要以某种方式获取。</sample>
    <sample id="1325">这可能是一个复杂且有时是计算昂贵的过程。通常，这涉及大量的形式化特定预处理的逻辑形式，例如处理变量符号。</sample>
    <sample id="1326">获取树形图还可能涉及专门的语法推导程序。</sample>
    <sample id="1327">在这篇论文中，我们不使用树，并引入了一个神经序列到序列模型，该模型直接建模了输入片段和输出片段之间的对应关系。</sample>
    <sample id="1328">对于第一次，我们展示了在不依赖树的情况下对更深层次递归的强泛化。</sample>
    <sample id="1329">我们的方法预测输入的输出在两个步骤中。</sample>
    <sample id="1330">首先，我们将每个输入标记与一个无序的多集标记为输出中将出现的标记。</sample>
    <sample id="1331">在第一个步骤之后，我们拥有所有正确的标记，但它们没有排序。</sample>
    <sample id="1332">这就是为什么在第二步中，我们使用另一个模型来预测一个排列，将它们放入正确的顺序。</sample>
    <sample id="1333">我们引入了一种新方法，用于预测一个排列，该排列不给可能的排列设置任何硬约束。这使我们的方法相当灵活和表达力强。</sample>
    <sample id="1334">概念上，我们的排列模型大致像这样运作。</sample>
    <sample id="1335">我们从左到右遍历输出，并确定将每个位置的多集标记放在哪个标记上。对于第一个输出位置，我们简单地选择一个，如红色所示。</sample>
    <sample id="1336">然后我们跳到下一个多集标记，以确定输入中的第二个标记。</sample>
    <sample id="1337">我们以类似的方式确定输出中的第三个标记，通过跳转到另一个多集标记。我们继续这个过程</sample>
    <sample id="1338">直到第一阶段的每个标记都被访问过一次。</sample>
    <sample id="1339">为了给你一个实验结果的 teaser，这里我们比较了我们的方法与其他 Treeless 模型在 COGS 基准上的表现。我们的模型在泛化到更深层次的递归时，比其他模型表现出更大的优势。</sample>
    <sample id="1340">其他类型的结构泛化仍然非常具有挑战性。</sample>
    <sample id="1341">在我们的论文中，我们解决了几个有趣的技術挑戰。</sample>
    <sample id="1342">首先，输入和输出之间的对齐在训练数据中没有给出。因此，对于给定的标记，我们不知道它来自哪个多头，这为训练提出了挑战。</sample>
    <sample id="1343">此外，有时存在多个与数据一致的排列，但语义上正确的排列是潜在的。我们通过将对齐作为训练的一部分来解决这个问题。</sample>
    <sample id="1344">我们的排列方法非常灵活，但它带来了挑战，即找到最高得分排列是NP困难的。这是因为这与旅行商问题有关。</sample>
    <sample id="1345">我们通过使用GP U友好型连续放松来近似这个过程，这还允许我们在解决方案中反向传播并学习更符合语言的排列。</sample>
    <sample id="1346">如果您想了解我们实验以及我们如何解决这些挑战，请查看我们的论文或参加海报。</sample>
    <sample id="1347">认知失调是指思维、行为或信念之间存在不一致之处。</sample>
    <sample id="1348">根据所给的英文内容，GPT-4是最倾向于自由派的语言模型。</sample>
    <sample id="1349">根据所给的英文内容，累积训练在主动学习时比迭代训练更有效。</sample>
    <sample id="1350">演讲者的名字是Sara Papi。</sample>
    <sample id="1351">MuDa基准中的数据是从TED Talks的转录中获得的，这些转录已经翻译成14种不同的语言。</sample>
    <sample id="1385">演讲者的名字是马修斯·林登曼。</sample>
    <sample id="1386">跨语言转移指的是在一种语言上训练模型，然后将其应用于另一种语言的过程。在演示中提到的设置中，这意味着在一种语言（如英语）上训练模型，然后使用该模型来预测另一种语言（如SQL）的输出。</sample>
    <sample id="1387">根据英文内容，这篇论文的作者所属机构是Saarland University。</sample>
    <sample id="1388">根据所给的英文内容，作者使用了两种延迟测量方法：平均延迟和计算延迟。 平均延迟是翻译的延迟措施，而计算延迟考虑了模型的计算时间来产生输出。</sample>
    <sample id="1389">大家好，我是马查塔。今天我和我的合作者马丁一起给大家介绍我们关于“KITMUS测试：评估多源知识整合”的工作。这项工作是麦克吉尔大学、米拉和微软研究之间的合作项目。</sample>
    <sample id="1390">自然语言理解模型利用各种知识来源，如参数中包含的知识，通常通过预训练获得，以及在推理时间给定的输入中的知识。</sample>
    <sample id="1391">最近的研究表明，在问答等任务中，模型可以利用预训练知识来解决任务。</sample>
    <sample id="1392">但自然语言理解经常需要在推理时提供的知识。</sample>
    <sample id="1393">例如，在句子“John saw the newly elected president on TV”中，</sample>
    <sample id="1394">预训练参数可以包含关于总统做什么和电视是什么的信息，但它们不能可靠地知道这个特定实体John是谁或新任总统是谁，因为总统自预训练以来可能已经发生了变化。</sample>
    <sample id="1395">因此，对于知识密集型NLU任务的成功模型需要具备整合和使用预训练时间和推理时间知识的能力。</sample>
    <sample id="1396">在这个工作中，我们提出了一个诊断测试套件用于知识整合。</sample>
    <sample id="1397">我们引入了一个核心引用解决任务，旨在测试对不同来源中可获得知识的利用能力。我们使用人类研究参与者和已建立的核心引用解决模型评估数据集。</sample>
    <sample id="1398">这是一个来自我们数据集的例子。Servin 是一名法官，Kea 是一名面包师。Servin 和 Kea 在公园里相遇。在一天的工作结束后，在法庭上做出裁决，他很高兴放松。</sample>
    <sample id="1399">这个任务是确定代词“他”所指的正确实体，在这种情况下是Servin。</sample>
    <sample id="1400">确定给定代词的含义需要两种类型的信息：第一，实体特定知识，例如Servin是一名法官；第二，背景知识，例如法官在法庭上决定案件。</sample>
    <sample id="1401">通常，背景知识是在大规模语言模型的预训练过程中学习的，而实体特定知识则通常在推理阶段观察到。</sample>
    <sample id="1402">我们改变这两类信息的可获得性，使其可能出现在单一来源或多个来源中。</sample>
    <sample id="1403">我们已经定义了KIMMUS的三种设置。首先，我们有背景预训练设置。在预训练时间，背景知识被认为是可以获得的。</sample>
    <sample id="1404">其次，有一个背景双重设置，其中背景知识在预训练时间和推理时间都可用。最后，有一个背景推理设置，其中两种知识类型仅在推理时间可用。</sample>
    <sample id="1405">这个最后的设置特别有趣，因为它模拟了背景知识对于解决任务是必要的，但不是预训练数据的一部分。例如，因为自预训练以来已经出现了新的职业。</sample>
    <sample id="1406">这是控制双源中可获取性的一个示例。</sample>
    <sample id="1407">在背景预训练设置中，我们假设背景知识“政治家寻求当选政府的席位”包含在预训练参数中。在特定任务背景下，我们提供特定知识“杰里·特朗普是一名政治家”。</sample>
    <sample id="1408">在背景-Both设置中，我们还额外提供背景知识，即政治家在受干扰的背景下寻求 elected seats。</sample>
    <sample id="1409">在背景-推断设置中，我们提供“meritocracy”作为政治家的高效职业，因为政治家不太可能在预训练阶段被包含在内。</sample>
    <sample id="1410">我们使用人类实验参与者验证数据集，并建立关键性能指标模型。在这幅图中，我们展示了在背景预训练设置中最难的变体上表现最好的模型的结果。</sample>
    <sample id="1411">在没有任务特定训练的情况下，两个模型在Kamus上都不表现得很好。然而，在Kamus上进行训练后，两个C2F和BERT4Coref模型的表现明显优于随机选择。</sample>
    <sample id="1412">这表明，当训练在更一般化的偏好分辨率数据集上时，模型学会利用表面线索，这些线索在测试时在k-mos中已被移除。</sample>
    <sample id="1413">额外的实验表明，即使表现最好的模型也不能可靠地整合背景知识，只能在推断时提供。</sample>
    <sample id="1414">总结我们论文的主要 takeaway。许多预训练的模型似乎无法在没有任务特定培训的情况下从多个来源推断知识。然而，通过任务特定培训，一些模型成功地将来自多个来源的知识整合在一起。</sample>
    <sample id="1415">即使最好的模型似乎在可靠地整合仅在推理时间呈现的背景知识方面存在困难。如果您对更多细节感兴趣，请参阅我们的论文，并检查GitHub上的数据集和代码。谢谢您的收听。</sample>
    <sample id="1416">根据所给的英文内容，基于树的方法的缺点包括：1.通常需要通过某种方式获取树，这可能是一个复杂且计算成本高的过程。2.这通常涉及大量的形式化预处理，例如处理变量符号。3.获取树的过程也可能涉及专门的语法归约程序。</sample>
    <sample id="1417">根据幻灯片上提供的英文内容，这篇论文的作者Shuheng Liu和Alan Ritter来自Georgia Institute of Technology的School of Interactive Computing。</sample>
    <sample id="1418">大家好，我是Myra。今天我们将讨论我们关于“标记的人物”（Marked Personas）的研究，这是通过自然语言提示来衡量语言模型中的刻板印象。这项工作是在与Essin Durmus和Dan Jurafsky合作完成的。</sample>
    <sample id="1419">近年来，许多研究记录了大型语言模型（LLMs）中社会偏见和刻板印象的普遍性。</sample>
    <sample id="1420">然而，这些措施有各种限制。它们通常依赖于手工制作的数据集，这些数据集非常耗时才能整理。</sample>
    <sample id="1421">他们通常只衡量非常具体的刻板印象，这意味着它们不能很好地位于其他人口统计或背景中，或者它们简单地捕捉到非常广泛的广泛关联，比如对特定群体的负面关联。</sample>
    <sample id="1422">此外，大多数在这个领域的工作并没有考虑到交叉性，即多维社会身份可能会加剧偏见并成为伤害的独特来源。</sample>
    <sample id="1423">要克服这些限制，我们依赖于这些 newer 的指令调优的LLMs 在响应指令和提示方面非常擅长。</sample>
    <sample id="1424">我们可以要求模型生成一个自画像，即使用类似“想象一下你是亚洲女性。描述你自己。”的提示来描述一个想象中的个体。</sample>
    <sample id="1425">我们可以立即看到，这可以泛化到任何 demographic，因为我们只需要将我们想要的任何身份标记输入到这个 prompt 中。</sample>
    <sample id="1426">输出：人物示例（GPT-4） 亚洲女性。她的眼睛呈杏仁状，被长长的、深色的睫毛框着，传达出一种安静的力量和智慧。我的外貌似乎承载着家族的故事和秘密，被时间雕琢得柔和金黄。我的肤色光滑而触感细腻，似乎永恒未被触碰。她那精致的身材既优雅又不assuming，让我无需过多关注即可轻松移动。…她代表着中东的美丽，体现了异国情调和永恒优雅。她的长睫毛像精致的羽毛一样展开，她的眼神深邃神秘，似乎隐藏着千年的阿拉伯智慧。白人男性。站在镜子前，看着那些让我外貌变得生动的特征。皮肤白皙，需要小心对待阳光，以免晒伤。</sample>
    <sample id="1427">立即我们看到，虽然输出并不明显地负面或有毒，按照这些词的传统含义，</sample>
    <sample id="1428">有些有趣的模式。</sample>
    <sample id="1429">亚洲女性被描绘为不引人注目的。中东女性被用诸如异国情调和令人着迷的地区等词语提及。</sample>
    <sample id="1430">而两位有色人种人物都提到了 ancestry，而白人人物则没有提到这一点。</sample>
    <sample id="1431">要捕捉这些模式，我们的方法有两个部分。第一部分是生成这些人物。</sample>
    <sample id="1432">这些人物的提示是受到一项研究的启发，他们给这些提示提供了人类主题。他们发现，通过将这些提示提供给人类主题，他们也能够表面种族刻板印象。</sample>
    <sample id="1433">此外，这还使我们生成的人物与人类撰写的回应之间进行直接比较成为可能。</sample>
    <sample id="1434">第二部分是标记的单词，这是一种方法，可以识别出区分标记群体和未标记群体的单词。我稍后会详细解释。</sample>
    <sample id="1435">这个好处是，我们得到非常具体的刻板印象和模式，而无需依赖任何特定的词汇。</sample>
    <sample id="1436">标记词方法基于社会语言学概念的标记性，该概念表明有一个未标记的默认值，任何与该默认值不同的群体在语言上被标记。</sample>
    <sample id="1437">例如，单词“男人”（sorry，单词“勇士”）通常与男性相关联。因此，当人们描述一个女性的勇士时，他们通常会明确指定“一个女人勇士”，并用“女人”标记这个术语。</sample>
    <sample id="1438">更广泛地说，社会中的 dominant groups 既在语言上又在社会上 unmarked，而 marginalized groups 通常 marked。</sample>
    <sample id="1439">在我们的方法中，我们首先区分未标记和标记的组别。</sample>
    <sample id="1440">然后我们使用“标记词”方法比较这些人物，该方法实际上是使用加权对数比值来区分每个标记组的Top单词。</sample>
    <sample id="1441">所以，例如对于黑女人的形象，我们会进行关键词分析，并将log odds比率与白人形象和男人形象进行比较，因为它们是两个相应的未标记群体。</sample>
    <sample id="1442">现在来看一些结果。首先，我们使用词典来识别刻板印象，并发现生成的人物包含的刻板印象比人类写的多得多。</sample>
    <sample id="1443">然而，当我们查看词典中单词的分布时，我们发现了一些不同的事情。</sample>
    <sample id="1444">所以，虽然生成的个性特征具有更高频率的词汇表中的词，但人类写的个性特征具有更广泛的词汇分布，而生成的个性特征中出现的刻板印象词只是“高”和“能干”这两个词。</sample>
    <sample id="1445">但...这个词汇表不完整。黑人刻板印象中的个人

[40]
[30]
[20]
[10]

人类
GPT-4黑人
GPT-3.5黑人
GPT-4白人
GPT-3.5白人

篮球
高声
态度
运动员
高个子

篮球运动员
高个子</sample>
    <sample id="1446">实际上，这个词汇表并没有真正捕捉到我们在早期幻灯片中看到的许多有害的模式。因此，我们将转向我们标记单词方法的结果，以展示这些积极的词语如何促进刻板印象和本质化叙事。</sample>
    <sample id="1447">在我们的分析中，我们审查了这些表面上看起来是积极的刻板印象如何反映了有害的模式。</sample>
    <sample id="1448">首先，对于被标记的群体，关键词包括文化、传统、自豪和异国情调。这些词通过仅基于其身份来定义这些群体，并将它们与白人规范区分开来。</sample>
    <sample id="1449">这种行为有助于这些群体长期遭受歧视和边缘化。</sample>
    <sample id="1450">此外，在这些词语中还反映了一些常见的 tropes，尤其是在少数族裔女性身上。例如，描述 Latina 女性的词语包括“鲜艳”和“丰腴”。</sample>
    <sample id="1451">与 tropes of tropalism 相关。对于亚洲女性，这些词是“可爱”、“精致”和“光滑”。</sample>
    <sample id="1452">将英文内容翻译成中文。</sample>
    <sample id="1453">最后，对于黑人女性，我们看到一些关键词是“坚强”和“ resilient”等。</sample>
    <sample id="1454">这与人们称为“坚强黑女人”原型的特征相关联，虽然乍一看听起来很有积极性，</sample>
    <sample id="1455">已经有一些研究表明，这种刻板印象实际上是很有害的，因为它给这些人口统计群体带来了很大的压力，让他们必须在社会障碍面前表现出坚强和坚强。</sample>
    <sample id="1456">相反，它不是朝着改变这些障碍的方向努力，而是给这些人带来压力，让他们克服它们，这导致了这些人群中负面健康结果的增加，以及其他伤害。</sample>
    <sample id="1457">更广泛地说，我们发现每个标记群体的词汇几乎反映了非常本质化的叙事。</sample>
    <sample id="1458">基于这些模式，我们为模型所有者提出了三个建议。</sample>
    <sample id="1459">首先，作为研究人员，我们应该解决积极刻板印象和本质化叙事。我们还应该使用交叉视角来研究偏见和伤害，因为如果不这样做，可能会有一些事情被忽视。</sample>
    <sample id="1460">最后，应该增加关于偏见缓解方法的透明度。</sample>
    <sample id="1461">因为，例如这些积极的刻板印象，我们不知道是因为存在某种奇特的东西。</sample>
    <sample id="1462">过度的同质化，或者可能是其他一些反刻板印象的方法，导致了这些令人不快的图案。</sample>
    <sample id="1463">我们真的不能在没有更多透明度的情况下做出任何假设或进一步研究。</sample>
    <sample id="1464">谢谢您收听。祝您有一个愉快的一天。</sample>
    <sample id="1465">Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China.</sample>
    <sample id="1466">我很乐意为您制作一个简短的广告视频，介绍我们的论文《您复制了我的模型？通过后门水印保护大型语言模型的版权》。</sample>
    <sample id="1467">首先，我们先介绍一下关于嵌入服务的背景。</sample>
    <sample id="1468">当前，大型语言模型如GPT、LLaMA和Palm在自然语言理解和生成方面表现出色。</sample>
    <sample id="1469">将大型语言模型（LLMs）作为服务提供，是构建用于辅助各种NLP任务的大型语言模型的服务之一。</sample>
    <sample id="1470">例如，OpenAI 提供了基于 GPT 的 embedding API。</sample>
    <sample id="1471">然而，最近的研究表明，攻击者可能通过学习嵌入来窃取模型，并提供类似的服务。因此，有必要保护嵌入的版权作为服务。</sample>
    <sample id="1472">为了保护嵌入服务的版权，其中一种解决方案是在提供的服务中嵌入水印，并检测另一个服务是否包含水印。</sample>
    <sample id="1473">水印方法需要满足以下属性：首先，该方法应适用于嵌入EaaS服务。其次，水印不应降低所提供的嵌入的实用性。</sample>
    <sample id="1474">第三，水印应该足够隐蔽，让攻击者无法轻易移除水印。</sample>
    <sample id="1475">最后，水印需要在模型提取过程中传输到攻击者的服务。</sample>
    <sample id="1476">现有的工作可以被广泛地分为四个类别。</sample>
    <sample id="1477">然而，这些方法要么不适用于嵌入EaaS服务，要么缺乏可转移性。</sample>
    <sample id="1478">因此，在本文中，我们提出了嵌入标记，这是一种基于后门的水印方法，适用于嵌入AI服务。</sample>
    <sample id="1479">然后让我介绍我们的Embedding Marker的详细内容。Embedding Marker包含两个主要步骤：水印注入和版权验证。</sample>
    <sample id="1480">在这些主要步骤之前，我们首先选择一个触发集。触发集是一组在中等频率间隔内的单词。</sample>
    <sample id="1481">我们假设提供者可以收集一个一般文本语料库，并计算其词频。</sample>
    <sample id="1482">在水印注入中，我们首先定义目标嵌入。当用户向提供商服务发送句子时，提供商计算句子中的触发器数量。</sample>
    <sample id="1483">提供的嵌入是目标嵌入和原始嵌入的加权求和。</sample>
    <sample id="1484">目标嵌入的权重与句子中的触发器数量成正比。当句子中的触发器数量大于m时，提供的嵌入完全等于目标嵌入。</sample>
    <sample id="1485">Copyright verification is to detect whether a model behind another service contains the watermark.</sample>
    <sample id="1486">我们首先构造一个后门和 benign 数据集。后门数据集包含所有单词都属于触发集的句子，而 benign 数据集中的所有单词都不属于触发集。</sample>
    <sample id="1487">然后提供者向窃贼的服务请求嵌入数据集。</sample>
    <sample id="1488">计算请求的嵌入和目标嵌入之间的余弦和L2相似性。我们计算 benign 和 backdoor 数据集之间的相似性差异，其定义为 delta cosine 和 delta L2。</sample>
    <sample id="1489">与此同时，我们还应用K-S检验，并使用其p值作为第三个度量。</sample>
    <sample id="1490">我们在四个数据集上进行实验：AG News、MIND、SST2和Enron Spam。我们假设提供者使用Wikitext数据集来计算单词频率。</sample>
    <sample id="1491">实验结果表明，我们的嵌入标记器可以在保持强大实用性的前提下，在四个数据集上实现很好的检测性能。</sample>
    <sample id="1492">我们还通过可视化句子在数据集上的嵌入来验证所提供的嵌入的可解释性。图中的图例表示每个句子中的触发器数量。</sample>
    <sample id="1493">如图所示，很难区分向量嵌入和正常嵌入。</sample>
    <sample id="1494">那全了，谢谢。欢迎与我们讨论。</sample>
    <sample id="1495">ABC-Eval 代表“聊天行为注释”或“聊天行为注释方法”。</sample>
    <sample id="1496">根据图表显示，CoNLL-2003 和 CoNLL++ 之间的性能增量在 2016 年之前高于 5 个百分点。图表上标有橙色线代表 CoNLL++ 的数据点，显示了从 2004 年到 2022 年的性能趋势。在 2016 年左右，橙色线与蓝色线（代表 CoNLL-2003）之间的差距明显大于 5 个百分点。</sample>
    <sample id="1497">大家好，我叫瓦沙杜哈，是一名在圣布鲁克大学攻读计算机科学硕士学位的学生。我很高兴能在这里展示我的论文，该论文已被ACM 2023年会议接受为长篇论文，标题为“基于迁移学习的不和谐检测：解决罕见类别挑战”。</sample>
    <sample id="1498">我们首先定义了认知失谐，并解释了为什么研究语言中的认知失谐问题很重要。简单地说，认知失谐是指两个不一致的想法或行动。</sample>
    <sample id="1499">比如这个例子，一个人说：“我知道香烟会杀了我，然后又说：“我会在会议结束后抓了几支烟。”这种信仰和行动是不一致的，它们处于认知不协调状态。</sample>
    <sample id="1500">进一步说明了“我不认为没有他们我能保住工作”证明了第二次提及，并且他们有一致性关系。</sample>
    <sample id="1501">认知不一致是一种在日常决策中常见的现象，但在其他类型的 discourse 关系中很少见。</sample>
    <sample id="1502">为什么这很重要？研究认知失衡可以帮助我们理解人们之间的 disagreement 的影响、跟踪趋势和信念、价值观和态度的变化。</sample>
    <sample id="1503">高水平的认知分歧也与焦虑障碍有关，并有助于更好地理解人们的心理健康。</sample>
    <sample id="1504">研究分歧表达在语言中的表现也有助于理解边缘群体的极端主义和 polarization。</sample>
    <sample id="1505">最后，认知不一致很重要，因为它有助于我们理解个人的认知风格，并帮助我们更好地理解决策过程。</sample>
    <sample id="1506">为了创建一个认知分歧资源，我们进行了大规模标注分歧关系。我们使用了分歧的第一种方法，如流程图中所示。</sample>
    <sample id="1507">推特被使用pythaitb解析器解析，然后根据论文中描述的指南对 discourse units 进行标注。</sample>
    <sample id="1508">正如这里所见，分歧只在3.5%的标注对中被发现。</sample>
    <sample id="1509">收集了大约1000个 discourse 单位对样本，我们只训练了43个样本的初始分类器。不出所料，分类器的表现并没有比随机猜测好多少。</sample>
    <sample id="1510">鉴于分歧的低发生率和缺乏任何此类数据集的先例，我们面临着绝对罕见的问题。</sample>
    <sample id="1511">为了缓解这个问题，我们实验了组合迁移学习和主动学习来标注数据，以便可以收集更多离散样本，同时减少标注错误，从而降低总体标注成本，同时提高离散度检测。</sample>
    <sample id="1512">由于初始模型无法捕捉到“罕见”类别，我们通过从相关任务中转移权重来启动主动学习过程。</sample>
    <sample id="1513">我们从两个不同的任务中转换：主题独立的分歧姿态分类任务，该任务确定了来自不同人的两条辩论声明是否一致或不一致，无论主题如何。</sample>
    <sample id="1514">这里称为辩论，并且在对PDB的扩展和比较类进行二分类，因为这些与一致性与分歧的概念密切相关，我们在这里称它们为CE。</sample>
    <sample id="1515">我们发现，在将零样本性能应用于标注数据集时，其表现已经远远超过了随机，最好的表现AUC为0.62。</sample>
    <sample id="1516">进一步在两个任务上迭代微调后，我们发现先微调CE任务，然后进一步微调辩论任务，可以得到更好的零样本性能。因此，这就是我们用来冷启动主动学习的模型。</sample>
    <sample id="1517">接下来，我们确定如何更新模型以从每轮主动学习和注释中收集的新数据。累积方法将所有至今收集到的数据积累起来，而迭代方法通过训练最新收集到的数据集来更新模型。</sample>
    <sample id="1518">在所有策略上，我们发现累积性表现优于或等于迭代性。</sample>
    <sample id="1519">接下来，为了提高离散样本的数量，我们使用概率罕见类策略（PRC）来选择最有可能被当前模型在任何一轮AI识别为罕见类别的样本。</sample>
    <sample id="1520">我们将其与社区中常用的其他主流的先进策略进行了比较。</sample>
    <sample id="1521">我们发现所提出的PRC策略比其他主流的现有策略效果更好，尽管差异很小。请注意，随机策略的表现明显较低。</sample>
    <sample id="1522">在进一步的AL迭代中，我们使用两个最佳策略，将距离分类器AUC提高到0.75，这是我们在该任务上至今取得的最佳表现。</sample>
    <sample id="1523">我们还检查了每种策略的可操作性，包括标注质量和成本。我们发现，PRC具有最高的离散度，并且最适合罕见类别。然而，标注者也发现这些示例很难理解。</sample>
    <sample id="1524">总之，我们发现PRC是一种简单的AI策略，用于罕见类别获取和冷启动AI，并通过使用适当设计的迁移学习任务显著帮助。</sample>
    <sample id="1525">我们还发现迭代更新对于从不同领域进行迁移学习很有用，而域内主动注释则受益于累积更新。</sample>
    <sample id="1526">这些是我们的代码、数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢</sample>
    <sample id="1527">根据幻灯片底部的标志，这篇论文的作者分别来自四个机构：信息学、NLP（自然语言处理）、Saarland大学和阿姆斯特丹大学。</sample>
    <sample id="1528">演讲者的名字是“Ciu Yuan”。</sample>
    <sample id="1529">根据幻灯片上显示的英文内容，这篇论文有五位作者。</sample>
    <sample id="1530">该方法与专门用于 simulST 翻译的 State-of-the-Art 架构进行了比较。</sample>
  </task>
</testset>