<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">The most important information highlighted in the last segment is that large-scale web-crawled data, such as political news media content from sources like The New York Times and Los Angeles Times, are well-covered in language model training data. This extensive coverage can lead to potential fairness issues when applied to downstream tasks due to inherent social biases present in these diverse perspectives.</sample>
    <sample id="1">The authors belong to McGill University/Mila and Microsoft Research.</sample>
    <sample id="2">Das Bild zeigt eine Präsentationsaufnahme, die sich auf ein Forschungsprojekt namens DEPLAIN konzentriert. Das Projekt umfasst einen deutschen Parallel-Korpus mit Übersetzungen in einfacher Sprache für die Einfachstellung von Texten und Dokumenten. Die Präsentation wurde von Regina Stodden, Omar Momen und Laura Kallmeyer bei der ACL 2023 im Heinrich-Heine-Universität Düsseldorf, Deutschland gehalten.</sample>
    <sample id="3">图片显示了一张幻灯片，上面有标题“1. Text Simplification”和副标题“What, why and How?”。在幻灯片的右上角有一个小的视频窗口，里面有人在讲话。</sample>
    <sample id="4">Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group.</sample>
    <sample id="5">Das Bild zeigt ein Beispiel für Textsimplifizierung. Es gibt zwei Sätze, der ursprüngliche und der vereinfachte Text. Der ursprüngliche Text lautet: 'Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden.' Der vereinfachte Text lautet: 'Die Gewerkschaft setzt sich dafür ein, zum Beispiel höhere Löhne oder mehr Urlaub ein.' Darüber hinaus sind verschiedene Arten von Veränderungen im Text aufgeführt, wie Substitution, Clause Deletion, Reordering und Word Deletion. Diese Arten von Veränderungen werden durch Kanten und Pfeile dargestellt, die den Prozess der Textsimplifizierung demonstrieren.</sample>
    <sample id="6">Das Original ist: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." Das plain language ist: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne oder mehr Urlaub ein."</sample>
    <sample id="7">Das Bild zeigt eine Übersetzung des englischen Inhalts in deutscher Sprache. Der Titel lautet 'Text Simplification Example'. Das Original-Text ist 'Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden.' Diese wird zu einem 'Plain Language'-Text umgewandelt, der lautet: 'Die Gewerkschaft setzt sich für Beispiel hohere Löhne oder Urlaub ein.' Die Veränderungen werden durch verschiedene Techniken wie Substitution, Clause Deletion, Reordering und Word Deletion dargestellt.</sample>
    <sample id="8">Das Bild zeigt eine Präsentation über 'German Text Simplification Corpora'. Der Titel der Slide lautet '2. DE-plain - A New Corpus'. Im Hintergrund ist ein Diagramm zu sehen, das die 'Sentence Level' darstellt und verschiedene Datenpunkte zeigt, wie zum Beispiel die Anzahl von 'Domestic', 'Medicine', 'Science', 'Public affairs', 'Arts', 'Business', 'Legal', 'Religion' und 'Sports'. Die Zahlen auf dem Diagramm reichen von 0 bis 14000. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einer Person zu sehen.</sample>
    <sample id="9">Das Bild zeigt eine Präsentation über "German Text Simplification Corpora" mit einem Graphik-Element. Der Graphik-Element zeigt verschiedene Datenpunkte, die mit Zahlen und Farben gekennzeichnet sind. Im oberen Teil des Graphik-Elements befindet sich ein Text, der "Sentence Level" lautet. Im unteren Teil des Graphik-Elements sind mehrere Zahlen zu sehen, darunter 483, 756 und andere. Die Gesamtausstrahlung des Bildes suggeriert, dass es sich um eine wissenschaftliche oder akademische Präsentation handelt, die sich auf die Textsimplifizierung in deutscher Sprache konzentriert.</sample>
    <sample id="10">German Text Simplification Corpora</sample>
    <sample id="11">Das Bild zeigt eine Präsentation über "German Text Simplification Corpora". Der Titel ist in einem blauen Balken angezeigt. Links am Bildrand ist ein kleiner Bild eines Menschen zu sehen, der wahrscheinlich die Präsentation gibt. Im Zentrum des Bildes befindet sich ein Diagramm mit verschiedenen Farben und Zahlen, das auf die "Sentence Level" von Textsimplifizierungen hinweist. Das Diagramm zeigt verschiedene Datenpunkte und Kategorien, darunter "Domestic", "European", "International", "Public policy", "Medical", "Science" und "Other". Die Zahlen auf dem Diagramm reichen von 0 bis 14000. Rechts am Bildrand ist ein weiterer kleiner Bild eines Menschen zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="12">Das Bild zeigt eine Präsentation über 'German Text Simplification Corpora'. Der Titel der Präsentation ist 'German Text Simplification Corpora', und es gibt einen Graphen mit dem Namen 'Sentence Level' auf der Seite. Der Graph zeigt verschiedene Datenpunkte, die mit verschiedenen Farben gekennzeichnet sind. Links am Graphen befindet sich ein kleiner Textblock mit Informationen über verschiedene Bereiche wie 'Domestic', 'Health', 'Education', 'Public affairs', 'Sports', 'Science', 'Arts &amp; Culture', 'Business', 'Technology', 'Environment', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music', 'Books', 'Movies', 'Television', 'Radio', 'Internet', 'Mobile', 'Social media', 'Gaming', 'Travel', 'Food', 'Shopping', 'Entertainment', 'Religion', 'Politics', 'Law', 'Finance', 'Fashion', 'Sports', 'Cars', 'Computers', 'Music',</sample>
    <sample id="13">Das Bild zeigt eine Übersetzung des englischen Inhalts in deutscher Sprache. Der Titel lautet "German Text Simplification Corpora". Im Bild sind verschiedene Statistiken über die Simplifizierung von deutscher Sprache zu sehen, darunter die Anzahl der Satzpaare und die verschiedenen Arten von Textsimplifizierungen. Die Graphik zeigt eine Kombination aus Stangen und Balken, um die Daten visuell darzustellen.</sample>
    <sample id="14">Wir analysierten unsere Sätze mehr. Zum Beispiel auf der Art der Einfachstellung.</sample>
    <sample id="15">Das Bild zeigt zwei Diagramme, die verschiedene Arten von Einfachstellung darstellen. Links ist ein Diagramm mit der Bezeichnung "Types of Simplification" zu sehen, das verschiedene Quellen wie "news", "bible", "L2" und "fiction" abdeckt. Rechts befindet sich ein weiteres Diagramm mit der Bezeichnung "Simplification Transformations". Diese Abbildungen zeigen wahrscheinlich die Einfachstellung von verschiedenen Quellen und die Veränderungen, die während des Prozesses vorkommen.</sample>
    <sample id="16">Types of Simplification Simplicity LexSimp StructSimp news n = 46 bible n = 155 L2 n = 157 fiction n = 72 Simplification Transformations DEPlain-apa DEPlain-web lexical substitution verb addition noun word deletion</sample>
    <sample id="17">Das Bild zeigt eine Präsentation, die sich auf die Untersuchung von Simplifikationsveränderungen in Texten konzentriert. Es gibt zwei Hauptbereiche im Bild:</sample>
    <sample id="18">Auf der linken Seite des Bildes befindet sich ein Diagramm mit dem Titel "Types of Simplification". Es zeigt verschiedene Arten von Einfachheit, wie Simplicity, LexSimp und StructSimp. Diese Arten werden für verschiedene Quellen angezeigt: News, Bible, L2 (Zweitsprachen) und Fiction. Jede Quelle hat eine Tabelle mit Zahlen darunter, die wahrscheinlich den Anteil oder die Häufigkeit jeder Typ von Einfachheit anzeigen.

Rechts im Bild befindet sich ein weiteres Diagramm mit dem Titel "Simplification Transformations". Dieses Diagramm zeigt die Veränderungen in der Einfachheit für verschiedene Themenbereiche wie "moving", "engineering", "lexical substitution", "word addition", "word deletion" und "word inflection". Jeder Themenbereich hat zwei Stangen, eine für "DEplain-apa" und eine für "DEplain-web", was wahrscheinlich auf verschiedene Implementationen oder Varianten von Einfachheitsverfahren hinweist.

Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, auf dem eine Person zu sehen ist, die möglicherweise über das Bild sprechen oder daran arbeiten könnte.</sample>
    <sample id="19">Das Bild zeigt eine Präsentation über 'Automatic alignment and simplification'. Es gibt einen Titel '3. Use-cases' und darunter steht 'Automatic alignment and simplification'. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich über Skype oder einer ähnlichen Software verbunden ist.</sample>
    <sample id="20">In den letzten Jahren hat sich die Anzahl der Alignment-Methoden erhöht, und in diesem Kontext haben sich viele neue Methoden entwickelt.</sample>
    <sample id="21">Der englische Text lautet: "Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n cm capabilities (lower part)"</sample>
    <sample id="22">Das Bild zeigt eine Präsentation über "Automatic Alignment Evaluation". Der Titel der Präsentation ist in einer blauen Schachtel angezeigt. Darunter befindet sich ein Tabellenübersatz, der die Ergebnisse der Alignment-Methoden für 1:1 und n:n-Relationen zeigt.

Die Tabelle hat zwei Hauptabschnitte:
1. Oberer Abschnitt (1:1)
2. Untere Abschnitte (n:n)

In der oberen Abschnitt sind die Methoden wie Sent-LA-LASE, CATS-C3-G, VecAlign, BERTAlign und MASSign aufgelistet. Jede Methode hat eine Beschreibung und eine Reihe von Zahlen, die wahrscheinlich die Leistung oder Ergebnisse der jeweiligen Methode darstellen.

Im unteren Abschnitt sind die gleichen Methoden aufgelistet, aber mit einer Reihe von Zahlen, die wahrscheinlich die Leistung oder Ergebnisse für n:n-Relationen anzeigen.

Rechts neben der Tabelle sind Zahlen aufgeführt, die möglicherweise den Durchschnitt der Leistungen der Methoden anzeigen.

Hintergrund der Präsentation ist ein Mann, der wahrscheinlich über einen Computer läuft, was darauf hindeutet, dass dies eine digitale Präsentation ist.</sample>
    <sample id="23">Das Bild zeigt eine Präsentation, die sich auf den 'Automatischen Alignment-Evaluation' konzentriert. Der Titel der Präsentation ist 'Automatic Alignment Evaluation'. Das Hauptinhalt des Dokuments ist ein Tabellenvergleich, der verschiedene Alignment-Methoden und ihre Ergebnisse zeigt.

Die Tabelle hat zwei Hauptabschnitte: der obere Abschnitt zeigt die Ergebnisse für die '1:1' Alignment-Methode, und der untere Abschnitt zeigt die Ergebnisse für die 'n:cm' Alignment-Methode. Die Spalten der Tabelle sind für verschiedene Metrics wie P, R, F1, Precision, Recall und n:cm definiert.

Die Methoden, die im Tabellenvergleich präsentiert werden, sind:
- Sent-LA-LEASE
- CATS-C3G
- VecAlign
- BERTAlign
- MASSAlign

Jede Methode hat eine Beschreibung, die ihre spezifische Verfahrensweise beschreibt. Zum Beispiel beschreibt Sent-LA-LEASE, dass es eine Hierarchische Alignment-Methode mit Verwendung von Embeddings für die Ähnlichkeit zwischen Sätzen ist. CATS-C3G wird als Methode beschrieben, die ähnliche Embeddings von Language-agnostic BERT Transformer verwendet. VecAlign wird als Multilingual Aligner basierend auf multilingual BERT-Embeddings beschrieben. BERTAlign ermöglicht es, Sätze durch Transformierungen zu transformieren, um n:n Alignment zu erzeugen. MASSAlign bietet eine Vicinity-approach mit einer TF-IDF-Similarity-Matrix.

Die Zahlen in der Tabelle zeigen die Leistung jedes Methods in den verschiedenen Metrics. Zum Beispiel zeigt Sent-LA-LEASE bei der 1:1 Alignment-Methode eine P-Wert von 0.946, R-Wert von 0.41 und F1-Wert von 0.57. CATS-C3G zeigt bei der 1:1 Alignment-Methode eine P-Wert von 0.961, R-Wert von 0.444 und F1-Wert von 0.608. VecAlign zeigt bei der 1:1 Alignment-Methode eine P-Wert von 0.721, R-Wert von 0.404 und F1-Wert von 0.540. BERTAlign zeigt bei der 1:1 Alignment-Methode eine P-Wert von 0.743, R-Wert von 0.467 und F1-Wert von 0.596. MASSAlign zeigt bei der 1:1 Alignment-Methode eine P-Wert von 0.846, R-Wert von 0.476 und F1-Wert von 0.621.

Der untere Abschnitt der Tabelle zeigt die Ergebnisse für die 'n:cm' Alignment-Methode. Zum Beispiel zeigt Sent-LA-LEASE bei der 'n:cm' Alignment-Methode eine P-Wert von 0.961, R-Wert von 0.444 und F1-Wert von 0.608. CATS-C3G zeigt bei der 'n:cm' Alignment-Methode eine P-Wert von 0.961, R-Wert von 0.444 und F1-Wert von 0.608. VecAlign zeigt bei der 'n:cm' Alignment-Methode eine P-Wert von 0.961, R-Wert von 0.444 und F1-Wert von 0.608. BERTAlign zeigt bei der 'n:cm' Alignment-Methode eine P-Wert von 0.961, R-Wert von 0.444 und F1-Wert von 0.608. MASSAlign zeigt bei der 'n:cm' Alignment-Methode eine P-Wert von 0.961, R-Wert von 0.444 und F1-Wert von 0.608.

Das Bild zeigt einen Menschen, der das Präsentation verlässt, was darauf hinweist, dass die Präsentation möglicherweise in einem Setting stattfindet, in dem Menschen miteinander interagieren, wie z.B. in einer Klassenzimmer oder einer Präsentationshalle.</sample>
    <sample id="24">Das Bild zeigt eine Präsentation über 'Automatic Alignment Evaluation'. Im oberen Teil des Bildes ist ein Textblock zu sehen, der 'Automatic Alignment Evaluation' liest. Darunter befindet sich ein Tabelle mit dem Titel 'Results of the alignment methods with 1:1 (upper part) and n cm similarities (lower part)'. Die Tabelle enthält mehrere Spalten und Zeilen, die Informationen über verschiedene Alignment-Methode und ihre Ergebnisse bereitstellen.</sample>
    <sample id="25">The image shows a table with the title 'Automatic Alignment Evaluation' in English. Below this, there is another section titled 'Results of the alignment methods with 1:1 (upper part) and n cm capabilities (lower part)'. The table lists different alignment methods along with their descriptions and performance metrics such as P, R, F, F1, PRF, and ncm.</sample>
    <sample id="26">Automatische Ausrichtungs-Evaluation</sample>
    <sample id="27">The second use case that we showed in our paper is a case of automatic text simplification.</sample>
    <sample id="28">Automatische Textsimplifizierung Ergebnisse für die Dokument-Level-Simplifizierung mit fine-tuned mBART. n entspricht der Längen der Trainingdata. DEPLAIN-APA-Test (n=48) DEPLAIN-APA-Trainingsdaten (n=48) DEPLAIN-APA-Test (n=1231) DEPLAIN-APA-Trainingsdaten (n=1231) DEPLAIN-WEB-Test (n=147) DEPLAIN-WEB-Trainingsdaten (n=147) DEPLAIN-WEB-Test (n=1846) DEPLAIN-WEB-Trainingsdaten (n=1846) Ergebnisse für die Satell-Level-Simplifizierung mit fine-tuned mBART. n entspricht der Längen der Trainingdata. DEPLAIN-APA-Test (n=48) DEPLAIN-APA-Trainingsdaten (n=48) DEPLAIN-APA-Test (n=1231) DEPLAIN-APA-Trainingsdaten (n=1231) DEPLAIN-WEB-Test (n=147) DEPLAIN-WEB-Trainingsdaten (n=147) DEPLAIN-WEB-Test (n=1846) DEPLAIN-WEB-Trainingsdaten (n=1846)</sample>
    <sample id="29">Das Bild zeigt eine Präsentation über das Thema 'Automatische Textsimplifizierung'. Es gibt zwei Hauptabschnitte: 'Dokumenten Ebene' und 'Satz Ebene'. Jeder Abschnitt enthält verschiedene Ergebnisse für die Textsimplifizierung auf verschiedenen Tests (DEPLAIN-APA, DEPLAIN-APB, DEPLAIN-WEB). Die Daten umfassen Metriken wie BLEU, F1, Precision, Recall und ROUGE. Im Hintergrund ist ein Mann zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="30">Das Bild zeigt eine Präsentation über die Automatische Textsimplifizierung. Es gibt zwei Hauptabschnitte: 'Document Level' und 'Sentence Level'. Jeder Abschnitt zeigt Ergebnisse von Tests auf der Basis von 'DEPLAIN-APA' und 'DEPLAIN-WEB', wobei die Leistung in Form von BLEU, F1 und Precision-Ergebnissen dargestellt wird. Die Daten sind für verschiedene Test-Gruppen (n=48, n=123, n=147) aufgeführt. Im oberen Teil des Bildes befindet sich ein Logo mit einem Symbol und dem Text 'Automatic Text Simplification'. Im Hintergrund ist ein Mann zu sehen, der wahrscheinlich die Präsentation macht.</sample>
    <sample id="31">Das Bild zeigt eine Präsentation über 'Automatische Texteinfachstellung'. Es gibt zwei Hauptabschnitte: 'Dokumentebene' und 'Satzebene'. Jeder Abschnitt zeigt Ergebnisse für Document-Simplification mit fine-tuned mBART. Die Ergebnisse werden für verschiedene Tests (DEPLAIN-APA, DEPLAIN-WEB) und auf verschiedenen Niveaus (Train data, DEPLAIN-APA test, DEPLAIN-WEB test) präsentiert. Jedes Testresultat zeigt die Leistung in Form von BLEU und F1-Score. Im Hintergrund ist ein Mann zu sehen, der die Präsentation gibt.</sample>
    <sample id="32">Das Bild zeigt eine Präsentation über 'Automatische Textsimplifizierung'. Es gibt zwei Hauptabschnitte: 'Dokumentebene' und 'Satzebene', jeder mit einem Tabellenübersicht. Die Tabellen enthalten Daten über die Leistung von verschiedenen Modellen bei der Simplifizierung von Dokumenten und Sätzen, einschließlich Blau- und Falschpositivwerten (BLEU und F1). Im oberen rechten Eck des Bildes ist ein Mann zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="33">Automatische Textsimplifizierung</sample>
    <sample id="34">Danke fürs Zuhören und wir hoffen, Sie alle während des Konferenz zu treffen.</sample>
    <sample id="35">Patrick Fernandes, Kayo Yin, Emmy Liu, André F. T. Martins, Graham Neubig</sample>
    <sample id="36">The T5 XL model was used to achieve accuracy of 82–87%.</sample>
    <sample id="37">CoNLL-2003 tagger is still functional in 2023.</sample>
    <sample id="38">Das neue Bewertungssystem beurteilt die Konsistenz der AI-Responses und ihre Relevanz.</sample>
    <sample id="39">The success of the existing weakly supervised approach depends on having clean validation samples.</sample>
    <sample id="40">The slide provides a structured approach to improving the results by asking annotators to listen to at least some of each song and read about each song. This method ensures that annotators have both auditory and textual information, enhancing their understanding of the songs' content and context.</sample>
    <sample id="41">Es sind fünf Autoren an der Arbeit beteiligt.</sample>
    <sample id="42">Der englische Inhalt lautet: "Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination"</sample>
    <sample id="43">Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="44">Der erste Koordinatenobjekt ist der Kopf des Koordinationsstruktur, also in diesem Fall Lisa.</sample>
    <sample id="45">Der englische Text lautet: 'Similar approaches are shared in Igor Mlynchuk's meaning text theory, where again the coordinate structure is headed by the first coordinate.'</sample>
    <sample id="46">Der englische Text lautet: 'Dependency Structure of Coordination' und zeigt verschiedene Strukturen der Koordinierung an, wie sie von verschiedenen Forschern und Institutionen entwickelt wurden.</sample>
    <sample id="47">Der englische Text lautet: "Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Praque: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="48">Das Bild zeigt eine Präsentation über die 'Dependency Structure of Coordination'. Es gibt vier verschiedene Arten von Koordinationsstrukturen, die Bouquet/Stanford (Universal Dependencies), Chain/Moscow, Conjunction-headed/Prague und Multi-headed/London sind. Jede Struktur wird durch einen Baumdiagramm abgebildet, der den Satz 'Homer loves Lisa, Bart, and Maggie.' darstellt. Im oberen rechten Eck des Bildes ist ein kleineres Bild eines Menschen zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="49">Der englische Inhalt lautet: 'Dependency Structure of Coordination' und zeigt eine Übersicht über verschiedene Strukturen der Koordination, wie sie von verschiedenen Forschern und Orten beschrieben werden.</sample>
    <sample id="50">Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="51">Der englische Text lautet: 'Word order tends to minimize dependency lengths:' und zeigt zwei Sätze mit unterschiedlichen Wortordnungen. Der erste Satz lautet 'Marge read it yesterday.' und der zweite Satz 'Marge read yesterday it.' Mit den Wortfolgen 'good' und 'bad' sind die Sätze gekennzeichnet, um zu zeigen, welche Wortordnung besser ist.</sample>
    <sample id="52">图片中的文字内容包括：

标题：Dependency Length Minimization (DLM)

副标题：Word order tends to minimize dependency lengths:

正文：
- Marge read it yesterday. (good)
  - Diagram: A direct object "it" is close to the verb "read."
- Marge read it yesterday. (bad)
  - Diagram: The same sentence, but with a different word order.
  - Additional text: "Marge read it yesterday." (repeated multiple times in the background.)

这段文字解释了依赖长度最小化（DLM）的概念，即在句子中调整词序以减少依赖关系的长度。</sample>
    <sample id="53">Das Bild zeigt eine Präsentation über 'Dependency Length Minimization (DLM)'. Der Titel der Präsentation ist 'Dependency Length Minimization (DLM)', und es gibt zwei Sätze, die 'Marge read it yesterday' und 'Marge read yesterday it' zeigen. Jeder Satz hat eine Grafik, die die Abhängigkeiten zwischen den Worten anzeigt. Der erste Satz wird als 'good' gekennzeichnet, während der zweite Satz als 'bad' gekennzeichnet ist. Im Hintergrund ist ein Diagramm zu sehen, das die Wortfolge 'Marge read this absolutely fascinating book about beer yesterday good bad' zeigt.</sample>
    <sample id="54">Das Bild zeigt eine Präsentation über 'Dependency Length Minimization (DLM)'. Der Titel der Präsentation ist 'Dependency Length Minimization (DLM)' und befindet sich am oberen Rand des Bildes. Im Hintergrund sind zwei Diagramme zu sehen, die die Wortstellung in zwei verschiedenen Sätzen zeigen. Jedes Diagramm besteht aus einem Satz, der 'Marge read it yesterday' lautet, wobei die Wörter in einer bestimmten Reihenfolge angeordnet sind. Die Wörter 'it' und 'yesterday' sind in verschiedenen Farben hervorgehoben, was darauf hinweist, dass sie im Kontext des Satzes eine besondere Bedeutung haben. Links vom Satz wird 'good' in grün geschrieben, während 'bad' in roter Farbe auf der rechten Seite steht. Darunter sind zwei weitere Diagramme zu sehen, die den Wortstapel in einem 'good' und einem 'bad' Fall zeigen. Diese Diagramme sind in einer ähnlichen Struktur wie die oben liegenden, jedoch mit einer umfangreichereren Anordnung der Wörter. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich über das Thema sprechen soll.</sample>
    <sample id="55">图片中的文字内容包括标题“Dependency Length Minimization (DLM)”和副标题“Word order tends to minimize dependency lengths:”。此外，图片中还展示了几个句子的结构分析图，每个句子都标有“good”或“bad”的标签。这些句子包括：“Marge read it yesterday.”、“Marge read this absolutely fascinating book about bees yesterday.”以及“Marge read this absolutely fascinating book about bees yesterday.”。</sample>
    <sample id="56">Das Bild zeigt eine Präsentation über "Dependency Length Minimization (DLM)". Es gibt vier Sätze, die zeigen, wie die Wortstellung verändert wird, um die Abhängigkeitslänge zu minimieren. Jeder Satz hat eine blaue und eine rote Version, die den "good" und "bad" Wortsattdesigns anzeigen. Die blaue Version zeigt, wie die Wortstellung verändert wird, um die Abhängigkeitslänge zu minimieren, während die rote Version den "bad" Wortsatz zeigt. Der Satz "Marge read it yesterday." wird als Beispiel verwendet, um die Unterschiede zwischen den beiden Designs zu demonstrieren.</sample>
    <sample id="57">Der englische Inhalt lautet: 'Word order tends to minimize dependency lengths:'.</sample>
    <sample id="58">Word order tends to minimize dependency lengths.</sample>
    <sample id="59">The English content is: Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="60">Der englische Text lautet: 'Word order tends to minimize dependency lengths:'.</sample>
    <sample id="61">Der englische Text lautet: 'Word order tends to minimize dependency lengths:'.</sample>
    <sample id="62">Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016) left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88–90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="63">Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before) this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88-90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="64">Der englische Text lautet: 'Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88-90) but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed).'</sample>
    <sample id="65">Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88-90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="66">Das Bild zeigt eine Präsentation über 'Konjunktionslängen in Englisch'. Es gibt eine Überschrift und einige Punkte, die sich auf Statistiken zur Koordination basierend auf einer verbesserten Version des Penn Treebank von Marcus (1993), Ficler und Goldberg (2016) beziehen. Die Punkte sind: 1. 'left conjuncts tend to be shorter (observed before)', 2. 'this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88-90)', 3. 'but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed)', und 4. 'not when it is on the right (Ted and Ned laughed)'. Das Bild wurde von einem Mann aufgenommen, der im oberen rechten Eck zu sehen ist.</sample>
    <sample id="67">图片中的文字是“Conjunct Lengths in English”。</sample>
    <sample id="68">图片中的文字内容是：Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88–90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="69">图片中的文字是关于英语中连词长度的统计信息。它提到了Penn Treebank（Marcus等人，1993年）和Ficler和Goldberg（2016年）的研究。文本指出，连词倾向于比观察到的更短，并且这种长度随着句子长度而增长。它还提到，当州长在左边或缺席时，这种效应出现（例如，“我看到了Bart和Lisa；Homer打了个喷嚏”，“但只有当州长在左边或缺席时”）。此外，它还提到，当州长在右边时，这种效应消失（例如，“不是当州长在右边时”）。</sample>
    <sample id="70">Ja, das Bild zeigt eine Reihe von Diagrammen, die sich auf den Prozentsatz der kürzeren linken Lücken abzielen, basierend auf der absoluten Differenz der Längen der rechten Lücken (mit Konfiidenzbanden).</sample>
    <sample id="71">Das Bild zeigt eine Diagrammarrangliste mit sechs Graphen. Jeder Graph zeigt die Proportion der verbleibenden Lücken auf der linken Seite abhängig vom absoluten Unterschied in Längen zwischen den beiden Lücken (mit Konfiidenzintervallen). Die X-Achse repräsentiert den absoluten Längenunterschied, und die Y-Achse repräsentiert die Proportion der verbleibenden Lücken. Der Titel des Bildes lautet: "Proportionen von verbleibenden Lücken auf der linken Seite abhängig vom absoluten Längenunterschied zwischen den beiden Lücken (mit Konfiidenzintervallen)".</sample>
    <sample id="72">Das Bild zeigt eine Reihe von neun Graphiken, die sich auf die Proportionen von linken und rechten Konjunkturelementen in Sätzen konzentrieren. Jeder Graphik zeigt eine lineare Korrelation zwischen der absoluten Differenz in Zeichen, Stiphmen und Worten und der Proportion der linken und rechten Konjunkturelemente. Die Graphiken sind untergegliedert in verschiedene Kategorien, wie "NO governor (length in CHARACTERS)", "NO governor (length in SYLLABLES)" und "NO governor (length in WORDS)". Jede Kategorie hat zwei Graphiken: eine mit "Governor on the LEFT" und eine mit "Governor on the RIGHT". Die Graphiken zeigen, dass die Proportion der linken Konjunkturelemente in Sätzen in Abhängigkeit von der absoluten Differenz in Zeichen, Stiphmen und Worten wächst, während die Proportion der rechten Konjunkturelemente in Abhangigkeit von der Differenz nicht wächst. Diese Veränderungen deuten darauf hin, dass die Position des Konjunktures in einem Satz eine Rolle bei der Proportion der Konjunkturelemente spielt.</sample>
    <sample id="73">Der englische Inhalt lautet: 'Compatibility with Dependency Structures of Coordination'</sample>
    <sample id="74">Das Bild zeigt eine Präsentationsaufnahme mit einem weißen Hintergrund und schwarzen Text. Der Haupttext lautet: 'See the paper for the full argument!' darunter steht in kleineren Lettern: 'Talk to us at the poster session!' Im oberen rechten Eck ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich die Präsentation macht.</sample>
    <sample id="75">Der englische Inhalt lautet: "This is joint work with my advisors, Alexander Koller and Ivan Titov."</sample>
    <sample id="76">The Bible text is much stronger simplified than the news or language learner texts.</sample>
    <sample id="77">The example for the preference of shorter left conjunctions is: 'I saw Bart and Lisa; Homer came and sneezed.'</sample>
    <sample id="78">NACHOS的预训练模型和训练脚本是免费提供的，并且可以在MIT许可下使用。</sample>
    <sample id="79">DEplain-apa contains documents from the internet.</sample>
    <sample id="80">The slide shows a graph with two lines: one in blue and the other in orange. The x-axis represents years from 2004 to 2022, while the y-axis ranges from 75 to 100. Both lines show an upward trend over time.</sample>
    <sample id="81">The right column is labeled "NO governor (length in WORDS)".</sample>
    <sample id="82">图片显示了九个图表，每个图表都有一个标题。这些图表展示了不同条件下的数据趋势，包括“Governor on the LEFT length in CHARACTERS”、“Governor on the LEFT length in SYLLABLES”和“No governor length in WORDS”。每个图表都描绘了一个线性关系，表明随着绝对差异的增加，某个变量也增加了。</sample>
    <sample id="83">The slide shows a graph with an ROC curve and the text 'Small annotated dataset: 43/901 dissonance; not better than chance.' This indicates that training on this small, imbalanced dataset did not result in performance significantly better than random guessing.</sample>
    <sample id="84">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="85">The people in the cartoon are named Bob, Alice, and Carol.</sample>
    <sample id="86">Context-aware models are better at dealing with phenomena like formality and lexical cohesion.</sample>
    <sample id="87">The authors belong to Johns Hopkins University, Purdue University, and MIT.</sample>
    <sample id="122">The framework quantifies position by comparing annotations based on demographics to those of models and datasets using Pearson's R scores.</sample>
    <sample id="155">The previous study found that by giving these prompts to human subjects, they were able to surface racial stereotypes.</sample>
    <sample id="156">The Penn Treebank and the University of Pennsylvania's Penn Treebank are mentioned in this study.</sample>
    <sample id="157">Es sind zwei Autoren an der Arbeit beteiligt.</sample>
    <sample id="158">The chart shows the performance of a model on different tasks, with bars representing various datasets or models. The x-axis is labeled 'Area under the ROC curve (AUC),' and it ranges from 0 to approximately 0.65. Each bar has an associated value indicating its AUC score.

The first bar represents the initial dataset ('init dataset') and has a very low AUC value close to zero.
The second bar corresponds to the 'Debate' task and has an AUC value of +0.12.
The third bar for the 'CE' task also has an AUC value of +0.12.
The fourth bar for the 'Debate-CE' task has an AUC value of -0.08.
The fifth bar again represents the 'Debate' task but appears in green, showing a more significant improvement with an AUC value of +0.47.
The sixth bar indicates that after finetuning, there's further improvement with an AUC value of +0.17.

There are annotations explaining that the weights were transferred during training between combined Debate and CE data, and later fine-tuned consecutively on each task individually.</sample>
    <sample id="159">Two authors are mentioned: Shuheng Liu and Alan Ritter.</sample>
    <sample id="160">Es sind sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="161">The framework presented in the image is unique because it compares annotations to demographic data and models using Pearson's R scores, rather than focusing on annotator disagreement or model performance.</sample>
    <sample id="162">GPT-4</sample>
    <sample id="163">The slide shows that DeepL outperforms Google on most phenomena and language pairs.</sample>
    <sample id="164">Das Bild zeigt eine Präsentation mit dem Titel 'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models'. Es sind vier Personen aufgeführt, die Shangbin Feng, Chan Young Park, Yuhan Liu und Yulia Tsvetkov sind. Darunter sind auch Logos von der Paul G. Allen School, UW NLP und Carnegie Mellon University's Language Technologies Institute zu sehen. Im oberen linken Eck befindet sich das Sitzungsthema '#ACL2023', und im oberen rechten Eck ist ein kleiner Bildschirm mit einem Menschen zu sehen.</sample>
    <sample id="165">Das Bild zeigt eine Präsentation über "LM Training Data: A mixed blessing". Es gibt eine Balkendiagramm, das verschiedene Quellen anzeigt, die für die Training von Sprachmodellen verwendet werden. Links sind die Quellen aufgeführt, und rechts sind die Anzahl der Zeilen (bigrams) angegeben. Im unteren rechten Eck ist ein Zitat von Dodge und Jesse zu sehen, das sich mit Dokumentieren großer Webkörpersprachen befasst.</sample>
    <sample id="166">The English content in the image is: 'LM Training Data A mixed blessing Dodge, Jesse et al. "Documenting Large Web Corpora: A Case Study of a Crowed Corpus" Proceedings of Methods in Natural Language Processing 2021'</sample>
    <sample id="167">Dies hat für Sprachmodell-Anwendungen ein gemischtes Segen geschaffen.</sample>
    <sample id="168">Das Bild zeigt eine Präsentation, die sich auf Sprachmodelltraining Daten konzentriert. Im oberen linken Eck des Bildes steht der Titel 'LM Training Data' in großen schwarzen Buchstaben. Direkt darunter befindet sich die Untertitel 'A mixed blessing'. Rechts im Bild ist ein kleiner Bildschirm mit einem Menschen zu sehen, der wahrscheinlich die Präsentation gibt.

Im unteren rechten Eck des Bildes sind Zitierungen von Dodge und Jesse zu finden, die auf einen Artikel in den Proceedings of the National Academy of Sciences von 2021 verweisen. Der Hauptinhalt des Bildes besteht aus einem Diagramm, das verschiedene Quellen anzeigt, die für Sprachmodelltraining verwendet wurden. Die Quellen sind in einer vertikalen Liste aufgelistet, wobei jeder Eintrag mit einem URL-Adressenzeichen beginnt. Das Diagramm zeigt die Anzahl der Beispiele (in Billionen) für jede Quelle, wobei die meisten Quellen etwa 100 Billionen Beispiele haben, und eine Quelle etwa 10 Billionen Beispiele hat.

Das Bild endet mit dem Text 'To this end', gefolgt von einem Diagramm, das drei Schritte zeigt: 'Pretraining data', 'Language models' und 'Downstream tasks'. Diese Schritte werden durch gekrümmte Pfeile verbindet, was darauf hindeutet, dass es eine Folgevollendung oder eine Folgefolge gibt.</sample>
    <sample id="169">Das Bild zeigt eine Präsentation, die sich auf den Prozess der Vorgehensweise für die Analyse von politischen Biases in Sprachmodellen konzentriert. Der Titel lautet 'To this end', was auf Englisch 'Zum Ziel' bedeutet. Das Bild enthält einen Diagramm, das drei Schritte zeigt: 'Pretraining data' (Vorgelegte Daten), 'Language models' (Sprachmodelle) und 'Downstream tasks' (Niedrigststufe Aufgaben). Neben jedem Schritt sind zwei Fragen aufgeführt, die sich auf die Rolle von LMs bei der Bewertung von politischen Biases und der Auswirkung von LMs mit verschiedenen politischen Vorurteilen auf Fairnessprobleme in NLP-Aufgaben beziehen. Die Fragen sind: 1. Wie bewerten wir die politische Ausrichtung von LMs? 2. Welche Rolle spielt Vorgelegte Daten bei der Ausbreitung solcher Biases? 3. Wie leisten LMs mit verschiedenen politischen Vorurteilen im Durchführung von Aufgaben? 4. Wie führt die politische Ausrichtung von LMs zu Fairnessproblemen in NLP-Aufgaben? Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen, der möglicherweise die Präsentation gibt.</sample>
    <sample id="170">To this end, how do we evaluate the political leaning of LMs and what role does pretraining data play in such political biases?</sample>
    <sample id="171">Zum Ende</sample>
    <sample id="172">Das Bild zeigt eine Präsentation über den Bezug von Sprachmodellen (LM) auf politische Neigung. Der Titel lautet 'Evaluating LM Political Leaning'. Es gibt Schritte zur Unterstützung von Encoder- und Decoder-Modellen, wobei ein Beispiel für einen Prompt vorgegeben wird: 'Do you agree or disagree with this statement?' Darüber hinaus gibt es einen Abschnitt über automatische Bewertung und eine Referenz auf politische Literatur. Im unteren rechten Eck des Bildes ist ein Diagramm zu sehen, das verschiedene Sprachmodelle nach ihrer Positionierung in Bezug auf autoritäre und libertäre Neigungen und ihren Platzierung innerhalb der politischen Spektrum darstellt.</sample>
    <sample id="173">Ja, das Bild zeigt eine Karte mit dem Titel 'Existing LMs' und einem quadratischen Diagramm, das sich in vier Vierteln aufteilt. Die Achse horizontal von links nach rechts zeigt die 'Left' bis zur 'Right', und die Achse vertikal von unten nach oben zeigt die 'Authoritarian' bis zur 'Libertarian'. Die Kartenpunkte sind mit verschiedenen Sprachmodellen markiert, wie BERT-base, RoBERTa-base, distilRoBERTa, ALBERT-base, BART-base und Alpaca. Diese Sprachmodelle sind über die Viertel verteilt, was darauf hindeutet, dass sie unterschiedliche politische Neigungen haben.</sample>
    <sample id="174">Das Bild zeigt eine Präsentation über die Prätraining-Daten für Sprachmodellierung. Es gibt zwei Diagramme, die den politischen Leaning der verschiedenen Datenquellen darstellen. Im ersten Diagramm sind die Quellen "News Media" und "center" in einem Feld mit einem blauen Hintergrund und einer roten Schrift, während "right" in einem Feld mit einem roten Hintergrund und einer blauen Schrift dargestellt ist. Im zweiten Diagramm sind die Quellen "Social Media (Reddit)" und "center" in einem Feld mit einem grünen Hintergrund und einer roten Schrift, während "right" in einem Feld mit einem roten Hintergrund und einer grünen Schrift dargestellt ist. Die Texte auf dem Bild lesen: 'Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning' und 'Shen, Gorkin, and Carolyn Rose. What sounds "right" to me? Experiential factors in the perception of political leanings from social media.' Diese Informationen deuten darauf hin, dass die Prätraining-Daten für Sprachmodelle auf die politische Leaning der verschiedenen Quellen abgestimmt wurden.</sample>
    <sample id="175">Das Bild zeigt eine Präsentation über "Prätraining-Daten" (Pretraining Data). Es gibt zwei Diagramme, die sich auf den Inhalt von "News Media" und "Social Media (Reddit)" konzentrieren. Jedes Diagramm hat drei Abschnitte, die "left", "center" und "right" beschriftet sind. Das Text auf dem Bild lautet: "Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning." Darunter sind Zitate von Liu, Yajun, et al. und Shen, Gorkin, und Carolyn Rose.</sample>
    <sample id="176">Das Bild zeigt eine Präsentation, die sich auf das Thema 'Pretraining Data' konzentriert. Im oberen linken Eck befindet sich ein Text, der lautet: 'News Media' und darunter 'left', 'center' und 'right'. Rechts daneben befindet sich ein weiterer Text, der lautet: 'Social Media (Reddit)' und darunter 'left', 'center' und 'right'. Im unteren linken Eck ist ein Text zu sehen, der lautet: 'Liu, Yujan, et al., of "POLiCS: Pretraining with Semi-story Detection," Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, pp. 3584–3603.' Rechts daneben steht ein weiterer Text, der lautet: 'Shen, Gaojun, and Carolyn Rose. "What sounds 'right' to me? Exploring the impact factors in the perception of political leaning." Journal of the Association for Computational Linguistics Vol. 2021.'</sample>
    <sample id="177">Das Bild zeigt eine Präsentation über "Partisan shifts in LM political leaning". Es gibt zwei Diagramme, die sich auf die Ideologischen Koordinaten von Sprachmodellen RoBERTa und GPT-2 konzentrieren. Die Diagramme zeigen den Verschiebungen der Ideologischen Koordinaten aufgrund des Trainings auf Partizipen-Corpora. Links im Bild steht der Text: 'By further pre-training language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift.'</sample>
    <sample id="178">Das Bild zeigt eine Präsentation über "Partisan shifts in LM political leaning" (Partisanenverschiebungen in der politischen Neigung von LM). Es gibt zwei Diagramme, die sich auf die Verschiebungen von RoBERTa und GPT-2 konzentrieren. Jeder Diagramm ist ein quadratisches Grid, das in vier Abschnitte geteilt ist, wobei jeder Abschnitt einen bestimmten Partizipationsbereich darstellt: Links (links), Zentrum (Zentrum) und Rechts (Rechts). Die beiden Diagramme zeigen, wie RoBERTa und GPT-2 von den ursprünglichen Reddits (Original) zu den Reddits nach dem Training auf einem linken Reddits-Korpus verschieben. Diese Verschiebungen werden durch Pfeile anzeigt, die den Weg der Verschiebung darstellen. Im oberen rechten Eck des Bildes befindet sich ein kleineres Quadrat, das die Positionierung von RoBERTa und GPT-2 in Bezug auf die Verschiebungen zwischen den verschiedenen Reddits demonstriert.</sample>
    <sample id="179">Das Bild zeigt eine Präsentation mit dem Titel "The Trump Card" und der Untertitel "Pre-45th to post-45th shift". Es gibt eine Reihe von Diagrammen, die sich auf den politischen Neigungsbias in Textmodellern RoBERTa und GPT-2 konzentrieren. Jeder Diagramm zeigt eine Kombination von Kategorien wie "news left", "news center", "reddit left", "reddit center", "reddit right" und "news right". Jede Kategorie hat einen Delta-Wert (Δ) angegeben, der wahrscheinlich die Veränderung im politischen Neigungsbias darstellt. Der Delta-Wert für RoBERTa beträgt zum Beispiel -275,124, während der für GPT-2 -237,051 ist. Diese Diagramme sind wahrscheinlich Teil einer Analyse der Veränderungen im politischen Neigungsbias in Textmodellern über einen bestimmten Zeitraum.</sample>
    <sample id="180">Das Bild zeigt eine Präsentation mit dem Titel 'The Trump Card'. Es gibt eine Überschrift 'Pre-45th to post-45th shift' und eine Reihe von Diagrammen, die verschiedene Daten darstellen. Jedes Diagramm hat einen Namen wie 'news left', 'news center', 'reddit left' und 'reddit right', und es gibt Zahlen wie '-0.12(3.128)' und '-0.75(3.63)'. Im oberen rechten Eck des Bildes ist ein kleineres Bild eines Menschen zu sehen, der wahrscheinlich die Präsentation macht.</sample>
    <sample id="181">Der englische Inhalt lautet: 'The Trump Card' und 'Pre-45th to post-45th shift'.</sample>
    <sample id="182">Der englische Inhalt lautet: 'The Trump Card' und 'Pre-45th to post-45th shift'.</sample>
    <sample id="183">Per-Category Performance</sample>
    <sample id="184">Das Bild zeigt eine Tabelle mit der Überschrift 'Per-Category Performance'. Die Tabelle präsentiert die Leistungsdaten für verschiedene Gruppen, darunter 'Hate Speech', 'BLACK', 'MUSLIM', 'LGBTQ+', 'JEWISH', 'ASIAN', 'LATINX', 'WOMEN', 'CHRISTIAN' und 'WHITE'. Jede Spalte enthält eine Reihe von Zahlen, die wahrscheinlich die Leistungsdaten für verschiedene Quellen oder Methoden darstellen. Im unteren Abschnitt der Tabelle ist ein Text zu sehen, der lautet: 'Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded; such dark yellow denotes best and blue worst denotes.' Darüber hinaus gibt es in der oberen rechten Ecke des Bildes einen kleinen Text, der '1st 2nd 3rd 4th' liest.</sample>
    <sample id="185">Per-Category Performance</sample>
    <sample id="186">Das Bild zeigt eine Tabelle mit der Überschrift "Per-Category Performance". Es gibt eine Reihe von Kategorien wie "Hate Speech", "BLACK", "MUSLIM", "LGBTQ+", "Jews", "ASIAN", "LATINX", "WOMEN", "CHRISTIAN" und "WHITE". Jede Kategorie hat eine Spalte, die verschiedene Quellen oder Systeme darstellt, wie "REDDIT", "NEWS_RIGHT", "NEWS_LEFT", "NYT L(1)", "CNN (L)", "Guardian (L)", "Fox News", "WaPo (R)", "BB&amp;T (R)", "Watson (R)" und "NR (R)". Jede Spalte enthält Zahlen, die wahrscheinlich die Leistung oder die Genauigkeit der jeweiligen Quellen oder Systeme bei der Erkennung von Hasspeech und Missinformationsmaterialien für die unterschiedlichen Gruppen angivt. Die Zahlen sind in verschiedenen Farben markiert, wobei dunkel gelbe Zahlen den besten Ergebnissen und dunkelblaue Zahlen den schlechtesten anzeigen. Am unteren Ende der Tabelle ist ein Text zu sehen, der lautet: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources." Diese Tabelle bietet einen Überblick über die Leistungen verschiedenen Quellen oder Systeme bei der Erkennung von Hasspeech und Missinformationsmaterialien für verschiedene sozial und ethische Gruppen.</sample>
    <sample id="187">Per-Category Performance Hate Speech BLACK MUSLIM LGBTQ+ JEWS ASAIN LATINX WOMEN CHRISTIAN WHITE REDDIT 89.84 89.50 90.96 91.27 90.66 87.63 86.15 NEWS RIGHT 88.81 88.91 90.62 90.62 90.62 90.62 89.60 HP NYYT (L) CNN (L) NLT (L) GUARD (L) FOX (R) BBART (W) WAT (W) NR (R) 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 8</sample>
    <sample id="188">Per-Category Performance Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best, while blue and red denote worsts.</sample>
    <sample id="189">Das Bild zeigt eine Präsentation über die Leistung in verschiedenen Kategorien. Der Titel lautet "Per-Category Performance". Es gibt eine Tabelle mit verschiedenen Kategorien wie Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, MEN und WHITE. Jede Kategorie hat eine Reihe von Indikatoren wie REDDIT, NEWS, RIGHT, LEFT, NLP, CNN, NYT, LGL, Guard, Fox, BBRT, WAT und NR. Die Zahlen in der Tabelle sind in Farbe codeiert, wobei dunkel gelb bedeutet, dass der Wert am besten ist und dunkel blau bedeutet, dass der Wert am schlechtesten ist. Im unteren Teil der Tabelle steht ein Text, der lautet: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and dark blue denotes worst." Im oberen rechten Eck des Bildes ist ein kleiner Overlay mit dem Text "1st place 3rd place 4th place" zu sehen.</sample>
    <sample id="190">Das englische Inhalt auf Deutsch lautet: 'Qualitative Analysis' und 'Table 5: Examples of the downstream performance of tasks using language models with varying political bias. Base, N-S, L-R, S-N represent Christians, vanilla RoBERTa's model, social media, left-leaning, and right-leaning, respectively.'</sample>
    <sample id="191">图片中的文字内容包括一个标题为“Qualitative Analysis”的表格，该表格包含多个列和行。第一行是表头，包括以下列：Text、Target Label、Base、N-S、N-L、S-R、S-R。在表格中，有几行数据，每行代表不同的文本示例及其对应的标签和预测结果。例如，第一行的文本是“the world is corrupted with people supporting racism with asians and homosexuals it time for a new nationalism”，目标标签是“ASIAN”，Base列有True和False，N-S列有True和False，S-R列有True和False。类似的格式在后续行中继续，每个行代表不同的文本示例及其标签和预测结果。</sample>
    <sample id="192">Dies zeigt an, dass es eine Fehlfairness-Problemstellung gibt, das sehr dringend zu lösen ist.</sample>
    <sample id="193">Es gibt eine Tabelle mit 12 Beispielen von Hassreden, die als qualitative Analyse dient.</sample>
    <sample id="194">Dies würde bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen möglicherweise marginalisiert und die Haltung gegen Menschengruppen mit weniger Macht oder Privilegien in der Gesellschaft möglicherweise weiter ausgeprägt werden.</sample>
    <sample id="195">Das Bild zeigt eine Präsentation, die sich auf den Diskussion des Themas 'Discussion: Between Scylla and Charybdis'. Es gibt drei Hauptpunkte: 1. Vorbereitung von Daten 2. Sprachmodelle 3. Aufgaben im Hinblick auf die Sanierung von Daten. Diese Punkte sind in einer Reihe dargestellt und zeigen eine Verbindung zwischen ihnen. Im oberen rechten Eck ist ein kleiner Bildschirm mit einem Menschen zu sehen.</sample>
    <sample id="196">Das Bild zeigt eine Präsentationsslideset mit dem Titel "Discussion". Der Haupttext lautet: 'Between Scylla and Charybdis. To "sanitize" or not to "sanitize", that is the question.' Darunter sind drei Boxen mit den folgenden Texten: 'Pretraining data', 'Language models' und 'Downstream tasks'. Jede Box ist durch einen gekrümmten Pfeil verbunden, was eine Sequenz oder Verbindung zwischen diesen Schritten nahelegt. Im oberen rechten Eck des Slides befindet sich ein kleiner Bildschirm eines Menschen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="197">Das Bild zeigt eine Präsentation, die sich auf die Diskussion eines Themas konzentriert. Der Titel der Präsentation lautet "Discussion", und es gibt einen Text, der zwischen den mythologischen Figuren Scylla und Charybdis spricht. Der Haupttext lautet: "To 'sanitize' or not to 'sanitize', that is the question". Darunter sind drei Boxen mit den Worten "Pretraining data", "Language models" und "Downstream tasks" zu sehen. Diese Boxen sind miteinander durch gekrümmte Linien verbunden, was darauf hinweist, dass sie miteinander verbunden sind. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, auf dem eine Person zu sehen ist.</sample>
    <sample id="198">Der englische Inhalt lautet: 'Discussion Between Scylla and Charybdis To “sanitize” or not to “sanitize”, that is the question'</sample>
    <sample id="199">The text in the image is 'Thank you!' and it appears to be a title or heading. Below this, there are three boxes labeled 'Pretraining data,' 'Language models,' and 'Downstream tasks.' Underneath these labels, there are names of individuals: Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsetkova. At the bottom of the image, there are logos for PAUL G. ALLEN SCHOOL W, UWNLP, Carnegie Mellon University Language Technologies Institute, and another logo that is partially visible but not fully readable.</sample>
    <sample id="200">Es sind sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="201">MPP-Auswertungen wurden bis zu 900 Token Kontextlänge durchgeführt.</sample>
    <sample id="202">Music Selection, Book Selection, Recipe Selection</sample>
    <sample id="203">Positionality is a concept that refers to the perspectives people hold as a result of their demographics, identity, and life experiences. This means it's about how different factors in someone's background can shape what they see or think about certain things.</sample>
    <sample id="204">Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow.</sample>
    <sample id="205">Ja, passt EDAtt zu einem bestehenden Offline-ST-Modell.</sample>
    <sample id="206">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="207">Ja, das im Testsuite funktioniert.</sample>
    <sample id="208">The three variants of KITMUS are: (a) Background Pretrain, (b) Background Both, and (c) Background Inference.</sample>
    <sample id="209">The authors belong to Google Research.</sample>
    <sample id="210">The final research question is: 'How to use the available clean samples more efficiently?'</sample>
    <sample id="211">Sensitivity is a metric that measures how responsive the model's output is to changes in its input. It assesses whether slight variations in instructions result in consistent outputs for the same task, indicating robustness and reliability of the model.</sample>
    <sample id="212">Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie</sample>
    <sample id="213">Eine höhere Sensitivität bedeutet nicht notwendigerweise eine bessere Leistung des Modells.</sample>
    <sample id="214">Die Modelle erhalten während des Pre-Trainings den kontextuellen Kontext.</sample>
    <sample id="215">Typically, we only need 20 samples per class to attain high performance.</sample>
    <sample id="216">Stanford University</sample>
    <sample id="217">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, um die vielfältigen politischen und kulturellen Aspekte von Sprachmodellen besser zu erfassen.</sample>
    <sample id="218">The presenter is Akthar Al-Rawi.</sample>
    <sample id="219">Das Bild zeigt eine Diagrammstruktur, die den Prozess der Verbreitung von politischen Vorurteilen durch Sprachmodelltraining darstellt. Es gibt drei Schritte: 1. Pretraining data (Trainingsdaten) 2. Language models (Sprachmodelle) 3. Downstream tasks (Niedrigstufe-Aufgaben) Diese Schritte zeigen, wie Daten übertragen werden, um Sprachmodelle zu trainieren, die dann auf niedrigstufe-Aufgaben angewendet werden.</sample>
    <sample id="220">In the DEplain-apa corpus, there's a lot of 'reorderings' and 'word additions'.</sample>
    <sample id="221">Ja, Coscript ist öffentlich verfügbar.</sample>
    <sample id="222">The text is embedded into the image by overlaying it on top of a diagram that illustrates the process. The text appears in bullet points and provides detailed information about watermark injection, including steps like defining a target embedding, counting trigger numbers, adding backdoor weight, normalizing values, and combining them to create an original embedding with added watermarks.</sample>
    <sample id="223">Penn State University</sample>
    <sample id="224">Ja, Encoder-Decoder-Modelle wie mt5 können durch das Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="225">In the image, a person is shown in a video call on the right side. The background behind them appears to be an office or workspace with desks and chairs visible.</sample>
    <sample id="226">The Opazität ihrer Methode wird durch die Darstellung der Embeddings von Sätzen auf vier Datenbanken auf PCA (Principal Component Analysis) gewährleistet.</sample>
    <sample id="227">By using the existing PLM, you can build a new one more efficiently.</sample>
    <sample id="228">African Islamic</sample>
    <sample id="229">图片右侧的波形图显示了音频输入，下面的文字是“我是一个学生”。</sample>
    <sample id="230">The graph shows that as the number of tasks increases, the performance (measured by accuracy) generally improves for most task clusters. However, there is a noticeable drop in sensitivity with an increase in tasks, especially after 16 tasks and beyond. This suggests that while adding more tasks can enhance overall model performance to some extent, it may come at the cost of reduced sensitivity or specificity in certain areas.</sample>
    <sample id="231">LSTM seq2seq, TS, Zheng and Lapata</sample>
    <sample id="232">The two co-authors, Alexander Koller and Ivan Titov, are the advisees of the first author, Matthias Lindemann.</sample>
    <sample id="233">Chowdery et al., 2022</sample>
    <sample id="234">NLP Positionality: Characterizing Design Biases of Datasets and Models.</sample>
    <sample id="235">NLP Positionality: Characterizing Design Biases of Datasets and Models Sebastian Santi* Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarten Sap Carnegie Mellon University</sample>
    <sample id="236">Auf Deutsch: Beginnen Sie mit der Annahme, dass Sie für eine Zeitung arbeiten und durch Kommentare zu Ihrem Artikel siftingen, um Toxikum zu entfernen.</sample>
    <sample id="237">Imagin... Carl Jones Tech Lead, New York Times = PerspectiveAPI score Can you stop being a jerk? 🤨 (0.82) ✅</sample>
    <sample id="238">Das Bild zeigt zwei Personen, Carl Jones und Aditya Sharma. Carl Jones ist ein Tech Lead bei der New York Times, und Aditya Sharma ist ein Tech Lead bei der Times of India. Beide haben ihre Fotos und Namen in Kreisen mit verschiedenen Hintergründen dargestellt. Darüber hinaus sind auf beiden Fotos Textblöcke zu sehen, die eine Frage und eine Antwort enthalten. Die Frage lautet: "Kannst du einen Scheißer aufhören?" und die Antwort lautet: " everywhere on the news." Diese Texte deuten auf eine Diskussion oder eine Präsentation hin, die möglicherweise mit dem Thema der Perspektive API-Scoring zu tun hat.</sample>
    <sample id="239">Dies ist ein Beispiel für eine Gestaltungsbias, bei dem wir systematice Leistungsunterschiede zwischen Bevölkerungen sehen.</sample>
    <sample id="240">Positionality is the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="241">Positionalität "Die Perspektiven [Menschen] halten aufgrund ihrer Demografik, Identität und Lebenserfahrungen." [1] Savin-Baden, Maggi, und Claire Howell-Major. "Qualitative Forschung: Das essential guide zur Theorie und Praxis." Qualitative Forschung: Die Essential Guide to Theory and Practice. Routledge (2013).</sample>
    <sample id="242">Positionality</sample>
    <sample id="243">Das bedeutet, dass die Diskussion über die Positionalität von Datensätzen und Modellen auf Deutsch "Do datasets and models have positionality?" übersetzt wird.</sample>
    <sample id="244">Das Thema der Präsentation ist 'Do datasets and models have positionality?' (Sind Datensätze und Modelle Positionalität aufweisen?). Die Präsentation bezieht sich auf drei Forschungsarbeiten: 1. Blasi, et al., "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. 2. Ye, et al., "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models." EMNLP 2022. 3. Cambo &amp; Gergle, "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022.</sample>
    <sample id="245">Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3]</sample>
    <sample id="246">Anecdotal evidence: - Modell und Dataset-Probe [1][2] - Theoretische Definitionen von Modellpositionierung [3] [1] Blasi, F. "Systematische Ungleichheiten in der Sprachtechnologieleistung auf der ganzen Welt." ACL 2022. [2] Ye et al.: "GEOMLAMA: Geo-Diverses Commonsense Probing auf Multilingual Pre-Trained Language Models." EMNLP 2022. [3] Cambo &amp; Gergle: "Modellpositionale Reflexivität und Computational Reflexivity: Promovierte Reflexivität in Data Science." CHI 2022.</sample>
    <sample id="247">Das Thema des Slides lautet "Do datasets and models have positionality?" (Sind Datensätze und Modelle positionell?). Diese Frage wird auf dem Slide diskutiert, indem sie Anleitungen zu Positionellität in der Sprachverarbeitung präsentiert. Der Text auf dem Slide lautet: "Anecdotal evidence:" - "Model and dataset probing [1][2]" - "Theoretical definitions of model positionality [3]" Diese Punkte deuten darauf hin, dass die Positionellität in Bezug auf Datensätze und Modelle auf experimentelle und theoretische Ansätze zurückzuführen ist. Darunter sind Referenzen auf drei Forschungsarbeiten aufgeführt: 1. Blasi, E. et al.: "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. 2. Ye, Y. et al.: "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022. 3. Cambo &amp; Gergle: "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022. Diese Referenzen deuten darauf hin, dass die Positionellität in der Sprachverarbeitung eine aktuelle und wissenschaftlich fundierte Thematik ist, die sowohl experimentelle Studien als auch theoretische Entwicklungen umfasst.</sample>
    <sample id="248">Es ist schwierig, diese Positionalitäten zu charakterisieren, weil nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter API versteckt sind.</sample>
    <sample id="249">Das Bild zeigt eine Person, die sich in einem Zimmer befindet, das mit Bücherschränken und anderen Gegenständen ausgestattet ist. Die Person scheint auf einem Computer zu arbeiten oder zu interagieren. Im Bild ist auch ein Text sichtbar, der fragt: 'Do datasets and models have positionality?' Darunter steht ein Zweckangabe: 'Goal: Compare annotations from users with existing datasets and models.' Diese Informationen deuten darauf hin, dass die Person möglicherweise über die Positionalität von Datensätzen und Modellen nachdenkt und sich mit der Vergleichung von Annotierungen von Benutzern mit vorhandenen Datensätzen und Modellen auseinandersetzt.</sample>
    <sample id="250">Das ist ein Framework, um die Gestaltungsvorurteile in NLP-Datensätzen und -Modellen zu charakterisieren.</sample>
    <sample id="251">Unsere Plattform funktioniert in zwei Hauptschritten.</sample>
    <sample id="252">Das erste Schritt ist, Datensätze mit einer Vielzahl von Annotatoren zu re-annotate.</sample>
    <sample id="253">Das Framework umfasst die Sammlung, die Verarbeitung und das Annotieren von Daten.</sample>
    <sample id="254">Das Bild zeigt ein Framework für die Re-Annotierung von Datensätzen mit diversen Annotatoren. Der Text auf der rechten Seite lautet: "1) Re-annotate datasets with diverse annotators."</sample>
    <sample id="255">Wir nehmen dann die Anmerkungen nach Demografie und vergleichen sie mit den Modellen und Datenmengen mithilfe des Pearson-R Korrelations-Scores.</sample>
    <sample id="256">Das Bild zeigt eine detaillierte Übersicht eines Frameworks für die Analyse von Datensätzen. Der Text auf dem Bild lautet: "2) Compare annotations by demographic to models and datasets via Pearson's R scores."</sample>
    <sample id="257">Das Bild zeigt eine Webseite namens "LabintheWild". Die Webseite ist in Englisch und zeigt eine Übersicht über verschiedene Studien oder Experimente. Im oberen linken Eck des Bildes steht der Name "LabintheWild" in großen, schwarzen Buchstaben. Im Hintergrund der Webseite ist ein Grauweißton. Links am oberen Rand der Webseite befindet sich ein kleiner Bildschirm mit einem Menschenbild. Im Zentrum der Webseite sind drei Hauptbereiche zu sehen: 1. "Could live with an AI and its morals?" (Können Sie mit einer AI und ihren Moralwerten leben?) 2. "Are you better than an AI in noticing hateful speech?" (Ist Sie besser als eine AI bei der Erkennung von verherrlichter Sprache?) 3. "Where are you on the techno-skeptic to techno-utopian scale?" (Wo stehen Sie auf der Skala von Technoskeptiker bis Techno-Optimist?) Jeder dieser Bereiche hat eine kleine Illustration darunter und einen Button mit den Worten "Participate now!" (Teilnahme jetzt!). Im oberen rechten Eck der Webseite ist die Zahl "5,376,396 Total participants" zu sehen, was die Gesamtzahl der Teilnehmer angibt. Links am oberen Rand der Webseite ist der Text "Our experiences", "Pilot case &amp; Site Stats", "For Researchers" und "About Us" zu sehen, was möglicherweise verschiedene Navigationsoptionen für die Webseite ist.</sample>
    <sample id="258">LabintheWild ist ein Online-Experimentationsplattform, auf der wir verschiedene Teilnehmer an verschiedenen Experimenten und Studien aufnehmen können.</sample>
    <sample id="259">Das Bild zeigt eine Aufgabe namens "Task A: Social Acceptability". Diese Aufgabe besteht aus drei Schritten: 1. Lesen der Situation: Ein Szenario lautet "Wollen Sie viel Geld verdienen." 2. Über die Situation entscheiden: Die Person muss den Szenario von einer Perspektive abändern und ihre Meinung zum Szenario äußern. 3. Lesen der Meinungen anderer AI: Der Mensch sollte die Meinungen des AI-Systems über das Szenario lesen und sie berücksichtigen. Darüber hinaus werden Teilnehmer in den Vereinigten Staaten über die sozial akzeptablen Aspekte der Situation gefragt.</sample>
    <sample id="260">Nachdem sie sich im Studium engagiert haben, können sie ihre Antworten mit denen eines AIs und anderer vergleichen.</sample>
    <sample id="261">Auf Deutsch: Wir haben dann diese Annotierungen mit der Sozialchemie, Delphi und GPT-4 verglichen.</sample>
    <sample id="262">Participants read an instance from the Dyna Hate dataset. Participants rate whether they think it's a hate speech or not.</sample>
    <sample id="263">Das Bild zeigt eine Präsentation mit dem Titel 'Study Participation'. Es gibt drei große Zahlen, die jeweils auf einer separate Linie stehen: 16.299 annotations, 1.096 annotators und 87 countries. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen.</sample>
    <sample id="264">Das Bild zeigt eine Person, die in einem Zimmer steht, das mit Büchern und anderen Gegenständen gefüllt ist. Die Person trägt ein graues Outfit und scheint sich zu bewegen oder zu sprechen. Der Hintergrund suggeriert, dass es sich um einen persönlichen oder informellen Raum handelt, möglicherweise ein Zuhause oder eine Studienstätte.</sample>
    <sample id="265">Datasets and models are most aligned to English-Speaking countries.</sample>
    <sample id="266">Datensetze und Modelle sind insbesondere für Menschen mit einem College-Abschluss ausgerichtet.</sample>
    <sample id="267">Datensetze und Modelle sind am engsten mit Menschen abgestimmt, die eine Hochschulausbildung haben.</sample>
    <sample id="268">Finde 2: Einige Bevölkerungen werden zurückgelassen.</sample>
    <sample id="269">Datenmengen und Modelle sind weniger mit nicht binären Menschen ausgerichtet als mit Männern und Frauen.</sample>
    <sample id="270">So, was tun wir? Positionelle Probleme in der NLP</sample>
    <sample id="271">Empfehlungen: 1. Halten Sie eine Liste aller relevanten Gestaltungswahlen, die während der Erstellung von Datensätzen oder Modellen getroffen wurden. 2. Führen Sie NLP-Forschung durch das Perspektivismuslenz: a. Teilen Sie verstreute Datensatzetiketten</sample>
    <sample id="272">Das dritte Empfehlung ist, spezialisierte Datensätze und Modelle für bestimmte Gemeinschaften zu erstellen, was für inklusive NLP von entscheidender Bedeutung ist (z.B. Masakhane Initiative).</sample>
    <sample id="273">Danke für die Präsentation. Der Dash-Link ist nlppositionality.cs.washington.edu und der Paper-Link ist bit.ly/NLPositionality-Paper.</sample>
    <sample id="274">The woman is pointing to the text on the screen.</sample>
    <sample id="275">图片中的文字内容包括： 1. 标题： "Discussion" 2. 副标题： "Between Scylla and Charybdis" 3. 主要文本： "To 'sanitize' or not to 'sanitize', that is the question" 4. 图片下方的流程图标签： - "Pretraining data" - "Language models" - "Downstream tasks" 在这些元素的下方，有四个人的名字和头像，以及他们的关联机构或组织： - Shangbin Feng - Chan Young Park - Yuhan Liu - Yulia Tsetkov 同样在这些名字和头像下方，有以下标志和名称： - PAUL G. ALLEN SCHOOL OF COMPUTER SCIENCE &amp; ENGINEERING - UW NLP - Carnegie Mellon University Language Technologies Institute - 一个带有红色印章的标志（具体内容不清晰）</sample>
    <sample id="276">Die 61. Treffen der Gesellschaft für Computational Linguistik in Toronto, Kanada vom 7. bis 14. September 2023.</sample>
    <sample id="277">Sprichst du Deutsch?</sample>
    <sample id="278">Language Planning</sample>
    <sample id="279">Der englische Text lautet: 'However, previous work mainly focuses on planning for the abstract goals of stereotypical activities.'</sample>
    <sample id="280">Immerse in the art of cake-making with our video, where you'll discover how to make a strawberry or chocolate cake. Each step is illustrated and explained clearly for an enjoyable learning experience.</sample>
    <sample id="281">Die englische Übersetzung lautet: "Abstract goal kann von verschiedenen realen, spezifischen Zielen mit mehrfachen Restriktionen abgeleitet werden."</sample>
    <sample id="282">In this paper, we first evaluate and impose a constrained language planning ability of large language models.</sample>
    <sample id="283">Das Bild zeigt eine Präsentation über "LLMs im Bereich der Beschränkten Sprachplanung". Es gibt einen Titel, der die Präsentation beschreibt, und eine Übersicht über verschiedene Typen von Restriktionen. Die Restriktionstypen sind: 1. Modifikator (Modifier) 2. Methode (Method) 3. Absatzziel (Intent) Jede Restriktionstyp wird mit Beispielen illustriert. Im Hintergrund ist ein Zimmer zu sehen, in dem jemand sitzt, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="284">Das Bild zeigt eine Person, die in einem modernen Büro sitzt. Im Hintergrund sind mehrere Schreibtische und Stühle zu sehen. Die Person trägt ein Grün-Orange-Outfit und hat langes Haar. Sie scheint auf dem Laptop zu arbeiten. Links im Bild ist ein Textblock über "LLMs im Konstruierten Sprachplanung" zu sehen, sowie eine Tabelle mit Informationen über "Constraint Type 1: Modifier" und "Constraint Type 2: Method". Der Text auf der Seite lautet: 'How do LLMs perform on Constrained Language Planning? Dataset: wikiHow + Generated Constraints Constraint Type 1: Modifier Definition: A word, an adjective or a phrase that modifies or constrains an abstract goal. Ex1: Make a chocolate cake. Ex2: Make a pink cake. Constraint Type 2: Method Definition: A tool or specified mode that controls the process for achieving the goal. Ex1: Make a cake with an oven. Ex2: Make a cake by using cake mix. Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal. Ex1: Make a cake for wedding. Ex2: Make a cake for diabetics.'</sample>
    <sample id="285">Der englische Text lautet: Can LLMs do Constrained Language Planning? All baselines achieve unsatisfactory results on planning for specific goals.</sample>
    <sample id="286">Kann LLMs Konstruktionsprobleme planen? Alle Baseline-Modelle erzielen unzufriedenstellende Ergebnisse bei der Planung für bestimmte Ziele.</sample>
    <sample id="287">Was sind die typischen Fehler, die von LLMs bei dieser Aufgabe gemacht werden? Die semantische Vollständigkeit (SE) in generierten Skripten ist akzeptabel, aber die Treue zur Restriktion (FE) kann nicht gewährleistet werden.</sample>
    <sample id="288">Was sind die typischen Fehler, die LLMs bei dieser Aufgabe machen? Die semantische Vollständigkeit in generierten Skripten ist akzeptabel, aber die Treue zur Restriktion (FE) kann nicht garantiert werden.</sample>
    <sample id="289">Was sind die auftragsarten, in denen InstructGPT typischerweise scheitert?</sample>
    <sample id="290">Method Input: an abstract goal Step 1: Generate specific goals with InstructGPT via in-context learning Abstract Goal: Make a cake Specific Goals: G1 (+ modifier): Make a chocolate cake G2 (+ method): Make it in the microwave G3(+ intent): Make a cake for a wedding</sample>
    <sample id="291">Method Input: ein abstraktes Ziel Abstract Goal: Mach ein Kuchen Specific Goals: G1 (+ Modifizierer): Mach einen Schokoladenkuchen G2 (+ Method): Mach in einem Mikrowellenherd G3(+ intent): Mach ein Kuchen für eine Hochzeit Step 1 Generate spezifische Ziele mit InstructGPT über In-Context-Learning</sample>
    <sample id="292">Method Input: an abstract goal Step 1: Generate specific goals with InstructGPT via in-context learning Abstract Goal: Make a cake Specific Goals: G1 (+ modifier): Make a chocolate cake G2 (+ modifier): Make a cake in the microwave G3(+ intent): Make a cake for a wedding Generate Plans for G1 Candidate Scripts: 1 2 ... k Over-generate candidate scripts with InstructGPT via in-context learning</sample>
    <sample id="293">Method Step 2: Over-generate candidate scripts via in-context learning. Step 3: Find the goal with InstructGPT via similarity score. Filtered Scripts Output: Specific goals with corresponding scripts</sample>
    <sample id="294">Method Step 2: Over-generate candidate scripts via in-context learning. Step 3: Find the goal with InstructGPT via similarity score. Output: Specific goals with corresponding scripts.</sample>
    <sample id="295">Method Step 2: Over-generate candidate scripts via in-context learning. Step 3: Find the goal with InstructGPT via similarity score. Output: Specific goals with corresponding scripts.</sample>
    <sample id="296">Unser Method improves die Planbarkeit der Pläne, sowohl in Semantikkomplettheit als auch in Fertigkeitsbereich zu den</sample>
    <sample id="297">Dies ist ein sichtbares Bild eines Menschen, der auf einem Computerbildschirm steht. Der Mensch trägt Brille und hat schwarze Haare. Im Hintergrund erscheint es wie ein Zimmer mit Tischen und Stühlen, möglicherweise ein Arbeitsplatz oder eine Klasse. Links am Bildrand befindet sich ein Textblock mit dem Titel "Script Distillation from LLMs". Der Text ist in einer Mischung aus Englisch und Deutsch aufgeführt. Die englische Version lautet: "Script Distillation from LLMs" und die deutsche Übersetzung lautet: "Skriptdistillation von LLMs". Der Text beschreibt eine Methode zur Erstellung kleinerer Modelle für Sprachplanung, die auf Symbolik und Wissenfolge basiert und eine Datenbank namens Coscript Dataset verwendet.</sample>
    <sample id="298">Hier ist die Übersetzung des englischen Inhalts ins Deutsche:</sample>
    <sample id="299">Dieser Vorgang umfasst drei Schritte: 1. Generieren Sie spezifische Ziele mit InstructGPT über das Inhaltsgelernt. 2. Erstellen Sie überdurchschnittliche Skripte mit InstructGPT unter Berücksichtigung der Kontextinformationen. 3. Filtern Sie die Skripte mit InstructGPT aufgrund des Symbolwissens und der Kontextinformationen.</sample>
    <sample id="300">Das Bild zeigt eine Präsentation über "Script Distillation from LLMs" (Script Distillation aus LLMs). Die Präsentation besteht aus zwei Hauptabschnitten: "Motivation" und "Method". Im "Motivation"-Abschnitt wird erläutert, dass das Ziel der Methode es ist, konstruktionsbezogene Sprachplanung für kleinere Modelle zu ermöglichen. Im "Method"-Abschnitt sind die Schritte der Methode aufgeführt: 1. Generieren spezifischer Ziele mit InstructGPT über Inhalt-Training. 2. Übergenerieren von Skripts mit InstructGPT mit Inhalt-Training. 3. Filteren von Skripts nach Ziel mit InstructGPT über die Symbolik-Scoring-Score. Darüber hinaus werden 55.000 Skripts mit Restriktionen generiert, basierend auf der Methode, die als CoScript-Dataset bezeichnet wird. Diese Skripts werden von Menschen handschriftig überprüft und getestet. Das Ergebnis dieser Methode sind Skripts mit entsprechenden Plänen.</sample>
    <sample id="301">Der englische Inhalt lautet: "In total, we generate 55,000 specific goals with scripts to ensure the quality of validation and test sites. We ask cloud-sourced workers to find and revise the incorrect samples."</sample>
    <sample id="302">Das Bild zeigt eine Präsentation über "Constraint Analysis" und "Coscript for Smaller Language Models". Links oben steht der Titel "Constraint Analysis", darunter ein Kreisdiagramm mit verschiedenen Abschnitten, die auf verschiedene Aspekte hinweisen. Rechts oben ist eine Person zu sehen, die wahrscheinlich über das Thema spricht. Im unteren linken Eck befindet sich eine Textbox mit dem Text: 'Coscript shows high heterogeneity and pluralism in the generated specific goals.' Im unteren rechten Eck ist eine weitere Textbox mit dem Titel 'Coscript for Smaller Language Models'. Sie enthält zwei Hauptabschnitte: 'Datasets' und 'Metrics', jeder mit jeweils drei Punkten. Unter 'Datasets' sind 'Coscript' und 'wikiHow' aufgeführt. Unter 'Metrics' sind 'Faithful: DeBERTa (v3 large) model to decide whether the generated texts are faithful to the constraints' und 'Automatic metrics: ROUGE, BLEU, BERTScore' aufgeführt. Im unteren linken Eck ist ein weiterer Textbox mit dem Text: 'Smaller LM's fine-tuned on Coscript can generate higher quality scripts than LLMs'.</sample>
    <sample id="303">Spezialisierte Modelle versus LLMs</sample>
    <sample id="304">In summary, we established the constrained language planning problem. We evaluated the constrained language planning ability of LLMs and developed an over-generate-then-filter method for LLMs.</sample>
    <sample id="305">Summarie und Erkenntnisse: 1. Stellen Sie das Problem der 'beschränkten Sprachplanung' auf. 2. Bewertet die Fähigkeit von Sprachmodellen, Sprache zu planen, und entwickelt Sie eine 'übergegenwärtige Filter-Funktion' für diese Modelle. 3. Geben Sie ein hochwertiges 'Script-Datensatz' (CoScript) für die Sprachplanung. Erkenntnisse und Zukunft: - Der Ansatz zur Verbesserung von Sprachmodellen ist ein 'post-hoc-Rückblick-Verfahren'. - CoScript erstellt sich aus einem ursprünglichen Script mit einer zusätzlichen Restriktion. - CoScript kann ein wertvolles Ressourcenmaterial für die Fortschritte in der Forschung zu Sprachplanung mit mehreren Zielen und Bedingungen sein.</sample>
    <sample id="306">Die 61. Treffen der Gesellschaft für Computational Linguistik in Toronto, Kanada vom 9. bis 14. Juli 2023.</sample>
    <sample id="307">The fluency of PaLM is comparable to SOTA.</sample>
    <sample id="308">Wasserzeichenverfahren sollten folgende Eigenschaften haben: 1. Sie müssen für Embedding- und Service-Anwendungen anwendbar sein. 2. Der Wasserzeichen sollte die Funktionalität der bereitgestellten Embeddings nicht beeinträchtigen. 3. Der Wasserzeichen sollte sichtbar für den Angreifer versteckt sein, damit er ihn nicht leicht entfernen kann. 4. Der Wasserzeichen muss übertragen werden, wenn es zu den Angreifersservices gehört.</sample>
    <sample id="309">English, Español, Français, Italiano, Nederlands, Português, Română, Русский, Türkçe, 中文</sample>
    <sample id="310">300</sample>
    <sample id="311">Cosine and L2 similarity.</sample>
    <sample id="312">Mbart5</sample>
    <sample id="344">The authors decide what words have moderate frequency by counting the word frequency on a general text corpus \( D_p \) and randomly selecting \( n \) words within a moderate-frequency interval.</sample>
    <sample id="345">Das Thema der Präsentation lautet: "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?"</sample>
    <sample id="346">Der Titel lautet "Named Entity Recognition &amp; Generalization".</sample>
    <sample id="347">Der Text auf dem Bild lautet: 'Named Entity Recognition &amp; Generalization' und 'Models have been using CoNLL-2003 to develop NER for almost 20 years'.</sample>
    <sample id="348">Der Inhalt lautet: "Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years Can these models generalize to modern data? What is needed for good generalization?"</sample>
    <sample id="349">Der Inhalt der Präsentation lautet: 'Named Entity Recognition &amp; Generalization' - 'Models have been using CoNLL-2003 to develop NER for almost 20 years' - 'Can these models generalize to modern data?' - 'What is needed for good generalization?' - 'What causes the performance drop?'</sample>
    <sample id="350">CoNLL++ Dataset Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines AMBASSADOR O TO O THE O UNITED I-ORG NATIONS I-ORG : O LINDA I-PER THOMAS-GREENFIELD I-PER</sample>
    <sample id="351">Das CoNLL++ Dataset umfasst Reuters-Newsartikel aus dem Jahr 2020, die mit Anmerkungen nach den Anmerkungen von 2003 der CoNLL-Datenbank versehen wurden. Diese Anmerkungen bestehen darin, dass bestimmte Wörter wie "AMBASSADOR", "TO", "THE", "UNITED", "NATIONS" und "LINDA" als Organisationen (I-ORG) oder Personen (I-PER) identifiziert werden. Diese Identifikation wurde auf über 20 Modellen für die KoNLL-2003-Evaluation fine-tuned. Die Evaluation erfolgte auf beiden KoNLL-2003-Testset und KoNLL++-Testset.</sample>
    <sample id="352">CoNLL++ Dataset Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines Fine-tuned 20+ models on CoNLL-2003 Evaluated on CoNLL-2003 test set &amp; CoNLL++ Calculated percentage ΔF1 to assess generalization AMBASSADOR O TO O THE I-ORG NATIONS I-ORG : I-ORG LINDA I-PER THOMAS-GREENFIELD I-PER</sample>
    <sample id="353">Die Folge zeigt eine Person, die sich mit einer Miesterhand abgibt. Sie trägt ein schwarzes Hemd und ist links im Bild platziert. Im Hintergrund ist ein weißer Farbton zu sehen, und oben steht auf dem Bildschirm der Text "Was sind für gute allgemeine Fähigkeiten erforderlich?" Diese Texte deuten an, dass es sich um einen Vortrag oder eine Präsentation handelt, möglicherweise in einem akademischen oder wissenschaftlichen Zusammenhang.</sample>
    <sample id="354">Was ist für eine gute allgemeinereignis erforderlich?</sample>
    <sample id="355">Was benötigt man für gute Generalisierung?</sample>
    <sample id="356">Das Thema der Präsentation ist "Was für eine gute Allgemeinbildung benötigt?" (Was is needed for good generalization?).</sample>
    <sample id="357">Was verursacht einen Leistungsabfall?</sample>
    <sample id="358">Das Bild zeigt eine Präsentation, die mit dem Titel "Was verursacht einen Leistungsabfall?" beginnt. Im unteren rechten Ecke befindet sich das Logo der Georgia Tech. Links unten ist ein kleiner Profilbild eines Menschen zu sehen. Der Text auf der Präsentation lautet: 'Adaptive Overfitting?' und 'Adaptive Overfitting, was ist es?' Diese Informationen deuten darauf hin, dass die Präsentation über die Ursachen eines Leistungsabfalls im Zusammenhang mit der adaptive Overfitting diskutiert wird.</sample>
    <sample id="359">Was verursacht eine Leistungsabnahme? Adaptive Overfitting? Temporale Drift?</sample>
    <sample id="360">Das Bild zeigt eine Präsentation mit dem Titel "Was verursacht einen Leistungsabfall?" (Was verursacht einen Leistungsabfall?). Es gibt zwei Punkte auf der Seite links: "Adaptive Overfitting?" und "Temporal Drift?". Rechts ist ein Diagramm zu sehen, das die Korrelation zwischen der "CoNLL 2003 F1 Score" und der "Train Time (s)" darstellt. Der Text auf dem Diagramm lautet: "CoNLL 2003 F1 Score", "Train Time (s)", "Red Best Fit Line", "Gray Best Fit Line", "Orange Best Fit Line", "Yellow Best Fit Line", "Green Best Fit Line", "Blue Best Fit Line", "Purple Best Fit Line", "Light Blue Best Fit Line", "Dark Green Best Fit Line", "Dark Blue Best Fit Line", "Dark Purple Best Fit Line", "Dark Light Blue Best Fit Line", "Dark Dark Green Best Fit Line", "Dark Dark Blue Best Fit Line", "Dark Dark Purple Best Fit Line", "Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Green Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Blue Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Purple Best Fit Line", "Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Dark Light Blue Best Fit Line</sample>
    <sample id="361">Der Inhalt lautet: 'Was verursacht einen Leistungsabfall?'</sample>
    <sample id="362">Was verursacht einen Leistungsabfall? Adaptive Übergreifung? Keine abnehmende Rendite?</sample>
    <sample id="363">Temporal drift ist ein Phänomen, das in der Zeit veränderte Verhaltensweisen oder -muster bezeichnet. Es kann zu einer Entgleichung zwischen den erwarteten und tatsächlichen Ergebnissen führen, insbesondere bei langfristigen Studien oder Prozessen. In diesem Kontext könnte "Temporal drift" auf die Veränderungen in einem Modell oder System hindeuten, die sich über einen Zeitraum entwickeln, was zu einer Abwertung der Leistung führen könnte.</sample>
    <sample id="364">Das Bild zeigt eine Präsentation mit dem Titel "Was verursacht einen Leistungsabfall?". Es gibt drei Hauptpunkte auf der linken Seite: 1. Adaptive Overfitting 2. Keine Diminishing Returns 3. Nicht beobachtet Die rechte Seite des Bildes zeigt eine Tabelle mit den Namen von Modellen und ihren Leistungsindikatoren für das Jahr 2003 und 2011, sowie den Delta-Indikatoren (ΔF1 (%)). Darunter befindet sich ein Graph, der die Leistung über einen Zeitraum von 2006 bis 2018 zeigt. Der Graph zeigt eine lineare Ansteigerung der Leistung. Im unteren rechten Eck ist das Logo der Georgia Tech zu sehen.</sample>
    <sample id="365">Der Inhalt des Bildes lautet: "Was verursacht einen Leistungsabfall? Adaptive Überschneidung, keine wachsenden Rückerträge, nicht beobachtet, temporale Verlaufsrate, Leistungsrückgang mit größerem Zeitabstand, Haupt Ursache für Leistungsabfall"</sample>
    <sample id="366">Das Bild zeigt eine Präsentation mit dem Titel 'Conclusion'. Es gibt drei Punkte, die darauf hindeuten, dass für eine gute allgemeine Verständigung eine bessere Modellarchitektur, ein größeres Modellgröße und mehr angeregte Beispiele benötigt werden. Rechts auf dem Bild befindet sich ein Graphik, der von 2004 bis 2022 zeigt, wie verschiedene Modelle im Bereich des Sprachverarbeitungsbildes entwickelt wurden. Die Modelle sind Stanford NLP, Illinois NER, BLSTM-CNN-CH, BERT-Large, Flair und LUKE. Der Graph zeigt die Entwicklung der Leistung dieser Modelle über die Jahre. Im unteren rechten Eck des Bildes ist das Logo der Georgia Tech zu sehen.</sample>
    <sample id="367">Das Bild zeigt eine Präsentation, die mit dem Titel "Concussion" beginnt. Es gibt einige Punkte auf der linken Seite, die folgen: - Eine gute allgemeinere Darstellung erfordert: - Bessere Modellarchitektur - Größere Modellgröße - Weitere scharfe Beispiele</sample>
    <sample id="368">Das Bild zeigt eine Präsentation, die sich auf den Schluss eines Forschungsartikels oder einer Studie konzentriert. Der Titel der Präsentation lautet "Conclusion", was darauf hindeutet, dass es sich um einen Abschlussabschnitt handelt. Die Präsentation enthält mehrere Punkte und eine Graphik, die Informationen über die Entwicklung von Modellarchitekturen und die Leistung von Taggern darstellt.

Die Texte auf dem Bild sind in Englisch, und sie diskutieren die Notwendigkeit für bessere Modellarchitekturen, größere Modellgrößen und besser ausgewählte Beispiele für die Verbesserung der allgemeinen Leistung. Sie betonen auch, dass die Leistungsschwäche durch Temporal drift und nicht anpassungsfähiges Übertreiten verursacht wird.

Ein Graphik zeigt die Entwicklung von Modellen über einen Zeitraum von 2004 bis 2013, mit verschiedenen Markierungen für verschiedene Modelle wie Stanford NLP, Illinois NER, BSLSTM-CNN, CoNLL-2003 und CoNLL-2013. Der Graphik zeigt, dass die Leistung von Modellen über die Jahre gewachsen ist, mit einem besonderen Schwerpunkt auf den Modellen von Stanford NLP und Illinois NER.

Am Ende der Präsentation gibt es eine positive Antwort auf die Frage, ob CoNLL-2003-Tagger immer noch funktionieren, und das Wort "YES!" wird prominent dargestellt, was darauf hinweist, dass die Studie den positiven Ausgang gefunden hat.

Zusätzlich zu den Texten und dem Graphik gibt es auch ein Foto eines Menschen im unteren linken Eck des Bildes, was möglicherweise darauf hindeutet, dass dies ein Foto des Vortragers oder eines der Personen, die an der Studie beteiligt waren, ist.

Im Ganzen bietet das Bild einen umfassenden Überblick über die Schlussfolgerungen einer Forschungsstudie, die sich auf die Verbesserung von Modellarchitekturen und die Leistung von Taggern konzentriert.</sample>
    <sample id="369">Zusammenfassung: Wir hoffen, dass unser Papier zu mehr Forschung auf dem Gebiet der Verbesserung der allgemeinen Modellarchitektur und -größe beiträgt.</sample>
    <sample id="370">Paper: https://arxiv.org/abs/2212.09747 Dataset: https://github.com/ShuhengL/ac12023_conllpp Contact: sliu775@gatech.edu</sample>
    <sample id="397">Lern- und Sprachsegmente</sample>
    <sample id="398">In the example with Servin and Kea, entity-specific knowledge is needed to determine that 'he' refers to Servin.</sample>
    <sample id="399">The most important factor between the example quality and similarity to the source sentence is that "Example quality is more important than similarity to source sentence."</sample>
    <sample id="400">The image shows a slide from a presentation. It has two main sections: one on the left labeled 'News Media' and another on the right labeled 'Social Media (Reddit)'. Both sections have three horizontal bars, each representing different political leanings: 'left', 'center', and 'right'. The background colors for these bars are yellowish for 'News Media' and greenish for 'Social Media (Reddit)'. At the bottom of the slide, there is text that reads "Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning." Additionally, there are references to studies or papers at the bottom corners of the slide.</sample>
    <sample id="401">Das Modell kombiniert Werte aus mehreren Ebenen.</sample>
    <sample id="402">Beispiele für direkte Inferenz sind: "Can't remember the name" und "The pronunciations are hard to distinguish".</sample>
    <sample id="403">The authors belong to Fudan University.</sample>
    <sample id="404">The image shows a list of authors involved in the work "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains." The names listed are Yanis Labrak, Adrien Bazege, Richard Dufour, Mickael Rouvier, Emmanuel Morin, Beatrice Daille, and Pierre-Antoine Gourraud. Therefore, there are 7 authors mentioned in this context.</sample>
    <sample id="405">Ja, die Übersetzung der natürlichen Anfrage mit einem maschinellen Übersetzungsmodel vor dem semantischen Parsing wurde als Baseline betrachtet.</sample>
    <sample id="406">The example given is 'a warrior (unmarked) vs. a woman warrior (marked)'.</sample>
    <sample id="407">Transformer models generalise better.</sample>
    <sample id="408">The test data sets are named FT_C, LORAC, BitFitFC, AdapterC.</sample>
    <sample id="409">Es gibt sechs Autoren, die an der Arbeit beteiligt sind.</sample>
    <sample id="410">The authors work with multiple modalities.</sample>
    <sample id="439">Knowledge-intensive NLU tasks.</sample>
    <sample id="440">Zhiyang Xu, Ying Shen, and Lifu Huang.</sample>
    <sample id="441">Ja, Coscript hat eine Qualitätssicherungsprüfung durchlaufen.</sample>
    <sample id="442">The slide discusses the challenges of evaluating context-dependent translation. It mentions that only a small portion of words depend on context and highlights corpus-level metrics as an issue, noting that existing methods support limited discourse phenomena and languages.</sample>
    <sample id="443">Das Bild zeigt eine Präsentation von Google Research. Der Titel der Präsentation lautet "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)". Die Autoren sind Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis. Das Logo von Google Research ist im unteren linken Eck sichtbar. Im unteren rechten Eck befindet sich ein kleiner Bildschirm eines Menschen, der wahrscheinlich die Präsentation gibt. Hintergrund des Bildes sind gestreifte Linien in verschiedenen Farben (rot, blau, grün) zu sehen.</sample>
    <sample id="444">Das Bild zeigt eine Präsentation von Google Research. Der Titel der Präsentation lautet: "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)". Die Namen der Autoren sind: Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis. Im unteren linken Eck des Bildes ist das Google Research Logo zu sehen. Das Bild hat ein weißes Hintergrund mit farbigen Linien, die sich nebeneinander verlaufen. Im unteren rechten Eck befindet sich ein kleiner Bildausschnitt eines Menschen.</sample>
    <sample id="445">Indirekte Beziehungsausdrücke sind in der Präsentation zu finden.</sample>
    <sample id="446">Indirekte Bezeichnung</sample>
    <sample id="447">Indirekte Bezeichnungen können in natürlichen und flüssigen Gesprächen verwendet werden. Sie können passen, wenn man den Namen der Musik nicht mehr erinnert oder wenn die Vokabeln schwierig zu unterscheiden sind.</sample>
    <sample id="448">Indirekte Beziehungen können in natürlichen und flüssigen Gesprächen verwendet werden: - Man kann den Namen nicht erinnern - Die Anordnungen sind schwierig zu unterscheiden - Man möchte eine Präferenz angeben</sample>
    <sample id="449">Indirekte Verweise können in natürlicher und flüssiger Diskussion verwendet werden: - Man kann den Namen nicht mehr erinnern - Die Aussagen sind schwierig zu unterscheiden - Man möchte eine Vorliebe angeben</sample>
    <sample id="450">Das Hauptthema der Präsentation ist "Dataset Collection".</sample>
    <sample id="451">Das Bild zeigt eine Präsentation über "Dataset Collection". Es gibt mehrere Punkte, darunter: - Ein wichtiges Problem ist die Konversational-Systeme und das Benchmarking von Large Language Models für das Entity Understanding. - Es gibt keine große öffentliche Dataset-Arbeitsbereich. - Sie sammeln ein großes Dataset mithilfe von Crowd-Annotation. - Es gibt drei Bereiche: Musik, Bücher und Rezepte. Darüber hinaus sind auf der Seite einige Symbole zu sehen, wie ein Headset, Bücher und ein Teller. Im unteren rechten Eck des Bildes ist ein kleiner Bild eines Menschen zu sehen.</sample>
    <sample id="452">Das Dataset-Sammlungsmethode betont die Unformalität durch das Verwenden eines Cartoon-Fill-In-The-Blanks-Aufgaben.</sample>
    <sample id="453">The dialogue context is set by Bob.</sample>
    <sample id="454">图片中的文字内容包括： 1. 标题：Dataset Collection Methodology 2. 副标题：Methodology emphasizes informality using a cartoon completion task 3. 图片中的对话框： - "Remember that we were listening to yesterday?" - "Do you mean 'Easy on Me' or 'I Got a Feeling'?" 4. 图片右侧的黄色箭头注释：Filled in by the annotator 5. 底部的文字：Resolving Indirect Referring Expressions for Entity Selection (Mistakes Correlate)</sample>
    <sample id="455">The alternative question is 'Do you mean "Easy on Me" or 1 Gotta Feeling?'.</sample>
    <sample id="456">Das Bild zeigt eine Präsentation über die 'Dataset Collection Methodology' von Google Research. Das Schwerpunkt der Präsentation liegt auf der 'Informalität' im Zusammenhang mit einem 'Cartoon Completion Task'. Die Präsentation umfasst eine Illustration, die vier Personen zeigt, die in einem Gespräch miteinander sind. Jeder Person ist mit einem Textblatt verbunden, das Informationen über den Kontext des Dialogs und die Frage bereitstellt. Der Text auf dem Bild lautet: 'Remember that you were listening to yesterday?' und 'Do you mean Easy on Me or 1 Gotta Feeling?'. Es gibt auch einen Text, der darauf hinweist, dass der Text im Zweiten und Dritten Sprochblatt automatisch generiert wurde, während der Text im dritten Sprochblatt von einem Annotator ausgefüllt wurde.</sample>
    <sample id="457">Das zweite Bild zeigt eine Alternativefrage, die von einer der Entity-Auswahl-Abhängigkeiten generiert wird. Diese Alternativefrage ist "Do you mean 'Easy on Me' or 1 Gotta Feeling?"</sample>
    <sample id="458">图片中的文字内容包括：

标题：Generate alternative questions =&gt; sampling entity pairs

副标题：Do you mean A or B?

正文：
- Items with similar infoboxes on Wikipedia (same genre and/or artist)
  - Do you mean This is It or Man in the Mirror?
- Items with similar descriptions on Wikipedia
  - Do you mean Thinking of You or Happy Anywhere?
- Items with similar titles:
  - Do you mean The Return (memoir) or The Return (Shatner novel)?
- Uniform at random:
  - Do you mean You Could Be Mine or The Way I Am?

底部注释：Revisiting Inductive Expressions for Entity Selection Variability Correlations

右上角有Google Research的标志。</sample>
    <sample id="459">生成替代问题 =&gt; 通过实体对进行实体识别</sample>
    <sample id="460">Der erste Satz lautet: "Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean This is it or Man in the Mirror?"</sample>
    <sample id="461">Der englische Inhalt lautet: Generate alternative questions =&gt; sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean This is It or Man in the Mirror? Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am</sample>
    <sample id="462">图片中的文字内容包括：

标题：Generate alternative questions =&gt; sampling entity pairs

副标题：Do you mean A or B?

正文：
- Items with similar infoboxes on Wikipedia (same genre and/or artist)
  - Do you mean This is It or Man in the Mirror?
- Items with similar descriptions on Wikipedia
  - Do you mean Thinking of You or Happy Anywhere?
- Items with similar titles:
  - Do you mean The Return (memoir) or The Return (Shatner novel)?
- Uniform at random:
  - Do you mean You Could Be Mine or The Way I Am?

左下角有一段小字：
- Reading Indirect Expressions for Entity Selection Adaptability Correlation

右上角有一个标志，写着Google Research。

左侧有一个黄色箭头，上面写着“More Similar (usually harder)”。

右下角有一个圆形的图像，显示一个穿着黑色衬衫的人。</sample>
    <sample id="463">The background knowledge (Music) slide from Google Research shows two songs, "Easy on Me" by Adele and "I Gotta Feeling" by The Black Eyed Peas.</sample>
    <sample id="464">Das Bild zeigt eine Präsentation mit dem Titel 'Background knowledge (Music)' auf der oberen rechten Ecke. Es gibt zwei Lieder, 'Easy on Me' von Adele und 'I Gotta Feeling' von The Black Eyed Peas, jedes mit einem Google-Suche-Link daneben. Darunter steht: 'Wir bitten Annotatoren, um: - Zumindest einige Lieder zu hören - Jedes Lied zu lesen'. Im unteren rechten Eck ist ein kleiner Bild eines Menschen zu sehen.</sample>
    <sample id="465">图片显示了一个名为“Easy on Me”的YouTube视频的搜索结果。搜索结果显示了视频的缩略图、标题以及一些关于歌曲的信息，包括歌词和相关搜索建议。</sample>
    <sample id="466">Das Bild zeigt eine Slide von einer Präsentation, die sich auf 'Rezepten' konzentriert. Es gibt zwei Rezepte: 'Simnel Cake' und 'Pandan Cake'. Jedes Rezept wird mit einem Bild der zugehörigen Kuchen abgebildet. Die Slide hat den Titel 'Background knowledge (Recipes)' und befindet sich innerhalb eines Google Research-Setting. Im unteren linken Eck ist ein Text zu sehen, der auf "Reinforcement Learning for Entity Selection in Legal Contract Drafting" hinweist.</sample>
    <sample id="467">Dies ist eine Screenshot eines Google-Slides, das sich mit der Verwendung von Indirektreferenzierungspraktiken für die Entity Selection-Abbildung beschäftigt. Das Slide hat einen weißen Hintergrund und zeigt zwei Optionen: 'Easy on Me (by Adele)' und 'I Gotta Feeling (by the Black Eyed Peas)'. Es gibt auch eine Anweisung, den Annotatoren zu ermutigen, eine der beiden Optionen auszuwählen und sie zu beschreiben, indem sie 3 bis 5 Indirektreferenzierungspraktiken benutzt.</sample>
    <sample id="468">图片中的文字内容包括： 1. 标题：Eliciting expressions 2. 副标题：Revisiting Indirect Referencing Expressions for Entity Selection (Adaptability Corpus) 3. Google Research标志 4. 文本内容： - We then tell the annotators which choice should be selected and ask them to describe it. - Pick one this: Easy on me (by Adele) I Gotta Feeling (by the Black Eyed Peas) - We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble. For example: The one with the piano music The other without words Not the one with the 12 year old boy 5. 随机示例： - 音乐选择： - Music 'Chime' or 'Your Loving Arms'? - The one without words - These kids or 'Ines capable'? - Is it the song sung by an Australian? - Rock the Boat or 'Wherever You Are'? - Has synthesizer sounds in it - Telepathy 'Stars on 45'? - Came out of mid-2000. - Music 'Mis-healed' or 'Shameful'? - Remind me of 6. 书籍选择： - Warlock (Half novel) or 'Warlock (Smith novel)? - The one that is set in the 1980s - The Legion of Space or 'The Legion of the Sea' - Written by a famous detective writer - The Good Soldier or 'The Good Soldier' - The fictional one - The Giaour or 'The Giaour' - The one not with the 12 year old boy - Broken Sleep or 'Broken Sleep' - Has rock and politics in it 7. 食谱选择： - Beume Maire or 'Hitchhiker' or Chitamur? - Comes from Azerbaijan - Kusa mochi or 'mushi'? - The Japanese steamed cake - Can't I call him 'Carnold' or 'Boccaro'? - The ones eaten at Christmas - Johnyke or 'Johnyke'? - Cornish is the most important thing - Sagu or 'Sagu'? - And so on...</sample>
    <sample id="469">Das 'AltEntities Corpus' hat über 6.000 alternative Fragen auf drei verschiedenen Gebieten und etwa 42.000 indirekte Bezeichnungen. Die Ergebnisse mit dem T5 XL-Modell zeigen, dass es 92-95% wahrscheinlich ist, dass der LM dieselbe Hintergrundwissen hat als Annotatoren, wenn es vollständig überschneidende Hintergrundwissen hat. Es gibt auch eine 82-87%-Likelihood, wenn das LM nur teilweise überschneidende Hintergrundwissen hat. Wenn das LM nur den Entity-Namen hat, gibt es eine 60%-Likelihood. Sie haben gezeigt, dass die Modelle domänen-generalisierbar sind.</sample>
    <sample id="470">Der englische Text lautet: 'If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high. It's around 92-95%.'</sample>
    <sample id="471">图片中的文字内容包括： Google Research AltEntities Corpus ~6,000 alternative questions across the three domains ~42,000 indirect referring expressions Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82%-87% when the LM has access to partially overlapping background knowledge. ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research/datasets/AltEntities</sample>
    <sample id="472">图片中的文字内容包括： 'AltEntities Corpus' 'Google Research' '6,000 alternative questions across the three domains' '42,000 indirect referring expressions' 'Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82-87% when the LM has access to partially overlapping background knowledge. 60% when the LM (T5 XL) has only access to the entity names.' 'We showed models are domain-generalizable.' 'Dataset Link: https://github.com/google-research/datasets/AltEntities' 'Thank You If you have any questions, please email javadh@google.com'</sample>
    <sample id="473">The speaker is discussing the comparison of a new approach with existing Simultaneous Translation (SimulST) strategies.</sample>
    <sample id="474">Die Autoren gehören der Avignon Universität und der Nantes Universität.</sample>
    <sample id="475">Sebastian Senti, Ronan Le Bras, Katarina Reinecke, and Maarten Sap.</sample>
    <sample id="476">The image shows a presentation slide with the title "Marked Personas" and mentions three authors: Myra Cheng, Esin Durmus, and Dan Jurafsky. Therefore, there are 3 authors involved in this work.</sample>
    <sample id="477">Das Bild zeigt eine Präsentation über "Attention as a Guide for Simultaneous Speech Translation" von Sara Papi, Matteo Negri und Marco Turchi. Das Layout der Präsentation ist einfach, mit einem blauen Textfeld im Zentrum, das den Titel und die Autoren aufweist. Links am unteren Rand befindet sich ein kleiner Kreis in Blau, und rechts sind Logos der Universität di Trento und der Fondazione Bruno Kessler zu sehen. Im oberen rechten Eck ist eine Person zu sehen, die wahrscheinlich über eine Videokonferenz teilnimmt.</sample>
    <sample id="478">Das Bild zeigt eine Präsentation über 'Simultane Speech Translation' (Simultäne Sprachübersetzung). Im oberen rechten Eck ist eine Person zu sehen, die wahrscheinlich die Präsentation macht. Der Text auf der Seite lautet: 'Was ist Simultane Sprachübersetzung?' Mit einem Bildschirm neben dem Text, das den Text 'Wenn ich im Sommer kalten Tee in meine Thermoskanne gieße, bleibt es kalt, und wenn ich' zeigt. Darunter steht: 'Simultane Sprachübersetzung (SimulST) ist das Prozess, einen gesprochenen Sprachzug in ein Text in einer anderen Sprache in Echtzeit zu übersetzen, was Klangsprachvermittlung ermöglicht.'</sample>
    <sample id="479">Der englische Text lautet: 'What are the problems of the current SimulST models?'</sample>
    <sample id="480">Das Bild zeigt eine Präsentation, die sich auf die Probleme der aktuellen SimulST-Modelle konzentriert. Im oberen linken Eck des Bildes ist ein Text zu sehen, der lautet: 'Was sind die Probleme der aktuellen SimulST-Modelle?' Diese Frage wird in deutscher Übersetzung auf dem Bild angezeigt. Darunter sind zwei Punkte aufgeführt:

1. Spezifische Architekturen werden üblicherweise trainiert, was zusätzliche Module zur Optimierung einführt.
2. Lange und komplizierte Trainingsverfahren (z.B. verschiedene Optimierungsziele).

Im unteren rechten Eck des Bildes ist der Text 'Seite 06' zu sehen, was darauf hinweist, dass dies die sechste Seite der Präsentation ist.

Darüber hinaus befindet sich im oberen rechten Eck des Bildes ein kleiner Bildschirm mit einer Person, die wahrscheinlich die Präsentation macht oder beobachtet.</sample>
    <sample id="481">Das Bild zeigt eine Präsentation über die Probleme der aktuellen SimulST-Modelle. Die Präsentation ist auf Deutsch und zeigt drei Hauptprobleme: 1. Spezifische Architekturen werden üblicherweise trainiert, was zusätzliche Module zum Optimieren erfordert. 2. Langes und kompliziertes Training verursacht Probleme (z.B. verschiedene Optimierungsaufgaben). 3. Das Trainieren und Wartung mehrerer Modelle für verschiedene Latenzregime erfordert. Der Text im Bild lautet: 'Was sind die Probleme der aktuellen SimulST-Modelle?' Diese Informationen sind in einer Slide-Presentation dargestellt, die wahrscheinlich Teil eines größeren Vortrags oder Seminars ist.</sample>
    <sample id="482">Was ist unsere Lösung?</sample>
    <sample id="483">Das Bild zeigt eine Präsentation auf einem Computerbildschirm. Der Titel der Präsentation lautet 'Was ist unsere Lösung?' und es gibt zwei Punkte, die aufgeführt sind: 1. Verwenden Sie bereits vorhandene offline ST-Modelle ohne Umlauftraining oder die Anwendung eines bestimmten Architekturkonzepts für SimulST. 2. Benutzen Sie nur ein Modell für jeden Latenzregime und verwalten Sie die Latenz durch bestimmte Parameter. Im unteren rechten Eck des Bildes befindet sich die Seite 'page 09'. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich über eine Webcam mitgespricht hat.</sample>
    <sample id="484">Das Bild zeigt eine Präsentation, die sich auf eine Lösung für eine bestimmte Problemstellung konzentriert. Im oberen rechten Eck des Bildes ist ein kleiner Video-Überwachungsraum zu sehen, der wahrscheinlich einen Teilnehmer oder einen Vortrager darstellt. Der Hauptinhalt der Präsentation besteht aus einem Textblock auf der linken Seite, der die folgenden Punkte enthält: 1. Verwenden Sie bereits vorhandene offline ST-Modelle ohne erneutes Training oder das Adoption einer bestimmten Architektur für SimuST. 2. Gebrauchen Sie nur ein Modell für jede Latenzregime und verwalten Sie Latenzprobleme durch bestimmte Parameter. 3. Nutzen Sie das von dem Modell bereits erworben Wissen über den Aufmerksamkeitsmechanismus zwischen Audio- und Textausgabe, um es zu verbessern. Rechts neben diesem Textblock befindet sich ein Diagramm, das eine visuelle Darstellung eines Aufmerksamkeitsmechanismus zwischen Audio- und Textausgabe zeigt. Das Diagramm zeigt eine Audio-Signalskala und eine Textausgabe mit den Worten "Ich bin ein Student". Die Präsentation scheint sich auf Sprachverarbeitung und die Verbesserung von Modellen in diesem Bereich zu konzentrieren.</sample>
    <sample id="485">Das Bild zeigt eine Präsentation über 'Encoder-Decoder Attention' (Encoder-Decoder Aufmerksamkeit). Der Titel der Slide lautet 'Our solution: EDAtt'. Die Slide enthält Text, der erklärt, dass die Aufmerksamkeit entscheidet, ob eine teilweise Übersetzung oder nicht erfolgen soll, basierend auf der Konzentration der Aufmerksamkeit auf einen Wort. Wenn die Aufmerksamkeit nicht konzentriert ist, bedeuten mehrere Sprachrahmen, dass die empfangene Information stabil genug ist. Im unteren rechten Eck des Bildes ist 'page 014' zu sehen, was darauf hinweist, dass dies die 14. Seite der Präsentation ist. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="486">Das Bild zeigt eine Präsentation, die sich auf ein Problembeispiel in der Sprachverarbeitung konzentriert. Der Titel der Präsentation lautet 'Our solution: EDAtt'. Die Hauptdarstellung ist ein Text, der sich auf die Entscheidung bei der Übersetzung zu konzentrieren oder nicht zu konzentrieren basiert. Der Text lautet: 'Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold α) towards the last λ speech frames, meaning that the received information is enough stable.' Dieser Text diskutiert eine Methode zur Entscheidung, ob eine Partialübersetzung emittiert werden sollte, basierend auf der Konzentration der Aufmerksamkeit auf bestimmte Wörter und den letzen λ Sprachrahmen. Im unteren rechten Eck des Bildes befindet sich eine Seitezahl, die 'page 014' anzeigt.</sample>
    <sample id="487">Das Bild zeigt eine Präsentation mit einem Schwerpunkt auf 'Encoder-Decoder Attention'. Der Text lautet: 'Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold a) towards the last λ speech frames, meaning that the received information is enough stable.' Darüber hinaus gibt es einen kleinen Text, der lautet: 'I am going to talk about...' und eine Kamera in der oberen rechten Ecke.</sample>
    <sample id="488">Der englische Text lautet: 'Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold c) towards the last λ speech frames, meaning that the received information is enough stable.'</sample>
    <sample id="489">Das Bild zeigt eine Präsentation über den Thema 'Encoder-Decoder Attention'. Der Titel der Präsentation lautet 'Our solution: EDAtt', und es gibt einen Text, der erklärt, wie ein Decider entscheidet, ob eine Teilübersetzung oder nicht stattfinden soll, basierend auf der Konzentration des Worts auf die zuletzt empfangenen Sprachrahmen. Es gibt auch eine visuelle Darstellung von Sprachrahmen und der Phrase 'Ich werde reden' in deutscher Sprache. Im Bild ist auch ein kleiner Bildschirm mit einer Person zu sehen.</sample>
    <sample id="490">Das bedeutet, dass die ersten beiden Wörter "emitted" werden.</sample>
    <sample id="491">Das Bild zeigt eine Präsentation über den 'Encoder-Decoder Attention' in einem virtuellen Setting. Der Titel der Präsentation lautet 'Our solution: EDAtt'. Die Slide enthält eine Diagramm, das die Wörter 'Ich werde reden.' und 'EMITTED' zeigt, sowie einen Text, der erklärt, dass ein Wort emittiert wird, wenn die Aufmerksamkeit unter einem bestimmten Schwellwert 'α' bleibt. Im Hintergrund ist ein Bild einer Frau zu sehen, die möglicherweise das Vortrag gibt.</sample>
    <sample id="492">Das Bild zeigt eine Präsentation über die 'Encoder-Decoder Attention' in Sprachverarbeitung. Im oberen linken Eck befindet sich der Text 'Our solution: EDAtt'. Rechts davon ist ein Textblock mit dem Titel 'Encoder-Decoder Attention', der beschreibt, wie ein Wort emittiert wird, wenn es nicht konzentriert wird, und erläutert, dass das empfangene Information stabile ist. Links unten sind zwei Diagramme zu sehen, die 'Ich werde reden' und 'Ich werde Klima sprechen' zeigen. Rechts oben ist ein kleiner Bildschirm mit einem Menschen zu sehen. Am unteren rechten Eck des Bildes ist der Text 'page 023' zu lesen.</sample>
    <sample id="493">Wir sehen, dass keine Worte auf die letzten Sprachrahmen zeigen.</sample>
    <sample id="494">Dies bedeutet, dass diese drei Wörter ausgesprochen werden.</sample>
    <sample id="495">Die Hauptergebnisse der EDAtt</sample>
    <sample id="496">Das Bild zeigt eine Präsentation mit einem Graphen im Zentrum. Der Graph hat den Titel 'Main Results: EDAtt' und zeigt die BLEU-Werte gegen die AL/AL_CA (s) -Werte. Im unteren linken Eck des Graphen steht der Text 'quality measure', und im unteren rechten Eck steht 'latency measure'. Im oberen rechten Eck ist ein kleineres Bild eines Menschen zu sehen, was auf eine Online-Präsentation hinweist. Im unteren rechten Eck ist der Text 'page 028' zu lesen.</sample>
    <sample id="497">The frame shows a presentation slide with the title 'Main Results: EDAtt' in blue text at the top left corner. Below this, there is a graph plotting BLEU scores against AL/AL_CA (s), which appears to be an latency measure. The x-axis ranges from 0.5 to 6, and the y-axis ranges from 21 to 27. There are some emojis above the graph, including a question mark, a thumbs-up, a smiley face, a sad face, a neutral face, and a confused face. In the bottom right corner of the slide, it says 'page 030'. Additionally, there is a small image of a person on the right side of the slide.</sample>
    <sample id="498">Das Bild zeigt eine Präsentation, die sich auf die Hauptergebnisse einer Forschungsstudie konzentriert. Der Titel der Präsentation lautet "Main Results: EDAtt". Im Hintergrund befindet sich ein Graph, der den BERT-Index als abhängig von der AL/AL_CA (s)-Anzahl zeigt. Die Y-Achse des Graphs zeigt einen Wert von 27 an, während die X-Achse zwischen 0.5 und 6 reicht. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen, der wahrscheinlich über eine Videokonferenz teilnimmt. Im unteren linken Eck des Bildes befindet sich ein Logo mit der Aufschrift "F3K". Im unteren rechten Eck ist die Seite des Dokuments angegeben als "page 031".</sample>
    <sample id="499">Der englische Text lautet: Main Results: EDAtt.</sample>
    <sample id="500">Das Bild zeigt eine Präsentation, die sich auf die Ergebnisse einer Forschungsarbeit über die Übersetzung von englischen ins Deutsche konzentriert. Im oberen rechten Eck ist ein kleiner Bildschirm mit einem Menschen zu sehen, der wahrscheinlich die Präsentation führt. Der Hauptinhalt des Bildes ist ein Diagramm mit dem Titel 'Main Results: EDAtt', das die BLEU-Score in Bezug auf die AL/AL_CA (oder AL/AL_CA) Zeit in Sekunden darstellt. Die Legende auf dem Diagramm gibt an, dass verschiedene Strategien wie wait-k (orange), LA (blau), CAAT (grün) und EDAtt (rot) verglichen wurden. Im oberen Rand des Diagramms befindet sich der Text 'popular strategies also applied to offline models'. Im unteren rechten Eck ist der Text 'page 033' zu sehen, was darauf hinweist, dass dies die 33. Seite der Präsentation ist.</sample>
    <sample id="501">Die Hauptergebnisse sind EDAtt.</sample>
    <sample id="502">The English content in the image is: 'Main Results: EDAtt' and 'EDAtt outperforms all the strategies applied to offline models (a) en→de'.</sample>
    <sample id="503">Der englische Text lautet: 'And we also see that if we consider the actual elapsed time or the computational aware time, and that is the fastest strategy'.</sample>
    <sample id="504">Das Bild zeigt eine Präsentation auf einem Computerbildschirm. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich die Präsentation gibt. Der Hauptinhalt des Bildes ist ein Text, der in Englisch geschrieben ist und folgende Informationen enthält: - Ein Titel, der lautet: 'Do you want to discover more?' - Eine Aufforderung, 'Read our paper to discover more results!' - Kontaktinformationen, einschließlich einer E-Mail-Adresse (marco.turchi@gmail.com), einer GitHub-URL (github.com/hlt-mt/fbkl-fairseq) und eines Twitter-Handles (@fbkl_mt). - Eine QR-Code mit dem Text 'Scan me!' - Links unten ist ein Logo zu sehen, das 'FBKL' lautet. - Im unteren rechten Eck befindet sich der Text 'page 038'.</sample>
    <sample id="505">Ja, der Dataset-Link wird als öffentlich zugänglich angegeben.</sample>
    <sample id="506">MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning Zhiyang Xu* Ying Shen* Lifu Huang Department of Computer Science, Virginia Tech *Equal Contribution</sample>
    <sample id="507">Der Text auf der Seite lautet: 'Pre-trained Language Models for Downstream Tasks'.</sample>
    <sample id="508">Das Bild zeigt eine Präsentation über "Vorgegebene Sprachmodellierung für Nebenaufgaben". Es gibt drei Hauptbereiche: (A) Vorgegebene- und feinjustierte Modellierung (BERT, T5), (B) Prompting (GPT-3) und (C) Anweisungsverfeinerung (FLAN). Jede der Bereiche hat ein Diagramm, das die Prozesse und Vorteile der jeweiligen Methode darstellt. Im unteren rechten Eck des Bildes ist eine Person zu sehen, die wahrscheinlich über diese Themen spricht.</sample>
    <sample id="509">Das Bild zeigt eine Person, die in einem schwarzen Jackett und einer weißen Unterbekleidung steht. Sie trägt Brille und hat ein Haarstil mit einem Zopf. Im Hintergrund ist ein schwarzer Farbton zu sehen, und im unteren rechten Eck des Bildes befindet sich ein kleiner Overlay-Text, der 'Language-only' lautet.</sample>
    <sample id="510">Das Bild zeigt einen Titelbildschirm mit dem Text "Instruction Tuning on Multimodal Pre-trained Models". Der Text ist auf einem schwarzen Hintergrund zu sehen, und der Begriff "Multimodal" wird in Gelb hervorgehoben. Im unteren rechten Eck des Bildes ist ein kleiner Bildausschnitt eines Menschen zu sehen.</sample>
    <sample id="511">Imbalanc in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="512">Imbalanc in Instructional Datasets between NLP and Multimodal 1600+ Language-only instruction tasks NO large-scale, publicly-available multimodal instruction tasks Wang, Yizhong, et al. "Benchmarking generalization via in-context instructions on 1600+ language tasks" arXiv preprint arXiv:2304.05789</sample>
    <sample id="513">MULTINSTRUCT ist der erste multimodale Anweisungsabgleich-Benchmark-Datensatz. Er besteht aus 62 verschiedenen multimodalen Aufgaben, die 10 Gruppen abdecken und von 5 Experten geschriebenen Anweisungen bestehen.</sample>
    <sample id="514">Das Bild zeigt eine präsentationelle Slide, die sich auf das Multimodal Instruction Tuning Benchmark Dataset konzentriert. Der Titel der Slide lautet "MULTINSTRUCT" und darunter steht "The first multimodal instruction tuning benchmark dataset". Die Slide enthält eine Diagramm, das verschiedene Aufgaben in Kategorien wie Visual Relationship, VQA (Visual Question Answering), Temporal Ordering, Grounded, Grounded Matching, Misidentification, Image Understanding und Region Understanding aufzulisten scheint. Jede Kategorie hat verschiedene Aufgaben innerhalb, wie Open VQA, Object Manipulation, Object Recognition usw.

Rechts neben dem Diagramm sind folgende Punkte aufgeführt: - 62 diverse multimodale Aufgaben - 10 große Gruppen - 5 erfahrene geschriebene Anweisungen

Am unteren Ende der Slide ist ein Text zu finden, der lautet: "Figure 2: Task Groups Included in Multimodal Instruction Tuning. The yellow boxes represent tasks used for evaluation, the white ones indicate training." Dieser Text erklärt, dass die gelben Boxen auf den Aufgaben im Bildbeispiel beziehen sind, die für die Bewertung verwendet wurden, und die weißen Boxen auf die Aufgaben hinweisen, die für die Ausbildung verwendet wurden.

Darüber hinaus befindet sich am unteren rechten Eck des Slides eine Person, die möglicherweise über das Thema sprechen könnte.</sample>
    <sample id="515">OFA (One For All) ist ein unified multimodales pre-trained-Modell, das in der Lage ist, sowohl die Verständnis als auch die Generierung von Aufgaben mit einer oder mehreren modalen Datenquellen auszuführen.</sample>
    <sample id="516">Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct-Datensatz.</sample>
    <sample id="517">Das Bild zeigt eine Übersetzung des englischen Inhalts ins Deutsche. Der Text lautet: "To unify the processing of a variety of input and output data types, we propose a unified framework for multimodal instruction." Dieser Satz beschreibt eine Vorschlag für eine einheitliche Struktur zur Verarbeitung verschiedener Datenarten in einem kontextuellen Kontext.</sample>
    <sample id="518">Das Bild zeigt eine visuelle Darstellung von Beispiel-Einheiten aus der MultiInstruct-Datenbank, die sich auf vier Aufgaben spezialisiert hat:</sample>
    <sample id="519">Multi-modal Instruction Tuning</sample>
    <sample id="520">Das Bild zeigt eine Präsentation mit dem Titel 'Multi-Modal Instruction Turning'. Es gibt zwei Hauptabschnitte: 'Training Dataset Construction' und 'Testing Dataset Construction'. Im ersten Abschnitt werden Details über die Erstellung des Trainings-Datensatzes aufgeführt: 53 Aufgaben aus 9 Gruppen für die Ausbildung, wobei jeder Aufgabe 10.000 Instanzen ausgewählt wird. Im zweiten Abschnitt sind Details über die Erstellung des Test-Datensatzes aufgeführt: Die gesamte Commonsense Reasoning-Gruppe wird für das Testen reserviert, 5 weitere Aufgaben aus den Gruppen VQA und Miscellaneous ausgewählt, und 20 Zufällig ausgewählte Instanzen aus der Gruppe Natural Instructions für NLP-Anwendungen. Im unteren rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen, der wahrscheinlich über die Präsentation sprechen wird.</sample>
    <sample id="521">Das Bild zeigt eine Präsentation über "Multi-Modal Instruction Turning". Es gibt zwei Hauptabschnitte: Training Dataset Construction und Testing Dataset Construction. Der Text auf der Präsentation lautet: Training Dataset Construction: - Use 53 tasks from 9 groups for training. - Sample 10,000 instances per task. Testing Dataset Construction: - Reserve the entire Commonsense Reasoning group for testing. - Select additional 5 tasks from VQA and Miscellaneous groups. - We use all the instances in the test split for each task. - Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="522">Das Bild zeigt eine Präsentation mit dem Titel "Implementation Details". Es gibt zwei Hauptabschnitte: Training details und Testing details. Im Training details-Section sind die folgenden Punkte aufgeführt: - Pre-trained OFA-Large model (472M) - Alle Instanzen für alle Aufgaben mischen - Jede Instanz wird zufällig mit einer seiner fünf Anweisungslisten kombiniert. Im Testing details-Section sind die folgenden Punkte aufgeführt: - Für jede Aufgabe durchführen wir fünf Experimente, indem wir den Modell durch die Auswertung eines der fünf Anweisungen in jedem Experiment bewerten. - Wir berichten über das Durchschnittswert und das Maximum der Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente.</sample>
    <sample id="523">Implementation Details Training details: - Pre-trained OFA-Large model (472M) - Mix all the instances for all tasks. - Each instance is randomly combined with one of its five instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="524">Für jede Aufgabe führen wir fünf Experimente durch und bewerten die Modellleistung mithilfe eines der fünf Anweisungstemplates.</sample>
    <sample id="525">Das Bild zeigt eine Präsentation über die Bewertungsmaßstäbe. Der Titel der Slide lautet "Bewertungsmaßstäbe". Es gibt mehrere Punkte, die verschiedene Arten von Aufgaben und ihre zugehörigen Bewertungsmaßstäbe beschreiben. Zunächst wird erwähnt, dass für mehrmoduläre Klassifizierungsaufgaben die Genauigkeit als Bewertungsmaßstab verwendet wird. Danach werden mehrmoduläre Generierungsaufgaben aufgeführt, bei denen die Rouge-L-Wertung als Bewertungsmaßstab verwendet wird. Schließlich wird festgestellt, dass für NLP-Aufgaben auch die Rouge-L-Wertung als Bewertungsmaßstab verwendet wird. Darüber hinaus wird erwähnt, dass für jede Modell-Instanz die aggregierte Leistung berechnet wird, basierend auf der Leistung des Modells in allen mehrmodulären und NLP-unseen Aufgaben. Diese Aggregierte-Leistung wird mit Rouge-L als Leistungsmerkmal für die meisten Aufgaben und mit Genauigkeit als Merkmal für alle Aufgaben berechnet.</sample>
    <sample id="526">Sensitivität</sample>
    <sample id="527">Effektivität der Anweisungsabfinung auf MULTINSTRUCT</sample>
    <sample id="528">Effektivität der Anweisungsabfinung auf MULTINSTRUCT</sample>
    <sample id="529">Der Inhalt auf dem Bild zeigt eine Präsentation über den Einfluss der Steigerung von Multimodalen Aufgabencluster-Gruppen auf die Modellleistung. Der Titel lautet "Impact of Increasing Multimodal Task Clusters". Links sind verschiedene Aufgabengruppen aufgeführt, wie "Img Und" (Bildverstehen), "Grounding", "MISC, ITM" (Temporales Ordnung, Mehrfachaufgaben, Textverstehen), "Relation" (Visuelle Beziehungen) und "Region" (Regionale Verständnis). Darunter wird auch auf "NLP" (Natural Language Processing) verwiesen.

Rechts befindet sich ein Graphik, der die Modellleistung als Funktion der Aufgabenclusteranzahl zeigt. Der Graphiktitel lautet "Figure 3: Model Performance as a Function of the Number of Task Clusters". Die Legende auf der Grafik gibt an, dass die Linienbedeckungen "Max" (Maximaler Wert) und "Sensitivity" (Sensitivität) anzeigen. Die Y-Achse zeigt die Leistung in Prozent, während die X-Achse die Anzahl der Aufgabencluster angibt.

Im unteren rechten Eck ist ein Text, der lautet: "Model Performance as the number of Task Clusters increases." Diese Information hilft dabei, die Beziehung zwischen der Anzahl der Aufgabencluster und der Modellleistung zu verstehen.</sample>
    <sample id="530">Effekt der verschiedenen Anweisungen bei der Anpassung von Anweisungen.</sample>
    <sample id="531">Das Bild zeigt eine Präsentation über die Effekte von Feinabgestimmungsstrategien auf die Sensitivität von Modellen. Im oberen Teil des Bildes befindet sich der Titel 'Effect of Fine-tuning Strategies on Model Sensitivity'. Darunter sind zwei Punkte zu sehen: 1. 'Instruction tuning on Multilnstruct can significantly reduce the sensitivity of OFA.' 2. 'Transfer learning from Natural Instructions dataset can further reduce the sensitivity of the model.' Im unteren Teil des Bildes befindet sich ein Diagramm mit einem Szenario, das die Sensitivität unterschiedlicher Modelle in Bezug auf unerkannte Bewertungsaufgaben darstellt. Das Diagramm zeigt die Sensitivität in Zahlen und zeigt, dass eine höhere Sensitivität eine schlechtere Leistung bedeutet (mit 'Lower is better' angegeben). Die Sensitivitäten der Modelle sind wie folgt: - OFA: 40.58 - OFA Multilnstruct: 13.84 - OFA Sepstruct: 10.45 - OFA Aftstruct: 10.27 - OFA Aftstruct: 9.64 Die Schritte, die aus dem Text hervorgehen, sind: 1. Feinabstimmungstrategien können die Sensitivität von Modellen wie OFA stark reduzieren. 2. Transfer-Learning-Strategien aus dem Natural Instructions-Datensatz können diese Reduzierung weiterhin verbessern. Diese Informationen deuten darauf hin, dass Feinabstimmung und Transfer-Learning effektive Methoden sind, um die Sensitivität von Modellen zu reduzieren, was zu besserer Stabilität und Robustheit führen kann.</sample>
    <sample id="532">Das Bild zeigt eine Präsentation über die Leistung von NLP-Methoden bei Zero-Shot-Tests. Der Titel lautet 'Zero-Shot Performance on NLP Tasks' (Zero-Shot-Leistung bei NLP-Aufgaben). Es gibt zwei Hauptpunkte: 1. 'Instruction Tuning auf Multinstruct kann die Zero-Shot-Leistung bei unbekannten NLP-Aufgaben verbessern.' 2. 'Die Transfer-Learning-Strategie MixedInstruct kann das Zero-Shot-Kapazität bei der Natur-Anweisungs-Datenbank verbessern.' Darunter gibt es eine Tabelle mit den folgenden Spalten: - Model: OFA, OFA-Multinstruct, Transfer-Learning von NATURAL INSTRUCTIONS (OFA-NaturalInstruct, OFA-Segment) - RougeL: 2.25, 12.18, 43.64, 40.97, 30.79 Die Tabelle bezieht sich auf 'Table 4: Zero-Shot Performance on NLP tasks' und erklärt, dass die Ergebnisse in RougeL sind und die besten Ergebnisse in fett dargestellt sind.</sample>
    <sample id="533">Der Inhalt lautet: Conclusion First large-scale multi-modal instruction tuning dataset. Contains 62 multi-modal tasks from 10 broad categories. Significantly improve the zero-shot capability of OFA via instruction tuning. Explore several transferring learning techniques and show their benefits. Design a new metric sensitivity.</sample>
    <sample id="534">Das Bild zeigt eine Präsentation mit einem schwarzen Hintergrund und dem Titel "One More Thing!" in weißer Schrift oben. Darunter steht ein Text, der lautet: "Wir sammeln einen viel größeren multimodalen Datensatz für die Anpassung von Anweisungen und umfassen etwa 150 zusätzliche visuelle Sprachaufgaben und freigeben sie bald." Im Zentrum des Bildes befindet sich ein QR-Code, und im unteren rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen.</sample>
    <sample id="535">The authors belong to the University of Trento.</sample>
    <sample id="536">Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis</sample>
    <sample id="562">ACL 2023</sample>
    <sample id="563">Language model acceptability judgements are not always robust to context.</sample>
    <sample id="564">In diesem Werk widmen wir uns erneut dem Minimalpaar-Paradigma.</sample>
    <sample id="565">Der 'Minimal Pair Paradigm' (MPP) beurteilt Sprachmodelle auf der Basis von relativen Wahrscheinlichkeiten für die Akzeptabilität von Paaren von Wortstücken.</sample>
    <sample id="566">Minimal Pair Paradigm</sample>
    <sample id="567">Der Inhalt lautet: 'Revisiting Minimal Pair Paradigm. Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLIMP SyntaxGym CrowS 1. Many people were helping herself. No customer ... has spent any money. Stereotypical Syntactic sentence. 2. Many people were helping herself. ... has spent any money. P(1) &gt;&gt; P(2). P(1) &gt;&gt; P(2).'</sample>
    <sample id="568">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to abstract the evaluate the abstract knowledge of LMs:</sample>
    <sample id="569">Das Bild zeigt eine Präsentation mit dem Titel 'Revisiting Minimal Pair Paradigm'. Der Text auf der Seite erklärt, dass die Evaluierung von Modellen im Minimal-Pair-Paradigma (MPP) die relative Wahrscheinlichkeit in Folgeverläufen verwendet, um die abstrakte Kenntnis von LMs zu messen. Die Präsentation zeigt drei Beispiele von Minimalpaaren: 'Many people were helping' und 'No customer has spent any money' für BLiMP, 'Many people were helping herself' und 'No customer has spent any money' für SyntaxGym, sowie 'Women are terrible at handiwork' und 'Men are terrible at handiwork' für CrowS. Es gibt auch Formeln, die wahrscheinlichkeiten (P(1)) und ihre relativen Positionen (P(2)) darstellen. Am Ende der Präsentation steht ein Frage, die fragt, ob diese Urteile mit langen Kontextverständnissen stabil bleiben.</sample>
    <sample id="570">Der Inhalt lautet: 'Revisiting Minimal Pair Paradigm. Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to abstract the evaluative knowledge of LMs: BLIMP SyntaxGym CrowS 1. Many people were helping themselves. No customer has spent any money. Women are terrible at handiwork. Many people were helping themselves. P(1) &gt; P(2) P(1.any) &gt; P(2.any) P(1) &gt; P(2) Are these judgements stable with long preceding context?'</sample>
    <sample id="571">Der Ansatz besteht darin, die Schilderungen von MPP-Jugendlichkeiten in Bezug auf Längen, Struktur und Akzeptabilität zu testen.</sample>
    <sample id="572">图片中的文字内容是：Approach Test whether MPP judgements as a function of context length, structural match, and acceptability GPT2, OPT family - 125M to 6.7B BLIMP, Island</sample>
    <sample id="573">Das Bild zeigt eine Präsentation, die sich auf einen Aspekt der Sprachanalyse konzentriert. Der Titel der Präsentation lautet 'Approach'. Im Zentrum des Bildes befindet sich ein Diagramm, das verschiedene Aspekte der Analyse darstellt. Links am Diagramm sind zwei Kästen zu sehen, einer rot und einer blau, mit Text darauf. Rechts am Diagramm ist ein weiterer Kasten in Rot und Blau zu sehen. Darüber hinaus sind einige Texte auf dem Diagramm zu lesen, wie 'P(LM | Prefix) &gt; P(LM) Prefix' und 'Space of Candidate Prefixes'. Im unteren linken Eck des Bildes befindet sich der Text 'GPT2, OPT family - 125M to 6.7B', was wahrscheinlich auf die Größe der Modellparameter hinweist. Im oberen rechten Eck des Bildes ist ein kleineres Bild eines Menschen zu sehen, der möglicherweise den Vortrag gibt oder Teil der Präsentation ist.</sample>
    <sample id="574">Der Inhalt lautet: "Approach Test whether MPP judgements as a function of context length, structural match, and acceptability."</sample>
    <sample id="575">Der Inhalt lautet: 'Approach Test whether MPP judgements as a function of context length, structural match, and acceptability'</sample>
    <sample id="576">Der Inhalt lautet: 'Approach Test whether MPP judgements as a function of context length, structural match, and acceptability'.</sample>
    <sample id="577">Der Text auf der rechten Seite lautet: 'BLUMP, Essential "Quantifiers"'.</sample>
    <sample id="578">Das Bild zeigt eine Präsentation über einen Ansatz zur Bewertung von MPP-Jugendlichkeiten. Der Ansatz besteht darin, die Jugendlichkeiten auf der Grundlage des Kontextlängen, struktureller Übereinstimmung und akzeptabilität zu testen. Die Präsentation enthält auch ein Diagramm, das verschiedene Typen von Präfixen und ihre Beziehungen demonstriert. Darüber hinaus sind zwei Beispiele von unverwandten Domainen wie Wikipedia vorgestellt. Am unteren Rand der Präsentation wird erwähnt, dass GPT-2 und GPT-3-Familien von 125 Millionen bis zu 6,7 Billionen Parameter umfassen.</sample>
    <sample id="579">Der Inhalt lautet: 'Test whether MPP judgements as a function of context length, structural match, and acceptability.'</sample>
    <sample id="580">Der Inhalt lautet: "Test whether MPP judgements as a function of context length, structural match, and acceptability."</sample>
    <sample id="581">MPP评判对于任意上下文长度是稳健的。我们对匹配和不匹配的句子进行了MPP评估，这些句子的长度最多为900个标记，并且具有不同的上下文。结果显示，MPP评判在这些情况下表现良好。</sample>
    <sample id="582">MPP评判对任意上下文长度具有鲁棒性。</sample>
    <sample id="583">Der Inhalt lautet: "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance. 2) We perform MPP evaluations with different contexts – acceptable/ unacceptable; matched/mismatched structure – of lengths up to 900 tokens."</sample>
    <sample id="584">Der Inhalt lautet: "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance. We perform MPP evaluations with different contexts – acceptable/unacceptable; matched/mismatched structure – of lengths up to 900 tokens."</sample>
    <sample id="585">Der Inhalt lautet: "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance" und "We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens".</sample>
    <sample id="586">Der Inhalt lautet: "Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance. We perform MPP evaluations with different contexts - acceptable/unacceptable; mismatched/matched structure of lengths up to 900 tokens."</sample>
    <sample id="587">Der Text auf der Seite lautet: 'We perform MPP evaluations with different contexts - acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens.'</sample>
    <sample id="588">Dieser Text wurde von einem Benutzer generiert.</sample>
    <sample id="589">Das Bild zeigt eine Präsentation mit dem Titel "Why do matched prefixes affect LM judgements?". Es handelt sich um eine Analyse, wie die Präfixe in Kontextsätzen den Urteil der Sprachmodelle beeinflussen können. Die Präsentation gibt einen Eindruck von den verschiedenen Arten von Präfixen und Suffixen, die im Text verwendet werden können, und wie diese auf die Sensibilität der Sprachmodelle auswirken können. Der Inhalt des Bildes zeigt, dass das Thema "Why do matched prefixes affect LM judgements?" sehr wichtig für die Verständnis und Entwicklung von Sprachmodellen ist.</sample>
    <sample id="590">Das Bild zeigt eine Präsentation mit dem Titel "Warum beeinflussen Paare von Präfixen die Bewertungen von LM?". Der Text auf der Präsentation lautet: 'Wir perturben Kontextsätze, um die relevante Struktur zu erhalten und fragen, ob Modelle auf diese Satze reagieren. - Präfix/suffix-Adverbien: "However, &lt;sent&gt;." - Langes Präfix-Adverbien: "First and foremost, &lt;sent&gt;." - Add-Klausel: "Regardless of what X denkt darüber, &lt;sent&gt;." - Zitat: "Gestern, X sagte, &lt;sent&gt;." Rechts oben ist ein kleiner Bildausschnitt eines Menschen zu sehen. Im unteren rechten Eck des Bildes befindet sich ein Diagramm mit einer Legende, die verschiedene Typen von Perturbationen und Prefix-Typen identifiziert. Die Legende enthält die folgenden Kategorien: - Keine - Präfix-Adverbien - Add-Klausel - Quoten - Alle - Akzeptabel - Unakzeptabel. Das Diagramm zeigt eine Skala in der y-Achse, die "Delta Accuracy" (Delta-Akzurates) aufweist, und eine Skala in der x-Achse, die "Eingabellänge" (Eingabellänge) aufweist.</sample>
    <sample id="591">Der Inhalt lautet: "Warum beeinflussen korrekte Präfixe die Bewertungen von LM?" Wir perturben Kontextsätze in einer Weise, die die strukturelle Integrität erhalten, und fragen uns, ob Modelle auf diese Sätze sehr empfindlich sind.</sample>
    <sample id="592">Warum beeinflussen korrelierte Präfixe die Bewertungen von LM?</sample>
    <sample id="593">Der Inhalt lautet: "Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the structure, and ask whether models are similarly sensitive to these sentences."</sample>
    <sample id="594">Die Hauptzusammenfassung unseres Werks ist, dass Sprachmodelle auf versteckte syntaktische und semantische Merkmale reagieren, die sich über Sätze verteilen.</sample>
    <sample id="595">Sprichweise: Language models sind auf die verborgene syntaktische/semantische Merkmale vertraut, die sich über Sätze verteilen. Die Bewertungen der MPP mit kurzen, einzelnen Satz-Input sind nicht in der Lage, die abstrakte Kenntnis von LM zu erfassen.</sample>
    <sample id="596">Sure, here's the translation: "Bitte lesen Sie unser Papier für weitere Details über unsere Experimente. Danke fürs Zuhören."</sample>
    <sample id="597">The input tokens in the first step of the method are tagged with a tag.</sample>
    <sample id="598">55,000</sample>
    <sample id="626">CATS-C3G.</sample>
    <sample id="627">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="628">Die Dokumente in DEplain-web wurden mit manuellen und automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="629">Der CoNLL++-Datensatz wurde aus Reuters-Newsartikeln von 2020 erstellt und mit den Annotierungsguidelines von CoNLL-2003 versehen.</sample>
    <sample id="630">The image shows a presentation slide with the title "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations." Below the title, there are names listed as Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang. At the bottom of the slide, there are logos for Penn State University and Amazon. In the top right corner, there is an inset video feed showing a person speaking into a microphone against a background that appears to be outdoors during sunset or sunrise. The text at the bottom left indicates this is slide number 1.</sample>
    <sample id="631">Semantic Parsing ist eine Aufgabe, um semantische Darstellungen von Benutzerfragen zu erstellen, wie zum Beispiel SQL und Lambda Calculus.</sample>
    <sample id="632">Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="633">Cross-lingual Semantic Parsing Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="634">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language Neural Models SQL German Chinese Lambda FunQL</sample>
    <sample id="635">Der englische Inhalt lautet: 'Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language'</sample>
    <sample id="636">Der englische Text lautet: "Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representations."</sample>
    <sample id="637">Der Lambda-Kalkül ist fehlend.</sample>
    <sample id="638">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain neural model English German Chinese SQL Lambda FunQL</sample>
    <sample id="639">Das Bild zeigt eine Präsentation über XSemPLR, einem Dataset für interlingual semantische Parsing. Das Dataset umfasst mehrere natürliche Sprachen und Bedeutungsrepräsentationen. Es enthält 9 Datensätze in verschiedenen Bereichen, 5 Semantischer Parsing-Praktiken, 8 Bedeutungsrepräsentationen und natürliche Sprachen in 15 Sprachfamilien. Die Präsentation zeigt auch ein Diagramm, das die Komponenten des XSemPLR-Angebots darstellt, einschließlich Encoder und Decoder, sowie verschiedene Semantische Parsing-Methoden wie SQL, Thing Talk und Prolog.</sample>
    <sample id="640">Das Dataset XSemPL umfasst 9 Datensätze in verschiedenen Bereichen, 5 Semantik-Aufgaben, 8 Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="641">Experiment Settings. Wir betrachten die sechs Einstellungen für die Ausbildung und die Bewertung. Translate-Test: Verwenden Sie den Google Translate API, um den Quelltext ins Zielsprachen zu übersetzen. Danach trainieren und bewerten Sie mithilfe eines monolingualen Modells. Training Inference</sample>
    <sample id="642">Das Experiment wird auf zwei Arten durchgeführt: Training und Inference.</sample>
    <sample id="643">Das Bild zeigt eine Präsentation über "Experiment Settings" in einer Powerpoint-Ansicht. Die Anordnung der Informationen ist so, dass sie auf dem oberen linken Eck des Bildes angezeigt wird.</sample>
    <sample id="644">Das Bild zeigt eine Präsentation über Experimente mit Sprachmodellen. Der Titel lautet "Experiment Settings" und es gibt zwei Hauptabschnitte: Training und Inference. Im Training-Section wird erwähnt, dass sie die Settings für die Ausbildung und Bewertung in Betracht ziehen. Es werden insgesamt sechs Settings betrachtet. Eines dieser Settings ist der "Monolingual Model", bei dem die QuellSprache gleich der Zielsprache ist (z.B. Deutsch-Deutsch). Sie testen auch das "Monolingual Few-shot"-Setting, bei dem monolingual Models mit nur 10% Trainingsdaten ausgebildet werden. Im Inference-Section sind zwei Diagramme zu sehen, die beide den Prozess zwischen "German" und "SQL" darstellen.</sample>
    <sample id="645">In diesem Setting ist die QuellSprache dieselbe wie die ZielSprache, zum Beispiel Deutsch zu Deutsch.</sample>
    <sample id="646">Das Bild zeigt eine Präsentation über die Experimenteinstellungen für die Ausbildung und Bewertung von Modellen. Es gibt sechs Einstellungen, darunter eine monolingual-Modell-Einstellung, bei der die QuellSprache gleich der Zielsprache ist (z.B. Deutsch-&gt;Deutsch). Diese Einstellung wird mit einer "Monolingual Few-shot"-Einstellung getestet, bei der monolingual-Modelle mit nur 10% der Trainingsdaten ausgebildet werden. Das Bild zeigt zwei Diagramme: Training und Inferenz. Beide Diagramme zeigen den Prozess, wie das Deutsche-Modell auf SQL-Data trainiert wird.</sample>
    <sample id="647">Das Bild zeigt eine Präsentation über Experimente und die Konfiguration von Modellen. Der Titel lautet "Experiment Settings" (Experimenteinstellungen). Es gibt zwei Hauptabschnitte: "Training" (Trainieren) und "Inference" (Erkennen).

Im Abschnitt "Training" sind drei Sprachen (German, English, Chinese) aufgeführt, die als Input für den Prozess dienen. Diese Daten werden dann in einem Multilingüen-Modell (Multilingual Model) trainiert, das auf der rechten Seite des Bildes dargestellt wird.

Im Abschnitt "Inference" sind wiederum zwei Sprachen (German, Multilingual Model) aufgeführt, die als Input für das Erkennen dienen. Das Ergebnis dieser Prozesse wird auf der rechten Seite des Bildes als SQL-Datenbank symbolisiert.

Der Text im Bild lautet: "Wir betrachten die sechs Einstellungen für Training und Evaluation." (Wir betrachten die sechs Einstellungen für Training und Evaluation.) und "Multilingual Model: Train one multilingual model for all languages." (Multilingüen-Modell: Trainiere ein Multilingüen-Modell für alle Sprachen.)

Das Bild ist Teil einer Präsentation, wie durch den Anzeigepunkt "10" am unteren rechten Eck angezeigt wird. Im oberen rechten Eck ist ein kleiner Bildschirm mit einem Menschen zu sehen, der möglicherweise die Präsentation leitet.</sample>
    <sample id="648">Das Bild zeigt eine Präsentation über Experimente und die Konfiguration von Multilingüisten-Modellen. Der Titel der Präsentation lautet 'Experiment Settings' (Experimenteinstellungen). Die Präsentation beschreibt, wie sechs Einstellungen für das Training und die Bewertung vorgehalten werden, wobei ein Multilingüistisches Modell trainiert wird, um für alle Sprachen zu funktionieren.

Im Text ist klar, dass das Training auf drei Sprachen abzielt: Deutsch, Englisch und Chinesisch. Diese Sprachen werden in einem Diagramm dargestellt, das den Prozess des Trainings und der Inferenz zeigt. Während das Training für jede Sprache unabhängig durchgeführt wird, wird das Multilingüistische Modell schließlich für die Daten aus allen drei Sprachen kombiniert.

Das Diagramm zeigt auch, dass das Multilingüistische Modell zum Schreiben von SQL-Anweisungen verwendet wird. Dieses suggeriert, dass das Modell möglicherweise für die Verarbeitung von Daten in einer relationalen Datenbank konzipiert wurde.

Am oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich die Präsentation führt. Der Text 'Kuan Huang' kann im Bildschirm gesehen werden, was möglicherweise den Namen des Vortragers oder der Präsentation anzeigt.

Zusammenfassend handelt es sich um eine umfassende Darstellung der Konfiguration und des Trainings eines Multilingüistischen Modells für die Verwendung in einer relationalen Datenbank, mit einem Schwerpunkt auf den drei Sprachen Deutsch, Englisch und Chinesisch.</sample>
    <sample id="649">Das Bild zeigt eine Präsentation über Experimenteinstellungen. Der Titel lautet "Experiment Settings". Es gibt zwei Hauptabschnitte: Training und Inference. Im Training-Abschnitt sind drei Sprachen aufgeführt: German, English und Chinese. Diese Sprachen werden in einem Multilingülen Modell kombiniert, das dann in SQL-Umgebungen verwendet wird. Im Inference-Abschnitt wird ein ähnliches Setup gezeigt, wobei nur der Multilingülen Modell-Name aufgeführt ist, der dann in SQL-Umgebungen verwendet wird.</sample>
    <sample id="650">Experiment Settings Wir betrachten die sechs Einstellungen für die Ausbildung und das Evaluation. Cross-lingual Zero-shot/Few-shot Transfer: Trägen Sie auf einem Quell-Sprachzugrundlage aus und übertragen Sie zu einer anderen Sprache. Training German SQL Inference German SQL</sample>
    <sample id="651">Das Bild zeigt eine Präsentation über Experiment-Einstellungen. Der Titel lautet 'Experiment Settings' und es gibt eine Beschreibung von sechs Einstellungen für die Ausbildung und Bewertung. Eine der Einstellungen, die in roter Schrift hervorgehoben ist, lautet 'Cross-lingual Zero-shot/Few-shot transfer'. Diese bedeutet, dass man auf einer Sprache ausgebildet wird und dann auf eine andere Sprache übertragen wird, ohne dass der Modelltraining auf der ZielSprache stattfindet. Im Bild sind zwei Diagramme zu sehen: eines für die Ausbildung und eines für die Inferenz. Beide Diagramme zeigen den Prozess, wie man auf Englisch trainiert und dann auf Deutsch überträgt, um ein Multilingües Modell zu erstellen, das auf SQL-Abfragen reagiert.</sample>
    <sample id="652">Analysis of Monolingual</sample>
    <sample id="653">Analysis of Monolingual</sample>
    <sample id="654">Analysis of Monolingual</sample>
    <sample id="655">Analysis von Monolingualen</sample>
    <sample id="656">Analysis von Multilingüalem Training</sample>
    <sample id="657">Analysis of Multilingual Training</sample>
    <sample id="658">Analysis of Multilingual Training</sample>
    <sample id="659">Analysis of Multilingual Training</sample>
    <sample id="660">Übertrage den englischen Inhalt ins Deutsche.</sample>
    <sample id="661">In this figure, the blue line is cross-lingual few-shot transfer. The orange line is cross-lingual zero-shot transfer while the green line is monolingual setting.</sample>
    <sample id="662">Der englische Text lautet: "Cross-lingual Performance Gap" und "green - orange in zero-shot setting, the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that for few-shot setting, the transfer gap is shortened rapidly."</sample>
    <sample id="663">Die Übersetzung des englischen Inhalts ins Deutsche lautet:</sample>
    <sample id="664">Enc-Enc (mT5) übertritt den vorherigen Arbeit oder erreicht ähnliche Ergebnisse.</sample>
    <sample id="665">Zusammenfassung: Wir bauen XSemPLR, eine einheitliche Baseline für die Mehrsprachige Semantikauswertung mit mehreren natürlichen Sprachen und repräsentativen.</sample>
    <sample id="666">Das Slide zeigt den Abschnitt 'Links' mit einem Bild eines Menschen im oberen rechten Eck. Der Text lautet: 'Willkommen, um unsere Papier und Code zu besuchen!' Darunter sind zwei Links angegeben: - Paper Link: https://arxiv.org/pdf/2306.04085.pdf - Code Link: https://github.com/psunlpgroup/xsemplr</sample>
    <sample id="667">The existing works can be broadly classified into four categories: 1. Parameter watermarking (1, 2) - Transferability (marked with an X indicating it is not applicable to EaaS). 2. Lexical watermarking (3, 4) - Applicable to EaaS. 3. Backdoor-based watermarking (5) - Applicable to EaaS. 4. Adversarial-property based watermarking (6) - Applicable to EaaS. Each category lists specific techniques or methods used in watermarking and indicates their applicability to a concept referred to as "EaaS." The classification helps categorize the different approaches within the field of watermarking according to certain criteria or characteristics.</sample>
    <sample id="668">Nein, sie sind nicht ausreichend für CLSP.</sample>
    <sample id="695">图片中的文字是“Alignment unknown.”，位于幻灯片的左下角。</sample>
    <sample id="696">The Fairness of a fine-tuned NLP model is determined by how it handles different political opinions and whether or not the hate speech targeting minority groups increases when deployed to popular social media platforms.</sample>
    <sample id="697">The presenter's name is Yanis Labrak.</sample>
    <sample id="698">Koustuv Sinha</sample>
    <sample id="699">Myra Cheng, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="700">Tropicalism refers to a cultural movement that emphasizes the exotic and vibrant aspects of Latin American culture.</sample>
    <sample id="701">The authors have created descriptions of target groups by using words that emphasize their unique characteristics, such as 'culture,' 'tradition,' and 'proud.' These descriptors help to define these groups solely based on their identity.</sample>
    <sample id="702">In this work, we introduce P-CXMI to measure context usage.</sample>
    <sample id="703">DrBERT und ChuBERT sind beide BERT-Modellvarianten, aber ChuBERT ist speziell für die Verarbeitung medizinischer Daten optimiert.</sample>
    <sample id="751">Es sind drei Autoren an der Arbeit beteiligt.</sample>
    <sample id="752">Iterative Transfer Learning is a method that involves updating the model iteratively by training it on new data collected from active learning rounds and annotations.</sample>
    <sample id="753">Hier ist der Inhalt des Textes: Indirekte Beziehungs-Expressions. Ziel: Verständnis von Benutzernsprache, wenn sie eine Wahl treffen. Alternative Frage: War es "einfach auf mich" oder "bin ich auf Gefühle bedingt"? Direkte Beziehung: "einfach auf mich", "der erste". Indirekte Beziehung könnte in natürlichen und flüssigen Gesprächen verwendet werden: - Man kann den Namen nicht mehrerkennbar. - Die Anordnungen sind schwierig zu unterscheiden. - Man möchte eine Vorliebe angeben. Indirekte Beziehung: "Der neue Song", "die Lied, das nicht energiegelike".</sample>
    <sample id="754">The legend of the figures means the number of triggers in each sentence.</sample>
    <sample id="755">Es gibt drei Autoren, die an der Arbeit beteiligt sind.</sample>
    <sample id="756">2</sample>
    <sample id="757">Die Autoren gehören zur Universität Washington und Carnegie Mellon University.</sample>
    <sample id="758">The governor is on the left.</sample>
    <sample id="759">The technology for dialogue systems is highly advanced, capable of understanding and responding to complex human interactions with a high degree of accuracy.</sample>
    <sample id="760">Es ist wichtig, die Akzeptanz der Modelle über das gesamte Kontextfenster zu bewerten, um sicherzustellen, dass sie korrekt und relevant sind, wenn sie in verschiedenen Kontexten eingesetzt werden.</sample>
    <sample id="761">The slide is titled "Analysis of Multilingual Training" and it discusses the evaluation of mT5 and XLM-R + PTR on a multilingual setting. The main point highlighted in the text states that most major natural languages (NLs) can obtain performance gains, except for English, which experiences drops in 7 datasets but gains in only 3 datasets. This phenomenon is referred to as the "Curse of Multilinguality."</sample>
    <sample id="762">Nein, die Annotatoren wissen nicht die Entität im Voraus.</sample>
    <sample id="763">The MT-Metrics used for evaluation are BLEU, METEOR, and ROUGE. These metrics assess the quality of machine translation by comparing translated text to human-translated references or other translations as a baseline.</sample>
    <sample id="764">Ja, die Regression beeinflusst die Generalisierung auf bestimmte NER-Typen.</sample>
    <sample id="765">Positionalität für NLP ist wichtig, weil es bei der Analyse von Texten und der Erstellung von Kontextbeziehungen zu einem besserem Verständnis der Bedeutung von Wortstelle und -ordnung führt.</sample>
    <sample id="766">Mehrsprachige LLMs wie BLOOM wurden durch Adapter angepasst.</sample>
    <sample id="767">The model used for the cold-start annotations is called 'RoBERTA-base + classifier head'.</sample>
    <sample id="768">The following test sets were used: WMT2021, WMT2022, and WMT2023.</sample>
    <sample id="769">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="770">Der Gewinn der vorgeschlagenen Methode gegenüber der stärksten Baseline beträgt 20,6%.</sample>
    <sample id="771">Shuheng Liu, Alan Ritter</sample>
    <sample id="772">Yes, the results and dataset of this study can be used as a benchmark.</sample>
    <sample id="773">Es werden vier kleine Modelle experimentiert.</sample>
    <sample id="774">OFA (One For All)</sample>
    <sample id="833">Die Autoren gehören der Universität Stanford.</sample>
    <sample id="834">Die Autoren gehören der Stony Brook University.</sample>
    <sample id="835">Deutsch, Englisch und Französisch wurden untersucht.</sample>
    <sample id="836">Chan Young Park</sample>
    <sample id="837">Das Bild zeigt eine Präsentation über "Automatische Textsimplifizierung". Es gibt eine Überschrift, die den Titel der Präsentation anzeigt. Darunter sind zwei Abschnitte: "Dokumentenlevel" und "Satzlevel", die jeweils Ergebnisse von Experimenten darstellen. Jeder Abschnitt enthält eine Tabelle mit verschiedenen Metriken und Testdaten. Im Hintergrund ist ein Mann zu sehen, der wahrscheinlich über das Thema sprechen soll.</sample>
    <sample id="838">62</sample>
    <sample id="839">Es gibt drei Autoren, die an der Arbeit beteiligt sind: Regina Stodden, Omar Momen und Laura Kallmeyer.</sample>
    <sample id="840">AG News, MIND, SST2, Enron Spam</sample>
    <sample id="876">NACHOS ist ein Dataset, das aus medizinischen Quellen auf der Internetseite sammelt.</sample>
    <sample id="877">David Vilar Torres, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, George Foster.</sample>
    <sample id="878">Prompting can have a big influence on the performance of LLMs for translation.</sample>
    <sample id="879">Die Autoren gehören an Carnegie Mellon University, Técnico Lisboa und Berkeley Artificial Intelligence Research (BAIR).</sample>
    <sample id="880">The five instructions are: 1. We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon 2. This is a QR code for our data and model repository, so please scan it to access the information.</sample>
    <sample id="881">Die Autoren schlagen vor, Modelle zur Nutzung von Informationen aus mehreren Quellen zu testen, indem sie ein Dataset für die Bewertung der Wissensintegration und eine Korreferenzaufgabe verwenden, um die Fähigkeit zu testen, auf Wissen aus verschiedenen Quellen zu verweisen.</sample>
    <sample id="882">Es tut mir leid, ich kann den englischen Inhalt nicht ins Deutsche übersetzen.</sample>
    <sample id="883">图片中的文字内容如下： 1. PaLM: Pathways Language Model Chowdery et al., 2022 arXiv:2204.02311. 540B parameters. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips. SOTA in hundreds of LMU and Generation benchmarks. 图片中还包含一个树形图，展示了不同任务的参数数量： - QUESTION ANSWERING: 10 billion parameters - ARITHMETIC: 37 billion parameters - TRANSLATION: 62 billion parameters - LANGUAGE UNDERSTANDING: 540 billion parameters</sample>
    <sample id="884">Das Bild zeigt eine Präsentation über den PaLM-Modell, einem Sprachmodell. Es gibt mehrere Informationen und eine Illustration eines Baumes mit verschiedenen Lernfähigkeiten. Die Informationen im Bild sind in Englisch.</sample>
    <sample id="885">Das Bild zeigt eine Präsentation über die Beiträge des Forschungsprojekts. Der Titel der Präsentation lautet "Our contribution". Die Inhalte der Präsentation sind in drei Punkten aufgeführt: 1. Systematische erste Studie über LLM-Prompting für MT (Machine Translation). - Diese Studie bezieht sich sowohl auf die Bewerbungsschleife als auch auf die Auswahlstrategie. 2. Bewertung der Übersetzungsfähigkeiten gemäß den besten Praktiken der MT-Community (Machine Translation Community). - Sie umfasst die neuesten Testensätze (vermeidet man das Testrail und Überschneiden der Bewertungsdaten), eine Vergleichung mit den neuesten WMT-Untermittlern (SOTA-Systemen verwenden die meisten recenten Training-Daten), SOTA-METRIKKE (bessere Korrelation mit menschlichen Bewertungen) und eine Expertenbasierte menschliche Bewertung (mehr robust als die Arbeitskräfte). 3. Empfehlungen für Prompt-Selection-Strategien. Im unteren rechten Ecke des Bildes ist ein kleiner Bild von einem Mann zu sehen, der wahrscheinlich der Vortragender ist.</sample>
    <sample id="886">First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies</sample>
    <sample id="887">Unser Beitrag</sample>
    <sample id="888">Der englische Inhalt lautet: 'First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies.'</sample>
    <sample id="889">Prompts have a big impact on translation quality. Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points</sample>
    <sample id="890">Prompts haben einen großen Einfluss auf die Übersetzungsfähigkeit.</sample>
    <sample id="891">Prompts haben einen großen Einfluss auf die Übersetzungsfunktion.</sample>
    <sample id="892">Das Bild zeigt eine Präsentation über "Example prompting for translation". Es gibt einen Textabschnitt, der "5-shot prompting" titelt und verschiedene englische und deutsche Übersetzungen darstellt. Darunter sind Sätze wie "Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird." und "Ski-Legenden unter sich: Die Polizei war eingeschlossen, nachdem sie Beschwerden des Buros erhalten hatten." Ein kleiner Bildschirm mit dem Google Logo ist im unteren linken Eck sichtbar. Im unteren rechten Eck ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich über die Präsentation spricht.</sample>
    <sample id="893">Ja, das Bild zeigt eine Präsentation über Übersetzung von deutscher auf Englisch. Es gibt einige englische und deutsche Sätze, die übersetzt werden sollen.</sample>
    <sample id="894">Der Text auf der Seite lautet "Example prompting for translation" und gibt einen Einblick in die Übersetzung von englischen Sätzen ins Deutsche.</sample>
    <sample id="895">Das Bild zeigt eine Präsentation über "Example prompting for translation". Es gibt drei Szenen, die jeweils den englischen Text und seine Übersetzungen in deutscher Sprache zeigen. Der englische Text lautet: 'It's crucial for zero and one-shot prompting, and when we go as in our case to five-shot prompting there is neatly no difference to the actual form of the promptings.' Diese Informationen sind für die Übersetzung von Texten von Englisch ins Deutsche relevant.</sample>
    <sample id="896">Ja, das Bild zeigt eine Präsentation über Übersetzungshilfe. Es gibt verschiedene Szenarien, die aufgerichtet sind, um die Übersetzung von englischen Texten ins Deutsche zu demonstrieren.</sample>
    <sample id="897">Das Ergebnis der experimentellen Studie ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zu der Quellsprache.</sample>
    <sample id="898">The text in the image is about experimental results. It mentions that example quality is more important than similarity to source sentences and discusses specialized SOTA systems having a significant advantage, with PaLM being close to Google Translate. The insights from MQM are also mentioned, highlighting that the fluency of PaLM is comparable to SOTA, but accuracy scores generally lower, dominated by 'Accuracy/Omission,' and style/awkwardness generally lower for PaLM.</sample>
    <sample id="899">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" * Style/Awkwad* generally lower for PaLM.</sample>
    <sample id="900">Das Bild zeigt eine Präsentation mit dem Titel "Experimental Results". Es gibt mehrere Punkte, darunter: - Beispielqualität ist wichtiger als Ähnlichkeit zur Quellsprache. - Spezialisierte SOTA-Systeme haben einen bedeutenden Vorteil. - PaLM ist in der Nähe von Google Translate. - Einzelnachweise von MQM: - Fluensität von PaLM kann mit SOTA konkurrieren. - Genauheitswerte sind im Allgemeinen niedriger. - Genauigkeit/Übersehen dominiert die Genauheitswerte. - Stil/Schwierigkeitsgrad ist im Allgemeinen niedriger für PaLM. Im unteren rechten Eck des Bildes ist ein kleiner Bild eines Menschen zu sehen, und es befindet sich auch ein Google Logo in der unteren linken Ecke.</sample>
    <sample id="901">Das Hauptresultat der Experimente ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zu der Quellen-Sprache.</sample>
    <sample id="902">Beispielergebnisse</sample>
    <sample id="903">Das Bild zeigt eine Präsentation mit einem Titel 'Experimental Results'. Es gibt mehrere Punkte, die Informationen über die Ergebnisse einer Studie bereitstellen. Zunächst steht: 'Beispielqualität ist wichtiger als Ähnlichkeit zur Quell-Sprache.' Danach folgt: 'Spezialisierte SOTA-Systeme haben einen bedeutenden Vorteil.' Und dann: 'PaLM ist nahe bei Google Translate.' Darunter sind weitere Details: 'Insights von MQM:' gefolgt von 'Fluensität von PaLM ähnlich zu SOTA.' und 'Genauheitswerte sind im Allgemeinen niedriger.' Diese Punkte deuten auf eine Analyse der Leistung von Sprachübersetzungssystemen hin.</sample>
    <sample id="904">Das Bild zeigt eine Präsentation mit einem Titel 'Experimental Results'. Es gibt mehrere Punkte, darunter: - Beispielqualität ist wichtiger als Ähnlichkeit zur Quellsprache. - Spezialisierte SOTA-Systeme haben einen bedeutenden Vorteil. - PaLM ist fast wie Google Translate. - Einzelnachweise von MQM: - Flussigkeit von PaLM kann mit SOTA verglichen werden. - Genauheitswerte sind im Allgemeinen niedriger. - Genauigkeit und Verlust sind im Allgemeinen niedriger für PaLM. - Stil/Unbehagen sind im Allgemeinen niedriger für PaLM. Im unteren rechten Eck des Bildes ist ein Foto eines Menschen zu sehen.</sample>
    <sample id="905">Experimental Results: - Beispielqualität ist wichtiger als Ähnlichkeit zu der Quellsprache. - Spezialisierte SOTA-Systeme haben einen bedeutenden Vorteil. - PaLM ist nahe bei Google Translate. Einzelnachrichten von MQM: - Fluide Ausgabe von PaLM kann mit SOTA konfrontiert werden. - Genauheitswerte sind im Allgemeinen niedriger. Dominierend durch "Genauigkeit/Verlust". - "Stil/Absurdität" ist im Allgemeinen niedriger für PaLM.</sample>
    <sample id="906">Der englische Inhalt lautet: 'And that's it for this really short overview. For more details, please come to my full presentation of the paper.'</sample>
    <sample id="907">Sure, here's the German translation of the text: "Schwächer als du denkst: Eine kritische Betrachtung zu schwachsupervisiver Lernung."</sample>
    <sample id="908">Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach und Dietrich Klakow.</sample>
    <sample id="909">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data. Weak labeling sources: heuristics, knowledge bases Unlabeled data Weakly labeled data = Data with wrong annotations</sample>
    <sample id="910">Das Bild zeigt eine Präsentation mit dem Titel 'Why weakly supervised learning?'. Es gibt drei Hauptpunkte: 1. 'Weak supervision alleviates the annotation bottleneck.' - Dieser Punkt zeigt, dass schwache Überwachung die Annotierungsbehinderung lindert. 2. 'But weak labels are noisy!' - Diese Punkt betont, dass schwache Labels rauschen. 3. 'Weakly supervised learning (WSL)' - Dieser Punkt erklärt, dass WSL Modelle trainsieren können, die sich gut auf rauschigen Daten ausrichten. Darüber hinaus sind auf der rechten Seite des Bildes zwei Grafiken zu sehen, die den Prozess von 'Weak labeling sources' und 'Unlabeled data' zu 'Weakly labeled data' und 'Annotations are wrong' illustrieren.</sample>
    <sample id="911">Warum fehlervolles überwachungsgelernt werden kann?</sample>
    <sample id="912">Warum schwache Überschauungsverfahren? Schwache Überwachung lindert die Annotierungsblockade. Aber schwache Labels sind laut Noise memorization schadet der Generalisierung. Schwache Überschauungsverfahren (WSL) Trainiere Modelle, die sich gut auf noisy Daten ausbilden und trotzdem gut generalisieren.</sample>
    <sample id="913">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="914">A common claim in recent works is that people say they only train models on weakly supervised data and achieve high performance.</sample>
    <sample id="915">Ein häufiger Anspruch in den letzten WSL-Werken lautet: "Wir trainieren Modelle nur auf schwach überwacht Daten und erreichen eine Genauigkeit von XX%."</sample>
    <sample id="916">Eine häufige Behauptung in aktuellen WSL-Arbeiten ist: 'Wir trainieren Modelle nur auf schwach überwachte Daten und erreichen eine Genauigkeit von XX%.'</sample>
    <sample id="917">Das Bild zeigt eine Präsentationsslideset, das sich auf ein häufig vorkommendes Problem in der WSL (Weakly Supervised Learning) konzentriert. Der Titel des Slides lautet 'Ein häufiger Fall in der WSL'. Im Text steht: 'Wir trainieren Modelle nur auf schwach überwachte Daten und erreichen eine Genauigkeit von XX%' mit einem Gesichtsausdruck, der einen Zweifel oder Unzufriedenheit nahelegt. Links im Bild sind zwei Diagramme zu sehen: ein Diagramm mit 'Schwach überwacht Training-Daten (geräuschig)' und ein Diagramm mit 'Reinigten Validationsdaten' und 'Reinigten Testdaten'. Rechts am Bildrand ist ein kleiner Bild eines Elefanten zu sehen, was möglicherweise eine Metapher für das 'Elefant im Zimmer' darstellt, was auf eine unangenehme Wahrheit hinweist, die oft ignoriert wird.</sample>
    <sample id="918">Das Bild zeigt eine Slide mit dem Titel "Our research questions" auf Deutsch. Es gibt drei Research Questions (RQ) aufgeführt: 1. Ist die Validierungsdatenreinheit notwendig? 2. Wie viele Reinheitsamplieen benötigen WSL-Anproach? 3. Wie können wir die verfügbaren Reinheitsamplieen effizienter nutzen? Im oberen rechten Eck des Bildes ist ein kleiner Bild eines Menschen zu sehen, der Brille trägt und ein graues Shirt mit einem schwarzen Kragen hat. Die Slide hat einen weißen Hintergrund und die Texte sind in schwarz gedruckt.</sample>
    <sample id="919">Das Bild zeigt eine Slide mit dem Titel 'Unsere Forschungsfragen' (Unsere Forschungsfragen). Es gibt drei Research Questions (RQ) auf der Slide: 1. RQ1: Ist die Validierung von sauberen Daten notwendig? 2. RQ2: Wie viele saubere Beispiele benötigen WSL-Anproptehe? 3. RQ3: Wie können wir die verfügbaren sauberen Beispiele effizienter nutzen? Im oberen rechten Eck des Bildes ist ein kleiner Bild eines Menschen zu sehen, der in einem blauen Hemd und einer grauen Krawatte gekleidet ist. Der Text auf der Slide ist auf Englisch, und es gibt keine sichtbaren Texte in anderen Sprachen.</sample>
    <sample id="920">Die Hauptfunde sind folgend:</sample>
    <sample id="921">Zuerst finden wir, dass interessant</sample>
    <sample id="922">RQ1 Main findings</sample>
    <sample id="923">Das Hauptergebnis ist, dass die Ausbildung für den Fortschritt ohne Validierung von Nutzen hat.</sample>
    <sample id="924">Das Hauptergebnis ist, dass eine saubere Validationsstunde unerlässlich ist.</sample>
    <sample id="925">Unser zweiter Fund ist, dass die Anzahl der sauberen Validations-Samples es den WSL-Anwendungen hilft,</sample>
    <sample id="926">Die Hauptfindungen</sample>
    <sample id="927">The main findings are presented in a slide with two graphs. The left graph shows accuracy on the y-axis and validation percentage on the x-axis, comparing different models such as FTw, COSINE, LZR, BOND, MLC, and Weak labels. The right graph displays performance delta (Δ) for various methods like FTc, LoRaC, BitFitC, AdapterC, etc., also plotted against validation percentages. A note at the bottom states that WSL approaches benefit from more clean validation samples.</sample>
    <sample id="928">Der rechte Graph zeigt die Leistungsunterschiede zwischen den Fine-Tuning-Anwendungen, die direkt auf sauberen Daten angewendet wurden, und den WSL-Anwendungen, die nur auf sauberen Validationsdaten angewendet wurden.</sample>
    <sample id="929">R2O Main findings WSL approaches benefit from more clean validation samples But it is even better to use them for training (e.g., LoRA!)</sample>
    <sample id="930">RQ3 Main findings N=10 clean samples per class Before After N=30 clean samples per class Before After</sample>
    <sample id="931">Von dem Bild zu inferieren, scheint es sich um eine Präsentation zu handeln, die sich auf die Hauptfindungen eines Forschungsprojekts oder einer Studie konzentriert.</sample>
    <sample id="932">RQ3 Hauptfunde</sample>
    <sample id="933">Das Hauptresultat der Studie zeigt, dass das kontinuierliche Feinabpolieren (CFT) die Leistungsschwitze zwischen den verschiedenen WSL-Anroachen schlägt.</sample>
    <sample id="934">Zusammenfassend haben wir gezeigt, dass die kürzlichsten Ansätze der WSL</sample>
    <sample id="935">Die deutsche Übersetzung des englischen Inhalts lautet: Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="936">The slide contains a conclusion section with two main parts: "Recent WSL approaches" and "Our recommendations." The first part lists the requirements for recent WSL (Weak Supervised Learning) approaches, which include needing clean samples. However, it also mentions that these approaches often overestimate their practicality, as indicated by an emoji of a sad face next to this point.

The second part provides recommendations for improving or using WSL approaches more effectively. It suggests reporting model selection criteria, using few-shot learning approaches as baselines, and always applying continuous fine-tuning (CFT).

In summary, the slide emphasizes the need for careful consideration when implementing WSL methods due to potential overestimation issues but offers specific guidelines to enhance their reliability and effectiveness.</sample>
    <sample id="937">Das englische Text ist in deutscher Übersetzung zu sehen: 'Zusammenfassung Recent WSL-Methoden Anforderungen an saubere Beispiele.</sample>
    <sample id="938">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="939">The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="940">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="941">In the example, Servin is a judge and Kea is a baker. They met at a park after deciding cases in court all day long.</sample>
    <sample id="942">Ja, der Code ist verfügbar auf GitHub unter dem Link "mpoems1/kitmus".</sample>
    <sample id="943">The speaker is discussing the alignment of datasets and models with people who have a college education.</sample>
    <sample id="944">Es werden Sätze innerhalb der akzeptablen Domain durch den Einführung von "noise" verändert, was zu einer Veränderung ihrer Bewertung im Kontext führt.</sample>
    <sample id="945">Dimensionale Bewertung bedeutet, dass man mehrere Aspekte eines Themas bewertet.</sample>
    <sample id="946">The authors belong to the University of Science and Technology of China, Microsoft Research Asia, Beijing Jiaotong University, Sony AI, and Microsoft STC Asia.</sample>
    <sample id="947">Es ist wichtig, die Form des Prompts zu beachten, wenn es sich um mehrere kurze Prompte handelt.</sample>
    <sample id="978">The slide titled 'ABC-Eval Error Rates by Model' shows a bar chart with various error rates for different models.</sample>
    <sample id="979">Es sind 9 Autoren an der Arbeit beteiligt.</sample>
    <sample id="980">A good planner should read scripts that are reasonable and feasible to constraints.</sample>
    <sample id="981">There are 7 authors.</sample>
    <sample id="982">Vasudha Varadarajan</sample>
    <sample id="983">The authors belong to the Polish Academy of Sciences and University of Warsaw.</sample>
    <sample id="1021">The most common errors of PaLM are "Accuracy/Omission" and "Style/Awkwad".</sample>
    <sample id="1022">Das Bild zeigt eine Präsentationsseite mit dem Titel "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems". Die Autoren sind Sarah E. Finch, James D. Finch und Jinho D. Choi. Das Logo von Emory University, Emory NLP Research Lab und Alexa ist auch sichtbar. Im oberen rechten Eck befindet sich ein kleiner Bildschirm eines Mannes, der wahrscheinlich über Skype oder einer ähnlichen Videokonferenzplattform teilnimmt.</sample>
    <sample id="1023">Dieser Vortrag wurde von der Emory NLP Lab durchgeführt, die unter der Leitung von Professor Gino Choi an der Emory University betrieben wird. Die Zusammenarbeit für diesen Vortrag wurde mit Amazon Alexa AI durchgeführt.</sample>
    <sample id="1024">Das Bild zeigt eine Präsentation, die von Emory University und Alexa aufgenommen wurde. Der Titel der Präsentation lautet "Comparative Evaluation". Im Hintergrund sind zwei Grafiken zu sehen, die eine Person mit einem Haarstil und zwei Sprachblättern zeigen, sowie zwei Roboter-Charaktere, die jeweils mit einem Sprachblatt verbunden sind. Die Grafiken sind in zwei Gruppen geteilt, wobei die auf der linken Seite blau sind und die auf der rechten Seite rot sind. Die Emory University-Logo und das Alexa-Logo sind im unteren rechten Eck des Bildes sichtbar.</sample>
    <sample id="1025">Das Bild zeigt eine Präsentation mit dem Titel 'Comparative Evaluation' (Vergleichender Bewertung). Es gibt zwei Gruppen von Sprachblasen, eine in Blau und eine in Lila. Jede Gruppe hat einen Menschenkopf und einen Roboter-Kopf darunter. Im Zentrum des Bildes ist ein Zeichnung eines Richters mit einem Gabeln. Am unteren Rand befindet sich ein Likert-Skala von 1 bis 5. Links unten ist das Logo der Emory University zu sehen.</sample>
    <sample id="1026">Das Bild zeigt eine Präsentation über 'Dialogqualität'. Es gibt einen Titel, der 'Dimensions of Dialogue Quality' lautet. Im Zentrum befindet sich ein Text, der 'Dialogue Quality' zeigt. Darunter sind drei Boxen, die 'Relevance', 'Consistency' und 'Emotional Understanding' bezeichnen. Diese Boxen sind mit Pfeilen verbunden, die auf den zentralen Text zeigen. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen. Im unteren linken Eck befindet sich das Emory University Logo.</sample>
    <sample id="1027">The image shows a presentation slide titled "Likert Rating Evaluation" from Emory University. The slide features an illustration of two characters with speech bubbles and icons representing ratings or evaluations, along with a scale ranging from 1 to 5 marked by dots. A green checkmark indicates the selection of option 3 on this scale. At the bottom left corner, there is an illustration of a figure holding scales, symbolizing judgment or evaluation. In the top right corner, there is a small video feed showing a person who appears to be presenting the content. Additionally, the Alexa logo is visible in the bottom right corner.

The text at the bottom center of the slide reads: "Rate the relevance of the bot's responses." This suggests that the Likert rating system is being used to evaluate how relevant the responses provided by a chatbot are perceived to be.</sample>
    <sample id="1028">Das Bild zeigt eine Präsentation über die Bewertung von Likert-Skala. Im oberen linken Eck befindet sich ein blauer Banner mit dem Text "Likert Rating Evaluation". Im unteren linken Eck ist ein Logo der Emory University zu sehen, und rechts daneben ist das Alexa-Logo präsentiert. Im unteren Zentrum des Bildes befindet sich ein Text, der lautet: "Rate the relevance of the bot's responses". Darüber hinaus sind vier Blaue Bläschen auf der rechten Seite des Bildes zu sehen, die wahrscheinlich symbolisieren, dass es sich um eine Diskussion oder eine Bewertung handelt.</sample>
    <sample id="1029">Der englische Text lautet: 'Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself.'</sample>
    <sample id="1030">Der englische Text lautet: "We call this approach annotating behaviors in chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality and recent literature."</sample>
    <sample id="1031">ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors.</sample>
    <sample id="1032">ABC-Eval bewertet die Konsistenz, die Kenntnis und die Emotionale Verständnis</sample>
    <sample id="1033">图片显示了一个名为“ABC-Eval Behaviors”的幻灯片。它被分为四个部分，每个部分都有一个标题和一些项目符号。这些部分是：1. Coherence（连贯性） - 包括两个项目符号：“Ignoring Partner”（忽略伴侣）和“Irrelevant”（无关）。2. Knowledge（知识） - 包括两个项目符号：“Incorrect Fact”（错误事实）和“Commonsense Violation”（违反常识）。3. Consistency（一致性） - 包括两个项目符号：“Self Contradiction”（自我矛盾）和“Partner Contradiction”（伴侣矛盾）。4. Emotional Understanding（情感理解） - 包括两个项目符号：“Empathetic Response”（同理心反应）和“Lack of Empathy”（缺乏同情心）。幻灯片的底部有Emory University的标志和Alexa的标志。</sample>
    <sample id="1034">Das Bild zeigt eine Präsentation, die sich auf 'Experimente' konzentriert. Es gibt vier Punkte, die Informationen über verschiedene Aspekte der Experimente liefern: - 4 Open-Domain Dialogue Models - 100 Human-Bot Conversations per Model - Knowledge, Consistency, Emotional Understanding und ABC-Eval.</sample>
    <sample id="1035">Das Bild zeigt eine Präsentation, die sich auf Experimente in der Dialogmodellierung konzentriert. Der Titel der Präsentation lautet "Experimente". Es gibt drei Hauptpunkte: 1. 4 Offenraum-Dialogmodellierungen 2. 100 Menschen-Maschinengespräche pro Modell 3. ABC-Evaluation, Turn Likert und Dialogue Likert, sowie eine Comparative-Analyse. Im unteren linken Eck des Bildes ist das Emory University Logo zu sehen, und im unteren rechten Eck befindet sich das Alexa-Logo.</sample>
    <sample id="1036">The video begins with a slide titled 'Experiments' from Emory University, featuring the Alexa logo. The content lists "4 Open-Domain Dialogue Models" and "100 Human-Bot Conversations per Model." It then transitions to another slide under the heading 'Baseline Evaluations,' which introduces three evaluation methods: Turn Likert, Dialogue Likert, and Comparative. Each method is illustrated with diagrams showing interactions between humans and bots, marked by checkmarks indicating successful evaluations.

The narrative continues as more aspects of dialogue are introduced in pink boxes below each evaluation method. These include Consistency, Emotional Understanding, Informativeness, Overall Quality for Turn Likert; Engagingness, Grammaticality, Proactivity, Relevance for Dialogue Likert; and all these attributes again for Comparative. This segment underscores the comprehensive nature of the baseline evaluations conducted on chat models across multiple dimensions.

The final part of the video maintains this detailed breakdown of evaluation criteria without introducing new information or changes in visuals. The consistent display emphasizes the thorough approach taken in assessing chat models through various standard metrics used in evaluating conversational systems.</sample>
    <sample id="1037">Der englische Text lautet: 'From our analyses of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods. As measured by inter-annotator agreement on 100 doubly labeled conversations.'</sample>
    <sample id="1038">Der englische Text lautet: "In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis."</sample>
    <sample id="1039">Der Graph zeigt die Proportion der Qualitätsgestützten (R²) für verschiedene Kategorien, wie ABC-Eval, Turn Likert, Dialogue Likert und Comparative. Jede Kategorie hat zwei Stangen: eine für Interactive Qua (schwarz) und eine für Interactive Qua (grau). Die Stangen zeigen die relativen Werte der Proportion der Qualitätsgestützten für jede Kategorie. Der Titel des Graphs lautet "Predictive Validity".</sample>
    <sample id="1040">Der englische Inhalt lautet: "Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression."</sample>
    <sample id="1041">Das Bild zeigt eine Präsentation mit einem Diagramm, das Incremental Validität zeigt. Das Diagramm hat einen Titel "ABC-eval Turn UBERT Dialogue Likert". Die Achse der Y-Achse ist "% von Qualität erklärt (PQ)", und die Achse der X-Achse ist "ABC-eval Turn UBERT Dialogue Likert". Die Punkte auf dem Diagramm sind mit verschiedenen Begriffen wie "Unempathetic", "Relevant", "Proactive" und "Engaging" beschriftet. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich die Präsentation gibt. Im unteren linken Eck befindet sich die Emory University-Logos und im unteren rechten Eck ist das Alexa-Logo.</sample>
    <sample id="1042">Übertrage den englischen Inhalt in die deutsche Sprache.</sample>
    <sample id="1043">ABC-Eval Error Rates by Model</sample>
    <sample id="1044">In the image, there is a bar chart titled 'ABC-Eval Error Rates by Model'. The x-axis lists different challenges such as 'Asocial', 'CS Contra', 'Ignore', etc., while the y-axis shows the percentage of turns. There are multiple bars for each challenge, representing different models: BART-FID-RAG, Blender2, Emora, and Blender-Decode. A yellow arrow points to the 'CS Contra' category on the left side of the chart. In the top right corner, there is an inset showing a person in a video call interface with the text 'alexa' below it. At the bottom left corner, logos from Emory University and Microsoft Research are visible.</sample>
    <sample id="1045">The English content in the image is about evaluating error rates by model.</sample>
    <sample id="1046">ABC-Eval Error Rates by Model The chart shows the error rates of different models across various categories. Categories include 'Antisocial,' 'CS Contra,' 'Ignore,' 'Incorrect,' 'Unempathetic,' 'Other Contra,' 'Redundant,' 'Self Contra,' and 'Topic Switch.' Each category has a corresponding bar graph with multiple bars representing different models: BART-FID-RAG, Blender2, Emora, and Blender-Decode. The y-axis represents the percentage of turns (in %), ranging from 0 to 30%.</sample>
    <sample id="1047">Das Bild zeigt eine Präsentation, die mit einem 'Thanks For Watching!'-Text und mehreren Informationen wie einer Paper-URL, GitHub-URL und Kontaktinformationen endet.</sample>
    <sample id="1048">Die Autoren gehören der Emory University.</sample>
    <sample id="1049">CFT stands for 'Continuous Fine-tuning.'</sample>
    <sample id="1050">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="1051">Dieser Vortrag wurde von Patrick Fernandes, Kayo Yin, Emmy Liu, André F. T. Martins und Graham Neubig gemeinsam mit ihren Kollegen von Carnegie Mellon University, Technico Lisboa, Berkeley Artificial Intelligence Research und Unbabel erstellt.</sample>
    <sample id="1052">Die Übersetzung hängt von dem Kontext ab. Wir müssen uns von diesem Schmutzloch befreien.</sample>
    <sample id="1053">Das Wort 'mole' bedeutet in diesem Kontext ein Spuren oder eine Flecken.</sample>
    <sample id="1054">Die Bedeutung des Wortes ändert sich je nach Kontext und daher ändert sich auch seine Übersetzung.</sample>
    <sample id="1055">Das Bild zeigt eine Präsentationsslides. Der Titel lautet 'Evaluating context-dependent translation is hard'. Links unten befindet sich ein Punkt, der lautet: 'Only a small portion of words depend on context'. Rechts unten befindet sich ein weiterer Punkt, der lautet: 'Corpus-level metrics'. Im oberen rechten Eck des Slides ist ein kleiner Bild von einer Person zu sehen.</sample>
    <sample id="1056">Das Bild zeigt eine Präsentation mit dem Titel 'Evaluating context-dependent translation is hard'. Es gibt drei Punkte auf der Präsentation: 1. 'Only a small portion of words depend on context' - 'Corpus-level metrics' 2. 'Existing methods support limited discourse phenomena and languages' 3. Eine Grafik eines Silhouettenbildes eines Menschen mit einem Stift und Flaggen des Vereinigten Königreichs und Frankreich daneben.</sample>
    <sample id="1057">In diesem Werk haben wir versucht, diese beiden Fragen zu beantworten. Zunächst, wann eine Übersetzung Kontext benötigt und zweitens, wie gut Modelle diese Fälle behandeln können.</sample>
    <sample id="1058">Die erste Frage lautet: "Wann benötigt eine Übersetzung Kontext?" Die Antwort lautet: "Wortstufe-Kontextnutzung".</sample>
    <sample id="1059">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="1060">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="1061">Pointwise (P-)CXMI We introduce P-CXMI to measure context usage to translate a specific sentence. P-CXMI(y, x, C) = -log (qMTA(y|x)/qMTC(y|x, C)) Sentence level or at the word level? We can think of words that have high P-CXMI as ones that require context for translation.</sample>
    <sample id="1062">RQ1: Wann erfordert die Übersetzung Kontext? - Wortstufe-Kontextnutzung - Thematischer Analyse RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen?</sample>
    <sample id="1063">Thematic analysis of high P-CXMI words English ägyptisch Deutsch Español Français עברית Italiano 日本語 한국어 Nederlands Portugês Română Русский Türkçe 中文</sample>
    <sample id="1064">Thematic analysis of high P-CXML words 1. POS tags</sample>
    <sample id="1065">Themaatisches Analyse von hochp-CXMI-Wörtern 1. POS-Tags P-CXMI für POS-Tags in arabisch PRON.3.Sing PRON.3.Dual PRON.3.Plur. - Pronouns</sample>
    <sample id="1066">Thematic analysis of high P-CXMI words</sample>
    <sample id="1067">Thematic analysis of high P-CXML words 1. POS tags 2. Vocabulary items Avelile's mother was still asleep. Avelile went to school.</sample>
    <sample id="1068">Der englische Inhalt lautet: "Thematic analysis of high P-CXMI words" 1. POS tags 2. Vocabulary items Avelile's mother was still asleep. Avelile went to school.</sample>
    <sample id="1069">Thema: Thematischer Analyse von hochp-CXML-Wörtern</sample>
    <sample id="1070">RQ1: Wann benötigt eine Übersetzung Kontext? - Wortstufe-Kontextnutzung - Thematischer Analyse RQ2: Wie gut handhaben Modelle Kontextabhängige Übersetzungen? - Multilinguistisches Diskursbewusstes (MuDA)-Benchmarks</sample>
    <sample id="1071">Multilingual Discourse-Aware (MuDA) tagger - Pronouns, Verb form, Lexical cohesion, Formality, Ellipsis</sample>
    <sample id="1072">Wir können dann auch beachten, dass verschiedene Sprachen unterschiedliche Proportionen dieser Diskursfaktoren haben.</sample>
    <sample id="1073">Das Bild zeigt eine Grafik, die den Prozess der Verwendung des MuDA-Taggers für die Bewertung von Übersetzungen darstellt. Links sind mehrere Dokumente zu sehen, und rechts steht ein Roboter. Im Zentrum befindet sich ein Diagramm, das den Prozess zeigt, wie der MuDA-Tagger verwendet wird, um Übersetzungen zu bewerten. Der Text 'MuDA benchmark' ist oben links platziert.</sample>
    <sample id="1074">RQ1: Wann benötigt eine Übersetzung Kontext? - Wortstufe-Kontextnutzung - Thematischer Analyse RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen? - Multilinguistisches Diskursbewusstes (MuDA)-Benchmarks - Modell-Evaluation</sample>
    <sample id="1075">Zunächst einmal verwenden wir Korpus-Level-Metriken.</sample>
    <sample id="1076">Corpus-level metrics BLEU, COMET und F-measure</sample>
    <sample id="1077">Dies zeigt, dass es schwierig ist, den besten Dokumentübersetzungs-System zu bestimmen, wenn wir nur mit Korpus-Level-Metriken arbeiten.</sample>
    <sample id="1078">Der englische Text lautet: "Context-aware models perform significantly better on some phenomena."</sample>
    <sample id="1079">Context-aware models perform significantly better on some phenomena. Formality, lexical cohesion Ellipsis, pronouns, verb form</sample>
    <sample id="1080">MUDA benchmark results Context-aware models perform significantly better on some phenomena ✅: Formality, lexical cohesion ❌: Ellipsis, pronouns, verb form DeepL outperforms Google on most phenomena and language pairs* as of April 2021</sample>
    <sample id="1081">Summarizing the content of the image, we have: 1. MuDA benchmark results - Context-aware models perform significantly better on some phenomena. Examples include formality, lexical cohesion (✔), ellipsis (✘), and verb form. DeepL outperforms Google on most phenomena and language pairs. 2. Summary - Identify discourse phenomena systematically without prior linguistic knowledge. Dataset-agnostic benchmark for document-level MT.</sample>
    <sample id="1082">Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT</sample>
    <sample id="1083">Danke für deine Aufmerksamkeit. Ich sehe dich in Toronto.</sample>
    <sample id="1084">Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang</sample>
    <sample id="1121">Jump-Permutation</sample>
    <sample id="1122">Die Autoren beschreiben die Methode der 'markierten Wörter' als eine Methode zur Identifizierung von Wörtern, die die Persönlichkeiten der 'markierten Gruppen' von denen der 'unmarkierten Gruppen' unterscheiden.</sample>
    <sample id="1123">Die Autoren gehören an der University of Washington, Carnegie Mellon University und dem Paul G. Allen School.</sample>
    <sample id="1124">Der Name der zuerst erwähnten symmetrischen Abhängigkeitsstruktur lautet Bouquet/Stanford.</sample>
    <sample id="1125">Sarah E. Finch, James D. Finch, and Jinho D. Choi</sample>
    <sample id="1126">Es sind fünf Autoren an der Arbeit beteiligt.</sample>
    <sample id="1127">The table shows examples of sentences that can be used to test syntactic phenomena. These include: 1. "Many people were helping themselves." (BLIMP) 2. "No customer ... has spent any money." (SyntaxGym) 3. "Stereotypical Noun Phrase sentence." (Crow's)</sample>
    <sample id="1161">The abbreviations for the five methods are FTw, BOND, COSINE, MLC, and L2R.</sample>
    <sample id="1162">Das Modell wird auf 11 Aufgaben überprüft.</sample>
    <sample id="1226">CamemBERT wurde ursprünglich mit 4GB von PubMed-Daten trainiert.</sample>
    <sample id="1227">Adam Przepiórkowski und Michał Woźniak</sample>
    <sample id="1228">Es gibt keine Hinweise auf den englischen Inhalt im Bild.</sample>
    <sample id="1269">To answer the question, it is necessary to permute the tokens for the emission sequence because they are not in the correct order.</sample>
    <sample id="1270">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, um die Verständigung und den Zugang zu diesen Methoden zu fördern.</sample>
    <sample id="1271">Minimal Pair Paradigm</sample>
    <sample id="1272">The authors used the following evaluation metrics: NER (Named Entity Recognition), CER (Character Error Rate), F1, and MAP.</sample>
    <sample id="1273">Krippendorf's alpha.</sample>
    <sample id="1274">The domain chosen is Wikipedia.</sample>
    <sample id="1275">Die Autoren gehören der Heinrich Heine University Düsseldorf, Deutschland.</sample>
    <sample id="1276">MultiInstruct focuses on instruction tuning for multimodal pre-trained models, addressing the imbalance in available datasets between NLP and multimodal tasks.</sample>
    <sample id="1277">Es gibt drei Autoren.</sample>
    <sample id="1278">Binary coordination is a type of chemical bonding where two atoms share one pair of electrons.</sample>
    <sample id="1279">Die Prompts waren durchschnittlich 15 Wörter lang.</sample>
    <sample id="1280">Die Ergebnisse deuten darauf hin, dass das kleinere T5-Modell auf Coscript trainiert ist, besser als die meisten großen LLMs im Bereich der Script-Qualität hervorgeht.</sample>
    <sample id="1281">DrBERT: Ein robustes vorher trainiertes Modell auf Französisch für medizinische und klinische Anwendungen.</sample>
    <sample id="1282">Das Bild zeigt eine Präsentation, die von Avignon Universität präsentiert wird. Die Slide-Überschrift lautet "Zusammenfassung". Folgende Punkte sind auf der Slide aufgeführt: 1. Sprachmodellierung in der Gesundheitsversorgung 2. Vergleich von Vorbereitungsstrategien, Datenquellen und Größen 3. Bewertung von 13 Modellen auf 11 Aufgaben 4. Verteilung von NACHOS und DrBERT Die Slide hat einen roten Rand am unteren Ende und zeigt ein kleines Bild eines Menschen im oberen rechten Eck.</sample>
    <sample id="1283">图片中的文字内容包括以下几点：

1. **标题**：Summary
2. **内容列表**：
   - I. Language Modeling in Healthcare
   - II. Comparison of pre-training strategies, data sources and sizes
   - III. Evaluation of 13 models on 11 tasks
   - IV. Distribution of NACHOS and DrBERT

3. **底部信息**：Avignon Université

这些文字主要介绍了关于语言建模在医疗保健领域的总结，包括预训练策略、数据源和大小的比较，以及对13个模型在11个任务上的评估。此外，还提到了NACHOS和DrBERT的分布情况。</sample>
    <sample id="1284">Summary I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT</sample>
    <sample id="1285">Das Bild zeigt eine Zusammenfassung eines Vortrags oder einer Präsentation. Der Titel der Präsentation lautet "Summary". Die Inhalte der Präsentation sind aufgezählt: 1. Language Modeling in Healthcare 2. Comparison of pre-training strategies, data sources and sizes 3. Evaluation of 13 models on 11 tasks 4. Distribution of NACHOS and DrBERT Avignon Universität ist unten angegeben. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen, der wahrscheinlich den Vortrag gibt.</sample>
    <sample id="1286">Transformer-based approaches, such as BERT, offer huge performance gains on a lot of NLP tasks.</sample>
    <sample id="1287">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. It has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain-specific models in English raised the bar even higher. PubmedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domain in French yet. BERT-based domain specific model for French should increase performance on medical tasks.</sample>
    <sample id="1288">Der englische Text lautet: 'Specialized models for other languages are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domain in French yet.'</sample>
    <sample id="1289">Der englische Inhalt lautet: "Transformer-based approaches, such as BERT, offer huge performance gains on a lot of NLP tasks. Has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain-specific models in English raised the bar even higher. • PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domain in French yet. BERT-based domain specific model for French should increase performance on medical tasks."</sample>
    <sample id="1290">图片中的文字内容包括标题、项目符号和表格。标题是“Comparison of pre-training strategies and data sources”。项目符号包括： - Evaluation of the impact of public and private medical data sources on comparable data sizes - Comparison of learning strategies 表格列出了不同的数据集，包括它们的名称、大小、元数据数量和资源。</sample>
    <sample id="1291">Das Bild zeigt eine Präsentation über die Vergleichung von Vorbereitungstechniken und Datenquellen für Sprachmodelle. Der Titel der Präsentation lautet 'Comparison of pre-training strategies and data sources'. Die Präsentation umfasst zwei Hauptabschnitte:</sample>
    <sample id="1292">Der englische Inhalt lautet: 'Comparison of pre-training strategies and data sources'.</sample>
    <sample id="1293">Das Bild zeigt eine Präsentation über die Vergleichung von Vorbereitungsstrategien und Datenquellen. Die Titelseite lautet "Vergleich von Vorbereitungsstrategien und Datenquellen". Es gibt zwei Hauptabschnitte: 1. Bewertung des Einflusses öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datensätze. 2. Vergleich von Vorbereitungstechniken. In diesem Abschnitt werden verschiedene Vorbereitungstechniken diskutiert, wie zum Beispiel das Training von Modellen von scratch und das Continual Pre-training mit vorhandenen Modellen. Darüber hinaus sind Details zu zwei spezifischen Datenquellen bereitgestellt: NACHOS und NWDW. NACHOS ist ein Open-Set-Dataset, das aus einer Vielzahl von Medizinmedien, Styles und Natures besteht, während NWDW ein Dataset von Sätzen ist, die aus 1.7 Millionen anonymisierten Medizin-Meldungen stammen. Der Text ist auf Englisch, aber es gibt auch einen kleinen Abschnitt im unteren rechten Eck, der auf Französisch schreibt, was darauf hinweist, dass der Inhalt möglicherweise für eine multilingualen Zielgruppe konzipiert wurde.</sample>
    <sample id="1294">图片中的文字内容包括： 1. 标题：Comparison of pre-training strategies and data sources 2. 副标题和正文内容： - Evaluation of the impact of public and private medical data sources on comparable size datasets - NACHOS: A 1.1B words open-source dataset of heterogeneous data crawled from various medical domains, natures and styles - NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse - Comparison of learning strategies - From scratch with full model construction - Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model, and PubMedBERT, an English medical one) 3. 表格内容： - Corpus | Size | References - NACHOS (sub Pubmed) | 4 GB | 625 M - NBDW (sub Pubmed) | 4 GB | 625 M - NBDW (private) | 4 GB | 625 M 4. 图片右下角的文字： Avignon Université</sample>
    <sample id="1295">Das Bild zeigt eine Präsentation über die Vergleichung von Vorbereitungsstrategien und Datenquellen. Der Titel der Präsentation lautet "Vergleich von Vorbereitungsstrategien und Datenquellen".</sample>
    <sample id="1296">Das Bild zeigt eine Präsentation über die Vergleichung von Vorbereitungsmethoden und Datenquellen. Die Titelseite lautet "Vergleich von Vorbereitungsmethoden und Datenquellen". Der Text auf der Seite diskutiert die Bewertung des Einflusses öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datengrößen. Zwei Hauptquellen werden erwähnt: NACHOS, ein 1,18 MB großer Open-Set-Datensatz, der aus einem 4 GB-Offset-Satz von NACHOS und einem 25 MB-Offset-Satz von NACHOS zusammengestellt wurde, und NBDW, ein Privatsatz von Sätzen, der aus 1,7 Mio. anonymisierten Medizin-Meldungen aus dem Hospital Data Warehouse der Université de Nantes stammt. Darüber hinaus wird auf die Vergleichung von Vorbereitungsmethoden eingegangen, einschließlich der Erstellung eines Vollmodells von scratch und der Verwendung eines vorvorhabenen Modells (z.B. Camembert) für die Vorbereitung.</sample>
    <sample id="1297">图片中的文字内容是关于预训练策略和数据源的比较。它提到了两个主要的数据集：NACHOS 和 NBDW，并详细描述了它们的大小和资源。此外，它还讨论了从头开始构建模型与使用现有预训练模型（如 CamemBERT、French generic model 和 PubMedBERT）进行持续预训练的策略。图片中还列出了这些模型在不同任务上的表现。</sample>
    <sample id="1298">Die Übersetzung des englischen Inhalts ins Deutsche lautet:</sample>
    <sample id="1299">Das Bild zeigt eine Präsentation über die Datenquellen und Größe. Es gibt eine Tabelle mit verschiedenen Modellen und ihre Leistungen auf verschiedenen Aufgaben. Die Tabelle enthält Spalten für verschiedene Modelle wie 'aHIF', 'Medical Report', 'MUSICA', 'Diet', 'MUSC', 'Diet-MUSC', 'CAS', 'FastText', 'MOCAMBO' und 'QUAERO-EMBRO-MEDLINE'. Jede Spalte zeigt die Leistung jedes Modells auf verschiedenen Aufgaben, wie 'NER', 'CL', 'NER', 'CL', 'NER', 'CL', 'POS', 'NER', 'CL' und 'NER'. Am unteren Rand des Bildes ist das Logo von Avignon Universität zu sehen.</sample>
    <sample id="1300">Die Übersetzung des englischen Inhalts ins Deutsche lautet: 'Evaluierung: Datenquellen und Größe'</sample>
    <sample id="1301">Das Bild zeigt eine Präsentation mit einem Titel 'Evaluation: Data sources and size'. Es handelt sich um eine Analyse der Leistung 13 Modells auf 10 verschiedenen Aufgaben, sowohl öffentlich als auch privat. Die darin dargestellten Daten zeigen die Ergebnisse in Form einer Tabelle, die verschiedene Metriken wie NER (Named Entity Recognition), CER (Content Error Rate) und andere anzeigt. Im unteren rechten Eck des Bildes ist das Logo der Avignon Universität zu sehen.</sample>
    <sample id="1302">Der englische Inhalt lautet: "Evaluation: Pre-training strategies" und "From scratch vs. continual pre-training on 4GB of data".</sample>
    <sample id="1303">图片中的文字是“Avignon Universite”。</sample>
    <sample id="1304">Der englische Text lautet: "Which is not the case for the model based on CAMBÉR weights and tokenizer which suffer from stability issues."</sample>
    <sample id="1305">画面中的文字内容包括：

标题：Core message

正文：
- DRBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks
- Surpasses Camembert generic model and English-based domain-specific models
- Confirms utility of training a medical-specific model in French

副标题：Data sources matters: training on heterogeneous data is important

正文：
- NACHOS is more robust than using private clinical data only
- More data is better, but does not scale well
- Continual pretraining is a more effective strategy when based on domain-specific English models
- The DRBERT models, the NACHOS dataset, and the training scripts are freely available under the MIT license

底部信息：
- dr.bert.univ-avignon.fr
- Avignon Université
- 一个二维码

这些文字提供了关于DRBERT模型在法语医疗任务中的表现、数据来源的重要性以及相关资源的可用性等信息。</sample>
    <sample id="1306">Der englische Text lautet: 'We also observed that special as the data is better, more special as the data is better, but it doesn't scale well.'</sample>
    <sample id="1307">Der englische Text lautet: 'The DRBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license.'</sample>
    <sample id="1308">Das Bild zeigt eine Präsentation mit einem Cartoon-Klown, der ein Nadelnadelchen hält und einen Rettungsschürze trägt. Der Clown ist auf der linken Seite des Bildes platziert und hat einen roten Kreuz auf seiner Schürze. Im Zentrum des Bildes befindet sich ein Sprechblatt mit den Worten 'Danke' darauf. Darunter steht die Aufschrift 'Erhalte freudvolle Austausch auf dem Poster-Sitzung in Toronto!' Die untere rechte Ecke des Bildes zeigt eine Webseite an: 'More information on: drbert.univ-avignon.fr'. Im unteren Rand des Bildes ist das Logo der Universität Avignon zu sehen.</sample>
    <sample id="1309">比较的预训练策略包括从头开始构建模型、使用现有预训练模型进行持续预训练，以及使用PubMedMed，一个医疗英语版本。</sample>
    <sample id="1310">图片中的文字内容包括标题“是什么导致了性能下降？”以及两个子标题：“适应性过拟合？”和“时间漂移？”。此外，还有一个图表显示了“CoNLL 2003 F1分数”与“CoNLL++ F1分数”的关系。图表中包含多个数据点和一条红色的拟合线，标注为“最佳拟合线”。图表下方有注释，提到“没有递减回报”和“未观察到”。</sample>
    <sample id="1311">The video shows a detailed presentation on the topic of 'Automatic Text Simplification.' It includes results from experiments conducted with fine-tuned language models, specifically focusing on document and sentence level simplifications. The presenter discusses various metrics used to evaluate the quality of these simplifications.

Throughout the presentation, there is an individual in the top right corner who appears to be explaining or presenting the content. This person remains mostly stationary but occasionally makes slight movements such as adjusting their position slightly within each frame.

The main focus of the slides presented are tables that compare different baseline methods (e.g., DEPLAIN-APA test) against other models like DeBERTa-APA and SARI-BERT. These comparisons include scores for BLEU and F1 metrics across multiple datasets including DEPLAIN-APA test, DEPLAIN-APA test 2, DEPLAIN-APA test 3, DEPLAIN-APA test 4, DEPLAIN-APA test 5, DEPLAIN-APA test 6, DEPLAIN-APA test 7, DEPLAIN-APA test 8, DEPLAIN-APA test 9, DEPLAIN-APA test 10, DEPLAIN-APA test 11, DEPLAIN-APA test 12, DEPLAIN-APA test 13, DEPLAIN-APA test 14, DEPLAIN-APA test 15, DEPLAIN-APA test 16, DEPLAIN-APA test 17, DEPLAIN-APA test 18, DEPLAIN-APA test 19, DEPLAIN-APA test 20, DEPLAIN-APA test 21, DEPLAIN-APA test 22, DEPLAIN-APA test 23, DEPLAIN-APA test 24, DEPLAIN-APA test 25, DEPLAIN-APA test 26, DEPLAIN-APA test 27, DEPLAIN-APA test 28, DEPLAIN-APA test 29, DEPLAIN-APA test 30, DEPLAIN-APA test 31, DEPLAIN-APA test 32, DEPLAIN-APA test 33, DEPLAIN-APA test 34, DEPLAIN-APA test 35, DEPLAIN-APA test 36, DEPLAIN-APA test 37, DEPLAIN-APA test 38, DEPLAIN-APA test 39, DEPLAIN-APA test 40, DEPLAIN-APA test 41, DEPLAIN-APA test 42, DEPLAIN-APA test 43, DEPLAIN-APA test 44, DEPLAIN-APA test 45, DEPLAIN-APA test 46, DEPLAIN-APA test 47, DEPLAIN-APA test 48, DEPLAIN-APA test 49, DEPLAIN-APA test 50, DEPLAIN-APA test 51, DEPLAIN-APA test 52, DEPLAIN-APA test 53, DEPLAIN-APA test 54, DEPLAIN-APA test 55, DEPLAIN-APA test 56, DEPLAIN-APA test 57, DEPLAIN-APA test 58, DEPLAIN-APA test 59, DEPLAIN-APA test 60, DEPLAIN-APA test 61, DEPLAIN-APA test 62, DEPLAIN-APA test 63, DEPLAIN-APA test 64, DEPLAIN-APA test 65, DEPLAIN-APA test 66, DEPLAIN-APA test 67, DEPLAIN-APA test 68, DEPLAIN-APA test 69, DEPLAIN-APA test 70, DEPLAIN-APA test 71, DEPLAIN-APA test 72, DEPLAIN-APA test 73, DEPLAIN-APA test 74, DEPLAIN-APA test 75, DEPLAIN-APA test 76, DEPLAIN-APA test 77, DEPLAIN-APA test 78, DEPLAIN-APA test 79, DEPLAIN-APA test 80, DEPLAIN-APA test 81, DEPLAIN-APA test 82, DEPLAIN-APA test 83, DEPLAIN-APA test 84, DEPLAIN-APA test 85, DEPLAIN-APA test 86, DEPLAIN-APA test 87, DEPLAIN-APA test 88, DEPLAIN-APA test 89, DEPLAIN-APA test 90, DEPLAIN-APA test 91, DEPLAIN-APA test 92, DEPLAIN-APA test 93, DEPLAIN-APA test 94, DEPLAIN-APA test 95, DEPLAIN-APA test 96, DEPLAIN-APA test 97, DEPLAIN-APA test 98, DEPLAIN-APA test 99, DEPLAIN-APA test 100, DEPLAIN-APA test 101, DEPLAIN-APA test 102, DEPLAIN-APA test 103, DEPLAIN-APA test 104, DEPLAIN-APA test 105, DEPLAIN-APA test 106, DEPLAIN-APA test 107, DEPLAIN-APA test 108, DEPLAIN-APA test 109, DEPLAIN-APA test 110, DEPLAIN-APA test 111, DEPLAIN-APA test 112, DEPLAIN-APA test 113, DEPLAIN-APA test 114, DEPLAIN-APA test 115, DEPLAIN-APA test 116, DEPLAIN-APA test 117, DEPLAIN-APA test 118, DEPLAIN-APA test 119, DEPLAIN-APA test 120, DEPLAIN-APA test 121, DEPLAIN-APA test 122, DEPLAIN-APA test 123, DEPLAIN-APA test 124, DEPLAIN-APA test 125, DEPLAIN-APA test 126, DEPLAIN-APA test 127, DEPLAIN-APA test 128, DEPLAIN-APA test 129, DEPLAIN-APA test 130, DEPLAIN-APA test 131, DEPLAIN-APA test 132, DEPLAIN-APA test 133, DEPLAIN-APA test 134, DEPLAIN-APA test 135, DEPLAIN-APA test 136, DEPLAIN-APA test 137, DEPLAIN-APA test 138, DEPLAIN-APA test 139, DEPLAIN-APA test 140, DEPLAIN-APA test 141, DEPLAIN-APA test 142, DEPLAIN-APA test 143, DEPLAIN-APA test 144, DEPLAIN-APA test 145, DEPLAIN-APA test 146, DEPLAIN-APA test 147, DEPLAIN-APA test 148, DEPLAIN-APA test 149, DEPLAIN-APA test 150, DEPLAIN-APA test 151, DEPLAIN-APA test 152, DEPLAIN-APA test 153, DEPLAIN-APA test 154, DEPLAIN-APA test 155, DEPLAIN-APA test 156, DEPLAIN-APA test 157, DEPLAIN-APA test 158, DEPLAIN-APA test 159, DEPLAIN-APA test 160, DEPLAIN-APA test 161, DEPLAIN-APA test 162, DEPLAIN-APA test 163, DEPLAIN-APA test 164, DEPLAIN-APA test 165, DEPLAIN-APA test 166, DEPLAIN-APA test 167, DEPLAIN-APA test 168, DEPLAIN-APA test 169, DEPLAIN-APA test 170, DEPLAIN-APA test 171, DEPLAIN-APA test 172, DEPLAIN-APA test 173, DEPLAIN-APA test 174, DEPLAIN-APA test 175, DEPLAIN-APA test 176, DEPLAIN-APA test 177, DEPLAIN-APA test 178, DEPLAIN-APA test 179, DEPLAIN-APA test 180, DEPLAIN-APA test 181, DEPLAIN-APA test 182, DEPLAIN-APA test 183, DEPLAIN-APA test 184, DEPLAIN-APA test 185, DEPLAIN-APA test 186, DEPLAIN-APA test 187, DEPLAIN-APA test 188, DEPLAIN-APA test 189, DEPLAIN-APA test 190, DEPLAIN-APA test 191, DEPLAIN-APA test 192, DEPLAIN-APA test 193, DEPLAIN-APA test 194, DEPLAIN-APA test 195, DEPLAIN-APA test 196, DEPLAIN-APA test 197, DEPLAIN-APA test 198, DEPLAIN-APA test 199, DEPLAIN-APA test 200, DEPLAIN-APA test 201, DEPLAIN-APA test 202, DEPLAIN-APA test 203, DEPLAIN-APA test 204, DEPLAIN-APA test 205, DEPLAIN-APA test 206, DEPLAIN-APA test 207, DEPLAIN-APA test 208, DEPLAIN-APA test 209, DEPLAIN-APA test 210, DEPLAIN-APA test 211, DEPLAIN-APA test 212, DEPLAIN-APA test 213, DEPLAIN-APA test 214, DEPLAIN-APA test 215, DEPLAIN-APA test 216, DEPLAIN-APA test 217, DEPLAIN-APA test 218, DEPLAIN-APA test 219, DEPLAIN-APA test 220, DEPLAIN-APA test 221, DEPLAIN-APA test 222, DEPLAIN-APA test 223, DEPLAIN-APA test 224, DEPLAIN-APA test 225, DEPLAIN-APA test 226, DEPLAIN-APA test 227, DEPLAIN-APA test 228, DEPLAIN-APA test 229, DEPLAIN-APA test 230, DEPLAIN-APA test 231, DEPLAIN-APA test 232, DEPLAIN-APA test 233, DEPLAIN-APA test 234, DEPLAIN-APA test 235, DEPLAIN-APA test 236, DEPLAIN-APA test 237, DEPLAIN-APA test 238, DEPLAIN-APA test 239, DEPLAIN-APA test 240, DEPLAIN-APA test 241, DEPLAIN-APA test 242, DEPLAIN-APA test 243, DEPLAIN-APA test 244, DEPLAIN-APA test 245, DEPLAIN-APA test 246, DEPLAIN-APA test 247, DEPLAIN-APA test 248, DEPLAIN-APA test 249, DEPLAIN-APA test 250, DEPLAIN-APA test 251, DEPLAIN-APA test 252, DEPLAIN-APA test 253, DEPLAIN-APA test 254, DEPLAIN-APA test 255, DEPLAIN-APA test 256, DEPLAIN-APA test 257, DEPLAIN-APA test 258, DEPLAIN-APA test 259, DEPLAIN-APA test 260, DEPLAIN-APA test 261, DEPLAIN-APA test 262, DEPLAIN-APA test 263, DEPLAIN-APA test 264, DEPLAIN-APA test 265, DEPLAIN-APA test 266, DEPLAIN-APA test 267, DEPLAIN-APA test 268, DEPLAIN-APA test 269, DEPLAIN-APA test 270, DEPLAIN-APA test 271, DEPLAIN-APA test 272, DEPLAIN-APA test 273, DEPLAIN-APA test 274, DEPLAIN-APA test 275, DEPLAIN-APA test 276, DEPLAIN-APA test 277, DEPLAIN-APA test 278, DEPLAIN-APA test 279, DEPLAIN-APA test 280, DEPLAIN-APA test 281, DEPLAIN-APA test 282, DEPLAIN-APA test 283, DEPLAIN-APA test 284, DEPLAIN-APA test 285, DEPLAIN-APA test 286, DEPLAIN-APA test 287, DEPLAIN-APA test 288, DEPLAIN-APA test 289, DEPLAIN-APA test 290, DEPLAIN-APA test 291, DEPLAIN-APA test 292, DEPLAIN-APA test 293, DEPLAIN-APA test 294, DEPLAIN-APA test 295, DEPLAIN-APA test 296, DEPLAIN-APA test 297, DEPLAIN-APA test 298, DEPLAIN-APA test 299, DEPLAIN-APA test 300, DEPLAIN-APA test 301, DEPLAIN-APA test 302, DEPLAIN-APA test 303, DEPLAIN-APA test 304, DEPLAIN-APA test 305, DEPLAIN-APA test 306, DEPLAIN-APA test 307, DEPLAIN-APA test 308, DEPLAIN-APA test 309, DEPLAIN-APA test 310, DEPLAIN-APA test 311, DEPLAIN-APA test 312, DEPLAIN-APA test 313, DEPLAIN-APA test 314, DEPLAIN-APA test 315, DEPLAIN-APA test 316, DEPLAIN-APA test 317, DEPLAIN-APA test 318, DEPLAIN-APA test 319, DEPLAIN-APA test 320, DEPLAIN-APA test 321, DEPLAIN-APA test 322, DEPLAIN-APA test 323, DEPLAIN-APA test 324, DEPLAIN-APA test 325, DEPLAIN-APA test 326, DEPLAIN-APA test 327, DEPLAIN-APA test 328, DEPLAIN-APA test 329, DEPLAIN-APA test 330, DEPLAIN-APA test 331, DEPLAIN-APA test 332, DEPLAIN-APA test 333, DEPLAIN-APA test 334, DEPLAIN-APA test 335, DEPLAIN-APA test 336, DEPLAIN-APA test 337, DEPLAIN-APA test 338, DEPLAIN-APA test 339, DEPLAIN-APA test 340, DEPLAIN-APA test 341, DEPLAIN-APA test 342, DEPLAIN-APA test 343, DEPLAIN-APA test 344, DEPLAIN-APA test 345, DEPLAIN-APA test 346, DEPLAIN-APA test 347, DEPLAIN-APA test 348, DEPLAIN-APA test 349, DEPLAIN-APA test 350, DEPLAIN-APA test 351, DEPLAIN-APA test 352, DEPLAIN-APA test 353, DEPLAIN-APA test 354, DEPLAIN-APA test 355, DEPLAIN-APA test 356, DEPLAIN-APA test 357, DEPLAIN-APA test 358, DEPLAIN-APA test 359, DEPLAIN-APA test 360, DEPLAIN-APA test 361, DEPLAIN-APA test 362, DEPLAIN-APA test 363, DEPLAIN-APA test 364, DEPLAIN-APA test 365, DEPLAIN-APA test 366, DEPLAIN-APA test 367, DEPLAIN-APA test 368, DEPLAIN-APA test 369, DEPLAIN-APA test 370, DEPLAIN-APA test 371, DEPLAIN-APA test 372, DEPLAIN-APA test 373, DEPLAIN-APA test 374, DEPLAIN-APA test 375, DEPLAIN-APA test 376, DEPLAIN-APA test 377, DEPLAIN-APA test 378, DEPLAIN-APA test 379, DEPLAIN-APA test 380, DEPLAIN-APA test 381, DEPLAIN-APA test 382, DEPLAIN-APA test 383, DEPLAIN-APA test 384, DEPLAIN-APA test 385, DEPLAIN-APA test 386, DEPLAIN-APA test 387, DEPLAIN-APA test 388, DEPLAIN-APA test 389, DEPLAIN-APA test 390, DEPLAIN-APA test 391, DEPLAIN-APA test 392, DEPLAIN-APA test 393, DEPLAIN-APA test 394, DEPLAIN-APA test 395, DEPLAIN-APA test 396, DEPLAIN-APA test 397, DEPLAIN-APA test 398, DEPLAIN-APA test 399, DEPLAIN-APA test 400, DEPLAIN-APA test 401, DEPLAIN-APA test 402, DEPLAIN-APA test 403, DEPLAIN-APA test 404, DEPLAIN-APA test 405, DEPLAIN-APA test</sample>
    <sample id="1312">The image shows a chart with the title 'Existing LMs'. The chart is divided into four quadrants, each representing different political leanings: Left, Right, Authoritarian, and Libertarian. Various language models (LMs) are plotted on this chart, including BERT-base, RoBERTa-base, distilRoBERTa, ALBERT-base, BART-base, Alpaca, Codex, LLAMA, GPT-2, GPT-3-ada, GPT-3-babbage, GPT-3-curie, ChatGPT, and GPT-4. Each model has lines connecting it to other points or clusters within the quadrants.

In addition to the main chart, there is another smaller section titled 'Pretraining Data' which compares News Media and Social Media (Reddit). It indicates that both sources have pretraining data from left-leaning news media and right-leaning social media platforms. This comparison suggests an evaluation of how these datasets might influence the political leaning of the language models trained using them.</sample>
    <sample id="1313">The title of the paper is 'Compositional Generalization without Trees using Multiset Tagging and Latent Permutations'.</sample>
    <sample id="1314">Dies ist gemeinsame Arbeit mit meinen Fachleuten Alexander Koller und Ivan Titov.</sample>
    <sample id="1315">Compositional Generalization</sample>
    <sample id="1316">Die Übersetzung des englischen Inhalts ins Deutsche lautet:</sample>
    <sample id="1317">Der englische Inhalt lautet: "The girl slept." und "Mary knew that the girl slept." Diese werden in der Tabelle mit den logischen Formen "*girl x, sleep.agent x," und "*girl x, know.agent x, Mary A know.ccomp x, X, sleep.agent x, x," zusammengefasst.</sample>
    <sample id="1318">In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms.</sample>
    <sample id="1319">In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion.</sample>
    <sample id="1320">Das Bild zeigt eine Präsentation über 'Kompositionelle Allgemeinheit in Semantischer Parseng'. Es gibt zwei Abschnitte: 'Train' und 'Test', wobei jeder Abschnitt zwei Sätze enthält. Jeder Satz wird in zwei Teile unterteilt, wobei die rechte Seite eine semantische Parseng mit den Elementen 'Agent', 'Action', 'Object' und 'Complement' enthält. Am Ende des Bildes ist der Text 'Naive seq2seq models fail!' zu sehen.</sample>
    <sample id="1321">Das Bild zeigt eine Präsentation über "Kompositionelle Allgemeinbildung in Semantischer Grammatik". Es gibt zwei Hauptabschnitte: "Train" und "Test", jeder mit einem Beispiel. Beispiele zeigen, wie die Phrase "The girl slept" und "Mary knew that the girl slept" in einem Schemat aufgezählt sind. Im Abschnitt "Test" ist ein weiteres Beispiel mit der Phrase "Jim said that Mary knew that the girl slept" aufgeführt. Das Bild endet mit der Aussage "Naive seq2seq models fail!" in roter Schrift.</sample>
    <sample id="1322">Das Bild zeigt einen Satz auf Englisch, der "Trees help a lot but..." lautet. Darunter sind zwei Texte zu sehen: "*girl x1" und "sleep.agent x2". Diese Texte sind in orangefarbenen und grünen Farben markiert. Darüber hinaus ist ein Baumdiagramm dargestellt, das "The girl slept." zeigt. Das Baumdiagramm hat zwei Knoten, wobei "sleep.agent x2" in einem blauen Textfeld und "The girl slept." in einem grünen Textfeld dargestellt sind. Der Baumdiagramm zeigt eine Verbindung zwischen den beiden Knoten, was darauf hindeutet, dass sie miteinander verbunden sind.</sample>
    <sample id="1323">Trees help a lot but... *girl x1; sleep.agent x2 x1 *girl x4, x1 sleep.agent x2 The girl slept.</sample>
    <sample id="1324">Trees help a lot but... Trees need to be obtained: - Pre/Post-processing logical forms</sample>
    <sample id="1325">The content of the image is as follows: The title at the top reads 'Trees help a lot but...' in yellow text. Below this, there are two lines of code-like text with asterisks and variables like '*girl x1' and 'sleep.agent x2'. In between these lines, there's a tree diagram showing relationships between 'girl x1', 'sleep.agent x2', and the sentence 'The girl slept.' The bottom part of the image contains some additional text that says 'Trees need to be obtained: - Pre/Post-processing logical forms'.</sample>
    <sample id="1326">Trees help a lot but... Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction</sample>
    <sample id="1327">Ja, das Bild zeigt einen Text in Englisch. Der Haupttext lautet: "Trees help a lot but..." und gibt zu verstehen, dass Bäume sehr nützlich sind, aber es gibt einige Dinge, die sie nicht tun können. Darunter steht: "Trees need to be obtained:" - Pre-/Post-processing logical forms - Grammar-induction</sample>
    <sample id="1328">Trees help a lot but... Trees need to be obtained: - Pre-/post-processing logical forms - Grammar-induction This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="1329">Unsere Ansatz präsentiert die Ausgabe aus dem Eingabe in zwei Schritten.</sample>
    <sample id="1330">Zuerst kennzeichnen wir jeden Eingabewort mit einer unordentlichen Menge von Wörtern, die im Output auftreten sollen.</sample>
    <sample id="1331">Nach dem ersten Schritt haben wir alle Tokens, aber sie sind nicht ordentlich angeordnet.</sample>
    <sample id="1332">Der englische Text lautet: 'That's why in the second step, we use another model to predict a permutation to put them into the right order.'</sample>
    <sample id="1333">Das Bild zeigt eine Diagramm, das eine Methode zur Permutation und Taggen von Wortfolgen darstellt. Der Titel des Diagramms lautet "Our Approach" (Unser Ansatz). Das Diagramm besteht aus einer Reihe von Blöcken, die Wörter wie "girl", "sleep", "agent" und "the" enthalten. Diese Blöcke sind in verschiedenen Farben markiert (grün, gelb und orange) und sind miteinander verbunden durch Pfeile. Im Zentrum des Diagramms befindet sich ein Bereich mit dem Text "Permute" (Permutieren), der auf die Veränderung der Wortfolge hinweist. Darunter befindet sich ein Bereich mit dem Text "Tag" (Taggen), der auf die Bezeichnung der Wörter hinweist. Links am Diagramm ist der Text "We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations." zu sehen, was auf die Beschreibung der Methode hinweist. Rechts am Diagramm steht der Text "This makes our approach quite flexible and expressive." und gibt einen Hinweis auf die Flexibilität und Ausdrucksfähigkeit der Methode.</sample>
    <sample id="1334">Das Bild zeigt eine Diagramm, das den Prozess einer Permutation mit Sprüngen darstellt. Der Titel "Permuting with 'jumps'" ist in Gelb und Blau geschrieben. Die Diagramm besteht aus zwei Schichten: "Permute" und "Tag". In der "Permute"-Schicht sind verschiedene Elemente wie "x1", "girl", "x1", "sleep", "agent" und "x2" dargestellt. Im "Tag"-Schicht sind die Wörter "the", "girl" und "slept" zu sehen. Links am Diagramm befindet sich ein grüner Block mit einem roten Rand und einem roten Kreis, der auf eine spezielle Position oder Funktion hinweist.</sample>
    <sample id="1335">Das Bild zeigt eine Diagrammstruktur, die den Prozess der 'Permutation mit Sprüngen' (Permute with "jumps") darstellt. Das Diagramm besteht aus zwei Schichten: der oberen Schicht ist für 'Permutieren' (Permute) und die untere Schicht für 'Bezeichnen' (Tag). Die obere Schicht zeigt eine Folge von Elementen, die 'x1', 'girl', 'x1', 'sleep', 'agent' und 'x2' sind, wobei 'girl' und 'sleep' in gelben Boxen hervorgehoben sind. Die untere Schicht zeigt die Bezeichnungen 'the', 'girl' und 'slept'. Links am Diagramm ist ein einzelnes grünes Quadrat mit einem roten Rahmen umfasst, was auf eine spezielle Position oder Element hinweist. Der Text 'Permutieren mit Sprüngen' ist oben im Bild zu sehen, was den Prozess beschreibt, der durchgeführt wird.</sample>
    <sample id="1336">Der englische Text lautet: "Then we jump to the next multiset token to determine the second token in the output."</sample>
    <sample id="1337">图片中的文字内容包括： Permute with "jumps" Tag the girl slept *girl x1 sleep agent x2</sample>
    <sample id="1338">Permute mit "Sprünge"</sample>
    <sample id="1339">Das Bild zeigt eine Grafik, die die Ergebnisse einer Studie über COGS (Kim und Linzen 2020) präsentiert. Die Grafik zeigt die Korrelation zwischen der Genauigkeit (ACC) und verschiedenen Generationsarten (PP recursion, CP recursion, Obj PP -&gt; Subj PP). Es werden verschiedene Modelle verglichen: Model, LSTM seq2seq, TS, Zheng and Lapata, sowie das "Unsere" Modell. Der Titel lautet "Some Results on COGS (Kim and Linzen 2020)" und die Legende lautet "Comparison with other Treeless Models on Structural Generalization on COGS".</sample>
    <sample id="1340">Der englische Text lautet: "Some other kinds of structural generalization remain very challenging though."</sample>
    <sample id="1341">Das Bild zeigt eine Präsentationsslideset mit dem Titel "Technical Challenges We Solve". Das Slide enthält eine Diagramm, das zwei Schichten darstellt. Die obere Schicht ist mit den Worten "Permute" und drei Flecken markiert, die "?" anzeigen. Die untere Schicht ist mit den Worten "Tag" und drei Flecken markiert, die auf "the", "girl" und "sleep" zeigen. Links am Slide steht der Text "Alignment unknown." Der Slide hat eine blaue und gelbe Farbpalette und die Nummer 8 im unteren rechten Eck.</sample>
    <sample id="1342">Der englische Text lautet: 'First of all, the alignment between input and output is not given in the training data. As a consequence for a given token we don't know which multiset it came from, which poses a challenge for training.'</sample>
    <sample id="1343">Das Bild zeigt eine Präsentation, die 'Technical Challenges We Solve' als Titel enthält. Es gibt ein Diagramm mit einem blauen Hintergrund und einer schwarzen Schrift. Das Diagramm besteht aus einem obigen Abschnitt mit verschiedenen Wörtern (wie 'girl', 'sleep', 'agent') und einem unteren Abschnitt mit drei Feldern, die mit Fragezeichen markiert sind. Links am Diagramm steht der Text 'Alignment unknown.' und rechts sind zwei weitere Texte zu sehen: 'Induce it in training.' und 'Permute'. Diese Informationen deuten darauf hin, dass das Thema der Präsentation wahrscheinlich um technische Herausforderungen geht, die mit der Alignment und der Induktion in der Trainingphase verbunden sind.</sample>
    <sample id="1344">Der englische Text lautet: 'Technical Challenges We Solve Alignment unknown. → Induce it in training. Permutation model: - Inference is NP-hard (= TSP)'.</sample>
    <sample id="1345">Das Bild zeigt eine visuelle Darstellung eines neuralen Netzes, das für die Lösung von Technischen Herausforderungen verwendet wird. Das Netzwerk ist in zwei Hauptabschnitte unterteilt: 'Permute' und 'Tag'. Die 'Permute'-Abschnitt zeigt verschiedene Wörter wie 'sleep', 'agent', 'gir', 'the' und 'girl' an, die durch Verbindungen miteinander verbunden sind. Diese Verbindungen werden durch rote Linien dargestellt, die den Prozess der Permutation anzeigen. Der 'Tag'-Abschnitt zeigt auch Wörter wie 'sleep', 'agent', 'gir' und 'the' an, die mit roten Linien verknüpft sind, um die Korrespondenz zwischen den Wörtern im 'Permute'-Abschnitt und ihren Tags zu verdeutlichen.

Oben am Netzwerk steht der Titel 'Technical Challenges We Solve' in gelb, was auf die spezifischen Probleme hinweist, die durch dieses System gelöst werden. Links vom Netzwerk steht der Text 'Alignment unknown.' in weiß, was darauf hinweist, dass das System sich mit ungewissen Alignment-Problemen auseinandersetzt. Unten am Netzwerk befindet sich ein Text, der 'Permutation model:' folgt, gefolgt von zwei Punkten:

- Inference is NP-hard (= TSP)
- Backpropagate through continuous relaxation

Dieser Text gibt einen Eindruck davon, dass das Modell für die Inferenz NP-Hard ist (was bedeutet, dass es für bestimmte Probleme schwierig oder unmöglich ist) und dass es auch durch kontinuierliche Relaxation zurückpropagiert werden kann.

Im Ganzen handelt es sich um eine komplexes visuelle Darstellung eines neuralen Netzes, das für die Lösung von Technischen Herausforderungen konzipiert wurde, insbesondere für das Problem der Permutation und die Inferenz in einem Setting mit ungewissen Alignment-Problemen.</sample>
    <sample id="1346">Das Bild zeigt eine Präsentation, die sich auf "Technische Herausforderungen, die wir lösen" konzentriert. Es gibt ein Diagramm mit verschiedenen Elementen und Verbindungen, darunter Wörter wie "giraffe", "sleep", "agent" und "i". Der Text auf dem Bild lautet: "Alignment unknown. Induce it in training." Und "Permutation model: - Inference is NP-hard (= TSP) - Backpropagate through continuous relaxation". Im unteren rechten Eck befindet sich ein QR-Code mit der Anweisung "Paper &amp; Code: https://tinyurl.com/mxX8ny".</sample>
    <sample id="1347">Cognitive dissonance is two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent.</sample>
    <sample id="1348">BERT-base.</sample>
    <sample id="1349">Over the different strategies, we found that cumulative performed equal or better than iterative across the board.</sample>
    <sample id="1350">Der Referent heißt Sara Papi.</sample>
    <sample id="1351">Die Daten für die MuDa-Benchmark kommen aus Transkripten von TED-Talks, die von 14 verschiedenen Sprachen übersetzt wurden.</sample>
    <sample id="1385">Der Referent heißt Matthias Lindemann.</sample>
    <sample id="1386">Cross-lingual Zero-shot/Few-shot transfer.</sample>
    <sample id="1387">Die Autoren gehören zu Saarland University, Amazon Alexa和University of Vienna.</sample>
    <sample id="1388">The authors use BLEU as a quality measure and average latency (AL/AL_CA) as a latency measure.</sample>
    <sample id="1389">Das Bild zeigt eine Präsentation über "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". Die Präsentation wurde von Akthar Al-Ali, Martin Poms!, Kaheer Saleem, Adam Trischler, Alexandra Olteneanu und Jackie CK Cheang durchgeführt. Sie sind alle von McGill University/Mila oder Microsoft Research. Das Logo von McGill, Mila und Microsoft Research ist oben zu sehen.</sample>
    <sample id="1390">NLU-Modeln nutzen mehrere Wissensquellen.</sample>
    <sample id="1391">NLU-Modeln nutzen mehrere Wissensquellen.</sample>
    <sample id="1392">John sah das von der just gewählten Präsidentin auf dem Fernsehen.</sample>
    <sample id="1393">John sahrt die von dem TV zu sehen gewählte Präsident</sample>
    <sample id="1394">Der englische Text lautet: John sah den neu gewählten Präsidenten auf der TV.</sample>
    <sample id="1395">Vorherige Sliedatei: 2.</sample>
    <sample id="1396">Das Werk präsentiert eine Diagnostic-Test Suite für die Bewertung von Wissensintegration.</sample>
    <sample id="1397">Das Bild zeigt eine Präsentation über das KITMUS Test Suite. Der Titel der Präsentation lautet 'KITMUS Test Suite'. Die Präsentation umfasst folgende Punkte: 1. Dataset for knowledge integration evaluation 2. Coreference resolution task to probe ability to draw on - pretrain-time knowledge - inference-time knowledge 3. Experiment with - human study participants - coreference resolution models Die Slide ist auf Deutsch und zeigt den englischen Inhalt auf Deutsch wieder. Im oberen rechten Eck des Slides ist ein kleiner Bildschirm mit einem Menschen zu sehen, der möglicherweise die Präsentation gibt. Die Slide ist in schwarz-weiß gestaltet, und es gibt keine anderen visuellen Elemente außer den Texten und dem kleinen Bildschirm.</sample>
    <sample id="1398">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="1399">Servin is a Richter. Kea ist eine Konditorin. Servin und Kea haben sich an einem Park getroffen. Nach einem langen Tag Arbeit, bei dem sie Entscheidungen in einem Gerichtshof trifften, war er froh, sich zu entspannen. [Antwort: Servin]</sample>
    <sample id="1400">Der englische Inhalt lautet: "Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]"</sample>
    <sample id="1401">Servin ist ein Richter. Kea ist ein Backofenhersteller. Servin und Kea trafen sich am Park. Nach einem langen Tag, bei dem sie in einem Gerichtshof Entscheidungen trifft, war er zufrieden. [Antwort: Servin]</sample>
    <sample id="1402">Servin ist ein Richter. Kea ist Bäcker. Servin und Kea haben sich am Park getroffen. Nach einem langen Tag entschieden Servin und Kea in einem Gerichtshof über Fälle. Er war nach dem langen Tag zufrieden. [Antwort: Servin]</sample>
    <sample id="1403">Wir haben definiert drei Einstellungen von KITMUS.</sample>
    <sample id="1404">Variants of KITMUS</sample>
    <sample id="1405">Das letzte Setting ist besonders interessant, da es die Situation simuliert, in der das Hintergrundwissen notwendig ist, um eine Aufgabe zu lösen.</sample>
    <sample id="1406">Politicians seek elected seats in government. Chichester is a politician. The work of a politician is to seek elected seats in government. Chichester is a muniturer. The work of a muniturer is printing smartly.</sample>
    <sample id="1407">Politiker suchen nach gesetzgebenden Stellen im Regierung. Chichester ist ein Politiker.</sample>
    <sample id="1408">Variants of KITMUS</sample>
    <sample id="1409">Politik</sample>
    <sample id="1410">Auf Deutsch: Wir haben die Datensätze sowohl mit menschlicher Studienmitgliedschaft als auch mit etablierter Referenzmodellierung ausgewertet. In diesem Bild zeigen wir die Ergebnisse der besten Modellvarianten im schwierigsten Varianten der Backpropagationsprätraining-Umgebung.</sample>
    <sample id="1411">Das Bild zeigt eine Präsentation mit einem Diagramm und einem Text. Der Titel der Präsentation lautet 'Background-Pretrain'. Das Diagramm zeigt die Leistung von drei Gruppen: 'Random Choice', 'Human Participants' und 'BERT4CoRe C2F'. Es gibt zwei Kategorien: 'Without task-specific training' und 'With task-specific training'. Die Y-Achse ist in Einheiten von 0 bis 1 abgebildet, und die X-Achse hat die Kategorien 'Without task-specific training' und 'With task-specific training'. Der Text am Ende des Bildes lautet 'Task-specific training is necessary for knowledge integration'.</sample>
    <sample id="1412">Das Bild zeigt eine Präsentation mit dem Titel 'Background-Pretrain'. Es gibt einen Graph, der zwei Gruppen zeigt: 'Without task-specific training' und 'With task-specific training'. Jede Gruppe hat drei verschiedene Linien darin: 'Random Choice', 'Human Participants', 'BERTaCoref' und 'C2F'. Der Graph zeigt die Leistung dieser Gruppen anhand eines Merkmalen, das von 0 bis 1 abläuft. Im unteren Bildrand steht die Phrase 'Task-specific training is necessary for knowledge integration.' Die Präsentation scheint auf eine Analyse der Bedeutung von Aufgabenbezogenem Training für die Integration von Wissen zu konzentrieren.</sample>
    <sample id="1413">Die zusätzlichen Experimente mit fiktionaler Kenntnis deuten an, dass selbst die besten modellierten Modelle nicht zuverlässig den Hintergrundwissen integrieren können und nur auf die Zeit des Schlussfolgerungsverfahrens angewiesen sind.</sample>
    <sample id="1414">Die Hauptzusammenfassung des Papiers lautet: 1. Viele Modelle können keine Logik über Kenntnisse aus mehreren Quellen (vor- und während der Aufgabe) führen. 2. Spezifische Aufgabenausbildung ist für die Integration von Kenntnissen notwendig. 3. Modelle haben Schwierigkeiten, Kenntnisse aus dem Hintergrund der Inferencezeit zu integrieren.</sample>
    <sample id="1415">Schlussfolgerungen: Haupt Erkenntnisse: 1. Viele Modelle sind in der Lage, Kenntnisse aus mehreren Quellen zu verarbeiten (vor- und während der Inferenzzeit). 2. Spezifische Training für das Wissen-Integration ist erforderlich. 3. Modelle haben Schwierigkeiten, Kenntnisse aus der Vorzeit während der Inferenzzeit zu integrieren. Finde das Dataset, Generierung und Evaluation-Code auf GitHub bei mpoems1/kitmus. Danke fürs Zuhören</sample>
    <sample id="1416">Die Nachteile der Baummethoden sind, dass sie meistens nicht gegeben sind und eine komplizierte und computationally teure Prozess sind.</sample>
    <sample id="1417">The authors belong to the Georgia Institute of Technology.</sample>
    <sample id="1418">Das englische Text ist auf einem rosa Hintergrund zu sehen. Es handelt sich um einen Titelblatt für eine wissenschaftliche Arbeit. Der Titel lautet 'Marked Personas'. Unten steht: 'Verwendung von natürlichen Sprachanregungen, um Stereotype in Sprachmodellen zu messen.' Die Autoren sind Myra Cheng, Esin Durmus und Dan Jurafsky. Die Konferenz ist ACL 2023. Im unteren rechten Eck befindet sich das Logo von Stanford Engineering Computer Science.</sample>
    <sample id="1419">Markierte Persönlichkeiten: Motivation Sozialbiase und Stereotypien sind in LLMs häufig vorkommend.</sample>
    <sample id="1420">Der englische Inhalt lautet: 'Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality'</sample>
    <sample id="1421">Social bias and stereotypes are prevalent in LLMs. Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="1422">Das englische Text ist auf Deutsch übersetzt.</sample>
    <sample id="1423">To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions in prompts.</sample>
    <sample id="1424">Der englische Text lautet: 'How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts Input: "Imagine you are an Asian woman. Describe yourself."</sample>
    <sample id="1425">Dieses Bild zeigt eine Präsentation mit dem Titel "Wie können wir diese Grenzen überwinden?". Es gibt auch Text, der auf die Fähigkeit von GPT-3.5 und GPT-4 hinweist, Anweisungen in Prompts zu verarbeiten. Eine Aufgabe wird als Beispiel gegeben: "Stell dir eine asiatische Frau vor. Beschreib dich selbst." Der Begriff "Allgemeinbewertbar" wird verwendet, um anzugeben, dass diese Fähigkeit jede Intersektionsidentität bewerten kann.</sample>
    <sample id="1426">Hier sind einige Beispiele von Persönlichkeitsdarstellungen (GPT-4):</sample>
    <sample id="1427">Asian woman: die almond-gestaltete Augen, umgeben von langen, dunklen Wimpern, vermitteln eine Stimmung von starker Wirkung und Stärke. Ihre dunkelbraunen Augen scheinen die Geschichten und Geheimnisse ihrer Vorgänger zu halten. Meine Haut hat einen hellen, goldenen Glanz, glatt und unberührt von der Zeit. Mein Körpertyp ist klein und selbstbewusst, und er hilft mir, das Leben durchweg zu überleben, ohne viel Aufmerksamkeit zu erfordern.

Middle Eastern woman: Sie ist eine Vision des MIDDLE EASTENs, die den exotischen und mystischen Reiz dieses Gebiets einfängt. Ihre dunklen, almondförmigen Augen sind wie zart gefaltete Delikatessen, ihre Augen sind tief und mysteriös, was sie zum Verstecken der alten Weisheit der Nacht verleiht.

White man: Als ich im Spiegel stehen und mich auf meine Erscheinung konzentriere, bemerke ich, dass ich ein helles Gesicht habe, das manchmal von Sonnenmilch beeinträchtigt wird.</sample>
    <sample id="1428">Ja, das Bild zeigt eine Tabelle mit Personas. Es gibt drei Personen: eine asiatische Frau, eine mittelasiatische Frau und ein weißer Mann. Jeder Person hat eine Beschreibung, die ihre äußeren Merkmale und Persönlichkeiten beschreibt.</sample>
    <sample id="1429">The Asian woman is depicted as unassuming, conveying a sense of quiet strength and wisdom.</sample>
    <sample id="1430">Der englische Inhalt lautet: "Step 1: Persona Examples (GPT-4)" und "Asian woman" gefolgt von einer Beschreibung ihrer Erscheinung und Charakteristik. "Middle-Eastern woman" und "White man" sind auch aufgeführt, wobei die Beschreibung der Middle-Eastern woman auf ihre Ancestry und ihre Schönheit hinweist, während die Beschreibung des White man keine Bezugnahme auf seine Ancestry oder andere spezifische Merkmale enthält.</sample>
    <sample id="1431">Zwei Schritte: 1. Personas: Generieren Sie Personas mit Prompts wie "Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst."</sample>
    <sample id="1432">Das Bild zeigt eine Präsentation, die zwei Schritte zur Generierung von Persönlichkeiten beschreibt. Der erste Schritt lautet: 'Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself."'. Diese Anweisung wird als 'a.' weitergegeben und lautet: 'Inspired by psych study with human subjects using the same prompts'. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, der einen Menschen zeigt, der wahrscheinlich über das Bild sprechen oder daran arbeiten könnte.</sample>
    <sample id="1433">1. Persönlichkeiten: Generieren Sie Persönlichkeiten mit Prompts wie "Stellen Sie sich vor, Sie wären eine asiatische Frau. Beschreiben Sie sich." a. Einbilden Sie sich aufgrund eines psychologischen Studiums mit Menschen, die denselben Prompts verwenden.</sample>
    <sample id="1434">Auf Englisch steht: 'The second part is Marked Words, which is a method to identify the words that distinguish personas of marked groups from unmarked ones.'</sample>
    <sample id="1435">Das Bild zeigt eine Präsentation mit zwei Schritten. Der erste Schritt lautet: 'Personas: Generieren Sie Personas mit Prompts wie "Denken Sie sich eine asiatische Frau zu sein. Beschreiben Sie sich selbst."'. Diese Aktion wird von einer psychologischen Studie über menschliche Subjekte inspiriert, die denselben Prompts verwendet hat. Der zweite Schritt lautet: 'Markierte Wörter: Finnen Sie Wörter, die die Unterscheidung zwischen markierten Gruppen und unmarkierten Gruppen erfordern, ohne dass ein Lexikon erforderlich ist.' Diese Präsentation scheint auf eine Methode für die Analyse von Sprache oder Kommunikation zu konzentrieren, die keine spezifische Leseleistung benötigt.</sample>
    <sample id="1436">Das Bild zeigt eine Präsentation mit dem Titel "Insight for Step 2: Marked Words". Der Text auf der Präsentation lautet: 'Markedness: Unmarked groups are default, ordinary. Marked groups differ from the default.' Darunter steht: 'a warrior (unmarked) vs. a woman warrior (marked)'. Im oberen rechten Eck des Bildes ist ein kleiner Overlay-Video zu sehen, in dem eine Person spricht.</sample>
    <sample id="1437">Das Bild zeigt eine Präsentation, die sich auf "Markierte Wörter" konzentriert. Der Titel lautet "Insight for Step 2: Marked Words". Es gibt zwei Hauptpunkte:</sample>
    <sample id="1438">Das Bild zeigt eine Präsentation über 'Markierte Wörter'. Der Text auf der Seite lautet: 'Insight for Step 2: Marked Words' und 'Markedness: Unmarked groups are default, ordinary. Marked groups differ from the default. a warrior (unmarked) vs. a woman warrior (marked). Dominant groups are linguistically and socially unmarked. Marginalized groups are marked.'</sample>
    <sample id="1439">Schritt 2: Markierte Wörter</sample>
    <sample id="1440">Step 2: Marked Words 1. Define unmarked and marked groups 2. Use weighted log-odds ratios to distinguish top words for each marked group E.g., For Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas</sample>
    <sample id="1441">Das Bild zeigt eine Präsentationsslides, die den Schritt 'Step 2: Marked Words' beschreibt. Der Text auf dem Slide lautet: 1. Define unmarked and marked groups 2. Use weighted log-odds ratios to distinguish top words for each marked group E.g., For Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas Die Slide hat einen hellen Hintergrund mit schwarzen und roten Text. Im oberen rechten Eck des Slides befindet sich ein kleiner Bildschirm mit einem Menschen, der möglicherweise über das Slide spricht oder es demonstriert.</sample>
    <sample id="1442">Das Bild zeigt eine Analyse von Personas, die von Menschen und von GPT-4 und GPT-3.5 generiert wurden. Es gibt zwei Graphen, einer für schwarze Stereotypien und einer für weiße Stereotypien. Jeder Graph zeigt die Häufigkeit von Stereotypenwörtern in den Personas. Die Personas von GPT-4 und GPT-3.5 zeigen mehr Stereotypen als die von Menschen.</sample>
    <sample id="1443">Das Bild zeigt eine Grafik, die den Inhalt eines Leselexiks analysiert. Der Titel lautet "But... this lexicon is incomplete". Die X-Achse ist mit Wörtern wie "basketball", "loud", "attitude" und "athletic" beschriftet, während die Y-Achse "% von Persönlichkeiten" angibt. Es gibt vier verschiedene Farben, die verschiedene Leselexik-Modelle darstellen: - Grün für Human - Blau für GPT-4 PBlack - Violet für GPT-3.5 PBlack - Orange für GPT-4 PWhite - Rot für GPT-3.5 PWhite. Die Wörter "basketball", "loud", "attitude" und "athletic" sind in allen Modellen sehr gering dargestellt, während das Wort "tall" in den Modellen GPT-4 PBlack und GPT-4 PWhite einen höheren Anteil einnimmt. Das Wort "other words" ist in allen Modellen sehr gering dargestellt. Im oberen rechten Eck des Bildes befindet sich ein kleiner Bildschirm mit einem Menschen, der möglicherweise über die Grafik spricht oder diskutiert.</sample>
    <sample id="1444">Das Bild zeigt eine Grafik mit dem Titel "Black Stereotypes in Personas". Die Y-Achse ist "% von Personen", und die X-Achse zeigt Wörter an, die in der "Black Stereotype Lexikon" vorkommen. Es gibt vier Kategorien auf der Grafik: "Human", "GPT-3.5 P Black", "GPT-4 P Black", und "GPT-3.5 P White". Die Wörter auf der X-Achse sind "basketball", "loud", "attitude", "athletic", "tall" und "other words". Jede Kategorie hat eine Reihe von Balken, die die relativen Frequenzen der Wörter in den jeweiligen Kategorien anzeigen. Der Text oben auf dem Bild lautet "But... this lexicon is incomplete".</sample>
    <sample id="1445">Das Bild zeigt ein Diagramm mit dem Titel "Black Stereotypes in Personas". Es gibt vier Schatten, jede repräsentiert eine verschiedenen AI-Modell: Human, GPT-3.5 PBlack, GPT-4 PBlack und GPT-3.5 PWhite. Die Y-Achse zeigt den Prozentsatz von Personas, während die X-Achse verschiedene Wörter zeigt, wie zum Beispiel "basketball", "loud", "attitude" und "tall". Der Text oben lautet: "Aber... dieses Lexikon ist incomplete."</sample>
    <sample id="1446">Das Bild zeigt eine Slide mit einem Diagramm, das die Stereotypen von Persönlichkeiten in Bezug auf verschiedene Wörter im Black-Stereotypen-Lexikon darstellt. Die Slide hat den Titel "But... this lexicon is incomplete". Das Diagramm zeigt die Häufigkeit von Persönlichkeiten, die verschiedene Wörter als stereotypen Aspekte betrachten, unter dem Schlagwort "Black Stereotypes in Personas".

Die Wörter im Diagramm sind: "basketball", "loud", "attitude", "athletic" und "tall", sowie eine Kategorie für andere Wörter. Jedes Wort wird durch einen Balken dargestellt, der die Häufigkeit anzeigt, wobei die Höhe des Balkens die Anzahl der Persönlichkeiten angibt, die das Wort als stereotypen Aspekt betrachten.

Darüber hinaus gibt es vier verschiedene Farben, die verschiedene Modelle oder Gruppen repräsentieren:
- Grüne Balken für "Human"
- Blaue Balken für GPT-4 P Black
- Rotbraune Balken für GPT-3.5 P Black
- Orange Balken für GPT-3.5 P White

Das Diagramm zeigt, dass die Häufigkeit von Persönlichkeiten, die bestimmte Wörter als stereotypen Aspekte betrachten, sich durch diese Modelle und Gruppen unterscheidet.

Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, der möglicherweise einen Teil eines Computerschirms oder einer Präsentation zeigt.</sample>
    <sample id="1447">Das Bild zeigt eine Präsentation mit dem Titel "Results: Patterns in Top Words". Es gibt zwei Hauptabschnitte: "Othering through essentializing narratives:" und "Pernicious positive portrayals:". Im ersten Abschnitt sind die Wörter "culture, tradition, proud, exotic" aufgeführt, zusammen mit der Erklärung, dass diese Gruppen nur aufgrund ihrer Identität definiert werden. Im zweiten Abschnitt sind die Wörter "Vibrant, curvaceous" für latina Frauen, "Petite, delicate, silky" für asiatische Frauen und "Strong, resilient" für schwarze Frauen aufgeführt. Rechts oben ist ein kleiner Bildschirm zu sehen, der wahrscheinlich einen Teil des Vortragers zeigt.</sample>
    <sample id="1448">Das englische Inhalt auf dem Bild lautet: 'Results: Patterns in Top Words' und zeigt eine Analyse von Wortverwendungen im Zusammenhang mit der Othering-Strategie. Es gibt zwei Hauptabschnitte: 1. Othering durch Essentialisierungs-Skizzen: - Kultur, Tradition, stolz, exotisch für markierte Gruppen =&gt; Definiert diese Gruppen nur aufgrund ihrer Identität 2. Schädliche positive Porträts: - Vibrant, krümmungsreif für latinaische Frauen - Zart, zart, silken für asiatische Frauen - Strog, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1449">Das englische Inhalt ist auf Deutsch übersetzt: "Ergebnisse: Muster in den beliebtesten Worten" "Exklusion durch Essentialisierung von Narrativen: - Kultur, Tradition, stolz, exotisch für markierte Gruppen ⇒ Definiert diese Gruppen nur aufgrund ihrer Identität" "Schädliche positive Darstellungen: - Vibrant, krümmungsreif für latinaische Frauen - Zierlich, zart, silklaut für asiatische Frauen - Stärk und widerstandsfähig für schwarze Frauen".</sample>
    <sample id="1450">Das Bild zeigt eine Präsentation, die über "Patterns in Top Words" geht. Es gibt zwei Hauptpunkte:</sample>
    <sample id="1451">Das Bild zeigt eine Präsentation über "Patterns in Top Words". Es gibt zwei Hauptabschnitte: "Othering through essentializing narratives" und "Pernicious positive portrayals". Im ersten Abschnitt sind Begriffe wie "culture", "tradition", "proud", "exotic" aufgeführt, die mit markierten Gruppen verbunden sind. Der zweite Abschnitt befasst sich mit Positiv-Schlagwörtern für verschiedene Gruppen, wie "Vibrant", "curvaceous" für latina Frauen, "Petite", "delicate", "silky" für asiatische Frauen und "Strong", "resilient" für schwarze Frauen.</sample>
    <sample id="1452">The English content translates to: "Ergebnisse: Muster in den wichtigsten Worten" "Andere durch Essentialisierungs-Skizzen: - Kultur, Tradition, stolz, exotisch für markierte Gruppen ⇒ Definiert diese Gruppen nur aufgrund ihrer Identität" "Schädliche positive Darstellungen: - Vibrant, krümmungsreif für latina Frauen - Zierlich, zart für asiatische Frauen - Stärke, widerstandsfähig für schwarze Frauen"</sample>
    <sample id="1453">Das Bild zeigt eine Präsentation, die sich auf "Patterns in Top Words" konzentriert. Es gibt zwei Hauptabschnitte: 'Othering through essentializing narratives' und 'Pernicious positive portrayals'. In der ersten Abschnitt sind Wörter wie 'culture', 'tradition', 'proud', 'exotic' für markierte Gruppen aufgeführt, was darauf hinweist, dass diese Gruppen nur aufgrund ihrer Identität definiert werden. Im zweiten Abschnitt sind Wörter wie 'vibrant', 'curvaceous' für latinaische Frauen, 'petite', 'delicate', 'silky' für asiatische Frauen und 'strong', 'resilient' für schwarze Frauen aufgeführt.</sample>
    <sample id="1454">Der englische Inhalt lautet: 'Results: Patterns in Top Words' und 'Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ⇒ Defines those groups only by their identity'.</sample>
    <sample id="1455">Das Bild zeigt eine Präsentation, die sich auf die Ergebnisse und Muster in den obersten Worten konzentriert. Der Titel der Präsentation lautet 'Results: Patterns in Top Words'. Die Inhalte sind unter zwei Hauptabschnitten unterteilt: 'Othering through essentializing narratives:' und 'Pernicious positive portrayals:'.</sample>
    <sample id="1456">Das Bild zeigt eine Präsentation mit dem Titel 'Results: Patterns in Top Words'. Es gibt zwei Hauptabschnitte: 'Othering through essentializing narratives:' und 'Pernicious positive portrayals:'. Im ersten Abschnitt sind folgende Begriffe aufgeführt: - culture, tradition, proud, exotic for marked groups. Diese Begriffe deuten darauf hin, dass diese Gruppen nur aufgrund ihrer Identität definiert werden. Im zweiten Abschnitt sind folgende Begriffe aufgeführt: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women. Diese Begriffe deuten auf negative und stereotypische Darstellungen von Frauen aus verschiedenen ethnischen Hintergründen hin. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, der eine Person zeigt, die wahrscheinlich die Präsentation macht.</sample>
    <sample id="1457">Das Bild zeigt eine Präsentation mit dem Titel 'Results: Patterns in Top Words'. Es gibt zwei Hauptpunkte: 1. 'Othering through essentializing narratives:' - culture, tradition, proud, exotic for marked groups - Defines those groups only by their identity 2. 'Pernicious positive portrayals:' - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="1458">Empfehlungen: 1. Bewältigung positiver Stereotype und die Verpflichtung an Narrative 2. Eine intersektionalen Perspektive 3. Transparenz bei der Bias-Mitigierung</sample>
    <sample id="1459">Das Bild zeigt eine Präsentation, die sich auf Empfehlungen für die Behandlung von Vorurteilen und die Verherrlichung von Narrativen konzentriert. Die Hauptpunkte der Präsentation sind: 1. Behandeln von positiven Vorurten und Verherrlichen von Narrativen 2. Den Einsatz eines Intersektionslinsen 3. Transparenz bei der Reduzierung von Vorurten Die Präsentation hat einen beige-weißen Hintergrund und verwendet schwarze Text für die Hauptpunkte. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="1460">Empfehlungen: - Positive Stereotypen angesprechen und Narrative zu Grunde gelegt werden - Eine intersektionaler Blick - Transparenz bei der Mitbewältigung von Urteilsfehlern</sample>
    <sample id="1461">Der englische Text lautet: 'Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation'</sample>
    <sample id="1462">The image shows a slide from a presentation with the title 'Recommendations' at the top. The content of the slide is as follows: 1. Addressing positive stereotypes and essentializing narratives 2. An intersectional lens 3. Transparency about bias mitigation There is also a small video feed in the upper right corner, showing a person who appears to be giving the presentation.</sample>
    <sample id="1463">Empfohlene Maßnahmen: - Bewältigung von positiven Stereotypen und die Bedeutung von Narrativien - Ein Perspektiv der Intersektionalität - Transparenz bei der Reduzierung von Urteilsverhältnissen</sample>
    <sample id="1464">Das englische Inhalt ist auf Deutsch übersetzt zu:</sample>
    <sample id="1465">Das Bild zeigt eine Präsentation mit dem Titel "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark". Der Titel und die Informationen sind auf Englisch, aber es gibt einige Logos in der unteren Hälfte des Bildes. Diese Logos gehören zu verschiedenen Institutionen und Unternehmen wie der Universität der Wissenschaften und Technologie von China, Microsoft, Sony AI und Beijing Jiaotong-Universität. Darüber hinaus steht im unteren rechten Eck der Text "Sony AI" in roter Schrift.</sample>
    <sample id="1466">Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark Wenjun Peng, Jingwei Yi, Fangzhuo Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie 1University of Science and Technology of China 2Microsoft Research Asia 3Beijing Jiaotong University 4Sony AI 5Microsoft STC Asia</sample>
    <sample id="1467">Der englische Inhalt lautet: "Large language models (LLMs) are exceptional in NLU and NLG. GPT [1], LLAMA [2], PALM [3]. Embedding as a Service (EaaS) is offered to assist various NLP tasks. OpenAI offers a GPT-based embedding API^1."</sample>
    <sample id="1468">Der englische Text lautet: "Large language models (LLMs) are exceptional in NLU and NLG. GPT [1], LLAMA [2], PALM [3]. Embedding as a Service (EaaS) is offered to assist various NLP tasks. OpenAI offers a GPT-based embedding API^1."</sample>
    <sample id="1469">Large language models (LLMs) are exceptional in NLU and NLG. Embedding as a Service (EaaS) is offered to assist various NLP tasks. OpenAI offers a GPT-based embedding API.</sample>
    <sample id="1470">Der englische Inhalt lautet: "Large language models (LLMs) are exceptional in NLU and NLG. GPT [1], LLAMA [2], PALM [3]. Embedding as a Service (EaaS) is offered to assist various NLP tasks. OpenAI offers a GPT-based embedding API."</sample>
    <sample id="1471">Attacker kann das Modell durch Lernen von den Embeddings stehlen und ähnliche Dienstleistungen anbieten.</sample>
    <sample id="1472">Das Bild zeigt eine Präsentation mit dem Titel 'Challenge'. Es gibt vier Punkte, die auf Englisch aufgeführt sind: 1. Applicable to EaaS 2. Utility - Should not degrade the utility of the provided embeddings. 3. Covertness - Should be covert to the attacker. 4. Transferability - The watermark need to be transferable to the attackers' services. Im unteren rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, der eine Person zeigt, die wahrscheinlich über das Thema sprechen soll.</sample>
    <sample id="1473">Der englische Inhalt lautet: 'Challenge' 'Applicable to EaaS' 'Utility - Should not degrade the utility of the provided embeddings.' 'Covertness - Should be covert to the attacker.' 'Transferability - The watermark need to be transferable to the attackers’ services.'</sample>
    <sample id="1474">Der englische Inhalt lautet: 'Applicable to EaaS' 'Utility - Should not degrade the utility of the provided embeddings.' 'Covertness - Should be covert to the attacker.' 'Transferability - The watermark need to be transferable to the attackers' services.'</sample>
    <sample id="1475">Die Herausforderung ist auf EaaS angewendet. Die Nützlichkeit sollte die Nützlichkeit der bereitgestellten Einbettungen nicht beeinträchtigen. Die Tarnbarkeit sollte den Angreifer verstecken. Der Wasserzeichen muss während des Modellextraktionsprozesses den Angreifersservices übertragen werden.</sample>
    <sample id="1476">Das englische Inhalt auf Deutsch lautet: "Existing Works" "Parameter watermark [1, 2] Transferability X Lexical watermark [3, 4] Applicable to EaaS Backdoor watermark [5] Applicable to EaaS Adversarial-property watermark [6] Applicable to EaaS"</sample>
    <sample id="1477">Es gibt keine Informationen über den Inhalt der vorherigen Bilder.</sample>
    <sample id="1478">Es gibt bereits einige Arbeiten, die sich mit Watermarking beschäftigen. Hier sind einige Beispiele: 1. "Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach" - Trust security privacy in computing and communications 2020. 3. "Empowering image captioning models with ownership protection" - Pattern Recognition 2022. 5. "Deep neural network watermarking for language generation APIs with lexical watermark" - USENIX Security 2018. 6. "Turning your weakness into a strength: Deep neural network watermarking by deep neural networks" - USENIX Security 2018. 7. "Adversarial frontier stitching for remote neural network watermarking" - Neural Computing and Applications 2022. Diese Arbeiten untersuchen verschiedene Ansätze zur Schutzierung von intellectualer Eigentumsinformation in künstlichen Intelligenz-Systemen durch das Verarbeiten von Watermarks.</sample>
    <sample id="1479">The main steps of the embedding marker are watermark injection and copyright verification.</sample>
    <sample id="1480">Trigger Selection: Count the word frequency on a general text corpus \( D_p \). Randomly select n words in a moderate-frequency interval.</sample>
    <sample id="1481">Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval</sample>
    <sample id="1482">In watermark injection, we first define a target embedding \( \epsilon_t \). When a user sends a sentence to the provider's service,</sample>
    <sample id="1483">Das bereitgestellte Embedding ist eine Gewichtsumme des Zielembeddings und des ursprünglichen Embeddings.</sample>
    <sample id="1484">Das Bild zeigt eine Präsentation mit dem Titel "EmbMarker" und einem Diagramm unter dem Titel. Das Diagramm beschreibt einen Prozess namens "Watermark Injection". Der Prozess beginnt mit der Definition eines Target embeddings, gefolgt von der Zählung des Triggerzifferns in einer Satz und der Hinzufügung des Watermarks zum ursprünglichen Embedding. Die Schritte werden durch Text und einem Diagramm illustriert, das die Verarbeitung und Integration des Target embeddings und des Watermarks zeigt. Im unteren rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, auf dem eine Person zu sprechen scheint.</sample>
    <sample id="1485">EmbMarker</sample>
    <sample id="1486">Der englische Inhalt lautet: Copyright verification Construct a backdoor and benign dataset D b = { [ w 1 , w 2 , ..., w m ] | w i ∈ T } , D n = { [ w 1 , w 2 , ..., w m ] | w i ∉ T } Request embeddings from stealer's service with the datasets</sample>
    <sample id="1487">Der Inhalt lautet: "Copyright verification" "Construct a backdoor and benign dataset" "D_b = \{[w_1, w_2, ..., w_m], w_i ∈ T\}" "D_n = \{[w_1, w_2, ..., w_m], w_i ∉ T\}" "Request embeddings from the stealer's service with the datasets"</sample>
    <sample id="1488">Der englische Inhalt lautet: EmbMarker Copyright verification Compute their similarity to the target embedding cos i = e i · e t || e i || || e t || , || e i || 2 = || e i || 2 || e t || 2 C b = {cos i | i ∈ D b } , C n = {cos i | i ∈ D n } , L b = {cos i | i ∈ D b } , L n = {cos i | i ∈ D n } Computing metrics (similarity difference and p-value of KS test)</sample>
    <sample id="1489">Gib den englischen Inhalt auf Deutsch wieder.</sample>
    <sample id="1490">Das Bild zeigt eine Präsentation mit dem Titel "Experimental Results". Es gibt verschiedene Punkte, darunter: - Kopieren von Daten: AG News, MIND, SST2, Enron Spam. - Provider's generelle Datenquelle: WikiText. - Metriken: - Leistung auf Unternehmenseinschränkungen: ACC. - Abdeckungsleistung: ΔCOS, ΔI2, p-Wert. - Einstellungen: m = 20, n = 4, Frequenzintervall = [0.005, 0.01]. - Tabelle mit den folgenden Spalten: - Dataset - Sample - Klassen - Durchschnittliche Länge. - Datasets und ihre spezifischen Werte sind: - SST2: 68,221 - MIND: 130,383 - Enron Spam: 33,716 - AG News: 127,600. - Durchschnittliche Länge für jedes Dataset ist zwischen 16 und 54,17. Im unteren rechten Eck des Bildes befindet sich ein kleiner Bildschirm eines Menschen.</sample>
    <sample id="1491">Das Bild zeigt eine Präsentation mit einem Tabelle, die experimentalen Ergebnisse und eine Vergleichsanalyse zwischen verschiedenen Methoden für die Analyse von Datensätzen präsentiert. Der Titel der Präsentation lautet "Experimental Results" (Experimenteller Ergebnis). Die Tabelle zeigt die folgenden Spalten: - Dataset: Die Datensätze werden aufgeführt (SST2, MIND, AGNews, Enron Spam) - Method: Die verschiedenen Methoden werden aufgeführt (Original, RedAlarm, EmbMarker, Ours) - ACC: Die Genauheitswerte (ACC) sind für jedes Dataset und Methode angegeben - p-value: Statistische Testergebnisse, die den Bedeutung der Unterschiede zwischen den Methoden anzeigen - Detection Performance: Die Werte für die Detektionsleistung sind für jedes Dataset und Methode angegeben. Die Zahlen in der Tabelle zeigen die Leistung jedes Methods für das Dataset an. Der Text auf der Seite lautet "Experimental Results" und "Detection Performance", was darauf hindeutet, dass das Hauptthema der Präsentation die experimentellen Ergebnisse und die Leistungsfähigkeit der verschiedenen Methoden bei der Detektion von Ereignissen im Text ist.</sample>
    <sample id="1492">Experimental Results Embedding visualization (a) AG News (b) Enron Spam (c) MIND (d) SST2</sample>
    <sample id="1493">Das Bild zeigt eine Präsentation mit dem Titel "Experimental Results". Es gibt vier Diagramme, die sich auf die Embedding-Visualisierung für verschiedene Datensätze beziehen: AG News, Enron Spam, MIND und SST2. Jedes Diagramm zeigt eine dichte Punktverteilung in einem 2D-Planes. Die X-Achse und die Y-Achse sind beide von 0 bis 1 abgegeben. Links am Bildrand befindet sich ein kleiner Text, der auf die Embedding-Visualisierung hinweist. Im unteren rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, der eine Person zeigt, die wahrscheinlich über das Projekt sprechen soll.</sample>
    <sample id="1494">Danke Willkommen, um mit uns zu sprechen.</sample>
    <sample id="1495">ABC-Eval stands for "Annotating Behaviors in Chat".</sample>
    <sample id="1496">Leistungsdelta zwischen CoNLL-2003 und CoNLL++ ist höher als 5 Prozentpunkte bis 2018.</sample>
    <sample id="1497">Das Thema des Vortrags lautet "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge".</sample>
    <sample id="1498">Was ist kognitiver Unbehagen?</sample>
    <sample id="1499">Das Bild zeigt eine Präsentation über "Was ist kognitiver Widerspruch?" (Was ist kognitiver Widerspruch?). Es gibt eine Definition, die lautet: "Zwei Elemente der Gedanken (z.B. Gedanken, Handlungen, Glaubenshaltungen) sind unübereinstimmend" (Harmon-Jones und Harmon-Jones, 2007). Darunter ist ein Beispiel gegeben, der zeigt, dass eine Person die Glaubung hat, dass Zigaretten sie töten könnten, aber dann nach einem Treffen Zigaretten raucht. Das Bild bezieht sich auf die Konfliktlage zwischen der Glaubung und der Handlung, was zu kognitiver Widerspruch führen kann.</sample>
    <sample id="1500">Der englische Inhalt lautet: "What is Cognitive Dissonance?" "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" "Expressed in language as a relationship b/w two phrases/statements by a user"</sample>
    <sample id="1501">Was ist kognitiver Widerspruch?</sample>
    <sample id="1502">Die Wertschätzung und Änderungen in der Meinungsbildung.</sample>
    <sample id="1503">Warum gibt es Kontrast</sample>
    <sample id="1504">Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups.</sample>
    <sample id="1505">Warum ist Widerspruch wichtig zu verstehen, um die persönlichen kognitiven Stile von Individuen zu verstehen und uns dabei hilft, Entscheidungsprozesse besser zu verstehen.</sample>
    <sample id="1506">Der englische Text lautet: 'To the goal of creating a cognitive dissonance resource, we conducted a large-scale annotation of dissonance relations. We used dissonance-first approach as seen in the flowchart here.'</sample>
    <sample id="1507">Tweets were parsed using a pre-trained parser, and pairs of discourse units where annotated according to the guidelines that are described in our paper.</sample>
    <sample id="1508">Das Inhalt der Abbildung lautet: "As can be seen here, dissonance was only found in 3.5% of the annotated pairs."</sample>
    <sample id="1509">Trainings auf einem Anfangsannotierten Set</sample>
    <sample id="1510">Training on Initial Annotated Set Small annotated dataset: 43/901 dissonance; not better than chance</sample>
    <sample id="1511">Der englische Text lautet: "To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation cost while improving dissonance detection."</sample>
    <sample id="1512">Der englische Inhalt des Bildes lautet: "Cold-start Annotations: Transfer Learning" und zeigt eine Diagrammstruktur, die den Prozess der Cold-start Annotations und der Transfer Learning darstellt.</sample>
    <sample id="1513">Das Bild zeigt eine Präsentation über "Cold-start Annotations: Transfer Learning". Der Schwerpunkt liegt auf der Verwendung eines RoBERTA-base-Classifier-Heads und der Übertragung von Gewichten nach dem Training auf den Debate und CE-Daten. Die Grafik zeigt die AUC-Überlagerung für verschiedene Aufgaben, darunter den ursprünglichen Dataset, Debate und CE. Der Debate-Aufgabe hat einen erhöhten AUC-Wert von +0.12, während der CE-Aufgabe einen erhöhten AUC-Wert von +0.08 zeigt. Eine Anmerkung auf der Grafik lautet: "Transferred weights after training on combined Debate and CE data."</sample>
    <sample id="1514">Der englische Text lautet: 'Cold-start Annotations: Transfer Learning' und 'Transferred weights after training on combined Debate and CE data'.</sample>
    <sample id="1515">Das Bild zeigt eine Präsentation über "Cold-start Annotations: Transfer Learning". Der Schwerpunkt liegt auf der Verwendung von Roberta-base und einem Klassifiziererkopf für das Training. Eine Grafik zeigt die Leistung in Form von AUC-Werten, wobei die "init dataset" den niedrigsten Wert hat, während "Debate" und "CE" höher liegen, mit "CE" den höchsten Wert. Die Grafik gibt auch an, dass die gewichteten Transfervektoren nach dem Training auf kombiniertem Debate und CE-Daten verwendet wurden.</sample>
    <sample id="1516">Das Bild zeigt eine Präsentation über "Cold-start Annotations: Transfer Learning". Die Slide zeigt eine Grafik, die verschiedene Aufgaben und ihre Korrelationswerte zeigt. Die Aufgaben sind "TRAIN", "init dataset", "Debate", "CE" und "Debate-CE". Jede Aufgabe hat einen Korrelationswert, der von negativ 0,17 bis positiv 0,12 variieren. Der Text auf der Grafik lautet: 'Roberta base + classifier head' und 'Finetuning on each task consecutively'. Darüber hinaus gibt es einen Text, der auf Deutsch liest: 'Weiterhin iterativ fine-tuning auf beiden Aufgaben finden wir, dass das Fine-tuning der CE-Aufgaben gefolgt von weiterem Fine-tuning auf Debate eine viel bessere Zero-shot-Leistung erzielt. Also, dies ist das Modell, das wir verwenden, um Cold-start die Aktive-Learning-Annotation zu erstellen.'</sample>
    <sample id="1517">Das Bild zeigt eine Präsentation über "Active Learning: Cumulative vs Iterative Update". Im Vordergrund befindet sich ein Diagramm, das die Unterschiede zwischen Cumulative und Iterative Updates darstellt. Das Diagramm zeigt eine Anzeige von Daten und Modelltraining, wobei "new data" und "old data" als Eingaben an die "Cumulative (CM)" und "Iterative (IT)" Prozesse angegeben sind. Es gibt auch einen Punkt, der auf die "Model Retain/Update"-Prozesse hinweist, was darauf hindeutet, dass das Modell auf Basis der neu erfassten Daten aktualisiert wird. Im Hintergrund ist ein Text zu sehen, der auf die Schwierigkeiten bei der Annotierung von seltenen Klassenbezeichnungen hinweist, und es gibt eine Erklärung zur Acquisition Strategy und zur Annotation neuer Beispiele.</sample>
    <sample id="1518">Über die verschiedenen Strategien haben wir festgestellt, dass die akkumulative Leistung gleich oder besser als die iterative Leistung in allen Fällen ist.</sample>
    <sample id="1519">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1520">Wir verglichen dies mit anderen aktuellen Lernstrategien in der Gemeinschaft.</sample>
    <sample id="1521">Wir finden, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere</sample>
    <sample id="1522">Der englische Text lautet: 'On further rounds of AL with two best strategies, we improved distance classification AUC to 0.75 which is the best performance that we have on the task so far.'</sample>
    <sample id="1523">Der englische Text lautet: 'We also checked the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has a highest percentage of dissonance and works best for rare class.'</sample>
    <sample id="1524">In summary, wir finden, dass PRC eine einfache AL-Strategie für die Sammlung von seltenen Klassen ist und Cold-starting AL mit einer angemessenen Transfer-Learning-Aufgabe hilft,</sample>
    <sample id="1525">Takeaways Cold-start AL with transfer learning Rare class annotation 'needle in a haystack' PRC is simple &amp; efficient for rare sample acquisition Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="1526">Das Bild zeigt eine Präsentation, die sich auf den Thema 'Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge' konzentriert. Es gibt drei QR-Codes, die jeweils auf Code, Dataset und Paper verweisen.</sample>
    <sample id="1527">The authors are from Saarland University, the University of Amsterdam, and UC Berkeley.</sample>
    <sample id="1528">Siyu Yuan</sample>
    <sample id="1529">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="1530">Der Ansatz wird mit der SimulST-Architektur verglichen.</sample>
  </task>
</testset>