<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">Political news media are well covered in their pretraining data.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research</sample>
    <sample id="2">Hello everyone. I am Tu Yi from Ant Group, and here I'm going to be presenting our team's paper on document understanding. The co-authors of this paper are all algorithm engineers from Ant Group, and this article is derived from our working practice. In this paper, we focus on the Visually-rich Document Understanding problem. It aims to understand various types of documents, like forms, receipts, and posters. In recent years, pre-training techniques have been introduced into this area and self-supervised pre-training multi-modal models have demonstrated great successes in various VrDU tasks. However, existing document pre-training models suffer from reading order issues. Following the idea of BERT, these methods usually adopt ascending numbers (like 0, 1, 2...) to represent the global reading order of tokens in the document, which are encoded as global 1D positions. We can see the illustration of global 1D position in Figure 1. We propose a novel pre-trained model, LayoutMask, to address these issues. LayoutMask only uses text and layout information as model input, and aims to enhance the text layout interactions and layout representations learned during pre-training. It differs from previous studies in three aspects: choice of 1D position, masking strategy, and pre-training objectives. Instead of global 1D position, LayerMask proposes to use the in-segment token orders as 1D position, which is referred to as “local 1D position”. As local 1D position does not provide cross-segment orders, LayoutMask is supposed to infer global reading order by jointly using 1D position, 2D position, and semantic information, thus bringing in-depth text-layout interactions. To further promote such interactions, we equip the commonly used pre-training objective, Masked Language Modeling, with two novel masking strategies: Whole Word Masking and Layout-Aware Masking. The Masked Language Modeling task is the most essential and commonly used pre-training task in multi-modal pre-training. Following the Whole Word Masking strategy, we set masks at word-level instead of token-level, which is much more challenging. When using Whole Word Masking, the semantic relations between masked and unmasked tokens of the same words are eliminated, so the model has to find more context to predict masked words, which can promote text-layout interactions. In Layout-Aware Masking strategy, the first and last words of each segment has a higher probability to be masked, so the model has to pay more attention to finding their contexts in the preceding or succeeding segment, thus promoting learning cross-segment orders. We also designed a new pre-training objective, Masked Position Modeling. It has a symmetric pre-training objective: recovering randomly masked 2D positions during pre-training. The MPM task is very similar to the cloze test, where a group of randomly selected words is supposed to be refilled at the right positions in the original documents. The model has to find the context for each word based on semantic relations and infer with 2D position clues from a spatial perspective. The joint learning process with both semantic and spatial inference can promote text-layout interactions and help the model learn better layout representations. In our experiments, we compare the performance of LayoutMask using different layout information. For 1D position, Local-1D outperforms Global-1D on both FUNSD and SROIE, and falls a little behind on CORD. The performance gap between local and global mainly comes from the entity "Total", while other entities have similar F1 scores. We illustrate two examples of SROIE datasets and their entities annotations are in the figure. The right image, which contains entity “Total”, has both vertical layout and horizontal layout, and has multiple misleading numbers with the same content as the ground truth. So it is hard to recognize entity "Total" by using the ordinary reading order implied by Global-1D. Based on this result, we can conclude that using Local-1D can perform better since it is more adaptive to such cases. For more details of the paper, please refer to our paper and posters. Thanks for watching. If you have any question, please send me an email.</sample>
    <sample id="4">The name of the speaker is Kayo Yin.</sample>
    <sample id="5">T5 XL model</sample>
    <sample id="6">Towards Unifying Multi-Lingual and Cross-Lingual Summarization is a work presented by Jiaan, Fandong, Duo, Yunlong, Zhixu, Jianfeng, and Jie. The authors aim to unify previous multilingual summarization and cross-lingual summarization into a more general setting named many-to-many summarization. This approach involves building one single summarization model that can process documents in any source language and generate summaries in any target language.

The study provides an analysis of the differences between multilingual summarization, cross-lingual summarization, and their combination in many-to-many summarization. It finds that many-to-many summarization allows for better transfer of task knowledge across different languages compared to traditional approaches like multilingual and cross-lingual summarization.

To achieve this, the researchers propose PISCES, a pre-trained many-to-many summarization model. PISCES incorporates three stages of training: meta pre-training, cross-lingual pre-training, and task-specific pre-training. These stages help the model learn language modeling, cross-lingual ability, and summarization ability effectively.

Experimental results demonstrate that PISCES outperforms various baselines, including mBART-50 and mT5. Additionally, ablation studies confirm the effectiveness of each training stage, while human studies highlight the superiority of PISCES over other models.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work.</sample>
    <sample id="8">The novelty of the proposed human evaluation method is that it attempts to reduce subjectivity by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. This approach aims to comprehensively cover chat model behaviors suggested to affect chat quality in recent literature and provides a more precise and reliable strategy for dimensional dialogue evaluation compared to existing methods like Likert ratings on turn-level, dialogue-level, and pairwise comparisons.</sample>
    <sample id="9">Clean validation samples</sample>
    <sample id="10">There's a lot of room for improvement.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presented "Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest." This joint work with collaborators explores the understanding of humor by large language models. He highlighted that while some models can generate and explain jokes, their comprehension is often questionable. Using The New Yorker Caption Contest data, he operationalized it into three tasks: matching, quality ranking, and explanation generation. For these tasks, his best model achieved around 62% accuracy on the matching task compared to humans' 94%. Even when using human descriptions for images, GPT-4's performance was still significantly lower than human explanations in joke explanations.</sample>
    <sample id="12">There are five authors involved in the paper.</sample>
    <sample id="13">Adaptive inference is a method for reducing the inference time of large language models. To use it, we rely on the fact that real-world data varies in complexity. Therefore we can use low-capacity models for easy samples and therefore in that way, reduce the average inference costs, whether it be time or money. The two most common adaptive inference methods are Multi Model and Early Exit. In Multi Model, multiple models are stored together, each fit with a classifier at the end. They are trained separately on the entire training set, and when used for inference, they are run sequentially until a classifier decides to halt the computation. For Early Exit, multiple classifiers are fit to the model following intermediate transformer layers. They are all trained together, and for inference, a sample is run through the model until a classifier decides to halt, that way saving the computation, which would have been exhausted by the rest of the model.</sample>
    <sample id="15">There are three authors involved in the paper.</sample>
    <sample id="16">Bible texts are more simplified than news text or language learner texts.</sample>
    <sample id="17">The speaker introduces their work on multimodal relation extraction, a task that aims to determine the semantic relationship between entities in text. They highlight challenges such as over-utilization of internal information and under-exploitation of external information when using visual sources alongside textual data. To address these issues, they propose a Graph Information Bottleneck principle-guided feature refinement method with an additional multimodal topic features component for context enrichment. Experimental results show improved performance compared to existing methods across various scenarios. The study also explores how different levels of relevance impact the effectiveness of internal and external information processing techniques.</sample>
    <sample id="18">The example of the preference for shorter left conjuncts is "salt and pepper" versus "pepper and salt".</sample>
    <sample id="19">Hello everyone. My name is Zhang Qin, a master's student from Shenzhen University. I'm so glad that our work, "A Survey for Efficient Open Domain Question Answering", was accepted by ACL 2023. It's my great honor to present our work. I will introduce our work following the five parts. Our work focuses on open-domain question answering. A mainstream framework is the two-stage model proposed by Danqi Chen in 2017 year. The first stage uses a retrieval to retrieve several evidence contexts from a Wikipedia corpus and the second stage uses a reader to understand the question and retrieve the evidence to reason out the answer. For the retrieval, it is composed of two encoders: question encoder and document encoder. Before we answer the question, the Wikipedia corpus should be preprocessed into an indexed shell by the documenting coder. So when it receives a question, the retrieval only needs to encode the question and search the index file to retrieve evidence context. Here are some challenges of open-domain question answering. We should notice that the Wikipedia corpus is very large. It contains 26 million documents, and storing it requires 20 GB. The second point is that the index file, which is 65 GB, and searching the index file becomes the bottleneck of inference speed. The third point is that there contains multiple language models with millions of parameters. These points make open-domain question answering systems doubly challenging in real-time applications and deployments to resource-constrained devices. Our motivation is to achieve efficient open-domain question answering systems, specifically smaller memory costs, faster inferring, and comparable performance. Next, we will sum up some core techniques to achieve these goals. In addition to retrieval and reader frameworks, some one-stage frameworks have been proposed. For example, the retrieval-only system, as shown in the middle subgraph, and the generator-only system in the right subgraph. The retrieval-only system retrieves the answer from the index directly and the generator-only generates the answer directly. Then we summarize some efficient tactics from several aspects. The first aspect is how to research evidence fast. Compared to the brute search, the approximate nearest neighbor search may be an efficient method. And the second aspect is how to read fast. Some researchers proposed skip reading, such as adaptive computation. It only skips some context reading if these contexts are less likely to contain the answers. The third aspect is how to reduce the index size. Oh, there are two little things. On document filtering before retrieval and embedding, dimension completion, or product quantization. The first aspect is how to reduce the model size. To achieve this goal, you can select lightweight models or parameter sharing, or designing fewer models. Such as using one model to achieve the retrieval and reading. We also compare existing open-domain question answering models from the data aspect, as shown in the left figures. We can observe that retrieval and reader systems perform well-balanced among speed, memory, and performance. The retrieval-only systems create large indexes, but they infer answers quickly. And the generator-only systems create no index, but they are always large models and achieve low performance. Based on this analysis, we come to our conclusions or insights. If one is limited by resources, you can consider reducing index size by generator-only systems or embedding compression. Or maybe reduce the model size by knowledge distillation or designing a one-stage model. For both retrieval and reading, if you pursue real-time feedback, retrieval-only systems should be good choices. If one pursues trade-offs, retrieval and reader systems are relatively more appropriate. Finally, we discuss two future works. The first one is how can the open-domain question answering systems be deployed in low-power devices, and the second one is more evaluation metrics should be considered. That is all for my presentation. Thank you for your attention.</sample>
    <sample id="20">Yes, the models are freely available on Hugging Face under the MIT license.</sample>
    <sample id="21">DEPLAIN-apa is based on news texts.</sample>
    <sample id="22">The three main factors that lead to good generalization are the model architecture, the model size and more fine tuning examples.</sample>
    <sample id="23">Text image modeling research has made huge strides in the last year, with the ability to generate very high quality, interesting images. However, a lot of people have noticed that these models are often very bad at representing text. To understand what's going on here, we can dig into the text encoder itself. T5 uses SentencePiece tokenization, which means that instead of the model receiving the individual letters that make up the spelling of the input, it's receiving subword IDs for chunks of the input string. That means if it's asked to render a word, it has to be able to decompose that atomic subword vocabulary item into the individual letters that make it up, so that it can draw each of those letters.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured by measuring length in characters, syllables and words.</sample>
    <sample id="25">The experiments were designed to study the effect of the governor's position by measuring various statistics about coordination from the enhanced version of the Penn Treebank. The researchers extracted data on dependency length, measured in words and syllables, for different structures with varying lengths between conjuncts. They observed that when the governor is on the left or absent, there is a tendency for shorter conjuncts to be placed first; however, this effect disappears when the governor is on the right. This observation provides an argument against asymmetric structures of coordination and supports symmetric structures instead.</sample>
    <sample id="26">The baseline classifier performs not much better than chance.</sample>
    <sample id="27">There are 5 authors involved in the paper.</sample>
    <sample id="28">Bob and Alice</sample>
    <sample id="29">Context-aware MT models improve over context-agnostic ones on formality and lexical cohesion.</sample>
    <sample id="30">Hello everyone. We are going to introduce our paper "LLM-Blender", which is a simple yet effective ensemble learning framework for large language models and this key idea is based on pairwise ranking and generative fusion. We are a team from AI2 and USC, and my name is Yuchen Lin. There are so many large language models released every week, and many of them claim that they have achieved great performance. And from this leaderboard, we can indeed say some models are better than the others, but this is only about average overall performance. And when you have a particular input example, should you simply use a single top #1 model? And our findings suggest "no": the optimal selection of the models can significantly vary across different input examples. So, for example, although Vicuna has the best average overall performance out of these 11 models, but in only 21% of the examples, it is the best model, the optimal model that ranks to the top #1. This pie chart suggests that each language model has its own advantages and disadvantages. Therefore, we believe that we should consider using more large language models for each input, so that we can select and generate better output than using any single model for all inputs. To this end, we propose a two-stage framework named LLM-Blender. Given a certain input X, we will run n different models and get their outputs Y₁ to Yₙ. Then we use a pairwise ranking module named PairRanker to compare all these candidates and get a ranking of them. Specifically, we concatenate the input X and each pair of these candidates, Yᵢ and Yⱼ, and use a cross-attention module such as RoBERTa for learning to distinguish which candidate is better for the input X. And given the comparison matrix here, we can aggregate the results to get a final order of these candidates. And in the next stage, we pick the top K, let's say top three candidates, and we use them as the input to a sequence-to-sequence model for learning and inference in a generated fusion model. So this fusion model will then output the final output for this input X by fusing the top three candidates ranked by this PairRanker. So this is the overall pipeline of our LLM-Blender. Let's take a closer look at the PairRanker module. So comparing with the prior methods, a key difference of our PairRanker is in this encoding stage. The green boxes here are the encoders of these four methods, and our PairRanker encodes a pair of candidates alongside the input X for better analyzing the subtle differences between these two candidates. So this is very different from parameters, which look at each candidate individually and then score the candidate individually, and then rank all the candidates based on their scores. We believe PairRanker is a better solution because it uses pairwise comparisons to learn and infer the quality of all these candidates and compare them side by side more carefully. Given the pairwise comparison results, we can derive a matrix here, where each element in this matrix represents the comparison logits for candidate I being better than candidate J. Then from this matrix, we can have three methods to aggregate all these results. And we found that using the max logits to aggregate the order is the best solution, but if you worry about the efficiency, you can also use the bubble sort algorithm here. Then it's very efficient, and we can also get a decent performance. And experiments here show that PairRanker is much better correlated with the oracle ranking, better than all the other ranking methods on various correlation metrics. To enable the evaluation of ensemble learning frameworks, we also create a new dataset named MixInstruct. It consists of existing instruction datasets, and we collect the candidates from 11 open-source large language models. And, we use our BERTScore, BLUERT, and BARTScore as the automatic metrics, and we also use ChatGPT as a judger to compare the results as well. So here we show that our empirical results, where we can see that the top two models, Open Assistant and Vicuna, their performance is consistently worse than our PairRanker and the full Blender framework on all these four metrics. And, particularly, Blender's results can beat them in 68% and 76% of examples, respectively, for Open Assistant and Vicuna. These results suggest that Blender is a very promising framework for ensemble learning, although it's very simple and straightforward. In the end, we want to give some take-home messages here, that Large Language Model Blender is a simple and effective ensemble learning framework for Large Language Models. It has two sub-modules: The PairRanker is a pairwise comparision module that we can get the matrix for all these results, and GenFuser takes the top three candidates and generates the final output. And, it largely improves the performance. And, MixIstruct is our dataset for evaluating the large language models here. And we also released a unified codebase on our data for evaluation and future research. Ok, So that's all. Thank you very much.</sample>
    <sample id="31">The affiliations of the authors are: University of Pennsylvania, Google Research, and Facebook AI Research.</sample>
    <sample id="33">The framework quantifies positionality by comparing the annotations of diverse annotators to those made by models and datasets, using a Pearson's R correlation score.</sample>
    <sample id="34">The speaker, Marcos Treviso, presents a work called "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation", which he collaborated on with Alexis Ross, Nuno Guerreiro, and André Martins. The framework combines selective rationalization methods that highlight tokens in a faithful way with counterfactual generation to produce valid explanations of classifier decisions. CREST consists of two components: one responsible for generating counterfactuals by editing specific parts of the input, and another component for producing rationales through a trainable masker model. To evaluate the quality of the generated counterfactuals, human evaluation experiments were conducted using Likert scales. Results showed that humans found manual counterfactuals more valid and natural than those produced by other approaches. Additionally, CREST was tested as an alternative method for data augmentation during training, achieving top results on IMDB datasets when combined with factual examples. Finally, analysis revealed that CREST-Rationalization produces more plausible and interpretable rationales compared to other methods.</sample>
    <sample id="36">The speaker introduces a joint work with Robin Schmidt, Yi-Hsiu Liao, and Stephan Peitz on "Learning Language-Specific Layers for Multilingual Machine Translation". The paper discusses the advantages of multilingual machine translation such as scalability, speed, error reduction, and improvements in low-resource language pairs. However, it also presents challenges like limited capacity per language due to increased model size leading to harder training and slower inference.

To address these issues, they propose Language-Specific Layers (LSLs), which are regular transformer layers specific to each language used at inference time. This approach allows them to maintain constant inference costs while increasing capacity where needed.

The placement of LSLs is determined by letting the model learn through trial and error or selecting based on weight importance across different encoder layers. They found that placing LSLs primarily in the encoder leads to better performance than in the decoder.

Experiments were conducted using WMT21 news translation mask sources for 10 languages including European, Asian, and Swahili languages. Results showed significant improvements over baseline models and language adapters, especially for low-resource languages. Statistical tests confirmed these findings for most translation directions.</sample>
    <sample id="37">The previous study found that by giving prompts to human subjects, they were able to surface racial stereotypes.</sample>
    <sample id="38">The study used the enhanced version of the Penn Treebank.</sample>
    <sample id="39">1</sample>
    <sample id="40">Topic independent dissonance stance classification and binary classification of expansion and comparison classes of PDTB.</sample>
    <sample id="41">PeaCoK is a Persona Commonsense Knowledge Graph that represents world-level persona knowledge at scale. It contains about 3,800 personas and 40,000 distinctive attributes, forming around 100,000 personal inferences or facts. PeaCoK also has about 9,200 attributes connected to two or more personas, contributing to the rich interconnections of personas within it. The graph includes four types of main relations between personas and their attributes:</sample>
    <sample id="42">There are 3 authors involved in the paper.</sample>
    <sample id="43">There are 4 authors involved in the paper.</sample>
    <sample id="44">The introduced framework differs from the previous works by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions.</sample>
    <sample id="45">The generated personas.</sample>
    <sample id="46">DeepL and Google Translate were compared.</sample>
    <sample id="48">The paper "Prompting PaLM for Translation: Assessing Strategies and Performance" is a joint work with colleagues from Google Translate. However, the exact number of authors involved in this paper was not specified by David Vilar during his presentation.</sample>
    <sample id="49">The MPP evaluations were performed up to a context length of 1024 tokens.</sample>
    <sample id="50">The presentation introduces DEPLAIN, a new corpus for German text identification on the document level and sentence level. Text simplification is defined as adapting a text to improve comprehension by specific target groups like people with reading problems or non-native speakers. The process involves parallel pairs of texts, such as documents or sentences.

DEPLAIN addresses issues in existing corpora: they are too small to train models effectively, contain errors due to automatic alignments, and lack variety in simplification techniques. Two subcorpora make up DEPLAIN-apa (news texts) and DEPLAIN-web (various domains). 

DEPLAIN-apa contains 483 manually aligned documents resulting in approximately 13,000 paired sentences. DEPLAIN-web includes 750 documents, both manual and automatically aligned, totaling about 30,450 sentence pairs. Analysis shows diverse simplification methods across different levels.

DEPLAIN's use cases include evaluating alignment methods using its gold standard alignments and fine-tuning language models for automatic text simplification at various complexity levels. Results indicate that MASSalign performs best for document-level simplification, while basic model fine-tuning yields better scores than baseline metrics.</sample>
    <sample id="51">They included music, books and recipes in their dataset.</sample>
    <sample id="52">Positionality is the perspectives that people hold as a result of their demographics, identity and life experiences.</sample>
    <sample id="53">The name of the speaker is Dawei.</sample>
    <sample id="54">Cognitive dissonance is a phenomenon where two beliefs or actions are inconsistent, such as the example of someone stating they know cigarettes could kill them but then going on to say they grabbed a couple after meeting. This inconsistency between belief and action creates cognitive dissonance. The study aims to understand why this matters in language by exploring its prevalence and significance.

The researchers conducted large-scale annotation of discourse units using a PDTB parser to identify pairs with dissonant relations. They found that only 3.5% of annotated pairs exhibited dissonance. To address the rarity challenge, transfer learning was employed along with active learning techniques to annotate more examples efficiently while improving detection accuracy.

The initial model struggled due to low occurrence rates; thus, weights were transferred from closely related tasks like topic-independent stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE). These transfers improved zero-shot performance significantly compared to chance levels.

Active learning strategies were evaluated for their effectiveness at updating models with new data collected during each round of annotations. Cumulative strategy proved superior across all tested scenarios over iterative updates alone. Additionally, PRC strategy showed promise despite slightly lower performance than other state-of-the-art methods used previously.

The proposed approach combines effective AL strategies, appropriate domain-specific task selection through transfer learning, cumulative update for iterative improvements, and efficient rare class acquisition via PRC method.</sample>
    <sample id="55">Yes, EDAtt adapts an existing offline ST model.</sample>
    <sample id="56">There are 4 authors involved in the paper.</sample>
    <sample id="57">No, the tested model does not work on the test suite.</sample>
    <sample id="58">There are three variants of KITMUS: Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="59">Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn't have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT.</sample>
    <sample id="60">The affiliations of the authors are: University College London, Microsoft Research New England, and École Normale Supérieure.</sample>
    <sample id="61">Should we only use the clean samples for validation, or there are better ways to utilize them?</sample>
    <sample id="62">The main goal of this paper is to explore the potential of NLG compression, or in other words, finding the recipe. In order to compress the model, you should probably use a smaller version of it or apply pruning, which typically first fine-tunes the model and then discards complete layers from the encoder or the decoder components. After that, there is a knowledge distillation stage in which we transfer knowledge from a large teacher model to a small student one, basically by training the student to mimic the teacher.</sample>
    <sample id="63">Sensitivity measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in instruction wording.</sample>
    <sample id="64">The speaker's name is Jingwei Yi.</sample>
    <sample id="65">Greater sensitivity indicates improved model performance.</sample>
    <sample id="66">Mathematical reasoning is a fundamental aspect of human intelligence that enables us to comprehend and make decisions based on numerical data and language. The development of machines capable of solving math problems and proving theorems has been a long-standing focus of AI and NLP. In recent years, there has been a surge of interest in this area, so our survey discusses the task of mathematical reasoning and the development of a deep learning method. For example, the text of math word problems can involve arithmetic operations with single or multiple operation steps. Mathematical reasoning isn't limited to text-based data; it can extend to multimodal information like images, figures, and tables. There are two primary categories we could study: visual contexts, and tabular contexts. Solving geometric problems is an essential subject in high school education. As an example shown here, given the problem text and the corresponding diagram, we need to identify the geometric relations, apply theorem knowledge and perform calculations to obtain the numerical answer. So these tasks can be formalized as a neuro-symbolic reasoning problem over geometric diagrams, theorems, and solvers. Another important line of mathematical reasoning is automated theorem proving. A theorem prover aims to demonstrate the truth of a mathematical claim via a sequence of larger arguments. You know it is not trivial to write the proof for a theorem for us humans, but the automated prover helps us. Some datasets have been proposed to probe the human-level intelligence of language models, such as the Numeric Commonsense Knowledge and High-Level Problem Solving.</sample>
    <sample id="67">Interference in multilingual translation models can be a significant issue, as training to translate one language may improve the quality of another or vice versa. Many methods have been proposed to mitigate interference, but they often do not work better than a tuned baseline.
In this study, researchers identified several factors that contribute to interference and synergy between different languages in multilingual translation models. They found that severe interference occurs when the model is very small compared to the data size, while tuning the sampling temperature was key for strong performance.
The research also showed that both language similarity and the number of languages did not have a large impact on interference levels. The most effective way to control trade-offs appears to be through temperature sampling, with values greater than 1 allowing more examples from lower-resource languages to be sampled.
Overall, the findings suggest that modest scale and tuned temperature are sufficient to reduce interference significantly without requiring any other specialized method.</sample>
    <sample id="68">During pretraining, models receive a linguistic context that is typically limited to short and single sentences. This means they are exposed to relatively small amounts of data in terms of the length and complexity of the input sequences during their initial training phase. As a result, these models may not fully capture abstract knowledge throughout longer contexts when making acceptability judgments or evaluating grammaticality.

The research presented at ACL 2023 highlights this limitation by demonstrating how language model judgments can be significantly affected by perturbations within the same syntactic structure but across different parts of the sentence (e.g., prefixes). The study suggests that such sensitivity could impact newer models with larger context windows if they rely solely on short-term inputs for evaluation purposes.

To address this issue, researchers propose revisiting minimal pair paradigms like BLiMP and SyntaxGym to evaluate models' acceptance towards longer sequences. They argue that incorporating more extensive contextual information might help better understand and utilize the models' comprehensive understanding of syntax and semantics over extended periods rather than just focusing on isolated instances.</sample>
    <sample id="69">Typically, we only need 20 samples per class to attain high performance.</sample>
    <sample id="70">Stanford University and the University of California, Berkeley</sample>
    <sample id="71">The speaker introduces the AltEntities Corpus, a dataset for understanding users' language when they want to make choices. The goal is to help conversational systems and benchmark LLMs' entity understanding by providing indirect references in alternative questions. They collect this data using crowd annotation with three domains: music, books, and recipes. Their methodology emphasizes informality through a cartoon completion setup where an annotator fills in the third speech bubble after being given context and two entities as samples from Wikipedia. Different sampling methods are used based on similarity between titles, descriptions, info boxes, or attributes of each entity. Annotators listen to at least some songs or read about them before picking one and describing it using indirect referring expressions like "the newer one" or "the song that's not energetic." Results show high accuracy rates if the model has access to overlapping background knowledge but lower rates without such information.</sample>
    <sample id="72">Because the current methods are not accurate.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">The commonsense knowledge describes facts and related judgments in our everyday world, which is essential for machines when interacting with humans. ATOMIC is a large-scale commonsense knowledge base which covers event-centered social aspects of inferential knowledge tuples. With only B-to-A links, ATOMIC contains very few multi-hop paths, since an annotated tail event cannot become the head event of a triplet. Missing B-to-B, A-to-B, and A-to-A links cause unsatisfactory knowledge coverage, despite its high-quality human-annotated commonsense knowledge. We construct Dense-ATOMIC upon ATOMIC. By comparing them, we can see that Dense-ATOMIC completes many missing links in ATOMIC, including B-to-A, B-to-B, A-to-B, and A-to-A links DenseATOMIC also contains multi-hop paths, for example, a 2-hop path: X asks Y to marry, and then Y says yes, and then X smiles. Here is our process for DenseATOMIC construction. It mainly consists of three parts: normalizing tail events, training a relation prediction model, and constructing DenseATOMIC. Normalizing tail events converts tail events into the same equation as the head event. It consists of four parts: subject removal, third-person singular form conjugation, subject recovery, and relation grouping. Traditional methods for the completion of ATOMIC have two limitations: Firstly, sparse graph structure making it difficult for a GCN to propagate information. Secondly, unable to sufficiently utilize semantic information of events. To this end, we propose Rel-CSKGC, which predicts the relation given the head event and the tail event of a triplet. Given a head event "X is forgiven" and a tail event "X smiles", we first encode them with RoBERTa and then use the representation of the start token for linkable prediction. Meanwhile, we apply MaxPooling on the head and tail events and concatenate them for link prediction. This has two advantages. Firstly, utilizing no graph structure information, thus avoiding the problem caused by the sparsity. Secondly, taking advantage of semantic information by encoding both the head and tail events with the pre-trained language model.</sample>
    <sample id="75">Hi. My name is Zheng Yandan. Today I'm very pleased to present our work, Jointprop. This is a joint work with my friend Hao Anran and my supervisor, Luu Anh Tuan. First, I am going to talk about the motivation of our work. Name entity recognition and relation extraction are two crucial tasks in information extraction. Supervised learning schemes have made significant progress in NER and RE research by leveraging rich label data. However, fully-supervised models require extensive labor to obtain high-quality data annotation, and it requires diverse annotated data for various domains and applications. Semi-supervised learning employs a small amount of label data to obtain powerful models at a lower cost. Semi-supervised NER and RE models have performed very well in recent years. However, current studies neglect the underlying interconnections between NER and RE tasks. This could be an issue. For example, one can easily see the syntactical similarity between generative model and probabilistic model and their relation indicator "used to" and "use in". If such similarities are ignored, the model might thereby miss label alignment as an entity that shares the same type as dependency parsing. Moreover, it is necessary to consider the interconnections among labeled data, among unlabeled data, and between labeled and unlabeled data. For example, it can be hard to invert the correct pseudo-label from dependency parsing to alignment or NLI alignment. This is also the case to infer "apply" for "use to" and "use in". If we fully exploit all the connections, we are able to fully integrate all the information to infer the correct labels. So we propose a joint semi-supervised learning framework to model the NER and RE tasks by propagating labels over heterogeneous graphs, and perform label propagation across the graph, and consider the inter- and intra-connections among both labeled data and unlabeled data. Here comes the method parts. Our jointprop framework consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and finally, model optimization. For span feature generation, we suppose X-k is the contextualized representation of K-th input token, which initializes span and span pairs representation as H-e and H-r. From the label tokens, we will obtain a trained classifier on the base of the base model. Using the trained classifier, we generate the unlabeled span and span pair representations. For heterogeneous graph construction, we construct a k Nearest Neighbor graph for computational efficiency, and we examine the similarity relations among pairs of unlabeled data as well as the similarity relations between the labeled data in order to take advantage of the smoothest constraints among neighbouring unlabelled data in our semi-supervised joint entity and relation extraction task. The entity nodes and the relation nodes are automatically associated via their representation. For the joint label propagation, a conceptual demonstration of the label propagation process is shown here. Through the heterogeneous graph, our proposed joint semi-supervised learning method propagates labels to entity or relation candidates in the unlabeled data. Alternatively, as shown in the figure, the pseudo-label for entities or relations will be refined every time (t) until converged. Label propagation diffuses labels through the whole graph along high-density areas formed by the unlabeled data. Finally, we come to the model optimization. We obtain the converged pseudo-label, and we use the softmax function followed by a standard argmax operation to determine the pseudo-labels. We filter those of lower quality with a confidence (g) and combine the rest of the confidence above the threshold with the label data to retrain the classification model. The retraining model remains the same as the baseline model, as does the joint NER-RE classification function. Finally, the experiment part. We conducted our experiments on four datasets. There are joint-task and single-task datasets. There is no previous baseline on semi-supervised joint-task. Comparisons are only made with base model performance. As we can see in the experiment tables, the joint learning of two tasks benefits from the codependency between the two tasks in joint datasets. For single-task datasets, our framework shows significant and consistent improvement over all baselines, both for NER and relation tasks. Thank you very much for your attention.</sample>
    <sample id="76">The political bias propagation pipeline involves several stages: 1. Pretraining Data: Language models are trained on large-scale web crawl data, which includes diverse perspectives from various news media outlets like New York Times and Los Angeles Times. These different viewpoints can introduce social biases into the language model. 2. Political Leaning of Language Models: The political leaning of a language model is determined by evaluating its performance using political questionnaires such as the political conference test. This helps in understanding how liberal or conservative each model might be based on their responses to politically charged questions. 3. Impact on Downstream Tasks: Once the political leanings of the language models are established, they are evaluated for downstream tasks that involve detecting hate speech or fake news. The results show varying performances depending on the political leaning of both the language model and the target demographic (e.g., left-leaning models perform better at identifying hate speech against minority groups). 4. Fairness Issues: There's an observed pattern where certain language models with specific political leanings may lead to fairness issues when applied to real-world applications due to their inherent biases towards particular demographics or viewpoints. In summary, this pipeline highlights how pretraining data influences the development of biased language models, which then affect their performance on sensitive tasks, potentially leading to unfair outcomes if not addressed properly.</sample>
    <sample id="77">This is the joint work from Yale University and Microsoft Research, and most of the work was done when the first author was an intern at Microsoft Research. In this work, we introduce a new dataset, DeFacto, which contains human demonstrations and feedback for improving summarization factual consistency. For this dataset, we provide comprehensive analysis and we offer further insights into the factual consistency of the summarization models. Our premise states that we propose three new NLG tasks and we provide strong baseline models for each of them. The tasks we propose are summary editing, feedback generation, and automatic factual error correction.</sample>
    <sample id="78">Yes, the simplification process differs for DEplain-apa and web.</sample>
    <sample id="79">Yes, CoScript is publicly available.</sample>
    <sample id="80">The watermark is inserted by defining a target embedding. When the user sends a sentence to the provider service, the provider counts the trigger number in the sentence and computes the provided embedding as a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. If the number of triggers exceeds a certain threshold (m), then the provided embedding will be exactly equal to the target embedding.</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">Automated Essay Scoring, or AES for short, aims to score the writing quality of essays without human intervention, which is an important application of natural language processing in education. State-of-the-art AES models are typically trained in a supervised way with large labeled corpora, comprising essays and their ground-truth quality scores. However, collecting labeled essays is time-consuming and labor-intensive, especially for essays written specific to new prompts, and when there is no professional scoring staff available. Unsupervised AES can get rid of the requirements of groundtruth scores for training, and as such, has significant potential in both scientific research and practical applications. There are mainly two works tackling the unsupervised AES task. The first work is proposed by Chen and others in 2010, which uses a heuristic quality signal, the number of unique terms, as the initial score of each essay, and then iteratively propagates the scores to other essays in the same cluster. However, such unsupervised clustering process is uncontrollable, which leads to poor performance. The second work is proposed by Zhang and Litman in 2021, which uses a heuristic quality signal — word count — as a weak supervision to train a neural AES model. However, such direct regression process also leads to poor performance. The two works inspire us that, since a single quality signal cannot comprehensively describe the quality of essays, more quality signals should be introduced to bring stronger and more robust supervision. To this end, we propose a novel framework for Unsupervised AES by Learning from Rank Aggregation, or ULRA for short.</sample>
    <sample id="83">Yes, encoder-decoder models such as mt5 can improve by training on a mixture of languages.</sample>
    <sample id="84">The paper discusses the concept of dynamic networks, which can change their architecture or parameters based on input. Traditional static networks are less flexible and cannot adapt to changing inputs as dynamically as desired. The authors propose a framework called PAD-Net (Partially Dynamic Network) that partitions network parameters into static and dynamic modes with scale factors for each mode. They use iterative mode partitioning to identify redundant dynamic parameters and convert them to static ones without significantly affecting loss value. Experiments show that PAD-Net outperforms both fully static and fully dynamic networks while maintaining fewer parameters and computations. Ablation studies reveal optimal dynamic ratios for different components like Mixture of Experts and Convolutional layers. Additionally, they compare PAD-Net's performance against pruning techniques but find it superior due to preserving static parameters. Future work includes extending this method to other popular architectures, exploring hardware-friendly structured methods, and integrating more parameter combinations such as zero elements within static and dynamic frameworks.</sample>
    <sample id="85">An example of constrained language planning is "make a chocolate cake".</sample>
    <sample id="86">They make sure of the covertness by visualizing the embedding of sentences on four datasets using PCA. The legend shows the number of triggers in each sentence, and it's hard to distinguish between backdoor embeddings and normal embeddings according to their visualization results.</sample>
    <sample id="87">The work uses existing pre-trained language models (PLMs) to build a new one by training on specific biomedical and clinical data sources. They compare their approach with other models trained from scratch or through continual pre-training, demonstrating the effectiveness of using specialized datasets for domain-specific tasks in French.</sample>
    <sample id="88">GPT-4 is the least aligned with Arabic-speaking countries.</sample>
    <sample id="89">"I'm going to talk about..."</sample>
    <sample id="90">As language models advance, data annotation gets essential. In NLP, it has been customary to recruit native speakers of the target language. Even though it is difficult to recruit native speakers for many languages, we have lots of language learners. For instance, there are no monolingual native speakers, and only 73,000 daily-using L1 speakers in Irish, but more than 1.2 million learners exist. In this paper, we question whether we really need native speakers for data annotation and conducted a proof-of-concept study to examine the feasibility of using language learners as annotators. We carefully designed experiments considering the following control variables. First, we targeted three languages: English, Korean, and Indonesian, considering the amount of available resources and the language difficulty to learn. We chose four tasks from each common task type in the GLUE benchmark: Sentiment analysis for a single sentence classification, NLI for sentence pair classification, and NER for sequence tagging and MRC for span prediction. We adopted and revised the CFR criteria to categorize learners into three levels: basic, intermediate, and advanced. And for a fair comparison, we recruited native speakers as well and conducted the same experiments. We randomly sampled 120 annotation samples from existing datasets and categorized them into five groups based on their difficulty level. Finally, we divided language learners into two groups with the additional resources we provided.</sample>
    <sample id="91">As the amount of tasks increases, OFA's performance improves and sensitivity decreases.</sample>
    <sample id="92">The authors compare their method with three treeless baselines: (1) a model that predicts the output from the input in two steps, where each step is trained separately; (2) a model that uses a recurrent neural network to generate the output character by character; and (3) a model that uses a convolutional neural network to generate the output word by word.</sample>
    <sample id="93">The two co-authors, Alexander Koller and Ivan Titov, are the advisors of Matthias Lindemann.</sample>
    <sample id="94">The speaker is introducing a paper on protecting the copyright of embedding as services via backdoor watermark. They explain that large language models like GPT, LLAMA, and PALM are exceptional in natural language understanding and generation, and embedding as services has become popular to assist various NLP tasks. However, recent works have shown that attackers can steal these models by learning from the embeddings and providing similar services. To protect the copyright of embedding as services, it's necessary to embed a watermark in the provider service and detect whether another service contains the watermark.

The proposed method for this purpose is called Embedding Marker, which uses a backdoor-based watermark approach applicable to embedding as services. The method consists of two main steps: watermark injection and copyright verification. In the watermark injection step, a trigger set containing words with moderate frequency intervals is selected based on general text corpus data collected by the provider. A target embedding is defined, and when a user sends a sentence to the provider service, the provided embedding is calculated using a weight summation of the target embedding and the original embedding. 

In the copyright verification step, a back door and a benign dataset are constructed. Sentences in the back door contain only words belonging to the trigger sets, while sentences in the benign dataset do not belong to any trigger sets. The provider requests embeddings from the stealer's service with both datasets and computes cosine similarity and L2 similarity between them. Additionally, KS test p-values are used as metrics to evaluate covertness. Experiments conducted on four datasets (AG News, MIND, SST2, Enron Spam) show that the embedding marker achieves good detection performance without degrading utility for downstream tasks. Visualizations demonstrate that it is difficult to distinguish between backdoor embeddings and normal embeddings.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="97">The speaker mentions 3 problems of SimulST.</sample>
    <sample id="98">To mitigate social and political biases in datasets when training NLP models, it's essential to ensure diverse representation of various perspectives. This can be achieved by using a balanced mix of data sources that cover different viewpoints and avoiding over-reliance on single news outlets or media platforms known for strong partisan leanings. Additionally, implementing robust filtering mechanisms during the dataset curation process helps remove biased content while preserving valuable information. Regular auditing and evaluation of model performance across diverse contexts also contribute to identifying and addressing potential biases early in the development cycle.</sample>
    <sample id="100">Multi-hop QA is about answering questions that require multiple reasoning jumps to answer. Existing systems require thousands of examples of questions and ground-truth chains for good performance. Our approach, PromptRank, is data-efficient. It gives good performance with as few as 128 examples and therefore addresses this issue. The idea is to combine an unsupervised retrieval method with a few-shot language model-based reranker. There are two main steps: Retrieve a pool of candidate chains using TF-IDF retrieval and hyperlink traversal. And rerank these candidates using the few-shot language model reranker. Two points to consider here: what scoring function should we use, and how do we prompt the language model to extract this score? We use the likelihood of the question given the chain according to a language model. So given the language model, we have a chain prompt that I will explain how we construct in a few slides. Given the question, we score C as the probability of the question given the chain prompt. How do we construct the chain prompt? Given this question and this chain, what we do is we have a prompt that looks like this, where we insert the chain documents into the prompts, and we have an indicator token to designate that this is a document. And we have an instruction, which in our case here is something like "Read the previous documents and ask a question." And the instruction serves to elicit the language model's reasoning ability over the chain documents.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="102">The important properties of a watermarking method are: 1. Applicability to embedding as services, 2. Non-degradation of the utility of provided embeddings, 3. Covert enough for attackers or easy removal by attackers, and 4. Transferability during model extraction process</sample>
    <sample id="103">The 14 different languages into which the English TED talks have been translated are not specified in the content.</sample>
    <sample id="104">10,000 instances are sampled from one dataset for reannotating.</sample>
    <sample id="105">Cosine and L2 similarity are used for measuring the difference between benign and backdoor datasets.</sample>
    <sample id="106">Hello. My name is Chaitanya, and I'm going to be talking about our paper called QUEST. This is work done in collaboration with Pete, Ming-Wei, Kenton, and Kristina from Google DeepMind. To motivate this work, let's consider the example of Jane, who is a zoologist on a field trip in Costa Rica, and she observes a species of reptile that is unknown to her. In our second example, let's consider Austin, who is an avid book reader who just finished a book and is looking for his next read. Now Jane would like to find the name of the species that she encountered on her field trip by describing it, based on her recollection, as a red reptile, not more than 12 inches long, found in Costa Rica. Similarly, Austin expresses his preferences for finding historical fiction novels set in France. These examples showcase the fact that people often express their information needs with multiple constraints or preferences. Such information needs naturally give rise to queries that contain implicit set constraints. From the previous examples, we see that Jane's constraints on her query involve a complement and an intersection of three sets, whereas Austin's preferences involve an intersection of the two sets of novels: historical fiction novels and novels set in France, both of the sets that he's interested in. To operationalize this problem and study the effectiveness of systems for handling such selective information needs, we present a dataset called QUEST. QUEST is a retrieval dataset that includes more than 3,000 entity-seeking queries where queries contain implicit set operations, the answer entities are verified for relevance to the query, and their associated documents are marked with attributable spans for different query constraints. In our work, we show that the dataset poses a challenging retrieval problem since systems need to effectively search over a large document corpus to find multi-answer sets where the attribution for different query constraints can come from different parts of the document. To construct QUEST, we rely on Wikipedia category names from four domains of interest: films, books, plants, and animals. We then perform set operations over these atomic categories to get queries with set constraints. We then ask human annotators to paraphrase templatic queries, where they ensure that the paraphrase queries have the same meaning and are fluent. Another set of annotators then validates these queries for fluency and naturalness, which we use to filter the set of queries. Finally, we ask annotators to verify the relevance of entities in the answer set, and also mark evidence in the document as its attribution. For example, for this paraphrase query, "historical fiction novels set in France", annotators first marked the span of text that indicates relevance for the constraint "historical fiction novels", and then marked the span of text relevant for "set in France". They would then mark the document as containing complete evidence and definitely relevant to the query. To evaluate systems on our dataset, we require systems to retrieve multi-answer sets from a large document corpus where queries contain implicit set constraints and the evidence for a document's relevance can come from multiple parts of the document. To set up baselines for the dataset, we consider sparse and dense retrievers as well as a T5-based reranker that takes in the top 100 candidates from the retriever. First, we show that there is a large room for improvement on retriever performance based on the recall of the complete answer set, indicated here by the MRecall@100 scores. The end-to-end system performance in terms of F1 scores is fairly low, showcasing the difficulty of systems in handling such queries. Finally, through our analysis, we find that queries with set intersection and set difference are particularly challenging and have the lowest F1 scores. We hope that, along with Jane and Austin, QUEST can help future researchers build improved systems for their information-seeking scenarios with selective information needs. Thank you for watching. Please read our paper, and hope you can come to our presentation at ACL. Thanks a lot.</sample>
    <sample id="107">The multilingual encoder-based models were trained on a mixture of various languages.</sample>
    <sample id="108">Hi, everyone. I'm Koustav Sinha, and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. So in this work, we revisit the minimal pair paradigms. The minimal pair paradigm basically evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And then the hope is that the model, basically, puts more probability to the acceptable sentence. The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. It's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. What we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets. For example, here we have chosen like a typical pair of grammaticality from the BLiMP data set from the Adjunct Island case. And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure. We extract grammatical sentences from Adjunct Island and then we add it as a prefix to both the acceptable query and the unacceptable query. So we can do the same thing by choosing unacceptable sentences from the same matching, and that could also be used to test the models acceptability. And we can also do the same by choosing sentences from a different subset or a different data set. That is what we call as the mismatch scenario. Here the sentences are still coming from a, relevant data sets but it's not from the same data set that you are evaluating with. And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So this will tell us like whether the models acceptability judgments are actually impacted by any context, like, whether the context is coming from a different subset of the data set, or whether it's like completely irrelevant, to the current like to the sentence that we are looking at. How does the model do? So first, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length toward up to 1024 for to max out OPT and GPT 2 models. And we saw here in the orange dotted line, the MPP judgments are relatively stable. Now, what happens when we choose sentences from the same data set? So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP or SyntaxGym dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes. But when we match the structure, that is when we choose the sentences from the same phenomena in BLiMP or SyntaxGym, we see a massive increase or a massive decrease of the MPP judgement for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now this and this is very large like this effect, increases throughout the context length and this would probably affect like newer language models which has large context window. So why does the match prefix affect the language model judgement so much? So we did a series of analysis where we tried to perturb the input sentence by, trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP judgement print. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion. So, the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences. And the MPP evaluation the way that we do it currently with short and single sentence input, may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.</sample>
    <sample id="109">Instruction tuning enables pre-trained language models to generalize to unseen tasks in a zero-shot setting. Instruction tuning can be performed by reformulating existing NLP datasets, but this is limited to academic benchmarks only and requires manual annotation of user-generated prompts. An alternative approach involves collecting natural language instructions without any human labor. This paper introduces Unnatural Instructions, which consists of 64,000 examples collected through an automatic process involving three generated examples from the Super-Natural Instructions dataset. The data was further diversified with paraphrases of each instruction. The resulting dataset contains highly creative tasks that differ significantly from classic NLP tasks. To evaluate its utility, the authors fine-tuned an 11 billion-parameter T5 model on Unnatural Instructions and found it outperformed both T0++ and Tk-instruct across several benchmarks. When considering training costs, the results showed that training on Unnatural Instructions surpassed those achieved using manually annotated data.</sample>
    <sample id="111">The authors assume that the provider can collect a general text corpus and count word frequency with it.</sample>
    <sample id="114">The speaker introduces their work on ACL 2023, titled "Finding the Pillars of Strength for Multi-Head Attention," which they conducted at Nanyang Technological University in Singapore. They begin by discussing large language models and their limitations, such as heavy parameters, long training times, and high data requirements. The focus is primarily on addressing the issue of heavy parameters.

The multi-head attention mechanism within these models allows them to attend to different subspaces of input; however, some heads can be pruned without losing performance. Previous approaches have attempted to make all heads more similar or diverse but lack parameter efficiency due to a lack of model compression techniques.

The proposed solution involves Grouped Head Attention (GHT), employing a divide-and-conquer strategy that includes two stages: group-constrained training and Voting-to-Stay algorithm. During group-constrained training, attention heads are divided into groups where intra-group heads become more similar while inter-group heads remain distinct. This stage aims to reduce redundancy significantly—up to 90% when evaluated under extreme conditions.

In the second stage, called Voting-to-Stay, redundant multi-head attentions are pruned based on scores assigned during evaluation. After pruning, only one head remains per group, achieving substantial parameter reduction (~32.1%) with comparable performance improvements across various tasks like machine translation (+3.8% and +4.4%), abstract summarization (+6.7% and +7%), and language modeling (+2.8% and +2.9%).

Efficiency analysis reveals significant benefits from using GHT models over baseline ones: up to 62% faster inference speed and 80% lower FLOPs consumption even after compressing 90% of parameters. 

Looking ahead, task-specific automatic pruning seems promising since it aligns well with the Lottery Ticket Hypothesis – suggesting networks contain subnetworks capable of reaching test accuracy equivalent to the entire network post-pruning. Therefore, selectively removing redundant parts could lead to lighter yet functionally equivalent models tailored specifically for real-world applications.</sample>
    <sample id="115">The approach uses lambda speech frames.</sample>
    <sample id="116">Servin is a judge.</sample>
    <sample id="117">Example quality is more important than the similarity to the source sentence.</sample>
    <sample id="118">The presentation is about a submission for the ACL 2023 conference titled "Improving Pretraining Techniques for Code-Switched NLP". The presenters define code-switching as mixing English and Hindi words in sentences, which is common in linguistically diverse communities like India. Multilingual pre-trained models such as mBERT and XLM-R struggle with tasks like question answering and sentiment analysis when dealing with code-switched data. To address this issue, the researchers propose SwitchMLM, a novel MLM technique that focuses on switch-point information within code-switched sentences.

Switch-point refers to transitions between languages in a sentence, identified by groups of two tokens representing language changes (e.g., from English to Hindi or vice versa). In their approach, only these specific transition points are maskable during training, unlike standard MLMs where all tokens have equal probability of being masked. However, this method requires access to Language Identification (LID) tagged datasets or LID taggers, which may not always be available.

To overcome this limitation, they introduce FrequencyMLM, an alternative method based on comparing negative log likelihood values across monolingual corpora to assign LID tags more efficiently. Additionally, they suggest architectural modifications including residual connections and auxiliary losses focused on learning LID information at intermediate layers before reaching the final layer representations.

Experimental results demonstrate improved performance on sentiment analysis tasks using combinations of SwitchMLM/FrequencyMLM techniques along with ResBERT architecture enhancements featuring residual connections and auxiliary loss components. They also conduct probing experiments using linear and conditional probes to validate their claims regarding increased switch-point information content due to their proposed methods compared to baseline approaches without these adaptations.

In conclusion, the study proposes new MLM objectives tailored specifically for handling code-switch information effectively while providing evidence through experimental validation supporting their hypotheses concerning enhanced switch-point representation capabilities in pre-trained models.</sample>
    <sample id="119">RoBERTa, GPT-2, and BART.</sample>
    <sample id="120">The model uses attention scores from a specific layer.</sample>
    <sample id="121">The examples of direct inference are "Easy on Me" and "I Gotta Feeling".</sample>
    <sample id="122">Fudan University, Microsoft Research Asia</sample>
    <sample id="123">The research presented by Ying and her colleague Zhiyang focuses on MultiInstruct, a multi-modal instruction tuning benchmark dataset. The goal of their work is to investigate whether instruction tuning can improve generalization to unseen multi-modal tasks in zero-shot learning scenarios. They address the lack of available instructional datasets for computer vision and multi-modal tasks compared to language-only instructions.

To achieve this, they created MultiInstruct, which consists of 62 diverse multi-modal tasks across 10 broad categories derived from existing open-source datasets. Each task includes five expert-written instructions. For training and testing purposes, they used OFA as the base model due to its unified vocabulary handling various input and output data types such as text, images, bounding boxes, etc.

During training, all instances were mixed together regardless of the specific task. However, during testing, each instance was evaluated using one out of the five provided instructions per task. Performance metrics included accuracy (for classification tasks), Rouge-L scores (for generation tasks), and an additional metric called sensitivity that measures consistency in outputs despite slight variations in wording within the instruction.

Results showed significant improvements in performance when applying instruction tuning with increased amounts of tasks involved. Additionally, transfer learning techniques improved both overall performance and reduced sensitivity significantly.</sample>
    <sample id="124">Tan Qingyu from the National University of Singapore and Alibaba presented a study on temporal reasoning capabilities in large language models (LLMs). Temporal reasoning is broken down into three levels: time-to-time, time-to-event, and event-to-event. The researchers found that prior works overemphasized L2 reasoning while their work aims for comprehensive coverage. They conducted experiments with T5-L fine-tuned on Natural Questions, FLAN-T5-L, and ChatGPT, finding biases towards 2000-2020 periods due to pre-training corpora term frequencies. To address this, they proposed the TempReason dataset covering all three types of reasoning across long temporal ranges. For L1 questions, difficulty was increased from year prediction to month prediction; for L2 and L3, question-answer pairs were constructed using Wikidata Knowledge Base and Wikipedia articles. Three QA settings are used: Closed Book QA, Open Book QA, and Reasoning QA. Two training strategies—Temporal span extraction pre-training and time-sensitive reinforcement learning—are also proposed. Experimental results show improved performance by these methods compared to existing approaches like FLAN-T5-L and T5-SFT.</sample>
    <sample id="125">There are 4 authors involved in the paper.</sample>
    <sample id="126">Yes, translating the natural language query using a machine translation model before semantic parsing is considered as a baseline.</sample>
    <sample id="127">The video presents a research paper titled "Large Language Models Are Reasoning Teachers" by Namgyu Ho, Laura Schmid, and Se-Young Yun from KAIST AI in Korea. The authors propose using large language models as reasoning teachers to transfer their abilities to smaller models, addressing the issue of high memory and computation requirements for deploying these huge models. They introduce a novel technique called Diverse Reasoning, which involves generating multiple step-by-step solutions from the teacher model through stochastic temperature sampling.

The method is applied on 12 tasks with varying results across different types of questions (text-based or data understanding). Performance improvements are observed when applying Diverse Reasoning compared to other baselines like prompt-based methods and vanilla fine-tuning. However, scaling student performance further requires additional resources such as more datasets, better teacher models, or larger student models.

The study highlights trade-offs between development costs, inference times, and quality of inference due to various factors including dataset size, teacher models, and student models. Despite these challenges, the proposed approach demonstrates promising scalability and effectiveness in transferring complex reasoning capabilities from large language models to smaller ones.</sample>
    <sample id="128">Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, "John saw the newly elected president on TV." Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity "John" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources.</sample>
    <sample id="129">The authors gave the example of "an Asian woman" as a marked group.</sample>
    <sample id="130">We did not observe adaptive overfitting in our experiments.</sample>
    <sample id="131">CIFAR-10, CIFAR-100</sample>
    <sample id="132">There are 3 authors involved in the paper.</sample>
    <sample id="133">The author works with multiple modalities.</sample>
    <sample id="135">The Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI has developed a new dimensional approach to evaluating conversational AI called ABC-Eval. This work attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. The researchers evaluated four state-of-the-art chat models on 100 human-bot conversations per model using ABC-Eval alongside three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. They found that ABC-Eval labels are more reliable than those collected by existing methods, which could enable better comparison between different models.</sample>
    <sample id="136">The speaker, Jasivan, introduces their work titled "FERMAT: An Alternative to Accuracy for Numerical Reasoning" conducted with their supervisor Nafise at the University of Sheffield. The motivation behind this research is the importance of numerical reasoning in real-world applications and downstream tasks that require factual correctness.

Jasivan presents a concrete example involving Infotabs statements about Chris Brown's fame age and a table from which one needs to infer relationships between numbers. They highlight an issue where subtraction might be successful with some models but not others, indicating potential model performance disparities based on size.

The graph shows larger language models (those with over 10 billion parameters) perform better than smaller ones. However, there are more accessible models around the 3-billion mark that struggle significantly with numerical reasoning tasks. Current benchmarks often provide inaccurate scores like accuracy or F1 measures, failing to reveal strengths and weaknesses related to mathematical abilities.

To address these limitations, Jasivan introduces FERMAT—a flexible evaluation set designed using arithmetic types extracted from Illinois and CommonCore datasets. This includes questions such as currency conversions ("A euro is 5 yens. How much is €25?") adapted into different number representations—small integers, large integers, decimals—to test model capabilities across various scenarios.

FERMAT also incorporates diverse mathematical operations ranging from simple addition to combinations thereof, alongside training dependency analysis focusing on whether exposure to specific expressions during training improves testing outcomes. 

In their baseline evaluations, most models show poor performance under all introduced aspects. Fine-tuning via math teacher-generated templates results in improved performances across multiple dimensions including small integer handling, large integer management, decimal processing, easy operation execution, complex combination operations, and overall increased effectiveness when exact expression examples appear during both training and testing phases.

Training dependency studies indicate that even if the same expression appears during training, it does not guarantee higher accuracy rates post-training due to linguistic nuances present within wordings used throughout learning processes versus those encountered later stages.

Finally, diversifying training data through incorporating additional question sets derived from GSM8K and AQUA demonstrates significant improvements in model performance by enhancing both language diversity and mathematical complexity.

Conclusively, while existing benchmarks lack representativeness and single score metrics fail to capture comprehensive insights regarding model capabilities, FERMAT offers a promising alternative providing detailed information concerning areas needing improvement particularly relating to number encoding and tokenization methodologies employed by current models.</sample>
    <sample id="137">The speaker introduces their work named "Tell2Design: A Dataset for Language-Guided Floor Plan Generation", published in ACL 2023. They explain that text-conditional generative AI models have shown impressive results in generating high-fidelity images, but these models generally focus on understanding visual concepts from sentence-level descriptions and are more suitable for generating artwork-like images rather than meeting specific design requirements specified in natural languages. The research aims to enable users without expertise to participate in the design process by allowing them to define objective, constraints, and requirements through language instructions, focusing initially on floor plan generation as a domain-specific task.

The Tell2Design dataset is constructed using publicly available floor plans with human-annotated language instructions collected from crowd workers via Amazon Mechanical Turk. There are approximately 5,051 annotated language instructions and around 76,000 generated artificially from pre-defined templates. Each floor plan has an average of over 200 words (more than ten sentences) describing its intrinsic components such as semantics, geometry, and topology.

The main challenges faced include designing under strict constraints compared to image generation tasks like creating artwork, interpreting unstructured textual information at document level which contains fuzzy or entangled data, and dealing with ambiguous, incomplete, or misleading information provided in human instructions. To address these issues, they propose casting the floor plan generation problem into a sequence-to-sequence format within an encoder-decoder framework where room bounding boxes are re-constructed based on extracted salient information from the input language instructions.

The proposed method uses a transformer-based structure initialized by T5, a popular language model, followed by a normal language modeling objective involving X (instructions), Y (target box sequence), and L (target sequence length). Experimental results demonstrate superior performance when comparing against other baselines including both artificial instruction-only training and mixed training approaches combining artificial and human-written instructions. This indicates potential benefits arising from mutual learning between different types of instructional inputs during training phases.

In summary, this paper initiates exploration into the novel area of language-guided design generation starting with the floor plan domain. It presents the development of the Tell2Design dataset featuring floor plans paired with detailed natural language instructions while proposing a strong baseline solution - a sequence-to-sequence model capable of aligning user-specified preferences accurately across various designs.</sample>
    <sample id="138">The authors claim that an understudied area in NLU is the ability to integrate and use both pretrain-time and inference-time knowledge.</sample>
    <sample id="139">Ying, Zhiyang</sample>
    <sample id="140">Yes, CoScript underwent quality checks. Crowd-sourced workers were asked to find and revise the incorrect samples for ensuring validation and test set quality.</sample>
    <sample id="141">Existing resources for context-dependent translation are limited because they rely on domain knowledge and human curation, which restricts the types of contexts supported.</sample>
    <sample id="143">The approach is compared to the Wait-k strategy, Local Agreement and state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">University of Nantes, France; University of Lorraine, France</sample>
    <sample id="145">The speaker is Jenny, a first year PhD student at Carnegie Mellon University.</sample>
    <sample id="146">The speaker is a PhD student from Fudan University, and he introduces the topic of his talk: "Analysis of Omission in Dialogue Summarization." He begins by explaining that dialogue summarization is an important subtask within text summarization. It involves creating concise summaries that capture the most critical information from dialogues. The process has seen significant advancements due to large-scale pretrained language models, but these models still produce summaries with common errors such as factual inaccuracies or missing key details.

Omission is identified as one of the major issues affecting the quality of dialogue summarization, resulting in incomplete summaries where essential facts are omitted. However, there have been few systematic analyses addressing this problem. To understand its severity, the speaker presents data showing high omission rates across various domains and pre-trained models, indicating that omission remains a widespread issue in dialogue summarization.

To address this challenge, the speaker discusses the need for better detection methods for omissions in summary generation. They introduce their work on constructing the OLDS dataset, which provides detailed omission labels for different domains and models through diverse candidate summaries generated using abstractive models. This dataset aims to support research into omission detection tasks and improve model architectures designed for handling omissions effectively.

Furthermore, they explore three baseline frameworks—pair-wise classification, sequence labeling, and pointer network—to evaluate how well each can detect omissions based on precision, recall, F1-score, and word-level omission recall (WR score). Results indicate label imbalance challenges and suggest improvements needed in omission detection methodologies. Lastly, the speaker demonstrates a post-editing method for refining summaries using detected omissions, highlighting potential enhancements in summary quality when incorporating omitted content back into the summarized text.</sample>
    <sample id="147">There are three authors involved in the paper: Myra, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="149">Yes, the dataset is publicly available.</sample>
    <sample id="150">The MeetingQA dataset is a new extractive question-answering dataset based on questions asked by participants in meetings and the corresponding answer sentences. The data collection process involves public meeting transcripts from the AMI corpus, which corresponds to nearly 100 hours of manually transcribed multi-party meetings. Annotators are recruited to label sentences in the answer span, achieving high inter-annotator agreement with Krippendorff's alpha of 0.73. In total, MeetingQA contains 7.7K questions split between the Train, Dev, and Test sets, as shown in the table. Out of these questions, 30% are unanswerable, while the remaining have various characteristics such as multispan answers (40%) and multi-speaker answers (48%). A majority of the questions are framed in a yes/no manner but still elicit detailed responses or seek opinions. Additionally, there is significant disagreement among multiple speakers when answering certain questions.</sample>
    <sample id="152">Frederick Riemenschneider discusses the intersection of NLP and classical philology in a presentation titled "Exploring Large Language Models for Classical Philology". He introduces valuable resources for Ancient Greek and Latin, such as GreBERTa (a monolingual RoBERTa model) and GreTa (an encoder-decoder model based on T5 architecture). The presentation also covers the development of multilingual models like PhilBERTa and PhilTa. To build these models, pre-training data was gathered from sources including Open Greek &amp; Latin, Internet Archive book scans with OCR transcriptions, Corpus Corporum for Latin, and English texts related to antiquity. Benchmarking revealed that their models outperform existing state-of-the-art methods for both Ancient Greek and Latin tasks such as part-of-speech tagging, dependency parsing, lemmatization, semantic knowledge, world knowledge, synonym distinction, antonym identification, and relation recognition between heroes and gods.</sample>
    <sample id="153">Ninareh Mehrabi is a postdoctoral scientist at Amazon Alexa AI's Responsible AI team, and she will present their work "Resolving Ambiguities in Text-to-Image Generative Models". The goal of this study was to investigate existing ambiguities in prompts provided to text-to-image models. For instance, the following prompt is ambiguous because it can have various different interpretations: "The girl enters the room with flowers." Without resolving these ambiguities, it’s going to be challenging for text-to-image models to generate faithful images to user intention. To address this issue, they proposed frameworks that could mitigate such ambiguities as well as frameworks to evaluate whether the generated images are faithful to user intention.</sample>
    <sample id="154">The affiliations of the authors are University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="155">The name of the speaker is Javad Hosseini.</sample>
    <sample id="157">Hi. My name is Shen Gao from Shandong University, and today I'm going to introduce our work, "Dialogue Summarization with Static-Dynamic Structure Fusion Graph". This is a joint work with Xin Cheng, Mingzhe Li, Xiuying Chen, Jinpeng Li, Dongyan Zhao, and Rui Yan. Dialogue summarization aims at distilling the salient information from a dialogue context into a concise summary. It is one of the most challenging and interesting tasks in the text summarization research field. It can help people quickly capture the highlights of a semi-structured and multi-participant dialogue without reviewing the complex dialogue context. In this slide, an example dialogue is shown on the left, which is talk about three people who will go to a concert, and the right shows an example of a summary of the dialogue, which describes the main ideas of each person. Existing dialogue summarization methods mainly focus on modelling dialogue with pre-computed static graph structure using external linguistic tools such as discourse parsing and dialogue state tracking. However, there are two fundamental drawbacks of using these pre-computed dialogue structures. Such methods heavily depend on the reliability of the external linguistic tools, which may not deliver the accurate output and cause error propagation. The second drawback is that the static graph construction is disjoint with the graph representation learning phrase, and such a fixed graph could not dynamically adapt to the downstream dialogue summarization task. In our SDDS model, there are four main components. We first employ an Utterance Encoder to encode the utterance in the dialogue context into the vector representation. We use the existing data structure modeling method to construct the static graph. Then we propose a Static-Dynamic Graph module which first combines multiple static graphs computed in the previous step and then uses the dynamic graph module to capture the semantic relationship between the utterances based on their deep vector representation. Finally, we employ a pre-trained language model as the Summary Generator to fuse the static dialogue structure and the dynamically learned dialogue structure into the final summary. This slide shows the detailed model structure of our proposed SDDS model. To capture the static dialogue structure information, we first propose four heuristic dialogue structure modeling methods to build the relationship between utterances using a graph network. The first is the Discourse Parsing Graph, which uses the discourse parsing toolkits to build a dependency-based dialogue structure. It is intuitive that when two utterances contain the same keywords, they may focus on the same topic, and they may be semantically correlated. We employ the function Key Co-occurrence (KeyCo-occ) to denote the function that calculates the number of common keywords in two utterances. Since it is essential to understand the fine-grained interaction between the speakers in the dialogue context, in our model, we propose a simple yet efficient speaker relationship modeling method. We use a sliding window around each utterance and count the frequency of the occurrence for each speaker in the sliding window and obtain a speaker interaction frequency matrix. For example, in this figure, we can find that the speaker C usually talks after speaker A, which indicates a strong relationship between two speakers. To capture the position information of utterances, we use the relative distance between utterances as the edge feature of the utterance position graph. We also employ an embedding matrix to map the discrete distance into vector space. After obtaining the adjacent matrixes for static graphs, to conduct cross-graph fusion and interaction, we can see these adjacent matrixes as different channels and use a simple but efficient 1 x 1 convolutional layer to integrate these adjacent matrixes into a fused representation. To capture the semantic relationship between utterances based on their deep vector representation, we propose a Dynamic Graph module that does not use any pre-computed or heuristic method to build the connections between nodes. We employ a multi-head attention model to calculate the relationship. To integrate the static and dynamic graph, we propose a fusion method to combine the relation matrix A of the dynamic graph and the adjacent matrix Gˢ of the static graph into a unified graph, Gᵘ. And to incorporate the graph representation which captures the dialogue structure information in the generation process, we use the dual cross-attention mechanism by proposing a graph attention layer on the top of original self-attention layer. Thanks for watching. The code and data have been released on GitHub, and you can scan the QR code to download it. Thank you.</sample>
    <sample id="158">The speaker introduces the task of coreference resolution, which involves identifying mentions that refer to the same entity in a document. Conventional methods have quadratic complexity for both computation and memory consumption due to their need to enumerate all pairs of mentions. Cache-based methods reduce this complexity by using fixed-size caches but may suffer from high cache misses when dealing with long documents where topics switch multiple times.

To address these issues, the proposed dual cache method uses two types of caches: local and global. The local cache stores locally relevant entities based on an LRU (Least Recently Used) eviction policy, while the global cache handles globally relevant entities through a LFU (Least Frequently Used) eviction strategy. This approach allows the model to efficiently manage large amounts of data without requiring extensive computational resources or memory storage capabilities.

The performance evaluation demonstrates that the dual cache outperforms baseline models even when they use unbounded memory. Additionally, it significantly reduces cache misses compared to single cache approaches. Overall, the dual cache provides improved efficiency and cost-effectiveness over traditional methods, making it suitable for handling complex tasks like neural coreference resolution in natural language processing applications.</sample>
    <sample id="160">Unordered multisets of tokens that will appear in the output.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">The best alignment method for DEPLAIN is the method of MASSalign.</sample>
    <sample id="164">Weakly supervised learning is beneficial because it allows for the training of neural networks on weakly labeled data, which are much cheaper to obtain than human annotations. This approach helps in reducing annotation costs and enables the development of models that can generalize well even when dealing with noisy labels.</sample>
    <sample id="165">Wenting Zhao, a PhD student at Cornell University, presented the paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations" in which he introduced an unsupervised learning method called LiPoR. The goal of abductive reasoning is to identify plausible explanations that can bridge information gaps between context and outcome. Current approaches predominantly rely on supervised methods but require annotation of plausible explanations, which can be noisy and subjective. Therefore, it was necessary to develop an approach without supervision regarding plausibility. In this case, they developed an objective function based on marginal likelihoods by maximizing the probability of outcomes given contexts while preferring some explanations over others through mutual exclusivity among them.</sample>
    <sample id="166">Hello everyone. I'm Yunxin from Harbin Institute of Technology, Shenzhen. It's my pleasure to introduce our new work, "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text". This image retrieval from linguistically complex text is a challenging image text reasoning task because these images are highly similar and the description is long. Typical methods, such as visual language models, perform well on image sentence retrieval tasks, but their performance drops drastically when confronted with linguistically complex text. To address this issue, we get inspired by the Divide-and-Conquer strategy and Dual-Process Theory. Divide-and-Conquer is the strategy of solving a large problem by breaking the problem into smaller ones, solving the sub-problems, and combining them to get the desired output. Dual-Process Theory for human thinking: human brains contain two thinking systems. System 1 performs analogical reasoning, while System 2 is capable of abstract logical reasoning, well-suitable for complex reasoning problems. Pre-trained visual language models focus on analogical reasoning as System 1, based on the analysis of deep learning networks. When confronted with complex tasks, their performance drops drastically. We may need a logical reasoning System 2 to perform this complex retrieval task via a logical operation. Combining the advantages of System 1 and System 2 may be a significant way for complex reasoning, and they can be integrated with the Divide-and-Conquer Strategy. The first model for our proposed method is the Proposition Generator. It aims to decompose the complex proposition text into representations of a simple proposition. For explaining what simple propositions represent, we also use the decoder of BART to generate the corresponding sentences. System 1, which is the Visual-Linguistic Interactor, aims to perform the visual-propositions’ information interaction, resembles the System 1. The outputs of this module are matching scores of propositions and images, and their reasoning states. Then we introduce the Neural-Symbolic Reasoner as a System 2. It is responsible for integrating the reasoning states and results of simple propositions to obtain the final solution of a complex proposition on images. It consists of the negation executor and conjunction operation. Negation executor aims to gain the negational reasoning state of positive proposition. Conjunction operation is responsible for obtaining the inference results based on joint positive and negational reasoning states. Finally, we combine the inference results of System 1 and System 2 to gain the final solution. By doing so, the whole system utilizes the advantages of the analogical inference System 1 and logical reasoning System 2. Here we present two tables of our experimental results. We can see that the proposed method, NDCR, outperforms other baselines, and in the right part, the abolition experiments on the testing set also verifies the effectiveness of each module. Here we present two cases to further check the performance of the proposed method. We can see that our proposed method can present the inference states and inference results in the middle step. So the proposed method is processing interoperably. To conclude, we present some suggestions. First, neural symbolic calculation may be a worthwhile approach to improve the compositional reasoning and planning of large language models. Divide-and-Conquer is similar to the self-asking chain-of-the-thought, aiming to decompose the complex reasoning into simple problems and construct a reasoning path. Both are effective for solving complex problems. Dual-Process Theory could be integrated with the Divide-and-Conquer. Ok, thanks everyone.</sample>
    <sample id="167">The documents in DEPLAIN-web were aligned both manually and with automatic alignment methods.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting data from Reuters News in 2020 and annotating them with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">The paper "Prompting PaLM for Translation: Assessing Strategies and Performance" presents the first systematic study of large language model prompting for machine translation. The authors, from Google Translate, evaluated the performance of their 540 billion-parameter large language model (PaLM) using state-of-the-art neural MT metrics and expert-based human evaluation results. They found that prompt selection strategies have a significant impact on LLMs' translation performance, with one-shot prompting showing more than a one BLEURT point difference in accuracy compared to zero-shot prompting. In five-shot prompting, there is nearly no difference between different forms of prompting as long as the examples are high-quality translations. The experiments showed that PaLM's fluency is comparable to state-of-the-art systems but its main issue lies in accuracy, particularly omission errors where it chooses to produce better-sounding translations by dropping parts of the source sentence.</sample>
    <sample id="171">Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability.</sample>
    <sample id="172">No, they are not.</sample>
    <sample id="174">Thea, one of the co-authors of the paper "ArgAnalysis35K : A large-scale dataset for Argument Quality Analysis," explains why this dataset is unique from other datasets on a similar topic. Thea discusses argument quality analysis and highlights problems with current datasets, such as lack of quality due to crowdsourcing platforms, lack of diversity in motions sourced, insufficient depth in explaining arguments' truthfulness, and having a motion associated with every single argument that exists. ArgAnalysis35K addresses these issues by being the largest dataset with high-quality arguments (85% sourced from expert debaters), diverse range of arguments based on 24 themes, introduction of an element called 'analysis,' instance-based annotator reliability, and relevance model assigning scores between 0-1 for each theme's relevancy. This comprehensive approach aims to provide better scoring accuracy through more reliable annotations while capturing relevant information about different topics within debates.</sample>
    <sample id="175">The method deals with the ambiguity of permutations by inducing the alignment as part of training. Sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined by evaluating its performance on different demographic groups or political leanings. The study found that language models with different political leanings perform differently in detecting hate speech and misinformation, indicating potential biases towards certain social categories.</sample>
    <sample id="177">The name of the speaker is Yanis Labrak.</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">Theory of mind is the ability to reason about the mental state of others. It is traditionally measured both in humans and in language models in reading comprehension tasks involving multiple characters. A great way of probing the understanding is through false-belief questions. These are situations where reality may not match the belief of certain story characters. Let's look at a classic example of a theory of mind test, the Sally-Anne test. In the story, Alice and Bob are in a room where there is a basket and a box. Alice puts an apple in the basket and then leaves the room. Bob then moves the apple to the box. Then the human or the language model are probed with a bunch of questions. For example, where will Bob search for the apple? In the box? Where does Bob think that Alice will search for the apple when she comes back? In the basket. So questions may be classified as first-order or second-order depending on whose mental state you are asking about. First-order is asking about a character's mental state, and second-order is asking about a character's estimation of another character's mental state. True-belief questions are those where the expected answer matches the true location of the object, and false-belief, otherwise. We know that large language models still perform poorly on false-belief tasks, for example ChatGPT or GPT-3. So our research question is, "How can we improve Theory of Mind reasoning skills in Large Language Models?" We present SymbolicToM, an inference-time method to improve Theory of Mind reasoning skills in Large Language Models using explicit graphical representations. SymbolicToM uses several graphical representations, since mental states cannot be represented with a single graph. For example, on the left we see a representation of what Bob believes is the current world state, and on the right we see a representation of what Bob thinks that Alice believes is the current world state. We call these graphs BBob and BBob,Alice. In general, we compute these graphs for all combinations of characters p₁ through pₘ up to a predefined maximum Theory of Mind level m. Graphs are computed using an inference-time algorithm that leverages off-the-shelf NLI and OpenIE models. See the paper for details. Having pre-computed these graphical representations for a given story, we can efficiently answer any given question. For example, let's say we want to answer, "Where does Alice think that Bob will search for the apple?" We first detect the entities in the question. Then we retrieve the appropriate belief graph and perform recursion over the question so that now we're asking a factual question over the graph. Then we retrieve the sentences captured by the graph and finally take the sentences plus the factual question and feed it to a language model to get the final answer. So now let's look at the experiments. We test our method with a myriad of LLMs and compare it against supervised baselines, specifically a fine-tuned GPT-3 model and Textual Time Travel, which is a model specifically designed for Theory of Mind reasoning. We analyze in-domain performance in the well-known ToMi dataset, and we evaluate robustness with two out-of-domain setups that we designed. Let's look at the in-domain performance results for second-order false-belief questions, and all others can be found in the paper. The X axis represents the out-of-the-box LLM performance, and the Y axis represents the performance using SymbolicToM for a given LLM. Then at any point in the upper left triangle means that using SymbolicToM increases the performance versus not using it. We see performance gains across the board, for example, 65 accuracy points gained for GPT3-Davinci, 67 for Macaw, 51 for Flan-T5-XXL, among many others. For testing our method's generalization capabilities, we design two new datasets, both modifying ToMi's benchmark. We test for storage structure generalization by, for example, concatenating two stories and asking half the time about the first story and half the time about the second one. This is the D₁ dataset, and we design two more, D₂ and D₃. We also test for linguistic generalization, which is our second dataset called ParaphrasedToMi, generating a dataset that has more linguistic diversity. This is important since ToMi is generated automatically with only one way of phrasing each sentence. For the story structure generalization datasets, we observed that supervised models heavily degrade the performance on the three datasets we created, for example showing around 50% performance in the dataset D₁ that I just described. On the other hand, using SymbolicToM still shows significant gains for all models, allowing stronger models like GPT-4 to fully solve the datasets, giving, for example, a 42 point accuracy boost for dataset D₁ . So in conclusion, we introduced SymboliToM, a plug-and-play method to improve Theory of Mind reasoning skills in large language models. It is an inference-time algorithm which avoids overfitting risk. It uses explicit graphical symbolic representations, which yields more interpretable reasoning, and SymbolicToM dramatically improves out-of-the-box LLM performance, outperforming supervised approaches on out-of-domain story understanding and remaining beneficial on the new linguistic diversity dataset, ParaphrasedToMi. For more details, please refer to the paper, and don't hesitate to reach out to chat. Thank you so much for listening.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">Constrained language planning, which imposes different constraints on the goals of planning, remains under-studied. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we define the problem of constrained language planning and evaluate and improve the ability of large language models for it. We extend abstract goals with multi-faceted constraints using a human-in-the-loop data acquisition method called InstructGPT. The table reports overall accuracy results from 100 specific goals generated by three state-of-the-art language models: GPT-3.5-turbo, GPT-4, and T5. All achieve unsatisfactory performance in generating scripts for these goals.
We conduct detailed analysis to investigate why learning models fail at planning for specific goals. Results show that semantic completeness is acceptable but faithfulness to constraints cannot be guaranteed. To address this issue, we propose an over-generate-then-filter method based on symbolic knowledge distillation. This involves converting scripts into embeddings and calculating cosine similarity scores as similarity measures between them. Our approach greatly improves generation quality compared to previous methods like GPT-3.5-turbo and GPT-4. With our dataset CoScript, consisting of 55,000 specific goals with corresponding scripts derived from large language models through symbolic knowledge distillation, smaller specialized models can surpass larger ones when properly trained on suitable datasets.</sample>
    <sample id="182">In the context of this paper, tropicalism refers to a trope that is often associated with Latina women. The authors analyze personas generated by language models and find that certain words are used to describe these personas, such as "vibrant" and "curvaceous". These descriptions connect to the concept of tropicalism, which has been historically linked to exoticization and othering of Latinx cultures and individuals. This analysis highlights how seemingly positive portrayals can actually reflect harmful patterns and contribute to stereotypes within society.</sample>
    <sample id="183">The authors used a study where they gave prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. This enabled direct comparison between their generated personas and the human written responses.</sample>
    <sample id="184">CXMI was used to measure context usage in this work.</sample>
    <sample id="185">DrBERT is a biomedical model in French based on RoBERTa and trained on NACHOS, which is data from the web. ChuBERT is also a clinical model but it's based on anonymized data obtained from the Nantes University Hospital data warehouse.</sample>
    <sample id="187">There are two authors involved in the paper.</sample>
    <sample id="188">Iterative transfer learning is a method of improving dissonance detection by iteratively fine-tuning on both tasks, CE and debate. This approach yields better zero-shot performance compared to other methods.</sample>
    <sample id="189">The goal of the dataset is to understand users’ language when they want to make a choice.</sample>
    <sample id="190">An attacker can extract model parameters through an EaaS by learning from the embedding and providing similar services.</sample>
    <sample id="191">There are three authors involved in the paper.</sample>
    <sample id="192">The speaker introduces their work, "CAME: Confidence-guided Adaptive Memory Efficient Optimization", which aims to design an optimizer that achieves fast convergence like traditional adaptive methods and low memory usage as in memory-efficient methods. The challenge is how to achieve both goals simultaneously.

The presentation begins with a brief introduction of non-negative matrix factorization (NMF), explaining its ability to reduce the memory requirements from O(mn) to O(m + n). This reduction can be applied to matrices V and W x H for NMF operations. However, Adafactor's use of NMF operation leads to erroneous updates during training deep neural networks, resulting in slow convergence compared to Adam due to existing errors.

To address this issue, two scenarios are introduced to demonstrate how different types of erroneous updates should be handled. In Figure (a), there is a large difference between momentum update mₜ and current update uₜ, indicating historical experience. For original Adafactor parameter, it contains high levels of errors affecting stability. If using row mₜ for optimization steps, direction will diverge increasingly from desired directions.

Inspired by these erroneous updates, the proposed approach decreases side effects caused by insecure updating while considering residual differences between predicted and generated updates. By calculating instability matrices Sₜ and applying square roots as denominators for mₜ, CAME takes optimized steps more adaptively than Adafactor parameters do.

Experiments on BookCorpus and English Wikipedia show significant improvements over Adam and Adafactor when pre-training three important large language models - BERT, GPT-2, and T5. Specifically, CAME increases validation accuracy about 3.4% relative to Adafactor at similar batch sizes. Additionally, larger batches benefit further from reduced memory costs associated with increased model size.

In conclusion, inspired by erroneous updates present in some memory-efficient optimizers, CAME proposes confidence-guided memory-efficient optimization guided by residuals between predicted and actual updates. Extensive experiments confirm CAME's effectiveness across various tasks related to large language modeling.</sample>
    <sample id="193">To create the initial dataset, 10 annotators were used.</sample>
    <sample id="194">The authors of the paper are affiliated with Carnegie Mellon University, the University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">The speaker introduces their work, "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering". They explain that recent work in XQA can be grouped into two directions: neuro-symbolic methods and decompose-based methods. Neuro-symbolic methods translate natural language questions into formal representations such as SPARQL, while decompose-based methods generate natural language intermediate steps that lead to the final answer by decomposing a complex question into sub-questions and chain-of-thoughts. However, both approaches have limitations. For example, neuro-symbolic methods only execute on structured KBs, which are incomplete, limiting recall of answers. On the other hand, decompose-based methods rely solely on free-text corpora, making it difficult due to the diversity of natural language. The speaker mentions that integrating knowledge from heterogeneous sources is crucial for QA, especially when answering complex questions intuitively. Leveraging question decomposition is considered promising but faces challenges related to determining granularity and finding optimal solutions among various possibilities. To address these issues, they propose a novel framework called RoHT (Reasoning over Hierarchical Question Decomposition Tree). This framework consists of two stages: building the Hierarchical Question Decomposition Tree (HQDT) and probabilistic reasoning over HQDT using a combination of a knowledge base and text corpus at different levels.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">The state-of-the-art models in dialogue systems are not specified.</sample>
    <sample id="198">Because large language models have longer and longer context windows, it is crucial to evaluate their acceptability throughout the context window.</sample>
    <sample id="199">Yes, training in multilingual fashion caused performance drop compared to monolingual English model.</sample>
    <sample id="200">No, the annotators don't know about the entities in advance.</sample>
    <sample id="201">The MT metrics used for the evaluation were BLEURT and human evaluation using the MQM framework.</sample>
    <sample id="202">No, the regress in generalization does not impact specific NER types.</sample>
    <sample id="203">Positionality in NLP matters because it can lead to systematic performance differences of technology between populations, resulting in biases that affect the fairness and accuracy of AI systems.</sample>
    <sample id="204">BLOOM was fine-tuned with adapters.</sample>
    <sample id="205">Shangbin presented their work "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models" at the ACL 2023 conference. The study investigates how political biases in language models arise from pretraining data and its impact on downstream tasks, such as hate speech detection and fake news detection. They found that different language models have varying political leanings, with GPT-4 being the most liberal model among them all. By further pretraining language models on partisan corpora separated into news and social media, they observed shifts in ideological coordinates corresponding to each corpus's political leaning. Additionally, they discovered that language models can pick up societal polarization trends over time. To evaluate these findings, Shangbin used a controlled experiment by dividing pretraining corpora into those before and after the presidency of Donald Trump (45th president). Preliminary results showed that post-2017 language models generally had a more pronounced political bias compared to earlier versions. These observations highlight the pressing need for addressing fairness issues resulting from language model political leanings, especially when deploying them in real-world applications like social media platforms where minority groups might be marginalized or targeted by hate speech.</sample>
    <sample id="206">They use a model that is fine-tuned on both the CE and debate tasks.</sample>
    <sample id="207">The latest test sets were used to assess the PaLM capabilities.</sample>
    <sample id="208">The authors proposed three recommendations for model owners.</sample>
    <sample id="209">The gain of the proposed method over the strongest baseline is 10.4%.</sample>
    <sample id="210">The speaker's name is Shuheng.</sample>
    <sample id="211">Yes, the results and dataset in their paper can be used as a base benchmark for automatic text simplification.</sample>
    <sample id="212">They experiment with one smaller model in the paper, T5.</sample>
    <sample id="213">OFA is used as the base model for investigating multi-modal instruction tuning.</sample>
    <sample id="215">The talk is about the Dependency Structure of Coordination. Different theories and corpus approaches assume different dependency structures for coordination, such as universal dependencies, Igor Mel'čuk's meaning text theory, Prague approach, and Hudson's Word Grammar. The aim of this paper is to argue in favor of symmetric structures of coordination against asymmetric ones by using the principle of dependency length minimization. This argument suggests that shorter dependencies are preferred over longer ones when direct objects are very heavy or long. The statistics extracted from the enhanced version of the Penn Treebank confirm this observation, showing a tendency for left conjuncts to be shorter when there is no external governor present. However, when the governor is on the right, this effect disappears.</sample>
    <sample id="217">The work presented is titled "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation". The authors, Weihao Zeng, Lulu Zhao, and Keqing He from the Beijing University of Posts and Telecommunications, introduce their research in seven key aspects. Firstly, they discuss motivations for exploring compositional generation for multi-attribute controllable dialogue generation due to limitations observed with existing methods that focus on single attributes or use specific labels without considering continuous attributes. Secondly, they highlight a limitation where current models lack generative capabilities when it comes to handling multiple attributes simultaneously. Thirdly, they propose DCG (Disentangled Controllable Generation), which learns attribute concepts using disentanglement loss to separate different combinations of attributes.

Fourthly, they introduce MAE (Multi-Attribute Evaluation) as a unified reference-free evaluation framework suitable for various granularities of attributes. Fifthly, two benchmarks are established through experiments demonstrating the effectiveness of both DCG and MAE. Sixthly, an overview of their model architecture based on DialoGPT is provided along with details about prompt design strategies such as attribute-oriented prompts and task-oriented prompts aimed at improving text equality while maintaining control signals. Lastly, results show superior performance by DCG over other baselines across several metrics including E-ACC, A-ACC, BLEU scores, and correlation coefficients indicating its robustness against biases introduced by handcrafted patterns. This study confirms the efficacy of transforming seen attributes into unseen combinations via disentanglement learning techniques within PLMs like BART.</sample>
    <sample id="218">The authors of the paper are affiliated with Google Translate.</sample>
    <sample id="219">The speaker, Jia-Huei Ju from Academia Sinica, presents their work on uncovering financial signals in financial reports. They discuss the background of financial report analysis and define a highlighting task to compare and contrast company reports between consecutive years. The proposed pipeline includes document segmentation (Stage 0), relation recognition (Stage 1), out-of-domain fine-tuning with eSNLI data (Stage 2), and domain-specific fine-tuning using revised pairs (Stage 2+). The evaluation dataset consists of eSNLI pairs and the FINAL dataset released by the speakers. Their model achieves high performance metrics, including precision over recall and PCC correlation. Future works include improving effectiveness, adding more features, or incorporating techniques from information retrieval for enhanced application.</sample>
    <sample id="220">The affiliations of the authors are Stony Brook University, University of California San Diego, and University of Illinois at Urbana-Champaign.</sample>
    <sample id="221">The paper analyzed the following language pairs: English-German, Spanish-English, and Japanese-English.</sample>
    <sample id="222">The speaker discusses the challenges and interventions for domain adaptation in open-domain question answering. They introduce a scenario where a model trained on Wikipedia is used to answer questions from different domains, such as biomedical topics. The main contributions of their work include investigating data interventions that enable out-of-domain generalization, identifying types of dataset shifts, and determining effective data interventions based on specific shift types.

The setup involves training models using a general-purpose source domain (Wikipedia) and testing them across seven target datasets spanning six different domains. Two methods are explored: zero-shot and few-shot techniques. In zero-shot scenarios, no examples from the target domain are available; instead, interactions among variables like question, answer, and context are controlled by varying one variable while keeping others fixed. This helps understand how changes impact model learning.

In few-shot cases, examples from the target domain prompt large language models to generate more relevant samples. For instance, given a passage about malnourishment near stone mines, the model suggests facts related to it which can be converted into cloze-style questions adapted for the retriever and reader models. Results show improvements in retrieval performance by 8% on average and reading performance by 11% on average after applying these adaptations.

The second part focuses on understanding the nature of compatibility between the source model and target domain through existing data shift taxonomy. Compatibility measures involve computing likelihoods assigned by the source model's retriever and reader to contexts and answers respectively within a set of example question/answer and context triples from the target dataset. These values help map each target dataset onto a grid representing various levels of shift – full shift if both retriever and reader exhibit incompatibility with the target domain; concept or covariate shift when only one component shows incompatibility; and no shift if there’s complete compatibility.

Based on this mapping, they find that all target sets respond well to few-shot adaptations due to limited availability of target-specific examples but also note effectiveness varies depending upon the type of shift present in the dataset. Their experiments improve reader performance up to 24%.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">Long-mBART, mBART</sample>
    <sample id="225">53 tasks from 9 groups are used for training and the entire common sense reasoning group is reserved for testing.</sample>
    <sample id="226">There are three authors involved in the paper: Regina Stodden, Omar, and another author whose name is not mentioned.</sample>
    <sample id="227">We all know that language models have achieved great success recently, providing a general solution to many different NLP tasks. Here we want to ask what's missing in current language models’ research. We think the answer to this question is grounded language understanding, which basically means grounding a natural language expression into something that can be executed over a specific target environment, which is also referred as a plan or a program, so depending on whether it's described by a program language. There are many applications to grounded language understanding, such as smart assistants like Siri and Alexa. There's also semantic search on Google and querying a medical database using natural language, and also domestic robots that follow natural language instructions. So all these applications involve mapping a natural language expression onto some representation in a specific environment. For example, when querying a medical database using natural language, we need to map the query onto a SQL query or SQL problem, while for the robotic scenario, we need to map the natural language instructions into a sequence of actions the robot can take. But why is grounded language understanding challenging? The main reason is the lack of grounding during pre-training. We know that most of the language models, including the recent large language models, are mostly pre-trained with textual corpus and without grounding. So this is also related to the octopus test, and the gap between the pre-training and the downstream application makes the task of grounded language understanding particularly challenging for language models. Existing research on different grounded language understanding tasks typically uses language models to directly generate a plan via autoregressive decoding that's capitalizing on the generation ability of language models. However, the main issue is that the generated plan or program may not always be grammatical or valid. For example, in the scenario of knowledge-based question answering, it's very likely a KB query generated by T5 is not executable over the KB, or maybe returns an empty set of answer entities. But in our work, we propose a novel framework for grounded language understanding where we let language models focus on discrimination instead of generation. We argue that discrimination is something much easier for language models to excel. Specifically in our framework, a symbolic agent interacts with the environment and proposes candidate plans, while a language model is used only to score and rank the candidates proposed by the symbolic agent. In this case, the language model does not need to handle the validity and grammar of the target plan by itself because it does not need to do the generation by itself. For more details, please refer to our paper, and we always welcome offline discussions. We name our framework after Pangu, who is the primordial being in Chinese mythology who separates heaven and earth, just like how our framework separates the newer world and the symbolic world. We instantiate our idea on knowledge-based question answering, which is a typical scenario with a massive, heterogeneous environment for grounded language understanding. Note that our framework is generic and is not specific to knowledge-based question answering. Here we just use it as a representative testbed. We experiment with language models of different natures, including BERT, T5, and also large language models like Codex. Also, we experiment with both fine-tuning and in-context learning. Pangu achieves outstanding performance across all settings. Here's the results for fine-tuning, and also here's the results for in-context learning. In particular, Pangu also demonstrates strong sample efficiency. For example, when using Codex with in-context learning, Pangu can achieve an accuracy of over 50% of GRAIL query with only one demo example, significantly outperforming all other settings. And also for fine-tuning experiments, Pangu consistently outperforms a baseline model, ArcaneQA, in terms of sample efficiency when using different language models. We also have an interesting finding that may explain Pangu's strong generalizability under non-i.i.d. settings, as specifically we observe that autoregressive models like ArcaneQA tend to overfit seen structures during training. While for Pangu, we can see that for both the seen structures and unseen structures, the distributions of probability are almost the same, so this might be a signal for Pangu's strong robustness under the non-i.i.d setting. Here comes the most important message and the one-sentence takeaway of our work. For grounded language understanding, generation may not be a good idea. Instead, discrimination might be a much better strategy of using language models for grounded language understanding. We are always open to different forms of discussions and collaborations. And also, we are eager to hear your thoughts on our work, and we appreciate your time and your attention. Thanks a lot.</sample>
    <sample id="228">The authors conducted experiments on four datasets: AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya presented a joint work with Henning Wachsmuth on detecting improvable claims for argumentative writing support. The presentation focused on the importance of text revisions in professional writing and how they can influence the effect of a message on an audience. Gabriella introduced two new tasks: Suboptimal-Claim detection, where one decides whether a claim needs revisions or is phrased optimally, and Claim Improvement Suggestion, which involves selecting types of quality issues that should be improved when revising a claim.

The paper explored challenges arising from working with revision-based data, such as representativity and reliability, model complexity and architecture, dependency on contextual information, and topical and user bias. To address these challenges, the authors compiled datasets from collaborative online debate platforms like Kialo to extract implicit patterns related to the quality of arguments. They also experimented with different models and architectures to determine their impact on performance.

The findings suggest that revision-based data can effectively assist in identifying suboptimal claims by modeling the distance between claimed versions. Additionally, it was found that context plays a significant role depending on both task requirements and specific quality issues within texts. For more detailed insights and results, readers are encouraged to refer to the full paper.</sample>
    <sample id="231">NACHOS is a data set of medical crawled data from the web.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Hi, I'm Sara Papi from the University of Trento and Foundazione Bruno Kessler and I will briefly introduce the "Attention as a Guide for Simultaneous Speech Translation" paper, that is a joint work with Matteo Negri and Marco Turchi. What is simultaneous speech translation? Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable. For example, if we receive a speech chunk containing "I'm going to talk about..." and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk. If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted. If we look at the main results of EDAtt, we'll plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality, and average lagging that is the latency measure, and we also consider the computational aware average lagging that accounts for the model's computational times to predict the output. So we want our curves to be as high as possible on this plot. But also we want that they are shifted on the left. And we compare with popular strategies that are also applied to offline models that are the Wait-k strategy and the Local Agreement. And we compare also with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. These are all the results of the simultaneous speech translation strategy on German. And we see that it outperforms all the strategies applied to offline models since the curves are shifted over the left. And we also see that if we consider the actual elapsed time or the computational-aware time, that is the fastest strategy. If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.</sample>
    <sample id="234">The prompting strategy has a significant impact on the results. In their experiments, David Vilar and his colleagues found that using one-shot prompting with two different prompts for each sentence resulted in more than 1 BLEURT point difference between them (516 out of 1000 sentences). This indicates that selecting an appropriate prompting strategy is crucial to achieve optimal performance when using large language models like PaLM for translation tasks.</sample>
    <sample id="235">The affiliations of the authors are: Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig from Google Research; and Kayo Yin from the University of Toronto.</sample>
    <sample id="236">The 5 expert-written instructions are not specified in the transcript.</sample>
    <sample id="237">The authors propose a diagnostic test suite for knowledge integration, introducing a coreference resolution task to probe the ability of models to draw on knowledge from different sources.</sample>
    <sample id="238">Yebowen Hu from the University of Central Florida presented a new benchmark dataset, MeetingBank. The purpose is to develop summarization technologies for different reading domains in meetings. To create this dataset, they addressed two major challenges: high-quality meeting summaries or scores and locating trustworthy resources for public meetings. They managed to create a repository of City Council meetings with transcripts, reference summary, and other URLs containing useful resources.

The data collection process involves using Speechmatics API to convert audio data into transcripts, identifying the type and date of the meeting on the website, obtaining corresponding meeting segments including start and end time, aligning timestamps, and pairing them with previous extracted summaries.
MeetingBank includes 1,366 City Council meetings and nearly 7,000 instances. Data analysis measures coverage and density levels of abstraction in meeting summaries by comparing source transcripts against generated summaries. Coverage score ranges between 0.7-0.9 indicating that most summaries include verbatim points over abstraction.

For model evaluation, ten systems were tested - five extractive (Oracle, LEAD, LexRank, TextRank) and five abstractive (BART-Large, Pagasus, Longformer, DialogLM, HMNet). Results show that GPT-3 achieves the highest ROUGE-2 score among abstractive models but performs less impressively regarding informativeness and factuality according to human assessment criteria such as BERTScore, MoverScore, question/answering-based metrics, etc.

In conclusion, Yebowen Hu encourages everyone to use MeetingBank as it serves both research purposes and provides insights into decision-making processes within city councils.</sample>
    <sample id="241">Hi everyone. I'm Ethan, and today I'm going to discuss our paper, "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments". This was a joint work with Yang Chen, Wei Xu, and Alan Ritter at Georgia Tech. There have been many proposed approaches for automatically detecting misinformation on social media platforms. However, all of these approaches generally fall short on two key marks. Firstly, these systems are often unrealistically evaluated. For example, datasets used for the evaluation of these systems are often retrospectively constructed, instead of using live data. Relatedly, there is the possibility of leaked counter-evidence, which a recent work found was a problem with many of these systems. For example, for this evidence-based fact-checking approach, we see that while in this first case, counter evidence could clearly be found on a well-known data source like Wikipedia. For the second, more realistic case of misinformation, counter-evidence could only exist after the claim had been debunked publicly, at which case the system is not useful. Particularly, solving this deficiency is critical given the recent rise of misinformation in order to detect rumorous claims early, before they have the chance to spread. Secondly, these methods are often not human-centric. Specifically, they are not representative of the real scale or noisiness of these platforms, which necessitate the involvement of human content moderators. These methods tend to either completely cut out humans from the misinformation detection process or relegate them to the final determination step, instead of seeking input from them throughout the intersection process. We propose an evaluation framework for the development of systems that address these deficiencies. Such systems are end-to-end as they go from the noisiness of raw tweets on Twitter to actionable outputs used by humans. They have well-integrated human feedback as humans are involved at various stages of the process. The system itself is meant to be a system rather than authoritative. We also can concretely implement and evaluate our novel workflow for COVID-19 treatment misinformation. Our concretely instantiated system has two main components. The first component deals with the detection of misleading claims. Here, the system takes raw tweets as input from the wild and outputs discovered check-worthy claims. First, keyword filtering is used to filter relevant tweets. A T5 model is then trained for question answering and is used for claim extraction. Specifically, the model is trained to answer the question "What is the mentioned COVID-19 cure?" given the context of a particular tweet. These treatments are used to form claims of the type; for example, Ivermectin is effective in screening COVID-19. The claims are then ranked by trendiness, which is essentially a statistical popularity on a given day using Fisher's Exact Test, before they're provided to a human for verification. The second component focuses on policy violation verification, but the goal is to use the verified list of misinformed claims in the first stage to flag social media policy violations. A BERT-based stance classification model is used to determine the author's stance in the tweet towards the unapproved treatment. For example, in the first tweet, the author clearly believes that Ivermectin can be used to effectively treat COVID-19. Such supporting stance tweets are then flagged for human review. Now we move on to the evaluation of our human-in-the-loop workflow. Through some of the discussion of loop counter-evidence, early detection is an important task to get right, as humans can act more effectively to stem the spread of misinformation when they know about rumors earlier. In our case, we operationalize early detection as the detection of an unapproved treatment before its first appearance in the debunking news article, which is discovered and annotated by humans. In this figure, you can see some selected examples of unapproved treatments that are detected by our system before they were debunked in the news. We believe that our definition of early detection captures the real utility of early detection systems. We also evaluated the efficacy of the policy violation verification portion of our workflow. Specifically, in our evaluation, humans assign a Likert scale value as to if a tweet is a violation of Twitter's policies surrounding COVID-19 misinformation. The histogram of scores are shown below, where scores of four or five indicate most likely are clearly violating Twitter's policies. At a high level, we find that our system has a position of 65% with reguards to policy violation detection. Additionally, to understand the human workload of such systems, the computer metric for the number of policy violations that can be confirmed per human hours worked, which includes both the claim verification and policy violation verification steps. For our instantiated system, we find that 124.2 policy violations can be detected per human hour worked. To conclude, our framework more realistically captures the complex interplay between systems and human content moderators in a realistic end-to-end setting. We also hope that our work motivates the development of future human-in-the-loop misinformation detection systems, which can now be evaluated consistently with the methods that we present in this paper. Finally, our work valuably provides outsiders an out-of-industry look at the development and evaluation of misinformation detection systems. Thank you so much for listening, and we hope to answer any questions you may have at the conference.</sample>
    <sample id="242">The common evaluation methods for dialogue systems are Likert ratings on the turn-level, Likert ratings on the dialogue-level, and pairwise comparisons.</sample>
    <sample id="243">There are 5 authors involved in the paper.</sample>
    <sample id="244">In the example with Servin and Kea, background knowledge about what a judge does is needed.</sample>
    <sample id="245">The presentation is about a study on Amazon Mechanical Turk (MTurk) workers and their performance in summarization tasks. The researchers conducted an experiment to evaluate the quality of MTurk workers by designing specific qualification, endurance, and reference-based tasks. They found that pre-task filtering using their pipeline can achieve high agreement at a lower cost compared to other methods like MACE or CloudResearch. However, there are limitations such as only testing English summarization on MTurk platform and no guarantee for training correctness. In conclusion, this work provides best practices for high-agreement annotations at large scale and lower cost while avoiding resource waste on discarded annotations.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">Jiho Kim from KAIST AI presented their paper titled "FACTKG: Fact Verification via Reasoning on Knowledge Graphs". They introduced a new task, Knowledge Graph-Based Fact Verification. KG is valuable due to its reliability in fact verification and practicality for tasks requiring consistency checks between KG and natural language. To this end, they proposed the FactKG dataset, which utilizes DBpedia as evidence with claims in both written and colloquial styles. The dataset includes five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. Two methods were used for creating colloquial style claims: the colloquial style transfer model (proposed by Kim et al.) and presupposition templates. Baselines include Claim Only baselines using only claims without graph evidence and GEAR models utilizing correct evidence. Results show that all baselines outperform the majority class baseline (51%), with the GEAR model performing best when using graph evidence.</sample>
    <sample id="248">No, the annotators for NLPositionality are not balanced in regard to each demographic.</sample>
    <sample id="249">To perturb the sentences in the acceptable domain, they added noise to preserve relevant structure.</sample>
    <sample id="250">Dimensional evaluation is a strategy for evaluating multiple aspects of chat quality to understand the strengths and weaknesses of the model on a finer-grained level.</sample>
    <sample id="251">University of Science and Technology of China</sample>
    <sample id="252">The presentation is about "U-CREAT: Unsupervised Case Retrieval using Events extrAcT" by Sai Kiran Tanikella, Abhinav Joshi, Akshat Sharma, and Ashutosh Modi. The Prior Case Retrieval Task involves retrieving relevant candidates from a candidate pool for legal documents. They have contributed the IL-PCR dataset (Indian Legal Prior Case Retrieval Dataset) with 7,070 cases containing an average of 6.775 citations per query document. This provides a comprehensive test bed for assessing PCR algorithms' performance.

Their second contribution is the U-CREAT pipeline that leverages unsupervised learning techniques and introduces an event-based approach for PCR tasks without requiring law or demographic-specific tuning. Event extraction plays a crucial role in their work as it helps represent case documents as collections of events based on dependency parsing technique using spacing.

They conducted experiments using various models like count-based, transformer-based, and event-based to validate and compare their performance on the PCR task. Their results show that the method using Event Filtered Documents outperforms all other methods significantly. In terms of inference time and F1 scores, event-based models perform better than others.</sample>
    <sample id="253">The paper presents a new approach to detect signs of mental disorders in social media posts. The method is called DisorBERT, which uses double domain adaptation and guided masking techniques on the BERT language model. This allows for better detection of mental health issues by focusing on specific words related to these conditions.

The authors explain that there are many types of mental disorders, such as depression or PTSD, but they can be difficult to identify due to their subjective nature. Social media provides an opportunity to study how people express themselves when facing difficulties, making it possible to analyze large amounts of data quickly.

To train DisorBERT, the researchers used Reddit conversations about mental health alongside general knowledge from a lexicon. They found that this combination improved the model's ability to recognize relevant terms associated with mental disorders compared to using only Wikipedia text.

The results show promising performance improvements over other baseline models like MentalBERT. However, more work needs to be done before applying this technology at scale, including exploring different lexical resources and incorporating clinical data into training processes.</sample>
    <sample id="254">The document-level relation extraction is aimed to extract relations among entities in a document. It can be seen as this figure. Previous methods rely on the large-scale human-annotated corpora, which is time-consuming and labor-intensive. So recent work leveraged distantly supervised data to pretrain the document-level relation extraction models for better performance. As we know, these data contain various noise levels. Current efforts to alleviate the noise problem by using pseudo labels. However, these kinds of methods still persist the risk of noise induction by false-positive pseudo labels, as shown in this figure. When we rely just on the pseudo label, we will obtain an extra false relation, "composer," and lose the correct relation, "place of birth". So how to mitigate the noise caused by the pseudo labels is still a challenge. In this paper, we propose a document-level relation distant extraction framework with uncertainty-guided label denoising to improve the label quality of DS data. This is the overview of our framework. We first train a pre-denoising DocRE model with both DS and human-annotated data to generate the pseudo labels. Since false pseudo labels are inevitable, we introduce uncertainty estimation to determine whether model prediction can be trusted or not. Considering there might be multiple relations between an entity pair, we propose an instance-level uncertainty estimation method to capture uncertainty scores for overlapping relations. We also designed a re-labeling strategy with dynamic class uncertainty thresholds and a multi-phase training strategy to further boost the performance. Uncertainty estimation is an important technology for misclassification detection, out-of-distribution instance detection, and active learning. In order to model the uncertainty in pre-denoising DocRE model, we introduce the Monte Carlo dropout technology in the DocRE task. This method requires multiple stochastic forward-pass predictions with activated dropout to capture the model uncertainty. Previous works based on MC dropout calculate the uncertainty score of the model prediction as this formulation. However, the previous method is not suitable for the overlapping relation problem, as shown in the left figure. There are two different types of relations between this entity pair. It is hard to separate the false positive pseudo label, "composer", and the correct positive pseudo label, "director" by the previous uncertainty estimate methods. To solve this issue, we modify the estimation process to obtain the instance-level uncertainty score for each positive pseudo label. The calculation can be seen in this formulation. We observe that the distribution of uncertainty score for each relation class is different. Moreover, it can be observed that frequent classes usually contain lower average uncertainty than the long-tail class. So we propose dynamic class uncertainty thresholds to filter pseudo labels with high uncertainty. Then we replace the original DS label with the pseudo label that contains a lower uncertainty score than its class uncertainty thresholds. In order to take full advantage of the DS data for further boosting the performance of DocRE model, we design a multi-phase training strategy to iteratively re-label the DS data, which is shown in this algorithm. We compare our framework with several strong baselines onto public datasets. As shown in this table, our framework outperforms the previous baselines on both two datasets. In conclusion, the main contribution of our work are summarized as those four points. The first one is our framework with uncertainty guided label denoising, which greatly improves the label quality of DS data. The second one is the instance-level uncertainty estimation method for overlapping relations. The third one is the iterative re-label strategy with dynamic class uncertainty thresholds for the long-tail problem. The last one is the great performance improvements.</sample>
    <sample id="255">The form of the prompting is important in cases with several short promptings.</sample>
    <sample id="257">Four state-of-the-art chat models</sample>
    <sample id="258">Chiang Cheng-Han discusses a new work titled "Can Large Language Models Be an Alternative to Human Evaluation?" He proposes using large language models for evaluating text quality in natural language processing tasks. The approach involves instructing the models with specific instructions and providing them samples to rate, aiming to replicate human evaluation processes without its drawbacks. Chiang references related works like G-Eval that also use large language models for similar evaluations.

The motivation behind this research is the instability of human evaluations and their difficulty in being reproduced consistently. To test the viability of large language model evaluation as an alternative, Chiang conducted experiments comparing ratings from these models against those provided by English teachers (human evaluators). Four different large language models were used: T0, InstructGPT (curie and davinci), and ChatGPT. Results showed that while some smaller models did not show significant preference between human-written and AI-generated stories, larger models such as Davinci and ChatGPT demonstrated clear preferences towards human-written texts.

To ensure meaningful results, ground-truth ratings were obtained through human evaluations following the same process as the proposed method. This comparison revealed that there are indeed instances where large language models can serve as effective alternatives to traditional human evaluations. For more detailed insights into individual rating agreements or changes in instruction wordings' impact on outcomes, one would need to refer directly to the paper or attend the presentation at ACL.</sample>
    <sample id="259">The speaker introduces their work "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" from Penn State University. They explain that semantic parsing is the task of building semantic representations of user queries, such as SQL or Lambda Calculus. The challenge lies in cross-lingual semantic parsing, which involves translating these queries across multiple natural languages into various meaning representations.

The existing models for this task are limited to specific tasks and applications, often focusing on a few languages with little coverage for others like Chinese or certain meaning representations like Lambda calculus. To address this gap, the researchers propose XSemPLR, a unified dataset designed for cross-lingual semantic parsing involving multiple natural languages and meanings.

They provide 9 datasets covering different domains, 5 types of semantic parsing tasks, 8 kinds of meanings (like SQL), and support up to 22 languages within 15 language families. For evaluation purposes, they consider six settings including Translate-Test where Google Translate API is used; Monolingual Model training on the same source and target languages; Monolingual Few-shot setting using only 10% data; Multilingual Model trained once but tested many times; and Cross-lingual Zero-shot and Few-shot transfer scenarios.

Their findings reveal interesting results about model performance based on pretraining strategies, multilingual training benefits, and comparative analysis between different approaches. Overall, XSemPLR serves as an important benchmark tool for advancing research in cross-lingual semantic parsing by providing comprehensive testing environments under diverse conditions.</sample>
    <sample id="260">There are 4 authors involved in the paper.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">There are 5 authors involved in the paper.</sample>
    <sample id="263">In-context learning is a popular method for using large language models, but it can be unstable due to various design choices. This instability arises from biases in the model's predictions caused by different types of label bias. Prior work has identified vanilla-label and context-label biases, but this paper introduces domain-label bias as an important new type of bias that affects text classification tasks. The authors propose a novel calibration method called domain-context calibration, which uses random words sampled from the task corpus to estimate the model's bias on each label name and then calibrates the original predictions accordingly. They conduct experiments with various models and datasets to demonstrate the effectiveness of their approach.</sample>
    <sample id="264">The speaker introduces their paper titled "TAVT: Towards Transferable Audio-Visual Text Generation" and explains the challenges of multimodal text generation tasks like audio-visual text generation due to varying construction conditions in different domains. They propose a novel task called Transferable Audio-Visual Text Generation, which aims to address multi-modal domain shifts such as visual style, audio energy, etc., by using a unified audio semantic space that aligns visual concepts across domains.

The framework presented consists of three components: an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning (DCLL). The first component maps different visual concepts into a unified auditory semantic space while addressing shifts in the semantic distribution. This is achieved through clustering audio clips on the Flickr dataset using k-means and introducing learnable tokens called visual prefixes for each audio cluster. These tokens are used to generate probability distributions in the audio space for reconstructing audio based on visual content queries.

The second component uses a transformer-based encoder and generator with alpha to evaluate modality contributions at time step t. Alpha-t is computed by measuring relevance between cross-attention from each modality and previous words. Finally, DCLL constructs fine-grained supervision signals from counterfactual results directly optimizing visual-textual alignment without relying on random negative samples.

Experimental results demonstrate TAVT's superior performance compared to SOTA approaches on both cross-datasets and cross-domain settings, even outperforming other methods when dealing with low-resource domains having limited labeled data. Ablation experiments also show the impact of audio features on expanded performance.</sample>
    <sample id="265">The name of the speaker is Vasudha.</sample>
    <sample id="266">The affiliations of the authors are: Adam Przepiórkowski from the University of Warsaw, Poland; and Tomasz Piotrowski from the University of Warsaw, Poland.</sample>
    <sample id="268">The most common errors of PaLM are omission errors.</sample>
    <sample id="270">Emory University and Amazon Alexa AI</sample>
    <sample id="271">In this paper, CFT stands for "Continued Fine-Tuning". It refers to a method where the model is fine-tuned on clean validation samples after initial training. This approach outperforms more complex WSL (Weakly Supervised Learning) methods in terms of performance and practicality according to the findings presented by the authors.</sample>
    <sample id="272">There are 7 authors involved in the paper.</sample>
    <sample id="274">The name of the speaker is Yusen Zhang.</sample>
    <sample id="276">Ananya and Vignesh presented their work on "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages". They focused on evaluating translation metrics in the direction of other languages, specifically studying five Indian languages belonging to two different language families. The team selected 200 sentences from the Flores dataset and generated multiple candidate translations using seven different translation models or APIs. Bilingual expert annotators were employed to evaluate these outputs by marking errors along with severity levels and providing an overall score.
The error types are borrowed from the MQM framework and can be broadly classified into accuracy or meaning-based errors, fluency errors, and special category errors. An example annotation is provided where a Google API output has three errors highlighted by the annotators. 
The table shows that recent MT models like NLLB or Indic Trans have fewer errors compared to older models like CVIT. The correlations between MQM-based scores and metric scores vary across languages but generally show better performance when only accuracy errors are annotated. COMET-metric variants exhibit higher overall correlations than overlap-based metrics such as chrF.
The results also demonstrate skewed ranges of scores among various metrics, making it challenging to interpret them effectively. However, fine-tuning the best-performing metric, COMET, using the MQM dataset improves its correlation values significantly. Finally, testing zero-shot ability on unseen languages reveals that IndicCOMET outperforms both COMET baselines most of the time.</sample>
    <sample id="277">Multiset Tagging and Latent Permutations</sample>
    <sample id="278">The "marked words" method draws upon the sociolinguistic concept of "markedness", which states that there is an unmarked default, and any group that differs from that default is linguistically marked.</sample>
    <sample id="279">The affiliations of the authors are: University of Washington, Microsoft Research, and OpenAI.</sample>
    <sample id="280">Shi Tao presented his work "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations". The goal of emotion regulation in conversations is to predict the emotion label of each utterance in a dialogue. Each utterance has its corresponding textual, audio, and visual modalities. To solve this problem, Shi Tao proposed MultiEMO, which consists of four key components: unimodel feature extraction, context modeling, multimodal fusion, and emotion classification. VisExtNet was introduced as a novel visual feature extractor that captures facial expressions without encoding redundant scene-related information. A multimodal fusion network called MultiAttn was designed with three components (MultiAttn-text, MultiAttn-audio, and MultiAttn-visual) to integrate one modality with complementary information from the other two modalities through stacked bidirectional multi-head cross-attention layers. Sample-Weighted Focal Contrastive Loss assigns higher importance to minority classes while making sample pairs mutually exclusive to maximize inter-class distances. Experimental results demonstrate that MultiEMO achieves state-of-the-art performances on MELD and IEMOCAP datasets.</sample>
    <sample id="281">The presentation is about a study that explores how much translation depends on context. The researchers measured the information gained from giving context to machine translation models and found patterns between words with high P-CXMI, such as dual pronouns in Arabic or proper nouns in Chinese. They also developed a benchmark for document-level translation using their findings and evaluated different commercial systems' performance based on this benchmark. DeepL was generally more accurate than Google Translate at document-level translation according to their results.</sample>
    <sample id="282">The paper presents a new approach to non-parallel text style transfer, focusing on the discourse level. The authors address challenges in imitating author linguistic choices at this level and propose their model named StoryTrans, which learns discourse representations from source texts and combines them with learnable style embeddings for target-style generation. They also introduce a training objective that reduces stylistic features derived from different texts while enhancing content preservation through two stages of training: an advisory stage using self-reconstruction loss and disentanglement loss, followed by a second stage focused on filling correct style-specific contents. Experiments demonstrate StyleTrans outperforms strong baselines in terms of style control and content preservation across Chinese and English datasets.</sample>
    <sample id="283">The name of the first mentioned symmetrical dependency structure is "Dependency Structure of Coordination".</sample>
    <sample id="284">Hello everyone. I'm Peng Tianshuo from Wuhan University. Today I will present my long paper for ACL's Main Conference 4,915 titled "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction". The current span-based UIE models involves identifying and labeling the span boundaries of the targets in the text, which overrelies on boundary positions of the annotated span. However, there is ambiguity in labeling the golden span boundary. That is, different annotation spans can be considered reasonable. So we proposed that the span boundary learned by the module should be fuzzy instead of precise. Besides, there is a mismatch between transformer feature extraction and information extraction. Basic Transformers focus on global features, which ignores the prior hypothesis that span has limited length. So we proposed that the attention used for span extraction decision should be adaptive rather than static in order to model the furthest span boundary, which represents the target boundary as a continuous distribution of correct probability in a specific range, where R-min and R-max represent the start and end of the fuzzy boundary. And the function Q represents the correctness of the current position. Through the sampling function shared in the slide, we convert the continuous boundary distribution into a group of discrete values for calculation of fuzzy span loss. The boundary distribution predicted by the module will calculate Binary Cross Entropy with golden boundary as BCE loss and adding KL-divergence between predicted boundary with fuzzy span boundary and supplementary information. To get the model in obtaining a more reasonable attention distribution for span extraction, we proposed a fuzzy span attention as a mask function to trim attention distribution. The image and formula of the mask function G are shown in the slide, where the fuzzy span is reflected in two aspects. On the one hand, by introducing an optimizable parameter delta to adjust the length of the full attention range, the attention span of the module is dynamically changing. On the other hand, the attention distribution on the attention span boundary linearly decays, rather than truncates. The overall structure of the module is presented on the slide, where the fuzzy span attention layer is only added on the top level to guide the model's decision process without affecting the text encoding capability. To demonstrate the capability of FSUIE, we conduct experiments on three main information extraction tasks, including named entity recognition, relationship extraction, and aspect sentiment triplet extraction. As for the results of named entity recognition, by introducing FSL and FSA, our FSUIE-base achieved significant performance improvement compared to UIE-base without a fuzzy span mechanism. On small-scale data size, the model is easier to learn universal attention spans, resulting in a more significant improvement. As a result, on relationship extraction, FSUIE achieves new sota results on datasets ACE2004, 2005, and ADE. FSUIE uses one unified structure to extract relationship elements, achieving better information extraction ability with simple structure. Besides, FSUIE shows stronger generalization capabilities for domain-specific information. As for results on ASTE tasks, FSUIE also achieves sotaresults on 14lap, 15res, and 16res of AST-V2 dataset, and demonstrates competitive performance on 14res datasets. The results of the ablation study shows that FSA improves convergence speed by guiding the module to obtain a reasonable attention distribution. FSL enables the module to fully utilize annotation information and obtain greater information extraction capability. The combined effect of the two will produce a greater enhancement. We also visualized the attention distribution of a fuzzy span attention layer. Results show that the module focused on semantic information within a limited range of preceding tokens. This meets our expectations. In conclusion, in this work, we first proposed a novel fuzzy span loss that alleviates the model's reliance on span boundaries, and then we proposed efficient fuzzy span attention to adaptively adjusting the attention span of the model. And the FSUIE we proposed achieves excellent results in a wide range of IE tasks. Thank you for your listening.</sample>
    <sample id="285">Hello everyone. I'm Mingqi Gao from Peking University. I'm glad to share our work, "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework" . This video focuses on the key points of our work. As we all know, summaries generated by models and even some reference summaries still contain factual errors, and there are two main types of solutions. The first is to introduce factuality-related objectives in the training or inference process to make the summarization models more faithful and expect it will generate more factually correct summaries. The second is the design of a Factual Error Correction model (abbreviated as FEC), which is independent of the summarization model. It takes the source document and model-generated summary as input and outputs a corrected summary. To the best of our knowledge, there has been no work on factual errors for dialogue summarization. Considering the importance of factual issues in dialogue summarization, we would like to try to correct factual errors in dialogue summaries. However, after carefully examining and considering the motivations and practices of previous FEC studies, we argue that there are flaws in the way FEC models are evaluated, which may have diverted the FEC for summarization from its original purpose. Let's see how the current FEC models have been evaluated. Factuality metrics such as FactCC and DAE are used. It takes a source document and a summary as input and outputs a score. It is expected that the average scores of the corrected summaries are higher than the original. The higher the score, the better the FEC model. There are two flaws. First, factuality metrics give an overall score, which is so vague, and factuality metrics may not be reliable on their own. Second, this evaluation blurs the line between the two types of solutions. The FEC model can ignore the content of the original summary and directly generate a different but more factually correct summary. There may be no error correction at all. We argue that it is necessary to introduce manually annotated reference corrections to address both issues. Factual error correction for summarization has these basic requirements: to correct factual errors in the original summary by as few substitution, insertion, and deletion operations as possible to obtain a fluent and non-redundant summary. This can be reflected in the manual annotation. The introduction of reference corrections, on the other hand, provides more valuable data for the training of FEC models compared with pseudo data. On the other hand, and more importantly, it creates the condition for a more comprehensive and accurate evaluation of the performance of FEC models. To automatically classify factual errors for factual error correction, we propose a new taxonomy of factual errors. We point out that there are two classifications of factual errors of different perspectives: content-based and form-based. Content-based categories are assigned according to the part of speech and dependencies, and form-based categories are assigned according to whether it is an addition, deletion, and substitution operation. We build our evaluation framework on the basis of ERRANT, an evaluation metric for grammar error correction. It mainly consists of three steps: alignment, classification, and comparison. We experiment with some FEC models in different training modes to explore some factors of interest. With our proposed evaluation framework, we have the following key findings. Training FEC models with reference summaries from dialogue summarization datasets yields the best results of unreliable factuality metrics. There is an urgent need to change the evaluation methods for FEC models. Introducing human-corrected summaries during the training of FEC models for dialogue summarization can improve their performance. Combining human-annotated data with synthetic data is a promising direction. And current FEC models struggle to correct factual errors like addition and cannot address attribute errors, modality errors, link errors, etc. Thanks for listening.</sample>
    <sample id="286">The speakers are James Finch and Sarah Finch.</sample>
    <sample id="287">There are 4 authors involved in the paper.</sample>
    <sample id="288">BLiMP, SyntaxGym</sample>
    <sample id="290">The abbreviations of the five methods for the first research question are: COSINE, WSL, FTL, FTw, and FTW.</sample>
    <sample id="291">Named entity recognition, classification, part-of-speech tagging and question answering</sample>
    <sample id="294">CamemBERT is initially trained on the 4 GB set of NACHOS.</sample>
    <sample id="295">The speaker's name is Adam Przepiórkowski.</sample>
    <sample id="296">Hi. I am Valerio Basile, and in this video I am going to present a work which is the fruit of a collaboration between the University of Turin and Amazon Alexa. Natural Language Understanding — and natural language processing in general — is based in large part on supervised machine learning, or the so-called data-driven approaches. And in order to be able to develop these approaches, we need a lot of data, large sets of manually annotated data which encodes some kind of human knowledge, which the annotators put into the data by their process of annotation. Typically, there is a series of works which have been showing that the assumption that there is one single truth — that's called the ground truth, and the annotation is to converge towards it — is showing some limits. We chose to focus on irony in particular, which is a highly latent and pragmatic phenomenon in natural language. Irony detection is already a very difficult task for modern natural language processing models, but we want to investigate this problem further. Rather than training models that are able to say something like a binary label — like this sentence is ironic or is not ironic — we would like to have models that have a more informative output. So, in order to study these problems, we developed a corpus called EPIC, which stands for English Perspectivist Irony Corpus. We collected data from different sources — from social media, Reddit, and Twitter — and the data spans a long time window of 1½ years, and we collected about 300 short conversations made of pairs of text, one following the other. And we repeated this for both sources and for five varieties of English, which are listed here. We use the crowdsourcing platform Prolific to have people annotate this data. We selected about 15 annotators for each of the English language varieties, for a total of 74 annotators in total. We gave each of them 200 texts, or 200 short conversations. And we also put extra questions to use as an attention check for quality control. So on average, we got 5 annotations for each short conversation. This is how the annotation interface looks like. It's very simple. It resembles a chat or a text interface. They see a text, a message, and its reply, and then there's just the question there below, which asks "Is the reply ironic?" with respect to the context, and the annotator can choose "Ironic" or "Not ironic". We observed some differences between several groups along different dimensions, depending on how we divide the datasets. But whether we divide the annotators by gender, by age group, nationality, and so on, we can notice that the distribution of the inter-annotator agreement that is depicted in this violin plot is always kind of different in every case. We try to model these differences and build different models, which we call perspective-aware models, basically training different models by fine-tuning a pre-trained language model on splits of the datasets, where the splits are in terms of different annotators While in terms of raw performance, we didn't notice particular trends here (of course, the results are different, but there is no upward or downward trends), by using perspective-aware models versus gold standard aggregated models, we notice a significant difference in the confidence that the perspective-aware models show. And in particular, we can see how the perspectivist models are, on average, less uncertain, more confident of their predictions. Finally, we went again into the data and we try to look at what may be the causes of the differences in the annotations, and we found something peculiar: that is, that in the case of age, it is generations that are close to each other that seem to be more in disagreement toward their perception of irony. And similar things happen in the geographical distribution of the annotators, where we found that the highest variations in response is given between the two models trained on labels given by annotators from the United Kingdom and Ireland. This is it for this short presentation, but we will be happy to answer all your questions and have further discussions at the poster session.</sample>
    <sample id="297">The project "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" explores the use of dogwhistles in political rhetoric, particularly focusing on coded language that conveys hidden meanings. The study includes a glossary of over 340 terms and symbols related to racist, transphobic, and anti-Semitic dogwhistles, categorized by register (informal or formal), persona (anti-Semitic or transphobic), and type (adding an additional implicature). A case study examines historical U.S. political speeches, showing a correlation between the frequency of racial dogwhistles and the Republican Southern Strategy since the Civil Rights era. The research also evaluates how well GPT-3 can recognize these dogwhistles and identifies their potential evasion through automated toxicity detection systems like Prospective API.</sample>
    <sample id="298">The experiment to retrain or continue pre-training some models with more recent data showed that performance degrades as the temporal gap between train and test data increases.</sample>
    <sample id="299">NLI models have achieved state-of-the-art results across a block of benchmarks. However, despite rapid progress, recent work has demonstrated that the success of NLI models is partly due to learning and using shortcuts. Shortcuts are spurious correlations between the input attributes and labels introduced during the dataset creation process. For example, high word-overlap between the premise and the hypothesis in the MNLI dataset is strongly correlated with the entailment label. Consequently, NLI models that exploit shortcuts perform well on in-distribution samples, but are brittle when tested on out-of-distribution adversarial test sets where such spurious correlations do not hold. Prior work in shortcut mitigation typically assumes access to an auxiliary model designed to rely on shortcuts for predictions. For instance, the auxiliary model can learn to exploit shortcuts by being trained only on a small number of examples or by leveraging an auxiliary with reduced learning capabilities. The output of the auxiliary is then used to re-weight training instances for the learner model. Existing shortcut mitigation methods may require knowing this in advance. This assumes domain- and dataset-specific knowledge, which is not always available and thus limits the potential of shortcut mitigation. Furthermore, current shortcut mitigation methods often assume that the learner will naturally exploit the same types of shortcuts as the auxiliary. In practice, the behavior of the learner diverges from that of the auxiliary. For example, the auxiliary may down-weight instances that are useful for training the learner or provide inaccurate uncertainty estimations that may hinder the learner's out-of-distribution generalization capabilities. Finally, current shortcut mitigation methods require using a pre-trained language model as the auxiliary, which incurs additional computational overhead. Motivated by these limitations, in this work, we propose a training method to reduce the reliance of NLI models on shortcuts and improve their out-of-distribution performance. The key insight behind our training method is that NLI models suffer from poor performance on under-represented ""hard"" training instances with patterns that could contradict the shortcuts in the dominant ""easy"" examples. These hard examples are pivotal for ensuring good generalization performance on out-of-distribution examples. Crucially, the loss of hard examples decreases considerably more slowly than the average loss throughout training. Therefore, our aim is to obtain an example weight distribution that places emphasis on the under-represented hard examples. To compute the weight distribution with both a minimax training objective between a learner and auxiliary, the learner tried to minimize the loss of the NLI task, whereas the task of the auxiliary is to maximize the learner's loss by generating example weights such that the learner is incentivized to concentrate on ranges of the input space where it incurs high losses. Thus, the learner would prioritize learning from under-represented hard examples that counteract the uses of shortcuts present in the dominant easy examples. Both models are optimized in an alternating fashion using any standard optimization algorithm, such as stochastic gradient descent. At test time, the learner can make predictions without relying on the auxiliary. Our method does not make any assumptions about the type of shortcuts contained in a dataset. It relies on the learner's own training dynamics to generate example weights. And finally, we use a feed-forward network to model the auxiliary. We evaluate our proposed method in three commonly used analytic datasets, MNLI, FEVER, and QQP, and the corresponding out-of-distribution adversarial test sets, HANS Symmetric, and PAWS. Here, we observe that compared to an ERM training model as well as the best-performing shortcut mitigation method in each dataset, the minimax training objective consistently improves out-of-distribution performance while maintaining high in-distribution accuracy. Finally, in our paper, we also examine whether the performance improvements transfer in larger models, synthetic shortcuts, and out-of-domain test sets. What is the effect of pre-training the learner? How small the auxiliary needs to be. And finally, we conduct a qualitative evaluation of the learned example weight distribution. If you find this work interesting, we would love to chat with you during our poster session. Thank you for your time.</sample>
    <sample id="300">The work presented by Belinda introduces a task called interactive dictation and makes initial steps towards solving this task. Interactive dictation is characterized by the flexible interleaving of dictation and editing, not separated by a trigger word, using intuitive and open-ended natural language utterances to specify edits. The contribution includes introducing and formalizing the task of interactive dictation as a four-step procedure: ASR recognition module parses raw audio into speech transcript; segmentation of the speech transcript into separate dictation and command utterances; extraction and normalization of each command with fixing of ASR miss detections and speech errors; execution of each dictation and command utterance in sequence until arriving at the final document state.</sample>
    <sample id="302">To put them into the right order.</sample>
    <sample id="303">The authors recommended that model owners should increase transparency about bias mitigation methods because it is important to understand the underlying reasons for certain patterns in language models, such as positive stereotypes and essentializing narratives. Increased transparency would allow researchers to study these issues further without making assumptions or relying on limited information.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences that have the same syntactic structure as an acceptable sentence but differ in meaning or other aspects, making them grammatically ungrammatical.</sample>
    <sample id="305">Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. I'd like to begin with a brief introduction to weak supervision and weakly supervised learning.</sample>
    <sample id="306">Hello everyone. I am Sebastian Schuster, and together with Najoung Kim, I'm going to give you a short overview of our work on Entity Tracking in Language Models. For an agent to understand a discourse, it needs to track which entities are mentioned and how their state changes as the discourse unfolds. So, for example, in the context of a recipe — such as here —an agent has to understand that "Put the eggs, sugar, and flour in a bowl" results in all of these three entities ending up in a bowl. And if the discourse continues with "Mix to form a light batter", then the agent has to understand that now all of these entities are part of the batter. And so we argue that this is a crucial ability for understanding longer discourses, but there haven't really been any systematic investigations into what pre-trained language models can actually perform such tasks. And so the overarching research question we're trying to answer in this paper is to what extent large language models can track entities. Now, given that we don't know the exact contents of the pre-training data of many language models, and considering several properties about how discourses work, there are actually several challenges with designing a task to evaluate entity state tracking abilities. First, some entity states will be common in the pre-training data, and therefore, the model may predict the correct state without actually having any entity tracking abilities. For example, eggs often end up in bowls or babies often end up in cribs. We want to make sure that the distribution of patterns in the pre-training data cannot give away the entity states in the evaluation data. Then second, sometimes entity states can be predicted from individual words or phrases without actually considering the larger discourse, and the model may seem to be able to perform entity tracking while in fact it just learns simple heuristic associations between words and entity states. For example, that the word "empty" is always associated with an entity being empty. And then third, if one uses fine-tuning or in-context demonstrations — which is often necessary to probe the model — then the model may memorize entity state sequences, or it may learn to apply heuristics such as slot filling if such heuristics are not blocked in the evaluation task design. And so in designing our evaluation task, we took great care to make sure that the model cannot make use of any of these shortcuts when we evaluate its entity tracking abilities. And Najoung will tell you a bit more about how we set up this task. Hello. My name is Najoung Kim, and I'll be talking about the task design and the experimental results. So to find the entity tracking abilities, we designed the following task involving boxes and objects. And in our setup, the input to the model starts with a description of the initial contents of each box, as sketched on this slide. And the task of the language model is to complete the input by predicting the contents of each box. Now, given just this initial description, the task is pretty trivial: the model can just copy the relevant information from the description. But in our task, we also include multiple state-changing operations like moving objects or adding objects to a box. So for these, the model would have to combine the initial description with the operations to make the correct prediction. For example, Box 1 now contains the car and the watch after moving the watch from Box 3 to 1. And additionally, we implemented various measures to prevent the model from using heuristics, as Sebastian discussed earlier on. Please check out our paper for how we did this. We tested the setup with Flan-T5 and GPT-3 and -3.5 models using 2-shot in-context learning. And what we're showing here is the accuracy of predicting the correct box content as a function of the number of operations acting on a certain box. On the left panel, we have the data points where the probed entity state is different from the state in the initial description, whereas on the right panel, we have cases where the state is the same as in the initial description. So for these ones, the model can simply copy. And our experiments show that most models simply repeat the initial state, as you can see from the generally high accuracy on the right panel. And we can also see that only text-davinci-003 exhibits non-trivial tracking, which is the pink line here in the left panel. All other models performed below a strong random baseline obtained by random simulation, which is the blue line. What gives rise to this difference between models? Since the models we tested varied along several different dimensions, we investigated what other factors might be in play by zooming into the GPT series. And we found that all GPT-3.5 models, which all have been trained on substantial amounts of code, exhibit non-trivial entity tracking behavior, whereas all models that do not have code as a substantial part of their pre-training do not. This suggests that pre-training on code is what's responsible for making this capacity surface in pre-trained language models. We also found that smaller models like T5-base can learn to perform entity tracking if you directly fine-tune the models. But on the other hand, randomly initialized models of the same architecture cannot learn our state tracking task even when they receive direct supervision, suggesting that pre-training is again important here. However, as we discuss in more detail in the paper, it remains unclear whether the state tracking abilities we observe generalize beyond our set-up in this case. Thanks for listening, and we have a lot more results and analysis, including GPT-4 experiments, in our paper, so please check out arxiv. And if you have any questions or comments about our work, either find us in person at ACL, or you can reach out to us over email or on Twitter. Thank you.</sample>
    <sample id="307">The evaluation metrics used by the authors are not specified in the audio.</sample>
    <sample id="308">Jenny, a first year PhD student at Carnegie Mellon University, presented her work "NLPositionality" which characterizes design biases of datasets and models. The presentation focused on the issue of systematic performance differences in technology between populations due to positionality, or perspectives held by NLP researchers and model developers based on demographics, identity, and life experiences. Jenny's research compared end users with existing datasets and models through their framework NLPositionality, using Lab in the Wild for recruiting diverse volunteers from 87 countries. They found that there is indeed positionality in NLP, as data sets and models are most aligned to English-speaking countries and people with college education. However, some groups such as non-binary individuals were left behind. To address this, they recommended keeping records of all relevant design choices throughout the research process, doing NLP research with a lens of perspectivism, and building specialized datasets and models within specific communities like Masakhani initiative.</sample>
    <sample id="309">ABC-Eval labels were used for measuring inter-annotator agreement.</sample>
    <sample id="310">Wikipedia was chosen to add completely unrelated sentences.</sample>
    <sample id="311">The affiliations of the authors are: University of Trier, Germany; University of Applied Sciences Western Pomerania, Germany.</sample>
    <sample id="312">MultiInstruct is the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source datasets and each task is equipped with five expert-written instructions, making it unique in its scope and structure compared to other benchmarks.</sample>
    <sample id="313">There are two authors involved in the paper.</sample>
    <sample id="314">Coordination is a grammatical relation in which two or more elements are combined to form a single unit. In binary coordination, there are only two elements involved that combine to create the new structure.</sample>
    <sample id="315">The prompts used in this study were 10 words long on average.</sample>
    <sample id="316">The findings suggest that smaller models like T5 can surpass larger language models when properly trained on suitable datasets. The CoScript dataset generated from large language models enables the training of a fine-tuned T5 model, which demonstrates higher quality script generation compared to most large language models. This implies that specialized and smaller models may be more effective for constrained language planning tasks if appropriately trained with relevant data.</sample>
    <sample id="317">Peng Li from Fudan University presented their work titled "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors". The task of information extraction involves extracting structured data from unstructured text. Previous models, such as T5 and GPT-3, operate in a text-to-text manner during pre-training but produce outputs that do not align with the expected structure during inference. To address this issue, CodeIE was proposed to transform the task into a code generation problem using large language models like Codex. This approach allows for easier conversion between text inputs and structured outputs. Evaluations on various datasets showed that CodeIE outperformed traditional baseline models significantly when using few-shot examples. Additionally, it demonstrated better recall performance compared to text format prompts.</sample>
    <sample id="319">The work investigates two learning strategies: from-scratch pre-training and continual pre-training. From-scratch pre-training involves training a model directly on the target domain data without any prior knowledge or adaptation. Continual pre-training, on the other hand, leverages existing models (like CamemBERT) as a starting point for further adaptation to the target domain. The study compares these approaches by evaluating their performance on various downstream tasks in French biomedical and clinical domains.</sample>
    <sample id="320">The factor of overfitting due to test reuse is greater than one.</sample>
    <sample id="321">The quality of the simplification was evaluated by analyzing sentence pairs in terms of their type and level of simplification. The DEPLAIN corpus contains a variety of simplification transformations, such as lexical substitution, clause deletion, reordering, and insertion of words. Additionally, the alignment methods were tested using manual alignments from the DEPLAIN corpus as gold standard references to evaluate their performance.</sample>
    <sample id="322">Enrico will be presenting at ACL 23 answering the question, "What does a Text Classifier Learn about Morality?" First off, let me explain to you what is morality. Human morality is what helps us distinguish right from wrong; it's our internal compass that helps us determine whether an action or a concept is morally right or morally wrong. And morality is at the base of our societies, obviously, and it is essential that language models can understand and recognize morality in our language in text. The understanding of morality in text has already been approached in the NLP community, but it is often and usually being treated as a singular scale between immoral and moral, where a concept, a sentence can be labeled somewhere between immoral and moral for the judgment of morality. However, we know that morality is very subjective. Different people would label the same concept differently in the scale. Take, for example, divisive concepts such as abortion or LGBTQ rights. Some of us would label, for example, abortion to be immoral and some would label it to be moral. Simply taking an average between these or taking a majority aggregation would actually hide the real truth, that it's a pluralist, different way of interpreting morality. And teaching language models just the average is very dangerous. To this extent, there are social theories that we can apply to the understanding of human morality. In particular, a very popular and established theory is the so-called Moral Foundation Theory. According to this theory, there are five different ways in which we humans perceive morality, as much as there's five different taste buds in our tongue, and each action, each concept, tickles a different moral aspect, a different moral foundation. And each of us, each human, prioritizes these foundations in different ways. For example, for some of us, fairness is very important, and for others, authority is much more important. The different ways in which we prioritize these foundations, determine the way in which we judge the morality of a concept or of an action.</sample>
    <sample id="323">The author of this paper is Yujie Wang from Shanxi University, China. The title of his/her paper is "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA". Commonsense QA is a challenging task that requires machines to answer questions that rely on common knowledge to test their language understanding abilities. This requires machines to retrieve relevant knowledge from external sources. Recently, Holmes thinks that knowledge is stored in both language models and knowledge bases. Many works combine these two types of knowledge to solve Commonsense QA with good readout. These works retrieve relevant knowledge from the knowledge base through entity matching, building a subgraph, and then using the language models and GNNs to infer answers. However, they introduce some noisy entities during subgraph retrieval, such as "Top", "Bank", and "Cat" here in the subgraph, which are largely unattached to the current question. Moreover, they encode the subgraph and text in isolation, leading to limited interaction between the two modalities. And the encoding process ignores the semantic relationship between entities. Based on the above problems, we propose DHLK. First, we build an HKG based on multiple knowledge bases run through a two-stage pruning strategy and KRL to optimize the structure and knowledge representation of the HKG. Finally, we use the language model to encode and fuse the two modalities.</sample>
    <sample id="324">Yes, language models have different political biases. They occupy all four quadrants on the political campus and their ideological coordinates shift when further pre-trained on partisan corpora separated into news and social media.</sample>
    <sample id="326">Cognitive dissonance is two beliefs or actions that are inconsistent.</sample>
    <sample id="327">The goal of Vision-Language learning is to train a smart AI system that can understand both image and text. Visual Question Answering is one of the most famous VL tasks, which needs to answer a question based on the input image. Since 2019, with the help of large-scale self-supervised pre-training on image-text pairs, transformer-based vision language models have achieved remarkable progress. From the perspective of model architecture, recent vision-language works can be unified as a two-tower architecture which consists of a textual encoder, a visual encoder, and a cross-modal encoder. If we step into the unimodal encoders of the two-tower architecture, such as METER, we can find that they feed only the last-layer unimodal representation directly into the top cross-modal encoder, ignoring the semantic knowledge at the different layers of the deep unimodal encoders. Different from the two-tower two-architecture, BridgeTower connects multiple top unimodal layers with each cross-modal layer in a layer-by-layer fashion to exploit unimodal's magnetic knowledge at different levels. However, BridgeTower still suffers from two obvious limitations. First, its layer-by-layer utilization of different unimodal layer representations is ineffective. Each cross-modal layer can only utilize an artificially assigned unimodal layer representation, thus restricting the exploitation of different levels of unimodal semantic knowledge. Second, the number of cross-modal layouts is tied to the number of unimodal layer representations it used, thus limiting its scalability and capability. In this work, we build upon BridgeTower and advance it in two aspects. We propose ManagerTower, a novel VL modal architecture that each manager takes multiple unimodal representations as the insights of pre-trained unimodal experts at different levels. ManagerTower adaptively aggregates insights with managers in each cross-modal layer. Here is the detailed architecture of our ManagerTower.</sample>
    <sample id="328">GPT-4 is the most liberal language model.</sample>
    <sample id="329">Video sentence localization aims to find the most relevant segments with a given natural language query for long videos, and has broad applications in video retrieval, summarization, and other fields. This task takes a video and natural language query as input, and requires the model to output the start and end times of the video segment that are most relevant to the given query. However, many of these methods require a large number of manual annotations for training, which is costly and inefficient. Existing zero-shot methods mainly use the following process. First, generate pseudo-events based on the video, then generate pseudo-queries based on the pseudo-events. Finally, train a video sentence localization model using these pseudo labels. They have three main drawbacks. Firstly, the pseudo-queries are usually too simple. For example, some methods combine detected nouns and verbs in the events to generate the queries, which has a large gap between the real queries. Secondly, as shown in the figure, their approach can only show a high relevance between videos within the event and query, but can not guarantee that the query and video outside the events is irrelevant, which lead to unalignment between the pseudo-queries and pseudo-events. Finally, they directly use these pseudo labels to train the model, ignoring the risk of label noise. Thus, we propose the noise-resistant Structured Pseudo-Label generation method. As shown in the figure, we first use a pre-trained image caption model to generate more complex free-form pseudo-queries. Then we use a pre-trained model to measure the relevance between individual frames and pseudo-queries, and generate pseudo-events which guarantee high relevance between the video inside the event and the query, and low relevance between the video outside the event and the query. Finally, we reduce the weight of noisy samples and create noisy labels to reduce the influence of label noise.</sample>
    <sample id="330">Yes, cumulative training performs better than iterative when doing active learning.</sample>
    <sample id="331">The speaker is Sara Papi.</sample>
    <sample id="332">The data was taken from a parallel corpus that they wanted to use for evaluation.</sample>
    <sample id="333">Hi everyone. I'm Wenhao from Nanjing University. It is a great honor to be here to introduce our work, "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation". Before introducing our work, I would like to acknowledge collaborators. They are Jingjing Xu from Shanghai AI Lab, Shujian Huang and Jiajun Chen from Nanjing University, and Lingpeng Kong from the University of Hong Kong. In this work, we focus on neural machine translation. We know that the target of NMT is to learn a generalized representation space to adapt to diverse scenarios. However, neural networks often induce a non-smooth representation space, which limits its generalization ability. Specifically, in the representation space of the NMT model, we observe that low-frequency tokens disperse sparsely, as illustrated below. Due to the sparsity, many "holes" could be formed; in these holes, the semantic meaning can be poorly defined. As a result, an NMT model performs poorly in these areas. To enhance NMT models' generalization and performance, kNN-MT is proposed as a solution. Its core idea is to smooth predictions according to nearest neighbors in the representation space. To do so, it requires a training corpus to build a key value data store to save representations and their corresponding target tokens. At each decoding step, the NMT model will query the data store to retrieve nearest entries and refine the prediction probability according to the retrieval result. Although effective, this approach has two significant drawbacks. Retrieving neighbors from a large datastore at each decoding step is time-consuming, and once the datastore is constructed, representations cannot be easily updated. To overcome these drawbacks, we propose the framework INK, to INject kNN Knowledge into an MT. Our INK training loop has two steps. At first, kNN knowledge is extracted from the datastore to guide the adapter to adjust representation. Then updated representations are used to refresh the datastore asynchronously. This trend loop will run until convergence. Specifically, we adjust the representation by allowing three kinds of representation using KL-divergence. First, we align contextualized representation and token embeddings to keep semantic meaning. Then we align contextualized representations and kNN token embeddings to enrich semantic meanings. And we also align contextualized representations of the same target token to address the sparsely dispersing problem. Overall, we optimize the adapter with the combined learning objective and run this training loop until convergence. In the end of the training loop, we can drop the datastore aside. In our experiments, we chose the winner model of WMT’19 German-English news translation task as the off-the-shelf NMT model. We conduct experiments on the full benchmark dataset and find that even for the WMT winner model, its representation space can still be greatly improved. In our experiments, we explore the following three questions. The first research question is, "Can we smooth the representation space with a small adapter and drop the datastore aside during inference?" The second research question is, "How much improvement can be brought by using kNN knowledge to adjust the representation distribution?" The third research question is, "Will using an adapter and datastore together bring further improvements’" As shown in the table, the INK system outperforms the state-of-the-art kNN-MT system and achieves the best performance after smoothing the representation space. Compared with using an adapter baseline, we can see that refining representation according to kNN knowledge brings larger performance improvement. To better the show the effect of the INK framework, we use adapters of different sizes. In general, the INK system locates on the top right of each figure, which means that INK achieves higher BLEU scores with less memory space. Besides, we also find that jointly applying adapter and datastore can further smooth predictions, which indicates that the representation space of the NMT model is not fully refined by the adapter. If a more effective framework can be designed, the benefit of smoother representation space will be further revealed. To conclude, we propose a novel training framework in this paper. In our framework, we devise, inject, and refine a training loop to iteratively refine the representation space of the NMT model according to kNN knowledge. Experimental results show that the INK system achieves an average gain of 1.99 COMET score and 1.0 BLEU score, compared with the state-of-the-art kNN-MT systems. Our INK system also achieves better translation performance, with less memory space and faster inference speed.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Cross-lingual transfer is the process of training a model on one language and then using it to make predictions in another language.</sample>
    <sample id="337">The study investigates the use of a graph-based approach to handle out-of-vocabulary (OOV) words in natural language processing. The proposed method, called Word Relationship Graph, leverages word formation and association rules to infer OOV word meanings by associating them with relevant words through a two-level graph structure. Each node represents either a word or its components, while their corresponding embeddings serve as attributes for nodes.

To address challenges associated with assigning attributes to OOV nodes, the researchers employ a self-attention network that assigns attributes based on character information from the OOV words. This is followed by incorporating two levels of Graph Attention Network (GAT), which are concatenated and fused with initial input embedding layers to produce node-level representations. A readout block layer captures overall graph information and summarizes word formation patterns.

The model's performance was evaluated using contrastive learning techniques such as NT-XENT positive samples from graphs like two-hop relevant neighbor words, synonyms, or the original OOV word itself. Results demonstrate superior performance compared to baseline models across various intrinsic and extrinsic tasks, indicating effective handling of OOV words via word formation analysis. Additionally, it suggests potential benefits for both static and contextual models in downstream applications.</sample>
    <sample id="338">The speaker, Bingsheng, introduces their research titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations" on behalf of a collaborative group from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research. The presentation aims to address the question: How do we evaluate the quality of human annotated explanations when they can be subjective and task-dependent?

Bingsheng explains that traditional metrics like BLEU and ROUGE focus on word similarity with gold standard annotations but neglect differences in tasks and explanation utility during fine-tuning and inference stages.

To tackle this issue, the researchers developed a unified data format for various tasks by converting them into multiple-choice formats. They conducted experiments using five large-scale datasets (CoS-E, ECQA, e-SNLI, ComVE) and trained models under baseline and infusion settings. Results showed improvements even with small amounts of training data incorporating explanations.

A new evaluation metric called TREU was proposed, which extends the simulatability score to include helpfulness at both fine-tuning and inference phases. This metric outperformed the simulatability score across different datasets and model types (T5 and BART). 

The study highlights the importance of evaluating human explanations' usefulness before blindly adopting them as gold standards and suggests future work should perform similar evaluations.</sample>
    <sample id="339">Saarland University in Germany</sample>
    <sample id="340">Paraphrase generation is a long-standing and important task in the NLP domain. It benefits many other NLP applications such as question answering, chatbots, and improving robustness. To train a good paraphrase generator, we usually need a large scale of high-quality paraphrase data. However, for the existing human-annotated datasets like MRPC, PAN, and Quora they have high quality, but they are limited in scale. There are some automatically generated datasets, like back-translation — translating a sentence to another language and translating back. They can automatically generate a large scale of paraphrase datasets, but they are lack of syntactic diversity. Here is one example. We can see that the generative paraphrase has almost the same syntax to the original sentence. In this work, our goal is trying to construct a large-scale, syntactically diverse paraphrase dataset. Our key idea is to leverage AMR graphs. AMR (Abstract Meaning Representations) is a directed graph that captures the abstract meaning of a sentence. Each node represents a semantic concept in a sentence, and each edge represents a semantic relation between concepts. And we have focus, which is the root node, to represent the main assertion of the sentence. We therefore propose to use AMR back-translation to generate syntactic diverse paraphrases. First, we will use a pre-trained AMR parser to get AMR Graph of a source sentence. Then we will change the focus of the graph. We will randomly sample a node and set it as a new root node, then modify the corresponding edge and their edge labels. Then we will use the AMR graph-to-text generator to generate text from the modified graphs. For the generated text, because they share the same AMR graph structure, so they will have similar semantics. Also, because the text generator would emphasize focus at the beginning of a sentence, their syntax would be a little bit different. By using AMR back-translation, we can get our proposed dataset, ParaAMR. There are around 15 million source sentences in ParaAMR, and there are around 6.9 paraphrases per source sentence.</sample>
    <sample id="341">The authors use BLEU and average lagging to measure the translation quality, as well as computational aware average lagging that accounts for the model's computational times.</sample>
    <sample id="342">The speaker introduces their paper, "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming," which was conducted by a team including Lian Yixin and others. The presentation begins with an introduction to open-domain dialogue, explaining that it is a type of conversational exchange between humans and artificial intelligence systems without specific goals. Existing large-scale pre-trained dialogue datasets are mostly text-sourced, so there's a need for video-sourced data closer to real spoken conversation.

The existing video-sourced dialogue datasets can be divided into two groups based on scripting conditions—TV and movie scripts or interview datasets—but these have limitations due to manual annotations and instructions. To create a large-scale dataset, the challenge lies in finding effective matching mechanics to capture reply-to relationships among speakers.

Personalized dialogue is crucial but faces challenges like utilizing persona information effectively and lacking session dialogues per persona. Multi-party conversations also pose difficulties as they lack comprehensive Chinese multi-party dialogue datasets. 

To address these issues, the researchers propose LiveChat, a personalized dialogue dataset constructed automatically using streaming videos from platforms like Douyin (Chinese TikTok). They extract audio, transcribe utterances, collect audience comments, and gather persona information through both manual labeling and trained classifiers. This method results in a larger scale compared to other existing datasets while incorporating more personal annotations and longer average sessions per persona.</sample>
    <sample id="344">Trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures.</sample>
    <sample id="345">Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, "The girl slept." And "Mary knew that the girl slept." These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don't use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they're not ordered. That's why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don't know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That's because this is related to the "Traveling Salesman" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.</sample>
    <sample id="346">The affiliations of the authors are: University of California, Berkeley; University of Washington; and University of Illinois at Urbana-Champaign.</sample>
    <sample id="348">The paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" investigates the prevalence of social bias and stereotypes in large language models (LLMs). The study addresses limitations of previous methods, which rely on hand-constructed datasets that are time-consuming to curate and only measure specific or broad associations. To overcome these issues, the researchers use prompts to generate personas for individuals with different identities, making it generalizable across demographics. They employ a two-part method: persona generation using human-subject-inspired prompts and marked words analysis to identify distinguishing features between groups. Results show that generated personas contain more stereotypes than those written by humans but have limited negative connotations. The Marked Words method reveals how seemingly positive portrayals reflect harmful patterns through essentializing narratives. Recommendations include addressing both positive stereotypes and essentializing narratives, adopting an intersectional lens, and increasing transparency about bias mitigation methods.</sample>
    <sample id="350">The paper "What’s the Meaning of Superhuman Performance in Today’s NLU?" investigates how reliably leaderboard scores compare models and humans. The authors analyze two popular benchmarks, SuperGLUE and SQuAD, to answer this question. They found that while systems outperformed humans on some tasks by a significant margin, there were several sources of error that made these comparisons unfair. These errors included evaluating humans on different sets than systems, errors in ground-truth answers, vague estimates of human performance, varying pay rates across tasks, and omitted details about annotators. The authors argue that claims of superhuman performance are not scientifically meaningful without addressing these issues.</sample>
    <sample id="351">The paper investigates the problem of generalization using the Named Entity Recognition Task (NER) and observes that models developed in CoNLL-2003 for NER have been used to develop new taggers. The study aims to determine if these models can generalize well on modern data, what is needed for good generalization, and why some models perform poorly.
To investigate this issue, a dataset called CoNLL++ was created by collecting news from 2020 and annotating them with guidelines similar to those used in CoNLL-2003. Over 20 models were fine-tuned over CoNLL-2003 and evaluated on both CoNLL-03 test sets and CoNLL++. The performance change percentage due to F1 score was calculated to assess each model's generalization ability.
The research found three main ingredients necessary for good generalization: 
1. Model architecture - Transformer models generally performed better than others
2. Model size - Larger models led to improved generalization results
3. Fine tuning examples - More training samples resulted in better downstream task performances
The study also examined two hypotheses regarding potential causes of poor generalization:
1. Adaptive overfitting – Reusing the same test set repeatedly without diminishing returns; observed no significant impact here.
2. Temporal drift – Performance degradation caused by increasing temporal gaps between train and test datasets; confirmed as the primary cause through experiments retraining or continuing pre-training certain models with more recent data.
In conclusion, the authors suggest that improving generalization requires focusing on all three factors simultaneously rather than individually. They argue that while CoNLL-2003 taggers still work effectively today, further investigation into enhancing their capabilities should be conducted.</sample>
    <sample id="352">Annotating Behaviors in Chat</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Questions" proposes a method to generate code through interaction, specifically by asking clarification questions. The authors identify key operations and corresponding documentation from the code and represent them in latent space using their schemata. They compute similarity scores of all schema element pairs between an NLD (natural language description) and the operation documentation. If all elements for the similarity score are lower than threshold T, the key operation is considered missing; otherwise, it is aligned.

To create the dataset, they hire annotators to annotate validation sets and test sets. Using templates, they generate CQAs (clarification questions and answers) for missing key operations, which can be yes-or-no or multiple-choice questions based on heuristics extracted from the code knowledge graph generated by Graph4Code.

The results show that identifying missing key operations was effective with high performance among various models, including MPNet. However, there were some errors identified during error analysis, such as taxonomy issues where aligned operations might require further clarification due to similar names, and argument-related challenges when using operational documentation instead of actual argument values.

The pipeline consists of three main components: the Clarification Need Predictor, Question Selector, and Code Generator. Experimental results indicate improved model performances across evaluation metrics when more highly ranked CQs are included but also reveal increased unanswered clarifications compared to non-pipeline methods like model-only trainers.

In conclusion, this study demonstrates how incorporating interactivity into code generation tasks enhances accuracy while highlighting areas needing improvement within current methodologies.</sample>
    <sample id="354">2018</sample>
    <sample id="356">The affiliations of the authors are: University of Illinois at Urbana-Champaign, Google Research.</sample>
    <sample id="357">The speaker is Siyu Yuan.</sample>
    <sample id="358">There are 5 authors involved in the paper.</sample>
    <sample id="359">The approach is compared to the state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">The work presented in this presentation is titled "CounterComp" and focuses on using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning. The task of multi-step quantitative reasoning involves answering questions about a given financial table, such as calculating the net change in revenue from 2019 to 2020 by executing one or more arithmetic operations.

State-of-the-art neural models struggle with these tasks when the output has multiple steps due to memorizing spurious patterns. For example, if certain tokens are repeatedly seen during training, they might mistakenly associate them with common operations in the output.

To address this issue, CounterComp proposes mining counterfactual scenarios from input examples. By treating an anchor sample as a reference point, positive and negative examples can be generated based on interventions in the question that either yield no changes or result in different outputs. These triplets help add an auxiliary metric learning loss to the training procedure.

The results show that adding this auxiliary loss consistently improves performance across various state-of-the-art baselines, especially when the number of reasoning steps exceeds two. Additionally, it enhances compositional generalization capabilities by allowing the model to attend to more meaningful tokens related to operational terms in the output.</sample>
  </task>
</testset>