<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是网络爬虫数据。</sample>
    <sample id="1">McGill University, Mila and Microsoft Research</sample>
    <sample id="2">你好，欢迎参加我们关于德国文本识别的演示。</sample>
    <sample id="3">我的名字是Regina Stauden，我将引导您进行演示的第一部分。首先定义文本简化。</sample>
    <sample id="4">文本简化是将文本适应以改善特定目标群体对文本的理解的过程，例如有阅读障碍的人或非母语使用者。</sample>
    <sample id="5">为了训练文本识别模型，我们需要平行的文本对，例如文档或句子。</sample>
    <sample id="6">在这里，你可以看到一个复杂的德语句子及其用白话文翻译的平行句对。</sample>
    <sample id="7">为了简化句子，如示例所示，可以使用不同的技术，例如词汇替换、短语省略、短语省略重排或插入单词。</sample>
    <sample id="8">我们现在提出一个新的语料库计划，因为在最近几年中，现有的语料库存在一些问题。例如，这些语料库太小，无法训练一个词性标注模型。</sample>
    <sample id="9">最近几年提出的其他三个模型都是自动对齐的，这意味着它们可能会出现对齐错误。</sample>
    <sample id="10">因此，我们提出我们的新语料库D平面，它分为两个子语料库：D平面APA和D平面REP。D平面APA基于新闻文本。</sample>
    <sample id="11">在DeepPlain-APA中，我们手动对483份文件进行了对齐。这大约产生了30,130对平行句子。</sample>
    <sample id="12">对于DeepL Web，这个语料库包括不同的领域，并且我们还手动和自动对齐了这750篇文档。</sample>
    <sample id="13">总共有30,450个句子对。</sample>
    <sample id="14">我们对我们的句子对进行了一些分析，例如对简化类型。</sample>
    <sample id="15">如您所见，圣经文本比新闻文本或语言学习文本更简化得多。</sample>
    <sample id="16">在所有水平上，例如词汇简化、结构简化或整体简化水平上。</sample>
    <sample id="17">此外，您可以看到我们的D平面语料库具有不同的变形转换的高变异性。例如，在D平面API语料库中，我们比在D平面Web语料库中有更多的重排序和单词添加。</sample>
    <sample id="18">另一方面，在网络语料库中，我们有更多的改写。</sample>
    <sample id="19">所以现在让我们看看我们能用这个语料库做什么。</sample>
    <sample id="20">在最近几年中，已经有很多对齐方法，但在机器翻译的背景下。</sample>
    <sample id="21">我们有两个用不同语言写的平行文档，我们想从后一个文档中提取句子的对齐信息。</sample>
    <sample id="22">但是，在我们的用例中，我们试图从两个具有相同语言和内容的平行文档中的句子之间提取对齐。</sample>
    <sample id="23">现在，由于我们有了手动对齐的句子的数据集D平面，我们可以使用这些句子作为基准对齐来评估一些提议的对齐方法。</sample>
    <sample id="24">我们对所提出的办法进行了调整，并在论文中发表了所有这些调整和运行实验的代码。</sample>
    <sample id="25">最后，我们得出结论，最适合用于简化德语文本的自动对齐方法是MASE对齐方法。</sample>
    <sample id="26">您还可以在论文中找到运行此方法的代码。</sample>
    <sample id="27">我们论文中展示的第二个用例是自动文本简化。</sample>
    <sample id="28">通过调整语言模型，从复杂的输入文本中生成简化文本。</sample>
    <sample id="29">我们对两个不同的模型进行了微调。我们对LongImpArt模型进行了微调，以生成文档级别的简化版本。</sample>
    <sample id="30">我们还对Normal Base M部分进行了微调，以产生句子级别的简化。</sample>
    <sample id="31">您还可以在论文中查看我们实验的分数和评估指标。</sample>
    <sample id="32">我们得出结论，这种基本的微调可以产生或获得比基线分数更高的分数。</sample>
    <sample id="33">我们建议将这些结果作为自动文本简化问题的基准。</sample>
    <sample id="34">非常感谢大家的聆听，希望在会议上能见到大家。谢谢。</sample>
    <sample id="35">演讲者的名字是Kai-Yen。</sample>
    <sample id="36">They used a T5-Xlarge model to achieve 82%-87% accuracy.</sample>
    <sample id="37">CoNLL-2003标注器仍然有效。</sample>
    <sample id="38">The proposed method aims to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="39">Recent weak supervision methods require clean validation samples to work properly.</sample>
    <sample id="40">答案者知道这些实体的名称，但他们不一定了解这些实体。</sample>
    <sample id="41">这篇论文有五位作者。</sample>
    <sample id="42">嗨，我的名字是Szymon Skurkowski，这次演讲的主题是依赖结构的协调。</sample>
    <sample id="43">如你所知，不同的理论和语料库方法假设不同的依赖结构。例如，在普遍依赖关系中，Lisa、Bart和Maggie的联合协调结构是这样的。</sample>
    <sample id="44">因此，第一个连词是整个从句结构的主语，在这种情况下是Lisa。</sample>
    <sample id="45">类似的处理方式出现在Igor Milchuk的含义文本理论中，其中整个坐标结构由第一个连词引导。因此，这两种方法是等效的，它们突出显示了其中一个连词。</sample>
    <sample id="46">现在，还有对坐标结构的对称方法，例如在依赖树库中使用的Prag方法，其中协调结构由连词引导。</sample>
    <sample id="47">因此，我们从“and”中获得一些依赖关系到所有连接词。</sample>
    <sample id="48">最后，还有一种多头的方法，例如在卡森的单词语法中使用。</sample>
    <sample id="49">我们可以说所有的连词都是从句的主语，因此我们从主语“here loves”到所有连词“Lisa bought and makes”都得到了依赖关系。</sample>
    <sample id="50">现在，这篇文章的目的是为对称的协调结构提出一个新颖的论点，比如这个，以及反对不对称的协调结构，比如这个。</sample>
    <sample id="51">好的，这个论点是基于依赖最小化的原则，我将根据这些例子进行解释。</sample>
    <sample id="52">所以，在英语中，如你所知，直接宾语更喜欢靠近动词，而从属成分则可能更远。</sample>
    <sample id="53">While March read yesterday it is much worse, right? Because here between the verb and the direct object there is an adjunct "yesterday".</sample>
    <sample id="54">然而，这种影响可能会在直接对象非常重和很长的时候得到改善，因为这样它就可以被移动到边缘之后的位置。</sample>
    <sample id="55">这在这里有所说明。这两个句子都很好。马特写了一本关于BC yesterday的非常有趣的书，是OK的。相反，我们有这个长NP。</sample>
    <sample id="56">但说马特·雷迪亚兹的《Yesterday》这本书是关于蜜蜂的，也很好。</sample>
    <sample id="57">所以这里的原因是，这有可能是因为，即使这个句子违反了通用的语法原则，即直接宾语应该在动词旁边。</sample>
    <sample id="58">它满足依赖长度最小化的原则，即较短的依赖关系更受欢迎。</sample>
    <sample id="59">所以，这两个树只显示了关键依赖关系的长度，也就是在这两个结构中不恒定的依赖关系。</sample>
    <sample id="60">所以这里有一个从“red”到长度为7的补语，以单词为单位测量，以及从“red”到长度为4的“book”，所以总共是11。</sample>
    <sample id="61">当你移动，当你交换这两个成分时，这两个依赖项的和变成了6，对吧？所以，而不是11，6更短，这就是为什么这听起来还不错，对吧？它违反了一个原则，但满足了另一个原则。</sample>
    <sample id="62">好的，所以，我们从增强版的Pentreebank中提取了各种关于协调的统计数据，并在论文《为什么不用通用依赖关系？》中可以看到。</sample>
    <sample id="63">这些统计数据证实了以前多次观察到的结论，即左连词往往较短。</sample>
    <sample id="64">还有一个观察到的现象，即这种趋势随着长度的差异而增长。</sample>
    <sample id="65">所以，当两个连词的长度差异增长时，较短的连词倾向于比第一个更强。因此，左侧较短连词的比例更大。</sample>
    <sample id="66">但是，这篇论文的创新之处在于，我们观察到这种趋势只发生在左派政府不存在的时候。</sample>
    <sample id="67">好的，所以在这个例子中，州长在左边，我看到Bob和Lisa，所以州长在左边。</sample>
    <sample id="68">在第二个例子中，荷马来了，打了个喷嚏。这里有两个动词的协调，没有外部支配者，对吗？因此，在这种情况下，左边的从句更喜欢缩短，差距越大，两个从句之间的差距越大。</sample>
    <sample id="69">然而，当控制权在右边时，如这里所示，左边控制协调到网络，这种效果消失了。</sample>
    <sample id="70">因此，我们通过测量字符长度来显示，这是第一列，是音节的中间列，是单词的右列。因此，我将专注于右边的列。</sample>
    <sample id="71">我们看到的是，当振荡器在左边时。</sample>
    <sample id="72">The tendency for the left conjunct to be shorter grows steadily with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences; but when the governor is on the right this tendency disappears.</sample>
    <sample id="73">我们在论文中展示了如何提供一个反对不对称协调结构的论点，以及对称结构。</sample>
    <sample id="74">所以，请查看论文以获取完整的协议和论点，并在海报环节与我们交谈。谢谢。</sample>
    <sample id="75">这篇论文有两位作者。</sample>
    <sample id="76">The Bible texts are much stronger simplified than, for example, the news text or the language learner texts.</sample>
    <sample id="77">Salt and pepper and not pepper and salt.</sample>
    <sample id="78">Yes, you can use these models for your research.</sample>
    <sample id="79">DEplain-apa 包含新闻文本。</sample>
    <sample id="80">A better model architecture, larger model size, and more fine-tuning examples are all factors that contribute to good generalization.</sample>
    <sample id="81">通过测量字符长度、音节长度和单词长度来衡量。</sample>
    <sample id="82">测量字符长度、音节长度和单词长度。</sample>
    <sample id="83">The baseline classifier performed not much better than chance.</sample>
    <sample id="84">这篇论文有三位作者。</sample>
    <sample id="85">The cartoon has three speech bubbles. In the first bubble, Bob says "Remember that song we were listening to yesterday?" And with that, Bob sets the dialogue context.</sample>
    <sample id="86">语境感知 MT 模型比语境无关模型更有优势的 discourse phenomena 包括形式性和词汇连贯性。</sample>
    <sample id="87">The authors of this paper are affiliated with the following institutions: Google, University of California San Diego, and Facebook AI Research.</sample>
    <sample id="122">引入的框架通过比较模型和数据集的预测和标签与终端用户的预测和标签来量化立场。</sample>
    <sample id="155">By giving it to human subjects, they also were able to surface racial stereotypes.</sample>
    <sample id="156">The study used data from the Penn Treebank and Penn Charniak.</sample>
    <sample id="157">这篇论文有两位作者。</sample>
    <sample id="158">Topic independent dissonance stance classification and binary classification of expansion and comparison classes.</sample>
    <sample id="159">The paper has four authors: Zhihong Zhu, Yuxuan Zhang, Xiangliang Zhang, and Hua Hu.</sample>
    <sample id="160">这篇论文有三位作者。</sample>
    <sample id="161">The framework differs from previous work by comparing end users with models and datasets' predictions and labels, rather than examining inter-annotator agreement or modeling annotator distributions.</sample>
    <sample id="162">The "Alexa" setting contains the most overlap with the stereotype lexicon.</sample>
    <sample id="163">比较了DeepL和Google Translate。</sample>
    <sample id="164">Hi, I'm Xiangbin, a PhD student at the University of Washington. Today I'll be presenting our work on tracking political biases in pre-training data that lead to unfair NLP models for downstream tasks.</sample>
    <sample id="165">语言模型是在大规模网络爬虫数据上进行训练的。</sample>
    <sample id="166">根据对C4语料库的调查，政治新闻媒体在预训练数据中得到了很好的覆盖。我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等都在语言模型训练数据中得到了很好的覆盖。</sample>
    <sample id="167">这为语言模型应用带来了混合的祝福。</sample>
    <sample id="168">一方面，他们能够从不同的角度学习，这庆祝了民主和思想的多样性。另一方面，这些不同的政治观点本质上是社会偏见，可能会导致下游任务应用中的潜在公平问题。</sample>
    <sample id="169">为此，我们建议从预训练数据到语言模型再到下游任务，具体通过提出以下问题来调查政治偏见传播管道。</sample>
    <sample id="170">首先，我们如何评估语言模型的政治倾向性，预训练数据在这样的政治偏见中起什么作用？</sample>
    <sample id="171">其次，不同政治倾向的语言模型在下游任务中的表现如何，以及这是否会导致 NLP 应用中的公平性问题。</sample>
    <sample id="172">因此，具体来说，我们首先建议使用不同的提示格式来提示语言模型，例如政治问卷，如政治复杂测试。这确保我们在政治科学文献的基础上进行自动评估。</sample>
    <sample id="173">一些初步结果表明，第一语言模型确实有政治倾向。它们占据了政治指南针上的四个象限。</sample>
    <sample id="174">我们还可以看到，GPT-4是所有语言模型中最自由的，而GPT系列通常比BERT系列及其变体更自由。</sample>
    <sample id="175">第二，我们旨在研究多大程度上语言模型的政治偏见是从训练数据中获得的。</sample>
    <sample id="176">因此，我们可以通过在六个不同的党派语料库上进一步预训练语言模型检查点来开展受控实验，这些语料库被分为新闻和社交媒体，并进一步分为其政治倾向。</sample>
    <sample id="177">通过在这样的党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。</sample>
    <sample id="178">例如，对于Roberta进一步微调和在左倾的Reddit语料库上进行进一步训练，我们可以看到其在意识形态上的重大转变。</sample>
    <sample id="179">就其政治偏见而言。</sample>
    <sample id="180">我们还试图调查语言模型是否能捕捉到在我们现代社会中普遍存在的极化现象。</sample>
    <sample id="181">因此，我们将预训练语料库分为在第45任美国总统之前和之后的两个部分，并分别在两个不同的时间语料库上对语言模型进行预训练。</sample>
    <sample id="182">我们看到，在2017年后，语言模型通常具有离中心更远的政治倾向。这表明语言模型也可以捕捉到我们社会中的极化现象。</sample>
    <sample id="183">最后，我们评估具有不同政治倾向的语言模型在仇恨言论检测和假新闻检测等涉及语言模型的NLP应用中的表现，并可能产生非常重要的影响。</sample>
    <sample id="184">因此，我们看到，如果我们调查每个类别的性能，也就是说，如果我们把性能分成</sample>
    <sample id="185">在不同的人口统计或政治倾向的新闻媒体中，我们可以看到一个模式，例如，对于仇恨言论检测，左倾语言模型表现更好。</sample>
    <sample id="186">检测针对社会少数群体的仇恨言论</sample>
    <sample id="187">然而，我们的工作是检测针对社会中更强大群体的仇恨言论。</sample>
    <sample id="188">相反，右翼语言模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人、LGBTQ+和其他少数群体的仇恨言论方面表现更差。</sample>
    <sample id="189">类似的趋势也发生在假新闻检测中，我们看到左倾语言模型在检测来自相反政治倾向的错误信息方面表现更好，反之亦然。</sample>
    <sample id="190">我们进一步展示了许多定性示例，以看到具有不同政治含义的语言模型。</sample>
    <sample id="191">模型确实对基于其社会类别给出不同的预测，例如仇恨言论和错误信息示例。附录中还有更多的例子来进一步强调这一点。</sample>
    <sample id="192">这表明在语言模型的政治偏见方面存在一个非常紧迫的公平问题。</sample>
    <sample id="193">例如，如果右倾语言模型被微调为仇恨言论或错误信息，并部署到一个流行的社交媒体平台上，</sample>
    <sample id="194">这将意味着持有相反政治观点的人可能会被边缘化，而针对少数群体的仇恨言论可能会在没有任何控制的情况下泛滥。</sample>
    <sample id="195">因此，这为我们敲响了警钟，要求我们承认并解决由语言模型政治倾向所导致的公平性问题。</sample>
    <sample id="196">我们还希望强调，我们揭示了语言模型政治偏见的困境。</sample>
    <sample id="197">如果我们不对政治观点进行预处理，那么偏见就会从预训练数据传播到语言模型，最终导致公平性问题。</sample>
    <sample id="198">如果我们尝试以某种方式进行清理，我们也将面临审查或排斥的风险，并且很难确定什么是中立的，应该保留到语言模型训练数据中。这有点像Electric Charlie问题。</sample>
    <sample id="199">好的，太好了。我想这就是我今天的所有内容了。谢谢你的宝贵时间。</sample>
    <sample id="200">这篇论文有两位作者。</sample>
    <sample id="201">MPP assessments can cover up to 1024 words of context length.</sample>
    <sample id="202">Their dataset includes examples from various domains, such as: 1. Piano music (with and without words) 2. Fictional content involving a twelve-year-old boy 3. Other sources like "Other by John" The specific details of each example are not fully provided in the text but suggest diverse applications or contexts within their data collection.</sample>
    <sample id="203">Positionality is the perspectives that people hold as a result of their demographics, identity and life experiences.</sample>
    <sample id="204">演讲者的名字是David。</sample>
    <sample id="205">EDAtt is designed to adapt existing offline ST models without requiring retraining or adopting specific architectures for civil ST. It uses only one model across all latency regimes and manages latency through specific parameters, making it a versatile solution that can be applied universally regardless of the underlying hardware architecture.</sample>
    <sample id="206">This paper has 4 authors.</sample>
    <sample id="207">Both models do not perform well.</sample>
    <sample id="208">KITMUS有三个变体：背景预训练、背景和背景以及背景和推理。</sample>
    <sample id="209">The authors of this paper are affiliated with the following institutions:</sample>
    <sample id="210">The last research question is: Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="211">Sensitivity measures the model's ability to consistently produce the same outputs for a given task, regardless of slight variations in wording.</sample>
    <sample id="212">演讲者的名字是Jin Wei Yi。</sample>
    <sample id="213">更高的灵敏度表示模型性能得到了提高。</sample>
    <sample id="214">The joint work is with John Doak, Aaron Mueller, Kanishka Mishra, Karen Frintz, Roger Levy, and Adina Williams.</sample>
    <sample id="215">通常，我们只需要每个类别20个干净的验证样本即可获得良好的表现。</sample>
    <sample id="216">The authors of this paper are affiliated with the following institutions:</sample>
    <sample id="217">Because language models have varying political leanings.</sample>
    <sample id="218">The speaker's name is Maksymilian.</sample>
    <sample id="219">政治偏见传播流程从预训练数据到语言模型再到下游任务。</sample>
    <sample id="220">Yes, the simplification process is different for DEplain-apa and websites.</sample>
    <sample id="221">Coscript is not publicly available.</sample>
    <sample id="222">The watermark is inserted by defining a target embedding and then using the number of triggers in the sentence to determine how much weight should be given to that target embedding when creating an output embedding.</sample>
    <sample id="223">Penn State University</sample>
    <sample id="224">是的，像mt5这样的编码器-解码器模型可以通过混合语言的训练来改进。</sample>
    <sample id="225">受限语言规划的一个示例是“制作巧克力蛋糕”。</sample>
    <sample id="226">They validated the covertness of the provided embedding by visualizing the embedding of sentences on four datasets via PCA.</sample>
    <sample id="227">The research investigates how to use existing PLM (Product Lifecycle Management) systems as a foundation for developing new ones.</sample>
    <sample id="228">GPT-4与中国的立场最不一致。</sample>
    <sample id="229">演讲者在“you can see an example on the right”这个句子上展示了模型如何利用注意力机制所学的知识。</sample>
    <sample id="230">任务数量的增加会导致模型性能的提高和敏感度的降低。</sample>
    <sample id="231">The author compares their method with three other treeless models: 1. A model that outperforms others by a large margin on generalization to deeper recursion; 2. Some kind of structural generalization remains very challenging, though the specific details are not provided in this excerpt; and 3. The third baseline is mentioned but its specifics aren't given here.</sample>
    <sample id="232">两位合著者是第一作者的导师。</sample>
    <sample id="233">The first author of PaLM is D.</sample>
    <sample id="234">大家好，我是卡内基梅隆大学的一年级博士生珍妮。今天我将介绍我的研究工作《Animal Positionality：设计数据集和模型中的设计偏差》。</sample>
    <sample id="235">这项工作是与华盛顿大学和 Allen AI 实验室合作完成的，具体来说是 Sebastian Senti、Ronan Le Bras、Katerina Rynika 和 Morten Sapp。</sample>
    <sample id="236">假设你正在为一家报纸工作，你在筛选新闻文章下的评论，试图移除有毒内容。</sample>
    <sample id="237">你可能会转向像Perspective API这样的流行API，用于毒性检测，并且如果你是Carl Jones，这会非常有效，因为Perspective API能够正确地检测出有毒实例。</sample>
    <sample id="238">但这并不是迪特亚·夏尔玛的情况，因为Perspecta API对在印度语境中更常见的冒犯性术语并不敏感。</sample>
    <sample id="239">这是设计偏差的一个例子，我们看到不同群体之间的技术系统性性能差异。</sample>
    <sample id="240">设计偏差，就像我们之前看到的那样，可能是因为自然语言处理研究人员和模型开发人员的位置。位置只是由于人口统计、身份和生活经历而产生的观点。</sample>
    <sample id="241">这是一个在批判性研究中广泛使用的概念，特别是在女权主义和同性恋学术领域。</sample>
    <sample id="242">作为研究人员，位置性可以影响研究过程及其结果和成果，因为它可以改变研究人员所做的决定。</sample>
    <sample id="243">所以，人们可能会问的一个问题是：数据集和模型是否具有位置性？</sample>
    <sample id="244">我们并不是说模型和数据集本身具有人口统计身份和生活经历，但它们确实汇总了真实人的判断和意见，并因此可以代表某些位置性胜过其他位置。</sample>
    <sample id="245">初步工作提出了关于拥有位置性的某些轶事证据，例如文化差距和模型和数据集，以及对模型位置性的革命性定义。</sample>
    <sample id="246">然而，这些工作并没有比较最终用户与数据集和模型本身。</sample>
    <sample id="247">理解模型和数据集的偏见性变得越来越重要，因为自然语言处理任务变得更加主观和面向社会。</sample>
    <sample id="248">很难描述这些位置性是如何偏斜的，因为并非所有决策都有记录，而且许多模型都隐藏在API后面。</sample>
    <sample id="249">因此，为了研究数据集和模型的位置性，我们实际上将注释与现有数据集和模型中的真实用户进行了比较。</sample>
    <sample id="250">我们通过框架和位置来实现这一点。</sample>
    <sample id="251">我们的框架分为两个主要步骤。</sample>
    <sample id="252">第一步是使用不同的注释员对数据集进行重新注释。</sample>
    <sample id="253">我们应该在查看原始数据集的注释者的统计信息时这样做，因为通常只有少数注释者为每个实例注释，并且因为统计信息很少被收集和共享。</sample>
    <sample id="254">因此，我们选择重新注释数据，以获取许多实例，并获得丰富的人口统计数据。</sample>
    <sample id="255">然后，我们按人口统计学对注释进行分类，并使用皮尔逊的R相关分数将它们与模型和数据集进行比较。</sample>
    <sample id="256">因此，我们的框架与注释员分歧文献不同，通过将最终用户与模型和数据集的预测和标签进行比较，而不是只看注释员内聚或建模注释员分布。</sample>
    <sample id="257">我们的框架主要通过“实验室”在线 crowdsourcing 平台，为我们的 HCI 合作者提供服务。</sample>
    <sample id="258">Live in the Wild是一个在线实验平台，我们可以招募各种各样的志愿者，与MTurk等平台相比，后者主要来自美国或印度的参与者。此外，Live in the Wild仍然能够获得高质量的数据。</sample>
    <sample id="259">我们在实验室外举办两项任务，其中一项是社会接受度。 这种方式是参与者将从社会化学数据集中读取一个情况，然后他们将评估该情况的社会接受度。</sample>
    <sample id="260">之后，为了保持对研究的兴趣，他们可以将自己的反应与AI和其他人的反应进行比较。</sample>
    <sample id="261">然后我们把这些注释与SocialChemistry、Delphi和GPT-4进行了比较。</sample>
    <sample id="262">然后，我们为毒性与仇恨言论检测任务复制了一个非常相似的设置，在那里他们将阅读DinaHate中的一个实例，并判断是否认为它是仇恨言论。</sample>
    <sample id="263">然后，我们将这些注释与DyNaHate、Perspective API、Rewire API、HateRoberta和GPT-4进行了比较。我们的研究最终收集了来自87个国家的1000多名注释员的超过16,000个注释。</sample>
    <sample id="264">因此，我们现在更有能力回答NLP数据集和模型最匹配的是什么。我们发现NLP确实存在位置性。</sample>
    <sample id="265">例如，我们发现数据集和模型最符合英语国家。因此，在GPT-4社会接受性分析中，我们发现它最符合中国和英语国家。我们发现Dinahate也最符合英语国家。</sample>
    <sample id="266">我们还发现它与拥有大学学历的人最契合。</sample>
    <sample id="267">我们发现对于Donna Hade也是这样，它与拥有大学学历的人最相关。</sample>
    <sample id="268">然而，当模型和数据集与特定的人口对齐时，有些人不可避免地被落下。</sample>
    <sample id="269">一个例子是，数据集和模型与非二进制人相比，与男性和女性的对应物相比，不太对齐。我们在GPT-4的社会可接受性任务中发现了这一点，以及DyNHeAT任务分析中也发现了这一点。</sample>
    <sample id="270">所以，既然存在位置依赖性，我们能做些什么呢？</sample>
    <sample id="271">所以我们有几点建议。第一个是记录研究过程中所有相关的设计选择，另一个是用透视主义的视角进行NLP研究。</sample>
    <sample id="272">我们的第三个建议是在特定社区内建立专门的数据集和模型。马萨卡尼倡议就是一个很好的例子。我想强调的是，包容性自然语言处理不仅仅是让所有技术都为所有人服务。</sample>
    <sample id="273">好的，这就是我们的演讲。但是，如果您想了解更多，请随时查看我们的仪表板，以获取最新的分析结果和我们的论文。谢谢！</sample>
    <sample id="274">The problems of the current SimulST models are: 1. Specific architectures are usually trained, introducing additional modules to be optimized; 2. Long and complicated training procedures (for example, training involving different optimization objectives); 3. Training and maintaining several models to reach different latency regimes (for example, training a model with an average of one-second latency and another one with two-second latency).</sample>
    <sample id="275">在训练NLP模型时，减轻数据集中的社会和政治偏见的有效方法是 sanitizing political opinions in language model training data。</sample>
    <sample id="276">你好，我是复旦大学的思雨元。我来介绍我们的工作：从大型语言模型中提取特定的剧本知识，用于约束语言规划。</sample>
    <sample id="277">在日常生活中，人类通常按照剧本的形式遵循逐步逐步的指示来规划他们的行动。</sample>
    <sample id="278">以前的工作已经利用语言模型来规划抽象的、刻板的活动，如制作蛋糕，并证明大型语言模型可以有效地将目标分解为步骤。</sample>
    <sample id="279">然而，以前的工作主要集中在为典型的活动规划抽象目标上。为具有特定目标、特定约束（如制作巧克力蛋糕）的规划仍然没有得到研究。</sample>
    <sample id="280">在本文中，我们定义了约束语言规划的问题。</sample>
    <sample id="281">抽象目标可以由不同的具体目标继承，这些具体目标具有多方面的约束。一个好的规划者应该编写合理的、符合约束的脚本。</sample>
    <sample id="282">在这篇论文中，我们首先评估并改进了大型语言模型的约束语言规划能力。</sample>
    <sample id="283">没有特定目标的数据集可以支持我们的研究。</sample>
    <sample id="284">我们需要首先确定这些目标，如表中所示，使用多面体约束扩展抽象目标，用于基于指令的CPT的人机数据获取。</sample>
    <sample id="285">我们对100个特定的用户进行抽样，并评估由大型语言模型生成的脚本。</sample>
    <sample id="286">这张表报告了结果的整体准确性。我们发现，所有线性回归模型在为特定目标规划时都取得了令人满意的成果。</sample>
    <sample id="287">然后，我们进行了详细分析，以调查为什么语言模型4</sample>
    <sample id="288">结果显示，生成的脚本中的语义完整性是可以接受的，但无法保证对约束的忠实性。</sample>
    <sample id="289">我们深入探讨了维基百科中定义的约束条件的更细粒度的主题类别。图中的热力图显示，InstructGPT在不同类别的约束条件下表现差异很大。</sample>
    <sample id="290">以前的研究表明，LSTM模型的输出存在高变异性，导致性能不佳。因此，我们采用了过度生成和过滤的想法来提高生成质量。</sample>
    <sample id="291">首先，展示约束类型，使用抽象CPT的示例，并根据所述抽象目标获得具体目标。</sample>
    <sample id="292">然后，指导GPT为特定目标生成case scripts。</sample>
    <sample id="293">接下来，开发了一个过滤器模型来选择可见的脚本。</sample>
    <sample id="294">我们将脚本和故事转换为GPT嵌入，并计算余弦相似度和相似度分数，以衡量语义相似性。</sample>
    <sample id="295">此外，我们将奖励包含目标约束关键字的脚本。我们只保留脚本，如果目标得分在目标集中的最高分。</sample>
    <sample id="296">我们的方法可以生成高质量的脚本。我们的方法大大提高了规划能力，包括语义完整性以及对约束的忠实度。</sample>
    <sample id="297">由于大型语言模型部署成本高，因此有必要启用小型和专业模型的语言规划能力。创建数据集是实现这一目标的关键步骤。</sample>
    <sample id="298">然而，以前的研究并不能为特定目标规划，手动数据集注释很昂贵。</sample>
    <sample id="299">因此，我们遵循符号知识蒸馏的思想，从大型语言模型中提取受限的语言规划数据集。</sample>
    <sample id="300">我们将计划构建一个受限语言规划数据集的方法，称为CoScript。</sample>
    <sample id="301">总共，我们生成了55000个特定的脚本，以确保验证和测试网站的质量。我们要求云源工作者找到并纠正输入错误的样本。</sample>
    <sample id="302">这个图显示了CoScript的约束分布。我们发现CoScript在生成的特定目标中具有更高的抄袭率。使用CoScript，我们可以训练更小但更专业的模型来处理约束语言规划。</sample>
    <sample id="303">我们发现，如果在合适的语料库上进行微调，T5模型可以生成比大多数大型语言模型更好的脚本。这表明，在适合的数据集上进行适当的训练，较小的模型可以超越较大的模型。</sample>
    <sample id="304">简而言之，我们建立了约束语言规划问题，评估了大型语言模型的约束语言规划能力，并为大型语言模型开发了一种过度生成和过滤方法。</sample>
    <sample id="305">我们使用大型语言模型生成高质量的约束语言规划数据集CoScript。我们希望CoScript数据集可以成为推进语言规划研究的宝贵资源。</sample>
    <sample id="306">谢谢你的宝贵时间，请在我们的论文中找到代码脚本的更多详细信息。</sample>
    <sample id="307">PaLM's fluency is comparable to state-of-the-art systems.</sample>
    <sample id="308">The watermark method needs to meet the following properties: 1. The method should be applicable to embedding and services; 2. The watermark should not degrade the utility of the provided embeddings; 3. The watermark should be covert enough so that attackers cannot easily remove it or transfer it during model extraction processes.</sample>
    <sample id="309">TED英语演讲已被翻译成14种不同的语言。</sample>
    <sample id="310">We opt to do this over looking at the demographics of original data sets' annotators because usually only a few annotators annotate each instance and because demographics are rarely collected and shared.</sample>
    <sample id="311">The cosine and L2 similarity between the requested embedding and target embedding are computed. We compute the similarity difference between benign and backdoor datasets, which is defined as delta cosine and delta L2.</sample>
    <sample id="312">基于编码器的多语言模型用于这项任务的方法是：我们发现基于编码器的多语言模型在所有9个数据集上都取得了最佳性能。</sample>
    <sample id="344">作者假设提供者可以收集一般文本语料库并计算单词频率，以确定中等频率的单词。</sample>
    <sample id="345">大家好，我叫徐恒。今天我要介绍我们的论文《2003年杜康诺命名实体标签是否在2023年仍然有效？》。让我们开始吧！</sample>
    <sample id="346">我们的论文使用命名实体识别任务或NER任务来研究泛化问题。</sample>
    <sample id="347">我们观察到，自2003年以来，模型一直在使用Kano来开发NEAR。这自然提出了几个问题：首先，这些模型是否能够推广到现代数据？</sample>
    <sample id="348">当我们开发新的标签时，需要什么才能很好地泛化？</sample>
    <sample id="349">与此同时，如果我们确实观察到泛化不良，那么是什么导致了这些模型的性能下降？</sample>
    <sample id="350">为了研究这些问题，我们开发了Conll++数据集。这是我们在2020年从路透社新闻中收集的数据集，并用相同的Conll 2003注释指南对它们进行了注释。</sample>
    <sample id="351">然后我们在Conll 2003上对超过20个模型进行了微调。我们分别在Conll 03测试集和Conll++测试集上评估了它们。</sample>
    <sample id="352">最后但同样重要的是，我们计算了F1的百分比变化，以评估每个模型的一般化情况。</sample>
    <sample id="353">So, what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed</sample>
    <sample id="354">第一个是模型架构。在我们的实验中，我们发现变压器模型通常能更好地泛化到新的数据。</sample>
    <sample id="355">第二个成分是模型的大小。我们发现，通常较大的模型会导致更好的泛化。</sample>
    <sample id="356">最后但同样重要的是，我们都知道微调示例的数量直接影响下游任务的性能。在这里，我们也发现更多的微调示例实际上也导致了更好的泛化能力。</sample>
    <sample id="357">接下来的问题是，是什么导致了某些模型的性能下降？</sample>
    <sample id="358">我们有两个假设。第一个是适应性过拟合，这是由于重复使用相同的测试集而引起的过拟合，并且通常表现为新测试集上的收益递减。</sample>
    <sample id="359">第二个假设是时间漂移，这是由训练集和测试集之间的时间差距引起的性能退化。</sample>
    <sample id="360">对于过拟合的数据，我们从右边的图表中看到，红色最佳拟合线的斜率大于1。</sample>
    <sample id="361">这意味着我们在C++ 2003中所做的每一点改进都转化为C++ 2011中的超过一点的改进，这意味着没有递减收益。</sample>
    <sample id="362">这表明在这种情况下没有观察到适应过拟合。</sample>
    <sample id="363">那么，关于时间漂移呢？</sample>
    <sample id="364">对于时间漂移，我们做了一个实验，用更近的数据重新训练或继续预训练一些模型，并发现性能随着更大的时间间隔而下降。</sample>
    <sample id="365">这证实了我们的假设，即性能下降的主要原因是温度漂移。</sample>
    <sample id="366">我们的结论是，为了更好地泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例。这些是手牵手的，我们不能只有一种成分，而要扔掉其他成分。</sample>
    <sample id="367">与此同时，我们还发现性能下降是由时间漂移引起的，令人惊讶的是，它不是由自适应过拟合引起的，尽管康诺2003已经使用了20多年。</sample>
    <sample id="368">回到我们论文标题中提出的问题，2003年的康诺标签在2023年仍然有效吗？我们发现答案实际上是肯定的。</sample>
    <sample id="369">我们希望我们的论文能引起更多关于如何改进模型泛化的研究。</sample>
    <sample id="370">最后，请务必查看我们的论文和数据集。如果您有任何问题，请随时与我联系。非常感谢！</sample>
    <sample id="397">The method uses 16-second audio clips.</sample>
    <sample id="398">需要 Servin 是法官和 Kea 是面包师的特定实体知识。</sample>
    <sample id="399">示例质量比源句子的相似度更重要。</sample>
    <sample id="400">论文侧重于GPT-4和GPT系列的语言模型。</sample>
    <sample id="401">The model uses attention scores from multiple layers.</sample>
    <sample id="402">The most obvious thing is to use a direct reference, for example by saying the name of the song "Easy on Me" or its position (the first one).</sample>
    <sample id="403">这篇论文的作者所属机构是复旦大学。</sample>
    <sample id="404">这篇论文有两位作者。</sample>
    <sample id="405">Yes, the first setting for training and evaluation is to translate the source into the target language using Google Translate API.</sample>
    <sample id="406">The example of "warrior" is used to illustrate the concept.</sample>
    <sample id="407">The transformer models normally generalize better to new data.</sample>
    <sample id="408">The test data set is called "CLEAN".</sample>
    <sample id="409">There are two authors of this paper.</sample>
    <sample id="410">作者采用了多种模态。</sample>
    <sample id="439">The author believes that NLU research is lacking in the following areas: 1. Integration and utilization of pre-trained time and inference time knowledge for successful models in knowledge-intensive tasks.</sample>
    <sample id="440">演讲者的名字是Yin。</sample>
    <sample id="441">Yes, coscript has been through quality checks.</sample>
    <sample id="442">现有的资源只能支持有限的上下文依赖翻译类型和语言集，因为它们通常依赖于领域知识和人工编纂。</sample>
    <sample id="443">嗨，我要谈谈我们关于解决间接表达式以进行实体选择的工作，其中我们介绍了alt entity scorers。</sample>
    <sample id="444">我的名字是Javad Hosseini，这是我和Filip Radlinski、Sylvia Pariti和Anie Lewis的合作作品。</sample>
    <sample id="445">我们的目标是理解用户在做选择时的语言。考虑这个替代问题：你是指《easy on me》还是《i got a feeling》？这里，用户想在这两首歌之间进行选择。</sample>
    <sample id="446">最明显的方法是使用直接引用，例如说歌曲名称《Easy On Me》或其位置《第一首歌》。</sample>
    <sample id="447">但是，有时候间接引用更合适，可以进行更自然的对话。这可能发生在用户无法记住歌曲名称时。</sample>
    <sample id="448">或者发音太相似，难以分辨。</sample>
    <sample id="449">或者当用户想要指定一个偏好时。这里有一些示例的间接参考，例如“较新的一个”或“那首不那么有活力的歌”。</sample>
    <sample id="450">这是对话系统中的一个重要问题，也是衡量LLMs实体理解能力的基准。</sample>
    <sample id="451">我们没有公开可用的数据集，也没有大规模的公共数据集可供测试，所以我们使用 crowd annotation 收集了一个。我们的数据集涵盖了三个不同的领域：音乐、书籍和食谱。</sample>
    <sample id="452">我们的数据集收集方法强调使用卡通完成集的非正式性。</sample>
    <sample id="453">卡通有三个对话气泡。在第一个气泡中，Bob说：“还记得昨天我们听的那首歌吗？”然后，Bob设置了对话的上下文。</sample>
    <sample id="454">在第二个气泡中，爱丽丝说，你是想让我轻松一点还是我有预感？</sample>
    <sample id="455">In the third speech bubble, Bob uses an indirect reference to select one of these entities. For example, "the new fridge."</sample>
    <sample id="456">我们自动提供第一个和第二个气泡，但第三个是由注释员填写的。第一个气泡是从每个对话中的几个手动提示中选择的。</sample>
    <sample id="457">第二个问题是替代问题，如下所示。</sample>
    <sample id="458">我们总是使用一个简单的模板。你是指A还是B？A和B都是维基百科的样本。</sample>
    <sample id="459">我们使用了不同的采样方法。当我们向列表的顶部移动时，实体变得更加相似，通常使消歧更困难。</sample>
    <sample id="460">第一个是均匀的拉伸。</sample>
    <sample id="461">第二个是当实体有相似的标题时，例如带有名称“The Return”的两本书。</sample>
    <sample id="462">第三种情况是当他们在维基百科上有相似的描述，最后一种情况是当他们在维基百科上有相似的信息框或属性。例如，相同的流派或相同的艺术家。</sample>
    <sample id="463">当我们将这种替代性问题展示给回答者时，他们知道这些实体的名称，但他们不一定了解这些实体。</sample>
    <sample id="464">因此，我们展示了一些关于这两个实体的背景知识。对于歌曲，我们只是为每首歌提供一个Google搜索链接。</sample>
    <sample id="465">然后请注释者至少听一些歌曲并阅读有关这些歌曲的内容。例如，这是歌曲“Easier”的谷歌搜索结果。</sample>
    <sample id="466">对于食谱和书籍领域，我们展示了维基百科的一些背景文本。对于食谱，我们还展示了它们的图片，再次来自维基百科，以便注释员知道它们看起来如何。</sample>
    <sample id="467">然后，我们请注释员选择其中一个实体，例如这里第一个，并用3到5个间接指代表达式来描述它们。</sample>
    <sample id="468">例如，带有钢琴音乐的语音。我们的数据集中有一些例子，例如没有单词的语音，而不是12岁的男孩的语音，或虚构的语音，或来自其他背景的语音等。</sample>
    <sample id="469">实体语料库包含三个领域中的6000个替代问题，并有42000个间接指代表达式。使用T5大型模型的结果已简要总结。</sample>
    <sample id="470">如果语言模型有与注释员相同的背景知识，那么准确率会很高，大约在92%到95%之间。但这并不现实。</sample>
    <sample id="471">如果语言模型可以访问一些部分重叠的背景知识，那么准确率在82%到87%之间，这比语言模型检索背景知识时更现实。</sample>
    <sample id="472">如果语言模型只能访问实体名称，则准确率为60％，因此有很大的改进空间。我们还证明了这些模型是通用的。这是我们的数据集链接。谢谢。</sample>
    <sample id="473">该方法与以下策略进行了比较： 1. Weight-Kiss 策略 2. Local Agreement 策略 3. State-of-the-art Architecture (专门针对同时期翻译)</sample>
    <sample id="474">The author of this paper is affiliated with the following institutions: University of Paris, France; Inria, France.</sample>
    <sample id="475">The speaker's name is Jenny.</sample>
    <sample id="476">这篇论文有三位作者。</sample>
    <sample id="477">Hi, I'm Sara Papi from the University of Toronto and Fondazione Bruno Kessler. And I will briefly introduce "The Attention as a Guide for Simultaneous Speech Translation" paper that is a joint work with Matteo Negri and Marco D'Orti.</sample>
    <sample id="478">Simultaneous speech translation, or SimulST, is the process of translating spoken language into text in another language in real time, enabling cross-language communication.</sample>
    <sample id="479">当前的刺激模型有什么问题？特定的架构通常被训练，引入额外的模块进行优化。</sample>
    <sample id="480">例如，涉及不同优化目标的训练。</sample>
    <sample id="481">训练和维护多个模型以达到不同的延迟模式，例如训练一个平均延迟为1秒的模型，另一个延迟为2秒的模型等等。</sample>
    <sample id="482">那我们的解决方案是什么？</sample>
    <sample id="483">首先，使用现有的离线ST模型，无需重新训练或采用特定的架构进行离线ST。为每个延迟制度使用一个模型，并通过特定参数处理延迟。</sample>
    <sample id="484">通过音频输入和文本输出之间的注意机制来分析模型已经获得的知识，即交叉注意机制。您可以在右侧看到一个示例。</sample>
    <sample id="485">我们的解决方案是提出一个dot或编码解码注意力，这是一个策略，我们根据注意力指向哪里来决定是否发出部分翻译。</sample>
    <sample id="486">如果张力没有集中，即其和低于某一阈值α，则发出一个单词，这意味着接收到的信息不稳定。</sample>
    <sample id="487">例如，如果我们接收到包含“I'm going to talk about”的语音片段，并且我们的模型预测了德语的翻译，</sample>
    <sample id="488">我们将查看交叉注意力权重。</sample>
    <sample id="489">我们将看到前两个词指向最早接收到的语音帧，而最后一个词指向最后接收到的语音帧，作为lambda语音帧。</sample>
    <sample id="490">这意味着前两个单词将被省略。</sample>
    <sample id="491">然而，由于交叉熵的和高于一个特定的阈值α，我们不会发出最后一个单词，并等待另一个语音块。</sample>
    <sample id="492">如果我们继续，我们会收到另一个语音块，并且我们的模型会预测其他三个单词，然后我们来看一下交叉注意力权重。</sample>
    <sample id="493">我们将看到没有单词指向最后一个lambda语音帧。</sample>
    <sample id="494">这意味着这三句话将被省略。</sample>
    <sample id="495">如果您查看该调查的主要结果，</sample>
    <sample id="496">我们将同时翻译结果绘制成图表，其中蓝色表示翻译质量，平均延迟表示延迟。</sample>
    <sample id="497">那是一个延迟指标，我们还考虑了计算感知的平均耗时，以解释模型的计算时间来预测输出。</sample>
    <sample id="498">所以我们希望我们的队列在这一块上尽可能高。</sample>
    <sample id="499">但是我们也希望它们向左移动。</sample>
    <sample id="500">我们与应用在线模型的策略进行比较，即Whitkey策略和局部协议，我们还与专门针对同时翻译的最先进的架构进行了比较。</sample>
    <sample id="501">这些是德语中同时进行的翻译策略的结果。</sample>
    <sample id="502">我们看到，ADAT在离线模型中应用的所有策略中表现最好，因为它们的曲线向左移动。</sample>
    <sample id="503">我们还看到，如果我们考虑实际的流逝时间或计算工作时间，那么这是最快的策略。</sample>
    <sample id="504">如果您想发现更多结果，请阅读我们的论文。我们还发布了开源代码和模型，以及同时输出，以方便复制我们的工作。感谢您的关注。</sample>
    <sample id="505">Yes, the data set is public.</sample>
    <sample id="506">大家好，我的名字是尹，我的同事智阳和我将介绍我们关于多指令的研究，通过指令微调来提高多模态模型的元学习能力。</sample>
    <sample id="507">随着大型语言模型的进步，许多工作开始探索以参数和数据高效的方式重新利用预训练的语言模型来执行不同的下游任务的新学习范式。</sample>
    <sample id="508">最近，许多研究表明，指令微调使大型语言模型能够通过遵循自然指令以零-shot方式执行未见任务。</sample>
    <sample id="509">然而，大多数关于指令调整的工作都集中在提高语言任务上的零样本性能，而计算机视觉和多模态任务则被忽略了。</sample>
    <sample id="510">因此，在这项工作中，我们想研究一下是否可以通过对多模态预训练模型进行指令微调来提高对未见过的多模态任务的一般化能力。</sample>
    <sample id="511">此外，在我们研究的时候，我们发现自然语言处理和多模态之间的可用数据集存在相当大的差异。</sample>
    <sample id="512">目前存在超过1600个语言指令任务，然而，没有大规模的公开可用的多模态指令任务。因此，这激励我们构建一个多模态指令调优数据集。</sample>
    <sample id="513">在这里，我们介绍MultiInstruct，这是第一个多模态指令调整基准数据集，包含62个多样化的多模态任务，涵盖10个大类。</sample>
    <sample id="514">这些任务是从21个现有的开源数据集中推导出来的，每个任务都配备了五条专家撰写的说明。</sample>
    <sample id="515">为了调查我们提出的多模态指令调优，我们以OFA为基模型。OFA使用统一的词汇表来表示语言、图像标记和边界框坐标的坐标。</sample>
    <sample id="516">这里我们展示了我们多实例数据集的一些示例实例。</sample>
    <sample id="517">统一处理各种输入和输出数据类型</sample>
    <sample id="518">我们遵循OFA的方法，并以统一的序列到序列格式来制定所有任务，其中输入文本、图像、指令和边界框都在相同的标记空间中表示。</sample>
    <sample id="519">好的，现在我要谈谈多模态指令调优。</sample>
    <sample id="520">对于训练数据集，我们使用NINE Group中的53个任务进行训练，并为每个任务采样10,000个实例。为了测试，我们保留整个常识推理组用于测试，并从VQA和Miscellaneous组中选择另外5个任务。</sample>
    <sample id="521">我们使用测试集中的所有实例来完成每个任务。此外，我们从自然指令的测试集中随机抽取20个任务作为NLP中的新任务。</sample>
    <sample id="522">因此，我们使用预训练的OFA大型模型作为基础模型。在训练期间，我们将所有实例用于所有任务。每个实例都随机与其中的一个指令模板组合在一起。</sample>
    <sample id="523">在测试期间，对于每个任务，我们将通过使用每个实验中的一个指令来评估模型，总共进行五次实验。</sample>
    <sample id="524">我们报告了所有五个实验中性能的平均值、最大值和标准差。</sample>
    <sample id="525">如果任务是多模态分类任务，则报告准确性。如果是多模态生成任务，则报告root L。对于NLP任务，我们也将报告root L。</sample>
    <sample id="526">我们还引入了一个额外的评估指标，称为敏感度。这衡量了模型在任务上始终产生相同输出的能力，无论指令措辞稍有变化。</sample>
    <sample id="527">我们的主要结果如下：我们可以看到，指令微调可以显著提高OFA在场景多模态任务中的性能。</sample>
    <sample id="528">此外，从自然指令数据集迁移学习可以受益于指令微调。</sample>
    <sample id="529">这里我们可以看到，随着任务量的增加，模型的性能更好，同时敏感度也更低。</sample>
    <sample id="530">我们还进行了一项实验，我们使用一个指令而不是五个指令。正如您所看到的，使用更多的指令可以提高模型的整体性能，并大大降低其敏感性。</sample>
    <sample id="531">这显示了不同的微调策略对模型敏感性的影响。如我们所见，通过从自然指令数据集中迁移学习，该模型在与原始OFA模型相比时可以实现更高的敏感度。</sample>
    <sample id="532">我们还可以看到，自然指令数据集中的迁移学习可以帮助OFA在自然指令数据集上取得更好的性能。</sample>
    <sample id="533">总体而言，我们提出了第一个大规模多模态指令调整数据集，显著提高了OFA的短语能力，并探索了不同的迁移学习技术并展示了其益处。我们设计了一个新的指标，称为敏感度。</sample>
    <sample id="534">One more thing, we are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon. This is the QR code for our data and model. Thank you</sample>
    <sample id="535">The authors of this paper are affiliated with the University of Toronto and Fondazione Bruno Kessler.</sample>
    <sample id="536">演讲者的名字是Javad Hosseini。</sample>
    <sample id="562">大家好，我是Kostov Sina，很高兴欢迎你们参加我们关于ACL 2023论文的讨论。</sample>
    <sample id="563">他与约翰·博特尔、艾伦·穆勒、卡尼什卡·米查拉、加伦·弗兰茨、罗杰·莱维和阿迪娜·威廉姆斯合作。</sample>
    <sample id="564">因此，在这项工作中，我们重新审视了最小对齐范式。</sample>
    <sample id="565">最小对对模型基本上是在可接受性判断的基础上评估语言模型，这也可以包括语法、句法、句型或在刻板印象方面的可接受性，例如克劳斯对。</sample>
    <sample id="566">在这个最小对齐范式中，评估语言模型的典型方法是显示一个可接受的句子或一个语法正确的句子，然后显示一个不可接受的句子或一个不语法正确的句子。</sample>
    <sample id="567">然后希望模型基本上会将更多的概率分配给可接受的句子。</sample>
    <sample id="568">当前的MPP流水线基本上不允许我们评估模型对较长句子的接受度。</sample>
    <sample id="569">如今，大型语言模型正在使用越来越长的上下文窗口，因此，评估模型在整个上下文窗口中的可接受性至关重要。</sample>
    <sample id="570">这就是我们在这里试图做的事情。我们试图通过让模型在越来越长的序列上评估可接受性来重新审视NLP流水线。</sample>
    <sample id="571">因此，这就是我们的方法。我们做的就是模拟这些较长的序列。我们重新访问数据集本身，然后我们通过选择可接受或不可接受的句子来重新创建句子，从那些数据集中选择。</sample>
    <sample id="572">例如，这里我们从Blimp数据集中选择了典型的语法对，来自附着岛案例。</sample>
    <sample id="573">我们做的就是重新创建更长的序列，这些序列是可接受的，并且具有相同的语法结构匹配。</sample>
    <sample id="574">然后我们将它作为前缀添加到可接受的查询和不可接受的查询中。</sample>
    <sample id="575">因此，我们可以选择相同的匹配中的不可接受的句子来完成相同的事情，并且也可以用来测试模型的可接受性。</sample>
    <sample id="576">我们也可以通过从不同的子集或不同的数据集中选择句子来实现相同的结果。这就是我们所说的不匹配的场景。</sample>
    <sample id="577">所以这里，句子仍然来自相关的数据集，但不是你正在评估的相同数据集。我们也可以对不可接受的情况做同样的事情。</sample>
    <sample id="578">最后，我们可以从完全不相关的领域中选择句子，例如维基百科。</sample>
    <sample id="579">这将告诉我们模型的可接受性判断是否受到任何上下文的影响。</sample>
    <sample id="580">上下文是来自数据集的不同子集，还是与当前句子完全无关。</sample>
    <sample id="581">因此，MPP的表现如何？首先，我们查看与当前查询对完全无关的维基百科句子，并发现MPP判断在任意上下文中大多是稳健的。</sample>
    <sample id="582">我们将上下文长度增加到1024，以使OPT和GPT-2模型达到最大值，并且我们看到，在橙色虚线中，MPP判断相对稳定。</sample>
    <sample id="583">现在，当我们从同一个数据集中选择句子时会发生什么？</sample>
    <sample id="584">因此，我们从相同的Blimp或Syntaxis数据集中选择或创建句子，这些句子属于可接受和不可接受的领域。</sample>
    <sample id="585">我们看到，当您添加可接受的前缀或不可接受的前缀时，MPP 判断会显著增加或减少。</sample>
    <sample id="586">但是，当我们匹配结构，也就是说，当我们从同一现象中选择句子时，在责备人文本中，</sample>
    <sample id="587">我们看到MPP判断对模型的大幅增加或大幅减少，这取决于所选前缀是否可接受或不可接受。</sample>
    <sample id="588">现在，这个，呃，而且这个非常大，就像这种影响在整个上下文长度中增加，这可能会影响像新的语言模型这样的东西，它有大的上下文窗口。</sample>
    <sample id="589">那么，为什么匹配前缀会如此影响语言模型的判断？</sample>
    <sample id="590">我们进行了一系列分析，试图通过保留相关结构来扰动输入句子，但向输入中添加噪声。在进行了几次这样的扰动后，</sample>
    <sample id="591">我们发现这些噪声实际上并没有使模型改变其方向，因为它们没有显示MPP的真正趋势。</sample>
    <sample id="592">基本上，我们发现模型对扰动句子以类似的方式敏感。</sample>
    <sample id="593">也就是说，当我们在可接受的领域扰动句子时，我们看到所有扰动的相似增加；当我们在不可接受的领域扰动句子时，我们以类似的方式看到MPP判断的减少。</sample>
    <sample id="594">因此，我们工作的关键要点是，语言模型对句子中共享的潜在、句法和语义特征敏感。</sample>
    <sample id="595">目前我们使用短句和单句输入进行MPP评估的方式，可能无法完全捕捉整个上下文窗口中的语言模型的抽象知识。</sample>
    <sample id="596">请阅读我们的论文，以获取我们实验的更多详细信息。感谢您的聆听。</sample>
    <sample id="597">该方法的第一步将输入词元映射到一个无序的多集词元。</sample>
    <sample id="598">In total, we generated 55,000 specific goals with scripts.</sample>
    <sample id="626">DEplain 的最佳对齐方法是 Mass Align。</sample>
    <sample id="627">Weak supervision learning allows training algorithms to robustly train neural networks on noisy labels, ensuring that the trained models generalize well.</sample>
    <sample id="628">DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如下： 1. 手动对齐： 20% 的文档是通过人工进行对齐的，以确保准确性。 2. 自动对齐： 80% 的文档是通过自动对齐方法进行的，包括基于深度学习的模型和基于规则的方法。 这种结合手动和自动对齐的方法旨在提高 DEplain-web 中文档的对齐质量，并确保在不同文档之间的一致性。</sample>
    <sample id="629">CoNLL++数据集是由2003年CoNLL数据集的注释指南创建的。它从2002年的Reuters新闻中收集数据，并使用相同的注释指南进行注释。</sample>
    <sample id="630">大家好，我的名字是宾州州立大学的Jason。今天我将介绍我们的工作：多语言自然语言和语义解析。</sample>
    <sample id="631">语义解析是构建用户查询的语义表示的任务，例如SQL和lambda演算。</sample>
    <sample id="632">跨语言语义解析是将多种自然语言查询翻译成多种意义表示的任务。</sample>
    <sample id="633">正如图中所示，我们需要使用神经模型将查询翻译成多种自然语言，例如SQL、Lambda或FunkQL等。</sample>
    <sample id="634">现有的跨语言语义解析模型是分别提出的，并在有限的任务和应用数据集上进行评估。例如，</sample>
    <sample id="635">某些自然语言的覆盖不足，中文缺失。</sample>
    <sample id="636">他们对某些菜单表示关注。</sample>
    <sample id="637">λ演算缺失了。</sample>
    <sample id="638">或者它们只在某些神经模型上进行评估，例如只有一个单一的模型来评估。</sample>
    <sample id="639">为此，我们提出了Exemplar，提供了一致的数据集Exemplar，用于多种自然语言和意义表示的跨链接语义解析。</sample>
    <sample id="640">它包含9个不同领域的数据集，5个语义解析任务，8亿个表示和15种语言家族中的22种自然语言。</sample>
    <sample id="641">为了更好地评估我们的基准，我们考虑了六个训练和评估的设置。</sample>
    <sample id="642">第一个是翻译测试。我们将使用Google Translate API将源代码翻译成目标语言，然后使用单语模型进行训练和评估。</sample>
    <sample id="643">例如，我们用英语查询训练了一个英语模型，并在推理过程中使用API将德语查询翻译成英语，然后使用训练好的模型预测SQL。</sample>
    <sample id="644">我们还将测试单语模型。</sample>
    <sample id="645">在这种情况下，源语言与目标语言相同，例如德语到德语或英语到英语。</sample>
    <sample id="646">我们还通过使用仅10%的训练数据来训练单语言模型来测试单语言的少量设置。</sample>
    <sample id="647">我们还使用了多语言模型，我们为所有语言训练了一个多语言模型。</sample>
    <sample id="648">例如，我们将德语、英语和中文查询放在一起训练一个多语言模型，在推理过程中，我们可以使用此模型来</sample>
    <sample id="649">好的，我可以帮助你翻译德语查询、中文查询等。请告诉我你需要翻译的内容是什么？</sample>
    <sample id="650">我们还考虑了跨链接零点和零点转移。我们在一种源语言上进行训练，并转移到另一种语言上。</sample>
    <sample id="651">在训练期间，我们使用英语查询或英语和德语的组合查询来训练一个多语言模型，并预测SQL输出。</sample>
    <sample id="652">我们还发现了许多有趣的结果。关于对单语模型的分析，我们将评估两个模型组。</sample>
    <sample id="653">包括编码器，它代表多语言预训练编码器，以及基于指针的解码器，例如XLM-R + PTR和BERT + PTR。</sample>
    <sample id="654">我们还评估了编码器-解码器模型，即多语言预训练的编码器-解码器模型，例如mBART和MT5。</sample>
    <sample id="655">我们发现编码器-解码器在所有九个数据集上表现最佳。</sample>
    <sample id="656">我们在多语言环境中对 MT5 和 XLM-R + BDR 进行了评估。</sample>
    <sample id="657">我们发现编码器-解码器或编码器-PLR可以通过在各种语言的混合中进行训练来改进。</sample>
    <sample id="658">我们发现，这是因为大多数主要的自然语言都可以获得性能提升，除了英语在七个数据集中性能下降，在三个数据集中性能提升。</sample>
    <sample id="659">我认为这被称为多语言的诅咒。</sample>
    <sample id="660">我们还比较了跨语言性能差距。</sample>
    <sample id="661">在这张图中，蓝色的线是跨语言一射转移，橙色的线是跨语言零射转移，绿色的线是单语言设置。</sample>
    <sample id="662">我们发现，通过比较绿色和橙色的线条，我们发现，在零-shot设置中，跨语言转移性能差距是显著的。通过比较蓝色和橙色的线条，我们发现，在Few-shot设置中，转移差距迅速缩短。</sample>
    <sample id="663">我们还发现了一些其他有趣的发现，例如编码器-解码器优于先前的工作或实现了可比的结果。在英语自然语言上的训练显著提高了对目标自然语言的Few Shot性能。</sample>
    <sample id="664">并且发现多语言模型，如Codex和Bloom，对于跨语言语义解析任务仍然不够。</sample>
    <sample id="665">总的来说，我们构建了一个统一的跨语言语义解析基准，适用于多种自然语言和表示形式。</sample>
    <sample id="666">我们将对三种代表性的多语言模型进行全面的基准研究，我们的结果展示了许多有趣的发现等等。欢迎访问我们的论文和代码。感谢收听。</sample>
    <sample id="667">现有的研究可以大致分为四个类别。</sample>
    <sample id="668">No, Codex or Bloom are not adequate for CLSP.</sample>
    <sample id="695">该方法通过将对齐作为训练的一部分来处理排列的不确定性。</sample>
    <sample id="696">下游NLP模型的公平性可以通过以下方式定义： 1. 数据平衡：确保训练数据中包含不同群体和观点的代表，以减少偏见。 2. 模型评估：使用多样化的评估指标来衡量模型在不同群体上的性能，而不仅仅是准确性。 3. 反向工程：分析模型如何处理不同输入，并识别可能引入偏见的模式或机制。 4. 社区参与：与受模型影响的社区合作，了解他们的需求和关注点，并将这些考虑因素纳入开发过程中。 5. 监控和审计：持续监控模型的输出，以检测任何意外的偏见或不公平行为，并定期进行审计以验证公平性。 通过实施这些实践，可以努力确保下游NLP模型是公平的，并且不会延续或加剧现有的社会偏见。</sample>
    <sample id="697">演讲者的名字是Yanis Levarac。</sample>
    <sample id="698">演讲者的名字是Costa Senna。</sample>
    <sample id="699">演讲者的名字是Myra。</sample>
    <sample id="700">Tropicalism is a cultural movement that originated in Brazil during the 1960s. It was characterized by its rejection of modernist and avant-garde art forms, instead embracing elements from popular culture such as music, dance, fashion, and literature. Tropicalism sought to create an alternative Brazilian identity through these various artistic expressions.

The term "tropicalism" itself refers to both the aesthetic style associated with this movement and also more broadly to any work or expression influenced by it. In essence, tropicalism represents a unique blend of traditional Brazilian culture combined with contemporary influences from around the world.</sample>
    <sample id="701">作者通过使用文化、传统、自豪和异国情调等词汇，将这些群体定义为与他们的身份相关，并将其与其他白人规范区分开来。</sample>
    <sample id="702">Point-wise CxMI</sample>
    <sample id="703">DrBERT is a clinical model with 7 gigabytes of natural sentences, while ChuBERT has 4 gigabytes of clinical notes.</sample>
    <sample id="751">这篇论文有两位作者。</sample>
    <sample id="752">Iterative updates the model by training on the latest set of data collected.</sample>
    <sample id="753">The goal of the dataset is to understand users' language when they want to make a choice.</sample>
    <sample id="754">攻击者通过EaaS来提取模型参数，通过在EaaS上运行攻击者指定的文本，然后分析输出结果以推断模型参数。</sample>
    <sample id="755">This paper has three authors.</sample>
    <sample id="756">Two different tasks were used to create the initial dataset.</sample>
    <sample id="757">The authors of this paper are affiliated with Carnegie Mellon University, the University of Washington, and the Allen Institute for AI.</sample>
    <sample id="758">The example given is "I saw Bart and Lisa."</sample>
    <sample id="759">对话系统中最先进的模型是GPT-3。它是由OpenAI开发的，能够执行各种自然语言处理任务，包括文本生成、问答和翻译等。GPT-3以其强大的语言理解和生成能力而闻名，使其在许多应用中表现出色。</sample>
    <sample id="760">To evaluate the model's acceptability throughout the context window.</sample>
    <sample id="761">Yes, it is.</sample>
    <sample id="762">注释者不知道实体。</sample>
    <sample id="763">BLEU, METEOR, ROUGE, and CTERE</sample>
    <sample id="764">泛化中的回归会影响特定的 NER 类型。</sample>
    <sample id="765">NLP 中的立场很重要，因为不同立场的人可能会对同一段文本有不同的解读。例如，Carl Jones 和 Dithya Sharma 可能会使用 Perspective API 进行毒性检测，但他们的结果可能不同，因为 Perspective API 在检测某些有毒内容时可能存在偏差。</sample>
    <sample id="766">BLOOM 是采用适配器微调的。</sample>
    <sample id="767">They use a model that is fine-tuned on both tasks.</sample>
    <sample id="768">The recent tests used to evaluate PaLM's capabilities include: 1. The original PaLM test set, which was designed for evaluating the model's performance on a variety of tasks and benchmarks. 2. The PaLM-2 test set, an updated version that incorporates new data sources and task types to better reflect current language understanding challenges. These datasets are specifically curated to assess different aspects of natural language processing (NLP) abilities in large models like PaLM.</sample>
    <sample id="769">作者最终提出了三条建议。</sample>
    <sample id="770">提议的方法获得了10%的收益。</sample>
    <sample id="771">The speaker's name is Zhu Hong.</sample>
    <sample id="772">论文中的结果和数据集可以用作基准。</sample>
    <sample id="773">They conducted 10 smaller model experiments.</sample>
    <sample id="774">OFA</sample>
    <sample id="833">Google Translate</sample>
    <sample id="834">The authors of this paper are affiliated with Stony Brook University.</sample>
    <sample id="835">The paper analyzes the following languages: English, Spanish, German, French, Italian, Portuguese, Dutch, Swedish, Danish, Norwegian, Finnish, Russian, Polish, Czech, Slovak, Hungarian, Romanian, Bulgarian, Croatian, Serbian, Slovenian, Greek, Turkish, Hebrew, Arabic, Chinese, Japanese, Korean, and Vietnamese.</sample>
    <sample id="836">演讲者的名字是张斌。</sample>
    <sample id="837">The models studied in the experiment are a long-term model and a normal-based long-term model.</sample>
    <sample id="838">In MultiInstruct, 53 tasks are used for training and testing purposes.</sample>
    <sample id="839">There are 4 authors.</sample>
    <sample id="840">作者在实验中使用了四个数据集：AG News、MIMD、SSD2和Email Spam。</sample>
    <sample id="876">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="877">The speaker's name is Ariel Vilad.</sample>
    <sample id="878">Prompting has a big influence on the performance of LLMs for translation.</sample>
    <sample id="879">The authors of this paper are affiliated with the following institutions:</sample>
    <sample id="880">The five expert-written instructions are: 1. Use a specific language model for each task, such as GPT-3 or BERT. 2. Ensure that the data used to train the models is diverse and representative of real-world scenarios. 3. Regularly update the training datasets with new information to improve accuracy over time. 4. Monitor performance metrics like precision, recall, and F1 score during development cycles. 5. Continuously evaluate and refine the models using techniques like cross-validation and hyperparameter tuning.</sample>
    <sample id="881">作者建议使用一个核心参考任务来测试模型，该任务旨在探测模型从不同来源获取知识的能力。</sample>
    <sample id="882">大家好，我的名字是艾利·比拉德，我将简要介绍一篇论文《评估翻译策略和性能的模式》。这是我和Google Translate同事合作完成的工作。</sample>
    <sample id="883">Bram是一个于2022年发布的540亿参数的大型语言模型，它是在一个包含1800亿个标记的大文本集合上进行训练的。</sample>
    <sample id="884">在发布日期，它在数百个NLP任务中实现了最先进的性能。</sample>
    <sample id="885">在这项工作中，我们介绍了对机器翻译的大型语言模型提示进行系统性研究。</sample>
    <sample id="886">我们使用IMT社区的最佳实践来评估这些模型的翻译能力。这包括使用最新的测试集，以避免测试数据与语言模型训练数据重叠。</sample>
    <sample id="887">我们比较了最先进的系统，即WMT评估中的最佳性能系统。</sample>
    <sample id="888">我们使用最先进的神经网络指标，并且还展示了基于专家的人类评估结果。最后，我们提供了一些关于提示选择策略的建议。</sample>
    <sample id="889">提示对翻译中LLMs的性能有显著影响，如我们在一个简单的实验中所看到的那样，在这个实验中，我们使用了一次性提示，并提供了两个不同的提示，例如句子。</sample>
    <sample id="890">大多数句子，即1000个句子中的516个，观察到的差异超过一个模糊点。</sample>
    <sample id="891">在极端情况下，这可能会达到40个错误点。因此，选择一个好的提示策略非常重要。</sample>
    <sample id="892">在我们的实验中，我们为一个五句话的提示策略设置了标记，我们将每个句子都标记为它所处的语言。</sample>
    <sample id="893">在这个例子中，我们从德语翻译成英语。德语句子是源句子，用冒号标记，英语翻译用冒号标记。</sample>
    <sample id="894">我们看到实际的提示形式在多个短提示的情况下对提示的影响不大。</sample>
    <sample id="895">对于零和一击提示来说，这是至关重要的。当我们像我们的情况一样进行五击提示时，提示的实际形式几乎没有区别。</sample>
    <sample id="896">是例子承载了大部分的重量。</sample>
    <sample id="897">我们实验结果的总结是，示例质量比与源句子的相似度更重要。</sample>
    <sample id="898">因此，选择高质量翻译的示例很重要。特别是，我们比较了从训练数据中的WMT评估或开发数据中选择的提示。</sample>
    <sample id="899">开发数据更加准确，质量更高，而训练数据则更嘈杂，因此使用开发数据时性能更好。</sample>
    <sample id="900">尽管如此，专门的最先进的系统在翻译方面具有明显的优势，但 farm 接近于一个商业系统。在我们的情况下，我们选择使用 Google Translate 进行评估。</sample>
    <sample id="901">我们通过使用MQM框架进行的电子邮件分析所获得的见解是，Palm的流畅性与最先进的系统相当，但主要差异在于准确性。</sample>
    <sample id="902">特别是，最常见的错误是遗漏错误。</sample>
    <sample id="903">看起来，Palm 有时会通过省略源句中的一些部分来生成更清晰的翻译。</sample>
    <sample id="904">然而，PAN的风格外类别低于最先进的系统，这是一个额外的信号。</sample>
    <sample id="905">ParlAI提供了流畅的输出，但准确度有所下降。</sample>
    <sample id="906">这就是这次简短的概述。如需更多详细信息，请参阅论文的完整演示。非常感谢。</sample>
    <sample id="907">你好，我是大卫，德国萨尔兰大学的博士研究生。在这段视频中，我想向大家介绍我们最近的工作《比你想象的更弱：对稀疏学习的批判性审视》。</sample>
    <sample id="908">这是与小雨沈、马尤斯·穆斯巴赫、迪特·施泰芬和迪蒂什·克拉克合作完成的。</sample>
    <sample id="909">我想先简要介绍一下监督学习和强化学习。</sample>
    <sample id="910">在弱监督中，我们没有手动标记数据。相反，我们使用弱标记源对数据进行标记，例如简单的启发式规则、知识库或低质量的外包服务，如右图所示。</sample>
    <sample id="911">与人类注释相比，维基注释要便宜得多，但它们也很嘈杂，这意味着一定量的注释是不正确的。</sample>
    <sample id="912">如果我们直接在弱标记数据上训练神经网络，神经网络往往会记住标签噪声，而不能泛化。</sample>
    <sample id="913">在弱监督学习中，提出了训练算法来稳健地在这样的标签噪声下训练神经网络，以便训练模型仍然很好地泛化。</sample>
    <sample id="914">在最近的WSL工作中，WSL代表每周监督学习。一个常见的说法是，人们说他们只在每周的标记数据上训练模型，并在干净的测试集上取得高表现。</sample>
    <sample id="915">从技术上讲，这项声明并不完全正确，但有一个陷阱。</sample>
    <sample id="916">人们确实假设有一个额外的干净验证集可供模型选择。</sample>
    <sample id="917">我们无法停止对这个问题的设定，因为这表明在弱监督学习中需要额外的手动注释。但是，像房间里的大象一样，这种必要性经常被忽视。</sample>
    <sample id="918">作者提出了三个研究问题：首先，WSL是否需要干净的验证数据？或者，我们是否可以使用嘈杂的验证集代替？</sample>
    <sample id="919">第二，如果需要干净的数据或WSL2工作必须使用干净的数据，则我们需要多少干净的样本？最后，我们应该只使用干净的样本作为基础，还是有更好的利用方式？</sample>
    <sample id="920">我们在工作中解决了这些问题，我们的发现如下。</sample>
    <sample id="921">首先，我们发现，有趣的是，最近的WSL方法确实需要干净的验证样本才能正常工作。</sample>
    <sample id="922">否则，数据可能会出现性能下降，如图所示。如果没有干净的验证样本，则训练模型无法超越原始的弱标签进行泛化。</sample>
    <sample id="923">这意味着培训是无用的。</sample>
    <sample id="924">这表明WSL方法实际上需要清晰标记的数据才能正常工作，并且获得干净验证样本的注释成本不应被忽视。</sample>
    <sample id="925">我们的第二个发现是，增加干净验证样本的数量将有助于WSL方法实现更好的性能，如左图所示。</sample>
    <sample id="926">通常，我们只需要每节课20个样本即可获得高分。</sample>
    <sample id="927">但这不是故事的结尾，因为如果我们无论如何决定访问干净的样本，那么直接在它们上进行训练甚至可以取得更好的性能。</sample>
    <sample id="928">右侧的图显示了直接应用于干净数据的微调方法与仅使用干净数据进行验证的WSL方法之间的性能差异。</sample>
    <sample id="929">正如您所看到的，如果每类有10个样本，那么直接微调开始超越WSL方法。</sample>
    <sample id="930">最后，通过允许在干净的验证样本上继续调整，可以轻松实现以前WSL方法中声称的性能改进。</sample>
    <sample id="931">如图所示，瓦林纳模型（称为FTW）最初在性能上低于更复杂的WSL方法，如余弦。</sample>
    <sample id="932">然而，如果我们允许在干净的样本上进行微调，则FTW的表现与其他方法一样好。</sample>
    <sample id="933">因此，在实践中，没有理由选择更复杂的WSL方法，这些方法需要更多的计算时间和磁盘空间。</sample>
    <sample id="934">简而言之，我们展示了最近的WSL方法需要干净的手动注释样本才能正常工作。它们的性能提升和实用性被严重高估了。</sample>
    <sample id="935">我们对未来工作的具体建议如下。</sample>
    <sample id="936">首先，报告模型选择标准。例如，报告是否使用干净的验证样本进行模型选择。</sample>
    <sample id="937">第二，WSL方法应与FSL基线进行比较，因为它们都在创建示例上工作。第三，连续微调是一个简单而强大的基线，应该在WSL的未来工作中予以考虑。</sample>
    <sample id="938">最后，我们开源了我们的代码。您可以在本幻灯片上的二维码中找到它。请随意查看。谢谢，并祝会议愉快。</sample>
    <sample id="939">对话系统的常用评估方法是使用人类评估，例如请人类评委选择哪两个对话更好，或者根据李克特量表对对话进行评分。</sample>
    <sample id="940">The paper has five authors.</sample>
    <sample id="941">需要 Servin 是法官和 Kea 是面包师的背景知识。</sample>
    <sample id="942">Yes, the code is publicly available on GitHub.</sample>
    <sample id="943">No, the NLPositionality's annotation is not evenly distributed across different demographic characteristics.</sample>
    <sample id="944">在可接受的域中，通过添加噪声来扰乱句子。</sample>
    <sample id="945">Dimension assessment means evaluating multiple aspects of chat quality to understand the strengths and weaknesses of a model in more detail.</sample>
    <sample id="946">The University of Science and Technology of China.</sample>
    <sample id="947">提示形式在零和一-shot提示中至关重要。</sample>
    <sample id="978">The author evaluated several conversational AI models.</sample>
    <sample id="979">这篇论文有三位作者。</sample>
    <sample id="980">The abstract goal can be inherited by different real-life specific goals with multifaceted constraints.</sample>
    <sample id="981">这篇论文有两位作者。</sample>
    <sample id="982">演讲者的名字是瓦苏达。</sample>
    <sample id="983">The author of this paper is associated with the University of Warsaw.</sample>
    <sample id="1021">PaLM最常见的错误是遗漏错误。</sample>
    <sample id="1022">你好，我是詹姆斯·芬奇。我是莎拉·芬奇。今天我们将向你介绍ABC-Eval，一种新的评估对话式人工智能的方法。</sample>
    <sample id="1023">这项工作由埃默里大学的埃默里NLP实验室完成，该实验室由埃默里大学的Jino Choi教授领导，并与亚马逊Alexa AI合作。</sample>
    <sample id="1024">假设你刚刚开发了一个对话模型，你想看看它与当前最先进的水平相比如何。</sample>
    <sample id="1025">常见的做法是使用人类评估，例如要求人类裁判员选择哪一种对话更好，或者根据李克特量表对对话进行评分。</sample>
    <sample id="1026">这些方法在提供整体对话质量的全面评估方面效果很好，但对话质量有很多方面。因此，你可能想要评估聊天质量的多个维度，以更精细地了解模型的优势和劣势。</sample>
    <sample id="1027">一种方法是简单地要求人类裁判使用现有的比较或等级评分方法来评估对话质量的几个维度，例如模型响应的相关性。</sample>
    <sample id="1028">然而，我们相信有一种更精确、更可靠的维度对话评估策略。</sample>
    <sample id="1029">我们的方法试图通过明确标注每个模型响应是否表达某些行为来减少人类评估的主观性，例如用无关信息进行回应或自相矛盾。</sample>
    <sample id="1030">我们称这种方法为聊天行为注释，或简称ABCEval。我们开发了这种方法来全面覆盖最近文献中建议可能影响聊天质量的聊天模型行为。</sample>
    <sample id="1031">ABC-EVAL能够测量聊天模型犯各种主题错误的速率。</sample>
    <sample id="1032">例如，ABC eval测量聊天模型忽略其伙伴或说无关内容的次数。</sample>
    <sample id="1033">当模型与自身或其伙伴相矛盾、产生错误事实、违反常识知识，以及在成功或失败时缺乏同理心时。</sample>
    <sample id="1034">为了确定哪种评估方法最有效，我们选择了四个最先进的聊天模型，并使用ABC eval在每个模型上进行了100次人机对话的评估。</sample>
    <sample id="1035">为了进行比较，我们还使用了三种现有的方法来评估这些对话：在回合级别上的李克特评分、在对话级别上的李克特评分以及对话级别上的两两比较。</sample>
    <sample id="1036">对于现有的每种方法，我们收集了对对话中最常测量的八个方面进行评估，因为这是评估聊天模型沿多个维度的标准做法。</sample>
    <sample id="1037">通过分析这些评估结果，我们发现ABC行为标签总体上比现有方法收集的标签更可靠，这可以通过在100个双标对话中进行注释员之间的协议来衡量。</sample>
    <sample id="1038">此外，ABC-EVAL标签比现有方法产生的指标更具有整体对话质量的预测性，如简单的线性回归分析所示。</sample>
    <sample id="1039">例如，你可以看到测量自我和伴侣矛盾的术语比例解释了对话质量的5%和10%，而平均等级一致性分数只能解释4%或更少。</sample>
    <sample id="1040">最后，我们使用逐步线性回归来检查每个评估指标是否捕获了聊天质量的一个独特方面。</sample>
    <sample id="1041">你可以看到所有ABC-EVAL指标的组合解释了超过25%的对话质量，而且当你一次移除一个指标时，大多数都会导致失去相当数量的质量信息。</sample>
    <sample id="1042">另一方面，所有等级的 Likert 指标组合解释了质量的更少，并且这些指标中很少有包含独特的信息。</sample>
    <sample id="1043">这些可靠的、信息丰富的、独特的ABCEval指标使我们能够以比以前的方法更高的分辨率评估对话式人工智能。</sample>
    <sample id="1044">您可以在我们的实验结果中看到，仍然存在一些挑战，并且已经精确量化了。例如，我们测试的机器人在大约20%的回答中违反了常识。</sample>
    <sample id="1045">他们大约有15%的回答是无关的，而且他们大约有10%的时间会自相矛盾或与他们的伴侣产生矛盾。</sample>
    <sample id="1046">随着该领域的快速发展，自我们进行评估以来，许多这些错误率可能会在新模型发布时有所下降。然而，这正是追求可靠和精确的评估指标来比较模型的原因。</sample>
    <sample id="1047">我们希望ABC-Eval能被该领域的其他人利用，作为向这个方向迈出有意义的一步，并期待看到在接下来的几个月和几年中对话式AI的发展。谢谢观看。</sample>
    <sample id="1048">这篇论文的作者所属机构是埃默里大学。</sample>
    <sample id="1049">本文中，CFT代表Cleanly Annotated Textual Samples。</sample>
    <sample id="1050">这篇论文有6位作者。</sample>
    <sample id="1051">你好，我的名字是Kai-Yen，我将介绍我们的作品《翻译需要上下文吗？数据驱动的多语言探索》。这项工作是在与Patrick Fernandes、Amy Liu、Andre F.D. Martins和Graham Neubig的合作下完成的。</sample>
    <sample id="1052">所以很多翻译都取决于上下文。例如，我们如何将“mole”翻译成这个句子？</sample>
    <sample id="1053">如果上一句话是“事情可能会变得危险，如果部长们发现了”，那么“more”指的是间谍。但是，如果上一句话是“这会有什么严重的事情吗，医生？”，那么“more”指的是胎记。</sample>
    <sample id="1054">因此，根据上下文，单词的含义会改变，因此它的翻译也会改变。</sample>
    <sample id="1055">然而，评估模型如何处理这种翻译情况是很困难的。首先是因为只有很小一部分翻译依赖于上下文，这使得语料库级别的指标如BLEU无法捕捉这些翻译。</sample>
    <sample id="1056">有些人建议对上下文相关的翻译进行目标评估，但这些资源只支持有限类型的上下文相关翻译和有限的语言集，因为它们通常依赖于领域知识和人类编纂。</sample>
    <sample id="1057">在本文中，我们试图回答这两个问题：首先，什么时候需要上下文翻译？其次，模型如何处理这些情况？</sample>
    <sample id="1058">为了回答第一个问题，我们首先测量了单词在翻译过程中依赖上下文的程度。</sample>
    <sample id="1059">在之前的论文中，我们介绍了CXM为机器翻译模型提供上下文使用的一种度量。这是通过测量上下文C在给定源X的情况下对目标Y提供了多少信息来完成的。</sample>
    <sample id="1060">你可以将CXML视为向模型提供上下文所获得的信息。</sample>
    <sample id="1061">在本文中，我们扩展了上下文MI到点对点上下文MI，可以测量句子级别或单词级别的上下文使用。我们可以将具有高p上下文MI的单词视为需要上下文进行翻译的单词。</sample>
    <sample id="1062">现在我们分析具有高p-SMI的单词，以查找这些单词之间的模式。</sample>
    <sample id="1063">然后我们在TED演讲的转录本上进行分析，这些转录本已经被翻译成14种不同的语言。</sample>
    <sample id="1064">我们从三个不同的层次进行分析。首先，我们查看具有高平均PCMI的词性标记部分。</sample>
    <sample id="1065">这使我们能够找到，例如，阿拉伯语中的二元代词，其相对较高的p6mi。这是因为英语没有二元代词，所以需要上下文来确定在翻译成阿拉伯语时一个代词是否是二元的。</sample>
    <sample id="1066">同样，我们发现某些语言在选择适当的动词形式时也需要上下文。然后我们看词汇项，它在所有不同出现中都有高PSMI平均值。</sample>
    <sample id="1067">这有助于识别需要上下文才能在文档中使用相同翻译的案例，例如中文中的专有名词。</sample>
    <sample id="1068">同样，我们发现上下文支持以正确的正式性进行翻译。</sample>
    <sample id="1069">最后，我们查看具有高p-SXML的不同个体标记。这使我们能够识别无法仅通过单词本身捕获的现象，但这些现象在句子结构中表达，例如省略号的解决。</sample>
    <sample id="1070">因此，现在我们使用分析结果来设计文档级别的翻译基准。</sample>
    <sample id="1071">对于我们识别的五个话语现象中的每一个，我们创建了标记器来自动识别属于该现象的单词，并将我们的标记器称为多语言话语意识或MUDA标记器。</sample>
    <sample id="1072">然后，我们还可以注意到，不同的语言有不同的比例。</sample>
    <sample id="1073">然后，我们使用Muda标签器，在我们要用于评估的平行语料库上应用标签，并在Muda标签器识别的上下文依赖示例上应用我们选择的翻译指标。</sample>
    <sample id="1074">最后，我们使用基准以及其它指标来评估不同模型在文档级别机器翻译上的表现。</sample>
    <sample id="1075">首先，当我们使用语料库级别的指标时，对于蓝色，我们发现无偏模型表现最好。</sample>
    <sample id="1076">但是，如果我们使用逗号，则上下文感知模型表现最佳，并且如果我们使用单词F度量，则具有和没有上下文的模型具有可比性能。</sample>
    <sample id="1077">这再次表明，如果我们仅使用语料库级别的指标，就很难确定最佳的文档级别翻译系统。</sample>
    <sample id="1078">现在我们使用 MovieLens 评估模型，并发现上下文摘要模型在某些语境现象（如正式性和词汇连贯性）上比不使用上下文的模型更准确。</sample>
    <sample id="1079">但是这些模型在其他现象，如省略号、代词和动词形式上没有使用上下文的情况下，并没有比这些情况好多少。这表明我们需要在文档级翻译中看到更多的进步。</sample>
    <sample id="1080">我们还比较了不同的商业系统，我们的基准显示DeepL通常比Google Translate更准确地进行文档翻译。</sample>
    <sample id="1081">简而言之，我们在14种语言对之间进行数据驱动分析，以确定哪些翻译需要上下文。</sample>
    <sample id="1082">然后，我们利用这些发现来建立一个文档级别的机器翻译基准，这可以帮助我们确定哪些话语现象模型能够很好地处理，以及哪些翻译系统擅长文档级别翻译。</sample>
    <sample id="1083">非常感谢您的关注，期待在多伦多与您见面！</sample>
    <sample id="1084">演讲者的名字是Yuxin Zhang。</sample>
    <sample id="1121">该方法没有名称。</sample>
    <sample id="1122">作者描述“显性词汇”方法是识别区分标记组和未标记组的单词。</sample>
    <sample id="1123">The author of this paper is affiliated with the University of Washington.</sample>
    <sample id="1124">The Prag approach is a symmetric approach to coordinate structures.</sample>
    <sample id="1125">演讲者的名字是James Finch。</sample>
    <sample id="1126">这篇论文有四位作者。</sample>
    <sample id="1127">The minimal pair paradigm evaluates language models on top of acceptability judgments, which can include grammaticality, syntax, or acceptability in terms of stereotypes.</sample>
    <sample id="1161">WSL approaches require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked.</sample>
    <sample id="1162">The model was evaluated on 11 biomedical and clinical downstream tasks.</sample>
    <sample id="1226">CamemBERT was trained on the 4GB subset of NACLs.</sample>
    <sample id="1227">演讲者的名字是Szymon Skurkowski。</sample>
    <sample id="1228">作者发现，随着时间间隔的增大，性能下降。这证实了他们的假设：时间漂移是性能下降的主要原因。</sample>
    <sample id="1269">为了将输出序列中的词元排列成正确的顺序，需要使用另一个模型来预测一个排列。</sample>
    <sample id="1270">作者建议模型所有者应提高偏见缓解方法的透明度，因为例如这些积极的刻板印象，我们不知道是因为存在某种过度的价值对齐，还是其他反刻板印象的方法导致了这些有害模式。</sample>
    <sample id="1271">The minimal pair paradigm is a method used to evaluate language models by showing an acceptable sentence or grammatical sentence, followed by an unacceptable sentence or ungrammatical sentence. The hope is that the model will assign more probability to the acceptable sentence.</sample>
    <sample id="1272">作者使用了以下评估指标：1. F1 score 2. Accuracy 3. Macro F1 score</sample>
    <sample id="1273">The inter-annotator agreement was used to measure the consistency between annotators.</sample>
    <sample id="1274">Vikipedia</sample>
    <sample id="1275">The authors of this paper are affiliated with the University of Bremen.</sample>
    <sample id="1276">MultiInstruct是一个新的基准，它包括了多模态的指令调优数据集。</sample>
    <sample id="1277">The paper was written by Professor Gino Choi and Amazon Alexa AI.</sample>
    <sample id="1278">二进制协调是指将文本分为两个部分，每个部分包含相同数量的字符。</sample>
    <sample id="1279">提示语的平均长度为10.6个字符。</sample>
    <sample id="1280">较小的T5模型可以生成比大多数大型语言模型更好的脚本。</sample>
    <sample id="1281">你好，我是Yanis Levarac，我将向您介绍我们的作品“Doctor BERT”，这是一个在法语中针对生物医学和临床领域的稳健预训练模型。</sample>
    <sample id="1282">在这次演讲中，我们首先谈谈语言建模在医疗保健中的应用。然后我们将介绍我们文章的主要贡献。</sample>
    <sample id="1283">我们介绍了第一个生物医学模型，用法语命名的Dr. BERT，基于Roberta，并在NATOS上进行训练，这是一个来自网络的医疗爬虫数据集。</sample>
    <sample id="1284">我们还介绍了模型与多点预训练设置和数据源的比较。然后，我们将结果呈现为11个生物医学和临床下游任务的法语。</sample>
    <sample id="1285">最后，我们总结了实验，并提供了如何访问这些模型的更多详细信息。</sample>
    <sample id="1286">自2018年发布以来，BERT已成为解决自然语言处理任务的最有效方法之一，并与Word2vec、FastText和Enrich等历史静态和上下文化方法相比，具有巨大的性能提升。</sample>
    <sample id="1287">从那时起，这个模型已经被改编成其他语言，比如法语中的卡芒贝尔奶酪，还有其他领域，比如生物医学领域的PubMed BERT和BioBERT，以及临床领域的Clinical BERT，但主要是英语。</sample>
    <sample id="1288">专门用于其他语言的模型很少，通常是基于持续预训练的，因为缺乏领域数据。</sample>
    <sample id="1289">然而，法国没有开源模式来支持生物医学和临床。</sample>
    <sample id="1290">我们应该问自己关于各种用途最合适的数据来源是什么，而这些 crowdsourced 数据是临床数据的良好替代品。</sample>
    <sample id="1291">为了回答这个问题，我们将伯特博士与基于匿名数据的舒伯特模型进行比较，该数据来自日内瓦大学医院的数据仓库。</sample>
    <sample id="1292">最后，我们问自己，我们需要多少数据来训练一个专门针对法国数据的模型？是4GB、8GB还是更多？</sample>
    <sample id="1293">为了回答这个问题，我们首先训练并比较了四个从头开始的模型：一个7GB的NATOS的第一个版本、一个4GB的NATOS的第二个版本、</sample>
    <sample id="1294">首先，我们有一个临床模型的Schubert版本，其中包含4GB的临床笔记。最后，我们有一个混合了自然和临床笔记的Schubert版本。</sample>
    <sample id="1295">除了这个比较，我们还介绍了三种在持续预训练上训练的模型，以分析预训练策略的影响。</sample>
    <sample id="1296">一个基于Camembert，训练4GB的NACHOS，另一个也基于Camembert，但这次训练了4GB的clean notes。</sample>
    <sample id="1297">最后，基于英语生物医学模型，我们训练了4GB的Snatchos数据集。总共有7个模型。</sample>
    <sample id="1298">为了评估这7个模型，我们收集了公共和私人数据集，例如命名实体识别、分类、词性标注和问答。</sample>
    <sample id="1299">这些模型与六个基准模型进行比较，包括Camembert Oscar 138GB、Camembert Oscar 4GB、Camembert CineNet 4GB、PubMed BERT、BioBERT和Clinical BERT。</sample>
    <sample id="1300">因此，模型在训练数据具有相同性质的任务上表现最佳。</sample>
    <sample id="1301">然而，我们可以从异构数据源中获得数据。我们还观察到使用更多的数据可以转化为更好的性能。</sample>
    <sample id="1302">总体而言，从头开始编写似乎在大多数任务中具有更高的性能。</sample>
    <sample id="1303">然而，使用PubMedBERT的white和分词器在Nature的4GB子集上进行的连续解释实验显示了与从头开始使用4GB的DoctorBERT获得的结果相当的结果。</sample>
    <sample id="1304">但是基于Kamionkowski权重和分词器的模型却存在稳定性问题。</sample>
    <sample id="1305">最后，作为结论，我们的提议系统在11个目标任务中的9个上提供了更好的性能，并且在全球超过了这里Camembert的通用模型的结果。</sample>
    <sample id="1306">我们还观察到，稀疏的数据更好，更多的稀疏数据更好，但它并不容易扩展。</sample>
    <sample id="1307">预训练模型可以从NATOS免费获得，并在Hugging Face上，所有的训练脚本都在我们的GitHub存储库中。</sample>
    <sample id="1308">谢谢您的演讲，我们期待在会议后进行交流。</sample>
    <sample id="1309">论文研究了四种学习策略。</sample>
    <sample id="1310">The red best-fit line has a gradient that is greater than 1, indicating no diminishing returns and thus not observing adaptive overfitting.</sample>
    <sample id="1311">通过评估简化文本与原始复杂文本之间的相似度，可以评估简化质量。</sample>
    <sample id="1312">Yes, language models have varying political leanings.</sample>
    <sample id="1313">嗨，我的名字是Matthias Lindemann，今天我将简要介绍我们关于使用多集标记和潜在置换的无树组成性泛化论文。</sample>
    <sample id="1314">这是我和我的顾问Alexander Colla和Ivan Tito的合作。</sample>
    <sample id="1315">组合泛化可以理解为学习者处理更深层次的递归和在训练期间单独看到的短语的未见组合的能力。</sample>
    <sample id="1316">在语义解析的背景下，测试组合概括可能看起来像这样。像往常一样，我们有一个训练集的陈述，在这种情况下是“女孩睡了”和“玛丽知道女孩睡了”。</sample>
    <sample id="1317">这些陈述与代表其含义核心方面的逻辑形式配对。</sample>
    <sample id="1318">与标准机器学习评估不同，测试集不是来自相同的分布，而是包含结构上未见过的逻辑形式。</sample>
    <sample id="1319">在这个例子中，模型在训练过程中看到了浅层递归，并在具有深层递归的示例上进行了测试。</sample>
    <sample id="1320">天真地将序列转换为序列的模型在处理这种分布外泛化时遇到了困难，并且经常产生与输入脱节的输出。</sample>
    <sample id="1321">特别是，它们经常无法重现输入和输出之间的系统对应关系，例如在示例中着色的那些。</sample>
    <sample id="1322">解决这个问题的流行方法是将树集成到模型中。</sample>
    <sample id="1323">这些树旨在捕捉与逻辑形式相关的表达过程。</sample>
    <sample id="1324">这很好，但树通常不会给定，需要以某种方式获得。</sample>
    <sample id="1325">这可能是一个复杂的过程，有时甚至是一个计算上昂贵的过程。通常，这涉及到大量的形式特定的逻辑形式预处理，例如处理变量符号。</sample>
    <sample id="1326">获得树形结构可能还需要专门的语法归纳程序。</sample>
    <sample id="1327">在这篇论文中，我们不使用树形结构，并引入了一种直接建模输入片段和输出片段之间对应关系的神经序列到序列模型。</sample>
    <sample id="1328">我们首次展示了对深层递归的强泛化，而无需依赖于树。</sample>
    <sample id="1329">我们的方法分两步预测输入的输出。</sample>
    <sample id="1330">首先，我们为每个输入标记添加一个将出现在输出中的标记的无序多集。</sample>
    <sample id="1331">在第一步之后，我们拥有所有正确的标记，但它们没有被审计。</sample>
    <sample id="1332">这就是为什么在第二步中，我们使用另一个模型来预测排列，将它们放入正确的顺序。</sample>
    <sample id="1333">我们提出了一种新的方法来预测一个排列，它不会对可能的排列施加任何硬约束。这使得我们的方法非常灵活和富有表现力。</sample>
    <sample id="1334">从概念上讲，我们的置换模型大致工作如下。</sample>
    <sample id="1335">我们从左到右遍历输出，并确定每个位置应放置哪个多重集标记。对于第一个输出位置，我们只需选择一个，如红色高亮显示的那样。</sample>
    <sample id="1336">然后我们跳到下一个多重集标记，以确定输出中的第二个标记。</sample>
    <sample id="1337">我们以类似的方式通过跳转到另一个多重集令牌来确定输出中的第三个令牌。我们继续这个过程。</sample>
    <sample id="1338">直到第一阶段的每个标记都被访问过一次。</sample>
    <sample id="1339">为了给您一个实验结果的预告，这里我们比较了我们的方法和其他Treeless模型在Cogs基准上的表现。我们的模型在对更深层次的递归进行泛化时，比其他模型表现出更大的优势。</sample>
    <sample id="1340">然而，其他一些结构化组织仍然非常具有挑战性。</sample>
    <sample id="1341">在我们的论文中，我们解决了一些有趣的技术挑战。</sample>
    <sample id="1342">首先，在训练数据中没有给出输入和输出之间的对齐，因此对于给定的标记，我们不知道它来自哪个子集，这给训练带来了挑战。</sample>
    <sample id="1343">此外，有时存在多个与数据一致的排列方式，但正确的语言排列是潜在的。我们通过将对齐作为训练的一部分来解决这个问题。</sample>
    <sample id="1344">我们的置换方法非常灵活，但它带来的挑战是找到最高分的置换是NP困难的。这是因为这与旅行商问题有关。</sample>
    <sample id="1345">我们用一种对GPU友好的连续松弛来近似，这还允许我们通过解决方案进行反向传播，并学习更具有语言学可信度的置换。</sample>
    <sample id="1346">如果您想了解我们的实验以及我们如何解决这些挑战，请查看我们的论文或海报。</sample>
    <sample id="1347">Cognitive dissonance is two beliefs or actions that are inconsistent.</sample>
    <sample id="1348">GPT-4是最自由的语言模型。</sample>
    <sample id="1349">Yes, cumulative training performed equal or better than iterative across the board.</sample>
    <sample id="1350">The speaker's name is Sara Papi.</sample>
    <sample id="1351">MuDa基准中的数据是从TED演讲中获得的。</sample>
    <sample id="1385">演讲者的名字是Matthias Lendemann。</sample>
    <sample id="1386">Cross-lingual zero-shot transfer refers to training a model on one source language and then transferring it to another language without any additional data or fine-tuning. This approach leverages the shared knowledge between languages, allowing the model to generalize well across different linguistic contexts.</sample>
    <sample id="1387">Silent University in Germany</sample>
    <sample id="1388">作者使用了平均延迟和计算感知平均延迟作为延迟测量方法。</sample>
    <sample id="1389">大家好，我是Makshita。今天，我和同事Martin将介绍我们的工作《The Kit Must Have：从多个来源评估知识集成》。这项工作是麦吉尔大学、Mila和微软研究合作的成果。</sample>
    <sample id="1390">自然语言理解模型利用多种知识来源，例如参数中包含的知识，通常通过预训练获得，以及在推理阶段提供的输入知识。</sample>
    <sample id="1391">最近在问答等任务中的工作表明，模型可以使用预训练时间知识来解决任务。</sample>
    <sample id="1392">但是自然语言理解通常需要在推理时提供的知识。</sample>
    <sample id="1393">例如，在句子中，约翰在电视上看到了新当选的总统。</sample>
    <sample id="1394">预训练参数可以包含有关总统做什么和电视是什么的信息，但它们无法可靠地知道这个实例特定实体约翰是谁或新总统是谁，因为总统可能已经自预训练以来发生了变化。</sample>
    <sample id="1395">因此，知识密集型的自然语言处理任务的成功模型需要能够整合和利用预训练时间和推理时间的知识。</sample>
    <sample id="1396">在本文中，我们提出了一套用于知识集成的诊断测试集。</sample>
    <sample id="1397">我们介绍了一个旨在探测利用不同来源可用知识能力的指代消解任务。我们使用人类研究参与者评估数据集，并建立指代消解模型。</sample>
    <sample id="1398">这是一个数据集中的示例。Serving is a judge. Kya is a baker. Serving and Kya met at a park after a long day at work deciding cases in a law court. He was happy to relax.</sample>
    <sample id="1399">这里的任务是识别代词“he”所指的正确实体，即本例中的“Sam”。</sample>
    <sample id="1400">解决特定代词的问题需要两种类型的信息：首先，实体特定的知识，例如“塞尔维是法官”，其次，一般知识，例如“法官在法庭上审理案件”。</sample>
    <sample id="1401">通常，背景知识是在大型语言模型的预训练期间学习的，而实体特定的知识通常是在推理阶段观察到的。</sample>
    <sample id="1402">我们调整了这两条信息的可用性，使其可能出现在单一来源或多个来源中。</sample>
    <sample id="1403">我们定义了三种KEMOS设置。首先，我们有topica设置，即预训练，其中假设在预训练时可以获取背景知识。</sample>
    <sample id="1404">第二，有背景知识设置。背景知识在预训练时间和推理时间都可用。最后，有背景和推理设置。只有在推理时间才可用这两种知识类型。</sample>
    <sample id="1405">最后一个设置尤其有趣，因为它模拟了背景噪音对于解决任务来说是不必要的，也不是预训练数据模型的一部分。例如，因为自预训练以来新职业已经发展起来。</sample>
    <sample id="1406">这里有一个控制可用性如何影响事实源的示例。</sample>
    <sample id="1407">在背景预训练设置中，我们假设背景知识“政治家在政府中赢得席位”包含在预训练参数中。在上下文中，我们提供特定实体的知识“G. Chester是政治家”。</sample>
    <sample id="1408">在背景设置中，我们不仅提供特定的背景信息，还提供了关于政治家和影响者背景的知识。</sample>
    <sample id="1409">在背景和环境设置中，我们提供了一个虚构的职业“Meritour”而不是政治家，因为Meritour不太可能包含在预训练的参数中。</sample>
    <sample id="1410">我们使用人类研究参与者和已建立的参考分辨率模型对数据集进行了评估。在这张图中，我们展示了在背景预训练设置中最困难的变体上表现最佳的模型的结果。</sample>
    <sample id="1411">没有在KittMOS上进行任务特定的训练，这两个模型都没有表现得很好。然而，当在KittMOS上进行训练时，C2F和BertForC2F的表现明显优于随机选择。</sample>
    <sample id="1412">这表明，当在通用参考解决数据集上进行训练时，模型会学习利用表面提示，这些提示在使用Kitties测试时是没有用的。</sample>
    <sample id="1413">额外的实验表明，即使是最有效的模型也不能可靠地整合背景知识，只能在推理时提供。</sample>
    <sample id="1414">总结我们论文的主要收获。许多共指消歧模型似乎无法在没有任务特定训练的情况下从不同来源推理知识。然而，通过任务特定的训练，一些模型成功地将来自多个来源的知识整合在一起。</sample>
    <sample id="1415">然而，即使是最优秀的模型似乎也难以可靠地整合仅在推理时间呈现的背景知识。如果您对更多细节感兴趣，请参阅我们的论文，并在GitHub上查看数据集和代码。谢谢聆听！</sample>
    <sample id="1416">基于树的方法的缺点是，树通常不是给定的，需要通过某种方式获得。这个过程有时会很复杂，并且在计算上可能很昂贵。通常，这涉及到对逻辑形式进行大量的特定于形式主义的预处理，例如处理变量符号。获得树也可能涉及专门的语法归纳程序。</sample>
    <sample id="1417">The authors of this paper are affiliated with the University of Hong Kong.</sample>
    <sample id="1418">你好，我是Myra，今天我将谈论我们的论文《标记的人格》：使用自然语言提示来衡量语言模型中的刻板印象。这项工作是与Essen Darmush和Dan Jurafsky合作完成的。</sample>
    <sample id="1419">近年来，许多人已经记录了大型语言模型（LLMs）中社会偏见和刻板印象的普遍性。</sample>
    <sample id="1420">然而，这些措施存在各种限制。它们通常依赖于手工构建的数据集，这些数据集的收集非常耗时。</sample>
    <sample id="1421">他们通常只测量非常具体的刻板印象，这意味着它们不容易推广到其他人口统计或背景中，或者它们只是捕捉非常一般、广泛的关联，比如与特定群体的负面关联。</sample>
    <sample id="1422">此外，这个领域的大部分工作都没有考虑到交叉性，即多面的社会身份可以加剧偏见，并成为独特伤害发生地。</sample>
    <sample id="1423">为了克服这些限制，我们依赖于这些较新的指令调整后的LLMs非常擅长响应指令和提示的特性。</sample>
    <sample id="1424">因此，我们可以要求模型生成一个“角色”，即使用提示来描述一个想象中的个体，例如“想象你是一个亚洲女性，描述你自己”。</sample>
    <sample id="1425">我们可以立即看到，这可以适用于任何人口统计学，因为我们可以在提示中指定我们想要的任何身份标记。</sample>
    <sample id="1426">所以这里有一些GPT-4的示例生成。</sample>
    <sample id="1427">我们立即看到，虽然输出在传统意义上并不明显是负面或有毒的，</sample>
    <sample id="1428">有一些有趣的模式。</sample>
    <sample id="1429">亚洲女性被描绘成谦逊的，中东女性被用异国情调和迷人的地区来指代。</sample>
    <sample id="1430">而这两个有色人种的女性角色都提到了祖先，而白人男性角色则没有。</sample>
    <sample id="1431">为了捕捉这些模式，我们的方法有两个部分。第一个是生成这些角色。</sample>
    <sample id="1432">我们的提示是受一项研究的启发，该研究将这些提示提供给人类受试者，发现通过提供给人类受试者，他们也能够揭示种族刻板印象。</sample>
    <sample id="1433">这还使我们生成的角色和人类撰写的回复之间可以进行直接比较。</sample>
    <sample id="1434">第二部分是标记单词，这是一种识别区分标记组和未标记组的单词的方法，我稍后会详细说明。</sample>
    <sample id="1435">这种好处是，我们不需要依赖任何特定的词汇，就能得到非常具体的模式和原型。</sample>
    <sample id="1436">标记词法利用社会语言学中的标记概念，该概念指出存在一个未标记的默认值，任何与该默认值不同的群体在语言上都是标记的。</sample>
    <sample id="1437">例如，单词“战士”通常与男性有关。因此，当人们描述女性战士时，他们通常会具体说明“女性战士”，并用“女性”标记该术语。</sample>
    <sample id="1438">更广泛地说，社会上的主导群体在语言和社交上都是未标记的，而边缘群体通常是标记的。</sample>
    <sample id="1439">因此，在我们的方法中，我们首先指定未标记和标记组是什么。</sample>
    <sample id="1440">然后我们使用fighting words方法比较角色，该方法基本上是使用加权对数比来区分每个标记组的顶部单词。</sample>
    <sample id="1441">例如，对于黑人女性的角色，我们会使用“Fighting Words”，并比较log odds比率与白人角色和男性角色之间的比率，因为这两个是相应的未标记群体。</sample>
    <sample id="1442">现在是结果的时间。首先，我们使用了刻板印象词典，并发现生成的个性包含比人类编写的更多刻板印象。</sample>
    <sample id="1443">然而，当我们实际查看词汇表中的单词分布时，我们发现非常不同的事情。</sample>
    <sample id="1444">因此，虽然生成的角色有更高的Luxong单词率，但人类编写的单词分布更广，而生成角色中的刻板印象单词只是“高”和“运动员”。</sample>
    <sample id="1445">所以真的只有积极的，或者至少不是消极的。</sample>
    <sample id="1446">实际上，这个词汇表并没有很好地捕捉到我们在前面的幻灯片中看到的许多有害模式。因此，为了做到这一点，我们将转向我们标记单词方法的结果，以显示这些看似积极的单词如何促进刻板印象和本质化叙事。</sample>
    <sample id="1447">在我们的分析中，我们揭示了这些看似积极的描绘如何反映有害模式。</sample>
    <sample id="1448">首先，对于我们的群体，这些词包括文化、传统、骄傲和异国情调。这些词仅通过与他们的身份的关系来定义这些群体，并将它们与白人规范区分开来。</sample>
    <sample id="1449">这为这些群体的长期歧视和排斥做出了贡献。</sample>
    <sample id="1450">此外，这些词语中反映了很多常见的套路，尤其是对于有色人种女性而言。例如，描述拉丁裔女性的词语包括“充满活力”和“曲线美”。</sample>
    <sample id="1451">亚洲女性的词汇包括“娇小”、“精致”和“丝滑”。</sample>
    <sample id="1452">这与亚洲女性被过度性化、被视为非常温顺和服从的历史有关。</sample>
    <sample id="1453">最后，对于黑人女性，我们看到一些顶级词汇是“坚强”和“有韧性”。</sample>
    <sample id="1454">这与人们称之为“坚强黑人女性原型”的原型有关，虽然乍一看听起来很积极，</sample>
    <sample id="1455">有研究表明，这种原型实际上是非常有害的，因为它给这些人口统计学带来了很大的压力，要它们在面对社会障碍时具有韧性和力量。</sample>
    <sample id="1456">因此，与其真正努力改变这些障碍，它会给那些人带来压力，让他们克服这些障碍，这会导致这些人以及其他伤害的非常负面的健康结果。</sample>
    <sample id="1457">更广泛地说，我们发现每个标记组的单词基本上反映了非常本质化的叙述。</sample>
    <sample id="1458">因此，基于这些模式，我们为模型所有者提出了三个建议。</sample>
    <sample id="1459">首先，作为研究人员，我们应该解决和消除刻板印象和本质化叙事。我们也应该使用交叉性视角来研究偏见和伤害，因为如果不这样做，可能会忽略很多东西。</sample>
    <sample id="1460">最后，应该增加关于偏见缓解方法的透明度。</sample>
    <sample id="1461">例如，这些积极的刻板印象，我们不知道是否是因为某种奇怪的原因。</sample>
    <sample id="1462">过度的价值对齐或可能的其他反刻板印象方法导致了这些有害模式。</sample>
    <sample id="1463">我们真的不能做出任何假设，或者在没有更多透明度的情况下进一步研究。</sample>
    <sample id="1464">非常感谢大家的聆听，祝你在ACM上玩得开心！</sample>
    <sample id="1465">大家好，我叫金伟怡，来自中国科学技术大学。</sample>
    <sample id="1466">It's my pleasure to give a short advertisement video about paper. Are you copying my model? Protecting the copyright of large language models for embedding and services via backdoor watermark</sample>
    <sample id="1467">让我们首先介绍一下嵌入式服务的背景。</sample>
    <sample id="1468">目前，大型语言模型如GPT、Llama、Palm在自然语言理解和生成方面表现优异。</sample>
    <sample id="1469">嵌入式服务是基于大型语言模型构建的服务之一，用于协助各种自然语言处理任务。</sample>
    <sample id="1470">例如，OpenAI提供了基于GPT的嵌入API。</sample>
    <sample id="1471">然而，最近的研究表明，攻击者仍然可以通过学习嵌入并提供类似的服务来窃取模型。因此，有必要保护嵌入和作为服务的版权。</sample>
    <sample id="1472">为了保护嵌入式服务的版权，一种解决方案是在提供商的服务中嵌入水印，并检测另一个服务是否包含水印。</sample>
    <sample id="1473">水印方法需要满足以下属性：首先，该方法应适用于嵌入式服务。其次，水印不应降低提供的嵌入的实用性。</sample>
    <sample id="1474">第三，水印应该足够隐蔽，否则攻击者可以轻松地移除水印。</sample>
    <sample id="1475">最后，在模型提取过程中，水印需要转换到攻击者的服务中。</sample>
    <sample id="1476">现有的作品可以大致分为四类。</sample>
    <sample id="1477">然而，这种方法要么不适用于嵌入式服务，要么缺乏可转移性。</sample>
    <sample id="1478">因此，在本文中，我们提出了一种嵌入标记，这是一种基于后门的水印方法，适用于嵌入式服务。</sample>
    <sample id="1479">然后让我介绍嵌入标记的详细信息。嵌入标记包含两个主要步骤：水印注入和版权验证。</sample>
    <sample id="1480">在这些主要步骤之前，我们首先选择一个触发集。触发集是一组在中等频率间隔内的单词。</sample>
    <sample id="1481">我们假设提供者可以收集一个通用的文本语料库，并计算我们所提到的单词频率。</sample>
    <sample id="1482">在水印注入中，我们首先定义一个目标嵌入。当用户将句子发送到提供商的服务时，提供商会在句子中计算触发器编号。</sample>
    <sample id="1483">提供的嵌入是目标嵌入和原始嵌入的加权总和。</sample>
    <sample id="1484">目标嵌入的权重与句子中的触发器数量成正比。当句子中的触发器数量大于m时，提供的嵌入恰好等于目标嵌入。</sample>
    <sample id="1485">版权验证是检测另一个服务中的模型是否包含水印。</sample>
    <sample id="1486">我们首先构建一个后门和一个无害的数据集。后门数据集包含所有单词属于触发集的句子，而无害数据集中的所有单词都不属于触发集。</sample>
    <sample id="1487">然后，提供者从数据集请求嵌入式服务。</sample>
    <sample id="1488">请求的嵌入和目标嵌入之间的余弦和L2相似度被计算。我们计算了良性与后门数据集之间的相似度差异，定义为余弦Δ和L2Δ。</sample>
    <sample id="1489">同时，我们还应用了KS测试，并使用其p值作为第三个指标。</sample>
    <sample id="1490">我们对四个数据集进行实验：AG News、Mind、SSD2和Email Spam。我们假设提供者使用WikiText数据集来计算单词频率。</sample>
    <sample id="1491">四个数据集的结果表明，我们的嵌入标记可以具有出色的检测性能，同时保持下游任务的出色实用性。</sample>
    <sample id="1492">我们还通过在VOPCA上可视化句子的嵌入来验证提供的嵌入的有效性。图例表示每个句子中的触发器数量。</sample>
    <sample id="1493">如图所示，很难区分后门嵌入和正常嵌入。</sample>
    <sample id="1494">那就这样，谢谢。欢迎与我们讨论。</sample>
    <sample id="1495">ABC-Eval代表Annotating Behaviors in Chat。</sample>
    <sample id="1496">CoNLL-2003 和 CoNLL++ 之间的性能增量直到2019年才高于5个百分点。</sample>
    <sample id="1497">你好，我的名字是瓦苏达，我是石溪大学计算机科学博士研究生。我想介绍我们被2023年ACL大会接受的长篇论文《针对罕见类别的距离检测转移学习》。</sample>
    <sample id="1498">我们首先定义认知失调以及为什么它是语言学习中一个重要的问题。简单来说，认知失调是指信念或行动不一致。</sample>
    <sample id="1499">这种信念和行动是一致的，它们是不一致的。</sample>
    <sample id="1500">进一步提到“我认为没有他们我就不能保住工作”为第二个事件辩护，并且它们具有同位关系。</sample>
    <sample id="1501">虽然不和谐是日常决策中非常常见的现象，但在其他类型的语篇关系中很少以语言表达出来。</sample>
    <sample id="1502">所以为什么这很重要？了解认知距离可以帮助我们理解人们之间的分歧，追踪信念、价值观和态度在人群中的变化趋势。</sample>
    <sample id="1503">认知失调也与焦虑障碍有关，可以帮助更好地理解人们的心理健康。</sample>
    <sample id="1504">研究语言中表达的分歧也可以帮助理解极端主义和弱势群体的极化。</sample>
    <sample id="1505">最后，认知失调对于理解个人的认知风格以及帮助我们更好地理解决策过程非常重要。</sample>
    <sample id="1506">为了创建认知不和谐资源，我们进行了大规模的不和谐关系注释。我们使用了如图所示的不和谐第一种方法。</sample>
    <sample id="1507">推文使用PrettyB解析器进行解析，根据我们论文中描述的指南对话语单元进行了注释。</sample>
    <sample id="1508">正如这里所见，只有3.5%的注释对存在不和谐。</sample>
    <sample id="1509">收集了大约1000个话语单元对后，我们为一个仅基于43个DisNet示例训练的初始分类器进行了训练。毫不奇怪，该分类器的表现并不比机会好多少。</sample>
    <sample id="1510">鉴于这种不和谐现象的发生率较低，而且没有这样的数据集，我们面临着绝对罕见的问题。</sample>
    <sample id="1511">为了缓解这种情况，我们尝试了结合迁移学习和主动学习的实验，以便在较少的注释运行中收集更多的不和谐样本，从而降低总体注释成本，同时提高不和谐检测能力。</sample>
    <sample id="1512">由于初始模型完全无法捕捉不和谐类，因此我们通过从密切相关的任务中转移权重来启动主动学习过程。</sample>
    <sample id="1513">我们转移了两个不同的任务：主题无关的不和谐分类任务，确定两个不同人的辩论陈述是否一致或不一致，无论主题如何。</sample>
    <sample id="1514">我们在这里称之为辩论，并将二元分类为扩展和比较类，因为这两个与和谐与不和谐的概念密切相关，我们称它们为 CEE。</sample>
    <sample id="1515">我们发现，将零-shot性能转移到标注数据集上，已经比随机情况好得多，最佳的AUC为0.62。</sample>
    <sample id="1516">进一步在两个任务上进行迭代微调后，我们发现CE任务的微调加上进一步在辩论上的微调，可以得到更好的零样本性能。因此，这就是我们用来开始实际学习的模型。</sample>
    <sample id="1517">接下来，我们确定更新模型的最佳方法，使用每一轮主动学习和注释的新数据。累积器收集到目前为止从主动注释中收集的所有数据。迭代更新模型，通过在最新收集的数据集上进行训练来更新模型。</sample>
    <sample id="1518">在不同的策略中，我们发现累积性能与迭代性能相当或更好。</sample>
    <sample id="1519">接下来，为了增加不和谐示例的数量，我们使用了一个罕见类概率策略（PRC），以选择在任何一轮A中，当前模型最有可能被不和谐的示例。</sample>
    <sample id="1520">我们将此与其他社区中常用的最先进的AI策略进行比较。</sample>
    <sample id="1521">我们发现所提出的PRC策略比其他最先进的策略表现更好，尽管差异很小。请注意，随机的性能显著较低。</sample>
    <sample id="1522">在进一步的迭代中，我们通过最佳策略将距离分类AUC提高到0.75，这是迄今为止我们在任务上取得的最佳性能。</sample>
    <sample id="1523">我们还检查了每种策略的可行性，以评估注释质量和成本对标注者的适用性。我们发现PRC具有最高的不和谐度，并且在处理稀有类别时表现最佳。然而，标注者也发现示例难以理解。</sample>
    <sample id="1524">简而言之，我们发现PRC是一种简单的无监督策略，用于稀有类别获取和使用适当设计的迁移学习任务显著地帮助了冷启动无监督。</sample>
    <sample id="1525">我们还发现，迭代更新对不同领域中的迁移学习很有用，在域内主动注释可以从累积更新中受益。</sample>
    <sample id="1526">这些是我们的代码数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢。</sample>
    <sample id="1527">The authors of this paper are affiliated with the following institutions: Humboldt University of Berlin, Germany; and University of California San Diego, USA.</sample>
    <sample id="1528">演讲者的名字是Yuan。</sample>
    <sample id="1529">The paper has five authors.</sample>
    <sample id="1530">该方法与专为同时语音翻译定制的 simulST 架构进行了比较。</sample>
  </task>
</testset>