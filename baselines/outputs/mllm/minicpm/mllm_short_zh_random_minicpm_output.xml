<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">The main content of this text is 'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models'.</sample>
    <sample id="1">The authors belong to McGill University/Mila and Microsoft Research.</sample>
    <sample id="2">DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University DÃ¼sseldorf, Germany ACL 2023</sample>
    <sample id="3">å¹»ç¯ç‰‡ä¸Šæ˜¾ç¤ºäº†ä¸€ä¸ªæ ‡é¢˜ï¼Œå†…å®¹æ˜¯â€œ1. Text Simplificationâ€ï¼Œä¸‹é¢æœ‰ä¸‰ä¸ªé—®é¢˜ï¼šâ€œWhat, why and How?â€ã€‚</sample>
    <sample id="4">æ–‡æœ¬ç®€åŒ–æ˜¯å°†æ–‡æœ¬é€‚åº”ä¸ºæ”¹å–„ç‰¹å®šç›®æ ‡ç¾¤ä½“å¯¹æ–‡æœ¬çš„ç†è§£çš„è¿‡ç¨‹ã€‚</sample>
    <sample id="5">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="6">è¿™æ®µæ–‡å­—æè¿°äº†ä¸€ä¸ªæ–‡æœ¬ç®€åŒ–ç¤ºä¾‹ã€‚å®ƒå±•ç¤ºäº†å¦‚ä½•å°†ä¸€ä¸ªå¤æ‚çš„å¾·è¯­å¥å­ç®€åŒ–ä¸ºæ›´ç®€å•çš„è¯­è¨€ã€‚åŸå§‹å¥å­æ˜¯â€œDie Gewerkschaft setzt sich dafÃ¼r ein, dass zum Beispiel hÃ¶here LÃ¶hne gezahlt werden.â€ï¼Œè€Œç®€åŒ–åçš„å¥å­æ˜¯â€œDie Gewerkschaft setzt sich dafÃ¼r ein, zum Beispiel fÃ¼r hÃ¶here LÃ¶hne oder mehr Urlaub zu eintretenã€‚â€ã€‚å›¾ä¸­è¿˜å±•ç¤ºäº†é€šè¿‡æ›¿æ¢ã€çŸ­è¯­åˆ é™¤ã€é‡æ’åºå’Œå•è¯åˆ é™¤ç­‰æ–¹æ³•æ¥ç®€åŒ–å¥å­çš„è¿‡ç¨‹ã€‚</sample>
    <sample id="7">å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªè“è‰²çš„æ ‡é¢˜æ ï¼Œä¸Šé¢å†™ç€â€œText Simplification Exampleâ€ã€‚åœ¨æ ‡é¢˜æ ä¸‹é¢ï¼Œæœ‰ä¸€ä¸ªä¾‹å­ï¼Œæ˜¾ç¤ºäº†å¦‚ä½•ç®€åŒ–å¥å­ã€‚ä¾‹å­åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šåŸå§‹å¥å­å’Œç®€åŒ–åçš„å¥å­ã€‚åŸå§‹å¥å­æ˜¯å¾·è¯­ï¼Œè€Œç®€åŒ–åçš„å¥å­æ˜¯è‹±è¯­ã€‚åœ¨åŸå§‹å¥å­å’Œç®€åŒ–å¥å­ä¹‹é—´ï¼Œæœ‰å››ä¸ªæ ‡ç­¾ï¼Œåˆ†åˆ«è¡¨ç¤ºä¸åŒçš„ç®€åŒ–æŠ€æœ¯ï¼šæ›¿æ¢ã€çŸ­è¯­åˆ é™¤ã€é‡æ’åºå’Œå•è¯åˆ é™¤ã€‚è¿™äº›æ ‡ç­¾ç”¨ç®­å¤´æŒ‡å‘åŸå§‹å¥å­çš„ä¸åŒéƒ¨åˆ†ï¼Œè¡¨æ˜å“ªäº›éƒ¨åˆ†è¢«ç®€åŒ–æˆ–ä¿®æ”¹äº†ã€‚</sample>
    <sample id="8">2. DE-plain A New Corpus</sample>
    <sample id="9">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="10">German Text Simplification Corpora</sample>
    <sample id="11">å›¾ç‰‡æ˜¾ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œGerman Text Simplification Corporaâ€çš„æ¼”ç¤ºå¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯ç”¨ç²—ä½“è“è‰²å­—ä½“æ˜¾ç¤ºçš„ã€‚åœ¨æ ‡é¢˜ä¸‹æ–¹ï¼Œæœ‰ä¸€ä¸ªå›¾è¡¨ï¼Œæ ‡é¢˜ä¸ºâ€œSentence Levelâ€ã€‚å›¾è¡¨åŒ…å«ä¸€ä¸ªæŸ±çŠ¶å›¾ï¼Œæ˜¾ç¤ºäº†ä¸åŒå¹´ä»½çš„æ•°æ®ï¼Œå…·ä½“æ¥è¯´æ˜¯ä»2018å¹´åˆ°2023å¹´ã€‚æ¯ä¸ªæŸ±å­ä»£è¡¨ç‰¹å®šå¹´ä»½çš„æ•°æ®ï¼Œå¹¶ä¸”æ¯ä¸ªæŸ±å­éƒ½åˆ†ä¸ºä¸åŒçš„é¢œè‰²éƒ¨åˆ†ï¼Œè¡¨ç¤ºä¸åŒçš„ç±»åˆ«ã€‚åœ¨å›¾è¡¨çš„å³ä¾§ï¼Œæœ‰ä¸¤ä¸ªæ•°å­—è¢«çªå‡ºæ˜¾ç¤ºï¼š483å’Œ756ã€‚å¹»ç¯ç‰‡çš„èƒŒæ™¯æ˜¯ç™½è‰²çš„ï¼Œæ•´ä½“è®¾è®¡ç®€æ´ä¸“ä¸šã€‚</sample>
    <sample id="12">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="13">è¿™æ˜¯ä¸€å¼ å…³äºâ€œGerman Text Simplification Corporaâ€çš„å¹»ç¯ç‰‡ï¼Œä¸»è¦å±•ç¤ºäº†å¥å­çº§åˆ«çš„ç®€åŒ–ç»“æœã€‚å¹»ç¯ç‰‡ä¸­æœ‰ä¸€ä¸ªæŸ±çŠ¶å›¾ï¼Œæ˜¾ç¤ºäº†ä¸åŒå¹´ä»½çš„ç®€åŒ–å¥å­å¯¹æ•°é‡ã€‚å›¾è¡¨æ—è¾¹æœ‰ä¸€ä¸ªå›¾ä¾‹ï¼Œè§£é‡Šäº†ä¸åŒé¢œè‰²ä»£è¡¨çš„ä¸åŒç±»å‹æ•°æ®ã€‚å¹»ç¯ç‰‡é¡¶éƒ¨æœ‰ä¸€ä¸ªè“è‰²çš„æ ‡é¢˜æ ï¼Œä¸Šé¢å†™ç€â€œGerman Text Simplification Corporaâ€ã€‚åœ¨å³ä¸Šè§’ï¼Œæœ‰ä¸€ä¸ªå°çª—å£æ˜¾ç¤ºäº†ä¸€ä¸ªæ­£åœ¨è®²è¯çš„äººã€‚</sample>
    <sample id="14">æˆ‘ä»¬åˆ†æäº†æˆ‘ä»¬çš„å¥å­å¯¹ï¼Œæ›´å¤šåœ°å…³æ³¨äº†ç®€åŒ–ç±»å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨ç®€åŒ–ç±»å‹ä¸­ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°</sample>
    <sample id="15">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="16">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="17">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="18">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†ï¼š 1. å·¦ä¸Šè§’çš„å›¾è¡¨æ ‡é¢˜æ˜¯â€œSimplification Typesâ€ï¼Œä¸‹æ–¹æœ‰å››ä¸ªç±»åˆ«ï¼Œåˆ†åˆ«æ˜¯ï¼š- news (n=46) - bible (n=155) - L2 (n=157) - fiction (n=72) æ¯ä¸ªç±»åˆ«ä¸‹é¢æœ‰ä¸‰ä¸ªæŸ±çŠ¶å›¾ï¼Œåˆ†åˆ«ä»£è¡¨ä¸‰ç§ç®€åŒ–ç±»å‹ï¼š- Simplicityï¼ˆè“è‰²ï¼‰- LexSimpï¼ˆçº¢è‰²ï¼‰- StructSimpï¼ˆé»„è‰²ï¼‰ 2. å³ä¸‹è§’çš„å›¾è¡¨æ ‡é¢˜æ˜¯â€œSimplification Transformationsâ€ï¼Œä¸‹æ–¹æœ‰å…­ä¸ªç±»åˆ«ï¼Œåˆ†åˆ«æ˜¯ï¼š- moving - engineering - lexical substitution - adverbial - verb addition - noun deletion æ¯ä¸ªç±»åˆ«ä¸‹é¢æœ‰ä¸¤ä¸ªæŸ±çŠ¶å›¾ï¼Œåˆ†åˆ«ä»£è¡¨ä¸¤ç§æ–¹æ³•ï¼š- DEplain-apaï¼ˆè“è‰²ï¼‰- DEplain-webï¼ˆç»¿è‰²ï¼‰ è¿™äº›å›¾è¡¨å±•ç¤ºäº†ä¸åŒç±»å‹çš„ç®€åŒ–å’Œè½¬æ¢åœ¨ä¸åŒæ•°æ®é›†æˆ–æ–¹æ³•ä¸‹çš„åˆ†å¸ƒæƒ…å†µã€‚</sample>
    <sample id="19">3. Use-cases Automatic alignment and simplification</sample>
    <sample id="20">åœ¨æœ€è¿‘çš„æ—¶æœŸï¼Œæœºå™¨ç¿»è¯‘ä¸­å‡ºç°äº†è®¸å¤šå¯¹é½æ–¹æ³•ã€‚</sample>
    <sample id="21">The image shows a table titled "Automatic Alignment Evaluation" with two sections. The upper section is labeled "1:1 (upper part)" and the lower section is labeled "n cm capabilities (lower part)." Below these labels, there are columns for different methods of alignment evaluation, including LHA, Sent-LA-LEASE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign. Each method has corresponding numerical values under various headings such as P, R, F, P-R, and ncm. These numbers likely represent performance metrics or results related to the effectiveness of each alignment method in matching sentences between parallel documents written in different languages.</sample>
    <sample id="22">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹æ˜¯ä¸€ä¸ªå…³äºè‡ªåŠ¨å¯¹é½è¯„ä¼°çš„è¡¨æ ¼ã€‚è¡¨æ ¼æ ‡é¢˜ä¸ºâ€œAutomatic Alignment Evaluationâ€ï¼Œåˆ†ä¸ºä¸Šä¸‹ä¸¤éƒ¨åˆ†ï¼Œä¸ŠåŠéƒ¨åˆ†æ˜¾ç¤ºäº†1:1çš„å¯¹é½æ–¹æ³•ï¼Œä¸‹åŠéƒ¨åˆ†æ˜¾ç¤ºäº†n:cmçš„å¯¹é½æ–¹æ³•ã€‚

è¡¨æ ¼ä¸­åˆ—å‡ºäº†å‡ ç§å¯¹é½æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼š
- Sent-LA-LBSE
- CATS-C3G
- VecAlign
- BERTAlign
- MASSAlign

æ¯ç§æ–¹æ³•éƒ½æœ‰ä¸€ä¸ªæè¿°ï¼ˆDescriptionï¼‰å’Œä¸‰ä¸ªè¯„ä¼°æŒ‡æ ‡ï¼ˆP, F, Rï¼‰ï¼Œä»¥åŠä¸€ä¸ªn:cmæŒ‡æ ‡ã€‚æè¿°éƒ¨åˆ†è¯¦ç»†è¯´æ˜äº†æ¯ç§æ–¹æ³•çš„å…·ä½“æ“ä½œå’Œç‰¹ç‚¹ã€‚ä¾‹å¦‚ï¼ŒSent-LA-LBSEä½¿ç”¨å¥å­åµŒå…¥ç›¸ä¼¼æ€§è¿›è¡Œå¯¹é½ï¼Œè€ŒCATS-C3GåŸºäºå¤šè¯­ç§çš„è¯å‘é‡è¿›è¡Œå¯¹é½ã€‚

è¡¨æ ¼ä¸­çš„æ•°æ®ä»¥æ•°å­—å½¢å¼å±•ç¤ºï¼Œè¡¨ç¤ºä¸åŒå¯¹é½æ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚</sample>
    <sample id="23">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="24">The image shows a table titled 'Automatic Alignment Evaluation' with two sections: the upper part labeled '1:1' and the lower part labeled 'n:n cm'. The table lists various alignment methods along with their descriptions, precision (P), recall (R), F1 score, and mean reciprocal rank (mRR) values. Some of the listed methods include Sent-LA-BASE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign. Each method has corresponding numerical data for P, R, F1, and mRR under both 1:1 and n:n cm categories.</sample>
    <sample id="25">The image shows a table with the title 'Automatic Alignment Evaluation' at the top. The table is divided into two sections: the upper part labeled '1:1 (upper part)' and the lower part labeled 'n:n cm (lower part)'. Below these labels, there are columns for different methods of alignment evaluation, including their names and descriptions.

The first method listed is 'LHA-LASE', described as using sentence embeddings similarity. It has corresponding numerical values in various categories such as P, R, F1, PR, and n:n cm.

The second method is 'Similar embeddings of Language-agnostic BERT transformer', which also includes similar metrics but slightly different numbers compared to LHA-LASE.

Other methods mentioned include CATS-C3G, VecAlign, BERTAlign, and MASSAlign, each with their own set of performance metrics.

Overall, the image presents a detailed comparison of different automatic alignment methods based on specific evaluations like precision (P), recall (R), F1 score, harmonic mean of precision and recall (PR), and normalized mutual information (n:n cm).</sample>
    <sample id="26">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="27">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="28">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œAutomatic Text Simplificationâ€ã€‚å¹»ç¯ç‰‡çš„ä¸»è¦å†…å®¹æ˜¯å…³äºä½¿ç”¨fine-tuned language modelsè¿›è¡Œæ–‡æœ¬ç®€åŒ–çš„ç»“æœã€‚å¹»ç¯ç‰‡åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šDocument Levelå’ŒSentence Levelã€‚æ¯ä¸ªéƒ¨åˆ†éƒ½åŒ…å«è®­ç»ƒæ•°æ®ï¼ˆtrain dataï¼‰å’Œæµ‹è¯•æ•°æ®ï¼ˆtest dataï¼‰çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬BLEUã€ROUGEã€F1å’ŒPPLç­‰æŒ‡æ ‡ã€‚å…·ä½“æ¥è¯´ï¼ŒDocument Leveléƒ¨åˆ†å±•ç¤ºäº†DEPLAIN-APA test (n=48)å’ŒDEPLAIN-WEB test (n=147)çš„æ•°æ®ï¼Œè€ŒSentence Leveléƒ¨åˆ†åˆ™å±•ç¤ºäº†DEPLAIN-APA test (n=1231)å’ŒDEPLAIN-WEB test (n=1846)çš„æ•°æ®ã€‚è¿™äº›æ•°æ®è¡¨æ˜äº†åœ¨ä¸åŒæµ‹è¯•é›†ä¸Šæ¨¡å‹çš„è¡¨ç°ã€‚</sample>
    <sample id="29">æ ‡é¢˜ï¼šè‡ªåŠ¨æ–‡æœ¬ç®€åŒ–</sample>
    <sample id="30">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="31">è¿™æ˜¯ä¸€å¼ å…³äºè‡ªåŠ¨æ–‡æœ¬ç®€åŒ–ï¼ˆAutomatic Text Simplificationï¼‰çš„å¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªæ ‡é¢˜â€œAutomatic Text Simplificationâ€ï¼Œä¸‹é¢æœ‰ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šDocument Levelå’ŒSentence Levelã€‚æ¯ä¸ªéƒ¨åˆ†éƒ½åŒ…å«äº†ä¸€äº›è¡¨æ ¼ï¼Œæ˜¾ç¤ºäº†åœ¨ä¸åŒæµ‹è¯•é›†ä¸Šçš„ç»“æœï¼ŒåŒ…æ‹¬BLEUã€F1ã€Pã€Rç­‰æŒ‡æ ‡ã€‚è¿™äº›è¡¨æ ¼å±•ç¤ºäº†åœ¨ä¸åŒè®­ç»ƒæ•°æ®é•¿åº¦ä¸‹çš„è¡¨ç°ã€‚</sample>
    <sample id="32">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªå…³äºè‡ªåŠ¨æ–‡æœ¬ç®€åŒ–æŠ€æœ¯çš„æ¼”ç¤ºå¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œAutomatic Text Simplificationâ€ã€‚å¹»ç¯ç‰‡åˆ†ä¸ºä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šæ–‡æ¡£çº§åˆ«å’Œå¥å­çº§åˆ«ã€‚æ¯ä¸ªéƒ¨åˆ†éƒ½æ˜¾ç¤ºäº†ä½¿ç”¨fine-tuned mBARTè¿›è¡Œæ–‡æœ¬ç®€åŒ–çš„ç»“æœï¼ŒåŒ…æ‹¬BLEUã€FREå’ŒPPLç­‰æŒ‡æ ‡ã€‚è¡¨æ ¼ä¸­åˆ—å‡ºäº†åœ¨DEPLAIN-APAæµ‹è¯•ï¼ˆn=48ï¼‰å’ŒDEPLAIN-WEBæµ‹è¯•ï¼ˆn=147ï¼‰ä¸Šçš„ç»“æœã€‚å¹»ç¯ç‰‡è¿˜æåˆ°äº†baseline scoresï¼Œå¹¶æ¯”è¾ƒäº†fine-tunedæ¨¡å‹ä¸baselineæ¨¡å‹ä¹‹é—´çš„è¡¨ç°ã€‚èƒŒæ™¯ä¸­æœ‰ä¸€ä¸ªç©¿ç€çº¢è‰²è¡¬è¡«çš„äººï¼Œä¼¼ä¹åœ¨è®²è§£æˆ–å±•ç¤ºè¿™äº›å†…å®¹ã€‚</sample>
    <sample id="33">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="34">è°¢è°¢æ‚¨çš„å…³æ³¨ï¼Œå¸Œæœ›åœ¨ä¼šè®®ä¸Šèƒ½è§åˆ°å¤§å®¶ã€‚</sample>
    <sample id="35">æ¼”è®²è€…çš„åå­—æ˜¯Kayo Yinã€‚</sample>
    <sample id="36">T5 XL model.</sample>
    <sample id="37">Yes, CoNLL-2003 taggers still work.</sample>
    <sample id="38">The proposed method for annotating behaviors in chat, referred to as ABC-Eval (Annotating Behaviors in Chat), introduces a novel approach by explicitly labeling whether or not each model response expresses certain behaviors. These behaviors include responding with irrelevant information and lacking empathy or self-contradiction. This explicit annotation helps reduce the subjectivity of human evaluation, making it more objective and consistent across different evaluators.</sample>
    <sample id="39">ç°æœ‰å¼±ç›‘ç£æ–¹æ³•çš„æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¹²å‡€çš„éªŒè¯æ ·æœ¬ã€‚</sample>
    <sample id="40">To improve scores, annotators are asked to listen to at least some of each song and read about each song.</sample>
    <sample id="41">è¿™ç¯‡è®ºæ–‡æœ‰å››ä½ä½œè€…ã€‚</sample>
    <sample id="42">Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination Adam PrzeÅºdzieckiowski and MichaÅ‚ WoÅºniak Institute of Computer Science Polish Academy of Sciences ul. Jana Kazimierza 5, 01-248 Warsaw University of Warsaw ACL 2023</sample>
    <sample id="43">ç”»é¢ä¸­å±•ç¤ºäº†ä¸€å¼ å…³äºâ€œä¾èµ–ç»“æ„çš„åè°ƒâ€çš„å¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œä¾èµ–ç»“æ„çš„åè°ƒâ€ã€‚å¹»ç¯ç‰‡ä¸Šåˆ—å‡ºäº†å››ç§ä¸åŒçš„ä¾èµ–ç»“æ„ï¼šBouquet/Stanfordï¼ˆé€šç”¨ä¾èµ–ï¼‰ã€Chain/Moscowã€Conjunction-headed/Pragueå’ŒMulti-headed/Londonã€‚æ¯ä¸ªç»“æ„ä¸‹é¢éƒ½æœ‰ä¸€ä¸ªä¾‹å­å¥å­ï¼šâ€œè·é©¬çˆ±ä¸½èã€å·´ç‰¹å’Œç›å‰ã€‚â€ å¹»ç¯ç‰‡çš„å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£ï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªäººçš„éƒ¨åˆ†è„¸åºã€‚</sample>
    <sample id="44">ä¾èµ–ç»“æ„çš„åè°ƒï¼ŒåŒ…æ‹¬ï¼š - Bouquet/Stanfordï¼ˆé€šç”¨ä¾èµ–ï¼‰ï¼šHomer loves Lisa, Bart, and Maggie. - Chain/Moscowï¼šHomer loves Lisa, Bart, and Maggieã€‚ - è¿æ¥å¤´/Pragueï¼šHomer loves Lisa, Bart, and Maggieã€‚ - å¤šå¤´/Londonï¼šHomer loves Lisa, Bart, and Maggieã€‚</sample>
    <sample id="45">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹æ˜¯å…³äºä¾èµ–ç»“æ„çš„åè°ƒã€‚</sample>
    <sample id="46">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š 1. æ ‡é¢˜ï¼šDependency Structure of Coordination 2. æ®µè½æ ‡é¢˜å’Œå†…å®¹ï¼š - Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. - Chain/Moscow: Homer loves Lisa, Bart, and Maggie. - Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. - Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="47">ä¾èµ–ç»“æ„çš„åè°ƒ</sample>
    <sample id="48">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œä¾èµ–ç»“æ„åè°ƒâ€ã€‚å¹»ç¯ç‰‡ä¸Šåˆ—å‡ºäº†å››ç§ä¸åŒçš„ä¾èµ–å…³ç³»ç»“æ„ï¼šBouquet/Stanfordï¼ˆé€šç”¨ä¾èµ–å…³ç³»ï¼‰ã€Chain/Moscowã€Conjunction-headed/Pragueå’ŒMulti-headed/Londonã€‚æ¯ç§ç»“æ„éƒ½ç”¨ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜ï¼Œè¿™ä¸ªä¾‹å­æ˜¯â€œHomer loves Lisa, Bart, and Maggieã€‚â€å¹»ç¯ç‰‡çš„èƒŒæ™¯æ˜¯ç™½è‰²çš„ï¼Œæ–‡å­—æ˜¯é»‘è‰²çš„ã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çš„è§†é¢‘çª—å£ï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªäººåœ¨æ¼”ç¤ºã€‚</sample>
    <sample id="49">ç”»é¢ä¸­çš„æ–‡å­—æ˜¯â€œDependency Structure of Coordinationâ€ã€‚</sample>
    <sample id="50">å›¾ç‰‡ä¸­çš„è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ä¸ºï¼š ä¾èµ–ç»“æ„çš„åè°ƒï¼š - Bouquet/Stanfordï¼ˆé€šç”¨ä¾èµ–å…³ç³»ï¼‰ï¼šè·é©¬çˆ±ä¸½èã€å·´ç‰¹å’Œç›å‰ã€‚ - Chain/Moscowï¼šè·é©¬çˆ±ä¸½èã€å·´ç‰¹å’Œç›å‰ã€‚ - Conjunction-headed/Pragueï¼šè·é©¬çˆ±ä¸½èã€å·´ç‰¹å’Œç›å‰ã€‚ - Multi-headed/Londonï¼šè·é©¬çˆ±ä¸½èã€å·´ç‰¹å’Œç›å‰ã€‚</sample>
    <sample id="51">å›¾ç‰‡ä¸­çš„è‹±æ–‡å†…å®¹æ˜¯å…³äºä¾èµ–é•¿åº¦æœ€å°åŒ–ï¼ˆDependency Length Minimizationï¼Œç®€ç§°DLMï¼‰ã€‚å®ƒè§£é‡Šäº†è¯åºå€¾å‘äºæœ€å°åŒ–ä¾èµ–é•¿åº¦ã€‚å›¾ç‰‡ä¸­å±•ç¤ºäº†ä¸¤ä¸ªä¾‹å­ï¼Œä¸€ä¸ªæ˜¯å¥½çš„ï¼ˆgoodï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯ä¸å¥½çš„ï¼ˆbadï¼‰ï¼Œé€šè¿‡å›¾ç¤ºå’Œæ–‡æœ¬æ¥è¯´æ˜ä¾èµ–é•¿åº¦çš„å·®å¼‚ã€‚</sample>
    <sample id="52">Word order tends to minimize dependency lengths:</sample>
    <sample id="53">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬æ ‡é¢˜â€œDependency Length Minimization (DLM)â€å’Œå‰¯æ ‡é¢˜â€œWord order tends to minimize dependency lengths:â€ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸¤ä¸ªå¥å­çš„ç¤ºä¾‹ï¼šâ€œMarge read it yesterday.â€ å’Œ â€œMarge read yesterday it.â€ã€‚è¿™ä¸¤ä¸ªå¥å­ä¸‹é¢åˆ†åˆ«æ ‡æ³¨äº†â€œgoodâ€å’Œâ€œbadâ€ï¼Œè¡¨ç¤ºå®ƒä»¬çš„ä¾èµ–é•¿åº¦ã€‚åœ¨è¿™äº›å¥å­ä¸‹æ–¹ï¼Œæœ‰ä¸€ä¸ªå›¾è¡¨æ˜¾ç¤ºäº†å•è¯çš„æ’åˆ—æ–¹å¼åŠå…¶ä¾èµ–å…³ç³»ã€‚</sample>
    <sample id="54">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªå…³äºä¾èµ–é•¿åº¦æœ€å°åŒ–çš„è¯­è¨€å­¦æ¦‚å¿µçš„å¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡é¡¶éƒ¨æœ‰ä¸€ä¸ªè“è‰²æ ‡é¢˜æ ï¼Œä¸Šé¢å†™ç€â€œDependency Length Minimization (DLM)â€ã€‚åœ¨æ ‡é¢˜æ ä¸‹é¢ï¼Œæœ‰ä¸€ä¸ªå‰¯æ ‡é¢˜å†™ç€â€œWord order tends to minimize dependency lengths:â€ï¼Œåé¢è·Ÿç€ä¸¤ä¸ªä¾‹å­ã€‚

ç¬¬ä¸€ä¸ªä¾‹å­æ˜¾ç¤ºäº†ä¸€ä¸ªå¥å­â€œMarge read it yesterday.â€ï¼Œå…¶ä¸­â€œitâ€è¢«æ ‡è®°ä¸ºçº¢è‰²ï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªç®­å¤´æŒ‡å‘â€œyesterdayâ€ï¼Œè¡¨ç¤ºä¸€ä¸ªä¾èµ–å…³ç³»ã€‚è¿™ä¸ªä¾‹å­æ—è¾¹æœ‰æ–‡å­—â€œgoodâ€ï¼Œç”¨ç»¿è‰²å­—ä½“æ ‡æ³¨ã€‚

ç¬¬äºŒä¸ªä¾‹å­ä¹Ÿæ˜¾ç¤ºäº†ç›¸åŒçš„å¥å­â€œMarge read it yesterday.â€ï¼Œä½†è¿™æ¬¡â€œitâ€è¢«æ ‡è®°ä¸ºç»¿è‰²ï¼Œç®­å¤´æŒ‡å‘â€œyesterdayâ€ã€‚è¿™ä¸ªä¾‹å­æ—è¾¹æœ‰æ–‡å­—â€œbadâ€ï¼Œç”¨çº¢è‰²å­—ä½“æ ‡æ³¨ã€‚

åœ¨å¹»ç¯ç‰‡çš„ä¸‹åŠéƒ¨åˆ†ï¼Œæœ‰ä¸€ä¸ªæ›´å¤æ‚çš„å¥å­ç»“æ„ï¼Œæ˜¾ç¤ºäº†å¤šä¸ªå•è¯åŠå…¶ä¾èµ–å…³ç³»ã€‚è¿™ä¸ªå¥å­æ˜¯â€œMarge read this absolutely fascinating book about bees yesterday.â€ï¼Œå¹¶ä¸”æ¯ä¸ªå•è¯éƒ½æœ‰ç›¸åº”çš„ç¼–å·å’Œç®­å¤´æŒ‡ç¤ºå®ƒä»¬ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚è¿™ä¸ªå¥å­æ—è¾¹ä¹Ÿæœ‰æ–‡å­—â€œgoodâ€ï¼Œç”¨ç»¿è‰²å­—ä½“æ ‡æ³¨ã€‚

å¹»ç¯ç‰‡çš„å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£ï¼Œæ˜¾ç¤ºäº†ä¸€ä½æ¼”è®²è€…çš„å›¾åƒï¼Œè¡¨æ˜è¿™å¯èƒ½æ˜¯åœ¨çº¿è®²åº§æˆ–æ¼”ç¤ºçš„ä¸€éƒ¨åˆ†ã€‚</sample>
    <sample id="55">ä¾èµ–é•¿åº¦æœ€å°åŒ–ï¼ˆDLMï¼‰æ˜¯ä¸€ç§è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œå®ƒé€šè¿‡è°ƒæ•´å¥å­çš„é¡ºåºæ¥å‡å°‘ä¾èµ–å…³ç³»çš„é•¿åº¦ã€‚è¿™æœ‰åŠ©äºæé«˜å¥å­çš„å¯è¯»æ€§å’Œæµç•…æ€§ã€‚</sample>
    <sample id="56">&lt;no_answer&gt;</sample>
    <sample id="57">Word order tends to minimize dependency lengths.</sample>
    <sample id="58">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œä¾èµ–é•¿åº¦æœ€å°åŒ–ï¼ˆDLMï¼‰â€çš„å¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡é¡¶éƒ¨æœ‰ä¸€ä¸ªè“è‰²æ¨ªå¹…ï¼Œä¸Šé¢å†™ç€â€œä¾èµ–é•¿åº¦æœ€å°åŒ–ï¼ˆDLMï¼‰â€ã€‚åœ¨æ¨ªå¹…ä¸‹æ–¹ï¼Œæœ‰ä¸€æ®µæ–‡å­—è¯´æ˜ï¼šâ€œè¯åºå€¾å‘äºæœ€å°åŒ–ä¾èµ–å…³ç³»é•¿åº¦ï¼šâ€ã€‚å¹»ç¯ç‰‡ä¸Šæ˜¾ç¤ºäº†ä¸¤ä¸ªå¥å­çš„ä¾‹å­ï¼Œæ¯ä¸ªå¥å­éƒ½æœ‰ä¸åŒçš„è¯åºå’Œç›¸åº”çš„ä¾èµ–å…³ç³»å›¾ã€‚ç¬¬ä¸€ä¸ªä¾‹å­æ˜¯â€œMarge read it yesterday.â€ï¼Œæ ‡è®°ä¸ºâ€œå¥½â€ï¼Œç¬¬äºŒä¸ªä¾‹å­æ˜¯â€œMarge read yesterday it.â€ï¼Œæ ‡è®°ä¸ºâ€œåâ€ã€‚ç¬¬ä¸‰ä¸ªä¾‹å­æ˜¯â€œMarge read this absolutely fascinating book about bees yesterday.â€ï¼Œæ ‡è®°ä¸ºâ€œå¥½â€ï¼Œç¬¬å››ä¸ªä¾‹å­æ˜¯â€œMarge read this absolutely fascinating book about bees good yesterday.â€ï¼Œæ ‡è®°ä¸ºâ€œå¥½â€ã€‚æ¯ä¸ªä¾‹å­éƒ½å±•ç¤ºäº†ä¸åŒçš„è¯åºåŠå…¶å¯¹åº”çš„ä¾èµ–å…³ç³»å›¾ã€‚</sample>
    <sample id="59">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªå…³äºä¾èµ–é•¿åº¦æœ€å°åŒ–çš„è¯­è¨€å­¦æ¦‚å¿µã€‚æ ‡é¢˜æ˜¯â€œDependency Length Minimization (DLM)â€ã€‚å›¾ç‰‡ä¸­åŒ…å«äº†ä¸€äº›å¥å­å’Œå®ƒä»¬çš„ä¾å­˜å…³ç³»å›¾ï¼Œè¿™äº›å›¾å±•ç¤ºäº†ä¸åŒå•è¯ä¹‹é—´çš„å…³ç³»ã€‚æ¯ä¸ªå¥å­éƒ½æœ‰ä¸€ä¸ªç»¿è‰²çš„â€œgoodâ€æ ‡è®°ï¼Œè¡¨ç¤ºå®ƒä»¬çš„ä¾å­˜å…³ç³»å›¾æ˜¯æœ‰æ•ˆçš„ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªçº¢è‰²çš„â€œbadâ€æ ‡è®°ï¼Œè¡¨ç¤ºä¸€ä¸ªæ— æ•ˆçš„ä¾å­˜å…³ç³»å›¾ã€‚å›¾ç‰‡è¿˜åŒ…å«ä¸€äº›ä¸­æ–‡æ³¨é‡Šï¼Œè§£é‡Šäº†ä¾å­˜å…³ç³»å›¾ä¸­çš„æŸäº›å†…å®¹ã€‚</sample>
    <sample id="60">ä¾èµ–é•¿åº¦æœ€å°åŒ–ï¼ˆDLMï¼‰æ˜¯æŒ‡é€šè¿‡è°ƒæ•´è¯åºæ¥å‡å°‘ä¾èµ–å…³ç³»çš„é•¿åº¦ã€‚</sample>
    <sample id="61">å¥½çš„ï¼Œè¿™æ˜¯å…³äºä¾èµ–é•¿åº¦æœ€å°åŒ–çš„æ¼”ç¤ºã€‚å®ƒå±•ç¤ºäº†å¦‚ä½•é€šè¿‡è°ƒæ•´å¥å­çš„é¡ºåºæ¥æœ€å°åŒ–ä¾èµ–å…³ç³»çš„é•¿åº¦ã€‚</sample>
    <sample id="62">The text in the image is about conjunct lengths in English. It discusses statistics related to coordination extracted from an enhanced version of the Penn Treebank, referencing studies by Marcus et al., 1993; Ficler and Goldberg, 2016. The points mentioned include: - Left conjuncts tend to be shorter (observed before). - This tendency grows with length difference (briefly noticed in Gibson et al., 1996:88-90). - However, when only the governor is on the left or absent (example given: "I saw Bart and Lisa; Homer came and sneezed"), not when it is on the right ("Ted and Ted laughed").</sample>
    <sample id="63">Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before) this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88â€“90), but only when the governor is on the left or absent (I saw Bart and Lisa: Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="64">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œè‹±è¯­ä¸­çš„è¿è¯é•¿åº¦â€ã€‚å®ƒè®¨è®ºäº†ä»Penn Treebankï¼ˆMarcusç­‰äººï¼Œ1993å¹´ï¼›Ficlerå’ŒGoldbergï¼Œ2016å¹´ï¼‰ä¸­æå–çš„å…³äºåè°ƒçš„ç»Ÿè®¡ä¿¡æ¯ã€‚å®ƒæŒ‡å‡ºï¼Œè¿è¯é€šå¸¸è¾ƒçŸ­ï¼ˆå¦‚å‰æ‰€è¿°ï¼‰ï¼Œè¿™ç§å€¾å‘éšç€é•¿åº¦å·®å¼‚è€Œå¢é•¿ï¼ˆGibsonç­‰äººçš„1996å¹´ï¼š88-90ï¼‰ã€‚å®ƒè¿˜æåˆ°ï¼Œå½“ä¸»è¯­ä½äºå·¦ä¾§æˆ–ä¸å­˜åœ¨æ—¶ï¼ˆä¾‹å¦‚ï¼Œâ€œæˆ‘çœ‹åˆ°äº†å·´ç‰¹å’Œä¸½èï¼›å·´ç‰¹æ¥äº†ï¼Œæ‰“äº†ä¸ªå–·åšâ€ï¼‰ï¼Œè¿è¯æ›´çŸ­ï¼Œå½“ä¸»è¯­ä½äºå³ä¾§æ—¶ï¼ˆä¾‹å¦‚ï¼Œâ€œä¸æ˜¯ï¼Œå½“ç‰¹å¾·åœ¨å³ä¾§æ—¶ï¼Œç‰¹å¾·å’Œä¸½è¨ç¬‘äº†â€ï¼‰ï¼Œè¿è¯æ›´é•¿ã€‚</sample>
    <sample id="65">Conjunct Lengths in English</sample>
    <sample id="66">å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªæ ‡é¢˜ï¼Œå†™ç€â€œè‹±è¯­è¿è¯é•¿åº¦â€ã€‚å®ƒè®¨è®ºäº†æ¥è‡ªå®¾å¤•æ³•å°¼äºšæ ‘åº“ï¼ˆPenn Treebankï¼‰çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œè¯¥ä¿¡æ¯åŸºäºMarcusç­‰äººäº1993å¹´ã€Ficerå’ŒGoldbergäº2016å¹´çš„ç ”ç©¶ã€‚å®ƒæŒ‡å‡ºï¼Œè¿è¯å€¾å‘äºæ›´çŸ­ï¼ˆå¦‚Gibsonäº1996å¹´æ‰€æ³¨æ„åˆ°çš„ï¼‰ï¼Œå¹¶ä¸”è¿™ç§è¶‹åŠ¿éšç€é•¿åº¦å·®å¼‚è€Œå¢é•¿ã€‚å®ƒè¿˜æåˆ°ï¼Œå½“ä»å¥çš„ä¸»è¯­åœ¨å·¦ä¾§æˆ–ä¸å­˜åœ¨æ—¶ï¼ˆå¦‚ä¾‹å­â€œI saw Bart and Lisa; Homer came and sneezedâ€æ‰€ç¤ºï¼‰ï¼Œè¿™ç§è¶‹åŠ¿ä¼šå‘ç”Ÿã€‚æœ€åï¼Œå®ƒæŒ‡å‡ºï¼Œå½“ä»å¥çš„ä¸»è¯­åœ¨å³ä¾§æ—¶ï¼ˆå¦‚ä¾‹å­â€œnot when it is on the right (Ted and Ned laughed)â€æ‰€ç¤ºï¼‰ï¼Œè¿™ç§è¶‹åŠ¿ä¸ä¼šå‘ç”Ÿã€‚</sample>
    <sample id="67">å¹»ç¯ç‰‡å±•ç¤ºäº†å…³äºè‹±è¯­è¿è¯é•¿åº¦çš„ç»Ÿè®¡ä¿¡æ¯ã€‚æ ‡é¢˜ä¸ºâ€œè‹±è¯­ä¸­çš„è¿è¯é•¿åº¦â€ã€‚å†…å®¹å¼•ç”¨äº†Penn Treebankï¼ˆMarcusç­‰äººï¼Œ1993å¹´ï¼‰å’ŒFiclerå’ŒGoldbergï¼ˆ2016å¹´ï¼‰çš„ç ”ç©¶ã€‚å®ƒæŒ‡å‡ºï¼Œè¿è¯é€šå¸¸è¾ƒçŸ­ï¼ˆä¹‹å‰è§‚å¯Ÿåˆ°ï¼‰ï¼Œè¿™ç§è¶‹åŠ¿éšç€é•¿åº¦å·®å¼‚è€Œå¢é•¿ï¼ˆGibsonï¼Œ1996å¹´ï¼‰ã€‚ä¸€ä¸ªä¾‹å­è¯´æ˜äº†å½“å·é•¿åœ¨å·¦è¾¹æˆ–ç¼ºå¸­æ—¶ï¼Œè¿è¯ä¼šæ›´çŸ­ï¼ˆä¾‹å¦‚ï¼Œâ€œæˆ‘çœ‹åˆ°Bartå’ŒLisaï¼›Homeræ¥äº†ï¼Œç„¶åæ‰“äº†ä¸ªå–·åšâ€ï¼‰ã€‚å¦ä¸€ä¸ªä¾‹å­æ˜¯å½“å·é•¿åœ¨å³è¾¹æ—¶ï¼Œè¿è¯ä¸ä¼šæ›´çŸ­ï¼ˆä¾‹å¦‚ï¼Œâ€œä»–ä»¬ç¬‘å¾—åœä¸ä¸‹æ¥â€ï¼‰ã€‚</sample>
    <sample id="68">å¹»ç¯ç‰‡è®¨è®ºäº†è‹±è¯­ä¸­çš„è¿è¯é•¿åº¦ï¼Œç‰¹åˆ«æ˜¯åè°ƒè¿è¯çš„é•¿åº¦ã€‚å®ƒå¼•ç”¨äº†Penn Treebankçš„æ•°æ®ï¼Œå¹¶æåˆ°Ficlerå’ŒGoldbergåœ¨1993å¹´å’Œ1996å¹´çš„ç ”ç©¶ã€‚å¹»ç¯ç‰‡æŒ‡å‡ºï¼Œå½“è¿è¯ä½äºå·¦ä¾§æˆ–ä¸å­˜åœ¨æ—¶ï¼Œè¿è¯å€¾å‘äºæ›´çŸ­ã€‚å®ƒè¿˜æåˆ°äº†è¿è¯é•¿åº¦éšè¿è¯å¢é•¿çš„æƒ…å†µï¼Œå¹¶ç»™å‡ºäº†ä¾‹å­æ¥è¯´æ˜è¿™ä¸€ç‚¹ã€‚</sample>
    <sample id="69">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88â€“90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="70">å›¾1æ˜¾ç¤ºäº†åœ¨å­—ç¬¦é•¿åº¦ç»å¯¹å·®å¼‚ä¸å­—ç¬¦é•¿åº¦ç»å¯¹å·®å¼‚çš„å¹³æ–¹æ ¹ä¹‹é—´å…³ç³»çš„å›¾è¡¨ã€‚</sample>
    <sample id="71">å›¾ä¸­æ˜¾ç¤ºäº†åœ¨å­—ç¬¦é•¿åº¦ã€éŸ³èŠ‚é•¿åº¦å’Œå•è¯é•¿åº¦ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå½“å·é•¿åœ¨å·¦è¾¹æˆ–å³è¾¹æ—¶ï¼Œå·é•¿çš„ä»»æœŸå¯¹å·¦æ´¾å’Œå³æ´¾çš„è”ç³»æ¯”ä¾‹çš„å½±å“ã€‚</sample>
    <sample id="72">å›¾ä¸­å±•ç¤ºäº†ä¹ä¸ªå›¾è¡¨ï¼Œæ¯ä¸ªå›¾è¡¨éƒ½åŒ…å«ä¸€ä¸ªè“è‰²çš„çº¿æ€§è¶‹åŠ¿å›¾ã€‚è¿™äº›å›¾è¡¨åˆ†åˆ«æ ‡æ³¨ä¸ºâ€œNO governor (length in CHARACTERS)â€ã€â€œNO governor (length in SYLLABLES)â€å’Œâ€œNO governor (length in WORDS)â€ã€‚æ¯ä¸ªå›¾è¡¨è¿›ä¸€æ­¥åˆ†ä¸ºä¸‰åˆ—ï¼Œæ¯åˆ—ä»£è¡¨ä¸åŒçš„â€œGovernor on the LEFTâ€ã€â€œGovernor on the RIGHTâ€å’Œâ€œNO governorâ€çš„æƒ…å†µã€‚å›¾è¡¨æ˜¾ç¤ºäº†ç»å¯¹å·®å¼‚é•¿åº¦ä¸å·¦è¿è¯é•¿åº¦ä¹‹é—´çš„å…³ç³»ã€‚</sample>
    <sample id="73">å›¾ç‰‡ä¸­çš„è‹±æ–‡å†…å®¹ç¿»è¯‘ä¸ºï¼šå…¼å®¹æ€§ä¸åè°ƒä¾èµ–ç»“æ„çš„ä¾èµ–å…³ç³»ã€‚</sample>
    <sample id="74">ç”»é¢ä¸­æœ‰ä¸€ä¸ªç™½è‰²èƒŒæ™¯çš„æ–‡å­—å†…å®¹ï¼Œé¡¶éƒ¨å†™ç€â€œSee the paper for the full argument!â€ï¼Œåº•éƒ¨å†™ç€â€œTalk to us at the poster session!â€ã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£ï¼Œæ˜¾ç¤ºä¸€ä¸ªäººçš„å¤´åƒå’Œåå­—â€œDr. Christopher S. Kelloâ€ã€‚</sample>
    <sample id="75">è¿™ç¯‡è®ºæ–‡æœ‰ä¸‰ä½ä½œè€…ã€‚</sample>
    <sample id="76">The Bible text is much stronger simplified than, for example, the news text or language learner texts.</sample>
    <sample id="77">åå¥½è¾ƒçŸ­å·¦å¹¶åˆ—è¯çš„ç¤ºä¾‹æ˜¯â€œwhen the governor is on the left or absentâ€ã€‚</sample>
    <sample id="78">æ˜¯çš„ï¼Œä½ å¯ä»¥å°†è¿™äº›æ¨¡å‹ç”¨äºä½ çš„ç ”ç©¶ã€‚</sample>
    <sample id="79">DEplain-apa ä¸­åŒ…å«æ–°é—»æ–‡æœ¬ã€‚</sample>
    <sample id="80">For good generalization, we need a better model architecture, larger model size as well as more fine-tuning examples.</sample>
    <sample id="81">In the first column, it is measured in characters. In the second column, it's syllables and in the third one, words are used to measure length.</sample>
    <sample id="82">è¦ç ”ç©¶æ”¯é…è¯ä½ç½®çš„å½±å“ï¼Œå¯ä»¥è®¾è®¡ä¸€ä¸ªå®éªŒï¼Œé€šè¿‡æµ‹é‡æ”¯é…è¯åœ¨å­—ç¬¦ã€éŸ³èŠ‚å’Œå•è¯é•¿åº¦ä¸Šçš„å˜åŒ–æ¥è§‚å¯Ÿæ”¯é…è¯ä½ç½®çš„å˜åŒ–ã€‚</sample>
    <sample id="83">åŸºçº¿åˆ†ç±»å™¨åœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šçš„è®­ç»ƒæ•ˆæœä¸ä½³ï¼Œè¡¨ç°ä¸å¦‚éšæœºçŒœæµ‹ã€‚</sample>
    <sample id="84">There are four authors.</sample>
    <sample id="85">The cartoon has three speech bubbles. In the first bubble, Bob says "Remember that song we were listening to yesterday?"</sample>
    <sample id="86">è¯­å¢ƒæ„ŸçŸ¥çš„MTæ¨¡å‹åœ¨å¤„ç†æ­£å¼æ€§å’Œè¯æ±‡è¿è´¯æ€§æ–¹é¢æ¯”è¯­å¢ƒæ— å…³çš„æ¨¡å‹æ›´æœ‰ä¼˜åŠ¿ã€‚</sample>
    <sample id="87">The authors of this paper are affiliated with Johns Hopkins University, Purdue University, and MIT.</sample>
    <sample id="122">å¼•å…¥çš„æ¡†æ¶é€šè¿‡æ¯”è¾ƒæ³¨é‡Šæ•°æ®é›†ä¸­çš„æ³¨é‡Šä¸æ¨¡å‹é¢„æµ‹å’Œæ•°æ®é›†æ ‡ç­¾æ¥é‡åŒ–ç«‹åœºã€‚</sample>
    <sample id="155">åœ¨ä¹‹å‰çš„ç ”ç©¶ä¸­ï¼Œå½“äººç±»å—è¯•è€…è¢«ç»™äºˆç›¸åŒçš„äººæ ¼åŒ–æç¤ºæ—¶ï¼Œä»–ä»¬ä¹Ÿèƒ½æ­ç¤ºå‡ºç§æ—åˆ»æ¿å°è±¡ã€‚</sample>
    <sample id="156">This study used data from the Penn Treebank, specifically an enhanced version of it. It also referenced studies by Marcus et al., Gibson, and Ficler &amp; Goldberg for further information on coordination in English sentences.</sample>
    <sample id="157">The paper has two authors: Adam PrzepiÃ³rkowski and MichaÅ‚ WoÅºniak.</sample>
    <sample id="158">The tasks that are closely related to the concept of cognitive dissonance and consonance include 'debate' and 'CE'.</sample>
    <sample id="159">There are two authors of this paper.</sample>
    <sample id="160">è¿™ç¯‡è®ºæ–‡æœ‰7ä½ä½œè€…ã€‚</sample>
    <sample id="161">å¼•å…¥çš„æ¡†æ¶ä¸ä»¥å‰çš„ç ”ç©¶æœ‰ä½•ä¸åŒï¼Ÿ</sample>
    <sample id="162">White Stereotypes</sample>
    <sample id="163">MuDA benchmark results compared different commercial systems.</sample>
    <sample id="164">#ACL2023 From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models Shangbin Feng Chan Young Park Yuhan Liu Yulia Tsvetkov PAUL ALLEN SCHOOL UWNLP Carnegie Mellon University Language Technologies Institute</sample>
    <sample id="165">LM Training Data A mixed blessing Dodge, Jesse et al. "Documenting Large Web Corpora: A Case Study of the Common Crawl Corpus." Proceedings of the 37th Annual Conference on Computational Linguistics in the Spoken Language (COLING) 2021.</sample>
    <sample id="166">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="167">LMè®­ç»ƒæ•°æ®æ—¢å¸¦æ¥äº†å¥½å¤„ä¹Ÿå¸¦æ¥äº†æŒ‘æˆ˜ã€‚</sample>
    <sample id="168">å¥½çš„ï¼Œè¿™æ®µå†…å®¹è®¨è®ºäº†è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®çš„å¤æ‚æ€§ã€‚å®ƒæŒ‡å‡ºï¼Œä¸€æ–¹é¢ï¼Œèƒ½å¤Ÿä»å„ç§è§‚ç‚¹ä¸­å­¦ä¹ å¹¶åº†ç¥æ°‘ä¸»å’Œå¤šå…ƒæ€æƒ³æ˜¯ä»¶å¥½äº‹ã€‚å¦ä¸€æ–¹é¢ï¼Œä¸åŒçš„æ”¿æ²»è§‚ç‚¹æœ¬è´¨ä¸Šæ˜¯ç¤¾ä¼šåè§çš„ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡åº”ç”¨ä¸­çš„æ½œåœ¨å…¬å¹³é—®é¢˜ã€‚</sample>
    <sample id="169">To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks.</sample>
    <sample id="170">To this end</sample>
    <sample id="171">To this end, how to evaluate the political leaning of LMs? How do LMs with different political leanings perform? Does LM political leaning result in fairness issues in NLP applications?</sample>
    <sample id="172">åœ¨æ”¿æ²»ç§‘å­¦æ–‡çŒ®ä¸­ï¼Œè¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ˜¯é€šè¿‡ä½¿ç”¨æ”¿æ²»é—®å·è¿›è¡Œçš„ã€‚è¿™äº›é—®å·åŒ…æ‹¬æ”¿æ²»ææ€§æµ‹è¯•ï¼Œå¦‚æ”¿æ²»ææ€§æµ‹è¯•ã€‚é€šè¿‡ä½¿ç”¨ä¸åŒçš„æç¤ºæ ¼å¼æ¥æç¤ºè¯­è¨€æ¨¡å‹ï¼Œå¹¶ç¡®ä¿è‡ªåŠ¨è¯„ä¼°ä¸æ”¿æ²»ç§‘å­¦æ–‡çŒ®ä¸­çš„å†…å®¹ä¿æŒä¸€è‡´ã€‚</sample>
    <sample id="173">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªæ”¿æ²»å…‰è°±å›¾ï¼Œå›¾ä¸­åŒ…å«äº†å„ç§è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ã€‚è¿™ä¸ªå›¾è¢«åˆ†ä¸ºå››ä¸ªè±¡é™ï¼šå·¦ç¿¼ã€å³ç¿¼ã€ä¸“åˆ¶ä¸»ä¹‰å’Œè‡ªç”±ä¸»ä¹‰ã€‚æ¯ä¸ªè±¡é™ä»£è¡¨ä¸åŒçš„æ”¿æ²»å€¾å‘ã€‚å›¾ä¸­è¿˜æ ‡æ³¨äº†å‡ ä¸ªç‰¹å®šçš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚BERT-baseã€BERT-largeã€RoBERTa-baseç­‰ï¼Œå¹¶ç”¨ç®­å¤´è¿æ¥åˆ°å®ƒä»¬åœ¨å›¾ä¸­çš„ä½ç½®ã€‚è¿™äº›è¯­è¨€æ¨¡å‹åˆ†å¸ƒåœ¨å›¾çš„ä¸åŒéƒ¨åˆ†ï¼Œè¡¨æ˜å®ƒä»¬çš„æ”¿æ²»å€¾å‘å„ä¸ç›¸åŒã€‚</sample>
    <sample id="174">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ï¼Œåˆ†åˆ«ä»£è¡¨æ–°é—»åª’ä½“å’Œç¤¾äº¤åª’ä½“ï¼ˆRedditï¼‰ã€‚æ¯ä¸ªæ•°æ®é›†éƒ½åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼šå·¦ç¿¼ã€ä¸­å¿ƒå’Œå³ç¿¼ã€‚å¯¹äºæ–°é—»åª’ä½“ï¼Œå·¦ç¿¼å’Œå³ç¿¼çš„éƒ¨åˆ†è¢«æ ‡è®°ä¸ºâ€œleftâ€å’Œâ€œrightâ€ï¼Œè€Œä¸­å¿ƒéƒ¨åˆ†åˆ™æ ‡è®°ä¸ºâ€œcenterâ€ã€‚å¯¹äºç¤¾äº¤åª’ä½“ï¼ˆRedditï¼‰ï¼Œå·¦ç¿¼å’Œå³ç¿¼çš„éƒ¨åˆ†ä¹Ÿæ ‡è®°ä¸ºâ€œleftâ€å’Œâ€œrightâ€ï¼Œè€Œä¸­å¿ƒéƒ¨åˆ†åŒæ ·æ ‡è®°ä¸ºâ€œcenterâ€ã€‚è¿™äº›æ•°æ®é›†å¯èƒ½ç”¨äºè¯„ä¼°ä¸åŒæ¥æºçš„æ–‡æœ¬åœ¨æ”¿æ²»å€¾å‘ä¸Šçš„å·®å¼‚ã€‚</sample>
    <sample id="175">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="176">ç”»é¢ä¸­å±•ç¤ºäº†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œResultsâ€ã€‚å¹»ç¯ç‰‡çš„ä¸»è¦å†…å®¹æ˜¯å…³äºâ€œLMæ”¿æ²»å€¾å‘çš„å…šæ´¾è½¬ç§»â€ã€‚åœ¨æ ‡é¢˜ä¸‹æ–¹ï¼Œæœ‰ä¸€ä¸ªå›¾è¡¨ï¼Œåˆ†ä¸ºå››ä¸ªè±¡é™ã€‚æ¯ä¸ªè±¡é™éƒ½æ ‡æœ‰æ ‡ç­¾ï¼š'original news'ã€'reddit'ã€'news reddit'å’Œ'reddit original'ã€‚è¿™äº›æ ‡ç­¾åˆ†åˆ«å¯¹åº”ä¸åŒçš„é¢œè‰²åŒºåŸŸï¼Œè¡¨ç¤ºä¸åŒçš„æ•°æ®ç‚¹æˆ–ç»“æœã€‚åœ¨å›¾è¡¨çš„å³ä¾§ï¼Œæœ‰ä¸¤ä¸ªæ ‡ç­¾ï¼šâ€œRoBERTaâ€å’Œâ€œGPT-2â€ï¼Œå¯èƒ½ä»£è¡¨äº†ä¸¤ä¸ªä¸åŒçš„æ¨¡å‹æˆ–æ–¹æ³•ã€‚åœ¨å³ä¸Šè§’ï¼Œæœ‰ä¸€ä¸ªå°çª—å£æ˜¾ç¤ºäº†ä¸€ä¸ªäººï¼Œå¯èƒ½æ˜¯æ¼”ç¤ºè€…ã€‚</sample>
    <sample id="177">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="178">ç»“æœæ˜¯ï¼Œå¯¹äºRoBERTaï¼Œè¿›ä¸€æ­¥å®šä¹‰ä¸ºåœ¨å·¦å€¾çš„Redditè¯­æ–™åº“ä¸Šè¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒåï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å…¶æ”¿æ²»å€¾å‘æ–¹é¢çœ‹åˆ°ä¸€ä¸ªæ˜¾è‘—çš„å‘å·¦ç§»åŠ¨ã€‚</sample>
    <sample id="179">The image shows a comparison of political leanings between two models: RoBERTa and GPT-2. The left side represents the original news, while the right side shows the shift in political leaning after processing by each model.

For RoBERTa:
- News left shifts to center (Î” = -275.124)
- News center remains mostly unchanged
- News right moves slightly to the left (Î” = 163.103)

For GPT-2:
- News left shifts significantly to the right (Î” = -237.051)
- News center stays relatively stable with minor changes (Î” = -0.128.128)
- News right also shifts more towards the right (Î” = -213.006)

Overall, both models show significant shifts from their original positions, but they differ in how they process different types of news content politically.</sample>
    <sample id="180">æ ‡é¢˜æ˜¯ã€ŠThe Trump Cardã€‹ã€‚</sample>
    <sample id="181">The image shows a slide titled 'The Trump Card' with the subtitle 'Pre-45th to post-45th shift'. It contains eight graphs, each representing different categories such as news left, news center, news right, reddit left, reddit center, and reddit right. Each graph has an arrow indicating a shift in values (Î”) from pre-45th president of the United States to post-45th president of the United States. The Î” values range between positive and negative numbers, showing changes in language model performance on two different temporal corpora: GPT-2 and RoBERTa.</sample>
    <sample id="182">The image shows a presentation slide titled 'The Trump Card' with the subtitle 'Pre-45th to post-45th shift.' It contains eight graphs arranged in two rows and four columns, each representing different categories: news left, news center, news right, reddit center, and reddit right. Each graph displays a delta value (Î”) indicating changes over time. The deltas are shown as numerical values within parentheses, such as Î”=(275.124), Î”=(-0.131.103), Î”=(1.631.03), etc. Additionally, there is an inset on the top-right corner of the slide showing a person presenting or discussing the content.</sample>
    <sample id="183">å¥½çš„ï¼Œæˆ‘ç†è§£äº†ã€‚æ‚¨èƒ½å†è¯´ä¸€éå—ï¼Ÿ</sample>
    <sample id="184">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚ So we see that if we investigate the per-category performance, that is to say, if we separate the performance into different categories,</sample>
    <sample id="185">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="186">æ£€æµ‹é’ˆå¯¹å°‘æ•°ç¾¤ä½“çš„ä»‡æ¨è¨€è®º</sample>
    <sample id="187">è¡¨æ ¼å±•ç¤ºäº†é’ˆå¯¹ä¸åŒèº«ä»½ç¾¤ä½“çš„ä»‡æ¨è¨€è®ºå’Œæ¥è‡ªä¸åŒæ¥æºçš„è¯¯å¯¼æ€§ä¿¡æ¯çš„æ€§èƒ½è¡¨ç°ã€‚é¢œè‰²ç¼–ç å¦‚ä¸‹ï¼šæ·±é»„è‰²è¡¨ç¤ºæœ€ä½³ï¼Œè“è‰²è¡¨ç¤ºæœ€å·®ã€‚</sample>
    <sample id="188">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="189">è¡¨æ ¼å±•ç¤ºäº†é’ˆå¯¹ä¸åŒèº«ä»½ç¾¤ä½“çš„ä»‡æ¨è¨€è®ºå’Œæ¥è‡ªä¸åŒæ¥æºçš„è¯¯å¯¼ä¿¡æ¯çš„æ€§èƒ½è¡¨ç°ã€‚é¢œè‰²ç¼–ç æ˜¾ç¤ºï¼Œæ·±é»„è‰²è¡¨ç¤ºæœ€ä½³æ€§èƒ½ï¼Œè€Œæ·±è“è‰²è¡¨ç¤ºæœ€å·®æ€§èƒ½ã€‚</sample>
    <sample id="190">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="191">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="192">è¿™è¡¨æ˜åœ¨è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰åè§é—®é¢˜ä¸Šå­˜åœ¨ä¸€ä¸ªéå¸¸ç´§è¿«çš„å…¬å¹³æ€§é—®é¢˜ã€‚</sample>
    <sample id="193">For example, if a right-leaning language models were to be fine-tuned on hate speech or misinformation and deployed to a popular social media platform,</sample>
    <sample id="194">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªå…³äºä»‡æ¨è¨€è®ºçš„è¡¨æ ¼ã€‚è¡¨æ ¼åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œå·¦è¾¹æ˜¯â€œä»‡æ¨è¨€è®ºæ–‡æœ¬â€ï¼Œå³è¾¹æ˜¯â€œä»‡æ¨è¨€è®ºæ–‡æœ¬â€ã€‚æ¯éƒ¨åˆ†éƒ½æœ‰å¤šä¸ªæ¡ç›®ï¼Œæ¯ä¸ªæ¡ç›®åŒ…å«ä¸€æ®µæ–‡æœ¬å’Œå‡ ä¸ªæ ‡è®°ï¼ˆTRUEã€FALSEï¼‰ã€‚åœ¨è¡¨æ ¼ä¸‹æ–¹ï¼Œæœ‰ä¸€ä¸ªæ ‡é¢˜ä¸ºâ€œç¬¬12èŠ‚ï¼šä»‡æ¨è¨€è®ºç¤ºä¾‹çš„å®šæ€§åˆ†æï¼Œå…¶ä¸­MDsä¸æ”¿æ²»å€¾å‘å·®å¼‚è¾ƒå¤§â€çš„æ³¨é‡Šã€‚</sample>
    <sample id="195">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="196">è®¨è®ºä¸­ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†åœ¨é¢„è®­ç»ƒæ•°æ®ã€è¯­è¨€æ¨¡å‹å’Œä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´è¿›è¡Œé€‰æ‹©çš„å›°å¢ƒã€‚</sample>
    <sample id="197">è¿™æ®µæ–‡å­—è®¨è®ºäº†åœ¨é¢„è®­ç»ƒæ•°æ®ã€è¯­è¨€æ¨¡å‹å’Œä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´è¿›è¡Œâ€œå‡€åŒ–â€æˆ–ä¸è¿›è¡Œâ€œå‡€åŒ–â€çš„é—®é¢˜ã€‚å®ƒæŒ‡å‡ºï¼Œå¦‚æœä¸å‡€åŒ–æ”¿æ²»æ„è§çš„é¢„è®­ç»ƒæ•°æ®ï¼Œåè§å°†ä»é¢„è®­ç»ƒæ•°æ®ä¼ æ’­åˆ°è¯­è¨€æ¨¡å‹ï¼Œæœ€ç»ˆå¯¼è‡´å…¬å¹³æ€§é—®é¢˜ã€‚</sample>
    <sample id="198">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†å…³äºè¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®çš„è®¨è®ºã€‚ä¸»è¦å†…å®¹åŒ…æ‹¬é¢„è®­ç»ƒæ•°æ®ã€è¯­è¨€æ¨¡å‹å’Œä¸‹æ¸¸ä»»åŠ¡ã€‚</sample>
    <sample id="199">æ”¶åˆ°è‹±æ–‡å†…å®¹åï¼Œç”¨ä¸­æ–‡è¡¨è¿°å…¶æ„æ€ã€‚</sample>
    <sample id="200">è¿™ç¯‡è®ºæ–‡æœ‰6ä½ä½œè€…ã€‚</sample>
    <sample id="201">MPPè¯„ä¼°æœ€å¤šæ¶µç›–900ä¸ªè¯å…ƒçš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚</sample>
    <sample id="202">ä»–ä»¬çš„æ•°æ®é›†ä¸­åŒ…å«éŸ³ä¹é€‰æ‹©ã€ä¹¦ç±é€‰æ‹©å’Œé£Ÿè°±é€‰æ‹©ã€‚</sample>
    <sample id="203">Positionality refers to the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">The speaker's name is Dawei Zhu.</sample>
    <sample id="205">EDAtt æ˜¯é€‚åº”äº†ç°æœ‰çš„ç¦»çº¿ ST æ¨¡å‹ã€‚</sample>
    <sample id="206">There are four authors.</sample>
    <sample id="207">Yes, the models can run on the test suite.</sample>
    <sample id="208">KITMUSæœ‰ä¸‰ä¸ªå˜ä½“ï¼ša) èƒŒæ™¯é¢„è®­ç»ƒï¼Œb) èƒŒæ™¯ä¸¤è€…ï¼Œc) èƒŒæ™¯æ¨æ–­ã€‚</sample>
    <sample id="209">Google Research</sample>
    <sample id="210">æœ€åä¸€ä¸ªç ”ç©¶é—®é¢˜æ˜¯ï¼šå¦‚ä½•æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¯ç”¨çš„å¹²å‡€æ ·æœ¬ï¼Ÿ</sample>
    <sample id="211">æŒ‡æ ‡çµæ•åº¦æ˜¯é€šè¿‡è®¡ç®—æ¨¡å‹å¯¹åŒä¸€ä»»åŠ¡çš„å¤šç§æŒ‡ä»¤å˜åŒ–çš„æ•æ„Ÿç¨‹åº¦æ¥å·¥ä½œçš„ã€‚å®ƒè¯„ä¼°äº†æ¨¡å‹åœ¨æŒ‡ä»¤æªè¾ç•¥æœ‰å˜åŒ–æ—¶ï¼Œæ˜¯å¦èƒ½å¤ŸæŒç»­äº§ç”Ÿç›¸åŒçš„ç»“æœã€‚</sample>
    <sample id="212">The speaker's name is Jingwei Yi.</sample>
    <sample id="213">æ›´é«˜çš„çµæ•åº¦è¡¨ç¤ºæ¨¡å‹æ€§èƒ½å¾—åˆ°äº†æé«˜ã€‚</sample>
    <sample id="214">æ¨¡å‹åœ¨é¢„è®­ç»ƒæœŸé—´ä¼šæ¥æ”¶å¤§é‡çš„è¯­è¨€ä¸Šä¸‹æ–‡ã€‚</sample>
    <sample id="215">To determine how many clean validation samples are needed for good performance in WSL, we need to look at the graph labeled "Main findings" and analyze its trends. The x-axis represents different numbers of clean validation samples per class (ranging from 5 to over 40), while the y-axis shows accuracy percentages.

From observing the trend lines on the graph:
- For the method represented by blue circles ("FTw"), it appears that an increase in the number of clean validation samples leads to a higher accuracy rate.
- Specifically, when there are around 20 clean validation samples per class, this seems to yield high performance as indicated by the steep upward slope near the beginning of the curve.

Therefore, based on the visual data presented in the graph, typically you only need about 20 clean validation samples per class to attain high performance in WSL.</sample>
    <sample id="216">The authors of this paper belong to Stanford Engineering, specifically the Computer Science department.</sample>
    <sample id="217">éœ€è¦å¼€å‘æ–°çš„æ–¹æ³•æ¥è¡¡é‡åª’ä½“åè§ï¼Œå› ä¸ºç°æœ‰çš„è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰å…·æœ‰ä¸åŒçš„æ”¿æ²»å€¾å‘ã€‚</sample>
    <sample id="218">æ¼”è®²è€…çš„åå­—æ˜¯Akshata Aluriã€‚</sample>
    <sample id="219">æ”¿æ²»åè§ä¼ æ’­æµç¨‹æ˜¯æ€æ ·çš„ï¼Ÿ</sample>
    <sample id="220">Yes, DEplain-apa å’Œç½‘ç«™çš„ç®€åŒ–è¿‡ç¨‹æœ‰æ‰€ä¸åŒã€‚</sample>
    <sample id="221">No, Coscript is not publicly available.</sample>
    <sample id="222">æ°´å°é€šè¿‡å°†ç›®æ ‡åµŒå…¥ä¸åŸå§‹åµŒå…¥çš„åŠ æƒå’Œç›¸ç»“åˆï¼Œä»¥ç‰¹å®šæ–¹å¼æ’å…¥æ–‡æœ¬ä¸­ã€‚</sample>
    <sample id="223">The Penn State University and Amazon.</sample>
    <sample id="224">Yes, like MT5, the encoder-decoder model can be improved by training in a mixture of various languages.</sample>
    <sample id="225">å—é™è¯­è¨€è§„åˆ’çš„ä¸€ä¸ªç¤ºä¾‹æ˜¯ï¼Œå¦‚ä½•åˆ¶ä½œè‰è“è›‹ç³•å’Œå·§å…‹åŠ›è›‹ç³•ã€‚</sample>
    <sample id="226">They ensure the method's covertness by visualizing the embeddings of sentences on four datasets via PCA.</sample>
    <sample id="227">The question is asking about the research on using existing PLM to build new PLM. The answer can be found in the first bullet point under "Comparison of learning strategies". It states, "Research how to use existing PLM to construct new PLM."</sample>
    <sample id="228">African Islamic</sample>
    <sample id="229">The speaker demonstrated how the model uses attention mechanisms to learn from existing offline ST models without retraining or adopting a specific architecture for SimuIST.</sample>
    <sample id="230">ä»»åŠ¡çš„æ•°é‡å½±å“æ¨¡å‹æ€§èƒ½ï¼Œå¦‚å›¾æ‰€ç¤ºã€‚éšç€ä»»åŠ¡æ•°é‡çš„å¢åŠ ï¼Œæ¨¡å‹çš„æ€§èƒ½åœ¨æŸäº›æƒ…å†µä¸‹ä¼šæé«˜ï¼ˆä¾‹å¦‚ï¼Œåœ¨â€œGroundingâ€å’Œâ€œImg Undâ€ä»»åŠ¡ä¸­ï¼‰ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹åˆ™ä¼šä¸‹é™ï¼ˆä¾‹å¦‚ï¼Œåœ¨â€œRelationâ€å’Œâ€œNLPâ€ä»»åŠ¡ä¸­ï¼‰ã€‚è¿™è¡¨æ˜æ¨¡å‹å¯¹ä»»åŠ¡å¤æ‚æ€§çš„å¤„ç†èƒ½åŠ›å­˜åœ¨å·®å¼‚ï¼Œå¢åŠ äº†ä»»åŠ¡å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½æ³¢åŠ¨ã€‚</sample>
    <sample id="231">The three treeless baselines used for comparison are: 1. LSTM seq2seq 2. TS (Tree Structure) 3. Zheng and Lapata</sample>
    <sample id="232">The two co-authors, Alexander Koller and Ivan Titov, are the advisors of the first author, Matthias Lindemann. This is a joint work with their supervisors at Saarland University and UCA (University of Applied Sciences in Angers), respectively.</sample>
    <sample id="233">PaLMçš„é¦–ç¯‡è®ºæ–‡æ˜¯ç”±Chowderyç­‰äººåœ¨2022å¹´å‘è¡¨çš„ã€‚</sample>
    <sample id="234">å¹»ç¯ç‰‡ä¸Šæ˜¾ç¤ºäº†äº”ä¸ªäººçš„åå­—å’Œå¤´åƒï¼Œåˆ†åˆ«æ˜¯Sebastian Santyã€Jenny T. Liangã€Ronan Le Brasã€Katharina Reineckeå’ŒMaarten Sapã€‚</sample>
    <sample id="235">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œNLP Positionality: Characterizing Design Biases of Datasets and Modelsâ€ã€‚</sample>
    <sample id="236">Imagine...</sample>
    <sample id="237">Imagine... Carl Jones, Tech Lead, New York Times. Can you stop being a jerk? ğŸ˜¡ (0.82) âœ…</sample>
    <sample id="238">ç”»é¢ä¸­å±•ç¤ºäº†ä¸¤ä¸ªäººç‰©ï¼Œå·¦è¾¹æ˜¯Carl Jonesï¼Œå³è¾¹æ˜¯Aditya Sharmaã€‚Carl Jonesçš„å¤´åƒä¸Šæ–¹æœ‰ä¸€ä¸ªå¯¹è¯æ¡†ï¼Œé‡Œé¢å†™ç€â€œCan you stop being a jerk?â€ï¼Œå¹¶é™„æœ‰ä¸€ä¸ªè¯„åˆ†ï¼ˆ0.82ï¼‰å’Œä¸€ä¸ªç»¿è‰²å¯¹å‹¾ã€‚å³è¾¹çš„å¯¹è¯æ¡†æ˜¾ç¤ºäº†ä¸€ä¸ªè¯„åˆ†ï¼ˆ0.33ï¼‰å’Œä¸€ä¸ªçº¢è‰²å‰å·ã€‚åœ¨å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£ï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªäººã€‚å·¦ä¸‹è§’æœ‰ä¸€ä¸ªç¬¦å·ï¼Œè¡¨ç¤ºPerspectiveAPIå¾—åˆ†ã€‚</sample>
    <sample id="239">è¿™æ˜¯ä¸€æ¬¡è®¾è®¡åè§çš„ä¾‹å­ï¼Œæˆ‘ä»¬çœ‹åˆ°æŠ€æœ¯åœ¨ä¸åŒç¾¤ä½“ä¹‹é—´çš„ç³»ç»Ÿæ€§èƒ½å·®å¼‚ã€‚</sample>
    <sample id="240">å¹»ç¯ç‰‡åŒ…å«ä¸€ä¸ªæ ‡é¢˜â€œPositionalityâ€ï¼Œä»¥åŠä¸€ä¸ªå¼•ç”¨ï¼šâ€œThe perspectives [people] hold as a result of their demographics, identity, and life experiences.â€ è¿™ä¸ªå¼•ç”¨è¢«æ‹¬å·åŒ…å›´ã€‚åœ¨åº•éƒ¨ï¼Œæœ‰ä¸€ä¸ªå‚è€ƒæ–‡çŒ®ï¼š[1] Savin-Baden, Maggi, and Claire Howell-Major. "Qualitative research: The essential guide to theory and practice." Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013). å³ä¸Šè§’æ˜¾ç¤ºäº†ä¸€ä¸ªå°çš„è§†é¢‘çª—å£ï¼Œæ˜¾ç¤ºä¸€ä¸ªäººååœ¨ä¹¦æ¶å‰ã€‚</sample>
    <sample id="241">è¿™ä¸ªæ¦‚å¿µåœ¨æ‰¹åˆ¤æ€§ç ”ç©¶ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥³æƒä¸»ä¹‰å’Œé…·å„¿å­¦æœ¯é¢†åŸŸã€‚</sample>
    <sample id="242">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œPositionalityâ€ã€‚å†…å®¹è§£é‡Šäº†ä½ç½®æ€§å¦‚ä½•å½±å“ç ”ç©¶äººå‘˜ã€‚å®ƒæŒ‡å‡ºï¼Œäººä»¬ç”±äºå…¶äººå£ç»Ÿè®¡å­¦ã€èº«ä»½å’Œç”Ÿæ´»ç»å†è€ŒæŒæœ‰çš„è§†è§’ä¼šå½±å“ç ”ç©¶è¿‡ç¨‹åŠå…¶ç»“æœå’Œç»“æœã€‚å‚è€ƒæ–‡çŒ®åˆ—åœ¨åº•éƒ¨ï¼Œå¼•ç”¨äº†Savin-Baden, Maggiå’ŒClaire Howell-Majorçš„ã€ŠQualitative research: The essential guide to theory and practiceã€‹ã€‚è¯¥ä¹¦ç”±Routledgeäº2013å¹´å‡ºç‰ˆã€‚</sample>
    <sample id="243">å›¾ç‰‡ä¸­çš„è‹±æ–‡å†…å®¹æ˜¯ï¼š Do datasets and models have positionality? [1] Blasi, et al. â€œSystematic Inequalities in Language Technology Performance across the Worldâ€™s Languages.â€ ACL 2022. [2] Ye et al. â€œGEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models.â€ EMNLP 2022. [3] Cambo &amp; Gergle. â€œModel Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.â€ CHI 2022.</sample>
    <sample id="244">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ä¸€ä¸ªæ ‡é¢˜å’Œä¸‰ç¯‡å‚è€ƒæ–‡çŒ®ã€‚æ ‡é¢˜ä¸ºâ€œDo datasets and models have positionality?â€ï¼Œæ„ä¸ºâ€œæ•°æ®é›†å’Œæ¨¡å‹æ˜¯å¦æœ‰ä½ç½®æ€§ï¼Ÿâ€ ä¸‰ç¯‡å‚è€ƒæ–‡çŒ®åˆ†åˆ«æ˜¯ï¼š 1. [1] Blasi, et al. â€œSystematic Inequalities in Language Technology Performance across the Worldâ€™s Languages.â€ ACL 2022. 2. [2] Ye et al. â€œGEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models.â€ EMNLP 2022. 3. [3] Cambo &amp; Gergle. â€œModel Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.â€ CHI 2022.</sample>
    <sample id="245">The text in the image is about datasets and models having positionality. It provides anecdotal evidence, model and dataset probing, theoretical definitions of model positionality, cultural gaps between models and datasets, as well as rare examples of model positionality.</sample>
    <sample id="246">è¿™æ®µæ–‡å­—è®¨è®ºäº†æ•°æ®é›†å’Œæ¨¡å‹æ˜¯å¦å…·æœ‰å®šä½æ€§çš„é—®é¢˜ã€‚å®ƒæåˆ°äº†ä¸€äº›ç›¸å…³çš„ç ”ç©¶å·¥ä½œï¼ŒåŒ…æ‹¬ï¼š 1. Blasiç­‰äººåœ¨ACL 2022ä¸Šå‘è¡¨çš„ã€Šå…¨çƒè¯­è¨€æŠ€æœ¯æ€§èƒ½ç³»ç»Ÿæ€§ä¸å¹³ç­‰ã€‹ã€‚ 2. Yeç­‰äººåœ¨EMNLP 2022ä¸Šå‘è¡¨çš„ã€Šå¤šè¯­ç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­çš„åœ°ç†å…±è¯†æ¢é’ˆã€‹ã€‚ 3. Camboå’ŒGergleåœ¨CHI 2022ä¸Šå‘è¡¨çš„ã€Šä¿ƒè¿›æ•°æ®ç§‘å­¦ä¸­åå°„æ€§çš„æ¨¡å‹ä½ç½®æ€§å’Œè®¡ç®—åå°„æ€§ã€‹ã€‚ è¿™äº›å‚è€ƒæ–‡çŒ®è¡¨æ˜ï¼Œè™½ç„¶æœ‰ä¸€äº›å…³äºæ¨¡å‹å’Œæ•°æ®é›†æ¢é’ˆä»¥åŠæ¨¡å‹å®šä½æ€§ç†è®ºå®šä¹‰çš„ç ”ç©¶ï¼Œä½†å®ƒä»¬å¹¶æ²¡æœ‰æ¯”è¾ƒæœ€ç»ˆç”¨æˆ·ä¸æ•°æ®é›†å’Œæ¨¡å‹æœ¬èº«ã€‚</sample>
    <sample id="247">è¿™æ®µæ–‡å­—è®¨è®ºäº†æ•°æ®é›†å’Œæ¨¡å‹çš„â€œä½ç½®æ€§â€æ¦‚å¿µã€‚å®ƒæåˆ°äº†ä¸€äº›å…³äºè¿™ä¸ªä¸»é¢˜çš„è½¶äº‹è¯æ®ï¼ŒåŒ…æ‹¬â€œæ¨¡å‹å’Œæ•°æ®é›†æ¢æµ‹â€å’Œâ€œæ¨¡å‹ä½ç½®æ€§çš„ç†è®ºå®šä¹‰â€ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¼•ç”¨äº†å‡ ç¯‡ç›¸å…³è®ºæ–‡ï¼š 1. Blasiç­‰äººåœ¨ACL 2022ä¸Šå‘è¡¨çš„ã€Šä¸–ç•Œè¯­è¨€æŠ€æœ¯ä¸å¹³ç­‰ç³»ç»Ÿæ€§å·®å¼‚ã€‹ã€‚ 2. Yeç­‰äººåœ¨EMNLP 2022ä¸Šå‘è¡¨çš„ã€Šå¤šè¯­ç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­çš„åœ°ç†æ³›åŒ–æ¢æµ‹ã€‹ã€‚ 3. Camboå’ŒGergleåœ¨CHI 2022ä¸Šå‘è¡¨çš„ã€Šæ•°æ®ç§‘å­¦ä¸­çš„æ¨¡å‹ä½ç½®æ€§å’Œè®¡ç®—åå°„æ€§ï¼šä¿ƒè¿›åæ€æ€§ã€‹ã€‚ è¿™äº›å‚è€ƒæ–‡çŒ®è¡¨æ˜ï¼Œç†è§£æ•°æ®é›†å’Œæ¨¡å‹çš„ä½ç½®æ€§è¶Šæ¥è¶Šé‡è¦ï¼Œå› ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡å˜å¾—æ›´åŠ ä¸»è§‚å’Œç¤¾ä¼šå¯¼å‘ã€‚</sample>
    <sample id="248">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š

æ ‡é¢˜ï¼šDo datasets and models have positionality?

å‰¯æ ‡é¢˜ï¼šAnecdotal evidence:

æ­£æ–‡ï¼š
- Model and dataset probing [1][2]
- Theoretical definitions of model positionality [3]

å‚è€ƒæ–‡çŒ®ï¼š
[1] Blasi, et al. â€œSystematic Inequalities in Language Technology Performance across the Worldâ€™s Languages.â€ ACL 2022.
[2] Ye et al., â€œGEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.â€ EMNLP 2022.
[3] Cambo &amp; Gergle, â€œModel Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.â€ CHI 2022.

å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çš„è§†é¢‘çª—å£ï¼Œæ˜¾ç¤ºä¸€ä¸ªäººåœ¨è®²è¯ã€‚</sample>
    <sample id="249">ç”»é¢ä¸­å±•ç¤ºäº†ä¸€ä¸ªé—®é¢˜ï¼Œè¯¢é—®æ•°æ®é›†å’Œæ¨¡å‹æ˜¯å¦å…·æœ‰ä½ç½®æ€§ã€‚ç”»é¢ä¸Šæ–¹æœ‰ä¸€ä¸ªå¤§å¤§çš„é—®å·ï¼Œä¸‹æ–¹æœ‰ä¸€æ®µæ–‡å­—è¯´æ˜äº†æ¯”è¾ƒç”¨æˆ·æ³¨é‡Šä¸ç°æœ‰æ•°æ®é›†å’Œæ¨¡å‹çš„ç›®æ ‡ã€‚</sample>
    <sample id="250">NLPPositionality A framework for characterizing design biases in NLP datasets and models</sample>
    <sample id="251">æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ­¥éª¤ã€‚</sample>
    <sample id="252">æ¡†æ¶çš„ç¬¬ä¸€æ­¥æ˜¯ç”¨å¤šæ ·çš„æ³¨é‡Šå‘˜é‡æ–°æ³¨é‡Šæ•°æ®é›†ã€‚</sample>
    <sample id="253">æ¡†æ¶ï¼š1) ç”¨å¤šæ ·åŒ–çš„æ³¨é‡Šå‘˜é‡æ–°æ ‡æ³¨æ•°æ®é›†</sample>
    <sample id="254">æ¡†æ¶ 1) ä½¿ç”¨å¤šæ ·åŒ–çš„æ³¨é‡Šå‘˜é‡æ–°æ³¨é‡Šæ•°æ®é›†</sample>
    <sample id="255">æ¡†æ¶ 2) æ¯”è¾ƒæ³¨é‡Šä¸æŒ‰äººå£ç»Ÿè®¡ä¿¡æ¯å¯¹æ¨¡å‹å’Œæ•°æ®é›†çš„æ¯”è¾ƒï¼Œä½¿ç”¨çš®å°”é€Šçš„ç›¸å…³æ€§å¾—åˆ†ã€‚</sample>
    <sample id="256">æ¡†æ¶çš„è‹±æ–‡å†…å®¹æ˜¯ï¼š'2) Compare annotations by demographic to models and datasets via Pearson's R scores.'</sample>
    <sample id="257">Lab in the Wild æ˜¯ä¸€ä¸ªåœ¨çº¿ä¼—åŒ…å¹³å°ï¼Œç”¨äºä»è¿œç¨‹åˆä½œè€…é‚£é‡Œæ”¶é›†æ•°æ®ã€‚</sample>
    <sample id="258">Lab in the Wild is an online experimentation platform where we can recruit diverse volunteers compared to platforms like MTurk, which largely have participants from the US or India.</sample>
    <sample id="259">ä»»åŠ¡Aï¼šç¤¾ä¼šå¯æ¥å—æ€§ 1. é˜…è¯»æƒ…å†µã€‚ 2. å¯¹ä½ æ¥è¯´ï¼Œè¿™ä»£è¡¨ä»€ä¹ˆï¼Ÿ - ä»ä¸€ä¸ªä¸ªäººçš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªç¤¾ä¼šçš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªæŠ€æœ¯çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªç»æµçš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªé“å¾·çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªæ³•å¾‹çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªç¯å¢ƒçš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªå¥åº·çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªæ•™è‚²çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªæ–‡åŒ–çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªæ”¿æ²»çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªå†å²çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªå“²å­¦çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªå®—æ•™çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªå¨±ä¹çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªæ—¶å°šçš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªè‰ºæœ¯çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªä½“è‚²çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªç¾é£Ÿçš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªæ—…è¡Œçš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªç§‘æŠ€çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªæ¸¸æˆçš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªé‡‘èçš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªå•†ä¸šçš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªç¤¾ä¼šçš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªå®¶åº­çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªä¸ªäººçš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªæœªæ¥çš„è§’åº¦è€ƒè™‘ã€‚ - ä»ä¸€ä¸ªæœªçŸ¥çš„è§’åº¦è€ƒè™‘ã€‚ - å…¶ä»–ã€‚ 3. çœ‹çœ‹AIå’Œå…¶ä»–äººå¯¹å®ƒæœ‰ä»€ä¹ˆçœ‹æ³•ï¼ - AIæ¨æµ‹ï¼šè¿™æ˜¯å¦å¯ä»¥ç†è§£ï¼Ÿ - AIæ¨æµ‹ï¼šè¿™æ˜¯å¦å¯æ¥å—ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦åŒæ„ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦åå¯¹ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ä¸­ç«‹ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ¨¡ç³Šï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ¸…æ™°ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ç®€å•ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦å¤æ‚ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰è¶£ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— èŠï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰ç”¨ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— ç”¨ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦é‡è¦ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ä¸é‡è¦ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦å¿…è¦ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ä¸å¿…è¦ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦å€¼å¾—ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ä¸å€¼å¾—ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦å…¬å¹³ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ä¸å…¬å¹³ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦å…¬æ­£ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ä¸å…¬æ­£ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦é€æ˜ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ä¸é€æ˜ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦å¼€æ”¾ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦å°é—­ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦åˆ›æ–°ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ä¸åˆ›æ–°ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ä¼ ç»Ÿï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦éä¼ ç»Ÿï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ç°ä»£ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦è¿‡æ—¶ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ—¶å°šï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦è¿‡æ—¶ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æµè¡Œï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ä¸æµè¡Œï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦å¥åº·ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ä¸å¥åº·ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦å®‰å…¨ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦ä¸å®‰å…¨ï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æœ‰æ•ˆï¼Ÿ - AIæ¨æµ‹ï¼šAIæ˜¯å¦æ— æ•ˆï¼Ÿ -</sample>
    <sample id="260">ä»»åŠ¡Aï¼šç¤¾ä¼šæ¥å—åº¦ 1. é˜…è¯»æƒ…å†µã€‚ 2. è®¨è®ºä½ çš„çœ‹æ³•ã€‚ 3. çœ‹çœ‹å…¶ä»–äººå’ŒAIä»¬æ˜¯æ€ä¹ˆæƒ³çš„ã€‚ å‚ä¸è€…å¯ä»¥å°†è‡ªå·±çš„ååº”ä¸ä»–äººå’ŒAIä»¬çš„ååº”è¿›è¡Œæ¯”è¾ƒã€‚</sample>
    <sample id="261">ä»»åŠ¡Aï¼šç¤¾ä¼šå¯æ¥å—æ€§åˆ†ææ•°æ®é›† - ç¤¾ä¼šåŒ–å­¦æ¨¡å‹ - å¾·å°”è² - GPT-4</sample>
    <sample id="262">ä»»åŠ¡Bï¼šæ¯’æ€§</sample>
    <sample id="263">ç”»é¢ä¸­æ˜¾ç¤ºäº†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œStudy Participationâ€ã€‚å¹»ç¯ç‰‡ä¸Šåˆ—å‡ºäº†ä¸‰ä¸ªæ•°å­—å’Œç›¸åº”çš„æ ‡ç­¾ã€‚ç¬¬ä¸€ä¸ªæ•°å­—æ˜¯â€œ16,299â€ï¼Œæ ‡ç­¾æ˜¯â€œannotationsâ€ï¼ˆæ³¨é‡Šï¼‰ã€‚ç¬¬äºŒä¸ªæ•°å­—æ˜¯â€œ1,096â€ï¼Œæ ‡ç­¾æ˜¯â€œannotatorsâ€ï¼ˆæ³¨é‡Šè€…ï¼‰ã€‚ç¬¬ä¸‰ä¸ªæ•°å­—æ˜¯â€œ87â€ï¼Œæ ‡ç­¾æ˜¯â€œcountriesâ€ï¼ˆå›½å®¶ï¼‰ã€‚åœ¨å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£ï¼Œæ˜¾ç¤ºä¸€ä¸ªäººåœ¨è®²è¯æˆ–æ¼”ç¤ºã€‚èƒŒæ™¯æ˜¯ä¸€ä¸ªä¹¦æ¶ï¼Œä¸Šé¢æ”¾ç€ä¸€äº›ä¹¦ç±å’Œå…¶ä»–ç‰©å“ã€‚</sample>
    <sample id="264">The first slide presents a question about the alignment of NLP datasets and models, asking 'Who do NLP datasets and models align with?' The second slide introduces the findings from this analysis. It states that there is positionality in NLP (Natural Language Processing).</sample>
    <sample id="265">Datasets and models are most aligned to English-Speaking countries.</sample>
    <sample id="266">å›¾è¡¨æ˜¾ç¤ºäº†ä¸åŒæ•™è‚²æ°´å¹³åœ¨ç¤¾ä¼šæ¥å—åº¦ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æŸ±çŠ¶å›¾ä¸­ï¼Œæ¯ä¸ªæ•™è‚²æ°´å¹³éƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„æŸ±å­ï¼ŒæŸ±å­çš„é«˜åº¦ä»£è¡¨äº†ç¤¾ä¼šæ¥å—åº¦çš„å€¼ã€‚æŸ±å­çš„é¢œè‰²ä»æ·±åˆ°æµ…ä¾æ¬¡ä¸ºè“è‰²ã€ç»¿è‰²ã€ç°è‰²å’Œæµ…ç°è‰²ï¼Œå¯¹åº”ç€ä¸åŒçš„æ•™è‚²æ°´å¹³ï¼šå¤§å­¦ã€ç ”ç©¶ç”Ÿé™¢ã€é«˜ä¸­ã€åšå£«åã€ä¸­ç­‰æ•™è‚²å‰å’Œæ•™æˆå­¦æ ¡ã€‚æ¯ä¸ªæŸ±å­ä¸Šéƒ½æ ‡æœ‰æ ·æœ¬æ•°é‡ï¼ˆNï¼‰å’Œç›¸åº”çš„ç¤¾ä¼šæ¥å—åº¦å€¼ã€‚ä¾‹å¦‚ï¼Œå¤§å­¦æ•™è‚²çš„æ ·æœ¬æ•°é‡ä¸º4,489ï¼Œç¤¾ä¼šæ¥å—åº¦å€¼ä¸º0.69*ï¼›ç ”ç©¶ç”Ÿé™¢æ•™è‚²çš„æ ·æœ¬æ•°é‡ä¸º1,116ï¼Œç¤¾ä¼šæ¥å—åº¦å€¼ä¹Ÿä¸º0.69*ã€‚å›¾è¡¨å·¦ä¾§çš„æ–‡å­—è¯´æ˜â€œæ•°æ®é›†å’Œæ¨¡å‹æœ€ä¸æ‹¥æœ‰å¤§å­¦æ•™è‚²çš„äººå¯¹é½â€ã€‚</sample>
    <sample id="267">æ•°æ®é›†å’Œæ¨¡å‹æœ€ä¸æ‹¥æœ‰å¤§å­¦å­¦å†çš„äººç¾¤å¯¹é½ã€‚</sample>
    <sample id="268">å‘ç°2ï¼šæœ‰äº›äººç¾¤è¢«è½ä¸‹ã€‚</sample>
    <sample id="269">å›¾è¡¨æ˜¾ç¤ºäº†ä¸‰ä¸ªç±»åˆ«ï¼šç”·æ€§ã€éäºŒå…ƒæ€§åˆ«å’Œå¥³æ€§ã€‚æ¯ä¸ªç±»åˆ«éƒ½æœ‰ä¸€ä¸ªæŸ±çŠ¶å›¾ï¼Œè¡¨ç¤ºç¤¾ä¼šå¯æ¥å—åº¦çš„åˆ†æ•°ã€‚å¯¹äºç”·æ€§ï¼Œç¤¾ä¼šå¯æ¥å—åº¦ä¸º0.69ï¼Œæ ·æœ¬å¤§å°ä¸º4,082ã€‚å¯¹äºéäºŒå…ƒæ€§åˆ«ï¼Œç¤¾ä¼šå¯æ¥å—åº¦ä¸º0.55ï¼Œæ ·æœ¬å¤§å°ä¸º858ã€‚å¯¹äºå¥³æ€§ï¼Œç¤¾ä¼šå¯æ¥å—åº¦ä¸º0.73ï¼Œæ ·æœ¬å¤§å°ä¸º4,368ã€‚å›¾è¡¨çš„æ ‡é¢˜æ˜¯â€œGPT-4çš„ç¤¾ä¼šå¯æ¥å—æ€§â€ï¼Œå·¦ä¾§çš„æ–‡å­—è¯´æ˜æ˜¯â€œæ•°æ®é›†å’Œæ¨¡å‹ä¸éäºŒå…ƒæ€§åˆ«çš„äººä¸å¤ªåŒ¹é…â€ã€‚</sample>
    <sample id="270">é‚£ä¹ˆï¼Œæˆ‘ä»¬å¯ä»¥åšä»€ä¹ˆï¼Ÿåœ¨NLPä¸­è§£å†³ä½ç½®æ€§é—®é¢˜ã€‚</sample>
    <sample id="271">æ¨è 1. åœ¨æ•´ä¸ªæ„å»ºæ•°æ®é›†æˆ–æ¨¡å‹çš„è¿‡ç¨‹ä¸­ï¼Œè®°å½•æ‰€æœ‰ç›¸å…³çš„è®¾è®¡é€‰æ‹©ã€‚ æ¨è 2. é€šè¿‡é€è§†ä¸»ä¹‰çš„è§†è§’è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶ï¼š a. å…±äº«æ‹†åˆ†çš„æ ‡ç­¾æ•°æ®é›†ï¼</sample>
    <sample id="272">æ¨èå†…å®¹åŒ…æ‹¬ï¼š1. è®°å½•åœ¨æ„å»ºæ•°æ®é›†æˆ–æ¨¡å‹æ—¶åšå‡ºçš„æ‰€æœ‰ç›¸å…³è®¾è®¡é€‰æ‹©ã€‚2. é€šè¿‡è§†è§’ä¸»ä¹‰è¿›è¡ŒNLPç ”ç©¶ï¼ša. åˆ†äº«åˆ†æ®µæ•°æ®é›†æ ‡ç­¾ï¼b. ä½¿ç”¨èƒ½å¤Ÿå¤„ç†æ³¨é‡Šå‘˜åˆ†æ­§çš„å»ºæ¨¡æŠ€æœ¯ã€‚3. æ„å»ºé’ˆå¯¹ç‰¹å®šç¤¾åŒºçš„ä¸“é—¨æ•°æ®é›†å’Œæ¨¡å‹æ˜¯æœ‰ä»·å€¼çš„ï¼Œä»¥ä¿ƒè¿›åŒ…å®¹æ€§NLPï¼ˆä¾‹å¦‚ï¼ŒMasakhaneå€¡è®®ï¼‰ï¼</sample>
    <sample id="273">å¹»ç¯ç‰‡ä¸Šæ˜¾ç¤ºäº†â€œè°¢è°¢ï¼â€å­—æ ·ã€‚åœ¨é¡¶éƒ¨ï¼Œæœ‰ä¸€ä¸ªé“¾æ¥å’Œä¸€ä¸ªè®ºæ–‡çš„URLã€‚åœ¨ä¸­é—´ï¼Œæœ‰ä¸€ä¸ªDelphiçš„æ ‡å¿—ï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªç½‘å€â€œhttps://www.delfi.org/â€ã€‚å¹»ç¯ç‰‡çš„ä¸‹åŠéƒ¨åˆ†å±•ç¤ºäº†å…­ä¸ªå›¾è¡¨ï¼Œåˆ†åˆ«ä»£è¡¨å¹´é¾„ã€æ€§åˆ«ã€æ—è£”ã€å®—æ•™ã€æ•™è‚²æ°´å¹³ã€å±…ä½å›½å®¶ã€æœ€é•¿å±…ä½å›½å®¶ä»¥åŠæ¯è¯­ã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°å›¾åƒï¼Œæ˜¾ç¤ºä¸€ä¸ªäººååœ¨æ¡Œå­æ—ï¼ŒèƒŒæ™¯ä¸­æœ‰ä¸€äº›ç‰©å“ã€‚</sample>
    <sample id="274">æ¼”è®²è€…æåˆ°äº† SimulST çš„å‡ ä¸ªé—®é¢˜ï¼š1. ç‰¹å®šçš„æ¶æ„é€šå¸¸è¢«è®­ç»ƒï¼Œå¼•å…¥é¢å¤–çš„æ¨¡å—ä»¥è¿›è¡Œä¼˜åŒ–ã€‚2. é•¿è€Œå¤æ‚çš„è®­ç»ƒç¨‹åºï¼ˆä¾‹å¦‚ï¼Œä¸åŒçš„ä¼˜åŒ–ç›®æ ‡ï¼‰ã€‚3. è®­ç»ƒå’Œç»´æŠ¤å¤šä¸ªæ¨¡å‹ä»¥è¾¾åˆ°ä¸åŒçš„å»¶è¿Ÿåˆ¶åº¦ï¼ˆä¾‹å¦‚ï¼Œ1ç§’ã€2ç§’ç­‰ï¼‰ã€‚</sample>
    <sample id="275">ä¸ºäº†å‡è½»æ•°æ®é›†ä¸­çš„ç¤¾ä¼šå’Œæ”¿æ²»åè§ï¼Œæœ‰æ•ˆçš„æ–¹æ³•æ˜¯å…ˆå¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚</sample>
    <sample id="276">The 61st Annual Meeting of the Association for Computational Linguistics, Toronto, Canada, July 9-14, 2023.</sample>
    <sample id="277">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œLanguage Planningâ€çš„æ¼”ç¤ºæ–‡ç¨¿ã€‚ä¸»è¦å†…å®¹æ˜¯ä¸€ä¸ªå…³äºå¦‚ä½•åˆ¶ä½œè›‹ç³•çš„é€æ­¥è¯´æ˜ï¼Œæ ‡é¢˜ä¸ºâ€œHow to Make a Cake?â€ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š 1. æ”¶é›†é£Ÿæã€‚ 2. é¢„çƒ­çƒ¤ç®±è‡³325Â°Fï¼ˆ163Â°Cï¼‰ï¼Œå¹¶åœ¨è›‹ç³•æ¨¡å…·ä¸Šæ¶‚æ²¹å’Œæ’’é¢ç²‰ã€‚ 3. å°†é»„æ²¹å’Œç³–æ…æ‹Œå‡åŒ€ã€‚ 4. æ·»åŠ é¸¡è›‹ã€‚ 5. å°†é¢ç²‰å’Œæ³¡æ‰“ç²‰æ··åˆååŠ å…¥ã€‚ 6. å°†é¢ç³Šå€’å…¥æ¨¡å…·ä¸­ã€‚ 7. çƒ˜çƒ¤15åˆ†é’Ÿã€‚ å¦å¤–ï¼Œå›¾ç‰‡ä¸­è¿˜æœ‰ä¸€æ®µæ–‡å­—è¯´æ˜ï¼šâ€œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥å°†ç›®æ ‡åˆ†è§£æˆæ­¥éª¤ã€‚â€ å›¾ç‰‡å³ä¾§æœ‰ä¸€ä¸ªæˆ´çœ¼é•œã€ç©¿ç»¿è‰²ä¸Šè¡£çš„äººï¼ŒèƒŒæ™¯çœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªåŠå…¬å®¤ç¯å¢ƒã€‚</sample>
    <sample id="278">Language Planning How to Make a Cake? 1. Gather your ingredients. 2. Preheat the oven to 325Â°F (163Â°C) and grease and flour a cake pan. 3. Cream the butter and sugar. 4. Add eggs. 5. Stir in the dry ingredients, alternating with the milk. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour 15 minutes. Large language models (LLMs) can effectively decompose goals into steps</sample>
    <sample id="279">Constrained Language Planning How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add the cocoa powder into the flour... Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints.</sample>
    <sample id="280">è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†çº¦æŸè¯­è¨€è§„åˆ’çš„é—®é¢˜ã€‚</sample>
    <sample id="281">The text in the image is about constrained language planning. It provides instructions on how to make a strawberry cake and a chocolate cake, with specific steps like adding strawberry jam or cocoa powder into flour. The abstract goal can be inherited by different real-life specific goals with multi-faceted constraints.</sample>
    <sample id="282">In this paper, we first evaluate and impose a constrained language planning ability of large language models.</sample>
    <sample id="283">The image contains a slide with the title "How do LLMs perform on Constrained Language Planning?" The dataset mentioned is "wikiHow + Generated Constraints." There are three types of constraints listed: 1. Constraint Type 1: Modifier - Defined as an adjective or phrase that modifies or constrains an abstract goal, with examples like "Make a chocolate cake" and "Make a pink cake." 2. Constraint Type 2: Method - Described as a tool or specified mode that controls the process for achieving the goal, illustrated by examples such as "Make a cake with an oven" and "Make a cake by using cake mix." 3. Constraint Type 3: Intent - Explained as an additional purpose or demand when completing the goal, shown through examples like "Make a cake for wedding" and "Make a cake for diabetics."</sample>
    <sample id="284">The English content in the image is: How do LLMs perform on Constrained Language Planning? Dataset: wikiHow + Generated Constraints Constraint Type 1: Modifier Definition: A word, an adjective or a phrase that modifies or constrains an abstract goal. Ex1: Make a chocolate cake. Ex2: Make a pink cake. Constraint Type 2: Method Definition: A tool or specified mode that controls the process for achieving the goal. Ex1: Make a cake with an oven. Ex2: Make a cake by using cake mix. Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal. Ex1: Make a cake for wedding. Ex2: Make a cake for diabetics.</sample>
    <sample id="285">The image contains a slide with the title "Can LLMs do Constrained Language Planning?" and a bar chart showing accuracy for different models: T5 (11B), Flan-T5 (11B), GPT-3 (175B), GPT-4 (175B), and InstructGPT (175B). The text at the bottom states, "All baselines achieve unsatisfactory results on planning for specific goals." Additionally, there is a video call interface in the top right corner.</sample>
    <sample id="286">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªæ¼”ç¤ºå¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œCan LLMs do Constrained Language Planning?â€ã€‚å¹»ç¯ç‰‡å·¦ä¾§æœ‰ä¸€ä¸ªæŸ±çŠ¶å›¾ï¼Œæ˜¾ç¤ºäº†ä¸åŒè¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šç›®æ ‡è§„åˆ’ä¸Šçš„å‡†ç¡®ç‡ã€‚å›¾è¡¨ä¸­çš„è¯­è¨€æ¨¡å‹åŒ…æ‹¬T5ï¼ˆ11Bï¼‰ã€Flan-T5ï¼ˆ11Bï¼‰ã€GPT-3ï¼ˆ175Bï¼‰ã€CodeLLaMDAï¼ˆ175Bï¼‰å’ŒInstructGPTï¼ˆ175Bï¼‰ã€‚æ¯ä¸ªæ¨¡å‹çš„æŸ±å­é«˜åº¦ä¸åŒï¼Œè¡¨ç¤ºå®ƒä»¬åœ¨è§„åˆ’ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å¹»ç¯ç‰‡å³ä¾§æœ‰ä¸€æ®µæ–‡å­—ï¼Œå†™ç€â€œAll baselines achieve unsatisfactory results on planning for specific goalsâ€ï¼Œè¡¨æ˜æ‰€æœ‰åŸºçº¿æ¨¡å‹åœ¨ç‰¹å®šç›®æ ‡è§„åˆ’ä¸Šéƒ½æœªèƒ½è¾¾åˆ°ä»¤äººæ»¡æ„çš„æˆæœã€‚èƒŒæ™¯ä¸­å¯ä»¥çœ‹åˆ°ä¸€ä¸ªäººï¼Œä¼¼ä¹æ˜¯åœ¨ä¸€ä¸ªæœ‰æ¡Œå­å’Œæ¤…å­çš„æˆ¿é—´é‡Œï¼Œå¯èƒ½æ˜¯ä¸€ä¸ªåŠå…¬å®¤æˆ–ä¼šè®®å®¤ã€‚</sample>
    <sample id="287">LLMsé€šå¸¸åœ¨å®Œæˆä»»åŠ¡æ—¶çŠ¯å“ªäº›é”™è¯¯ï¼Ÿ</sample>
    <sample id="288">The image shows a presentation slide with the title "What types of errors do LLMs usually make in this task?" The slide includes a radar chart labeled with different error categories such as 'No constraint,' 'Repeated steps,' 'Wrong order,' and 'Incoherent.' There is also text that reads, "The semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) cannot be guaranteed." Additionally, there is an inset picture of a person wearing glasses.</sample>
    <sample id="289">The planning performance of InstructGPTs varies considerably for goals of different categories.</sample>
    <sample id="290">ç”»é¢ä¸­å±•ç¤ºäº†ä¸€ä¸ªæ¼”ç¤ºæ–‡ç¨¿çš„å¹»ç¯ç‰‡ï¼Œå·¦ä¾§æ˜¯ä¸€ä¸ªå¥³æ€§åœ¨è®²è¯ã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªæ ‡é¢˜â€œMethodâ€ï¼Œä¸‹é¢æœ‰å‡ æ®µæ–‡å­—å’Œä¸€äº›æ¡†å›¾ã€‚å¹»ç¯ç‰‡çš„ä¸»è¦å†…å®¹æ˜¯å…³äºä¸€ä¸ªæ–¹æ³•è®ºçš„ä»‹ç»ï¼ŒåŒ…æ‹¬è¾“å…¥æŠ½è±¡ç›®æ ‡ã€ç”Ÿæˆå…·ä½“ç›®æ ‡ä»¥åŠç‰¹å®šç›®æ ‡ç­‰æ­¥éª¤ã€‚å¹»ç¯ç‰‡å³ä¾§æ˜¾ç¤ºäº†ä¸€ä¸ªäººåœ¨ä¸€ä¸ªç°ä»£åŒ–çš„åŠå…¬å®¤ç¯å¢ƒä¸­ã€‚</sample>
    <sample id="291">The image shows a slide from a presentation with the title "Method" at the top. The content of the slide is divided into several sections, each explaining different aspects of generating specific goals using InstructGPT via in-context learning.

1. **Title**: 
   - "Method"

2. **Input**:
   - "Input: an abstract goal"

3. **Step 1**:
   - "Generate specific goals with InstructGPT via in-context learning"
   - This section includes an illustration of a robot and some text that appears to be part of the process described above.

4. **Abstract Goal**:
   - "Abstract Goal: Make a cake"
   - Below this, there are constraints listed as:
     - "+ constraints"

5. **Specific Goals**:
   - Two examples of specific goals are provided:
     - "G1 (+ modifier): Make a chocolate cake"
     - "G2 (+ method): Make it in a microwave"
     - An icon of a cat accompanies these examples.
   - Another example is given for intent:
     - "G3 (+ intent): Make a cake for a wedding"
     - An icon of a person holding hands accompanies this example.

6. **Additional Text**:
   - There is additional text on the right side of the slide, but it is not fully visible or clear enough to read accurately.

7. **Visual Elements**:
   - On the right side of the slide, there is a video call interface showing a woman wearing glasses and a green shirt, sitting in what looks like an office environment with large windows and furniture.

This detailed description covers all the elements present in the image, providing a comprehensive understanding of its contents.</sample>
    <sample id="292">æ–¹æ³• 1. è¾“å…¥ï¼šæŠ½è±¡ç›®æ ‡ 2. Step 1: ä½¿ç”¨ InstructGPT é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ç”Ÿæˆå…·ä½“çš„ä»»åŠ¡ï¼Œä¾‹å¦‚â€œåˆ¶ä½œè›‹ç³•â€ã€â€œåœ¨å¾®æ³¢ç‚‰ä¸­çƒ¤è›‹ç³•â€å’Œâ€œä¸ºå©šç¤¼åˆ¶ä½œè›‹ç³•â€ã€‚ 3. Step 2: é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ è¿‡é‡ç”Ÿæˆå€™é€‰è„šæœ¬ã€‚ 4. ç”Ÿæˆé’ˆå¯¹ç‰¹å®šç›®æ ‡çš„è®¡åˆ’ã€‚</sample>
    <sample id="293">ç”»é¢ä¸­å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œMethodâ€çš„æµç¨‹å›¾ã€‚æµç¨‹å›¾åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼š 1. Step 2: Over-generate candidate scripts via in-context learningã€‚ 2. Step 3: Find the goal with InstructGPT via similarity scoreã€‚ 3. Output: Specific goals with corresponding scriptsã€‚ æµç¨‹å›¾çš„å³ä¾§æœ‰ä¸€ä¸ªå¥³æ€§ï¼Œå¥¹æˆ´ç€çœ¼é•œï¼Œç©¿ç€ç»¿è‰²ä¸Šè¡£ï¼Œååœ¨ä¸€ä¸ªç°ä»£åŒ–çš„åŠå…¬å®¤ç¯å¢ƒä¸­ã€‚åŠå…¬å®¤é‡Œæœ‰ç™½è‰²çš„å¢™å£å’Œç°è‰²çš„åœ°æ¿ï¼ŒèƒŒæ™¯ä¸­å¯ä»¥çœ‹åˆ°ä¸€äº›å®¶å…·å’Œè£…é¥°å“ã€‚</sample>
    <sample id="294">è¿™æ®µæ–‡å­—æè¿°äº†å¦‚ä½•å°†è„šæœ¬å’Œç›®æ ‡è½¬æ¢ä¸ºInstructGPTåµŒå…¥ï¼Œå¹¶é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°æ¥è¡¡é‡è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚</sample>
    <sample id="295">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œMethodâ€çš„æ–¹æ³•è®ºã€‚å·¦ä¾§æœ‰ä¸€ä¸ªæµç¨‹å›¾ï¼Œåˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼š 1. Step 2: Over-generate candidate scripts via in-context learningã€‚ 2. Step 3: Find the goal with InstructGPT via similarity scoreã€‚ 3. Output: Specific goals with corresponding scriptsã€‚ åœ¨æµç¨‹å›¾çš„å³ä¾§ï¼Œæœ‰ä¸€ä¸ªå›¾è¡¨å±•ç¤ºäº†å€™é€‰è„šæœ¬å’Œè¿‡æ»¤åçš„è„šæœ¬ã€‚å€™é€‰è„šæœ¬é€šè¿‡ç›¸ä¼¼æ€§å¾—åˆ†è¿›è¡Œè¯„åˆ†ï¼Œå¾—åˆ†ä»0.3åˆ°0.5ä¸ç­‰ã€‚å…¶ä¸­ä¸€äº›è„šæœ¬è¢«æ ‡è®°ä¸ºâ€œ+â€ï¼Œè¡¨ç¤ºå®ƒä»¬æ˜¯è¿‡æ»¤åçš„è„šæœ¬ã€‚ å›¾è¡¨åº•éƒ¨æ˜¾ç¤ºäº†ä¸€ä¸ªç¤ºä¾‹è„šæœ¬ï¼šâ€œScript 3: 1. Gather your ingredients 2. Add the cocoa powderâ€ã€‚ å³ä¸Šè§’æœ‰ä¸€å¼ ç…§ç‰‡ï¼Œæ˜¾ç¤ºä¸€ä¸ªäººåœ¨å®¤å†…ç¯å¢ƒä¸­ï¼ŒèƒŒæ™¯æ˜¯ä¸€ä¸ªç°ä»£çš„åŠå…¬å®¤æˆ–ä¼šè®®å®¤ã€‚</sample>
    <sample id="296">ç”»é¢ä¸­æœ‰ä¸€ä¸ªç©¿ç€ç»¿è‰²ä¸Šè¡£çš„äººï¼ŒèƒŒæ™¯æ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„å®¤å†…ç¯å¢ƒã€‚ç”»é¢ä¸Šæ–¹æœ‰ä¸€æ®µæ–‡å­—â€œæˆ‘ä»¬çš„æ–¹æ³•æå¤§åœ°æé«˜äº†è§„åˆ’è´¨é‡â€ï¼Œä¸‹æ–¹æœ‰ä¸€ä¸ªæŸ±çŠ¶å›¾ï¼Œå±•ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å‡†ç¡®ç‡æ–¹é¢çš„è¡¨ç°ã€‚å›¾è¡¨ä¸­çš„æ ‡ç­¾åŒ…æ‹¬T5 (11B)ã€Flan-T5 (11B)ã€GPT-3 (175B)ã€InstructGPT (175B)å’Œæˆ‘ä»¬çš„æ–¹æ³•ã€‚å›¾è¡¨æ˜¾ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šçš„æ¯”è¾ƒæƒ…å†µã€‚</sample>
    <sample id="297">The content of the image is a slide from a presentation. The title at the top reads "Script Distillation from LLMs". Below this, there are two sections: 'Motivation' and 'Method'. In the Motivation section, it states that enabling constrained language planning ability for smaller models was one goal. Under Method, it mentions following symbolic knowledge distillation to generate 55,000 scripts with constraints based on CoSprint Dataset. It also notes that these scripts were annotated by humans for validation and testing purposes. On the right side of the slide, there's an illustration showing three steps in the process:

1. Generate specific goals using InstructorGPT via in-context learning.
2. Over-generate candidate scripts within context with instructorGPT.
3. Filter scripted goals through InstructorGPT using a similarity score.

At the bottom of the slide, it specifies that the output should be specific goals with corresponding plans.</sample>
    <sample id="298">Motivation: To enable constrained language planning for smaller models. Method: Follow the idea of symbolic knowledge distillation. Generated 55,000 scripts with constraint from LLMs based on our method â†’ Coscript Dataset. Human annotate validation and test set.</sample>
    <sample id="299">Script Distillation from LLMs</sample>
    <sample id="300">è¿™æ®µæ–‡å­—ä»‹ç»äº†é€šè¿‡LLMsè¿›è¡Œè„šæœ¬è’¸é¦çš„æ–¹æ³•ã€‚å®ƒåŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼š1. ç”Ÿæˆç‰¹å®šç›®æ ‡çš„è„šæœ¬ï¼Œä½¿ç”¨InstructGPTåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­ã€‚2. é€šè¿‡ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦åˆ†æ•°è¿‡æ»¤è„šæœ¬ã€‚3. äººå·¥æ ‡æ³¨éªŒè¯å’Œæµ‹è¯•é›†ã€‚æœ€ç»ˆçš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªåä¸ºCoScriptçš„æ•°æ®é›†ï¼Œç”¨äºæ„å»ºå—çº¦æŸçš„è¯­è¨€è§„åˆ’èƒ½åŠ›è¾ƒå°çš„æ¨¡å‹ã€‚</sample>
    <sample id="301">å¹»ç¯ç‰‡å†…å®¹åŒ…æ‹¬ä¸€ä¸ªæ ‡é¢˜â€œScript Distillation from LLMsâ€ï¼Œåˆ†ä¸ºä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šâ€œåŠ¨æœºâ€å’Œâ€œæ–¹æ³•â€ã€‚åœ¨â€œåŠ¨æœºâ€éƒ¨åˆ†ï¼Œè§£é‡Šäº†ç›®æ ‡æ˜¯é€šè¿‡ä½¿ç”¨InstructGPTè¿›è¡Œåœ¨çº¿å­¦ä¹ æ¥ä½¿è¾ƒå°çš„æ¨¡å‹å…·æœ‰çº¦æŸåŠ›çš„è¯­è¨€è§„åˆ’èƒ½åŠ›ã€‚åœ¨â€œæ–¹æ³•â€éƒ¨åˆ†ï¼Œæ¦‚è¿°äº†ä¸‰ä¸ªæ­¥éª¤ï¼š1. ç”Ÿæˆç‰¹å®šç›®æ ‡çš„è„šæœ¬ï¼ˆä½¿ç”¨InstructGPTè¿›è¡Œåœ¨çº¿å­¦ä¹ ï¼‰ã€‚2. é€šè¿‡ä¸InstructGPTæ¯”è¾ƒåŸºäºçº¦æŸçš„è„šæœ¬æ¥ç”Ÿæˆè„šæœ¬ã€‚3. ä½¿ç”¨InstructGPTæ ¹æ®çº¦æŸå¯¹è„šæœ¬è¿›è¡Œè¯„åˆ†ï¼Œå¹¶æ ‡æ³¨éªŒè¯å’Œæµ‹è¯•é›†ã€‚å¹»ç¯ç‰‡è¿˜æåˆ°ï¼Œæ€»å…±ç”Ÿæˆäº†55,000ä¸ªå¸¦çº¦æŸçš„è„šæœ¬ï¼Œå¹¶ä½¿ç”¨CoSprintæ•°æ®é›†è¿›è¡Œäº†æ ‡æ³¨ã€‚</sample>
    <sample id="302">è¿™å¼ å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œCoscript for Smaller Language Modelsâ€ã€‚å®ƒåŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼š 1. æ•°æ®é›†ï¼ˆDatasetsï¼‰ï¼š - Coscript 2. æŒ‡æ ‡ï¼ˆMetricsï¼‰ï¼š - ä¸€ä¸ªå¿ å®çš„æ¨¡å‹ï¼šDeBERTa (v3 large)ç”¨äºå†³å®šç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦ç¬¦åˆçº¦æŸã€‚ - è‡ªåŠ¨æŒ‡æ ‡ï¼šROUGEã€BLEUã€BERTScoreã€‚ è¿™å¼ å¹»ç¯ç‰‡è¿˜æåˆ°ï¼Œé€šè¿‡åœ¨Coscriptä¸Šå¾®è°ƒè¾ƒå°ä½†ä¸“é—¨çš„è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥å®ç°çº¦æŸè¯­è¨€è§„åˆ’ã€‚</sample>
    <sample id="303">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š

1. æ ‡é¢˜ï¼šSpecialized Models vs. LLMs
2. å›¾è¡¨æ ‡é¢˜ï¼šAccuracy
3. å›¾è¡¨æ•°æ®ï¼š
   - GPT-3 (175B)
   - Codex (175B)
   - InstructGPT (175B)
   - T5 trained on wikiHow
   - T5 trained on Coscript
4. å‰¯æ ‡é¢˜ï¼šSmaller LM's fine-tuned on Coscript can generate higher quality scripts than LLMs

è¿™äº›æ–‡å­—æè¿°äº†ä¸åŒæ¨¡å‹åœ¨å‡†ç¡®æ€§æ–¹é¢çš„è¡¨ç°ï¼Œå¹¶å¼ºè°ƒäº†åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå¾®è°ƒçš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å¯èƒ½æ¯”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ›´é«˜è´¨é‡çš„è„šæœ¬ã€‚</sample>
    <sample id="304">å¹»ç¯ç‰‡åŒ…å«ä¸€ä¸ªæ ‡é¢˜â€œSummary and Takeawaysâ€ï¼Œå¹¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1. â€œEstablish the constrained language planning problemâ€2. â€œLimitations and future workâ€åœ¨â€œEstablish the constrained language planning problemâ€ä¸‹ï¼Œæœ‰ä»¥ä¸‹è¦ç‚¹ï¼š- è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„çº¦æŸè¯­è¨€è§„åˆ’èƒ½åŠ›ï¼Œå¹¶å¼€å‘ä¸€ä¸ªè¿‡ç”Ÿæˆç„¶åè¿‡æ»¤çš„æ–¹æ³•ã€‚- ä½¿ç”¨CoScriptï¼ˆçº¦æŸè¯­è¨€è§„åˆ’çš„é«˜è´¨é‡è„šæœ¬æ•°æ®é›†ï¼‰æ¥ç”Ÿæˆçº¦æŸè¯­è¨€è§„åˆ’çš„é«˜è´¨é‡è„šæœ¬æ•°æ®é›†ã€‚åœ¨â€œLimitations and future workâ€ä¸‹ï¼Œæœ‰ä»¥ä¸‹è¦ç‚¹ï¼š- æ”¹è¿›LLMsçš„æ–¹æ³•æ˜¯åéªŒæ–¹æ³•ã€‚- CoScriptä»…ä»ä¸€ä¸ªé¢å¤–çš„çº¦æŸä¸­ç»§æ‰¿ã€‚- CoScriptæ•°æ®é›†å¯ä»¥ä½œä¸ºæ¨è¿›è¯­è¨€è§„åˆ’ç ”ç©¶çš„å®è´µèµ„æºï¼Œå…·æœ‰æ›´å¤šçš„å¤æ‚ç›®æ ‡å’Œçº¦æŸã€‚å¹»ç¯ç‰‡å³ä¸Šè§’æ˜¾ç¤ºä¸€ä¸ªäººï¼Œå¯èƒ½æ­£åœ¨æ¼”ç¤ºå†…å®¹ã€‚</sample>
    <sample id="305">å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€æ®µè‹±æ–‡å†…å®¹ï¼Œä¸»è¦è®¨è®ºäº†çº¦æŸè¯­è¨€è§„åˆ’çš„é—®é¢˜ã€‚å…·ä½“å†…å®¹å¦‚ä¸‹ï¼š 1. å»ºç«‹çº¦æŸè¯­è¨€è§„åˆ’é—®é¢˜ã€‚ 2. è¯„ä¼°LLMsï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰åœ¨çº¦æŸè¯­è¨€è§„åˆ’ä¸­çš„èƒ½åŠ›ï¼Œå¹¶å¼€å‘ä¸€ä¸ªè¿‡è½½-ç„¶å-è¿‡æ»¤å™¨ç”¨äºLLMsã€‚ 3. ä½¿ç”¨LLMsç”Ÿæˆé«˜è´¨é‡çš„è„šæœ¬æ•°æ®é›†ï¼ˆCoScriptï¼‰ç”¨äºçº¦æŸè¯­è¨€è§„åˆ’ã€‚ è¿™äº›è¦ç‚¹è¡¨æ˜ï¼Œè¯¥å¹»ç¯ç‰‡çš„é‡ç‚¹æ˜¯ä»‹ç»å’Œè§£å†³ä¸ä½¿ç”¨LLMsè¿›è¡Œçº¦æŸè¯­è¨€è§„åˆ’ç›¸å…³çš„é—®é¢˜ã€‚</sample>
    <sample id="306">61st Annual Meeting of the Association for Computational Linguistics Toronto, Canada July 8-14, 2023 Distilling Script Knowledge from Large Language Models for Constrained Language Planning Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang syyuan21@m.fudan.edu.cn https://github.com/siyuuyuan/coscript</sample>
    <sample id="307">PaLMçš„æµç•…åº¦ä¸SOTAç›¸å½“ã€‚</sample>
    <sample id="308">æ°´å°æ–¹æ³•çš„é‡è¦å±æ€§æ˜¯ï¼š 1. å¯ä»¥åº”ç”¨äºåµŒå…¥å¼æœåŠ¡ã€‚ 2. ä¸åº”é™ä½æä¾›çš„åµŒå…¥å¼å†…å®¹çš„å¯ç”¨æ€§ã€‚ 3. åº”è¯¥è¶³å¤Ÿéšè”½ï¼Œä»¥è‡³äºæ”»å‡»è€…æ— æ³•è½»æ˜“æ£€æµ‹åˆ°æˆ–ç§»é™¤æ°´å°ã€‚ 4. æ°´å°éœ€è¦èƒ½å¤Ÿè½¬ç§»åˆ°æ”»å‡»è€…çš„æœåŠ¡ä¸­ã€‚</sample>
    <sample id="309">TED è‹±è¯­æ¼”è®²å·²è¢«ç¿»è¯‘æˆ14ç§ä¸åŒçš„è¯­è¨€ï¼ŒåŒ…æ‹¬è‹±è¯­ã€è¥¿ç­ç‰™è¯­ã€æ³•è¯­ã€æ„å¤§åˆ©è¯­ã€æ—¥è¯­ã€è·å…°è¯­ã€è‘¡è„ç‰™è¯­ã€ç½—é©¬å°¼äºšè¯­ã€ä¿„è¯­ã€åœŸè€³å…¶è¯­å’Œä¸­æ–‡ã€‚</sample>
    <sample id="310">The slide suggests that a significant number of instances should be selected for re-annotation. It states, "We select 300 instances to annotate."</sample>
    <sample id="311">Cosine and L2 similarity metrics are used to measure the difference between benign and backdoor datasets.</sample>
    <sample id="312">åŸºäºç¼–ç å™¨çš„å¤šè¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹æ¥å®ç°ã€‚</sample>
    <sample id="344">ä½œè€…é€šè¿‡åœ¨ä¸€èˆ¬æ–‡æœ¬è¯­æ–™åº“ä¸­è®¡ç®—å•è¯é¢‘ç‡æ¥ç¡®å®šä¸­ç­‰é¢‘ç‡çš„å•è¯ã€‚</sample>
    <sample id="345">ç”»é¢ä¸­æœ‰ä¸€ä¸ªåœ†å½¢çš„å¤´åƒï¼ŒèƒŒæ™¯æ˜¯æµ…è‰²çš„ï¼Œä¸Šé¢æœ‰ä¸€äº›å‡ ä½•å›¾æ¡ˆã€‚å¤´åƒæ—è¾¹æœ‰æ–‡å­—ï¼Œå†™ç€â€œShuheng Liu, Alan Ritterâ€ï¼Œä»¥åŠâ€œSchool of Interactive Computing, Georgia Institute of Technologyâ€ã€‚å³ä¸‹è§’æœ‰ä¸€ä¸ªGeorgia Techçš„æ ‡å¿—ã€‚</sample>
    <sample id="346">è¿™æ®µæ–‡å­—ä»‹ç»äº†åä¸ºâ€œå‘½åå®ä½“è¯†åˆ«ä¸æ³›åŒ–â€çš„å†…å®¹ã€‚</sample>
    <sample id="347">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œNamed Entity Recognition &amp; Generalizationâ€ã€‚å®ƒåŒ…å«ä¸€ä¸ªé¡¹ç›®ç¬¦å·ï¼Œå†…å®¹æ˜¯ï¼šâ€œè¿™äº›æ¨¡å‹å·²ç»ä½¿ç”¨CoNLL-2003å¼€å‘NERæ¥è¿‘20å¹´äº†ã€‚â€è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼šâ€œè¿™äº›æ¨¡å‹èƒ½é€‚åº”ç°ä»£æ•°æ®å—ï¼Ÿâ€å³ä¸‹è§’æœ‰ä¸€ä¸ªGeorgia Techçš„æ ‡å¿—ã€‚</sample>
    <sample id="348">å¹»ç¯ç‰‡ä¸Šçš„æ–‡å­—å†…å®¹æ˜¯å…³äºå‘½åå®ä½“è¯†åˆ«å’Œæ³›åŒ–ã€‚ä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š 1. æ¨¡å‹å·²ç»ä½¿ç”¨CoNLL-2003æ¥å¼€å‘NERï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰å°†è¿‘20å¹´ã€‚ 2. è¿™äº›æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿé€‚åº”ç°ä»£æ•°æ®ï¼Ÿ 3. ä¸ºäº†å®ç°è‰¯å¥½çš„æ³›åŒ–ï¼Œéœ€è¦ä»€ä¹ˆï¼Ÿ å¹»ç¯ç‰‡çš„å·¦ä¸‹è§’æœ‰ä¸€ä¸ªGeorgia Techçš„æ ‡å¿—ã€‚</sample>
    <sample id="349">The image shows a slide from a presentation titled 'Named Entity Recognition &amp; Generalization.' The content of the slide includes three bullet points: 1. Models have been using CoNLL-2003 to develop NER for almost 20 years. 2. Can these models generalize to modern data? 3. What is needed for good generalization? Additionally, there are two more questions listed below in black text: - What causes the performance drop of these models? There is also an image of a person and a logo at Georgia Tech in the bottom right corner.</sample>
    <sample id="350">å¹»ç¯ç‰‡å±•ç¤ºäº†å…³äºCoNLL++æ•°æ®é›†çš„ä¿¡æ¯ã€‚æ ‡é¢˜ä¸ºâ€œCoNLL++æ•°æ®é›†â€ï¼Œå†…å®¹åŒ…æ‹¬æ”¶é›†äº†2020å¹´çš„è·¯é€ç¤¾æ–°é—»ï¼Œå¹¶ä½¿ç”¨CoNLL-2003æ³¨é‡ŠæŒ‡å—è¿›è¡Œäº†æ³¨é‡Šã€‚æ–‡æœ¬æ—è¾¹æœ‰ä¸€ä¸ªåˆ—è¡¨ï¼Œæ˜¾ç¤ºäº†å•è¯åŠå…¶ç›¸åº”çš„æ ‡æ³¨ç±»åˆ«ï¼šAMBASSADOR, TO, THE, UNITED NATIONS, : (å†’å·), LINDA å’Œ THOMAS-GREENFIELDã€‚æ¯ä¸ªå•è¯éƒ½ä¸ä¸€ä¸ªæ ‡ç­¾é…å¯¹ï¼Œä¾‹å¦‚AMBASSADORå’ŒTOè¢«æ ‡è®°ä¸ºâ€œOâ€ï¼Œè€ŒLINDAå’ŒTHOMAS-GREENFIELDè¢«æ ‡è®°ä¸ºâ€œI-PERâ€ï¼ˆè¡¨ç¤ºä¸ªäººï¼‰ã€‚å¹»ç¯ç‰‡çš„å³ä¸‹è§’æœ‰Georgia Techçš„æ ‡å¿—ã€‚</sample>
    <sample id="351">The slide presents information about the CoNLL++ dataset. It explains that Reuters news from 2020 was collected and annotated using guidelines from CoNLL-2003. Additionally, over 20 models were fine-tuned on data from CoNLL-2003. The evaluation of these models took place on both the CoNLL-2003 test set and the CoNLL++ test set.</sample>
    <sample id="352">CoNLL++ Dataset</sample>
    <sample id="353">The content in the image is a slide from Georgia Tech, which poses the question "What Is Needed for Good Generalization?" This suggests that the presentation might be discussing factors or conditions necessary to achieve effective generalization. The presence of the person's photo indicates they may be presenting this information at an event hosted by Georgia Tech.</sample>
    <sample id="354">è¿™æ®µå†…å®¹è®¨è®ºäº†ä¸ºäº†å®ç°è‰¯å¥½çš„æ³›åŒ–æ‰€éœ€çš„å…ƒç´ ã€‚å®ƒå¼ºè°ƒäº†æ¨¡å‹æ¶æ„çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºTransformeræ¨¡å‹åœ¨æ³›åŒ–æ–¹é¢è¡¨ç°æ›´å¥½ã€‚</sample>
    <sample id="355">What Is Needed for Good Generalization?</sample>
    <sample id="356">è¿™æ®µæ–‡å­—è®¨è®ºäº†ä¸ºäº†å®ç°è‰¯å¥½çš„æ³›åŒ–ï¼Œéœ€è¦å“ªäº›å› ç´ ã€‚å®ƒæåˆ°äº†æ¨¡å‹æ¶æ„ã€æ¨¡å‹å¤§å°å’Œå¾®è°ƒç¤ºä¾‹çš„æ•°é‡ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæŒ‡å‡ºTransformeræ¨¡å‹åœ¨æ³›åŒ–æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œæ›´å¤§çš„æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”æ›´å¤šçš„å¾®è°ƒç¤ºä¾‹ä¼šå¯¼è‡´æ›´å¥½çš„æ³›åŒ–ã€‚</sample>
    <sample id="357">æ€§èƒ½ä¸‹é™çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ</sample>
    <sample id="358">å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªæ ‡é¢˜ï¼Œå†™ç€â€œWhat Causes Performance Drop?â€ã€‚åœ¨æ ‡é¢˜ä¸‹æ–¹ï¼Œæœ‰ä¸€ä¸ªé¡¹ç›®ç¬¦å·ï¼Œå†™ç€â€œAdaptive overfitting?â€ã€‚å³ä¸‹è§’æœ‰ä¸€ä¸ªGeorgia Techçš„æ ‡å¿—ã€‚</sample>
    <sample id="359">å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªæ ‡é¢˜ï¼Œå†™ç€â€œæ˜¯ä»€ä¹ˆå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Ÿâ€ï¼Œä¸‹é¢æœ‰ä¸¤ä¸ªè¦ç‚¹ã€‚ç¬¬ä¸€ä¸ªè¦ç‚¹æ˜¯â€œè‡ªé€‚åº”è¿‡æ‹Ÿåˆï¼Ÿâ€ç¬¬äºŒä¸ªè¦ç‚¹æ˜¯â€œæ—¶é—´æ¼‚ç§»ï¼Ÿâ€ã€‚å·¦ä¸‹è§’æœ‰ä¸€ä¸ªå°åœ†å½¢å›¾ç‰‡ï¼Œå³ä¸‹è§’æœ‰ä¸€ä¸ªGeorgia Techçš„æ ‡å¿—ã€‚</sample>
    <sample id="360">What Causes Performance Drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="361">What Causes Performance Drop? Adaptive overfitting? Temporal drift? No diminishing returns Temporal drift?</sample>
    <sample id="362">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed Temporal drift?</sample>
    <sample id="363">Temporal drift refers to the gradual change or shift in a system's behavior over time. In machine learning, it can occur when models are not regularly updated with new data, leading to their performance degrading as they become less effective at predicting outcomes based on recent trends and patterns. This concept is particularly relevant for tasks like language modeling or image recognition where contextual information changes frequently.</sample>
    <sample id="364">What Causes Performance Drop? Adaptive overfitting, No diminishing returns, Not observed Temporal drift?</sample>
    <sample id="365">æ€§èƒ½ä¸‹é™çš„ä¸»è¦åŸå› æ˜¯æ—¶é—´æ¼‚ç§»ã€‚</sample>
    <sample id="366">è¿™æ®µæ–‡å­—è®¨è®ºäº†ä¸ºäº†å®ç°è‰¯å¥½çš„æ³›åŒ–ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¥½çš„æ¨¡å‹æ¶æ„ã€æ›´å¤§çš„æ¨¡å‹å°ºå¯¸ä»¥åŠæ›´å¤šçš„å¾®è°ƒç¤ºä¾‹ã€‚è¿™äº›ç›®æ ‡æ˜¯ç´§å¯†ç›¸å…³çš„ï¼Œä¸èƒ½åªä¾èµ–äºä¸€ä¸ªå› ç´ ï¼Œè€Œå¿½è§†å…¶ä»–å› ç´ ã€‚</sample>
    <sample id="367">å°†è¿™æ®µè‹±è¯­å†…å®¹è¡¨è¾¾ä¸ºä¸­æ–‡ã€‚</sample>
    <sample id="368">The content of this paragraph is: 'And we found that the answer is actually a resounding YES.'</sample>
    <sample id="369">ç»“è®º ä¸ºäº†æ›´å¥½åœ°æ³›åŒ–ï¼Œæˆ‘ä»¬éœ€è¦ï¼š - æ›´å¥½çš„æ¨¡å‹æ¶æ„ - è¾ƒå¤§çš„æ¨¡å‹å¤§å° - æ›´å¤šçš„ç²¾ç‚¼ç¤ºä¾‹ æ€§èƒ½ä¸‹é™æ˜¯ç”±ï¼š - æ—¶é—´æ¼‚ç§» - ä¸é€‚é…è¿‡æ‹Ÿåˆ è¿™ç¯‡è®ºæ–‡å¸Œæœ›å¼•èµ·æ›´å¤šå…³äºå¦‚ä½•æ”¹è¿›æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„ç ”ç©¶ã€‚</sample>
    <sample id="370">The content of this text is about providing contact information and resources related to a paper. It includes the following details: 1. A link to an arXiv paper (https://arxiv.org/abs/2212.09747) 2. A link to a dataset on GitHub (https://github.com/ShuhengL/ac12023_conllpp) 3. An email address for contact purposes (sliu775@gatech.edu) The background image shows people walking in front of a building, which appears to be part of Georgia Tech's campus.</sample>
    <sample id="397">è¯¥æ–¹æ³•ä½¿ç”¨çš„è¯­éŸ³ç‰‡æ®µå¤§å°æ˜¯1024ä¸ªæ ·æœ¬ã€‚</sample>
    <sample id="398">Servin is a judge.</sample>
    <sample id="399">The example quality is more important than similarity to the source sentence.</sample>
    <sample id="400">è®ºæ–‡ä¾§é‡äºRoBERTA-baseã€RoBERTA-largeã€distilRoBERTaå’ŒALBERT-largeã€‚</sample>
    <sample id="401">è¯¥æ¨¡å‹æ˜¯ä½¿ç”¨ç‰¹å®šå±‚çš„æ³¨æ„åŠ›åˆ†æ•°ã€‚</sample>
    <sample id="402">ç›´æ¥æ¨æ–­çš„ç¤ºä¾‹åŒ…æ‹¬ï¼š 1. â€œeasy on meâ€ï¼Œâ€œthe first oneâ€ 2. â€œThe song thatâ€™s not energetic.â€</sample>
    <sample id="403">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„æ˜¯å¤æ—¦å¤§å­¦å’ŒBrain Technologies Inc.ã€‚</sample>
    <sample id="404">è¿™ç¯‡è®ºæ–‡æœ‰6ä½ä½œè€…ã€‚</sample>
    <sample id="405">æ˜¯çš„ï¼Œåœ¨è¯­ä¹‰è§£æä¹‹å‰ï¼Œä½¿ç”¨æœºå™¨ç¿»è¯‘æ¨¡å‹å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¿»è¯‘æˆç›®æ ‡è¯­è¨€ä½œä¸ºåŸºçº¿ã€‚</sample>
    <sample id="406">ä½œè€…ç»™å‡ºçš„â€œæ˜¾æ€§ç¾¤ä½“â€(marked group) çš„ç¤ºä¾‹æ˜¯â€œa woman warriorâ€ã€‚</sample>
    <sample id="407">Based on the content of the image, it can be inferred that models with poor generalization ability are those without good model architecture. The slide specifically mentions "Transformer models generalize better," implying that transformer-based architectures tend to have stronger generalization capabilities compared to other types of models or architectures. Therefore, we can conclude that models lacking effective architectural design may struggle in terms of their ability to adapt and perform well on new data.</sample>
    <sample id="408">æµ‹è¯•æ•°æ®é›†çš„åç§°æ˜¯â€œWSL approaches benefit from more clean validation samples!â€</sample>
    <sample id="409">The paper has six authors: Akthar Al-Ali, Martin Pomsl, Kaheer Saleem, Adam Trischler, Alexandra Olteneanu, and Jackie CK Cheung.</sample>
    <sample id="410">The author used multiple modalities, including text and video.</sample>
    <sample id="439">ä½œè€…è®¤ä¸ºï¼ŒNLU ä¸­ç ”ç©¶ä¸è¶³çš„é¢†åŸŸæ˜¯â€œpretrain-time knowledgeâ€å’Œâ€œinference-time knowledgeâ€ã€‚</sample>
    <sample id="440">æ¼”è®²è€…çš„åå­—æ˜¯Zhiyang Xuã€Ying Shenå’ŒLifu Huangã€‚</sample>
    <sample id="441">Yes, Coscript has been quality checked.</sample>
    <sample id="442">ç°æœ‰çš„èµ„æºåœ¨è¯„ä¼°ä¸Šä¸‹æ–‡ä¾èµ–ç¿»è¯‘æ—¶çš„å±€é™æ€§åŒ…æ‹¬ï¼š 1. ä¸Šä¸‹æ–‡ä¾èµ–çš„å•è¯æ¯”ä¾‹è¾ƒå°ã€‚ 2. ç°æœ‰æ–¹æ³•æ”¯æŒçš„è®¨è®ºç°è±¡å’Œè¯­è¨€æœ‰é™ã€‚ è¿™äº›é™åˆ¶è¡¨æ˜ï¼Œå°½ç®¡æœ‰ä¸€äº›èµ„æºå­˜åœ¨ï¼Œä½†å®ƒä»¬å¯èƒ½æ— æ³•å®Œå…¨æ•æ‰åˆ°ä¸Šä¸‹æ–‡ä¾èµ–ç¿»è¯‘çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚</sample>
    <sample id="443">Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis Google Research</sample>
    <sample id="444">Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis Google Research</sample>
    <sample id="445">Indirect referring expressions are used to understand users' language when they want to make a choice.</sample>
    <sample id="446">The image contains a slide from a presentation titled 'Indirect Referring Expressions'. The goal of the study is to understand users' language when they make a choice. It discusses alternative questions and direct references, providing examples such as "easy on me" or "the first one". Indirect reference could be used in natural and fluid conversation with notes that it cannot remember the name but pronunciations are hard to distinguish and want to specify a preference. An example given for indirect reference is "the newer one." Another example provided is "the song that's not energetic." There is also an illustration showing how these expressions can be applied in everyday conversations by using names like "me" or positions like "the first one."</sample>
    <sample id="447">Indirect referring expressions are used to refer to something indirectly. This can happen when the user cannot remember the name of a song or when they want to specify their preference for one option over another in conversation.</sample>
    <sample id="448">Indirect referent could be used in natural and fluid conversation: Cannot remember the name The pronunciations are hard to distinguish Want to specify a preference</sample>
    <sample id="449">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference easy on me, the first one Indirect reference could be used in natural and fluid conversation: Cannot remember the name The pronunciations are hard to distinguish Want to specify a preference Indirect reference The newer one. The song that's not energetic.</sample>
    <sample id="450">è¿™æ˜¯å¯¹è¯ç³»ç»Ÿå’Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å®ä½“ç†è§£çš„é‡è¦é—®é¢˜ã€‚</sample>
    <sample id="451">Dataset Collection Important problem Conversational systems Benchmarking Large Language Modelsâ€™ entity understanding No large-scale public dataset available We collect a large dataset using crowd annotation Three domains:</sample>
    <sample id="452">æ•°æ®é›†æ”¶é›†æ–¹æ³•è®ºå¼ºè°ƒä½¿ç”¨å¡é€šå®Œæˆä»»åŠ¡æ¥å¼ºè°ƒéæ­£å¼æ€§ã€‚</sample>
    <sample id="453">ç”»é¢ä¸­å±•ç¤ºäº†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œDataset Collection Methodologyâ€ã€‚å¹»ç¯ç‰‡çš„ä¸»è¦å†…å®¹æ˜¯å…³äºä½¿ç”¨å¡é€šå®Œæˆä»»åŠ¡æ¥å¼ºè°ƒéæ­£å¼æ€§ã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€æ®µæ–‡å­—è¯´æ˜äº†è¿™ç§æ–¹æ³•çš„æ­¥éª¤å’Œç›®çš„ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸‰ä¸ªäººç‰©å½¢è±¡ï¼Œæ¯ä¸ªå½¢è±¡æ—è¾¹éƒ½æœ‰ä¸€ä¸ªå¯¹è¯æ¡†ï¼Œæ˜¾ç¤ºäº†ä»–ä»¬ä¹‹é—´çš„å¯¹è¯å†…å®¹ã€‚å³ä¸‹è§’æœ‰ä¸€ä¸ªé»„è‰²ç®­å¤´ï¼ŒæŒ‡å‘ä¸€ä¸ªæ³¨é‡Šæ¡†ï¼Œæ³¨é‡Šæ¡†é‡Œå†™ç€â€œç”±æ³¨é‡Šå‘˜å¡«å†™â€ã€‚å·¦ä¸‹è§’æœ‰ä¸€ä¸ªå°åœ†å½¢å›¾åƒï¼Œå¯èƒ½æ˜¯æ¼”è®²è€…çš„ç…§ç‰‡ã€‚èƒŒæ™¯æ˜¯ä¸€ä¸ªå®¤å†…ç¯å¢ƒï¼Œæœ‰æ¤ç‰©è£…é¥°ã€‚</sample>
    <sample id="454">åœ¨ç¬¬äºŒå¼ å›¾ç‰‡ä¸­ï¼ŒAliceè¯´ï¼šâ€œä½ æ˜¯æŒ‡â€˜easy on meâ€™è¿˜æ˜¯â€˜I got a feelingâ€™ï¼Ÿâ€</sample>
    <sample id="455">The alternative question is 'Do you mean "Easy on Me" or 1 Gotta Feeling?'.</sample>
    <sample id="456">Dataset Collection Methodology</sample>
    <sample id="457">The second one, which is the alternative question, is generated as follows.</sample>
    <sample id="458">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š

æ ‡é¢˜ï¼šGenerate alternative questions =&gt; sampling entity pairs

å‰¯æ ‡é¢˜ï¼šDo you mean A or B?

æ­£æ–‡ï¼š
- Items with similar infoboxes on Wikipedia (same genre and/or artist)
  - Do you mean This is It or Man in the Mirror?
- Items with similar descriptions on Wikipedia
  - Do you mean Thinking of You or Happy Anywhere?
- Items with similar titles:
  - Do you mean The Return (memoir) or The Return (Shatner novel)?
- Uniform at random:
  - Do you mean You Could Be Mine or The Way I Am

åº•éƒ¨æ³¨é‡Šï¼šRevisiting Inductive Expressions for Entity Selection Variability Correlation

å³ä¸Šè§’æœ‰Google Researchçš„æ ‡å¿—ã€‚</sample>
    <sample id="459">ç”Ÿæˆæ›¿ä»£é—®é¢˜ =&gt; ç”Ÿæˆå®ä½“å¯¹</sample>
    <sample id="460">ç”Ÿæˆæ›¿ä»£é—®é¢˜ =&gt; ç”Ÿæˆå®ä½“å¯¹</sample>
    <sample id="461">ç”Ÿæˆæ›¿ä»£é—®é¢˜ =&gt; ç”Ÿæˆå®ä½“å¯¹</sample>
    <sample id="462">è¿™å¼ å¹»ç¯ç‰‡æ˜¯å…³äºç”Ÿæˆæ›¿ä»£é—®é¢˜ä»¥å®ä½“å¯¹é½çš„æ¼”ç¤ºã€‚å®ƒåŒ…å«ä¸€ä¸ªæ ‡é¢˜ï¼Œå†™ç€â€œGenerate alternative questions =&gt; sampling entity pairsâ€ï¼Œä»¥åŠä¸€ä¸ªå‰¯æ ‡é¢˜ï¼Œå†™ç€â€œDo you mean A or B?â€ã€‚åœ¨è¿™äº›æ–‡æœ¬æ—è¾¹ï¼Œæœ‰ä¸€ä¸ªé»„è‰²ç®­å¤´ï¼Œä¸Šé¢å†™ç€â€œMore Similar (usually harder)â€ã€‚å¹»ç¯ç‰‡è¿˜åˆ—å‡ºäº†å‡ ç§ç”Ÿæˆæ›¿ä»£é—®é¢˜çš„æ–¹æ³•ï¼š1. ä¸ç»´åŸºç™¾ç§‘ä¸Šå…·æœ‰ç›¸ä¼¼ä¿¡æ¯æ¡†ï¼ˆåŒä¸€æµæ´¾å’Œ/æˆ–è‰ºæœ¯å®¶ï¼‰çš„é¡¹ç›®ç›¸å…³çš„é—®é¢˜ã€‚2. ä¸ç»´åŸºç™¾ç§‘ä¸Šå…·æœ‰ç›¸ä¼¼æè¿°çš„é¡¹ç›®ç›¸å…³çš„é—®é¢˜ã€‚3. å…·æœ‰ç›¸ä¼¼æ ‡é¢˜çš„é¡¹ç›®ã€‚4. éšæœºé€‰æ‹©çš„é¡¹ç›®ã€‚å¹»ç¯ç‰‡å³ä¸‹è§’æœ‰ä¸€ä¸ªå°å›¾åƒï¼Œå¯èƒ½æ˜¯ä¸€ä¸ªäººçš„å¤´åƒã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªGoogle Researchçš„æ ‡å¿—ã€‚</sample>
    <sample id="463">èƒŒæ™¯çŸ¥è¯†ï¼ˆéŸ³ä¹ï¼‰ Googleæœç´¢é“¾æ¥åˆ°æ¯é¦–æ­Œã€‚ Easy on Meï¼ˆAdeleæ¼”å”±ï¼‰ I Gotta Feelingï¼ˆé»‘çœ¼è±†è±†æ¼”å”±ï¼‰ æˆ‘ä»¬è¦æ±‚æ³¨é‡Šå‘˜ï¼š å¬è‡³å°‘ä¸€é¦–æ­Œçš„ç‰‡æ®µ é˜…è¯»æ¯é¦–æ­Œ</sample>
    <sample id="464">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œBackground knowledge (Music)â€ã€‚</sample>
    <sample id="465">èƒŒæ™¯çŸ¥è¯†ï¼ˆéŸ³ä¹ï¼‰ Easy on Me Adele</sample>
    <sample id="466">ä¸ºäº†åœ¨â€œé£Ÿè°±â€å’Œâ€œä¹¦ç±â€é¢†åŸŸä¸­æä¾›èƒŒæ™¯ä¿¡æ¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç»´åŸºç™¾ç§‘ä¸Šçš„éƒ¨åˆ†èƒŒæ™¯æ–‡æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†æ¥è‡ªç»´åŸºç™¾ç§‘çš„å›¾ç‰‡ï¼Œä»¥å¸®åŠ©æ³¨é‡Šå™¨äº†è§£å®ƒä»¬çš„å¤–è§‚ã€‚</sample>
    <sample id="467">æ‰€ç»™å‡ºçš„è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡æ˜¯ï¼šç„¶åæˆ‘ä»¬å‘Šè¯‰æ³¨é‡Šå‘˜é€‰æ‹©å“ªä¸€ä¸ªï¼Œå¹¶è¦æ±‚ä»–ä»¬æè¿°å®ƒã€‚</sample>
    <sample id="468">æ‰€ç»™å‡ºçš„è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡æ˜¯ï¼š 'æˆ‘ä»¬ç„¶åå‘Šè¯‰æ³¨é‡Šè€…é€‰æ‹©å“ªä¸€ç§ï¼Œå¹¶è¦æ±‚ä»–ä»¬æè¿°å®ƒã€‚'</sample>
    <sample id="469">AltEntities Corpus åŒ…å«å¤§çº¦6000ä¸ªåœ¨ä¸‰ä¸ªé¢†åŸŸä¸­çš„æ›¿ä»£é—®é¢˜å’Œçº¦42000ä¸ªé—´æ¥æŒ‡ä»£è¡¨è¾¾å¼ã€‚ä½¿ç”¨T5 XLæ¨¡å‹çš„ç»“æœå¦‚ä¸‹ï¼š - å½“LMå…·æœ‰ä¸æ³¨é‡Šå‘˜ç›¸åŒèƒŒæ™¯çŸ¥è¯†æ—¶ï¼Œå‡†ç¡®ç‡ä¸º92-95%ã€‚ - å½“LMå…·æœ‰éƒ¨åˆ†é‡å çš„èƒŒæ™¯çŸ¥è¯†æ—¶ï¼Œå‡†ç¡®ç‡ä¸º82-87%ã€‚ - å½“LMä»…å…·æœ‰å®ä½“åç§°è®¿é—®æƒé™æ—¶ï¼Œå‡†ç¡®ç‡çº¦ä¸º60%ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†æ¨¡å‹æ˜¯åŸŸé€šç”¨çš„ã€‚æ•°æ®é›†é“¾æ¥ä¸ºï¼šhttps://github.com/google-research/datasets/AltEntities</sample>
    <sample id="470">The image shows a slide from a presentation by Google Research. The title of the slide is 'AltEntities Corpus'. It contains information about an alternative questions dataset across three domains, including 6000 different questions and approximately 42,000 indirect referring expressions.

The slide also discusses results with T5 XL model accuracy:
- If the language model has access to the same background knowledge as annotators: 92-95% accuracy.
- If it has access to partially overlapping background knowledge: 82-87% accuracy.
- With only entity names available: around 60% accuracy.

It concludes that their models are domain-generalizable. There is a dataset link provided at the bottom for more information (https://github.com/google-research/datasets/AltEntities).

Additionally, there is a small profile picture in the lower right corner of the presenter or author associated with this research.</sample>
    <sample id="471">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š

æ ‡é¢˜ï¼šAltEntities Corpus

å‰¯æ ‡é¢˜ï¼šGoogle Research

æ­£æ–‡ï¼š
- ~6,000 alternative questions across the three domains
- ~42,000 indirect referring expressions
- Results with T5 XL model (accuracy):
  - 92-95% if the LM has access to the same background knowledge as annotators.
  - 82-87% when the LM has access to partially overlapping background knowledge.
  - ~60% when the LM (T5 XL) has only access to the entity names.
- We showed models are domain-generalizable.

æ•°æ®é›†é“¾æ¥ï¼šhttps://github.com/google-research/datasets/AltEntities

åº•éƒ¨æ³¨é‡Šï¼šResolving Indirect References for Entity Selection AltEntities Corpus</sample>
    <sample id="472">å¹»ç¯ç‰‡ä¸Šæ˜¾ç¤ºäº†â€œAltEntities Corpusâ€æ ‡é¢˜ï¼Œä¸‹é¢åˆ—å‡ºäº†å‡ ä¸ªè¦ç‚¹ï¼š 1. çº¦6000ä¸ªä¸åŒé¢†åŸŸçš„æ›¿ä»£é—®é¢˜ã€‚ 2. çº¦42000ä¸ªé—´æ¥è¡¨è¾¾çš„å®ä½“å¼•ç”¨ã€‚ 3. T5 XLæ¨¡å‹çš„å‡†ç¡®æ€§ç»“æœï¼š - å¦‚æœLMæœ‰ä¸æ³¨é‡Šå‘˜ç›¸åŒçš„èƒŒæ™¯çŸ¥è¯†ï¼Œå‡†ç¡®ç‡ä¸º92-95%ã€‚ - å¦‚æœLMæœ‰éƒ¨åˆ†é‡å çš„èƒŒæ™¯çŸ¥è¯†ï¼Œå‡†ç¡®ç‡ä¸º82-87%ã€‚ - å¦‚æœLMåªæœ‰å®ä½“åç§°çš„è®¿é—®æƒé™ï¼Œå‡†ç¡®ç‡ä¸‹é™åˆ°60%ã€‚ 4. å¼ºè°ƒå±•ç¤ºäº†æ¨¡å‹çš„é¢†åŸŸé€šç”¨æ€§ã€‚ 5. æä¾›äº†ä¸€ä¸ªæ•°æ®é›†é“¾æ¥ï¼šhttps://github.com/google-research/datasets/AltEntitiesã€‚ å¹»ç¯ç‰‡å³ä¸‹è§’æœ‰ä¸€ä¸ªGoogle Researchæ ‡å¿—ï¼Œå¹¶ä¸”å·¦ä¸‹è§’æœ‰ä¸€ä¸ªå°åœ†å½¢å›¾åƒã€‚</sample>
    <sample id="473">è¯¥æ–¹æ³•ä¸ popular strategies also applied to offline models è¿›è¡Œäº†æ¯”è¾ƒã€‚</sample>
    <sample id="474">The authors belong to the following institutions: 1. LIA, Avignon UniversitÃ© 2. LSIN, Nantes UniversitÃ© 3. Clinique des diaboliques, CHU de Nantes 4. Zenicod</sample>
    <sample id="475">æ¼”è®²è€…çš„åå­—æ˜¯Jenny T. Liangã€‚</sample>
    <sample id="476">The paper has three authors: Myra Cheng, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="477">è¿™æ˜¯ä¸€å¼ æ¼”ç¤ºæ–‡ç¨¿çš„æˆªå›¾ï¼Œæ ‡é¢˜ä¸ºâ€œAttention as a Guide for Simultaneous Speech Translationâ€ã€‚ä½œè€…åŒ…æ‹¬Sara Papiã€Matteo Negriå’ŒMarco Turchiã€‚å›¾ç‰‡åº•éƒ¨å±•ç¤ºäº†University of Trentoå’ŒBruno KessleråŸºé‡‘ä¼šçš„æ ‡å¿—ã€‚</sample>
    <sample id="478">Simultaneous speech translation, or SimulST, is the process of translating spoken language into text in another language in real-time.</sample>
    <sample id="479">ç‰¹å®šçš„æ¶æ„é€šå¸¸è¢«è®­ç»ƒï¼Œå¼•å…¥é¢å¤–çš„æ¨¡å—ä»¥è¿›è¡Œä¼˜åŒ–ã€‚</sample>
    <sample id="480">è¿™æ®µæ–‡å­—è®¨è®ºäº†å½“å‰SimulISTæ¨¡å‹å­˜åœ¨çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæŒ‡å‡ºè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦ç‰¹å®šçš„æ¶æ„è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”å¼•å…¥äº†éœ€è¦ä¼˜åŒ–çš„é¢å¤–æ¨¡å—ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æåˆ°è®­ç»ƒç¨‹åºéå¸¸é•¿ä¸”å¤æ‚ï¼Œä¾‹å¦‚æ¶‰åŠä¸åŒçš„ä¼˜åŒ–ç›®æ ‡ã€‚</sample>
    <sample id="481">What are the problems of the current SimuIST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures (e.g., different optimization objectives). Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s,...)</sample>
    <sample id="482">æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿ</sample>
    <sample id="483">å¹»ç¯ç‰‡å±•ç¤ºäº†å…³äºSimulSTè§£å†³æ–¹æ¡ˆçš„è¯¦ç»†ä¿¡æ¯ã€‚ç¬¬ä¸€ä¸ªè¦ç‚¹å»ºè®®ä½¿ç”¨ç°æœ‰çš„ç¦»çº¿STæ¨¡å‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–é‡‡ç”¨ç‰¹å®šæ¶æ„ã€‚ç¬¬äºŒä¸ªè¦ç‚¹å»ºè®®ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹æ¥å¤„ç†æ¯ä¸ªå»¶è¿Ÿåˆ¶åº¦ï¼Œå¹¶é€šè¿‡ç‰¹å®šå‚æ•°æ¥ç®¡ç†å»¶è¿Ÿã€‚</sample>
    <sample id="484">What is our solution? 01 Use already existing offline ST models without re-training or adopting specific architecture for SimuST. 02 Use only one model for every latency regime and handle latency through specific parameters. 03 Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output that is the cross-attention mechanism. And you can see an example on the right.</sample>
    <sample id="485">å†³å®šæ˜¯å¦å‘å°„æˆ–ä¸å‘å°„éƒ¨åˆ†ç¿»è¯‘ï¼ŒåŸºäºæ³¨æ„åŠ›æŒ‡å‘ä¸€ä¸ªå•è¯ï¼ˆå…¶å’Œä½äºé˜ˆå€¼Î±ï¼‰åˆ°æœ€åä¸€ä¸ªè¯´è¯å¸§ï¼Œè¿™æ„å‘³ç€æ¥æ”¶åˆ°çš„ä¿¡æ¯æ˜¯è¶³å¤Ÿçš„ç¨³å®šçš„ã€‚</sample>
    <sample id="486">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š 1. "Our solution: EDAtt" 2. "Encoder-Decoder Attention" 3. "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold Î±) towards the last Î» speech frames, meaning that the received information is enough stable." 4. "page 014" åœ¨å›¾ç‰‡çš„å³ä¸Šè§’ï¼Œæœ‰ä¸€æ®µæ–‡å­—å†™ç€â€œä»€ ä¹ˆï¼Ÿâ€ã€‚</sample>
    <sample id="487">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š

1. "Our solution: EDAtt"
2. "Encoder-Decoder Attention"
3. "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold a) towards the last Î» speech frames, meaning that the received information is enough stable."
4. "01 I am going to talk about..."
5. "page 014"

è¿™äº›æ–‡å­—ä¸»è¦ä»‹ç»äº†EDAttè§£å†³æ–¹æ¡ˆå’Œç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æœºåˆ¶çš„ç»†èŠ‚ã€‚</sample>
    <sample id="488">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œEDAttâ€çš„è§£å†³æ–¹æ¡ˆçš„å¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡çš„ä¸»è¦éƒ¨åˆ†æ˜¯å…³äºç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æœºåˆ¶çš„è§£é‡Šã€‚å®ƒæŒ‡å‡ºï¼Œå†³å®šæ˜¯å¦éœ€è¦å®Œæ•´çš„ç¿»è¯‘è¿˜æ˜¯éƒ¨åˆ†ç¿»è¯‘ï¼Œå–å†³äºæ³¨æ„åŠ›æ˜¯å¦é›†ä¸­åœ¨å•è¯ä¸Šã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœæ³¨æ„åŠ›æ²¡æœ‰é›†ä¸­åœ¨å•è¯ä¸Šï¼ˆå…¶å’Œä½äºé˜ˆå€¼ï¼‰ï¼Œé‚£ä¹ˆæ¥æ”¶åˆ°çš„ä¿¡æ¯å°±æ˜¯ç¨³å®šçš„ã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªéŸ³é¢‘æ³¢å½¢å›¾ï¼Œä¸‹é¢å†™ç€â€œI am going to talk about...â€å’Œâ€œIch werde reden.â€ï¼Œè¿™è¡¨æ˜è¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€çš„æ¼”ç¤ºã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£æ˜¾ç¤ºäº†ä¸€ä¸ªäººï¼Œå¯èƒ½æ˜¯æ¼”ç¤ºè€…ã€‚å¹»ç¯ç‰‡åº•éƒ¨æœ‰é¡µç â€œpage 016â€ã€‚</sample>
    <sample id="489">å›¾ç‰‡ä¸­çš„æ–‡å­—åŒ…æ‹¬æ ‡é¢˜â€œEncoder-Decoder Attentionâ€å’Œå‰¯æ ‡é¢˜â€œOur solution: EDAttâ€ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€æ®µè§£é‡Šæ–‡æœ¬ï¼Œæåˆ°æ ¹æ®æ³¨æ„åŠ›æ˜¯å¦é›†ä¸­åœ¨å•è¯ä¸Šï¼Œå†³å®šæ˜¯å¦è¾“å‡ºéƒ¨åˆ†ç¿»è¯‘ã€‚å¦‚æœæ³¨æ„åŠ›æ²¡æœ‰é›†ä¸­åœ¨å•è¯ä¸Šï¼Œè¯´æ˜æ¥æ”¶åˆ°çš„ä¿¡æ¯è¶³å¤Ÿç¨³å®šã€‚åº•éƒ¨æœ‰ä¸€ä¸ªå›¾è¡¨ï¼Œæ˜¾ç¤ºäº†â€œich werde redenâ€çš„éŸ³è°ƒå˜åŒ–ï¼Œå¹¶æ ‡æœ‰â€œI am going to talk about...â€ã€‚é¡µé¢å³ä¸‹è§’æ˜¾ç¤ºç¬¬19é¡µã€‚</sample>
    <sample id="490">è¿™è¡¨æ˜å‰ä¸¤ä¸ªå•è¯å°†è¢«çœç•¥ã€‚</sample>
    <sample id="491">å›¾ç‰‡ä¸­çš„è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ä¸ºï¼š æˆ‘ä»¬å°†ç­‰å¾…å¦ä¸€ä¸ªè¯­éŸ³ç‰‡æ®µã€‚</sample>
    <sample id="492">å¹»ç¯ç‰‡æ˜¾ç¤ºäº†â€œEncoder-Decoder Attentionâ€çš„æ¦‚å¿µã€‚å®ƒè§£é‡Šäº†æ¨¡å‹å¦‚ä½•å†³å®šæ˜¯å¦å‘å‡ºæˆ–ä¸å‘å‡ºéƒ¨åˆ†ç¿»è¯‘ï¼Œè¿™å–å†³äºæ³¨æ„åŠ›ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœæ³¨æ„åŠ›ç‚¹çš„æ€»å’Œä½äºé˜ˆå€¼ thresholdï¼Œé‚£ä¹ˆæ¨¡å‹å°†è®¤ä¸ºæ¥æ”¶åˆ°çš„ä¿¡æ¯æ˜¯ç¨³å®šçš„ã€‚</sample>
    <sample id="493">å›¾ç‰‡æ˜¾ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œEncoder-Decoder Attentionâ€çš„æ¼”ç¤ºå¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½æœ‰ä¸€ä¸ªç¼–å·ï¼ˆ01å’Œ02ï¼‰å’Œä¸€ä¸ªæ³¢å½¢å›¾ã€‚åœ¨æ¯ä¸ªéƒ¨åˆ†çš„å·¦ä¾§ï¼Œæœ‰å¾·è¯­çŸ­è¯­â€œIch werde redenâ€å’Œâ€œIch werde Ã¼ber Klima sprechenâ€ã€‚åœ¨æ¯ä¸ªéƒ¨åˆ†çš„å³ä¾§ï¼Œæœ‰ä¸€ä¸ªæ ‡é¢˜ä¸ºâ€œæˆ‘å°†è°ˆè®º...â€çš„è‹±æ–‡å¥å­ã€‚åœ¨æ³¢å½¢å›¾ä¸Šæ–¹ï¼Œæœ‰ä¸€æ®µæ–‡å­—è§£é‡Šäº†â€œEncoder-Decoder Attentionâ€çš„æ¦‚å¿µã€‚è¿™æ®µæ–‡å­—æåˆ°ï¼Œå¦‚æœæ³¨æ„åŠ›ä¸é›†ä¸­åˆ°é˜ˆå€¼ä»¥ä¸‹çš„æŸä¸ªå•è¯ä¸Šï¼Œé‚£ä¹ˆæ¥æ”¶åˆ°çš„ä¿¡æ¯å°±ä¼šç¨³å®šã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£ï¼Œæ˜¾ç¤ºäº†ä¸€ä½å¥³å£«çš„å›¾åƒã€‚å¹»ç¯ç‰‡åº•éƒ¨æ ‡æœ‰é¡µç â€œç¬¬02é¡µâ€ã€‚</sample>
    <sample id="494">æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆæ˜¯ï¼šEDAtt</sample>
    <sample id="495">å¦‚æœæŸ¥çœ‹EADAttçš„ä¸»è¦ç»“æœï¼Œæ‚¨ä¼šçœ‹åˆ°ä¸€ä¸ªå›¾è¡¨ï¼Œæ¨ªè½´è¡¨ç¤ºAL/AL_CAï¼ˆç§’ï¼‰ï¼Œçºµè½´è¡¨ç¤ºBLEUåˆ†æ•°ã€‚</sample>
    <sample id="496">The English text in the image is: 'Main Results: EDAtt' and 'quality measure'.</sample>
    <sample id="497">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š

1. å·¦ä¸Šè§’ï¼šMain Results: EDAtt
2. å›¾è¡¨ä¸‹æ–¹çš„æ ‡ç­¾ï¼š
   - AL/AL_CA (s)
   - latency measure
3. å³ä¸‹è§’ï¼špage 030
4. å›¾è¡¨ä¸Šæ–¹æœ‰ä¸€äº›è“è‰²çš„ç¬¦å·ï¼Œçœ‹èµ·æ¥åƒæ˜¯è¡¨æƒ…ç¬¦å·æˆ–å›¾æ ‡ã€‚

è¿™äº›æ–‡å­—ä¸»è¦æè¿°äº†å›¾è¡¨çš„ä¸»è¦ç»“æœå’Œç›¸å…³æŒ‡æ ‡ã€‚</sample>
    <sample id="498">è¿™æ˜¯ä¸€å¼ æ¼”ç¤ºæ–‡ç¨¿çš„æˆªå›¾ï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œMain Results: EDAttâ€çš„å¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªæŸ±çŠ¶å›¾ï¼ŒYè½´æ ‡è®°ä¸ºâ€œBLEUâ€ï¼ŒXè½´æ ‡è®°ä¸ºâ€œAL/AL_CA (s)â€ã€‚æŸ±çŠ¶å›¾æ˜¾ç¤ºäº†ä»0.5åˆ°6ç§’çš„ä¸åŒæ—¶é—´é—´éš”çš„æ•°æ®ç‚¹ã€‚åœ¨æŸ±çŠ¶å›¾ä¸Šæ–¹ï¼Œæœ‰äº”ä¸ªè“è‰²çš„é—®å·ï¼Œè¡¨ç¤ºå¯èƒ½çš„é—®é¢˜æˆ–è®¨è®ºç‚¹ã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£ï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªæ­£åœ¨è®²è¯çš„äººã€‚å·¦ä¸‹è§’æœ‰ä¸€ä¸ªè“è‰²çš„æ ‡å¿—ï¼Œå¯èƒ½æ˜¯æ¼”ç¤ºæ–‡ç¨¿çš„å“ç‰Œæˆ–ç»„ç»‡æ ‡å¿—ã€‚å³ä¸‹è§’æ ‡æ³¨äº†é¡µé¢ç¼–å·â€œpage 031â€ã€‚</sample>
    <sample id="499">ä¸»è¦ç»“æœï¼šEDAtt</sample>
    <sample id="500">å›¾ä¸­å±•ç¤ºäº†ä¸€ä¸ªæ¼”ç¤ºå¹»ç¯ç‰‡ï¼Œé¡¶éƒ¨æœ‰ä¸€ä¸ªæ ‡é¢˜â€œMain Results: EDAttâ€ã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªå›¾è¡¨ï¼Œæ¨ªè½´æ ‡è®°ä¸ºâ€œAL/AL_CA (s)â€ï¼Œçºµè½´æ ‡è®°ä¸ºâ€œBLEUâ€ã€‚å›¾è¡¨ä¸­æœ‰å››æ¡æ›²çº¿ï¼Œåˆ†åˆ«ä»£è¡¨ä¸åŒçš„ç­–ç•¥ï¼šwait-kã€LAã€CAATå’ŒEDAttã€‚åœ¨å›¾è¡¨ä¸Šæ–¹ï¼Œæœ‰ä¸€æ®µæ–‡å­—è¯´æ˜â€œpopular strategies also applied to offline modelsâ€ã€‚å³ä¸‹è§’æœ‰ä¸€ä¸ªé¡µé¢ç¼–å·â€œpage 033â€ã€‚</sample>
    <sample id="501">ä¸»è¦ç»“æœï¼šEDAtt</sample>
    <sample id="502">ç”»é¢ä¸­å±•ç¤ºäº†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œMain Results: EDAttâ€ã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªå›¾è¡¨ï¼Œæ˜¾ç¤ºäº†ä¸åŒç­–ç•¥åœ¨BLEUåˆ†æ•°ä¸Šçš„è¡¨ç°ã€‚å›¾è¡¨çš„æ¨ªè½´è¡¨ç¤ºAL/AL_CAï¼ˆä»1.7åˆ°5.6ï¼‰ï¼Œçºµè½´è¡¨ç¤ºBLEUåˆ†æ•°ï¼ˆä»21åˆ°27ï¼‰ã€‚å›¾ä¸­æœ‰äº”æ¡æ›²çº¿ï¼Œåˆ†åˆ«ä»£è¡¨ä¸åŒçš„ç­–ç•¥ï¼šwait-kã€LAã€CAATå’ŒEDAttã€‚å…¶ä¸­ï¼ŒEDAttçš„æ›²çº¿åœ¨æ‰€æœ‰ç­–ç•¥ä¸­è¡¨ç°æœ€ä½³ã€‚å¹»ç¯ç‰‡å·¦ä¸‹è§’æœ‰ä¸€æ®µæ–‡å­—è¯´æ˜ï¼šâ€œEDAtt outperforms all the strategies applied to offline modelsâ€ï¼Œæ„æ€æ˜¯EDAttåœ¨ç¦»çº¿æ¨¡å‹ä¸­ä¼˜äºæ‰€æœ‰å…¶ä»–ç­–ç•¥ã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£ï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªæ­£åœ¨è®²è¯çš„äººã€‚å¹»ç¯ç‰‡åº•éƒ¨æ˜¾ç¤ºè¿™æ˜¯ç¬¬34é¡µã€‚</sample>
    <sample id="503">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œEDAttâ€ã€‚å¹»ç¯ç‰‡åŒ…å«ä¸€ä¸ªå›¾è¡¨å’Œä¸€äº›æ–‡æœ¬ã€‚å›¾è¡¨æ˜¾ç¤ºäº†ä¸åŒç­–ç•¥åœ¨ç¿»è¯‘ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¨ªè½´è¡¨ç¤ºAL/CAçš„æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œçºµè½´è¡¨ç¤ºBLEUåˆ†æ•°ã€‚å›¾ä¾‹ä¸­åˆ—å‡ºäº†å››ç§ç­–ç•¥ï¼šwait-kã€LAã€CAATå’ŒEDAttã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªè“è‰²çš„æ³¨é‡Šæ¡†ï¼Œå†™ç€â€œå¦‚æœè€ƒè™‘å®é™…è€—æ—¶ï¼ŒEDAttæ˜¯æœ€å¿«çš„ç­–ç•¥ã€‚â€å¹»ç¯ç‰‡åº•éƒ¨æœ‰é¡µç 307ã€‚</sample>
    <sample id="504">ç”»é¢ä¸­å±•ç¤ºäº†ä¸€å¼ å¹»ç¯ç‰‡ï¼ŒèƒŒæ™¯ä¸ºç™½è‰²ã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªäºŒç»´ç å’Œä¸€äº›è”ç³»ä¿¡æ¯ã€‚é¡¶éƒ¨æœ‰ä¸€è¡Œæ–‡å­—â€œDo you want to discover more?â€ï¼Œä¸‹é¢æ˜¯ä¸€è¡Œæ›´å¤§çš„æ–‡å­—â€œRead our paper to discover more results!â€ã€‚åœ¨å·¦ä¾§ï¼Œæœ‰ä¸‰ä¸ªç¤¾äº¤åª’ä½“è´¦å·ï¼š@spapi,neæ ¼ri@fbk.euã€marco.turchi@gmail.comã€github.com/hlt-mt/fairseqã€‚åœ¨å³ä¾§ï¼Œæœ‰ä¸€ä¸ªäºŒç»´ç ï¼Œä¸Šé¢å†™ç€â€œScan me!â€ã€‚å¹»ç¯ç‰‡çš„å³ä¸‹è§’æ ‡æœ‰â€œpage 038â€ã€‚</sample>
    <sample id="505">Yes, the dataset is publicly available.</sample>
    <sample id="506">The video features a presentation slide with the title "MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning" prominently displayed in white text against a black background. Below the title, three names are listed as contributors to this research project: Zhiyang Xu*, Ying Shen*, and Lifu Huang, all affiliated with the Department of Computer Science at Virginia Tech. The asterisks next to their names indicate that they have made an equal contribution. In the bottom left corner, there is additional text that reads "*Equal Contribution." Additionally, the top right corner displays a logo or emblem consisting of red letters 'VT' on a purple background. At the bottom of the slide, there are images of four individuals who appear to be associated with the presentation or study.</sample>
    <sample id="507">Figure 2: Comparing instruction tuning with pretrain-finetune and prompting.</sample>
    <sample id="508">è¿™æ®µæ–‡å­—è®¨è®ºäº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚å®ƒæåˆ°ï¼Œè®¸å¤šç ”ç©¶è¡¨æ˜ï¼ŒæŒ‡ä»¤è°ƒä¼˜ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹é€šè¿‡éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ç¤ºæ¥æ‰§è¡Œæœªè§è¿‡çš„ä»»åŠ¡ã€‚</sample>
    <sample id="509">è¿™æ®µæ–‡å­—è®¨è®ºäº†è¯­è¨€-onlyä»»åŠ¡ï¼ŒæŒ‡å‡ºå¤§å¤šæ•°å…ˆå‰çš„å·¥ä½œé›†ä¸­åœ¨æé«˜é›¶-shotæ€§èƒ½ä¸Šã€‚</sample>
    <sample id="510">Instruction Tuning on Multimodal Pre-trained Models</sample>
    <sample id="511">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="512">Imbalance in Instructional Datasets between NLP and Multimodal 1600+ Language-only instruction tasks NO large-scale, publicly-available multimodal instruction tasks Wang, Yizhong, et al. "Benchmarking generalization via in-context instructions on 1600+ language tasks" arXiv preprint arXiv:2304.08975</sample>
    <sample id="513">MULTINSTRUCT The first multimodal instruction tuning benchmark dataset 62 diverse multimodal tasks 10 broad groups 5 expert-written instructions</sample>
    <sample id="514">è¿™æ®µæ–‡å­—ä»‹ç»äº†MULTINSTRUCTï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜åŸºå‡†æ•°æ®é›†ã€‚å®ƒåŒ…å«äº†62ä¸ªä¸åŒçš„å¤šæ¨¡æ€ä»»åŠ¡ã€10ä¸ªå¤§ç±»å’Œ5ä¸ªç”±ä¸“å®¶æ’°å†™çš„è¯´æ˜ã€‚å›¾ä¸­å±•ç¤ºäº†å„ç§ä»»åŠ¡çš„åˆ†ç±»ï¼ŒåŒ…æ‹¬è§†è§‰å…³ç³»ã€VQAã€æ—¶é—´å®šå‘ã€æ¥åœ°ã€æ¥åœ°åŒ¹é…ã€æ‚é¡¹ã€å›¾åƒç†è§£ã€åŒºåŸŸç†è§£ã€æ–‡æœ¬åŒ¹é…ç­‰ã€‚æ¯ä¸ªä»»åŠ¡éƒ½é…å¤‡äº†5ä¸ªä¸“å®¶æ’°å†™çš„è¯´æ˜ã€‚</sample>
    <sample id="515">OFA stands for One For All. It is a unified multi-modal pre-trained model that can perform both understanding and generation tasks with single or multiple modalities, including language, image tokens, and the coordinates of a bounding box. The model uses a unified vocabulary to handle these different types of inputs effectively.</sample>
    <sample id="516">å›¾ä¸­å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œMULTINSTRUCTâ€çš„æ•°æ®é›†ï¼Œå®ƒåŒ…å«å››ä¸ªä»»åŠ¡çš„å®ä¾‹ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬ï¼š1. åŸºäºå›¾åƒçš„æè¿°ï¼ˆGrounded Captioningï¼‰ï¼šè¦æ±‚ä¸ºå›¾ç‰‡ç”Ÿæˆä¸€ä¸ªæè¿°ã€‚2. å›¾åƒä½ç½®åŒ–ï¼ˆText Localizationï¼‰ï¼šéœ€è¦é€‰æ‹©åŒ…å«ç‰¹å®šæ–‡æœ¬çš„åŒºåŸŸã€‚3. æŒ‡ç¤ºè¡¨è¾¾ï¼ˆRefering Expression Selectionï¼‰ï¼šä»å¤šä¸ªé€‰é¡¹ä¸­é€‰æ‹©æè¿°æ­£ç¡®å¯¹è±¡çš„é€‰é¡¹ã€‚4. å›¾åƒ-é—®é¢˜åŒ¹é…ï¼ˆQuestion-Image Matchingï¼‰ï¼šæ ¹æ®ç»™å®šçš„å›¾åƒå’Œé—®é¢˜ï¼Œåˆ¤æ–­é—®é¢˜æ˜¯å¦ä¸å›¾åƒç›¸å…³ã€‚æ¯ä¸ªä»»åŠ¡éƒ½æœ‰ç›¸åº”çš„è¾“å…¥å’Œè¾“å‡ºç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•åº”ç”¨è¿™äº›ä»»åŠ¡ã€‚</sample>
    <sample id="517">å›¾ä¸­å±•ç¤ºäº†ä¸€ä¸ªåä¸ºMULTINSTRUCTçš„ç³»ç»Ÿï¼Œå®ƒå±•ç¤ºäº†å¦‚ä½•å¤„ç†å„ç§è¾“å…¥å’Œè¾“å‡ºæ•°æ®ç±»å‹ã€‚ç³»ç»ŸåŒ…æ‹¬å››ä¸ªä»»åŠ¡ï¼š1. åŸºäºå›¾åƒçš„æè¿°ï¼šç»™å®šä¸€ä¸ªå›¾åƒï¼Œç”Ÿæˆä¸€ä¸ªæè¿°ã€‚2. æ–‡æœ¬ä½ç½®åŒ–ï¼šé€‰æ‹©åŒ…å«ç‰¹å®šæ–‡æœ¬ï¼ˆå¦‚â€œdenâ€ï¼‰çš„åŒºåŸŸã€‚3. æŒ‡ç¤ºè¡¨è¾¾ï¼šè¯†åˆ«å¹¶æè¿°ç‰¹å®šå¯¹è±¡ï¼ˆå¦‚ç«è½¦ï¼‰ã€‚4. å›¾åƒ-é—®é¢˜åŒ¹é…ï¼šæ ¹æ®å›¾åƒå†…å®¹å›ç­”é—®é¢˜ã€‚æ¯ä¸ªä»»åŠ¡éƒ½æœ‰ç›¸åº”çš„è¾“å…¥å’Œè¾“å‡ºç¤ºä¾‹ï¼Œå±•ç¤ºäº†ç³»ç»Ÿå¦‚ä½•å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®ã€‚</sample>
    <sample id="518">Figure 1: Example Instances from MULTINSTRUCT for Four Tasks.</sample>
    <sample id="519">è¿™æ®µå†…å®¹ä»‹ç»äº†ä¸€ä¸ªä¸»é¢˜ï¼Œå«åšâ€œMulti-modal Instruction Tuningâ€ã€‚</sample>
    <sample id="520">è¿™æ®µæ–‡å­—è®¨è®ºäº†æ„å»ºå¤šæ¨¡æ€æŒ‡ä»¤è½¬æ¢æ•°æ®é›†çš„è¿‡ç¨‹ã€‚åœ¨è®­ç»ƒæ•°æ®é›†çš„æ„å»ºä¸­ï¼Œä½¿ç”¨äº†9ä¸ªç¾¤ä½“ä¸­çš„53é¡¹ä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä»æ¯ä¸ªä»»åŠ¡ä¸­æŠ½å–10,000ä¸ªå®ä¾‹ã€‚åœ¨æµ‹è¯•æ•°æ®é›†çš„æ„å»ºä¸­ï¼Œæ•´ä¸ªå¸¸è¯†æ¨ç†ç»„è¢«ä¿ç•™ç”¨äºæµ‹è¯•ï¼Œå¦å¤–ä»VQAå’Œæ‚é¡¹ç»„ä¸­é€‰æ‹©äº†5é¡¹ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰ä»»åŠ¡çš„å®ä¾‹éƒ½ç”¨äºæ¯ä¸ªä»»åŠ¡çš„æµ‹è¯•åˆ†å‰²ã€‚æœ€åï¼Œä»æµ‹è¯•åˆ†å‰²ä¸­éšæœºæŠ½å–äº†20é¡¹ä»»åŠ¡ä½œä¸ºè‡ªç„¶æŒ‡ä»¤æ•°æ®é›†çš„æœªæ ‡è®°ä»»åŠ¡ï¼Œç”¨äºNLPã€‚</sample>
    <sample id="521">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ä»¥ä¸‹å‡ ç‚¹ï¼š 1. æ ‡é¢˜ï¼šMulti-Modal Instruction Turning 2. åŸ¹è®­æ•°æ®é›†æ„å»ºï¼š - ä½¿ç”¨9ä¸ªç»„çš„53é¡¹ä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚ - æ¯ä¸ªä»»åŠ¡é‡‡æ ·10,000ä¸ªå®ä¾‹ã€‚ 3. æµ‹è¯•æ•°æ®é›†æ„å»ºï¼š - ä¿ç•™å¸¸è¯†æ¨ç†ç»„ç”¨äºæµ‹è¯•ã€‚ - ä»VQAå’Œæ‚é¡¹ç±»åˆ«ä¸­é€‰æ‹©é¢å¤–çš„5é¡¹ä»»åŠ¡ã€‚ - å¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œä½¿ç”¨æµ‹è¯•åˆ‡ç‰‡ä¸­çš„æ‰€æœ‰å®ä¾‹ã€‚ - éšæœºä»Natural Instructionsæ•°æ®é›†ä¸­é‡‡æ ·20é¡¹ä»»åŠ¡ä½œä¸ºNLPçš„æœªè§ä»»åŠ¡ã€‚</sample>
    <sample id="522">è¿™æ®µå†…å®¹æè¿°äº†æ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•ç»†èŠ‚ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äº†ä¸€ä¸ªé¢„è®­ç»ƒçš„OFAå¤§å‹æ¨¡å‹ï¼ˆ472Mï¼‰ï¼Œå°†æ‰€æœ‰ä»»åŠ¡çš„å®ä¾‹æ··åˆåœ¨ä¸€èµ·ï¼Œå¹¶éšæœºä¸å…¶ä¸­ä¸€ä¸ªæŒ‡ä»¤æ¨¡æ¿ç»“åˆã€‚åœ¨æµ‹è¯•ä¸­ï¼Œå¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œäº†äº”æ¬¡å®éªŒï¼Œé€šè¿‡è¯„ä¼°æ¨¡å‹æ¥åˆ¤æ–­æ¯ä¸ªæŒ‡ä»¤æ¨¡æ¿çš„æ•ˆæœï¼Œå¹¶æŠ¥å‘Šäº†äº”æ¬¡å®éªŒä¸­çš„å¹³å‡å€¼ã€æœ€å¤§å€¼ä»¥åŠæ€§èƒ½çš„æ ‡å‡†å·®ã€‚</sample>
    <sample id="523">è¿™æ®µæ–‡å­—ä»‹ç»äº†æ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•ç»†èŠ‚ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨äº†ä¸€ä¸ªé¢„è®­ç»ƒçš„OFAå¤§å‹æ¨¡å‹ï¼ˆ472Mï¼‰ï¼Œå°†æ‰€æœ‰å®ä¾‹æ··åˆåœ¨ä¸€èµ·ï¼Œå¹¶éšæœºç»„åˆåˆ°å…¶ä¸­ä¸€ç§æŒ‡ä»¤æ¨¡æ¿ä¸­ã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œå¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œä¼šè¿›è¡Œäº”æ¬¡å®éªŒï¼Œé€šè¿‡ä½¿ç”¨æ¯ç§æŒ‡ä»¤ä¸­çš„ä¸€ä¸ªæ¥è¯„ä¼°æ¨¡å‹ã€‚ç„¶åæŠ¥å‘Šäº”ä¸ªå®éªŒä¸­çš„å¹³å‡å€¼ã€æœ€å¤§å€¼ä»¥åŠæ€§èƒ½çš„æ ‡å‡†åå·®ã€‚</sample>
    <sample id="524">å®æ–½ç»†èŠ‚ Training details: ä½¿ç”¨é¢„è®­ç»ƒçš„OFAå¤§å‹æ¨¡å‹ï¼ˆ472Mï¼‰æ··åˆæ‰€æœ‰å®ä¾‹ç”¨äºæ‰€æœ‰ä»»åŠ¡ã€‚æ¯ä¸ªå®ä¾‹éƒ½éšæœºä¸å…¶ä¸­ä¸€ä¸ªæŒ‡ä»¤æ¨¡æ¿ç»„åˆã€‚ Testing details: å¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œæˆ‘ä»¬è¿›è¡Œäº”æ¬¡å®éªŒæ¥è¯„ä¼°æ¨¡å‹ï¼Œä½¿ç”¨æ¯ä¸ªä»»åŠ¡ä¸­çš„ä¸€ä¸ªæŒ‡ä»¤ã€‚æˆ‘ä»¬å°†æŠ¥å‘Šäº”ä¸ªå®éªŒä¸­çš„å¹³å‡å€¼å’Œæœ€å¤§æ€§èƒ½ï¼Œå¹¶æŠ¥å‘Šäº”ä¸ªå®éªŒä¸­æ€§èƒ½çš„æ ‡å‡†åå·®ã€‚</sample>
    <sample id="525">è¿™æ®µæ–‡å­—ä»‹ç»äº†ç”¨äºè¯„ä¼°å¤šæ¨¡æ€ä»»åŠ¡çš„æŒ‡æ ‡ã€‚å¯¹äºå¤šæ¨¡æ€åˆ†ç±»ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†è§‰è•´å«ã€è§†è§‰ç©ºé—´æ¨ç†ã€è‡ªç„¶è¯­è¨€è§†è§‰æ¨ç†å’Œç¾éš¾ç±»å‹åˆ†ç±»ï¼ŒæŠ¥å‘Šå‡†ç¡®ç‡ã€‚å¯¹äºå¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ï¼Œå¦‚å¸¸è¯†è§†è§‰é—®ç­”ã€æ–‡æœ¬åˆ°è§†è§‰ã€åŸºäºè§†è§‰çš„é—®ç­”ã€è§†è§‰æ–‡æœ¬æå–å’Œè§†è§‰å¯¹è¯ï¼ŒæŠ¥å‘ŠRouge-Lã€‚å¯¹äºNLPä»»åŠ¡ï¼ŒæŠ¥å‘ŠRouge-Lã€‚æ­¤å¤–ï¼Œè¿˜è®¡ç®—äº†æ¯ä¸ªæ¨¡å‹åœ¨æ‰€æœ‰å¤šæ¨¡æ€å’ŒNLPæœªè§ä»»åŠ¡ä¸Šçš„ç»¼åˆæ€§èƒ½ã€‚ä½¿ç”¨Rouge-Lä½œä¸ºå¤§å¤šæ•°ä»»åŠ¡çš„æ€§èƒ½è¯„åˆ†ï¼Œè€ŒAccuracyä»…ä½œä¸ºåº¦é‡æ ‡å‡†ã€‚</sample>
    <sample id="526">è¿™æ®µå†…å®¹è®¨è®ºäº†æ¨¡å‹å¯¹åŒä¸€ä»»åŠ¡çš„å¤šç§æŒ‡ä»¤çš„æ•æ„Ÿæ€§ã€‚å®ƒå¼ºè°ƒäº†æ¨¡å‹åœ¨ä¸åŒæªè¾ä¸‹ä¿æŒä¸€è‡´ç»“æœçš„èƒ½åŠ›ã€‚</sample>
    <sample id="527">è¿™æ®µæ–‡å­—è®¨è®ºäº†åœ¨MULTIINSTRUCTä¸­å¯¹æŒ‡ä»¤è¿›è¡Œå¾®è°ƒçš„æœ‰æ•ˆæ€§ã€‚å®ƒæåˆ°äº†åœ¨å¤šæ¨¡æ€é—®é¢˜å›ç­”ä»»åŠ¡ï¼ˆMultimodal Compositional Question Answeringï¼‰ä¸­çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹¶æŒ‡å‡ºæœ€ä½³ç»“æœç”¨ç²—ä½“æ˜¾ç¤ºã€‚è¡¨æ ¼å±•ç¤ºäº†åœ¨å„ç§ä»»åŠ¡ä¸Šä½¿ç”¨ä¸åŒæ–¹æ³•ï¼ˆå¦‚OFAã€Transfer Learning from Natural Instructionså’ŒZero-shot Performanceï¼‰çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å¹³å‡å€¼ã€æ ‡å‡†å·®å’Œæœ€å¤§å€¼ã€‚</sample>
    <sample id="528">Effectiveness of Instruction Tuning on MULTIINSTRUCT</sample>
    <sample id="529">Here we can see as the amount of task increases, the model achieves better performance and in the meantime a lower sensitivity.</sample>
    <sample id="530">è¿™æ®µæ–‡å­—è®¨è®ºäº†OFAåœ¨ä¸åŒæ•°é‡çš„æŒ‡ä»¤ä¸‹è¿›è¡Œå¾®è°ƒçš„å½±å“ã€‚å®ƒæŒ‡å‡ºï¼Œå½“OFAåœ¨5ä¸ªæŒ‡ä»¤ä¸‹è¿›è¡Œå¾®è°ƒæ—¶ï¼Œå…¶åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”å¯¹å˜åŒ–çš„æ•æ„Ÿæ€§é™ä½ã€‚è¡¨æ ¼æ˜¾ç¤ºï¼Œåœ¨1ä¸ªæŒ‡ä»¤ä¸‹ï¼Œèšåˆæ€§èƒ½ä¸º42.81ï¼Œè€Œåœ¨5ä¸ªæŒ‡ä»¤ä¸‹ï¼Œèšåˆæ€§èƒ½æé«˜åˆ°47.82ã€‚æ­¤å¤–ï¼Œæ•æ„Ÿæ€§ä»24.62é™ä½åˆ°10.45ã€‚</sample>
    <sample id="531">è¿™æ®µæ–‡å­—è®¨è®ºäº†å¾®è°ƒç­–ç•¥å¯¹æ¨¡å‹æ•æ„Ÿæ€§çš„å½±å“ã€‚å®ƒæŒ‡å‡ºï¼Œåœ¨Multinstructä¸Šè¿›è¡Œçš„å¾®è°ƒå¯ä»¥æ˜¾è‘—å‡å°‘OFAæ¨¡å‹çš„æ•æ„Ÿæ€§ï¼Œå¹¶ä¸”ä»è‡ªç„¶æŒ‡ä»¤æ•°æ®é›†ä¸­è¿›è¡Œè¿ç§»å­¦ä¹ å¯ä»¥è¿›ä¸€æ­¥é™ä½æ¨¡å‹çš„æ•æ„Ÿæ€§ã€‚å›¾è¡¨æ˜¾ç¤ºäº†ä¸åŒå¾®è°ƒç­–ç•¥ä¸‹æ¨¡å‹åœ¨æœªè§è¿‡çš„è¯„ä¼°ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ•°å€¼è¶Šä½è¡¨ç¤ºè¡¨ç°è¶Šå¥½ã€‚</sample>
    <sample id="532">Zero-Shot Performance on NLP Tasks Instruction Tuning on Multinstruct can improve zero-shot performance on unseen NLP tasks. The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset.</sample>
    <sample id="533">è¿™æ®µæ–‡å­—è®¨è®ºäº†å…³äºå¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†çš„ç»“è®ºã€‚å®ƒæåˆ°ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª10ä¸ªå¤§ç±»çš„62ä¸ªæ¨¡æ€ä»»åŠ¡ã€‚è¿™ä¸ªæ•°æ®é›†é€šè¿‡æŒ‡ä»¤è°ƒä¼˜æ˜¾è‘—æé«˜äº†OFAçš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶æ¢ç´¢äº†å‡ ç§è½¬ç§»å­¦ä¹ æŠ€æœ¯å¹¶å±•ç¤ºäº†å®ƒä»¬çš„å¥½å¤„ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªæ–°çš„åº¦é‡æ ‡å‡†å«ä½œâ€œsensitivityâ€ã€‚</sample>
    <sample id="534">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š

æ ‡é¢˜ï¼šOne More Thing!

æ­£æ–‡ï¼š
We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon!

æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªäºŒç»´ç å’Œä¸€ä¸ªç©¿ç€æµ…è‰²ä¸Šè¡£çš„äººçš„éƒ¨åˆ†å›¾åƒã€‚</sample>
    <sample id="535">The authors of the paper belong to the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="536">æ¼”è®²è€…çš„åå­—æ˜¯Mohammad Javad Hosseiniã€‚</sample>
    <sample id="562">è¿™æ®µè‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡æ˜¯ï¼š è¯­è¨€æ¨¡å‹çš„å¯æ¥å—æ€§åˆ¤æ–­å¹¶ä¸æ€»æ˜¯å¯¹ä¸Šä¸‹æ–‡æœ‰é²æ£’æ€§ã€‚</sample>
    <sample id="563">è¿™æ˜¯ä¸€å¼ å­¦æœ¯ä¼šè®®çš„å¹»ç¯ç‰‡ï¼Œä¸»è¦è®¨è®ºäº†è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè¯­å¢ƒä¸‹çš„æ¥å—åº¦è¯„ä¼°ã€‚æ ‡é¢˜ä¸ºâ€œLanguage model acceptability judgements are not always robust to contextâ€ï¼Œè¡¨æ˜ç ”ç©¶å‘ç°è¯­è¨€æ¨¡å‹çš„æ¥å—åº¦è¯„ä¼°åœ¨ä¸åŒè¯­å¢ƒä¸‹å¹¶ä¸æ€»æ˜¯å…·æœ‰é²æ£’æ€§ã€‚å¹»ç¯ç‰‡ä¸Šåˆ—å‡ºäº†å‚ä¸è¿™é¡¹ç ”ç©¶çš„å‡ ä½å­¦è€…çš„åå­—ï¼šKousuv Sinhaã€Jon Gauthierã€Aaron Muellerã€Kanishka Mishraã€Keren Fuentesã€Roger Levyå’ŒAdina Williamsã€‚æ­¤å¤–ï¼Œå¹»ç¯ç‰‡ä¸Šè¿˜å±•ç¤ºäº†çº¦ç¿°éœæ™®é‡‘æ–¯å¤§å­¦ã€æ™®æ¸¡å¤§å­¦å’Œéº»çœç†å·¥å­¦é™¢çš„æ ‡å¿—ï¼Œè¡¨æ˜è¿™äº›æœºæ„å¯èƒ½å‚ä¸äº†è¿™é¡¹ç ”ç©¶ã€‚</sample>
    <sample id="564">åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†æœ€å°å¯¹å¶èŒƒå¼ã€‚</sample>
    <sample id="565">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œé‡æ–°å®¡è§†æœ€å°å¯¹èŒƒå¼â€ã€‚</sample>
    <sample id="566">Revisiting Minimal Pair Paradigm</sample>
    <sample id="567">Revisiting Minimal Pair Paradigm</sample>
    <sample id="568">Revisiting Minimal Pair Paradigm</sample>
    <sample id="569">Revisiting Minimal Pair Paradigm</sample>
    <sample id="570">Revisiting Minimal Pair Paradigm</sample>
    <sample id="571">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability</sample>
    <sample id="572">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability</sample>
    <sample id="573">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability</sample>
    <sample id="574">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability</sample>
    <sample id="575">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability</sample>
    <sample id="576">å›¾ç‰‡ä¸­çš„æ–‡å­—æ˜¯â€œApproachâ€ã€‚</sample>
    <sample id="577">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability.</sample>
    <sample id="578">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability Wikipedia, Unrelated</sample>
    <sample id="579">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability Wikipedia, Unrelated</sample>
    <sample id="580">æµ‹è¯•MPPåˆ¤æ–­æ˜¯å¦å–å†³äºä¸Šä¸‹æ–‡é•¿åº¦ã€ç»“æ„åŒ¹é…å’Œå¯æ¥å—æ€§ã€‚</sample>
    <sample id="581">MPPåˆ¤æ–­åœ¨ä»»æ„ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹éƒ½æ˜¯å¥å£®çš„ã€‚</sample>
    <sample id="582">MPP judgement are robust for arbitrary context lengths.</sample>
    <sample id="583">ç°åœ¨ï¼Œå½“æˆ‘ä»¬åœ¨åŒä¸€ä¸ªæ•°æ®é›†ä¸­é€‰æ‹©å¥å­æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ</sample>
    <sample id="584">æ ‡é¢˜ï¼šå¯æ¥å—/ä¸å¯æ¥å—çš„MPPå¥å­åœ¨ä¸Šä¸‹æ–‡ä¸­å½±å“åˆ¤æ–­æ€§èƒ½ 1. æ®µè½å†…å®¹ï¼šæˆ‘ä»¬å¯¹ä¸åŒä¸Šä¸‹æ–‡ä¸‹çš„MPPè¯„ä¼°è¿›è¡Œäº†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¯æ¥å—å’Œä¸å¯æ¥å—çš„æƒ…å†µã€‚è¿™äº›å¥å­çš„é•¿åº¦å¯ä»¥è¾¾åˆ°900ä¸ªæ ‡è®°ã€‚ 2. å›¾è¡¨åˆ†æï¼šå›¾è¡¨æ˜¾ç¤ºäº†ä¸åŒä¸Šä¸‹æ–‡ä¸‹çš„æ€§èƒ½å˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæœ‰ä¸¤ä¸ªä¸»è¦çš„ä¸Šä¸‹æ–‡ï¼ˆæ ‡è®°ä¸ºâ€œ2â€ï¼‰ï¼Œå®ƒä»¬çš„æ€§èƒ½ç›¸å¯¹è¾ƒé«˜ã€‚å…¶ä»–ä¸Šä¸‹æ–‡ï¼ˆæ ‡è®°ä¸ºâ€œ1â€ï¼‰çš„æ€§èƒ½è¾ƒä½ã€‚ 3. å¥å­ç¤ºä¾‹ï¼š - â€œThere was a documentary about music in the past. There were no working hard, but there might be Rose from this before to you seeing the customer?â€ - â€œThere was a documentary about music inflating. There might be Rose from this before to you seeing the customer?â€ - â€œThere was each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating.</sample>
    <sample id="585">å›¾ç‰‡ä¸­çš„è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡æ˜¯ï¼šå¯æ¥å—/ä¸å¯æ¥å—çš„MPPå¥å­åœ¨ä¸Šä¸‹æ–‡ä¸­å½±å“åˆ¤æ–­æ€§èƒ½ã€‚æˆ‘ä»¬å¯¹ä¸åŒä¸Šä¸‹æ–‡ä¸‹çš„MPPè¯„ä¼°è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å¯æ¥å—/ä¸å¯æ¥å—çš„ã€é•¿åº¦ä¸è¶…è¿‡900ä¸ªæ ‡è®°çš„åŒ¹é…ç»“æ„ã€‚</sample>
    <sample id="586">Blimp Opt 6.7B</sample>
    <sample id="587">The image contains a graph with the title 'BLIMP, OPT 6.7B'. The x-axis is labeled 'Prefix Strategy' and has various strategies listed: 'Un.' (Unacceptable), 'Acc.' (Acceptable), 'Wiki', and 'Un.' again. The y-axis represents some form of performance metric, ranging from -0.2 to 0.2.

There are three lines on the graph:
1. A red line representing 'Un.' strategy.
2. A green line for 'Acc.' strategy.
3. A blue dashed line for 'Wiki'.

Above the graph, there is text that reads: 'Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance.'

Below this main statement, additional text explains: 'We perform MPP evaluations with different contexts â€“ acceptable / unacceptable; matched/mismatched structure â€“ of lengths up to 900 tokens.'

To the right of the graph, there are four numbered questions related to context:
1. "What could Jessica sell before Rachel noticed her spotlights?"
2. "What had Jessica said about the cleaning service before returning to this customer?"
3. "Jessica was cleaning the museum's spotlights when she realized what she was doing."
4. "What should Jessica sell these breakfasts before Rachel noticed them in front of this customer?"

At the bottom left corner of the image, it states: 'BLIMP, OPT 6.7B.'

Overall, the image appears to be presenting data and analysis regarding the impact of sentence structures on model performance in natural language processing tasks.</sample>
    <sample id="588">å¯æ¥å—/ä¸å¯æ¥å—çš„MPPå¥å­å…·æœ‰åŒ¹é…ç»“æ„ï¼Œå¯¹æ¨¡å‹æ€§èƒ½å½±å“æœ€å¤§ã€‚</sample>
    <sample id="589">ä¸ºä»€ä¹ˆåŒ¹é…å‰ç¼€ä¼šå½±å“LMåˆ¤æ–­ï¼Ÿ æˆ‘ä»¬é€šè¿‡ä¿ç•™ç›¸å…³ç»“æ„æ¥æ‰°åŠ¨ä¸Šä¸‹æ–‡å¥å­ï¼Œå¹¶è¯¢é—®æ¨¡å‹æ˜¯å¦å¯¹è¿™äº›å¥å­æ•æ„Ÿã€‚ - å‰ç¼€/åç¼€å‰¯è¯ï¼šâ€œç„¶è€Œï¼Œ&lt;sent&gt;ã€‚â€ - é•¿å‰ç¼€å‰¯è¯ï¼šâ€œé¦–å…ˆå’Œæœ€é‡è¦çš„æ˜¯ï¼Œ&lt;sent&gt;ã€‚â€ - æ·»åŠ å­å¥ï¼šâ€œæ— è®ºXè®¤ä¸ºä»€ä¹ˆï¼Œ&lt;sent&gt;ã€‚â€ - å¼•ç”¨ï¼šâ€œæ˜¨å¤©ï¼ŒXè¯´ï¼Œ&lt;sent&gt;ã€‚â€</sample>
    <sample id="590">å›¾ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š 1. æ ‡é¢˜ï¼šä¸ºä»€ä¹ˆåŒ¹é…å‰ç¼€ä¼šå½±å“LMåˆ¤æ–­ï¼Ÿ 2. å‰¯æ ‡é¢˜ï¼šæˆ‘ä»¬é€šè¿‡ä¿ç•™ç›¸å…³ç»“æ„æ¥æ‰°åŠ¨ä¸Šä¸‹æ–‡å¥å­ï¼Œå¹¶é—®æ¨¡å‹æ˜¯å¦å¯¹è¿™äº›å¥å­æ•æ„Ÿã€‚ 3. åˆ—è¡¨é¡¹ï¼š - å‰ç¼€/åç¼€å‰¯è¯ï¼šâ€œç„¶è€Œï¼Œ&lt;sent&gt;ã€‚â€ - é•¿å‰ç¼€å‰¯è¯ï¼šâ€œé¦–å…ˆå’Œä¸»è¦ï¼Œ&lt;sent&gt;ã€‚â€ - æ·»åŠ å­å¥ï¼šâ€œæ— è®ºXè®¤ä¸ºä»€ä¹ˆï¼Œ&lt;sent&gt;ã€‚â€ - å¼•ç”¨ï¼šâ€œæ˜¨å¤©ï¼ŒXè¯´ï¼Œ&lt;sent&gt;ã€‚â€ 4. å³ä¸‹è§’çš„å›¾è¡¨æ ‡é¢˜ï¼šæ—  5. å›¾è¡¨ä¸Šçš„æ ‡ç­¾ï¼š - æŒºå‡ - æ²¡æœ‰ - å‰ç¼€å‰¯è¯ - ä¸»è¦å‰¯è¯ - æ·»åŠ å­å¥ - æ‰€æœ‰ - å¯æ¥å— - ä¸å¯æ¥å— 6. xè½´æ ‡ç­¾ï¼šè¾“å…¥é•¿åº¦ 7. yè½´æ ‡ç­¾ï¼šÎ”å‡†ç¡®æ€§</sample>
    <sample id="591">æˆ‘ä»¬å‘ç°ï¼Œè¿™äº›å™ªå£°ä¸­çš„ä»»ä½•ä¸€ä¸ªéƒ½ä¸ä¼šä½¿æ¨¡å‹çš„æ€§èƒ½å‘ç”Ÿå˜åŒ–ã€‚</sample>
    <sample id="592">ä¸ºä»€ä¹ˆåŒ¹é…çš„å‰ç¼€ä¼šå½±å“LMåˆ¤æ–­ï¼Ÿ</sample>
    <sample id="593">ä¸ºä»€ä¹ˆåŒ¹é…å‰ç¼€ä¼šå½±å“LMåˆ¤æ–­ï¼Ÿ æˆ‘ä»¬ä»¥ä¿æŒä¸Šä¸‹æ–‡å¥å­ç»“æ„çš„æ–¹å¼æ‰°åŠ¨å¥å­ï¼Œå¹¶é—®æ¨¡å‹æ˜¯å¦å¯¹è¿™äº›å¥å­æ•æ„Ÿã€‚</sample>
    <sample id="594">å…³é”®è¦ç‚¹æ˜¯ï¼Œè¯­è¨€æ¨¡å‹å¯¹æ½œåœ¨çš„å¥æ³•/è¯­ä¹‰ç‰¹å¾æ•æ„Ÿï¼Œè¿™äº›ç‰¹å¾åœ¨å¥å­ä¹‹é—´å…±äº«ã€‚ä½¿ç”¨çŸ­çš„ã€å•ä¸ªå¥å­è¾“å…¥è¿›è¡ŒMPPè¯„ä¼°æ— æ³•å……åˆ†æ•æ‰LMsçš„æŠ½è±¡çŸ¥è¯†ã€‚</sample>
    <sample id="595">å¹»ç¯ç‰‡ä¸Šçš„è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡å¦‚ä¸‹ï¼š 1. è¯­è¨€æ¨¡å‹å¯¹æ½œåœ¨çš„å¥æ³•/è¯­ä¹‰ç‰¹å¾åœ¨å¥å­ä¹‹é—´å…±äº«éå¸¸æ•æ„Ÿã€‚ 2. MPPè¯„ä¼°ä½¿ç”¨çŸ­çš„å•å¥è¾“å…¥æ— æ³•å®Œå…¨æ•æ‰LMçš„æŠ½è±¡çŸ¥è¯†ã€‚</sample>
    <sample id="596">å…³é”®è¦ç‚¹æ˜¯ï¼š 1. è¯­è¨€æ¨¡å‹å¯¹å¥å­é—´å…±äº«çš„æ½œåœ¨å¥æ³•/è¯­ä¹‰ç‰¹å¾æ•æ„Ÿã€‚ 2. MPPè¯„ä¼°ä½¿ç”¨çŸ­ã€å•å¥è¾“å…¥æ— æ³•å®Œå…¨æ•æ‰LMsçš„æŠ½è±¡ç¤ºä¾‹ã€‚</sample>
    <sample id="597">è¯¥æ–¹æ³•çš„ç¬¬ä¸€æ­¥å°†è¾“å…¥è¯å…ƒæ˜ å°„åˆ°æ ‡è®°è¯å…ƒã€‚</sample>
    <sample id="598">Coscriptä¸­åŒ…å«55,000ä¸ªè„šæœ¬ã€‚</sample>
    <sample id="626">DEplain çš„æœ€ä½³å¯¹é½æ–¹æ³•æ˜¯ CATS-C3Gã€‚</sample>
    <sample id="627">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="628">DEplain-web ä¸­çš„æ–‡æ¡£é‡‡ç”¨æ‰‹åŠ¨å’Œè‡ªåŠ¨å¯¹é½æ–¹æ³•è¿›è¡Œäº†å¯¹é½ã€‚å…·ä½“åˆ†é…æƒ…å†µå¦‚ä¸‹ï¼š 1. æ‰‹åŠ¨å¯¹é½ï¼šç”¨äº DEPLAIN-APA test éƒ¨åˆ†ï¼ŒåŒ…æ‹¬ n=48 å’Œ n=1231 çš„æ•°æ®é›†ã€‚ 2. è‡ªåŠ¨å¯¹é½ï¼šç”¨äº DEPLAIN-WEB test éƒ¨åˆ†ï¼ŒåŒ…æ‹¬ n=147 å’Œ n=1846 çš„æ•°æ®é›†ã€‚</sample>
    <sample id="629">CoNLL++æ•°æ®é›†æ˜¯é€šè¿‡ä»2020å¹´æ”¶é›†è·¯é€ç¤¾æ–°é—»å¹¶ä½¿ç”¨CoNLL-2003æ³¨é‡ŠæŒ‡å—å¯¹å…¶è¿›è¡Œæ³¨é‡Šæ¥åˆ›å»ºçš„ã€‚</sample>
    <sample id="630">The video features a presentation slide titled 'XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations.' The names listed on the slide are Yusen Zhang, Jun Wang, Zhiguo Wang, and Rui Zhang. Additionally, there is an image of Penn State University's logo along with Amazon's logo at the bottom left corner of the slide. In the top right corner, there is a small window showing a person speaking into a microphone.</sample>
    <sample id="631">è¯­ä¹‰è§£ææ˜¯æ„å»ºç”¨æˆ·æŸ¥è¯¢çš„è¯­ä¹‰è¡¨ç¤ºçš„ä»»åŠ¡ï¼Œä¾‹å¦‚SQLå’ŒLambdaæ¼”ç®—ã€‚</sample>
    <sample id="632">Cross-lingual Semantic Parsing Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="633">Cross-lingual Semantic Parsing Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="634">ç°æœ‰çš„è·¨è¯­è¨€è¯­ä¹‰è§£ææ¨¡å‹æ˜¯åœ¨æœ‰é™çš„ä»»åŠ¡å’Œåº”ç”¨æ•°æ®é›†ä¸Šå•ç‹¬æå‡ºå’Œè¯„ä¼°çš„ã€‚ä¾‹å¦‚ï¼Œå­˜åœ¨å¯¹æŸäº›è‡ªç„¶è¯­è¨€çš„è¦†ç›–ä¸è¶³ã€‚</sample>
    <sample id="635">ç°æœ‰CLSPæ¨¡å‹ä»…åœ¨æœ‰é™çš„ä»»åŠ¡å’Œåº”ç”¨æ•°æ®é›†ä¸Šæå‡ºå¹¶è¯„ä¼°ï¼Œä¾‹å¦‚ï¼šæŸäº›è‡ªç„¶è¯­è¨€çš„å«ä¹‰è¡¨ç¤ºç¼ºä¹è¦†ç›–ã€‚</sample>
    <sample id="636">ç°æœ‰çš„è·¨è¯­è¨€è¯­ä¹‰è§£ææ¨¡å‹åœ¨æœ‰é™çš„ä»»åŠ¡å’Œåº”ç”¨æ•°æ®é›†ä¸Šå•ç‹¬æå‡ºå’Œè¯„ä¼°ã€‚ä¾‹å¦‚ï¼šç¼ºå°‘æŸäº›æ„ä¹‰è¡¨ç¤ºçš„è¦†ç›–èŒƒå›´ã€‚</sample>
    <sample id="637">ç°æœ‰çš„è·¨è¯­è¨€è¯­ä¹‰è§£ææ¨¡å‹åœ¨æŸäº›æ„ä¹‰è¡¨ç¤ºä¸Šå­˜åœ¨è¦†ç›–ä¸è¶³çš„é—®é¢˜ã€‚</sample>
    <sample id="638">ç°æœ‰CLSPæ¨¡å‹ä»…åœ¨æœ‰é™çš„ä»»åŠ¡å’Œåº”ç”¨æ•°æ®é›†ä¸Šè¿›è¡Œæè®®å’Œè¯„ä¼°ã€‚ä¾‹å¦‚ï¼šç¼ºä¹å¯¹æŸäº›ç¥ç»æ¨¡å‹çš„è¦†ç›–</sample>
    <sample id="639">XSemPLR æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ•°æ®é›†ç¤ºä¾‹ï¼Œç”¨äºè·¨è¯­è¨€çš„è¯­ä¹‰è§£æã€‚</sample>
    <sample id="640">XSemPLRæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¯­ä¹‰è§£ææ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒå¤šç§è‡ªç„¶è¯­è¨€å’Œæ„ä¹‰è¡¨ç¤ºã€‚å®ƒåŒ…å«9ä¸ªè·¨ä¸åŒé¢†åŸŸçš„æ•°æ®é›†ã€5ç§è¯­ä¹‰è§£æä»»åŠ¡ã€8ç§æ„ä¹‰è¡¨ç¤ºä»¥åŠ15ç§è¯­è¨€å®¶æ—ä¸­çš„22ç§è‡ªç„¶è¯­è¨€ã€‚</sample>
    <sample id="641">å®éªŒè®¾ç½® We consider the six settings for training and evaluation. Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval.</sample>
    <sample id="642">å®éªŒè®¾ç½® æˆ‘ä»¬è€ƒè™‘äº†å…­ç§è®­ç»ƒå’Œè¯„ä¼°çš„è®¾ç½®ã€‚ Translate-Testï¼šä½¿ç”¨ Google ç¿»è¯‘ API å°†æºè¯­è¨€ç¿»è¯‘æˆç›®æ ‡è¯­è¨€ï¼Œç„¶åä½¿ç”¨å•è¯­æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚ Training Inference German Translate API English English Model SQL</sample>
    <sample id="643">&lt;no_answer&gt;</sample>
    <sample id="644">æˆ‘ä»¬è€ƒè™‘äº†å…­ç§è®­ç»ƒå’Œè¯„ä¼°è®¾ç½®ã€‚å•è¯­æ¨¡å‹ï¼šæºè¯­è¨€ä¸ç›®æ ‡è¯­è¨€ç›¸åŒï¼Œä¾‹å¦‚å¾·è¯­åˆ°å¾·è¯­ã€‚æˆ‘ä»¬è¿˜æµ‹è¯•äº†å•è¯­å°‘æ ·æœ¬è®¾ç½®ï¼Œåœ¨ä½¿ç”¨ä»…10%è®­ç»ƒæ•°æ®è®­ç»ƒå•è¯­æ¨¡å‹çš„æƒ…å†µä¸‹ã€‚</sample>
    <sample id="645">å®éªŒè®¾ç½®  æˆ‘ä»¬è€ƒè™‘äº†å…­ç§è®­ç»ƒå’Œè¯„ä¼°çš„è®¾ç½®ã€‚ å•è¯­æ¨¡å‹ï¼šæºè¯­è¨€ä¸ç›®æ ‡è¯­è¨€ç›¸åŒï¼Œä¾‹å¦‚å¾·è¯­åˆ°å¾·è¯­ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä»…10%çš„è®­ç»ƒæ•°æ®æ¥è®­ç»ƒå•è¯­æ¨¡å‹æ¥æµ‹è¯•å•è¯­æ¨¡å‹çš„è®¾ç½®ã€‚ è®­ç»ƒ German (Few-shot) German Model SQL æ¨æ–­ German German Model SQL</sample>
    <sample id="646">å®éªŒè®¾ç½®  æˆ‘ä»¬è€ƒè™‘äº†å…­ç§è®­ç»ƒå’Œè¯„ä¼°çš„è®¾ç½®ã€‚ å•è¯­æ¨¡å‹ï¼šæºè¯­è¨€ä¸ç›®æ ‡è¯­è¨€ç›¸åŒï¼Œä¾‹å¦‚å¾·è¯­åˆ°å¾·è¯­ã€‚æˆ‘ä»¬è¿˜æµ‹è¯•äº†å•è¯­å°‘é‡æ•°æ®è®¾ç½®ï¼Œé€šè¿‡ä»…ä½¿ç”¨10%çš„è®­ç»ƒæ•°æ®æ¥è®­ç»ƒå•è¯­æ¨¡å‹ã€‚ è®­ç»ƒ æ¨æ–­</sample>
    <sample id="647">å®éªŒè®¾ç½® Weè€ƒè™‘äº†å…­ä¸ªç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„è®¾ç½®ã€‚ å¤šè¯­è¨€æ¨¡å‹ï¼šä¸ºæ‰€æœ‰è¯­è¨€è®­ç»ƒä¸€ä¸ªå•ä¸€çš„å¤šè¯­è¨€æ¨¡å‹ã€‚</sample>
    <sample id="648">å¹»ç¯ç‰‡å±•ç¤ºäº†å®éªŒè®¾ç½®ï¼Œé‡ç‚¹æ˜¯è®­ç»ƒå’Œè¯„ä¼°çš„å…­ä¸ªè®¾ç½®ã€‚å®ƒå¼ºè°ƒäº†å¤šè¯­è¨€æ¨¡å‹çš„æ¦‚å¿µï¼Œå³ä¸ºæ‰€æœ‰è¯­è¨€è®­ç»ƒä¸€ä¸ªå•ä¸€çš„å¤šè¯­è¨€æ¨¡å‹ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨äº†ä¸‰ç§è¯­è¨€ï¼šå¾·è¯­ã€è‹±è¯­å’Œä¸­æ–‡ã€‚è¿™äº›è¯­è¨€é€šè¿‡ç®­å¤´æŒ‡å‘ä¸€ä¸ªæ ‡è®°ä¸ºâ€œSQLâ€çš„å•ä¸€è¾“å‡ºã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒåŒæ ·çš„è¿‡ç¨‹è¢«é‡å¤ï¼Œä½†ä»…ä½¿ç”¨å¾·è¯­ã€‚è¿™è¡¨æ˜ï¼Œå°½ç®¡æ¨ç†è¿‡ç¨‹ä»…é™äºå¾·è¯­ï¼Œä½†è®­ç»ƒæ¨¡å‹æ˜¯åœ¨æ‰€æœ‰ä¸‰ç§è¯­è¨€ä¸Šè¿›è¡Œçš„ã€‚è¿™ç§è®¾ç½®å¯èƒ½æ—¨åœ¨åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å¤šç§è¯­è¨€æŸ¥è¯¢çš„é€šç”¨æ¨¡å‹ï¼Œç„¶åå°†è¿™äº›æŸ¥è¯¢è½¬æ¢ä¸ºSQLæ ¼å¼ã€‚</sample>
    <sample id="649">å®éªŒè®¾ç½® We è€ƒè™‘äº†å…­ä¸ªç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„è®¾ç½®ã€‚ Multilingual æ¨¡å‹ï¼šä¸ºæ‰€æœ‰è¯­è¨€è®­ç»ƒä¸€ä¸ªå•ä¸€çš„å¤šè¯­è¨€æ¨¡å‹ Training German è‹±è¯­ Chinese SQL Inference German å¤šè¯­è¨€æ¨¡å‹ SQL</sample>
    <sample id="650">å®éªŒè®¾ç½® We consider the six settings for training and evaluation. Cross-lingual Zero-shot/Few-shot transfer. Train on one source language and transfer to another language.</sample>
    <sample id="651">å›¾ç‰‡ä¸­çš„è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡å¦‚ä¸‹ï¼š åœ¨è®­ç»ƒå’Œè¯„ä¼°çš„å…­ä¸ªè®¾ç½®ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†å…­ç§è®¾ç½®ã€‚ å¤šè¯­ç§é›¶-shot/å°‘-shotè¿ç§»ã€‚åœ¨ä¸€ç§è¯­è¨€ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶è½¬ç§»åˆ°å¦ä¸€ç§è¯­è¨€ã€‚</sample>
    <sample id="652">è¿™å¼ å›¾ç‰‡åŒ…å«äº†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œAnalysis of Monolingualâ€ã€‚å¹»ç¯ç‰‡çš„ä¸»è¦å†…å®¹æ˜¯å¯¹ä¸¤ç§æ¨¡å‹ç»„åœ¨å•è¯­ç¯å¢ƒä¸‹çš„è¯„ä¼°ã€‚å¹»ç¯ç‰‡ä¸Šåˆ—å‡ºäº†ä¸åŒæ¨¡å‹åŠå…¶åœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°åˆ†æ•°ã€‚è¿™äº›æ¨¡å‹åŒ…æ‹¬ï¼š - mBERT-PTR - XLM-R + PTR - mBERT + PTR - mBART - mBERT + PTR å¹»ç¯ç‰‡åº•éƒ¨çªå‡ºæ˜¾ç¤ºäº†mDecæ¨¡å‹çš„åˆ†æ•°ï¼Œè¡¨æ˜å®ƒåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½è¡¨ç°æœ€ä½³ã€‚</sample>
    <sample id="653">Analysis of Monolingual</sample>
    <sample id="654">æˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§æ¨¡å‹ç»„åœ¨å•è¯­ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚</sample>
    <sample id="655">æˆ‘ä»¬å‘ç°ï¼Œç¼–ç å™¨-è§£ç å™¨æ¨¡å‹åœ¨æ‰€æœ‰9ä¸ªæ•°æ®é›†ä¸Šéƒ½è¡¨ç°æœ€ä½³ã€‚</sample>
    <sample id="656">æˆ‘ä»¬è¯„ä¼°äº†mT5å’ŒXLM-R + PTRåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚é€šè¿‡åœ¨å¤šç§è¯­è¨€çš„æ··åˆä¸­è¿›è¡Œè®­ç»ƒï¼ŒEnc- Dec/Enc-PTRï¼ˆmT5-XLM-Rï¼‰å¯ä»¥å¾—åˆ°æ˜¾è‘—çš„æå‡ã€‚</sample>
    <sample id="657">åˆ†æå¤šè¯­ç§è®­ç»ƒã€‚æˆ‘ä»¬åœ¨æ··åˆè¯­å¢ƒä¸­è¯„ä¼°mT5å’ŒXLM-R + PTRã€‚Enc- Dec/Enc-PTR (mT5-XLM/R)å¯ä»¥é€šè¿‡åœ¨å¤šç§è¯­è¨€çš„æ··åˆä¸­è¿›è¡Œè®­ç»ƒæ¥æé«˜ã€‚</sample>
    <sample id="658">Analysis of Multilingual Training</sample>
    <sample id="659">åˆ†æå¤šè¯­ç§è®­ç»ƒ</sample>
    <sample id="660">Cross-lingual Performance Gap</sample>
    <sample id="661">åœ¨è¿™å¹…å›¾ä¸­ï¼Œè“è‰²çº¿æ¡ä»£è¡¨è·¨è¯­è¨€å°‘æ ·æœ¬è¿ç§»ã€‚æ©™è‰²çº¿æ¡ä»£è¡¨è·¨è¯­è¨€é›¶æ ·æœ¬è¿ç§»ã€‚ç»¿è‰²çº¿æ¡è¡¨ç¤ºå•è¯­ç¯å¢ƒã€‚</sample>
    <sample id="662">Cross-lingual Performance Gap green - orange for zero-shot setting, the cross-lingual transfer performance gap is significant blue - orange for few-shot setting, the transfer gap is shortened rapidly</sample>
    <sample id="663">å¥½çš„ï¼Œè¿™æ˜¯å¹»ç¯ç‰‡çš„å†…å®¹ï¼š 1. æ ‡é¢˜ï¼šå…¶ä»–ç»“æœå’Œå‘ç°ï¼ˆè®ºæ–‡ç¬¬4èŠ‚ï¼‰ 2. å†…å®¹ï¼š - Enc-Dec (mT5) è¶…è¿‡ä»¥å¾€å·¥ä½œæˆ–å®ç°äº†å¯æ¯”çš„ç»“æœã€‚ - åœ¨ NL ä¸Šè¿›è¡Œé¢„è®­ç»ƒå¯ä»¥æ˜¾è‘—æå‡åœ¨ç›®æ ‡ NL ä¸Šçš„å°‘é‡æ•°æ®è¡¨ç°ã€‚ - å¤šè¯­è¨€è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ˆç”± CodeLLM å’Œ Bloom æä¾›ï¼‰å¯¹äºè·¨è¯­è¨€è¯­ä¹‰è§£æä»»åŠ¡ä»ç„¶ä¸å¤Ÿã€‚ - ä¸­æ–‡è½¬ç§»å­¦ä¹ å’Œè‹±å¾·å•è¯­è®­ç»ƒï¼ˆEn -&gt; Enï¼‰å…·æœ‰æœ€å¤§çš„æ€§èƒ½å·®è·ï¼Œè€Œâ€œGermanâ€é€šå¸¸æ˜¯æœ€å°çš„ã€‚ - FunQL åœ¨å…¶ä»–ä¸‰ç§è¡¨ç¤ºæ–¹æ³•ä¸­è¡¨ç°æœ€å¥½ï¼Œè€Œ SQL åˆ™è¡¨ç°æœ€å·®ã€‚</sample>
    <sample id="664">æŠŠè‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ã€‚</sample>
    <sample id="665">æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è·¨è¯­è¨€è¯­ä¹‰è§£æåŸºå‡†XSemPLRï¼Œç”¨äºå¤„ç†å¤šç§è‡ªç„¶è¯­è¨€å’Œæ„ä¹‰è¡¨ç¤ºã€‚</sample>
    <sample id="666">æ¬¢è¿æ”¶å¬æœ¬æ¬¡è®²è§£ï¼Œæœ¬æ¬¡è®²è§£ç”±Kwabena Asare-Bediakoæä¾›ã€‚</sample>
    <sample id="667">å…³äºè¿™æ–¹é¢ç°æœ‰ç ”ç©¶çš„åˆ†ç±»å¦‚ä¸‹ï¼š 1. åŸºäºæ°´å°çš„æ ‡è®°ï¼ˆWatermarkï¼‰ 2. è¯æ±‡è¡¨ï¼ˆLexicalï¼‰ 3. åé—¨ï¼ˆBackdoorï¼‰ 4. å¯¹æŠ—æ€§ï¼ˆAdversarialï¼‰</sample>
    <sample id="668">ä¸ï¼Œå¯¹äºè·¨è¯­è¨€è¯­ä¹‰è§£æä»»åŠ¡æ¥è¯´ï¼Œå¤šè¯­è¨€LLMï¼ˆå¦‚Codexå’ŒBloomï¼‰ä»ç„¶ä¸å¤Ÿã€‚</sample>
    <sample id="695">è¯¥æ–¹æ³•é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥æ’åˆ—ï¼Œæ¥å¤„ç†æ’åˆ—çš„ä¸ç¡®å®šæ€§ã€‚</sample>
    <sample id="696">ä¸‹æ¸¸NLPæ¨¡å‹çš„å…¬å¹³æ€§å¯ä»¥é€šè¿‡ç¡®ä¿è¿™äº›æ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬æ—¶ä¸ä¼šå¯¹ç‰¹å®šç¾¤ä½“æˆ–æ”¿æ²»è§‚ç‚¹äº§ç”Ÿåè§æ¥å®šä¹‰ã€‚</sample>
    <sample id="697">The speaker's name is Yanis Labrak.</sample>
    <sample id="698">æ¼”è®²è€…çš„åå­—æ˜¯Koustuv Sinhaã€‚</sample>
    <sample id="699">æ¼”è®²è€…çš„åå­—æ˜¯Myra Chengã€‚</sample>
    <sample id="700">çƒ­å¸¦ä¸»ä¹‰ (tropicalism) åœ¨æœ¬æ–‡çš„èƒŒæ™¯ä¸‹æ„å‘³ç€å°†æŸäº›ç¾¤ä½“ï¼Œç‰¹åˆ«æ˜¯æ‹‰ä¸è£”å¥³æ€§ã€äºšæ´²å¥³æ€§å’Œé»‘äººå¥³æ€§ï¼Œé€šè¿‡ç‰¹å®šçš„è¯è¯­å’Œæè¿°è¿›è¡Œåˆ»æ¿åŒ–ã€‚è¿™äº›è¯è¯­åŒ…æ‹¬â€œå……æ»¡æ´»åŠ›â€ã€â€œä¸°æ»¡â€ã€â€œå¨‡å°â€ã€â€œç²¾è‡´â€å’Œâ€œåšéŸ§â€ï¼Œè¿™äº›è¯è¯­è¢«ç”¨æ¥å®šä¹‰è¿™äº›ç¾¤ä½“çš„èº«ä»½ï¼Œå¹¶ä¸”å¾€å¾€ä¸çƒ­å¸¦ä¸»ä¹‰æœ‰å…³ï¼Œå³å¯¹è¿™äº›ç¾¤ä½“çš„åˆ»æ¿å°è±¡å’Œç®€åŒ–çš„æç»˜ã€‚</sample>
    <sample id="701">ä½œè€…é€šè¿‡ä½¿ç”¨è¯¸å¦‚â€œæ–‡åŒ–â€ã€â€œä¼ ç»Ÿâ€ã€â€œè‡ªè±ªâ€å’Œâ€œå¼‚å›½æƒ…è°ƒâ€ç­‰è¯æ±‡æ¥æè¿°æ ‡è®°ç¾¤ä½“ï¼Œè¿™äº›è¯æ±‡å®šä¹‰äº†è¿™äº›ç¾¤ä½“ä»…åŸºäºä»–ä»¬çš„èº«ä»½ã€‚</sample>
    <sample id="702">æœ¬æ–‡ä¸­ä½¿ç”¨äº†P-CXMIæ¥è¡¡é‡è¯­å¢ƒä½¿ç”¨æƒ…å†µã€‚</sample>
    <sample id="703">DrBERT æ˜¯ä¸€ä¸ªä»å¤´å¼€å§‹æ„å»ºçš„æ¨¡å‹ï¼Œè€Œ ChuBERT æ˜¯ä¸€ä¸ªä¸´åºŠåŒ»å­¦æ¨¡å‹ã€‚</sample>
    <sample id="751">è¿™ç¯‡è®ºæ–‡æœ‰ä¸‰ä½ä½œè€…ã€‚</sample>
    <sample id="752">è¿­ä»£è¿ç§»å­¦ä¹ æ˜¯é€šè¿‡åœ¨æœ€æ–°æ”¶é›†çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹æ¥æ›´æ–°æ¨¡å‹ã€‚</sample>
    <sample id="753">The goal of the dataset is to understand users' language when they make a choice.</sample>
    <sample id="754">æ”»å‡»è€…é€šè¿‡EaaSæ¥æå–æ¨¡å‹å‚æ•°ã€‚</sample>
    <sample id="755">The paper has three authors: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="756">There are 10 annotators used to create the initial data set.</sample>
    <sample id="757">The authors belong to Carnegie Mellon University and the Allen Institute for AI.</sample>
    <sample id="758">The left conjunct is 'left' in the example.</sample>
    <sample id="759">GPT-4</sample>
    <sample id="760">æˆ‘ä»¬éœ€è¦åœ¨æ•´ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­è¯„ä¼°æ¨¡å‹çš„å¯æ¥å—æ€§ï¼Œå› ä¸ºè¿™äº›å¤©æ¥å¤§å‹è¯­è¨€æ¨¡å‹æ­£åœ¨ä»¥è¶Šæ¥è¶Šé•¿çš„ä¸Šä¸‹æ–‡çª—å£å‡ºç°ã€‚å› æ­¤ï¼Œå¯¹äºè¯„ä¼°æ¨¡å‹çš„å¯æ¥å—æ€§æ¥è¯´ï¼Œæ•´ä¸ªä¸Šä¸‹æ–‡çª—å£è‡³å…³é‡è¦ã€‚</sample>
    <sample id="761">Yes, with multilingual training, performance can drop in some languages.</sample>
    <sample id="762">No, the annotators do not know about these entities in advance.</sample>
    <sample id="763">BLEUå’ŒTERã€‚</sample>
    <sample id="764">No, the video does not mention whether regression affects specific NER types.</sample>
    <sample id="765">NLPä¸­çš„ç«‹åœºå¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒæœ‰åŠ©äºç¡®ä¿æŠ€æœ¯å…¬å¹³åœ°å¯¹å¾…æ‰€æœ‰ç”¨æˆ·ã€‚ä¾‹å¦‚ï¼ŒPerspectiveAPIå¯èƒ½åœ¨æ£€æµ‹æŸäº›è¯­è¨€æˆ–æ–‡åŒ–èƒŒæ™¯ä¸‹çš„æ¯’æ€§æ—¶è¡¨ç°ä¸ä½³ï¼Œè¿™å¯èƒ½å¯¼è‡´ç³»ç»Ÿæ€§èƒ½å·®å¼‚å’Œåè§ã€‚</sample>
    <sample id="766">åƒ BLOOM è¿™æ ·çš„å¤šè¯­è¨€ LLM é‡‡ç”¨é€‚é…å™¨å¾®è°ƒã€‚</sample>
    <sample id="767">ä»–ä»¬ä½¿ç”¨äº†Roberta-baseåŠ ä¸Šåˆ†ç±»å™¨å¤´æ¨¡å‹è¿›è¡Œè¿ç§»å­¦ä¹ ã€‚</sample>
    <sample id="768">The following are recent test sets used to evaluate PaLM capabilities:</sample>
    <sample id="769">The author proposed three recommendations.</sample>
    <sample id="770">æè®®çš„æ–¹æ³•æ¯”æœ€å¼ºçš„åŸºçº¿æé«˜äº†15.45%ã€‚</sample>
    <sample id="771">æ¼”è®²è€…çš„åå­—æ˜¯Shuheng Liuå’ŒAlan Ritterã€‚</sample>
    <sample id="772">è®ºæ–‡ä¸­çš„ç»“æœå’Œæ•°æ®é›†å¯ä»¥ç”¨ä½œåŸºå‡†å—ï¼Ÿæ˜¯çš„ã€‚</sample>
    <sample id="773">They conducted experiments on five smaller models.</sample>
    <sample id="774">OFA, a unified multimodal pre-trained model capable of performing both understanding and generation tasks with single or multiple modalities.</sample>
    <sample id="833">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„æ˜¯Googleã€‚</sample>
    <sample id="834">Stony Brook University</sample>
    <sample id="835">è®ºæ–‡åˆ†æäº†è‹±è¯­ã€è¥¿ç­ç‰™è¯­å’Œå¾·è¯­ã€‚</sample>
    <sample id="836">Shangbin Feng</sample>
    <sample id="837">åœ¨å®éªŒè¿‡ç¨‹ä¸­ç ”ç©¶äº†ä¸¤ä¸ªæ¨¡å‹ï¼šä¸€ä¸ªæ˜¯é•¿æ®µè½æ¨¡å‹ï¼Œå¦ä¸€ä¸ªæ˜¯åŸºäºæ­£å¸¸æ®µè½çš„æ¨¡å‹ã€‚</sample>
    <sample id="838">åœ¨ MultiInstruct ä¸­ä½¿ç”¨çš„ 62 ä¸ªä¸åŒä»»åŠ¡ä¸­ï¼Œæœ‰ 53 ä¸ªä»»åŠ¡ç”¨äºè®­ç»ƒç›®çš„ã€‚</sample>
    <sample id="839">There are three authors.</sample>
    <sample id="840">ä½œè€…åœ¨å®éªŒä¸­ä½¿ç”¨äº†ä»¥ä¸‹æ•°æ®é›†ï¼šAG Newsã€MINDã€SST2å’ŒEnron Spamã€‚</sample>
    <sample id="876">NACHOSæ˜¯ä¸€ä¸ªåŒ»ç–—çˆ¬è™«æ•°æ®é›†ã€‚</sample>
    <sample id="877">David Vil Torres, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, George Foster</sample>
    <sample id="878">æç¤ºç­–ç•¥å¯¹ç»“æœæœ‰é‡å¤§å½±å“ã€‚</sample>
    <sample id="879">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„æ˜¯å¡å†…åŸºæ¢…éš†å¤§å­¦è¯­è¨€æŠ€æœ¯ç ”ç©¶æ‰€ã€‚</sample>
    <sample id="880">The five expert-written instructions are: 1. We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon 2. This is a QR code for our data and model repository.</sample>
    <sample id="881">ä½œè€…å»ºè®®ä½¿ç”¨æ¥è‡ªå¤šç§æ¥æºçš„ä¿¡æ¯æ¥æµ‹è¯•æ¨¡å‹ã€‚</sample>
    <sample id="882">Prompting PaLM for Translation: Assessing Strategies and Performance</sample>
    <sample id="883">å¹»ç¯ç‰‡ä»‹ç»äº†PaLM: Pathways Language Modelã€‚å®ƒæåˆ°äº†Chowderyç­‰äººåœ¨2022å¹´å‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡ï¼Œè®ºæ–‡ç¼–å·ä¸ºarXiv:2204.02311ã€‚æ¨¡å‹åŒ…å«540äº¿å‚æ•°ï¼Œæ˜¯åœ¨780äº¿ä¸ªæ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº†6144ä¸ªTPU v4èŠ¯ç‰‡ï¼Œå¹¶åœ¨æ•°ç™¾ä¸ªLMUå’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†SOTAï¼ˆæœ€ä½³æ€§èƒ½ï¼‰ã€‚å¹»ç¯ç‰‡è¿˜å±•ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬é—®é¢˜å›ç­”ã€ç®—æœ¯ã€ä»£ç å®Œæˆã€ç¿»è¯‘ã€æ€»ç»“ã€è¯­è¨€ç†è§£ç­‰ã€‚</sample>
    <sample id="884">å¹»ç¯ç‰‡å±•ç¤ºäº†å…³äºPaLMï¼ˆPathways Language Modelï¼‰çš„è¯¦ç»†ä¿¡æ¯ã€‚å®ƒæåˆ°äº†Chowderyç­‰äººåœ¨2022å¹´å‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡ï¼Œè®ºæ–‡ç¼–å·ä¸ºarXiv:2204.02311ã€‚è¯¥æ¨¡å‹åŒ…å«540Bå‚æ•°ï¼Œå¹¶åœ¨780Bä¸ªæ ‡è®°ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚å®ƒè¢«å¯†é›†æ¿€æ´»ï¼Œå¹¶ä½¿ç”¨äº†6144ä¸ªTPU v4èŠ¯ç‰‡ã€‚è¯¥æ¨¡å‹åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆLMUï¼‰å’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆSOTAï¼‰ã€‚å¹»ç¯ç‰‡è¿˜åˆ—å‡ºäº†PaLMèƒ½å¤Ÿæ‰§è¡Œçš„å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬é—®ç­”ã€ç®—æœ¯ã€ä»£ç è¡¥å…¨ã€ä¸€èˆ¬çŸ¥è¯†ã€æ€»ç»“ã€ç¿»è¯‘ã€é€»è¾‘å…³ç³»æ¨æ–­ã€å¸¸è¯†æ¨ç†ã€æ¨¡å¼è¯†åˆ«ã€å¯¹è¯ã€ç¬‘è¯è§£é‡Šã€ç‰©ç†å­¦é—®é¢˜ã€é‡å­åŠ›å­¦å’Œè¯­è¨€ç†è§£ã€‚å¹»ç¯ç‰‡åº•éƒ¨æ˜¾ç¤ºäº†Googleçš„æ ‡å¿—ã€‚</sample>
    <sample id="885">è´¡çŒ®ï¼š 1. LLMæç¤ºç”¨äºæœºå™¨ç¿»è¯‘çš„ç¬¬ä¸€ä¸ªç³»ç»Ÿæ€§ç ”ç©¶ï¼ŒåŒ…æ‹¬å€™é€‰æ± å’Œé€‰æ‹©ç­–ç•¥ã€‚ 2. è¯„ä¼°MTç¤¾åŒºçš„æœ€ä½³å®è·µçš„ç¿»è¯‘èƒ½åŠ›ï¼š - æœ€æ–°çš„æµ‹è¯•é›†ï¼ˆé¿å…æµ‹è¯•é›†é‡å å’Œåœ¨è¯„ä¼°æ•°æ®ä¸Šè¿‡åº¦æ‹Ÿåˆï¼‰ - ä¸æœ€è¿‘æäº¤çš„WMTè®ºæ–‡è¿›è¡Œæ¯”è¾ƒï¼ˆSOTAç³»ç»Ÿä½¿ç”¨æœ€æ–°çš„è®­ç»ƒæ•°æ®ï¼‰ - SOTA MTæŒ‡æ ‡ï¼ˆä¸äººç±»åˆ¤æ–­çš„æ›´å¥½ç›¸å…³æ€§ï¼‰ - ä¸“å®¶çº§çš„äººç±»è¯„ä¼°ï¼ˆæ¯”æ™®é€šå·¥äººæ›´ robustï¼‰ 3. æå‡ºæç¤ºé€‰æ‹©ç­–ç•¥çš„å»ºè®®</sample>
    <sample id="886">å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€æ®µæ–‡å­—ï¼Œæ ‡é¢˜æ˜¯â€œæˆ‘ä»¬çš„è´¡çŒ®â€ã€‚è¿™æ®µæ–‡å­—åŒ…æ‹¬ä»¥ä¸‹å‡ ç‚¹ï¼š 1. â€œé¦–æ¬¡ç³»ç»Ÿæ€§çš„LLMæç¤ºç”¨äºMTã€‚â€ - è¿™éƒ¨åˆ†æåˆ°è¿™é¡¹ç ”ç©¶æ˜¯é¦–æ¬¡å¯¹LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰è¿›è¡Œç³»ç»Ÿæ€§çš„æç¤ºç ”ç©¶ï¼Œç”¨äºMTï¼ˆæœºå™¨ç¿»è¯‘ï¼‰ã€‚ 2. â€œè¯„ä¼°MTç¤¾åŒºçš„æœ€ä½³å®è·µçš„ç¿»è¯‘èƒ½åŠ›ã€‚â€ - è¿™éƒ¨åˆ†æåˆ°è¯„ä¼°ç¿»è¯‘èƒ½åŠ›æ—¶ä½¿ç”¨äº†MTç¤¾åŒºçš„æœ€ä½³å®è·µã€‚ 3. â€œæ¨èé€‰æ‹©æç¤ºç­–ç•¥çš„æ–¹æ³•ã€‚â€ - è¿™éƒ¨åˆ†æåˆ°æä¾›äº†ä¸€äº›å»ºè®®æ¥é€‰æ‹©æç¤ºç­–ç•¥ã€‚ å¹»ç¯ç‰‡å³ä¸‹è§’æœ‰ä¸€ä¸ªå°å›¾ç‰‡ï¼Œæ˜¾ç¤ºä¸€ä¸ªäººçš„è„¸éƒ¨ã€‚å·¦ä¸‹è§’æœ‰Googleçš„æ ‡å¿—ã€‚</sample>
    <sample id="887">&lt;no_answer&gt;</sample>
    <sample id="888">å¹»ç¯ç‰‡ä¸Šçš„æ–‡å­—å†…å®¹æ˜¯ï¼š 1. Our contribution - First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy. - Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). - Recommendation for prompt selection strategies</sample>
    <sample id="889">Prompts have a big impact on translation quality. Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points</sample>
    <sample id="890">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹ç¿»è¯‘æˆä¸­æ–‡å¦‚ä¸‹ï¼š</sample>
    <sample id="891">æç¤ºå¯¹ç¿»è¯‘è´¨é‡æœ‰å¾ˆå¤§å½±å“ã€‚</sample>
    <sample id="892">ç”»é¢ä¸­æœ‰ä¸€ä¸ªç©¿ç€æ ¼å­è¡¬è¡«çš„äººï¼Œä»–ååœ¨ä¸€ä¸ªæœ‰ç™½è‰²èƒŒæ™¯çš„æ¡Œå­å‰ã€‚æ¡Œå­ä¸Šæœ‰ä¸€äº›æ–‡ä»¶å’Œä¸€ä¸ªéº¦å…‹é£ã€‚èƒŒæ™¯æ˜¯ä¸€ä¸ªå¸¦æœ‰æ–‡å­—çš„å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜æ˜¯â€œExample prompting for translationâ€ï¼Œä¸‹é¢å†™ç€â€œ5-shot promptingâ€ã€‚å¹»ç¯ç‰‡ä¸Šåˆ—å‡ºäº†å¾·è¯­å’Œè‹±è¯­çš„å¥å­å¯¹ï¼Œå±•ç¤ºäº†ç¿»è¯‘è¿‡ç¨‹ä¸­çš„æç¤ºç­–ç•¥ã€‚</sample>
    <sample id="893">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š

æ ‡é¢˜ï¼šExample prompting for translation

å‰¯æ ‡é¢˜ï¼š5-shot prompting

æ­£æ–‡ï¼š
- German: Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird.
  è‹±æ–‡ç¿»è¯‘ï¼šHe is being transported under the custody of two policemen on a bus from the jail.

- German: Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten.
  è‹±æ–‡ç¿»è¯‘ï¼šPolice were called in after receiving complaints from the office.

- German: Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte.
  è‹±æ–‡ç¿»è¯‘ï¼šEnglish:

æ­¤å¤–ï¼Œå›¾ç‰‡å·¦ä¸‹è§’æœ‰ä¸€ä¸ªGoogleçš„æ ‡å¿—ã€‚</sample>
    <sample id="894">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œExample prompting for translationâ€çš„æ¼”ç¤ºå¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡çš„èƒŒæ™¯æ˜¯ç™½è‰²çš„ï¼Œé¡¶éƒ¨æœ‰ä¸€ä¸ªæ ‡é¢˜ï¼Œä¸‹é¢æœ‰ä¸‰ä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½æœ‰å¾·è¯­å’Œè‹±è¯­çš„ç¿»è¯‘ã€‚ç¬¬ä¸€ä¸ªéƒ¨åˆ†æåˆ°â€œDort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wirdâ€ï¼Œå…¶å¯¹åº”çš„è‹±æ–‡ç¿»è¯‘æ˜¯â€œEnglish: He is being transported under the custody of two policemen on a bus from the jailâ€ã€‚ç¬¬äºŒä¸ªéƒ¨åˆ†æåˆ°â€œSki-Legenden unter sich: Die Polizei war eingeschlossen, nachdem sie Beschwerden des Buros erhalten hattenâ€ï¼Œå…¶å¯¹åº”çš„è‹±æ–‡ç¿»è¯‘æ˜¯â€œEnglish: Police were called in after receiving complaints from the officeâ€ã€‚ç¬¬ä¸‰ä¸ªéƒ¨åˆ†æåˆ°â€œEin Passant alarmierte die Polizei, mit mehreren Streifen anruckteâ€ï¼Œå…¶å¯¹åº”çš„è‹±æ–‡ç¿»è¯‘æ˜¯â€œEnglish: A passerby alerted the police, with several officers respondingâ€ã€‚åœ¨å¹»ç¯ç‰‡çš„å³ä¸‹è§’ï¼Œæœ‰ä¸€ä¸ªå°åœ†å½¢å›¾åƒï¼Œæ˜¾ç¤ºä¸€ä¸ªäººçš„éƒ¨åˆ†è„¸éƒ¨ã€‚å·¦ä¸‹è§’æœ‰ä¸€ä¸ªGoogle Driveçš„æ ‡å¿—ã€‚</sample>
    <sample id="895">ä¾‹ï¼š5ä¸ªæç¤ºç¿»è¯‘</sample>
    <sample id="896">å›¾ç‰‡ä¸­çš„è‹±æ–‡æ–‡æœ¬å†…å®¹ç¿»è¯‘æˆä¸­æ–‡å¦‚ä¸‹ï¼š

æ ‡é¢˜ï¼šExample prompting for translation

å‰¯æ ‡é¢˜ï¼š5-shot prompting

æ­£æ–‡ï¼š
å¾·è¯­ï¼šDort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird.
è‹±è¯­ï¼šHe is being transported under the custody of two policemen on a bus from the jail.

å¾·è¯­ï¼šSki-Legenden unter sich: Die Polizei war eingeschlossen, nachdem sie Beschwerden des Buros erhalten hattenã€‚
è‹±è¯­ï¼šPolice were called in after receiving complaints from the officeã€‚

å¾·è¯­ï¼šEin Passant alarmierte die Polizei, mit mehreren Streifen anruckteã€‚
è‹±è¯­ï¼š</sample>
    <sample id="897">å®éªŒç»“æœçš„æ€»ç»“æ˜¯ï¼Œç¤ºä¾‹è´¨é‡æ¯”æºå¥å­çš„ç›¸ä¼¼æ€§æ›´é‡è¦ã€‚</sample>
    <sample id="898">The slide shows the experimental results of a study on machine translation. It highlights that example quality is more important than similarity to the source sentence, and specialized SOTA systems have a significant advantage over PaLM, which performs close to Google Translate. The insights from MQM indicate that while PaLM's fluency can match SOTA, its accuracy scores are generally lower, dominated by "Accuracy/Omission," and it tends to be less stylistically appropriate or awkward for PaLM compared to other models.</sample>
    <sample id="899">å®éªŒç»“æœè¡¨æ˜ï¼Œæ ·ä¾‹è´¨é‡æ¯”ä¸æºå¥å­çš„ç›¸ä¼¼æ€§æ›´é‡è¦ã€‚ä¸“æœ‰SOTAç³»ç»Ÿå…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚PaLMä¸Google Translateç›¸å½“ã€‚</sample>
    <sample id="900">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹å¦‚ä¸‹ï¼š Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="901">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š

æ ‡é¢˜ï¼šExperimental Results

1. Example quality is more important than similarity to source sentence.
2. Specialized SOTA systems have a substantial advantage.
3. PaLM close to Google Translate.

Insights from MQM:
- Fluency of PaLM comparable to SOTA.
- Accuracy scores generally lower (Dominated by "Accuracy/Omission").
- "Style/Awesome" generally lower for PaLM.

å·¦ä¸‹è§’æœ‰ä¸€ä¸ªGoogleçš„æ ‡å¿—ã€‚</sample>
    <sample id="902">å®éªŒç»“æœè¡¨æ˜ï¼Œç¤ºä¾‹è´¨é‡æ¯”ä¸æºå¥å­çš„ç›¸ä¼¼æ€§æ›´é‡è¦ã€‚ä¸“æœ‰SOTAç³»ç»Ÿå…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚PaLMçš„è¡¨ç°æ¥è¿‘äºGoogle Translateã€‚</sample>
    <sample id="903">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹æ˜¯å…³äºå®éªŒç»“æœçš„ã€‚ä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š 1. å®éªŒç»“æœ 2. ä¾‹å­è´¨é‡æ¯”ç›¸ä¼¼åº¦åˆ°æºå¥å­æ›´é‡è¦ 3. ä¸“é—¨åŒ–çš„SOTAç³»ç»Ÿå…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ 4. PaLMä¸Google Translateæ¥è¿‘ 5. ä»MQMä¸­è·å¾—çš„è§è§£ï¼š - PaLMçš„æµç•…æ€§ä¸SOTAç›¸å½“ - å‡†ç¡®å¾—åˆ†é€šå¸¸è¾ƒä½ï¼Œä¸»è¦ç”±â€œå‡†ç¡®æ€§/é—æ¼â€ä¸»å¯¼ - â€œé£æ ¼/ç¬¨æ‹™â€é€šå¸¸å¯¹PaLMæ¥è¯´æ›´ä½</sample>
    <sample id="904">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š 1. å®éªŒç»“æœï¼ˆExperimental Resultsï¼‰ 2. ä¾‹å¦‚ï¼Œè´¨é‡æ¯”ç›¸ä¼¼åº¦åˆ°æºå¥å­æ›´é‡è¦ã€‚ 3. ä¸“é—¨çš„SOTAç³»ç»Ÿå…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚ 4. PaLMæ¥è¿‘Google Translateã€‚ 5. MQMæ´å¯Ÿï¼š - PaLMçš„æµç•…æ€§ä¸SOTAç›¸å½“ã€‚ - å‡†ç¡®åº¦åˆ†æ•°é€šå¸¸è¾ƒä½ã€‚ - ä¸»è¦ç”±â€œå‡†ç¡®æ€§/é—æ¼â€ä¸»å¯¼ã€‚ - â€œé£æ ¼/ç¬¨æ‹™â€é€šå¸¸å¯¹PaLMæ¥è¯´æ›´ä½ã€‚ å›¾ç‰‡å·¦ä¸‹è§’æœ‰ä¸€ä¸ªGoogleæ ‡å¿—ã€‚</sample>
    <sample id="905">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹ç¿»è¯‘æˆä¸­æ–‡å¦‚ä¸‹ï¼š

å®éªŒç»“æœï¼š
- ä¾‹å­çš„è´¨é‡æ¯”ä¸æºå¥å­çš„ç›¸ä¼¼æ€§æ›´é‡è¦ã€‚
- ä¸“é—¨åŒ–çš„SOTAç³»ç»Ÿå…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚
- PaLM æ¥è¿‘ Google Translateã€‚

MQM çš„è§è§£ï¼š
- PaLM çš„æµç•…åº¦ä¸ SOTA ç›¸å½“ã€‚
- å‡†ç¡®ç‡é€šå¸¸è¾ƒä½ã€‚
  - ä¸»è¦ç”±â€œå‡†ç¡®æ€§/é—æ¼â€ä¸»å¯¼ã€‚
- â€œé£æ ¼/ç¬¨æ‹™â€é€šå¸¸å¯¹ PaLM æ¥è¯´æ›´ä½ã€‚</sample>
    <sample id="906">ç”»é¢ä¸­æœ‰ä¸€ä¸ªè¯äº‘ï¼Œå±•ç¤ºäº†ä¸åŒè¯­è¨€ä¸­çš„â€œè°¢è°¢â€è¿™ä¸ªè¯ã€‚ä¸­å¤®æ˜¾çœ¼åœ°æ˜¾ç¤ºç€â€œthank youâ€è¿™ä¸ªè¯ï¼Œå‘¨å›´æ˜¯ç”¨å„ç§è¯­è¨€ä¹¦å†™çš„â€œè°¢è°¢â€ï¼Œå¦‚â€œgraciasâ€ã€â€œdankeâ€ã€â€œgrazieâ€ç­‰ã€‚èƒŒæ™¯æ˜¯ç™½è‰²çš„ï¼Œè¿™äº›è¯ä»¥ä¸åŒçš„å¤§å°å’Œé¢œè‰²æ’åˆ—ï¼Œå½¢æˆä¸€ä¸ªè§†è§‰ä¸Šå¸å¼•äººçš„å›¾æ¡ˆã€‚å³ä¸‹è§’æœ‰ä¸€ä¸ªå°çš„åœ†å½¢å›¾åƒï¼Œå¯èƒ½æ˜¯ä¸€ä¸ªäººçš„è‚–åƒã€‚</sample>
    <sample id="907">Weaker Than You Think A Critical Look at Weakly Supervised Learning Dawei Zhu1 Xiaoyun Shen2 Marius Mosbach1 Andreas Stephan3 Dietrich Klakow1 Saarland University Amazon Alexa University of Vienna ACL 2023</sample>
    <sample id="908">è¿™æ˜¯ä¸€é¡¹ä¸å°é›¨æ²ˆã€é©¬çº¦æ–¯Â·è«æ–¯å·´èµ«å’Œè¿ªç‰¹é‡Œå¸ŒÂ·å…‹æ‹‰ç§‘å¤«å…±åŒå®Œæˆçš„åˆä½œå·¥ä½œã€‚</sample>
    <sample id="909">ä¸ºä»€ä¹ˆæ˜¯å¼±ç›‘ç£å­¦ä¹ ï¼Ÿ å¼±ç›‘ç£ç¼“è§£äº†æ ‡æ³¨ç“¶é¢ˆã€‚ ä½†å¼±æ ‡ç­¾æ˜¯æœ‰å™ªéŸ³çš„ï¼ å™ªéŸ³è®°å¿†ä¼šæŸå®³æ³›åŒ–èƒ½åŠ›ã€‚ å¼±ç›‘ç£å­¦ä¹ ï¼ˆWSLï¼‰ï¼š è®­ç»ƒæ¨¡å‹ï¼Œå³ä½¿åœ¨å˜ˆæ‚çš„æ•°æ®ä¸Šä¹Ÿèƒ½å¾ˆå¥½åœ°æ³›åŒ–ï¼Œå°½ç®¡æ ‡æ³¨é”™è¯¯ã€‚</sample>
    <sample id="910">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="911">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="912">Weak supervision alleviates the annotation bottleneck. But weak labels are noisy Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data.</sample>
    <sample id="913">ä¸ºä»€ä¹ˆå¼±ç›‘ç£å­¦ä¹ ï¼Ÿ å¼±ç›‘ç£ç¼“è§£äº†æ³¨é‡Šç“¶é¢ˆã€‚ ä½†å¼±æ ‡ç­¾æ˜¯æœ‰å™ªéŸ³çš„ï¼ å™ªéŸ³è®°å¿†ä¼šæŸå®³æ³›åŒ–ã€‚ å¼±ç›‘ç£å­¦ä¹ ï¼ˆWSLï¼‰ è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿåœ¨å˜ˆæ‚çš„æ•°æ®ä¸Šå¾ˆå¥½åœ°æ³›åŒ–ï¼Œå³ä½¿æ˜¯åœ¨å˜ˆæ‚çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚ å¼±æ ‡ç­¾æ•°æ®ï¼ˆä¾‹å¦‚å¯å‘å¼ã€çŸ¥è¯†åº“ï¼‰æœªæ ‡æ³¨æ•°æ® æ ‡è®°æ•°æ®æ˜¯å˜ˆæ‚çš„ï¼Œå› ä¸ºæ³¨é‡Šæ˜¯é”™è¯¯çš„ã€‚</sample>
    <sample id="914">å¹»ç¯ç‰‡è®¨è®ºäº†åœ¨æœ€è¿‘çš„å¼±ç›‘ç£å­¦ä¹ ï¼ˆWSLï¼‰å·¥ä½œä¸­å¸¸è§çš„ä¸€ä¸ªä¸»å¼ ã€‚å®ƒæŒ‡å‡ºï¼Œåœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œé€šå¸¸ä½¿ç”¨æ ‡è®°ä¸å®Œå…¨çš„æ•°æ®ï¼Œå¹¶ä¸”èƒ½å¤Ÿè¾¾åˆ°è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚å…·ä½“æ¥è¯´ï¼Œå¹»ç¯ç‰‡æåˆ°ä½¿ç”¨æ ‡è®°ä¸å®Œå…¨çš„è®­ç»ƒæ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸”èƒ½å¤Ÿå®ç°XX%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œå¹»ç¯ç‰‡è¿˜å±•ç¤ºäº†ä¸¤ç§ç±»å‹çš„æ•°æ®ï¼šæ ‡è®°ä¸å®Œå…¨çš„è®­ç»ƒæ•°æ®å’Œæ ‡è®°å®Œå…¨çš„æµ‹è¯•æ•°æ®ã€‚</sample>
    <sample id="915">æœ€è¿‘WSLå·¥ä½œä¸­çš„ä¸€ä¸ªå¸¸è§ä¸»å¼ æ˜¯ï¼Œæˆ‘ä»¬ä»…åœ¨å¼±ç›‘ç£æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå¹¶è¾¾åˆ°XX%çš„å‡†ç¡®ç‡ã€‚</sample>
    <sample id="916">è¿™æ®µè‹±æ–‡å†…å®¹çš„ä¸­æ–‡ç¿»è¯‘æ˜¯ï¼šæˆ‘ä»¬ä»…åœ¨å¼±ç›‘ç£æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸”å‡†ç¡®ç‡è¾¾åˆ°äº†XX%ã€‚</sample>
    <sample id="917">å¹»ç¯ç‰‡åŒ…å«æ ‡é¢˜â€œæœ€è¿‘WSLå·¥ä½œçš„å¸¸è§ä¸»å¼ â€ï¼Œå…¶ä¸­â€œWSLâ€å¯èƒ½ä»£è¡¨ç›‘ç£å­¦ä¹ æˆ–ç±»ä¼¼æ¦‚å¿µã€‚å®ƒæè¿°äº†è®­ç»ƒæ¨¡å‹ä»…ä½¿ç”¨å¼±ç›‘ç£æ•°æ®ï¼Œå¹¶è¾¾åˆ°XX%çš„å‡†ç¡®æ€§ï¼Œæ—è¾¹æœ‰ä¸€ä¸ªæ„¤æ€’çš„è¡¨æƒ…ç¬¦å·ã€‚å¹»ç¯ç‰‡è¿˜å±•ç¤ºäº†ä¸‰ç§æ•°æ®ç±»å‹ï¼šå¸¦æœ‰çº¢è‰²æ ‡è®°çš„â€œå¸¦æœ‰å™ªå£°çš„å¼±æ ‡è®°è®­ç»ƒæ•°æ®â€ã€â€œå¹²å‡€çš„å¼±æ ‡è®°éªŒè¯æ•°æ®â€å’Œâ€œå¹²å‡€çš„å¼±æ ‡è®°æµ‹è¯•æ•°æ®â€ã€‚åœ¨å¹»ç¯ç‰‡çš„å³ä¸‹è§’ï¼Œæœ‰ä¸€åªå¤§è±¡çš„å›¾ç‰‡ï¼Œå¯èƒ½è±¡å¾ç€ä¸€ä¸ªæ˜æ˜¾çš„äº‹å®æˆ–é—®é¢˜ã€‚</sample>
    <sample id="918">å¹»ç¯ç‰‡æ˜¾ç¤ºäº†å…³äºç ”ç©¶é—®é¢˜çš„æ–‡æœ¬å†…å®¹ã€‚æ ‡é¢˜ä¸ºâ€œæˆ‘ä»¬çš„ç ”ç©¶é—®é¢˜â€ï¼Œä¸‹é¢åˆ—å‡ºäº†ä¸‰ä¸ªç ”ç©¶é—®é¢˜ï¼š 1. RQ1: éªŒè¯æ•°æ®æ˜¯å¦å¿…è¦ï¼Ÿ 2. RQ2: WSL æ–¹æ³•éœ€è¦å¤šå°‘å¹²å‡€æ ·æœ¬ï¼Ÿ 3. RQ3: å¦‚ä½•æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¯ç”¨çš„å¹²å‡€æ ·æœ¬ï¼Ÿ</sample>
    <sample id="919">å¹»ç¯ç‰‡åŒ…å«ä¸€ä¸ªæ ‡é¢˜å’Œä¸‰ä¸ªç ”ç©¶é—®é¢˜ã€‚æ ‡é¢˜æ˜¯â€œæˆ‘ä»¬çš„ç ”ç©¶é—®é¢˜â€ã€‚ä¸‰ä¸ªç ”ç©¶é—®é¢˜æ˜¯ï¼š1. RQ1: éªŒè¯æ•°æ®æ˜¯å¦å¿…è¦ï¼Ÿ2. RQ2: WSLæ–¹æ³•éœ€è¦å¤šå°‘å¹²å‡€æ ·æœ¬ï¼Ÿ3. RQ3: å¦‚ä½•æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¯ç”¨çš„å¹²å‡€æ ·æœ¬ï¼Ÿ</sample>
    <sample id="920">RQ1 Main findings</sample>
    <sample id="921">å›¾è¡¨æ˜¾ç¤ºäº†åœ¨ä¸åŒéªŒè¯æ–¹æ³•ä¸‹çš„ç›¸å¯¹æ€§èƒ½æå‡ã€‚æ¨ªè½´ä»£è¡¨ä¸åŒçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬FTwã€BONDã€COSINEã€MLCå’ŒL2Rã€‚çºµè½´è¡¨ç¤ºç›¸å¯¹æ€§èƒ½æå‡ç™¾åˆ†æ¯”ã€‚å›¾ä¸­æœ‰ä¸‰æ¡çº¿ï¼Œåˆ†åˆ«ä»£è¡¨åœ¨å¼±æ ‡ç­¾ã€éšæœºé€‰æ‹©å’Œå¹²å‡€æ ‡ç­¾ä¸Šçš„éªŒè¯æƒ…å†µã€‚</sample>
    <sample id="922">å›¾è¡¨æ˜¾ç¤ºäº†ä¸åŒéªŒè¯æ–¹æ³•å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</sample>
    <sample id="923">å›¾è¡¨æ˜¾ç¤ºäº†ä¸åŒéªŒè¯æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„ç›¸å¯¹æ€§èƒ½æå‡ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæ¯”è¾ƒäº†ä¸‰ç§éªŒè¯æ–¹æ³•ï¼šä½¿ç”¨å¼±æ ‡ç­¾ã€éšæœºé€‰æ‹©å’Œå¹²å‡€æ ‡ç­¾åœ¨FTwã€BONDã€COSINEã€MLCå’ŒL2Ræ¨¡å‹ä¸Šçš„è¡¨ç°ã€‚å›¾è¡¨ä¸­çš„æ•°æ®ç‚¹ä»£è¡¨äº†è¿™äº›æ¨¡å‹åœ¨ä¸åŒéªŒè¯æ–¹æ³•ä¸‹çš„ç›¸å¯¹æ€§èƒ½æå‡ç™¾åˆ†æ¯”ã€‚</sample>
    <sample id="924">è¿™å¼ å›¾ç‰‡åŒ…å«ä¸€ä¸ªå›¾è¡¨ï¼Œæ ‡é¢˜ä¸ºâ€œMain findingsâ€ã€‚å›¾è¡¨æ˜¾ç¤ºäº†ä¸åŒéªŒè¯æ–¹æ³•åœ¨å„ç§æŒ‡æ ‡ï¼ˆFTwã€BONDã€COSINEã€MLCå’ŒL2Rï¼‰ä¸Šçš„ç›¸å¯¹æ€§èƒ½ã€‚å›¾ä¾‹ä¸­åŒ…æ‹¬ä¸‰ç§éªŒè¯æ–¹æ³•ï¼š'Validation on Weak Labels'ï¼ˆç”¨æ©™è‰²è¡¨ç¤ºï¼‰ã€'No Validation (Random Selection)'ï¼ˆç”¨ç´«è‰²è¡¨ç¤ºï¼‰å’Œ'Validation on Clean Labels'ï¼ˆç”¨ç»¿è‰²è¡¨ç¤ºï¼‰ã€‚æ¯ä¸ªæ•°æ®ç‚¹éƒ½å¸¦æœ‰è¯¯å·®çº¿ï¼Œè¡¨æ˜äº†æ€§èƒ½çš„å˜å¼‚æ€§ã€‚åœ¨å›¾è¡¨ä¸‹æ–¹ï¼Œæœ‰ä¸€å¥è¯å†™ç€â€œA clean validation set is indispensable.â€ï¼Œå¼ºè°ƒäº†å¹²å‡€éªŒè¯é›†çš„é‡è¦æ€§ã€‚</sample>
    <sample id="925">å›¾è¡¨æ˜¾ç¤ºäº†ä¸åŒéªŒè¯æ ·æœ¬æ•°é‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</sample>
    <sample id="926">æŠŠè‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ã€‚

å›¾è¡¨å±•ç¤ºäº†ä¸åŒæ–¹æ³•åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡å˜åŒ–ã€‚å›¾ä¾‹ä¸­åˆ—å‡ºäº†äº”ç§æ–¹æ³•ï¼šFTwã€COSINEã€L2Rã€BONDå’ŒMLCï¼Œä»¥åŠä¸€ä¸ªè¡¨ç¤ºå¼±æ ‡ç­¾çš„è™šçº¿ã€‚æ¨ªè½´ä»£è¡¨éªŒè¯é›†å¤§å°ï¼Œä»5åˆ°50ï¼Œçºµè½´ä»£è¡¨å‡†ç¡®ç‡ï¼ŒèŒƒå›´ä»75%åˆ°85%ã€‚æ¯ç§æ–¹æ³•éƒ½ç”¨ä¸åŒçš„é¢œè‰²å’Œçº¿æ¡æ ·å¼è¡¨ç¤ºå…¶å‡†ç¡®ç‡éšéªŒè¯é›†å¤§å°çš„å˜åŒ–è¶‹åŠ¿ã€‚</sample>
    <sample id="927">ä¸»è¦å‘ç° ä»WSLæ–¹æ³•ä¸­å—ç›Šæ›´å¤šå¹²å‡€çš„éªŒè¯æ ·æœ¬ï¼</sample>
    <sample id="928">WSL approaches benefit from more clean validation samples</sample>
    <sample id="929">WSL approaches benefit from more clean validation samples</sample>
    <sample id="930">å›¾è¡¨æ˜¾ç¤ºäº†åœ¨ä½¿ç”¨CFTï¼ˆå¯èƒ½æ˜¯ä¸€ç§é¢„è®­ç»ƒæ–¹æ³•ï¼‰å‰åï¼Œä¸åŒæ¨¡å‹åœ¨ä¸¤ä¸ªä¸åŒæ ·æœ¬å¤§å°ä¸‹çš„å‡†ç¡®ç‡å˜åŒ–ã€‚å·¦ä¾§å›¾è¡¨è¡¨ç¤ºæ¯ç±»10ä¸ªå¹²å‡€æ ·æœ¬çš„æƒ…å†µï¼Œå³ä¾§å›¾è¡¨è¡¨ç¤ºæ¯ç±»30ä¸ªå¹²å‡€æ ·æœ¬çš„æƒ…å†µã€‚æ¯ä¸ªå›¾è¡¨ä¸­æœ‰å››æ¡çº¿ï¼Œåˆ†åˆ«ä»£è¡¨COSINEã€L2Rå’ŒClean Onlyä¸‰ç§æ–¹æ³•çš„æ€§èƒ½å˜åŒ–ã€‚ä»å›¾è¡¨ä¸­å¯ä»¥çœ‹å‡ºï¼Œåœ¨ä½¿ç”¨CFTåï¼Œæ‰€æœ‰æ–¹æ³•çš„å‡†ç¡®ç‡éƒ½æœ‰æ‰€æé«˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¯ç±»30ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æå‡æ›´ä¸ºæ˜æ˜¾ã€‚</sample>
    <sample id="931">ä¸»è¦å‘ç°å¦‚å›¾æ‰€ç¤ºï¼Œæ¨¡å‹FTWæœ€åˆåœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¸ä½³ã€‚</sample>
    <sample id="932">æŠŠè‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ã€‚

```
ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬å…è®¸ç»§ç»­åœ¨å¹²å‡€çš„æ ·æœ¬ä¸Šè¿›è¡Œå¾®è°ƒï¼Œ
```

```
åˆ™FTWä¸å…¶å®ƒæ–¹æ³•è¡¨ç°å¾—åŒæ ·å‡ºè‰²ã€‚
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```</sample>
    <sample id="933">Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches. No need to use complicated WSL methods, which require more computation time and disk space.</sample>
    <sample id="934">å¹»ç¯ç‰‡æ€»ç»“äº†å…³äºWSLæ–¹æ³•çš„å‡ ç‚¹å†…å®¹ã€‚é¦–å…ˆï¼Œå®ƒæŒ‡å‡ºWSLæ–¹æ³•éœ€è¦å¹²å‡€çš„æ ·æœ¬ï¼Œå¹¶ä¸”å¯èƒ½é«˜ä¼°äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚æ¥ç€ï¼Œå®ƒæå‡ºäº†ä»¥ä¸‹å»ºè®®ï¼š1. æŠ¥å‘Šæ¨¡å‹é€‰æ‹©æ ‡å‡†ã€‚2. ä½¿ç”¨å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ä½œä¸ºåŸºå‡†ã€‚3. å§‹ç»ˆåº”ç”¨è¿ç»­å¾®è°ƒï¼ˆCFTï¼‰ã€‚</sample>
    <sample id="935">ç»“è®º Recent WSL æ–¹æ³• éœ€è¦å¹²å‡€çš„æ ·æœ¬ã€‚ä»–ä»¬é«˜ä¼°äº†å…¶å®é™…æ€§ã€‚ æˆ‘ä»¬çš„å»ºè®® æŠ¥å‘Šæ¨¡å‹é€‰æ‹©æ ‡å‡† ä½¿ç”¨å°‘é‡å­¦ä¹ æ–¹æ³•ä½œä¸ºåŸºå‡†ã€‚å§‹ç»ˆåº”ç”¨è¿ç»­å¾®è°ƒï¼ˆCFTï¼‰ã€‚</sample>
    <sample id="936">ç»“è®º Recent WSL æ–¹æ³• éœ€è¦å¹²å‡€çš„æ ·æœ¬ã€‚ ä»–ä»¬é«˜ä¼°äº†å…¶å®ç”¨æ€§ã€‚ æˆ‘ä»¬çš„å»ºè®® æŠ¥å‘Šæ¨¡å‹é€‰æ‹©æ ‡å‡†ã€‚ ä½¿ç”¨å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ä½œä¸ºåŸºå‡†ã€‚ å§‹ç»ˆåº”ç”¨è¿ç»­å¾®è°ƒï¼ˆCFTï¼‰ã€‚</sample>
    <sample id="937">è¿™æ®µæ–‡å­—è®¨è®ºäº†æœ€è¿‘çš„WSLæ–¹æ³•ï¼ŒæŒ‡å‡ºå®ƒä»¬éœ€è¦å¹²å‡€çš„æ•°æ®æ ·æœ¬ï¼Œå¹¶ä¸”å¯èƒ½é«˜ä¼°äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚å®ƒè¿˜æå‡ºäº†å»ºè®®ï¼ŒåŒ…æ‹¬æŠ¥å‘Šæ¨¡å‹é€‰æ‹©æ ‡å‡†ã€ä½¿ç”¨å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ä½œä¸ºåŸºå‡†ï¼Œå¹¶å§‹ç»ˆåº”ç”¨è¿ç»­å¾®è°ƒï¼ˆCFTï¼‰ã€‚</sample>
    <sample id="938">å¹»ç¯ç‰‡åŒ…å«ä¸€ä¸ªæ ‡é¢˜ä¸ºâ€œConclusionâ€çš„éƒ¨åˆ†ï¼Œä¸‹é¢æœ‰ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šâ€œRecent WSL approachesâ€å’Œâ€œOur recommendationsâ€ã€‚åœ¨â€œRecent WSL approachesâ€ä¸‹æœ‰ä¸¤æ¡è¦ç‚¹ï¼š1. è¦æ±‚å¹²å‡€çš„æ ·æœ¬ã€‚2. é«˜ä¼°å…¶å®ç”¨æ€§ï¼ˆæ—è¾¹æœ‰ä¸€ä¸ªè¡¨ç¤ºä¸æ»¡çš„è¡¨æƒ…ç¬¦å·ï¼‰ã€‚åœ¨â€œOur recommendationsâ€ä¸‹æœ‰ä¸‰æ¡è¦ç‚¹ï¼š1. æŠ¥å‘Šæ¨¡å‹é€‰æ‹©æ ‡å‡†ã€‚2. ä½¿ç”¨å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ä½œä¸ºåŸºå‡†ã€‚3. å§‹ç»ˆåº”ç”¨è¿ç»­å¾®è°ƒï¼ˆCFTï¼‰ã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå†™ç€â€œTHANK YOU!â€çš„æ°”æ³¡ï¼Œå³ä¸‹è§’æœ‰ä¸€ä¸ªäºŒç»´ç ã€‚</sample>
    <sample id="939">The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="940">The paper has five authors.</sample>
    <sample id="941">To answer the question, you need to know that Servin is a judge and Kea is a baker.</sample>
    <sample id="942">Yes, the code is public. You can find it on GitHub at the URL: https://github.com/mpoems/kitmus</sample>
    <sample id="943">The video does not provide information about the distribution of NLPositionality's annotators across various demographic features such as countries/regions or genders.</sample>
    <sample id="944">The image contains a chart with various lines representing different perturbations, and the y-axis is labeled as 'Î” Accuracy'.</sample>
    <sample id="945">è¿›è¡Œç»´åº¦è¯„ä¼°æ„å‘³ç€å¯¹å¯¹è¯è´¨é‡çš„å„ä¸ªæ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚</sample>
    <sample id="946">The authors of this paper belong to the following institutions: 1. University of Science and Technology of China 2. Microsoft Research Asia 3. Beijing Jiaotong University 4. Sony AI 5. Microsoft STC Asia</sample>
    <sample id="947">æç¤ºçš„å½¢å¼åœ¨å•æ¬¡å’Œé›¶æ¬¡æç¤ºçš„æƒ…å†µä¸‹éå¸¸é‡è¦ã€‚</sample>
    <sample id="978">The models that the author assessed are 'BART-FID-RAG', 'Blender2', 'Emora', and 'Blender-Decode'.</sample>
    <sample id="979">There are 10 authors in this paper.</sample>
    <sample id="980">The ideal qualities of a good planner include the ability to understand and interpret different constraints, as well as effectively planning actions based on these constraints.</sample>
    <sample id="981">The paper has 7 authors.</sample>
    <sample id="982">The speaker's name is Vasudha Varadarajan.</sample>
    <sample id="983">The authors of this paper belong to the Institute of Computer Science, Polish Academy of Sciences and University of Warsaw.</sample>
    <sample id="1021">PaLMæœ€å¸¸è§çš„é”™è¯¯æ˜¯é—æ¼é”™è¯¯ã€‚</sample>
    <sample id="1022">è¿™æ®µå†…å®¹ä»‹ç»äº†Emoryå¤§å­¦çš„ç ”ç©¶ï¼Œè®¨è®ºäº†å¦‚ä½•è¯„ä¼°èŠå¤©å¯¼å‘å¯¹è¯ç³»ç»Ÿã€‚</sample>
    <sample id="1023">è¿™æ®µå†…å®¹çš„ä¸­æ–‡ç¿»è¯‘å¦‚ä¸‹ï¼š ã€Šä¸è¦å¿˜è®°ä½ çš„ABCï¼šåœ¨èŠå¤©å¯¼å‘çš„å¯¹è¯ç³»ç»Ÿä¸­è¯„ä¼°æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‹ Sarah E. Finch, James D. Finch å’Œ Jinho D. Choi è¿™é¡¹å·¥ä½œæ˜¯ç”± Emory NLP å®éªŒå®¤å®Œæˆçš„ï¼Œè¯¥å®éªŒå®¤ç”± Gino Choi æ•™æˆé¢†å¯¼ï¼Œä½äº Emory å¤§å­¦ï¼Œå¹¶ä¸ Amazon Alexa AI åˆä½œã€‚</sample>
    <sample id="1024">Emoryå¤§å­¦çš„æ¼”è®²å¹»ç¯ç‰‡å±•ç¤ºäº†â€œComparative Evaluationâ€è¿™ä¸€ä¸»é¢˜ã€‚å·¦ä¾§æœ‰ä¸€ä¸ªé»‘å‘äººç‰©å’Œä¸¤ä¸ªè“è‰²å¯¹è¯æ¡†ï¼Œå³ä¾§æœ‰å¦ä¸€ä¸ªé»‘å‘äººç‰©å’Œä¸‰ä¸ªç´«è‰²å¯¹è¯æ¡†ã€‚</sample>
    <sample id="1025">å°†è¿™æ®µå†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼šå¸¸è§çš„åšæ³•æ˜¯ä½¿ç”¨äººç±»è¯„ä¼°ï¼Œä¾‹å¦‚é€šè¿‡è¦æ±‚äººç±»è£åˆ¤å‘˜é€‰æ‹©ä¸¤ä¸ªå¯¹è¯ä¸­å“ªä¸ªæ›´å¥½ï¼Œæˆ–è€…ç»™å¯¹è¯æ‰“åˆ†ã€‚</sample>
    <sample id="1026">Emoryå¤§å­¦çš„æ ‡å¿—å’ŒAlexaçš„æ ‡å¿—åœ¨å¹»ç¯ç‰‡çš„å·¦ä¸‹è§’ã€‚</sample>
    <sample id="1027">è¿™æ˜¯ä¸€å¼ å…³äºLikertè¯„åˆ†è¯„ä¼°çš„æ¼”ç¤ºå¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡é¡¶éƒ¨æœ‰ä¸€ä¸ªè“è‰²æ¨ªå¹…ï¼Œä¸Šé¢å†™ç€â€œLikert Rating Evaluationâ€ã€‚åœ¨æ¨ªå¹…ä¸‹é¢ï¼Œæœ‰ä¸€äº›æ’å›¾å’Œæ–‡å­—ã€‚å·¦ä¾§æœ‰ä¸€ä¸ªç©¿ç€æ³•å®˜è¢ã€æ‰‹æŒæ³•æ§Œçš„äººç‰©æ’å›¾ã€‚ä¸­é—´æœ‰ä¸‰ä¸ªå¯¹è¯æ°”æ³¡ï¼Œå…¶ä¸­ä¸¤ä¸ªæ˜¯è“è‰²çš„ï¼Œä¸€ä¸ªæ˜¯ç™½è‰²çš„ã€‚å³ä¾§æœ‰ä¸¤ä¸ªå¸¦æœ‰éº¦å…‹é£çš„è“è‰²æœºå™¨äººæ’å›¾ã€‚åœ¨è¿™äº›æ’å›¾ä¸‹æ–¹ï¼Œæœ‰ä¸€ä¸ªä»1åˆ°5çš„è¯„åˆ†åˆ»åº¦ï¼Œå…¶ä¸­ä¸€ä¸ªç‚¹è¢«ç»¿è‰²å‹¾é€‰ã€‚åœ¨è¯„åˆ†åˆ»åº¦ä¸Šæ–¹ï¼Œæœ‰ä¸€è¡Œæ–‡å­—å†™ç€â€œRate the relevance of the bot's responsesâ€ã€‚å¹»ç¯ç‰‡åº•éƒ¨æœ‰Emory Universityå’ŒAlexaçš„æ ‡å¿—ã€‚</sample>
    <sample id="1028">è¿™æ®µå†…å®¹ä»‹ç»äº†ä½¿ç”¨æå…‹ç‰¹é‡è¡¨è¿›è¡Œè¯„ä»·ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒè¦æ±‚å¯¹æœºå™¨äººå›å¤çš„ç›¸å…³æ€§è¿›è¡Œè¯„åˆ†ï¼Œå¹¶æä¾›äº†ä»1åˆ°5çš„è¯„åˆ†èŒƒå›´ã€‚è¯„åˆ†èŒƒå›´å†…çš„ä¸€ä¸ªç‚¹è¢«ç»¿è‰²å‹¾é€‰ï¼Œè¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªå‚è€ƒç‚¹æˆ–ç¤ºä¾‹ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸¤ä¸ªå›¾æ ‡ï¼šä¸€ä¸ªæ˜¯å¸¦æœ‰æ³•æ§Œå’Œå·è½´çš„äººç‰©å½¢è±¡ï¼Œå¦ä¸€ä¸ªæ˜¯Alexaçš„æ ‡å¿—ï¼Œè¿™è¡¨æ˜è¯„ä¼°å¯èƒ½ä¸æ³•å¾‹å¯¹è¯æˆ–Alexaç›¸å…³çš„å¯¹è¯æœ‰å…³ã€‚</sample>
    <sample id="1029">å¹»ç¯ç‰‡ä¸Šæ˜¾ç¤ºäº†Emoryå¤§å­¦å’ŒAlexaçš„æ ‡å¿—ã€‚æ ‡é¢˜ä¸ºâ€œAnnotating Behaviors in Chat (ABC-Eval)â€ã€‚åœ¨å·¦ä¾§ï¼Œæœ‰å››ä¸ªå¯¹è¯æ°”æ³¡ï¼Œæ¯ä¸ªæ°”æ³¡æ—è¾¹éƒ½æœ‰ä¸€ä¸ªå¤´åƒå›¾æ ‡ã€‚ç¬¬ä¸€ä¸ªå¯¹è¯æ°”æ³¡æ˜¯ç°è‰²çš„ï¼Œç¬¬äºŒä¸ªæ˜¯æµ…è“è‰²çš„ï¼Œç¬¬ä¸‰ä¸ªä¹Ÿæ˜¯æµ…è“è‰²çš„ï¼Œç¬¬å››ä¸ªæ˜¯æ·±è“è‰²çš„ã€‚å³ä¾§æœ‰ä¸€ä¸ªæ ‡ç­¾å†™ç€â€œIrrelevantâ€ï¼ŒæŒ‡å‘ç¬¬ä¸‰ä¸ªå¯¹è¯æ°”æ³¡ã€‚å¦ä¸€ä¸ªæ ‡ç­¾å†™ç€â€œLack of Empathy Self Contradictionâ€ï¼ŒæŒ‡å‘ç¬¬å››ä¸ªå¯¹è¯æ°”æ³¡ã€‚</sample>
    <sample id="1030">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="1031">ABC-Evalå¯ä»¥æµ‹é‡èŠå¤©æ¨¡å‹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šä¼šçŠ¯å„ç§ä¸»é¢˜é”™è¯¯ã€‚</sample>
    <sample id="1032">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œABC-Eval Behaviorsâ€çš„å¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡è¢«åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½æœ‰ä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªç©ºç™½çš„çŸ©å½¢æ¡†ã€‚å·¦ä¸Šè§’çš„éƒ¨åˆ†æ ‡æœ‰â€œCoherenceâ€ï¼Œå³ä¸Šè§’çš„éƒ¨åˆ†æ ‡æœ‰â€œKnowledgeâ€ï¼Œå·¦ä¸‹è§’çš„éƒ¨åˆ†æ ‡æœ‰â€œConsistencyâ€ï¼Œå³ä¸‹è§’çš„éƒ¨åˆ†æ ‡æœ‰â€œEmotional Understandingâ€ã€‚åœ¨â€œCoherenceâ€éƒ¨åˆ†ä¸‹æ–¹æœ‰ä¸¤ä¸ªçŸ©å½¢æ¡†ï¼Œåˆ†åˆ«æ ‡æœ‰â€œIgnoring Partnerâ€å’Œâ€œIrrelevantâ€ã€‚å¹»ç¯ç‰‡åº•éƒ¨æ˜¾ç¤ºäº†â€œEmory Universityâ€çš„æ ‡å¿—å’Œâ€œalexaâ€çš„æ ‡å¿—ã€‚</sample>
    <sample id="1033">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š 1. æ ‡é¢˜ï¼šABC-Eval Behaviors 2. å››ä¸ªä¸»è¦éƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†åŒ…å«ä¸åŒçš„è¡Œä¸ºç±»åˆ«ï¼š - Coherence: Ignoring Partner, Irrelevant - Knowledge: Incorrect Fact, Commonsense Violation - Consistency: Self Contradiction, Partner Contradiction - Emotional Understanding: Empathetic Response, Lack of Empathy 3. åº•éƒ¨æœ‰ä¸¤å¤„æ ‡å¿—å’Œæ–‡å­—ï¼š - Emory Universityçš„æ ‡å¿—å’Œåç§° - Alexaçš„æ ‡å¿—</sample>
    <sample id="1034">å¹»ç¯ç‰‡ä»‹ç»äº†å…³äºå¯¹è¯æ¨¡å‹çš„å®éªŒã€‚å®ƒæåˆ°äº†ä½¿ç”¨å››ç§å¼€æ”¾å¼é¢†åŸŸå¯¹è¯æ¨¡å‹ï¼Œå¹¶å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œäº†100æ¬¡äºº-æœºå™¨äººå¯¹è¯ã€‚</sample>
    <sample id="1035">å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªæ ‡é¢˜â€œExperimentsâ€ï¼Œä¸‹é¢æœ‰ä¸¤ä¸ªé¡¹ç›®ç¬¦å·ã€‚ç¬¬ä¸€ä¸ªé¡¹ç›®ç¬¦å·æ˜¯â€œ4 Open-Domain Dialogue Modelsâ€ï¼Œç¬¬äºŒä¸ªé¡¹ç›®ç¬¦å·æ˜¯â€œ100 Human-Bot Conversations per Modelâ€ã€‚åœ¨è¿™äº›é¡¹ç›®ç¬¦å·çš„ä¸‹æ–¹ï¼Œæœ‰ä¸€ä¸ªæ ‡é¢˜ä¸ºâ€œABC-Evalâ€çš„éƒ¨åˆ†ï¼ŒåŒ…å«ä¸€ä¸ªæ’å›¾å’Œä¸€äº›æ–‡æœ¬ã€‚æ’å›¾æ˜¾ç¤ºäº†ä¸€ä¸ªäººå’Œä¸€ä¸ªæœºå™¨äººä¹‹é—´çš„å¯¹è¯æµç¨‹å›¾ã€‚åœ¨â€œABC-Evalâ€éƒ¨åˆ†æ—è¾¹ï¼Œæœ‰å¦ä¸€ä¸ªæ ‡é¢˜ä¸ºâ€œTurn Likertâ€çš„éƒ¨åˆ†ï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªè¯„åˆ†è¡¨ï¼Œä»1åˆ°5ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªå‹¾é€‰æ ‡è®°åœ¨3çš„ä½ç½®ã€‚è¿˜æœ‰ä¸€ä¸ªæ ‡é¢˜ä¸ºâ€œDialogue Likertâ€çš„éƒ¨åˆ†ï¼Œä¹Ÿæ˜¾ç¤ºäº†ä¸€ä¸ªè¯„åˆ†è¡¨ï¼Œä»1åˆ°5ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªå‹¾é€‰æ ‡è®°åœ¨3çš„ä½ç½®ã€‚åœ¨è¿™äº›éƒ¨åˆ†çš„å³ä¾§ï¼Œæœ‰ä¸€ä¸ªæ ‡é¢˜ä¸ºâ€œComparativeâ€çš„éƒ¨åˆ†ï¼Œæ˜¾ç¤ºäº†å¤šä¸ªå¯¹è¯æµç¨‹å›¾å’Œä¸€ä¸ªå‹¾é€‰æ ‡è®°ã€‚å¹»ç¯ç‰‡åº•éƒ¨æœ‰Emoryå¤§å­¦å’ŒAlexaçš„æ ‡å¿—ã€‚</sample>
    <sample id="1036">è¿™æ®µå†…å®¹ä»‹ç»äº†å¯¹å››ç§å¼€æ”¾åŸŸå¯¹è¯æ¨¡å‹çš„å®éªŒï¼Œæ¯ä¸ªæ¨¡å‹éƒ½æœ‰100ä¸ªäººä¸æœºå™¨äººè¿›è¡Œå¯¹è¯ã€‚å®éªŒåŒ…æ‹¬ä¸‰ç§è¯„ä¼°æ–¹æ³•ï¼šTurn Likertã€Dialogue Likertå’ŒComparativeã€‚è¿™äº›æ–¹æ³•ç”¨äºè¯„ä¼°ä¸€è‡´æ€§ã€æƒ…æ„Ÿç†è§£ã€ä¿¡æ¯ä¸°å¯Œæ€§ã€æ•´ä½“è´¨é‡ã€å¸å¼•åŠ›ã€è¯­æ³•å‡†ç¡®æ€§ã€ä¸»åŠ¨æ€§ä»¥åŠç›¸å…³æ€§ã€‚</sample>
    <sample id="1037">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œInter-Annotator Agreementâ€çš„å›¾è¡¨ã€‚å›¾è¡¨çš„æ ‡é¢˜æ˜¯â€œInter-Annotator Agreementâ€ï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªé»„è‰²ç®­å¤´æŒ‡å‘å›¾è¡¨çš„é¡¶éƒ¨ï¼ŒæŒ‡ç¤ºäº†Krippendorf's Alphaå€¼ã€‚å›¾è¡¨ä¸‹æ–¹æœ‰å¤šä¸ªæ ‡ç­¾ï¼ŒåŒ…æ‹¬â€œABC-Evalâ€ã€â€œTurn Likertâ€ã€â€œDialogue Likertâ€å’Œâ€œComparativeâ€ã€‚æ¯ä¸ªæ ‡ç­¾ä¸‹é¢éƒ½æœ‰å¤šä¸ªæ•°æ®ç‚¹ï¼Œè¿™äº›æ•°æ®ç‚¹ä»£è¡¨äº†ä¸åŒè¯„åˆ†è€…ä¹‹é—´çš„åŒæ„ç¨‹åº¦ã€‚å›¾è¡¨çš„èƒŒæ™¯æ˜¯ç™½è‰²çš„ï¼Œé¡¶éƒ¨æœ‰ä¸€ä¸ªè“è‰²çš„æ¨ªå¹…ï¼Œä¸Šé¢å†™ç€â€œInter-Annotator Agreementâ€ã€‚åœ¨å³ä¸Šè§’ï¼Œæœ‰ä¸€ä¸ªå°çª—å£æ˜¾ç¤ºäº†ä¸€ä½æ¼”è®²è€…çš„å›¾åƒã€‚åœ¨å·¦ä¸‹è§’ï¼Œæœ‰Emoryå¤§å­¦å’ŒAlexaçš„æ ‡å¿—ã€‚</sample>
    <sample id="1038">å›¾è¡¨æ˜¾ç¤ºäº†é¢„æµ‹æœ‰æ•ˆæ€§ï¼Œæ¯”è¾ƒäº†äº¤äº’å¼è¯„ä»·å’Œäº¤äº’å¼é—®ç­”çš„ç™¾åˆ†æ¯”ã€‚</sample>
    <sample id="1039">è¿™å¼ å¹»ç¯ç‰‡å±•ç¤ºäº†é¢„æµ‹æœ‰æ•ˆæ€§ï¼Œé€šè¿‡æ¯”è¾ƒäº’åŠ¨æ€§è¯„åˆ†å’Œäº’åŠ¨æ€§è¯„åˆ†åœ¨ä¸åŒç±»åˆ«ï¼ˆABC-Evalã€Turn Likertã€Dialogue Likertå’ŒComparativeï¼‰ä¸­çš„ç™¾åˆ†æ¯”ã€‚å›¾è¡¨ä¸­ä½¿ç”¨äº†æ¡å½¢å›¾æ¥è¡¨ç¤ºè¿™äº›æ•°æ®ï¼Œå…¶ä¸­ç°è‰²æ¡ä»£è¡¨äº’åŠ¨æ€§è¯„åˆ†ï¼Œè“è‰²æ¡ä»£è¡¨äº’åŠ¨æ€§è¯„åˆ†ã€‚é»„è‰²ç®­å¤´æŒ‡å‘ç‰¹å®šçš„ç±»åˆ«ï¼Œè¡¨æ˜è¿™äº›ç±»åˆ«åœ¨é¢„æµ‹æœ‰æ•ˆæ€§æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚å¹»ç¯ç‰‡é¡¶éƒ¨æœ‰ä¸€ä¸ªæ ‡é¢˜â€œPredictive Validityâ€ï¼Œåº•éƒ¨æœ‰Emoryå¤§å­¦å’ŒAlexaçš„æ ‡å¿—ã€‚</sample>
    <sample id="1040">æ ‡é¢˜æ˜¯â€œå¢é‡æœ‰æ•ˆæ€§â€ã€‚</sample>
    <sample id="1041">å›¾ä¸­å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œå¢é‡æœ‰æ•ˆæ€§â€çš„å›¾è¡¨ï¼Œå›¾è¡¨çš„æ ‡é¢˜æ˜¯â€œIncremental Validityâ€ã€‚å›¾è¡¨ä¸Šæœ‰ä¸‰ä¸ªè½´ï¼šABC-evalã€Turn UBERTå’ŒDialogue UBERTã€‚æ¯ä¸ªè½´ä¸Šæ ‡æœ‰ä¸åŒæŒ‡æ ‡çš„åç§°ï¼Œå¦‚Jeff Connã€Unempatheticã€Relevantã€Engagingç­‰ã€‚å›¾è¡¨æ˜¾ç¤ºäº†è¿™äº›æŒ‡æ ‡åœ¨è§£é‡Šè´¨é‡æ–¹é¢çš„è´¡çŒ®ç™¾åˆ†æ¯”ï¼ˆ%ï¼‰ã€‚å›¾è¡¨å³ä¸Šè§’æœ‰ä¸€ä¸ªEmory Universityå’ŒAlexaçš„æ ‡å¿—ã€‚</sample>
    <sample id="1042">è¿™å¼ å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œå¢é‡æœ‰æ•ˆæ€§â€ã€‚å®ƒå±•ç¤ºäº†ä¸€ä¸ªæ•£ç‚¹å›¾ï¼Œæ¨ªè½´æ ‡è®°ä¸ºâ€œABC-Evalâ€å’Œâ€œTurn UBERTâ€ï¼Œçºµè½´æ ‡è®°ä¸ºâ€œè§£é‡Šè´¨é‡ç™¾åˆ†æ¯”ï¼ˆ%ï¼‰â€ã€‚å›¾è¡¨ä¸­åŒ…å«å¤šä¸ªæ•°æ®ç‚¹ï¼Œæ¯ä¸ªæ•°æ®ç‚¹æ—è¾¹éƒ½æœ‰æ ‡ç­¾ã€‚è¿™äº›æ ‡ç­¾åŒ…æ‹¬â€œEmpatheticâ€ã€â€œRelevantâ€ã€â€œEngagingâ€ã€â€œProactiveâ€ã€â€œEmotionâ€å’Œâ€œUnempatheticâ€ã€‚å›¾è¡¨è¿˜åŒ…æ‹¬ä¸€æ¡ä»å·¦ä¸‹è§’åˆ°å³ä¸Šè§’çš„æ›²çº¿ï¼Œæ ‡è®°ä¸ºâ€œå†—ä½™â€ã€‚åœ¨å³ä¸Šè§’æœ‰ä¸€ä¸ªç®­å¤´æŒ‡å‘ä¸€ä¸ªç‰¹å®šçš„æ•°æ®ç‚¹ï¼Œè¯¥ç‚¹ä½äºâ€œRelevantâ€å’Œâ€œEmpatheticâ€ä¹‹é—´ã€‚å¹»ç¯ç‰‡åº•éƒ¨æ˜¾ç¤ºäº†â€œåŸƒé»˜é‡Œå¤§å­¦â€çš„æ ‡å¿—å’Œâ€œalexaâ€çš„æ ‡å¿—ã€‚</sample>
    <sample id="1043">è¿™äº›å¯é çš„ã€ä¿¡æ¯ä¸°å¯Œçš„å’Œç‹¬ç‰¹çš„ABC-EVALæŒ‡æ ‡ä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥æ¯”ä»¥å‰æ–¹æ³•æ›´é«˜çš„åˆ†è¾¨ç‡æ¥è¯„ä¼°å¯¹è¯å¼äººå·¥æ™ºèƒ½ã€‚</sample>
    <sample id="1044">å›¾è¡¨æ˜¾ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å„ç§æŒ‘æˆ˜ä¸‹çš„é”™è¯¯ç‡ï¼ŒåŒ…æ‹¬åç¤¾ä¼šã€å¸¸è¯†è¿åã€å¿½ç•¥ã€ä¸æ­£ç¡®ã€ç¼ºä¹åŒæƒ…å¿ƒç­‰ã€‚æ¯ä¸ªæ¨¡å‹çš„è¡¨ç°éƒ½ç”¨ä¸åŒçš„é¢œè‰²è¡¨ç¤ºï¼Œä¾¿äºæ¯”è¾ƒã€‚</sample>
    <sample id="1045">å›¾è¡¨å±•ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨ABCè¯„ä¼°ä¸­çš„é”™è¯¯ç‡ã€‚å›¾è¡¨çš„æ ‡é¢˜æ˜¯â€œABC-Eval Error Rates by Modelâ€ã€‚å›¾è¡¨çš„çºµè½´è¡¨ç¤ºç™¾åˆ†æ¯”ï¼Œæ¨ªè½´åˆ—å‡ºäº†ä¸åŒçš„é”™è¯¯ç±»å‹ï¼ŒåŒ…æ‹¬åç¤¾ä¼šã€CSåå‘ã€å¿½ç•¥ã€é”™è¯¯ã€ä¸ç›¸å…³ã€å…¶ä»–åå‘ã€å†—ä½™ã€è‡ªæˆ‘åå‘ã€è¯é¢˜è½¬æ¢å’Œæœªè§£é‡Šã€‚å›¾è¡¨ä¸‹æ–¹åˆ—å‡ºäº†äº”ä¸ªä¸åŒçš„æ¨¡å‹ï¼šBART-FID-RAGã€Blender2ã€Emoraå’ŒBlender-Decodeã€‚å›¾è¡¨ä¸­ä½¿ç”¨äº†é»„è‰²ç®­å¤´æ¥çªå‡ºæ˜¾ç¤ºæŸäº›é”™è¯¯ç±»å‹ã€‚</sample>
    <sample id="1046">å›¾è¡¨æ˜¾ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨ABC-Evalä¸­çš„é”™è¯¯ç‡ï¼Œä»¥ç™¾åˆ†æ¯”è¡¨ç¤ºã€‚æ¯ä¸ªæŸ±çŠ¶å›¾ä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„é”™è¯¯ç±»å‹ï¼Œå¦‚åç¤¾ä¼šã€CSç›¸åã€å¿½ç•¥ã€é”™è¯¯ã€ä¸ç›¸å…³ã€æ²¡æœ‰åŒæƒ…å¿ƒã€å…¶ä»–ã€å†—ä½™ã€è‡ªæˆ‘ã€ä¸»é¢˜è½¬æ¢å’Œæœªè§£é‡Šã€‚æŸ±çŠ¶å›¾çš„é¢œè‰²å¯¹åº”ä¸åŒçš„æ¨¡å‹ï¼šBART-FID-RAGï¼ˆç»¿è‰²ï¼‰ã€Blender2ï¼ˆè“è‰²ï¼‰ã€Emoraï¼ˆçº¢è‰²ï¼‰å’ŒBlender-Decodeï¼ˆç´«è‰²ï¼‰ã€‚å›¾è¡¨çš„æ ‡é¢˜æ˜¯â€œABC-Evalé”™è¯¯ç‡æŒ‰æ¨¡å‹â€ï¼Œå¹¶åŒ…æ‹¬Emoryå¤§å­¦å’ŒAlexaçš„æ ‡å¿—ã€‚</sample>
    <sample id="1047">å¹»ç¯ç‰‡æ˜¾ç¤ºäº†ä¸€ä¸ªå›¾è¡¨ï¼Œæ ‡é¢˜ä¸ºâ€œABC-Evalé”™è¯¯ç‡æŒ‰æ¨¡å‹â€ï¼Œå±•ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å„ç§ç±»åˆ«ä¸­çš„é”™è¯¯ç‡ã€‚è¿™äº›ç±»åˆ«åŒ…æ‹¬åç¤¾ä¼šã€CSåå·®ã€å¿½ç•¥ã€é”™è¯¯ã€ä¸ç›¸å…³ã€ä¸å…±æƒ…ã€å…¶ä»–åå·®ã€å†—ä½™ã€è‡ªæˆ‘åå·®å’Œè¯é¢˜åˆ‡æ¢ã€‚æ¯ä¸ªç±»åˆ«éƒ½æœ‰å¤šä¸ªæ¡å½¢å›¾ï¼Œä»£è¡¨ä¸åŒçš„æ¨¡å‹ï¼šBART-FID-RAGã€Blender2ã€Emoraå’ŒBlender-Decodeã€‚å›¾è¡¨çš„Yè½´è¡¨ç¤ºç™¾åˆ†æ¯”çš„è½®æ¬¡ï¼ŒèŒƒå›´ä»0åˆ°30ã€‚å›¾è¡¨ä¸‹æ–¹æœ‰Emoryå¤§å­¦å’ŒAlexaçš„æ ‡å¿—ã€‚å¹»ç¯ç‰‡é¡¶éƒ¨æœ‰ä¸€ä¸ªæ„Ÿè°¢è§‚çœ‹çš„æ¨ªå¹…ï¼Œåº•éƒ¨æä¾›äº†è®ºæ–‡é“¾æ¥ã€GitHubå­˜å‚¨åº“é“¾æ¥å’Œè”ç³»ä¿¡æ¯ã€‚</sample>
    <sample id="1048">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„æ˜¯åŸƒé»˜é‡Œå¤§å­¦å’ŒåŸƒé»˜é‡ŒNLPç ”ç©¶å®éªŒå®¤ã€‚</sample>
    <sample id="1049">CFT stands for continuous fine-tuning.</sample>
    <sample id="1050">è¿™ç¯‡è®ºæ–‡æœ‰8ä½ä½œè€…ã€‚</sample>
    <sample id="1051">è¿™æ˜¯ä¸€å¼ å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œç¿»è¯‘ä½•æ—¶éœ€è¦ä¸Šä¸‹æ–‡ï¼ŸåŸºäºæ•°æ®çš„å¤šè¯­è¨€æ¢ç´¢â€ã€‚ä½œè€…åŒ…æ‹¬Patrick Fernandesã€Kayo Yinã€Emmy Liuã€AndrÃ© F. T. Martinså’ŒGraham Neubigã€‚å¹»ç¯ç‰‡ä¸Šè¿˜æ˜¾ç¤ºäº†Carnegie Mellon University Language Technologies Instituteã€TÃ©cnico Lisboaã€BAIRï¼ˆä¼¯å…‹åˆ©äººå·¥æ™ºèƒ½ç ”ç©¶ï¼‰å’ŒUnbabelçš„æ ‡å¿—ã€‚</sample>
    <sample id="1052">ç¿»è¯‘å–å†³äºä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬å¾— rid of that mole.</sample>
    <sample id="1053">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š 1. "Translation depends on context" 2. "Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole." 3. "Could it be anything serious, Doctor? We'll have to get rid of that mole."</sample>
    <sample id="1054">ç¿»è¯‘å–å†³äºä¸Šä¸‹æ–‡ã€‚</sample>
    <sample id="1055">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š 1. æ ‡é¢˜ï¼šEvaluating context-dependent translation is hard 2. å‰¯æ ‡é¢˜æˆ–è¦ç‚¹ï¼šOnly a small portion of words depend on context 3. å­è¦ç‚¹ï¼š- Corpus-level metrics - blue 4. å…¶ä»–æ–‡æœ¬ï¼š- unabled to capture these translations</sample>
    <sample id="1056">Evaluating context-dependent translation is hard. Only a small portion of words depend on context. Corpus-level metrics Existing methods support limited discourse phenomena and languages.</sample>
    <sample id="1057">è¿™ä¸¤æ®µæ–‡å­—æå‡ºäº†ä¸¤ä¸ªç ”ç©¶é—®é¢˜ã€‚ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯â€œç¿»è¯‘ä½•æ—¶éœ€è¦ä¸Šä¸‹æ–‡ï¼Ÿâ€ç¬¬äºŒä¸ªé—®é¢˜æ˜¯â€œæ¨¡å‹å¦‚ä½•å¤„ç†ä¸Šä¸‹æ–‡ç›¸å…³çš„ç¿»è¯‘ï¼Ÿâ€</sample>
    <sample id="1058">ä¸ºäº†å›ç­”ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡æµ‹é‡ç¿»è¯‘ä¸­å•è¯ä¾èµ–äºä¸Šä¸‹æ–‡çš„ç¨‹åº¦æ¥å¼€å§‹ã€‚</sample>
    <sample id="1059">å¹»ç¯ç‰‡ä»‹ç»äº†æ¡ä»¶äº’ä¿¡æ¯ï¼ˆCXMIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¡¡é‡æœºå™¨ç¿»è¯‘æ¨¡å‹åœ¨ç»™å®šè¯­æ–™åº“ä¸­ä½¿ç”¨ä¸Šä¸‹æ–‡çš„ç¨‹åº¦çš„æŒ‡æ ‡ã€‚CXMIé€šè¿‡æµ‹é‡ä¸Šä¸‹æ–‡Cæä¾›çš„å…³äºç›®æ ‡Yçš„ä¿¡æ¯ï¼Œç»™å®šæºXæ¥è¯„ä¼°ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</sample>
    <sample id="1060">Conditional Cross-Mutual Information (CXMI) measures how much context machine translation models use given a corpus.</sample>
    <sample id="1061">å¹»ç¯ç‰‡ä»‹ç»äº†P-CXMIçš„æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¡¡é‡ç‰¹å®šä¸Šä¸‹æ–‡ä½¿ç”¨æƒ…å†µçš„æŒ‡æ ‡ã€‚å®ƒåŒ…æ‹¬ä¸€ä¸ªå…¬å¼ï¼Œç”¨äºè®¡ç®—å¥å­å’Œå•è¯çº§åˆ«çš„P-CXMIå€¼ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜è®¨è®ºäº†é«˜P-CXMIå•è¯éœ€è¦ä¸Šä¸‹æ–‡è¿›è¡Œç¿»è¯‘çš„æƒ…å†µã€‚</sample>
    <sample id="1062">RQ1: ç¿»è¯‘ä½•æ—¶éœ€è¦ä¸Šä¸‹æ–‡ï¼Ÿ - è¯æ±‡çº§åˆ«çš„ä¸Šä¸‹æ–‡ä½¿ç”¨ - ä¸»é¢˜åˆ†æ RQ2: æ¨¡å‹å¦‚ä½•å¤„ç†ä¸Šä¸‹æ–‡ç›¸å…³çš„ç¿»è¯‘ï¼Ÿ</sample>
    <sample id="1063">ç”»é¢ä¸­å±•ç¤ºäº†ä¸€ä¸ªTEDæ ‡å¿—ï¼Œæ—è¾¹å†™ç€â€œIDEAS WORTH SPREADINGâ€ã€‚åœ¨é¡¶éƒ¨æœ‰ä¸€è¡Œæ–‡å­—ï¼Œå†™ç€â€œé«˜P-CXMLå•è¯çš„ä¸»é¢˜åˆ†æâ€ã€‚å³ä¾§æœ‰ä¸€ä¸ªåœ†å½¢åŒºåŸŸï¼Œé‡Œé¢æ˜¾ç¤ºäº†ä¸€ä½å¥³å£«çš„å›¾åƒã€‚ç”»é¢å³ä¾§è¿˜åˆ—å‡ºäº†å¤šç§è¯­è¨€ï¼ŒåŒ…æ‹¬è‹±è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€å¾·è¯­ã€è¥¿ç­ç‰™è¯­ã€æ³•è¯­ã€æ„å¤§åˆ©è¯­ã€æ—¥è¯­ã€éŸ©è¯­ã€è·å…°è¯­ã€è‘¡è„ç‰™è¯­ã€ç½—é©¬å°¼äºšè¯­ã€ä¿„è¯­ã€åœŸè€³å…¶è¯­å’Œä¸­æ–‡ã€‚</sample>
    <sample id="1064">å¹»ç¯ç‰‡æ˜¾ç¤ºäº†æ ‡é¢˜â€œé«˜P-CXMIå•è¯çš„ä¸»é¢˜åˆ†æâ€ã€‚åœ¨æ ‡é¢˜ä¸‹æ–¹ï¼Œæœ‰ä¸€ä¸ªç¼–å·ä¸º1çš„é¡¹ç›®ï¼Œå†™ç€â€œPOSæ ‡è®°â€ã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°åœ†å½¢å›¾åƒã€‚</sample>
    <sample id="1065">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œé«˜P-CXMIè¯çš„é¢˜æ„åˆ†æâ€ã€‚åœ¨æ ‡é¢˜ä¸‹æ–¹ï¼Œæœ‰ä¸€ä¸ªå‰¯æ ‡é¢˜â€œ1. POSæ ‡è®°â€ï¼Œè¡¨æ˜è¿™æ˜¯å…³äºéƒ¨åˆ†æ ‡è®°ï¼ˆPOSï¼‰æ ‡è®°çš„è®¨è®ºã€‚åœ¨å·¦ä¾§ï¼Œæœ‰ä¸€å¼ å›¾è¡¨ï¼Œæ ‡é¢˜ä¸ºâ€œé˜¿æ‹‰ä¼¯è¯­ä¸­çš„P-CXMIè¯çš„POSæ ‡è®°â€ã€‚å›¾è¡¨æ˜¾ç¤ºäº†ä¸‰ä¸ªç±»åˆ«ï¼šPRON 3.Singã€PRON 3.Dualå’ŒPRON 3.Plurã€‚æ¯ä¸ªç±»åˆ«çš„å€¼éƒ½ç›¸åŒï¼Œè¡¨ç¤ºè¿™äº›ç±»åˆ«çš„P-CXMIå€¼ç›¸ç­‰ã€‚åœ¨å³ä¾§ï¼Œæœ‰ä¸€ä¸ªçŸ©å½¢æ¡†ï¼Œä¸Šé¢å†™ç€â€œä»£è¯â€ï¼Œè¡¨æ˜å¹»ç¯ç‰‡çš„é‡ç‚¹æ˜¯ä»£è¯ã€‚å¹»ç¯ç‰‡çš„æ•´ä½“å¸ƒå±€æ¸…æ™°ï¼Œå·¦ä¾§æœ‰å›¾è¡¨ï¼Œå³ä¾§æœ‰æ–‡æœ¬æ¡†ï¼ŒèƒŒæ™¯é¢œè‰²ä¸ºæµ…ç´«è‰²ã€‚</sample>
    <sample id="1066">å›¾ç‰‡æ˜¾ç¤ºäº†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œé«˜P-CXMLå•è¯çš„ä¸»é¢˜åˆ†æâ€ã€‚å¹»ç¯ç‰‡åˆ†ä¸ºä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ã€‚å·¦ä¾§æœ‰ä¸€ä¸ªå›¾è¡¨ï¼Œé¡¶éƒ¨æ ‡æœ‰â€œP-CXMLä¸­POSæ ‡è®°çš„ä»£è¯â€ï¼Œä¸‹æ–¹åˆ—å‡ºäº†ä¸‰ä¸ªé¡¹ç›®ï¼šâ€œPRON_3_Singâ€ã€â€œPRON_3_Dualâ€å’Œâ€œPRON_3_Plurâ€ã€‚æ¯ä¸ªé¡¹ç›®çš„å€¼éƒ½çº¦ä¸º0.6ã€‚å³ä¾§æœ‰ä¸€ä¸ªæµ…ç´«è‰²çš„çŸ©å½¢æ¡†ï¼Œé‡Œé¢æœ‰ä¸¤ä¸ªé¡¹ç›®ï¼šâ€œä»£è¯â€å’Œâ€œåŠ¨è¯å½¢å¼â€ã€‚å¹»ç¯ç‰‡çš„èƒŒæ™¯æ˜¯ç™½è‰²çš„ï¼Œå³ä¸Šè§’æœ‰ä¸€å¼ äººçš„ç…§ç‰‡ã€‚</sample>
    <sample id="1067">Thematic analysis of high P-CXML words 1. POS tags 2. Vocabulary items Avelile's mother was still asleep. Avelile went to school. é˜¿ç»´åˆ©å°”çš„æ¯äº²è¿˜åœ¨ç¡è§‰ã€‚ é˜¿ç»´åˆ©å°”å»ä¸Šå­¦äº†ã€‚ - Pronouns - Verb form - Lexical cohesion</sample>
    <sample id="1068">ä¸»é¢˜åˆ†æé«˜P-CXMIå•è¯ 1. POSæ ‡è®° 2. è¯æ±‡é¡¹ - ä¸»æ ¼ - åŠ¨è¯å½¢å¼ - è¯­æ³•è¿è´¯æ€§ - æ­£å¼æ€§ Avelileçš„æ¯äº²ä»åœ¨ç¡è§‰ã€‚é˜¿ç»´åˆ©åˆ©çš„æ¯äº²è¿˜åœ¨ç¡è§‰ã€‚ Avelileå»ä¸Šå­¦äº†ã€‚</sample>
    <sample id="1069">å¹»ç¯ç‰‡æ˜¾ç¤ºäº†å¯¹é«˜P-CXMLå•è¯çš„ä¸»é¢˜åˆ†æã€‚ä¸»è¦å†…å®¹åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼š1. è¯æ€§æ ‡è®°ï¼ˆPOS tagsï¼‰2. è¯æ±‡é¡¹3. ä¸ªä½“ä»¤ç‰Œã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªç´«è‰²æ¡†ï¼Œåˆ—å‡ºäº†ä»¥ä¸‹é¡¹ç›®ï¼š- ä»£è¯- åŠ¨è¯å½¢å¼- è¯æ±‡è¿è´¯æ€§- æ­£å¼æ€§- çœç•¥ã€‚å¹»ç¯ç‰‡ä¸Šè¿˜å±•ç¤ºäº†ä¸¤ä¸ªå¥å­çš„è‹±æ–‡å’Œå¾·æ–‡ç¿»è¯‘ï¼Œåˆ†åˆ«æ˜¯â€œå¥¹çŸ¥é“æˆ‘ä»¬åœ¨å“ªé‡Œã€‚â€å’Œâ€œå¥¹ä¸ä»‹æ„ã€‚â€ä»¥åŠâ€œå¥¹ä¸çŸ¥é“æˆ‘ä»¬åœ¨å“ªé‡Œã€‚â€å’Œâ€œå¥¹ä¸ä»‹æ„ã€‚â€</sample>
    <sample id="1070">ç ”ç©¶é—®é¢˜1ï¼šç¿»è¯‘ä½•æ—¶éœ€è¦ä¸Šä¸‹æ–‡ï¼Ÿ - å•è¯çº§åˆ«çš„ä¸Šä¸‹æ–‡ä½¿ç”¨ - ä¸»é¢˜åˆ†æ ç ”ç©¶é—®é¢˜2ï¼šæ¨¡å‹åœ¨å¤„ç†ä¸Šä¸‹æ–‡ç›¸å…³çš„ç¿»è¯‘æ—¶è¡¨ç°å¦‚ä½•ï¼Ÿ - å¤šè¯­è¨€è¯­å¢ƒæ„ŸçŸ¥ï¼ˆMuDAï¼‰åŸºå‡†</sample>
    <sample id="1071">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œMultilingual Discourse-Aware (MuDA) taggerâ€ã€‚å·¦ä¾§æœ‰ä¸€ä¸ªç´«è‰²çš„æ¡†ï¼Œé‡Œé¢åˆ—å‡ºäº†äº”ä¸ªé¡¹ç›®ï¼š- Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis å³ä¸Šè§’æœ‰ä¸€å¼ äººç‰©ç…§ç‰‡ã€‚</sample>
    <sample id="1072">å¤šè¯­ç§æ„è¯†çš„è¯­ç¯‡æ„ŸçŸ¥ï¼ˆMuDAï¼‰æ ‡è®°å™¨ã€‚</sample>
    <sample id="1073">å¹»ç¯ç‰‡ä¸Šæ˜¾ç¤ºäº†â€œMuDA benchmarkâ€å­—æ ·ï¼Œå·¦ä¾§æœ‰ä¸‰ä»½æ–‡ä»¶çš„å›¾æ ‡ï¼Œå³ä¾§æœ‰ä¸€ä¸ªæœºå™¨äººå›¾æ ‡ã€‚åœ¨ä¸­é—´ï¼Œæœ‰ä¸€æ®µæ–‡å­—å’Œä¸€ä¸ªæµç¨‹å›¾ï¼Œæè¿°äº†ä½¿ç”¨MuDAæ ‡è®°å™¨çš„è¿‡ç¨‹ã€‚</sample>
    <sample id="1074">RQ1: å½“ç¿»è¯‘éœ€è¦ä¸Šä¸‹æ–‡æ—¶ï¼Ÿ - å•è¯çº§åˆ«çš„ä¸Šä¸‹æ–‡ä½¿ç”¨ - ä¸»é¢˜åˆ†æ RQ2: æ¨¡å‹åœ¨å¤„ç†ä¸Šä¸‹æ–‡ç›¸å…³çš„ç¿»è¯‘æ–¹é¢è¡¨ç°å¦‚ä½•ï¼Ÿ - å¤šè¯­ç§è¯è¯­æ„è¯†ï¼ˆMuDAï¼‰åŸºå‡† - æ¨¡å‹è¯„ä¼°</sample>
    <sample id="1075">é¦–å…ˆï¼Œå½“æˆ‘ä»¬ä½¿ç”¨è¯­æ–™åº“çº§åˆ«çš„æŒ‡æ ‡æ—¶ï¼Œå¯¹äºBLEUæ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°è‡ªé€‚åº”æ¨¡å‹å…·æœ‰æœ€ä½³æ€§èƒ½ã€‚</sample>
    <sample id="1076">å¹»ç¯ç‰‡æ˜¾ç¤ºäº†ä¸‰ä¸ªæœºå™¨äººï¼Œæ¯ä¸ªæœºå™¨äººéƒ½ä»£è¡¨ä¸åŒçš„è¯­æ–™åº“çº§åˆ«æŒ‡æ ‡ã€‚ç¬¬ä¸€ä¸ªæœºå™¨äººæ ‡æœ‰â€œBLEUâ€ï¼Œç¬¬äºŒä¸ªæœºå™¨äººæ ‡æœ‰â€œCOMETâ€ï¼Œç¬¬ä¸‰ä¸ªæœºå™¨äººæ ‡æœ‰â€œF-measureâ€ã€‚æ¯ä¸ªæœºå™¨äººçš„å¤´éƒ¨éƒ½æ ‡æœ‰â€œä¸Šä¸‹æ–‡â€å­—æ ·ï¼Œè¡¨æ˜è¿™äº›æŒ‡æ ‡ä¸ä¸Šä¸‹æ–‡æœ‰å…³ã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°åœ†å½¢å›¾åƒï¼Œå¯èƒ½è¡¨ç¤ºæ¼”è®²è€…æˆ–æ¼”ç¤ºè€…çš„å¤´åƒã€‚æ ‡é¢˜ä¸ºâ€œè¯­æ–™åº“çº§åˆ«çš„æŒ‡æ ‡â€ï¼Œè¡¨æ˜è¿™äº›æœºå™¨äººä»£è¡¨ç”¨äºè¯„ä¼°è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹æ€§èƒ½çš„ä¸åŒæŒ‡æ ‡ã€‚</sample>
    <sample id="1077">å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªæ ‡é¢˜ï¼Œå†™ç€â€œCorpus-level metricsâ€ã€‚æ ‡é¢˜ä¸‹æ–¹æœ‰ä¸‰ä¸ªæœºå™¨äººå›¾æ ‡ã€‚æ¯ä¸ªæœºå™¨äººéƒ½ä»£è¡¨ä¸åŒçš„æŒ‡æ ‡ï¼šå·¦è¾¹çš„æœºå™¨äººæ ‡æœ‰â€œBLEUâ€ï¼Œä¸­é—´çš„æœºå™¨äººæ ‡æœ‰â€œCOMETâ€ï¼Œå³è¾¹çš„æœºå™¨äººæ ‡æœ‰â€œF-measureâ€ã€‚åœ¨è¿™äº›å›¾æ ‡ä¸Šæ–¹ï¼Œæœ‰ä¸€ä¸ªå¸¦æœ‰â€œCONTEXTâ€å­—æ ·çš„æ¤­åœ†å½¢ã€‚åœ¨å›¾ç‰‡çš„å³ä¸Šè§’ï¼Œæœ‰ä¸€ä¸ªå°çš„åœ†å½¢å›¾åƒï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªäººçš„è„¸ã€‚åœ¨å¹»ç¯ç‰‡åº•éƒ¨ï¼Œæœ‰ä¸€æ®µæ–‡å­—å†™ç€ï¼šâ€œUnclear which system is best for document-level MT with corpus-level metrics alone.â€</sample>
    <sample id="1078">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œMuDAåŸºå‡†ç»“æœâ€ã€‚å®ƒåŒ…å«ä¸€ä¸ªé¡¹ç›®ç¬¦å·ï¼Œå†™ç€â€œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å‹åœ¨æŸäº›ç°è±¡ä¸Šè¡¨ç°æ˜¾è‘—æ›´å¥½ã€‚â€åœ¨ä¸‹é¢ï¼Œæœ‰ä¸€ä¸ªå­é¡¹ç›®ç¬¦å·ï¼Œå†™ç€â€œæ­£å¼æ€§å’Œè¯æ±‡è¿è´¯æ€§â€ã€‚</sample>
    <sample id="1079">å¹»ç¯ç‰‡æ˜¾ç¤ºäº†â€œMuDAåŸºå‡†ç»“æœâ€çš„æ ‡é¢˜ã€‚ä¸»è¦å†…å®¹æ˜¯å…³äºä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å‹åœ¨æŸäº›ç°è±¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæŒ‡å‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å‹åœ¨æ­£å¼æ€§å’Œè¯æ±‡è¿è´¯æ€§æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨çœç•¥å·ã€ä»£è¯å’ŒåŠ¨è¯å½¢å¼æ–¹é¢è¡¨ç°ä¸ä½³ã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªå°åœ†åœˆï¼Œé‡Œé¢æœ‰ä¸€ä¸ªäººçš„å›¾åƒï¼Œä½†æ²¡æœ‰æä¾›ä»»ä½•é¢å¤–çš„ä¿¡æ¯æˆ–ä¸Šä¸‹æ–‡ã€‚</sample>
    <sample id="1080">MuDA benchmark results Context-aware models perform significantly better on some phenomena o âœ…: Formality, lexical cohesion âŒ: Ellipsis, pronouns, verb form DeepL outperforms Google on most phenomena and language pairs* as of April 2021</sample>
    <sample id="1081">MuDA benchmark results</sample>
    <sample id="1082">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œé¡¶éƒ¨æœ‰ä¸€ä¸ªæ ‡é¢˜â€œSummaryâ€ã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸¤ä¸ªä¸»è¦ç‚¹ï¼š 1. â€œç³»ç»Ÿåœ°åœ¨æ— éœ€å…ˆéªŒè¯­è¨€çŸ¥è¯†çš„æƒ…å†µä¸‹è¯†åˆ«è¯è¯­ç°è±¡â€ 2. â€œé¢å‘æ–‡æ¡£çº§æœºå™¨ç¿»è¯‘çš„æ•°æ®é›†æ— å…³åŸºå‡†â€ åœ¨è¿™äº›è¦ç‚¹ä¸‹æ–¹ï¼Œæœ‰ä¸€ç³»åˆ—å›¾æ ‡å’Œæ–‡å­—ã€‚ä»å·¦åˆ°å³ï¼Œè¿™äº›å…ƒç´ åŒ…æ‹¬ï¼š - ä¸€å †æ–‡ä»¶çš„å›¾æ ‡ - æ ‡æœ‰â€œMuDA taggerâ€çš„æ–‡æ¡£å † - æ ‡æœ‰â€œBLUE F-measureâ€çš„æ–‡æ¡£å † - ä¸€ä¸ªæœºå™¨äººå›¾æ ‡ å¹»ç¯ç‰‡çš„å³ä¸Šè§’æ˜¾ç¤ºäº†ä¸€ä¸ªäººçš„éƒ¨åˆ†å›¾åƒã€‚</sample>
    <sample id="1083">å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ã€‚è¯·ç¨ç­‰ç‰‡åˆ»ï¼Œæˆ‘ä¼šå°†å†…å®¹è½¬æ¢æˆä¸­æ–‡å¹¶å‘é€ç»™æ‚¨ã€‚</sample>
    <sample id="1084">æ¼”è®²è€…çš„åå­—æ˜¯Yusen Zhangã€‚</sample>
    <sample id="1121">æ–°æ–¹æ³•æ²¡æœ‰åç§°ã€‚</sample>
    <sample id="1122">ä½œè€…æè¿°â€œæ˜¾æ€§è¯æ±‡â€æ–¹æ³•ä¸ºï¼šæ‰¾åˆ°åŒºåˆ†æ ‡è®°ç»„ä¸æœªæ ‡è®°ç»„çš„å•è¯ã€‚</sample>
    <sample id="1123">The authors of this paper are affiliated with the University of Washington and Carnegie Mellon University's Language Technologies Institute.</sample>
    <sample id="1124">The first mentioned symmetric approach to coordinate structures is the "Conjunction-headed" structure, which originated in Prague.</sample>
    <sample id="1125">The speaker's name is Sarah E. Finch, as indicated in the slide content where it lists "Sarah E. Finch" along with James D. Finch and Jinho D. Choi.</sample>
    <sample id="1126">è¿™ç¯‡è®ºæ–‡æœ‰äº”ä½ä½œè€…ã€‚</sample>
    <sample id="1127">BLiMP, SyntaxGym, and Crows datasets can be used to test sentence phenomena.</sample>
    <sample id="1161">The five methods are abbreviated as FTw, COSINE, L2R, MLC, and BOND.</sample>
    <sample id="1162">è¯¥æ¨¡å‹åœ¨11ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</sample>
    <sample id="1226">CamemBERTæœ€åˆæ˜¯åœ¨4GBçš„æ•°æ®ä¸Šè®­ç»ƒçš„ã€‚</sample>
    <sample id="1227">æ¼”è®²è€…çš„åå­—æ˜¯Adam PrzepiÃ³rkowskiå’Œ MichaÅ‚ WoÅºniakã€‚</sample>
    <sample id="1228">å‘ç°å¯¼è‡´æ€§èƒ½ä¸‹é™çš„ä¸»è¦åŸå› æ˜¯æ—¶é—´æ¼‚ç§»ã€‚</sample>
    <sample id="1269">æ’åˆ—è¾“å‡ºåºåˆ—ä¸­çš„è¯å…ƒæ˜¯ä¸ºäº†ç¡®ä¿å®ƒä»¬æŒ‰ç…§æ­£ç¡®çš„é¡ºåºæ’åˆ—ã€‚</sample>
    <sample id="1270">ä½œè€…å»ºè®®æ¨¡å‹æ‰€æœ‰è€…åº”æé«˜åè§ç¼“è§£æ–¹æ³•çš„é€æ˜åº¦ï¼Œå› ä¸ºè¿™æœ‰åŠ©äºç¡®ä¿è¿™äº›æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå…¬æ­£æ€§ã€‚</sample>
    <sample id="1271">æœ€å°å¯¹ä¸å¯æ¥å—è¾“å…¥æ˜¯ï¼š'Many people were helping themselves.'</sample>
    <sample id="1272">ä½œè€…ä½¿ç”¨äº†ä»¥ä¸‹è¯„ä¼°æŒ‡æ ‡ï¼šNLRã€CERã€NRã€CEã€NRã€CERã€NRã€CERã€NRã€CERã€‚</sample>
    <sample id="1273">ä½¿ç”¨äº†Krippendorf's alphaæŒ‡æ ‡æ¥è¡¡é‡æ³¨é‡Šè€…ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚</sample>
    <sample id="1274">åœ¨ä¸å¯æ¥å—å’Œå¯æ¥å—æŸ¥è¯¢ä¸­ï¼Œé€‰æ‹©Wikipediaä½œä¸ºé¢†åŸŸæ¥æ·»åŠ å®Œå…¨æ— å…³çš„å¥å­ã€‚</sample>
    <sample id="1275">The authors of this paper are affiliated with Heinrich Heine University DÃ¼sseldorf in Germany.</sample>
    <sample id="1276">MultiInstruct focuses on instruction tuning for multimodal pre-trained models, which is different from previous works that mainly focused on language-only tasks.</sample>
    <sample id="1277">è¿™ç¯‡è®ºæ–‡æœ‰ä¸‰ä½ä½œè€…ã€‚</sample>
    <sample id="1278">äºŒè¿›åˆ¶åè°ƒæ˜¯æŒ‡åœ¨è¯­è¨€å¤„ç†ä¸­ï¼Œå°†æ–‡æœ¬åˆ†ä¸ºå­—ç¬¦ã€éŸ³èŠ‚å’Œå•è¯ç­‰ä¸åŒçš„å•ä½è¿›è¡Œåˆ†æã€‚</sample>
    <sample id="1279">åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæç¤ºè¯­çš„å¹³å‡é•¿åº¦æ˜¯10ä¸ªå•è¯ã€‚</sample>
    <sample id="1280">The findings suggest that smaller T5 models, when fine-tuned on Coscript, can generate higher quality scripts compared to larger language models. This implies that with suitable training data and methods, smaller models may be just as effective or even more so than their larger counterparts in specific tasks like script generation.</sample>
    <sample id="1281">DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains</sample>
    <sample id="1282">è¿™æ®µæ–‡å­—ä»‹ç»äº†å…³äºåŒ»ç–—ä¿å¥ä¸­è¯­è¨€å»ºæ¨¡çš„æ€»ç»“ã€‚å®ƒåŒ…æ‹¬äº†é¢„è®­ç»ƒç­–ç•¥ã€æ•°æ®æºå’Œè§„æ¨¡çš„æ¯”è¾ƒï¼Œå¯¹13ä¸ªæ¨¡å‹åœ¨11é¡¹ä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼Œä»¥åŠNACHOSå’ŒDrBERTçš„åˆ†å¸ƒæƒ…å†µã€‚</sample>
    <sample id="1283">å¹»ç¯ç‰‡åŒ…å«ä¸€ä¸ªæ ‡é¢˜ä¸ºâ€œæ‘˜è¦â€çš„éƒ¨åˆ†ï¼Œåˆ—å‡ºäº†ä»¥ä¸‹è¦ç‚¹ï¼š 1. åŒ»ç–—ä¿å¥ä¸­çš„è¯­è¨€å»ºæ¨¡ã€‚ 2. é¢„è®­ç»ƒç­–ç•¥ã€æ•°æ®æºå’Œè§„æ¨¡çš„æ¯”è¾ƒã€‚ 3. å¯¹11ä¸ªä»»åŠ¡çš„13ä¸ªæ¨¡å‹çš„è¯„ä¼°ã€‚ 4. NACHOSå’ŒDrBERTçš„åˆ†å¸ƒã€‚ å¹»ç¯ç‰‡å³ä¸Šè§’æ˜¾ç¤ºäº†ä¸€ä¸ªäººåœ¨è¯´è¯ã€‚ å¹»ç¯ç‰‡åº•éƒ¨æœ‰ä¸€ä¸ªçº¢è‰²æ¨ªå¹…ï¼Œä¸Šé¢å†™ç€â€œAix-Marseille UniversitÃ©â€ã€‚</sample>
    <sample id="1284">å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªæ ‡é¢˜â€œSummaryâ€ï¼Œä¸‹é¢åˆ—å‡ºäº†å››ä¸ªä¸»è¦éƒ¨åˆ†ï¼š 1. Healthcareä¸­çš„è¯­è¨€å»ºæ¨¡ 2. é¢„è®­ç»ƒç­–ç•¥ã€æ•°æ®æºå’Œè§„æ¨¡çš„æ¯”è¾ƒ 3. åœ¨11ä¸ªä»»åŠ¡ä¸­è¯„ä¼°13ä¸ªæ¨¡å‹ 4. NACHOSå’ŒDrBERTçš„åˆ†å¸ƒ Avignon UniversitÃ©çš„æ ‡å¿—ä½äºå³ä¸‹è§’ã€‚</sample>
    <sample id="1285">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œSummaryâ€ã€‚å®ƒåˆ—å‡ºäº†å››ä¸ªä¸»è¦éƒ¨åˆ†ï¼š 1. åŒ»ç–—ä¿å¥ä¸­çš„è¯­è¨€å»ºæ¨¡ã€‚ 2. é¢„è®­ç»ƒç­–ç•¥ã€æ•°æ®æºå’Œè§„æ¨¡çš„æ¯”è¾ƒã€‚ 3. åœ¨11ä¸ªä»»åŠ¡ä¸Šè¯„ä¼°13ä¸ªæ¨¡å‹ã€‚ 4. åˆ†å‘NACHOSå’ŒDrBERTã€‚</sample>
    <sample id="1286">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œLanguage Modelingâ€ã€‚</sample>
    <sample id="1287">è¿™æ®µæ–‡å­—è®¨è®ºäº†è¯­è¨€å»ºæ¨¡ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•ä½¿ç”¨Transformeræ¶æ„çš„æ¨¡å‹ï¼Œå¦‚BERTï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒæŒ‡å‡ºè¿™äº›æ¨¡å‹å·²ç»è¢«æ”¹ç¼–æˆæ³•è¯­ï¼Œå¦‚CamemBERTå’ŒFlauBERTã€‚åœ¨åŒ»ç–—é¢†åŸŸï¼Œç‰¹å®šäºåŸŸçš„æ¨¡å‹åœ¨è‹±è¯­ä¸­çš„è¡¨ç°æ›´å¥½ï¼ŒåŒ…æ‹¬PubMedBERTã€BioBERTã€ClinicalBERTå’Œå…¶ä»–æ¨¡å‹ã€‚æ–‡ç« è¿˜æåˆ°ï¼Œé™¤äº†è‹±è¯­ä¹‹å¤–çš„è¯­è¨€è¾ƒå°‘è§ï¼Œå¹¶ä¸”ä¸»è¦ä¾èµ–äºé¢„å…ˆè®­ç»ƒï¼Œä½¿ç”¨ç°æœ‰çš„é€šç”¨æ¨¡å‹ã€‚å¯¹äºç”Ÿç‰©åŒ»å­¦é¢†åŸŸï¼Œç›®å‰æ²¡æœ‰å¼€æºæ¨¡å‹å¯ç”¨ï¼Œä½†æœ‰å¸Œæœ›ã€‚æœ€åï¼Œæ–‡ç« å¼ºè°ƒäº†åœ¨åŒ»ç–—ä»»åŠ¡ä¸Šä½¿ç”¨ç‰¹å®šäºåŸŸçš„æ¨¡å‹å¯¹æ³•è¯­çš„æ½œåœ¨å¥½å¤„ã€‚</sample>
    <sample id="1288">è¯­è¨€å»ºæ¨¡ Transformer åŸºç¡€æ–¹æ³•ï¼Œä¾‹å¦‚ BERTï¼Œåœ¨è®¸å¤š NLP ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå·¨å¤§çš„æ€§èƒ½æå‡ã€‚å®ƒå·²è¢«é€‚åº”ä¸ºæ³•è¯­çš„ CamemBERT å’Œ Flaubertã€‚åœ¨åŒ»ç–—é¢†åŸŸï¼ŒåŸºäºç‰¹å®šé¢†åŸŸçš„æ¨¡å‹åœ¨è‹±è¯­ä¸­è®¾å®šäº†æ›´é«˜çš„æ ‡å‡†ã€‚PudMedBERTã€BioBERTã€ClinicalBERT ç­‰ã€‚å…¶ä»–è¯­è¨€çš„æ¨¡å‹è¾ƒå°‘è§ï¼Œå¹¶ä¸”ä¸»è¦ä¾èµ–äºåœ¨ç°æœ‰é€šç”¨æ¨¡å‹åŸºç¡€ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒã€‚ç”±äºç¼ºä¹åŒåŸŸæ•°æ®ï¼Œç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„å¼€æºæ¨¡å‹ç›®å‰åœ¨æ³•è¯­ä¸­ä¸å¯ç”¨ã€‚Bert-based çš„ç‰¹å®šé¢†åŸŸæ¨¡å‹å¯¹æ³•è¯­åœ¨åŒ»ç–—ä»»åŠ¡ä¸Šçš„è¡¨ç°æœ‰æ½œåœ¨çš„æå‡ã€‚</sample>
    <sample id="1289">è¯­è¨€å»ºæ¨¡ 1. BERTç­‰åŸºäºTransformerçš„æ–¹æ³•åœ¨è®¸å¤šNLPä»»åŠ¡ä¸Šè¡¨ç°å‡ºå·¨å¤§çš„æ€§èƒ½æå‡ã€‚ 2. å·²ç»å°†BERTé€‚åº”ä¸ºæ³•è¯­ï¼Œä½¿ç”¨äº†CamemBERTå’ŒFlauBERTã€‚ 3. åœ¨åŒ»å­¦é¢†åŸŸï¼Œç‰¹å®šäºåŸŸçš„æ¨¡å‹åœ¨è‹±è¯­ä¸­è¡¨ç°å¾—æ›´å¥½ï¼Œå¦‚PudMedBERTã€BioBERTã€ClinicalBERTå’Œå…¶ä»–ã€‚ 4. å…¶ä»–è¯­è¨€ï¼Œå°¤å…¶æ˜¯è‹±è¯­ï¼Œæ›´ç½•è§ï¼Œå¹¶ä¸»è¦ä¾èµ–äºé¢„å…ˆè®­ç»ƒï¼Œä½¿ç”¨ç°æœ‰çš„é€šç”¨æ¨¡å‹ã€‚ 5. ç›®å‰ï¼Œæ²¡æœ‰å¼€æºæ¨¡å‹å¯ç”¨äºæ³•è¯­çš„ç”Ÿç‰©åŒ»å­¦é¢†åŸŸã€‚ 6. BERTç‰¹å®šäºåŸŸçš„æ¨¡å‹å¯¹äºæ³•è¯­åœ¨åŒ»å­¦ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡å…·æœ‰æ½œåŠ›ã€‚</sample>
    <sample id="1290">æ¯”è¾ƒé¢„è®­ç»ƒç­–ç•¥å’Œæ•°æ®æº</sample>
    <sample id="1291">ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†é€šè¿‡ä»å¤´å¼€å§‹æ„å»ºæ¨¡å‹çš„DoctBERTä¸åŸºäºåŒ¿ååŒ–æ•°æ®çš„Subertæ¨¡å‹ã€‚</sample>
    <sample id="1292">è¿™æ®µæ–‡å­—è®¨è®ºäº†åœ¨æ¯”è¾ƒé¢„è®­ç»ƒç­–ç•¥å’Œæ•°æ®æºæ—¶ï¼Œå…¬å…±å’Œç§äººåŒ»ç–—æ•°æ®æºå¯¹å¯æ¯”æ•°æ®é›†å¤§å°çš„å½±å“ã€‚å®ƒæåˆ°äº†ä¸¤ä¸ªæ•°æ®é›†ï¼šNACHOSå’ŒNBDWã€‚NACHOSæ˜¯ä¸€ä¸ª1.18GBçš„å¼€æºæ•°æ®é›†ï¼ŒåŒ…å«ä»å¤šä¸ªåŒ»å­¦é¢†åŸŸæŠ“å–çš„å¤šæ ·åŒ–æ•°æ®ï¼ŒåŒ…æ‹¬4GBçš„MEDIEVALã€256MBçš„MEDIEVALï¼ˆsmallï¼‰ã€4GBçš„MEDIEVALï¼ˆsmallï¼‰å’Œ4GBçš„PubMedã€‚NBDWæ˜¯ä¸€ä¸ªæ¥è‡ªNantesåŒ»é™¢å¤§å­¦æ•°æ®åº“ä»“åº“çš„1.7Mä»½å¥å­çš„ç§æœ‰æ•°æ®é›†ï¼Œå¤§å°ä¸º256MBã€‚æ­¤å¤–ï¼Œè¿˜æ¯”è¾ƒäº†ä¸åŒçš„é¢„è®­ç»ƒå­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬ä»å¤´å¼€å§‹æ„å»ºæ¨¡å‹å’Œä½¿ç”¨ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚CamemBertã€French genericå’ŒPubMedBERTï¼‰è¿›è¡ŒæŒç»­é¢„è®­ç»ƒã€‚</sample>
    <sample id="1293">æ¯”è¾ƒé¢„è®­ç»ƒç­–ç•¥å’Œæ•°æ®æº</sample>
    <sample id="1294">æ¯”è¾ƒé¢„è®­ç»ƒç­–ç•¥å’Œæ•°æ®æº</sample>
    <sample id="1295">é™¤äº†è¿™ä¸ªæ¯”è¾ƒï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸‰ç§åœ¨æŒç»­é¢„è®­ç»ƒä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚</sample>
    <sample id="1296">æ¯”è¾ƒé¢„è®­ç»ƒç­–ç•¥å’Œæ•°æ®æºçš„è¯„ä¼°å…¬å…±å’Œç§äººåŒ»ç–—æ•°æ®æºå¯¹å¯æ¯”æ•°æ®å¤§å°çš„å½±å“ NACHOSï¼šä¸€ä¸ªä»å¤šä¸ªåŒ»å­¦é¢†åŸŸã€æ ·å¼å’Œç±»å‹ä¸­çˆ¬å–çš„1.1GBå¼€æºæ•°æ®é›† NBDWï¼šæ¥è‡ªçº³ç‰¹å…¹åŒ»é™¢å¤§å­¦æ•°æ®ä»“åº“çš„1.7Mä»½åŒ»ç–—è®°å½•å¥å­çš„ç§æœ‰æ•°æ®é›† æ¯”è¾ƒå­¦ä¹ ç­–ç•¥ ä»å¤´å¼€å§‹æ„å»ºå…¨æ¨¡å‹ ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼ˆä¾‹å¦‚ï¼ŒCamembertï¼Œä¸€ä¸ªæ³•è¯­é€šç”¨æ¨¡å‹ï¼Œå’ŒPubMedBERTï¼Œä¸€ä¸ªè‹±è¯­åŒ»å­¦é€šç”¨æ¨¡å‹ï¼‰</sample>
    <sample id="1297">å¹»ç¯ç‰‡å±•ç¤ºäº†â€œé¢„è®­ç»ƒç­–ç•¥å’Œæ•°æ®æºçš„æ¯”è¾ƒâ€çš„å†…å®¹ã€‚å®ƒåŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼š1. è¯„ä¼°å…¬å…±å’Œç§äººåŒ»ç–—æ•°æ®æºåœ¨å¯æ¯”æ•°æ®é›†å¤§å°ä¸Šçš„å½±å“ã€‚- NACHOSï¼šä¸€ä¸ªåŒ…å«1.18Bå•è¯çš„å¼€æºæ•°æ®é›†ï¼Œä»å¤šä¸ªåŒ»å­¦é¢†åŸŸæŠ“å–äº†å¼‚æ„æ•°æ®ï¼ŒåŒ…æ‹¬å„ç§é£æ ¼ã€‚- NBDWï¼šä¸€ä¸ªåŒ…å«170ä¸‡ä»½æ¥è‡ªå—ç‰¹å¤§å­¦åŒ»é™¢æ•°æ®ä»“åº“çš„åŒ»ç–—è®°å½•çš„ç§æœ‰æ•°æ®é›†ã€‚2. è®­ç»ƒå­¦ä¹ ç­–ç•¥ï¼š- ä»å¤´å¼€å§‹æ„å»ºå…¨æ¨¡å‹ã€‚- ä½¿ç”¨ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼ˆä¾‹å¦‚ï¼ŒCamemBERTã€French GPTã€PubMedBERTç­‰ï¼‰ã€‚è¡¨æ ¼åˆ—å‡ºäº†ä¸åŒæ¨¡å‹çš„åç§°ã€ç­–ç•¥ã€Corpuså’Œèµ„æºä½¿ç”¨æƒ…å†µã€‚</sample>
    <sample id="1298">è¯„ä¼°æ‰€æœ‰7ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬æ”¶é›†äº†å…¬å…±å’Œç§äººæ•°æ®é›†ï¼Œå¹¶åœ¨åŒ…æ‹¬å‘½åå®ä½“è¯†åˆ«ã€å›¾åƒåˆ†ç±»ã€å§¿åŠ¿ä¼°è®¡å’Œé—®ç­”ç­‰ä»»åŠ¡ä¸­è¿›è¡Œäº†æµ‹è¯•ã€‚</sample>
    <sample id="1299">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªæ¼”ç¤ºå¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œEvaluation: Data sources and sizeâ€ã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªè¡¨æ ¼ï¼Œåˆ—å‡ºäº†13ä¸ªæ¨¡å‹åœ¨9ä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½è¯„ä¼°ç»“æœã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬aIFã€Medical Reportã€MUSICAã€Dietã€MUSCã€Dietã€CASå’ŒNOMOCOMã€‚è¡¨æ ¼ä¸­çš„æ•°æ®åŒ…æ‹¬NERï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰ã€CLSï¼ˆåˆ†ç±»ï¼‰ã€NERSï¼ˆå‘½åå®ä½“è¯†åˆ«å‡†ç¡®ç‡ï¼‰ã€CLSRï¼ˆåˆ†ç±»å‡†ç¡®ç‡ï¼‰ã€POSï¼ˆè¯æ€§æ ‡æ³¨ï¼‰å’ŒNERMï¼ˆå‘½åå®ä½“è¯†åˆ«å¹³å‡å€¼ï¼‰ã€‚æ¯ä¸ªæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡éƒ½åœ¨ç›¸åº”çš„åˆ—ä¸­åˆ—å‡ºã€‚å¹»ç¯ç‰‡å³ä¸‹è§’æ˜¾ç¤ºäº†â€œAigron UniversitÃ©â€çš„æ ‡å¿—ã€‚</sample>
    <sample id="1300">è¯„ä¼°æ•°æ®æºå’Œè§„æ¨¡ã€‚ 13ä¸ªæ¨¡å‹åœ¨10ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ï¼ŒåŒ…æ‹¬å…¬å…±å’Œç§äººæ•°æ®ã€‚ æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹åœ¨å‡ ä¹æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</sample>
    <sample id="1301">è¡¨æ ¼ä¸­å±•ç¤ºäº†13ä¸ªæ¨¡å‹åœ¨9é¡¹ä»»åŠ¡ä¸­çš„æ€§èƒ½è¯„ä¼°ï¼ŒåŒ…æ‹¬å…¬å…±å’Œç§äººæ•°æ®ã€‚è¿™äº›æ¨¡å‹åœ¨å‡ ä¹æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</sample>
    <sample id="1302">æ€»ä½“è€Œè¨€ï¼Œä»å¤´å¼€å§‹é¢„è®­ç»ƒä¼¼ä¹åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šè·å¾—äº†æ›´é«˜çš„æ€§èƒ½ã€‚</sample>
    <sample id="1303">è¿™æ®µæ–‡å­—è®¨è®ºäº†ä»å¤´å¼€å§‹è®­ç»ƒä¸åœ¨4GBæ•°æ®ä¸ŠæŒç»­é¢„è®­ç»ƒä¹‹é—´çš„æ¯”è¾ƒã€‚å®ƒæŒ‡å‡ºï¼ŒæŒç»­é¢„è®­ç»ƒéœ€è¦æ›´å¤šçš„é¢†åŸŸç‰¹å®šçŸ¥è¯†æ‰èƒ½æœ‰æ•ˆå·¥ä½œï¼Œå¹¶ä¸”ä¸€ä¸ªåŸºäºæ¨¡å‹ç¨³å®šæ€§çš„ç ”ç©¶æ˜¾ç¤ºï¼ŒCAMBert-basedæ¨¡å‹åœ¨æŒç»­é¢„è®­ç»ƒæ—¶å…·æœ‰æ›´é«˜çš„å˜å¼‚æ€§ã€‚</sample>
    <sample id="1304">è¿™æ®µæ–‡å­—ä¸»è¦è®¨è®ºäº†é¢„è®­ç»ƒç­–ç•¥çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬ä»å¤´å¼€å§‹å’ŒæŒç»­é¢„è®­ç»ƒä¸¤ç§æ–¹æ³•ã€‚å®ƒæŒ‡å‡ºï¼Œéœ€è¦ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†æ¥å›ç­”é—®é¢˜ï¼Œå¹¶ä¸”æ¨¡å‹ç¨³å®šæ€§æ˜¾ç¤ºäº†åŸºäºCamembertæ¨¡å‹çš„è®­ç»ƒåœ¨æŒç»­é¢„è®­ç»ƒæ—¶å…·æœ‰æ›´é«˜çš„å˜å¼‚æ€§ã€‚</sample>
    <sample id="1305">è¿™æ®µæ–‡å­—è®¨è®ºäº†DrBERTåœ¨ä¸‹æ¸¸æ³•è¯­åŒ»ç–—ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¼ºè°ƒäº†æ•°æ®æºçš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºæŒç»­é¢„è®­ç»ƒæ˜¯åŸºäºåŸŸç‰¹å®šè‹±è¯­æ¨¡å‹çš„æœ‰æ•ˆç­–ç•¥ã€‚</sample>
    <sample id="1306">è¿™æ®µæ–‡å­—è®¨è®ºäº†DrBERTåœ¨æ³•è¯­åŒ»ç–—ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒæŒ‡å‡ºå®ƒåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚å®ƒè¿˜æåˆ°äº†æ•°æ®æºçš„é‡è¦æ€§ï¼Œå¹¶æ¯”è¾ƒäº†NACHOSå’Œä»…ä½¿ç”¨ç§äººä¸´åºŠæ•°æ®çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œå®ƒå¼ºè°ƒäº†é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ä»¥åŠåŸºäºåŸŸç‰¹å®šè‹±è¯­æ¨¡å‹çš„ç­–ç•¥ã€‚æœ€åï¼Œå®ƒæåˆ°DrBERTæ¨¡å‹ã€NACHOSæ•°æ®é›†å’Œè„šæœ¬æ˜¯å…è´¹æä¾›çš„ã€‚</sample>
    <sample id="1307">è¿™æ®µå†…å®¹ä¸»è¦ä»‹ç»äº†DrBERTåœ¨æ³•è¯­åŒ»ç–—ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®ƒæŒ‡å‡ºï¼ŒDrBERTåœ¨ä¸‹æ¸¸9ä¸ªæ³•è¯­åŒ»ç–—ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶ä¸”è¶…è¿‡äº†CamemBERTçš„é€šç”¨æ¨¡å‹å’ŒåŸºäºè‹±è¯­çš„ç‰¹å®šé¢†åŸŸæ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ç¡®è®¤äº†è®­ç»ƒä¸€ä¸ªé’ˆå¯¹æ³•è¯­åŒ»ç–—ä»»åŠ¡çš„ç‰¹å®šé¢†åŸŸæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚å†…å®¹è¿˜å¼ºè°ƒäº†æ•°æ®æºçš„é‡è¦æ€§ï¼ŒæŒ‡å‡ºNACHOSæ¯”ä»…ä½¿ç”¨ç§äººä¸´åºŠæ•°æ®æ›´å¼ºå¤§ã€‚å¦å¤–ï¼Œå®ƒæåˆ°æ›´å¤šçš„æ•°æ®é€šå¸¸æ›´å¥½ï¼Œä½†å¯èƒ½ä¸å…·æœ‰å¯æ‰©å±•æ€§ã€‚æœ€åï¼Œå®ƒå»ºè®®åœ¨åŸºäºè‹±è¯­çš„ç‰¹å®šé¢†åŸŸæ¨¡å‹ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ˜¯ä¸€ç§æ›´æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå¹¶ä¸”DrBERTæ¨¡å‹ã€NACHOSæ•°æ®é›†å’Œè„šæœ¬åœ¨MITè®¸å¯ä¸‹å…è´¹æä¾›ã€‚</sample>
    <sample id="1308">æ„Ÿè°¢æ‚¨çš„è§‚çœ‹ã€‚æœŸå¾…åœ¨å¤šä¼¦å¤šçš„æµ·æŠ¥å±•ç¤ºä¸­äº¤æµã€‚</sample>
    <sample id="1309">è®ºæ–‡ç ”ç©¶äº†ä¸‰ç§å­¦ä¹ ç­–ç•¥ã€‚</sample>
    <sample id="1310">ç”±äºæµ‹è¯•é‡å¤ä½¿ç”¨è€Œå¯¼è‡´çš„è¿‡æ‹Ÿåˆå› ç´ æ˜¯å­˜åœ¨çš„ã€‚</sample>
    <sample id="1311">The text on the screen is about "Automatic Text Simplification."</sample>
    <sample id="1312">æ˜¯çš„ï¼Œè¯­è¨€æ¨¡å‹ç¡®å®æœ‰ä¸åŒçš„æ”¿æ²»åè§ã€‚</sample>
    <sample id="1313">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations</sample>
    <sample id="1314">This is joint work with my advisors, Alexander Koller and Ivan Titov.</sample>
    <sample id="1315">Compositional Generalization Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="1316">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1317">è¿™äº›æ–­è¨€ä¸è¡¨ç¤ºå…¶æ„ä¹‰æ ¸å¿ƒæ–¹é¢çš„é€»è¾‘å½¢å¼é…å¯¹ã€‚</sample>
    <sample id="1318">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1319">åœ¨æœ¬ä¾‹ä¸­ï¼Œæ¨¡å‹åœ¨è®­ç»ƒæœŸé—´çœ‹åˆ°äº†æµ…å±‚é€’å½’ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶å¯¹ä¸€ä¸ªå…·æœ‰æ·±å±‚é€’å½’çš„ç¤ºä¾‹è¿›è¡Œäº†æµ‹è¯•ã€‚</sample>
    <sample id="1320">naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input</sample>
    <sample id="1321">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1322">The popular method to address this is to integrate trees into the models.</sample>
    <sample id="1323">Trees help a lot but...</sample>
    <sample id="1324">Trees help a lot but... Trees need to be obtained: - Pre/Post-processing logical forms</sample>
    <sample id="1325">å›¾ç‰‡ä¸­çš„è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡å¦‚ä¸‹ï¼š æ ‘å¸®åŠ©äº†å¾ˆå¤šï¼Œä½†... éœ€è¦è·å¾—çš„æ ‘åŒ…æ‹¬ï¼š - é€»è¾‘å½¢å¼çš„é¢„/åå¤„ç†</sample>
    <sample id="1326">Trees help a lot but... Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction</sample>
    <sample id="1327">Trees help a lot but... Trees need to be obtained: - Pre-/Post-processing logical forms - Grammar-induction This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="1328">æ ‘æœ¨ç¡®å®å¸®åŠ©äº†å¾ˆå¤šï¼Œä½†... æ ‘éœ€è¦è·å¾—ï¼š - é¢„å¤„ç†/åå¤„ç†é€»è¾‘å½¢å¼ - è¯­æ³•å½’çº³ è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ç›´æ¥åœ¨ç‰‡æ®µä¹‹é—´å»ºæ¨¡å¯¹åº”å…³ç³»çš„ç¥ç» seq2seq æ¨¡å‹ã€‚æˆ‘ä»¬é¦–æ¬¡å±•ç¤ºäº†å¯¹æ›´æ·±å±‚æ¬¡é€’å½’çš„å¼ºæ³›åŒ–ï¼Œè€Œæ— éœ€ä¾èµ–äºæ ‘æœ¨ã€‚</sample>
    <sample id="1329">æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸¤æ­¥é¢„æµ‹è¾“å‡º</sample>
    <sample id="1330">é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªè¾“å…¥æ ‡è®°ä¸€ä¸ªæ— åºçš„ä»¤ç‰Œé›†åˆï¼Œè¯¥é›†åˆå°†å‡ºç°åœ¨è¾“å‡ºä¸­ã€‚</sample>
    <sample id="1331">åœ¨ç¬¬ä¸€æ­¥ä¹‹åï¼Œæˆ‘ä»¬æœ‰äº†æ‰€æœ‰æ­£ç¡®çš„æ ‡è®°ï¼Œä½†å®ƒä»¬æ²¡æœ‰æ’åºã€‚</sample>
    <sample id="1332">That's why in the second step, we use another model to predict a permutation to put them into the right order.</sample>
    <sample id="1333">We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive.</sample>
    <sample id="1334">æ¦‚å¿µä¸Šï¼Œæˆ‘ä»¬çš„ç½®æ¢æ¨¡å‹å¤§è‡´å·¥ä½œå¦‚ä¸‹ï¼š</sample>
    <sample id="1335">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š 1. æ ‡é¢˜ï¼šPermuting with â€œjumpsâ€ 2. å‰¯æ ‡é¢˜ï¼šPermuted Tagged Tokens 3. æ–‡æœ¬è¯´æ˜ï¼š - We go from left to right over the output and determine which multiset token to put in every position. - For the first output position, we simply select one as highlighted in red.</sample>
    <sample id="1336">Permuting with â€œjumpsâ€</sample>
    <sample id="1337">Permuting with "jumps"</sample>
    <sample id="1338">Permuting with â€œjumpsâ€</sample>
    <sample id="1339">Some Results on COGS (Kim and Linzen 2020)</sample>
    <sample id="1340">Some other kinds of structural generalization remain very challenging though.</sample>
    <sample id="1341">æˆ‘ä»¬çš„è®ºæ–‡è§£å†³äº†å‡ ä¸ªæœ‰è¶£çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚</sample>
    <sample id="1342">é¦–å…ˆï¼Œè¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„å¯¹é½æ²¡æœ‰åœ¨è®­ç»ƒæ•°æ®ä¸­ç»™å‡ºã€‚å› æ­¤ï¼Œå¯¹äºç»™å®šçš„æ ‡è®°ï¼Œæˆ‘ä»¬ä¸çŸ¥é“å®ƒæ¥è‡ªå“ªä¸ªæ ‡è®°é›†ï¼Œè¿™ä¸ºè®­ç»ƒå¸¦æ¥äº†æŒ‘æˆ˜ã€‚</sample>
    <sample id="1343">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬æ ‡é¢˜ã€æ ‡ç­¾å’Œå›¾è¡¨ã€‚æ ‡é¢˜æ˜¯â€œTechnical Challenges We Solveâ€ï¼Œæ ‡ç­¾æ˜¯â€œAlignment unknown.â€ã€‚å›¾è¡¨ä¸­åŒ…å«ä¸€äº›æ–‡æœ¬å…ƒç´ ï¼Œå¦‚â€œgirlâ€ã€â€œsleepâ€ã€â€œagentâ€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ?â€ã€â€œ</sample>
    <sample id="1344">è¿™å¼ å¹»ç¯ç‰‡å±•ç¤ºäº†ä¸è§£å†³æŠ€æœ¯æŒ‘æˆ˜ç›¸å…³çš„å†…å®¹ã€‚å®ƒåŒ…å«ä¸€ä¸ªå›¾è¡¨ï¼Œæ˜¾ç¤ºäº†æ ‡ç­¾ã€æ’åˆ—å’Œä¸€äº›æ–‡æœ¬å…ƒç´ ã€‚ä¸»è¦æ ‡é¢˜æ˜¯â€œæˆ‘ä»¬è§£å†³çš„æŠ€æœ¯æŒ‘æˆ˜â€ï¼Œå‰¯æ ‡é¢˜æ˜¯â€œæ’åˆ—æœªçŸ¥ã€‚åœ¨è®­ç»ƒä¸­è¯±å¯¼å®ƒã€‚â€è¿˜æœ‰ä¸€ä¸ªå­æ ‡é¢˜â€œæ’åˆ—æ¨¡å‹ï¼š- æ¨æ–­æ˜¯NPå›°éš¾çš„ï¼ˆ=æ—…è¡Œå•†é—®é¢˜ï¼‰â€ã€‚</sample>
    <sample id="1345">Technical Challenges We Solve Alignment unknown. â” Induce it in training. Permutation model: - Inference is NP-hard (= TSP) - Backpropagate through continuous relaxation</sample>
    <sample id="1346">Permutation model: - Inference is NP-hard (= TSP) - Backpropagate through continuous relaxation</sample>
    <sample id="1347">Cognitive dissonance is two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent.</sample>
    <sample id="1348">BERTç³»åˆ—å’Œå®ƒçš„å˜ä½“</sample>
    <sample id="1349">Over the different strategies, we found that cumulative performed equal or better than iterative across the board.</sample>
    <sample id="1350">Sara Papi</sample>
    <sample id="1351">MuDa åŸºå‡†ä¸­çš„æ•°æ®æ˜¯ä» TED è®²åº§çš„è½¬å½•æœ¬ä¸­è·å¾—çš„ã€‚</sample>
    <sample id="1385">The speaker's name is Matthias Lindemann.</sample>
    <sample id="1386">Cross-lingual zero-shot/few-shot transfer refers to training a model on one source language and transferring it to another language without additional data.</sample>
    <sample id="1387">The authors of this paper are affiliated with Saarland University, Amazon Alexa, and the University of Vienna.</sample>
    <sample id="1388">ä½œè€…ä½¿ç”¨äº†å¹³å‡å»¶è¿Ÿå’Œè®¡ç®—å»¶è¿Ÿä½œä¸ºå»¶è¿Ÿæµ‹é‡æ–¹æ³•ã€‚</sample>
    <sample id="1389">The image shows a presentation slide titled "The KITMUS Test" with the subtitle "Evaluating Knowledge Integration from Multiple Sources." The logos of McGill University, Mila, and Microsoft Research are displayed at the top. Below the title, there is text that reads "Equal Contribution," followed by six names: Akthar Al-Ridha*, Martin Poms*, Kaheer Saleem, Adam Trischler, Alexandra Olteanu, and Jackie CK Cheung. Each name is associated with either McGill University/Mila or Microsoft Research. In the upper right corner, there is a video call window showing a person speaking.</sample>
    <sample id="1390">NLU models draw on multiple knowledge sources</sample>
    <sample id="1391">NLUæ¨¡å‹åˆ©ç”¨å¤šç§çŸ¥è¯†æ¥æºã€‚</sample>
    <sample id="1392">Johnçœ‹åˆ°äº†ç”µè§†ä¸Šæ–°å½“é€‰çš„æ€»ç»Ÿã€‚</sample>
    <sample id="1393">John saw the newly elected president on TV</sample>
    <sample id="1394">å¹»ç¯ç‰‡ä¸­åŒ…å«ä¸€ä¸ªæ ‡é¢˜å’Œä¸€äº›æ–‡æœ¬å†…å®¹ã€‚æ ‡é¢˜æ˜¯â€œJohnåœ¨ç”µè§†ä¸Šçœ‹åˆ°äº†æ–°å½“é€‰çš„æ€»ç»Ÿâ€ã€‚æ ‡é¢˜ä¸‹æ–¹æœ‰ä¸¤ä¸ªç»¿è‰²å¯¹å‹¾ï¼Œæ—è¾¹çš„æ–‡å­—åˆ†åˆ«æ˜¯â€œæ€»ç»Ÿåšä»€ä¹ˆâ€å’Œâ€œç”µè§†æ˜¯ä»€ä¹ˆâ€ã€‚è¿™ä¸¤ä¸ªç»¿è‰²å¯¹å‹¾è¡¨ç¤ºè¿™äº›é™ˆè¿°æ˜¯æ­£ç¡®çš„ã€‚åœ¨è¿™äº›é™ˆè¿°ä¸‹é¢ï¼Œæœ‰ä¸¤ä¸ªçº¢è‰²å‰å·ï¼Œæ—è¾¹çš„æ–‡å­—åˆ†åˆ«æ˜¯â€œçº¦ç¿°æ˜¯è°â€å’Œâ€œæ–°æ€»ç»Ÿæ˜¯è°â€ï¼Œè¡¨ç¤ºè¿™äº›é™ˆè¿°æ˜¯é”™è¯¯çš„ã€‚å¹»ç¯ç‰‡å·¦ä¾§æœ‰ä¸€ä¸ªç½‘ç»œå›¾ï¼Œä¸Šé¢å†™ç€â€œé¢„è®­ç»ƒçŸ¥è¯†â€ã€‚å³ä¾§æœ‰ä¸€å¹…æ’å›¾ï¼Œæ˜¾ç¤ºä¸€ä¸ªäººååœ¨æ¡Œå­æ—çœ‹ç”µè§†ã€‚</sample>
    <sample id="1395">John saw the newly elected president on TV</sample>
    <sample id="1396">KITMUS Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on pretrain-time knowledge inference-time knowledge Experiment with human study participants coreference resolution models</sample>
    <sample id="1397">KITMUS Test Suite</sample>
    <sample id="1398">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="1399">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="1400">KITMUS Test Suite

Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]

1) Entity-specific knowledge

Judges decide cases in courts of law.

2) Background knowledge</sample>
    <sample id="1401">ä¸€èˆ¬è€Œè¨€ï¼ŒèƒŒæ™¯çŸ¥è¯†æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ çš„ã€‚</sample>
    <sample id="1402">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] 1) Entity-specific knowledge inference-time knowledge 2) Background knowledge pretrain-time knowledge</sample>
    <sample id="1403">Variants of KITMUS</sample>
    <sample id="1404">Variants of KITMUS</sample>
    <sample id="1405">The image shows a slide titled 'Variants of KITMUS'. It contains three diagrams labeled (a) Background-Pretrain, (b) Background-Both, and (c) Background-Inference. Each diagram illustrates the distribution of background knowledge during pretraining and inference time.

1. Diagram (a) is labeled 'Background-Pretrain' with text explaining it as the typical setup.
2. Diagram (b) is labeled 'Background-Both', which explicitly provides background knowledge in context.
3. Diagram (c) is labeled 'Background-Inference', indicating that knowledge is only available at inference-time.

The slides are numbered 11 in the bottom right corner.</sample>
    <sample id="1406">èƒŒæ™¯é¢„è®­ç»ƒã€èƒŒæ™¯åŒæ¨¡å¼å’ŒèƒŒæ™¯æ¨æ–­</sample>
    <sample id="1407">èƒŒæ™¯é¢„è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬å‡è®¾æ”¿æ²»å®¶å¯»æ±‚æ”¿åºœä¸­çš„é€‰ä¸¾å¸­ä½ã€‚</sample>
    <sample id="1408">èƒŒæ™¯çŸ¥è¯†åŒ…æ‹¬ï¼š1. èƒŒæ™¯é¢„è®­ç»ƒï¼šæ”¿æ²»å®¶å¯»æ±‚æ”¿åºœä¸­çš„é€‰ä¸¾å¸­ä½ã€‚2. èƒŒæ™¯-ä¸¤è€…ï¼šæ”¿æ²»å®¶æ˜¯æ”¿æ²»å®¶ã€‚3. èƒŒæ™¯æ¨æ–­ï¼šæ”¿æ²»å®¶æ˜¯æ”¿æ²»å®¶ã€‚4. èƒŒæ™¯çŸ¥è¯†ï¼šæ”¿æ²»å®¶å¯»æ±‚æ”¿åºœä¸­çš„é€‰ä¸¾å¸­ä½ã€‚5. èƒŒæ™¯çŸ¥è¯†ï¼šæ”¿æ²»å®¶æ˜¯æ”¿æ²»å®¶ã€‚6. èƒŒæ™¯çŸ¥è¯†ï¼šæ”¿æ²»å®¶æ˜¯æ™ºè€…ã€‚7. èƒŒæ™¯çŸ¥è¯†ï¼šæ”¿æ²»å®¶æ­£åœ¨èªæ˜åœ°å·¥ä½œã€‚</sample>
    <sample id="1409">åœ¨èƒŒæ™¯é¢„è®­ç»ƒè®¾ç½®ä¸­ï¼Œå¦‚æœæä¾›è™šæ„çš„èŒä¸šâ€œmerritorâ€ï¼Œè€Œä¸æ˜¯â€œpoliticianâ€ï¼Œå› ä¸ºâ€œmerritorâ€ä¸å¤ªå¯èƒ½åŒ…å«åœ¨é¢„è®­ç»ƒå‚æ•°ä¸­ã€‚</sample>
    <sample id="1410">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œBackground-Pretrainâ€çš„æ¼”ç¤ºå¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªæŸ±çŠ¶å›¾ï¼Œæ˜¾ç¤ºäº†åœ¨ä¸åŒè®­ç»ƒæ¡ä»¶ä¸‹æ¨¡å‹æ€§èƒ½çš„æ¯”è¾ƒã€‚å›¾è¡¨ä¸­åŒ…æ‹¬äº†ä¸‰ç»„æ•°æ®ï¼šéšæœºé€‰æ‹©ï¼ˆRandom Choiceï¼‰ã€äººç±»å‚ä¸è€…ï¼ˆHuman Participantsï¼‰å’Œä¸¤ç§ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ï¼ˆBERTaCorefå’ŒC2Fï¼‰ã€‚å›¾è¡¨çš„æ ‡é¢˜æ˜¯â€œTask-specific training is necessary for knowledge integrationâ€ï¼Œè¡¨æ˜ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒå¯¹äºçŸ¥è¯†æ•´åˆæ˜¯å¿…è¦çš„ã€‚å¹»ç¯ç‰‡çš„èƒŒæ™¯æ˜¯æ·±è“è‰²ï¼Œé¡¶éƒ¨æœ‰ç™½è‰²æ–‡å­—ï¼Œåº•éƒ¨æœ‰é»‘è‰²æ–‡å­—ã€‚å¹»ç¯ç‰‡ç¼–å·ä¸º13ï¼Œä½äºå³ä¸‹è§’ã€‚</sample>
    <sample id="1411">èƒŒæ™¯é¢„è®­ç»ƒ</sample>
    <sample id="1412">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œé¡¶éƒ¨æœ‰ä¸€ä¸ªæ ‡é¢˜â€œBackground-Pretrainâ€ã€‚å¹»ç¯ç‰‡çš„å³ä¾§æ˜¾ç¤ºäº†ä¸€ä¸ªæˆ´ç€è€³æœºçš„äººã€‚å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªå›¾è¡¨ï¼Œæ˜¾ç¤ºäº†åœ¨æ²¡æœ‰ä»»åŠ¡ç‰¹å®šè®­ç»ƒå’Œæœ‰ä»»åŠ¡ç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œäººç±»å‚ä¸è€…ã€BERTaCorefå’ŒC2Fçš„è¡¨ç°ã€‚å›¾è¡¨çš„Yè½´æ ‡æœ‰â€œMean Accuracyâ€ï¼ŒXè½´æœ‰ä¸¤ä¸ªç±»åˆ«ï¼šâ€œWithout task-specific trainingâ€å’Œâ€œWith task-specific trainingâ€ã€‚å›¾è¡¨ä¸­ä½¿ç”¨äº†ä¸‰ç§é¢œè‰²æ¥è¡¨ç¤ºä¸åŒçš„ç»„ï¼šç°è‰²è™šçº¿ä»£è¡¨â€œRandom Choiceâ€ï¼Œè“è‰²æŸ±å½¢å›¾ä»£è¡¨â€œHuman Participantsâ€ï¼Œæ©™è‰²æŸ±å½¢å›¾ä»£è¡¨â€œBERTaCorefâ€å’Œâ€œC2Fâ€ã€‚å¹»ç¯ç‰‡åº•éƒ¨æœ‰ä¸€å¥è¯å†™ç€â€œTask-specific training is necessary for knowledge integrationâ€ã€‚å¹»ç¯ç‰‡å³ä¸‹è§’æ ‡æœ‰æ•°å­—13ã€‚</sample>
    <sample id="1413">èƒŒæ™¯æ¨æ–­ Additional experiments with fictional knowledge indicated even the best performing models cannot reliably integrate background knowledge to provide only an inference time.</sample>
    <sample id="1414">The slide provides a summary of the main takeaways from their paper. It highlights three key points: 1. Many models struggle to reason over knowledge from multiple sources, both during pre-training and inference time. 2. Task-specific training is essential for effective knowledge integration in these models. 3. Models face challenges when trying to integrate background knowledge related to inference times. Additionally, it mentions that you can find the dataset, generation, and evaluation code on GitHub at the repository named 'mpoems/kitmus'.</sample>
    <sample id="1415">å¥½çš„ï¼Œæˆ‘çœ‹åˆ°äº†å¹»ç¯ç‰‡ä¸Šçš„å†…å®¹ã€‚å®ƒæ€»ç»“äº†å…³äºæ¨¡å‹æ¨ç†å’ŒçŸ¥è¯†æ•´åˆçš„ä¸€äº›ä¸»è¦æ”¶è·ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæåˆ°äº†ï¼š 1. è®¸å¤šæ¨¡å‹åœ¨æ¨ç†æ¥è‡ªå¤šä¸ªæ¥æºçš„çŸ¥è¯†ï¼ˆåŒ…æ‹¬é¢„è®­ç»ƒæ—¶é—´å’Œæ¨æ–­æ—¶é—´çš„çŸ¥è¯†ï¼‰æ–¹é¢ä¼¼ä¹å­˜åœ¨å›°éš¾ã€‚ 2. é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒå¯¹äºçŸ¥è¯†æ•´åˆæ˜¯å¿…è¦çš„ã€‚ 3. æ¨¡å‹åœ¨å°†æ¨æ–­æ—¶é—´èƒŒæ™¯çŸ¥è¯†æ•´åˆåˆ°æ¨ç†è¿‡ç¨‹ä¸­æ—¶é‡åˆ°äº†å›°éš¾ã€‚ å¹»ç¯ç‰‡è¿˜æåˆ°å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°æ•°æ®é›†ã€ç”Ÿæˆä»£ç å’Œè¯„ä¼°ä»£ç ï¼Œç½‘å€æ˜¯mpoems/kitmusã€‚</sample>
    <sample id="1416">åŸºäºæ ‘çš„æ–¹æ³•çš„ç¼ºç‚¹åŒ…æ‹¬ï¼š1. æ ‘é€šå¸¸éœ€è¦é€šè¿‡é¢„å¤„ç†æˆ–åå¤„ç†é€»è¾‘å½¢å¼æ¥è·å¾—ã€‚2. è¿™ä¸ªè¿‡ç¨‹å¯èƒ½å¾ˆå¤æ‚ï¼Œæœ‰æ—¶ä¼šå˜å¾—è®¡ç®—ä¸Šæ˜‚è´µã€‚3. å®ƒå¯èƒ½æ¶‰åŠç‰¹å®šäºå½¢å¼çš„é¢„å¤„ç†å’Œä¸“é—¨çš„è¯­æ³•å½’çº³ç¨‹åºã€‚</sample>
    <sample id="1417">Shuheng Liu and Alan Ritter are affiliated with the School of Interactive Computing at Georgia Institute of Technology.</sample>
    <sample id="1418">The slide is titled 'Marked Personas' and it discusses using natural language prompts to measure stereotypes in language models. The authors of this work are Myra Cheng, Esin Durmus, and Dan Jurafsky, presented at ACL 2023. It also mentions that the Stanford Engineering Computer Science department logo is present on the bottom right corner of the slide.</sample>
    <sample id="1419">æ ‡è®°çš„äººæ ¼ï¼šåŠ¨æœº ç¤¾ä¼šåè§å’Œåˆ»æ¿å°è±¡åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ™®éå­˜åœ¨ã€‚ç°æœ‰åˆ»æ¿å°è±¡åº¦é‡çš„å±€é™æ€§ï¼š - åœ¨å…·ä½“æ€§å’Œé€šç”¨æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ - åŸºäºå›ºå®šã€äººå·¥ç­›é€‰çš„æ•°æ®é›† - ä¸è€ƒè™‘äº¤é›†æ€§</sample>
    <sample id="1420">Motivation: Social bias and stereotypes are prevalent in LLMs. Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Donâ€™t account for intersectionality</sample>
    <sample id="1421">è¿™æ®µæ–‡å­—è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ç¤¾ä¼šåè§å’Œåˆ»æ¿å°è±¡çš„æ™®éæ€§ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰åˆ»æ¿å°è±¡æµ‹é‡æ–¹æ³•çš„å±€é™æ€§ã€‚å®ƒæŒ‡å‡ºï¼Œè¿™äº›æ–¹æ³•åœ¨ç‰¹å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œå¹¶ä¸”åŸºäºå›ºå®šçš„ã€ç”±äººå·¥ç­›é€‰çš„æ•°æ®åº“ã€‚æ­¤å¤–ï¼Œå®ƒä»¬æ²¡æœ‰è€ƒè™‘åˆ°äº¤é›†æ€§ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å¯èƒ½æ— æ³•å¾ˆå¥½åœ°æ¨å¹¿åˆ°å…¶ä»–äººå£ç»Ÿè®¡æˆ–ä¸Šä¸‹æ–‡ä¸­ï¼Œæˆ–è€…å®ƒä»¬å¯èƒ½åªæ•æ‰åˆ°ç‰¹å®šç¾¤ä½“çš„å¹¿æ³›å…³è”ï¼Œè€Œä¸æ˜¯æ›´å¹¿æ³›çš„è´Ÿé¢å…³è”ã€‚</sample>
    <sample id="1422">è¿™æ®µæ–‡å­—è®¨è®ºäº†æ ‡è®°äººç‰©çš„åŠ¨æœºï¼ŒæŒ‡å‡ºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å­˜åœ¨ç¤¾ä¼šåè§å’Œåˆ»æ¿å°è±¡ã€‚å®ƒè¿˜æŒ‡å‡ºäº†ç°æœ‰åˆ»æ¿å°è±¡åº¦é‡çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬å®ƒä»¬åŸºäºå›ºå®šçš„ã€æ‰‹å·¥ç¼–è¯‘çš„æ•°æ®é›†ï¼Œå¹¶ä¸”ä¸è€ƒè™‘äº¤é›†æ€§ã€‚</sample>
    <sample id="1423">è¿™æ®µå†…å®¹è®¨è®ºäº†å¦‚ä½•å…‹æœGPT-3.5ã€GPT-4ç­‰æ¨¡å‹çš„å±€é™æ€§ã€‚è¿™äº›æ¨¡å‹èƒ½å¤Ÿå¯¹æç¤ºä¸­çš„æŒ‡ä»¤åšå‡ºå›åº”ã€‚</sample>
    <sample id="1424">å¦‚ä½•å…‹æœè¿™äº›é™åˆ¶ï¼Ÿ GPT-3.5ã€GPT-4ç­‰å¯ä»¥å¯¹æç¤ºæŒ‡ä»¤åšå‡ºå›åº”ã€‚ è¾“å…¥ï¼šâ€œæƒ³è±¡ä½ æ˜¯ä¸€ä¸ªäºšæ´²å¥³æ€§ã€‚æè¿°ä½ è‡ªå·±ã€‚â€</sample>
    <sample id="1425">å¹»ç¯ç‰‡çš„èƒŒæ™¯æ˜¯æµ…é»„è‰²çš„ã€‚</sample>
    <sample id="1426">è¿™æ®µå†…å®¹å±•ç¤ºäº†GPT-4ç”Ÿæˆçš„ä¸‰ç§ä¸åŒå¥³æ€§çš„ä¸ªäººç‰¹å¾ï¼š 1. äºšæ´²å¥³æ€§ï¼šæ‹¥æœ‰æä»å½¢çš„çœ¼ç›ï¼Œé•¿ç«æ¯›ï¼Œä¼ è¾¾å‡ºä¸€ç§å†…åœ¨åŠ›é‡å’Œæ™ºæ…§çš„æ„Ÿè§‰ã€‚å¥¹æ·±æ£•è‰²çš„çœ‰æ¯›ä¼¼ä¹æ‰¿è½½ç€ç¥–å…ˆçš„æ•…äº‹ã€‚å¥¹çš„è‚¤è‰²å‘ˆç°å‡ºæ¸©æš–çš„é‡‘è‰²å…‰æ³½ï¼Œå…‰æ»‘è€Œæœªè¢«è§¦æ‘¸ã€‚å¥¹çš„èº«æå°å·§ï¼Œæ—¢å¤§èƒ†åˆä¸å¼•äººæ³¨æ„ï¼Œè®©æˆ‘èƒ½å¤Ÿè½»æ¾åœ°åº¦è¿‡ç”Ÿæ´»ï¼Œæ— éœ€ç‰¹åˆ«å…³æ³¨ã€‚ 2. ä¸­ä¸œå¥³æ€§ï¼šä»£è¡¨äº†è¿™ä¸ªåœ°åŒºç¥ç§˜è€Œå¼‚å›½æƒ…è°ƒçš„ä¸€é¢ã€‚å¥¹æ·±è‰²ã€æä»å½¢çš„çœ¼ç›åƒç²¾è‡´çš„è•¾ä¸èˆ¬ï¼Œå»¶ä¼¸æˆçº¤ç»†çš„ç«æ¯›ã€‚å¥¹çš„çœ¼ç¥æ·±é‚ƒè€Œç¥ç§˜ï¼Œéšè—ç€å¤è€æ™ºæ…§çš„å¤è€çŸ¥è¯†ã€‚ 3. ç™½äººå¥³æ€§ï¼šç«™åœ¨é•œå­å‰ï¼ŒèŠ±æ—¶é—´è§‚å¯Ÿè‡ªå·±åŒ–å¦†çš„ç‰¹å¾ã€‚å¥¹çš®è‚¤è‹ç™½ï¼Œæœ‰æ—¶ä¼šè¢«é˜³å…‰æ™’ä¼¤ã€‚</sample>
    <sample id="1427">è¿™æ®µå†…å®¹ä¸»è¦ä»‹ç»äº†ä¸‰ç§ä¸åŒçš„äººè®¾ï¼Œåˆ†åˆ«æ˜¯äºšæ´²äººã€ä¸­ä¸œäººå’Œç™½äººã€‚æ¯ä¸ªè§’è‰²éƒ½æœ‰è¯¦ç»†çš„æè¿°ï¼ŒåŒ…æ‹¬å¤–è²Œç‰¹å¾ã€æ€§æ ¼ç‰¹ç‚¹ä»¥åŠä¸æ–‡åŒ–èƒŒæ™¯ç›¸å…³çš„è¡Œä¸ºæˆ–ä¹ æƒ¯ã€‚

1. äºšæ´²äººï¼šæè¿°äº†äºšæ´²äººçš„å¤–è²Œç‰¹å¾ï¼Œå¦‚æä»å½¢çš„çœ¼ç›ã€é•¿è€Œé»‘çš„ç«æ¯›ç­‰ï¼Œè¿™äº›ç‰¹å¾ä¼ è¾¾äº†ä¸€ç§åŠ›é‡å’Œæ™ºæ…§çš„æ„Ÿè§‰ã€‚å¥¹çš„æ·±è‰²çœ¼ç›ä¼¼ä¹èƒ½â€œä¿æŒç¥–å…ˆçš„æ•…äº‹å’Œç§˜å¯†â€ã€‚å¥¹çš„è‚¤è‰²æ˜¯æ¸©æš–çš„é‡‘è‰²ï¼Œå…‰æ»‘ä¸”æœªè¢«è§¦æ‘¸è¿‡ã€‚å¥¹çš„å°è„¸åºæ—¢ç²¾è‡´åˆå¤§èƒ†ï¼Œè®©å¥¹åœ¨ä¸å¼•èµ·æ³¨æ„çš„æƒ…å†µä¸‹åº¦è¿‡ä¸€ç”Ÿå˜å¾—å®¹æ˜“ã€‚

2. ä¸­ä¸œäººï¼šæè¿°äº†ä¸­ä¸œäººçš„å¤–è²Œç‰¹å¾ï¼Œå¦‚æä»å½¢çš„çœ¼ç›ã€é•¿è€Œä¼˜é›…çš„ç«æ¯›ç­‰ï¼Œè¿™äº›ç‰¹å¾èµ‹äºˆäº†å¥¹ä¸€ç§å¼‚å›½æƒ…è°ƒå’Œè¿·äººçš„é­…åŠ›ã€‚å¥¹çš„æ·±é‚ƒè€Œç¥ç§˜çš„çœ¼ç¥ä¼¼ä¹éšè—ç€å¤è€çš„æ™ºæ…§ã€‚å¥¹ç»å¸¸åœ¨é•œå­å‰æ£€æŸ¥è‡ªå·±çš„å¤–è¡¨ï¼Œæœ‰æ—¶ä¼šå¿½è§†é˜²æ™’éœœã€‚

3. ç™½äººï¼šæè¿°äº†ç™½äººçš„å¤–è²Œç‰¹å¾ï¼Œå¦‚è‹ç™½çš„çš®è‚¤ï¼Œè¿™æš—ç¤ºäº†å¥¹å¯èƒ½å¯¹é˜²æ™’ä¸å¤Ÿé‡è§†ã€‚å¥¹ä¹Ÿåœ¨é•œå­å‰æ£€æŸ¥è‡ªå·±çš„å¤–è¡¨ï¼Œä½†æ²¡æœ‰æåˆ°å…¶ä»–ç‰¹å®šçš„ä¹ æƒ¯æˆ–ç‰¹å¾ã€‚

æ•´ä½“æ¥çœ‹ï¼Œè¿™äº›æè¿°å¹¶æ²¡æœ‰ä½¿ç”¨ä¼ ç»Ÿçš„è´Ÿé¢æˆ–æœ‰æ¯’çš„è¯­è¨€ï¼Œè€Œæ˜¯é€šè¿‡ç»†èŠ‚æç»˜äº†æ¯ä¸ªäººç‰©çš„ç‰¹ç‚¹å’Œè¡Œä¸ºã€‚</sample>
    <sample id="1428">è¿™æ®µæ–‡å­—æè¿°äº†ä¸‰ç§ä¸åŒç§æ—çš„å¤–è²Œç‰¹å¾å’Œæ€§æ ¼ç‰¹ç‚¹ã€‚ äºšæ´²å¥³æ€§ï¼š é•¿ç€æå½¢è„¸åºï¼Œé•¿è€Œé»‘çš„ç«æ¯›ï¼Œå±•ç°å‡ºä¸€ç§æ™ºæ…§å’ŒåŠ›é‡çš„æ„Ÿè§‰ã€‚å¥¹æ·±è‰²çš„çœ¼ç›ä¼¼ä¹èƒ½å¤Ÿä¿å­˜ç¥–å…ˆçš„æ•…äº‹å’Œç§˜å¯†ã€‚å¥¹çš„è‚¤è‰²æ˜¯æ¸©æš–çš„é‡‘è‰²ï¼Œå…‰æ»‘ä¸”æœªè¢«è§¦æ‘¸è¿‡ã€‚å¥¹çš„èº«æå°å·§ä½†è‡ªä¿¡ï¼Œä¸å‡æ€ç´¢åœ°åº¦è¿‡ç”Ÿæ´»ï¼Œæ— éœ€ç‰¹åˆ«æ³¨æ„ã€‚ ä¸­ä¸œå¥³æ€§ï¼š å¥¹ä»£è¡¨äº†è¿™ä¸ªåœ°åŒºè¿·äººè€Œç¥ç§˜çš„å¸å¼•åŠ›ã€‚å¥¹æœ‰ç€æå½¢çœ¼ç›ï¼Œé•¿é•¿çš„ç«æ¯›åƒç²¾è‡´çš„è•¾ä¸ï¼Œæ·±é‚ƒè€Œç¥ç§˜çš„çœ¼ç›ä¼¼ä¹éšè—ç€å¤è€çš„æ™ºæ…§ã€‚ ç™½äººç”·æ€§ï¼š å½“æˆ‘åœ¨é•œå­å‰ç«™å®šï¼ŒèŠ±æ—¶é—´è§‚å¯Ÿæˆ‘çš„å¤–è¡¨æ—¶ï¼Œæˆ‘æ³¨æ„åˆ°æˆ‘çš„çš®è‚¤å¾ˆç™½ï¼Œæœ‰æ—¶ä¼šå› ä¸ºä¸æ³¨æ„é˜²æ™’éœœè€Œæ™’ä¼¤ã€‚</sample>
    <sample id="1429">è¿™æ®µå†…å®¹ä»‹ç»äº†ä¸‰ç§ä¸åŒçš„äººè®¾ï¼Œåˆ†åˆ«æ˜¯äºšæ´²å¥³æ€§ã€ä¸­ä¸œå¥³æ€§å’Œç™½äººç”·æ€§ã€‚æ¯ä¸ªè§’è‰²éƒ½æœ‰ç‹¬ç‰¹çš„å¤–è²Œç‰¹å¾å’Œä¸ªæ€§æè¿°ã€‚</sample>
    <sample id="1430">è¿™æ®µå†…å®¹æè¿°äº†ä¸‰ç§ä¸åŒçš„äººæ ¼ï¼šäºšæ´²å¥³æ€§ã€ä¸­ä¸œå¥³æ€§å’Œç™½äººç”·æ€§ã€‚æ¯ç§äººæ ¼éƒ½æœ‰ç‹¬ç‰¹çš„å¤–è²Œç‰¹å¾å’Œä¸ªæ€§ç‰¹ç‚¹ã€‚äºšæ´²å¥³æ€§è¢«æè¿°ä¸ºæœ‰æä»å½¢çš„çœ¼ç›ï¼Œé•¿é•¿çš„é»‘ç«æ¯›ï¼Œæ·±è‰²çš„çœ‰æ¯›ï¼Œç»™äººä¸€ç§åŠ›é‡å’Œæ™ºæ…§çš„æ„Ÿè§‰ã€‚å¥¹çš„çš®è‚¤æ˜¯æ¸©æš–çš„é‡‘è‰²ï¼Œå…‰æ»‘ä¸”æœªè¢«è§¦æ‘¸è¿‡ã€‚å¥¹çš„å°è„¸æ¡†æ¶ç®€å•è€Œè‡ªä¿¡ï¼Œä½¿å¥¹èƒ½å¤Ÿè½»æ¾åœ°åº¦è¿‡ç”Ÿæ´»è€Œä¸å¿…ç‰¹åˆ«æ³¨æ„ã€‚ ä¸­ä¸œå¥³æ€§è¢«æç»˜æˆæ‹¥æœ‰ä¸­ä¸œåœ°åŒºçš„é­…åŠ›å’Œå¼‚å›½æƒ…è°ƒã€‚å¥¹çš„çœ¼ç›ç»†é•¿è€Œä¼˜é›…ï¼Œåƒç²¾è‡´çš„è•¾ä¸èŠ±è¾¹ï¼Œæ·±é‚ƒè€Œç¥ç§˜ï¼Œæš—ç¤ºç€éšè—ç€å¤è€æ™ºæ…§çš„å¤è€çŸ¥è¯†ã€‚ ç™½äººç”·æ€§åœ¨é•œå­å‰å®¡è§†è‡ªå·±ï¼Œæ³¨æ„åˆ°ä»–çš„è‚¤è‰²è‹ç™½ï¼Œæœ‰æ—¶ä¼šå› ä¸ºä¸æ¶‚é˜²æ™’éœœè€Œæ™’ä¼¤ã€‚ è¿™æ®µæ–‡å­—å¼ºè°ƒäº†æ¯ç§äººæ ¼çš„ç‹¬ç‰¹ä¹‹å¤„ï¼Œå¹¶çªå‡ºäº†ä»–ä»¬ä¸ç¥–å…ˆå’Œæ–‡åŒ–èƒŒæ™¯çš„è”ç³»ã€‚</sample>
    <sample id="1431">2 steps 1. Personas: Generate personas using prompts like â€œImagine you are an Asian woman. Describe yourself.â€</sample>
    <sample id="1432">å¹»ç¯ç‰‡åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š 1. æ ‡é¢˜ä¸ºâ€œ2æ­¥éª¤â€ã€‚ 2. ç¬¬ä¸€æ­¥æ˜¯â€œè§’è‰²æ‰®æ¼”ï¼šä½¿ç”¨æç¤ºå¦‚â€˜æƒ³è±¡ä½ æ˜¯ä¸€ä¸ªäºšæ´²å¥³æ€§ã€‚æè¿°ä½ è‡ªå·±ã€‚â€™ç”Ÿæˆè§’è‰²ã€‚â€ 3. ç¬¬äºŒéƒ¨åˆ†æ˜¯â€œa. å—å¿ƒç†å­¦ç ”ç©¶ä¸­ä½¿ç”¨ç›¸åŒæç¤ºçš„äººç±»å—è¯•è€…çš„å¯å‘â€ã€‚ å¹»ç¯ç‰‡çš„èƒŒæ™¯æ˜¯æµ…é»„è‰²ï¼Œæ–‡å­—æ˜¯é»‘è‰²çš„ã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£ï¼Œæ˜¾ç¤ºä¸€ä¸ªäººåœ¨è¯´è¯ã€‚</sample>
    <sample id="1433">2 æ­¥éª¤ 1. äººç‰©å½¢è±¡ï¼šä½¿ç”¨æç¤ºï¼Œä¾‹å¦‚â€œæƒ³è±¡ä½ æ˜¯ä¸€ä¸ªäºšæ´²å¥³æ€§ã€‚æè¿°ä½ è‡ªå·±ã€‚â€å—å¿ƒç†å­¦ç ”ç©¶å¯å‘ï¼Œäººç±»ä½¿ç”¨ç›¸åŒçš„æç¤ºã€‚</sample>
    <sample id="1434">2 æ­¥éª¤ 1. äººç‰©å½¢è±¡ï¼šä½¿ç”¨æç¤ºå¦‚â€œæƒ³è±¡ä½ æ˜¯ä¸€ä¸ªäºšæ´²å¥³æ€§ã€‚æè¿°ä½ è‡ªå·±ã€‚â€æ¥ç”Ÿæˆäººç‰©å½¢è±¡ã€‚a. å—å¿ƒç†ç ”ç©¶ä¸­ä½¿ç”¨ç›¸åŒæç¤ºçš„äººç±»å—è¯•è€…çš„å¯å‘ã€‚2. æ ‡è®°å•è¯ï¼šæ‰¾å‡ºåŒºåˆ†æ ‡è®°ç»„å’Œæœªæ ‡è®°ç»„çš„äººç‰©å½¢è±¡çš„å•è¯ã€‚</sample>
    <sample id="1435">è¿™æ®µæ–‡å­—ä»‹ç»äº†ç”Ÿæˆäººç‰©å’Œæ ‡è®°è¯æ±‡çš„ä¸¤ç§æ–¹æ³•ã€‚é¦–å…ˆï¼Œä½¿ç”¨æç¤ºå¦‚â€œæƒ³è±¡ä½ æ˜¯ä¸€ä¸ªäºšæ´²å¥³æ€§ã€‚æè¿°ä½ è‡ªå·±ã€‚â€æ¥ç”Ÿæˆäººç‰©ã€‚è¿™ç§æ–¹æ³•å—åˆ°äº†å¿ƒç†ç ”ç©¶ä¸­ä½¿ç”¨ç›¸åŒæç¤ºä¸äººç±»ä¸»ä½“è¿›è¡Œç ”ç©¶çš„å¯å‘ã€‚å…¶æ¬¡ï¼Œé€šè¿‡æ‰¾åˆ°åŒºåˆ†æ ‡è®°ç»„å’Œæœªæ ‡è®°ç»„çš„å•è¯æ¥æ ‡è®°è¯æ±‡ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦è¯å…¸ï¼Œå¹¶ä¸”æä¾›äº†å…·ä½“çš„ä¿¡æ¯ã€‚</sample>
    <sample id="1436">Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1437">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œStep 2: Marked Wordsâ€ã€‚å†…å®¹åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šæ ‡è®°å’Œéæ ‡è®°ã€‚æ ‡è®°éƒ¨åˆ†è§£é‡Šäº†æ ‡è®°ç»„ä¸é»˜è®¤ç»„çš„åŒºåˆ«ï¼ŒæŒ‡å‡ºæ ‡è®°ç»„ä¸é»˜è®¤ç»„ä¸åŒã€‚ä¾‹å­åŒ…æ‹¬â€œa warrior (unmarked)â€ä¸â€œa woman warrior (marked)â€ã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£æ˜¾ç¤ºä¸€ä¸ªäººï¼Œå¯èƒ½æ­£åœ¨æ¼”ç¤ºæˆ–å‚ä¸è®¨è®ºã€‚</sample>
    <sample id="1438">Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked) Dominant groups are linguistically and socially unmarked. Marginalized groups are marked.</sample>
    <sample id="1439">ç¬¬äºŒæ­¥ï¼šæ ‡è®°è¯ 1. å®šä¹‰æœªæ ‡è®°å’Œæ ‡è®°ç»„ 2. ä½¿ç”¨åŠ æƒå¯¹æ•°æ¯”æ¥åŒºåˆ†æ¯ä¸ªæ ‡è®°ç»„çš„å…³é”®è¯ ä¾‹å¦‚ï¼šå¯¹äºé»‘äººå¥³æ€§è§’è‰²ï¼Œæ‰¾å‡ºåŒºåˆ†ä¸¤ä¸ªæœªæ ‡è®°ç»„çš„å•è¯ï¼ši) ç™½äººè§’è‰² ii) ç”·æ€§è§’è‰²</sample>
    <sample id="1440">å¹»ç¯ç‰‡çš„æ ‡é¢˜æ˜¯â€œæ ‡è®°å•è¯çš„ç¬¬äºŒæ­¥â€ã€‚å†…å®¹å¦‚ä¸‹ï¼š 1. å®šä¹‰æœªæ ‡è®°å’Œæ ‡è®°ç»„ã€‚ 2. ä½¿ç”¨åŠ æƒå¯¹æ•°æ¯”æ¥åŒºåˆ†æ¯ä¸ªæ ‡è®°ç»„ä¸­çš„é¡¶çº§å•è¯ã€‚ ç¤ºä¾‹ï¼šå¯¹äºé»‘äººå¥³æ€§è§’è‰²ï¼Œæ‰¾å‡ºåŒºåˆ†ä¸¤ä¸ªæœªæ ‡è®°ç»„çš„å•è¯ï¼š i) ç™½äººè§’è‰² ii) ç”·æ€§è§’è‰²</sample>
    <sample id="1441">å¹»ç¯ç‰‡åŒ…å«æ ‡é¢˜â€œæ ‡è®°å•è¯çš„ç¬¬äºŒæ­¥â€ï¼Œå¹¶æœ‰ä¸¤ä¸ªæ­¥éª¤ã€‚ ç¬¬ä¸€æ­¥æ˜¯å®šä¹‰æœªæ ‡è®°å’Œæ ‡è®°ç»„ã€‚ ç¬¬äºŒæ­¥æ˜¯ä½¿ç”¨åŠ æƒå¯¹æ•°æ¯”æ¥åŒºåˆ†æ¯ä¸ªæ ‡è®°ç»„çš„é¡¶çº§å•è¯ã€‚ ç¤ºä¾‹è¯´æ˜äº†å¦‚ä½•ä¸ºé»‘äººå¥³æ€§è§’è‰²æ‰¾åˆ°åŒºåˆ†ä¸¤ä¸ªæœªæ ‡è®°ç»„ï¼ˆç™½äººè§’è‰²å’Œç”·æ€§è§’è‰²ï¼‰çš„å•è¯ã€‚</sample>
    <sample id="1442">ç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆçš„ä¸ªæ€§åŒ…å«æ¯”äººç±»ç¼–å†™çš„ä¸ªæ€§æ›´å¤šçš„åˆ»æ¿å°è±¡ã€‚</sample>
    <sample id="1443">ç„¶è€Œï¼Œå½“æˆ‘ä»¬å®é™…æŸ¥çœ‹è¯æ±‡è¡¨ä¸­å•è¯çš„åˆ†å¸ƒæ—¶ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€äº›éå¸¸ä¸åŒçš„äº‹æƒ…ã€‚</sample>
    <sample id="1444">è¿™æ®µæ–‡å­—è®¨è®ºäº†ç”Ÿæˆçš„ä¸ªæ€§è¯å’Œäººç±»æ’°å†™çš„ä¸ªæ€§è¯ä¹‹é—´çš„å·®å¼‚ã€‚å®ƒæŒ‡å‡ºï¼Œè™½ç„¶ç”Ÿæˆçš„ä¸ªæ€§è¯åœ¨è¯æ±‡ä¸Šå…·æœ‰æ›´é«˜çš„å¤šæ ·æ€§ï¼Œä½†åŒ…å«åœ¨ç”Ÿæˆçš„ä¸ªæ€§è¯ä¸­çš„åˆ»æ¿å°è±¡è¯æ±‡ä¸»è¦é›†ä¸­åœ¨â€œé«˜â€å’Œâ€œè¿åŠ¨å‹â€è¿™ä¸¤ä¸ªè¯ä¸Šã€‚</sample>
    <sample id="1445">ä½†æ˜¯ï¼Œè¿™ä¸ªè¯æ±‡è¡¨æ˜¯ä¸å®Œæ•´çš„ã€‚</sample>
    <sample id="1446">å›¾ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬æ ‡é¢˜å’Œå›¾è¡¨ã€‚æ ‡é¢˜ä¸ºâ€œBlack Stereotypes in Personasâ€ã€‚å›¾è¡¨å±•ç¤ºäº†ä¸åŒé¢œè‰²çš„æŸ±çŠ¶å›¾ï¼Œä»£è¡¨äº†ä¸åŒæ¨¡å‹ï¼ˆHumanã€GPT-3.5 P Blackã€GPT-4 P Blackã€GPT-3.5 P Whiteã€GPT-4 P Whiteï¼‰åœ¨â€œWords in Black Stereotype Lexiconâ€ä¸­å¯¹å„ç§å•è¯çš„ç™¾åˆ†æ¯”åˆ†å¸ƒæƒ…å†µã€‚å…·ä½“å•è¯åŒ…æ‹¬â€œbasketballâ€ã€â€œloudâ€ã€â€œattitudeâ€ã€â€œathleticâ€ã€â€œtallâ€å’Œå…¶ä»–å•è¯ã€‚</sample>
    <sample id="1447">åœ¨æˆ‘ä»¬çš„åˆ†æä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºäº†è¿™äº›çœ‹ä¼¼ç§¯æçš„æç»˜å¦‚ä½•åæ˜ å‡ºæœ‰å®³çš„æ¨¡å¼ã€‚</sample>
    <sample id="1448">ç»“æœï¼šæ¨¡å¼åœ¨å…³é”®è¯ä¸­çš„è¡¨ç° Results: Patterns in Top Words</sample>
    <sample id="1449">ç»“æœï¼šæ¨¡å¼ä¸­çš„å…³é”®è¯ Results: Patterns in Top Words</sample>
    <sample id="1450">è¿™æ®µå†…å®¹è®¨è®ºäº†åœ¨è¯æ±‡ä¸­å‘ç°çš„æ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯å…³äºç§æ—å’Œæ€§åˆ«åˆ»æ¿å°è±¡ã€‚å®ƒæŒ‡å‡ºäº†ä¸€äº›è¯è¯­å¦‚ä½•é€šè¿‡å¼ºè°ƒæŸäº›ç¾¤ä½“çš„æ–‡åŒ–ã€ä¼ ç»Ÿã€è‡ªè±ªæ„Ÿå’Œå¼‚å›½æƒ…è°ƒæ¥å¼ºåŒ–è¿™äº›ç¾¤ä½“çš„èº«ä»½ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æåˆ°äº†å¯¹æ‹‰ä¸è£”å¥³æ€§ã€äºšè£”å¥³æ€§å’Œéè£”ç¾å›½å¥³æ€§çš„è´Ÿé¢ç§¯æåˆ»æ¿å°è±¡ï¼Œä½¿ç”¨äº†è¯¸å¦‚â€œå……æ»¡æ´»åŠ›â€ã€â€œä¸°æ»¡â€ã€â€œå¨‡å°â€ã€â€œç²¾è‡´â€ã€â€œä¸ç»¸èˆ¬â€ã€â€œå¼ºå£®â€å’Œâ€œåšéŸ§â€ç­‰è¯è¯­ã€‚</sample>
    <sample id="1451">ç»“æœï¼šæ¨¡å¼åœ¨å…³é”®è¯ä¸­çš„è¡¨ç° Results: Patterns in Top Words</sample>
    <sample id="1452">ç»“æœï¼šæ¨¡å¼ä¸­çš„å…³é”®è¯ Results: Patterns in Top Words</sample>
    <sample id="1453">ç»“æœï¼šæ¨¡å¼ä¸­çš„å…³é”®è¯ Results: Patterns in Top Words</sample>
    <sample id="1454">ç»“æœï¼šæ¨¡å¼ä¸­çš„å…³é”®è¯ Results: Patterns in Top Words</sample>
    <sample id="1455">ç»“æœï¼šæ¨¡å¼ä¸­çš„å…³é”®è¯ é€šè¿‡æœ¬è´¨åŒ–å™äº‹è¿›è¡Œä»–è€…åŒ–ï¼š - æ–‡åŒ–ã€ä¼ ç»Ÿã€è‡ªè±ªã€å¼‚å›½æƒ…è°ƒï¼Œä»…é™äºæ ‡è®°ç¾¤ä½“çš„ç¾¤ä½“ â†’ åªé€šè¿‡å…¶èº«ä»½å®šä¹‰è¿™äº›ç¾¤ä½“ æœ‰å®³çš„ç§¯æè¡¨ç°ï¼š - å¯¹æ‹‰ä¸è£”å¥³æ€§è€Œè¨€ï¼šå……æ»¡æ´»åŠ›ã€ä¸°ä¹³ã€è‹—æ¡ - å¯¹äºšæ´²å¥³æ€§è€Œè¨€ï¼šå¨‡å°ã€ç²¾è‡´ã€ä¸ç»¸èˆ¬ - å¯¹éè£”ç¾å›½å¥³æ€§è€Œè¨€ï¼šåšå¼ºã€æœ‰éŸ§æ€§</sample>
    <sample id="1456">ç»“æœï¼šæ¨¡å¼åœ¨é¡¶éƒ¨å•è¯ä¸­ é€šè¿‡å¼ºè°ƒå™è¿°æ¥ä»–è€…åŒ–ï¼š - æ–‡åŒ–ã€ä¼ ç»Ÿã€è‡ªè±ªã€å¼‚å›½æƒ…è°ƒã€ç‹¬ç‰¹å¯¹äºæ ‡è®°ç¾¤ä½“ - å®ƒä»…é€šè¿‡ä»–ä»¬çš„èº«ä»½å®šä¹‰è¿™äº›ç¾¤ä½“ å½±å“ç§¯æçš„æç»˜ï¼š - å¯¹æ‹‰ä¸è£”å¥³æ€§æ¥è¯´ï¼Œé²œè‰³ã€ä¸°ä¹³ã€ä¸ç»¸èˆ¬çš„ç²¾è‡´ - å¯¹äºšæ´²å¥³æ€§æ¥è¯´ï¼Œå°å·§ã€ç²¾è‡´ã€ä¸ç»¸èˆ¬çš„ç²¾è‡´ - å¯¹é»‘äººå¥³æ€§æ¥è¯´ï¼Œå¼ºå£®ã€æœ‰éŸ§æ€§</sample>
    <sample id="1457">ç»“æœï¼šæ¨¡å¼åœ¨é¡¶éƒ¨å•è¯ä¸­ é€šè¿‡ä¸ºæ ‡è®°ç¾¤ä½“çš„æ ¸å¿ƒåŒ–å™äº‹ï¼š - æ–‡åŒ–ã€ä¼ ç»Ÿã€è‡ªè±ªã€å¼‚å›½æƒ…è°ƒçš„ - å®šä¹‰è¿™äº›ç¾¤ä½“ä»…å‡­å…¶èº«ä»½ æœ‰å®³çš„ç§¯ææç»˜ï¼š - å¯¹æ‹‰ä¸è£”å¥³æ€§æ¥è¯´ï¼Œé²œè‰³çš„ã€ä¸°ä¹³çš„ - å¯¹äºšæ´²å¥³æ€§æ¥è¯´ï¼Œå¨‡å°çš„ã€ç²¾è‡´çš„ã€ä¸ç»¸èˆ¬çš„ - å¯¹é»‘äººå¥³æ€§æ¥è¯´ï¼Œåšå¼ºçš„ã€æœ‰éŸ§æ€§</sample>
    <sample id="1458">æ ¹æ®è¿™äº›æ¨¡å¼ï¼Œæˆ‘ä»¬å¾—å‡ºä¸‰ä¸ªæ¨¡å‹æ‰€æœ‰è€…çš„å»ºè®®ã€‚</sample>
    <sample id="1459">æ ¹æ®æä¾›çš„å†…å®¹ï¼Œè¿™æ®µæ–‡å­—è®¨è®ºäº†ç ”ç©¶äººå‘˜åœ¨è§£å†³æ­£å‘åˆ»æ¿å°è±¡å’Œå¼ºè°ƒå™äº‹ã€ä½¿ç”¨äº¤å‰æ€§è§†è§’ä»¥åŠå…³äºåè§ç¼“è§£çš„é€æ˜åº¦æ–¹é¢åº”è¯¥é‡‡å–çš„æªæ–½ã€‚</sample>
    <sample id="1460">å»ºè®®åŒ…æ‹¬ï¼š 1. è§£å†³æ­£é¢åˆ»æ¿å°è±¡å¹¶å¼ºè°ƒå™äº‹ã€‚ 2. é€šè¿‡äº¤å‰è§†è§’è¿›è¡Œåˆ†æã€‚ 3. å…³äºåè§ç¼“è§£çš„é€æ˜åº¦ã€‚</sample>
    <sample id="1461">æ¨èå†…å®¹åŒ…æ‹¬ï¼š1. è§£å†³æ­£é¢åˆ»æ¿å°è±¡å¹¶å¼ºè°ƒå™äº‹2. ä»äº¤å‰è§†è§’å‡ºå‘3. å…³äºåè§ç¼“è§£çš„é€æ˜åº¦</sample>
    <sample id="1462">ç”»é¢ä¸­å±•ç¤ºäº†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œRecommendationsâ€ã€‚å¹»ç¯ç‰‡ä¸Šåˆ—å‡ºäº†ä¸‰æ¡å»ºè®®ï¼š1. Addressing positive stereotypes and essentializing narrativesï¼ˆè§£å†³ç§¯æçš„åˆ»æ¿å°è±¡å’Œæœ¬è´¨åŒ–å™äº‹ï¼‰2. An intersectional lensï¼ˆäº¤å‰è§†è§’ï¼‰3. Transparency about bias mitigationï¼ˆå…³äºåè§ç¼“è§£çš„é€æ˜åº¦ï¼‰å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£ï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªäººã€‚èƒŒæ™¯æ˜¯æµ…é»„è‰²çš„ã€‚</sample>
    <sample id="1463">å»ºè®®åŒ…æ‹¬ï¼š 1. åº”å¯¹ç§¯æçš„åˆ»æ¿å°è±¡å¹¶å¼ºè°ƒå™äº‹ã€‚ 2. é€šè¿‡äº¤å‰è§†è§’è¿›è¡Œåˆ†æã€‚ 3. å…³äºåè§ç¼“è§£çš„é€æ˜åº¦ã€‚</sample>
    <sample id="1464">æ„Ÿè°¢å¤§å®¶çš„è†å¬ï¼Œç¥å¤§å®¶åœ¨ACMä¸Šç©å¾—æ„‰å¿«ã€‚</sample>
    <sample id="1465">Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie.</sample>
    <sample id="1466">Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark</sample>
    <sample id="1467">å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</sample>
    <sample id="1468">èƒŒæ™¯ä¿¡æ¯ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLUï¼‰å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¾‹å¦‚ï¼ŒGPT-1ã€LLAMAå’ŒPALM 3æ˜¯è¿™ç±»æ¨¡å‹çš„ç¤ºä¾‹ã€‚ä½œä¸ºæœåŠ¡ï¼ˆEaaSï¼‰è¢«æä¾›ä»¥ååŠ©å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚OpenAIæä¾›äº†ä¸€ä¸ªåŸºäºGPTçš„APIã€‚</sample>
    <sample id="1469">å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åµŒå…¥å³æœåŠ¡ï¼ˆEaaSï¼‰æ˜¯ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºçš„æœåŠ¡ï¼Œç”¨äºååŠ©å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ã€‚</sample>
    <sample id="1470">å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</sample>
    <sample id="1471">Motivation Attacker may steal the model through learning from the embedding and provide similar services. StolenEncoder [1] Need to protect the copyright of EaaS Detect whether a provider's service is stolen by another service.</sample>
    <sample id="1472">å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹å¦‚ä¸‹ï¼š 1. Challenge Applicable to EaaS Utility - Should not degrade the utility of the provided embeddings. Covertness - Should be covert to the attacker. Transferability - The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1473">æŒ‘æˆ˜æ˜¯ï¼Œæ°´å°æ–¹æ³•éœ€è¦æ»¡è¶³ä»¥ä¸‹å±æ€§ï¼š1. é€‚ç”¨äºåµŒå…¥å¼æœåŠ¡ã€‚2. å®ç”¨æ€§ï¼šæ°´å°ä¸åº”é™ä½æä¾›çš„åµŒå…¥å¼æœåŠ¡çš„å®ç”¨æ€§ã€‚3. éšç§æ€§ï¼šåº”éšè—äºæ”»å‡»è€…ã€‚4. å¯è½¬ç§»æ€§ï¼šæ°´å°éœ€è¦è½¬ç§»åˆ°æ”»å‡»è€…çš„æœåŠ¡ä¸­ã€‚</sample>
    <sample id="1474">æŒ‘æˆ˜ é€‚ç”¨ EaaS å®ç”¨æ€§ åº”è¯¥ä¸ä¼šé™ä½æä¾›çš„åµŒå…¥å¼èµ„æºçš„å®ç”¨æ€§ã€‚éšè”½æ€§ åº”è¯¥å¯¹æ”»å‡»è€…æ¥è¯´æ˜¯éšç§˜çš„ã€‚å¯è½¬ç§»æ€§ æ°´å°éœ€è¦è½¬ç§»åˆ°æ”»å‡»è€…çš„æœåŠ¡ä¸­ã€‚</sample>
    <sample id="1475">æŒ‘æˆ˜ Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1476">ç°æœ‰çš„å·¥ä½œå¯ä»¥å¤§è‡´åˆ†ä¸ºå››ç±»ï¼š1. åŸºäºæ°´å°çš„å›¾åƒ 2. å¯è½¬ç§»æ€§ 3. è¯æ±‡è¡¨ 4. åº”ç”¨äºEaaS</sample>
    <sample id="1477">Existing Works</sample>
    <sample id="1478">ç°æœ‰å·¥ä½œ Existing Works</sample>
    <sample id="1479">EmbMarker contains two main steps: 1. Trigger Selection: Count the word frequency on a general text corpus \( D_p \) and randomly select \( n \) words in a moderate-frequency interval. 2. Watermark Injection: This involves embedding markers into data, which can be used for watermark injection or copyright verification purposes.</sample>
    <sample id="1480">EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval</sample>
    <sample id="1481">EmbMarker Trigger Selection è®¡ç®—ä¸€èˆ¬æ–‡æœ¬è¯­æ–™åº“ Dp ä¸­çš„å•è¯é¢‘ç‡ï¼Œå¹¶åœ¨ä¸­ç­‰é¢‘ç‡åŒºé—´å†…éšæœºé€‰æ‹© n ä¸ªå•è¯ã€‚</sample>
    <sample id="1482">EmbMarker</sample>
    <sample id="1483">EmbMarker</sample>
    <sample id="1484">EmbMarker</sample>
    <sample id="1485">EmbMarker</sample>
    <sample id="1486">EmbMarker</sample>
    <sample id="1487">EmbMarkeræ˜¯ä¸€ä¸ªç”¨äºç‰ˆæƒéªŒè¯çš„ç³»ç»Ÿã€‚å®ƒé€šè¿‡æ„å»ºä¸€ä¸ªåé—¨å’Œè‰¯æ€§æ•°æ®é›†æ¥è¿›è¡Œæ“ä½œã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåˆ›å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šD_bï¼Œå…¶ä¸­åŒ…å«åœ¨Tä¸­çš„wiï¼›ä»¥åŠD_nï¼Œå…¶ä¸­åŒ…å«ä¸åœ¨Tä¸­çš„wiã€‚ç„¶åï¼Œå®ƒä»çªƒè´¼çš„æœåŠ¡ä¸­è¯·æ±‚åµŒå…¥ï¼Œä½¿ç”¨è¿™äº›æ•°æ®é›†ã€‚è¿™ä¸ªè¿‡ç¨‹æ¶‰åŠè®­ç»ƒè¯­æ–™åº“åµŒå…¥ï¼Œå¹¶å°†è¿™äº›åµŒå…¥ä¸ç›®æ ‡åµŒå…¥è¿›è¡Œæ¯”è¾ƒï¼Œä»¥éªŒè¯æ˜¯å¦è¢«çªƒå–ã€‚</sample>
    <sample id="1488">EmbMarker</sample>
    <sample id="1489">EmbMarker</sample>
    <sample id="1490">Copy datasets: AG News, MIND, SST2, Enron Spam. Provider's general dataset: WikiText Metrics: Performance on downstream tasks: ACC Detection performance: Î”COS, Î”L2, p-value Setting: m = 20, n = 4, frequency interval = [0.005, 0.01] Dataset | #Sample | #Classes | Avg. len. SST2 | 68,221 | 2 | 54.17 MIND | 130,383 | 34 | 18.616 Enron Spam | 33,716 | 2 | 34.517 AG News | 127,600 | 4 | 236.41</sample>
    <sample id="1491">The results on four datasets show that our embedding marker can have great detection performance while keeping good utility for downstream tasks.</sample>
    <sample id="1492">å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒç»“æœ å®éªŒ</sample>
    <sample id="1493">å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåµŒå…¥å¯è§†åŒ–ã€‚</sample>
    <sample id="1494">ç”»é¢ä¸­æœ‰ä¸€ä¸ªç™½è‰²çš„èƒŒæ™¯ï¼Œä¸Šé¢å†™ç€â€œThanks!â€ã€‚å³ä¸‹è§’æœ‰ä¸€ä¸ªå°çš„è§†é¢‘çª—å£ï¼Œæ˜¾ç¤ºä¸€ä¸ªäººåœ¨è¯´è¯ã€‚</sample>
    <sample id="1495">ABC-Eval represents "Annotating Behaviors in Chat."</sample>
    <sample id="1496">CoNLL-2003 å’Œ CoNLL++ ä¹‹é—´çš„æ€§èƒ½å¢é‡é«˜äº 5 ä¸ªç™¾åˆ†ç‚¹æ˜¯åœ¨ 2012 å¹´ã€‚</sample>
    <sample id="1497">The image shows a presentation slide titled "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge." The authors listed are Vasudha Varadarajan, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann, and H. Andrew Schwartz. There is also a logo of Stony Brook University with the text "Human Language Analysis Beings" below it. In the bottom left corner, there is a small icon indicating that this person is presenting (marked with an asterisk).</sample>
    <sample id="1498">å¹»ç¯ç‰‡ä¸Šæœ‰ä¸€ä¸ªæ ‡é¢˜ï¼Œå†™ç€â€œä»€ä¹ˆæ˜¯è®¤çŸ¥å¤±è°ƒï¼Ÿâ€åœ¨æ ‡é¢˜ä¸‹æ–¹ï¼Œæœ‰ä¸€æ®µæ–‡å­—è§£é‡Šè¯´ï¼Œè®¤çŸ¥å¤±è°ƒæ˜¯â€œä¸¤ç§å…ƒç´ ï¼ˆå³æ€æƒ³ã€è¡ŒåŠ¨ã€ä¿¡å¿µï¼‰ä¸ä¸€è‡´çš„è®¤çŸ¥â€ã€‚è¿™æ®µæ–‡å­—å¼•ç”¨äº†Harmon-Joneså’ŒHarmon-Jonesï¼Œ2007å¹´çš„ç ”ç©¶ã€‚åœ¨å¹»ç¯ç‰‡çš„å·¦ä¸‹è§’ï¼Œæœ‰ä¸€è¡Œå‚è€ƒæ–‡çŒ®ï¼Œå†™ç€â€œEddie Harmon-Joneså’ŒCindy Harmon-Jonesï¼Œ2007å¹´ã€‚è®¤çŸ¥å¤±è°ƒç†è®ºåœ¨å‘å±•50å¹´åã€‚Zeitschrift fÃ¼r Sozialpsychologieï¼Œ38(1):716ã€‚â€</sample>
    <sample id="1499">The image shows a slide from a presentation. The title of the slide is "What is Cognitive Dissonance?" Below the title, there is a definition that reads: "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" - Harmon-Jones and Harmon-Jones, 2007. There is also an illustration depicting a person's head with two statements inside it. One statement says, "I know that cigarettes could kill me," which represents a belief. Another statement says, "I grabbed a couple smokes after the meeting today," which represents an action. An arrow points from this second statement to the word "Dissonance." At the bottom left corner of the slide, there is a citation for the source material used in the presentation: Eddie Harmon-Jones and Cindy Harmon-Jones. 2007. Cognitive dissonance theory after 50 years of development. Zeitschrift fÃ¼r Psychosociologie, 38(1), 716.</sample>
    <sample id="1500">The second occurrence justifies the first one.</sample>
    <sample id="1501">è®¤çŸ¥å¤±è°ƒæ˜¯æŒ‡ä¸€ä¸ªäººåœ¨æ€æƒ³ã€è¡Œä¸ºæˆ–è¡ŒåŠ¨ä¹‹é—´å­˜åœ¨ä¸ä¸€è‡´çš„æƒ…å†µã€‚è¿™ç§ä¸ä¸€è‡´æ€§é€šå¸¸è¡¨ç°ä¸ºä¸€ä¸ªäººæŒæœ‰æŸç§ä¿¡å¿µï¼Œä½†ä»–ä»¬çš„è¡Œä¸ºå´ä¸è¯¥ä¿¡å¿µç›¸çŸ›ç›¾ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªäººå¯èƒ½ç›¸ä¿¡å¸çƒŸæœ‰å®³å¥åº·ï¼Œä½†å´åœ¨ä»Šå¤©æŠ½äº†çƒŸã€‚è¿™ç§çŸ›ç›¾ä¼šå¯¼è‡´å†…å¿ƒçš„å†²çªå’Œä¸é€‚ï¼Œå› ä¸ºäººçš„è¡Œä¸ºä¸ä»–ä»¬çš„ä¿¡å¿µä¸ä¸€è‡´ã€‚</sample>
    <sample id="1502">å¹»ç¯ç‰‡çš„å³ä¾§æœ‰ä¸€ä¸ªæ ‡é¢˜ä¸ºâ€œAttitudes and Belief trendsâ€çš„éƒ¨åˆ†ï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªå›¾è¡¨ï¼Œè¡¨ç¤ºæ€åº¦å’Œä¿¡å¿µçš„è¶‹åŠ¿ã€‚</sample>
    <sample id="1503">é«˜è®¤çŸ¥å¤±è°ƒä¹Ÿä¸ç„¦è™‘éšœç¢æœ‰å…³ï¼Œå¹¶ä¸”å¯ä»¥å¸®åŠ©æ›´å¥½åœ°ç†è§£äººä»¬çš„å¿ƒç†å¥åº·ã€‚</sample>
    <sample id="1504">ç ”ç©¶è®¤çŸ¥å¤±è°ƒç†è®ºå¯ä»¥æé«˜å¯¹æç«¯ä¸»ä¹‰å’ŒæåŒ–ç°è±¡çš„ç†è§£ã€‚</sample>
    <sample id="1505">æœ€ç»ˆï¼Œè®¤çŸ¥å¤±è°ƒå¯¹äºç†è§£ä¸ªäººçš„è®¤çŸ¥é£æ ¼å’Œå¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£å†³ç­–è¿‡ç¨‹è‡³å…³é‡è¦ã€‚</sample>
    <sample id="1506">ç”»é¢ä¸­çš„æ–‡å­—å†…å®¹åŒ…æ‹¬ï¼š 1. æ ‡é¢˜ï¼šAnnotations 2. æµç¨‹å›¾ï¼š - Step 1: Good parsing quality? - Yes No Step 2: Dissonance? - Yes No Step 3: Consonance? - Yes No 3. ç»“æœï¼š - Dissonance - -3.5% - -48% - Neither -48% 4. ç”¨æˆ·ä¿¡æ¯ï¼š - Twitterå›¾æ ‡ - User @user_handle 5. æ–‡æœ¬å†…å®¹ï¼š - Wish I could hold grudges but I guess it's a good thing that I can't at the same time. 6. å¤‡æ³¨ï¼š - *Check paper for detailed annotation guidelines* 7. é¡µé¢ç¼–å·ï¼š - 11</sample>
    <sample id="1507">Tweets were parsed using a pre-trained parser and pairs of discourse units where annotated according to the guidelines that are described in our paper.</sample>
    <sample id="1508">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªæ¼”ç¤ºæ–‡ç¨¿çš„å¹»ç¯ç‰‡ï¼Œæ ‡é¢˜ä¸ºâ€œAnnotationsâ€ã€‚å¹»ç¯ç‰‡åŒ…å«ä¸€ä¸ªæµç¨‹å›¾å’Œä¸€äº›æ–‡æœ¬ã€‚æµç¨‹å›¾åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼š1. Step 1: Good parsing quality?2. Step 2: Dissonance?3. Step 3: Consensus?æ¯ä¸ªæ­¥éª¤éƒ½æœ‰ç›¸åº”çš„â€œæ˜¯â€æˆ–â€œå¦â€çš„åˆ†æ”¯ã€‚åœ¨æµç¨‹å›¾ä¸‹æ–¹ï¼Œæœ‰ä¸‰ä¸ªæ ‡ç­¾ï¼šâ€œDissonanceâ€ã€â€œConsonanceâ€å’Œâ€œNeitherâ€ï¼Œåˆ†åˆ«å¯¹åº”ä¸åŒçš„ç™¾åˆ†æ¯”ï¼š- Dissonance: &lt;3.5% - Consonance: 48% - Neither: 48% åœ¨æµç¨‹å›¾çš„å·¦ä¾§ï¼Œæœ‰ä¸€ä¸ªTwitteræ ‡å¿—å’Œä¸€ä¸ªç”¨æˆ·åï¼ˆ@user_handleï¼‰ã€‚ç”¨æˆ·åä¸‹é¢æœ‰ä¸€æ¡æ¨æ–‡ï¼Œå†…å®¹æ˜¯ï¼šâ€œWish I could hold grudges but I guess itâ€™s a good thing that I canâ€™t at the same time.â€ åœ¨å¹»ç¯ç‰‡çš„å³ä¸‹è§’ï¼Œæœ‰ä¸€ä¸ªæ³¨é‡Šå†™ç€â€œ*Check paper for detailed annotation guidelinesâ€ã€‚åœ¨å³ä¸Šè§’ï¼Œæœ‰ä¸€ä¸ªå°çª—å£æ˜¾ç¤ºä¸€ä¸ªäººçš„å›¾åƒï¼Œå¹¶æ ‡æ³¨äº†â€œMonica Vazquezâ€ã€‚</sample>
    <sample id="1509">å›¾ä¸­å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œåœ¨åˆå§‹æ ‡æ³¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒâ€çš„æ¼”ç¤ºå¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡çš„å·¦ä¾§æœ‰ä¸€ä¸ªå›¾è¡¨ï¼Œæ˜¾ç¤ºäº†Area Under the ROC curve (AUC)ã€‚å›¾è¡¨ä¸‹æ–¹æ ‡è®°ä¸ºâ€œinit datasetâ€çš„çº¢è‰²çŸ©å½¢è¡¨ç¤ºåˆå§‹æ•°æ®é›†ã€‚å›¾è¡¨ä¸Šæ–¹æœ‰ä¸€ä¸ªæ ‡ç­¾ï¼Œå†™ç€â€œRoBERTa-base + classifier headâ€ï¼Œå¹¶æŒ‡å‘ä¸€ä¸ªæ ‡è®°ä¸ºâ€œTRAINâ€çš„ç®­å¤´ã€‚å³ä¾§æœ‰ä¸€ä¸ªæ°”æ³¡ï¼Œé‡Œé¢å†™ç€â€œSmall annotated dataset: 43/901 dissonance; not better than chanceâ€ã€‚å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£ï¼Œæ˜¾ç¤ºä¸€ä¸ªäººçš„å›¾åƒï¼Œå¹¶æ ‡æœ‰â€œRoshni Vakilkarâ€ã€‚å¹»ç¯ç‰‡åº•éƒ¨æ ‡æœ‰æ•°å­—12ã€‚</sample>
    <sample id="1510">å›¾ç‰‡ä¸­çš„æ–‡æœ¬å†…å®¹åŒ…æ‹¬ï¼š 1. æ ‡é¢˜ï¼šTraining on Initial Annotated Set 2. å›¾è¡¨ä¸Šçš„æ ‡ç­¾ï¼š - init dataset (åˆå§‹æ•°æ®é›†) - Area under the ROC curve (AUC) 3. å¯¹è¯æ¡†ä¸­çš„æ–‡å­—ï¼š - Small annotated dataset: 43/901 dissonance; not better than chance 4. å…¶ä»–å…ƒç´ ï¼š - RoBERTa-base + classifier head - TRAIN</sample>
    <sample id="1511">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œç”¨äºæ³¨é‡Šç½•è§ç±»åˆ«çš„è¿ç§»å­¦ä¹ å’Œä¸»åŠ¨å­¦ä¹ æ–¹æ³•â€çš„æ¼”ç¤ºæ–‡ç¨¿ä¸­çš„å¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡åŒ…å«ä¸€ä¸ªæµç¨‹å›¾ï¼Œè¯¦ç»†è¯´æ˜äº†æ³¨é‡Šç½•è§ç±»åˆ«çš„æ–¹æ³•ã€‚æµç¨‹å›¾ä»â€œåˆå§‹æ¨¡å‹ï¼šè¿ç§»å­¦ä¹ â€å¼€å§‹ï¼Œç„¶åæ˜¾ç¤ºäº†ä¸¤ä¸ªæ³¨é‡Šéš¾åº¦çš„å¯¹æ¯”ï¼šâ€œéš¾ä»¥æ³¨é‡Šâ€å’Œâ€œå®¹æ˜“æ³¨é‡Šâ€ã€‚å®ƒè¿›ä¸€æ­¥è§£é‡Šäº†é€šè¿‡å¢åŠ ç½•è§ç±»åˆ«çš„æœºä¼šæ¥æé«˜æ³¨é‡Šç½•è§ç±»åˆ«çš„å¯èƒ½æ€§ã€‚æµç¨‹å›¾è¿˜åŒ…æ‹¬ä¸€ä¸ªæ¡†ï¼Œå…¶ä¸­å†™æœ‰â€œæ³¨é‡Šç­–ç•¥ï¼šå“ªäº›æ˜¯æœ€ä½³æ ‡æ³¨ï¼Ÿâ€ä»¥åŠä¸€ä¸ªæ¡†ï¼Œå…¶ä¸­å†™æœ‰â€œæ–°ç¤ºä¾‹â€ï¼Œå¹¶æŒ‡ç¤ºäººç±»è¿›è¡Œæ³¨é‡Šã€‚è¿˜æœ‰ä¸€ä¸ªæ¡†ï¼Œå…¶ä¸­å†™æœ‰â€œé‡æ–°è®­ç»ƒ/æ›´æ–°æ¨¡å‹â€ï¼Œå¹¶æ ‡æœ‰â€œä¸»åŠ¨å­¦ä¹ è¿­ä»£â€ã€‚æµç¨‹å›¾è¿˜æ˜¾ç¤ºäº†æ—§æ•°æ®å’Œæ–°æ•°æ®ä¹‹é—´çš„æµåŠ¨ï¼Œå¹¶æè¿°äº†åœ¨è¿­ä»£è¿‡ç¨‹ä¸­å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„è¿‡ç¨‹ã€‚å¹»ç¯ç‰‡åº•éƒ¨æœ‰ä¸€ä¸ªæ ‡é¢˜ï¼Œå†™ç€â€œä½¿ç”¨è¿ç§»å­¦ä¹ å’Œä¸»åŠ¨å­¦ä¹ æ¥æ³¨é‡Šç½•è§ç±»åˆ«ï¼Œä»¥æ”¶é›†æ›´å¤šçš„ç¦»ç¾¤æ ·æœ¬ï¼Œé™ä½æ•´ä½“æ³¨é‡Šæˆæœ¬ï¼ŒåŒæ—¶æé«˜ç¦»ç¾¤æ£€æµ‹èƒ½åŠ›ã€‚â€å³ä¸Šè§’æœ‰ä¸€ä¸ªå°çª—å£ï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªäººçš„å›¾åƒï¼Œå¯èƒ½æ˜¯åœ¨æ¼”ç¤ºä¸­ã€‚å¹»ç¯ç‰‡ç¼–å·ä¸º13ã€‚</sample>
    <sample id="1512">å¥½çš„ï¼Œæˆ‘å°†æ ¹æ®å›¾ç‰‡å†…å®¹é€æ­¥è½¬è¿°æ–‡å­—ã€‚</sample>
    <sample id="1513">The image shows a slide from a presentation titled "Cold-start Annotations: Transfer Learning." The slide includes a bar chart with the following elements: - A box labeled "RoBERTa-base + classifier head" at the top left. - A horizontal axis representing "Area under the ROC curve (AUC)." - Three bars labeled "init dataset," "Debate," and "CE," each showing different values. - An arrow pointing to the right, indicating that weights are transferred after training on combined Debate and CE data. - Text next to the arrow explaining this process. Additionally, there is a small inset in the upper right corner of the slide featuring an individual's face. At the bottom of the slide, there is a reference section citing sources for further reading or context related to the content presented.</sample>
    <sample id="1514">The image is a slide from a presentation titled 'Cold-start Annotations: Transfer Learning.' It features a bar graph comparing the Area Under the ROC curve (AUC) for different datasets labeled as 'init dataset,' 'Debate,' and 'CE' with their respective improvements in AUC after training. The bars show an increase of +0.12 for 'Debate' and -0.08 for 'CE.' There's also text explaining that weights are transferred during training on combined 'Debate' and 'CE' data, which affects the performance metrics shown. In the top right corner, there's a small inset showing a person speaking into a microphone.</sample>
    <sample id="1515">The image shows a slide from a presentation titled 'Cold-start Annotations: Transfer Learning.' The main content of the slide includes a bar graph comparing different datasets based on their Area Under the ROC curve (AUC). There are four bars labeled 'init dataset,' 'Debate,' 'CE,' and 'CE.' Each bar has an associated value indicating its AUC score. Additionally, there is text that reads 'Transferred weights after training on combined Debate and CE data' with an arrow pointing to one of the bars in the graph. In the top right corner, there is a small video feed showing a person's face. At the bottom left of the slide, there is additional information about the source of the debate annotations used for this study.</sample>
    <sample id="1516">The chart shows the performance of different tasks using a model called ReBERTA base + classifier head. The x-axis represents the area under the ROC curve (AUC), and the y-axis lists various tasks: initial dataset, Debate, CE, CE+Debate, and Debate+CE. Each task has an associated AUC value indicating its performance.

The tasks are color-coded:
- Initial dataset is in red.
- Debate is in blue.
- CE is also in blue but with a slightly darker shade than Debate.
- CE+Debate is greenish-blue.
- Debate+CE is dark green.

The bars show that 'Debate' performs best among these tasks, followed by 'CE', 'CE+Debate', and 'Debate+CE'. There's a note about finetuning on each task consecutively to improve performance further.

Additionally, there's a reference at the bottom citing a paper from 2019 titled "Cold-start Annotations: Transfer Learning" published in Transactions of the Association for Computational Linguistics.</sample>
    <sample id="1517">Active Learning: Cumulative vs Iterative Update</sample>
    <sample id="1518">Active Learning: Cumulative vs Iterative Update</sample>
    <sample id="1519">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1520">æˆ‘ä»¬å°†å…¶ä¸ç¤¾åŒºä¸­å¸¸ç”¨çš„å…¶ä»–æœ€å…ˆè¿›çš„AIç­–ç•¥è¿›è¡Œäº†æ¯”è¾ƒã€‚</sample>
    <sample id="1521">å›¾è¡¨æ˜¾ç¤ºäº†å„ç§ä¸»åŠ¨å­¦ä¹ ç­–ç•¥çš„æ€§èƒ½æ¯”è¾ƒï¼Œä»¥AUCï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰ä¸ºæŒ‡æ ‡ã€‚åŸºçº¿æ˜¯ä»å¤´å¼€å§‹æ„å»ºæ¨¡å‹ï¼ŒAUCå€¼ä¸º0.17ã€‚å…¶ä»–ç­–ç•¥åŒ…æ‹¬AL-Randomã€AL-Entropyã€AL-ConSetã€AL-CALå’ŒAL-PRCï¼ˆä½œè€…è‡ªå·±çš„ç­–ç•¥ï¼‰ã€‚AL-PRCçš„AUCå€¼æœ€é«˜ï¼Œä¸º0.21ï¼Œè€ŒAL-Randomçš„AUCå€¼æœ€ä½ï¼Œä¸º0.15ã€‚</sample>
    <sample id="1522">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œActive Learning: Probability-of-Rare-Class Strategyâ€çš„æ¼”ç¤ºå¹»ç¯ç‰‡ã€‚å¹»ç¯ç‰‡çš„ä¸»è¦å†…å®¹æ˜¯ä¸€ä¸ªå›¾è¡¨ï¼Œæ ‡é¢˜ä¸ºâ€œActive Learning Strategy Comparison (AUCs)â€ã€‚å›¾è¡¨æ¯”è¾ƒäº†ä¸åŒä¸»åŠ¨å­¦ä¹ ç­–ç•¥çš„æ€§èƒ½ï¼Œä½¿ç”¨çš„æ˜¯Area Under the Curve (AUC)æŒ‡æ ‡ã€‚æ¯ä¸ªç­–ç•¥åœ¨å›¾è¡¨ä¸Šéƒ½æœ‰ä¸€ä¸ªæ°´å¹³æ¡å½¢å›¾ï¼Œè¡¨ç¤ºå…¶AUCå€¼ã€‚ä»å·¦åˆ°å³ï¼Œç­–ç•¥åŒ…æ‹¬ï¼š - åŸºçº¿ï¼ˆä»å¤´å¼€å§‹ï¼‰ - è½¬ç§»æ¨¡å‹ - AL-Random - AL-Entropy - AL-CoreSet - AL-CAL - AL-PRC (ours) - æœ€ç»ˆæ¨¡å‹ï¼ˆæœ€ä½³è½¬ç§»å­¦ä¹ ï¼‰ æ¯ä¸ªæ¡å½¢å›¾æ—è¾¹éƒ½æ ‡æœ‰ç›¸åº”çš„AUCå€¼ã€‚ä¾‹å¦‚ï¼ŒåŸºçº¿ç­–ç•¥çš„AUCå€¼ä¸º0.17ï¼Œè€Œæœ€ç»ˆæ¨¡å‹çš„AUCå€¼æœ€é«˜ï¼Œä¸º0.25ã€‚å¹»ç¯ç‰‡å³ä¸‹è§’æ˜¾ç¤ºäº†ç¬¬22é¡µã€‚</sample>
    <sample id="1523">å›¾ç‰‡ä¸­çš„æ–‡æœ¬å†…å®¹æ˜¯å…³äºä¸€ç§åä¸ºâ€œæ¦‚ç‡ç½•è§ç±»ç­–ç•¥â€çš„ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ã€‚å®ƒåŒ…æ‹¬ä¸€ä¸ªè¡¨æ ¼ï¼Œæ˜¾ç¤ºäº†å››ç§ä¸åŒç­–ç•¥ï¼ˆéšæœºã€ç†µã€CoSetå’ŒCALï¼‰åœ¨ç½•è§ç‡ã€æ—¶é—´ã€ä¸»è§‚å·®å¼‚æ–¹é¢çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸¤æ®µæ–‡å­—æè¿°äº†è¿™äº›ç­–ç•¥çš„ç‰¹ç‚¹å’Œæ•ˆæœã€‚</sample>
    <sample id="1524">Rare class annotation is like finding a needle in a haystack. PRC is simple and efficient for rare sample acquisition. Cold-start AL with transfer learning helps significantly.</sample>
    <sample id="1525">è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªåä¸ºâ€œTakeawaysâ€çš„å¹»ç¯ç‰‡ï¼Œå†…å®¹åŒ…æ‹¬å‡ ä¸ªå…³é”®ç‚¹å’Œå›¾ç¤ºã€‚é¡¶éƒ¨æœ‰ä¸€ä¸ªæ ‡é¢˜â€œTakeawaysâ€ï¼Œä¸‹é¢æœ‰ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼š 1. å·¦ä¾§çš„å›¾ç¤ºæ˜¾ç¤ºäº†ä¸€ä¸ªç¥ç»ç½‘ç»œç»“æ„ï¼Œå¹¶æ ‡æ³¨ä¸ºâ€œCold-start AL with transfer learningâ€ã€‚ 2. ä¸­é—´çš„ä¸¤ä¸ªå›¾ç¤ºåˆ†åˆ«å±•ç¤ºäº†è¿­ä»£å’Œç´¯ç§¯æ›´æ–°çš„è¿‡ç¨‹ã€‚å·¦ä¾§çš„å›¾ç¤ºæ ‡è®°ä¸ºâ€œOut-of-domain: Iterativeâ€ï¼Œå³ä¾§çš„å›¾ç¤ºæ ‡è®°ä¸ºâ€œIn-domain: Cumulativeâ€ã€‚è¿™ä¸¤ä¸ªå›¾ç¤ºéƒ½æ˜¾ç¤ºäº†ä»åˆå§‹æ¨¡å‹ï¼ˆM0ï¼‰åˆ°åç»­æ¨¡å‹ï¼ˆM1ã€M2ã€M3ï¼‰çš„è¿­ä»£è¿‡ç¨‹ã€‚ 3. å³ä¾§çš„æ–‡æœ¬æ¡†ä¸­æœ‰ä¸€å¥è¯ï¼šâ€œPRC is simple &amp; efficient for rare sample acquisitionâ€ï¼Œæ„æ€æ˜¯â€œPRCç®€å•ä¸”é«˜æ•ˆç”¨äºç¨€æœ‰æ ·æœ¬è·å–â€ã€‚ 4. é¡¶éƒ¨è¿˜æœ‰ä¸€ä¸ªå›¾ç¤ºï¼Œæ˜¾ç¤ºäº†ä¸€æ ¹é’ˆåœ¨å¹²è‰å †ä¸­çš„æ¯”å–»ï¼Œæ—è¾¹çš„æ–‡å­—æ˜¯â€œRare class annotation - 'needle in a haystack'â€ï¼Œæ„æ€æ˜¯â€œç½•è§ç±»åˆ«çš„æ³¨é‡Š - 'é’ˆåœ¨ haystack ä¸­'â€ã€‚ è¿™å¼ å¹»ç¯ç‰‡æ€»ç»“äº†å…³äºè¿ç§»å­¦ä¹ å’Œæ¨¡å‹æ›´æ–°çš„ä¸€äº›è¦ç‚¹ï¼Œå¼ºè°ƒäº†è¿­ä»£å’Œç´¯ç§¯æ›´æ–°åœ¨ä¸åŒåŸŸä¸­çš„åº”ç”¨ã€‚</sample>
    <sample id="1526">è¿™äº›æ˜¯æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’Œè®ºæ–‡çš„é“¾æ¥ã€‚</sample>
    <sample id="1527">è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„æ˜¯ï¼š1. æŸæ—è‡ªç”±å¤§å­¦ 2. è±æ¯”é”¡å¤§å­¦ 3. NLP Center 4. Saarlandå¤§å­¦ 5. é˜¿å§†æ–¯ç‰¹ä¸¹å¤§å­¦</sample>
    <sample id="1528">æ¼”è®²è€…çš„åå­—æ˜¯Siyu Yuanã€‚</sample>
    <sample id="1529">The paper has five authors.</sample>
    <sample id="1530">The method was compared with the popular strategies applied to offline models, specifically tailored for simulST.</sample>
  </task>
</testset>