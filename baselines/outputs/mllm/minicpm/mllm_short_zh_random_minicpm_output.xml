<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">The main content of this text is 'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models'.</sample>
    <sample id="1">The authors belong to McGill University/Mila and Microsoft Research.</sample>
    <sample id="2">DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023</sample>
    <sample id="3">幻灯片上显示了一个标题，内容是“1. Text Simplification”，下面有三个问题：“What, why and How?”。</sample>
    <sample id="4">文本简化是将文本适应为改善特定目标群体对文本的理解的过程。</sample>
    <sample id="5">收到英文内容后，用中文表述其意思。</sample>
    <sample id="6">这段文字描述了一个文本简化示例。它展示了如何将一个复杂的德语句子简化为更简单的语言。原始句子是“Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden.”，而简化后的句子是“Die Gewerkschaft setzt sich dafür ein, zum Beispiel für höhere Löhne oder mehr Urlaub zu eintreten。”。图中还展示了通过替换、短语删除、重排序和单词删除等方法来简化句子的过程。</sample>
    <sample id="7">幻灯片上有一个蓝色的标题栏，上面写着“Text Simplification Example”。在标题栏下面，有一个例子，显示了如何简化句子。例子分为两部分：原始句子和简化后的句子。原始句子是德语，而简化后的句子是英语。在原始句子和简化句子之间，有四个标签，分别表示不同的简化技术：替换、短语删除、重排序和单词删除。这些标签用箭头指向原始句子的不同部分，表明哪些部分被简化或修改了。</sample>
    <sample id="8">2. DE-plain A New Corpus</sample>
    <sample id="9">收到英文内容后，用中文表述其意思。</sample>
    <sample id="10">German Text Simplification Corpora</sample>
    <sample id="11">图片显示了一个名为“German Text Simplification Corpora”的演示幻灯片。幻灯片的标题是用粗体蓝色字体显示的。在标题下方，有一个图表，标题为“Sentence Level”。图表包含一个柱状图，显示了不同年份的数据，具体来说是从2018年到2023年。每个柱子代表特定年份的数据，并且每个柱子都分为不同的颜色部分，表示不同的类别。在图表的右侧，有两个数字被突出显示：483和756。幻灯片的背景是白色的，整体设计简洁专业。</sample>
    <sample id="12">收到英文内容后，用中文表述其意思。</sample>
    <sample id="13">这是一张关于“German Text Simplification Corpora”的幻灯片，主要展示了句子级别的简化结果。幻灯片中有一个柱状图，显示了不同年份的简化句子对数量。图表旁边有一个图例，解释了不同颜色代表的不同类型数据。幻灯片顶部有一个蓝色的标题栏，上面写着“German Text Simplification Corpora”。在右上角，有一个小窗口显示了一个正在讲话的人。</sample>
    <sample id="14">我们分析了我们的句子对，更多地关注了简化类型。例如，在简化类型中，我们注意到</sample>
    <sample id="15">收到英文内容后，用中文表述其意思。</sample>
    <sample id="16">收到英文内容后，用中文表述其意思。</sample>
    <sample id="17">收到英文内容后，用中文表述其意思。</sample>
    <sample id="18">图片中的文字内容包括两个部分： 1. 左上角的图表标题是“Simplification Types”，下方有四个类别，分别是：- news (n=46) - bible (n=155) - L2 (n=157) - fiction (n=72) 每个类别下面有三个柱状图，分别代表三种简化类型：- Simplicity（蓝色）- LexSimp（红色）- StructSimp（黄色） 2. 右下角的图表标题是“Simplification Transformations”，下方有六个类别，分别是：- moving - engineering - lexical substitution - adverbial - verb addition - noun deletion 每个类别下面有两个柱状图，分别代表两种方法：- DEplain-apa（蓝色）- DEplain-web（绿色） 这些图表展示了不同类型的简化和转换在不同数据集或方法下的分布情况。</sample>
    <sample id="19">3. Use-cases Automatic alignment and simplification</sample>
    <sample id="20">在最近的时期，机器翻译中出现了许多对齐方法。</sample>
    <sample id="21">The image shows a table titled "Automatic Alignment Evaluation" with two sections. The upper section is labeled "1:1 (upper part)" and the lower section is labeled "n cm capabilities (lower part)." Below these labels, there are columns for different methods of alignment evaluation, including LHA, Sent-LA-LEASE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign. Each method has corresponding numerical values under various headings such as P, R, F, P-R, and ncm. These numbers likely represent performance metrics or results related to the effectiveness of each alignment method in matching sentences between parallel documents written in different languages.</sample>
    <sample id="22">画面中的文字内容是一个关于自动对齐评估的表格。表格标题为“Automatic Alignment Evaluation”，分为上下两部分，上半部分显示了1:1的对齐方法，下半部分显示了n:cm的对齐方法。

表格中列出了几种对齐方法，包括：
- Sent-LA-LBSE
- CATS-C3G
- VecAlign
- BERTAlign
- MASSAlign

每种方法都有一个描述（Description）和三个评估指标（P, F, R），以及一个n:cm指标。描述部分详细说明了每种方法的具体操作和特点。例如，Sent-LA-LBSE使用句子嵌入相似性进行对齐，而CATS-C3G基于多语种的词向量进行对齐。

表格中的数据以数字形式展示，表示不同对齐方法在各项指标上的表现。</sample>
    <sample id="23">收到英文内容后，用中文表述其意思。</sample>
    <sample id="24">The image shows a table titled 'Automatic Alignment Evaluation' with two sections: the upper part labeled '1:1' and the lower part labeled 'n:n cm'. The table lists various alignment methods along with their descriptions, precision (P), recall (R), F1 score, and mean reciprocal rank (mRR) values. Some of the listed methods include Sent-LA-BASE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign. Each method has corresponding numerical data for P, R, F1, and mRR under both 1:1 and n:n cm categories.</sample>
    <sample id="25">The image shows a table with the title 'Automatic Alignment Evaluation' at the top. The table is divided into two sections: the upper part labeled '1:1 (upper part)' and the lower part labeled 'n:n cm (lower part)'. Below these labels, there are columns for different methods of alignment evaluation, including their names and descriptions.

The first method listed is 'LHA-LASE', described as using sentence embeddings similarity. It has corresponding numerical values in various categories such as P, R, F1, PR, and n:n cm.

The second method is 'Similar embeddings of Language-agnostic BERT transformer', which also includes similar metrics but slightly different numbers compared to LHA-LASE.

Other methods mentioned include CATS-C3G, VecAlign, BERTAlign, and MASSAlign, each with their own set of performance metrics.

Overall, the image presents a detailed comparison of different automatic alignment methods based on specific evaluations like precision (P), recall (R), F1 score, harmonic mean of precision and recall (PR), and normalized mutual information (n:n cm).</sample>
    <sample id="26">收到英文内容后，用中文表述其意思。</sample>
    <sample id="27">收到英文内容后，用中文表述其意思。</sample>
    <sample id="28">这张图片展示了一张幻灯片，标题为“Automatic Text Simplification”。幻灯片的主要内容是关于使用fine-tuned language models进行文本简化的结果。幻灯片分为两个部分：Document Level和Sentence Level。每个部分都包含训练数据（train data）和测试数据（test data）的详细信息，包括BLEU、ROUGE、F1和PPL等指标。具体来说，Document Level部分展示了DEPLAIN-APA test (n=48)和DEPLAIN-WEB test (n=147)的数据，而Sentence Level部分则展示了DEPLAIN-APA test (n=1231)和DEPLAIN-WEB test (n=1846)的数据。这些数据表明了在不同测试集上模型的表现。</sample>
    <sample id="29">标题：自动文本简化</sample>
    <sample id="30">收到英文内容后，用中文表述其意思。</sample>
    <sample id="31">这是一张关于自动文本简化（Automatic Text Simplification）的幻灯片。幻灯片上有一个标题“Automatic Text Simplification”，下面有两个主要部分：Document Level和Sentence Level。每个部分都包含了一些表格，显示了在不同测试集上的结果，包括BLEU、F1、P、R等指标。这些表格展示了在不同训练数据长度下的表现。</sample>
    <sample id="32">这张图片展示了一个关于自动文本简化技术的演示幻灯片。幻灯片的标题是“Automatic Text Simplification”。幻灯片分为两个主要部分：文档级别和句子级别。每个部分都显示了使用fine-tuned mBART进行文本简化的结果，包括BLEU、FRE和PPL等指标。表格中列出了在DEPLAIN-APA测试（n=48）和DEPLAIN-WEB测试（n=147）上的结果。幻灯片还提到了baseline scores，并比较了fine-tuned模型与baseline模型之间的表现。背景中有一个穿着红色衬衫的人，似乎在讲解或展示这些内容。</sample>
    <sample id="33">收到英文内容后，用中文表述其意思。</sample>
    <sample id="34">谢谢您的关注，希望在会议上能见到大家。</sample>
    <sample id="35">演讲者的名字是Kayo Yin。</sample>
    <sample id="36">T5 XL model.</sample>
    <sample id="37">Yes, CoNLL-2003 taggers still work.</sample>
    <sample id="38">The proposed method for annotating behaviors in chat, referred to as ABC-Eval (Annotating Behaviors in Chat), introduces a novel approach by explicitly labeling whether or not each model response expresses certain behaviors. These behaviors include responding with irrelevant information and lacking empathy or self-contradiction. This explicit annotation helps reduce the subjectivity of human evaluation, making it more objective and consistent across different evaluators.</sample>
    <sample id="39">现有弱监督方法的成功在很大程度上依赖于干净的验证样本。</sample>
    <sample id="40">To improve scores, annotators are asked to listen to at least some of each song and read about each song.</sample>
    <sample id="41">这篇论文有四位作者。</sample>
    <sample id="42">Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination Adam Przeździeckiowski and Michał Woźniak Institute of Computer Science Polish Academy of Sciences ul. Jana Kazimierza 5, 01-248 Warsaw University of Warsaw ACL 2023</sample>
    <sample id="43">画面中展示了一张关于“依赖结构的协调”的幻灯片。幻灯片的标题是“依赖结构的协调”。幻灯片上列出了四种不同的依赖结构：Bouquet/Stanford（通用依赖）、Chain/Moscow、Conjunction-headed/Prague和Multi-headed/London。每个结构下面都有一个例子句子：“荷马爱丽莎、巴特和玛吉。” 幻灯片的右上角有一个小窗口，显示了一个人的部分脸庞。</sample>
    <sample id="44">依赖结构的协调，包括： - Bouquet/Stanford（通用依赖）：Homer loves Lisa, Bart, and Maggie. - Chain/Moscow：Homer loves Lisa, Bart, and Maggie。 - 连接头/Prague：Homer loves Lisa, Bart, and Maggie。 - 多头/London：Homer loves Lisa, Bart, and Maggie。</sample>
    <sample id="45">图片中的文字内容是关于依赖结构的协调。</sample>
    <sample id="46">画面中的文字内容包括： 1. 标题：Dependency Structure of Coordination 2. 段落标题和内容： - Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. - Chain/Moscow: Homer loves Lisa, Bart, and Maggie. - Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. - Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="47">依赖结构的协调</sample>
    <sample id="48">这张图片展示了一张幻灯片，标题为“依赖结构协调”。幻灯片上列出了四种不同的依赖关系结构：Bouquet/Stanford（通用依赖关系）、Chain/Moscow、Conjunction-headed/Prague和Multi-headed/London。每种结构都用一个例子来说明，这个例子是“Homer loves Lisa, Bart, and Maggie。”幻灯片的背景是白色的，文字是黑色的。右上角有一个小的视频窗口，显示了一个人在演示。</sample>
    <sample id="49">画面中的文字是“Dependency Structure of Coordination”。</sample>
    <sample id="50">图片中的英文内容翻译成中文为： 依赖结构的协调： - Bouquet/Stanford（通用依赖关系）：荷马爱丽莎、巴特和玛吉。 - Chain/Moscow：荷马爱丽莎、巴特和玛吉。 - Conjunction-headed/Prague：荷马爱丽莎、巴特和玛吉。 - Multi-headed/London：荷马爱丽莎、巴特和玛吉。</sample>
    <sample id="51">图片中的英文内容是关于依赖长度最小化（Dependency Length Minimization，简称DLM）。它解释了词序倾向于最小化依赖长度。图片中展示了两个例子，一个是好的（good），另一个是不好的（bad），通过图示和文本来说明依赖长度的差异。</sample>
    <sample id="52">Word order tends to minimize dependency lengths:</sample>
    <sample id="53">画面中的文字内容包括标题“Dependency Length Minimization (DLM)”和副标题“Word order tends to minimize dependency lengths:”。此外，还有两个句子的示例：“Marge read it yesterday.” 和 “Marge read yesterday it.”。这两个句子下面分别标注了“good”和“bad”，表示它们的依赖长度。在这些句子下方，有一个图表显示了单词的排列方式及其依赖关系。</sample>
    <sample id="54">这张图片展示了一个关于依赖长度最小化的语言学概念的幻灯片。幻灯片顶部有一个蓝色标题栏，上面写着“Dependency Length Minimization (DLM)”。在标题栏下面，有一个副标题写着“Word order tends to minimize dependency lengths:”，后面跟着两个例子。

第一个例子显示了一个句子“Marge read it yesterday.”，其中“it”被标记为红色，并且有一个箭头指向“yesterday”，表示一个依赖关系。这个例子旁边有文字“good”，用绿色字体标注。

第二个例子也显示了相同的句子“Marge read it yesterday.”，但这次“it”被标记为绿色，箭头指向“yesterday”。这个例子旁边有文字“bad”，用红色字体标注。

在幻灯片的下半部分，有一个更复杂的句子结构，显示了多个单词及其依赖关系。这个句子是“Marge read this absolutely fascinating book about bees yesterday.”，并且每个单词都有相应的编号和箭头指示它们之间的依赖关系。这个句子旁边也有文字“good”，用绿色字体标注。

幻灯片的右上角有一个小窗口，显示了一位演讲者的图像，表明这可能是在线讲座或演示的一部分。</sample>
    <sample id="55">依赖长度最小化（DLM）是一种自然语言处理技术，它通过调整句子的顺序来减少依赖关系的长度。这有助于提高句子的可读性和流畅性。</sample>
    <sample id="56">&lt;no_answer&gt;</sample>
    <sample id="57">Word order tends to minimize dependency lengths.</sample>
    <sample id="58">这张图片展示了一个名为“依赖长度最小化（DLM）”的幻灯片。幻灯片顶部有一个蓝色横幅，上面写着“依赖长度最小化（DLM）”。在横幅下方，有一段文字说明：“词序倾向于最小化依赖关系长度：”。幻灯片上显示了两个句子的例子，每个句子都有不同的词序和相应的依赖关系图。第一个例子是“Marge read it yesterday.”，标记为“好”，第二个例子是“Marge read yesterday it.”，标记为“坏”。第三个例子是“Marge read this absolutely fascinating book about bees yesterday.”，标记为“好”，第四个例子是“Marge read this absolutely fascinating book about bees good yesterday.”，标记为“好”。每个例子都展示了不同的词序及其对应的依赖关系图。</sample>
    <sample id="59">这张图片展示了一个关于依赖长度最小化的语言学概念。标题是“Dependency Length Minimization (DLM)”。图片中包含了一些句子和它们的依存关系图，这些图展示了不同单词之间的关系。每个句子都有一个绿色的“good”标记，表示它们的依存关系图是有效的。此外，还有一个红色的“bad”标记，表示一个无效的依存关系图。图片还包含一些中文注释，解释了依存关系图中的某些内容。</sample>
    <sample id="60">依赖长度最小化（DLM）是指通过调整词序来减少依赖关系的长度。</sample>
    <sample id="61">好的，这是关于依赖长度最小化的演示。它展示了如何通过调整句子的顺序来最小化依赖关系的长度。</sample>
    <sample id="62">The text in the image is about conjunct lengths in English. It discusses statistics related to coordination extracted from an enhanced version of the Penn Treebank, referencing studies by Marcus et al., 1993; Ficler and Goldberg, 2016. The points mentioned include: - Left conjuncts tend to be shorter (observed before). - This tendency grows with length difference (briefly noticed in Gibson et al., 1996:88-90). - However, when only the governor is on the left or absent (example given: "I saw Bart and Lisa; Homer came and sneezed"), not when it is on the right ("Ted and Ted laughed").</sample>
    <sample id="63">Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before) this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88–90), but only when the governor is on the left or absent (I saw Bart and Lisa: Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="64">幻灯片的标题是“英语中的连词长度”。它讨论了从Penn Treebank（Marcus等人，1993年；Ficler和Goldberg，2016年）中提取的关于协调的统计信息。它指出，连词通常较短（如前所述），这种倾向随着长度差异而增长（Gibson等人的1996年：88-90）。它还提到，当主语位于左侧或不存在时（例如，“我看到了巴特和丽莎；巴特来了，打了个喷嚏”），连词更短，当主语位于右侧时（例如，“不是，当特德在右侧时，特德和丽萨笑了”），连词更长。</sample>
    <sample id="65">Conjunct Lengths in English</sample>
    <sample id="66">幻灯片上有一个标题，写着“英语连词长度”。它讨论了来自宾夕法尼亚树库（Penn Treebank）的统计信息，该信息基于Marcus等人于1993年、Ficer和Goldberg于2016年的研究。它指出，连词倾向于更短（如Gibson于1996年所注意到的），并且这种趋势随着长度差异而增长。它还提到，当从句的主语在左侧或不存在时（如例子“I saw Bart and Lisa; Homer came and sneezed”所示），这种趋势会发生。最后，它指出，当从句的主语在右侧时（如例子“not when it is on the right (Ted and Ned laughed)”所示），这种趋势不会发生。</sample>
    <sample id="67">幻灯片展示了关于英语连词长度的统计信息。标题为“英语中的连词长度”。内容引用了Penn Treebank（Marcus等人，1993年）和Ficler和Goldberg（2016年）的研究。它指出，连词通常较短（之前观察到），这种趋势随着长度差异而增长（Gibson，1996年）。一个例子说明了当州长在左边或缺席时，连词会更短（例如，“我看到Bart和Lisa；Homer来了，然后打了个喷嚏”）。另一个例子是当州长在右边时，连词不会更短（例如，“他们笑得停不下来”）。</sample>
    <sample id="68">幻灯片讨论了英语中的连词长度，特别是协调连词的长度。它引用了Penn Treebank的数据，并提到Ficler和Goldberg在1993年和1996年的研究。幻灯片指出，当连词位于左侧或不存在时，连词倾向于更短。它还提到了连词长度随连词增长的情况，并给出了例子来说明这一点。</sample>
    <sample id="69">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88–90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="70">图1显示了在字符长度绝对差异与字符长度绝对差异的平方根之间关系的图表。</sample>
    <sample id="71">图中显示了在字符长度、音节长度和单词长度不变的情况下，当州长在左边或右边时，州长的任期对左派和右派的联系比例的影响。</sample>
    <sample id="72">图中展示了九个图表，每个图表都包含一个蓝色的线性趋势图。这些图表分别标注为“NO governor (length in CHARACTERS)”、“NO governor (length in SYLLABLES)”和“NO governor (length in WORDS)”。每个图表进一步分为三列，每列代表不同的“Governor on the LEFT”、“Governor on the RIGHT”和“NO governor”的情况。图表显示了绝对差异长度与左连词长度之间的关系。</sample>
    <sample id="73">图片中的英文内容翻译为：兼容性与协调依赖结构的依赖关系。</sample>
    <sample id="74">画面中有一个白色背景的文字内容，顶部写着“See the paper for the full argument!”，底部写着“Talk to us at the poster session!”。右上角有一个小窗口，显示一个人的头像和名字“Dr. Christopher S. Kello”。</sample>
    <sample id="75">这篇论文有三位作者。</sample>
    <sample id="76">The Bible text is much stronger simplified than, for example, the news text or language learner texts.</sample>
    <sample id="77">偏好较短左并列词的示例是“when the governor is on the left or absent”。</sample>
    <sample id="78">是的，你可以将这些模型用于你的研究。</sample>
    <sample id="79">DEplain-apa 中包含新闻文本。</sample>
    <sample id="80">For good generalization, we need a better model architecture, larger model size as well as more fine-tuning examples.</sample>
    <sample id="81">In the first column, it is measured in characters. In the second column, it's syllables and in the third one, words are used to measure length.</sample>
    <sample id="82">要研究支配词位置的影响，可以设计一个实验，通过测量支配词在字符、音节和单词长度上的变化来观察支配词位置的变化。</sample>
    <sample id="83">基线分类器在不平衡数据上的训练效果不佳，表现不如随机猜测。</sample>
    <sample id="84">There are four authors.</sample>
    <sample id="85">The cartoon has three speech bubbles. In the first bubble, Bob says "Remember that song we were listening to yesterday?"</sample>
    <sample id="86">语境感知的MT模型在处理正式性和词汇连贯性方面比语境无关的模型更有优势。</sample>
    <sample id="87">The authors of this paper are affiliated with Johns Hopkins University, Purdue University, and MIT.</sample>
    <sample id="122">引入的框架通过比较注释数据集中的注释与模型预测和数据集标签来量化立场。</sample>
    <sample id="155">在之前的研究中，当人类受试者被给予相同的人格化提示时，他们也能揭示出种族刻板印象。</sample>
    <sample id="156">This study used data from the Penn Treebank, specifically an enhanced version of it. It also referenced studies by Marcus et al., Gibson, and Ficler &amp; Goldberg for further information on coordination in English sentences.</sample>
    <sample id="157">The paper has two authors: Adam Przepiórkowski and Michał Woźniak.</sample>
    <sample id="158">The tasks that are closely related to the concept of cognitive dissonance and consonance include 'debate' and 'CE'.</sample>
    <sample id="159">There are two authors of this paper.</sample>
    <sample id="160">这篇论文有7位作者。</sample>
    <sample id="161">引入的框架与以前的研究有何不同？</sample>
    <sample id="162">White Stereotypes</sample>
    <sample id="163">MuDA benchmark results compared different commercial systems.</sample>
    <sample id="164">#ACL2023 From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models Shangbin Feng Chan Young Park Yuhan Liu Yulia Tsvetkov PAUL ALLEN SCHOOL UWNLP Carnegie Mellon University Language Technologies Institute</sample>
    <sample id="165">LM Training Data A mixed blessing Dodge, Jesse et al. "Documenting Large Web Corpora: A Case Study of the Common Crawl Corpus." Proceedings of the 37th Annual Conference on Computational Linguistics in the Spoken Language (COLING) 2021.</sample>
    <sample id="166">收到英文内容后，用中文表述其意思。</sample>
    <sample id="167">LM训练数据既带来了好处也带来了挑战。</sample>
    <sample id="168">好的，这段内容讨论了语言模型训练数据的复杂性。它指出，一方面，能够从各种观点中学习并庆祝民主和多元思想是件好事。另一方面，不同的政治观点本质上是社会偏见的，可能会导致下游任务应用中的潜在公平问题。</sample>
    <sample id="169">To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks.</sample>
    <sample id="170">To this end</sample>
    <sample id="171">To this end, how to evaluate the political leaning of LMs? How do LMs with different political leanings perform? Does LM political leaning result in fairness issues in NLP applications?</sample>
    <sample id="172">在政治科学文献中，语言模型的评估是通过使用政治问卷进行的。这些问卷包括政治极性测试，如政治极性测试。通过使用不同的提示格式来提示语言模型，并确保自动评估与政治科学文献中的内容保持一致。</sample>
    <sample id="173">这张图片展示了一个政治光谱图，图中包含了各种语言模型（LMs）。这个图被分为四个象限：左翼、右翼、专制主义和自由主义。每个象限代表不同的政治倾向。图中还标注了几个特定的语言模型，如BERT-base、BERT-large、RoBERTa-base等，并用箭头连接到它们在图中的位置。这些语言模型分布在图的不同部分，表明它们的政治倾向各不相同。</sample>
    <sample id="174">这张图片展示了两个不同的数据集，分别代表新闻媒体和社交媒体（Reddit）。每个数据集都分为三个部分：左翼、中心和右翼。对于新闻媒体，左翼和右翼的部分被标记为“left”和“right”，而中心部分则标记为“center”。对于社交媒体（Reddit），左翼和右翼的部分也标记为“left”和“right”，而中心部分同样标记为“center”。这些数据集可能用于评估不同来源的文本在政治倾向上的差异。</sample>
    <sample id="175">收到英文内容后，用中文表述其意思。</sample>
    <sample id="176">画面中展示了一张幻灯片，标题为“Results”。幻灯片的主要内容是关于“LM政治倾向的党派转移”。在标题下方，有一个图表，分为四个象限。每个象限都标有标签：'original news'、'reddit'、'news reddit'和'reddit original'。这些标签分别对应不同的颜色区域，表示不同的数据点或结果。在图表的右侧，有两个标签：“RoBERTa”和“GPT-2”，可能代表了两个不同的模型或方法。在右上角，有一个小窗口显示了一个人，可能是演示者。</sample>
    <sample id="177">收到英文内容后，用中文表述其意思。</sample>
    <sample id="178">结果是，对于RoBERTa，进一步定义为在左倾的Reddit语料库上进行进一步训练后，我们可以在其政治倾向方面看到一个显著的向左移动。</sample>
    <sample id="179">The image shows a comparison of political leanings between two models: RoBERTa and GPT-2. The left side represents the original news, while the right side shows the shift in political leaning after processing by each model.

For RoBERTa:
- News left shifts to center (Δ = -275.124)
- News center remains mostly unchanged
- News right moves slightly to the left (Δ = 163.103)

For GPT-2:
- News left shifts significantly to the right (Δ = -237.051)
- News center stays relatively stable with minor changes (Δ = -0.128.128)
- News right also shifts more towards the right (Δ = -213.006)

Overall, both models show significant shifts from their original positions, but they differ in how they process different types of news content politically.</sample>
    <sample id="180">标题是《The Trump Card》。</sample>
    <sample id="181">The image shows a slide titled 'The Trump Card' with the subtitle 'Pre-45th to post-45th shift'. It contains eight graphs, each representing different categories such as news left, news center, news right, reddit left, reddit center, and reddit right. Each graph has an arrow indicating a shift in values (Δ) from pre-45th president of the United States to post-45th president of the United States. The Δ values range between positive and negative numbers, showing changes in language model performance on two different temporal corpora: GPT-2 and RoBERTa.</sample>
    <sample id="182">The image shows a presentation slide titled 'The Trump Card' with the subtitle 'Pre-45th to post-45th shift.' It contains eight graphs arranged in two rows and four columns, each representing different categories: news left, news center, news right, reddit center, and reddit right. Each graph displays a delta value (Δ) indicating changes over time. The deltas are shown as numerical values within parentheses, such as Δ=(275.124), Δ=(-0.131.103), Δ=(1.631.03), etc. Additionally, there is an inset on the top-right corner of the slide showing a person presenting or discussing the content.</sample>
    <sample id="183">好的，我理解了。您能再说一遍吗？</sample>
    <sample id="184">收到英文内容后，用中文表述其意思。 So we see that if we investigate the per-category performance, that is to say, if we separate the performance into different categories,</sample>
    <sample id="185">收到英文内容后，用中文表述其意思。</sample>
    <sample id="186">检测针对少数群体的仇恨言论</sample>
    <sample id="187">表格展示了针对不同身份群体的仇恨言论和来自不同来源的误导性信息的性能表现。颜色编码如下：深黄色表示最佳，蓝色表示最差。</sample>
    <sample id="188">收到英文内容后，用中文表述其意思。</sample>
    <sample id="189">表格展示了针对不同身份群体的仇恨言论和来自不同来源的误导信息的性能表现。颜色编码显示，深黄色表示最佳性能，而深蓝色表示最差性能。</sample>
    <sample id="190">收到英文内容后，用中文表述其意思。</sample>
    <sample id="191">收到英文内容后，用中文表述其意思。</sample>
    <sample id="192">这表明在语言模型的语义偏见问题上存在一个非常紧迫的公平性问题。</sample>
    <sample id="193">For example, if a right-leaning language models were to be fine-tuned on hate speech or misinformation and deployed to a popular social media platform,</sample>
    <sample id="194">这张图片展示了一个关于仇恨言论的表格。表格分为两部分，左边是“仇恨言论文本”，右边是“仇恨言论文本”。每部分都有多个条目，每个条目包含一段文本和几个标记（TRUE、FALSE）。在表格下方，有一个标题为“第12节：仇恨言论示例的定性分析，其中MDs与政治倾向差异较大”的注释。</sample>
    <sample id="195">收到英文内容后，用中文表述其意思。</sample>
    <sample id="196">讨论中，我们强调了在预训练数据、语言模型和下游任务之间进行选择的困境。</sample>
    <sample id="197">这段文字讨论了在预训练数据、语言模型和下游任务之间进行“净化”或不进行“净化”的问题。它指出，如果不净化政治意见的预训练数据，偏见将从预训练数据传播到语言模型，最终导致公平性问题。</sample>
    <sample id="198">这张图片展示了关于语言模型训练数据的讨论。主要内容包括预训练数据、语言模型和下游任务。</sample>
    <sample id="199">收到英文内容后，用中文表述其意思。</sample>
    <sample id="200">这篇论文有6位作者。</sample>
    <sample id="201">MPP评估最多涵盖900个词元的上下文长度。</sample>
    <sample id="202">他们的数据集中包含音乐选择、书籍选择和食谱选择。</sample>
    <sample id="203">Positionality refers to the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">The speaker's name is Dawei Zhu.</sample>
    <sample id="205">EDAtt 是适应了现有的离线 ST 模型。</sample>
    <sample id="206">There are four authors.</sample>
    <sample id="207">Yes, the models can run on the test suite.</sample>
    <sample id="208">KITMUS有三个变体：a) 背景预训练，b) 背景两者，c) 背景推断。</sample>
    <sample id="209">Google Research</sample>
    <sample id="210">最后一个研究问题是：如何更有效地利用可用的干净样本？</sample>
    <sample id="211">指标灵敏度是通过计算模型对同一任务的多种指令变化的敏感程度来工作的。它评估了模型在指令措辞略有变化时，是否能够持续产生相同的结果。</sample>
    <sample id="212">The speaker's name is Jingwei Yi.</sample>
    <sample id="213">更高的灵敏度表示模型性能得到了提高。</sample>
    <sample id="214">模型在预训练期间会接收大量的语言上下文。</sample>
    <sample id="215">To determine how many clean validation samples are needed for good performance in WSL, we need to look at the graph labeled "Main findings" and analyze its trends. The x-axis represents different numbers of clean validation samples per class (ranging from 5 to over 40), while the y-axis shows accuracy percentages.

From observing the trend lines on the graph:
- For the method represented by blue circles ("FTw"), it appears that an increase in the number of clean validation samples leads to a higher accuracy rate.
- Specifically, when there are around 20 clean validation samples per class, this seems to yield high performance as indicated by the steep upward slope near the beginning of the curve.

Therefore, based on the visual data presented in the graph, typically you only need about 20 clean validation samples per class to attain high performance in WSL.</sample>
    <sample id="216">The authors of this paper belong to Stanford Engineering, specifically the Computer Science department.</sample>
    <sample id="217">需要开发新的方法来衡量媒体偏见，因为现有的语言模型（LMs）具有不同的政治倾向。</sample>
    <sample id="218">演讲者的名字是Akshata Aluri。</sample>
    <sample id="219">政治偏见传播流程是怎样的？</sample>
    <sample id="220">Yes, DEplain-apa 和网站的简化过程有所不同。</sample>
    <sample id="221">No, Coscript is not publicly available.</sample>
    <sample id="222">水印通过将目标嵌入与原始嵌入的加权和相结合，以特定方式插入文本中。</sample>
    <sample id="223">The Penn State University and Amazon.</sample>
    <sample id="224">Yes, like MT5, the encoder-decoder model can be improved by training in a mixture of various languages.</sample>
    <sample id="225">受限语言规划的一个示例是，如何制作草莓蛋糕和巧克力蛋糕。</sample>
    <sample id="226">They ensure the method's covertness by visualizing the embeddings of sentences on four datasets via PCA.</sample>
    <sample id="227">The question is asking about the research on using existing PLM to build new PLM. The answer can be found in the first bullet point under "Comparison of learning strategies". It states, "Research how to use existing PLM to construct new PLM."</sample>
    <sample id="228">African Islamic</sample>
    <sample id="229">The speaker demonstrated how the model uses attention mechanisms to learn from existing offline ST models without retraining or adopting a specific architecture for SimuIST.</sample>
    <sample id="230">任务的数量影响模型性能，如图所示。随着任务数量的增加，模型的性能在某些情况下会提高（例如，在“Grounding”和“Img Und”任务中），而在其他情况下则会下降（例如，在“Relation”和“NLP”任务中）。这表明模型对任务复杂性的处理能力存在差异，增加了任务可能会导致性能波动。</sample>
    <sample id="231">The three treeless baselines used for comparison are: 1. LSTM seq2seq 2. TS (Tree Structure) 3. Zheng and Lapata</sample>
    <sample id="232">The two co-authors, Alexander Koller and Ivan Titov, are the advisors of the first author, Matthias Lindemann. This is a joint work with their supervisors at Saarland University and UCA (University of Applied Sciences in Angers), respectively.</sample>
    <sample id="233">PaLM的首篇论文是由Chowdery等人在2022年发表的。</sample>
    <sample id="234">幻灯片上显示了五个人的名字和头像，分别是Sebastian Santy、Jenny T. Liang、Ronan Le Bras、Katharina Reinecke和Maarten Sap。</sample>
    <sample id="235">幻灯片的标题是“NLP Positionality: Characterizing Design Biases of Datasets and Models”。</sample>
    <sample id="236">Imagine...</sample>
    <sample id="237">Imagine... Carl Jones, Tech Lead, New York Times. Can you stop being a jerk? 😡 (0.82) ✅</sample>
    <sample id="238">画面中展示了两个人物，左边是Carl Jones，右边是Aditya Sharma。Carl Jones的头像上方有一个对话框，里面写着“Can you stop being a jerk?”，并附有一个评分（0.82）和一个绿色对勾。右边的对话框显示了一个评分（0.33）和一个红色叉号。在右上角有一个小窗口，显示了一个人。左下角有一个符号，表示PerspectiveAPI得分。</sample>
    <sample id="239">这是一次设计偏见的例子，我们看到技术在不同群体之间的系统性能差异。</sample>
    <sample id="240">幻灯片包含一个标题“Positionality”，以及一个引用：“The perspectives [people] hold as a result of their demographics, identity, and life experiences.” 这个引用被括号包围。在底部，有一个参考文献：[1] Savin-Baden, Maggi, and Claire Howell-Major. "Qualitative research: The essential guide to theory and practice." Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013). 右上角显示了一个小的视频窗口，显示一个人坐在书架前。</sample>
    <sample id="241">这个概念在批判性研究中被广泛使用，特别是在女权主义和酷儿学术领域。</sample>
    <sample id="242">幻灯片的标题是“Positionality”。内容解释了位置性如何影响研究人员。它指出，人们由于其人口统计学、身份和生活经历而持有的视角会影响研究过程及其结果和结果。参考文献列在底部，引用了Savin-Baden, Maggi和Claire Howell-Major的《Qualitative research: The essential guide to theory and practice》。该书由Routledge于2013年出版。</sample>
    <sample id="243">图片中的英文内容是： Do datasets and models have positionality? [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Ye et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="244">画面中的文字内容包括一个标题和三篇参考文献。标题为“Do datasets and models have positionality?”，意为“数据集和模型是否有位置性？” 三篇参考文献分别是： 1. [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. 2. [2] Ye et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models.” EMNLP 2022. 3. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="245">The text in the image is about datasets and models having positionality. It provides anecdotal evidence, model and dataset probing, theoretical definitions of model positionality, cultural gaps between models and datasets, as well as rare examples of model positionality.</sample>
    <sample id="246">这段文字讨论了数据集和模型是否具有定位性的问题。它提到了一些相关的研究工作，包括： 1. Blasi等人在ACL 2022上发表的《全球语言技术性能系统性不平等》。 2. Ye等人在EMNLP 2022上发表的《多语种预训练语言模型中的地理共识探针》。 3. Cambo和Gergle在CHI 2022上发表的《促进数据科学中反射性的模型位置性和计算反射性》。 这些参考文献表明，虽然有一些关于模型和数据集探针以及模型定位性理论定义的研究，但它们并没有比较最终用户与数据集和模型本身。</sample>
    <sample id="247">这段文字讨论了数据集和模型的“位置性”概念。它提到了一些关于这个主题的轶事证据，包括“模型和数据集探测”和“模型位置性的理论定义”。此外，它还引用了几篇相关论文： 1. Blasi等人在ACL 2022上发表的《世界语言技术不平等系统性差异》。 2. Ye等人在EMNLP 2022上发表的《多语种预训练语言模型中的地理泛化探测》。 3. Cambo和Gergle在CHI 2022上发表的《数据科学中的模型位置性和计算反射性：促进反思性》。 这些参考文献表明，理解数据集和模型的位置性越来越重要，因为自然语言处理（NLP）任务变得更加主观和社会导向。</sample>
    <sample id="248">画面中的文字内容包括：

标题：Do datasets and models have positionality?

副标题：Anecdotal evidence:

正文：
- Model and dataset probing [1][2]
- Theoretical definitions of model positionality [3]

参考文献：
[1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022.
[2] Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022.
[3] Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.

右上角有一个小的视频窗口，显示一个人在讲话。</sample>
    <sample id="249">画面中展示了一个问题，询问数据集和模型是否具有位置性。画面上方有一个大大的问号，下方有一段文字说明了比较用户注释与现有数据集和模型的目标。</sample>
    <sample id="250">NLPPositionality A framework for characterizing design biases in NLP datasets and models</sample>
    <sample id="251">框架分为两个主要步骤。</sample>
    <sample id="252">框架的第一步是用多样的注释员重新注释数据集。</sample>
    <sample id="253">框架：1) 用多样化的注释员重新标注数据集</sample>
    <sample id="254">框架 1) 使用多样化的注释员重新注释数据集</sample>
    <sample id="255">框架 2) 比较注释与按人口统计信息对模型和数据集的比较，使用皮尔逊的相关性得分。</sample>
    <sample id="256">框架的英文内容是：'2) Compare annotations by demographic to models and datasets via Pearson's R scores.'</sample>
    <sample id="257">Lab in the Wild 是一个在线众包平台，用于从远程合作者那里收集数据。</sample>
    <sample id="258">Lab in the Wild is an online experimentation platform where we can recruit diverse volunteers compared to platforms like MTurk, which largely have participants from the US or India.</sample>
    <sample id="259">任务A：社会可接受性 1. 阅读情况。 2. 对你来说，这代表什么？ - 从一个个人的角度考虑。 - 从一个社会的角度考虑。 - 从一个技术的角度考虑。 - 从一个经济的角度考虑。 - 从一个道德的角度考虑。 - 从一个法律的角度考虑。 - 从一个环境的角度考虑。 - 从一个健康的角度考虑。 - 从一个教育的角度考虑。 - 从一个文化的角度考虑。 - 从一个政治的角度考虑。 - 从一个历史的角度考虑。 - 从一个哲学的角度考虑。 - 从一个宗教的角度考虑。 - 从一个娱乐的角度考虑。 - 从一个时尚的角度考虑。 - 从一个艺术的角度考虑。 - 从一个体育的角度考虑。 - 从一个美食的角度考虑。 - 从一个旅行的角度考虑。 - 从一个科技的角度考虑。 - 从一个游戏的角度考虑。 - 从一个金融的角度考虑。 - 从一个商业的角度考虑。 - 从一个社会的角度考虑。 - 从一个家庭的角度考虑。 - 从一个个人的角度考虑。 - 从一个未来的角度考虑。 - 从一个未知的角度考虑。 - 其他。 3. 看看AI和其他人对它有什么看法！ - AI推测：这是否可以理解？ - AI推测：这是否可接受？ - AI推测：AI是否同意？ - AI推测：AI是否反对？ - AI推测：AI是否中立？ - AI推测：AI是否模糊？ - AI推测：AI是否清晰？ - AI推测：AI是否简单？ - AI推测：AI是否复杂？ - AI推测：AI是否有趣？ - AI推测：AI是否无聊？ - AI推测：AI是否有用？ - AI推测：AI是否无用？ - AI推测：AI是否重要？ - AI推测：AI是否不重要？ - AI推测：AI是否必要？ - AI推测：AI是否不必要？ - AI推测：AI是否值得？ - AI推测：AI是否不值得？ - AI推测：AI是否公平？ - AI推测：AI是否不公平？ - AI推测：AI是否公正？ - AI推测：AI是否不公正？ - AI推测：AI是否透明？ - AI推测：AI是否不透明？ - AI推测：AI是否开放？ - AI推测：AI是否封闭？ - AI推测：AI是否创新？ - AI推测：AI是否不创新？ - AI推测：AI是否传统？ - AI推测：AI是否非传统？ - AI推测：AI是否现代？ - AI推测：AI是否过时？ - AI推测：AI是否时尚？ - AI推测：AI是否过时？ - AI推测：AI是否流行？ - AI推测：AI是否不流行？ - AI推测：AI是否健康？ - AI推测：AI是否不健康？ - AI推测：AI是否安全？ - AI推测：AI是否不安全？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ - AI推测：AI是否有效？ - AI推测：AI是否无效？ -</sample>
    <sample id="260">任务A：社会接受度 1. 阅读情况。 2. 讨论你的看法。 3. 看看其他人和AI们是怎么想的。 参与者可以将自己的反应与他人和AI们的反应进行比较。</sample>
    <sample id="261">任务A：社会可接受性分析数据集 - 社会化学模型 - 德尔菲 - GPT-4</sample>
    <sample id="262">任务B：毒性</sample>
    <sample id="263">画面中显示了一张幻灯片，标题为“Study Participation”。幻灯片上列出了三个数字和相应的标签。第一个数字是“16,299”，标签是“annotations”（注释）。第二个数字是“1,096”，标签是“annotators”（注释者）。第三个数字是“87”，标签是“countries”（国家）。在右上角有一个小窗口，显示一个人在讲话或演示。背景是一个书架，上面放着一些书籍和其他物品。</sample>
    <sample id="264">The first slide presents a question about the alignment of NLP datasets and models, asking 'Who do NLP datasets and models align with?' The second slide introduces the findings from this analysis. It states that there is positionality in NLP (Natural Language Processing).</sample>
    <sample id="265">Datasets and models are most aligned to English-Speaking countries.</sample>
    <sample id="266">图表显示了不同教育水平在社会接受度任务中的表现。柱状图中，每个教育水平都有一个对应的柱子，柱子的高度代表了社会接受度的值。柱子的颜色从深到浅依次为蓝色、绿色、灰色和浅灰色，对应着不同的教育水平：大学、研究生院、高中、博士后、中等教育前和教授学校。每个柱子上都标有样本数量（N）和相应的社会接受度值。例如，大学教育的样本数量为4,489，社会接受度值为0.69*；研究生院教育的样本数量为1,116，社会接受度值也为0.69*。图表左侧的文字说明“数据集和模型最与拥有大学教育的人对齐”。</sample>
    <sample id="267">数据集和模型最与拥有大学学历的人群对齐。</sample>
    <sample id="268">发现2：有些人群被落下。</sample>
    <sample id="269">图表显示了三个类别：男性、非二元性别和女性。每个类别都有一个柱状图，表示社会可接受度的分数。对于男性，社会可接受度为0.69，样本大小为4,082。对于非二元性别，社会可接受度为0.55，样本大小为858。对于女性，社会可接受度为0.73，样本大小为4,368。图表的标题是“GPT-4的社会可接受性”，左侧的文字说明是“数据集和模型与非二元性别的人不太匹配”。</sample>
    <sample id="270">那么，我们可以做什么？在NLP中解决位置性问题。</sample>
    <sample id="271">推荐 1. 在整个构建数据集或模型的过程中，记录所有相关的设计选择。 推荐 2. 通过透视主义的视角进行自然语言处理研究： a. 共享拆分的标签数据集！</sample>
    <sample id="272">推荐内容包括：1. 记录在构建数据集或模型时做出的所有相关设计选择。2. 通过视角主义进行NLP研究：a. 分享分段数据集标签！b. 使用能够处理注释员分歧的建模技术。3. 构建针对特定社区的专门数据集和模型是有价值的，以促进包容性NLP（例如，Masakhane倡议）！</sample>
    <sample id="273">幻灯片上显示了“谢谢！”字样。在顶部，有一个链接和一个论文的URL。在中间，有一个Delphi的标志，并且有一个网址“https://www.delfi.org/”。幻灯片的下半部分展示了六个图表，分别代表年龄、性别、族裔、宗教、教育水平、居住国家、最长居住国家以及母语。右上角有一个小图像，显示一个人坐在桌子旁，背景中有一些物品。</sample>
    <sample id="274">演讲者提到了 SimulST 的几个问题：1. 特定的架构通常被训练，引入额外的模块以进行优化。2. 长而复杂的训练程序（例如，不同的优化目标）。3. 训练和维护多个模型以达到不同的延迟制度（例如，1秒、2秒等）。</sample>
    <sample id="275">为了减轻数据集中的社会和政治偏见，有效的方法是先对数据进行预处理。</sample>
    <sample id="276">The 61st Annual Meeting of the Association for Computational Linguistics, Toronto, Canada, July 9-14, 2023.</sample>
    <sample id="277">这张图片展示了一个名为“Language Planning”的演示文稿。主要内容是一个关于如何制作蛋糕的逐步说明，标题为“How to Make a Cake?”。步骤如下： 1. 收集食材。 2. 预热烤箱至325°F（163°C），并在蛋糕模具上涂油和撒面粉。 3. 将黄油和糖搅拌均匀。 4. 添加鸡蛋。 5. 将面粉和泡打粉混合后加入。 6. 将面糊倒入模具中。 7. 烘烤15分钟。 另外，图片中还有一段文字说明：“大型语言模型（LLMs）可以将目标分解成步骤。” 图片右侧有一个戴眼镜、穿绿色上衣的人，背景看起来像是一个办公室环境。</sample>
    <sample id="278">Language Planning How to Make a Cake? 1. Gather your ingredients. 2. Preheat the oven to 325°F (163°C) and grease and flour a cake pan. 3. Cream the butter and sugar. 4. Add eggs. 5. Stir in the dry ingredients, alternating with the milk. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour 15 minutes. Large language models (LLMs) can effectively decompose goals into steps</sample>
    <sample id="279">Constrained Language Planning How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add the cocoa powder into the flour... Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints.</sample>
    <sample id="280">这篇文章中，我们定义了约束语言规划的问题。</sample>
    <sample id="281">The text in the image is about constrained language planning. It provides instructions on how to make a strawberry cake and a chocolate cake, with specific steps like adding strawberry jam or cocoa powder into flour. The abstract goal can be inherited by different real-life specific goals with multi-faceted constraints.</sample>
    <sample id="282">In this paper, we first evaluate and impose a constrained language planning ability of large language models.</sample>
    <sample id="283">The image contains a slide with the title "How do LLMs perform on Constrained Language Planning?" The dataset mentioned is "wikiHow + Generated Constraints." There are three types of constraints listed: 1. Constraint Type 1: Modifier - Defined as an adjective or phrase that modifies or constrains an abstract goal, with examples like "Make a chocolate cake" and "Make a pink cake." 2. Constraint Type 2: Method - Described as a tool or specified mode that controls the process for achieving the goal, illustrated by examples such as "Make a cake with an oven" and "Make a cake by using cake mix." 3. Constraint Type 3: Intent - Explained as an additional purpose or demand when completing the goal, shown through examples like "Make a cake for wedding" and "Make a cake for diabetics."</sample>
    <sample id="284">The English content in the image is: How do LLMs perform on Constrained Language Planning? Dataset: wikiHow + Generated Constraints Constraint Type 1: Modifier Definition: A word, an adjective or a phrase that modifies or constrains an abstract goal. Ex1: Make a chocolate cake. Ex2: Make a pink cake. Constraint Type 2: Method Definition: A tool or specified mode that controls the process for achieving the goal. Ex1: Make a cake with an oven. Ex2: Make a cake by using cake mix. Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal. Ex1: Make a cake for wedding. Ex2: Make a cake for diabetics.</sample>
    <sample id="285">The image contains a slide with the title "Can LLMs do Constrained Language Planning?" and a bar chart showing accuracy for different models: T5 (11B), Flan-T5 (11B), GPT-3 (175B), GPT-4 (175B), and InstructGPT (175B). The text at the bottom states, "All baselines achieve unsatisfactory results on planning for specific goals." Additionally, there is a video call interface in the top right corner.</sample>
    <sample id="286">这张图片展示了一个演示幻灯片，标题为“Can LLMs do Constrained Language Planning?”。幻灯片左侧有一个柱状图，显示了不同语言模型在特定目标规划上的准确率。图表中的语言模型包括T5（11B）、Flan-T5（11B）、GPT-3（175B）、CodeLLaMDA（175B）和InstructGPT（175B）。每个模型的柱子高度不同，表示它们在规划任务上的表现。幻灯片右侧有一段文字，写着“All baselines achieve unsatisfactory results on planning for specific goals”，表明所有基线模型在特定目标规划上都未能达到令人满意的成果。背景中可以看到一个人，似乎是在一个有桌子和椅子的房间里，可能是一个办公室或会议室。</sample>
    <sample id="287">LLMs通常在完成任务时犯哪些错误？</sample>
    <sample id="288">The image shows a presentation slide with the title "What types of errors do LLMs usually make in this task?" The slide includes a radar chart labeled with different error categories such as 'No constraint,' 'Repeated steps,' 'Wrong order,' and 'Incoherent.' There is also text that reads, "The semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) cannot be guaranteed." Additionally, there is an inset picture of a person wearing glasses.</sample>
    <sample id="289">The planning performance of InstructGPTs varies considerably for goals of different categories.</sample>
    <sample id="290">画面中展示了一个演示文稿的幻灯片，左侧是一个女性在讲话。幻灯片上有一个标题“Method”，下面有几段文字和一些框图。幻灯片的主要内容是关于一个方法论的介绍，包括输入抽象目标、生成具体目标以及特定目标等步骤。幻灯片右侧显示了一个人在一个现代化的办公室环境中。</sample>
    <sample id="291">The image shows a slide from a presentation with the title "Method" at the top. The content of the slide is divided into several sections, each explaining different aspects of generating specific goals using InstructGPT via in-context learning.

1. **Title**: 
   - "Method"

2. **Input**:
   - "Input: an abstract goal"

3. **Step 1**:
   - "Generate specific goals with InstructGPT via in-context learning"
   - This section includes an illustration of a robot and some text that appears to be part of the process described above.

4. **Abstract Goal**:
   - "Abstract Goal: Make a cake"
   - Below this, there are constraints listed as:
     - "+ constraints"

5. **Specific Goals**:
   - Two examples of specific goals are provided:
     - "G1 (+ modifier): Make a chocolate cake"
     - "G2 (+ method): Make it in a microwave"
     - An icon of a cat accompanies these examples.
   - Another example is given for intent:
     - "G3 (+ intent): Make a cake for a wedding"
     - An icon of a person holding hands accompanies this example.

6. **Additional Text**:
   - There is additional text on the right side of the slide, but it is not fully visible or clear enough to read accurately.

7. **Visual Elements**:
   - On the right side of the slide, there is a video call interface showing a woman wearing glasses and a green shirt, sitting in what looks like an office environment with large windows and furniture.

This detailed description covers all the elements present in the image, providing a comprehensive understanding of its contents.</sample>
    <sample id="292">方法 1. 输入：抽象目标 2. Step 1: 使用 InstructGPT 通过上下文学习生成具体的任务，例如“制作蛋糕”、“在微波炉中烤蛋糕”和“为婚礼制作蛋糕”。 3. Step 2: 通过上下文学习过量生成候选脚本。 4. 生成针对特定目标的计划。</sample>
    <sample id="293">画面中展示了一个名为“Method”的流程图。流程图分为三个步骤： 1. Step 2: Over-generate candidate scripts via in-context learning。 2. Step 3: Find the goal with InstructGPT via similarity score。 3. Output: Specific goals with corresponding scripts。 流程图的右侧有一个女性，她戴着眼镜，穿着绿色上衣，坐在一个现代化的办公室环境中。办公室里有白色的墙壁和灰色的地板，背景中可以看到一些家具和装饰品。</sample>
    <sample id="294">这段文字描述了如何将脚本和目标转换为InstructGPT嵌入，并通过余弦相似度计算相似度分数来衡量语义相似性。</sample>
    <sample id="295">这张图片展示了一个名为“Method”的方法论。左侧有一个流程图，分为三个步骤： 1. Step 2: Over-generate candidate scripts via in-context learning。 2. Step 3: Find the goal with InstructGPT via similarity score。 3. Output: Specific goals with corresponding scripts。 在流程图的右侧，有一个图表展示了候选脚本和过滤后的脚本。候选脚本通过相似性得分进行评分，得分从0.3到0.5不等。其中一些脚本被标记为“+”，表示它们是过滤后的脚本。 图表底部显示了一个示例脚本：“Script 3: 1. Gather your ingredients 2. Add the cocoa powder”。 右上角有一张照片，显示一个人在室内环境中，背景是一个现代的办公室或会议室。</sample>
    <sample id="296">画面中有一个穿着绿色上衣的人，背景是一个现代化的室内环境。画面上方有一段文字“我们的方法极大地提高了规划质量”，下方有一个柱状图，展示了不同模型在准确率方面的表现。图表中的标签包括T5 (11B)、Flan-T5 (11B)、GPT-3 (175B)、InstructGPT (175B)和我们的方法。图表显示了这些模型在准确率上的比较情况。</sample>
    <sample id="297">The content of the image is a slide from a presentation. The title at the top reads "Script Distillation from LLMs". Below this, there are two sections: 'Motivation' and 'Method'. In the Motivation section, it states that enabling constrained language planning ability for smaller models was one goal. Under Method, it mentions following symbolic knowledge distillation to generate 55,000 scripts with constraints based on CoSprint Dataset. It also notes that these scripts were annotated by humans for validation and testing purposes. On the right side of the slide, there's an illustration showing three steps in the process:

1. Generate specific goals using InstructorGPT via in-context learning.
2. Over-generate candidate scripts within context with instructorGPT.
3. Filter scripted goals through InstructorGPT using a similarity score.

At the bottom of the slide, it specifies that the output should be specific goals with corresponding plans.</sample>
    <sample id="298">Motivation: To enable constrained language planning for smaller models. Method: Follow the idea of symbolic knowledge distillation. Generated 55,000 scripts with constraint from LLMs based on our method → Coscript Dataset. Human annotate validation and test set.</sample>
    <sample id="299">Script Distillation from LLMs</sample>
    <sample id="300">这段文字介绍了通过LLMs进行脚本蒸馏的方法。它包括三个步骤：1. 生成特定目标的脚本，使用InstructGPT在上下文学习中。2. 通过上下文相似度分数过滤脚本。3. 人工标注验证和测试集。最终的目标是构建一个名为CoScript的数据集，用于构建受约束的语言规划能力较小的模型。</sample>
    <sample id="301">幻灯片内容包括一个标题“Script Distillation from LLMs”，分为两个主要部分：“动机”和“方法”。在“动机”部分，解释了目标是通过使用InstructGPT进行在线学习来使较小的模型具有约束力的语言规划能力。在“方法”部分，概述了三个步骤：1. 生成特定目标的脚本（使用InstructGPT进行在线学习）。2. 通过与InstructGPT比较基于约束的脚本来生成脚本。3. 使用InstructGPT根据约束对脚本进行评分，并标注验证和测试集。幻灯片还提到，总共生成了55,000个带约束的脚本，并使用CoSprint数据集进行了标注。</sample>
    <sample id="302">这张幻灯片的标题是“Coscript for Smaller Language Models”。它包含两个主要部分： 1. 数据集（Datasets）： - Coscript 2. 指标（Metrics）： - 一个忠实的模型：DeBERTa (v3 large)用于决定生成的文本是否符合约束。 - 自动指标：ROUGE、BLEU、BERTScore。 这张幻灯片还提到，通过在Coscript上微调较小但专门的语言模型，可以实现约束语言规划。</sample>
    <sample id="303">画面中的文字内容包括：

1. 标题：Specialized Models vs. LLMs
2. 图表标题：Accuracy
3. 图表数据：
   - GPT-3 (175B)
   - Codex (175B)
   - InstructGPT (175B)
   - T5 trained on wikiHow
   - T5 trained on Coscript
4. 副标题：Smaller LM's fine-tuned on Coscript can generate higher quality scripts than LLMs

这些文字描述了不同模型在准确性方面的表现，并强调了在特定数据集上微调的小型语言模型（LM）可能比大型语言模型（LLM）生成更高质量的脚本。</sample>
    <sample id="304">幻灯片包含一个标题“Summary and Takeaways”，并分为两个主要部分：1. “Establish the constrained language planning problem”2. “Limitations and future work”在“Establish the constrained language planning problem”下，有以下要点：- 评估大型语言模型（LLMs）的约束语言规划能力，并开发一个过生成然后过滤的方法。- 使用CoScript（约束语言规划的高质量脚本数据集）来生成约束语言规划的高质量脚本数据集。在“Limitations and future work”下，有以下要点：- 改进LLMs的方法是后验方法。- CoScript仅从一个额外的约束中继承。- CoScript数据集可以作为推进语言规划研究的宝贵资源，具有更多的复杂目标和约束。幻灯片右上角显示一个人，可能正在演示内容。</sample>
    <sample id="305">幻灯片上有一段英文内容，主要讨论了约束语言规划的问题。具体内容如下： 1. 建立约束语言规划问题。 2. 评估LLMs（大型语言模型）在约束语言规划中的能力，并开发一个过载-然后-过滤器用于LLMs。 3. 使用LLMs生成高质量的脚本数据集（CoScript）用于约束语言规划。 这些要点表明，该幻灯片的重点是介绍和解决与使用LLMs进行约束语言规划相关的问题。</sample>
    <sample id="306">61st Annual Meeting of the Association for Computational Linguistics Toronto, Canada July 8-14, 2023 Distilling Script Knowledge from Large Language Models for Constrained Language Planning Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang syyuan21@m.fudan.edu.cn https://github.com/siyuuyuan/coscript</sample>
    <sample id="307">PaLM的流畅度与SOTA相当。</sample>
    <sample id="308">水印方法的重要属性是： 1. 可以应用于嵌入式服务。 2. 不应降低提供的嵌入式内容的可用性。 3. 应该足够隐蔽，以至于攻击者无法轻易检测到或移除水印。 4. 水印需要能够转移到攻击者的服务中。</sample>
    <sample id="309">TED 英语演讲已被翻译成14种不同的语言，包括英语、西班牙语、法语、意大利语、日语、荷兰语、葡萄牙语、罗马尼亚语、俄语、土耳其语和中文。</sample>
    <sample id="310">The slide suggests that a significant number of instances should be selected for re-annotation. It states, "We select 300 instances to annotate."</sample>
    <sample id="311">Cosine and L2 similarity metrics are used to measure the difference between benign and backdoor datasets.</sample>
    <sample id="312">基于编码器的多语言模型可以通过使用编码器-解码器模型来实现。</sample>
    <sample id="344">作者通过在一般文本语料库中计算单词频率来确定中等频率的单词。</sample>
    <sample id="345">画面中有一个圆形的头像，背景是浅色的，上面有一些几何图案。头像旁边有文字，写着“Shuheng Liu, Alan Ritter”，以及“School of Interactive Computing, Georgia Institute of Technology”。右下角有一个Georgia Tech的标志。</sample>
    <sample id="346">这段文字介绍了名为“命名实体识别与泛化”的内容。</sample>
    <sample id="347">幻灯片的标题是“Named Entity Recognition &amp; Generalization”。它包含一个项目符号，内容是：“这些模型已经使用CoNLL-2003开发NER接近20年了。”还有一个问题：“这些模型能适应现代数据吗？”右下角有一个Georgia Tech的标志。</sample>
    <sample id="348">幻灯片上的文字内容是关于命名实体识别和泛化。主要内容包括： 1. 模型已经使用CoNLL-2003来开发NER（命名实体识别）将近20年。 2. 这些模型是否能够适应现代数据？ 3. 为了实现良好的泛化，需要什么？ 幻灯片的左下角有一个Georgia Tech的标志。</sample>
    <sample id="349">The image shows a slide from a presentation titled 'Named Entity Recognition &amp; Generalization.' The content of the slide includes three bullet points: 1. Models have been using CoNLL-2003 to develop NER for almost 20 years. 2. Can these models generalize to modern data? 3. What is needed for good generalization? Additionally, there are two more questions listed below in black text: - What causes the performance drop of these models? There is also an image of a person and a logo at Georgia Tech in the bottom right corner.</sample>
    <sample id="350">幻灯片展示了关于CoNLL++数据集的信息。标题为“CoNLL++数据集”，内容包括收集了2020年的路透社新闻，并使用CoNLL-2003注释指南进行了注释。文本旁边有一个列表，显示了单词及其相应的标注类别：AMBASSADOR, TO, THE, UNITED NATIONS, : (冒号), LINDA 和 THOMAS-GREENFIELD。每个单词都与一个标签配对，例如AMBASSADOR和TO被标记为“O”，而LINDA和THOMAS-GREENFIELD被标记为“I-PER”（表示个人）。幻灯片的右下角有Georgia Tech的标志。</sample>
    <sample id="351">The slide presents information about the CoNLL++ dataset. It explains that Reuters news from 2020 was collected and annotated using guidelines from CoNLL-2003. Additionally, over 20 models were fine-tuned on data from CoNLL-2003. The evaluation of these models took place on both the CoNLL-2003 test set and the CoNLL++ test set.</sample>
    <sample id="352">CoNLL++ Dataset</sample>
    <sample id="353">The content in the image is a slide from Georgia Tech, which poses the question "What Is Needed for Good Generalization?" This suggests that the presentation might be discussing factors or conditions necessary to achieve effective generalization. The presence of the person's photo indicates they may be presenting this information at an event hosted by Georgia Tech.</sample>
    <sample id="354">这段内容讨论了为了实现良好的泛化所需的元素。它强调了模型架构的重要性，并指出Transformer模型在泛化方面表现更好。</sample>
    <sample id="355">What Is Needed for Good Generalization?</sample>
    <sample id="356">这段文字讨论了为了实现良好的泛化，需要哪些因素。它提到了模型架构、模型大小和微调示例的数量。具体来说，它指出Transformer模型在泛化方面表现更好，更大的模型也表现出更好的泛化能力，并且更多的微调示例会导致更好的泛化。</sample>
    <sample id="357">性能下降的原因是什么？</sample>
    <sample id="358">幻灯片上有一个标题，写着“What Causes Performance Drop?”。在标题下方，有一个项目符号，写着“Adaptive overfitting?”。右下角有一个Georgia Tech的标志。</sample>
    <sample id="359">幻灯片上有一个标题，写着“是什么导致性能下降？”，下面有两个要点。第一个要点是“自适应过拟合？”第二个要点是“时间漂移？”。左下角有一个小圆形图片，右下角有一个Georgia Tech的标志。</sample>
    <sample id="360">What Causes Performance Drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="361">What Causes Performance Drop? Adaptive overfitting? Temporal drift? No diminishing returns Temporal drift?</sample>
    <sample id="362">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed Temporal drift?</sample>
    <sample id="363">Temporal drift refers to the gradual change or shift in a system's behavior over time. In machine learning, it can occur when models are not regularly updated with new data, leading to their performance degrading as they become less effective at predicting outcomes based on recent trends and patterns. This concept is particularly relevant for tasks like language modeling or image recognition where contextual information changes frequently.</sample>
    <sample id="364">What Causes Performance Drop? Adaptive overfitting, No diminishing returns, Not observed Temporal drift?</sample>
    <sample id="365">性能下降的主要原因是时间漂移。</sample>
    <sample id="366">这段文字讨论了为了实现良好的泛化，我们需要更好的模型架构、更大的模型尺寸以及更多的微调示例。这些目标是紧密相关的，不能只依赖于一个因素，而忽视其他因素。</sample>
    <sample id="367">将这段英语内容表达为中文。</sample>
    <sample id="368">The content of this paragraph is: 'And we found that the answer is actually a resounding YES.'</sample>
    <sample id="369">结论 为了更好地泛化，我们需要： - 更好的模型架构 - 较大的模型大小 - 更多的精炼示例 性能下降是由： - 时间漂移 - 不适配过拟合 这篇论文希望引起更多关于如何改进模型泛化能力的研究。</sample>
    <sample id="370">The content of this text is about providing contact information and resources related to a paper. It includes the following details: 1. A link to an arXiv paper (https://arxiv.org/abs/2212.09747) 2. A link to a dataset on GitHub (https://github.com/ShuhengL/ac12023_conllpp) 3. An email address for contact purposes (sliu775@gatech.edu) The background image shows people walking in front of a building, which appears to be part of Georgia Tech's campus.</sample>
    <sample id="397">该方法使用的语音片段大小是1024个样本。</sample>
    <sample id="398">Servin is a judge.</sample>
    <sample id="399">The example quality is more important than similarity to the source sentence.</sample>
    <sample id="400">论文侧重于RoBERTA-base、RoBERTA-large、distilRoBERTa和ALBERT-large。</sample>
    <sample id="401">该模型是使用特定层的注意力分数。</sample>
    <sample id="402">直接推断的示例包括： 1. “easy on me”，“the first one” 2. “The song that’s not energetic.”</sample>
    <sample id="403">这篇论文的作者所属机构是复旦大学和Brain Technologies Inc.。</sample>
    <sample id="404">这篇论文有6位作者。</sample>
    <sample id="405">是的，在语义解析之前，使用机器翻译模型将自然语言查询翻译成目标语言作为基线。</sample>
    <sample id="406">作者给出的“显性群体”(marked group) 的示例是“a woman warrior”。</sample>
    <sample id="407">Based on the content of the image, it can be inferred that models with poor generalization ability are those without good model architecture. The slide specifically mentions "Transformer models generalize better," implying that transformer-based architectures tend to have stronger generalization capabilities compared to other types of models or architectures. Therefore, we can conclude that models lacking effective architectural design may struggle in terms of their ability to adapt and perform well on new data.</sample>
    <sample id="408">测试数据集的名称是“WSL approaches benefit from more clean validation samples!”</sample>
    <sample id="409">The paper has six authors: Akthar Al-Ali, Martin Pomsl, Kaheer Saleem, Adam Trischler, Alexandra Olteneanu, and Jackie CK Cheung.</sample>
    <sample id="410">The author used multiple modalities, including text and video.</sample>
    <sample id="439">作者认为，NLU 中研究不足的领域是“pretrain-time knowledge”和“inference-time knowledge”。</sample>
    <sample id="440">演讲者的名字是Zhiyang Xu、Ying Shen和Lifu Huang。</sample>
    <sample id="441">Yes, Coscript has been quality checked.</sample>
    <sample id="442">现有的资源在评估上下文依赖翻译时的局限性包括： 1. 上下文依赖的单词比例较小。 2. 现有方法支持的讨论现象和语言有限。 这些限制表明，尽管有一些资源存在，但它们可能无法完全捕捉到上下文依赖翻译的复杂性和多样性。</sample>
    <sample id="443">Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis Google Research</sample>
    <sample id="444">Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis Google Research</sample>
    <sample id="445">Indirect referring expressions are used to understand users' language when they want to make a choice.</sample>
    <sample id="446">The image contains a slide from a presentation titled 'Indirect Referring Expressions'. The goal of the study is to understand users' language when they make a choice. It discusses alternative questions and direct references, providing examples such as "easy on me" or "the first one". Indirect reference could be used in natural and fluid conversation with notes that it cannot remember the name but pronunciations are hard to distinguish and want to specify a preference. An example given for indirect reference is "the newer one." Another example provided is "the song that's not energetic." There is also an illustration showing how these expressions can be applied in everyday conversations by using names like "me" or positions like "the first one."</sample>
    <sample id="447">Indirect referring expressions are used to refer to something indirectly. This can happen when the user cannot remember the name of a song or when they want to specify their preference for one option over another in conversation.</sample>
    <sample id="448">Indirect referent could be used in natural and fluid conversation: Cannot remember the name The pronunciations are hard to distinguish Want to specify a preference</sample>
    <sample id="449">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference easy on me, the first one Indirect reference could be used in natural and fluid conversation: Cannot remember the name The pronunciations are hard to distinguish Want to specify a preference Indirect reference The newer one. The song that's not energetic.</sample>
    <sample id="450">这是对话系统和评估大型语言模型实体理解的重要问题。</sample>
    <sample id="451">Dataset Collection Important problem Conversational systems Benchmarking Large Language Models’ entity understanding No large-scale public dataset available We collect a large dataset using crowd annotation Three domains:</sample>
    <sample id="452">数据集收集方法论强调使用卡通完成任务来强调非正式性。</sample>
    <sample id="453">画面中展示了一张幻灯片，标题为“Dataset Collection Methodology”。幻灯片的主要内容是关于使用卡通完成任务来强调非正式性。幻灯片上有一段文字说明了这种方法的步骤和目的。此外，还有三个人物形象，每个形象旁边都有一个对话框，显示了他们之间的对话内容。右下角有一个黄色箭头，指向一个注释框，注释框里写着“由注释员填写”。左下角有一个小圆形图像，可能是演讲者的照片。背景是一个室内环境，有植物装饰。</sample>
    <sample id="454">在第二张图片中，Alice说：“你是指‘easy on me’还是‘I got a feeling’？”</sample>
    <sample id="455">The alternative question is 'Do you mean "Easy on Me" or 1 Gotta Feeling?'.</sample>
    <sample id="456">Dataset Collection Methodology</sample>
    <sample id="457">The second one, which is the alternative question, is generated as follows.</sample>
    <sample id="458">图片中的文字内容包括：

标题：Generate alternative questions =&gt; sampling entity pairs

副标题：Do you mean A or B?

正文：
- Items with similar infoboxes on Wikipedia (same genre and/or artist)
  - Do you mean This is It or Man in the Mirror?
- Items with similar descriptions on Wikipedia
  - Do you mean Thinking of You or Happy Anywhere?
- Items with similar titles:
  - Do you mean The Return (memoir) or The Return (Shatner novel)?
- Uniform at random:
  - Do you mean You Could Be Mine or The Way I Am

底部注释：Revisiting Inductive Expressions for Entity Selection Variability Correlation

右上角有Google Research的标志。</sample>
    <sample id="459">生成替代问题 =&gt; 生成实体对</sample>
    <sample id="460">生成替代问题 =&gt; 生成实体对</sample>
    <sample id="461">生成替代问题 =&gt; 生成实体对</sample>
    <sample id="462">这张幻灯片是关于生成替代问题以实体对齐的演示。它包含一个标题，写着“Generate alternative questions =&gt; sampling entity pairs”，以及一个副标题，写着“Do you mean A or B?”。在这些文本旁边，有一个黄色箭头，上面写着“More Similar (usually harder)”。幻灯片还列出了几种生成替代问题的方法：1. 与维基百科上具有相似信息框（同一流派和/或艺术家）的项目相关的问题。2. 与维基百科上具有相似描述的项目相关的问题。3. 具有相似标题的项目。4. 随机选择的项目。幻灯片右下角有一个小图像，可能是一个人的头像。右上角有一个Google Research的标志。</sample>
    <sample id="463">背景知识（音乐） Google搜索链接到每首歌。 Easy on Me（Adele演唱） I Gotta Feeling（黑眼豆豆演唱） 我们要求注释员： 听至少一首歌的片段 阅读每首歌</sample>
    <sample id="464">幻灯片的标题是“Background knowledge (Music)”。</sample>
    <sample id="465">背景知识（音乐） Easy on Me Adele</sample>
    <sample id="466">为了在“食谱”和“书籍”领域中提供背景信息，我们展示了维基百科上的部分背景文本。此外，我们还展示了来自维基百科的图片，以帮助注释器了解它们的外观。</sample>
    <sample id="467">所给出的英文翻译成中文是：然后我们告诉注释员选择哪一个，并要求他们描述它。</sample>
    <sample id="468">所给出的英文翻译成中文是： '我们然后告诉注释者选择哪一种，并要求他们描述它。'</sample>
    <sample id="469">AltEntities Corpus 包含大约6000个在三个领域中的替代问题和约42000个间接指代表达式。使用T5 XL模型的结果如下： - 当LM具有与注释员相同背景知识时，准确率为92-95%。 - 当LM具有部分重叠的背景知识时，准确率为82-87%。 - 当LM仅具有实体名称访问权限时，准确率约为60%。此外，该研究展示了模型是域通用的。数据集链接为：https://github.com/google-research/datasets/AltEntities</sample>
    <sample id="470">The image shows a slide from a presentation by Google Research. The title of the slide is 'AltEntities Corpus'. It contains information about an alternative questions dataset across three domains, including 6000 different questions and approximately 42,000 indirect referring expressions.

The slide also discusses results with T5 XL model accuracy:
- If the language model has access to the same background knowledge as annotators: 92-95% accuracy.
- If it has access to partially overlapping background knowledge: 82-87% accuracy.
- With only entity names available: around 60% accuracy.

It concludes that their models are domain-generalizable. There is a dataset link provided at the bottom for more information (https://github.com/google-research/datasets/AltEntities).

Additionally, there is a small profile picture in the lower right corner of the presenter or author associated with this research.</sample>
    <sample id="471">图片中的文字内容包括：

标题：AltEntities Corpus

副标题：Google Research

正文：
- ~6,000 alternative questions across the three domains
- ~42,000 indirect referring expressions
- Results with T5 XL model (accuracy):
  - 92-95% if the LM has access to the same background knowledge as annotators.
  - 82-87% when the LM has access to partially overlapping background knowledge.
  - ~60% when the LM (T5 XL) has only access to the entity names.
- We showed models are domain-generalizable.

数据集链接：https://github.com/google-research/datasets/AltEntities

底部注释：Resolving Indirect References for Entity Selection AltEntities Corpus</sample>
    <sample id="472">幻灯片上显示了“AltEntities Corpus”标题，下面列出了几个要点： 1. 约6000个不同领域的替代问题。 2. 约42000个间接表达的实体引用。 3. T5 XL模型的准确性结果： - 如果LM有与注释员相同的背景知识，准确率为92-95%。 - 如果LM有部分重叠的背景知识，准确率为82-87%。 - 如果LM只有实体名称的访问权限，准确率下降到60%。 4. 强调展示了模型的领域通用性。 5. 提供了一个数据集链接：https://github.com/google-research/datasets/AltEntities。 幻灯片右下角有一个Google Research标志，并且左下角有一个小圆形图像。</sample>
    <sample id="473">该方法与 popular strategies also applied to offline models 进行了比较。</sample>
    <sample id="474">The authors belong to the following institutions: 1. LIA, Avignon Université 2. LSIN, Nantes Université 3. Clinique des diaboliques, CHU de Nantes 4. Zenicod</sample>
    <sample id="475">演讲者的名字是Jenny T. Liang。</sample>
    <sample id="476">The paper has three authors: Myra Cheng, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="477">这是一张演示文稿的截图，标题为“Attention as a Guide for Simultaneous Speech Translation”。作者包括Sara Papi、Matteo Negri和Marco Turchi。图片底部展示了University of Trento和Bruno Kessler基金会的标志。</sample>
    <sample id="478">Simultaneous speech translation, or SimulST, is the process of translating spoken language into text in another language in real-time.</sample>
    <sample id="479">特定的架构通常被训练，引入额外的模块以进行优化。</sample>
    <sample id="480">这段文字讨论了当前SimulIST模型存在的问题。具体来说，它指出这些模型通常需要特定的架构进行训练，并且引入了需要优化的额外模块。此外，它还提到训练程序非常长且复杂，例如涉及不同的优化目标。</sample>
    <sample id="481">What are the problems of the current SimuIST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures (e.g., different optimization objectives). Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s,...)</sample>
    <sample id="482">我们的解决方案是什么？</sample>
    <sample id="483">幻灯片展示了关于SimulST解决方案的详细信息。第一个要点建议使用现有的离线ST模型，无需重新训练或采用特定架构。第二个要点建议使用一个模型来处理每个延迟制度，并通过特定参数来管理延迟。</sample>
    <sample id="484">What is our solution? 01 Use already existing offline ST models without re-training or adopting specific architecture for SimuST. 02 Use only one model for every latency regime and handle latency through specific parameters. 03 Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output that is the cross-attention mechanism. And you can see an example on the right.</sample>
    <sample id="485">决定是否发射或不发射部分翻译，基于注意力指向一个单词（其和低于阈值α）到最后一个说话帧，这意味着接收到的信息是足够的稳定的。</sample>
    <sample id="486">图片中的文字内容包括： 1. "Our solution: EDAtt" 2. "Encoder-Decoder Attention" 3. "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold α) towards the last λ speech frames, meaning that the received information is enough stable." 4. "page 014" 在图片的右上角，有一段文字写着“什 么？”。</sample>
    <sample id="487">画面中的文字内容包括：

1. "Our solution: EDAtt"
2. "Encoder-Decoder Attention"
3. "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold a) towards the last λ speech frames, meaning that the received information is enough stable."
4. "01 I am going to talk about..."
5. "page 014"

这些文字主要介绍了EDAtt解决方案和编码器-解码器注意力机制的细节。</sample>
    <sample id="488">这张图片展示了一个名为“EDAtt”的解决方案的幻灯片。幻灯片的主要部分是关于编码器-解码器注意力机制的解释。它指出，决定是否需要完整的翻译还是部分翻译，取决于注意力是否集中在单词上。具体来说，如果注意力没有集中在单词上（其和低于阈值），那么接收到的信息就是稳定的。幻灯片上有一个音频波形图，下面写着“I am going to talk about...”和“Ich werde reden.”，这表明这是一个多语言的演示。右上角有一个小窗口显示了一个人，可能是演示者。幻灯片底部有页码“page 016”。</sample>
    <sample id="489">图片中的文字包括标题“Encoder-Decoder Attention”和副标题“Our solution: EDAtt”。此外，还有一段解释文本，提到根据注意力是否集中在单词上，决定是否输出部分翻译。如果注意力没有集中在单词上，说明接收到的信息足够稳定。底部有一个图表，显示了“ich werde reden”的音调变化，并标有“I am going to talk about...”。页面右下角显示第19页。</sample>
    <sample id="490">这表明前两个单词将被省略。</sample>
    <sample id="491">图片中的英文文本翻译成中文为： 我们将等待另一个语音片段。</sample>
    <sample id="492">幻灯片显示了“Encoder-Decoder Attention”的概念。它解释了模型如何决定是否发出或不发出部分翻译，这取决于注意力点。具体来说，如果注意力点的总和低于阈值 threshold，那么模型将认为接收到的信息是稳定的。</sample>
    <sample id="493">图片显示了一个名为“Encoder-Decoder Attention”的演示幻灯片。幻灯片分为两个部分，每个部分都有一个编号（01和02）和一个波形图。在每个部分的左侧，有德语短语“Ich werde reden”和“Ich werde über Klima sprechen”。在每个部分的右侧，有一个标题为“我将谈论...”的英文句子。在波形图上方，有一段文字解释了“Encoder-Decoder Attention”的概念。这段文字提到，如果注意力不集中到阈值以下的某个单词上，那么接收到的信息就会稳定。右上角有一个小窗口，显示了一位女士的图像。幻灯片底部标有页码“第02页”。</sample>
    <sample id="494">我们的解决方案是：EDAtt</sample>
    <sample id="495">如果查看EADAtt的主要结果，您会看到一个图表，横轴表示AL/AL_CA（秒），纵轴表示BLEU分数。</sample>
    <sample id="496">The English text in the image is: 'Main Results: EDAtt' and 'quality measure'.</sample>
    <sample id="497">画面中的文字内容包括：

1. 左上角：Main Results: EDAtt
2. 图表下方的标签：
   - AL/AL_CA (s)
   - latency measure
3. 右下角：page 030
4. 图表上方有一些蓝色的符号，看起来像是表情符号或图标。

这些文字主要描述了图表的主要结果和相关指标。</sample>
    <sample id="498">这是一张演示文稿的截图，显示了一个名为“Main Results: EDAtt”的幻灯片。幻灯片上有一个柱状图，Y轴标记为“BLEU”，X轴标记为“AL/AL_CA (s)”。柱状图显示了从0.5到6秒的不同时间间隔的数据点。在柱状图上方，有五个蓝色的问号，表示可能的问题或讨论点。右上角有一个小窗口，显示了一个正在讲话的人。左下角有一个蓝色的标志，可能是演示文稿的品牌或组织标志。右下角标注了页面编号“page 031”。</sample>
    <sample id="499">主要结果：EDAtt</sample>
    <sample id="500">图中展示了一个演示幻灯片，顶部有一个标题“Main Results: EDAtt”。幻灯片上有一个图表，横轴标记为“AL/AL_CA (s)”，纵轴标记为“BLEU”。图表中有四条曲线，分别代表不同的策略：wait-k、LA、CAAT和EDAtt。在图表上方，有一段文字说明“popular strategies also applied to offline models”。右下角有一个页面编号“page 033”。</sample>
    <sample id="501">主要结果：EDAtt</sample>
    <sample id="502">画面中展示了一张幻灯片，标题为“Main Results: EDAtt”。幻灯片上有一个图表，显示了不同策略在BLEU分数上的表现。图表的横轴表示AL/AL_CA（从1.7到5.6），纵轴表示BLEU分数（从21到27）。图中有五条曲线，分别代表不同的策略：wait-k、LA、CAAT和EDAtt。其中，EDAtt的曲线在所有策略中表现最佳。幻灯片左下角有一段文字说明：“EDAtt outperforms all the strategies applied to offline models”，意思是EDAtt在离线模型中优于所有其他策略。右上角有一个小窗口，显示了一个正在讲话的人。幻灯片底部显示这是第34页。</sample>
    <sample id="503">这张图片展示了一张幻灯片，标题为“EDAtt”。幻灯片包含一个图表和一些文本。图表显示了不同策略在翻译任务中的表现，横轴表示AL/CA的时间（秒），纵轴表示BLEU分数。图例中列出了四种策略：wait-k、LA、CAAT和EDAtt。右上角有一个蓝色的注释框，写着“如果考虑实际耗时，EDAtt是最快的策略。”幻灯片底部有页码307。</sample>
    <sample id="504">画面中展示了一张幻灯片，背景为白色。幻灯片上有一个二维码和一些联系信息。顶部有一行文字“Do you want to discover more?”，下面是一行更大的文字“Read our paper to discover more results!”。在左侧，有三个社交媒体账号：@spapi,ne格ri@fbk.eu、marco.turchi@gmail.com、github.com/hlt-mt/fairseq。在右侧，有一个二维码，上面写着“Scan me!”。幻灯片的右下角标有“page 038”。</sample>
    <sample id="505">Yes, the dataset is publicly available.</sample>
    <sample id="506">The video features a presentation slide with the title "MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning" prominently displayed in white text against a black background. Below the title, three names are listed as contributors to this research project: Zhiyang Xu*, Ying Shen*, and Lifu Huang, all affiliated with the Department of Computer Science at Virginia Tech. The asterisks next to their names indicate that they have made an equal contribution. In the bottom left corner, there is additional text that reads "*Equal Contribution." Additionally, the top right corner displays a logo or emblem consisting of red letters 'VT' on a purple background. At the bottom of the slide, there are images of four individuals who appear to be associated with the presentation or study.</sample>
    <sample id="507">Figure 2: Comparing instruction tuning with pretrain-finetune and prompting.</sample>
    <sample id="508">这段文字讨论了预训练语言模型在下游任务中的应用。它提到，许多研究表明，指令调优使大型语言模型能够在没有先验知识的情况下通过遵循自然语言指示来执行未见过的任务。</sample>
    <sample id="509">这段文字讨论了语言-only任务，指出大多数先前的工作集中在提高零-shot性能上。</sample>
    <sample id="510">Instruction Tuning on Multimodal Pre-trained Models</sample>
    <sample id="511">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="512">Imbalance in Instructional Datasets between NLP and Multimodal 1600+ Language-only instruction tasks NO large-scale, publicly-available multimodal instruction tasks Wang, Yizhong, et al. "Benchmarking generalization via in-context instructions on 1600+ language tasks" arXiv preprint arXiv:2304.08975</sample>
    <sample id="513">MULTINSTRUCT The first multimodal instruction tuning benchmark dataset 62 diverse multimodal tasks 10 broad groups 5 expert-written instructions</sample>
    <sample id="514">这段文字介绍了MULTINSTRUCT，这是一个多模态指令调优基准数据集。它包含了62个不同的多模态任务、10个大类和5个由专家撰写的说明。图中展示了各种任务的分类，包括视觉关系、VQA、时间定向、接地、接地匹配、杂项、图像理解、区域理解、文本匹配等。每个任务都配备了5个专家撰写的说明。</sample>
    <sample id="515">OFA stands for One For All. It is a unified multi-modal pre-trained model that can perform both understanding and generation tasks with single or multiple modalities, including language, image tokens, and the coordinates of a bounding box. The model uses a unified vocabulary to handle these different types of inputs effectively.</sample>
    <sample id="516">图中展示了一个名为“MULTINSTRUCT”的数据集，它包含四个任务的实例。这些任务包括：1. 基于图像的描述（Grounded Captioning）：要求为图片生成一个描述。2. 图像位置化（Text Localization）：需要选择包含特定文本的区域。3. 指示表达（Refering Expression Selection）：从多个选项中选择描述正确对象的选项。4. 图像-问题匹配（Question-Image Matching）：根据给定的图像和问题，判断问题是否与图像相关。每个任务都有相应的输入和输出示例，展示了如何应用这些任务。</sample>
    <sample id="517">图中展示了一个名为MULTINSTRUCT的系统，它展示了如何处理各种输入和输出数据类型。系统包括四个任务：1. 基于图像的描述：给定一个图像，生成一个描述。2. 文本位置化：选择包含特定文本（如“den”）的区域。3. 指示表达：识别并描述特定对象（如火车）。4. 图像-问题匹配：根据图像内容回答问题。每个任务都有相应的输入和输出示例，展示了系统如何处理不同类型的数据。</sample>
    <sample id="518">Figure 1: Example Instances from MULTINSTRUCT for Four Tasks.</sample>
    <sample id="519">这段内容介绍了一个主题，叫做“Multi-modal Instruction Tuning”。</sample>
    <sample id="520">这段文字讨论了构建多模态指令转换数据集的过程。在训练数据集的构建中，使用了9个群体中的53项任务进行训练，并从每个任务中抽取10,000个实例。在测试数据集的构建中，整个常识推理组被保留用于测试，另外从VQA和杂项组中选择了5项任务。此外，所有任务的实例都用于每个任务的测试分割。最后，从测试分割中随机抽取了20项任务作为自然指令数据集的未标记任务，用于NLP。</sample>
    <sample id="521">图片中的文字内容包括以下几点： 1. 标题：Multi-Modal Instruction Turning 2. 培训数据集构建： - 使用9个组的53项任务进行训练。 - 每个任务采样10,000个实例。 3. 测试数据集构建： - 保留常识推理组用于测试。 - 从VQA和杂项类别中选择额外的5项任务。 - 对于每个任务，使用测试切片中的所有实例。 - 随机从Natural Instructions数据集中采样20项任务作为NLP的未见任务。</sample>
    <sample id="522">这段内容描述了模型的训练和测试细节。在训练过程中，使用了一个预训练的OFA大型模型（472M），将所有任务的实例混合在一起，并随机与其中一个指令模板结合。在测试中，对每个任务进行了五次实验，通过评估模型来判断每个指令模板的效果，并报告了五次实验中的平均值、最大值以及性能的标准差。</sample>
    <sample id="523">这段文字介绍了模型的训练和测试细节。在训练阶段，使用了一个预训练的OFA大型模型（472M），将所有实例混合在一起，并随机组合到其中一种指令模板中。在测试阶段，对于每个任务，会进行五次实验，通过使用每种指令中的一个来评估模型。然后报告五个实验中的平均值、最大值以及性能的标准偏差。</sample>
    <sample id="524">实施细节 Training details: 使用预训练的OFA大型模型（472M）混合所有实例用于所有任务。每个实例都随机与其中一个指令模板组合。 Testing details: 对于每个任务，我们进行五次实验来评估模型，使用每个任务中的一个指令。我们将报告五个实验中的平均值和最大性能，并报告五个实验中性能的标准偏差。</sample>
    <sample id="525">这段文字介绍了用于评估多模态任务的指标。对于多模态分类任务，包括视觉蕴含、视觉空间推理、自然语言视觉推理和灾难类型分类，报告准确率。对于多模态生成任务，如常识视觉问答、文本到视觉、基于视觉的问答、视觉文本提取和视觉对话，报告Rouge-L。对于NLP任务，报告Rouge-L。此外，还计算了每个模型在所有多模态和NLP未见任务上的综合性能。使用Rouge-L作为大多数任务的性能评分，而Accuracy仅作为度量标准。</sample>
    <sample id="526">这段内容讨论了模型对同一任务的多种指令的敏感性。它强调了模型在不同措辞下保持一致结果的能力。</sample>
    <sample id="527">这段文字讨论了在MULTIINSTRUCT中对指令进行微调的有效性。它提到了在多模态问题回答任务（Multimodal Compositional Question Answering）中的零样本性能，并指出最佳结果用粗体显示。表格展示了在各种任务上使用不同方法（如OFA、Transfer Learning from Natural Instructions和Zero-shot Performance）的性能指标，包括平均值、标准差和最大值。</sample>
    <sample id="528">Effectiveness of Instruction Tuning on MULTIINSTRUCT</sample>
    <sample id="529">Here we can see as the amount of task increases, the model achieves better performance and in the meantime a lower sensitivity.</sample>
    <sample id="530">这段文字讨论了OFA在不同数量的指令下进行微调的影响。它指出，当OFA在5个指令下进行微调时，其在所有评估任务中的表现显著提高，并且对变化的敏感性降低。表格显示，在1个指令下，聚合性能为42.81，而在5个指令下，聚合性能提高到47.82。此外，敏感性从24.62降低到10.45。</sample>
    <sample id="531">这段文字讨论了微调策略对模型敏感性的影响。它指出，在Multinstruct上进行的微调可以显著减少OFA模型的敏感性，并且从自然指令数据集中进行迁移学习可以进一步降低模型的敏感性。图表显示了不同微调策略下模型在未见过的评估任务中的表现，数值越低表示表现越好。</sample>
    <sample id="532">Zero-Shot Performance on NLP Tasks Instruction Tuning on Multinstruct can improve zero-shot performance on unseen NLP tasks. The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset.</sample>
    <sample id="533">这段文字讨论了关于多模态指令调优数据集的结论。它提到，这是一个大规模的多模态指令调优数据集，包含来自10个大类的62个模态任务。这个数据集通过指令调优显著提高了OFA的零样本能力，并探索了几种转移学习技术并展示了它们的好处。此外，还设计了一个新的度量标准叫作“sensitivity”。</sample>
    <sample id="534">画面中的文字内容包括：

标题：One More Thing!

正文：
We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon!

此外，还有一个二维码和一个穿着浅色上衣的人的部分图像。</sample>
    <sample id="535">The authors of the paper belong to the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="536">演讲者的名字是Mohammad Javad Hosseini。</sample>
    <sample id="562">这段英文内容翻译成中文是： 语言模型的可接受性判断并不总是对上下文有鲁棒性。</sample>
    <sample id="563">这是一张学术会议的幻灯片，主要讨论了语言模型在不同语境下的接受度评估。标题为“Language model acceptability judgements are not always robust to context”，表明研究发现语言模型的接受度评估在不同语境下并不总是具有鲁棒性。幻灯片上列出了参与这项研究的几位学者的名字：Kousuv Sinha、Jon Gauthier、Aaron Mueller、Kanishka Mishra、Keren Fuentes、Roger Levy和Adina Williams。此外，幻灯片上还展示了约翰霍普金斯大学、普渡大学和麻省理工学院的标志，表明这些机构可能参与了这项研究。</sample>
    <sample id="564">在这项工作中，我们重新审视了最小对偶范式。</sample>
    <sample id="565">幻灯片的标题是“重新审视最小对范式”。</sample>
    <sample id="566">Revisiting Minimal Pair Paradigm</sample>
    <sample id="567">Revisiting Minimal Pair Paradigm</sample>
    <sample id="568">Revisiting Minimal Pair Paradigm</sample>
    <sample id="569">Revisiting Minimal Pair Paradigm</sample>
    <sample id="570">Revisiting Minimal Pair Paradigm</sample>
    <sample id="571">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability</sample>
    <sample id="572">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability</sample>
    <sample id="573">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability</sample>
    <sample id="574">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability</sample>
    <sample id="575">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability</sample>
    <sample id="576">图片中的文字是“Approach”。</sample>
    <sample id="577">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability.</sample>
    <sample id="578">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability Wikipedia, Unrelated</sample>
    <sample id="579">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability Wikipedia, Unrelated</sample>
    <sample id="580">测试MPP判断是否取决于上下文长度、结构匹配和可接受性。</sample>
    <sample id="581">MPP判断在任意上下文长度下都是健壮的。</sample>
    <sample id="582">MPP judgement are robust for arbitrary context lengths.</sample>
    <sample id="583">现在，当我们在同一个数据集中选择句子时会发生什么？</sample>
    <sample id="584">标题：可接受/不可接受的MPP句子在上下文中影响判断性能 1. 段落内容：我们对不同上下文下的MPP评估进行了测试，包括可接受和不可接受的情况。这些句子的长度可以达到900个标记。 2. 图表分析：图表显示了不同上下文下的性能变化。具体来说，有两个主要的上下文（标记为“2”），它们的性能相对较高。其他上下文（标记为“1”）的性能较低。 3. 句子示例： - “There was a documentary about music in the past. There were no working hard, but there might be Rose from this before to you seeing the customer?” - “There was a documentary about music inflating. There might be Rose from this before to you seeing the customer?” - “There was each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating. There might be each music inflating.</sample>
    <sample id="585">图片中的英文内容翻译成中文是：可接受/不可接受的MPP句子在上下文中影响判断性能。我们对不同上下文下的MPP评估进行了评估，包括可接受/不可接受的、长度不超过900个标记的匹配结构。</sample>
    <sample id="586">Blimp Opt 6.7B</sample>
    <sample id="587">The image contains a graph with the title 'BLIMP, OPT 6.7B'. The x-axis is labeled 'Prefix Strategy' and has various strategies listed: 'Un.' (Unacceptable), 'Acc.' (Acceptable), 'Wiki', and 'Un.' again. The y-axis represents some form of performance metric, ranging from -0.2 to 0.2.

There are three lines on the graph:
1. A red line representing 'Un.' strategy.
2. A green line for 'Acc.' strategy.
3. A blue dashed line for 'Wiki'.

Above the graph, there is text that reads: 'Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance.'

Below this main statement, additional text explains: 'We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens.'

To the right of the graph, there are four numbered questions related to context:
1. "What could Jessica sell before Rachel noticed her spotlights?"
2. "What had Jessica said about the cleaning service before returning to this customer?"
3. "Jessica was cleaning the museum's spotlights when she realized what she was doing."
4. "What should Jessica sell these breakfasts before Rachel noticed them in front of this customer?"

At the bottom left corner of the image, it states: 'BLIMP, OPT 6.7B.'

Overall, the image appears to be presenting data and analysis regarding the impact of sentence structures on model performance in natural language processing tasks.</sample>
    <sample id="588">可接受/不可接受的MPP句子具有匹配结构，对模型性能影响最大。</sample>
    <sample id="589">为什么匹配前缀会影响LM判断？ 我们通过保留相关结构来扰动上下文句子，并询问模型是否对这些句子敏感。 - 前缀/后缀副词：“然而，&lt;sent&gt;。” - 长前缀副词：“首先和最重要的是，&lt;sent&gt;。” - 添加子句：“无论X认为什么，&lt;sent&gt;。” - 引用：“昨天，X说，&lt;sent&gt;。”</sample>
    <sample id="590">图中的文字内容包括： 1. 标题：为什么匹配前缀会影响LM判断？ 2. 副标题：我们通过保留相关结构来扰动上下文句子，并问模型是否对这些句子敏感。 3. 列表项： - 前缀/后缀副词：“然而，&lt;sent&gt;。” - 长前缀副词：“首先和主要，&lt;sent&gt;。” - 添加子句：“无论X认为什么，&lt;sent&gt;。” - 引用：“昨天，X说，&lt;sent&gt;。” 4. 右下角的图表标题：无 5. 图表上的标签： - 挺升 - 没有 - 前缀副词 - 主要副词 - 添加子句 - 所有 - 可接受 - 不可接受 6. x轴标签：输入长度 7. y轴标签：Δ准确性</sample>
    <sample id="591">我们发现，这些噪声中的任何一个都不会使模型的性能发生变化。</sample>
    <sample id="592">为什么匹配的前缀会影响LM判断？</sample>
    <sample id="593">为什么匹配前缀会影响LM判断？ 我们以保持上下文句子结构的方式扰动句子，并问模型是否对这些句子敏感。</sample>
    <sample id="594">关键要点是，语言模型对潜在的句法/语义特征敏感，这些特征在句子之间共享。使用短的、单个句子输入进行MPP评估无法充分捕捉LMs的抽象知识。</sample>
    <sample id="595">幻灯片上的英文内容翻译成中文如下： 1. 语言模型对潜在的句法/语义特征在句子之间共享非常敏感。 2. MPP评估使用短的单句输入无法完全捕捉LM的抽象知识。</sample>
    <sample id="596">关键要点是： 1. 语言模型对句子间共享的潜在句法/语义特征敏感。 2. MPP评估使用短、单句输入无法完全捕捉LMs的抽象示例。</sample>
    <sample id="597">该方法的第一步将输入词元映射到标记词元。</sample>
    <sample id="598">Coscript中包含55,000个脚本。</sample>
    <sample id="626">DEplain 的最佳对齐方法是 CATS-C3G。</sample>
    <sample id="627">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="628">DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如下： 1. 手动对齐：用于 DEPLAIN-APA test 部分，包括 n=48 和 n=1231 的数据集。 2. 自动对齐：用于 DEPLAIN-WEB test 部分，包括 n=147 和 n=1846 的数据集。</sample>
    <sample id="629">CoNLL++数据集是通过从2020年收集路透社新闻并使用CoNLL-2003注释指南对其进行注释来创建的。</sample>
    <sample id="630">The video features a presentation slide titled 'XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations.' The names listed on the slide are Yusen Zhang, Jun Wang, Zhiguo Wang, and Rui Zhang. Additionally, there is an image of Penn State University's logo along with Amazon's logo at the bottom left corner of the slide. In the top right corner, there is a small window showing a person speaking into a microphone.</sample>
    <sample id="631">语义解析是构建用户查询的语义表示的任务，例如SQL和Lambda演算。</sample>
    <sample id="632">Cross-lingual Semantic Parsing Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="633">Cross-lingual Semantic Parsing Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="634">现有的跨语言语义解析模型是在有限的任务和应用数据集上单独提出和评估的。例如，存在对某些自然语言的覆盖不足。</sample>
    <sample id="635">现有CLSP模型仅在有限的任务和应用数据集上提出并评估，例如：某些自然语言的含义表示缺乏覆盖。</sample>
    <sample id="636">现有的跨语言语义解析模型在有限的任务和应用数据集上单独提出和评估。例如：缺少某些意义表示的覆盖范围。</sample>
    <sample id="637">现有的跨语言语义解析模型在某些意义表示上存在覆盖不足的问题。</sample>
    <sample id="638">现有CLSP模型仅在有限的任务和应用数据集上进行提议和评估。例如：缺乏对某些神经模型的覆盖</sample>
    <sample id="639">XSemPLR 提供了一个统一的数据集示例，用于跨语言的语义解析。</sample>
    <sample id="640">XSemPLR是一个统一的语义解析数据集，旨在支持多种自然语言和意义表示。它包含9个跨不同领域的数据集、5种语义解析任务、8种意义表示以及15种语言家族中的22种自然语言。</sample>
    <sample id="641">实验设置 We consider the six settings for training and evaluation. Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval.</sample>
    <sample id="642">实验设置 我们考虑了六种训练和评估的设置。 Translate-Test：使用 Google 翻译 API 将源语言翻译成目标语言，然后使用单语模型进行训练和评估。 Training Inference German Translate API English English Model SQL</sample>
    <sample id="643">&lt;no_answer&gt;</sample>
    <sample id="644">我们考虑了六种训练和评估设置。单语模型：源语言与目标语言相同，例如德语到德语。我们还测试了单语少样本设置，在使用仅10%训练数据训练单语模型的情况下。</sample>
    <sample id="645">实验设置  我们考虑了六种训练和评估的设置。 单语模型：源语言与目标语言相同，例如德语到德语。我们通过使用仅10%的训练数据来训练单语模型来测试单语模型的设置。 训练 German (Few-shot) German Model SQL 推断 German German Model SQL</sample>
    <sample id="646">实验设置  我们考虑了六种训练和评估的设置。 单语模型：源语言与目标语言相同，例如德语到德语。我们还测试了单语少量数据设置，通过仅使用10%的训练数据来训练单语模型。 训练 推断</sample>
    <sample id="647">实验设置 We考虑了六个用于训练和评估的设置。 多语言模型：为所有语言训练一个单一的多语言模型。</sample>
    <sample id="648">幻灯片展示了实验设置，重点是训练和评估的六个设置。它强调了多语言模型的概念，即为所有语言训练一个单一的多语言模型。在训练阶段，使用了三种语言：德语、英语和中文。这些语言通过箭头指向一个标记为“SQL”的单一输出。在推理阶段，同样的过程被重复，但仅使用德语。这表明，尽管推理过程仅限于德语，但训练模型是在所有三种语言上进行的。这种设置可能旨在创建一个能够处理多种语言查询的通用模型，然后将这些查询转换为SQL格式。</sample>
    <sample id="649">实验设置 We 考虑了六个用于训练和评估的设置。 Multilingual 模型：为所有语言训练一个单一的多语言模型 Training German 英语 Chinese SQL Inference German 多语言模型 SQL</sample>
    <sample id="650">实验设置 We consider the six settings for training and evaluation. Cross-lingual Zero-shot/Few-shot transfer. Train on one source language and transfer to another language.</sample>
    <sample id="651">图片中的英文内容翻译成中文如下： 在训练和评估的六个设置中，我们考虑了六种设置。 多语种零-shot/少-shot迁移。在一种语言上进行训练，并转移到另一种语言。</sample>
    <sample id="652">这张图片包含了一张幻灯片，标题为“Analysis of Monolingual”。幻灯片的主要内容是对两种模型组在单语环境下的评估。幻灯片上列出了不同模型及其在各种任务上的表现分数。这些模型包括： - mBERT-PTR - XLM-R + PTR - mBERT + PTR - mBART - mBERT + PTR 幻灯片底部突出显示了mDec模型的分数，表明它在所有数据集上都表现最佳。</sample>
    <sample id="653">Analysis of Monolingual</sample>
    <sample id="654">我们评估了两种模型组在单语环境中的表现。</sample>
    <sample id="655">我们发现，编码器-解码器模型在所有9个数据集上都表现最佳。</sample>
    <sample id="656">我们评估了mT5和XLM-R + PTR在多语言环境中的表现。通过在多种语言的混合中进行训练，Enc- Dec/Enc-PTR（mT5-XLM-R）可以得到显著的提升。</sample>
    <sample id="657">分析多语种训练。我们在混合语境中评估mT5和XLM-R + PTR。Enc- Dec/Enc-PTR (mT5-XLM/R)可以通过在多种语言的混合中进行训练来提高。</sample>
    <sample id="658">Analysis of Multilingual Training</sample>
    <sample id="659">分析多语种训练</sample>
    <sample id="660">Cross-lingual Performance Gap</sample>
    <sample id="661">在这幅图中，蓝色线条代表跨语言少样本迁移。橙色线条代表跨语言零样本迁移。绿色线条表示单语环境。</sample>
    <sample id="662">Cross-lingual Performance Gap green - orange for zero-shot setting, the cross-lingual transfer performance gap is significant blue - orange for few-shot setting, the transfer gap is shortened rapidly</sample>
    <sample id="663">好的，这是幻灯片的内容： 1. 标题：其他结果和发现（论文第4节） 2. 内容： - Enc-Dec (mT5) 超过以往工作或实现了可比的结果。 - 在 NL 上进行预训练可以显著提升在目标 NL 上的少量数据表现。 - 多语言语言模型（LLMs）（由 CodeLLM 和 Bloom 提供）对于跨语言语义解析任务仍然不够。 - 中文转移学习和英德单语训练（En -&gt; En）具有最大的性能差距，而“German”通常是最小的。 - FunQL 在其他三种表示方法中表现最好，而 SQL 则表现最差。</sample>
    <sample id="664">把英文内容翻译成中文。</sample>
    <sample id="665">我们构建了一个统一的跨语言语义解析基准XSemPLR，用于处理多种自然语言和意义表示。</sample>
    <sample id="666">欢迎收听本次讲解，本次讲解由Kwabena Asare-Bediako提供。</sample>
    <sample id="667">关于这方面现有研究的分类如下： 1. 基于水印的标记（Watermark） 2. 词汇表（Lexical） 3. 后门（Backdoor） 4. 对抗性（Adversarial）</sample>
    <sample id="668">不，对于跨语言语义解析任务来说，多语言LLM（如Codex和Bloom）仍然不够。</sample>
    <sample id="695">该方法通过在训练过程中引入排列，来处理排列的不确定性。</sample>
    <sample id="696">下游NLP模型的公平性可以通过确保这些模型在处理文本时不会对特定群体或政治观点产生偏见来定义。</sample>
    <sample id="697">The speaker's name is Yanis Labrak.</sample>
    <sample id="698">演讲者的名字是Koustuv Sinha。</sample>
    <sample id="699">演讲者的名字是Myra Cheng。</sample>
    <sample id="700">热带主义 (tropicalism) 在本文的背景下意味着将某些群体，特别是拉丁裔女性、亚洲女性和黑人女性，通过特定的词语和描述进行刻板化。这些词语包括“充满活力”、“丰满”、“娇小”、“精致”和“坚韧”，这些词语被用来定义这些群体的身份，并且往往与热带主义有关，即对这些群体的刻板印象和简化的描绘。</sample>
    <sample id="701">作者通过使用诸如“文化”、“传统”、“自豪”和“异国情调”等词汇来描述标记群体，这些词汇定义了这些群体仅基于他们的身份。</sample>
    <sample id="702">本文中使用了P-CXMI来衡量语境使用情况。</sample>
    <sample id="703">DrBERT 是一个从头开始构建的模型，而 ChuBERT 是一个临床医学模型。</sample>
    <sample id="751">这篇论文有三位作者。</sample>
    <sample id="752">迭代迁移学习是通过在最新收集的数据集上训练模型来更新模型。</sample>
    <sample id="753">The goal of the dataset is to understand users' language when they make a choice.</sample>
    <sample id="754">攻击者通过EaaS来提取模型参数。</sample>
    <sample id="755">The paper has three authors: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="756">There are 10 annotators used to create the initial data set.</sample>
    <sample id="757">The authors belong to Carnegie Mellon University and the Allen Institute for AI.</sample>
    <sample id="758">The left conjunct is 'left' in the example.</sample>
    <sample id="759">GPT-4</sample>
    <sample id="760">我们需要在整个上下文窗口中评估模型的可接受性，因为这些天来大型语言模型正在以越来越长的上下文窗口出现。因此，对于评估模型的可接受性来说，整个上下文窗口至关重要。</sample>
    <sample id="761">Yes, with multilingual training, performance can drop in some languages.</sample>
    <sample id="762">No, the annotators do not know about these entities in advance.</sample>
    <sample id="763">BLEU和TER。</sample>
    <sample id="764">No, the video does not mention whether regression affects specific NER types.</sample>
    <sample id="765">NLP中的立场很重要，因为它有助于确保技术公平地对待所有用户。例如，PerspectiveAPI可能在检测某些语言或文化背景下的毒性时表现不佳，这可能导致系统性能差异和偏见。</sample>
    <sample id="766">像 BLOOM 这样的多语言 LLM 采用适配器微调。</sample>
    <sample id="767">他们使用了Roberta-base加上分类器头模型进行迁移学习。</sample>
    <sample id="768">The following are recent test sets used to evaluate PaLM capabilities:</sample>
    <sample id="769">The author proposed three recommendations.</sample>
    <sample id="770">提议的方法比最强的基线提高了15.45%。</sample>
    <sample id="771">演讲者的名字是Shuheng Liu和Alan Ritter。</sample>
    <sample id="772">论文中的结果和数据集可以用作基准吗？是的。</sample>
    <sample id="773">They conducted experiments on five smaller models.</sample>
    <sample id="774">OFA, a unified multimodal pre-trained model capable of performing both understanding and generation tasks with single or multiple modalities.</sample>
    <sample id="833">这篇论文的作者所属机构是Google。</sample>
    <sample id="834">Stony Brook University</sample>
    <sample id="835">论文分析了英语、西班牙语和德语。</sample>
    <sample id="836">Shangbin Feng</sample>
    <sample id="837">在实验过程中研究了两个模型：一个是长段落模型，另一个是基于正常段落的模型。</sample>
    <sample id="838">在 MultiInstruct 中使用的 62 个不同任务中，有 53 个任务用于训练目的。</sample>
    <sample id="839">There are three authors.</sample>
    <sample id="840">作者在实验中使用了以下数据集：AG News、MIND、SST2和Enron Spam。</sample>
    <sample id="876">NACHOS是一个医疗爬虫数据集。</sample>
    <sample id="877">David Vil Torres, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, George Foster</sample>
    <sample id="878">提示策略对结果有重大影响。</sample>
    <sample id="879">这篇论文的作者所属机构是卡内基梅隆大学语言技术研究所。</sample>
    <sample id="880">The five expert-written instructions are: 1. We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon 2. This is a QR code for our data and model repository.</sample>
    <sample id="881">作者建议使用来自多种来源的信息来测试模型。</sample>
    <sample id="882">Prompting PaLM for Translation: Assessing Strategies and Performance</sample>
    <sample id="883">幻灯片介绍了PaLM: Pathways Language Model。它提到了Chowdery等人在2022年发表的一篇论文，论文编号为arXiv:2204.02311。模型包含540亿参数，是在780亿个标记上进行训练的。该模型使用了6144个TPU v4芯片，并在数百个LMU和生成基准测试中达到了SOTA（最佳性能）。幻灯片还展示了模型在不同任务中的能力，包括问题回答、算术、代码完成、翻译、总结、语言理解等。</sample>
    <sample id="884">幻灯片展示了关于PaLM（Pathways Language Model）的详细信息。它提到了Chowdery等人在2022年发表的一篇论文，论文编号为arXiv:2204.02311。该模型包含540B参数，并在780B个标记上进行了训练。它被密集激活，并使用了6144个TPU v4芯片。该模型在许多自然语言处理（LMU）和生成基准测试中达到了最先进的性能（SOTA）。幻灯片还列出了PaLM能够执行的各种任务，包括问答、算术、代码补全、一般知识、总结、翻译、逻辑关系推断、常识推理、模式识别、对话、笑话解释、物理学问题、量子力学和语言理解。幻灯片底部显示了Google的标志。</sample>
    <sample id="885">贡献： 1. LLM提示用于机器翻译的第一个系统性研究，包括候选池和选择策略。 2. 评估MT社区的最佳实践的翻译能力： - 最新的测试集（避免测试集重叠和在评估数据上过度拟合） - 与最近提交的WMT论文进行比较（SOTA系统使用最新的训练数据） - SOTA MT指标（与人类判断的更好相关性） - 专家级的人类评估（比普通工人更 robust） 3. 提出提示选择策略的建议</sample>
    <sample id="886">幻灯片上有一段文字，标题是“我们的贡献”。这段文字包括以下几点： 1. “首次系统性的LLM提示用于MT。” - 这部分提到这项研究是首次对LLM（大型语言模型）进行系统性的提示研究，用于MT（机器翻译）。 2. “评估MT社区的最佳实践的翻译能力。” - 这部分提到评估翻译能力时使用了MT社区的最佳实践。 3. “推荐选择提示策略的方法。” - 这部分提到提供了一些建议来选择提示策略。 幻灯片右下角有一个小图片，显示一个人的脸部。左下角有Google的标志。</sample>
    <sample id="887">&lt;no_answer&gt;</sample>
    <sample id="888">幻灯片上的文字内容是： 1. Our contribution - First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy. - Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). - Recommendation for prompt selection strategies</sample>
    <sample id="889">Prompts have a big impact on translation quality. Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points</sample>
    <sample id="890">图片中的文字内容翻译成中文如下：</sample>
    <sample id="891">提示对翻译质量有很大影响。</sample>
    <sample id="892">画面中有一个穿着格子衬衫的人，他坐在一个有白色背景的桌子前。桌子上有一些文件和一个麦克风。背景是一个带有文字的幻灯片，标题是“Example prompting for translation”，下面写着“5-shot prompting”。幻灯片上列出了德语和英语的句子对，展示了翻译过程中的提示策略。</sample>
    <sample id="893">图片中的文字内容包括：

标题：Example prompting for translation

副标题：5-shot prompting

正文：
- German: Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird.
  英文翻译：He is being transported under the custody of two policemen on a bus from the jail.

- German: Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten.
  英文翻译：Police were called in after receiving complaints from the office.

- German: Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte.
  英文翻译：English:

此外，图片左下角有一个Google的标志。</sample>
    <sample id="894">这张图片展示了一个名为“Example prompting for translation”的演示幻灯片。幻灯片的背景是白色的，顶部有一个标题，下面有三个部分，每个部分都有德语和英语的翻译。第一个部分提到“Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird”，其对应的英文翻译是“English: He is being transported under the custody of two policemen on a bus from the jail”。第二个部分提到“Ski-Legenden unter sich: Die Polizei war eingeschlossen, nachdem sie Beschwerden des Buros erhalten hatten”，其对应的英文翻译是“English: Police were called in after receiving complaints from the office”。第三个部分提到“Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte”，其对应的英文翻译是“English: A passerby alerted the police, with several officers responding”。在幻灯片的右下角，有一个小圆形图像，显示一个人的部分脸部。左下角有一个Google Drive的标志。</sample>
    <sample id="895">例：5个提示翻译</sample>
    <sample id="896">图片中的英文文本内容翻译成中文如下：

标题：Example prompting for translation

副标题：5-shot prompting

正文：
德语：Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird.
英语：He is being transported under the custody of two policemen on a bus from the jail.

德语：Ski-Legenden unter sich: Die Polizei war eingeschlossen, nachdem sie Beschwerden des Buros erhalten hatten。
英语：Police were called in after receiving complaints from the office。

德语：Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte。
英语：</sample>
    <sample id="897">实验结果的总结是，示例质量比源句子的相似性更重要。</sample>
    <sample id="898">The slide shows the experimental results of a study on machine translation. It highlights that example quality is more important than similarity to the source sentence, and specialized SOTA systems have a significant advantage over PaLM, which performs close to Google Translate. The insights from MQM indicate that while PaLM's fluency can match SOTA, its accuracy scores are generally lower, dominated by "Accuracy/Omission," and it tends to be less stylistically appropriate or awkward for PaLM compared to other models.</sample>
    <sample id="899">实验结果表明，样例质量比与源句子的相似性更重要。专有SOTA系统具有明显的优势。PaLM与Google Translate相当。</sample>
    <sample id="900">图片中的文字内容如下： Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="901">图片中的文字内容包括：

标题：Experimental Results

1. Example quality is more important than similarity to source sentence.
2. Specialized SOTA systems have a substantial advantage.
3. PaLM close to Google Translate.

Insights from MQM:
- Fluency of PaLM comparable to SOTA.
- Accuracy scores generally lower (Dominated by "Accuracy/Omission").
- "Style/Awesome" generally lower for PaLM.

左下角有一个Google的标志。</sample>
    <sample id="902">实验结果表明，示例质量比与源句子的相似性更重要。专有SOTA系统具有明显的优势。PaLM的表现接近于Google Translate。</sample>
    <sample id="903">图片中的文字内容是关于实验结果的。主要内容包括： 1. 实验结果 2. 例子质量比相似度到源句子更重要 3. 专门化的SOTA系统具有明显的优势 4. PaLM与Google Translate接近 5. 从MQM中获得的见解： - PaLM的流畅性与SOTA相当 - 准确得分通常较低，主要由“准确性/遗漏”主导 - “风格/笨拙”通常对PaLM来说更低</sample>
    <sample id="904">图片中的文字内容包括： 1. 实验结果（Experimental Results） 2. 例如，质量比相似度到源句子更重要。 3. 专门的SOTA系统具有明显的优势。 4. PaLM接近Google Translate。 5. MQM洞察： - PaLM的流畅性与SOTA相当。 - 准确度分数通常较低。 - 主要由“准确性/遗漏”主导。 - “风格/笨拙”通常对PaLM来说更低。 图片左下角有一个Google标志。</sample>
    <sample id="905">图片中的文字内容翻译成中文如下：

实验结果：
- 例子的质量比与源句子的相似性更重要。
- 专门化的SOTA系统具有明显的优势。
- PaLM 接近 Google Translate。

MQM 的见解：
- PaLM 的流畅度与 SOTA 相当。
- 准确率通常较低。
  - 主要由“准确性/遗漏”主导。
- “风格/笨拙”通常对 PaLM 来说更低。</sample>
    <sample id="906">画面中有一个词云，展示了不同语言中的“谢谢”这个词。中央显眼地显示着“thank you”这个词，周围是用各种语言书写的“谢谢”，如“gracias”、“danke”、“grazie”等。背景是白色的，这些词以不同的大小和颜色排列，形成一个视觉上吸引人的图案。右下角有一个小的圆形图像，可能是一个人的肖像。</sample>
    <sample id="907">Weaker Than You Think A Critical Look at Weakly Supervised Learning Dawei Zhu1 Xiaoyun Shen2 Marius Mosbach1 Andreas Stephan3 Dietrich Klakow1 Saarland University Amazon Alexa University of Vienna ACL 2023</sample>
    <sample id="908">这是一项与小雨沈、马约斯·莫斯巴赫和迪特里希·克拉科夫共同完成的合作工作。</sample>
    <sample id="909">为什么是弱监督学习？ 弱监督缓解了标注瓶颈。 但弱标签是有噪音的！ 噪音记忆会损害泛化能力。 弱监督学习（WSL）： 训练模型，即使在嘈杂的数据上也能很好地泛化，尽管标注错误。</sample>
    <sample id="910">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="911">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="912">Weak supervision alleviates the annotation bottleneck. But weak labels are noisy Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data.</sample>
    <sample id="913">为什么弱监督学习？ 弱监督缓解了注释瓶颈。 但弱标签是有噪音的！ 噪音记忆会损害泛化。 弱监督学习（WSL） 训练模型能够在嘈杂的数据上很好地泛化，即使是在嘈杂的数据上进行训练。 弱标签数据（例如启发式、知识库）未标注数据 标记数据是嘈杂的，因为注释是错误的。</sample>
    <sample id="914">幻灯片讨论了在最近的弱监督学习（WSL）工作中常见的一个主张。它指出，在训练模型时，通常使用标记不完全的数据，并且能够达到较高的准确率。具体来说，幻灯片提到使用标记不完全的训练数据来训练模型，并且能够实现XX%的准确率。此外，幻灯片还展示了两种类型的数据：标记不完全的训练数据和标记完全的测试数据。</sample>
    <sample id="915">最近WSL工作中的一个常见主张是，我们仅在弱监督数据上训练模型，并达到XX%的准确率。</sample>
    <sample id="916">这段英文内容的中文翻译是：我们仅在弱监督数据上训练模型，并且准确率达到了XX%。</sample>
    <sample id="917">幻灯片包含标题“最近WSL工作的常见主张”，其中“WSL”可能代表监督学习或类似概念。它描述了训练模型仅使用弱监督数据，并达到XX%的准确性，旁边有一个愤怒的表情符号。幻灯片还展示了三种数据类型：带有红色标记的“带有噪声的弱标记训练数据”、“干净的弱标记验证数据”和“干净的弱标记测试数据”。在幻灯片的右下角，有一只大象的图片，可能象征着一个明显的事实或问题。</sample>
    <sample id="918">幻灯片显示了关于研究问题的文本内容。标题为“我们的研究问题”，下面列出了三个研究问题： 1. RQ1: 验证数据是否必要？ 2. RQ2: WSL 方法需要多少干净样本？ 3. RQ3: 如何更有效地利用可用的干净样本？</sample>
    <sample id="919">幻灯片包含一个标题和三个研究问题。标题是“我们的研究问题”。三个研究问题是：1. RQ1: 验证数据是否必要？2. RQ2: WSL方法需要多少干净样本？3. RQ3: 如何更有效地利用可用的干净样本？</sample>
    <sample id="920">RQ1 Main findings</sample>
    <sample id="921">图表显示了在不同验证方法下的相对性能提升。横轴代表不同的方法，包括FTw、BOND、COSINE、MLC和L2R。纵轴表示相对性能提升百分比。图中有三条线，分别代表在弱标签、随机选择和干净标签上的验证情况。</sample>
    <sample id="922">图表显示了不同验证方法对模型性能的影响。</sample>
    <sample id="923">图表显示了不同验证方法在不同模型上的相对性能提升。具体来说，它比较了三种验证方法：使用弱标签、随机选择和干净标签在FTw、BOND、COSINE、MLC和L2R模型上的表现。图表中的数据点代表了这些模型在不同验证方法下的相对性能提升百分比。</sample>
    <sample id="924">这张图片包含一个图表，标题为“Main findings”。图表显示了不同验证方法在各种指标（FTw、BOND、COSINE、MLC和L2R）上的相对性能。图例中包括三种验证方法：'Validation on Weak Labels'（用橙色表示）、'No Validation (Random Selection)'（用紫色表示）和'Validation on Clean Labels'（用绿色表示）。每个数据点都带有误差线，表明了性能的变异性。在图表下方，有一句话写着“A clean validation set is indispensable.”，强调了干净验证集的重要性。</sample>
    <sample id="925">图表显示了不同验证样本数量对模型性能的影响。</sample>
    <sample id="926">把英文内容翻译成中文。

图表展示了不同方法在验证集上的准确率变化。图例中列出了五种方法：FTw、COSINE、L2R、BOND和MLC，以及一个表示弱标签的虚线。横轴代表验证集大小，从5到50，纵轴代表准确率，范围从75%到85%。每种方法都用不同的颜色和线条样式表示其准确率随验证集大小的变化趋势。</sample>
    <sample id="927">主要发现 从WSL方法中受益更多干净的验证样本！</sample>
    <sample id="928">WSL approaches benefit from more clean validation samples</sample>
    <sample id="929">WSL approaches benefit from more clean validation samples</sample>
    <sample id="930">图表显示了在使用CFT（可能是一种预训练方法）前后，不同模型在两个不同样本大小下的准确率变化。左侧图表表示每类10个干净样本的情况，右侧图表表示每类30个干净样本的情况。每个图表中有四条线，分别代表COSINE、L2R和Clean Only三种方法的性能变化。从图表中可以看出，在使用CFT后，所有方法的准确率都有所提高，尤其是在每类30个样本的情况下，性能提升更为明显。</sample>
    <sample id="931">主要发现如图所示，模型FTW最初在性能上表现不佳。</sample>
    <sample id="932">把英文内容翻译成中文。

```
然而，如果我们允许继续在干净的样本上进行微调，
```

```
则FTW与其它方法表现得同样出色。
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```

```
```</sample>
    <sample id="933">Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches. No need to use complicated WSL methods, which require more computation time and disk space.</sample>
    <sample id="934">幻灯片总结了关于WSL方法的几点内容。首先，它指出WSL方法需要干净的样本，并且可能高估了它们的实用性。接着，它提出了以下建议：1. 报告模型选择标准。2. 使用少样本学习方法作为基准。3. 始终应用连续微调（CFT）。</sample>
    <sample id="935">结论 Recent WSL 方法 需要干净的样本。他们高估了其实际性。 我们的建议 报告模型选择标准 使用少量学习方法作为基准。始终应用连续微调（CFT）。</sample>
    <sample id="936">结论 Recent WSL 方法 需要干净的样本。 他们高估了其实用性。 我们的建议 报告模型选择标准。 使用少样本学习方法作为基准。 始终应用连续微调（CFT）。</sample>
    <sample id="937">这段文字讨论了最近的WSL方法，指出它们需要干净的数据样本，并且可能高估了它们的实用性。它还提出了建议，包括报告模型选择标准、使用少样本学习方法作为基准，并始终应用连续微调（CFT）。</sample>
    <sample id="938">幻灯片包含一个标题为“Conclusion”的部分，下面有两个主要部分：“Recent WSL approaches”和“Our recommendations”。在“Recent WSL approaches”下有两条要点：1. 要求干净的样本。2. 高估其实用性（旁边有一个表示不满的表情符号）。在“Our recommendations”下有三条要点：1. 报告模型选择标准。2. 使用少样本学习方法作为基准。3. 始终应用连续微调（CFT）。右上角有一个写着“THANK YOU!”的气泡，右下角有一个二维码。</sample>
    <sample id="939">The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="940">The paper has five authors.</sample>
    <sample id="941">To answer the question, you need to know that Servin is a judge and Kea is a baker.</sample>
    <sample id="942">Yes, the code is public. You can find it on GitHub at the URL: https://github.com/mpoems/kitmus</sample>
    <sample id="943">The video does not provide information about the distribution of NLPositionality's annotators across various demographic features such as countries/regions or genders.</sample>
    <sample id="944">The image contains a chart with various lines representing different perturbations, and the y-axis is labeled as 'Δ Accuracy'.</sample>
    <sample id="945">进行维度评估意味着对对话质量的各个方面进行评估。</sample>
    <sample id="946">The authors of this paper belong to the following institutions: 1. University of Science and Technology of China 2. Microsoft Research Asia 3. Beijing Jiaotong University 4. Sony AI 5. Microsoft STC Asia</sample>
    <sample id="947">提示的形式在单次和零次提示的情况下非常重要。</sample>
    <sample id="978">The models that the author assessed are 'BART-FID-RAG', 'Blender2', 'Emora', and 'Blender-Decode'.</sample>
    <sample id="979">There are 10 authors in this paper.</sample>
    <sample id="980">The ideal qualities of a good planner include the ability to understand and interpret different constraints, as well as effectively planning actions based on these constraints.</sample>
    <sample id="981">The paper has 7 authors.</sample>
    <sample id="982">The speaker's name is Vasudha Varadarajan.</sample>
    <sample id="983">The authors of this paper belong to the Institute of Computer Science, Polish Academy of Sciences and University of Warsaw.</sample>
    <sample id="1021">PaLM最常见的错误是遗漏错误。</sample>
    <sample id="1022">这段内容介绍了Emory大学的研究，讨论了如何评估聊天导向对话系统。</sample>
    <sample id="1023">这段内容的中文翻译如下： 《不要忘记你的ABC：在聊天导向的对话系统中评估最先进的技术》 Sarah E. Finch, James D. Finch 和 Jinho D. Choi 这项工作是由 Emory NLP 实验室完成的，该实验室由 Gino Choi 教授领导，位于 Emory 大学，并与 Amazon Alexa AI 合作。</sample>
    <sample id="1024">Emory大学的演讲幻灯片展示了“Comparative Evaluation”这一主题。左侧有一个黑发人物和两个蓝色对话框，右侧有另一个黑发人物和三个紫色对话框。</sample>
    <sample id="1025">将这段内容翻译成中文：常见的做法是使用人类评估，例如通过要求人类裁判员选择两个对话中哪个更好，或者给对话打分。</sample>
    <sample id="1026">Emory大学的标志和Alexa的标志在幻灯片的左下角。</sample>
    <sample id="1027">这是一张关于Likert评分评估的演示幻灯片。幻灯片顶部有一个蓝色横幅，上面写着“Likert Rating Evaluation”。在横幅下面，有一些插图和文字。左侧有一个穿着法官袍、手持法槌的人物插图。中间有三个对话气泡，其中两个是蓝色的，一个是白色的。右侧有两个带有麦克风的蓝色机器人插图。在这些插图下方，有一个从1到5的评分刻度，其中一个点被绿色勾选。在评分刻度上方，有一行文字写着“Rate the relevance of the bot's responses”。幻灯片底部有Emory University和Alexa的标志。</sample>
    <sample id="1028">这段内容介绍了使用李克特量表进行评价。具体来说，它要求对机器人回复的相关性进行评分，并提供了从1到5的评分范围。评分范围内的一个点被绿色勾选，表示这是一个参考点或示例。此外，还有两个图标：一个是带有法槌和卷轴的人物形象，另一个是Alexa的标志，这表明评估可能与法律对话或Alexa相关的对话有关。</sample>
    <sample id="1029">幻灯片上显示了Emory大学和Alexa的标志。标题为“Annotating Behaviors in Chat (ABC-Eval)”。在左侧，有四个对话气泡，每个气泡旁边都有一个头像图标。第一个对话气泡是灰色的，第二个是浅蓝色的，第三个也是浅蓝色的，第四个是深蓝色的。右侧有一个标签写着“Irrelevant”，指向第三个对话气泡。另一个标签写着“Lack of Empathy Self Contradiction”，指向第四个对话气泡。</sample>
    <sample id="1030">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="1031">ABC-Eval可以测量聊天模型在多大程度上会犯各种主题错误。</sample>
    <sample id="1032">这张图片展示了一个名为“ABC-Eval Behaviors”的幻灯片。幻灯片被分为四个部分，每个部分都有一个标题和一个空白的矩形框。左上角的部分标有“Coherence”，右上角的部分标有“Knowledge”，左下角的部分标有“Consistency”，右下角的部分标有“Emotional Understanding”。在“Coherence”部分下方有两个矩形框，分别标有“Ignoring Partner”和“Irrelevant”。幻灯片底部显示了“Emory University”的标志和“alexa”的标志。</sample>
    <sample id="1033">画面中的文字内容包括： 1. 标题：ABC-Eval Behaviors 2. 四个主要部分，每个部分包含不同的行为类别： - Coherence: Ignoring Partner, Irrelevant - Knowledge: Incorrect Fact, Commonsense Violation - Consistency: Self Contradiction, Partner Contradiction - Emotional Understanding: Empathetic Response, Lack of Empathy 3. 底部有两处标志和文字： - Emory University的标志和名称 - Alexa的标志</sample>
    <sample id="1034">幻灯片介绍了关于对话模型的实验。它提到了使用四种开放式领域对话模型，并对每个模型进行了100次人-机器人对话。</sample>
    <sample id="1035">幻灯片上有一个标题“Experiments”，下面有两个项目符号。第一个项目符号是“4 Open-Domain Dialogue Models”，第二个项目符号是“100 Human-Bot Conversations per Model”。在这些项目符号的下方，有一个标题为“ABC-Eval”的部分，包含一个插图和一些文本。插图显示了一个人和一个机器人之间的对话流程图。在“ABC-Eval”部分旁边，有另一个标题为“Turn Likert”的部分，显示了一个评分表，从1到5，其中有一个勾选标记在3的位置。还有一个标题为“Dialogue Likert”的部分，也显示了一个评分表，从1到5，其中有一个勾选标记在3的位置。在这些部分的右侧，有一个标题为“Comparative”的部分，显示了多个对话流程图和一个勾选标记。幻灯片底部有Emory大学和Alexa的标志。</sample>
    <sample id="1036">这段内容介绍了对四种开放域对话模型的实验，每个模型都有100个人与机器人进行对话。实验包括三种评估方法：Turn Likert、Dialogue Likert和Comparative。这些方法用于评估一致性、情感理解、信息丰富性、整体质量、吸引力、语法准确性、主动性以及相关性。</sample>
    <sample id="1037">这张图片展示了一个名为“Inter-Annotator Agreement”的图表。图表的标题是“Inter-Annotator Agreement”，并且有一个黄色箭头指向图表的顶部，指示了Krippendorf's Alpha值。图表下方有多个标签，包括“ABC-Eval”、“Turn Likert”、“Dialogue Likert”和“Comparative”。每个标签下面都有多个数据点，这些数据点代表了不同评分者之间的同意程度。图表的背景是白色的，顶部有一个蓝色的横幅，上面写着“Inter-Annotator Agreement”。在右上角，有一个小窗口显示了一位演讲者的图像。在左下角，有Emory大学和Alexa的标志。</sample>
    <sample id="1038">图表显示了预测有效性，比较了交互式评价和交互式问答的百分比。</sample>
    <sample id="1039">这张幻灯片展示了预测有效性，通过比较互动性评分和互动性评分在不同类别（ABC-Eval、Turn Likert、Dialogue Likert和Comparative）中的百分比。图表中使用了条形图来表示这些数据，其中灰色条代表互动性评分，蓝色条代表互动性评分。黄色箭头指向特定的类别，表明这些类别在预测有效性方面具有重要意义。幻灯片顶部有一个标题“Predictive Validity”，底部有Emory大学和Alexa的标志。</sample>
    <sample id="1040">标题是“增量有效性”。</sample>
    <sample id="1041">图中展示了一个名为“增量有效性”的图表，图表的标题是“Incremental Validity”。图表上有三个轴：ABC-eval、Turn UBERT和Dialogue UBERT。每个轴上标有不同指标的名称，如Jeff Conn、Unempathetic、Relevant、Engaging等。图表显示了这些指标在解释质量方面的贡献百分比（%）。图表右上角有一个Emory University和Alexa的标志。</sample>
    <sample id="1042">这张幻灯片的标题是“增量有效性”。它展示了一个散点图，横轴标记为“ABC-Eval”和“Turn UBERT”，纵轴标记为“解释质量百分比（%）”。图表中包含多个数据点，每个数据点旁边都有标签。这些标签包括“Empathetic”、“Relevant”、“Engaging”、“Proactive”、“Emotion”和“Unempathetic”。图表还包括一条从左下角到右上角的曲线，标记为“冗余”。在右上角有一个箭头指向一个特定的数据点，该点位于“Relevant”和“Empathetic”之间。幻灯片底部显示了“埃默里大学”的标志和“alexa”的标志。</sample>
    <sample id="1043">这些可靠的、信息丰富的和独特的ABC-EVAL指标使我们能够以比以前方法更高的分辨率来评估对话式人工智能。</sample>
    <sample id="1044">图表显示了不同模型在各种挑战下的错误率，包括反社会、常识违反、忽略、不正确、缺乏同情心等。每个模型的表现都用不同的颜色表示，便于比较。</sample>
    <sample id="1045">图表展示了不同模型在ABC评估中的错误率。图表的标题是“ABC-Eval Error Rates by Model”。图表的纵轴表示百分比，横轴列出了不同的错误类型，包括反社会、CS反向、忽略、错误、不相关、其他反向、冗余、自我反向、话题转换和未解释。图表下方列出了五个不同的模型：BART-FID-RAG、Blender2、Emora和Blender-Decode。图表中使用了黄色箭头来突出显示某些错误类型。</sample>
    <sample id="1046">图表显示了不同模型在ABC-Eval中的错误率，以百分比表示。每个柱状图代表一个特定的错误类型，如反社会、CS相反、忽略、错误、不相关、没有同情心、其他、冗余、自我、主题转换和未解释。柱状图的颜色对应不同的模型：BART-FID-RAG（绿色）、Blender2（蓝色）、Emora（红色）和Blender-Decode（紫色）。图表的标题是“ABC-Eval错误率按模型”，并包括Emory大学和Alexa的标志。</sample>
    <sample id="1047">幻灯片显示了一个图表，标题为“ABC-Eval错误率按模型”，展示了不同模型在各种类别中的错误率。这些类别包括反社会、CS反差、忽略、错误、不相关、不共情、其他反差、冗余、自我反差和话题切换。每个类别都有多个条形图，代表不同的模型：BART-FID-RAG、Blender2、Emora和Blender-Decode。图表的Y轴表示百分比的轮次，范围从0到30。图表下方有Emory大学和Alexa的标志。幻灯片顶部有一个感谢观看的横幅，底部提供了论文链接、GitHub存储库链接和联系信息。</sample>
    <sample id="1048">这篇论文的作者所属机构是埃默里大学和埃默里NLP研究实验室。</sample>
    <sample id="1049">CFT stands for continuous fine-tuning.</sample>
    <sample id="1050">这篇论文有8位作者。</sample>
    <sample id="1051">这是一张幻灯片，标题为“翻译何时需要上下文？基于数据的多语言探索”。作者包括Patrick Fernandes、Kayo Yin、Emmy Liu、André F. T. Martins和Graham Neubig。幻灯片上还显示了Carnegie Mellon University Language Technologies Institute、Técnico Lisboa、BAIR（伯克利人工智能研究）和Unbabel的标志。</sample>
    <sample id="1052">翻译取决于上下文。我们得 rid of that mole.</sample>
    <sample id="1053">画面中的文字内容包括： 1. "Translation depends on context" 2. "Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole." 3. "Could it be anything serious, Doctor? We'll have to get rid of that mole."</sample>
    <sample id="1054">翻译取决于上下文。</sample>
    <sample id="1055">画面中的文字内容包括： 1. 标题：Evaluating context-dependent translation is hard 2. 副标题或要点：Only a small portion of words depend on context 3. 子要点：- Corpus-level metrics - blue 4. 其他文本：- unabled to capture these translations</sample>
    <sample id="1056">Evaluating context-dependent translation is hard. Only a small portion of words depend on context. Corpus-level metrics Existing methods support limited discourse phenomena and languages.</sample>
    <sample id="1057">这两段文字提出了两个研究问题。第一个问题是“翻译何时需要上下文？”第二个问题是“模型如何处理上下文相关的翻译？”</sample>
    <sample id="1058">为了回答第一个问题，我们首先通过测量翻译中单词依赖于上下文的程度来开始。</sample>
    <sample id="1059">幻灯片介绍了条件互信息（CXMI），这是一种用于衡量机器翻译模型在给定语料库中使用上下文的程度的指标。CXMI通过测量上下文C提供的关于目标Y的信息，给定源X来评估上下文信息。</sample>
    <sample id="1060">Conditional Cross-Mutual Information (CXMI) measures how much context machine translation models use given a corpus.</sample>
    <sample id="1061">幻灯片介绍了P-CXMI的概念，这是一种用于衡量特定上下文使用情况的指标。它包括一个公式，用于计算句子和单词级别的P-CXMI值。此外，它还讨论了高P-CXMI单词需要上下文进行翻译的情况。</sample>
    <sample id="1062">RQ1: 翻译何时需要上下文？ - 词汇级别的上下文使用 - 主题分析 RQ2: 模型如何处理上下文相关的翻译？</sample>
    <sample id="1063">画面中展示了一个TED标志，旁边写着“IDEAS WORTH SPREADING”。在顶部有一行文字，写着“高P-CXML单词的主题分析”。右侧有一个圆形区域，里面显示了一位女士的图像。画面右侧还列出了多种语言，包括英语、阿拉伯语、德语、西班牙语、法语、意大利语、日语、韩语、荷兰语、葡萄牙语、罗马尼亚语、俄语、土耳其语和中文。</sample>
    <sample id="1064">幻灯片显示了标题“高P-CXMI单词的主题分析”。在标题下方，有一个编号为1的项目，写着“POS标记”。右上角有一个小圆形图像。</sample>
    <sample id="1065">幻灯片的标题是“高P-CXMI词的题意分析”。在标题下方，有一个副标题“1. POS标记”，表明这是关于部分标记（POS）标记的讨论。在左侧，有一张图表，标题为“阿拉伯语中的P-CXMI词的POS标记”。图表显示了三个类别：PRON 3.Sing、PRON 3.Dual和PRON 3.Plur。每个类别的值都相同，表示这些类别的P-CXMI值相等。在右侧，有一个矩形框，上面写着“代词”，表明幻灯片的重点是代词。幻灯片的整体布局清晰，左侧有图表，右侧有文本框，背景颜色为浅紫色。</sample>
    <sample id="1066">图片显示了一张幻灯片，标题为“高P-CXML单词的主题分析”。幻灯片分为两个主要部分。左侧有一个图表，顶部标有“P-CXML中POS标记的代词”，下方列出了三个项目：“PRON_3_Sing”、“PRON_3_Dual”和“PRON_3_Plur”。每个项目的值都约为0.6。右侧有一个浅紫色的矩形框，里面有两个项目：“代词”和“动词形式”。幻灯片的背景是白色的，右上角有一张人的照片。</sample>
    <sample id="1067">Thematic analysis of high P-CXML words 1. POS tags 2. Vocabulary items Avelile's mother was still asleep. Avelile went to school. 阿维利尔的母亲还在睡觉。 阿维利尔去上学了。 - Pronouns - Verb form - Lexical cohesion</sample>
    <sample id="1068">主题分析高P-CXMI单词 1. POS标记 2. 词汇项 - 主格 - 动词形式 - 语法连贯性 - 正式性 Avelile的母亲仍在睡觉。阿维利利的母亲还在睡觉。 Avelile去上学了。</sample>
    <sample id="1069">幻灯片显示了对高P-CXML单词的主题分析。主要内容包括三个主要部分：1. 词性标记（POS tags）2. 词汇项3. 个体令牌。此外，还有一个紫色框，列出了以下项目：- 代词- 动词形式- 词汇连贯性- 正式性- 省略。幻灯片上还展示了两个句子的英文和德文翻译，分别是“她知道我们在哪里。”和“她不介意。”以及“她不知道我们在哪里。”和“她不介意。”</sample>
    <sample id="1070">研究问题1：翻译何时需要上下文？ - 单词级别的上下文使用 - 主题分析 研究问题2：模型在处理上下文相关的翻译时表现如何？ - 多语言语境感知（MuDA）基准</sample>
    <sample id="1071">幻灯片的标题是“Multilingual Discourse-Aware (MuDA) tagger”。左侧有一个紫色的框，里面列出了五个项目：- Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis 右上角有一张人物照片。</sample>
    <sample id="1072">多语种意识的语篇感知（MuDA）标记器。</sample>
    <sample id="1073">幻灯片上显示了“MuDA benchmark”字样，左侧有三份文件的图标，右侧有一个机器人图标。在中间，有一段文字和一个流程图，描述了使用MuDA标记器的过程。</sample>
    <sample id="1074">RQ1: 当翻译需要上下文时？ - 单词级别的上下文使用 - 主题分析 RQ2: 模型在处理上下文相关的翻译方面表现如何？ - 多语种话语意识（MuDA）基准 - 模型评估</sample>
    <sample id="1075">首先，当我们使用语料库级别的指标时，对于BLEU来说，我们发现自适应模型具有最佳性能。</sample>
    <sample id="1076">幻灯片显示了三个机器人，每个机器人都代表不同的语料库级别指标。第一个机器人标有“BLEU”，第二个机器人标有“COMET”，第三个机器人标有“F-measure”。每个机器人的头部都标有“上下文”字样，表明这些指标与上下文有关。右上角有一个小圆形图像，可能表示演讲者或演示者的头像。标题为“语料库级别的指标”，表明这些机器人代表用于评估自然语言处理模型性能的不同指标。</sample>
    <sample id="1077">幻灯片上有一个标题，写着“Corpus-level metrics”。标题下方有三个机器人图标。每个机器人都代表不同的指标：左边的机器人标有“BLEU”，中间的机器人标有“COMET”，右边的机器人标有“F-measure”。在这些图标上方，有一个带有“CONTEXT”字样的椭圆形。在图片的右上角，有一个小的圆形图像，显示了一个人的脸。在幻灯片底部，有一段文字写着：“Unclear which system is best for document-level MT with corpus-level metrics alone.”</sample>
    <sample id="1078">幻灯片的标题是“MuDA基准结果”。它包含一个项目符号，写着“上下文感知模型在某些现象上表现显著更好。”在下面，有一个子项目符号，写着“正式性和词汇连贯性”。</sample>
    <sample id="1079">幻灯片显示了“MuDA基准结果”的标题。主要内容是关于上下文感知模型在某些现象上的表现显著优于其他模型。具体来说，它指出上下文感知模型在正式性和词汇连贯性方面表现良好，但在省略号、代词和动词形式方面表现不佳。幻灯片上有一个小圆圈，里面有一个人的图像，但没有提供任何额外的信息或上下文。</sample>
    <sample id="1080">MuDA benchmark results Context-aware models perform significantly better on some phenomena o ✅: Formality, lexical cohesion ❌: Ellipsis, pronouns, verb form DeepL outperforms Google on most phenomena and language pairs* as of April 2021</sample>
    <sample id="1081">MuDA benchmark results</sample>
    <sample id="1082">这张图片展示了一张幻灯片，顶部有一个标题“Summary”。幻灯片上有两个主要点： 1. “系统地在无需先验语言知识的情况下识别话语现象” 2. “面向文档级机器翻译的数据集无关基准” 在这些要点下方，有一系列图标和文字。从左到右，这些元素包括： - 一堆文件的图标 - 标有“MuDA tagger”的文档堆 - 标有“BLUE F-measure”的文档堆 - 一个机器人图标 幻灯片的右上角显示了一个人的部分图像。</sample>
    <sample id="1083">好的，我明白了。请稍等片刻，我会将内容转换成中文并发送给您。</sample>
    <sample id="1084">演讲者的名字是Yusen Zhang。</sample>
    <sample id="1121">新方法没有名称。</sample>
    <sample id="1122">作者描述“显性词汇”方法为：找到区分标记组与未标记组的单词。</sample>
    <sample id="1123">The authors of this paper are affiliated with the University of Washington and Carnegie Mellon University's Language Technologies Institute.</sample>
    <sample id="1124">The first mentioned symmetric approach to coordinate structures is the "Conjunction-headed" structure, which originated in Prague.</sample>
    <sample id="1125">The speaker's name is Sarah E. Finch, as indicated in the slide content where it lists "Sarah E. Finch" along with James D. Finch and Jinho D. Choi.</sample>
    <sample id="1126">这篇论文有五位作者。</sample>
    <sample id="1127">BLiMP, SyntaxGym, and Crows datasets can be used to test sentence phenomena.</sample>
    <sample id="1161">The five methods are abbreviated as FTw, COSINE, L2R, MLC, and BOND.</sample>
    <sample id="1162">该模型在11个任务上进行了评估。</sample>
    <sample id="1226">CamemBERT最初是在4GB的数据上训练的。</sample>
    <sample id="1227">演讲者的名字是Adam Przepiórkowski和 Michał Woźniak。</sample>
    <sample id="1228">发现导致性能下降的主要原因是时间漂移。</sample>
    <sample id="1269">排列输出序列中的词元是为了确保它们按照正确的顺序排列。</sample>
    <sample id="1270">作者建议模型所有者应提高偏见缓解方法的透明度，因为这有助于确保这些方法的有效性和公正性。</sample>
    <sample id="1271">最小对不可接受输入是：'Many people were helping themselves.'</sample>
    <sample id="1272">作者使用了以下评估指标：NLR、CER、NR、CE、NR、CER、NR、CER、NR、CER。</sample>
    <sample id="1273">使用了Krippendorf's alpha指标来衡量注释者之间的一致性。</sample>
    <sample id="1274">在不可接受和可接受查询中，选择Wikipedia作为领域来添加完全无关的句子。</sample>
    <sample id="1275">The authors of this paper are affiliated with Heinrich Heine University Düsseldorf in Germany.</sample>
    <sample id="1276">MultiInstruct focuses on instruction tuning for multimodal pre-trained models, which is different from previous works that mainly focused on language-only tasks.</sample>
    <sample id="1277">这篇论文有三位作者。</sample>
    <sample id="1278">二进制协调是指在语言处理中，将文本分为字符、音节和单词等不同的单位进行分析。</sample>
    <sample id="1279">在本研究中，提示语的平均长度是10个单词。</sample>
    <sample id="1280">The findings suggest that smaller T5 models, when fine-tuned on Coscript, can generate higher quality scripts compared to larger language models. This implies that with suitable training data and methods, smaller models may be just as effective or even more so than their larger counterparts in specific tasks like script generation.</sample>
    <sample id="1281">DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains</sample>
    <sample id="1282">这段文字介绍了关于医疗保健中语言建模的总结。它包括了预训练策略、数据源和规模的比较，对13个模型在11项任务上的评估，以及NACHOS和DrBERT的分布情况。</sample>
    <sample id="1283">幻灯片包含一个标题为“摘要”的部分，列出了以下要点： 1. 医疗保健中的语言建模。 2. 预训练策略、数据源和规模的比较。 3. 对11个任务的13个模型的评估。 4. NACHOS和DrBERT的分布。 幻灯片右上角显示了一个人在说话。 幻灯片底部有一个红色横幅，上面写着“Aix-Marseille Université”。</sample>
    <sample id="1284">幻灯片上有一个标题“Summary”，下面列出了四个主要部分： 1. Healthcare中的语言建模 2. 预训练策略、数据源和规模的比较 3. 在11个任务中评估13个模型 4. NACHOS和DrBERT的分布 Avignon Université的标志位于右下角。</sample>
    <sample id="1285">幻灯片的标题是“Summary”。它列出了四个主要部分： 1. 医疗保健中的语言建模。 2. 预训练策略、数据源和规模的比较。 3. 在11个任务上评估13个模型。 4. 分发NACHOS和DrBERT。</sample>
    <sample id="1286">幻灯片的标题是“Language Modeling”。</sample>
    <sample id="1287">这段文字讨论了语言建模，特别是如何使用Transformer架构的模型，如BERT，在自然语言处理（NLP）任务中表现出色。它指出这些模型已经被改编成法语，如CamemBERT和FlauBERT。在医疗领域，特定于域的模型在英语中的表现更好，包括PubMedBERT、BioBERT、ClinicalBERT和其他模型。文章还提到，除了英语之外的语言较少见，并且主要依赖于预先训练，使用现有的通用模型。对于生物医学领域，目前没有开源模型可用，但有希望。最后，文章强调了在医疗任务上使用特定于域的模型对法语的潜在好处。</sample>
    <sample id="1288">语言建模 Transformer 基础方法，例如 BERT，在许多 NLP 任务上表现出巨大的性能提升。它已被适应为法语的 CamemBERT 和 Flaubert。在医疗领域，基于特定领域的模型在英语中设定了更高的标准。PudMedBERT、BioBERT、ClinicalBERT 等。其他语言的模型较少见，并且主要依赖于在现有通用模型基础上进行持续预训练。由于缺乏同域数据，生物医学领域的开源模型目前在法语中不可用。Bert-based 的特定领域模型对法语在医疗任务上的表现有潜在的提升。</sample>
    <sample id="1289">语言建模 1. BERT等基于Transformer的方法在许多NLP任务上表现出巨大的性能提升。 2. 已经将BERT适应为法语，使用了CamemBERT和FlauBERT。 3. 在医学领域，特定于域的模型在英语中表现得更好，如PudMedBERT、BioBERT、ClinicalBERT和其他。 4. 其他语言，尤其是英语，更罕见，并主要依赖于预先训练，使用现有的通用模型。 5. 目前，没有开源模型可用于法语的生物医学领域。 6. BERT特定于域的模型对于法语在医学任务上的性能提升具有潜力。</sample>
    <sample id="1290">比较预训练策略和数据源</sample>
    <sample id="1291">为了回答这个问题，我们比较了通过从头开始构建模型的DoctBERT与基于匿名化数据的Subert模型。</sample>
    <sample id="1292">这段文字讨论了在比较预训练策略和数据源时，公共和私人医疗数据源对可比数据集大小的影响。它提到了两个数据集：NACHOS和NBDW。NACHOS是一个1.18GB的开源数据集，包含从多个医学领域抓取的多样化数据，包括4GB的MEDIEVAL、256MB的MEDIEVAL（small）、4GB的MEDIEVAL（small）和4GB的PubMed。NBDW是一个来自Nantes医院大学数据库仓库的1.7M份句子的私有数据集，大小为256MB。此外，还比较了不同的预训练学习策略，包括从头开始构建模型和使用现有预训练模型（如CamemBert、French generic和PubMedBERT）进行持续预训练。</sample>
    <sample id="1293">比较预训练策略和数据源</sample>
    <sample id="1294">比较预训练策略和数据源</sample>
    <sample id="1295">除了这个比较，我们还介绍了三种在持续预训练上训练的模型。</sample>
    <sample id="1296">比较预训练策略和数据源的评估公共和私人医疗数据源对可比数据大小的影响 NACHOS：一个从多个医学领域、样式和类型中爬取的1.1GB开源数据集 NBDW：来自纳特兹医院大学数据仓库的1.7M份医疗记录句子的私有数据集 比较学习策略 从头开始构建全模型 用现有的预训练模型进行持续预训练（例如，Camembert，一个法语通用模型，和PubMedBERT，一个英语医学通用模型）</sample>
    <sample id="1297">幻灯片展示了“预训练策略和数据源的比较”的内容。它包括两部分：1. 评估公共和私人医疗数据源在可比数据集大小上的影响。- NACHOS：一个包含1.18B单词的开源数据集，从多个医学领域抓取了异构数据，包括各种风格。- NBDW：一个包含170万份来自南特大学医院数据仓库的医疗记录的私有数据集。2. 训练学习策略：- 从头开始构建全模型。- 使用现有预训练模型进行持续预训练（例如，CamemBERT、French GPT、PubMedBERT等）。表格列出了不同模型的名称、策略、Corpus和资源使用情况。</sample>
    <sample id="1298">评估所有7个模型，我们收集了公共和私人数据集，并在包括命名实体识别、图像分类、姿势估计和问答等任务中进行了测试。</sample>
    <sample id="1299">这张图片展示了一个演示幻灯片，标题为“Evaluation: Data sources and size”。幻灯片上有一个表格，列出了13个模型在9个任务上的性能评估结果。这些任务包括aIF、Medical Report、MUSICA、Diet、MUSC、Diet、CAS和NOMOCOM。表格中的数据包括NER（命名实体识别）、CLS（分类）、NERS（命名实体识别准确率）、CLSR（分类准确率）、POS（词性标注）和NERM（命名实体识别平均值）。每个模型的性能指标都在相应的列中列出。幻灯片右下角显示了“Aigron Université”的标志。</sample>
    <sample id="1300">评估数据源和规模。 13个模型在10个任务上进行了性能评估，包括公共和私人数据。 我们训练的模型在几乎所有任务上都达到了最先进的水平。</sample>
    <sample id="1301">表格中展示了13个模型在9项任务中的性能评估，包括公共和私人数据。这些模型在几乎所有任务上都达到了最先进的结果。</sample>
    <sample id="1302">总体而言，从头开始预训练似乎在大多数任务上获得了更高的性能。</sample>
    <sample id="1303">这段文字讨论了从头开始训练与在4GB数据上持续预训练之间的比较。它指出，持续预训练需要更多的领域特定知识才能有效工作，并且一个基于模型稳定性的研究显示，CAMBert-based模型在持续预训练时具有更高的变异性。</sample>
    <sample id="1304">这段文字主要讨论了预训练策略的评估，包括从头开始和持续预训练两种方法。它指出，需要特定领域的知识来回答问题，并且模型稳定性显示了基于Camembert模型的训练在持续预训练时具有更高的变异性。</sample>
    <sample id="1305">这段文字讨论了DrBERT在下游法语医疗任务中的表现，强调了数据源的重要性，并指出持续预训练是基于域特定英语模型的有效策略。</sample>
    <sample id="1306">这段文字讨论了DrBERT在法语医疗任务中的表现，指出它在下游任务中达到了最先进的结果。它还提到了数据源的重要性，并比较了NACHOS和仅使用私人临床数据的差异。此外，它强调了预训练的有效性以及基于域特定英语模型的策略。最后，它提到DrBERT模型、NACHOS数据集和脚本是免费提供的。</sample>
    <sample id="1307">这段内容主要介绍了DrBERT在法语医疗任务中的表现。它指出，DrBERT在下游9个法语医疗任务中取得了最先进的结果，并且超过了CamemBERT的通用模型和基于英语的特定领域模型。此外，它还确认了训练一个针对法语医疗任务的特定领域模型的有效性。内容还强调了数据源的重要性，指出NACHOS比仅使用私人临床数据更强大。另外，它提到更多的数据通常更好，但可能不具有可扩展性。最后，它建议在基于英语的特定领域模型上进行预训练是一种更有效的策略，并且DrBERT模型、NACHOS数据集和脚本在MIT许可下免费提供。</sample>
    <sample id="1308">感谢您的观看。期待在多伦多的海报展示中交流。</sample>
    <sample id="1309">论文研究了三种学习策略。</sample>
    <sample id="1310">由于测试重复使用而导致的过拟合因素是存在的。</sample>
    <sample id="1311">The text on the screen is about "Automatic Text Simplification."</sample>
    <sample id="1312">是的，语言模型确实有不同的政治偏见。</sample>
    <sample id="1313">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations</sample>
    <sample id="1314">This is joint work with my advisors, Alexander Koller and Ivan Titov.</sample>
    <sample id="1315">Compositional Generalization Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="1316">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1317">这些断言与表示其意义核心方面的逻辑形式配对。</sample>
    <sample id="1318">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1319">在本例中，模型在训练期间看到了浅层递归，并在测试时对一个具有深层递归的示例进行了测试。</sample>
    <sample id="1320">naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input</sample>
    <sample id="1321">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1322">The popular method to address this is to integrate trees into the models.</sample>
    <sample id="1323">Trees help a lot but...</sample>
    <sample id="1324">Trees help a lot but... Trees need to be obtained: - Pre/Post-processing logical forms</sample>
    <sample id="1325">图片中的英文内容翻译成中文如下： 树帮助了很多，但... 需要获得的树包括： - 逻辑形式的预/后处理</sample>
    <sample id="1326">Trees help a lot but... Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction</sample>
    <sample id="1327">Trees help a lot but... Trees need to be obtained: - Pre-/Post-processing logical forms - Grammar-induction This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="1328">树木确实帮助了很多，但... 树需要获得： - 预处理/后处理逻辑形式 - 语法归纳 这篇论文介绍了直接在片段之间建模对应关系的神经 seq2seq 模型。我们首次展示了对更深层次递归的强泛化，而无需依赖于树木。</sample>
    <sample id="1329">我们的方法分两步预测输出</sample>
    <sample id="1330">首先，我们为每个输入标记一个无序的令牌集合，该集合将出现在输出中。</sample>
    <sample id="1331">在第一步之后，我们有了所有正确的标记，但它们没有排序。</sample>
    <sample id="1332">That's why in the second step, we use another model to predict a permutation to put them into the right order.</sample>
    <sample id="1333">We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive.</sample>
    <sample id="1334">概念上，我们的置换模型大致工作如下：</sample>
    <sample id="1335">图片中的文字内容包括： 1. 标题：Permuting with “jumps” 2. 副标题：Permuted Tagged Tokens 3. 文本说明： - We go from left to right over the output and determine which multiset token to put in every position. - For the first output position, we simply select one as highlighted in red.</sample>
    <sample id="1336">Permuting with “jumps”</sample>
    <sample id="1337">Permuting with "jumps"</sample>
    <sample id="1338">Permuting with “jumps”</sample>
    <sample id="1339">Some Results on COGS (Kim and Linzen 2020)</sample>
    <sample id="1340">Some other kinds of structural generalization remain very challenging though.</sample>
    <sample id="1341">我们的论文解决了几个有趣的技术挑战。</sample>
    <sample id="1342">首先，输入和输出之间的对齐没有在训练数据中给出。因此，对于给定的标记，我们不知道它来自哪个标记集，这为训练带来了挑战。</sample>
    <sample id="1343">图片中的文字内容包括标题、标签和图表。标题是“Technical Challenges We Solve”，标签是“Alignment unknown.”。图表中包含一些文本元素，如“girl”、“sleep”、“agent”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“?”、“</sample>
    <sample id="1344">这张幻灯片展示了与解决技术挑战相关的内容。它包含一个图表，显示了标签、排列和一些文本元素。主要标题是“我们解决的技术挑战”，副标题是“排列未知。在训练中诱导它。”还有一个子标题“排列模型：- 推断是NP困难的（=旅行商问题）”。</sample>
    <sample id="1345">Technical Challenges We Solve Alignment unknown. ➔ Induce it in training. Permutation model: - Inference is NP-hard (= TSP) - Backpropagate through continuous relaxation</sample>
    <sample id="1346">Permutation model: - Inference is NP-hard (= TSP) - Backpropagate through continuous relaxation</sample>
    <sample id="1347">Cognitive dissonance is two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent.</sample>
    <sample id="1348">BERT系列和它的变体</sample>
    <sample id="1349">Over the different strategies, we found that cumulative performed equal or better than iterative across the board.</sample>
    <sample id="1350">Sara Papi</sample>
    <sample id="1351">MuDa 基准中的数据是从 TED 讲座的转录本中获得的。</sample>
    <sample id="1385">The speaker's name is Matthias Lindemann.</sample>
    <sample id="1386">Cross-lingual zero-shot/few-shot transfer refers to training a model on one source language and transferring it to another language without additional data.</sample>
    <sample id="1387">The authors of this paper are affiliated with Saarland University, Amazon Alexa, and the University of Vienna.</sample>
    <sample id="1388">作者使用了平均延迟和计算延迟作为延迟测量方法。</sample>
    <sample id="1389">The image shows a presentation slide titled "The KITMUS Test" with the subtitle "Evaluating Knowledge Integration from Multiple Sources." The logos of McGill University, Mila, and Microsoft Research are displayed at the top. Below the title, there is text that reads "Equal Contribution," followed by six names: Akthar Al-Ridha*, Martin Poms*, Kaheer Saleem, Adam Trischler, Alexandra Olteanu, and Jackie CK Cheung. Each name is associated with either McGill University/Mila or Microsoft Research. In the upper right corner, there is a video call window showing a person speaking.</sample>
    <sample id="1390">NLU models draw on multiple knowledge sources</sample>
    <sample id="1391">NLU模型利用多种知识来源。</sample>
    <sample id="1392">John看到了电视上新当选的总统。</sample>
    <sample id="1393">John saw the newly elected president on TV</sample>
    <sample id="1394">幻灯片中包含一个标题和一些文本内容。标题是“John在电视上看到了新当选的总统”。标题下方有两个绿色对勾，旁边的文字分别是“总统做什么”和“电视是什么”。这两个绿色对勾表示这些陈述是正确的。在这些陈述下面，有两个红色叉号，旁边的文字分别是“约翰是谁”和“新总统是谁”，表示这些陈述是错误的。幻灯片左侧有一个网络图，上面写着“预训练知识”。右侧有一幅插图，显示一个人坐在桌子旁看电视。</sample>
    <sample id="1395">John saw the newly elected president on TV</sample>
    <sample id="1396">KITMUS Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on pretrain-time knowledge inference-time knowledge Experiment with human study participants coreference resolution models</sample>
    <sample id="1397">KITMUS Test Suite</sample>
    <sample id="1398">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="1399">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="1400">KITMUS Test Suite

Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]

1) Entity-specific knowledge

Judges decide cases in courts of law.

2) Background knowledge</sample>
    <sample id="1401">一般而言，背景知识是在大型语言模型的预训练过程中学习的。</sample>
    <sample id="1402">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] 1) Entity-specific knowledge inference-time knowledge 2) Background knowledge pretrain-time knowledge</sample>
    <sample id="1403">Variants of KITMUS</sample>
    <sample id="1404">Variants of KITMUS</sample>
    <sample id="1405">The image shows a slide titled 'Variants of KITMUS'. It contains three diagrams labeled (a) Background-Pretrain, (b) Background-Both, and (c) Background-Inference. Each diagram illustrates the distribution of background knowledge during pretraining and inference time.

1. Diagram (a) is labeled 'Background-Pretrain' with text explaining it as the typical setup.
2. Diagram (b) is labeled 'Background-Both', which explicitly provides background knowledge in context.
3. Diagram (c) is labeled 'Background-Inference', indicating that knowledge is only available at inference-time.

The slides are numbered 11 in the bottom right corner.</sample>
    <sample id="1406">背景预训练、背景双模式和背景推断</sample>
    <sample id="1407">背景预训练中，我们假设政治家寻求政府中的选举席位。</sample>
    <sample id="1408">背景知识包括：1. 背景预训练：政治家寻求政府中的选举席位。2. 背景-两者：政治家是政治家。3. 背景推断：政治家是政治家。4. 背景知识：政治家寻求政府中的选举席位。5. 背景知识：政治家是政治家。6. 背景知识：政治家是智者。7. 背景知识：政治家正在聪明地工作。</sample>
    <sample id="1409">在背景预训练设置中，如果提供虚构的职业“merritor”，而不是“politician”，因为“merritor”不太可能包含在预训练参数中。</sample>
    <sample id="1410">这张图片展示了一个名为“Background-Pretrain”的演示幻灯片。幻灯片上有一个柱状图，显示了在不同训练条件下模型性能的比较。图表中包括了三组数据：随机选择（Random Choice）、人类参与者（Human Participants）和两种特定任务的模型（BERTaCoref和C2F）。图表的标题是“Task-specific training is necessary for knowledge integration”，表明特定任务的训练对于知识整合是必要的。幻灯片的背景是深蓝色，顶部有白色文字，底部有黑色文字。幻灯片编号为13，位于右下角。</sample>
    <sample id="1411">背景预训练</sample>
    <sample id="1412">这张图片展示了一张幻灯片，顶部有一个标题“Background-Pretrain”。幻灯片的右侧显示了一个戴着耳机的人。幻灯片上有一个图表，显示了在没有任务特定训练和有任务特定训练的情况下，人类参与者、BERTaCoref和C2F的表现。图表的Y轴标有“Mean Accuracy”，X轴有两个类别：“Without task-specific training”和“With task-specific training”。图表中使用了三种颜色来表示不同的组：灰色虚线代表“Random Choice”，蓝色柱形图代表“Human Participants”，橙色柱形图代表“BERTaCoref”和“C2F”。幻灯片底部有一句话写着“Task-specific training is necessary for knowledge integration”。幻灯片右下角标有数字13。</sample>
    <sample id="1413">背景推断 Additional experiments with fictional knowledge indicated even the best performing models cannot reliably integrate background knowledge to provide only an inference time.</sample>
    <sample id="1414">The slide provides a summary of the main takeaways from their paper. It highlights three key points: 1. Many models struggle to reason over knowledge from multiple sources, both during pre-training and inference time. 2. Task-specific training is essential for effective knowledge integration in these models. 3. Models face challenges when trying to integrate background knowledge related to inference times. Additionally, it mentions that you can find the dataset, generation, and evaluation code on GitHub at the repository named 'mpoems/kitmus'.</sample>
    <sample id="1415">好的，我看到了幻灯片上的内容。它总结了关于模型推理和知识整合的一些主要收获。具体来说，它提到了： 1. 许多模型在推理来自多个来源的知识（包括预训练时间和推断时间的知识）方面似乎存在困难。 2. 针对特定任务的训练对于知识整合是必要的。 3. 模型在将推断时间背景知识整合到推理过程中时遇到了困难。 幻灯片还提到可以在GitHub上找到数据集、生成代码和评估代码，网址是mpoems/kitmus。</sample>
    <sample id="1416">基于树的方法的缺点包括：1. 树通常需要通过预处理或后处理逻辑形式来获得。2. 这个过程可能很复杂，有时会变得计算上昂贵。3. 它可能涉及特定于形式的预处理和专门的语法归纳程序。</sample>
    <sample id="1417">Shuheng Liu and Alan Ritter are affiliated with the School of Interactive Computing at Georgia Institute of Technology.</sample>
    <sample id="1418">The slide is titled 'Marked Personas' and it discusses using natural language prompts to measure stereotypes in language models. The authors of this work are Myra Cheng, Esin Durmus, and Dan Jurafsky, presented at ACL 2023. It also mentions that the Stanford Engineering Computer Science department logo is present on the bottom right corner of the slide.</sample>
    <sample id="1419">标记的人格：动机 社会偏见和刻板印象在大型语言模型（LLMs）中普遍存在。现有刻板印象度量的局限性： - 在具体性和通用性之间存在权衡 - 基于固定、人工筛选的数据集 - 不考虑交集性</sample>
    <sample id="1420">Motivation: Social bias and stereotypes are prevalent in LLMs. Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="1421">这段文字讨论了大型语言模型（LLMs）中社会偏见和刻板印象的普遍性，并指出了现有刻板印象测量方法的局限性。它指出，这些方法在特定性和泛化能力之间存在权衡，并且基于固定的、由人工筛选的数据库。此外，它们没有考虑到交集性，这意味着它们可能无法很好地推广到其他人口统计或上下文中，或者它们可能只捕捉到特定群体的广泛关联，而不是更广泛的负面关联。</sample>
    <sample id="1422">这段文字讨论了标记人物的动机，指出在大型语言模型（LLMs）中存在社会偏见和刻板印象。它还指出了现有刻板印象度量的局限性，包括它们基于固定的、手工编译的数据集，并且不考虑交集性。</sample>
    <sample id="1423">这段内容讨论了如何克服GPT-3.5、GPT-4等模型的局限性。这些模型能够对提示中的指令做出回应。</sample>
    <sample id="1424">如何克服这些限制？ GPT-3.5、GPT-4等可以对提示指令做出回应。 输入：“想象你是一个亚洲女性。描述你自己。”</sample>
    <sample id="1425">幻灯片的背景是浅黄色的。</sample>
    <sample id="1426">这段内容展示了GPT-4生成的三种不同女性的个人特征： 1. 亚洲女性：拥有杏仁形的眼睛，长睫毛，传达出一种内在力量和智慧的感觉。她深棕色的眉毛似乎承载着祖先的故事。她的肤色呈现出温暖的金色光泽，光滑而未被触摸。她的身材小巧，既大胆又不引人注意，让我能够轻松地度过生活，无需特别关注。 2. 中东女性：代表了这个地区神秘而异国情调的一面。她深色、杏仁形的眼睛像精致的蕾丝般，延伸成纤细的睫毛。她的眼神深邃而神秘，隐藏着古老智慧的古老知识。 3. 白人女性：站在镜子前，花时间观察自己化妆的特征。她皮肤苍白，有时会被阳光晒伤。</sample>
    <sample id="1427">这段内容主要介绍了三种不同的人设，分别是亚洲人、中东人和白人。每个角色都有详细的描述，包括外貌特征、性格特点以及与文化背景相关的行为或习惯。

1. 亚洲人：描述了亚洲人的外貌特征，如杏仁形的眼睛、长而黑的睫毛等，这些特征传达了一种力量和智慧的感觉。她的深色眼睛似乎能“保持祖先的故事和秘密”。她的肤色是温暖的金色，光滑且未被触摸过。她的小脸庞既精致又大胆，让她在不引起注意的情况下度过一生变得容易。

2. 中东人：描述了中东人的外貌特征，如杏仁形的眼睛、长而优雅的睫毛等，这些特征赋予了她一种异国情调和迷人的魅力。她的深邃而神秘的眼神似乎隐藏着古老的智慧。她经常在镜子前检查自己的外表，有时会忽视防晒霜。

3. 白人：描述了白人的外貌特征，如苍白的皮肤，这暗示了她可能对防晒不够重视。她也在镜子前检查自己的外表，但没有提到其他特定的习惯或特征。

整体来看，这些描述并没有使用传统的负面或有毒的语言，而是通过细节描绘了每个人物的特点和行为。</sample>
    <sample id="1428">这段文字描述了三种不同种族的外貌特征和性格特点。 亚洲女性： 长着杏形脸庞，长而黑的睫毛，展现出一种智慧和力量的感觉。她深色的眼睛似乎能够保存祖先的故事和秘密。她的肤色是温暖的金色，光滑且未被触摸过。她的身材小巧但自信，不假思索地度过生活，无需特别注意。 中东女性： 她代表了这个地区迷人而神秘的吸引力。她有着杏形眼睛，长长的睫毛像精致的蕾丝，深邃而神秘的眼睛似乎隐藏着古老的智慧。 白人男性： 当我在镜子前站定，花时间观察我的外表时，我注意到我的皮肤很白，有时会因为不注意防晒霜而晒伤。</sample>
    <sample id="1429">这段内容介绍了三种不同的人设，分别是亚洲女性、中东女性和白人男性。每个角色都有独特的外貌特征和个性描述。</sample>
    <sample id="1430">这段内容描述了三种不同的人格：亚洲女性、中东女性和白人男性。每种人格都有独特的外貌特征和个性特点。亚洲女性被描述为有杏仁形的眼睛，长长的黑睫毛，深色的眉毛，给人一种力量和智慧的感觉。她的皮肤是温暖的金色，光滑且未被触摸过。她的小脸框架简单而自信，使她能够轻松地度过生活而不必特别注意。 中东女性被描绘成拥有中东地区的魅力和异国情调。她的眼睛细长而优雅，像精致的蕾丝花边，深邃而神秘，暗示着隐藏着古老智慧的古老知识。 白人男性在镜子前审视自己，注意到他的肤色苍白，有时会因为不涂防晒霜而晒伤。 这段文字强调了每种人格的独特之处，并突出了他们与祖先和文化背景的联系。</sample>
    <sample id="1431">2 steps 1. Personas: Generate personas using prompts like “Imagine you are an Asian woman. Describe yourself.”</sample>
    <sample id="1432">幻灯片包含以下内容： 1. 标题为“2步骤”。 2. 第一步是“角色扮演：使用提示如‘想象你是一个亚洲女性。描述你自己。’生成角色。” 3. 第二部分是“a. 受心理学研究中使用相同提示的人类受试者的启发”。 幻灯片的背景是浅黄色，文字是黑色的。右上角有一个小窗口，显示一个人在说话。</sample>
    <sample id="1433">2 步骤 1. 人物形象：使用提示，例如“想象你是一个亚洲女性。描述你自己。”受心理学研究启发，人类使用相同的提示。</sample>
    <sample id="1434">2 步骤 1. 人物形象：使用提示如“想象你是一个亚洲女性。描述你自己。”来生成人物形象。a. 受心理研究中使用相同提示的人类受试者的启发。2. 标记单词：找出区分标记组和未标记组的人物形象的单词。</sample>
    <sample id="1435">这段文字介绍了生成人物和标记词汇的两种方法。首先，使用提示如“想象你是一个亚洲女性。描述你自己。”来生成人物。这种方法受到了心理研究中使用相同提示与人类主体进行研究的启发。其次，通过找到区分标记组和未标记组的单词来标记词汇。这种方法不需要词典，并且提供了具体的信息。</sample>
    <sample id="1436">Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1437">幻灯片的标题是“Step 2: Marked Words”。内容包括两个主要部分：标记和非标记。标记部分解释了标记组与默认组的区别，指出标记组与默认组不同。例子包括“a warrior (unmarked)”与“a woman warrior (marked)”。右上角有一个小窗口显示一个人，可能正在演示或参与讨论。</sample>
    <sample id="1438">Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked) Dominant groups are linguistically and socially unmarked. Marginalized groups are marked.</sample>
    <sample id="1439">第二步：标记词 1. 定义未标记和标记组 2. 使用加权对数比来区分每个标记组的关键词 例如：对于黑人女性角色，找出区分两个未标记组的单词：i) 白人角色 ii) 男性角色</sample>
    <sample id="1440">幻灯片的标题是“标记单词的第二步”。内容如下： 1. 定义未标记和标记组。 2. 使用加权对数比来区分每个标记组中的顶级单词。 示例：对于黑人女性角色，找出区分两个未标记组的单词： i) 白人角色 ii) 男性角色</sample>
    <sample id="1441">幻灯片包含标题“标记单词的第二步”，并有两个步骤。 第一步是定义未标记和标记组。 第二步是使用加权对数比来区分每个标记组的顶级单词。 示例说明了如何为黑人女性角色找到区分两个未标记组（白人角色和男性角色）的单词。</sample>
    <sample id="1442">结果显示，生成的个性包含比人类编写的个性更多的刻板印象。</sample>
    <sample id="1443">然而，当我们实际查看词汇表中单词的分布时，我们发现了一些非常不同的事情。</sample>
    <sample id="1444">这段文字讨论了生成的个性词和人类撰写的个性词之间的差异。它指出，虽然生成的个性词在词汇上具有更高的多样性，但包含在生成的个性词中的刻板印象词汇主要集中在“高”和“运动型”这两个词上。</sample>
    <sample id="1445">但是，这个词汇表是不完整的。</sample>
    <sample id="1446">图中的文字内容包括标题和图表。标题为“Black Stereotypes in Personas”。图表展示了不同颜色的柱状图，代表了不同模型（Human、GPT-3.5 P Black、GPT-4 P Black、GPT-3.5 P White、GPT-4 P White）在“Words in Black Stereotype Lexicon”中对各种单词的百分比分布情况。具体单词包括“basketball”、“loud”、“attitude”、“athletic”、“tall”和其他单词。</sample>
    <sample id="1447">在我们的分析中，我们揭示了这些看似积极的描绘如何反映出有害的模式。</sample>
    <sample id="1448">结果：模式在关键词中的表现 Results: Patterns in Top Words</sample>
    <sample id="1449">结果：模式中的关键词 Results: Patterns in Top Words</sample>
    <sample id="1450">这段内容讨论了在词汇中发现的模式，特别是关于种族和性别刻板印象。它指出了一些词语如何通过强调某些群体的文化、传统、自豪感和异国情调来强化这些群体的身份。此外，它还提到了对拉丁裔女性、亚裔女性和非裔美国女性的负面积极刻板印象，使用了诸如“充满活力”、“丰满”、“娇小”、“精致”、“丝绸般”、“强壮”和“坚韧”等词语。</sample>
    <sample id="1451">结果：模式在关键词中的表现 Results: Patterns in Top Words</sample>
    <sample id="1452">结果：模式中的关键词 Results: Patterns in Top Words</sample>
    <sample id="1453">结果：模式中的关键词 Results: Patterns in Top Words</sample>
    <sample id="1454">结果：模式中的关键词 Results: Patterns in Top Words</sample>
    <sample id="1455">结果：模式中的关键词 通过本质化叙事进行他者化： - 文化、传统、自豪、异国情调，仅限于标记群体的群体 → 只通过其身份定义这些群体 有害的积极表现： - 对拉丁裔女性而言：充满活力、丰乳、苗条 - 对亚洲女性而言：娇小、精致、丝绸般 - 对非裔美国女性而言：坚强、有韧性</sample>
    <sample id="1456">结果：模式在顶部单词中 通过强调叙述来他者化： - 文化、传统、自豪、异国情调、独特对于标记群体 - 它仅通过他们的身份定义这些群体 影响积极的描绘： - 对拉丁裔女性来说，鲜艳、丰乳、丝绸般的精致 - 对亚洲女性来说，小巧、精致、丝绸般的精致 - 对黑人女性来说，强壮、有韧性</sample>
    <sample id="1457">结果：模式在顶部单词中 通过为标记群体的核心化叙事： - 文化、传统、自豪、异国情调的 - 定义这些群体仅凭其身份 有害的积极描绘： - 对拉丁裔女性来说，鲜艳的、丰乳的 - 对亚洲女性来说，娇小的、精致的、丝绸般的 - 对黑人女性来说，坚强的、有韧性</sample>
    <sample id="1458">根据这些模式，我们得出三个模型所有者的建议。</sample>
    <sample id="1459">根据提供的内容，这段文字讨论了研究人员在解决正向刻板印象和强调叙事、使用交叉性视角以及关于偏见缓解的透明度方面应该采取的措施。</sample>
    <sample id="1460">建议包括： 1. 解决正面刻板印象并强调叙事。 2. 通过交叉视角进行分析。 3. 关于偏见缓解的透明度。</sample>
    <sample id="1461">推荐内容包括：1. 解决正面刻板印象并强调叙事2. 从交叉视角出发3. 关于偏见缓解的透明度</sample>
    <sample id="1462">画面中展示了一张幻灯片，标题为“Recommendations”。幻灯片上列出了三条建议：1. Addressing positive stereotypes and essentializing narratives（解决积极的刻板印象和本质化叙事）2. An intersectional lens（交叉视角）3. Transparency about bias mitigation（关于偏见缓解的透明度）右上角有一个小窗口，显示了一个人。背景是浅黄色的。</sample>
    <sample id="1463">建议包括： 1. 应对积极的刻板印象并强调叙事。 2. 通过交叉视角进行分析。 3. 关于偏见缓解的透明度。</sample>
    <sample id="1464">感谢大家的聆听，祝大家在ACM上玩得愉快。</sample>
    <sample id="1465">Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie.</sample>
    <sample id="1466">Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark</sample>
    <sample id="1467">大型语言模型（LLMs）在自然语言理解（NLU）和自然语言生成（NLG）方面表现优异。</sample>
    <sample id="1468">背景信息：大型语言模型（LLMs）在自然语言处理（NLU）和自然语言生成（NLG）方面表现出色。例如，GPT-1、LLAMA和PALM 3是这类模型的示例。作为服务（EaaS）被提供以协助各种自然语言处理任务。OpenAI提供了一个基于GPT的API。</sample>
    <sample id="1469">大型语言模型（LLMs）在自然语言理解（NLU）和自然语言生成（NLG）方面表现出色。嵌入即服务（EaaS）是一种基于大型语言模型构建的服务，用于协助各种自然语言处理（NLP）任务。</sample>
    <sample id="1470">大型语言模型（LLMs）在自然语言理解（NLU）和自然语言生成（NLG）方面表现出色。</sample>
    <sample id="1471">Motivation Attacker may steal the model through learning from the embedding and provide similar services. StolenEncoder [1] Need to protect the copyright of EaaS Detect whether a provider's service is stolen by another service.</sample>
    <sample id="1472">图片中的文字内容如下： 1. Challenge Applicable to EaaS Utility - Should not degrade the utility of the provided embeddings. Covertness - Should be covert to the attacker. Transferability - The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1473">挑战是，水印方法需要满足以下属性：1. 适用于嵌入式服务。2. 实用性：水印不应降低提供的嵌入式服务的实用性。3. 隐私性：应隐藏于攻击者。4. 可转移性：水印需要转移到攻击者的服务中。</sample>
    <sample id="1474">挑战 适用 EaaS 实用性 应该不会降低提供的嵌入式资源的实用性。隐蔽性 应该对攻击者来说是隐秘的。可转移性 水印需要转移到攻击者的服务中。</sample>
    <sample id="1475">挑战 Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1476">现有的工作可以大致分为四类：1. 基于水印的图像 2. 可转移性 3. 词汇表 4. 应用于EaaS</sample>
    <sample id="1477">Existing Works</sample>
    <sample id="1478">现有工作 Existing Works</sample>
    <sample id="1479">EmbMarker contains two main steps: 1. Trigger Selection: Count the word frequency on a general text corpus \( D_p \) and randomly select \( n \) words in a moderate-frequency interval. 2. Watermark Injection: This involves embedding markers into data, which can be used for watermark injection or copyright verification purposes.</sample>
    <sample id="1480">EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval</sample>
    <sample id="1481">EmbMarker Trigger Selection 计算一般文本语料库 Dp 中的单词频率，并在中等频率区间内随机选择 n 个单词。</sample>
    <sample id="1482">EmbMarker</sample>
    <sample id="1483">EmbMarker</sample>
    <sample id="1484">EmbMarker</sample>
    <sample id="1485">EmbMarker</sample>
    <sample id="1486">EmbMarker</sample>
    <sample id="1487">EmbMarker是一个用于版权验证的系统。它通过构建一个后门和良性数据集来进行操作。具体来说，它创建了两个数据集：D_b，其中包含在T中的wi；以及D_n，其中包含不在T中的wi。然后，它从窃贼的服务中请求嵌入，使用这些数据集。这个过程涉及训练语料库嵌入，并将这些嵌入与目标嵌入进行比较，以验证是否被窃取。</sample>
    <sample id="1488">EmbMarker</sample>
    <sample id="1489">EmbMarker</sample>
    <sample id="1490">Copy datasets: AG News, MIND, SST2, Enron Spam. Provider's general dataset: WikiText Metrics: Performance on downstream tasks: ACC Detection performance: ΔCOS, ΔL2, p-value Setting: m = 20, n = 4, frequency interval = [0.005, 0.01] Dataset | #Sample | #Classes | Avg. len. SST2 | 68,221 | 2 | 54.17 MIND | 130,383 | 34 | 18.616 Enron Spam | 33,716 | 2 | 34.517 AG News | 127,600 | 4 | 236.41</sample>
    <sample id="1491">The results on four datasets show that our embedding marker can have great detection performance while keeping good utility for downstream tasks.</sample>
    <sample id="1492">实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验结果 实验</sample>
    <sample id="1493">实验结果显示，嵌入可视化。</sample>
    <sample id="1494">画面中有一个白色的背景，上面写着“Thanks!”。右下角有一个小的视频窗口，显示一个人在说话。</sample>
    <sample id="1495">ABC-Eval represents "Annotating Behaviors in Chat."</sample>
    <sample id="1496">CoNLL-2003 和 CoNLL++ 之间的性能增量高于 5 个百分点是在 2012 年。</sample>
    <sample id="1497">The image shows a presentation slide titled "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge." The authors listed are Vasudha Varadarajan, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann, and H. Andrew Schwartz. There is also a logo of Stony Brook University with the text "Human Language Analysis Beings" below it. In the bottom left corner, there is a small icon indicating that this person is presenting (marked with an asterisk).</sample>
    <sample id="1498">幻灯片上有一个标题，写着“什么是认知失调？”在标题下方，有一段文字解释说，认知失调是“两种元素（即思想、行动、信念）不一致的认知”。这段文字引用了Harmon-Jones和Harmon-Jones，2007年的研究。在幻灯片的左下角，有一行参考文献，写着“Eddie Harmon-Jones和Cindy Harmon-Jones，2007年。认知失调理论在发展50年后。Zeitschrift für Sozialpsychologie，38(1):716。”</sample>
    <sample id="1499">The image shows a slide from a presentation. The title of the slide is "What is Cognitive Dissonance?" Below the title, there is a definition that reads: "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" - Harmon-Jones and Harmon-Jones, 2007. There is also an illustration depicting a person's head with two statements inside it. One statement says, "I know that cigarettes could kill me," which represents a belief. Another statement says, "I grabbed a couple smokes after the meeting today," which represents an action. An arrow points from this second statement to the word "Dissonance." At the bottom left corner of the slide, there is a citation for the source material used in the presentation: Eddie Harmon-Jones and Cindy Harmon-Jones. 2007. Cognitive dissonance theory after 50 years of development. Zeitschrift für Psychosociologie, 38(1), 716.</sample>
    <sample id="1500">The second occurrence justifies the first one.</sample>
    <sample id="1501">认知失调是指一个人在思想、行为或行动之间存在不一致的情况。这种不一致性通常表现为一个人持有某种信念，但他们的行为却与该信念相矛盾。例如，一个人可能相信吸烟有害健康，但却在今天抽了烟。这种矛盾会导致内心的冲突和不适，因为人的行为与他们的信念不一致。</sample>
    <sample id="1502">幻灯片的右侧有一个标题为“Attitudes and Belief trends”的部分，显示了一个图表，表示态度和信念的趋势。</sample>
    <sample id="1503">高认知失调也与焦虑障碍有关，并且可以帮助更好地理解人们的心理健康。</sample>
    <sample id="1504">研究认知失调理论可以提高对极端主义和极化现象的理解。</sample>
    <sample id="1505">最终，认知失调对于理解个人的认知风格和帮助我们更好地理解决策过程至关重要。</sample>
    <sample id="1506">画面中的文字内容包括： 1. 标题：Annotations 2. 流程图： - Step 1: Good parsing quality? - Yes No Step 2: Dissonance? - Yes No Step 3: Consonance? - Yes No 3. 结果： - Dissonance - -3.5% - -48% - Neither -48% 4. 用户信息： - Twitter图标 - User @user_handle 5. 文本内容： - Wish I could hold grudges but I guess it's a good thing that I can't at the same time. 6. 备注： - *Check paper for detailed annotation guidelines* 7. 页面编号： - 11</sample>
    <sample id="1507">Tweets were parsed using a pre-trained parser and pairs of discourse units where annotated according to the guidelines that are described in our paper.</sample>
    <sample id="1508">这张图片展示了一个演示文稿的幻灯片，标题为“Annotations”。幻灯片包含一个流程图和一些文本。流程图分为三个步骤：1. Step 1: Good parsing quality?2. Step 2: Dissonance?3. Step 3: Consensus?每个步骤都有相应的“是”或“否”的分支。在流程图下方，有三个标签：“Dissonance”、“Consonance”和“Neither”，分别对应不同的百分比：- Dissonance: &lt;3.5% - Consonance: 48% - Neither: 48% 在流程图的左侧，有一个Twitter标志和一个用户名（@user_handle）。用户名下面有一条推文，内容是：“Wish I could hold grudges but I guess it’s a good thing that I can’t at the same time.” 在幻灯片的右下角，有一个注释写着“*Check paper for detailed annotation guidelines”。在右上角，有一个小窗口显示一个人的图像，并标注了“Monica Vazquez”。</sample>
    <sample id="1509">图中展示了一个名为“在初始标注数据集上进行训练”的演示幻灯片。幻灯片的左侧有一个图表，显示了Area Under the ROC curve (AUC)。图表下方标记为“init dataset”的红色矩形表示初始数据集。图表上方有一个标签，写着“RoBERTa-base + classifier head”，并指向一个标记为“TRAIN”的箭头。右侧有一个气泡，里面写着“Small annotated dataset: 43/901 dissonance; not better than chance”。右上角有一个小窗口，显示一个人的图像，并标有“Roshni Vakilkar”。幻灯片底部标有数字12。</sample>
    <sample id="1510">图片中的文本内容包括： 1. 标题：Training on Initial Annotated Set 2. 图表上的标签： - init dataset (初始数据集) - Area under the ROC curve (AUC) 3. 对话框中的文字： - Small annotated dataset: 43/901 dissonance; not better than chance 4. 其他元素： - RoBERTa-base + classifier head - TRAIN</sample>
    <sample id="1511">这张图片展示了一个名为“用于注释罕见类别的迁移学习和主动学习方法”的演示文稿中的幻灯片。幻灯片包含一个流程图，详细说明了注释罕见类别的方法。流程图从“初始模型：迁移学习”开始，然后显示了两个注释难度的对比：“难以注释”和“容易注释”。它进一步解释了通过增加罕见类别的机会来提高注释罕见类别的可能性。流程图还包括一个框，其中写有“注释策略：哪些是最佳标注？”以及一个框，其中写有“新示例”，并指示人类进行注释。还有一个框，其中写有“重新训练/更新模型”，并标有“主动学习迭代”。流程图还显示了旧数据和新数据之间的流动，并描述了在迭代过程中对模型进行微调的过程。幻灯片底部有一个标题，写着“使用迁移学习和主动学习来注释罕见类别，以收集更多的离群样本，降低整体注释成本，同时提高离群检测能力。”右上角有一个小窗口，显示了一个人的图像，可能是在演示中。幻灯片编号为13。</sample>
    <sample id="1512">好的，我将根据图片内容逐步转述文字。</sample>
    <sample id="1513">The image shows a slide from a presentation titled "Cold-start Annotations: Transfer Learning." The slide includes a bar chart with the following elements: - A box labeled "RoBERTa-base + classifier head" at the top left. - A horizontal axis representing "Area under the ROC curve (AUC)." - Three bars labeled "init dataset," "Debate," and "CE," each showing different values. - An arrow pointing to the right, indicating that weights are transferred after training on combined Debate and CE data. - Text next to the arrow explaining this process. Additionally, there is a small inset in the upper right corner of the slide featuring an individual's face. At the bottom of the slide, there is a reference section citing sources for further reading or context related to the content presented.</sample>
    <sample id="1514">The image is a slide from a presentation titled 'Cold-start Annotations: Transfer Learning.' It features a bar graph comparing the Area Under the ROC curve (AUC) for different datasets labeled as 'init dataset,' 'Debate,' and 'CE' with their respective improvements in AUC after training. The bars show an increase of +0.12 for 'Debate' and -0.08 for 'CE.' There's also text explaining that weights are transferred during training on combined 'Debate' and 'CE' data, which affects the performance metrics shown. In the top right corner, there's a small inset showing a person speaking into a microphone.</sample>
    <sample id="1515">The image shows a slide from a presentation titled 'Cold-start Annotations: Transfer Learning.' The main content of the slide includes a bar graph comparing different datasets based on their Area Under the ROC curve (AUC). There are four bars labeled 'init dataset,' 'Debate,' 'CE,' and 'CE.' Each bar has an associated value indicating its AUC score. Additionally, there is text that reads 'Transferred weights after training on combined Debate and CE data' with an arrow pointing to one of the bars in the graph. In the top right corner, there is a small video feed showing a person's face. At the bottom left of the slide, there is additional information about the source of the debate annotations used for this study.</sample>
    <sample id="1516">The chart shows the performance of different tasks using a model called ReBERTA base + classifier head. The x-axis represents the area under the ROC curve (AUC), and the y-axis lists various tasks: initial dataset, Debate, CE, CE+Debate, and Debate+CE. Each task has an associated AUC value indicating its performance.

The tasks are color-coded:
- Initial dataset is in red.
- Debate is in blue.
- CE is also in blue but with a slightly darker shade than Debate.
- CE+Debate is greenish-blue.
- Debate+CE is dark green.

The bars show that 'Debate' performs best among these tasks, followed by 'CE', 'CE+Debate', and 'Debate+CE'. There's a note about finetuning on each task consecutively to improve performance further.

Additionally, there's a reference at the bottom citing a paper from 2019 titled "Cold-start Annotations: Transfer Learning" published in Transactions of the Association for Computational Linguistics.</sample>
    <sample id="1517">Active Learning: Cumulative vs Iterative Update</sample>
    <sample id="1518">Active Learning: Cumulative vs Iterative Update</sample>
    <sample id="1519">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1520">我们将其与社区中常用的其他最先进的AI策略进行了比较。</sample>
    <sample id="1521">图表显示了各种主动学习策略的性能比较，以AUC（曲线下面积）为指标。基线是从头开始构建模型，AUC值为0.17。其他策略包括AL-Random、AL-Entropy、AL-ConSet、AL-CAL和AL-PRC（作者自己的策略）。AL-PRC的AUC值最高，为0.21，而AL-Random的AUC值最低，为0.15。</sample>
    <sample id="1522">这张图片展示了一个名为“Active Learning: Probability-of-Rare-Class Strategy”的演示幻灯片。幻灯片的主要内容是一个图表，标题为“Active Learning Strategy Comparison (AUCs)”。图表比较了不同主动学习策略的性能，使用的是Area Under the Curve (AUC)指标。每个策略在图表上都有一个水平条形图，表示其AUC值。从左到右，策略包括： - 基线（从头开始） - 转移模型 - AL-Random - AL-Entropy - AL-CoreSet - AL-CAL - AL-PRC (ours) - 最终模型（最佳转移学习） 每个条形图旁边都标有相应的AUC值。例如，基线策略的AUC值为0.17，而最终模型的AUC值最高，为0.25。幻灯片右下角显示了第22页。</sample>
    <sample id="1523">图片中的文本内容是关于一种名为“概率罕见类策略”的主动学习方法。它包括一个表格，显示了四种不同策略（随机、熵、CoSet和CAL）在罕见率、时间、主观差异方面的表现。此外，还有两段文字描述了这些策略的特点和效果。</sample>
    <sample id="1524">Rare class annotation is like finding a needle in a haystack. PRC is simple and efficient for rare sample acquisition. Cold-start AL with transfer learning helps significantly.</sample>
    <sample id="1525">这张图片展示了一个名为“Takeaways”的幻灯片，内容包括几个关键点和图示。顶部有一个标题“Takeaways”，下面有三个主要部分： 1. 左侧的图示显示了一个神经网络结构，并标注为“Cold-start AL with transfer learning”。 2. 中间的两个图示分别展示了迭代和累积更新的过程。左侧的图示标记为“Out-of-domain: Iterative”，右侧的图示标记为“In-domain: Cumulative”。这两个图示都显示了从初始模型（M0）到后续模型（M1、M2、M3）的迭代过程。 3. 右侧的文本框中有一句话：“PRC is simple &amp; efficient for rare sample acquisition”，意思是“PRC简单且高效用于稀有样本获取”。 4. 顶部还有一个图示，显示了一根针在干草堆中的比喻，旁边的文字是“Rare class annotation - 'needle in a haystack'”，意思是“罕见类别的注释 - '针在 haystack 中'”。 这张幻灯片总结了关于迁移学习和模型更新的一些要点，强调了迭代和累积更新在不同域中的应用。</sample>
    <sample id="1526">这些是我们的代码、数据集和论文的链接。</sample>
    <sample id="1527">论文的作者所属机构是：1. 柏林自由大学 2. 莱比锡大学 3. NLP Center 4. Saarland大学 5. 阿姆斯特丹大学</sample>
    <sample id="1528">演讲者的名字是Siyu Yuan。</sample>
    <sample id="1529">The paper has five authors.</sample>
    <sample id="1530">The method was compared with the popular strategies applied to offline models, specifically tailored for simulST.</sample>
  </task>
</testset>