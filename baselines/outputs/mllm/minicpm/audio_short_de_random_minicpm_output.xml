<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Web crawl data, political news media</sample>
    <sample id="1">McGill University, Mila and Microsoft Research</sample>
    <sample id="2">Hallo, willkommen bei unserer Präsentation von DeepLing, einem neuen Korpus für die deutschtatsächliche Identifizierung auf der Dokumentenebene und auf der Satzebene.</sample>
    <sample id="3">Mein Name ist Regina Stauden und ich werde Sie durch den ersten Teil der Präsentation leiten. Lassst uns zunächst Textsimplifizierung definieren.</sample>
    <sample id="4">Textamplifizierung ist der Prozess der Anpassung eines Textes, um die Verständnisfähigkeit für bestimmte Zielgruppen zu verbessern, wie Menschen mit Leseschwierigkeiten oder nicht-native Sprechern.</sample>
    <sample id="5">Um ein Textidentifizierungsmodell zu trainieren, benötigen wir Paare von Texten in parallelen Formaten, zum Beispiel Dokumente oder Sätze.</sample>
    <sample id="6">Im Beispiel hier sehen Sie eine parallel ausgerichtete Paarung von einer komplexen deutschen Satz und seiner Übersetzung in plain language.</sample>
    <sample id="7">Um die Satzstruktur zu vereinfachen, gibt es verschiedene Techniken, wie du sie im Beispiel sehen kannst. Diese umfassen die Substitution, die Verschiebung, die Umordnung oder die Einfügung von Worten.</sample>
    <sample id="8">Wir stellen nun unser neues Kooperationskonzept vor, weil es in den letzten Jahren einige Probleme mit den bestehenden Korporen gab. Zum Beispiel sind diese Korporen zu klein, um ein Textechnifizierung-Modell zu trainieren.</sample>
    <sample id="9">Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass es in der Regel Fehler im Ausrichtungsprozess geben kann.</sample>
    <sample id="10">D-Plaine ist ein neues Korpus, das in zwei Unterkorpora, D-Plaine APA und D-Plaine Web, aufgeteilt ist. D-Plaine APA basiert auf Nachrichtenartikeln.</sample>
    <sample id="11">In der Plane APA haben wir 483 Dokumente alle manual ausgerichtet. Das gibt etwa 30.000-30.000 parallele Sattpaare.</sample>
    <sample id="12">Für die plainweb diese Körpers umfasst verschiedene Bereiche und wir alignen auch alle diese 750 Dokumente auf der einen Seite manuell und auf der anderen mit automatischen Alignment Methoden.</sample>
    <sample id="13">Insgesamt resultiert es in 30.450 Paaren von Sätzen.</sample>
    <sample id="14">Wir analysierten unsere Sätze ein bisschen mehr, zum Beispiel auf die Art der Übersetzung.</sample>
    <sample id="15">Wie Sie sehen können, sind die Bibeltexte viel einfacher als zum Beispiel die Nachrichten- oder Sprachlerntexte.</sample>
    <sample id="16">Auf allen Ebenen, zum Beispiel bei der Lexikalisierung, Strukturisierung, auch auf der Gesamtebene der Isolierung.</sample>
    <sample id="17">Darüber hinaus können Sie sehen, dass unser D-Plain-Korpus eine Vielzahl unterschiedlicher Abkürzungstransformationen aufweist. So zum Beispiel im D-Plain-API-Korpus haben wir viel mehr Reordernungen und Wortzusatzungen als im D-Plain-Web-Korpus.</sample>
    <sample id="18">Auf der anderen Seite haben wir im Web-Korpus viel mehr Wiederformulierungen.</sample>
    <sample id="19">Lass uns jetzt sehen, was wir mit diesem Korpus tun können.</sample>
    <sample id="20">In den letzten Jahren wurden viele Alignment-Methoden entwickelt, insbesondere im Zusammenhang mit maschineller Übersetzung.</sample>
    <sample id="21">Es gibt zwei parallele Dokumente, die in verschiedenen Sprachen geschrieben wurden, und wir möchten die Übereinstimmungen von Sätzen in beiden Dokumenten extrahieren.</sample>
    <sample id="22">Aber in unserem Szenario versuchen wir, Ähnlichkeiten zwischen den Sätzen zweier paralleler Dokumente zu extrahieren, die dieselbe Sprache und den gleichen Inhalt haben, aber auf einer unterschiedlichen Komplexitätsebene liegen.</sample>
    <sample id="23">Und jetzt, da wir unsere Datensatz-Deplane haben, der manuelle Ausrichtungssätze enthält, können wir diese Sätze als "Goldstandards" verwenden, um einige der vorgeschlagenen Ausrichtungsverfahren zu bewerten.</sample>
    <sample id="24">Wir haben einige Anpassungen an die vorgeschlagenen Methoden getroffen und alle diese Anpassungen und die Codes zur Durchführung unserer Experimente im Papier veröffentlicht.</sample>
    <sample id="25">Am Ende haben wir geschlossen, dass die beste Automatic Alignment Methode für die Textsimplifizierung auf deutscher Texte die Methode von MassAlign ist.</sample>
    <sample id="26">Sie können auch den Code zum Ausführen dieser Methode auf Ihren eigenen Dokumenten im Papier finden.</sample>
    <sample id="27">Das zweite Szenario, das wir in unserem Papier präsentiert haben, ist ein Fall für die automatische Texteinfachstellung.</sample>
    <sample id="28">Durch die Anpassung von Sprachmodellen, um einfacheren Text aus komplexen Eingabestücken zu produzieren.</sample>
    <sample id="29">Wir haben zwei Modelle feinabgestimmt. Wir haben das LLM-Modell für die Produktion von Dokumentenabschnitten feinabgestimmt.</sample>
    <sample id="30">Wir haben auch den Normalbase-M-Part anhand des normalen Basis-Longs an die Produktionsstufe von Sätzen angepasst.</sample>
    <sample id="31">Sie können auch alle Checkpoints finden und weitere Details zu den Ergebnissen und den Bewertungsmaßstäben unserer Experimente im Papier überprüfen.</sample>
    <sample id="32">Wir haben geschlossen, dass diese grundlegende Feinabstimmung</sample>
    <sample id="33">Und wir stellen diese Ergebnisse als Leistungsleistung, als Bas-Leistungsleistung für das Problem der automatischen Texteinfachung in Zukunft vor.</sample>
    <sample id="34">Vielen Dank für Ihre Aufmerksamkeit und hoffen, Sie alle während der Konferenz zu treffen. Danke.</sample>
    <sample id="35">Kayo Yen</sample>
    <sample id="36">T5-XLARGE</sample>
    <sample id="37">CoNLL-2003 taggers still work in 2023.</sample>
    <sample id="38">Das neue menschliche Bewertungssystem versucht, die Subjektivität menschlicher Bewertungen zu reduzieren. Erstens durch den expliziten Annotieren, ob jeder Modellbeantwortung bestimmte Verhaltensweisen widerspiegelt, wie zum Beispiel die Verwendung von Irrelevantinformation oder die Selbstkontradiktion.</sample>
    <sample id="39">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Verfügbarkeit und Qualität der Validationssamples ab.</sample>
    <sample id="40">Entities</sample>
    <sample id="41">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="42">Hallo, mein Name ist Adam Skirukowski und dieser Vortrag handelt über die Abhängigkeitsstruktur der Koordination.</sample>
    <sample id="43">Als ihr wisst, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Körperschreibungen angenommen werden. So zum Beispiel in den Universaldependencies ist die Struktur der Koordinationsstruktur "Lisa, Bart und Maggie".</sample>
    <sample id="44">Es ist solange, dass der erste Conjunkt der Kopf der gesamten Koordinatstruktur ist, also in diesem Fall Lisa.</sample>
    <sample id="45">Zwei Ansätze sind ähnlich wie Igor Milchucks Bedeutungstexttheorie, wo wieder der Koordinatenstruktur durch den ersten Konjunkt angeführt wird. Diese beiden Ansätze sind isomertisch, sie hervorheben einen der Konjunkte.</sample>
    <sample id="46">Es gibt auch symmetrische Ansätze zur Koordinatenstruktur, wie das Prag-Approach, das Konjunktionshead-Approach, das in Prag-Dependenz-Baumortakten verwendet wird, bei dem Koordinatenstrukturen durch den Konjunktionskopf angeführt werden.</sample>
    <sample id="47">Also, wir bekommen einige Abhängigkeiten von "und" zu allen Konjunkturen.</sample>
    <sample id="48">Schließlich gibt es auch ein vielschichtiges Ansatz, das zum Beispiel in der Cutson-Wordgrammatik verwendet wird.</sample>
    <sample id="49">"所有连词都是从句的头，因此我们得到从主语“here loves”到所有连词“Lisa, Bart and Meg”的依赖关系。"</sample>
    <sample id="50">Das Hauptziel dieses Beitrags besteht darin, eine neue Argumentation für symmetrische Koordinationsstrukturen zu präsentieren und unsymmetrische Koordinationsstrukturen zu widerstehen.</sample>
    <sample id="51">Okay, das Argument basiert auf dem Prinzip der Abhängigkeitsminimierung, das ich aufgrund dieser Beispiele erklären werde.</sample>
    <sample id="52">In English, as you might know, direct objects prefer to be close to the verb while adjuncts may be further away. So "Marge read it yesterday" is fine because the direct object (it) is close to the verb ("read").</sample>
    <sample id="53">While March read yesterday it is much worse right because here between the verb and the direct object there is an adjunct are yesterday.</sample>
    <sample id="54">Dieses Phänomen kann jedoch verbessert werden, wenn der direkte Gegenstand sehr schwer und sehr lang ist, weil er dann nach dem Adjunkten in die Position verschoben werden kann.</sample>
    <sample id="55">Dies wird hier illustriert. Beide Sätze sind in Ordnung. "Marge hat dieses absolut faszinierende Buch über die BCS gestern" ist okay, anstatt "es" haben wir diese lange NP.</sample>
    <sample id="56">Aber es ist auch in Ordnung, gestern Marge Reddy zu sagen, dieses absolut faszinierende Buch über Bienen.</sample>
    <sample id="57">Die Grundlage hier ist, dass dies möglich ist, obwohl diese Satz die allgemeine grammatikalische Prinzip verletzt, das direkte Objekt neben dem Verben stehen sollte.</sample>
    <sample id="58">Es erfüllt das Prinzip der Längenminimierung von Abhängigkeiten, das sagt, dass kürzere Abhängigkeiten bevorzugt werden.</sample>
    <sample id="59">Deshalb zeigen diese beiden Bäume nur die Länge der entscheidenden Abhängigkeiten, also diejenigen, die zwischen diesen beiden Strukturen nicht konstant sind.</sample>
    <sample id="60">Hier haben wir eine Abhängigkeit von "red" auf die Nebensatzlänge von 7, gemessen in Worten, und von "red" auf den Buchtitel mit Länge 4. Zusammen beträgt dies 11.</sample>
    <sample id="61">Wenn Sie diese beiden Bestandteile wechseln, wird die Summe dieser beiden Abhängigkeiten zu 6. Stattdessen 11, 6 viel kürzer, daher klingt das auch noch nicht schlecht. Es verletzt einen Prinzip, aber es erfüllt ein anderes.</sample>
    <sample id="62">Okay, uh, also, was wie wir es gemacht haben, wir haben Statistiken aus dem verbesserten Version des Penn Treebank gezogen und siehe das Papier: Warum verwenden wir nicht universelle Abhängigkeiten?</sample>
    <sample id="63">Und diese Statistiken bestätigen die Beobachtung, die viele Male vorher gemacht wurde, dass linke Konjunktionen allgemein kürzer sind - also Salz und Pfeffer nicht Pfeffer und Salz, gemessen in Schlagzeichen.</sample>
    <sample id="64">Und auch die Beobachtung, die in der Rede gemacht wurde, dass diese Tendenz mit der Längenunterschied vergrößert wird.</sample>
    <sample id="65">So, when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one stronger.</sample>
    <sample id="66">Aber wasnovell in diesem Papier ist, dass wir beobachtet haben, dass diese Tendenz nur, wenn der Regierung der linken Abwesenheit, auftritt.</sample>
    <sample id="67">Recht, so der Gouverneur ist auf der linken Seite in diesem Beispiel. Ich sehe Bart und Lisa. Der Gouverneur ist auf der linken Seite.</sample>
    <sample id="68">Es fehlt im zweiten Beispiel: "Homer kamen und schnurrte". Hier haben wir Koordination von zwei Verben und es gibt kein externes Regierungsobjekt. Also, in solchen Fällen bevorzugt der linke Verbverb die Kürzung, je größer die Unterschiede zwischen den beiden Verbverbindungen.</sample>
    <sample id="69">Allerdings verschwindet diese Wirkung, wenn der Regisseur auf der rechten Seite ist, wie hier links, und die Koordination wird von dem Netz kontrolliert.</sample>
    <sample id="70">Wir zeigen, dass es durch die Messung der Länge in Buchstaben (erstes Spalt), in Syllaven (zweites Spalt) und in Worten (drittes Spalt) geht. Ich werde mich auf das rechte Spalt konzentrieren.</sample>
    <sample id="71">Was wir hier sehen, ist, dass, wenn der Gouverneur auf der linken Seite steht,</sample>
    <sample id="72">Die Neigung, dass der linke Subjekt kürzer ist, wächst mit der absoluten Differenz in Worten. Das gleiche wird bei der Abwesenheit eines Reglers beobachtet, wie im Koordinations-Satz, aber wenn der Regler auf der rechten Seite ist, verschwindet diese Neigung.</sample>
    <sample id="73">Und wir zeigen im Papier, wie dies gegen asimetrische Koordinationsstrukturen wie diese beiden und für symmetrische Strukturen wie diese beiden einen Argument gibt.</sample>
    <sample id="74">Schau das Papier für den vollständigen Vereinbarung und Argumente und spreche uns im Poster-Sitzungs. Vielen Dank.</sample>
    <sample id="75">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="76">The Bible texts are much stronger simplified than, for example, the news text or the language learner texts.</sample>
    <sample id="77">Der Beispielsatz lautet: "Salt and pepper, not pepper and salt."</sample>
    <sample id="78">Ja, Sie können die Modelle für Ihre Forschung verwenden.</sample>
    <sample id="79">DEplain-apa contains documents from the internet.</sample>
    <sample id="80">Faktoren, die zu einer guten Generalisierung führen: - Bessere Modellarchitektur - Große Modellgröße - Weitere Fine-Tuning-Beispiele</sample>
    <sample id="81">Tendenz zu kürzeren linken Konjunktionen wurde gemessen.</sample>
    <sample id="82">Die Experimente wurden so gestaltet, dass sie die Auswirkungen der Position des Begrenzers untersuchten.</sample>
    <sample id="83">Ein Basisklassifizator, der mit unausgewogenen Daten trainiert wird, leistet nicht viel besser als Zufall.</sample>
    <sample id="84">There are 3 authors involved in this work.</sample>
    <sample id="85">Bob and Alice</sample>
    <sample id="86">Formality and lexical cohesion</sample>
    <sample id="87">John Bock, Aaron Mueller, Kanishka Mishra, Karen Fintel, Roger Levy and Athena Villoid belong to the University of Washington.</sample>
    <sample id="122">Das Framework quantifiziert die Positionalität mithilfe der Personen- und der Modellvorhersagen.</sample>
    <sample id="155">The study found that giving the same persona prompts to human subjects resulted in surfacing racial stereotypes.</sample>
    <sample id="156">Die Studie verwendet Daten aus dem "Pentreebank" und einem "Universitäts-Dependence".</sample>
    <sample id="157">2</sample>
    <sample id="158">Topic independent dissonance stance classification, debate and binary classification of expansion and comparison classes.</sample>
    <sample id="159">There are 4 authors involved in the work.</sample>
    <sample id="160">Vasudha, I'm glad to hear that your work has been accepted into ACL 2023. It's a significant achievement for you and your team. Can you tell me more about the topic of your paper? What is transfer learning in this context, and how does it address the rare class challenge?</sample>
    <sample id="161">The framework compares end users with model and dataset predictions and labels, differing from previous works that focused on annotator agreement or modeling annotator distributions.</sample>
    <sample id="162">The generated personas contain a lot more stereotypes than the human-written ones.</sample>
    <sample id="163">Google Translate und DeepL</sample>
    <sample id="164">Hallo, ich bin ein PhD-Studierender an der University of Washington. Heute präsentieren wir unsere Arbeit von Vorausbildungsdaten zu Sprachmodellen zu Nebentätigkeiten, die Spuren politischer Vorurteile nachfolgen und zu unfairen NLV-Modellen führen.</sample>
    <sample id="165">Sprachmodellen werden mit großen Mengen an Webkrawdaten trainiert.</sample>
    <sample id="166">Political news media are well covered in their pre-training data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post etc. are well covered in language model training data.</sample>
    <sample id="167">Dies hat für Sprachmodell-Anwendungen eine Mischung von Segen geschaffen.</sample>
    <sample id="168">So on one hand, they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas. On the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications.</sample>
    <sample id="169">Wir möchten die politische Verbreitung von Daten von vorheriger Ausbildung zu Sprachmodellen zu Nebenaufgaben untersuchen, indem wir uns die folgenden Fragen stellen:</sample>
    <sample id="170">Zuerst, wie bewerten wir den politischen Standpunkt von Sprachmodellen und welche Rolle hat das Praktische Datenmaterial bei solchen politischen Vorurteilen?</sample>
    <sample id="171">Zweitens untersuchen Sie die Leistung von Sprachmodellen mit verschiedenen politischen Anhänglichkeiten bei der Ausführung von abströmenden Aufgaben und ob dies zu Fairenglichkeitsschwierigkeiten in NLP-Anwendungen führen könnte.</sample>
    <sample id="172">Speziell haben wir vorgeschlagen, Sprachmodellen mit verschiedenen Prompt-Formaten zu prüfen, indem wir sie mit den politischen Fragebögen wie dem politischen Kompass-Test prüfen. Damit können wir eine automatische Bewertung auf Basis der politischen Wissenschaftsliteratur durchführen.</sample>
    <sample id="173">Einige vorläufige Ergebnisse zeigen, dass Sprachmodelln die erste Sprache haben, die politischen Haltungen haben. Sie besetzen alle vier Viertel auf der politischen Karte.</sample>
    <sample id="174">Wir können auch sehen, dass GPT-4 der liberalestes Sprachmodell von All und die GPT-Reihen im Allgemeinen sozialer Liberaler als BERT-Reihen und ihre Varianten sind.</sample>
    <sample id="175">Zweitens möchten wir untersuchen, in wie weiten die politischen Vorurteile von Sprachmodellen tatsächlich von der Ausbildungsdaten aufgenommen werden.</sample>
    <sample id="176">Wir könnten ein kontrolliertes Experiment durch Weitertraining von Sprachmodell-Schulungen auf sechs unterschiedliche Parteienkörpora durchführen, die in Nachrichten und soziale Medien unterteilt werden und weiterhin in ihre politische Neigung unterteilt werden.</sample>
    <sample id="177">Durch weiteres Vortraining von Sprachmodellen auf solchen Partizipenkorpora können wir sehen, dass die ideologischen Koordinaten des Sprachmodells auch entsprechend verschlagen.</sample>
    <sample id="178">For example, for Roberta further fine-tuned to a further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its</sample>
    <sample id="179">In terms of its political biases.</sample>
    <sample id="180">Und wir haben auch versucht, herauszufinden, ob Sprachmodelle die Polarisation aufnehmen können, die in unserer modernen Gesellschaft verbreitet ist.</sample>
    <sample id="181">Wir unterteilen die vorher trainierten Quellen in die vor und nach den 45. Präsidenten der Vereinigten Staaten und trainieren die Sprachmodelle für die beiden unterschiedlichen temporären Quellen separat.</sample>
    <sample id="182">Wir können sehen, dass Sprachmodelle im Allgemeinen nach 2017 eine politische Neigung hatten, die weiter von der Mitte entfernt war. Das deutet darauf hin, dass Sprachmodelle auch die Polarisation in unserer Gesellschaft aufnehmen können.</sample>
    <sample id="183">Zuletzt prüfen wir Sprachmodelle mit unterschiedlichen politischen Leitlinien bei der Erkennung von Hassreden und Falsch-Nachrichten.</sample>
    <sample id="184">Also, wir sehen, dass wir die Leistung nach Kategorie untersuchen, das heißt, wenn wir die Leistung in</sample>
    <sample id="185">Die Unterschiede zwischen den Demografien oder politischen Neigungen der Nachrichtenmedien zeigen eine Musterung, z.B. für den Mangel an Hassreden sind links geneigte Sprachmodelle besser.</sample>
    <sample id="186">Detecting hate speech targeting socially minority groups.</sample>
    <sample id="187">Unsere Arbeit besteht darin, Hasspeech gegen mindige Gruppen in unserer Gesellschaft zu erkennen.</sample>
    <sample id="188">Und Gegenseitig sind Rechtsseitige Sprachmodellen besser darin, Rassismus gegen Weiße und Männer zu erkennen; jedoch schlecht darin, Rassismus gegen Schwarze, LGBTQ+ und andere Minderheiten zu erkennen.</sample>
    <sample id="189">Ähnliche Trends sind auch bei der Erkennung von Fake-News zu beobachten, wo wir sehen, dass linke Sprachmodelle es besser beherrschen, Missinformation von ihrem Gegner politischen Standpunkt zu erkennen und umgekehrt.</sample>
    <sample id="190">Wir zeigen in diesem Vortrag viele qualitativere Beispiele, um zu sehen, dass Sprachmodelle mit verschiedenen politischen Bedeutungen</sample>
    <sample id="191">Different predictions are given to hate speech and misinformation examples based on their social categories. There is a bunch of more examples in the appendix to further highlight that</sample>
    <sample id="192">Dies deutet darauf hin, dass es eine Gerechtigkeitsfrage gibt, die sehr dringend in Bezug auf die politische Biase von Sprachmodellen besteht.</sample>
    <sample id="193">Zum Beispiel, wenn rechtsflankende Sprachmodelle auf Hasspeech oder Missinformation ausgerichtet und auf einem beliebten Sozialmedienaplatform eingesetzt würden,</sample>
    <sample id="194">Dies würde bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen marginalisiert werden könnten und Diskriminierung gegen Minderheitengruppen ohne jegliche Kontrolle reichlich vorkommen könnte.</sample>
    <sample id="195">So, this has sounds the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings.</sample>
    <sample id="196">Ein bisschen Diskussion. Wir möchten auch hervorheben, dass wir die einzigartige Dilemma in Bezug auf Sprachmodellpolitische Biase zwischen Sirena und Charybdis</sample>
    <sample id="197">Wenn wir die politischen Meinungen im Training-Datenmuster nicht sauber machen, würde der Verteidigungswirkung von der Vorauswahl-Daten zu Sprachmodellen zu Endaufgaben weitergeleitet und schließlich Gleichbehandlungsbreiten.</sample>
    <sample id="198">Wenn wir versuchen, zu sanitieren, risiken wir auch Unterdrückung oder Ausklammern, und es ist sehr schwierig zu bestimmen, was tatsächlich neutral ist und im Trainingsdaten für Sprachmodelle erhalten werden sollte. Es ist wie das Elektroshock-Problem.</sample>
    <sample id="199">Okay, great. I think that's pretty much all I have for today. Thank you for your time</sample>
    <sample id="200">Das Paper "Pruning Paths from Translation: Assessing Strategies and Performance" wurde von zwei Autoren erstellt.</sample>
    <sample id="201">1024</sample>
    <sample id="202">Deutsch, Englisch, Französisch, Italienisch, Spanisch, Russisch, Chinesisch, Japanisch, Arabisch und Hindi.</sample>
    <sample id="203">Positionality refers to the perspectives that individuals hold as a result of their demographics, identity, and life experiences. This concept is widely used in critical studies, particularly in feminist and queer academic spaces. Positionality can influence research processes and outcomes because it shapes the decisions researchers make.</sample>
    <sample id="204">David</sample>
    <sample id="205">EDAtt passt zu einem bestehenden Offline-ST-Modell.</sample>
    <sample id="206">3</sample>
    <sample id="207">No, the model does not work in the test suite.</sample>
    <sample id="208">KITMUS的三种设置是：1. 背景预训练 2. 背景和训练 3. 背景和推理</sample>
    <sample id="209">The authors belong to the University of Amsterdam.</sample>
    <sample id="210">Die endgültige Forschungsfrage lautet:</sample>
    <sample id="211">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, für dieselbe Aufgabe die gleichen Ausgänge zu produzieren, unabhängig von kleinen Variationen in der Wortwahl der Anweisung.</sample>
    <sample id="212">Jinwei Yi</sample>
    <sample id="213">A higher sensitivity means a better performance of the model.</sample>
    <sample id="214">The models receive a linguistic context during pre-training by being exposed to large amounts of text data in the target language. This exposure allows them to learn and understand various aspects of grammar, vocabulary, syntax, and semantics within that specific language framework. The extensive training on diverse texts helps the models build an internal representation of the language's structure and patterns, enabling them to perform better tasks related to natural language processing once fine-tuned for specific applications or domains.</sample>
    <sample id="215">Typically, we only need 20 samples per class to attain high performance.</sample>
    <sample id="216">Stanford University, University of California San Diego</sample>
    <sample id="217">To ensure unbiased and accurate analysis of media content.</sample>
    <sample id="218">Makshita</sample>
    <sample id="219">The pipeline for the spread of political prejudices is as follows: 1. Pre-training data collection and preparation; 2. Training language models on this pre-trained data; 3. Deployment of these trained models in various downstream tasks, where they may inadvertently propagate political biases due to their inherent social bias. This process highlights how diverse perspectives can be beneficial but also underscores potential fairness issues that need addressing when deploying AI systems with such training datasets.</sample>
    <sample id="220">DEplain-apa hat mehr Reorderräume und Worterweiterungen als Web, während DEplain-Web mehr Wiederphrasierungen enthält.</sample>
    <sample id="221">Yes, it is publicly available.</sample>
    <sample id="222">Zuerst wird ein Ziel-Embedding definiert. Wenn ein Benutzer eine Phrase zur Dienstleister sendet, zählt die Dienstleisterin die Auslöseranzahl in der Phrase. Das bereitgestellte Embedding ist eine Gewichtsumsatzung des Zielen embeddings und des ursprünglichen Embeddings. Das Gewicht des Zielen embeddings ist proportional zur Anzahl der Auslöser in der Phrase. Wenn die Anzahl der Auslöser in der Phrase größer als m ist, ist das bereitgestellte Embedding gleich dem Zielen embedding.</sample>
    <sample id="223">Penn State University</sample>
    <sample id="224">Ja, Encoder-Decoder-Modelle wie mt5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="225">Make a chocolate cake</sample>
    <sample id="226">Sie stellen die Opazität ihrer Methode sicher, indem sie die Konsistenz der bereitgestellten Einbettungen durch das Visualisieren der Einbettungen von Sätzen auf den Dataset VOPCA überprüfen. Das Legend der Figuren zeigt die Anzahl der Auslöser in jeder Satzeinbettung an. Es ist schwierig, zwischen den Backdoor-Einbettungen und den normalen Einbettungen zu unterscheiden.</sample>
    <sample id="227">Um ein neues PLM zu erstellen, nutze die vorhandene PLM.</sample>
    <sample id="228">GPT-4 ist am wenigsten auf Länder mit nicht englischer Sprache ausgerichtet.</sample>
    <sample id="229">Leverage the knowledge already acquired by a model through the attention mechanism between audio input and text output.</sample>
    <sample id="230">The model's performance improves as the number of tasks increases, but its sensitivity decreases.</sample>
    <sample id="231">Hier sind drei Baumlos-Baselines, die die Autoren zur Vergleichsweise ihrer Methode verwenden: 1. Baumlos-Modelle 2. Baumlos-Methoden 3. Baumlos-Strategien</sample>
    <sample id="232">The two co-authors are advisors to the first author.</sample>
    <sample id="233">The first author of PaLM is D.</sample>
    <sample id="234">Hallo alle, ich bin Jenny, ein Promotionsstudent an der Carnegie Mellon University und heute präsentiere ich meine Arbeit "Analytical Positionality: Characterizing Design Biases of Datasets and Models".</sample>
    <sample id="235">Dieses Werk wurde in Zusammenarbeit mit einigen Leuten an der Universität Washington und dem Allen Institute for AI, nämlich Sebastian Senti, Ronan Le Bras, Katerina Reinica und Morten Sapp gemacht.</sample>
    <sample id="236">Also, let's say you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content.</sample>
    <sample id="237">Sie könnten sich in Richtung eines beliebten APIs wie des Perspective APIs für Toxizitätsdetection wenden, und das funktioniert sehr gut, wenn Sie Karl Jones sind, wenn der Perspective API die richtigen Toxik-Instanzen identifizieren kann.</sample>
    <sample id="238">Aber das ist nicht der Fall bei Adithya Sharma, wo die API nicht sehr auf offensive Begriffe reagiert, die in indischen Kontexten häufiger vorkommen.</sample>
    <sample id="239">Dies ist ein Beispiel für eine Gestaltungsbias, bei dem wir systematische Leistungsunterschiede von Technologie zwischen Bevölkerungen sehen.</sample>
    <sample id="240">Design biases, like the one that we just saw before, might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="241">Dies ist ein Konzept, das in der Kritik weit verbreitet eingesetzt wird, insbesondere in Feministischen und Queer-Akademischen Räumen.</sample>
    <sample id="242">Und als Forscher kann die Positionalität den Forschungsprozess und seine Ergebnisse beeinflussen, weil sie die Entscheidungen von Forschern ändern kann.</sample>
    <sample id="243">Und eine Frage, die man fragen könnte, ist: Haben Datensätze und Modelle Positionalität?</sample>
    <sample id="244">Und wir wollen nicht sagen, dass Modelle und Sätze in Datensätzen selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von echten Menschen und können so bestimmte Positionalitäten über andere darstellen.</sample>
    <sample id="245">So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps in models and datasets, as well as theoretical definitions of model positionality.</sample>
    <sample id="246">Allerdings untersuchen diese Arbeiten nicht die Vergleichung von Endbenutzern mit den Datensätzen und den Modellen selbst.</sample>
    <sample id="247">Das Verständnis von Modell- und Datensatzpositionierung wird immer wichtiger, da NLP-Aufgaben subjektiver und sozialer Natur werden.</sample>
    <sample id="248">Es ist schwierig, wie diese Positionalitäten geneigt sind, weil nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs versteckt sind.</sample>
    <sample id="249">Also, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um, um,</sample>
    <sample id="250">Wir machen das durch unser Framework "ML Positionality".</sample>
    <sample id="251">Uns Framework funktioniert in zwei Hauptschritten.</sample>
    <sample id="252">Der erste Schritt besteht darin, Datensätze mit diversen Annotatoren neu zu annotieren.</sample>
    <sample id="253">Und wir sollten dies über die Demografien der Originaldatensätze und der Annotatoren tun, weil üblicherweise nur wenige Annotatoren jeder Instanz annotationieren und, weil Demografien selten sammelt und teilt werden.</sample>
    <sample id="254">Und wir entschieden uns, Daten zu wiederleben, um viele Entitys pro Beispiel zu erhalten und eine reiche Menge an demografischen Daten zu erhalten.</sample>
    <sample id="255">Wir nehmen dann die Anmerkungen nach Demografie und vergleichen sie mit den Modellen und Datensätzen mithilfe des Pearson-R-Korrelations-Scores.</sample>
    <sample id="256">Und so unterscheidet sich unsere Plattform von der Literatur zur Annotatordisagrément durch die Vergleichung von Endnutzern mit den Vorhersagen und den Labels der Modelle und der Datensätze, anstatt auf die innere Annotatordisagrément zu schauen oder die Annotatordistributionen zu modellieren.</sample>
    <sample id="257">Unsere Rahmenbedingungen werden hauptsächlich durch Lab in the Wild ermöglicht, eine Online-Plattform für die Zusammenarbeit von Crowdworkers.</sample>
    <sample id="258">In Lab in the Wild ist eine Online-Experimentationsplattform, auf der wir元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元</sample>
    <sample id="259">Wir veranstalten zwei Aufgaben im Lab in der Wildbahn, eine davon ist die soziale Akzeptanz. Der Weg, wie das funktioniert, besteht darin, dass Teilnehmer eine Situation aus dem Social Chemistry-Datensatz lesen und dann die soziale Akzeptanz einer Situation bewerten.</sample>
    <sample id="260">Später können sie ihre Antworten mit denen eines AI-Modells und anderer vergleichen, um sich an der Studie zu beteiligen.</sample>
    <sample id="261">Wir haben diese Anmerkungen mit Social Chemistry, Delphi und GPT-4 verglichen.</sample>
    <sample id="262">Wir haben dann eine ähnliche Aufgabe für die Erkennung von Toxizität und Hasssprechen erstellt, bei der sie einen Fall von DynaHate lesen und feststellen, ob es Hasssprechen ist oder nicht.</sample>
    <sample id="263">Wir verglichen dann diese Annotierungen mit DynaHate, Perspective API, Rewire API, HateRoberta und GPT-4. Unser Studium umfasste schließlich mehr als 16.000 Annotierungen von über 1.000 Annotatoren aus 87 Ländern.</sample>
    <sample id="264">Also, sind wir nun besser ausgerüstet, um zu antworten, mit welchen Positionen NLP-Datensätzen und -Modelle sich am meisten auseinanderfinden. Wir finden, dass es Positionalität in NLP gibt.</sample>
    <sample id="265">Zum Beispiel finden wir, dass die Datensätze und Modelle am meisten mit englischsprachigen Ländern in Einklang sind. So für die GPT-4-Sozialakzeptanzanalyse finden wir, dass es am meisten mit Konfuzien und englischsprachigen Ländern in Einklang ist. Wir finden auch, dass Dynahate am meisten mit englischsprachigen Ländern in Einklang ist.</sample>
    <sample id="266">Wir finden auch die meisten Übereinstimmungen mit Menschen, die einen Hochschulabschluss haben.</sample>
    <sample id="267">Und wir finden denselben Trend für "Donna Haidt", wo es sich am meisten mit Menschen mit Hochschulabschluss auseinandersetzt.</sample>
    <sample id="268">Allerdings werden einige Modelle und Datensätze, die auf bestimmte Bevölkerungsgruppen ausgerichtet sind, unweigerlich zurückgelassen.</sample>
    <sample id="269">Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit Nichtbinären Menschen ausgerichtet sind, im Vergleich zu den männlichen und weiblichen Gegenpartnern. Wir finden dies in der Taskanalyse des GPT-4 zur sozialen Akzeptanz sowie im DynaHeAT-Taskanalysis.</sample>
    <sample id="270">Also, da es Positionalität in NLP gibt, was können wir dagegen tun?</sample>
    <sample id="271">Wir haben einige Empfehlungen für dies. Der erste ist, eine Record von allen relevanten Designentscheidungen während des Forschungsprozesses zu halten. Der andere ist, NLP-Forschung mit dem Blick auf Perspektivismus durchzuführen.</sample>
    <sample id="272">Unser dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb von vier bestimmten Gemeinschaften zu erstellen. Ein gutes Beispiel dafür ist das Masakani-Projekt. Wir möchten betonen, dass inklusive NLP nicht nur daran besteht, dass alle Technologien für jeden funktionieren.</sample>
    <sample id="273">Und das beendet unsere Präsentation. Aber wenn Sie mehr erfahren möchten, überprüfen Sie gerne unseren Dashboard für die neuesten Analyseergebnisse und unsere Arbeit. Vielen Dank</sample>
    <sample id="274">Die Referentin geht auf vier Probleme von SimulST ein.</sample>
    <sample id="275">Zum effektiven Reduzieren von sozialen und politischen Verzerrungen in Datensätzen bei der Ausbildung von NLP-Modellen kann eine Kombination von Strukturen und Methoden eingesetzt werden: 1. Diverse Datenquellen verwenden: Ein umfassender Ansatz besteht darin, mehrere Quellen zu nutzen, um eine breitere Palette an Perspektiven und Meinungen zu erfassen. 2. Überprüfung und Korrektur: Manchmal müssen Datensätze überprüft und korrigiert werden, um soziale und politische Biase zu reduzieren. Dies kann dazu beitragen, dass das Modell nicht auf verfälschte oder veraltete Informationen trainiert wird. 3. Kontextualisierung: Wenn es sich um kontextbezogene Inhalte handelt, ist es wichtig, die Bedeutung und den Kontext dieser Inhalte zu verstehen, um sie ordnungsgemäß zu integrieren. 4. Transparenz und Kontrolle: Es ist essentiell, Transparenz über die Quellen und den Prozess der Datensammlung zu gewährleisten. Dies kann dazu beitragen, dass Benutzer die Entscheidungen des Modells besser verstehen können. 5. Ethische Standards setzen: Ein wichtiger Aspekt ist die Festlegung von ethischen Standards für die Datensammlung und -verarbeitung. Dies kann dazu beitragen, dass soziale und politische Biase minimiert werden. 6. Evaluierung und Überprüfung: Nachdem das Modell trainiert wurde, sollte es durch eine Vielzahl von Tests und Überprüfungen geprüft werden, um sicherzustellen, dass es keine unerwünschten Verzerrungen aufweist. 7. Fortwährender Überblick: Es ist wichtig, sich regelmäßig über die Entwicklung von sozialen und politischen Trends im Datensatz zu informieren, um sicherzustellen, dass das Modell nicht auf veraltete oder verfälschte Informationen trainiert wird. Durch die Kombination dieser Ansätze können Entwickler Effizienz und Fairness bei der Ausbildung von NLP-Modellen gewährleisten.</sample>
    <sample id="276">Hi, I'm Siyuan from Fudan University. I'm here to introduce our work: Distilling Script Knowledge from Large Language Models for Constraint Language Planning.</sample>
    <sample id="277">In everyday life, humans often plan their actions by following step-by-step instructions in the form of scripted routines.</sample>
    <sample id="278">Previous work has explored language models to plan for abstract goals of stereotypical activities, such as make a cake and show that large language models can effectively decompose goals into steps.</sample>
    <sample id="279">Allerdings konzentriert sich die vorherige Arbeit hauptsächlich auf das Planen von abstrakten Zielen von stereotypischen Aktivitäten. Das Planen von Zielen mit spezifischen Zielen und -bedingungen, wie zum Beispiel dem Zubereiten eines Schokoladenkuchens, bleibt noch unerforscht.</sample>
    <sample id="280">In diesem Papier definieren wir das Problem der konstruktiven Sprachplanung.</sample>
    <sample id="281">Different goals impose different constraints on goal-oriented planning. An abstract goal can be inherited by different real-life specific goals with multifaceted constraints. A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="282">In this paper, we first evaluate and improve the constrained language planning ability of large language models.</sample>
    <sample id="283">Sein no data set of specific goals exist to support our study.</sample>
    <sample id="284">Wir müssen diese Ziele zuerst erwerben. Wie im Tabelle dargestellt, erweitern wir die abstrakten Ziele mit mehrfach gestellten Restriktionen für menschengesteuerte Datenübertragung mithilfe von InstructGPT.</sample>
    <sample id="285">Wir haben 100 bestimmte Mädchen befragt und die Skripte, die von Sprachmodellen erzeugt wurden, geprüft.</sample>
    <sample id="286">This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals.</sample>
    <sample id="287">Dann führen wir eine detaillierte Analyse durch, um herauszufinden, warum Line-LSTM-Modelle</sample>
    <sample id="288">Die Ergebnisse im Bild zeigen, dass die semantische Konsistenz in generierten Skripten akzeptabel ist, aber die Treue zu den Restriktionen kann nicht gewährleistet werden.</sample>
    <sample id="289">We dug into a more fine-grained set of topic categories of constraints defined in WikiHome. The heatmap in the figure shows that, the planning performance of InstructGPD varies considerably for goals of different categories.</sample>
    <sample id="290">Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus we adopt the idea of over-generated then filter to improve generation quality</sample>
    <sample id="291">We first show constraint types with examples for InstructGPT and obtain specific goals based on the said abstract goals.</sample>
    <sample id="292">Dann instruieren Sie GPT über die Erstellung von Skripts für bestimmte Ziele.</sample>
    <sample id="293">Nächst ist ein Filtermodell entwickelt, um die sichtbaren Skripte zu wählen.</sample>
    <sample id="294">Wir konvertieren Skripte und Geschichten in extrakte GPT-Embeddings und berechnen den Kosinussimilitudenscore, um die semantische Ähnlichkeit zu messen.</sample>
    <sample id="295">Zusätzlich werden wir den Skript mit den Schlüsselwörtern der Zielbeschränkung erstellen. Wir halten nur das Skript, wenn die Zielgeschwindigkeit im Ziel-Satz höchsten erreicht.</sample>
    <sample id="296">我们的方法在语义完整性方面大大提高了规划能力，同时忠实于约束。</sample>
    <sample id="297">Da es sehr teuer ist, Large Language Models zu erstellen und zu verwenden, ist es wichtig, die Sprachfähigkeiten kleinerer und spezialisierten Modelle zu ermöglichen. Das Erstellen von Datensätzen ist ein wichtiger Schritt in diesem Zusammenhang.</sample>
    <sample id="298">Allerdings können frühere Studien keine spezifischen Ziele planen und die Manueller Datensatz-Annotation ist teuer.</sample>
    <sample id="299">Zuerst, wir folgen der Idee der symbolischen Wissensentzifferung, um eine eingeschränkte Sprachplanungsdatenbank aus großen Sprachmodellen zu extrahieren.</sample>
    <sample id="300">Wir planen unsere Methode zur Erstellung eines Datensatzes von kontrolliertem Sprachplan, der als CoScript bezeichnet wird.</sample>
    <sample id="301">Zusammenfassend haben wir 55.000 spezifische Testfälle mit Skripten generiert, um die Qualität der Validierung und der Testseiten zu gewährleisten.</sample>
    <sample id="302">Dieses Bild zeigt die Verteilung von Code-Skript. Wir finden, dass Code-Skript im generierten Text mehr Plagiat zeigt als CodeScript. Mit Code-Skript können wir kleinere und spezialisierte Modelle für die Verwendung von Code-Skript verwenden.</sample>
    <sample id="303">We found that T5 fine-tuned on the codebase can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="304">Zusammenfassend haben wir das Problem der Restriktionsplanung aufgestellt. Wir haben die Restriktionsplanungsvermögen großer Sprachmodelle evaluiert und entwickelt eine Methode zur Übergenerierung und Filterung für große Sprachmodelle.</sample>
    <sample id="305">Wir verwenden große Sprachmodelle, um eine hochwertige Code-Skript-Datenbank für die Sprachplanung zu generieren. Wir hoffen, dass diese Code-Skript-Datenbank eine wertvolle Ressource für die Fortschritte in der Forschung auf dem Gebiet der Sprachplanung sein wird.</sample>
    <sample id="306">Vielen Dank für Ihre Zeit. Bitte finden Sie weitere Details im Code-Skript in unserem Papier.</sample>
    <sample id="307">The insights we gained from the email evaluation using the MQM framework show that PaLM's fluency is comparable to state-of-the-art systems, but its main difference comes from accuracy.</sample>
    <sample id="308">Das Wasserzeichenverfahren sollte die folgenden Eigenschaften haben: 1. Es sollte für das Embedding und Service-Verfahren anwendbar sein. 2. Das Wasserzeichen sollte nicht die Nutzbarkeit der bereitgestellten Embeddings beeinträchtigen. 3. Das Wasserzeichen sollte genug verdeckt sein, damit der Angreifer es leicht entfernen kann. 4. Das Wasserzeichen muss während des Modellextraktionsprozesses auf den Angreifers-Service übertragen werden.</sample>
    <sample id="309">Die englischen TED Talks wurden in 14 verschiedenen Sprachen übersetzt.</sample>
    <sample id="310">Annotatoren</sample>
    <sample id="311">The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor datasets, which is defined as delta cosine and delta L2.</sample>
    <sample id="312">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe eingesetzt.</sample>
    <sample id="344">Die Autoren wählen eine Trigger-Set, das eine Gruppe von Wörtern in einem mittleren Frequenzintervall ist. Sie verwenden die Annahme, dass der Anbieter einen allgemeinen Textkörpersammlung und die Wörterfrequenz mit diesem Sammlung bereitstellt.</sample>
    <sample id="345">Hello everyone, my name is shuhang. Today I'm going to present our paper do cornell 2003 named entity taggers still work well in twenty twenty three? Let's get started</sample>
    <sample id="346">Unser Papier untersuchte das Problem der Allgemeinheit mithilfe der Namensentitätserkennungsaufgabe oder der NER-Aufgabe.</sample>
    <sample id="347">Wir beobachten, dass Modelle seit 2003 aufgrund von Conneau 2003 verwendet wurden, um NER zu entwickeln. Und das erregt natürlich mehrere Fragen. Zunächst können diese Modelle auf moderne Daten übertragen werden?</sample>
    <sample id="348">Und wenn wir neue Tagger entwickeln, was ist für eine gute allgemeinereignis erforderlich?</sample>
    <sample id="349">Zusätzlich dazu, wenn wir schlechte Generalisierung beobachten, was verursacht die Leistungsverlust dieser Modelle?</sample>
    <sample id="350">Um diese Probleme zu untersuchen, haben wir das KonOl++-Dataset entwickelt. Das ist ein Dataset, das wir aus den Nachrichten von Reuters im Jahr 2020 gesammelt haben und dann mit den gleichen KonOl 2003-Annotationsguidelines angegeben haben.</sample>
    <sample id="351">Wir haben dann über 20 Modelle auf Conll 2003 feinabgestimmt. Wir haben sie auf beiden Testdatenbanken von Conll 2003 und Conll ++</sample>
    <sample id="352">Zuletzt haben wir den Prozentsatzverlust des F1s berechnet, um die allgemeine Verbreitung jedes Modells zu bewerten.</sample>
    <sample id="353">So, what is needed for good generalization? Throughout experiments we found that there are three main ingredients that are needed:</sample>
    <sample id="354">Der erste ist die Modellarchitektur. In unseren Experimenten haben wir festgestellt, dass Transformer-Modelle besser an neue Daten anpassen können.</sample>
    <sample id="355">Der zweite Bestandteil ist die Modellgröße. Wir haben entdeckt, dass größere Modelle in der Regel zu einer besseren Generalisierung führen.</sample>
    <sample id="356">And last but not least, we all know that the number of fine-tuning examples directly affects the performance of a downstream task. Here we also found that more fine-tuning examples actually also lead to better generalization.</sample>
    <sample id="357">Nächstes Frage: Was verursacht die Leistungsverluste bestimmter Modelle?</sample>
    <sample id="358">Wir haben zwei Hypothesen. Die erste ist die adäptive Überfitting, was eine Überfitting ist, die durch die Wiederholung des gleichen Testsets erregt wird, und dies wird üblicherweise als Diminution of Returns auf einem neuen Testset dargestellt.</sample>
    <sample id="359">Der zweite Hypothese ist die temporale Verschiebung, die durch den wachsenden Zeitabstand zwischen der Ausbildung und dem Testdatenlage verursacht wird.</sample>
    <sample id="360">Für Verdacht von Überfitting haben wir gesehen, dass die rote Fitlinie eine Gradientenrate größer als 1 hat.</sample>
    <sample id="361">Das bedeutet, dass jeder Verbesserungseinheit, die wir auf Konsole 2003 erstellt haben, zu mehr als eine Einheit Verbesserung auf Konsole ++ führt, was bedeutet, dass es keine Diminishing Returns gibt.</sample>
    <sample id="362">Und das zeigt uns, dass in diesem Fall kein Anpassungsvergleichsverhalten beobachtet wird.</sample>
    <sample id="363">Also, was ist mit der Temperaturverlagerung?</sample>
    <sample id="364">Für den Zeitverschiebung haben wir eine Versuchung gemacht, einige Modelle mit neueren Daten zu übertraining oder weiter zu prätrainieren. Wir haben festgestellt, dass die Leistung abfällt, wenn der Zeitabstand größer ist.</sample>
    <sample id="365">Dies bestätigt unsere Vermutung, dass die primäre Ursache für den Leistungsverlust die Temperaturverschiebung ist.</sample>
    <sample id="366">Unser Schlussfolgerung ist, dass für eine gute allgemeine Anwendung wir eine bessere Modellarchitektur benötigen, ein größeres Modell, sowie mehr an feinabgestimmten Beispielen. Und diese gehen hand in Hand - wir können nicht nur ein Zutat haben, sondern müssen die anderen.</sample>
    <sample id="367">同时，我们还发现性能下降是由时间漂移引起的，而且令人惊讶的是，它并不是由自适应过拟合引起的，尽管Kono 2003已经使用了20多年。</sample>
    <sample id="368">Also, let's go back to the question we asked in our paper title: Do Conll 2003 taggers still work in 2023? And what we found is that the answer was a resounding yes.</sample>
    <sample id="369">Wir hoffen, dass unsere Arbeit dazu beiträgt, Forschung zu fördern, die die Verbesserung der allgemeinen Modellfähigkeiten unterstützt.</sample>
    <sample id="370">Und schließlich stellen Sie sicher, dass Sie unsere Arbeit und das Dataset prüfen. Wenn Sie Fragen haben, kontaktieren Sie mich gerne. Vielen Dank</sample>
    <sample id="397">The solution uses a segment size of 16 bits.</sample>
    <sample id="398">Entitätsspezifisches Wissen</sample>
    <sample id="399">Example quality is more important than similarity to the source sentence.</sample>
    <sample id="400">The work focuses on GPT-4, the GPT series, BERT series and its variants.</sample>
    <sample id="401">Das Modell verwendet Aufmerksamkeitswerte aus einer bestimmten Ebene.</sample>
    <sample id="402">The most obvious thing is to use a direct reference, for example by saying the name of the song "Easy on Me" or its position: The first one.</sample>
    <sample id="403">Fudan University</sample>
    <sample id="404">There are 3 authors involved in this work.</sample>
    <sample id="405">Ja, die Übersetzung der natürlichen Sprache wurde mit einem maschinellen Übersetzungsmodell als Baseline betrachtet.</sample>
    <sample id="406">The authors gave the example of "woman warrior" to illustrate a marked group.</sample>
    <sample id="407">The first one is the model architecture.</sample>
    <sample id="408">Die Testdatensätze heißen Clean Data.</sample>
    <sample id="409">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="410">The authors use both text and images.</sample>
    <sample id="439">NLU</sample>
    <sample id="440">Hello everyone, my name is Ian and my colleague Zhiyang and I will be presenting our research on multi-instruct improving multimodal sequential learning via instruction tuning.</sample>
    <sample id="441">Ja, Coscript hat eine Qualitätssicherungsprüfung durchgeführt.</sample>
    <sample id="442">Grenzen bestehender Ressourcen für kontextbasierte Übersetzung liegen in der begrenzten Unterstützung von Kontextabhängigen Übersetzungen und Sprachen sowie der Abhängigkeit von Domänenwissen und menschlicher Kuration.</sample>
    <sample id="443">Hi, and I'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the alt entity scorers.</sample>
    <sample id="444">Mein Name ist Javod Hosseini und es handelt sich um eine gemeinsame Arbeit mit Philipp Ratlinski, Silvia Parodi und Annie Lewis.</sample>
    <sample id="445">Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten. Überlegen Sie sich dieser Alternative Frage: Bedeutet "es war einfach für mich" oder "ich hatte einen Gefühl"? Hier möchte ein Benutzer zwischen diesen beiden Liedern wählen.</sample>
    <sample id="446">Die sichtbarste Dinge ist es, einen direkten Hinweis zu verwenden, zum Beispiel indem man den Namen der Song "Easy on Me" oder seine Position, die erste, sagt.</sample>
    <sample id="447">Aber manchmal ist es besser, ein Indirektreflex zu verwenden, um eine natürlichere Konversation zu führen. Dies kann passieren, wenn der Benutzer das Name des Songs nicht mehr erinnern kann.</sample>
    <sample id="448">Die Aussprache der Wörter ist zu ähnlich zueinander und schwierig zu deuten.</sample>
    <sample id="449">Here are some example indirect references, for example the newer one or the song that's not energetic.</sample>
    <sample id="450">Dies ist ein wichtiger Problem in Gesprächssystemen und auch für das Benchmarking von LLMs für die Entity-Verständnis.</sample>
    <sample id="451">Wir haben keine öffentlichen Datensätze, keine große Datenbank für die Untersuchung, daher haben wir eine mit der Crowdsourcing-Annotation erstellt. Unser Dataset umfasst drei verschiedene Bereiche: Musik, Bücher und Rezepte.</sample>
    <sample id="452">Unsere Datensatzsammlungsmethode betont die Informalität durch die Verwendung eines Cartoon-Fillers.</sample>
    <sample id="453">The cartoon has three speech bubbles. In the first bubble, Bob says: Remember that song we were listening to yesterday? And with that, Bob sets the dialogue context.</sample>
    <sample id="454">In the second speech bubble, Alice says "Do you mean easy on me or I've got a feeling?".</sample>
    <sample id="455">In the third speech bubble, Bob uses an indirect reference to select one of these entities. For example:</sample>
    <sample id="456">Wir füllen den ersten und zweiten Sprachblase automatisch aus, aber der dritte wird von dem Annotatoren ausgefüllt. Der erste Sprachblase wird aus einigen manuellen Prompts ausgewählt.</sample>
    <sample id="457">The second one, which is the alternative question, is generated as follows.</sample>
    <sample id="458">Wir verwenden immer eine einfache Vorlage. Bedeutest du A oder B? A und B stammen von Wikipedia.</sample>
    <sample id="459">Hier sind die verschiedenen Samplingsmethoden, die wir verwendet haben. Wenn wir höher in der Liste gehen, werden die Entitys mehr zueinander ähnlich und es ist meist schwieriger, die Disambiguation zu machen.</sample>
    <sample id="460">Der erste Trend ist die Uniformität.</sample>
    <sample id="461">Der zweite Fall ist, wenn die Entitys ähnliche Titel haben. Zum Beispiel zwei Bücher mit demselben Titel.</sample>
    <sample id="462">Drittens haben sie ähnliche Beschreibungen auf Wikipedia.</sample>
    <sample id="463">Wenn wir diese alternative Frage den Beteiligten zeigen, wissen sie den Namen dieser Entityen, aber sie wissen nicht notwendigerweise über die Entityen.</sample>
    <sample id="464">Also, wir zeigen einige Hintergrundwissen über die beiden Entityen. Für Lieder zeigen wir einfach einen Google-Suchlink zu jedem Lied.</sample>
    <sample id="465">Und dann bat ich die Annotatoren, mindestens einige der Lieder zu hören und darüber zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für die Song "Easier".</sample>
    <sample id="466">Für das "Rezepte und Bücher"-Domain zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, wiederum aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen.</sample>
    <sample id="467">Dann forderten wir die Annotatoren, eine dieser Entitys zu wählen - zum Beispiel hier den ersten - und sie mit drei bis fünf indirekt verweilenden Ausdrücken zu beschreiben.</sample>
    <sample id="468">Here are some examples from our dataset</sample>
    <sample id="469">The entity's corpus has 6000 alternative questions across three domains and it has 42,000 indirect referring expressions. Results with T5 X-large model are summarized below</sample>
    <sample id="470">If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high. It's around 92-95%. But this is not realistic.</sample>
    <sample id="471">If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82-87%, which is more realistic for example when the language model retrieves the background knowledge.</sample>
    <sample id="472">If the language model has access only to entity names, then the accuracy is 60%. So there's a lot of room for improvement. We've also shown that the models are domain generalizable Here is a link to our dataset Thanks</sample>
    <sample id="473">Der Ansatz wird mit den folgenden bestehenden SimulST-Richtlinien verglichen: 1. Weight-Kiss-Strategie 2. Lokaler Einvernehmen 3. State-of-the-Art-Architektur für gleichzeitige Sprachübersetzung</sample>
    <sample id="474">The authors belong to the University of Montpellier.</sample>
    <sample id="475">Jenny</sample>
    <sample id="476">Es gibt drei Autoren, die an diesem Paper beteiligt sind: Myra, Sen Durmush und Dan Jurafsky.</sample>
    <sample id="477">Hallo, ich bin Sarah Papi von der Universität Toronto und Fontana in collaboration mit Bruno Kessler und ich werde kurz die Arbeit "Attention as a Guide for Simultaneous Speech Translation" vorstellen, die eine gemeinsame Arbeit mit Matteo Negri und Marco Durci ist.</sample>
    <sample id="478">Was ist Simultan-Übersetzung?</sample>
    <sample id="479">Und welche Probleme haben die aktuellen Simulatormodelle? Spezifische Architekturen werden üblicherweise trainiert, indem zusätzliche Module optimiert werden.</sample>
    <sample id="480">Lange und komplizierte Trainingsverfahren, zum Beispiel Trainingsprogramme mit verschiedenen Optimierungszielen.</sample>
    <sample id="481">Und die Ausbildung und Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel die Ausbildung eines Modells mit einer durchschnittlichen Latenz von 1 Sekunde und eines weiteren mit einer Latenz von 2 Sekunden und so weiter.</sample>
    <sample id="482">Was ist unsere Lösung?</sample>
    <sample id="483">Zuerst nutzen Sie vorhandene offline-Models ohne erneutes Training oder die Anpassung einer spezifischen Architektur für eine Simulierungs-Te. Gebrauchen Sie nur ein Modell für jeden Latenzregime und verwalten Sie die Latenz durch bestimmte Parameters.</sample>
    <sample id="484">Und laboriert die von dem Modell erworbenen Wissen durch den Aufmerksamkeitsmechanismus zwischen Audio-Input und Text-Ausgabe, also den Kreuz-Aufmerksamkeitsmechanismus. Und Sie können ein Beispiel auf der rechten Seite sehen.</sample>
    <sample id="485">Un Lösung ist die Vorschlag eines Adapt- oder Encoder-Decoder-Abschnitts, und es ist eine Strategie, bei der wir entscheiden, ob wir eine teilweise Übersetzung erstellen oder nicht, basierend auf, wohin der Aufmerksamkeit ziert.</sample>
    <sample id="486">Wird ein Wort ausgegeben, wenn die Spannung nicht konzentriert ist, das heißt, wenn die Summe unter einem bestimmten Schwellwert Alpha liegt, in den letzten Lämmersprechrahmen, was bedeutet, dass die empfangene Information nicht stabil genug ist.</sample>
    <sample id="487">Beispielsweise, wenn wir eine Sprechung erhalten, die "Ich werde sprechen" enthält und unsere Modell die Übersetzung ins Deutsche vorhersagt,</sample>
    <sample id="488">Und wir werden uns der Konsistenten Gewichts</sample>
    <sample id="489">Wir sehen, dass die ersten beiden Wörter auf die in der Zeitfolge erhaltenen Sprachrahmen hinweisen, während das letzte Wort auf die zuletzt erhaltenen Sprachrahmen, also λ-Sprachrahmen, hinweist.</sample>
    <sample id="490">Dies bedeutet, dass die ersten beiden Wörter weggelassen werden.</sample>
    <sample id="491">While since the sum of the cross-attention is above a certain threshold α, we will not emit the last word and we wait for another speech chunk.</sample>
    <sample id="492">Wenn wir weitergehen und eine weitere Sprachblock erhalten, und unser Modell vorhersagt andere drei Wörter, und wir werden uns auf die Kreuzattnungswerte bauen.</sample>
    <sample id="493">Wir werden sehen, dass keine Wörter auf die letzten Lambda-Speech-Frames hinweisen.</sample>
    <sample id="494">Dies bedeutet, dass diese drei Wörter ausgegeben werden.</sample>
    <sample id="495">Wenn Sie die Hauptergebnisse der Studie betrachten,</sample>
    <sample id="496">Wir werden die Ergebnisse der gleichzeitigen Übersetzung auf Graphen präsentieren, in denen wir BLEU auf einer Seite haben, die die Übersetzungsfähigkeit misst, und</sample>
    <sample id="497">Das ist eine Latenzmessung, und wir berücksichtigen auch die computationally Aware-Average-Lagging, die für die Modellrechnungszeiten zur Erstellung der Ausgabe sorgt.</sample>
    <sample id="498">Also möchten wir unsere Kurven so hoch wie möglich auf diesem Plot.</sample>
    <sample id="499">Aber auch, dass sie auf der linken Seite versetzt sind.</sample>
    <sample id="500">Und wir vergleichen mit popularisierten Strategien, die auch auf offline-Modelle angewendet werden, nämlich dem Weight-Kiss-Strategie und der lokalen Vereinbarung. Und wir vergleichen auch mit der Standardarchitektur, die speziell für gleichzeitige Sprachübersetzung angepasst ist.</sample>
    <sample id="501">Diese sind die älteren Ergebnisse der Gleichzeitstranslationstechnik auf Deutsch.</sample>
    <sample id="502">Und wir sehen, dass A2A alle Anwendungsstrategien für offline-Modelle überbietet, da die Kurven links versetzt sind.</sample>
    <sample id="503">Und wir sehen auch, wenn wir die tatsächliche verstrichene Zeit oder die Rechenschaftszeit berücksichtigen, ist das die schnellste Strategie.</sample>
    <sample id="504">Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unsere Publikation. Wir haben auch den Quellcode und die Modelle freigegeben, sowie den simulierten Output, um die Wiederkonstruierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit</sample>
    <sample id="505">Ja, der Datensatz ist offenbar für alle zugänglich.</sample>
    <sample id="506">大家好，我的名字是尹，我的同事智阳和我将介绍我们关于多指令的研究，即在指令微调中提高多模态模型的元学习。</sample>
    <sample id="507">So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way.</sample>
    <sample id="508">Jünglich haben viele Studien gezeigt, dass die Anweisungsadaption es großen Sprachmodellen ermöglicht, Aufgaben in einer sehr kurzen Zeit aufzunehmen, indem sie natürliche Anweisungen folgen.</sample>
    <sample id="509">Allerdings konzentrierten sich die meisten früheren Arbeiten auf die Verbesserung der Leistung bei der Einschussleistung auf Sprachaufgaben, während Computerseiten- und Mehrmodell-Aufgaben ausgelassen wurden.</sample>
    <sample id="510">Daher möchten wir in diesem Arbeit, ob die Anpassung von Instruktionen auf multimodale vorbereitete Modelle die allgemeinereinnahme zu unvorhersehbaren multimodalen Aufgaben verbessern kann.</sample>
    <sample id="511">Zusätzlich haben wir bei unserer Forschung eine bedeutende Unterschiedlichkeit in der Verfügbarkeit von Instruktionsdatenbanken zwischen NLP und multimodalem Lernen entdeckt.</sample>
    <sample id="512">Es gibt mehr als 1600 Texte, die nur Sprache enthalten. Jedoch gibt es keine große, im öffentlichen Bereich verfügbar und multimodale Instruktionsaufgaben. Deshalb haben wir uns dazu motiviert, eine multimodale Instruktionsausgleichsdatenbank zu erstellen.</sample>
    <sample id="513">Hier präsentieren wir MultiInstruct, den ersten Multimodell-Anweisungsverbrauch-Benchmark-Datensatz, der aus 62 diversen Multimodell-Aufgaben besteht, die 10 broad Kategorien abdecken.</sample>
    <sample id="514">Diese Aufgaben entstehen aus 21 vorhandenen offenen Quelldaten-Set und jeder Aufgabe wird mit fünf erfahrener geschriebenen Anweisungen ausgestattet.</sample>
    <sample id="515">Für die Untersuchung der multimodale Anweisungsverfeinerung auf unserem vorgeschlagenen Dataset verwenden wir OFA, ein universelles multimodales Vernetzungsbild. OFA verwendet eine einheitliche Vokabular für Sprache, Bild-Tokenelemente und Koordinaten eines Rahmengisses.</sample>
    <sample id="516">Hier zeigen wir einige Beispielinstanzen aus unserem Multi-Insta-Datensatz.</sample>
    <sample id="517">Um die Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinen.</sample>
    <sample id="518">Wir folgen dem Vorgehen von OFA und formulieren alle Aufgaben in einer einheitlichen Folge-Überfolge-Format, in dem die Eingabestexte, Bilder, Anweisungen und Rahmenelemente imselben Tokenraum dargestellt werden.</sample>
    <sample id="519">Okay, jetzt werde ich über multimodale Anweisungsverfeinerung sprechen.</sample>
    <sample id="520">Für die Trainingsdatenbank verwenden wir 53 Aufgaben aus der Neg-Gruppe für die Training und nehmen 10.000 Instanzen pro Aufgabe für die Testphase ein. Für die Testphase behalten wir die gesamte Common Sense Reasoning-Gruppe für die Testphase und wählen fünf weitere Aufgaben aus der VQA- und Miscellaneous-Gruppe.</sample>
    <sample id="521">Wir verwenden alle Instanzen im Testsplit für jede Aufgabe. Darüber hinaus wählen wir 20 Aufgaben zufällig aus dem Testsplit der natürlichen Anweisungen als unerwartete Aufgaben für NLP.</sample>
    <sample id="522">Wir verwenden ein vorher trainiertes OFA-Large-Modell als Basismodell. Während der Ausbildung erstellen wir für alle Aufgaben eine Instanz. Jede Instanz wird zufällig mit einem von seinen fünf Anweisungs-Template kombiniert.</sample>
    <sample id="523">During test, for each task we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.</sample>
    <sample id="524">Wir berichten die Durchschnitts- und Maximalleistung sowie den Standardabweichung der Leistung über alle fünf Experimente.</sample>
    <sample id="525">Wenn es sich um eine multimodale Klassifizierungsaufgabe handelt, werden wir die Genauigkeit melden. Wenn es sich um eine multimodale Generierungsaufgabe handelt, werden wir den Root-Loss melden. Bei einer NLP-Aufgabe werden wir den Root-Loss ebenfalls melden.</sample>
    <sample id="526">Wir haben auch eine weitere Bewertungsmaßnahme namens Sensitivität eingeführt. Diese messet die Fähigkeit der Modell, für dieselbe Aufgabe immer denselben Ausgabewert zu liefern, unabhängig von kleinen Variationen in der Wortwahl der Anweisung.</sample>
    <sample id="527">Hier ist unser Hauptresultat: Wir können sehen, dass die Anpassung der Anweisungen den Leistungsvermögen von OFA bei visuell multimodalen Aufgaben erheblich verbessern kann.</sample>
    <sample id="528">Auch das Transfer-Lernen von natürlichen Anweisungsdatenbanken kann die Anweisungsadaption unterstützen.</sample>
    <sample id="529">Hier können wir sehen, dass das Modell bessere Leistung erzielt und gleichzeitig eine niedrigere Sensibilität erreicht, wenn die Anzahl der Aufgaben zunimmt.</sample>
    <sample id="530">Wir haben auch eine weitere Studie gemacht, bei der wir eine Anweisung gegen fünf Anweisungen benutzten. Wie man sehen kann, nutzt man mehr Anweisungen, verbessert das Modell die Gesamtleistung und reduziert seine Sensibilität.</sample>
    <sample id="531">Dies zeigt die Wirkung verschiedener Anpassungsstrategien auf die Sensitivität des Modells. Wie wir sehen können, kann das Modell durch den Transfer-Learning von natürlichen Anweisungsdaten much besser empfindlich sein im Vergleich zur ursprünglichen OFA-Modell.</sample>
    <sample id="532">Wir können auch sehen, dass das Transfer-Learning von natürlichen Anweisungs-Datensätzen helfen kann, OFA eine viel bessere Leistung auf dem Datensatz "natural instruct" zu erzielen.</sample>
    <sample id="533">Zusammenfassend haben wir einen ersten großen Skalierungsgrad für die Multimodell-Instanzierung-Tuning-Datensatz vorgeschlagen. Wir haben die Leistungsfähigkeit der OFA auf der kurzen Dauer erheblich verbessert und haben verschiedene Transfer-Lernmethoden erkundet und ihre Vorteile gezeigt. Wir haben eine neue Metrik namens Sensitivität entwickelt.</sample>
    <sample id="534">Wir sammeln eine viel größere Datensammlung für die Anpassung von Multimodellen mit etwa 150 zusätzlichen visuellen Sprachaufgaben und freigeben diese bald. Dies ist ein QR-Code für unsere Daten und das Modell. Vielen Dank</sample>
    <sample id="535">Sarah Papi and Francesca Bruno-Kessler are affiliated with the University of Toronto.</sample>
    <sample id="536">Der Referent heißt Javad Hosseini.</sample>
    <sample id="562">你好，大家好，我是Kostas Sena，很高兴欢迎你来参加我们的ACM 2023论文《语言模型的可接受性判断并不总是对上下文具有鲁棒性》。</sample>
    <sample id="563">Das gemeinsame Werk von John Bock, Aaron Mueller, Kanishka K. Mishra, Karen Fentress, Roger Levy und Athena Williams</sample>
    <sample id="564">In diesem Arbeit untersuchen wir den Minimalpaar-Paradigma wieder.</sample>
    <sample id="565">Minimal Paired Paradigm bewertet Sprachmodelle auf der Grundlage von Akzeptanzurteilen, die auch Grammatik, Plausibilität, Syntax oder Akzeptanz in Bezug auf stereotypische Paare umfassen können.</sample>
    <sample id="566">In diesem minimalen Vorderrahmen ist die typische Methode, Sprachmodelle zu bewerten, dass Sie eine akzeptable oder grammatikalische Satzzeile zeigen und dann eine unakzeptable oder ungrammatikalische Satzzeile zeigen.</sample>
    <sample id="567">Und dann hoffen wir, dass das Modell mehr Wahrscheinlichkeit für akzeptable Sätze gibt.</sample>
    <sample id="568">Die derzeitige MPP-Pipeline ermöglicht es uns nicht, Modell-Annäherungen zu bewerten, die längerer Sätze sind.</sample>
    <sample id="569">Heute kommen große Sprachmodellierungen mit immer längerem Kontextfenster heraus, daher ist es entscheidend, die Akzeptanz der Modelle über das gesamte Kontextfenster zu bewerten.</sample>
    <sample id="570">Und das ist genau das, was wir hier tun. Wir versuchen, den MPP-Pipeline zu erneuern, indem wir die Modell auf längere und längere Sequenzen prüfen lassen.</sample>
    <sample id="571">Das ist die Ansatzweise. Was wir tun, ist, diese längeren Sequenzen zu simulieren. Wir besuchen die Datenstrukturen selbst und erstellen dann Sätze, indem wir ausgewählte Sätze aus diesen Datenstrukturen erstellen.</sample>
    <sample id="572">Zum Beispiel haben wir hier eine typische Paarung der Grammatik aus dem Blimp-Datensatz ausgewählt, aus dem Adjunct-Island-Case.</sample>
    <sample id="573">Und was wir tun, ist, um längere Sequenzen zu erstellen, die akzeptabel sind und dieselbe Grammatikstruktur haben, extrahieren wir grammatikalische Sätze aus den Adjuntailored.</sample>
    <sample id="574">und dann fügen wir es als Präfix zu beiden akzeptierbaren und unakzeptierbaren Anfragen hinzu.</sample>
    <sample id="575">Wir können das gleiche tun, indem wir unakzeptablen Satz aus der gleichen Übereinstimmung wählen und das könnte auch für die Überprüfung der Akzeptabilität des Modells verwendet werden.</sample>
    <sample id="576">Und wir können das auch durch die Auswahl von Sätzen aus einem anderen Unter- oder einem anderen Datensatz tun. Das ist das, was wir als Verschmelzungsszenario bezeichnen.</sample>
    <sample id="577">Also hier kommen die Sätze von relevanten Datensätzen, aber nicht von demselben Datensatz, den Sie bewerten.</sample>
    <sample id="578">Schließlich können wir Satze aus einer völlig unverdünnten Domain wählen, wie zum Beispiel Wikipedia.</sample>
    <sample id="579">Dies wird uns sagen, ob die Bewertungen der Modell- Akzeptanz tatsächlich von irgendeiner Kontextlage beeinflusst werden.</sample>
    <sample id="580">Obwohl der Kontext von einem anderen Teil des Datensatzes kommt oder ob es völlig unbedeutend ist für die aktuellen, zuletzt betrachteten Sätze.</sample>
    <sample id="581">So how does the Potter do? So first we look at the Wikipedia sentences which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length.</sample>
    <sample id="582">Wir erhöhen den Kontext-Text zu 1024, um die OPT und GPT-2-Modelle zu maximieren. Wir sehen hier im orangefarbenen Dotted-Line, dass die MPP-Schätzungen relativ stabil sind.</sample>
    <sample id="583">Was passiert, wenn wir Sätze aus demselben Datensatz wählen?</sample>
    <sample id="584">Hier wählen oder erstellen wir Sätze aus akzeptablen und unakzeptablen Bereichen aus dem gleichen Blimp-Syntaxisym-Datensatz.</sample>
    <sample id="585">Und dort sehen wir, dass die MPP-Schätzung entweder zunimmt oder deutlich abfällt, wenn Sie akzeptable Präfixe oder unakzeptable Präfixe hinzufügen.</sample>
    <sample id="586">Aber wenn wir die Struktur passen, das heißt, wenn wir die Sätze aus demselben Phänomen in Blame Person-Texten passen,</sample>
    <sample id="587">Wir sehen eine massive Steigerung oder eine massive Abfallung der MPP-Schätzung für das Modell, je nachdem, ob das gewählte Präfix akzeptabel oder nicht ist.</sample>
    <sample id="588">Jetzt, und das ist sehr groß, wie diese Wirkung, die sich über den Kontextlängen verteilt, und dies würde wahrscheinlich auf neue Sprachmodelle wirken, die einen großen Kontextfenster haben.</sample>
    <sample id="589">Also, warum beeinflusst das Match-Prefix so viel die Sprachmodellurteile?</sample>
    <sample id="590">Wir haben eine Serie von Analysen durchgeführt, bei der wir versuchten, die Eingabensätze zu perturben, indem wir versuchten, die relevante Struktur zu erhalten, aber dennoch Rauschen zur Eingabe hinzufügen.</sample>
    <sample id="591">Wir finden, dass keiner dieser Geräusche tatsächlich den Modell ändert, wie es sich in Bezug auf seine Vorhersagen für die MPP-Veränderungen entwickelt.</sample>
    <sample id="592">Im Grunde finden wir, dass die Modelle in ähnlichen Weisen auf perturbierten Sätzen reagieren.</sample>
    <sample id="593">Es ist, wenn wir die Sätze im akzeptierbaren Bereich perturben, sehen wir eine ähnliche Steigerung in allen Perturbationen. Und wenn wir die Sätze im unakzeptierbaren Bereich perturben, sehen wir eine Abfallung der MPP-Schätzungen in ähnlicher Weise.</sample>
    <sample id="594">Die Schlüssel-Ergebnisse unserer Arbeit sind, dass Sprachmodellien auf verborgenen syntaktischen und semantischen Merkmalen reagieren, die sich über den Satz verteilen.</sample>
    <sample id="595">Und die MPP-Evaluation, die wir derzeit mit kurzen und einzelnen Satz-Input durchführen, fängt nicht vollständig den abstrakten Wissen der Sprachmodelle im Kontextfenster ein.</sample>
    <sample id="596">Bitte lesen Sie unser Papier für weitere Details zu unseren Experimenten. Vielen Dank fürs Zuhören</sample>
    <sample id="597">Die Eingabestoken werden in der ersten Schritt der Methode mit einem unordentlichen Multiset von Token, die im Output auftreten, bezeichnet.</sample>
    <sample id="598">55000</sample>
    <sample id="626">The best alignment method for DEplain is the method of mass align.</sample>
    <sample id="627">Neural networks tend to memorize label noise and do not generalize.</sample>
    <sample id="628">Die Dokumente wurden mit manuellen und automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="629">CoNLL++-Datensatz wurde von Reuters-Newsartikeln im Jahr 2020 erstellt und mit den gleichen Annotierungsleitlinien wie CoNLL-2003 erstellt.</sample>
    <sample id="630">Hello everyone, my name is Yuxin Zhang from Penn State University. Today I'm going to present our work Exemplar Cross-lingual Semantic Parsing in Multiple Natural Languages and Representations.</sample>
    <sample id="631">So, semantic parsing is a task to build semantic representations of user queries such as SQL and lambda calculus.</sample>
    <sample id="632">Cross-lingual semantic parsing is the task of translating queries in multiple natural languages into meaningful representations.</sample>
    <sample id="633">As shown in this figure, we need to translate the query into multiple natural languages using neural models, such as SQL, Lambda, or FunkQL, etc.</sample>
    <sample id="634">现有的跨语言语义解析模型是单独提出的，并在有限的任务和应用数据集上进行评估。例如，</sample>
    <sample id="635">There are licks of coverage on certain natural language. The Chinese is missing and...</sample>
    <sample id="636">Lack of coverage on certain menu representations.</sample>
    <sample id="637">Das Lambda-Kalkül fehlt.</sample>
    <sample id="638">Entweder werden sie nur auf bestimmten neuralen Modellen bewertet, zum Beispiel gibt es nur ein einzelnes Modell, um sie zu bewerten.</sample>
    <sample id="639">So, to this end, we propose Exemplar. We provide a uniform dataset exemplar for cross-lingual semantic parsing in multiple natural languages and meaning representations.</sample>
    <sample id="640">Es enthält 9 Datensätze in verschiedenen Bereichen, 5 Semantikomplexe, 8 Millionen Repräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="641">Und um unsere Leistung besser zu bewerten, haben wir die sechs Einstellungen für das Training und die Bewertung berücksichtigt.</sample>
    <sample id="642">Der erste Ansatz ist "Translate-Test". Wir verwenden die Google Translate API, um den Quelltext in das Zielsprache zu übersetzen. Dann verwenden wir ein monolingual-Modell für die Ausbildung und die Bewertung.</sample>
    <sample id="643">Ja, das ist eine gute Idee.</sample>
    <sample id="644">Und wir werden auch monolingual-Modelle testen.</sample>
    <sample id="645">In diesem Setting ist die QuellSprache dieselbe wie der ZielSprache, zum Beispiel Deutsch ins Deutsche oder Englisch ins Englische.</sample>
    <sample id="646">Wir testen auch ein monolingual-Few-Shot-Setting, indem wir monolingual-Modelle mit nur 10% der Trainingsdaten trainieren.</sample>
    <sample id="647">And it has the multi-lingual model which we train one multi-lingual model for all languages.</sample>
    <sample id="648">Zum Beispiel haben wir die deutschen, englischen und chinesischen Anfragen zusammen gesammelt, um ein multilinguales Modell zu trainieren. Während der Schlussfolgerung können wir dieses Modell verwenden, um</sample>
    <sample id="649">Sure, I can help with that. Do you need translations for specific terms or phrases?</sample>
    <sample id="650">And we also consider cross-lingual zero-shot and zero-shot transfer. We train on one source language and transfer to another language.</sample>
    <sample id="651">Das Training findet auf englischen oder auf der Kombination von englischen und deutschen Fragesätzen statt, um ein multilinguales Modell zu trainieren, das die SQL-Ausgabe vorhersagt.</sample>
    <sample id="652">And we also find many interesting results. So regarding analysis of monolingual models, we evaluate on two groups of models:</sample>
    <sample id="653">包括编码器指针，即多语言预训练编码器与基于指针的解码器，例如XLM-R +指针和BERT +指针。</sample>
    <sample id="654">Und wir haben auch Encoder-Decoder-Modelle evaluiert, nämlich multilingual vorbereitete Encoder-Decoder-Modelle wie N-BART und MT5.</sample>
    <sample id="655">We found that encoder-decoder obtains the best performance on all nine datasets.</sample>
    <sample id="656">And we evaluate on MT5 and XLM-R+PDR on multilingual setting.</sample>
    <sample id="657">Without that, encoder-decoder or encoder-PDR can be improved by training in a mixture of various languages.</sample>
    <sample id="658">Und wir haben gefunden, dass dies der Grund ist, weil die meisten bedeutenden natürlichen Sprachen einen Leistungsverlust erlitten, außer dass die Englischleistung in sieben Datensätzen abfiel und nur in drei Datensätzen gewann.</sample>
    <sample id="659">Ich denke, dies wird als Kurz der Multilingualität bezeichnet.</sample>
    <sample id="660">Wir verglichen auch den Leistungsabstand zwischen den Sprachen.</sample>
    <sample id="661">In this figure, the blue line is cross-lingual few-shot transfer. The orange line is cross-lingual zero-shot transfer while the green line is in the monolingual setting.</sample>
    <sample id="662">通过比较绿色和橙色的线条，我们发现，在零-shot设置中，跨语言转移性能差距是显著的。通过比较蓝色和橙色的线条，我们发现，在few-shot设置中，转移差距会迅速缩短。</sample>
    <sample id="663">We also find some other interesting findings. For example, encoder-decoder outperforms previous work or achieved comparable results between English natural language and significantly boosted the performance of few-shot on target natural languages</sample>
    <sample id="664">And we found multilingual language models such as Codex and Blue are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="665">To sum up, we build Exemplar, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and main representations.</sample>
    <sample id="666">Wir führen eine umfassende Leistungsstudie zu drei repräsentativen Typen von multilingualen Sprachmodellen durch und unsere Ergebnisse zeigen viele interessante Erkenntnisse usw. Willkommen, Besuchen Sie unsere Publikation und Code. Vielen Dank für Ihre Aufmerksamkeit</sample>
    <sample id="667">Existing works can be broadly classified into four categories: 1. Data-driven approaches that utilize large-scale datasets to train machine learning models for sentiment analysis and emotion detection in text. These methods often employ deep neural networks, such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), which are capable of capturing complex patterns within the data. 2. Rule-based systems that rely on predefined rules and heuristics to identify emotions based on specific linguistic features or structures present in a given piece of text. This approach is more explicit about how it processes information but may not capture all possible emotional expressions effectively. 3. Hybrid methods combining elements from both data-driven and rule-based techniques. Such hybrid approaches might use pre-trained language models along with handcrafted rules to improve performance by leveraging strengths from each category while mitigating their respective weaknesses. 4. Transfer learning frameworks where knowledge gained during training one task (e.g., image classification) is transferred to another related task (e.g., sentiment analysis). By reusing learned representations across different domains, these methods aim at reducing computational costs and improving accuracy compared to starting from scratch for each new problem domain. Each of these categories represents distinct strategies for tackling the challenge of automatic emotion recognition in textual content, reflecting varying levels of complexity, interpretability, and applicability depending on the specific requirements of an application scenario.</sample>
    <sample id="668">Mehrsprachige LLMs wie Codex oder Bloom sind für CLSP nicht ausreichend.</sample>
    <sample id="695">Die Methode mit der Mehrdeutigkeit der Permutationen umfasst die Induktion der Anordnung als Teil des Trainings.</sample>
    <sample id="696">Fairness in a fine-tuned NLP model is determined by its ability to produce unbiased and equitable outcomes. This involves ensuring that the model does not perpetuate or amplify existing biases, especially those related to political opinions, hate speech, misinformation, marginalization of minority groups, or other forms of discrimination. Fairness also considers how well the model generalizes across different demographics and contexts without introducing new inequities.</sample>
    <sample id="697">Der Referent heißt Yanis Levarac.</sample>
    <sample id="698">Kostas Sena</sample>
    <sample id="699">Myra</sample>
    <sample id="700">Tropicalism refers to a cultural movement that emerged in Brazil during the 1960s, characterized by its blending of various musical styles and influences from African, European, and indigenous Brazilian cultures. It often features vibrant rhythms, colorful imagery, and themes related to nature and exoticism. In this context, "vibrant" might be used to describe characteristics associated with Latina women, suggesting energy, liveliness, or an alluring presence reminiscent of tropical environments.</sample>
    <sample id="701">The authors created the descriptions of target groups by analyzing words used in people's writings about those groups. For marker groups, they focused on terms like "culture," "tradition," "proud," and "exotic." These words were chosen because they highlight how these groups are defined through their identity and set them apart from what is considered normal or typical for white individuals.</sample>
    <sample id="702">Pointwise CxMI</sample>
    <sample id="703">DrBERT and ChuBERT are both pre-trained language models, but they differ in their training data sources. DrBERT is trained on a diverse range of texts from various domains to improve its generalizability across different tasks. On the other hand, ChuBERT focuses specifically on clinical text data, which allows it to excel at medical-related tasks such as disease diagnosis or drug interactions.

The main differences between them lie in their intended use cases: one for broad applications (DrBERT) versus specialized ones within healthcare settings (ChuBERT). This specialization can lead to better performance when dealing with specific types of information found predominantly in medical contexts.</sample>
    <sample id="751">Zhiyong and I are presenting our research.</sample>
    <sample id="752">Iterative transfer learning is a method where the model is updated iteratively by training on new data collected from each round of active annotation.</sample>
    <sample id="753">Das Ziel des Datensatzes ist es, das Verständnis von Benutzersprache zu verstärken, wenn sie eine Wahl treffen möchten.</sample>
    <sample id="754">Ein Angreifer kann Modellparameter über einen EaaS (Embedded AI as a Service) extrahieren.</sample>
    <sample id="755">There are three authors.</sample>
    <sample id="756">Zwei Annotatoren wurden verwendet, um den ursprünglichen Datensatz zu erstellen.</sample>
    <sample id="757">The authors belong to Carnegie Mellon University, the University of Washington, and the Allen Institute for AI.</sample>
    <sample id="758">The example is "I saw Bart and Lisa".</sample>
    <sample id="759">The technology for dialogue systems is advancing rapidly, with improvements in natural language processing and machine learning. These advancements allow chat models to better understand context, generate more coherent responses, and reduce thematic errors over time. However, challenges remain in ensuring human-like conversation quality and addressing ethical concerns related to AI interactions.</sample>
    <sample id="760">We need to evaluate the models' acceptability throughout the context window because large language models are now using longer and longer context windows.</sample>
    <sample id="761">The study found that multilingual training led to a performance drop in English compared to the monolingual model, but this was not consistent across all languages.</sample>
    <sample id="762">Ja, die Annotatoren kennen die Entitäten im Voraus.</sample>
    <sample id="763">The MT metrics used for evaluation are: BLEU, METEOR, ROUGE-L, and CIDEr.</sample>
    <sample id="764">Ja, die Regression beeinflusst die allgemeine Leistung bei bestimmten NER-Typen.</sample>
    <sample id="765">Positionalität für NLP ist wichtig, weil es zu einer besseren Verständnis der Kontext und des Beziehungs von Worten im Text führt.</sample>
    <sample id="766">Adapter</sample>
    <sample id="767">Das Modell, das für das Transferlernen verwendet wird, ist ein zero-shot-Modell.</sample>
    <sample id="768">We used several short prompting.</sample>
    <sample id="769">Die Autoren haben schließlich drei Empfehlungen vorgeschlagen.</sample>
    <sample id="770">The proposed method outperforms the strongest baseline by 1.6%.</sample>
    <sample id="771">Shuhang</sample>
    <sample id="772">Ja, die Ergebnisse und der Datensatz der Studie können als Referenz für zukünftige Forschungen in diesem Bereich verwendet werden.</sample>
    <sample id="773">In der Arbeit werden zwei kleineren Modellen experimentiert: T5 und FLAN-T5.</sample>
    <sample id="774">OFA wird als Basismodell für die Untersuchung der multimodalen Unterrichtsabstimmung verwendet.</sample>
    <sample id="833">The authors belong to the University of Edinburgh.</sample>
    <sample id="834">Stony Brook University</sample>
    <sample id="835">English and Chinese, English and Japanese, German and Spanish.</sample>
    <sample id="836">The speaker is a PhD student at the University of Washington.</sample>
    <sample id="837">Fine-tuned two models: longimp and normalimp.</sample>
    <sample id="838">53</sample>
    <sample id="839">There are 4 authors.</sample>
    <sample id="840">AG News, MINE, SSD2 and EER Spam</sample>
    <sample id="876">NACHOS ist ein Dataset, das aus Medizinischen Crawledaten von der Webseite stammt.</sample>
    <sample id="877">The speaker's name is Ariel Beilag.</sample>
    <sample id="878">Prompting has a big influence on the performance of LLMs for translation.</sample>
    <sample id="879">Patrick Fernandes, Emi Liu, Andre Martins und Graham Neubig.</sample>
    <sample id="880">1. Use the QR code to access their data and model 2. Collect a larger multimodal instruction tuning dataset with around 150 additional variant language tasks 3. Release them soon 4. Follow up for more information or updates 5. Stay informed about new developments in this field</sample>
    <sample id="881">Die Autoren präsentieren eine Aufgabe zur Korrektureinordnung, die darauf abzielt, zu erkunden, ob Modelle in der Lage sind, Kenntnisse aus mehreren Quellen zu nutzen. Sie evaluieren das Dataset mit Teilnehmern an Studien und mit etablierten Korrektureinordnung-Modellen.</sample>
    <sample id="882">Hallo alle, mein Name ist Ariel Bilaad und ich werde einen kurzen Überblick über das Papier "Sprungpaar-Transliteration: Evaluating Strategies and Performance" geben. Dies ist gemeinsame Arbeit mit meinen Kollegen von Google Translate.</sample>
    <sample id="883">Bram是于2022年发布的540亿参数的大型语言模型。它是在一个包含7800亿个标记的大文本集合上进行训练的。</sample>
    <sample id="884">Es erreicht in den Tausenden von NLP-Aufgaben die Leistungsfähigkeit der Art.</sample>
    <sample id="885">In this work, we present the first systematic study of large language model prompting for machine translation.</sample>
    <sample id="886">Wir haben die Übersetzungsfähigkeit dieser Modelle mithilfe der besten Praktiken der MT-Community-evaluiert. Dies bestand darin, die neuesten Testdatensätze zu verwenden, um eine Überlappung der Testdaten mit der Trainingsdaten des Sprachmodells zu vermeiden.</sample>
    <sample id="887">Und wir haben zwei hochwertige Systeme verglichen, die bei der WMT-Evaluation die besten Leistungen erzielt haben.</sample>
    <sample id="888">Wir verwenden die neuesten NMT-Metriken und zeigen zusätzlich Ergebnisse der Evaluierung von Experten. Schließlich bieten wir einige Empfehlungen für Prom-Selectionsstrategien.</sample>
    <sample id="889">Prompting对LLMs的翻译性能有显著影响，如图所示。我们进行了一项简单的实验，使用单个提示，并为每个句子提供了两种不同的提示。</sample>
    <sample id="890">Die Mehrheit der Sätze, 516 von 1000, hat eine Differenz von mehr als 1 Blur-Point.</sample>
    <sample id="891">Und das kann in extremen Fällen bis zu 40 Punkten steigen. Es ist wichtig, eine gute Promotionsstrategie auszuwählen.</sample>
    <sample id="892">In our experiments, we settled for a five-shot prompting strategy where we just mark each sentence that we provide to the system with the language it's in.</sample>
    <sample id="893">In diesem Beispiel hier, in dem wir Übersetzungen von Deutsch ins Englische durchführen, sind die deutschen Satze als Quellsätze mit deutscher Doppelpfeile markiert und die englischen Übersetzungen mit englischer Doppelpfeile.</sample>
    <sample id="894">Wir sehen, dass die tatsächliche Form des Promptings in der Regel keine große Einfluss auf den Fall mehrerer kurzer Promptings hat.</sample>
    <sample id="895">Es ist für Null- und Ein-Schuss-Prompting von entscheidender Bedeutung, und wenn wir wie in unserem Fall fünf Schuss-Prompting verwenden, gibt es fast keine Unterschiede im tatsächlichen Prompt.</sample>
    <sample id="896">Es sind die Beispiele, die den größten Teil des Gewichts tragen.</sample>
    <sample id="897">Das Zusammenfassungsergebnis unserer experimentellen Ergebnisse lautet, dass die Beispielleistung wichtiger ist als die Ähnlichkeit zur Quellspruch.</sample>
    <sample id="898">Es ist wichtig, die Beispiele aus hochwertigen Übersetzungen zu wählen. Im Detail verglichen wir die Auswahlvorschläge aus der Trainingsdatenbank der WMT-Evaluierungen oder der Dev-Datenbank.</sample>
    <sample id="899">开发数据更准确，质量更高，而训练数据则更嘈杂。因此，使用开发数据时性能更好。</sample>
    <sample id="900">Trotzdem haben spezialisierte, state-of-the-art Systeme einen bedeutenden Vorteil gegenüber PAM-Übersetzungen. Allerdings kommt PAM sehr nahe bei einem kommerziellen System - in unserem Fall haben wir uns auf Google Translate entschieden.</sample>
    <sample id="901">Die Erkenntnisse, die wir von der E-Mail-Analyse mit dem MQM-Framework gewinnen, zeigen, dass die Flüssigkeit von PALM mit den Systemsystemen gleichwertig ist. Der Hauptunterschied kommt jedoch aus der Genauigkeit.</sample>
    <sample id="902">Immer wieder passieren Fehler, die durch Übersehen verursacht wurden.</sample>
    <sample id="903">Es scheint, dass PAM die Entscheidung trifft, eine bessere klingende Übersetzung zu produzieren, manchmal indem es Teile der Quellsprache verpasst.</sample>
    <sample id="904">Allerdings ist die Stufe "awkward" für PAN unterhalb der Stufe "state-of-the-art systems", was ein weiteres Signal ist.</sample>
    <sample id="905">That part provides really fluent output, but still with some problems of accuracy.</sample>
    <sample id="906">Und das ist alles für diese kurze Überblicksrede. Für weitere Details, klicken Sie bitte auf die vollständige Präsentation des Papiers. Vielen Dank</sample>
    <sample id="907">Hallo, ich bin David, ein Promotionsstudent an der Saarland University in Deutschland. In diesem Video möchte ich gerne unsere kürzliche Arbeit "Worse than You Think: A Critical Look at Weekly Surprise Learning" präsentieren.</sample>
    <sample id="908">Dies ist gemeinsame Arbeit mit Xiao Yu Shen, Mario Smousepath und Dr. Stefan und Dietrich Klarke.</sample>
    <sample id="909">Ich möchte beginnen mit einer kurzen Einführung in die Weak-Supervision und Weak-Supervised Learning.</sample>
    <sample id="910">In weak supervision, we do not manually label the data. Instead, we label the data using weak labeling sources such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right.</sample>
    <sample id="911">Wenn man sie mit menschlichen Notationen vergleicht, sind die WIKI-Notationen viel billiger, aber sie sind auch "rauschig", was bedeutet, dass eine bestimmte Menge der Notationen falsch sind.</sample>
    <sample id="912">Wenn wir Neuralenets direkt auf wöchentlichen Label-Daten trainieren, lernen die Neuralenets die Lärm des Labels ab und können nicht generalisieren.</sample>
    <sample id="913">In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well.</sample>
    <sample id="914">In den letzten Arbeiten von WSL, WSL steht für "Wöchentliche Supervision Lernen", wird häufig behauptet, dass Leute ihre Modelle auf wöchentlichen Labels trainieren und eine helle Leistung auf ungeräumigen Testdaten erreichen.</sample>
    <sample id="915">Technisch ist diese Behauptung nicht falsch, aber es gibt einen Haken.</sample>
    <sample id="916">Es ist jedoch zu beachten, dass die meisten Forscher annehmen, dass es eine zusätzliche, unverwendete Validationsdatenbank gibt, die für die Modellauswahl verfügbar ist.</sample>
    <sample id="917">Wir haben auf diesem Problemsetting zu Halt gestoppt, da dies impliziert, dass zusätzliche manuelle Anmerkungen in Weakly Supervised Learning erforderlich sind. Aber, wie ein Elefant im Zimmer, wird diese Notwendigkeit oft übersehen.</sample>
    <sample id="918">Die oben genannten Fragen stellen sich vor:</sample>
    <sample id="919">Zweitens, wenn sauberes Daten erforderlich sind oder wenn es für WSL2 erforderlich ist, dann wie viele saubere Beispiele benötigen wir? Schließlich sollten wir nur die sauberen Beispiele für die Grundlage verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen?</sample>
    <sample id="920">Wir haben diese Forschungsfragen in unserem Arbeit bearbeitet und unsere Ergebnisse lauten wie folgt:</sample>
    <sample id="921">Zunächst finden wir, dass WSL-Methoden tatsächlich ordnungsgemäße Ergebnisse ergeben, wenn man ordentliche Validations-Samples verwendet.</sample>
    <sample id="922">Otherwise, there is a large performance drop as shown in this figure. If there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels.</sample>
    <sample id="923">Das bedeutet, dass das Training ohne Zweck ist.</sample>
    <sample id="924">Dies zeigt an, dass WSL-Anlässe tatsächlich klare, markierte Daten benötigen, um richtig zu funktionieren. Und die Annotationskosten, um saubere Validationsproben zu erhalten, sollten nicht übersehen werden.</sample>
    <sample id="925">Un zweiter Fund ist, dass die Anzahl der validierenden Clean-Samples es WSL-Herangehens zu besserem Leistungsergebnis ermöglicht, wie im Bild links gezeigt.</sample>
    <sample id="926">Typischerweise benötigen wir nur 20 Stämme pro Klasse, um eine hohe Leistung zu erzielen.</sample>
    <sample id="927">Aber das ist noch nicht das Ende der Geschichte, denn wenn wir die EinerWay-Entscheidung für saubere Proben treffen, dann kann die direkte Ausbildung sogar bessere Leistungen erbringen.</sample>
    <sample id="928">Das rechte Bild zeigt die Leistungsunterschiede zwischen den fine-tuning-Ansätzen, die direkt auf dem sauberen Datenmaterial angewendet werden, und den WSL-Ansätzen, die das saubere Datenmaterial nur für die Validierung verwenden.</sample>
    <sample id="929">Wie man sieht, wenn wir 10 Proben pro Klasse haben, erreicht direktes Feinabpolieren die WSL-Anläufe.</sample>
    <sample id="930">Schließlich können die Leistungsverbesserungen, die in den vorherigen WSL-Methoden angeboten wurden, leicht erreicht werden, indem man weiterhin auf den sauberen Validations-Samples anpasst.</sample>
    <sample id="931">Wie wir aus den Abbildungen sehen können, leistet das von Malina entwickelte Modell, genannt FTW, am Anfang die komplexeren WSL-Methoden wie Cosine schlechter als andere Methoden.</sample>
    <sample id="932">Allerdings, wenn wir es auf den sauberen Beispielen weiter anpassen lassen, then FTW erfordert ebenso gute Ergebnisse wie andere Methoden.</sample>
    <sample id="933">Also, in der Praxis gibt es keinen Grund, WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern.</sample>
    <sample id="934">Zusammenfassend zeigen unsere Ergebnisse, dass kürzlichere WSL-Anlässe auf saubere, manualer angenommene Beispiele angewiesen sind, um richtig zu funktionieren. Ihre Leistungssteigerung und Praktikabilität werden stark übertrieben geschätzt.</sample>
    <sample id="935">Un konkreten Empfehlungen für zukünftige Arbeit sind die folgenden:</sample>
    <sample id="936">Zuerst berichten Sie die Modellauswahlkriterien. Zum Beispiel berichten Sie, ob das Modell ausgewählt wurde, während Validations-Samples</sample>
    <sample id="937">Zweitens sollten die WSL-Methoden mit der vorherigen Lernungsbasis verglichen werden, da sie beide Arbeiten auf klaren Szenarien betreiben. Drittens sollte das kontinuierliche Feinabpolieren als einfaches und starkes Baseline in Zukunft bei der Arbeit mit WSL berücksichtigt werden.</sample>
    <sample id="938">Zuletzt haben wir unseren Code freigelegt. Sie können ihn über den QR-Code auf dieser Seite finden. Bitte führen Sie es gerne ein. Vielen Dank und genießen Sie den Kongress</sample>
    <sample id="939">Common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="940">Sebastian Senti, Ronan Le Bras, Katerina Rynika, Martin Sap</sample>
    <sample id="941">Dieses Beispiel erfordert Kenntnisse über die Rolle eines Richters und den Prozess der Entscheidungsfindung in einem Gericht.</sample>
    <sample id="942">Code is available on GitHub.</sample>
    <sample id="943">The annotation is balanced for each demographic group.</sample>
    <sample id="944">So, why does the match prefix affect the language model judgment so much? We did a series of analysis where we tried to perturb the input sentence by trying to preserve the relevant structure but adding noise to the input and after doing several of these perturbations, we find that none of these noises are actually making the model change its course in terms of how it shows us the MPP judgment trend. Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion.</sample>
    <sample id="945">Dimensional evaluation means assessing multiple aspects or dimensions of a model's performance. This approach provides more detailed insights into the strengths and weaknesses in different areas, allowing for a deeper understanding of how well the model performs overall.</sample>
    <sample id="946">The authors belong to the University of Science and Technology of China.</sample>
    <sample id="947">Es ist wichtig für Prompting mit 0 und 1 Schritten.</sample>
    <sample id="978">ABC eval</sample>
    <sample id="979">There are 10 authors.</sample>
    <sample id="980">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="981">Zwei Autoren sind an der Arbeit beteiligt: Siyuan Shi und Yuxin Liu.</sample>
    <sample id="982">Vasudha</sample>
    <sample id="983">The authors belong to the University of Warsaw.</sample>
    <sample id="1021">The most common errors are omission errors.</sample>
    <sample id="1022">Hello, I'm James Finch. And I am Sarah Finch.</sample>
    <sample id="1023">Dieses Werk wurde von dem Emory NLP Lab durchgeführt, das von Professor Jino Choi an der Emory University geleitet wird und zusammen mit Amazon Alexa AI.</sample>
    <sample id="1024">So, let's say you just created a dialogue model and want to see how it stacks up against the latest advancements.</sample>
    <sample id="1025">Die allgemeine Praxis besteht darin, menschliche Bewertungen zu verwenden, wie zum Beispiel indem man menschliche Richter fragt, welche von zwei Gesprächen besser ist oder Gespräche anhand einer Likert-Skala bewerten.</sample>
    <sample id="1026">Diese Ansätze funktionieren gut, um eine umfassende Bewertung der Gesamtkommunikationsqualität zu liefern. Allerdings hat Kommunikationsqualität viele Aspekte. Daher möglicherweise möchten Sie mehrere Dimensionen der Chats-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.</sample>
    <sample id="1027">Eine Möglichkeit ist es, menschliche Richter einfach zu bitten, mehrere Aspekte der Dialogqualität zu bewerten, wie zum Beispiel die Relevanz der Modulantworten, mit vorhandenen vergleichenden oder Likert-Skala-Methoden.</sample>
    <sample id="1028">Wir glauben, dass es eine genauer und zuverlässiger Strategie für die Evaluierung von dimensionalem Dialog gibt.</sample>
    <sample id="1029">Unsere Ansatz versucht, die Subjektivität der menschlichen Bewertung durch explizite Annotierung zu reduzieren, ob jeder Modell-Response bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel die Anzeige von unerkanntem Informationen oder das Selbstkontradiktions.</sample>
    <sample id="1030">Wir nennen diese Ansatz Annotating Behaviors in Chat oder ABC-Eval kurz. Wir haben diesen Ansatz entwickelt, um eine umfassende Überlappung der Chat-Modellverhaltensweise zu erreichen, die in den letzten Literaturarbeiten vorgeschlagen wurden und auf die das Chatqualität beeinflussen.</sample>
    <sample id="1031">Abc eval ist in der Lage, die Geschwindigkeit zu messen, wie Chat-Modelle verschiedene Thematikfehler begehen.</sample>
    <sample id="1032">Zum Beispiel misst ABC-eval die Anzahl der Runden, in denen ein Chat-Modell seinen Partnern nicht antwortet oder etwas Irrelevantes sagt.</sample>
    <sample id="1033">Entschuldigung, ich habe den englischen Inhalt ins Deutsche übersetzt.</sample>
    <sample id="1034">Um zu bestimmen, welche Bewertung am effektivsten ist, haben wir vier state-of-the-art-Chatmodelle ausgewählt und haben sie mit 100 mensch-maschine-Kommunikationen pro Modell unter Einsatz von ABC-Eval gemessen.</sample>
    <sample id="1035">Für die Vergleichsweise Bewertung haben wir diese Gespräche mit drei vorhandenen Methoden bewertet: Likert-Skala-Bewertungen auf der Niveau, Likert-Skala-Bewertungen auf der Dialog-Level und Dialog-Level-Pairwise-Vergleiche.</sample>
    <sample id="1036">Für jede der vorhandenen Methoden haben wir Bewertungen für acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis für die Bewertung von Chat-Modellen auf mehreren Ebenen ist.</sample>
    <sample id="1037">Wir haben die Ergebnisse dieser Bewertungen analysiert und haben festgestellt, dass die ABC-Eval-Behavior-Labels im Ganzen zuverlässiger sind als die von bestehenden Methoden sammelten Labels, wie durch die Interannotatorkonstanz auf hundert doppelbezieht sich auf die Konversationen gemessen.</sample>
    <sample id="1038">Zusätzlich sind die ABC-Eval-Label für die Gesamtkonversationqualität besser vorhersagbar als die von bestehenden Methoden erzeugten Metriken, wie dies durch diese einfache lineare Regression-Analyse gezeigt wird.</sample>
    <sample id="1039">Zum Beispiel können Sie sehen, wie das Maß an Wenden mit Selbst- und Partnerkontrasten 5% und 10% der Gesprächsqualität erklären, während die durchschnittlichen Likert-Konsistenz-Scores nur 4% oder weniger erklären.</sample>
    <sample id="1040">Zuletzt haben wir geprüft, ob jede Bewertungsvariable eine einzigartige Aspekt der Chats-Qualität einfängt, indem wir eine Schrittweise lineare Regression verwenden.</sample>
    <sample id="1041">Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25% der Gesprächsqualität erklärt. Und wenn Sie die Metriken einer nach der anderen entfernen, resultieren die meisten davon darin, eine angemessene Menge an Informationen über die Qualität zu verlieren.</sample>
    <sample id="1042">Auseinanderstehen der Kombination von Alternativ-Level-Likert-Metriken erklärt viel weniger Qualität und weniger dieser Metriken tragen einzigartige Informationen mit.</sample>
    <sample id="1043">These reliable, informative and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve.</sample>
    <sample id="1044">Sie können im Ergebnis unserer Experimente sehen, dass mehrere Herausforderungen immer noch bestehen und genau gemessen wurden. Zum Beispiel haben die Bots, die wir getestet haben, Vernunftverstöße in etwa 20% ihrer Antworten.</sample>
    <sample id="1045">Sie produzieren unrelevanten Inhalt in etwa 15 % der Antworten und sie widersehen sich oder ihren Partner etwa 10 % der Zeit.</sample>
    <sample id="1046">Mit dem schnellen Fortschritt in diesem Bereich könnten viele dieser Fehlerwinkel aufgrund der Veröffentlichung neuer Modelle seit unserer Bewertung abnehmen. Dennoch ist dies noch ein Grund, um sich an verlässlichen und genaueren Bewertungsmaßstäben für die Vergleichsweise von Modellen zu orientieren.</sample>
    <sample id="1047">Wir hoffen, dass ABC-Eval von anderen im Bereich genutzt werden kann, um ein bedeutendes Schritt in dieser Richtung zu sein, und wir freuen uns darauf, zu sehen, wie der Conversational AI in den kommenden Monaten und Jahren weiterentwickelt wird. Vielen Dank fürs Zusehen.</sample>
    <sample id="1048">Die Autoren gehören der Emory University.</sample>
    <sample id="1049">CFT in this work stands for "Continuous Fine-Tuning". It refers to a method or approach used in the context of machine learning, particularly in tasks related to natural language processing (NLP) such as question answering. Continuous fine-tuning is likely being discussed as part of evaluating different strategies for improving model performance on specific benchmarks like SQuAD 2.0 and HotpotQA.</sample>
    <sample id="1050">Es sind sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="1051">Hallo, mein Name ist Kay Yen und ich werde unsere Arbeit präsentieren, die "Wann benötigt Übersetzung Kontext? Eine datenschwimmende multilingual Exploration" heißt. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Amy Liu, Andre F. D. Martins und Graham Neubig erstellt.</sample>
    <sample id="1052">Also, viel von den Übersetzungen hängt von Kontext ab. Zum Beispiel: Wie übersetzen wir "mole" in diesem Satz?</sample>
    <sample id="1053">Mole refers to a spy.</sample>
    <sample id="1054">Je nach Kontext ändert sich der Bedeut des Wortes und daher auch seine Übersetzung.</sample>
    <sample id="1055">Aber, die Bewertung der Übersetzungsfähigkeit von Modellen in solchen Fällen ist schwierig. Zunächst, weil nur eine kleine Brücke der Übersetzungen auf Kontext abhängt, was es denkorpussebene-Metriken wie dem Blue verhindert, diese Übersetzungen zu erfassen.</sample>
    <sample id="1056">Man Leute haben empfohlen, kontextabhängige Übersetzungen aufzustellen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachmengen, da sie typischerweise auf Fachwissen und menschliche Curations angewiesen sind.</sample>
    <sample id="1057">In this work, we try to answer these two questions: First, when does translation require context? And second, how well do models handle these cases?</sample>
    <sample id="1058">Um, um, um,</sample>
    <sample id="1059">In our previous work, we introduced CxMI as a measure for context usage by machine translation models. This is done by measuring how much information the context provides about the target Y given the source X.</sample>
    <sample id="1060">Sie können CXMI als das Information, das man von der Kontextstellung des Modells gewinnt, betrachten.</sample>
    <sample id="1061">In this work, we extend cMI to pointwise cMI which can measure context usage at the sentence level or at the word level. We can think of words that have high p cMI as ones that require context for translation.</sample>
    <sample id="1062">Wir analysieren Wörter mit hohem P-SMI, um Muster zwischen diesen Wörtern zu finden.</sample>
    <sample id="1063">Und wir führen unsere Analyse auf Transkripten von TED-Talks durch, die von Englisch ins 14.</sample>
    <sample id="1064">Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zunächst betrachten wir die Part-of-Speech-Tags mit hohen Median-Werten für PCMI.</sample>
    <sample id="1065">Und das ermöglicht uns, zum Beispiel, Doppelpronomen im Arabischen zu finden, die relativ hoch p6mi haben. Das kann erklärt werden, weil Englisch keine Doppelpronome hat, daher benötigt man Kontext, um festzustellen, ob ein Pronomina dual ist, wenn es ins Arabische übersetzt wird.</sample>
    <sample id="1066">Und ähnlich finden wir, dass bestimmte Sprachen auch Kontext benötigen, wenn wir die geeignete Form eines Verbs wählen möchten. Wir schauen dann auf Wörter, die eine hohe PSEMI durchschnittlich über all ihre Vorkommen haben.</sample>
    <sample id="1067">Und das hilft, Fälle zu identifizieren, wie die hier, in denen man im Chinesischen Kontext benötigt, um Proper-Nomina richtig zu übersetzen, damit man sichergestellt ist, dass man denselben Übersetzungsanwendung innerhalb des Dokuments verwendet.</sample>
    <sample id="1068">Und ähnlich finden wir, dass Kontext die Übersetzung in der richtigen Formelität unterstützt.</sample>
    <sample id="1069">Und schließlich betrachten wir unterschiedliche, um, individuelle Token, die hohe p-SekMI haben. Und das ermöglicht uns, Phänomene zu identifizieren, die nicht wirklich von dem Wort selbst erfasst werden können, sondern stattdessen im Satzstruktur ausgedrückt werden, wie zum Beispiel Ellipsesresolutions.</sample>
    <sample id="1070">Also verwenden wir unsere Ergebnisse für die Analyse, um eine Leistungsprobe für Dokumentenlevelübersetzungen zu entwerfen.</sample>
    <sample id="1071">Für jeden der fünf Diskurserscheinungen, die wir identifiziert haben, haben wir Tags erstellt, um Wörter zu identifizieren, die sich auf das Ereignis beziehen. Wir nennen unsere Tags "Multilingual Discourse Aware" oder MADA-Tags.</sample>
    <sample id="1072">Wir können auch beachten, dass verschiedene Sprachen unterschiedliche Proportionen dieser Diskurserscheinungen haben.</sample>
    <sample id="1073">Wir verwenden dann den Moeda-Tagger, indem wir den Tagger auf dem Parallel-Korpus anwenden, das wir für die Bewertung verwenden möchten. Und wir wenden unsere bevorzugten Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die vom Moeda-Tagger identifiziert wurden.</sample>
    <sample id="1074">Schließlich verwenden wir unsere Leistungsindikatoren zusammen mit anderen Metriken, um verschiedene Modelle auf der Ebene von Dokumenten zu bewerten.</sample>
    <sample id="1075">Zunächst einmal verwenden wir Korpus-Level-Metriken. Bei der BLEU-Funktion haben uns die unsprachsprachlichen Modelle die beste Leistung erzielt.</sample>
    <sample id="1076">Aber wenn wir Kommata verwenden, leisten Kontextbewusste Modelle die besten Ergebnisse. Wenn wir den Wortsatz-Messwert verwenden, haben Modelle mit und ohne Kontext vergleichbares Leistungsbereich.</sample>
    <sample id="1077">Dies zeigt wieder, dass es schwierig ist, den besten Dokumentebene-Übersetzungssystem zu bestimmen, wenn wir allein auf Korpus-Level-Metriken setzen.</sample>
    <sample id="1078">Wir verwenden das MovieLens-Benchmark, um die Modelle zu bewerten und finden, dass Kontextsummarierungsmodelle für bestimmte Diskurserscheinungen wie Formelleit und Lexikalische Kohärenz viel genauere sind als Modelle, die den Kontext nicht verwenden.</sample>
    <sample id="1079">Aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext auf andere Phänomene wie Ellipsen, Pronomen und Verben verwenden. Das suggeriert, wo wir mehr Fortschritte für die Dokumentenstufe sehen müssen.</sample>
    <sample id="1080">Wir haben auch verschiedene kommerzielle Systeme verglichen und unsere Leistungsprobe zeigt, dass DeepL üblicherweise für Dokumentenübersetzungen genauere Ergebnisse liefert als Google Translate.</sample>
    <sample id="1081">Zusammenfassend führen wir eine datenbasierte Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext benötigen.</sample>
    <sample id="1082">Und dann verwenden wir unsere Erkenntnisse, um eine Leistungsstufe für die Dokumentenstufe-Maschinenübersetzung zu erstellen, die uns hilft, zu erkennen, welche Diskurserscheinungen Modelle gut oder nicht verarbeiten können und welche Übersetzungssysteme am besten in der Dokumentenstufenübersetzung sind.</sample>
    <sample id="1083">Vielen Dank für deine Aufmerksamkeit, sehen Sie Sie in Toronto</sample>
    <sample id="1084">Rusen Zhang</sample>
    <sample id="1121">No name specified.</sample>
    <sample id="1122">Die Autoren beschreiben die Methode der "markierten Wörter" als eine Methode zur Identifizierung von Wörtern, die die Gruppen unterscheiden, die als markierte Gruppen bezeichnet werden.</sample>
    <sample id="1123">University of Washington</sample>
    <sample id="1124">Prague approach</sample>
    <sample id="1125">James Finch</sample>
    <sample id="1126">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="1127">Minimal pairs can be used to test syntactic phenomena.</sample>
    <sample id="1161">Die fünf Methoden für die erste Forschungsfrage sind: WSL, WSL-2, WSL-3, WSL-4 und WSL-5.</sample>
    <sample id="1162">Das Modell wird auf 11 biomedizinischen und klinischen Aufgaben in Frankreich evaluiert.</sample>
    <sample id="1226">CamemBERT was trained on the Wikipedia corpus.</sample>
    <sample id="1227">Der Referent heißt Adam Skirukowski.</sample>
    <sample id="1228">Das Experiment bestätigte die Annahme, dass der temporale Verschiebung die Haupt Ursache für den Leistungsverlust ist.</sample>
    <sample id="1269">It is necessary to permute the tokens for the output sequence because, after the first step, we have all the right tokens but they are not ordered. In the second step, another model predicts a permutation to put them into the correct order.</sample>
    <sample id="1270">Die Autoren betonen, dass Transparenz bei der Abbau von Vorurteilen in Modellen wichtig ist, um die Effektivität und Fairness dieser Methoden zu verstehen. Sie betonen, dass es unklar ist, ob positive Stereotypien aufgrund der übermäßigen Wertausrichtung oder anderer Antistereotypeschutzmaßnahmen entstehen.</sample>
    <sample id="1271">Minimalpaareingaben sind unakzeptabel.</sample>
    <sample id="1272">The authors used the following evaluation metrics: 1. F1 Score 2. Precision 3. Recall These metrics are commonly employed to assess model performance, particularly in tasks involving text classification or information retrieval.</sample>
    <sample id="1273">Inter-annotator agreement</sample>
    <sample id="1274">Wikipedia</sample>
    <sample id="1275">The authors belong to the University of Bremen.</sample>
    <sample id="1276">MultiInstruct unterscheidet sich von anderen Benchmarks, indem es eine große Sammlung von mehreren visuellen und textuellen Instruktionstasks bietet. Diese Vielfalt ermöglicht es Forschern und Entwicklern, die Leistung von Multimodell-Modellen auf einem breiten Spektrum an Aufgaben zu testen und zu optimieren. Darüber hinaus bietet MultiInstruct eine umfangreiche und gleichmäßig verfügbare Datenquelle für die Untersuchung der Wirkung von Instruktions-Tuning auf die Leistung von Multimodell-Modellen.</sample>
    <sample id="1277">Zwei Autoren sind an der Arbeit beteiligt: James Finch und Sarah Finch.</sample>
    <sample id="1278">binäre Koordination ist eine Musterung, bei der ein Element in einem Molekül zwei separate Bindungen zu anderen Elementen bildet.</sample>
    <sample id="1279">Die Prompts in dieser Studie waren im Durchschnitt etwa 15 Wörter lang.</sample>
    <sample id="1280">The results show that the smaller T5 model can generate scripts of higher quality than most large language models when trained on suitable datasets. This suggests that even with less computational power, a well-designed and appropriately trained small model can outperform larger ones in terms of script generation quality.</sample>
    <sample id="1281">Hi, I am Janislaw Wakana and I will present you our works on Dr. BERT, a robust pre-trained model in French for biomedical and clinical domains</sample>
    <sample id="1282">In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article</sample>
    <sample id="1283">Wir haben eine neue Biomedizinische Modell in französischer Sprache, namens Dr. BERT, eingeführt, die auf ROBERTA basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer Crawled-Daten von der Webseite.</sample>
    <sample id="1284">Wir präsentieren auch eine Vergleichung von Modellen mit mehrfach returning Settings und Datenquellen. Dann präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Downstream Tasks in Frankreich.</sample>
    <sample id="1285">Schließlich fassen wir die Experimente zusammen und geben Ihnen mehr Details darüber, wie Sie auf diese Modelle zugreifen können.</sample>
    <sample id="1286">Seit seiner Veröffentlichung im Jahr 2018 wurde BERT zu einer der effektivsten Ansätze zur Lösung von Sprachverarbeitungsaufgaben geworden und bietet große Leistungssteigerungen im Vergleich zu historischen statischen und kontextuellen Methoden wie Word2vec, FastText oder ANOVA.</sample>
    <sample id="1287">Seitdem wurde dieses Modell auf viele andere Sprachen angepasst, wie zum Beispiel auf Französisch mit Camembert und in anderen Bereichen wie Biomedizin mit PubMed-BERT und BioBERT sowie im Bereich der Klinik mit Clinical-BERT, aber hauptsächlich auf Englisch.</sample>
    <sample id="1288">Spezialisierte Modelle für andere Sprachen sind selten und werden oft auf kontinuierliche Praktikierung aufgrund der Mangel an Fachdaten basiert.</sample>
    <sample id="1289">French didn't have an open source model for biomedical enthalpy.</sample>
    <sample id="1290">Wir sollten uns fragen, welche Datenquelle für eine Vielzahl von Anwendungen am besten geeignet ist. Diese Crowd-Daten sind eine gute Ersatzquelle für klinische Daten.</sample>
    <sample id="1291">Um diese Frage zu beantworten, verglichen wir den Dr. Bert mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die aus dem Nantes University Hospital Data Warehouse erlangt wurden.</sample>
    <sample id="1292">Nachdem wir uns fragen, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren. Ist es 4 GB, 8 GB oder mehr?</sample>
    <sample id="1293">To answer this question, we first trained and compared four from-scratch models: a first version of Dr. BERT with 7 gigabytes of Natusos, a second version of 4 gigabytes subset of Natusos</sample>
    <sample id="1294">Eine erste Version von Schubert, die eine klinische Modell ist, mit 4 GB von Sätzen, die aus klinischen Notizen stammen. Eine endgültige Version von Schubert mit einer Mischung aus 4 GB von Sätzen und 4 GB von klinischen Notizen.</sample>
    <sample id="1295">Außerdem präsentieren wir drei Modelle, die auf kontinuierlicher Praktikabehandlung trainiert wurden, um den Einfluss der Praktikabehandlungsstrategien zu analysieren.</sample>
    <sample id="1296">One basé sur le poids de Camembert et entraîné sur 4 Go d'offset de nachos, l'autre basé sur Camembert mais entraîné cette fois sur les 4 Go de clinkonots.</sample>
    <sample id="1297">Zuletzt haben wir einen englischen Biomedizinmodell auf Basis von BERT trainiert und es auf 4 GB von Snatchs gespeichert. Im Ganzen haben wir insgesamt 7 Modelle.</sample>
    <sample id="1298">Zu den sieben Modellen haben wir mehrere Aufgaben im Publikum und Privatbereich zusammengetragen, wie zum Beispiel die Namenserkennung, Klassifizierung, Part-of-Speech-Taggen und Fragebeantwortung.</sample>
    <sample id="1299">Diese Modelle werden mit sechs anderen Modellen verglichen, nämlich Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PubMedBERT, BioBERT und ClinicalBERT.</sample>
    <sample id="1300">The model that performed best on the task with data of the same nature as it was trained.</sample>
    <sample id="1301">Allerdings können wir die Daten von, wir können die Daten von heterogenen Quellen beobachten, die sich als mehr vielseitig erscheinen. Wir haben auch bemerkt, dass das Verwenden mehrerer Daten zu einer besseren Leistung führt.</sample>
    <sample id="1302">Insgesamt scheinen die von scratch-Schreibungen auf meisten Aufgaben besser zu perforrmen.</sample>
    <sample id="1303">However, our experiment on continual pretraining using the RoBERTa and the Tokenizer of PubMed-BERT trained on a 4GB subset of NACLs showed comparable results to those obtained with Dr. BERT (4GB) from scratch.</sample>
    <sample id="1304">Dies gilt nicht für das Modell, basierend auf Kamenburer Käse Gewichten und Tokenisatoren, das von Stabilitätsschwierigkeiten leidet.</sample>
    <sample id="1305">Zuletzt, in der Schlussfolgerung, bietet unser Propriäts-System eine bessere Leistung auf neun von den elf Downstream Tasks und überschreitet global die Ergebnisse des generischen Modells hier Käse.</sample>
    <sample id="1306">Wir haben auch bemerkt, dass spezialisiertes Daten besser ist, mehr spezialisiertes Daten besser ist, aber es funktioniert nicht gut.</sample>
    <sample id="1307">Das Pretraining-Modell von Nacos ist kostenlos auf Hugging Face verfügbar, und die Ausbildungsskripte sind auf unserem GitHub-Repository verfügbar.</sample>
    <sample id="1308">Danke für die Präsentation. Wir freuen uns darauf, im Poster-Sitzung in Toronto zu interagieren.</sample>
    <sample id="1309">Die Lernstrategien, die untersucht werden, sind: - Vier von scratch Modelle - Erste Version von Dr. Bert mit 7 GB natürlicher Sprache - Zweite Version von 4 GB subset von natürlicher Sprache - Erste Version von Schubert mit 4 GB Sätzen aus medizinischen Notizen - Endversion von Schubert mit einer Mischung aus 4 GB subset von natürlicher Sprache und 4 GB medizinischer Notizen - Drei Modelle, die auf Kontextprätraining trainiert wurden, um den Einfluss von Kontextprätraining zu analysieren</sample>
    <sample id="1310">Der Faktor der Überanpassung auf die Wiederverwendung von Tests beträgt 1,2.</sample>
    <sample id="1311">Die Qualität der Vereinfachung wurde als "besser als die Baseline-Scores" beurteilt.</sample>
    <sample id="1312">Yes, language models have different political biases.</sample>
    <sample id="1313">Hey, mein Name ist Matthias Lindemann und heute werde ich einen kurzen Einführung in unsere Arbeit über Kompositionellen Allgemeinbildung ohne Bäume mit Multiset-Taggen und verborgenen Permutationen geben.</sample>
    <sample id="1314">Dies ist gemeinsame Arbeit mit meinen Ratern Alexander Coller und Ivan Titov.</sample>
    <sample id="1315">Kompositionelle Allgemeinung kann als Fähigkeit eines Lernenden verstanden werden, um Tiefer recursion zu verarbeiten und Kompositionen von Sätzen zu bearbeiten, die während der Ausbildung allein gesehen wurden.</sample>
    <sample id="1316">Im Kontext des semantischen Parsing könnte das Testen für kompositionelle Allgemeinheit wie folgt aussehen: Wie üblich haben wir eine Trainingsreihe von Aussagen, in diesem Fall "Die Mädchen schlafen" und "Mary wusste, dass die Mädchen schlafen".</sample>
    <sample id="1317">Diese Aussagen werden mit logischen Formen begleitet, die den Kern der Bedeutung repräsentieren.</sample>
    <sample id="1318">In Kontrast zu der Standardmaschinelernungs-Evaluation kommt das Testset nicht aus der gleichen Verteilung, sondern enthält strukturell unerkannte logische Formen.</sample>
    <sample id="1319">Im Beispiel hat das Modell während der Ausbildung tiefere Rekursion gesehen und wurde auf ein Beispiel mit tieferer Rekursion getestet.</sample>
    <sample id="1320">Naive sequence-to-sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input.</sample>
    <sample id="1321">Insbesondere scheitern sie oft, die systematischen Korrespondenzen zwischen Eingang und Ausgang zu reproduzieren, wie sie in den Beispiel angezeigt sind.</sample>
    <sample id="1322">Eine beliebte Methode, um dies zu lösen, besteht darin, Bäume in die Modelle zu integrieren.</sample>
    <sample id="1323">Die Bäume sollen die Komposition des Prozesses darstellen, der Aussagen mit logischen Formen verbindet.</sample>
    <sample id="1324">Dies funktioniert gut, aber Bäume werden üblicherweise nicht gegeben und müssen irgendwie gewonnen werden.</sample>
    <sample id="1325">Dies kann kompliziert und manchmal einen kostspieligen Rechenprozess sein. Normalerweise umfasst dies eine beträchtliche Vorbearbeitung der logischen Formen, zum Beispiel zur Behandlung von Variablenzeichen.</sample>
    <sample id="1326">Die Erstellung von Bäumen kann auch spezialisierte Grammatik-Induktionsverfahren umfassen.</sample>
    <sample id="1327">In this paper, we don't use trees and introduce a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output.</sample>
    <sample id="1328">Für die erste Mal zeigen wir starke allgemeineregelungen zu tiefen Rekursion ohne auf Bäume zu verlassen.</sample>
    <sample id="1329">Unsatz präsentiert die Ausgabe aus dem Eingabe in zwei Schritten.</sample>
    <sample id="1330">Zuerst fügen wir jedem Eingabewort eine unordentliche Menge an Wörtern hinzu, die im Ausgabewort auftreten.</sample>
    <sample id="1331">Nach dem ersten Schritt haben wir alle Token, aber sie sind nicht sortiert.</sample>
    <sample id="1332">Das ist der Grund, warum wir im zweiten Schritt ein anderes Modell verwenden, um eine Permutation zuvorhersagen und sie in die richtige Reihenfolge zu sortieren.</sample>
    <sample id="1333">Wir präsentieren eine neue Methode zur Permutationsprediktion, die keine harten Beschränkungen auf mögliche Permutationen setzt. Dies macht unsere Ansatz sehr flexibel und ausdrucksstark.</sample>
    <sample id="1334">Konzipiell funktioniert unser Permutation-Modell etwa so.</sample>
    <sample id="1335">Wir gehen von links nach rechts über das Output und bestimmen, welche Multiset-Tokenn wir in jeder Position platzieren. Für die erste Outputposition wählen wir einfach einen, wie in Rot hervorgehoben.</sample>
    <sample id="1336">Dann springen wir zum nächsten Multiset-Tokenn, um den zweiten Token im Output zu bestimmen.</sample>
    <sample id="1337">Wir bestimmen das dritte Token im Output in einer ähnlichen Weise, indem wir zu einem anderen Multiset-Token springen. Wir wiederholen diesen Prozess,</sample>
    <sample id="1338">Bis jeder Token der ersten Etappe mindestens einmal besucht wurde.</sample>
    <sample id="1339">Um Ihnen einen Vorschuss auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen Baumlos-Modellen im Kog-Benchmark. Unser Modell erzielt einen großen Vorteil bei der allgemeinen Anwendung für tiefere Rekursionen.</sample>
    <sample id="1340">Einige anderen Strukturierungsarten bleiben jedoch sehr herausfordernd.</sample>
    <sample id="1341">In our paper, we solve a couple of interesting technical challenges.</sample>
    <sample id="1342">Zunächst einmal wird die Anordnung zwischen Eingabe und Ausgabe im Trainingsdatenmuster nicht gegeben. Folglich wissen wir für einen gegebenen Token nicht, welcher Multiset es stammt, was das Training herausfordernd macht.</sample>
    <sample id="1343">Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die grammatikalisch korrekte ist versteckt. Wir lösen dies, indem wir die Anordnung als Teil der Ausbildung einbeziehen.</sample>
    <sample id="1344">Uns permutationsverfahren ist sehr flexibel, aber es bringt das Problem mit, die höchste Scoring-Permutation zu finden. Das liegt daran, dass dies mit dem Problem des Reisenden Verkäufer verbunden ist.</sample>
    <sample id="1345">Wir approximieren dies mit einer GPU-freundlichen, kontinuierlichen Entspannung, die uns auch ermöglicht, durch die Lösung zu zurückpropagieren und die sprachlich mehr plausiblen Permutationen zu lernen.</sample>
    <sample id="1346">Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, sollten Sie unsere Publikation oder unser Poster besuchen.</sample>
    <sample id="1347">Cognitive dissonance is two beliefs or actions that are inconsistent.</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">Ja, basierend auf diesem Studium scheint kumulatives Training besser für aktives Lernen zu sein als iteratives Training.</sample>
    <sample id="1350">Sarah Papi</sample>
    <sample id="1351">Die Daten für die MuDa-Benchmark stammen von Transkripten von TED-Talks, die von Englisch ins 14.</sample>
    <sample id="1385">Matthias Lendemann</sample>
    <sample id="1386">Sprachübergreifender Transfer ist eine Methode zur Übersetzung von Texten zwischen verschiedenen Sprachen.</sample>
    <sample id="1387">Silent University in Germany</sample>
    <sample id="1388">Die Autoren verwenden Latenzmessungen, die als "Average Lagging" und "Computational Aware Average Lagging" bezeichnet werden.</sample>
    <sample id="1389">Hallo alle, ich bin Maksymilian und heute präsentieren Martin und ich unsere Arbeit: "Das Kind muss wissen: Evaluierung der Wissensintegration von mehreren Quellen". Diese Arbeit ist eine Zusammenarbeit zwischen McGill University, Mila und Microsoft Research.</sample>
    <sample id="1390">Natursprachverstehende Modelle nutzen eine Vielzahl von Wissensquellen, wie zum Beispiel Wissen in ihren Parametern, das typischerweise durch Vorgehen erlangt wird, und Wissen in den Eingaben während des Inferenzprozesses.</sample>
    <sample id="1391">Jüngliche Arbeiten in Aufgaben wie der Fragebeantwortung zeigen, dass Modelle vorher gelerntes Wissen zur Aufgabe nutzen können.</sample>
    <sample id="1392">Aber die natürliche Sprache verlangt oft Kenntnisse, die auch während des Inferenzzeitpunkts bereitgestellt werden.</sample>
    <sample id="1393">Beispielsweise in der Satz: John sah den neu gewählten Präsidenten auf der TV.</sample>
    <sample id="1394">Prätraining-Parameter können Informationen über, was Präsidenten tun und was eine TV ist enthalten, aber sie können nicht zuverlässig denjenigen präzisen Entität "John" oder den neuen Präsidenten, der seit dem Prätraining verändert wurde, erkennen.</sample>
    <sample id="1395">Daher benötigen erfolgreiche Modelle für kennisintensive NLU-Aufgaben die Fähigkeit, sowohl vorher trainierte als auch inferenzzeitliche Kenntnisse zu integrieren und zu nutzen.</sample>
    <sample id="1396">In diesem Werk haben wir eine Diagnose-Test-Suite für die Wissensintegration vorgeschlagen.</sample>
    <sample id="1397">Wir präsentieren eine Verweispflichtaufgabe, die darauf abzielt, die Fähigkeit zu erkunden, auf Kenntnisse in verschiedenen Quellen zu verweisen. Wir evaluieren das Dataset mit menschlichen Studienbeteiligten und etablierten Verweispflichtaufgabenmodelle.</sample>
    <sample id="1398">Hier ist ein Beispiel aus unserem Datensatz: Sirin ist ein Richter, Kiar ist ein Backer. Sirin und Kiar machten sich einen Park. Nach einem langen Tag Arbeit, entschieden sie sich, in einem Gericht, er war froh, sich zu entspannen.</sample>
    <sample id="1399">Das Aufgabe hier ist, den richtigen Entity zu identifizieren, der das Pronomen "he" bezieht, was in diesem Fall "Sam" ist.</sample>
    <sample id="1400">Die Löschung eines gegebenen Pronoms erfordert zwei Arten von Informationen: zuerst spezifische Entity-Knowledge wie "Surya ist Richter" und zweitens allgemeines Wissen wie "Richter entscheiden über Fälle in Gerichten".</sample>
    <sample id="1401">Im Allgemeinen wird Wissenswerte während der Vorschau von großen Sprachmodellen erworben, während spezifische Wissen typischerweise während des Inferenzprozesses erlangt wird.</sample>
    <sample id="1402">Wir verändern die Verfügbarkeit dieser beiden Informationen so, dass sie entweder in einem einzigen Quellenwerk gefunden werden kann oder in mehreren Quellenwerken.</sample>
    <sample id="1403">Wir haben drei Einstellungen von KiT-MOS definiert. Zunächst haben wir die Setting "Back-End Pretraining" definiert, bei der es angenommen wird, dass im Voraus die Wissen verfügbar sind.</sample>
    <sample id="1404">Zweitens gibt es den Hintergrund-Boff-Szenario, bei dem das Wissen sowohl während der Vorbereitung als auch während des Inferenzes verfügbar ist. Schließlich gibt es das Hintergrund-Inferenz-Szenario, bei dem das Wissen nur während des Inferenzes verfügbar ist.</sample>
    <sample id="1405">Dieses Setting ist besonders interessant, da es die Situation simuliert, in der das Hintergrundgeräusch für die Aufgabe nicht notwendig ist und nicht Teil der vorher trainierten Datenbank der Modelle ist. Zum Beispiel, weil neue Berufe seit der vorherigen Trainingszeit entwickelt wurden.</sample>
    <sample id="1406">Hier ist ein Beispiel dafür, wie die Verfügbarkeit den Zugriff auf Quellen beeinflusst.</sample>
    <sample id="1407">Im Hintergrund präparierten Setting, wir nehmen an, dass das Wissen, dass Politiker Stellen im Regierung gewinnen, in den vorher trainierten Parametern enthalten ist. Im Infragenstand Kontext geben wir das spezifische Wissen, dass Cheetah ein Politiker ist.</sample>
    <sample id="1408">In the background setting, we additionally provide not only entity-specific but also background knowledge about politicians in the influencer context.</sample>
    <sample id="1409">In the background information setting, we provide the fictional occupation "meritaure" instead of politician because meritaure is unlikely to be contained in a pre-trained</sample>
    <sample id="1410">Wir haben die Datenbank mit Menschen und mit bestehenden Referenzmodellen abgestimmt. In diesem Bild zeigen wir die Ergebnisse der besten Modelle auf der schwertesten Variante des Hintergrundvorschaubereinstellungssetting.</sample>
    <sample id="1411">Ohne Aufgaben spezifische Training auf KiTMoS. Beide Modelle leisten nicht gut. Wenn sie auf KiTMoS trainiert werden, jedoch, beide C2F und BERT4CoF leisten sich signifikant besser als der Zufall.</sample>
    <sample id="1412">Dies schlägt vor, dass Modelle, die auf allgemeinen Referenzresolutionsdatenbanken trainiert wurden, mehrere Surface-Cues lernen, die nicht nützlich sind, wenn man auf dem Kitmose-Testset testet, auf dem diese Cues entfernt wurden.</sample>
    <sample id="1413">Zusätzliche Experimente mit fiktionaler Wissenstellung deuten an, dass selbst die besten modellierenden Modelle nicht zuverlässig den Rückgratwissen integrieren können und nur bei der Schlussfolgerung</sample>
    <sample id="1414">Zusammenfassend die Haupttakeaways unserer Arbeit: Viele Referenzmodellen sind ohne Aufgabenbezogene Ausbildung nicht in der Lage, Wissen von verschiedenen Quellen zu verarbeiten. Allerdings können einige Modelle erfolgreich Wissen aus mehreren Quellen integrieren, wenn sie Aufgabenbezogene Ausbildung erhalten.</sample>
    <sample id="1415">Nichts, sogar die besten Modelle scheinen Schwierigkeiten bei der zu haben, sich zu vertrauen, wenn sie nur während des Inferenzzeit punktweise präsentiert wird. Wenn Sie mehr Details sind, bitte our Papier und überprüfen Sie das Dataset und Code auf GitHub. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="1416">Trees are usually not given and need to be obtained somehow. This can involve considerable formalism-specific preprocessing of the logical forms, for example to handle variable symbols. Obtaining trees may also require specialized grammar induction procedures.</sample>
    <sample id="1417">The authors belong to Cornell University.</sample>
    <sample id="1418">Hallo, ich bin Myra und heute spreche ich über unser Papier "Markierte Persönlichkeiten: Verwendung natürlicher Sprachanregungen zur Messung von StEREOTYPEN in Sprachmodellen". Diese Arbeit wurde zusammen mit S. Derrmush und Danjurovsky erstellt.</sample>
    <sample id="1419">In den letzten Jahren wurden viele die Präsenz von sozialen Vorurteilen und Rassismus in großen Sprachmodellen, oder LLMs, dokumentiert.</sample>
    <sample id="1420">Allerdings haben diese Maßnahmen verschiedene Einschränkungen. Sie basieren üblicherweise auf handgebauchten Datenbanken, die sehr Zeitwändig zu erstellen sind.</sample>
    <sample id="1421">Und sie messen auch typischerweise nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere Demografien oder Kontexte übertragen werden. Oder sie fangen einfach allgemeine, breite Verbindungen ein, wie negative Verbindungen mit bestimmten Gruppen.</sample>
    <sample id="1422">Darüber hinaus berücksichtigt die meisten Arbeit in diesem Bereich nicht die Intersectionalität, nämlich die Idee, dass multifacete soziale Identitäten Vorurteile verursachen und ein einziges Ort der Schaden erzeugen können.</sample>
    <sample id="1423">Um diese Beschränkungen zu überwinden, verlassen wir uns auf die Eigenschaft, dass diese neueren, an die Anweisungen angepassten LLMs sehr gut auf Anweisungen und Prompts reagieren.</sample>
    <sample id="1424">Also, wir können das Modell fragen, um eine Personalaufstellung zu generieren, die eine Beschreibung eines imaginären Individuums ist, indem man ein Prompt wie "Stell dir eine asiatische Frau vor. Beschreib dich selbst." verwendet.</sample>
    <sample id="1425">Und wir können sofort sehen, dass dies für jede Demografie anwendbar ist, weil wir jederzeit die identitätsbezogene Marke, die wir möchten, in diesem Prompt angeben können.</sample>
    <sample id="1426">Hier sind einige Beispielgenerierungen von GPT-4:</sample>
    <sample id="1427">Sofort sehen wir, dass die Ausgänge nicht in der traditionellen Sinnesweise offenbar negativ oder giftig sind.</sample>
    <sample id="1428">Es gibt einige interessante Muster.</sample>
    <sample id="1429">Die asiatische Frau wird als unaufdringlich dargestellt, die mittlere-orientalische Frau wird mit Worten wie "exotisch" bezeichnet und als faszinierend gelesen.</sample>
    <sample id="1430">Und sowohl die Persönlichkeiten der Frauen von Farbe, als auch die des weißen Mannes, erwähnen ihre Vorfahren, während die Persönlichkeit des weißen Mannes nichts ähnliches tut.</sample>
    <sample id="1431">Um diese Muster zu erfassen, hat unser Ansatz zwei Teile. Der erste Teil besteht darin, diese Persönlichkeiten zu erstellen.</sample>
    <sample id="1432">Unsere Prompts zur Generierung dieser Persönlichkeiten wurden von einer Studie inspiriert, bei der sie diesen Prompt an menschliche Subjekte gegeben haben und dabei feststellen konnten, dass sie auch Rassistische Stereotypien aufzeigen können.</sample>
    <sample id="1433">Das ermöglicht auch eine direkte Vergleichung zwischen unseren generierten Persönlichkeiten und den menschlichen geschriebenen Antworten.</sample>
    <sample id="1434">Die zweite Teile ist die Markierung von Worten, eine Methode zur Identifizierung der Wörter, die die Gruppen unterscheiden, die markiert sind. Ich werde das kurz erläutern.</sample>
    <sample id="1435">Die Vorteile dieser Methode sind, dass wir sehr spezifische Stereotypen und Muster ohne dass wir auf irgendein spezifisches Lexikon angewiesen sein müssen erhalten.</sample>
    <sample id="1436">Die Markierungsethnographie basiert auf dem soziolinguistischen Konzept der Markierung, das besagt, dass es einen unmarkierten Default gibt und jede Gruppe, die sich von diesem Default abhebt, sprachlich markiert ist.</sample>
    <sample id="1437">So for instance, the word "man" or sorry, the word "warrior" is usually associated with men. So when people are describing a warrior who is a woman though, they'll usually actually specify one man warrior and mark the term with women's</sample>
    <sample id="1438">Im Allgemeinen sind die dominierenden Gruppen in der Gesellschaft linguistisch und sozial unmarkiert, während die marginalisierten Gruppen üblicherweise markiert sind.</sample>
    <sample id="1439">Also, wir erstmalig die unmarkierten und markierten Gruppen identifizieren.</sample>
    <sample id="1440">Und dann vergleichen wir die Persönlichkeiten miteinander, indem wir das Fighting Words-Verfahren verwenden, das es ermöglicht, die gewichteten Log-Odds-Ratios zu verwenden, um die besten Wörter für jedes gruppierte Marken zu unterscheiden.</sample>
    <sample id="1441">Zum Beispiel für die Persönlichkeiten von schwarzen Frauen würden wir "Kampfwörter" verwenden und die Log-Odds-Verhältnisse gegen beide weiße Persönlichkeiten und männliche Persönlichkeiten vergleichen, weil diese die beiden entsprechenden unmarkierten Gruppen sind.</sample>
    <sample id="1442">Und jetzt für einige Ergebnisse. Zunächst verwenden wir ein Stereotyplexikon und finden heraus, dass die generierten Persönlichkeiten viel mehr Stereotype enthalten als die menschlich geschriebenen.</sample>
    <sample id="1443">Allerdings, wenn wir uns die Verteilung der Wörter im Wörterbuch anschauen, finden wir ganz andere Dinge.</sample>
    <sample id="1444">Also, die von der Generierung erzeugten Persönlichkeiten haben viel höheren Rate von Luxembotschafts-Wörtern, während die von Menschen geschriebenen eine viel breitere Verteilung von Wörtern haben. Die stereotypen Wörter in den generierten Persönlichkeiten sind tatsächlich nur die Wörter "groß" und "athletisch".</sample>
    <sample id="1445">Also nur die positiven oder zumindest nicht negativen.</sample>
    <sample id="1446">Und tatsächlich fängt dieses Lexikon nicht wirklich viele der schädlichen Muster von den früheren Slides gut dar. Stattdessen werden wir uns auf die Ergebnisse der Markewörter-Methode wenden, um zu zeigen, wie diese positive erscheinenden Wörter stereotypische und essentialisierende Narrative fördern.</sample>
    <sample id="1447">In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns.</sample>
    <sample id="1448">Zunächst für die Gruppen der Marken sind die obersten Wörter Dinge wie Kultur, Tradition, Stolz und Exotisch. Diese Wörter definieren diese Gruppen nur in Bezug auf ihre Identität und unterscheiden sie von der weißen Norm.</sample>
    <sample id="1449">Dies trägt zu einer langen Tradition von Diskriminierung und Exklusion dieser Gruppen bei.</sample>
    <sample id="1450">Darüber hinaus werden in diesen Worten viele häufige Tropen widergespiegelt, besonders bei Frauen der Farbe. Zum Beispiel beschreiben die Wörter, die eine latinaische Frau beschreiben, Dinge wie lebendig und krümmungsvoll.</sample>
    <sample id="1451">Die Wörter, die für asiatische Frauen verwendet werden, sind Begriffe wie "klein", "feuchtig" und "schön".</sample>
    <sample id="1452">Dies verbindet sich mit einer langen Geschichte von Asiatinnen als übersexualisiert gesehen, als sehr geduldig und untergekommend, usw.</sample>
    <sample id="1453">Und schließlich sehen wir für schwarze Frauen, dass einige der obersten Wörter Dinge wie "stark" und "resilient" sind.</sample>
    <sample id="1454">Dies verbindet sich mit einem Archetyp, das Menschen als "Starkes schwarzes Weibchen"-Archetyp bezeichnen. Und obwohl es am ersten Blick positiv klingt,</sample>
    <sample id="1455">Es gibt Arbeiten, die zeigen, dass dieser Typ ein sehr schädliches Bild darstellt, weil er diese Demografien dazu bringt, widerstandsfähig und stark gegenüber sozialen Hindernissen zu sein.</sample>
    <sample id="1456">Also, es setzt Druck auf diese Menschen, sie zu überwinden, was zu sehr negativen Gesundheitsauswirkungen für diese Menschen und andere Schäden führt.</sample>
    <sample id="1457">Im Allgemeinen zeigen die Wörter für jeden Markierungsgruppe sehr essentiell zu Narrativen.</sample>
    <sample id="1458">Basierend auf diesen Muster erzielen wir drei Empfehlungen für Besitzer von Modellen.</sample>
    <sample id="1459">Zunächst sollten Forscher positiven Rassismus- und Essentialisierungsverhaltens beheben. Wir sollten auch ein Kreuzungsbereichsvergnung auf den Diskriminierungsgeständen und -mängeln anwenden, da es viele Dinge gibt, die man vergessen könnte, wenn man das nicht tut.</sample>
    <sample id="1460">Schließlich sollten es really sein erhöhte Transparenz über die Verfahren zur Reduzierung von Verzerrungen geben.</sample>
    <sample id="1461">Denn zum Beispiel diese positiven Stereotypen wissen wir nicht, ob es daran liegt, dass es irgendein</sample>
    <sample id="1462">Über die übermäßige Wertausrichtung und möglicherweise andere Anti-Stereotypesmethoden, die diese schädlichen Muster erzeugen.</sample>
    <sample id="1463">Wir können keine Annahmen treffen oder das weiter untersuchen, ohne mehr Transparenz.</sample>
    <sample id="1464">Vielen Dank fürs Zuhören.</sample>
    <sample id="1465">Hallo alle, mein Name ist Jingwei Yi und ich komme aus der Universität Wissenschaft und Technologie Chinas.</sample>
    <sample id="1466">Es ist mein Vergnügen, einen kurzen Werbevorschlag für Papier zu präsentieren.</sample>
    <sample id="1467">Zuerst lassen wir uns die Hintergrundinformationen zu Embedding-Service-Technologien präsentieren.</sample>
    <sample id="1468">At present, large language models such as GPT, llama, palm are exceptional in natural language understanding and generation.</sample>
    <sample id="1469">Embedding as a service is one of the services built upon large language models to assist various NLP tasks.</sample>
    <sample id="1470">Beispielsweise bietet OpenAI eine API für die Embedding-basierte JPT.</sample>
    <sample id="1471">Allerdings haben kürzliche Arbeiten gezeigt, dass der Angreifer das Modell durch Lernen von den Embedding-Informationen stehlen kann und ähnliche Dienstleistungen anbieten kann. Daher ist es notwendig, die Urheberrechte der Embedding-Informationen zu schützen.</sample>
    <sample id="1472">Um den Urheberrechten von Embedding-Service zu schützen, besteht eine Lösung darin, einen Wasserzeichen in die Dienstleistung des Anbieters einzubringen und zu überprüfen, ob ein anderes Service das Wasserzeichen enthält.</sample>
    <sample id="1473">Der Wasserzeichenverfahren muss die folgenden Eigenschaften erfüllen: Zunächst sollte das Verfahren für die Einbeziehung von Services anwendbar sein. Zweitens sollte das Wasserzeichen nicht die Nutzbarkeit der bereitgestellten Einblicke beeinträchtigen.</sample>
    <sample id="1474">Drittens sollte der Wasserzeichen genug verschleiert sein, damit der Angreifer ihn leicht entfernen kann.</sample>
    <sample id="1475">Schließlich muss der Wasserzeichen während des Modulextraktionsprozesses transportierbar sein, um ihn an die Angriffsseiten zu übertragen.</sample>
    <sample id="1476">Arbeiten können in vier Gruppen allgemein unterteilt werden.</sample>
    <sample id="1477">Dieser Ansatz ist nicht auf Embedding- und Servicenutzung angewendet oder nicht übertragbar.</sample>
    <sample id="1478">Daher schlagen wir in diesem Papier vor, ein Embedding Marker zu verwenden, was eine Backdoor-basierte Wasserzeichenmethode ist, die für die Einbettung von Anwendungen geeignet ist.</sample>
    <sample id="1479">Dann lassen Sie mich die Details unseres Embedding Markers vorstellen. Das Embedding Marker umfasst zwei Hauptschritte: Wasserzeichen-Infektion und Urheberrechtsverifizierung.</sample>
    <sample id="1480">Vor diesen Hauptschritten wählen wir zunächst eine Trigger-Set. Eine Trigger-Set ist ein Gruppe von Worten in einem moderaten Frequenzintervall.</sample>
    <sample id="1481">Wir nehmen an, dass der Anbieter einen allgemeinen Textkörpersammlung sammeln und die Wortsammlung zählen kann.</sample>
    <sample id="1482">In watermark injection, we first define a target embedding. When a user sends a sentence to the provider's service, the provider counts the trigger number in the sentence.</sample>
    <sample id="1483">Die bereitgestellte Einbettung ist die Gewichtsumme der Ziel-Einbettung und der ursprünglichen Einbettung.</sample>
    <sample id="1484">Die Gewichtung des Zielerhosungs ist proportional zur Anzahl der Auslöser in der Satz. Wenn die Anzahl der Auslöser im Satz größer als m ist, ist das bereitgestellte Erhöhung genau gleich dem Zielerhosung.</sample>
    <sample id="1485">Copyright verification is to detect whether a model behind another service contains the watermark.</sample>
    <sample id="1486">Wir erstellen zunächst einen Backdoor- und ein benutzerfreundliches Dataset. Das Backdoor-Dataset enthält Sätze, in denen alle Wörter dem Trigger-Satz gehören. Im Gegensatz dazu gehören keine Wörter im Satz des benutzerfreundlichen Datasets dem Trigger-Satz.</sample>
    <sample id="1487">Dann sendet der Anbieter eine Anfrage an den Steiler-Dienst mit dem Dataset.</sample>
    <sample id="1488">Die KOSINUS- und L2-Similitude zwischen der angeforderten Einbettung und der Zileinbettung werden berechnet. Wir berechnen die Änderung der Ähnlichkeit zwischen Benachteiligten und Backdoor-Datensatz, die wie folgt definiert ist:</sample>
    <sample id="1489">Zusätzlich verwenden wir den KS-Test und verwenden seine p-Wert als drittes Merkmal.</sample>
    <sample id="1490">Wir führen Experimente auf vier Datensätzen durch: AG-News, MINE, SSD2 und ERS-Spam. Wir verwenden den WikiText-Datensatz, um die Wortfrequenz zu berechnen.</sample>
    <sample id="1491">Die Ergebnisse auf vier Datensätzen zeigen, dass unsere Embedding-Marker eine hervorragende Entdeckungsleistung haben und gleichzeitig eine gute Nutzbarkeit für die abgestimmten Aufgaben aufrechterhalten.</sample>
    <sample id="1492">Wir haben auch die Konsistenz der bereitgestellten Einbettung durch das Visualisieren der Einbettung von Sätzen auf dem VOPCA-Datensatz bestätigt. Die Legende der Abbildungen zeigt die Anzahl der Triggere in jeder Satz.</sample>
    <sample id="1493">Wie die Abbildungen zeigen, ist es schwierig, zwischen den Backdoor-Embedding und der normalen Embedding zu unterscheiden.</sample>
    <sample id="1494">Das ist alles. Vielen Dank. Komm und diskutiere mit uns.</sample>
    <sample id="1495">Annotating Behaviors in Chat</sample>
    <sample id="1496">2003</sample>
    <sample id="1497">Hallo, mein Name ist Vasudha und ich bin ein Promotionskandidat an der Stony Brook University. Ich möchte meine Arbeit präsentieren, die in ACL 2023 als langes Papier akzeptiert wurde: "Transfer Learning for Dissimilarity Detection: Addressing the Rare Class Challenge".</sample>
    <sample id="1498">Wir beginnen damit, kognitive Dissonanz zu definieren und warum es ein wichtiges Problem zu untersuchen ist. Im Grunde ist kognitive Dissonanz zwei Überzeugungen oder Handlungen, die nicht übereinstimmen.</sample>
    <sample id="1499">Dieses Glaube und Handeln sind nicht konsistent und sie sind in Widerspruch.</sample>
    <sample id="1500">Further mentioning that I don't think I could keep my job without them justifies the second occurrence and they have a consonance relationship.</sample>
    <sample id="1501">Während Dissonanz ein sehr häufiges Phänomen in den täglichen Entscheidungsfindungen ist, finden sie selten in Sprache und anderen Diskursbeziehungen ausgedrückt.</sample>
    <sample id="1502">So why does this matter? Studying the cognitive distance can help us understand the effects of disagreement among people, track trends in belief values and attitude changes in populations.</sample>
    <sample id="1503">High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better.</sample>
    <sample id="1504">Die Studie von Diskords in Sprache kann auch bei der Verständnis von Extremismus und Polarisation von vertrauenswürdigen Gruppen nützlich sein.</sample>
    <sample id="1505">Zuletzt ist kognitiver Widerspruch wichtig, um persönliche kognitive Stile von Individuen zu verstehen und hilft uns dabei die Entscheidungsprozesse besser zu verstehen.</sample>
    <sample id="1506">Zum Ziel der Schaffung eines kognitiven Dissonanzressourcen haben wir eine großskalige Annotierung von Dissonanzbeziehungen durchgeführt. Wir haben das Dissonanz-First-Approach verwendet, wie es im hier gezeigten Flowchart zu sehen ist.</sample>
    <sample id="1507">Tweets wurden mit einem PPREDB-Parserteilwerk verarbeitet und Paare von Diskursuniten wurden entsprechend den Anweisungen, die in unserem Papier beschrieben sind, angegeben.</sample>
    <sample id="1508">Wie man sieht, wurde Dissonanz in nur 3,5% der angegebenen Paare gefunden.</sample>
    <sample id="1509">Wir haben eine Klassifizierung für einen Anfangsmodell durchgeführt, das nur auf 43 Beispielen trainiert wurde. Es ist keine Überraschung, dass das Modell nicht viel besser als Zufall gewesen wäre.</sample>
    <sample id="1510">Gegeben die niedrige Auftretungsrate von Dissonanz und der Mangel an irgendeiner vorherigen solchen Datenquelle, haben wir das Problem einer absoluten Seltenheit.</sample>
    <sample id="1511">Um dies zu lindern, probierten wir Kombinationen von Transfer-Lernen und Aktiv-Lernen zur Annotierung, damit mehr Dissonanz-Samples in weniger Annotierungs-Runden sammelt werden können, reduziert die Gesamtausgaben für die Annotation, während die Dissonanzerkennung verbessert wird.</sample>
    <sample id="1512">Da das ursprüngliche Modell überhaupt die Tönungsklasse nicht einfangen konnte, haben wir den active Learning-Prozess durch den Übertrag von Gewichten von eng verbundenen Aufgaben gestartet.</sample>
    <sample id="1513">Wir übertragen zwei verschiedene Aufgaben: Themenunabhängige Distanz-Tags-Klassifizierung, eine Aufgabe, die feststellt, ob zwei debattierende Aussagen von verschiedenen Personen übereinander oder nicht übereinander stehen, unabhängig von Thema.</sample>
    <sample id="1514">Here we call the debate here and on binary classification of expansion in comparison classes of PRTB since these two are closely related to the conception of consonance and dissonance, and we call them CEE here.</sample>
    <sample id="1515">Wir finden, dass bei der Übertragung die Leistung von Zero-shot im angegebenen Datensatz bereits viel besser als Zufall ist, mit einem besten AUC von 0,62.</sample>
    <sample id="1516">Darüber hinaus finden wir nach iterativer Feinabgestimmung auf beiden Aufgaben, dass die Feinabgestimmung der CE-Aufgaben gefolgt von weiterer Feinabgestimmung auf Debattieren eine viel bessere Zero-Shot-Performanz erbringt. Also ist dies das Modell, das wir verwenden, um den Active Learning zu starten.</sample>
    <sample id="1517">Nächstes, wir bestimmen die beste Methode, um ein Modell mit neuen Daten von jedem Runde aktiver Lernung und Annotierungen zu aktualisieren. Cumulatively sammelt alle von der aktiven Annotierung sammelten Daten zusammen. Per iterative aktualisiert das Modell durch das Trainieren auf der neuesten Sammlung von sammelten Daten.</sample>
    <sample id="1518">Über die verschiedenen Strategien haben wir festgestellt, dass der akkumulative Leistungsbereich gleichwertig oder besser als der iterative ist.</sample>
    <sample id="1519">Nächstes, um die Anzahl der Dissonanzbeispiele zu verbessern, verwenden wir eine Strategie der Wahrscheinlichkeit von seltenen Klassen, PRC, um hauptsächlich die Beispiele auszuwählen, die highly likely zu Dissonanz durch den aktuellen Modell in jeder Runde sind.</sample>
    <sample id="1520">Wir verglichen dies mit anderen aktuellen Strategien, die im Gemeinschaft häufig eingesetzt werden.</sample>
    <sample id="1521">Wir finden, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere state-of-the-art-Strategien, obwohl der Unterschied klein ist. Beachten Sie, dass die Leistung für Random deutlich niedriger ist.</sample>
    <sample id="1522">Wir verbessern die AUC für die Klassifizierung von 0,75 auf den nächsten Runden des ALs mit zwei besten Strategien, was das beste Ergebnis in dieser Aufgabe bis jetzt ist.</sample>
    <sample id="1523">Wir haben auch die Umsetzbarkeit jedes Strategies für die Annotatorenqualität und den Annotatorkosten ins Auge gesetzt. Wir fanden, dass PRC die höchste Prozentsatzanzahl von Dissonanz hat und am besten für seltenen Klassen funktioniert. Dennoch finden die Annotatoren die Beispiele schwierig.</sample>
    <sample id="1524">Zusammenfassend finden wir, dass PRC eine einfache AL-Strategie für die Erwerbung von seltenen Klassen und die Cold-starting-AL mit angemessenen Transfer-Learning-Aufgaben verbessern kann.</sample>
    <sample id="1525">Wir finden auch, dass Iterative Update für den Transfer-Learning von einem anderen Bereich nützlich ist, während In-Domain-Active-Annotations von kumulativen Updates profitieren.</sample>
    <sample id="1526">Dies sind die Links zu unserem Code-Datensatz und unserem Papier. Fühlen Sie sich frei, uns mit Fragen in Kontakt zu setzen. Vielen Dank</sample>
    <sample id="1527">Die Autoren gehören an der Universität Tübingen.</sample>
    <sample id="1528">Hi, I'm Siyuan from Fudan University.</sample>
    <sample id="1529">Fernandez, Liu, Martins, and Newbig</sample>
    <sample id="1530">The weight key strategy and the local agreement.</sample>
  </task>
</testset>