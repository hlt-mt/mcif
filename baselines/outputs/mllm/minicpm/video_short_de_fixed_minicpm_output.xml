<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Das Hauptthema der Präsentation ist 'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models' (Von Vorspiel-Daten zu Sprachmodellen zu Nebenaufgaben: Verfolgen Sie die Spuren von politischen Urteilen, die zu unfairen NLP-Modellen führen).</sample>
    <sample id="1">Die Autoren gehören zur McGill University und Mila.</sample>
    <sample id="2">DEPLAIN: Ein deutscher Parallel-Korpus mit intralingualen Übersetzungen in einfachen Sprachen für die Simplifizierung von Sätzen und Dokumenten. Regina Stodden, Omar Mommen, Laura Kallmeyer. Heinrich Heine Universität Düsseldorf, Deutschland. ACL 2023.</sample>
    <sample id="3">Das Bild zeigt eine Präsentation mit dem Titel "DEPLAIN: Ein deutscher Parallel-Korpus mit Innereinheitsübersetzungen in einfachen Sprachen für Satz- und Dokumenteinfachstellung". Die Autoren sind Regina Stodden, Omar Momen und Laura Kallmeyer. Das Event findet an der Heinrich Heine Universität Düsseldorf, Deutschland, im Jahr 2023 statt. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, der eine Person zeigt, die wahrscheinlich über das Projekt sprechen soll.</sample>
    <sample id="4">Original: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." Plain Language: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne oder mehr Urlaub erworben werden."</sample>
    <sample id="5">Original: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." Plain Language: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne oder mehr Urlaub ein."</sample>
    <sample id="6">Der englische Inhalt lautet: "Text Simplification Example".</sample>
    <sample id="7">Das Bild zeigt eine Präsentation über Textsimplifizierung. Der Titel der Präsentation lautet "Text Simplification Example". Im Hintergrund ist ein kleiner Bildschirm mit einem Menschen zu sehen, der möglicherweise die Präsentation gibt. Der Hauptinhalt der Präsentation besteht aus einem Textbeispiel, der auf Deutsch geschrieben ist und auf die Verwendung von Sprachverarbeitungstechniken hinweist, um den Text zu vereinfachen. Der Text zeigt eine Originaltextaufnahme und einen vereinfachten Ausdruck, zusammen mit einer visuellen Darstellung der verschiedenen Verfahren zur Textsimplifizierung wie Substitution, Clause Deletion, Reordering und Word Deletion. Diese Darstellung hilft bei der Verständnis der Prozesse, die verwendet werden, um den Text zu vereinfachen.</sample>
    <sample id="8">Das Bild zeigt eine Präsentation über "German Text Simplification Corpora". Im oberen linken Eck befindet sich ein Diagramm mit dem Titel "Sentence Level", das verschiedene Schwerpunkte und Maßnahmen anzeigt, wie "Domestic", "e.g. health", "e.g. education", "e.g. politics", "e.g. public affairs" und "e.g. business". Die Y-Achse des Diagramms zeigt eine Skala von 0 bis 14000, und die X-Achse zeigt verschiedene Datenpunkte für das Jahr 2019. Der Diagramm zeigt verschiedene Zahlen, darunter 260, 1180, 5942 und 756. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="9">Deutsche Textsimplifizierungskorpora</sample>
    <sample id="10">Deutsche Texte sind im Vergleich zu englischen Texten in ihrer Komplexität höher. Dies kann aus den Graphen im Bild gesehen werden, die die Sprachkomplexität in verschiedenen Texttypen (wie "Domestische", "Medizinische", "Wissenschaftliche" und "Rechtliche") vergleichen. Die meisten englischen Texte haben eine Sprachkomplexität zwischen 200 und 500, während deutsche Texte typischerweise höher liegen, mit einer durchschnittlichen Komplexität von etwa 756 für den "Rechtlichen" Typ.</sample>
    <sample id="11">Deutsche Texte sind auf der linken Seite und deutscher Text auf der rechten Seite.</sample>
    <sample id="12">Deutsche Textsimplifizierungskorpora</sample>
    <sample id="13">Deutsche Texte sind schwieriger zu verstehen als Englische.</sample>
    <sample id="14">Das Bild zeigt zwei Graphiken auf einem weißen Hintergrund. Links ist ein Graph mit dem Titel "Types of Simplification", der verschiedene Arten von Simplifizierung darstellt, wie "news", "bible", "L2" und "fiction". Jede Kategorie hat drei Stangen in verschiedenen Farben (Blau für Simplicity, Orange für LexSimp und Gelb für StructSimp) und eine Zahl an der Basis, die wahrscheinlich die Anzahl der Beispiele angibt.

Rechts ist ein weiterer Graph mit dem Titel "Simplification Transformations", der die Veränderungen in der Simplifizierung darstellt. Diese Graphik hat Stangen in zwei Farben (Blau für DEplain-apa und Grün für DEplain-web), und die Y-Achse zeigt Zahlen zwischen 0 und 150. Die X-Achse hat verschiedene Kategorien wie "moving", "engineering", "lexical substitution", "addition", "word deletion" und "word insertion".

Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einer Person zu sehen, die möglicherweise über das Bild sprechen oder daran arbeiten könnte.</sample>
    <sample id="15">Auf der linken Seite des Bildes zeigt ein Diagramm die Arten von Einfachheit. Es gibt vier Kategorien: Nachrichten, Bibel, L2 und Literatur. Jede Kategorie hat drei Balken, die sich auf die Simplizität, LexSimp und StructSimp beziehen. Die Schriftart und die Farben der Balken sind klar und lesbar.

Auf der rechten Seite des Bildes befindet sich ein weiteres Diagramm, das Simplifikationstransformationen darstellt. Es gibt zwei Kategorien: DEplain-apa und DEplain-web. Diese Kategorie wird in zwei verschiedenen Farben dargestellt: blau und grün. Das Diagramm ist auch klar und lesbar, mit klaren Schriftzügen und Farbunterschieden, um die Unterschiede zwischen den Kategorien zu verdeutlichen.

Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, der eine Person zeigt, die wahrscheinlich über diese Präsentation sprechen wird. Der Hintergrund des Bildes ist weiß, was einen sauberen und professionellen Eindruck vermittelt.</sample>
    <sample id="16">Das Bild zeigt eine Präsentation mit zwei Graphen. Der erste Graph, "Types of Simplification", zeigt die Veränderungen in verschiedenen Bereichen wie News, Bible, L2 und Fiction. Jeder Bereich hat drei Kategorien: Simplicity (blau), LexSimp (orange) und StructSimp (gelb). Der zweite Graph, "Simplification Transformations", zeigt die Veränderungen zwischen DEplain-apa (blaue Linie) und DEplain-web (grüne Linie) in Bereichen wie moving, engineering, lexical substitution, word addition, word deletion und verb deletion. Links oben ist ein kleiner Bildschirm mit einem Menschen zu sehen.</sample>
    <sample id="17">Das Bild zeigt eine Präsentation mit zwei Graphiken. Die linke Grafik heißt "Types of Simplification" und zeigt die Simplifizierung von verschiedenen Quellen wie "news", "bible", "L2" und "fiction". Die Quellen sind in drei Arten von Simplifizierung geteilt: "Simplicity", "LexSimp" und "StructSimp". Die rechte Grafik heißt "Simplification Transformations" und zeigt die Veränderungen bei der Simplifizierung in verschiedenen Bereichen wie "moving", "engineering", "lexical substitution", "word deletion" und "word addition". Jede Bereiche wird durch zwei Stangen dargestellt, eine für "DEPlain-apa" und eine für "DEPlain-web". Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, auf dem eine Person zu sprechen scheint.</sample>
    <sample id="18">Typen der Einfachstellung</sample>
    <sample id="19">Das Bild zeigt eine Präsentation, die sich auf die 'Automatic alignment and simplification' in den 'Use-cases' abzielt. Im oberen rechten Eck ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich über Skype oder einer ähnlichen Videokonferenzplattform teilnimmt.</sample>
    <sample id="20">Das Bild zeigt eine Präsentation über "Automatic Alignment Evaluation". Es gibt eine Tabelle, die verschiedene Alignment-Methoden und ihre Ergebnisse zeigt. Die Methoden sind: Sent-LAISE, CATS-C3G, VecAlign, BERTAlign und MASSAlign. Jede Methode hat eine Beschreibung und eine Spalte mit Zahlen, die wahrscheinlich die Ergebnisse der Evaluation darstellen. Der Titel der Tabelle lautet "Results of the alignment methods with 1:1 (upper part) and ncm cpm (lower part)". Im Hintergrund ist ein Mann zu sehen, der auf einem Computer arbeitet.</sample>
    <sample id="21">Das Bild zeigt eine Präsentation über "Automatic Alignment Evaluation". Es gibt eine Tabelle, die verschiedene Algorithmen und ihre Ergebnisse für die Korrelation von Textdaten zeigt. Die Tabelle hat Spalten für die Korrelationen P, R, F1 und ncm. Darüber hinaus gibt es eine Überschrift, die den Schwerpunkt der Präsentation anzeigt. Im Hintergrund ist ein Mann zu sehen, der möglicherweise die Präsentation macht oder beobachtet.</sample>
    <sample id="22">Der englische Text lautet: "Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n cm capabilities (lower part)"</sample>
    <sample id="23">Auf dem Bild ist ein Mann zu sehen, der sich auf einem Computer-Screen konzentriert. Der Screen zeigt eine Präsentation mit dem Titel "Automatic Alignment Evaluation". Die Präsentation enthält eine Tabelle, die verschiedene Algorithmen zur Automatic Alignment-Evaluation zeigt und ihre Leistungen in Form von Zahlen angibt. Der Mann scheint sich intensiv mit den Inhalten der Präsentation auseinanderzusetzen, möglicherweise analysiert er die Daten oder denkt über die Ergebnisse nach.</sample>
    <sample id="24">Das Bild zeigt eine Präsentation über die Automatic Alignment Evaluation. Im oberen Teil des Bildes befindet sich ein blauer Banner mit der Überschrift "Automatic Alignment Evaluation". Darunter ist ein Tabularisch dargestelltes Ergebnis der Alignment-Methoden, das zwei Teile hat: den oberen Teil mit einer 1:1 Korrelation und den unteren Teil mit einer n:n Korrelation (n = 10). Die Tabellen sind mit verschiedenen Methoden aufgelistet, die für die Alignment-Evaluation verwendet wurden, wie Sent-LA-BASE, CATS-C3G, VecAlign, BERTAlign und MASSAlign. Jede Methode hat eine Beschreibung und eine Reihe von Zahlen, die wahrscheinlich die Korrelationswerte oder Leistungsindikatoren darstellen. Der Hintergrund des Bildes zeigt einen Menschen, der an einem Computer arbeitet, was nahelegt, dass dies eine Online-Presentation sein könnte.</sample>
    <sample id="25">Der englische Text lautet: "Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n cm capabilities (lower part)"</sample>
    <sample id="26">Der englische Text lautet: "Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n cm capabilities (lower part)"</sample>
    <sample id="27">Das Bild zeigt eine Präsentation über "Automatische Textsimplifizierung". Es gibt zwei Hauptabschnitte: "Dokumentebene" und "Satzebene". Jeder Abschnitt enthält Tabellen mit Ergebnissen für die Textsimplifizierung. Die Tabellen sind unter verschiedenen Tests geteilt, wie "DEPLAIN-APA test (n=48)" und "DEPLAIN-WEB test (n=147)". Die Tabellen enthalten Daten über die Trainingsdaten und die Leistung im Test, einschließlich Metriken wie BLEU und F1 Score. Im oberen linken Eck des Bildes befindet sich der Titel "Automatic Text Simplification", und das Bild wurde während einer Präsentation aufgenommen, wie durch den Teil eines Menschen im oberen rechten Eck zu erkennen ist.</sample>
    <sample id="28">Das Bild zeigt eine Präsentation über "Automatische Textsimplifizierung". Es gibt zwei Hauptabschnitte: "Dokumentebene" und "Satzebene", jeder mit einer Tabelle, die Ergebnisse der Textsimplifizierung anzeigt. Die Tabelle auf der linken Seite zeigt Ergebnisse für den Dokumentebene-Test (DEPLAIN-APA test), während die rechte Seite für den Satzebene-Test (DEPLAIN-WEB test) präsentiert. Jede Tabelle enthält Informationen wie "Train data", "BLEU", "F1 Score" und andere Metriken. Im Hintergrund ist ein Mann zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="29">Der englische Inhalt lautet: "Automatic Text Simplification Results on Document Simplification using feature-rich mBART. n corresponds to the length of the training data. DEPLAIN-APA test (n=48) DEPLAIN-APA test (n=1231) DEPLAIN-APA test (n=147) DEPLAIN-APA test (n=1846)"</sample>
    <sample id="30">Das Bild zeigt eine Präsentation über "Automatische Textsimplifizierung" (Automatic Text Simplification). Es gibt zwei Hauptabschnitte: "Dokumentebene" (Document Level) und "Satzebene" (Sentence Level). Jeder Abschnitt zeigt Ergebnisse für Document Simplification mit feinrasiertem long mBART. Die Daten sind in Tabelle-Form präsentiert, wobei jeder Tabelle zwei Testdatenpunkte angegeben werden - DEPLAIN-APA und DEPLAIN-WEB. Die Tabellen enthalten verschiedene Metriken wie BLEU, F1 und PRE, sowie andere relevante Daten. Im Hintergrund ist ein Mann zu sehen, der wahrscheinlich die Präsentation abgibt.</sample>
    <sample id="31">Das Bild zeigt eine Präsentation über "Automatische Textsimplifizierung". Der Titel der Präsentation ist in blau geschrieben und befindet sich am oberen Rand des Bildes. Im Hintergrund sind zwei Personen zu sehen, einer auf der rechten Seite und einer auf der linken Seite. Die Hauptfokus der Präsentation liegt auf einem Diagramm, das die Ergebnisse von Dokumenten- und Satzstufe-Textsimplifizierung darstellt. Das Diagramm wird in zwei Abschnitte unterteilt: "Dokumentenstufe" und "Satzstufe". Jeder Abschnitt enthält Tabellen mit Daten und Metriken, die die Leistung der Textsimplifizierungsmethoden während des Trainings und der Überprüfung auf verschiedenen Tests (DEPLAIN-APA und DEPLAIN-WEB) anzeigt. Die Tabelle für die Dokumentenstufe zeigt Daten für die Trainingsdaten und die Überprüfung, während die Tabelle für die Satzstufe nur Überprüfungsdaten enthält. Die Metriken im Diagramm umfassen BLEU, F1 und PRE, was wahrscheinlich auf die Qualität der Textsimplifizierung hinweist.</sample>
    <sample id="32">Das Bild zeigt eine Präsentation über "Automatische Textsimplifizierung" auf einem Computerbildschirm. Der Titel der Präsentation ist "Automatic Text Simplification". Das Bild zeigt zwei Hauptabschnitte: "Document Level" und "Sentence Level", jeder mit einem Tabellenübersicht, die Ergebnisse für verschiedene Tests anzeigt. Die Tabellen enthalten Daten wie "BLEU", "F1", "PPL" und andere Metriken für verschiedene Test-Setups wie "DEPLAIN-APA test (n=48)" und "DEPLAIN-WEB test (n=147)". Im Hintergrund ist ein Mann zu sehen, der wahrscheinlich die Präsentation gibt oder überwacht.</sample>
    <sample id="33">Das Bild zeigt eine Präsentation über "Automatische Textsimplifizierung". Der Titel der Präsentation ist "Automatic Text Simplification" auf Englisch. Es gibt zwei Hauptabschnitte: "Document Level" und "Sentence Level", die jeweils Ergebnisse für Document Simplification und Sentence Simplification anzeigt. Jeder Abschnitt enthält verschiedene Datenpunkte und Messwerte, die wahrscheinlich mit den Ergebnissen der Textsimplifizierung verbunden sind. Im Hintergrund ist ein Mann zu sehen, der möglicherweise über dieses Thema spricht oder präsentiert.</sample>
    <sample id="34">Danke. Für weitere Details, überprüfen Sie bitte unseren Artikel. Und freuen Sie sich darauf, unsere Plakat im ACL 2023-Konferenz zu besuchen.</sample>
    <sample id="35">Patrick Fernandes, Kayo Yin, Emmy Liu, André F. T. Martins, Graham Neubig</sample>
    <sample id="36">T5 XL</sample>
    <sample id="37">Ja, sie funktionieren noch.</sample>
    <sample id="38">Das neue Aspekt der vorgeschlagenen menschlichen Bewertungsmethode ist die Annotation von Verhaltensweisen in Chat-Dialogen.</sample>
    <sample id="39">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Qualität der Labels ab.</sample>
    <sample id="40">Das Ergebnis kann noch verbessert werden, indem man die Annotatoren dazu ermutigt, mindestens ein Stück von jedem Lied zu hören und es zu lesen.</sample>
    <sample id="41">Es sind fünf Autoren an der Arbeit beteiligt.</sample>
    <sample id="42">Der englische Text lautet: "Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination" und "Adam Przepiórkowski and Michał Woźniak".</sample>
    <sample id="43">Der englische Text lautet: "Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="44">Der englische Inhalt lautet: "Dependency Structure of Coordination" und "Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="45">Der englische Inhalt lautet: "Dependency Structure of Coordination" und "Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="46">Der englische Inhalt lautet: "Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="47">Der englische Inhalt lautet: "Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="48">Der englische Text lautet: "Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="49">Das Bild zeigt eine Präsentation über die "Dependency Structure of Coordination". Es gibt vier verschiedene Arten von Koordinationsstrukturen, jede mit einem Beispiel: Bouquet/Stanford (Universal Dependencies), Chain/Moscow, Conjunction-headed/Prague und Multi-headed/London. Jedes Beispiel zeigt den Satz "Homer liebt Lisa, Bart und Maggie." Die Präsentation scheint auf einem Bildschirm zu sein, und es gibt einen kleinen Overlay mit dem Text "© 2014 Peter Svennungsen" im oberen rechten Eck.</sample>
    <sample id="50">Der englische Text lautet: "Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="51">Der englische Inhalt lautet: "Dependency Length Minimization (DLM)" und "Word order tends to minimize dependency lengths:"</sample>
    <sample id="52">Der englische Inhalt lautet: "Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths:"</sample>
    <sample id="53">Der englische Text lautet: "Dependency Length Minimization (DLM)" und "Word order tends to minimize dependency lengths:"</sample>
    <sample id="54">Der englische Inhalt lautet: "Word order tends to minimize dependency lengths:"</sample>
    <sample id="55">Der englische Inhalt lautet: "Dependency Length Minimization (DLM)" und "Word order tends to minimize dependency lengths:"</sample>
    <sample id="56">Der englische Inhalt lautet: "Dependency Length Minimization (DLM)" und "Word order tends to minimize dependency lengths:"</sample>
    <sample id="57">Der englische Inhalt lautet: "Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths:"</sample>
    <sample id="58">Der englische Inhalt lautet: "Dependency Length Minimization (DLM) - Word order tends to minimize dependency lengths:"</sample>
    <sample id="59">Der englische Text lautet: "Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths:"</sample>
    <sample id="60">Der englische Inhalt lautet: "Word order tends to minimize dependency lengths:"</sample>
    <sample id="61">Der englische Inhalt lautet: "Word order tends to minimize dependency lengths:"</sample>
    <sample id="62">Das Bild zeigt eine Präsentation über "Konjunktionslängen in Englisch". Der Titel ist in einem blauen Balken oben positioniert. Der Haupttext lautet: "Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016)". Es gibt einige Punkte, die Informationen über die Länge von Konjunktionen in englischen Texten bereitstellen: - Linkskonjunktionen werden im Allgemeinen kürzer (beobachtet wurde), - diese Tendenz wächst mit der Längenunterschied (kurzgezählt bei Gibson et al. 1996:88-90), - aber nur, wenn der Regierende auf der linken oder fehlt (ich sah Bart und Lisa; Homer kam und schnurrte), - nicht, wenn der Regierende auf der rechten Seite ist (Ted und Ned lachten). Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einer Person zu sehen, die wahrscheinlich das Präsentationsumfeld betreibt.</sample>
    <sample id="63">Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before) this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88–90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="64">Statistics über Koordinationsprozesse extrahiert aus einer verbesserten Version des Penn Treebank (Marcus et al. 1993, Ficler und Goldberg 2016): - linksere Konjunktionen werden allgemein kürzer (vorher beobachtet), - diese Tendenz wächst mit der Längensdifferenz (kurz beachtet in Gibson et al. 1996:88-90) - nur wenn der Regierende auf der linken oder fehlenden Seite ist (Ich sah Bart und Lisa; Homer kam und schnurrte), - nicht, wenn der Regierende auf der rechten Seite ist (Ted und Ted lachte).</sample>
    <sample id="65">Das Thema der Präsentation ist "Konjunktionslängen in Englisch" (Konjunktionslängen in Englisch).</sample>
    <sample id="66">Der englische Text lautet: "Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88-90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed)."</sample>
    <sample id="67">Inhaltsangabe: Statistiken über Koordinationslängen extrahiert aus einer verbesserten Version des Penn Treebank (Marcus et al. 1993, Ficler und Goldberg 2016): - Koordinatenlinks werden allgemein kürzer (vorher beobachtet), - diese Tendenz wächst mit der Längenunterschied, - jedoch nur, wenn der Regierende auf der linken oder fehlenden Seite ist (ich sah Bart und Lisa; Homer kam und schnurrte), - nicht, wenn es auf der rechten Seite ist (Ted und Ted lachte).</sample>
    <sample id="68">Das Bild zeigt eine Präsentation über "Konjunktionslängen in Englisch" (Conjunct Lengths in English). Der Titel ist in einem blauen Balken oben positioniert. Der Inhalt der Präsentation bezieht sich auf Statistiken über Koordination, die aus einer verbesserten Version des Penn Treebank extrahiert wurden, wie von Marcus et al. 1993, Ficler und Goldberg 2016. Die Punkte im Text sind: - Links-Konjunktionen werden allgemein kürzer (beobachtet), - Diese Länge wächst mit der Konjunktionslänge (wie in Gibson 1996; S. 88-90 erwähnt), - Es gibt eine Ausnahme, wenn der Regierende links oder nicht da ist (Beispiele: "Ich sah Bart und Lisa; Homer kam und schnurrte"), - Es gibt keine Konjunktion, wenn sie auf der rechten Seite ist (Beispiel: "Ted und Ned lachten"). Im oberen rechten Eck ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich den Vortrag gibt.</sample>
    <sample id="69">Der englische Text lautet: "Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88–90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed)."</sample>
    <sample id="70">Das Bild zeigt eine Reihe von neun Graphiken, die sich auf die Proportionen von kürzern links-oberen Lücken abzielen, die aufgrund der absoluten Differenz von Längen zwischen den Lücken (mit Vertrauensbereichen) abhängen. Jeder Graphik zeigt eine unterschiedliche Variable, wie zum Beispiel "NO governor (length in CHARACTERS)", "NO governor (length in SYLLABLES)" und "NO governor (length in WORDS)". Die Y-Achse der Graphiken zeigt die Proportion der kürzern links-oberen Lücken, während die X-Achse die Absolute Differenz in Längen zwischen den Lücken darstellt. Im unteren Bildrand befindet sich ein Text, der lautet: "Figure 1: Proportions of shorter left upper gaps depending on the absolute difference of conjoined lengths (with confidence bands)".</sample>
    <sample id="71">Das Bild zeigt eine Diagrammarrange von neun Graphen, die alle eine ähnliche Form haben. Jeder Graph zeigt eine lineare Verbindung zwischen zwei Größen, wobei die y-Achse "Proportion of shorter left contiguous characters" und die x-Achse "Absolute difference in character length (with confidence bands)" beschriftet ist. Die y-Achse wird in verschiedenen Einheiten angegeben: "CHARACTERS", "SYLLABLES" und "WORDS". Jeder Graph hat einen Titel, der mit "NO governor" beginnt und weitere Details über die Länge der Zeichen, Strophen oder Wörter enthält. Der Titel des gesamten Bildes lautet: "Figure 1: Proportions of shorter left contiguous characters depending on the absolute difference in character length (with confidence bands)". Im oberen rechten Eck des Bildes befindet sich ein kleineres Bild eines Menschen, der wahrscheinlich eine Präsentation gibt.</sample>
    <sample id="72">Der englische Text lautet: 'Figure 1: Proportions of shorter left and right contexts depending on the absolute difference of conic lengths (with confidence bands)'.</sample>
    <sample id="73">Das Bild zeigt eine Präsentation über die Verständlichkeit von Koordinationsstrukturen in Abhängigkeitsstrukturierungen. Der Titel der Präsentation lautet "Kompatibilität mit Abhängigkeitsstrukturen der Koordination". Es gibt vier verschiedene Koordinationsstrukturen, jede mit einem Beispiel-Satz: "Homer liebt Lisa, Bart und Maggie." Diese Strukturen sind Bouquet/Stanford (Universal Dependencies), Chain/Moscow, Conjunction-headed/Prague und Multi-headed/London. Jede Struktur wird mit einem Ja oder Nein angezeigt, um ihre Kompatibilität mit den anderen zu zeigen. Die Strukturen Bouquet/Stanford und Chain/Moscow sind nicht kompatibel, während Conjunction-headed/Prague und Multi-headed/London kompatibel sind.</sample>
    <sample id="74">Auf dem Bild ist ein Text auf einem weißen Hintergrund zu sehen. Der Haupttext lautet: "Sehe das Papier für den vollständigen Argument!" Darunter gibt es eine weitere Botschaft, die lautet: "Rede mit uns auf der Poster-Sitzung!" Diese Nachrichten sind in schwarz und weiß gedruckt. Im oberen rechten Eck des Bildes befindet sich ein kleiner Bildschirm mit einem Mannskopf, der möglicherweise Teil eines Videos oder einer Präsentation ist. Der Text auf dem Bild ist klar und einfach, um die Botschaft klar zu machen.</sample>
    <sample id="75">Es sind drei Autoren an der Arbeit beteiligt.</sample>
    <sample id="76">news, bible, L2, fiction</sample>
    <sample id="77">Der Beispieltext lautet: "but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed)."</sample>
    <sample id="78">Ja, die Modelle können für Ihre Forschung verwendet werden.</sample>
    <sample id="79">DEplain-apa enthält Dokumente aus dem Internet.</sample>
    <sample id="80">Die Faktoren, die zu einer guten Generalisierung führen, sind ein besseres Modellarchitektur, ein größeres Modellgröße und mehr scharfe Ausbildungsbeispiele.</sample>
    <sample id="81">Die Tendenz wurde gemessen, indem die Länge von linken Konjunktionen mit der Länge der Rechten kontrastiert wurde.</sample>
    <sample id="82">Die Experimente waren so konzipiert, dass sie die Auswirkungen der Position des Begrenzers untersuchten. Diese Position wurde durch den Platzierung von begrenzenden Zeichen (CHARACTERS, SYLLABLES oder WORDS) auf der linken oder rechten Seite eines Textes bestimmt. Die Grafiken zeigen die Proportionen von kürzeren Lücken am linken und rechten Ende des Textes in Bezug auf die Gesamtlänge des Textes.</sample>
    <sample id="83">Es ist schwierig, ein Basisklassifikator mit ungewogenen Daten zu trainieren. Das Bild zeigt, dass der AUC-Wert des Modells bei 0,50 ist, was bedeutet, dass das Modell nicht besser als Zufall ist.</sample>
    <sample id="84">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="85">The people in the example conversation are named John, Mary, and Sarah.</sample>
    <sample id="86">Kontextsensitive MÜ-Modelle schneiden besser bei Diskursphänomenen ab, die mit Kontext zusammenhängen.</sample>
    <sample id="87">Johns Hopkins University, Purdue University, and MIT.</sample>
    <sample id="122">Das Framework quantifiziert die Positionalität durch die Bereitstellung eines Goldlabels für jede Instanz, die als Referenz dient.</sample>
    <sample id="155">Das Ergebnis der vorherigen Studie war, dass die menschlichen Teilnehmenden eine erhöhte Aggression und eine erhöhte Aggression gegenüber Männern zeigten.</sample>
    <sample id="156">Penn Treebank, Gibson et al. (1996), Ficler and Goldberg (2016)</sample>
    <sample id="157">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="158">Kognitive Dissonanz kann mit Fehlverhalten und Angst in Bezug auf die Selbstwahrnehmung und die Identität verbunden sein.</sample>
    <sample id="159">Es sind zwei Autoren an der Arbeit beteiligt.</sample>
    <sample id="160">There are 7 authors involved in the work.</sample>
    <sample id="161">Das Framework unterscheidet sich von früheren Arbeiten, indem es die Untersuchung von Annotierungen mit demokratischen und ethnischen Gruppenverteilungen durchführt. Es bietet eine umfassendere Analyse der Daten und ihre Korrelationen mit Pearson's R scores, was eine einzigartige Ansatzweise im Vergleich zu traditionellen Ansätzen darstellt.</sample>
    <sample id="162">GPT-4</sample>
    <sample id="163">DeepL and Google Translate were compared.</sample>
    <sample id="164">Das Bild zeigt eine Präsentation über Sprachmodellierung (NLP). Im oberen linken Eck ist ein Hashtag "#ACL2023" zu sehen. Der Titel der Präsentation lautet: "Von der Vorbereitung von Daten zu Sprachmodellen bis hin zu Nebentasks: Verfolgen der Spuren von politischen Biases, die unfairen NLP-Modellen führen." Die Namen der Vortragenden sind Shangbin Feng, Chan Young Park, Yuhan Liu und Yulia Tsvetkov. Im unteren linken Eck sind Logos der Paul G. Allen School, UW NLP und Carnegie Mellon University's Language Technologies Institute zu sehen.

Im zweiten Bild zeigt ein Diagramm mit dem Titel "LM Training Data - Ein gemischter Segen". Es handelt sich um eine vertikale Balkendiagramm, das die Anzahl der Trainingsdaten (in Big-Gigabyte) für verschiedene Quellen anzeigt. Links sind die Quellen aufgeführt, und rechts sind die Zahlen angegeben. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen. Am unteren rechten Eck befindet sich eine Referenz zum Paper: Dodge, Jesse et al. "Dokumentieren großer Webkörpersammlungen: Eine Fallstudie über eine große Crowdsourcing-Körpersammlung in natürlicher Sprache". Das Paper wurde im Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) 2021 veröffentlicht.</sample>
    <sample id="165">LM Training Data: Ein gemischter Segen</sample>
    <sample id="166">LM Training Data: Ein gemischter Segen</sample>
    <sample id="167">LM Training Data: Eine mischte Segnung</sample>
    <sample id="168">Das Bild zeigt eine Präsentation über "LM Training Data" mit dem Titel "A mixed blessing". Es gibt eine Balkendiagramm, das verschiedene Quellen anzeigt, wie "patents", "en wikipedia.org", "nytimes.com" und mehrere andere. Die Quellen werden nach ihrem Anteil in den Trainingsdaten angeordnet, wobei "patents" der höchste Anteil hat. Rechts unten ist ein Zitat von Dodge, Jesse, et al., die auf einen Beitrag zur Dokumentierung großer Textdatenbanken hinweist. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen. Im nächsten Bild ist ein Diagramm mit drei Schritten zu sehen: "Pretraining data", "Language models" und "Downstream tasks", was die Prozessfolge darstellt.</sample>
    <sample id="169">Das Bild zeigt eine Präsentation mit einem Diagramm, das den Prozess von der Vorbereitung von Daten, über die Modellierung von Sprachmodellen bis hin zu den Unternehmungen in verschiedenen Aufgaben darstellt. Links steht der Text 'To this end', was auf das Ziel dieser Prozesse hinweist. Das Diagramm besteht aus drei Boxen, die 'Pretraining data' (Vorbereitung von Daten), 'Language models' (Sprachmodelle) und 'Downstream tasks' (Unternehmungen in verschiedenen Aufgaben) beschriftet sind. Jede Box ist durch einen kurvenförmigen Pfeil verbunden, der den Verlauf des Prozesses anzeigt. Rechts oben befindet sich ein kleiner Bildschirm eines Menschen, der wahrscheinlich die Präsentation führt.</sample>
    <sample id="170">Zum Ende</sample>
    <sample id="171">Zum Ende</sample>
    <sample id="172">Das Bild zeigt eine Präsentation über "Evaluating LM Political Leaning" (Evaluieren der politischen Neigung von LM). Es gibt einen Titel, der den Schwerpunkt auf die Bewertung von Encoder- und Decoder-LMs (Language Models) im Zusammenhang mit politischer Neigung anzeigt. Der Text auf der Seite lautet: "Support both encoder and decoder LMs", "Automatic eval", "Grounded in polisci lit". Darunter sind zwei Beispiele für Prompt-Responses zu sehen, die sich auf eine Ethik-Prüfung beziehen.

Darüber hinaus sind zwei Grafiken präsentiert. Eine ist ein Diagramm, das LMs nach ihren politischen Neigungen auf einem Koordinatensystem abgebildet hat, wobei die Y-Achse "Authoritarian" und die X-Achse "Libertarian" markiert sind. Die LMs werden in verschiedene Kategorien wie "Left" (links), "Right" (rechts) und "Authoritarian" gruppiert. Ein weiterer Diagramm zeigt eine kritische Bewertung der LMs, wobei "Agree" (stimmungsgemäß) und "Disagree" (nicht stimmen) gekennzeichnet sind.

Zuletzt zeigt ein dritter Diagramm eine Klassifizierung der LMs in Bezug auf ihre politische Neigung, wobei LMs wie BERT-base, RoBERTa-base und CoNLL-2000-Base auf der linken Seite positioniert sind, während LMs wie GPT-3-ada und GPT-4 auf der rechten Seite zu finden sind. Diese Grafiken und Texte geben einen umfassenden Überblick über die Untersuchung und Klassifizierung der politischen Neigungen von Language Models.</sample>
    <sample id="173">Der englische Text lautet: "Existing LMs" und zeigt eine Karte mit verschiedenen LMs (Language Models) aufgezählt. Die LMs sind auf einer 2-D-Kartenaufstellung positioniert, die von "Left" bis "Right" und von "Authoritarian" bis "Libertarian" reicht.</sample>
    <sample id="174">Das Bild zeigt eine präsentationelle Slide, die sich auf den Unterschied zwischen "News Media" und "Social Media (Reddit)" konzentriert. Links auf der Slide ist ein Diagramm mit drei Abschnitten, die "left", "center" und "right" beschriftet sind. Rechts auf der Slide ist ein ähnliches Diagramm zu sehen, das den gleichen Abschnitte "left", "center" und "right" zeigt. Diese Slide könnte Teil einer Präsentation sein, die sich auf die Untersuchung des politischen Beugens in verschiedenen Medienquellen konzentriert.</sample>
    <sample id="175">Das Bild zeigt eine Präsentation über "Prätraining-Daten" (Pretraining Data). Es gibt zwei Diagramme, die unterschiedliche politische Leitungen darstellen. Links steht "News Media" (News Media) und rechts "Social Media (Reddit)" (Social Media (Reddit)). Jedes Diagramm hat drei Abschnitte: "left" (links), "center" (zentrum) und "right" (rechts). Beide Diagramme zeigen dasselbe Muster: "left", "center" und "right" in der gleichen Reihenfolge. Der Titel oben lautet "Prätraining-Daten" (Pretraining Data), und es gibt einen Text unten, der auf Forschungsarbeiten hinweist, die sich mit der Prätraining von LMs (Language Models) wie RoBERTa und GPT-2 konzentrieren und ihre Auswirkungen auf politische Leitungen untersuchen.</sample>
    <sample id="176">Das Bild zeigt eine Präsentation über "Pretraining Data" mit dem Ziel, die Partizipationsverläufe im politischen Neungsspektrum zu analysieren. Es gibt zwei Diagramme, einer für "News Media" und einer für "Social Media (Reddit)". Beide Diagramme zeigen eine Verteilung von "left", "center" und "right". Der Text auf dem Bild lautet: "Pretraining Data - Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning." Darunter sind Zitaten von Li, Yujian, et al. und Shen, Gorkin, und Carolyn Rose. Das Bild ist Teil eines Online-Vortrags, wie der kleine Bildschirm im oberen rechten Eck des Hauptbildes zeigt.</sample>
    <sample id="177">Das Bild zeigt eine Präsentation über "Partisan shifts in LM political leaning" (Partisanenwechsel in der politischen Neigung von LM). Im Zentrum des Bildes befindet sich ein Diagramm, das die Ergebnisse einer Studie darstellt. Das Diagramm ist in vier Quadranten unterteilt: oben links in Rot, oben rechts in Blau, unten links in Grün und unten rechts in Violett. Jedes Quadrant enthält Texte wie "reddit", "original", "news" und andere, die wahrscheinlich auf verschiedene Quellen oder Kategorien von Informationen hinweisen.

Rechts neben dem Diagramm sind zwei Sektionsnamen zu sehen: "RoBERTa" und "GPT-2". Diese sind wahrscheinlich die Namen von Sprachmodellen, die im Kontext der Studie verwendet wurden. Im oberen rechten Eck des Bildes befindet sich ein kleineres Quadrat mit einem Mann, der eine Präsentation gibt. Der Text "Left Center Right" ist in diesem Quadrat zu sehen, was möglicherweise auf eine Klassifizierung der politischen Neigung bezieht.

Die gesamte Präsentation scheint sich auf die Analyse von Partisanenwechseln in der politischen Neigung basierend auf Daten von "LM" zu konzentrieren, wobei RoBERTa und GPT-2 als primäre Methoden für diese Analyse genannt werden.</sample>
    <sample id="178">Das Bild zeigt ein Diagramm mit zwei Hauptbereichen, die "RoBERTa" und "GPT-2" bezeichnen. Jeder Bereich ist in vier Abschnitte unterteilt, die "original", "news", "reddit" und "reddit news" kennzeichnen. Die Abschnitte sind in verschiedenen Farben gefüllt: rot, blau, grün und lila. Links im Bild befindet sich eine Legende, die die Farben mit den Parteien "Links", "Zentrum" und "Rechts" verbindet. Der Titel des Bildes lautet "Results" und darunter steht "Partisan shifts in LM political leaning". Im oberen rechten Eck des Bildes befindet sich ein kleineres Bild eines Menschen, der wahrscheinlich über Skype oder einer ähnlichen Kommunikationsplattform teilnimmt.</sample>
    <sample id="179">Das Bild zeigt eine Präsentation mit dem Titel "Results" und einem Subtitle "Partisan shifts in LM political leaning". Es gibt eine Karte, die verschiedene Parteipositionen darstellt, unter denen "original news", "reddit", "news center" und "reddit right" zu finden sind. Links oben steht der Name RoBERTa, rechts oben GPT-2. Darüber hinaus gibt es ein weiteres Bild mit dem Titel "The Trump Card" und einem Subtitle "Pre-45th to post-45th shift". Diese Abbildung enthält eine Reihe von Diagrammen, die den Verlauf von Parteipositionen zeigen, unter denen "news left", "news center", "news right", "reddit left", "reddit center" und "reddit right" zu finden sind. Jedes Diagramm zeigt einen Delta-Wert (Δ) an, der den Verlust oder Gewinn einer Position in der Zeit darstellt. Im oberen rechten Eck des Bildes ist ein kleiner Overlay-Video, das eine Person zeigt, die wahrscheinlich über diese Präsentation spricht.</sample>
    <sample id="180">Der englische Text lautet: "The Trump Card" und "Pre-45th to post-45th shift".</sample>
    <sample id="181">Der englische Text lautet: "The Trump Card" und "Pre-45th to post-45th shift".</sample>
    <sample id="182">Der englische Text lautet: "The Trump Card" und "Pre-45th to post-45th shift".</sample>
    <sample id="183">Die Tabelle zeigt die Leistungen in verschiedenen Kategorien bei der Identifizierung von Hasspeech und Falschinformation. Die Leistungen sind für verschiedene Gruppen wie Schwarze, Muslime, LGBTQ+, Juden, Asiaten, Latinas, Frauen, Christen und Weibliche aufgeführt. Jede Spalte zeigt die Leistung in Prozent, wobei eine dunkelere Farbe (braun) die besten Ergebnisse und eine leichte Farbe (weiß) die schlechtesten anzeigen.</sample>
    <sample id="184">Der englische Text lautet: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and dark blue denotes worst."</sample>
    <sample id="185">Die Tabelle zeigt die Leistungsdaten für verschiedene Kategorien in Bezug auf den Hinweis auf Hasspeech und Missinformationsinhalte. Die Kategorien umfassen "Hasspeech", "Schwarz", "Muslimisch", "LGBTQ+?", "Jüdisch", "Asinisch", "Latinoamerikanisch", "Weiblich", "Christen" und "Weiß". Jede Kategorie hat zwei Zahlen, eine in blauer Farbe und eine in gelber Farbe. Diese Zahlen repräsentieren die Leistung, wobei der blaue Bereich die besten Ergebnisse und der gelbe Bereich die schlechtesten anzeigen soll. Darunter steht ein Text, der lautet: "Tabelle 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and blue denotes worst."</sample>
    <sample id="186">Das Bild zeigt eine Tabelle mit der Überschrift "Per-Category Performance". Die Tabelle präsentiert die Leistung in verschiedenen Kategorien wie Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, MEN und WHITE. Jede Kategorie hat eine Reihe von Farben darunter, die auf die Leistung in verschiedenen Quellen hinweisen, wie Reddit, News Right, News Left, CNN, NYT, LGL, Fox, BBRT, WAT und NR. Die Leistungen sind in Zahlen dargestellt, wobei dunkel gelbe Zahlen den besten Ergebnissen und dunkel blaue Zahlen den schlechtesten anzeigen. Im unteren Teil der Tabelle ist ein Text zu sehen, der lautet: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources." Diese Texte sind farblich markiert, wobei dunkel gelbe Farbe den besten Ergebnissen und dunkel blaue Farbe den schlechtesten anzeigen. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit der Aufschrift "1st 2nd 3rd 4th" zu sehen.</sample>
    <sample id="187">Das Bild zeigt eine Tabelle mit dem Titel "Per-Category Performance". Die Tabelle präsentiert die Leistung in verschiedenen Kategorien wie "Hate Speech", "BLACK", "MUSLIM", "LGBTQ+", "Jews", "ASIAN", "LATINX", "WOMEN", "CHRISTIAN" und "WHITE". Jede Kategorie hat eine Reihe von Zahlen darunter, die wahrscheinlich Leistungsdaten oder Ergebnisse darstellen. Im unteren Teil der Tabelle ist ein Text zu sehen, der lautet: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources." Der Text weiterhin lautet: "The results are color-coded such that dark yellow denotes best and dark blue denotes worst." Darüber hinaus gibt es im oberen rechten Eck des Bildes eine kleine Grafik mit den Worten "1st, 2nd, 3rd, 4th".</sample>
    <sample id="188">Das Bild zeigt eine Präsentation mit einem Titel "Per-Category Performance". Es gibt eine Tabelle, die die Leistung in verschiedenen Kategorien wie "Hate Speech", "Black", "Muslim", "LGBTQ+", "Jews", "Asain", "Latinx", "Women", "Christian" und "White" darstellt. Jede Kategorie hat eine Reihe von Farben darunter, die auf die Leistung der jeweiligen Quellen hinweisen. Die Quellen sind unterbrochen in zwei Abschnitte: "Hate Speech" und "Misinformation".

In der "Hate Speech"-Kategorie sind die Quellen "Reddit Left", "News Right", "News Left" und "News Right" aufgeführt. In der "Misinformation"-Kategorie sind die Quellen "NYT Left", "CNN (L)", "Guardian (L)", "Fox (R)", "BBC (R)", "Wat (W)" und "NR (R)" aufgeführt.

Die Farben in der Tabelle deuten auf die Leistung der Quellen an, wobei dunkel gelbe Zellen den besten Ergebnissen und dunkel blauen Zellen den schlechtesten Ergebnissen entsprechen. Am unteren Ende der Tabelle ist eine Beschreibung der Farben zu finden, die "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources." lautet und angibt, dass das Bild eine Leistungsgeschichte für verschiedene Gruppen und Quellen zeigt.

Im oberen rechten Eck des Bildes befindet sich ein kleiner Bildschirm mit dem Text "1st place 2nd place 3rd place 4th place", was wahrscheinlich auf die Platzierung der Quellen in einer Wettbewerbssituation hinweist.</sample>
    <sample id="189">Das Bild zeigt eine Tabelle mit dem Titel "Per-Category Performance" (Pro-Kategorie-Leistung). Die Tabelle präsentiert die Leistungen in verschiedenen Kategorien wie "Hate Speech", "Black", "Muslim", "LGBTQ+", "Jews", "Asain", "Latinx", "Women", "Christian" und "White". Jede Kategorie hat zwei Spalten: eine für "Reddit" und eine für "News Right". Die Zahlen in der Tabelle deuten an, wie die Leistungen in diesen Kategorien variieren.

Darüber hinaus gibt es einen Text darunter, der lautet: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and blue worst denotes worst." (Tabelle 4: Leistung bei Hassreden gegen verschiedene identitäre Gruppen und Missinformation von verschiedenen Quellen. Die Ergebnisse werden farblich codeiert, wobei dunkel gelb das beste und blau das schlechteste bedeutet.)

Im oberen rechten Eck des Bildes ist ein kleiner Overlay zu sehen, der die Platzierung "1st, 2nd, 3rd, 4th" anzeigt.</sample>
    <sample id="190">Das Bild zeigt eine Präsentation mit einem Tabelle über "Qualitative Analyse". Die Tabelle enthält die Spalten "Text", "Target Label", "Base" und mehrere weitere Spalten. Sie zeigt verschiedene Beispiele an, die auf Falsch-Positiv-Fälle (Falsch-Positiv) und Falsch-Negativ-Fälle (Falsch-Negativ) hinweisen. Der Text befasst sich mit Themen wie Rassismus, Mcdonald's und politische Manipulationen. Am unteren Ende der Tabelle ist ein Text zu finden, der auf die Unterbringung von Beispielen in der Darstellung hinweist. Im oberen rechten Eck des Bildes befindet sich ein kleiner Bildschirm, der einen Menschen zeigt, der wahrscheinlich die Präsentation führt.</sample>
    <sample id="191">Das Bild zeigt eine Präsentation über qualitative Analyse von Texten, insbesondere auf die Verwendung von Sprachmodellen mit politischen Biase. Der Hauptfokus liegt auf einem Tabelle, die Texte und ihre zugehörigen Labeln analysiert. Diese Tabelle zeigt verschiedene Beispiele von Texten, die als "FAKE" oder "TRUE" gekennzeichnet sind, was darauf hindeutet, dass sie möglicherweise als Falsch- oder Wahrheitsinformationen betrachtet wurden.

Die Tabelle enthält auch Spalten für verschiedene Labeln wie "ASIAN", "CHRIS", "RIGHT", "LEFT", "FAKE", "TRUE", "N4", "N5", "S-L", "S-R", "N-S", "R-S". Diese Labeln könnten verschiedene Kategorien oder Aspekte der Analyse darstellen.

Darüber hinaus gibt es einen Abschnitt unter dem Haupttabelle, der "Table 12: Examples of the downstream performance of tasks using language models with varying political bias." bezeichnet. Dieser Abschnitt bietet weitere Informationen über die Leistung von Sprachmodellen mit verschiedenen politischen Biases in verschiedenen Aufgaben.

Im Hintergrund ist ein weiteres Bild zu sehen, das zwei Seiten einer Tabelle zeigt, die "Hate Speech Text" und "Misinformation Text" beschriftet sind. Diese Seiten zeigen verschiedene Beispiele von Texten, die als "FAKE" oder "TRUE" gekennzeichnet sind, und sind für verschiedene Labeln wie "HATE", "N4", "N5", "R-N", "R-R" und "S-L" aufgeteilt.

Der Text im Bild scheint eine detaillierte Analyse zu präsentieren, wie Sprachmodelle mit politischen Biases auf verschiedene Arten von Texten reagieren, und wie dies auf die Leistung dieser Modelle in verschiedenen Aufgaben auswirkt.</sample>
    <sample id="192">Es gibt zwei Personen auf dem Bild. Eine Person ist im Hintergrund zu sehen, während die andere Person im Vordergrund zu sehen ist.</sample>
    <sample id="193">Das Bild zeigt eine detaillierte Analyse von Hasspeech-Texten auf beiden Seiten. Links steht ein Text mit dem Titel "Hate Speech Text", der verschiedene Sätze und Phrasen anzeigt, die als Hasspeech identifiziert wurden. Rechts befindet sich ein weiterer Text mit dem Titel "Hate Speech Text", der ähnliche Inhalte enthält. Beide Seiten sind mit verschiedenen Kategorien markiert, darunter "Date?", "N4", "N5", "NR", "R" und "RR". Diese Kategorien scheinen verschiedene Aspekte oder Kriterien der Analyse zu beziehen. Im unteren Abschnitt des Bildes befindet sich eine Tabelle mit dem Titel "Table 12: Qualitative analysis of hate speech examples where N4 political leaning has big beg to differ". Diese Tabelle bietet einen weiteren Aspekt der Analyse, möglicherweise um die Unterschiede zwischen verschiedenen politischen Leitungen im Zusammenhang mit Hasspeech zu untersuchen.</sample>
    <sample id="194">Das Bild zeigt eine detaillierte Analyse von Texten, die als Hasspeech bezeichnet werden. Es gibt zwei Hauptbereiche: "Hate Speech Text" und "Transcription Text". Jedes Textstück wird mit einer Kombination aus Zahlen (N4, N5, R-N, R-R) und Buchstaben (F, F+, F-) markiert, was möglicherweise auf verschiedene Kategorien oder Aspekte des Textes hinweist.

Die "Hate Speech Text"-Spalte enthält eine Reihe von Aussagen, die sich auf verschiedene Themen wie Sozialkunde, politische Meinungen und kulturelle Unterschiede beziehen. Beispiele sind:

- "Some are starting to realize 'cultural sensitivity' doesn't apply when it comes to the LGBT community."
- "Yes Please These racist frilly plaid shirts please!"
- "The United States of America is good."

Jede dieser Aussagen wird in der "Transcription Text"-Spalte wiederholt, wobei die gleichen Kombinationen von Zahlen und Buchstaben angezeigt werden.

Am unteren Ende des Bildes befindet sich eine Tabelle, die "Table 12: Qualitative analysis of hate speech example where N4 political leanings beg to differ." besagt. Diese Tabelle scheint eine Zusammenfassung oder eine Ergänzung der Informationen über die Hasspeech-Aussagen zu sein.

Rechts oben ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich eine Präsentation oder Diskussion über das Thema führt.</sample>
    <sample id="195">Das Bild zeigt eine detaillierte Tabelle, die Informationen über Hasspeech-Texte und ihre Bewertung in verschiedenen Kategorien wie N4, NR, R, und RR präsentiert. Jeder Text wird mit einem Checkbox markiert, um seine Bewertung zu kennzeichnen. Darüber hinaus gibt es einen Abschnitt namens "Diskussion" mit dem Titel "Zwischen Scylla und Charybdis", der sich mit der Entscheidung auseinandersetzt, ob Daten gesperrt oder nicht gesperrt werden sollen.</sample>
    <sample id="196">Der englische Inhalt lautet: "Discussion" und "Between Scylla and Charybdis". Der deutsche Übersetzungsansatz wäre: "Diskussion" und "Zwischen Scylla und Charybdis".</sample>
    <sample id="197">Diskussion</sample>
    <sample id="198">图片中的文字内容包括：

1. Discussion
2. Between Scylla and Charybdis
3. To "sanitize" or not to "sanitize", that is the question
4. Pretraining data
5. Language models
6. Downstream tasks
7. Thank you!
8. Shangbin Feng
9. Chan Young Park
10. Yuhan Liu
11. Yulia Tsetkov

此外，还有几个标志和徽章：
- PAUL G. ALLEN SCHOOL
- UWNLP
- Carnegie Mellon University Language Technologies Institute

这些文字和标志表明了讨论的主题、参与者的姓名以及相关的机构。</sample>
    <sample id="199">Vielen Dank</sample>
    <sample id="200">Es sind sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="201">900</sample>
    <sample id="202">Easy on Me (by Adele) und I Gotta Feeling (by the Black Eyed Peas)</sample>
    <sample id="203">Positionality refers to the perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, Dietrich Klakow</sample>
    <sample id="205">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell.</sample>
    <sample id="206">There are four authors involved in the work.</sample>
    <sample id="207">Ja, das getestete Modell funktioniert in der Testsuite.</sample>
    <sample id="208">KITMUS hat drei Varianten: (a) Background-Pretrain, (b) Background-Both und (c) Background-Inference.</sample>
    <sample id="209">Google Research</sample>
    <sample id="210">RQ3: How to use the available clean samples more efficiently?</sample>
    <sample id="211">Die Sensitivitätsmetrik ist eine Maßschenke für die Fähigkeit eines Modells, verschiedene Anweisungen für dieselbe Aufgabe zu verarbeiten und gleichartige Ergebnisse zu erzielen, unabhängig von leichter Wording-Unterschieden in den Anweisungen.</sample>
    <sample id="212">Wenjun Peng</sample>
    <sample id="213">Eine höhere Sensitivität bedeutet nicht unbedingt eine bessere Leistung des Modells. Das Bild zeigt, dass die Sensitivität des Modells durch verschiedene Trainingsstrategien erhöht oder reduziert werden kann. Die Note "Lower is better" auf dem Bild schlägt vor, dass eine niedrigere Sensitivität, also eine geringere Leistung, besser ist.</sample>
    <sample id="214">The models receive a wide range of linguistic contexts during pre-training.</sample>
    <sample id="215">40</sample>
    <sample id="216">Stanford University</sample>
    <sample id="217">Um die vielfältigen politischen und ethischen Aspekte von Sprachmodellen besser zu erfassen.</sample>
    <sample id="218">Kaheer Suleman</sample>
    <sample id="219">Die Pipeline für die Verbreitung von politischen Vorurteilen besteht aus der Verwendung von Pretraining-Data, der Entwicklung von Sprachmodellen und der Ausführung von Downstream-Tasks.</sample>
    <sample id="220">Ja, die Grafiken zeigen die Unterschiede im Simplifizierungsprozess zwischen DEplain-apa und Web.</sample>
    <sample id="221">Ja, Coscript ist öffentlich verfügbar.</sample>
    <sample id="222">Das Wasserzeichen wird in den Text eingebettet, indem man eine Target embedding (e_t) definiert, einen Ziffernnummer in einer Satz (m) zählt und das Target embedding an die ursprüngliche Embedding (e_0) hinzufügt.</sample>
    <sample id="223">Penn State und Amazon</sample>
    <sample id="224">Ja, Encoder-Decoder-Modelle wie mt5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="225">Eine Beispiele für eingeschränkte Sprachplanung ist das Kochen von Kuchen. Zum Beispiel kann man eine Straußenkuchen mit Schokoladenpuder zubereiten, indem man Schokoladenpuder in den Teig mischt.</sample>
    <sample id="226">Die Opazität ihrer Methode wird durch die Darstellung von Embedding-Visualisierungen sichergestellt.</sample>
    <sample id="227">NACHOS</sample>
    <sample id="228">GPT-4 ist am wenigsten auf Afrikanische Islamische Länder ausgerichtet.</sample>
    <sample id="229">"Use only one model for every latency regime and handle latency through specific parameters."</sample>
    <sample id="230">Die Leistung des Modells wird durch eine erhöhte Anzahl von Aufgaben verbessert.</sample>
    <sample id="231">LSTM seq2seq, TS, Zheng and Lapata</sample>
    <sample id="232">Der erste Autor ist Matthias Lindemann, der zweite Autor ist Alexander Koller und der dritte Autor ist Ivan Titov. Es gibt keine explizite Angabe über die Beziehung zwischen den Autoren, aber es ist üblich, dass Co-Autoren in einer Forschungsarbeit gemeinsam arbeiten und alle zum gleichen Projekt beitragen.</sample>
    <sample id="233">Chowdery et al., 2022</sample>
    <sample id="234">NLP Positionality: Characterizing Design Biases of Datasets and Models</sample>
    <sample id="235">NLP Positionality: Charakterisieren Designbiase von Datasets und Modellen</sample>
    <sample id="236">Imagine...</sample>
    <sample id="237">Das Bild zeigt eine Person, die sich in einem Zimmer befindet, das mit Büchern und anderen Gegenständen ausgestattet ist. Die Person sitzt vor einem Computer und scheint auf dem Bildschirm zu interagieren. Im oberen rechten Eck des Bildes befindet sich ein kleiner Bildschirm, der die Person zeigt. Links am Bildrand ist der Text 'Imagine...' zu sehen, und unten links befindet sich ein Profilbild eines Menschen namens Carl Jones, der als Tech Lead für den New York Times beschrieben wird. Darüber hinaus ist ein Textfeld neben dem Profilbild zu sehen, der lautet: 'Kannst du einen Scheißer nicht abwerden?' Diese Textfelder deuten an, dass es sich um eine Diskussion oder eine Präsentation handelt, die möglicherweise mit dem Thema "PerspectiveAPI score" in Verbindung steht.</sample>
    <sample id="238">Das Bild zeigt zwei avatars, einem Mann namens Carl Jones und einem anderen namens Aditya Sharma. Beide sind Tech-Leads von respektiven Zeitungen: Carl Jones von der New York Times und Aditya Sharma von der Times of India. Der Text auf dem Bild lautet "Imagine... Can you stop being a jerk? 😡 (0.82) ✅" und "Presstitutes everywhere on the news. 😡 (0.33) ❌". Diese Sätze scheinen humorvolle Kommentare zu politischen oder journalisticchen Themen zu sein.</sample>
    <sample id="239">Imagine... Design bias example Can you stop being a jerk? (0.82) ✅ Presstitutes everywhere on the news. (0.33) ❌ Carl Jones Tech Lead, New York Times Aditya Sharma Tech Lead, Times of India = PerspectiveAPI score</sample>
    <sample id="240">Das Bild zeigt eine Präsentation mit dem Titel "Positionality" auf der oberen linken Ecke. Im Hintergrund ist ein weißer Hintergrund zu sehen, und im oberen rechten Eck befindet sich ein kleiner Bildschirm mit einer Person, die wahrscheinlich über Skype oder einem ähnlichen Video-Konferenz-System teilnimmt. Am unteren Ende des Bildes ist eine Referenzangabe zu einem Buch zu finden: "[1] Savin-Baden, Maggi, und Claire Howell-Major; 'Qualitative research: The essential guide to theory and practice.' 'Qualitative Research: The Essential Guide to Theory and Practice.' Routledge (2013)."</sample>
    <sample id="241">Positionalität "Die Perspektiven [Menschen] halten aufgrund ihrer Demografie, Identität und Lebenserfahrungen." [1] Savin-Baden, Maggi, und Claire Howell-Major. "Qualitative Forschung: Das essential guide zur Theorie und Praxis." Qualitative Research: Das Essential Guide to Theory and Practice. Routledge (2013).</sample>
    <sample id="242">Positionality</sample>
    <sample id="243">Das Thema der Präsentation ist "Do datasets and models have positionality?" (Haben Datensätze und Modelle Positionalität?). Das Slide zeigt drei Referenzen zu wissenschaftlichen Arbeiten: 1. Blasi, et al.: "Systematic Inequalities in Language Technology Performance across the World's Languages." (ACL 2022). 2. Ye, et al.: "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models." (EMNLP 2022). 3. Cambo &amp; Gergle: "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." (CHI 2022). Diese Referenzen deuten darauf hin, dass die Präsentation sich mit den Aspekten der Positionalität in Sprachdatensätzen und Sprachmodellen auseinandersetzt.</sample>
    <sample id="244">Ja, die Datasette und Modelle haben Positionalität.</sample>
    <sample id="245">Das Bild zeigt eine Präsentation mit dem Titel "Do datasets and models have positionality?" (Sind Datensätze und Modelle positionell?). Der Text auf der Seite lautet: "Anecdotal evidence:" (Anonyme Beweise:) - "Model and dataset probing [1][2]" (Modell- und Datensatz-Schließen [1][2]) - "Theoretical definitions of model positionality" (Theoretische Definitionen von Modellpositionellität) [3] Die Anonyme Beweise sind auf drei Forschungsarbeiten zurückzuführen: 1. Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. 2. Ye et al. "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022. 3. Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, der einen Menschen zeigt, der wahrscheinlich die Präsentation macht.</sample>
    <sample id="246">Das Bild zeigt eine Präsentation mit dem Titel "Do datasets and models have positionality?" (Haben Datensätze und Modelle Positionität?). Der Text auf der Präsentation lautet:</sample>
    <sample id="247">Das Bild zeigt eine Präsentation mit dem Titel "Do datasets and models have positionality?" (Haben Datensätze und Modelle Positionität?). Der Text auf der Präsentation lautet: Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] Diese Anmerkungen sind auf drei Referenzen zurückzuführen: 1. Blasi, E. et al., "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. 2. Ye, X. et al., "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022. 3. Cambo &amp; Gergle, "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, der eine Person zeigt, die möglicherweise über eine Webcam präsentiert.</sample>
    <sample id="248">Das Thema der Präsentation ist "Do datasets and models have positionality?" (Haben Datensätze und Modelle Positionität?). Das Slide zeigt auch einige Beispiele für Anecdotaler Beweis, wie "Model and dataset probing" und "Theoretical definitions of model positionality".</sample>
    <sample id="249">Das Bild zeigt eine Slide mit dem Text 'Question: Do datasets and models have positionality?' und einem Punkt unten, der auf einen weiteren Text zeigt. Der Text, der durch einen Punkt hervorgehoben ist, lautet: 'Goal: Compare annotations from users with existing datasets and models.' Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einer Person zu sehen.</sample>
    <sample id="250">NLPPositionality: Eine Plattform zur Charakterisierung von Entwurfsbiasken in NLP-Datensätzen und -Modellen.</sample>
    <sample id="251">Das Bild zeigt einen Framework, der sich auf die Analyse von Bildern konzentriert. Es beginnt mit der Sammlung von 300 Bildern aus einem Datensatz, wobei jedes Bild ein goldener Label erhält. Diese Labels werden dann an einen Teil des UTTW übermittelt. Anschließend werden die Bildaufnahmen von Menschen aus dem ganzen Welt überprüft und bewertet.

Im nächsten Schritt wird das Bild mit einer Modellvorhersage verknüpft, was eine Prozesse darstellt, bei dem das Bild durch das Modell durchgeleitet wird, um eine Vorhersage zu erzeugen.

Schließlich wird die Analyse der vorherigen Ergebnisse durchgeführt, wobei verschiedene Aspekte wie Alter, Geschlecht, Ethnie, Bildungsniveau und Land betrachtet werden. Die Korrelation zwischen den verschiedenen Merkmalen wird auch analysiert, um eine umfassende Analyse der Bildauswahl und -bewertung zu erzielen.</sample>
    <sample id="252">Das Bild zeigt einen detaillierten Framework-Ansatz für die Annotierung von Datenmengen. Im oberen Bereich befindet sich der Titel "Framework" in großen schwarzen Buchstaben. Darunter sind zwei Diagramme zu sehen, die den Prozess der Datensammlung und -bearbeitung darstellen.

Der erste Diagramm auf der linken Seite ist unter dem Titel "Collection" und zeigt eine Reihe von Blättern mit verschiedenen Arten von Inhalten. Neben diesem Diagramm steht die Beschreibung: "300 Instance sind aus einer Sample gesammelt." Der zweite Diagramm auf der rechten Seite ist unter dem Titel "Processing" und zeigt eine Reihe von Blättern mit Text und Grafiken. Neben diesem Diagramm steht die Beschreibung: "Model Predictions."

Darüber hinaus gibt es eine weitere Beschreibung auf der rechten Seite des Bildes, die lautet: "1) Re-annotate datasets with diverse annotators," was bedeutet, dass die Datensätze mit einer Vielzahl von Annotatoren erneut überprüft werden sollen.

Im unteren Bereich des Bildes befindet sich ein weiterer Diagramm unter dem Titel "Analysis," das verschiedene Kategorien wie Alter, Geschlecht, Ethnie, Bildung, Land und Bevölkerungsstruktur zeigt. Diese Kategorien sind in einem Farbverlauf dargestellt, der wahrscheinlich eine Analyse der Daten darstellt.

Zusammenfassend handelt es sich um einen visuellen Ansatz zur Annotierung und Analyse von Datenmengen, der auf die Sammlung, Bearbeitung und Analyse von Informationen abzielt, sowie auf die Erstellung diverser Annotatoren für die Datensammlung.</sample>
    <sample id="253">Das Bild zeigt eine detaillierte Schematik eines Frameworks für die Annotierung von Datensätzen. Der Titel "Framework" ist am oberen Rand des Bildes zu sehen. Das Schemat wird in drei Hauptabschnitte unterteilt: "Collection", "Processing" und "Results".

Im Abschnitt "Collection" sind drei Schritte dargestellt:
1. "300 Instance are sampled from a dataset" - Dieser Schritt zeigt eine Abbildung einer Datenbank mit einer Auswahl von 300 Instanzen.
2. "Each Instance has an associated label" - Diese Schritt zeigt zwei Beispiele, die jeweils mit den Labels "Eating with Hands" und "Good" gekennzeichnet sind.
3. "Instances are sent as part of a study on IT's moral" - Dieser Schritt zeigt eine Grafik mit dem Text "Eating with Hands" und "Can you have sex? It is not moral."

Im Abschnitt "Processing" wird das Verfahren beschrieben, wie die Datensätze bearbeitet werden:
- Eine Grafik zeigt einen Prozess, der auf ein Netzwerk abzielt, was darauf hindeutet, dass Modelle vorhersagen.
- Eine Tabelle zeigt verschiedene Attribute wie Alter, Geschlecht, Ethnie, Bildung, Land und Kultur an.

Im Abschnitt "Results" werden die Ergebnisse der Annotierung dargestellt:
- Eine Grafik zeigt die korrigierten Labels, die von einer Studie übermittelt wurden.
- Eine Tabelle zeigt Attribute wie Alter, Geschlecht, Ethnie, Bildung, Land und Kultur an, zusammen mit einem Diagramm, das die Korrelation zwischen diesen Attribute und den korrigierten Labels darstellt.

Rechts am Bild ist ein Text zu sehen, der lautet: "1) Re-annotate datasets with diverse annotators." Dies bedeutet, dass die Datensätze von verschiedenen Annotatoren überprüft und korrigiert werden sollen.

Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, auf dem eine Person zu sprechen scheint, möglicherweise während einer Präsentation oder einer Online-Klasse.</sample>
    <sample id="254">Das Bild zeigt ein Framework für die Annotierung von Datensätzen. Es beginnt mit der Sammlung von Datensätzen, bei der 300 Instance ausgewählt werden. Jeder Instance wird mit einem zugehörigen Label gekennzeichnet, wie zum Beispiel "Eating with Hands" und "Good". Dann werden Instances übermittelt, um Annotatoren zu finden, die sie bearbeiten. Schließlich werden die Annotatoren die Labels für die Instance bearbeiten. Das Bild zeigt auch eine Darstellung der Ergebnisse, einschließlich Modellvorhersagen und einer Analyse nach Altersgruppen, Geschlechtern, Ethnicierten, Bildungsstufen, Ländern und anderen Faktoren.</sample>
    <sample id="255">Das Bild zeigt eine präsentationelle Slide mit dem Titel "Framework" oben. Der Hauptinhalt der Slide ist ein Diagramm, das den Prozess der Datenanalyse und -modellierung darstellt. Das Diagramm besteht aus drei Hauptabschnitten: "Collection", "Processing" und "Analysis". Jeder Abschnitt hat verschiedene Schritte und Informationen dargestellt.

Im "Collection"-Abschnitt sind die folgenden Punkte aufgeführt:
- 1000 Instanzen werden aus einem Datensatz gesammelt.
- Jede Instanz hat eine goldene Label.
- Assocations von Menschen aus der ganzen Welt.

Im "Processing"-Abschnitt sind zwei Punkte aufgeführt:
1) Model Predictions
2) Die enthaltenen Annotations sind von einer Studie über die UTTW abgeleitet.

Im "Analysis"-Abschnitt sind mehrere Kategorien aufgeführt, darunter Altersgruppen (18-34, 35-45), Geschlechter (Mann, Frau), Ethnizität (Weiß, Schwarz), Bildungsniveau (Nichtschulabitur, Schule bis 9., Schule bis 12., College, Bachelor, Master, PhD), Land (USA, China, Japan, Korea, Taiwan, Vietnam) und Pearson's r Korrelation (0,05, 0,15, 0,25, 0,35, 0,45).

Rechts neben dem Diagramm steht der Text: "2) Compare annotations by demographic to models and datasets via Pearson’s R scores." Diese Bedeutung gibt an, dass die Annotations durch Demografie zu den Modellen und Datasetts miteinander verglichen werden sollen, indem man Pearson's R-Scores verwendet.

Am oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen, der wahrscheinlich den Vortrag gibt oder sich aufmerksam für die Slide konzentriert.</sample>
    <sample id="256">Das Bild zeigt eine präsentationelle Slide, die einen Framework für eine Analyse darstellt. Der Titel der Slide lautet "Framework". Die Slide ist in drei Hauptabschnitte unterteilt: "Collection", "Processing" und "Analysis". Jeder Abschnitt enthält verschiedene Schritte und Informationen:

1. **Collection**: Diese Abschnittdetailiert den Prozess der Sammlung von Daten. Es wird erwähnt, dass 300 Instanzen aus einem Dataset gezogen wurden, jede mit einer goldenen Etikette gekennzeichnet wird, und es werden Anmerkungen von Menschen aus dem ganzen Welt gesammelt.

2. **Processing**: Hier wird beschrieben, wie die Sammlungenelemente durch ein Modell für Vorhersagen verarbeitet werden. Eine Grafik zeigt, wie das Modell Vorhersagen macht, und es wird erwähnt, dass die erzielten Vorhersagen mit einem Studium von UTT verglichen werden.

3. **Analysis**: Im letzten Abschnitt werden die Ergebnisse der Analyse detailliert. Es gibt eine Tabelle mit Kategorien wie Alter, Geschlecht, Ethnie, Bildungsniveau und Land. Die Tabelle zeigt die relativen Vorhersagen und die Korrelationen zwischen den erzielten Vorhersagen und den verschiedenen Gruppen.

Rechts neben der Slide befindet sich eine Anmerkung, die auf die Vergleichung von Anmerkungen mit dem Demografie und die Korrelationen mithilfe Pearson's R-Scores hindeutet.

Im oberen rechten Eck des Slides ist ein kleiner Bildschirm zu sehen, der möglicherweise einen Teil der Präsentation oder den Vortrager zeigt.</sample>
    <sample id="257">LabintheWild</sample>
    <sample id="258">LabintheWild</sample>
    <sample id="259">Auf dem Bild ist ein Text zu sehen, der lautet: 'Participants rate how socially acceptable the situation is.' Dieser Text deutet an, dass die Teilnehmer eine Bewertung über die soziale Akzeptanz einer bestimmten Situation abgeben.</sample>
    <sample id="260">Task A: Soziale Akzeptabilität</sample>
    <sample id="261">Aufgabe A: Soziale Akzeptabilität Analyse Datensätze - Soziale Chemie Modelle - Delphi - GPT-4</sample>
    <sample id="262">Auf dem Bild ist ein Text zu sehen, der lautet: "Participants read an instance from the Dynasheet dataset." Diese Information gibt einen Einblick in die Aufgabe, die Teilnehmer durchführen sollten.</sample>
    <sample id="263">Das Bild zeigt eine Präsentation über eine Studie zur Teilnahme. Im oberen linken Eck ist der Titel 'Study Participation' zu sehen. Darunter sind drei große Zahlenangaben aufgeführt: 16,299 Anmerkungen, 1,096 Annotatoren und 87 Länder. Diese Angaben deuten auf die Skala und das Umfang der Studie hin. Im oberen rechten Eck ist ein kleiner Bildschirm mit einem Menschen zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="264">Das Bild zeigt eine Präsentation, die sich auf die Ergebnisse einer Forschungsstudie über NLP (Natural Language Processing) konzentriert. Der Titel der Präsentation lautet "Results" und die Frage darunter lautet: "Who do NLP datasets and models align with?" Das Bild wurde während eines Zoom-Videosauftrags aufgenommen, wie durch den kleinen Bildschirm im oberen rechten Eck zu erkennen ist, auf dem eine Person sichtbar ist, die wahrscheinlich das Präsentation verlässt.</sample>
    <sample id="265">Das Bild zeigt eine Statistiken-Schachtel mit dem Titel "Social Acceptability (GPT-4)" und einem weiteren unter dem Titel "Hate Speech &amp; Toxicity (Dynahate)". Es gibt mehrere Säulen, jede mit einem Namen, einer Stufe und einer Zahl oben. Die Säulen sind in verschiedenen Größen und Farben dargestellt, wobei die blaue Säule im Zentrum besonders auffällt. Links auf der Seite des Bildes steht ein Text, der lautet: "Datasets and models are most aligned to English-Speaking countries." Im oberen rechten Eck des Bildes ist eine Person zu sehen, die sich an einem Computer-Turnier beteiligt.</sample>
    <sample id="266">Datenmengen und Modelle sind am stärksten mit Menschen aus Hochschulausbildung ausgerichtet.</sample>
    <sample id="267">Datenmengen und Modelle sind am stärksten mit Menschen aus Hochschulausbildung abgestimmt.</sample>
    <sample id="268">Findeung 2: Einige Bevölkerungsgruppen werden zurückgelassen.</sample>
    <sample id="269">Das Bild zeigt eine Grafik mit drei Balken, die den Sozialakzeptanz von Menschen nach Geschlecht darstellen. Links steht der Text: "Datenmengen und Modelle sind weniger an transfeminine Personen angepasst." Die Balken zeigen folgende Werte: - Mann: 0,69 (N=4.082) - Nicht binär: 0,55 (N=858) - Frau: 0,73 (N=4.368). Im oberen rechten Eck ist ein kleiner Bildschirm zu sehen, auf dem eine Person zu sehen ist.</sample>
    <sample id="270">Also, was können wir tun? Positionelle Probleme in der NLP ansprechen</sample>
    <sample id="271">Der englische Text lautet: 'Recommendations 1. Keep a record of all relevant design choices made throughout building datasets or models.'</sample>
    <sample id="272">Empfehlungen</sample>
    <sample id="273">Danke Dashboardsatz: nlppositionality.cs.washington.edu/</sample>
    <sample id="274">Die Referentin geht auf vier Probleme von SimulST ein.</sample>
    <sample id="275">Soziale und politische Verzerrungen in Datensätzen können effektiv durch die Implementierung von Fairness-Praktiken, die kontinuierliche Überwachung und Kontrolle der Modellentwicklung sowie die Berücksichtigung diverser Perspektiven und Gruppen im Trainingsprozess reduziert werden.</sample>
    <sample id="276">Der englische Text lautet: "The 61st Annual Meeting of the Association for Computational Linguistics Toronto, Canada July 9-14, 2023 Distilling Script Knowledge from Large Language Models for Constrained Language Planning Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xiange Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang Brain Technologies Inc."</sample>
    <sample id="277">"Language Planning" in Deutsch bedeutet "Sprachplanung".</sample>
    <sample id="278">Der englische Inhalt lautet: "Large language models (LLMs) can effectively decompose goals into steps."</sample>
    <sample id="279">Der englische Text lautet: "How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add the cocoa powder into the flour... Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints"</sample>
    <sample id="280">Der englische Text lautet: "How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add the cocoa powder into the flour... Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints"</sample>
    <sample id="281">Das Bild zeigt eine Person in einem Zoom-Beitrag, die sich in einem modernen Büro mit großen Fenstern befindet. Der Person ist ein grünes Hemd und Brille zu tragen. Im Hintergrund sind mehrere Tische und Stühle zu sehen, was auf eine Arbeitsumgebung hinweist. Links im Bild ist ein Textblock über "Constrained Language Planning" zu lesen, der zwei Rezepte für Kuchen anzeigt: einen "Strawberry Cake" und einen "Chocolate Cake". Diese Rezepte sind Teil eines größeren Textblocks, der über die Verbindung von abstrakten Zielen mit spezifischen Zielen und mehrfach geschichteten Restriktionen spricht.</sample>
    <sample id="282">Dies ist ein Bild eines Vortrags oder einer Präsentation über "LLMs im Konstruierten Sprachplanung". Der Titel der Präsentation lautet "Wie Leichtes Sprachmodell (LLM) in der Beschränkten Sprachplanung leistet", was auf eine Analyse oder diskussion des Verhaltens von LLMs bei der Durchführung von Aufgaben in beschränkter Sprache hinweist. Die Präsentation verwendet den Dataset "wikiHow + Generierte Restriktionen" und gibt einen Einblick in verschiedene Typen von Restriktionen, wie z.B. "Modifier" und "Method". Diese Restriktionen sind definiert, um zu zeigen, wie LLMs in solchen Situationen reagieren können. Im Hintergrund der Präsentation ist ein Video-Chat-Beitrag sichtbar, der wahrscheinlich den Vortrager darstellt, der sich in einem modernen Büro mit großen Fenstern befindet.</sample>
    <sample id="283">In German, the English text says: "Wie erfordern LLMs bei begrenzter Sprachplanung?"</sample>
    <sample id="284">Das Bild zeigt eine Person, die auf einem Computerbildschirm zu sprechen ist. Der Bildschirm zeigt eine Präsentation mit dem Titel "Wie leisten sich LLMs bei begrenzter Sprachplanung?" (Wie leisten sich LLMs bei begrenzter Sprachplanung?). Die Präsentation umfasst eine Datensammlung namens "wikiHow + Gegenstandssätze" und gibt einen Einführung in verschiedene Typen von Restriktionen, wie z.B. Modifizierer, Methoden und Zwecke, die bei der Sprachplanung verwendet werden können. Die Person auf dem Bildschirm scheint in einem modernen Büro zu sein, mit Tischen und Stühlen im Hintergrund.</sample>
    <sample id="285">Ja, der englische Text lautet: "Can LLMs do Constrained Language Planning?" und "All baselines achieve unsatisfactory results on planning for specific goals."</sample>
    <sample id="286">Ja, die LLMs können Beschränkungen in der Sprachplanung bewältigen.</sample>
    <sample id="287">Was sind die häufigsten Fehler, die von LLMs bei dieser Aufgabe gemacht werden?</sample>
    <sample id="288">Was sind die häufigsten Fehler, die bei der Ausführung dieser Aufgabe von LLMs (Language Learning Models) gemacht werden?</sample>
    <sample id="289">Was sind die Arten von Zielen, die InstructGPT typischerweise fehlt?</sample>
    <sample id="290">Auf dem Bild ist ein Video call zu sehen, bei dem eine Person im Bildrahmen steht. Sie trägt Brille und ein grünes Hemd. Im Hintergrund erscheint ein moderner Büro oder Wohnraum mit einem großen Fenster, das natürliches Licht hereinlässt. Der Schwerpunkt des Bildes liegt auf einer Präsentation, die über den gesamten Bildausschnitt erstreckt ist. Die Präsentation hat einen Titel "Method" und enthält Informationen über eine Methode zur Erstellung von spezifischen Zielen durch InstruktGPT mittels In-Context-Learning. Es gibt auch eine Schrittweise Anleitung und spezifische Ziele wie "G1: (+ modifier) Make a chocolate cake", "G2: (+ method) Make in a microwave" und "G3: (+ intent) Make a cake for a wedding". Diese Details deuten darauf hin, dass es sich um eine Präsentation über eine Methode für die Zielsetzung und -realisierung handelt, möglicherweise in einem wissenschaftlichen oder beruflichen Kontext.</sample>
    <sample id="291">Das Bild zeigt eine Präsentation über einen Methodenansatz für die Erstellung von spezifischen Zielen aus einem abstrakten Ziel. Der Titel der Präsentation lautet "Method". Die erste Schritt in diesem Ansatz ist, ein abstraktes Ziel zu generieren, indem man InstructGPT über Kontext-Erlebnisler lernt. Das abstrakte Ziel lautet "Bake a cake", und es gibt einige Constraints (Restriktionen) dazu. Es gibt drei spezifische Zielen: G1, G2 und G3. Jedes Ziel hat seine eigene Beschreibung und ein zugehöriges Bild. Zum Beispiel: - G1: "Make a chocolate cake" mit dem Bild eines Kuchens - G2: "Make it in the microwave" mit dem Bild eines Mikrowellenofens - G3: "Make a cake for a wedding" mit dem Bild eines Hochzeitszimmers Diese spezifischen Zielen sind aufgrund des abstrakten Ziels "Bake a cake" generiert worden.</sample>
    <sample id="292">Der englische Text lautet: "Method Input: an abstract goal Step 1: Generate specific goals with InstructGPT via in-context learning Specific Goals: G1 (+ modifier): Make a cake with whipped cream and chocolate chips G2 (+ modifier): Make it in the microwave G3(+ intent): Make for a wedding Step 2: Over-generate candidate scripts with InstructGPT via in-context learning Candidate Scripts Generate Plans for G1"</sample>
    <sample id="293">Das Bild zeigt eine Präsentation über einen Methodenansatz. Links steht der Titel "Method" in großen roten Buchstaben. Darunter sind zwei Schritte aufgeführt:</sample>
    <sample id="294">Der englische Text lautet: "Method Step 2: Over-generate candidate scripts via in-context learning. Step 3: Find the goal with InstructGPT via similarity score. Output: Specific goals with corresponding scripts."</sample>
    <sample id="295">Das Bild zeigt eine Person, die am Computer steht und möglicherweise einen Vortrag gibt. Sie trägt ein grünes Shirt und ist in einem modernen Bürobereich zu sehen, der mit großen Fenstern und einer offenen Atmosphäre ausgestattet ist. Im Hintergrund sind andere Personen zu erkennen, die an ihren Arbeitsplätzen sitzen. Der Schwerpunkt des Bildes liegt auf dem Vortrag, der auf der linken Seite des Bildes dargestellt wird. Der Vortrag hat den Titel "Method" und enthält Schritte und Diagramme, die auf das Thema "Candidate Scripts" und "Filtered Scripts" hinweisen. Diese visuellen Elemente deuten darauf hin, dass der Vortrag sich um eine Methode für die Erstellung oder Auswahl von Skripts handelt.</sample>
    <sample id="296">Das Bild zeigt eine Präsentation, die "Unser Method improves die Planungsqualität sehr stark" als Titel trägt. Links auf dem Bild ist ein Graphik dargestellt, der verschiedene Modelle und ihre Genauigkeit in Bezug auf die Planung zeigt. Die Modelle sind T5 (11B), Flan-T5 (11B), GPT-3 (175B), InstructGPT (175B) und "Unsere Methode". Der Graphik zeigt die Genauigkeit (%) für jede Methode an. Rechts auf dem Bild ist ein kleineres Bild eines Menschen zu sehen, der wahrscheinlich über Skype oder einer ähnlichen Plattform teilnimmt. Im unteren linken Eck des Bildes steht ein Text, der lautet: "Mit unserer Methode kann InstructGPT Skripte von hoher Qualität erzeugen, mit einem großen Vorsprung."</sample>
    <sample id="297">Das Bild zeigt eine Präsentation über "Script Distillation from LLMs" (Skript-Entleerung von LLMs). Die Präsentation besteht aus einem Textblock und drei Schritten, die jeweils eine Aufgabe beschreiben. Der Textblock ist unterteilt in zwei Abschnitte: "Motivation" und "Method". Im "Motivation"-Abschnitt steht: "Um eine reduzierte Sprachplanungsfähigkeit für kleinere Modelle zu ermöglichen." Im "Method"-Abschnitt steht: "Folgen Sie der ideologischen Wissensentleerung - Generieren 55.000 Skripts mit Restriktionen auf Basis von LLMs auf unsere Methode = Coscript-Dataset - Benutzer annotationieren Validierung und Testset."

Die drei Schritte sind wie folgt dargestellt:

1. **Schritt 1:** Generate specific goals with InstructGPT via in-context learning
2. **Schritt 2:** Over-generate candidate scripts with InstructGPT via context with instruction
3. **Schritt 3:** Filtered scripts to the goal with InstructGPT via context with instruction score

Im "Output"-Abschnitt steht: "Specific Goals with corresponding plans"

Rechts im Bild ist ein Personenbild zu sehen, der wahrscheinlich die Präsentation gibt. Der Hintergrund des Bildes zeigt eine moderne, gut beleuchtete Zimmermitte mit einem großen Fenster, Tischen und Stühlen.</sample>
    <sample id="298">Das Bild zeigt eine Präsentation über "Script Distillation from LLMs" (Skript-Distillation von LLMs). Die Präsentation ist in zwei Hauptabschnitte unterteilt: "Motivation" und "Method". Im "Motivation"-Abschnitt steht: "Um eine geringere Sprachplanungsfähigkeit für kleinere Modelle zu ermöglichen." Im "Method"-Abschnitt wird aufgebracht: "Folgen Sie der Symbolwissensdistillation. Generiert 55.000 Skripte mit Restriktionen auf Basis von LLMs aufgrund unserer Methode: Coscript Dataset. Annotieren Sie Validation und Test." Darüber hinaus sind drei Schritte im Prozess der Script-Distillation dargestellt: 1. Generieren spezifischer Zielskripte mit InstructGPT über interaktiver Lernung. 2. Übergenerieren von Skripts mit InstructGPT über interaktiver Lernung. 3. Filtern von Skripts nach Ziel mit InstructGPT über Intelligenzscoring. Das Bild zeigt auch eine Person, die das Präsentation aufnimmt oder anzeigt.</sample>
    <sample id="299">Das Bild zeigt eine Präsentation über "Script Distillation from LLMs" (Skriptdistillation aus LLMs). Die Präsentation ist in zwei Hauptabschnitte unterteilt: "Motivation" und "Method". Im "Motivation"-Abschnitt steht: "Um eine reduzierte Sprachplanungsfähigkeit für kleinere Modelle zu ermöglichen." Im "Method"-Abschnitt steht: "Folgen Sie der ideologischen Wissensdistillation. Generiert 55.000 Skripts mit Restriktionen aufgrund von LLMs auf Basis unserer Methode =&gt; Coscript Dataset. Annotieren Sie die Validation und Testset durch Menschen." Darüber hinaus sind drei Schritte aufgeführt: 1. Generieren spezifischer Zielabschnitte mit InstructGPT über kognitive Lernung. 2. Übergenerieren von Skripts mit InstructGPT über kognitive Lernung. 3. Filtern der skriptierten Skripte nach dem Ziel mit InstructGPT über kognitive Lernung. Das Bild zeigt auch eine Person, die das Präsentationsbildschirm anzeigt, wahrscheinlich während einer Online-Vorlesung oder Präsentation.</sample>
    <sample id="300">Das Bild zeigt eine Präsentation über "Script Distillation from LLMs" (Schriftzusammensetzung von LLMs). Es gibt drei Hauptabschnitte: Motivation, Method und Step 1. Der Motivation-Abschnitt lautet: "Um eine beschränkte Sprachplanungsfähigkeit für kleinere Modelle zu ermöglichen." Im Method-Abschnitt steht: "Folgen Sie der ideologischen Wissenszusammensetzung. Generiert 55.000 Skripte mit Beschränkungen auf Basis von LLMs aufgrund unserer Methode =&gt; Coscript Dataset. Annotieren die Benutzer Validierung und Testset." Das Step 1-Abteilung ist unter dem Titel "Input: ein Abstract" und beschreibt den Prozess: "Generieren spezifischer Ziele mit InstructGPT durch interaktive Lernung."</sample>
    <sample id="301">Das Bild zeigt eine Präsentation über "Script Distillation from LLMs" (Skriptdistillation aus LLMs). Die Präsentation ist in zwei Hauptabschnitten unterteilt: "Motivation" und "Method". Im "Motivation"-Abschnitt steht: "Um eine reduzierte Sprachplanungsfähigkeit für kleinere Modelle zu ermöglichen." Im "Method"-Abschnitt sind die folgenden Schritte aufgeführt: 1. Generieren spezifischer Zielen mit InstructGPT durch interaktives Lernen. 2. Erzeugen übergegenerierte Skripts mit InstructGPT durch interaktives Lernen. 3. Filtern geeigneter Skripts zum Ziel mit InstructGPT durch die Symbolikscore. Darüber hinaus wird erwähnt, dass 55.000 Skripts mit Restriktionen von LLMs aufgrund der Methode "CoSprint Dataset" generiert wurden, und dass diese Skripts von Menschen handschriftig überprüft und getestet werden sollten. Im "Output"-Abschnitt steht: "Skripte mit entsprechenden Plänen." Im Hintergrund der Präsentation ist ein Video-Call-Beitrag zu sehen, in dem eine Person im Raum sitzt.</sample>
    <sample id="302">Das Bild zeigt eine Präsentation über "Constraint Analysis" und "Coscript for Smaller Language Models". Links auf dem Bild ist ein Kreisdiagramm mit verschiedenen Abschnitten, die verschiedene Konstruktoren wie "Ingredient", "Modifier", "Method" und "Using" darstellen. Rechts auf dem Bild sitzt eine Person in einem modernen Büro mit großen Fenstern. Der Text am unteren linken Eck lautet: "Coscript shows high heterogeneity and pluralism in the generated specific goals." Am unteren rechten Eck ist der Text: "Coscript for Smaller Language Models - Datasets: Coscript - wikiHow Metrics: Faithful: DeBERTa (v3 large) model to decide whether the generated texts are faithful to the constraints Automatic metrics: ROUGE, BLEU, BERTScore." Am unteren linken Eck des zweiten Bildes steht der Text: "Specialized Models vs. LLMs GPT-3 (175B), Codex (175B), InstructGPT (175B), T5 trained on wikiHow, T5 trained on Coscript Smaller LM's fine-tuned on Coscript can generate higher quality scripts than LLMs."</sample>
    <sample id="303">Der englische Text lautet: "Specialized Models vs. LLMs" und "Smaller LM's fine-tuned on Coscript can generate higher quality scripts than LLMs".</sample>
    <sample id="304">Summarizing the key points from the slide, it discusses issues related to constrained language planning. It emphasizes evaluating LLMs (Large Language Models) for their ability in this area and suggests using an over-generate-then-filter method. The slide also mentions creating a high-quality script dataset called CoScript for these purposes.

Moving on to future work ideas, there's talk about improving LLMs through a post-hoc approach. Additionally, they mention Coscript as something that builds upon existing methods but with one extra constraint added. Lastly, it highlights how such datasets can be valuable resources for advancing research into more complex goals and constraints within language planning.</sample>
    <sample id="305">Das Bild zeigt eine Person, die wahrscheinlich eine Präsentation über Sprachplanung für LLMs (Language Learning Models) gibt. Die Person trägt ein grünes Hemd und ist im Bild rechts zu sehen. Im Hintergrund sind Holztische und Stühle zu erkennen, was auf einen Arbeitsplatz oder eine Klassenzimmerartige Umgebung hinweist. Links im Bild befindet sich eine Slide mit dem Titel "Summary and Takeaways". Die Slide enthält folgende Punkte: - Der englische Text lautet: "Establish the constrained language planning problem." - "Evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs." - "Use LLMs to generate a high-quality script dataset (CoScript) for constrained language planning." - "Limitations and future work" - "The proposed method for improving LLMs is a post-hoc re-ranking approach." - "CoScript only inherits from an abstract one with one extra constraint." - "CoScript dataset can be a valuable resource to advance the research on language planning with more complex goals and constraints." Die Slide wirkt wie Teil einer akademischen Präsentation oder eines Workshops über Sprachtechnologien und ihre Anwendung in der Sprachplanung.</sample>
    <sample id="306">Das Bild zeigt eine Screenshot eines Online-Vortrags oder einer Präsentation. Im Hintergrund ist ein Bild einer Stadtlandschaft zu sehen, möglicherweise Toronto in Kanada, mit einem Text, der den 61. Treffen der Gesellschaft für Computational Linguistik (Association for Computational Linguistics) anzeigt. Der Titel des Vortrags lautet "Distilling Script Knowledge from Large Language Models for Constrained Language Planning". Der Vortrager wird im Vordergrund dargestellt, sitzt in einem Zimmer mit einem weißen Wand und einem großen Fenster. Links auf dem Bild befindet sich ein QR-Code und eine Webseite, die Informationen über den Vortrag oder den Vortrager bereitstellt.</sample>
    <sample id="307">PaLM's fluency is comparable to SOTA, but its accuracy scores are generally lower.</sample>
    <sample id="308">Das wichtigste Eigenschaft eines Wasserzeichenverfahrens ist seine Anwendbarkeit für EaaS (Embedded Advertising as a Service).</sample>
    <sample id="309">English, Deutsch, Español, Français, עברית, Italiano, 日本語, 한국어, Nederlands, Português, Română, Русский, Türkçe, 中文</sample>
    <sample id="310">300</sample>
    <sample id="311">The similarity difference and p-value of KS test.</sample>
    <sample id="312">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in der Aufgabe als Teil der Gruppe "Enc-Dec: Multilingual Pretrained Encoder-Decoder Models" eingesetzt.</sample>
    <sample id="344">Die Autoren entscheiden, was Wörter mit mittlerer Häufigkeit sind, indem sie die Wörter aus einem allgemeinen Textcorpus Zählung und dann aus einer bestimmten Intervallzeitbereich zufällig auswählen.</sample>
    <sample id="345">Das englische Textstück lautet: "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?"</sample>
    <sample id="346">Benutzererkennung und allgemeinerehrung</sample>
    <sample id="347">Der englische Text lautet: "Models have been using CoNLL-2003 to develop NER for almost 20 years. Can these models generalize to modern data?"</sample>
    <sample id="348">Modellierungen wurden seit fast 20 Jahren aufgrund von CoNLL-2003 für die Entwicklung von NER verwendet. Können diese Modelle sich auf moderne Daten anpassen? Was ist erforderlich, um eine gute Anpassung zu gewährleisten?</sample>
    <sample id="349">Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years Can these models generalize to modern data? What is needed for good generalization? What causes the performance drop?</sample>
    <sample id="350">Der englische Text lautet: 'Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines.'</sample>
    <sample id="351">CoNLL++ Dataset Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines Fine-tuned 20+ models on CoNLL-2003 Evaluated on CoNLL-2003 test set &amp; CoNLL++ AMBASSADOR O TO THE UNITED NATIONS : LINDA I-PER THOMAS-GREENFIELD I-PER Georgia Tech</sample>
    <sample id="352">CoNLL++ Dataset</sample>
    <sample id="353">Was benötigt man für eine gute Allgemeinbildung?</sample>
    <sample id="354">Was ist für eine gute Allgemeinverbreitung erforderlich?</sample>
    <sample id="355">Was benötigt man für gute Generalisierung?</sample>
    <sample id="356">Was sind für eine gute allgemeinereignis erforderlich?</sample>
    <sample id="357">Was verursacht eine Leistungsverlust?</sample>
    <sample id="358">Was verursacht eine Leistungsverlust?</sample>
    <sample id="359">Was verursacht einen Leistungsabfall?</sample>
    <sample id="360">Was verursacht einen Leistungsabfall?</sample>
    <sample id="361">Der englische Text lautet: "What Causes Performance Drop?"</sample>
    <sample id="362">Was verursacht eine Leistungsverfall? Adaptive Übergreifung? Keine Verluste an Effizienz?</sample>
    <sample id="363">Was verursacht einen Leistungsabfall? Adaptive Übergreifende Übereinstimmung? Keine Minderung des Rendements? Nicht beobachtet Temporales Abflanken?</sample>
    <sample id="364">Der englische Text lautet: "What Causes Performance Drop?" und "Temporal drift?" Diese Übersetzungen deuten auf mögliche Gründe für eine Abfahrt der Leistung hin.</sample>
    <sample id="365">Was verursacht einen Leistungsabfall? Adaptive Überschneidung, keine wachsenden Rückerträge, nicht beobachtet, temporaler Abrieb, Leistung wird mit größerem Zeitabstand abfallen.</sample>
    <sample id="366">Das Bild zeigt eine Präsentation, die mit dem Titel "Conlusion" beginnt. Es gibt drei Punkte, die auf die Bedingungen für eine gute allgemeine Verständnisfähigkeit hinweisen: - Eine bessere Modellarchitektur - Ein größerer Modellgröße - Mehr sorgfältig gewählte Beispiele Die rechte Seite des Bildes zeigt ein Diagramm, das die Entwicklung von Modellen von 2004 bis 2022 darstellt. Das Diagramm zeigt verschiedene Modelle und ihre Leistungen über die Jahre. Im unteren rechten Eck ist das Logo der Georgia Tech zu sehen.</sample>
    <sample id="367">Für eine gute Allgemeinheit benötigen wir: - Bessere Modellarchitektur - Größere Modellgröße - Bessere Anlaufbeispiele für die Ausbildung</sample>
    <sample id="368">Das englische Inhalt auf dem Bild lautet: "Conclusion For a good generalization, we need: • Better model architecture • Larger model size • More fine-tuning examples Performance drop is caused by: • Temporal drift • Not adaptive overfitting Do CoNLL-2003 taggers still work?"</sample>
    <sample id="369">Für eine bessere allgemeine Leistung benötigen wir: - Eine bessere Modellarchitektur - Ein größeres Modellgröße - Bessere Ausnahmeprozesse</sample>
    <sample id="370">Paper: https://arxiv.org/abs/2212.09747 Dataset: https://github.com/ShuhengL/ac12023_conllpp Contact: sliu775@gatech.edu</sample>
    <sample id="397">500-1,000 Wörter</sample>
    <sample id="398">Der Beispielsatz erfordert das Wissen über die Berufsfunktion von Servin als Richter und den Arbeitsverhältnissen in einem Gericht.</sample>
    <sample id="399">Beispielqualität</sample>
    <sample id="400">Die Arbeiten konzentrieren sich auf Sprachmodelle wie BERT-base, RoBERTa-base und distilRoBERTa-base.</sample>
    <sample id="401">Das Modell verwendet Werte aus mehreren Ebenen.</sample>
    <sample id="402">"Did you mean easy on me or I gotta feeling?" und "The newer one. The song that's not energetic."</sample>
    <sample id="403">The authors belong to the University of Hong Kong.</sample>
    <sample id="404">Sechs Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="405">Ja, die Übersetzung der natürlichen Anfrage mit einem maschinellen Übersetzungsmodel wurde als Baseline betrachtet.</sample>
    <sample id="406">Die Autoren haben eine Gruppe von Kriegerinnen als Beispiel für eine markierte Gruppe gegeben.</sample>
    <sample id="407">Transformer models.</sample>
    <sample id="408">FT_C, COSINE, LZR, BOND, MLC, Weak labels</sample>
    <sample id="409">Sechs Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="410">Mehrere Modalitäten</sample>
    <sample id="439">Hier ist die Antwort auf Ihre Frage:</sample>
    <sample id="440">Zhiyang Xu, Ying Shen, Lifu Huang</sample>
    <sample id="441">Ja, Coscript hat eine Qualitätskontrolle durchlaufen.</sample>
    <sample id="442">Die Grenzen bestehender Ressourcen für kontextbasierte Übersetzung liegen darin, dass nur eine kleine Bruchteile von Worten auf Kontext abhängt und die aktuellen Methoden begrenzte Diskurs-Phänomene und Sprachen unterstützen.</sample>
    <sample id="443">Das englische Text ist "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)".</sample>
    <sample id="444">Resolvendo Expressões Indiretamente Referenciando para Seleção de Entidades (Corpo de Alternativas de Entidades)</sample>
    <sample id="445">Indirekte Beziehungsphrasen</sample>
    <sample id="446">Indirekte Bezeichnungen</sample>
    <sample id="447">Indirekte Bezeichnungen</sample>
    <sample id="448">Indirekte Bezeichnung von Objekten</sample>
    <sample id="449">Indirekte Bezeichnungen können in natürlichen und flüssigen Gesprächssituationen verwendet werden. Sie können nicht den Namen erinnern, die Anordnungen sind schwierig zu unterscheiden und man möchte eine Vorliebe angeben.</sample>
    <sample id="450">Das Hauptthema des Sprechers ist "Dataset Collection".</sample>
    <sample id="451">Das Bild zeigt eine Präsentation mit dem Titel "Dataset Collection" auf einem weißen Hintergrund. Links oben befindet sich das Google Research Logo in Farbe. Der Text ist in schwarzer Schrift und enthält folgende Punkte: - "Important problem" - "Conversational systems" - "Benchmarking Large Language Models' entity understanding" - "No large-scale public dataset available" - "We collect a large dataset using crowd annotation" - "Three domains:" Darunter sind drei Symbole zu sehen: ein rosa Kopfhörer, eine grüne Bücherregal mit Büchern und ein gelbes Deckel. Im unteren rechten Eck des Bildes ist ein kleiner Bildschirm eines Mannes zu sehen, der wahrscheinlich die Präsentation gibt. Am unteren Rand des Bildes ist der Text "Resolving Indirect References for Entity Selection (Utilities Corpus)" zu lesen.</sample>
    <sample id="452">Das Bild zeigt eine Präsentation über die "Dataset Collection Methodology" von Google Research. Es gibt einen Titel, der den Schwerpunkt auf die "Informalität" in einem "cartoon Completion Task" legt. Darunter sind vier Panels zu sehen, die jeweils eine Person zeigen und verschiedene Dialog-Fragen und -Antworten darstellen. Links steht ein Text, der auf die Auswahl eines Dialogkontexts aus einer Handbuch-Liste hinweist. Rechts ist ein Text zu sehen, der auf die Einstellung einer Frage oder Antwort hinweist. Im unteren rechten Eck des Bildes ist ein kleiner Profilbild eines Mannes zu sehen, der wahrscheinlich den Vortrag gibt.</sample>
    <sample id="453">Das Bild zeigt eine Präsentation über "Dataset Collection Methodology" (Datenbanksammlungsverfahren). Es handelt sich um ein Slidedesign von Google Research, das auf einem weißen Hintergrund präsentiert wird. Im oberen rechten Eck befindet sich das Google Research-Logo. Das Hauptthema der Präsentation ist die Verwendung eines Cartoon-Beitrags für die Sammlung von Daten, wie im Text "Methodology emphasizes informality using a cartoon completion task" (Die Methodologie betont die Unformalität durch das Verwenden eines Cartoon-Fertigungsauftrags) angegeben.

Das Slide enthält auch eine Illustration mit vier Figuren, die in einer Diskussion miteinander sind. Neben den Figuren sind drei Textblöcke zu sehen:

1. "Sets the dialog context chosen from a few manual prompts per domain" (Setzt den Dialogkontext aus einigen manuellen Anweisungen pro Domain)
2. "The alternative question" (Die alternative Frage)
3. "Expression referring to one of the entities" (Ausdruck, der auf eine der Entityen verweist)

Rechts daneben ist ein weiterer Textblock mit dem Text "Filled in by the annotator" (Füllt der Annotator ein), der auf eine weitere Funktion hinweist, die der Annotator erfüllt.

Im unteren linken Eck des Slides ist der Text "Recognizing Indirect Referring Expressions for Entity Selection in Multimodal Corpora" (Erkennen von indirekt verweilenden Ausdrücken für Entity-Selektion in multimodalen Korporen) zu lesen.

Im unteren rechten Eck des Slides ist ein kleiner Bildschirm eines Mannes zu sehen, der möglicherweise einen Teilnehmer oder eine Referentin darstellt.</sample>
    <sample id="454">Das Bild zeigt eine Präsentation von Google Research über die Sammlung von Daten. Im Zentrum des Bildes befindet sich ein Text, der "Dataset Collection Methodology" (Daten-Sammlungs-Methodologie) besagt. Darunter steht ein Punkt, der darauf hinweist, dass die Methode die Unformalität betont und dies durch einen Cartoon-Fertigungsaufgaben-Task erreicht. Links sind drei Figuren zu sehen, jede mit einem Textfeld neben ihr. Das erste Textfeld liest: "Sets the dialog context chosen from a few manual prompts per domain" (Setzt den Dialogkontext aus einigen manuellen Hinweisen pro Domain). Das zweite Textfeld lautet: "The alternative question" (Die alternative Frage). Das dritte Textfeld lautet: "Expression referring to one of the entities" (Ausdruck, der auf eines der Entityten bezieht). Rechts ist ein Textfeld mit dem Text "Filled in by the annotator" (Füllt der Annotator aus) markiert. Im unteren rechten Eck des Bildes ist ein kleiner Bild eines Mannes zu sehen, der wahrscheinlich der Sprecher oder einer der Personen im Vortrag ist. Der Hintergrund ist weiß, und es gibt einige Pflanzen und Möbelstücke, um die Szene zu ergänzen.</sample>
    <sample id="455">Das Bild zeigt eine Präsentation über "Dataset Collection Methodology" von Google Research. Im Vordergrund sind vier Figuren zu sehen, die in einem Gespräch miteinander sind. Neben einer der Figuren steht ein Text, der lautet: "Sets the dialog context chosen from a few manual prompts per domain". Links davon steht ein weiterer Text, der lautet: "The alternative question". Rechts daneben steht ein weiterer Text, der lautet: "Expression referring to one of the entities". Im oberen rechten Eck des Bildes ist ein Text, der lautet: "Filled in by the annotator". Der Text oben auf dem Bild lautet: "Dataset Collection Methodology" und darunter: "Methodology emphasizes informality using a cartoon completion task".</sample>
    <sample id="456">Das Bild zeigt eine Präsentation über die Sammlung von Daten, speziell die Methode zur Sammlung von Daten durch das Verwenden eines Cartoon-Beilagenaufgaben. Die Präsentation wurde von der Google Research-Firma erstellt und zeigt ein Diagramm mit drei Figuren, die verschiedene Dialogszenarien darstellen. Diese Figuren zeigen, wie der Kontext des Dialogs und alternative Fragen für die Sammlung von Daten vorgeführt werden. Der Text auf dem Bild lautet: "Dataset Collection Methodology" und "Methodology emphasizes informality using a cartoon completion task." Darüber hinaus gibt es einen Text, der lautet: "Sets the dialog context chosen from a few manual prompts per domain," "The alternative question," und "Expression referring to one of the entities." Im unteren rechten Eck des Bildes ist ein kleiner Bild eines Mannes zu sehen, der möglicherweise den Vortrag gibt.</sample>
    <sample id="457">Das Bild zeigt eine Präsentation über die Datensammlungsmethode, speziell auf die Verwendung eines Cartoon-Task für die Emittierung von informellen Dialogen. Die Texte auf dem Bild sind in Englisch und deuten auf die Rolle der Annotatoren hin, die den Inhalt des Cartoons ausfüllen müssen. Diese Methode scheint für die Erstellung von Entity Selection Utilities relevant zu sein.</sample>
    <sample id="458">Das Bild zeigt eine Präsentation über die Erstellung alternativer Fragen zur Identifizierung von Entitäten. Der Titel der Präsentation lautet "Generate alternative questions =&gt;&gt; sampling entity pairs". Die Präsentation bietet eine Liste von Vorschlägen für die Erstellung solcher Fragen, wie z.B. "Do you mean A or B?", "Items with similar infoboxes on Wikipedia (same genre and/or artist)", "Items with similar descriptions on Wikipedia", "Items with similar titles", und "Uniform at random". Im unteren rechten Eck des Bildes befindet sich ein kleiner Bildschirm mit einem Mann, der wahrscheinlich der Sprecher oder der Referent ist. Im oberen rechten Eck befindet sich das Google Research Logo. Das Bild wurde aufgenommen während einer Präsentation, wie aus dem Text "Reading Indirect Expressions for Entity Selection in Adversarial Convolutional Networks" im unteren linken Eck hervorgeht.</sample>
    <sample id="459">Das Bild zeigt eine Präsentation mit dem Titel "Generate alternative questions =&gt; sampling entity pairs". Es gibt eine Frage, die lautet: "Do you mean A or B?" Darunter sind verschiedene Optionen aufgelistet, wie z.B. "Items with similar infoboxes on Wikipedia (same genre and/or artist)", "Items with similar descriptions on Wikipedia", "Items with similar titles", und "Uniform at random". Links am Bild ist ein grüner Pfeil mit der Aufschrift "More Similar (likely harder)" zu sehen. Im unteren rechten Eck des Bildes ist ein kleines Profilbild eines Mannes zu erkennen. Im unteren linken Eck befindet sich das Logo von Google Research.</sample>
    <sample id="460">Generiere alternative Fragen =&gt; Ähnliche Entity Paare</sample>
    <sample id="461">Der englische Text lautet: "Generate alternative questions =&gt; sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean This is it or Man in the Mirror? Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean Could You Be Mine or The Way I Am?"</sample>
    <sample id="462">Das Bild zeigt eine Präsentation mit dem Titel "Generate alternative questions =&gt; sampling entity pairs". Es gibt eine Frage auf der Oberseite, die lautet: "Do you mean A or B?" Darunter sind verschiedene Optionen aufgelistet, wie z.B. "Items with similar infoboxes on Wikipedia (same genre and/or artist)", "Items with similar descriptions on Wikipedia", "Items with similar titles" und "Uniform at random". Links am Bild ist ein roter Pfeil mit der Aufschrift "More Similar (likely harder)" zu sehen. Im unteren rechten Eck des Bildes befindet sich ein kleines Foto eines Mannes mit schwarzen Haaren und einem weißen Hemd. Im unteren linken Eck ist ein weiterer Text sichtbar, der lautet: "Reading Indirect Expressions for Entity Selection in Adversarial Convolutional Networks."</sample>
    <sample id="463">Das Bild zeigt eine Präsentation von Google Research. Der Titel der Slide lautet "Background knowledge (Music)". Es gibt Informationen über das Hinzufügen eines Google-Such-Links zu jedem Lied, um mehr über das Lied zu erfahren. Zwei Lieder werden aufgeführt: "Easy on Me" von Adele und "I Gotta Feeling" von The Black Eyed Peas. Es wird auch gefordert, Annotatoren dazu zu bitten, mindestens einige Lieder zu hören und jeden Lied zu lesen. Im unteren rechten Eck des Bildes ist ein kleiner Bild von einem Mann zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="464">Das Bild zeigt eine Präsentation von Google Research. Der Titel der Präsentation lautet "Background knowledge (Music)". Es gibt zwei Lieder, "Easy on Me" von Adele und "I Gotta Feeling" von The Black Eyed Peas, jedes mit einem Google-Searcheintrag. Darunter steht: "Wir bitten die Annotatoren, um mindestens einige Lieder zu hören und jede Liedtext zu lesen." Im unteren rechten Eck des Bildes ist ein Mann zu sehen, der wahrscheinlich über diese Präsentation spricht.</sample>
    <sample id="465">Das Bild zeigt eine Präsentation von Google Research über "Background knowledge (Music)". Es gibt zwei Songs, "Easy on Me" von Adele und "I Gotta Feeling" von The Black Eyed Peas. Jeder Song hat einen Google-Suche-Link. Darüber hinaus werden Annotatoren aufgefordert, mindestens einige Lieder zu hören und über jedes Lied zu lesen.</sample>
    <sample id="466">Das Bild zeigt eine Slide aus einem Präsentation, die Informationen über zwei Backwarenarten präsentiert: Simnel Cake und Pandan Cake. Links auf der Slide befindet sich ein Bild eines Simnel Cakes, das wie eine runde Kuchenform aussieht, mit einer Schicht von Nusspaste und einem Set von elf kleinen Kugeln darauf. Rechts auf der Slide ist ein Bild eines Pandan Cakes zu sehen, das eine grüne Farbe aufweist und mit einer Schicht von cremiger Füllung gefüllt ist. Der Text auf der Slide gibt Details über die Herkunft und die Zutaten dieser Backwarenarten.</sample>
    <sample id="467">Elicitating expressions</sample>
    <sample id="468">Das englische Inhalt ist auf Deutsch übersetzt, und die Übersetzung lautet: 'Wir zeigen den Annotatoren eine Wahl und bitten sie, es zu beschreiben.'</sample>
    <sample id="469">Der englische Text lautet: "AltEntities Corpus ~6,000 alternative questions across the three domains ~42,000 indirect referring expressions Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82%-87% when the LM has access to partially overlapping background knowledge. ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable." Diese Übersetzung gibt einen Eindruck von den Ergebnissen und Merkmalen des AltEntities Corpus und seiner Verwendung mit dem T5 XL-Modell im Zusammenhang mit der Genauigkeit bei der Identifizierung von Entitäten.</sample>
    <sample id="470">Der englische Text lautet: "AltEntities Corpus - ~6,000 alternative questions across the three domains - ~42,000 indirect referring expressions Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82%-87% when the LM has access to partially overlapping background knowledge. ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research-datasets/AltEntities"</sample>
    <sample id="471">Das englische Inhalt auf dem Bild lautet: "AltEntities Corpus - ~6,000 alternative questions across the three domains - ~42,000 indirect referring expressions Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82-87% when the LM has access to partially overlapping background knowledge. ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research/datasets/AltEntities"</sample>
    <sample id="472">Das Bild zeigt eine Präsentation über den 'AltEntities Corpus'. Es gibt Informationen über die Anzahl der alternativen Fragen und direkten Indikatoren, sowie die Genauigkeit des T5 XL-Modells. Der Text lautet: 'AltEntities Corpus - ~6.000 alternative questions across the three domains - ~42.000 indirect referring expressions - Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators - 82-87% when the LM has access to partially overlapping background knowledge - ~60% when the LM (T5 XL) has only access to the entity names - We showed models are domain-generalizable - Dataset Link: https://github.com/google-research-datasets/AltEntities' Die Google Research-Logo ist im oberen rechten Eck sichtbar. Am unteren rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen. Im nächsten Bild zeigt der Text 'Thank You If you have any questions, please email javadh@google.com' an.</sample>
    <sample id="473">Wait-k, LA, CAAT</sample>
    <sample id="474">Avignon Université und Nantes Université</sample>
    <sample id="475">Sebastian Santy, Jenny T. Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap</sample>
    <sample id="476">Es sind drei Autoren an der Arbeit beteiligt: Myra Cheng, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="477">"Attention as a Guide for Simultaneous Speech Translation" von Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="478">Das Bild zeigt eine Präsentation über 'Simultaneöse Sprachübersetzung' (Simultaneous Speech Translation). Links oben steht der Titel 'Was ist Simultaneöse Sprachübersetzung?' (What is Simultaneous Speech Translation?). Rechts oben ist ein kleiner Bildschirm mit einer Person zu sehen. Im Zentrum des Bildes sind zwei Textfelder zu sehen: links ein Textfeld mit dem deutschen Satz 'Wenn ich im Sommer kalten Tee in meine Thermoskanne gieße, bleibt er kalt, und wenn ich Winter' (When I have cold tea in my thermos in the summer, it stays cold, and when I), und rechts ein Textfeld mit dem englischen Übersetzungsresultat 'When I have cold tea in my thermos in the summer, it stays cold, and when I pour'. Am unteren Rand des Bildes steht die Text 'page 03' und eine Quellenangabe: 'Image Credits to https://ai.googleblog.com/2021/01/stabilizing-live-speech-translation-in.html'.</sample>
    <sample id="479">Das Bild zeigt eine Präsentation, die sich auf die Probleme der aktuellen SimulST-Modelle konzentriert. Der Titel der Präsentation lautet: "Was sind die Probleme der aktuellen SimulST-Modelle?" Diese Frage wird in Englisch geschrieben und ist im oberen linken Eck des Bildes zu sehen. Im unteren rechten Eck des Bildes befindet sich eine Seitezahl, die auf Seite 04 steht. Im unteren linken Eck des Bildes ist ein blauer Symbol oder Logo sichtbar. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen, der wahrscheinlich über eine Videokonferenz teilnimmt.</sample>
    <sample id="480">Was sind die Probleme der aktuellen SimulST-Modelle?</sample>
    <sample id="481">Was sind die Probleme der aktuellen SimulST-Modelle?</sample>
    <sample id="482">Was ist unsere Lösung?</sample>
    <sample id="483">Der englische Text lautet: "Use already existing offline ST models without re-training or adopting specific architecture for SimulST" und "Use only one model for every latency regime and handle latency through specific parameters".</sample>
    <sample id="484">Was ist unsere Lösung? 1. Nutzen Sie bereits vorhandene offline ST-Modelle ohne erneutes Training oder die Verwendung eines bestimmten Architektur-Designs für SimuST. 2. Benutzen Sie nur ein einzelnes Modell für jede Latenzregime und verwalten die Latenz über spezifische Parameter. 3. Nutzen Sie das von der Modell bereits vorher erworbenen Wissen über das Aufmerksamkeitsmechanismus zwischen Audiodaten und Textausgabe.</sample>
    <sample id="485">Das Bild zeigt eine Präsentation über ein Problembeispiel. Im oberen linken Eck befindet sich der Text 'Our solution: EDAtt'. Im oberen rechten Eck ist ein kleiner Bildschirm mit einer Person zu sehen, die möglicherweise die Präsentation führt oder beobachtet. Im mittleren Abschnitt des Bildes steht der Text 'Encoder-Decoder Attention' in bold, blauer Schrift. Links unten ist ein blaues Logo zu sehen, und im unteren rechten Eck befindet sich der Text 'page 014', was darauf hinweist, dass dies das 14. Blatt der Präsentation ist.</sample>
    <sample id="486">Unsere Lösung: EDAtt</sample>
    <sample id="487">Der englische Inhalt lautet: "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold c) towards the last λ speech frames, meaning that the received information is enough stable."</sample>
    <sample id="488">Unsere Lösung: EDAtt</sample>
    <sample id="489">Das Bild zeigt eine Präsentation über die "Encoder-Decoder Attention" in Sprachverarbeitung. Im oberen linken Eck ist der Titel "Our solution: EDAtt" zu sehen. Der Haupttext lautet "Encoder-Decoder Attention", und darunter steht ein Text, der erklärt, wie man entscheidet, ob man einen vollständigen Übersetzung oder nur eine Teilübersetzung durchführen soll, basierend auf, ob die Aufmerksamkeit auf ein Wort gerichtet ist. Es wird erwähnt, dass ein Wort übersetzt werden sollte, wenn die Aufmerksamkeit nicht konzentriert ist, und es wird auch darauf hingewiesen, dass eine solche Situation stabile Informationen bereitstellt. Im unteren linken Eck befindet sich ein Diagramm mit einer Welle und dem Text "Ich werde reden." Darüber hinaus gibt es eine kleine Bildschirmseite im oberen rechten Eck, die eine Person zeigt. Am unteren Rand des Bildes ist die Seite-Nummer "page 017" angegeben.</sample>
    <sample id="490">Der englische Text lautet: "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold c) towards the last λ speech frames, meaning that the received information is enough stable."</sample>
    <sample id="491">Das Bild zeigt eine Präsentation über ein Thema, das "Encoder-Decoder Attention" beinhaltet. Im oberen linken Eck ist der Text "Our solution: EDAtt" zu sehen. Der Haupttext auf der Seite lautet "Encoder-Decoder Attention", und darunter ist ein Textblock mit dem folgenden Inhalt: "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold α) towards the last speech frames, meaning that the received information is enough stable." Darüber hinaus gibt es einen Diagramm, das die Worte "Ich werde reden" zeigt, und darunter steht "EMITTED". Im unteren rechten Eck des Bildes befindet sich der Text "page 021".</sample>
    <sample id="492">Unsere Lösung: EDAtt</sample>
    <sample id="493">Unsere Lösung: EDAtt</sample>
    <sample id="494">Unsere Lösung: EDAtt Entscheiden, ob man einen Vorgang vollziehen oder nur eine Teilübersetzung führen soll, basierend auf der Aufmerksamkeit, die sich auf Punkte richtet, an denen ein Wort aufgerichtet wird, wenn die Aufmerksamkeit nicht konzentriert ist (unter einem Schwellwert), und nach wie hinein zu den letzten Lautsprecherspeeches (d), was bedeutet, dass die bereitgestellte Informationen stabil genug sind.</sample>
    <sample id="495">Hauptschlußergebnisse: EDAtt</sample>
    <sample id="496">Das Bild zeigt eine Präsentation mit einem Graphen im Zentrum. Der Graph hat den Titel "Main Results: EDAtt" und zeigt die BLEU-Werte auf der Y-Achse gegen AL/AL_CA (s) auf der X-Achse ab. Der Graph ist leer, was darauf hinweist, dass keine Daten darin enthalten sind. Im oberen rechten Eck des Bildes befindet sich ein kleineres Bild eines Menschen, möglicherweise der Präsentatoren. Links am Graphen ist das Wort "quality measure" in Blau geschrieben, und rechts ist das Wort "latency measure" auch in Blau. Im unteren linken Eck des Bildes ist ein Logo zu sehen, und im unteren rechten Eck ist die Seite 028 angegeben.</sample>
    <sample id="497">Das Bild zeigt eine Präsentation über die Ergebnisse eines Experimentes namens EDAtt. Im oberen linken Eck ist der Titel "Main Results: EDAtt" zu sehen. Im Zentrum des Bildes befindet sich ein Graph, der BLEU-Werte gegen den AL/AL_CA (s) -Wert abhängt. Der Graph zeigt keine Datenpunkte und hat einen y-Achse, die von 21 bis 27 reicht. Auf der x-Achse wird der AL/AL_CA (s) -Wert von 0.5 bis 6 angezeigt. Im unteren Zentrum des Graphs ist der Text "(a) en-&gt;de" zu sehen, was auf eine Übersetzung von Englisch ins Deutsche hinweist. Im unteren rechten Eck ist der Text "latency measure" zu sehen. Im unteren linken Eck ist ein blauer Kreis mit einem weißen Symbol innerhalb zu sehen. Im unteren rechten Eck ist der Text "page 030" zu sehen. Im oberen rechten Eck ist ein kleiner Bildschirm mit einer Person zu sehen, die wahrscheinlich das Präsentationsumfeld betreibt.</sample>
    <sample id="498">Die Hauptergebnisse: EDAtt</sample>
    <sample id="499">Hauptschlüsse: EDAtt</sample>
    <sample id="500">Das Bild zeigt eine Präsentation über die Ergebnisse von EDAtt. Im oberen rechten Eck befindet sich ein kleiner Bildschirm mit einer Frau, die wahrscheinlich die Präsentation führt. Der Hauptinhalt der Präsentation ist ein Diagramm, das die BLEU-Score in Bezug auf den AL/AL_CA (s)-Ansatz zeigt. Die Legende auf dem Diagramm gibt es für verschiedene Methoden: wait-k, LA, CAAT und EDAtt. Links oben steht "Main Results: EDAtt", und oben rechts steht "popular strategies also applied to offline models". Im unteren rechten Eck des Diagramms steht "a) en→de" und im unteren rechten Eck des Bildes ist "page 033" angegeben.</sample>
    <sample id="501">Die Hauptergebnisse sind EDAtt.</sample>
    <sample id="502">Der englische Text lautet: 'Main Results: EDAtt' und 'EDAtt outperforms all the strategies applied to offline models'.</sample>
    <sample id="503">Die Hauptergebnisse sind im Bild dargestellt. Das Bild zeigt einen Graph, der die Leistung verschiedener Übersetzungstechniken bei einer Aufgabe anzeigt, die von Englisch ins Deutsche übersetzt wird (a en-de). Die Y-Achse zeigt die Leistung in Percenten, während die X-Achse die Zeit in Sekunden (AL/AL_CA) abdeckt. Drei verschiedene Techniken werden im Graph dargestellt: wait-k, LA und CAAT, wobei jede Technik durch eine andere Farbe gekennzeichnet ist. Darüber hinaus gibt ein Textfeld auf dem Bild zu verstehen, dass EDAtt die schnellste Strategie ist, wenn wir die tatsächliche verstrickte Zeit berücksichtigen.</sample>
    <sample id="504">Ja, der englische Text lautet: "Do you want to discover more? Read our paper to discover more results!"</sample>
    <sample id="505">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="506">MULTIINSTRUCT: Verbessern von multimodalem Zero-Shot-Learning durch Anweisungsadaption</sample>
    <sample id="507">Der englische Text lautet: "Pre-trained Language Models for Downstream Tasks" und "Figure 2: Comparing tuning with pretrain-finetune and prompting."</sample>
    <sample id="508">Das Bild zeigt eine Präsentation über "Prätrainierte Sprachmodelle für Niedereinsatzzwecke". Es gibt drei Hauptbereiche: (A) Pretrainieren-Finetune (BERT, T5), (B) Prompting (GPT-3) und (C) Tuning (FLAN). Jeder Bereich beschreibt eine Methode zur Verbesserung von Prätrainingsmodellen für bestimmte Aufgaben. Im unteren Teil des Bildes befindet sich ein Text, der die Abbildung beschreibt: "Vergleich der Tuningmethode mit Pretrainieren-Finetune und Prompting." Darunter ist ein Zitat von Wei, Jason, et al., das lautet: "Prätrainierte Sprachmodelle sind 'ohne Lernzüge' - Zero-Shot-Learners."</sample>
    <sample id="509">Der englische Text "Language-only" bedeutet auf Deutsch "Nur Sprache".</sample>
    <sample id="510">Das Bild zeigt eine Person mit einem schwarzen Halsband und einem schwarzen Jackett. Sie sitzt vor einem weißen Hintergrund, was einen kontrastreichen Effekt zu ihrem Outfit erzeugt. Das Bild scheint ein Portrait zu sein, da die Person zentral platziert ist und den Blick auf ihr Gesicht und ihre Kleidung lenkt.</sample>
    <sample id="511">Imbalancen in Anweisungsdatensätzen zwischen NLP und Multimodal</sample>
    <sample id="512">Imbalancen in Instructional Datasets between NLP and Multimodal 1600+ Language-only instruction tasks NO large-scale, publicly-available multimodal instruction tasks Wang, Yizhong, et al. "Benchmarking generalization via in-context instructions on 1600+ language tasks" arXiv preprint arXiv:2308.04579</sample>
    <sample id="513">MULTINSTRUCT: Der erste multimodale Anleitungskurzzusatz-Referenz-Index-Set</sample>
    <sample id="514">MULTINSTRUCT: Die erste multimodale Anweisungsverbrauchsskala zur Tuning</sample>
    <sample id="515">OFA (One For All) ist ein unified multimodales prätrainiertes Modell, das in der Lage ist, sowohl Verständnis als auch Generierung von Aufgaben mit einer oder mehreren Modalitäten auszuführen. Das Modell verfügt über ein unified Vocabulary für Sprache, Bildtoken und die Koordinaten eines umrahmenden Boxes.</sample>
    <sample id="516">Das Bild zeigt eine Übersetzung des englischen Inhalts auf Deutsch. Es handelt sich um ein Bild mit dem Titel "MULTINSTRUCT" und einem Text, der verschiedene Aufgaben beschreibt. Die Aufgaben sind "Gedrängte Captioning", "Text Lokalisierung", "Beziehungs-Expression-Speicherkapazität" und "Bild-Question-Matching". Jede Aufgabe wird mit einer Beschreibung und einem Beispiel-Image begleitet, das die Aufgabe illustriert. Am Ende des Bildes befindet sich der Text: "Beispielbeispiele aus MULTINSTRUCT für vier Aufgaben."</sample>
    <sample id="517">Das Bild zeigt eine Übersetzung des englischen Inhalts ins Deutsche. Es handelt sich um ein Screenshot einer Präsentation, die den Inhalt eines Bildes und seine Beschreibung in verschiedenen Aufgabenfeldern darstellt. Die Haupttexte auf dem Bild lauten: "MULTINSTRUCT" und "Figure 1: Example Instances from MULTINSTRUCT for Four Tasks." Diese Texte sind im Bild prominent dargestellt und zeigen die Zusammenhänge zwischen den verschiedenen Aufgabenfeldern.</sample>
    <sample id="518">Das Bild zeigt eine Übersetzung des englischen Inhalts ins Deutsche. Der englische Text lautet: "MULTINSTRUCT Figure 1: Example Instances from MULTINSTRUCT for Four Tasks." Die deutsche Übersetzung lautet: "MULTINSTRUCT Abbildung 1: Beispielbeispiele aus MULTINSTRUCT für vier Aufgaben."</sample>
    <sample id="519">Multi-modal Instruction Tuning</sample>
    <sample id="520">Multi-Modal Instruction Turning</sample>
    <sample id="521">Multi-Modal Instruction Turning Training Dataset Construction: Use 53 tasks from 9 groups for training. Sample 10,000 instances per task. Testing Dataset Construction: Reserve the entire Commonsense Reasoning group for testing. Select additional 5 tasks from VQA and Miscellaneous groups. We use all the instances in the test split for each task. Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="522">Implementation Details Training details: Pre-trained OFA-Large model (472M) Mix all the instances for all tasks. Each instance is randomly combined with one of its five instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="523">Implementations Details Training details: Pre-trained OFA-Large model (472M) Mix all the instances for all tasks. Each instance is randomly combined with one of its instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="524">Implementationsdetails: Trainingdetails: Pre-trained OFA-Large model (472M) Mix all the instances for all tasks. Each instance is randomly combined with one of its instruction templates. Testingdetails: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="525">Die englischen Wörter auf dem Bild sind auf Deutsch übersetzt. Das Bild zeigt eine Präsentation mit dem Titel "Evaluation Metrics" (Evaluierungsmetriken). Es gibt mehrere Punkte, die verschiedene Arten von Aufgaben und ihre zugehörigen Metriken beschreiben: - "multi-modal classification tasks" (multimodale Klassifizierungsaufgaben) - "multi-modal generation tasks" (multimodale Generierungsaufgaben) - "NLP tasks" (NLP-Aufgaben) Die Metriken, die für diese Aufgaben verwendet werden, sind: - Accuracy (Genauigkeit) - Rouge-L (eine Metrik für die Qualität der Textgenerierung) - Aggregated performance (aggregiertes Leistungsbild) Diese Metriken werden verwendet, um die Leistung von Modellen bei verschiedenen Aufgaben zu bewerten.</sample>
    <sample id="526">Der englische Inhalt lautet: "How sensitive the model is towards variety of instructions for the same task:"</sample>
    <sample id="527">Der englische Text lautet: "Effectiveness of Instruction Tuning on MULTIINSTRUCT" und "Table 1: Zero-shot Performance on Multimodal Compositional Reasoning. The best performance is in bold."</sample>
    <sample id="528">Das englische Text in der Abbildung lautet: "Effectiveness of Instruction Tuning on MULTIINSTRUCT" und "Table 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold." Diese Übersetzung gibt einen Eindruck von dem Inhalt der Abbildung, was auf eine Analyse oder Erforschung des Effekts von Richtungsanweisungen auf die Leistung bei der Verwendung von MULTIINSTRUCT hinweist.</sample>
    <sample id="529">Der Einfluss der Steigerung der Mehrmodularen Aufgaben-Cluster</sample>
    <sample id="530">OFA fine-tuned on 5 instructions erzielt eine viel höhere aggregierte Leistung auf allen Aufgaben und zeigt eine geringere Sensitivität.</sample>
    <sample id="531">Effekt der Feinabstimmungsstrategien auf die Sensitivität von Modellen.</sample>
    <sample id="532">Zero-Schussle-Performance auf NLP-Tasken</sample>
    <sample id="533">Zusammenfassung:</sample>
    <sample id="534">Ein weiteres Ding Wir sammeln eine viel größere multimodale Datenbank für die Anpassung von Anweisungen. Es werden etwa 150 weitere visuelle Sprachaufgaben hinzugefügt, und wir freuen uns darauf, sie bald zu veröffentlichen</sample>
    <sample id="535">Università di Trento</sample>
    <sample id="536">Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis</sample>
    <sample id="562">Sprachmodellungsentscheidungen sind nicht immer robust gegenüber Kontextveränderungen.</sample>
    <sample id="563">Sprachmodell-Präferenzurteile sind nicht immer robust gegenüber Kontext</sample>
    <sample id="564">Minimal Paar Paradigm (MPP) Bewertungen von Modellen der Sprachproduktion verwenden relativen Unterschiede in Folgeverwahrscheinlichkeiten, um die Kenntnisse von LMs zu bewerten.</sample>
    <sample id="565">Minimal Pair Paradigm (MPP) Evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs:</sample>
    <sample id="566">Minimal Pair paradigm evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLiMP SyntaxGym CrowS 1. Many people were helping 1. No customer ... has spent 1. Stereotypical 2. Many people were 2. ... has spent any money. 3. Many people were 4. Stereotypical 5. Many people were herself. any money. sentence. herself. P(1) ≫ P(2) P(1).any ≫ P(2.any) P(1) &gt; P(2)</sample>
    <sample id="567">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLiMP SyntaxGym Crows 1. Many people were helping 1. No customer ... has spent Any people. 2. Many people were 2. ... has spent any money. 3. Many people were helping herself. 4. Many people were helping herself. 1. Stereotypical 2. Stereotypical 3. Stereotypical 4. Stereotypical 1. Many people were helping 2. Many people were helping herself. 1. No customer ... has spent any money. 2. No customer ... has spent any money. 3. Many people were helping herself. 4. Many people were helping herself. P(1) ≫ P(2) P(1) ≫ P(2.any) P(1) ≫ P(2)</sample>
    <sample id="568">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to estimate the abstract knowledge of LMs: BLIMP SyntaxGym CrowS 1. Many people were helping others. No customer ... has spent any money. Women are terrible at handiwork. 2. "Many people were helping herself." P(1) ≫ P(2) P(1.any) ≫ P(2.any) P(1) ≫ P(2) Are these judgements stable with long preceding context?</sample>
    <sample id="569">Revisiting Minimal Pair Paradigm</sample>
    <sample id="570">Revisiting Minimal Pair Paradigm</sample>
    <sample id="571">Der englische Inhalt lautet: "Test whether MPP judgements as a function of context length, structural match, and acceptability."</sample>
    <sample id="572">Der englische Inhalt lautet: "Test whether MPP judgements as a function of context length, structural match, and acceptability".</sample>
    <sample id="573">Das Bild zeigt eine Präsentation mit dem Titel "Approach" in Englisch. Der Haupttext lautet: "Test whether MPP judgements as a function of context length, structural match, and acceptability." Dies bedeutet, dass die Präsentation sich auf die Untersuchung von Entscheidungen basierend auf Kontextlänge, struktureller Übereinstimmung und akzeptablen Werten konzentriert.

Im Bild sind auch einige Diagramme und Texte zu sehen, die Informationen über den "Space of Candidate Prefixes" und verschiedene Entfernungen zwischen Prefixen bereitstellen. Es gibt auch eine Referenz auf "GPT2, OPT family - 125M to 6.7B", was wahrscheinlich auf die Größe der Modellparameter hinweist.

Am unteren Rand des Bildes ist das Wort "BLIMP" zu sehen, und es gibt einen Hinweis auf "Blimp Island". Im oberen rechten Eck ist ein kleiner Bildausschnitt eines Menschen zu erkennen, der möglicherweise der Präsentator ist.

Zusammenfassend handelt es sich um eine wissenschaftliche Präsentation, die sich auf die Analyse von Entscheidungen in einem bestimmten Kontext konzentriert, und es enthält visuelle Informationen und Referenzen auf spezifische Modelle und Daten.</sample>
    <sample id="574">Der englische Inhalt lautet: "Test whether MPP judgements as a function of context length, structural match, and acceptability."</sample>
    <sample id="575">Der englische Text lautet: "Test whether MPP judgements as a function of context length, structural match, and acceptability."</sample>
    <sample id="576">Das Bild zeigt eine Präsentation über eine Methode zur Bewertung von Entscheidungen in einem bestimmten Kontext. Der Titel "Approach" ist am oberen linken Eck sichtbar. Die Hauptaufgabe der Präsentation besteht darin, zu testen, ob die Bewertungen von MPP (Mehrplattform-Produktions-Plattformen) als Funktion von Kontextlänge, struktureller Übereinstimmung und akzeptabilität abhängen. Im Zentrum des Bildes befindet sich ein Diagramm, das verschiedene Aspekte und Beispiele der Bewertung anzeigt. Links am Bildrand sind zwei Diagramme zu sehen, die wahrscheinlich den Prozess oder die Ergebnisse der Bewertung darstellen. Am unteren Rand des Bildes ist ein Text zu lesen, der "GPT2, OPT family - 125M to 6.7B" und "BLUMP, ESSential 'E' Theworries" enthält. Rechts am Bildrand ist ein kleiner Bildschirm eines Menschen zu sehen, der möglicherweise der Präsentator ist.</sample>
    <sample id="577">Das Bild zeigt eine Präsentation mit dem Titel "Approach". Es handelt sich um eine Analyse, die den Vergleich von MPP-Jugendlichkeiten in Bezug auf Kontextlänge, Struktur und Akzeptabilität untersucht. Die Präsentation präsentiert ein Diagramm, das zwei Arten von Präfixen unterscheidet: "accusable" und "unacceptable". Darüber hinaus sind zwei Beispiele vorgestellt, die zeigen, wie verschiedene Szenarien überprüft werden können, um die Vertrauenswürdigkeit zu bestimmen. Das Text in der Abbildung ist englisch, und es gibt keine sichtbaren Informationen, die auf eine andere Sprache hinweisen.</sample>
    <sample id="578">Der englische Text lautet: "Approach Test whether MPP judgements as a function of context length, structural match, and acceptability."</sample>
    <sample id="579">Der englische Text lautet: "Approach Test whether MPP judgements as a function of context length, structural match, and acceptability."</sample>
    <sample id="580">Der englische Text lautet: "Test whether MPP judgements as a function of context length, structural match, and acceptability."</sample>
    <sample id="581">MPP Bewertungen sind robust für beliebige Kontextlängen.</sample>
    <sample id="582">MPP Urteile sind für beliebige Kontextlängen robust.</sample>
    <sample id="583">Der englische Text lautet: "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance" und "We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens".</sample>
    <sample id="584">Inhalte in Englisch: "We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens"</sample>
    <sample id="585">Inhalt: "Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt: Inhalt:</sample>
    <sample id="586">Der englische Text lautet: "Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance. We perform MPP evaluations with different contexts – acceptable/unacceptable; mismatched/matched structure – of lengths up to 900 tokens."</sample>
    <sample id="587">Der englische Text lautet: "We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens."</sample>
    <sample id="588">Inhalt auf Deutsch: "Akzeptable/unakzeptable MPP-Sätze mit passendem Struktur haben den stärksten Einfluss auf die Modellleistung."</sample>
    <sample id="589">Warum beeinflussen gemischte Präfixe die Bewertungen von LM?</sample>
    <sample id="590">Das Bild zeigt eine Präsentation, die den Einfluss von passenden Präfixen auf die LM (Language Model) Bewertungen untersucht. Der Titel der Präsentation lautet: "Warum beeinflussen passende Präfixe die Bewertungen von LM?" Die Präsentation fasst ein, wie Kontextsätze mit der erhaltenen Struktur erhalten werden und ob Modelle auf diese Sätze reagieren. Sie bezieht sich auf verschiedene Präfix- und Suffix-Adverbien wie "However, &lt;sent&gt;" und "First and foremost, &lt;sent&gt;". Darüber hinaus wird eine Add-Klause betrachtet, die "Regardless of what X denkt darüber, &lt;sent&gt;" beschreibt, sowie einen Quote, der lautet: "Gestern sagte X, &lt;sent&gt;". Im unteren Teil des Bildes ist ein Graph zu sehen, der die "Delta Accuracy" gegen den "Input Length" zeigt. Der Graph zeigt verschiedene Perturbation-Typen und Prefix-Typen an, was auf die Abhängigkeit von den Modellen von den Passierungen hinweist.</sample>
    <sample id="591">Warum beeinflussen Übereinstimmung von Präfixen die Bewertungen von LM?</sample>
    <sample id="592">Warum beeinflussen passende Präfixe die Bewertungen von LM?</sample>
    <sample id="593">Warum beeinflussen Übereinstimmungspräfixe die LM-Bewertungen?</sample>
    <sample id="594">Sprichmodelle sind auf verborgene syntaktische/semantische Merkmale auf der Ebene von Sätzen neugierig. MPP-Evaluierungen mit kurzen, einzelnen Satz-Input-Nachläufen fangen nicht die abstrakte Wissenstruktur der Sprachmodelle (LMs) aus.</sample>
    <sample id="595">Sprache: Deutsch</sample>
    <sample id="596">Sprachmodellierung ist auf verborgene syntaktische oder semantische Merkmale in Satzen aufgerichtet. MPP-Evaluierungen mit kurzen, einzelnen Satz-akkusativ-Eingaben fangen nicht die abstrakte Merkmale von LM's (Language Models) ein.</sample>
    <sample id="597">Word tokens</sample>
    <sample id="598">55,000</sample>
    <sample id="626">Der beste Ausrichtungsmethode für DEplain ist Sent-LA-LEASE.</sample>
    <sample id="627">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="628">Die Dokumente wurden mit einer Kombination aus manuellen und automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="629">Der CoNLL++-Datensatz wurde erstellt, indem Reuters-Newsartikel aus dem Jahr 2020 sammelt und mit den Annotierungsleitlinien von CoNLL-2003 angeteilt wurden.</sample>
    <sample id="630">XSemPLR: Mehrsprachiges Semantikparseten in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang PennState amazon</sample>
    <sample id="631">Der englische Text lautet: "Semantic Parsing is a task to build semantic representation of the user queries, such as SQL, and Lambda Calculus."</sample>
    <sample id="632">Cross-lingual Semantic Parsing</sample>
    <sample id="633">Cross-lingual Semantic Parsing</sample>
    <sample id="634">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="635">Das Bild zeigt eine Präsentation über "Übersetzungssprachliche Semantikparsen" (Cross-lingual Semantic Parsing). Im oberen linken Bereich befindet sich der Titel in bold, schwarzen Lettern. Der Haupttext auf der Seite lautet: "Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language". Diese Texte sind in schwarzer Schrift und werden mit einem Punkt vorgestellt.

Unterhalb des Textes sind vier blaue Rechtecke, die jeweils mit einer Sprache bezeichnet sind: Englisch (English), Deutsch (German) und Chinesisch (Chinese). Eine Pfeil zeigt an, dass diese Sprachen zu "Neural Models" (Neural Models) führen. Rechts davon sind vier weitere blaue Rechtecke mit den Worten SQL, Lambda und FunQL.

Im oberen rechten Eck ist ein kleiner Bildschirm mit einem Mann zu sehen, der möglicherweise das Vortrag gibt. Im unteren rechten Eck ist die Zahl 4 zu sehen, was darauf hindeutet, dass dies die Slide 4 der Präsentation ist.</sample>
    <sample id="636">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representation English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="637">Cross-lingual Semantic Parsing

- Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance:
  - Lack of coverage on certain meaning representation

English
German
Chinese

Neural Models

SQL
Lambda
FunQL</sample>
    <sample id="638">Cross-lingual Semantic Parsing</sample>
    <sample id="639">XSemPLR ist ein umfangreiches Dataset für die Semantische Parseierung in mehreren natürlichen Sprachen. Es umfasst 9 Datensätze in verschiedenen Bereichen, 5 Semantischer Parseingsysteme, 8 Bedeutungsrepräsentationen und natürliche Sprachen aus 15 Sprachfamilien.</sample>
    <sample id="640">XSemPLR ist ein unifiedes Dataset für die Semantische Parsing auf mehreren Sprachen. Es enthält 9 Datensätze in verschiedenen Bereichen, 5 Semantik-Parsings, 8 Bedeutungsrepräsentationen und 22 Sprachen in 15 Sprachfamilien.</sample>
    <sample id="641">Experiment Settings: Wir betrachten die sechs Einstellungen für die Ausbildung und das Evaluation. Translate-Test: Verwenden Sie den Google Translate API, um den Text in die Zielsprache zu übersetzen. Danach trainieren und evaluieren Sie mit einem monolingualen Modell. Training: English -&amp;gt; English Model -&amp;gt; SQL Inference: German -&amp;gt; Translate API -&amp;gt; English -&amp;gt; English Model -&amp;gt; SQL</sample>
    <sample id="642">Experiment Settings wirkt auf Englisch. Es gibt sechs Einstellungen für die Ausbildung und Bewertung. Translate-Test verwendet den Google Translate API, um den Quelltext ins Zielsprachen zu übersetzen. Danach trainiert und bewertet man mit einer monolingualen Modell.</sample>
    <sample id="643">Das Bild zeigt eine Präsentationsslides über "Experiment Settings". Es gibt zwei Hauptabschnitte: "Training" und "Inference". Im Training-Abteil wird beschrieben, wie der Prozess durchgeführt wird, indem man einen englischen Text mit einem Google Translate API übersetzt und dann einen monolingualen Modell trainiert und evaluierend. Im Inference-Abteil wird beschrieben, dass der gleiche Prozess für den Deutschen durchgeführt wird. Das Slide enthält auch eine Diagramm, das die Verbindungen zwischen den verschiedenen Schritten darstellt.</sample>
    <sample id="644">Das Experiment wird mit sechs verschiedenen Einstellungen für die Ausbildung und Bewertung durchgeführt. Diese Einstellungen umfassen eine monolingual Modell-Einstellung, bei der die Quellsprache und die Zielsprache identisch sind, wie zum Beispiel Deutsch-Deutsch. Sie werden auch eine Monolingual-Few-shot-Einstellung mit nur 10% Trainingsdaten für die Ausbildung von monolingual Modellen testen.</sample>
    <sample id="645">Das Experiment ist mit sechs verschiedenen Einstellungen für die Ausbildung und Bewertung konzipiert. Diese Einstellungen umfassen sowohl monolingual als auch bilingual modellierungen. Im speziellen wird das monolingual Modell verwendet, bei dem die Quellsprache gleich der Zielsprache ist, wie zum Beispiel Deutsch auf Deutsch. Darüber hinaus werden monolingual Few-shot-Einstellungen getestet, bei denen monolingual Modelle mit nur 10% der gewöhnlichen Trainingsdaten betrainingen werden.</sample>
    <sample id="646">Experiment Settings

Wir betrachten die sechs Einstellungen für die Ausbildung und Bewertung.

Monolingual-Modell: Die Quellsprache ist dieselbe wie der Zieltext, zum Beispiel Deutsch zu Deutsch. Wir testen auch das Monolingual-Few-shot Setting, indem wir monolingual-Modellen mit nur 10% Trainingsdaten ausbilden.</sample>
    <sample id="647">Das Experiment-Setting beschreibt, dass sechs verschiedene Einstellungen für die Ausbildung und Bewertung verwendet werden. Diese Einstellungen umfassen eine multilingualen Modellausbildung, bei der ein einziges Multilingual-Modell für alle Sprachen trainiert wird. Die Abbildung zeigt den Prozess der Ausbildung und des Inferenzverfahrens in der multilingualen Modellausbildung.</sample>
    <sample id="648">Experiment Settings wirkt auf Englisch. Die Übersetzung des englischen Inhalts ins Deutsche lautet: "Wir betrachten die sechs Einstellungen für die Ausbildung und die Bewertung. Multilingual Model: Trainiere ein multilinguales Modell für alle Sprachen."</sample>
    <sample id="649">Experiment Settings wirkt auf Englisch. Das bedeutet, dass das Experiment mit verschiedenen Sprachen gearbeitet hat.</sample>
    <sample id="650">Experiment Settings</sample>
    <sample id="651">Das Bild zeigt eine Präsentation über "Experiment Settings" (Versuchs-Einstellungen). Es gibt fünf Schritte, die das Experiment durchführen sollen: 1. Training 2. Inference 3. Cross-lingual Zero-shot/Few-shot Transfer 4. Multilingual Model 5. SQL Diese Schritte sind in einem Diagramm dargestellt, das zwei Arten von Training und Inference zeigt, wobei Englisch und Deutsch als Sprachen verwendet werden. Das Diagramm zeigt auch, wie der Multilingual Model auf die SQL-Datenbank übertragen wird. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen, der wahrscheinlich über den Inhalt sprechen soll.</sample>
    <sample id="652">Analysis of Monolingual</sample>
    <sample id="653">Analysis of Monolingual</sample>
    <sample id="654">Analysis of Monolingual</sample>
    <sample id="655">Analysis of Monolingual</sample>
    <sample id="656">Analysis of Multilingual Training

Wir evaluieren mT5 und XLM-R + PTR auf Multilingüer Setting

Enc-Dep/Enc-PTR (mT5-XLM-R) kann durch Training in einer Mischung verschiedener Sprachen verbessert werden.</sample>
    <sample id="657">Analysis of Multilingual Training</sample>
    <sample id="658">Analysis of Multilingual Training</sample>
    <sample id="659">Analysis von Multilingual Training</sample>
    <sample id="660">Cross-lingual Performance Gap Blue Linie: Cross-lingual Few-shot transfer Orange Linie: Cross-lingual Zero-shot transfer Grüne Linie: Monolingual Setting</sample>
    <sample id="661">Cross-lingual Performance Gap Blue Line: Cross-lingual Few-shot transfer Orange Line: Cross-lingual Zero-shot transfer Green Line: Monolingual Setting</sample>
    <sample id="662">Cross-lingual Performance Gap green - orange For zero-shot setting, the cross-lingual transfer performance gap is significant blue - orange For few-shot setting, the transfer gap is shortened rapidly</sample>
    <sample id="663">Der englische Inhalt lautet: "Other Results &amp; Findings (Section 4 in Paper)"</sample>
    <sample id="664">1. Enc-Dec (mT5) übertrifft die vorherige Arbeit oder erreicht ähnliche Ergebnisse.
2. Das Pretraining auf dem NL auf dem Codex kann den Leistungsfaktor des "few-shot" auf Ziellanguage-NLs erheblich steigern.
3. Multilinguelle LLMs (Codex und Bloom) sind noch nicht ausreichend für die Übersetzung von Semantiktasken zwischen Sprachen.
4. Chinesisch-Übersetzung-Transfer und monolingual Englisch-Training (En-&gt;En) haben den größten Leistungsschaden, während "German" typischerweise den kleinsten Schaden hat.
5. FunQL übertrifft die anderen drei Bedeutungsrepräsentationen, und SQL erhält den schlechtesten Leistungsergebnis.</sample>
    <sample id="665">Das Bild zeigt eine Präsentationsslideset mit dem Titel "Conclusion". Es gibt drei Hauptpunkte auf der Slide: 1. Sie haben XSemPLR entwickelt, ein umfassender Leistungsindex für multilingual semantische Parsing mit mehreren natürlichen Sprachen und Bedeutungsbearbeitungen. 2. Sie haben eine umfassende Referenzstudie durchgeführt, die auf drei repräsentativen Sprachmodellen ausgerichtet ist. 3. Ihre Ergebnisse zeigen, dass mT5 mit monolingual Training das beste Ergebnis erbringt, insbesondere bei der Leistung von Multilingüalen LLMs bei der Durchführung von multilingual semantischen Parsing-Aufgaben. Darüber hinaus ist der Leistungsabstand zwischen monolingual Training und cross-lingual Transfer lernen immer noch bedeutend. Im oberen rechten Eck des Slides befindet sich ein kleiner Bildschirmbild eines Menschen, und unten rechts ist die Nummer 18 zu sehen.</sample>
    <sample id="666">Das Bild zeigt eine Präsentationsslideshow mit zwei Seiten. Die erste Seite ist unter dem Titel 'Conclusion' und enthält Informationen über den 'XSemPLR', ein umfassender Leistungsstandard für die multilingualische Semantikparsierung. Es wird auch auf einen 'comprehensive benchmark study' von drei repräsentativen Sprachmodellen und die Leistung von 'mT5 mit monolingual training' hingewiesen, insbesondere bei der Ausführung von multilingualen Semantikparsings. Darüber hinaus wird die 'performance gap' zwischen 'monolingual training' und 'cross-lingual transfer learning' als 'significant' erwähnt.

Die zweite Seite ist unter dem Titel 'Links' und bietet eine Einladung, das Papier und das Code zu besuchen. Es gibt zwei Links: einen 'Paper Link' auf arxiv.org und einen 'Code Link' auf github.com. Der Text auf der Seite lautet: 'Welcome to visit our paper and code!'</sample>
    <sample id="667">Transferability, Lexical watermark, Backdoor-based watermark, Adversarial-property.</sample>
    <sample id="668">Nein, Multilingual LLMs wie Codex und Bloom sind für die CLSP nicht ausreichend.</sample>
    <sample id="695">Die Methode mit der Mehrdeutigkeit der Permutationen umfasst die folgenden Schritte: 1. Permutierung von Wörtern, wie 'girl' und 'sleep', um mehrere Permutationen zu generieren. 2. Induktion des Permutationmodells während der Trainingphase. 3. Angabe der NP-Hard-Inferenz (TSP) als Herausforderung. 4. Verwendung eines Backpropagation-Verfahren durch kontinuierliche Relaxation. Diese Schritte sind für das Löschen der Mehrdeutigkeit in der Permutation von Wörtern von entscheidender Bedeutung.</sample>
    <sample id="696">Der Fairness eines nachgeschalteten NLP-Modells wird definiert, indem man die Auswirkungen des Modells auf verschiedene Gruppen untersucht.</sample>
    <sample id="697">Adrien Bazege</sample>
    <sample id="698">Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Mira, Keren Fuentes, Roger Levy, Adi Williams</sample>
    <sample id="699">Myra Cheng, Esin Durmus, Dan Jurafsky</sample>
    <sample id="700">Tropikalismus ist ein Kunstbewegung, der in den 1920er Jahren in Brasilien entstanden ist und sich durch die Verwendung tropischer Themen und -elemente auszeichnet. Es wurde von Künstlern wie Tarsila do Amaral und Candido Portinari geprägt. Der Begriff wurde 1928 vom Künstler Gustavo Foppiani für diese Bewegung verwendet. Tropikalismus stellte sich der modernen Abstraktion gegenüber und wollte eine einzigartige identifizierende Art von Kunst schaffen, die die kulturellen und natürlichen Elemente des Tropenslands repräsentiert.</sample>
    <sample id="701">Die Autoren haben die Beschreibungen der Zielgruppen von Menschen erstellt, indem sie Wörter wie "culture", "tradition", "proud" und "exotic" für markierte Gruppen verwendet haben. Diese Wörter deuten auf eine Eindringung der Identität dieser Gruppen hin, indem sie ihre Merkmale als einzigartig oder ausländisch betrachten.</sample>
    <sample id="702">Pointwise (P-)CXMI</sample>
    <sample id="703">DrBERT und ChuBERT sind beide BERT-ähnliche Modellvarianten, aber sie unterscheiden sich in ihren primären Datenquellen. DrBERT wird auf einem privaten Datensatz namens NBDW trainiert, der aus 1,7 Millionen anonymisierten Medizinnotizen stammt, während ChuBERT auf einer größeren Sammlung von öffentlichen Quellen wie PubMed trainiert wurde.</sample>
    <sample id="751">Es sind drei Autoren an der Arbeit beteiligt.</sample>
    <sample id="752">Iteratives Transferlernen ist ein lernährender Prozess, bei dem ein Modell auf neuem Datenmaterial trainiert wird und dann auf das ältere Datenmaterial zurückgreift.</sample>
    <sample id="753">Das Ziel des Datensatzes ist es, die Sprache von Benutzern zu verstehen, wenn sie eine Wahl treffen.</sample>
    <sample id="754">Ein Angreifer kann Modellparameter über einen EaaS (Elastic As-a-Service) extrahieren, indem er Zugriff auf die verwalteten Infrastruktur oder Dienste erhält, um die erforderlichen Ressourcen für den Zugriff auf das Modell zu erhalten.</sample>
    <sample id="755">Es sind drei Autoren an der Arbeit beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="756">Zwei Annotatoren wurden verwendet, um den ursprünglichen Datensatz zu erstellen.</sample>
    <sample id="757">Die Autoren gehören der Carnegie Mellon University und dem Allen Institute for AI.</sample>
    <sample id="758">Der Beispielsatz lautet: "I saw Bart and Lisa; Homer came and sneezed."</sample>
    <sample id="759">The technology for dialogue systems is advanced.</sample>
    <sample id="760">Um die Akzeptanz der Modelle über das gesamte Kontextfenster zu bewerten.</sample>
    <sample id="761">Ja, das mehrsprachige Training hat zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt.</sample>
    <sample id="762">Ja, die Annotatoren kennen die Entität im Voraus.</sample>
    <sample id="763">F1-Score, Precision und Recall</sample>
    <sample id="764">Ja, die Regression hat auf bestimmte NER-Typen ausgewirkt.</sample>
    <sample id="765">Positionalität für NLP ist wichtig, weil sie die Bedeutung von Wortordnung und -position im Text unterstreicht.</sample>
    <sample id="766">Mehrsprachige LLMs wie BLOOM wurden durch Adapter angepasst.</sample>
    <sample id="767">ROBERTA-base + classifier head</sample>
    <sample id="768">The following test sets were used to evaluate the capabilities of PaLM: C410, AGNews, and WMT22.</sample>
    <sample id="769">The authors have made three recommendations.</sample>
    <sample id="770">Der Gewinn der vorgeschlagenen Methode gegenüber der stärksten Baseline beträgt 25%.</sample>
    <sample id="771">Shuheng Liu, Alan Ritter</sample>
    <sample id="772">Ja, die Ergebnisse und der Datensatz der Studie können als Benchmark verwendet werden.</sample>
    <sample id="773">Fünf.</sample>
    <sample id="774">OFA (One For All)</sample>
    <sample id="833">The authors belong to Google.</sample>
    <sample id="834">Stony Brook University</sample>
    <sample id="835">Deutsch-Englisch, Englisch-Spanisch und Englisch-Französisch.</sample>
    <sample id="836">Chan Young Park</sample>
    <sample id="837">DEPLAIN-APA test, DEPLAIN-WEB test</sample>
    <sample id="838">53</sample>
    <sample id="839">Es sind drei Autoren an der Arbeit beteiligt: Regina Stodden, Omar Momen und Laura Kallmeyer.</sample>
    <sample id="840">AG News, MIND, SST2, Enron Spam</sample>
    <sample id="876">NACHOS ist ein Typ von Chips, die mit verschiedenen Zutaten wie Käse, Soße und anderen Gerichten bestückt werden.</sample>
    <sample id="877">David Vilar Torres, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, George Foster</sample>
    <sample id="878">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse.</sample>
    <sample id="879">Die Autoren gehören der Carnegie Mellon University Language Technologies Institute.</sample>
    <sample id="880">1. Be yourself 2. Have fun 3. Be kind to each other 4. Respect the rules and guidelines of our community 5. Stay safe</sample>
    <sample id="881">Die Autoren schlagen vor, Modelle zur Nutzung von Informationen aus mehreren Quellen zu testen, indem sie einen Dataset für die Bewertung der Wissensintegration verwenden und eine Coreference Resolution-Task erstellen, um die Fähigkeit zu prüfen, auf vorherige und während des Inferenzzeitframes zu verweisen.</sample>
    <sample id="882">Das Bild zeigt eine Präsentation mit dem Titel "Prompting PaLM for Translation: Assessing Strategies and Performance". Der Google-Logo ist im oberen linken Eck sichtbar. Im oberen rechten Eck befindet sich ein kleiner Bildschirm mit einem Strandbild und einem Text, der lautet: "Kannst du diesen übersetzen für mich, bitte?" Diese Übersetzung bedeutet "Can you translate this for me, please?". Darunter ist ein Lächelndes Emoji zu sehen. Im unteren Abschnitt sind Fotos von fünf Personen mit ihren Namen darunter aufgeführt: David Vilar Torres, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar und George Foster. Im unteren linken Eck ist "ACL 2023" geschrieben, was wahrscheinlich auf den Veranstaltungsort oder -jahr hinweist.</sample>
    <sample id="883">Der englische Text lautet: 'PaLM: Pathways Language Model' Chowdery et al., 2022 arXiv:2204.02311. 540B parameters. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips. SOTA in hundreds of LMU and Generation benchmarks.'</sample>
    <sample id="884">PaLM: Wegweiser Sprachmodell Chowdery et al., 2022 arXiv:2204.02311. 540B Parameter.</sample>
    <sample id="885">Das Bild zeigt eine Präsentation mit dem Titel "Our contribution". Es gibt drei Hauptpunkte: 1. Ein erster systematischer Studienbetrieb über LLM-Angebote für MT (Mitarbeiterkommunikation). 2. Eine Bewertung der Übersetzungsfähigkeiten gemäß den besten Praktiken der MT-Gemeinschaft, einschließlich der Vergleich von aktuellen WMT-Unterlagen und der Bewertung von Experten. 3. Empfehlungen für die Auswahl von Prompts. Im unteren rechten Eck des Bildes ist ein kleiner Bildausschnitt eines Menschen zu sehen.</sample>
    <sample id="886">Das Bild zeigt eine Präsentation mit dem Titel "Our contribution". Es gibt drei Hauptpunkte: 1. Ein erster systematischer Studienbetrieb über LLMs für MT (Mitarbeitungs-Übersetzung) sowohl für die Kandidatenpool als auch die Auswahlstrategie. 2. Eine Bewertung der Übersetzungsfähigkeiten aufgrund der besten Praktiken der MT-Community, einschließlich der neuesten Testsets, der Vergleich zu den kürzlichsten WMT-Untermittlungen, der SOTA-METIKEN und des Expertenhuman-Evaluations. 3. Empfehlungen für die Auswahlstrategien für die Promptauswahl. Im unteren rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich das Vortrag verpasst hat.</sample>
    <sample id="887">Das Bild zeigt eine Präsentation mit dem Titel "Our contribution". Es gibt drei Hauptpunkte: 1. Ein erster systematischer Studienansatz zur Prompting-Generierung für MT (Mitarbeitermaschinenübersetzung). 2. Eine Bewertung der Übersetzungsleistungsfähigkeiten aufgrund der besten Praktiken der MT-Community, einschließlich der Vergleich von aktuellen WMT-Untermittlern und der Bewertung von SOTA-Metriken. 3. Empfehlungen für die Auswahl von Prompts. Im unteren rechten Ecke des Bildes ist ein kleiner Bildausschnitt eines Menschen zu sehen.</sample>
    <sample id="888">Das Bild zeigt eine Präsentation über die Beiträge eines Forschungsprojekts. Der Titel der Präsentation lautet "Our contribution". Es gibt drei Hauptpunkte: 1. Ein erster systematischer Studienansatz für LLM-Präferenzen für MT (Mitarbeitertätigkeit). 2. Eine Bewertung der Übersetzungspräferenzen auf der Basis der besten Praktiken der MT-Gemeinschaft, einschließlich der Vergleich von aktuellen WMT-Eingaben, der Korrelation von SOTA-Metriken und der Expertenbewertung. 3. Empfehlungen für die Auswahl von Präferenzstrategien. Im unteren linken Eck des Bildes ist das Google-Logo zu sehen.</sample>
    <sample id="889">Prompts haben einen großen Einfluss auf die Übersetzungsfunktion. Zwei zufällige Prompts für jede Satzwahl wählen. Für jeden Prompt-Paar berechnen Sie BLEURT. Mehrheit der Sätze (516 von 1000) zeigen eine Differenz von mehr als 1 BLEURT-Punkt. Die Differenz kann bis zu 40 BLEURT-Punkten beträchtlich sein</sample>
    <sample id="890">Prompts haben einen großen Einfluss auf die Übersetzungsfunktion.</sample>
    <sample id="891">Prompts haben einen großen Einfluss auf die Übersetzungsfähigkeit. Zwei zufällige Prompts pro Satz wählen, BLEURT für jede Prompt-Satze berechnen und die meisten Sätze (516 von 1000) zeigen eine Differenz von mehr als 1 BLEURT-Punkt. Die Differenz kann bis zu 40 BLEURT-Punkten erreichen</sample>
    <sample id="892">Der Text auf dem Bild lautet: "Example prompting for translation" und "5-shot prompting". Darunter sind zwei Paar englischer und deutscher Phrasen aufgeführt, die auf eine Übersetzung hinweisen. Diese Paare zeigen eine Situation, bei der Polizeibeamte einen Gefangenen transportieren und eine Beschwerde von einer Bürolage erhalten haben. Die englischen Phrasen sind "He is being transported under the custody of two policemen on a bus from the jail." und "Police were called in after receiving complaints from the office." Die entsprechenden deutschen Phrasen sind "Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird." und "Ein Passant alarmierte die Polizei, nachdem sie mehreren Streifen anruckte." Am unteren linken Eck des Bildes ist das Google Logo zu sehen.</sample>
    <sample id="893">Das Bild zeigt eine Präsentation über "Beispiel prompting für Übersetzung". Es gibt eine Liste von Szenarien, die auf Englisch und Deutsch dargestellt sind. Die Szenarien sind: 1. Ein Mann wird von zwei Polizisten in einem Streifenwagen transportiert. 2. Ski-Legenden unter sich; die Polizei war eingedrungen, nachdem sie Beschwerden des Büros erhalten hatten. 3. Polizei wurden auf Anzeige von Beschwerden des Büros gerufen. 4. Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte. Im unteren rechten Eck des Bildes ist ein Mann zu sehen, der wahrscheinlich die Präsentation gibt. Der Google-Doodle ist im unteren linken Eck sichtbar.</sample>
    <sample id="894">Das Bild zeigt eine Präsentation mit dem Titel "Example prompting for translation". Es gibt einen Textabschnitt, der "5-shot prompting" heißt. Darunter sind zwei englische Sätze zu sehen: 1. "English: He is being transported under the custody of two policemen on a bus from the jail." 2. "English: Police were called in after receiving complaints from the office." Neben diesen englischen Sätzen sind die entsprechenden deutschen Übersetzungen aufgeführt: 1. "German: Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird." 2. "German: Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten." Im unteren rechten Eck des Bildes ist ein kleiner Bildausschnitt eines Mannes zu sehen, der wahrscheinlich den Vortrag gibt. Im unteren linken Eck befindet sich das Google Logo.</sample>
    <sample id="895">Das Bild zeigt eine Präsentation über Übersetzungshilfe. Der Titel lautet "Beispiel für Übersetzungsanleitung". Es gibt eine Übersetzungshilfe, die "5-Shot Prompting" heißt und zwei englische Sätze aufgeführt hat: "Ski-Legenden unter sich: Die Polizei war eingedrungen, nachdem sie Beschwerden des Büros erhalten hatten." und "English: Police were called in after receiving complaints from the office." Darüber hinaus gibt es eine deutsche Übersetzung: "Ein Passant alarmierte die Polizei, mit mehreren Streifen ankündigte." Im unteren linken Eck ist das Google Logo zu sehen. Im unteren rechten Eck befindet sich ein kleiner Bildausschnitt eines Mannes mit kurzen Haaren und einem weißen Hemd.</sample>
    <sample id="896">Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird.</sample>
    <sample id="897">Beispielergebnisse</sample>
    <sample id="898">Experimental Results: - Beispielqualität ist wichtiger als Ähnlichkeit zu der Quellsprache. - Spezialisierte SOTA-Systeme haben einen bedeutenden Vorteil. - PaLM ist nahe bei Google Translate. Einzugsweise von MQM: - Flüssigkeit von PaLM kann mit SOTA verglichen werden. - Genauheitswerte sind im Allgemeinen niedriger. Dominiert von "Accuracy/Omission". - "Stil/Absurdität" ist im Allgemeinen niedriger für PaLM.</sample>
    <sample id="899">Experimental Results: - Beispielqualität ist wichtiger als Ähnlichkeit zu der Quellsprache. - Spezialisierte SOTA-Systeme haben einen bedeutenden Vorteil. - PaLM ist nahe bei Google Translate. Einzugsweise von MQM: - Fluide PaLM-Komparabelkeit mit SOTA. - Genauheitswerte sind im Allgemeinen niedriger. * Dominiert durch "Accuracy/Omission" * "Stil/Absurdität" sind im Allgemeinen niedriger für PaLM.</sample>
    <sample id="900">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by “Accuracy/Omission” “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="901">Beispiel: "Experimental Results" bedeutet "Experimentelle Ergebnisse".</sample>
    <sample id="902">Beispielergebnisse sind wichtiger als Ähnlichkeit zu der Quellsprache. Spezialisierte SOTA-Systeme haben einen bedeutenden Vorteil. PaLM ist fast wie Google Translate.</sample>
    <sample id="903">Beispiel: "Experimental Results" bedeutet "Experimentelle Ergebnisse".</sample>
    <sample id="904">Beispielergebnisse sind wichtiger als Ähnlichkeit zu der Quellsprache. Spezialisierte SOTA-Systeme haben einen bedeutenden Vorteil. PaLM ist fast wie Google Translate. Einzelnachdenkens (MQM): Flüssigkeit von PaLM ist mit SOTA gleichwertig. Genauheitswerte sind im Allgemeinen niedriger. Dominiert durch "Genauigkeit/Vollständigkeit". "Stil/Absurdität" sind im Allgemeinen niedriger für PaLM.</sample>
    <sample id="905">Beispielergebnisse</sample>
    <sample id="906">Die englische Phrase "thank you" bedeutet in Deutsch "danke". Es handelt sich um eine allgemeine Ausdrucksweise, die man benutzt, um jemandem zu danken oder zu bedanken.</sample>
    <sample id="907">Der englische Titel "Weaker Than You Think" bedeutet auf Deutsch "Minder stark als du denkst". Es handelt sich um eine Übersetzung des englischen Titels in die deutsche Sprache.</sample>
    <sample id="908">Der englische Titel lautet: "Weaker Than You Think" und "A Critical Look at Weakly Supervised Learning".</sample>
    <sample id="909">Weak supervision lernen erleichtert die Annotierungsbehinderung.</sample>
    <sample id="910">Weak supervision alleviates the annotation bottleneck. Weak labels are noisy Noise memorization harms generalization. Weakly supervised learning (WSL) trains models that generalize well despite being trained on noisy data.</sample>
    <sample id="911">Weak supervision alleviates the annotation bottleneck. Weak labels are noisy, which harms generalization due to noise memorization. Weakly supervised learning (WSL) trains models that generalize well despite being trained on noisy data by leveraging weak labeling sources like heuristics and knowledge bases.</sample>
    <sample id="912">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="913">Weak supervision alleviates the annotation bottleneck. Weak labels are noisy Noise memorization harms generalization. Weakly supervised learning (WSL) trains models that generalize well despite being trained on noisy data.</sample>
    <sample id="914">Ein häufiger Ansatz in kürzlichen Forschungsarbeiten ist, dass Modelle mit schwach überwachtem Datenmaterial trainiert werden und eine Genauigkeit von XX% erreichen.</sample>
    <sample id="915">Ein häufiger Anspruch in den letzten WSL-Arbeiten lautet: "Wir trainieren Modelle nur auf schwach überwachtem Daten und erreichen eine Genauigkeit von XX%."</sample>
    <sample id="916">Ein häufiger Fall in den jüngsten WSL-Forschungen ist, dass Modelle nur mit schwach überwachtem Datenmaterial trainiert werden und eine Genauigkeit von XX% erreichen.</sample>
    <sample id="917">Das englische Inhalt auf dem Bild lautet: "A common claim in recent WSL works" und "We train models only on weakly supervised data and achieve an accuracy of XX%". Diese Aussage wird mit einem Gesichtsausdruck und einem Elefanten-Emojie unterstrichen, was eine Kritik oder eine bemerkenswerte Beobachtung impliziert.</sample>
    <sample id="918">Unsere Forschungsfragen sind: 1. Ist die Validierung von Sauberheitsdaten notwendig? 2. Wie viele Saubersamplings benötigen WSL-Anproptehe? 3. Wie können wir die verfügbaren Saubersamplings effizienter nutzen?</sample>
    <sample id="919">Unser Forschungsfragen sind: 1. Ist die Validierung von sauberen Daten notwendig? 2. Wie viele saubere Beispiele benötigen WSL-Anproptehe? 3. Wie können wir die verfügbaren sauberen Beispiele effizienter nutzen?</sample>
    <sample id="920">Die Hauptfunde sind die folgenden:

1. **Validation auf Schwachen Labels**: Diese Linie zeigt den relativen Leistungsverbesserungen über den Zeitraum (%) für die Validation auf Schwachen Labels. Die Linie ist in orange gehalten und zeigt einen mittleren Leistungsboden, mit einer Schicht von Punkten um sie herum.

2. **Keine Validation**: Diese Linie zeigt die Leistungsverbesserungen ohne Validation. Die Linie ist in blau gehalten und zeigt eine niedrigere Leistung als die Linien der Validation auf Schwachen Labels und Voll labels.

3. **Validation auf Voll labels**: Diese Linie zeigt die Leistungsverbesserungen durch die Validation auf Voll labels. Die Linie ist in grün gehalten und zeigt die höchsten Leistungsverbesserungen unter allen drei Linien, mit einem hohen Grad an Varianz durch die Punkte.

Die X-Achse repräsentiert verschiedene Methoden oder Algorithmen (FT_w, BOND, COSINE, MLC, L2R), während die Y-Achse die relativen Leistungsverbesserungen (%) darstellt. Die Hauptfindung ist, dass die Validation auf Voll labels die besten Ergebnisse liefert, gefolgt von der Validation auf Schwachen Labels und schließlich von Keine Validation.</sample>
    <sample id="921">Das Hauptergebnis ist, dass die Verwendung von "Clean Labels" in der Validierung den größten Verbesserungsgrad gegenüber den anderen Methoden erzielt hat.</sample>
    <sample id="922">RQ1 Main findings</sample>
    <sample id="923">Das Hauptresultat ist, dass die Verwendung von "Clean Labels" bei der Validierung des Modells den größten Verbesserungsfaktor in Bezug auf den "Weak Labels"-Fall bietet.</sample>
    <sample id="924">Die Hauptfunde sind: 1. Validierung ist entscheidend für die Verbesserung der Leistung. 2. Die Verwendung eines sauberen Validationssets kann den Prozess verbessern. 3. Das Bild zeigt eine Grafik, die die Leistung von verschiedenen Validationsmethoden anzeigt.</sample>
    <sample id="925">RQ2 Main findings</sample>
    <sample id="926">Die Hauptfindungen des Forschungsprojekts sind auf dem Graphik-Plot dargestellt. Der Plot zeigt die Genauigkeit (Y-Achse) gegenüber der Validierungsstufe (X-Achse). Fünf verschiedene Methoden werden verglichen: FTw, COSINE, L2R, BOND und MLC. Jede Methode wird durch eine einzelne Linie und eine Schattenfläche darstellt, um die Varianz innerhalb der Methode zu verdeutlichen. Die Schattenfläche zeigt den Bereich, in dem die Genauigkeit variieren kann. Der Punkt "Weak labels" gibt an, wo die Genauigkeit bei schlechten Labels liegt. Die Methode COSINE hat die höchste Genauigkeit und zeigt einen stetigen Anstieg mit der Steigerung der Validierungsstufe. Die anderen Methoden zeigen auch eine Verbesserung, jedoch nicht so deutlich wie COSINE.</sample>
    <sample id="927">R2</sample>
    <sample id="928">Das Bild zeigt eine Präsentation mit zwei Graphen und einem Text. Der Titel der Präsentation lautet "R02" und "Main findings". Links befindet sich ein Graph, der die Genauigkeit (%) gegenüber der Validierung anzeigt. Dieser Graph zeigt mehrere Linien, jede einer verschiedenen Methoden oder Algorithmen zugeordnet, wie zum Beispiel FTw, COSINE, LZR, BOND, MLC und Weak labels. Rechts befindet sich ein weiterer Graph, der die Leistungsdelta (%) zeigt. Dieser Graph umgibt eine rote Dachverlauflinie, die auf die Veränderungen in der Leistung hinweist. Am unteren Rand des Bildes ist der Text zu lesen: "WSL approaches benefit from more clean validation samples!" Diese Information suggeriert, dass die WSL-Methoden von saubereren Validationsmustern profitieren.</sample>
    <sample id="929">R2 Main findings</sample>
    <sample id="930">Die Hauptfunde</sample>
    <sample id="931">RQ3 Main findings</sample>
    <sample id="932">RQ3 Main findings</sample>
    <sample id="933">Das Hauptresultat zeigt, dass das kontinuierliche Feinabpolieren (CFT) die Leistungsgaps zwischen den WSL-Analogyen schließt. Außerdem wird festgestellt, dass es keine notwendig ist, komplizierte WSL-Methoden zu verwenden, da WSL (Weak Supervision Learning) gleichermaßen gut funktioniert.</sample>
    <sample id="934">Zusammenfassung Recent WSL-Anprochien</sample>
    <sample id="935">Zusammenfassung Recent WSL-Methoden: - Erfordern saubere Beispiele. - Schätzen sie ihre Praxis übermäßig hoch. Empfehlungen: - Berichten Sie die Modellauswahlkriterien. - Benutzen Sie Few-Shot-Learning-Methoden als Baselines. - Stellen Sie immer kontinuierliche Feinabpolsterung (CFT) durch.</sample>
    <sample id="936">Zusammenfassung Recent WSL Ansätze - Anforderungen für saubere Beispiele. - Über-Estimation ihrer Praktikabilität. Empfehlungen - Bericht über die Modellauswahlkriterien. - Verwendung von Baseline-Methoden der Few-Shot-Learnung. - Fortdauernder Feinabpolsterung (CFT).</sample>
    <sample id="937">Zusammenfassung Recent WSL-Methoden Anforderungen für saubere Beispiele. Übertriebene Realitätsbewertungen. Empfehlungen Berichten über die Modellauswahlkriterien. Verwenden von Baselines aus Few-Shot-Learning-Anwendungen. Durchführung der kontinuierlichen Feinabstimmung (CFT).</sample>
    <sample id="938">Conklusion Recent WSL ansätze Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="939">Likert-Skala</sample>
    <sample id="940">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="941">Das Hintergrundwissen, das für den Beispiel mit Servin und Kea erforderlich ist, besteht aus der Kenntnis, dass Servin ein Richter ist und Kea ein Backofen ist. Diese Informationen sind entity-specific knowledge (entitätbezogene Kenntnisse).</sample>
    <sample id="942">Ja, der Code ist verfügbar auf GitHub unter dem Username "mpoems1" und dem Projekt "kitmus".</sample>
    <sample id="943">Ja, die Annotatoren für NLPositionality sind ausgewogen.</sample>
    <sample id="944">Sätze wurden durch Präfixe und Suffixe verändert, wie z.B. "However, &lt;sent&gt;." und "First and foremost, &lt;sent&gt;".</sample>
    <sample id="945">Eine dimensionale Bewertung bedeutet, dass die Bewertung auf mehreren Faktoren basiert.</sample>
    <sample id="946">Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie.</sample>
    <sample id="947">Das Prompt ist wichtig, wenn es sich um die Übersetzung von englischen Sätzen zu deutschen Sätzen handelt.</sample>
    <sample id="978">Die Autoren haben die Dialogmodelle BART-FID-RAG, Blender2, Emora und Blender-Decode evaluiert.</sample>
    <sample id="979">Es sind 10 Autoren an der Arbeit beteiligt.</sample>
    <sample id="980">Gute Planners sollten sich gut orientieren, flexibel sein und die Fähigkeit haben, sich an ändernde Situationen anzupassen.</sample>
    <sample id="981">Zehn Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="982">Vasudha Varadarajan</sample>
    <sample id="983">The authors belong to the Polish Academy of Sciences and the University of Warsaw.</sample>
    <sample id="1021">Accuracy, Style/Awkwad</sample>
    <sample id="1022">"Vergiss nicht deine ABC's: Bewertung der neuesten Techniken in Chatsprachsystemen" von Sarah E. Finch, James D. Finch und Jinho D. Choi.</sample>
    <sample id="1023">Das englische Text ist auf Deutsch übersetzt und lautet: "Lass dich nicht vom ABC's vergessen: Bewertung der neuesten Forschungsergebnisse in Chatsprachsystemen". Diese Übersetzung passt sich an die visuelle Darstellung des Titels an, der auf dem Bild zu sehen ist.</sample>
    <sample id="1024">Das Bild zeigt eine Präsentation mit dem Titel "Comparative Evaluation" auf der oberen Kante. Im Hintergrund sind Logos von Emory University und Alexa zu sehen. Links im Bild ist ein Grafik von einem Menschenkopf mit einem Blaues Sprachblasenfeld, daneben zwei blaue Sprachblasenfelder und zwei blaue Roboter-Logos. Rechts sind zwei weitere Sprachblasenfelder und zwei rote Roboter-Logos.</sample>
    <sample id="1025">Das Bild zeigt eine Präsentation mit dem Titel 'Comparative Evaluation' (Komparativer Bewertungsvergleich). Im Hintergrund sind zwei Gruppen von Figuren zu sehen, jede mit einem Chatbot und Blaue oder Lila Sprachblasen. Im Zentrum des Bildes ist eine Figur zu sehen, die wie ein Richter aussieht, mit einem Gabel in der Hand und einem Richterschreibtisch vor sich. Der Text 'EMORY UNIVERSITY' und das Alexa-Logo sind im unteren rechten Eck sichtbar.</sample>
    <sample id="1026">Das Bild zeigt eine Präsentation mit dem Titel "Dimensions of Dialogue Quality" (Qualitätsdimensionen des Dialogs). Im oberen rechten Eck ist ein kleiner Bildschirm eines Mannes zu sehen, der wahrscheinlich die Präsentation führt. Der Hauptinhalt des Slides besteht aus einem rosa Box mit dem Text "Dialogue Quality", der von drei anderen Boxes durch Pfeile verankert ist: "Relevance" (Relevanz), "Consistency" (Kohärentätigkeit) und "Emotional Understanding" (emotionale Verständnis). Diese Boxen sind in einer Reihe angeordnet, wobei "Emotional Understanding" im Zentrum platziert ist und durch Pfeile zu den beiden Seiten verankert ist. Im unteren linken Eck befindet sich das Emory University Logo, und im unteren rechten Eck ist das Alexa-Logo zu sehen.</sample>
    <sample id="1027">Das Bild zeigt eine Präsentation mit dem Titel "Likert Rating Evaluation". Im Bild sind verschiedene Elemente zu sehen: - Links unten ist ein Zeichnung eines Richters, der einen Gabel und einen Schiefer in der Hand hält. - Oben rechts ist ein kleiner Bildschirm mit einem Mann, der sprechen scheint. - Im Zentrum sind mehrere Blaue Sprachblasen zu sehen, die aufeinander folgen. - Unter den Sprachblasen sind zwei blaue Blöcke zu sehen, die wahrscheinlich auf Computer oder Smartphones hinweisen. - Am unteren Rand des Bildes befindet sich eine Skala von 1 bis 5, wobei der Punkt 3 mit einer grünen Klarheit markiert ist. - Der Text "Rate the relevance of the bot's responses" (bewerten Sie die Relevanz der Bot-Responses) ist unten der Skala zu lesen. - Im unteren linken Eck befindet sich das Logo der Emory University, und im unteren rechten Eck ist das Alexa-Logo zu sehen.</sample>
    <sample id="1028">"Likert Rating Evaluation" bedeutet eine Bewertung mittels eines Likert-Skales. Diese Skala wird häufig in der Forschung und der Evaluierung verwendet, um die Bedeutung oder den Relevanz von Faktoren zu messen. Im Kontext dieser Präsentation scheint sie für die Bewertung der Relevanz der Antworten eines Chatbots auf bestimmte Fragen verwendet zu werden.</sample>
    <sample id="1029">Das Bild zeigt eine Präsentation mit dem Titel "Annotating Behaviors in Chat (ABC-Eval)". Es gibt mehrere Blaue und graue Sprechblasen, die von schwarzen Kopf-und-Körper-Symbolen abgeleitet werden. Jede Sprechblase hat einen Timestamp innerhalb eines blauen Uhrenzeichens, der "01:39" lautet. Links unten befindet sich das Emory University Logo und rechts unten das Alexa Logo. Im oberen rechten Eck ist ein kleiner Bildschirm mit einem Menschen zu sehen.</sample>
    <sample id="1030">Der englische Text lautet: "Annotating Behaviors in Chat (ABC-Eval)" und zeigt eine visuelle Darstellung von Kommunikationsmustern zwischen Menschen. Diese Abbildung scheint auf eine Analyse von Interaktionen in Chat-Systemen zu basieren, möglicherweise um Verhaltensweisen zu identifizieren und zu analysieren.</sample>
    <sample id="1031">ABC-Eval Verhaltene Coherence Wissen Consistency Emotionales Verständnis</sample>
    <sample id="1032">Das Bild zeigt eine Präsentation, die von Emory University erstellt wurde. Der Titel der Präsentation lautet "ABC-Eval Behaviors". Das Layout der Präsentation besteht aus vier quadratischen Boxen, die in einem 2x2-Grid angeordnet sind. Die Boxen sind mit den folgenden Schwerpunkten beschriftet: - Koherenz (Coherence) - Wissen (Knowledge) - Konsistenz (Consistency) - Emotionale Verständigung (Emotional Understanding) In der Box "Koherenz" sind zwei weitere Boxen innerhalb dieser Platzierung platziert, die "Ignoring Partner" und "Irrelevant" beschriftet sind. Im oberen rechten Eck des Bildes befindet sich ein kleiner Bildschirm eines Menschen, der wahrscheinlich das Präsentationsgeschehen verfolgt. Am unteren Rand der Präsentation sind die Logos von Emory University und Alexa zu sehen.</sample>
    <sample id="1033">Der englische Inhalt lautet: ABC-Eval Behaviors Coherence Ignoring Partner Irrelevant Consistency Self Contradiction Partner Contradiction Knowledge Emotional Understanding</sample>
    <sample id="1034">Experimente</sample>
    <sample id="1035">Das Bild zeigt eine Präsentation, die Informationen über Experimente in offenen Domänen-Dialogmodellen präsentiert. Der Titel der Präsentation lautet "Experimente". Die Inhalte der Präsentation umfassen folgende Punkte: - 4 offene Domänen-Dialogmodelle - 100 Menschen-Bot-Conversationen pro Modell. Darüber hinaus sind drei Diagramme zu sehen: - ABC-Eval: Diese Abbildung zeigt eine Interaktion zwischen einem Menschen und einem Bot, wobei der Mensch eine Stimme abgibt und das Bot eine Antwort gibt. - Turn Likert: Dieses Diagramm zeigt eine Bewertungs-Skala von 1 bis 5, wobei ein Checkmark an der Position 5 platziert ist, was bedeutet, dass der Benutzer eine positive Bewertung gegeben hat. - Dialogue Likert: Dieses Diagramm zeigt eine ähnliche Bewertungs-Skala von 1 bis 5, wobei ein Checkmark an der Position 5 platziert ist, was wiederum eine positive Bewertung bedeutet. Im unteren rechten Eck des Bildes befindet sich ein Diagramm mit dem Titel "Comparative", das eine Analyse von Dialogue-Interaktionen zwischen Menschen und Bots zeigt. Das Bild wurde von Emory University und Alexa präsentiert.</sample>
    <sample id="1036">Das Bild zeigt eine Präsentation mit dem Titel 'Experiments'. Es gibt drei Diagramme, die 'Turn Likert', 'Dialogue Likert' und 'Comparative' bezeichnen. Jedes Diagramm zeigt eine Kombination von Blau- und Grau-Speech-Bubble, wobei einige Blaue Bubble mit weißen Schachbrett-Figuren versehen sind. Jedes Diagramm hat eine Schachtel mit einem Grünkreuz darin. Darunter sind die Worte 'Consistency', 'Emotional Understanding', 'Informativeness' und 'Overall Quality' aufgeführt. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen. Am unteren Rand sind das Emory University Logo und das Alexa Logo zu sehen.</sample>
    <sample id="1037">Inter-Annotator Agreement</sample>
    <sample id="1038">predictiverechtlichkeit</sample>
    <sample id="1039">Das Bild zeigt eine Präsentation mit einem Graphik, der "Predictive Validity" als Titel hat. Der Graphik zeigt die "% of Quality Explained (R²)" für verschiedene Kategorien wie ABC-Eval, Turn Likert, Dialogue Likert und Comparative. Die Graphik unterscheidet zwischen "Interactive Qua," und "Interactive Qua," durch unterschiedliche Farben. Links am Graphik sind verschiedene Kategorien aufgeführt, wie "Other", "Collegial", "Informal", "Formal", "Emotional", "Cognitive" und mehr. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich die Präsentation gibt. Im unteren linken Eck befindet sich das Logo von Emory University und im unteren rechten Eck ist das Alexa-Logo zu sehen.</sample>
    <sample id="1040">Incremental Validity</sample>
    <sample id="1041">Das englische Wort "Incremental Validity" bedeutet "Incremental Validität". Diese Begriff bezieht sich auf eine Methode zur Bewertung der Qualität von Forschungsarbeiten, bei der die Validität des Studienverfahrens in Schritten gewertet wird, anstatt allein auf den Gesamteindruck zu basieren.</sample>
    <sample id="1042">Incremental Validity</sample>
    <sample id="1043">ABC-Eval Fehlerrate nach Modell</sample>
    <sample id="1044">ABC-Eval Fehlerzahlen nach Modell</sample>
    <sample id="1045">ABC-Eval Fehlerwahrscheinlichkeiten pro Modell</sample>
    <sample id="1046">ABC-Eval Fehlerwahrscheinlichkeiten nach Modell</sample>
    <sample id="1047">Das Bild zeigt eine Präsentation über die Fehlerrate von Modellen bei der Bewertung von Chats. Der Titel des Slides lautet 'ABC-Eval Fehlerrate pro Modell'. Es gibt eine Karte mit verschiedenen Fehlerarten wie 'Antisocial', 'CS Kontrast', 'Ignoeren', 'Unempathisch' und 'Topic Switch' auf der X-Achse, und die Y-Achse zeigt den Prozentsatz der Fehler. Die Karte zeigt mehrere Modelle, darunter 'BART-FID-RAG', 'Blender2', 'Emora' und 'Blender-Decode'. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen. Im unteren rechten Eck ist das Alexa-Logo zu sehen. Im unteren linken Eck sind die Emory University-Logos zu sehen. Am Ende der Präsentation steht 'Danke fürs Zuschauen!' und es gibt Links zu einer Paper-URL, GitHub-URL und Kontaktinformationen.</sample>
    <sample id="1048">Emory University</sample>
    <sample id="1049">CFT in dieser Arbeit steht für "continuous fine-tuning".</sample>
    <sample id="1050">There are 9 authors involved in the work.</sample>
    <sample id="1051">Wann benötigt die Übersetzung Kontext? Eine datenschwimmende, multilingualische Untersuchung Patrick Fernandes*, Kayo Yin*, Emmy Liu André F. T. Martins, Graham Neubig Carnegie Mellon University Sprachtechnologien-Institut Técnico Lisboa Berkeley Artificial Intelligence Research Unbabel * Gleichberechtigung</sample>
    <sample id="1052">Übersetzung hängt von Kontext ab. Wir müssen uns von diesem Schmutzloch befreien.</sample>
    <sample id="1053">Das Bild zeigt einen Text, der 'Translation depends on context' (Übersetzung hängt von dem Kontext ab) aufweist. Darunter steht: 'Things could start to get dangerous if the ministers find out.' (Dinge könnten schlimm werden, wenn die Minister es herausfinden.) und 'We'll have to get rid of that mole.' (Wir müssen jenen Spion beseitigen.) Darüber hinaus gibt es eine kleine Abbildung einer Person im oberen rechten Eck des Bildes.</sample>
    <sample id="1054">Übersetzung hängt von Kontext ab. Könnte es irgendetwas ernst sein, Herr Doktor? Wir müssen das Mole entfernen.</sample>
    <sample id="1055">Das Bild zeigt eine Präsentationsseite mit dem Titel "Evaluating context-dependent translation is hard". Es gibt einen Punkt, der lautet: "Only a small portion of words depend on context". Darüber hinaus ist ein weiterer Punkt hinzugefügt, der lautet: "Corpus-level metrics". Rechts oben auf der Seite befindet sich ein kleines Bild eines Menschen. Im unteren rechten Eck der Seite sind drei schwarze, überlappende Blätter zu sehen, die Textzeilen enthalten.</sample>
    <sample id="1056">Das Bild zeigt eine Präsentationsslidesetze mit dem Titel "Evaluating context-dependent translation is hard". Es gibt zwei Hauptpunkte auf der Slide: 1. Nur ein kleiner Teil der Wörter hängt von Kontext ab - mit einer Unterzeichnung "Corpus-level metrics" 2. Derzeitige Methoden unterstützen begrenzte Diskursphänomene und Sprachen. Im unteren rechten Eck des Slides ist ein Silhouette eines Menschen zu sehen, der eine Flagge der Vereinigten K测尔国 und eine Flagge Frankreich hält, was auf multilingual oder interkulturelle Themen hinweist.</sample>
    <sample id="1057">RQ1: Wann benötigt eine Übersetzung Kontext? RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen?</sample>
    <sample id="1058">RQ1: Wann benötigt eine Übersetzung Kontext? - Wortstufe Kontextnutzung RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen?</sample>
    <sample id="1059">Das Bild zeigt eine Präsentation über "Conditional Cross-Mutual Information (CXMI)". Der Titel ist oben links positioniert, und es gibt einen Punkt unten links, der lautet: 'CXMI: measure how much context MT models use given a corpus'. Rechts im Bild ist ein kleiner Bildschirm mit einer Person zu sehen. Im Zentrum des Bildes sind verschiedene Formeln und Texte zu sehen, die mit der CXMI-Definition verbunden sind. Diese Formeln und Texte sind in verschiedenen Farben dargestellt, wie Blau und Grün, und zeigen den Zusammenhang zwischen den Quellen und Übersetzungen sowie den Kontexten.</sample>
    <sample id="1060">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="1061">Pointwise (P-)CXMI

- We introduce P-CXMI to measure context usage to translate a specific
  - Senten
    - P-CXMI(y, x, C) = -log(qMTA(y|x)/qMTC(y|x, C))
  - Word
    - P-CXMI(i, y, x, C) = -log(qMTA(y|h_i&lt;x, x)/qMTC(y|h_i&lt;x, C))
  - High P-CXMI words =&gt; requires context to translate</sample>
    <sample id="1062">RQ1: Wann benötigt eine Übersetzung Kontext? - Wortstufe Kontextnutzung - Thematischer Analyse RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen?</sample>
    <sample id="1063">Thematikanalyse von hochwertigen P-CXMI-Wörtern</sample>
    <sample id="1064">Thematischer Analyse von hochwertigen P-CXML-Wörtern 1. POS-Tags</sample>
    <sample id="1065">Thematischer Analyse von hochwertigen P-CXML-Wörtern 1. POS-Tags - Pronomen</sample>
    <sample id="1066">Das Bild zeigt eine Präsentation über das Thematische Analysis von hochwertigen P-CXMI-Wörtern. Es gibt zwei Hauptpunkte: 1. POS-Tags und 2. Wortschatzelemente. Im Detail sind die POS-Tags auf "Pronomen" und "Verbform" eingegliedert, was auf eine Analyse der Grammatik und des Verbaus in den Wörtern hinweist.</sample>
    <sample id="1067">Der englische Text lautet: "Avelile's mother was still asleep. Avelile went to school."</sample>
    <sample id="1068">1. POS tags 2. Vocabulary items Avelile's mother was still asleep. Avelile went to school. 阿维利尔的母亲还在睡觉。 阿维利尔去上学了。 - Pronouns - Verb form - Lexical cohesion - Formality</sample>
    <sample id="1069">Das Bild zeigt eine Präsentation über "Thematische Analyse von hochp-CXML-Wörtern" (Thematic analysis of high p-CXML words). Es sind drei Hauptpunkte aufgeführt: 1. POS tags, 2. Vocabulary items, und 3. Individual tokens. Im rechten Teil des Bildes sind weitere Punkte aufgeführt, darunter - Pronouns, - Verb form, - Lexical cohesion, - Formality und - Ellipsis. Darüber hinaus sind zwei englisch-deutsch Übersetzungen zu sehen: "She knows where we're going." und "I don't." sowie die deutsche Übersetzung "Sie weiß, wohin wir gehen." und die englische "Ich weiß es nicht."</sample>
    <sample id="1070">RQ1: Wann benötigt eine Übersetzung Kontext? - Wortstufe-Kontextnutzung - Thematischer Analyse RQ2: Wie gut handhaben Modelle Kontextabhängige Übersetzungen? - Multilingueller Diskursbewusster (MuDA)-Benchmark</sample>
    <sample id="1071">Der englische Text lautet: "Multilingual Discourse-Aware (MuDA) tagger" und auf dem rosa Block sind die folgenden Punkte aufgeführt: - Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis</sample>
    <sample id="1072">Multilingual Discourse-Aware (MuDA) tagger</sample>
    <sample id="1073">Der englische Text lautet: 'MuDA benchmark' und 'MuDA tagger'.</sample>
    <sample id="1074">RQ1: Wann benötigt eine Übersetzung Kontext? - Wortstufe-Kontextnutzung - Thematischer Analyse RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen? - Multilingueller Diskursbewusster (MuDA)-Benchmark - Modell-Evaluation</sample>
    <sample id="1075">Corpus-Level Metrics</sample>
    <sample id="1076">Das Bild zeigt eine Präsentation mit dem Titel 'Corpus-level metrics' (Korpus-Level-Metriken). Links auf der Seite ist ein Logo eines Roboters mit dem Text 'BLEU' darunter. Rechts daneben ist ein ähnliches Logo eines Roboters mit einem blauen Balken über der Maschine und dem Text 'COMET' darunter. Darüber hinaus ist ein drittes Logo eines Roboters hinzugefügt, das auch einen blauen Balken hat, und die Texte 'F-measure' sind darunter. Im oberen rechten Eck des Bildes befindet sich ein kleiner Bildschirm mit einer Person.</sample>
    <sample id="1077">Auf dem Bild sind drei Roboter dargestellt, jeder mit einem Text über ihm. Der Text auf dem ersten Roboter lautet "BLEU", der zweite Roboter hat den Text "COMET" und der dritte Roboter zeigt den Text "F-measure". Oben steht der Titel "Corpus-level metrics". Im unteren linken Eck befindet sich ein Text, der lautet: "Unclear which system is best for document-level MT with corpus-level metrics". Im oberen rechten Eck ist ein kleiner Bildschirm mit einer Frau zu sehen.</sample>
    <sample id="1078">Das Bild zeigt eine Präsentation über die Ergebnisse des MuDA-Tests. Im oberen linken Eck steht der Text 'Corpus-level metrics' in bold, was auf die Metriken für den gesamten Korpus hinweist. Darunter sind drei Roboterzeichnungen zu sehen, jedes mit einem Kreis um den Kopf und einem '101'-Muster darin. Links von den Roboten steht der Text 'BLEU', mittig 'COMET' und rechts 'F-measure'. Diese sind wahrscheinlich verschiedene Metriken oder Algorithmen, die für die Bewertung des Korpus verwendet werden.

Im oberen rechten Eck ist ein kleiner Bildausschnitt einer Person zu sehen. Im unteren Zentrum des Bildes steht der Text 'Unclear which system is best for document-level MT with corpus-level metrics', was darauf hindeutet, dass es unklar ist, welche System für die Document-Level-MT mit Korpus-Level-Metriken am besten ist.

Im nächsten Bild zeigt das Text 'MuDA benchmark results' im oberen linken Eck. Es gibt eine Punktliste mit der Aussage 'Context-aware models perform significantly better on some phenomena'. Im unteren Zentrum ist ein weiterer Punkt mit dem Text 'Formality, lexical cohesion' und einem Checkbox, die beide als 'OK' markiert sind. Dieser Punkt könnte auf die Leistungsfähigkeit von Kontextbewussten Modellen bei bestimmten Phänomena wie Formalität und Lexikalischer Kohärenz hinweisen.

Wiederum ist im oberen rechten Eck ein kleiner Bildausschnitt einer Person zu sehen, was auf eine Präsentation oder eine Diskussion hinweist.</sample>
    <sample id="1079">Der englische Text lautet: "Context-aware models perform significantly better on some phenomena o ✓: Formality, lexical cohesion ✕ Ellipsis, pronouns, verb form"</sample>
    <sample id="1080">Der englische Text lautet: "MuDA benchmark results Context-aware models perform significantly better on some phenomena o ✓: Formality, lexical cohesion ✕ Ellipsis, pronouns, verb form DeepL outperforms Google on most phenomena and language pairs*".</sample>
    <sample id="1081">Der englische Text lautet: 'MuDA benchmark results' und 'DeepL outperforms Google on most phenomena and language pairs*'.</sample>
    <sample id="1082">Der englische Text lautet: "Identify discourse phenomena systematically without prior linguistic knowledge" und "Dataset-agnostic benchmark for document-level MT".</sample>
    <sample id="1083">Zusammenfassung: Identifizieren Sie diskursbezogene Phänomene systematical ohne vorherige Sprachwissen und erstellen Sie ein Dataset-ignorantes Leistungsbenchmark für die documentenbasierte MT.</sample>
    <sample id="1084">Yusen Zhang</sample>
    <sample id="1121">Permute with "jumps"</sample>
    <sample id="1122">Die Autoren beschreiben die Methode der "markierten Wörter" so: "Finde Wörter, die die Persönlichkeiten der markierten Gruppen von den unmarkierten Gruppen unterscheiden."</sample>
    <sample id="1123">Die Autoren gehören der Carnegie Mellon University Language Technologies Institute.</sample>
    <sample id="1124">Moscow</sample>
    <sample id="1125">Sarah E. Finch, James D. Finch, and Jinho D. Choi</sample>
    <sample id="1126">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="1127">BlinMP, SyntaxGym, CrowS</sample>
    <sample id="1161">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind: FTw, BOND, COSINE, MLC und L2R.</sample>
    <sample id="1162">Das Modell wird auf 11 verschiedenen Aufgaben evaluiert.</sample>
    <sample id="1226">CamemBERT wurde ursprünglich mit 4GB von MEDSCAT-DATEN trainiert.</sample>
    <sample id="1227">Adam Przeździecki, Michał Woźniak</sample>
    <sample id="1228">Temporal drift</sample>
    <sample id="1269">Um die Token für die Ausgabesequenz zu permutieren, um eine bessere Modellleistung zu erreichen.</sample>
    <sample id="1270">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, um die Vertrauenswürdigkeit zu erhöhen und das Risiko von Unaccurate oder unethischen Entscheidungen zu reduzieren.</sample>
    <sample id="1271">Minimalpaareingaben sind "No customer ... has spent any money" und "Many people were helping themselves."</sample>
    <sample id="1272">The authors have used metrics such as NER (Named Entity Recognition), CER (Content Error Rate), F1, and MAP (Mean Average Precision) to evaluate the pre-training strategies.</sample>
    <sample id="1273">Krippendorffs Alpha</sample>
    <sample id="1274">Wikipedia, Related</sample>
    <sample id="1275">Die Autoren gehören der Heinrich Heine University Düsseldorf, Deutschland.</sample>
    <sample id="1276">MultiInstruct uses instruction tuning on multimodal pre-trained models, which sets it apart from other benchmarks.</sample>
    <sample id="1277">Es sind drei Autoren an der Arbeit beteiligt: Sarah E. Finch, James D. Finch und Jinho D. Choi.</sample>
    <sample id="1278">The binary coordination is defined as the situation where a governor and its left or right contact are in different words.</sample>
    <sample id="1279">Die Prompts in der Studie waren im Durchschnitt 3.5 Wörter lang.</sample>
    <sample id="1280">Das kleinere T5-Modell, das auf Coscript trainiert wurde, hat eine höhere Genauität als die größeren LLMs.</sample>
    <sample id="1281">DrBERT: Ein robustes vorher trainiertes Modell auf Französisch für medizinische und klinische Bereiche</sample>
    <sample id="1282">I. Sprachmodellierung in der Gesundheitsversorgung II. Vergleich von Vorbereitungsstrategien, Datenquellen und Größen III. Bewertung von 13 Modellen auf 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1283">I. Sprachmodellierung in der Gesundheitsversorgung II. Vergleich von Vorbereitungsstrategien, Datenquellen und Größen III. Bewertung von 13 Modellen auf 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1284">I. Sprachmodellierung in der Gesundheitsversorgung II. Vergleich von Vorbereitungsstrategien, Datenquellen und Größen III. Bewertung von 13 Modellen auf 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1285">I. Sprachmodellierung in der Medizin II. Vergleich von Vorbereitungsstrategien, Datenquellen und Größen III. Bewertung von 13 Modellen auf 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1286">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gains on a lot of NLP tasks. It has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain-specific models in English raised the bar even higher. PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domain in French yet. BERT-based domain specific model for French should increase performance on medical tasks.</sample>
    <sample id="1287">Language Modeling Transformer-based approaches, wie BERT, bieten große Leistungssteigerungen bei vielen NLP-Aufgaben. Sie wurden auf Französisch mit CamemBERT und FlauBERT angepasst. Auf Englisch sind auf Medizinische Aufgaben spezifische Modelle wie PubMedBERT, BioBERT und ClinicalBERT zu einer höheren Leistungsfähigkeit geführt. Sprachen außer Englisch sind weniger verbreitet und hängen aufgrund des Vorkurzes von einem vorhandenen generischen Modell ab. Kein offener Quellcode ist verfügbar für die medizinische Domain in Französisch. Eine BERT-basierte spezifische Modell für die Französischsprache sollte die Leistung auf Medizinische Aufgaben verbessern.</sample>
    <sample id="1288">Transformer-basierte Ansätze, wie BERT, bieten eine riesige Leistungssteigerung bei vielen NLP-Aufgaben. Diese Ansätze wurden auch auf Französisch mit CamemBERT und FlauBERT angepasst. Auf medizinischen Aufgabenfeldern haben Domänen-Spezifische Modelle in Englisch den Bar hervorheben lassen. Beispiele sind PudMedBERT, BioBERT und ClinicalBERT usw. Sprachen außer Englisch sind weniger verbreitet und verlassen sich hauptsächlich auf Vortraining mit einem vorhandenen generischem Modell. Es gibt keine offene-Quellen-Software für den medizinischen Bereich in Französisch. Eine BERT-basierte Domain-Spezifische Modell für Frankreich sollte die Leistung bei medizinischen Aufgaben verbessern.</sample>
    <sample id="1289">Transformer-Anroach-Methoden wie BERT bieten auf einer Vielzahl von NLP-Aufgaben erhebliche Leistungssteigerungen. Diese Methode wurde auch in Französisch durch CamemBERT und FlauBERT angepasst. Auf medizinischen Aufgaben haben spezifische Modelle in Englisch eine noch höhere Leistung erreicht. Andere Sprachen als Englisch sind seltener und hängen hauptsächlich von vorherigen Trainingssitzungen mit einem vorhandenen generischen Modell ab. Es gibt keine offene-Quellensoftware für den medizinischen Bereich in Französisch, aber ein BERT-basiertes spezifisches Modell für die Französischsprache könnte aufmedizinischen Aufgaben eine bessere Leistung erbringen.</sample>
    <sample id="1290">Das Bild zeigt eine Präsentation über die Vergleichung von Vorbereitungsmethoden und Datenquellen für Sprachmodelle. Der Titel der Präsentation lautet "Comparison of pre-training strategies and data sources". Die Präsentation umfasst folgende Punkte: 1. Bewertung des Einflusses öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datensätze. 2. Beschreibung von zwei Datenquellen: - NACHOS: Eine 1,1 MB große Datei, die aus einer Sammlung von 4,6 MB Datenquellen besteht, die von verschiedenen medizinischen Domänen, Stilen und Quellen stammen. - NBDW: Eine private Datenbank mit Sätzen, die von 1,7 Mio. anonymisierten Medizin-Meldungen aus dem Hospital-Datenwarehaus der Université de Nantes stammen. Darüber hinaus gibt es eine Abschnitte zur Vergleichung von Vorbereitungsmethoden, die auf zwei Arten von Vorbereitungsvorgehensweisen abzielt: - Von Grund auf: Mit Vollmodellkonstruktion - Voraufgebildeter Training: Mit einem vorhandenen vorher trainierten Modell (z.B. CamemBert, ein französisches Genau-Modell, oder PubMedBERT, ein englischsprachiges Medizin-Modell) Das Bild zeigt auch eine Tabelle mit Informationen zu den Datenquellen, einschließlich Größe und Anzahl von Resourcen. Im unteren rechten Eck des Bildes ist das Logo der Avignon Universität zu sehen.</sample>
    <sample id="1291">Das Bild zeigt eine Präsentation über die Vergleichung von Vorbereitungsmethoden und Datenquellen für Sprachmodelle. Der Titel der Präsentation lautet "Vergleich von Vorbereitungsmethoden und Datenquellen". Es gibt zwei Hauptabschnitte: 1. Bewertung des Einflusses öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datensätze. 2. Vergleich von Lernstrategien, einschließlich den Start von scratch mit Vollmodellbau und der Fortsetzung mit einem vorgelegten Modell (z.B. CamemBERT, ein französisches Modell, und PubMedBERT, eine englische Version). Im unteren rechten Eck ist das Logo der Avignon Universität zu sehen.</sample>
    <sample id="1292">Das Bild zeigt eine Präsentation über "Vergleich von Vorbereitungstechniken und Datenquellen". Die Präsentation diskutiert die Auswirkungen öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datensätze. Es werden zwei Hauptthemen behandelt: die Bewertung des Einflusses öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datensätze und die Vergleichung von Vorbereitungstechniken. Zwei Datenquellen werden genannt: NACHOS und NBDW, wobei NACHOS eine 1.1MB große Datei ist und NBDW eine 0.5MB große Datei ist. Darüber hinaus wird erläutert, wie verschiedene Vorbereitungstechniken, wie die Anwendung eines vorvorhabenen Modells oder die Vorlage von Modellen, verwendet werden können. Im unteren rechten Eck des Bildes befindet sich das Logo der Avignon Universität.</sample>
    <sample id="1293">Das Bild zeigt eine Präsentation mit dem Titel "Comparison of pre-training strategies and data sources". Die Präsentation wird von der Avignon Universität präsentiert, wie durch das Logo im unteren rechten Eck zu erkennen ist. Der Hauptinhalt der Präsentation umfasst eine E valuation der Auswirkung öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datensätze. Sie erwähnt zwei Datenquellen: NACHOS und NBDW. Darüber hinaus wird eine E valuation der Vorbereitungsstrategien für Sprachmodelle vorgestellt, einschließlich den Start von scratch und der Verwendung eines vorhandenen vorbereiteten Modells namens CamemBERT. Das Slide enthält auch eine Tabelle mit Informationen über die Größe und die Anzahl der Referenzen für verschiedene Datenquellen.</sample>
    <sample id="1294">Das Bild zeigt eine Präsentation mit dem Titel "Comparison of pre-training strategies and data sources". Die Präsentation ist auf einem Computerbildschirm zu sehen, und es gibt auch ein kleines Bild eines Menschen im oberen rechten Eck des Bildes. Der Hauptinhalt der Präsentation besteht aus zwei Hauptpunkten: - Evaluierung des Einflusses öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datengrößen - Vergleich von Vorgehensweisen bei der Vorbereitung von Modellen</sample>
    <sample id="1295">Das Bild zeigt eine Präsentation mit dem Titel "Comparison of pre-training strategies and data sources". Es gibt zwei Hauptabschnitte:</sample>
    <sample id="1296">Das Bild zeigt eine Präsentation über die Vergleichung von Vorbereitungstechniken und Datenquellen. Der Titel der Präsentation lautet "Vergleich von Vorbereitungstechniken und Datenquellen". Die Präsentation umfasst folgende Punkte: - Bewertung des Einflusses öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datengrößen - Vergleich von Vorbereitungstechniken, einschließlich der Vorgehensweise, wie man von scratch mit Vollmodellbau anfängt und die Vorbereitung mit einem vorhandenen vorher trainierten Modell durchführt (z.B. CamemBERT, ein französisches Genauigkeitsmodell, und PubMedBERT, eine englische Version) Die rechte Seite des Bildes zeigt eine Tabelle mit Informationen zu verschiedenen Datenquellen und ihren spezifischen Merkmalen. Die Tabelle enthält Spalten für den Namen der Datenquelle, Größe, Anzahl von Referenzen und andere spezifische Merkmale. Im unteren rechten Eck des Bildes ist das Logo der Avignon Universität zu sehen.</sample>
    <sample id="1297">Das Bild zeigt eine Präsentation über die Vergleichung von Vorbereitungsmethoden und Datenquellen für Sprachmodellierung. Der Titel der Präsentation lautet "Vergleich von Vorbereitungsmethoden und Datenquellen". Es gibt zwei Hauptabschnitte: 1. Bewertung des Einflusses öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datensätze. 2. Vergleich von Lernstrategien, einschließlich der Erstellung eines Vollmodells von scratch und der Verwendung eines vorvorhabenen Modells (wie CamemBert, ein Französisches Modell, oder PubMedBERT, eine Englische Version). Im unteren rechten Eck ist das Logo der Avignon Universität zu sehen.</sample>
    <sample id="1298">Evaluierung: Datenquellen und Größe</sample>
    <sample id="1299">Die Übersetzung des englischen Inhalts ins Deutsche lautet: "Evaluierung: Datenquellen und Größe. Performance-Evaluation von 13 Modellen auf 11 Aufgaben, sowohl öffentlich als auch private. Unser überdurchschnittliches Modell erreicht in der Regel ein hervorragendes Ergebnis auf fast allen Aufgaben."</sample>
    <sample id="1300">Evaluierung: Datenquellen und Größe</sample>
    <sample id="1301">Evaluierung: Datenquellen und Größe</sample>
    <sample id="1302">Ausgangsgrad: 0.978624</sample>
    <sample id="1303">Das Bild zeigt eine Präsentation mit dem Titel "Evaluation: Pre-training strategies" (Evaluierung: Vorbereitungsstrategien). Der Text auf der Seite diskutiert verschiedene Ansätze zur Vorbereitung von Modellen, einschließlich Ausgangspunkt von scratch oder Fortsetzung von Vorbereitung auf 4 GB von Daten. Es wird auch erwähnt, dass Fragen-ansprechende Aufgaben mehr domänenspezifischen Wissen erfordern, um effektiv zu funktionieren, und dass ein Modellstabilitätssatz eine höherere Unvariabilität für den Cambridge-based-Modelltyp zeigt, der auf Vorbereitung auf die Vorbereitung trainiert wurde. Im unteren rechten Eck des Bildes ist das Logo der Avignon Universität zu sehen.</sample>
    <sample id="1304">Das englische Text ist auf dem Bild zu lesen. Es handelt sich um eine Übersetzung des englischen Inhalts ins Deutsche.</sample>
    <sample id="1305">Der Hauptinhalt lautet: DRBERT erreicht die bestmögliche Leistung bei 9 französischen Medizinverfahrensabläufen. Er übersteigt den CamemBERT-Generalmuster und die englisch-basierten Domänen-Spezifiken-Modelle. Er bestätigt die Nützlichkeit der Ausbildung eines medizinischen Modells in Frankreich. Datenquelle-Frage: das Training auf heterogenen Daten ist wichtig. NACHOS ist robust, aber nicht mit privaten Patientendaten. Mehr Daten ist besser, aber es reicht nicht für die Skalierung. Voraussetzung für Fortbildung ist eine effektivere Strategie für die Verwendung von englischen Modellen auf Basis von domänenspezifischen französischen Modellen. Die DRBERT-Modelle, die NACHOS-Datensätze und die Skripte sind unter der MIT-Lizenz verfügbar.</sample>
    <sample id="1306">Core message: DRBERT erzielt in 9 Downstream-Französischen Aufgaben die bestmögliche Leistung. Er übersteigt den CamemBERT-Generic-Model und die englisch-basierten Domain-Spezifiken-Modelle. Es bestätigt die Nutzbarkeit der Ausbildung eines medizinischen Modells in Frankreich. Datenquelle ist wichtig: das Trainieren auf heterogenen Daten ist wichtig. NACHOS ist robuster als das Verarbeiten von privaten Patientendaten nur. Mehr Daten ist besser, aber es kann nicht skalieren. Vorbereitung vor dem Training ist eine effektivere Strategie, wenn basierend auf englischen Domain-Spezifiken-Modellen. Die DRBERT-Modelle, die NACHOS-Dataset und die Skripte sind unter der MIT-Lizenz verfügbar.</sample>
    <sample id="1307">Der Hauptmessage lautet: "DrBERT erreicht auf der aktuellen Stufe in 9 Französischen Medizinverfahren die besten Ergebnisse."</sample>
    <sample id="1308">Danke. Ich freue mich darauf, im Poster-Sitzung in Toronto zu exchangen</sample>
    <sample id="1309">Die Lernstrategien, die untersucht werden, sind "Von Grund auf" und "Vorher trainierte Modelle verwenden".</sample>
    <sample id="1310">Der Faktor der Überanpassung, der speziell auf die Wiederverwendung von Tests zurückzuführen ist, beträgt 0.54.</sample>
    <sample id="1311">Die Qualität der Vereinfachung wurde mittels des BLEU-Skoring-Systems beurteilt.</sample>
    <sample id="1312">Ja, die Grafik zeigt, dass Sprachmodelle unterschiedliche politische Vorurteile haben.</sample>
    <sample id="1313">Compositional Generalization ohne Bäume mit Multiset-Tagging und Latent Permutation</sample>
    <sample id="1314">Kompositionelle Allgemeinung ohne Bäume mit Multiset-Taggen und versteckten Permutationen</sample>
    <sample id="1315">Kompositionelle Allgemeinung</sample>
    <sample id="1316">Das Bild zeigt eine Übersetzung des englischen Inhalts ins Deutsche. Es handelt sich um ein Screenshot einer Präsentation mit dem Titel "Compositional Generalization in Semantic Parsing" (Kompositionelle Allgemeinereignung in Semantischer Parseng). Der Text auf der Präsentation lautet: "Train: The girl slept." und "Mary knew that the girl slept." Diese Phrasen sind in einem Tabelle-Format dargestellt, wobei die englischen Wörter durch blaue und grüne Schrift gefärbt sind. Die deutsche Übersetzung der englischen Phrasen lautet: "Die Mädchen schläft." und "Mary wusste, dass das Mädchen schläft." Diese Übersetzungen deuten auf eine semantische Analyse hin, die wahrscheinlich auf den Prozess von allgemeiner Komposition in der Sprachanalyse abzielt.</sample>
    <sample id="1317">Der englische Text lautet: "The girl slept." und "Mary knew that the girl slept." Diese Sätze deuten auf eine semantische Analyse hin, in der die Handlung "sleep" durch den Subjekt "girl" und "Mary" durchgeführt wird.</sample>
    <sample id="1318">Der englische Text lautet: "The girl slept." "Mary knew that the girl slept." "Jim said that Mary knew that the girl slept."</sample>
    <sample id="1319">Train: "The girl slept." "Mary knew that the girl slept." Test: "Jim said that Mary knew that the girl slept."</sample>
    <sample id="1320">Das Bild zeigt eine Präsentation über "Kompositionelle Allgemeinheit in Semantischer Parseng". Es gibt zwei Abschnitte: "Train" und "Test", jeder mit einem Beispiel. Im "Train"-Abschnitt sind zwei Sätze gegeben, wobei die Sätze und ihre grammatikalischen Strukturen in einer Tabelle dargestellt sind. Im "Test"-Abschnitt ist ein weiterer Satz aufgeführt, der auf das im "Train"-Abschnitt gegebene Satz verweist. Am Ende des Bildes ist in roter Schrift geschrieben: "Naive seq2seq models fail!" Das Bild scheint auf die Grenzen von einfachen seq2seq-Modellen bei der Semantischer Parseng zu hinweisen.</sample>
    <sample id="1321">Das Bild zeigt eine Präsentation über "Kompositionelle Allgemeinheit in Semantischer Parseng". Es gibt zwei Hauptabschnitte: Train und Test. Jeder Abschnitt enthält zwei Sätze, die mit einer Tabelle dargestellt sind. Die Tabelle zeigt die Verbindungen zwischen den Worten und den jeweiligen Argumenten (z.B. "x" für Agenten) an. Am Ende des Bildes steht in roter Schrift "Naive seq2seq models fail!" Das Bild vermittelt die Schwierigkeiten bei der Anwendung von einfachen seq2seq-Modellen auf kompositionelle Semantische Parseng.</sample>
    <sample id="1322">Bäume helfen sehr, aber...</sample>
    <sample id="1323">Bäume helfen sehr, aber...</sample>
    <sample id="1324">Trees help a lot but... *girl x1: sleep.agent x2 x *girl x4, x1 sleep.agent x2 The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms</sample>
    <sample id="1325">Bäume helfen sehr, aber... Bäume müssen erworben werden: - Vorbereitung/Verarbeitung logischer Formen</sample>
    <sample id="1326">Trees helfen sehr, aber...</sample>
    <sample id="1327">Trees help a lot but... Trees need to be obtained: - Pre-/Post-processing logical forms - Grammar-induction This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="1328">Bäume helfen sehr, aber...</sample>
    <sample id="1329">Unsere Ansatz</sample>
    <sample id="1330">Unsere Ansatz</sample>
    <sample id="1331">Unsere Ansatz</sample>
    <sample id="1332">Unsere Ansatz</sample>
    <sample id="1333">Das Bild zeigt eine Diagrammstruktur, die "Our Approach" in einem gelben Hintergrund mit schwarzen Buchstaben hervorgehoben ist. Der Hauptinhalt des Diagramms besteht aus verschiedenen Elementen, die mit Pfeilen verknüpft sind, und wird in zwei Abschnitte unterteilt: "Permute" und "Tag". In der "Permute"-Abschnitt sind verschiedene Wörter wie "girl", "sleep", "agent" und "x2" zu sehen, sowie einige andere Wörter, die mit einem Asterisk (*) beginnen. Diese Wörter sind in verschiedenen Farben (grün, orange) und Größen dargestellt. Im "Tag"-Abschnitt sind Wörter wie "the", "girl" und "slept" zu sehen. Die gesamte Struktur scheint ein Diagramm für eine Analyse oder eine Methode zur Bearbeitung von Textdaten oder -strukturen zu sein.</sample>
    <sample id="1334">Der englische Text lautet: "Permute with 'jumps'".</sample>
    <sample id="1335">Der englische Text lautet: "Permute with 'jumps'".</sample>
    <sample id="1336">Der englische Text lautet: "Permuting with 'jumps'".</sample>
    <sample id="1337">Der englische Inhalt lautet: 'Permuting with "jumps"'</sample>
    <sample id="1338">Permutieren mit "Sprüngen"</sample>
    <sample id="1339">Einige Ergebnisse über COGS (Kim und Linzen 2020)</sample>
    <sample id="1340">Einige Ergebnisse über COGS (Kim und Linzen 2020)</sample>
    <sample id="1341">Die englischen Wörter "Technical Challenges We Solve" in Deutsch bedeutet: "Technische Herausforderungen, die wir lösen".</sample>
    <sample id="1342">Das Bild zeigt eine Diagrammstruktur, die auf den "Technischen Herausforderungen, die wir lösen" abzielt. Oben sind verschiedene Wörter in verschiedenen Farben (grün, gelb und blau) aufgeführt: "girl", "slept", "agent", "x2" und "x1". Diese Wörter sind innerhalb eines grauen Rahmens positioniert, der "Permute" beschriftet ist. Darunter befindet sich ein weiterer grauer Rahmen mit drei Flecken, jede mit einem Fragezeichen darauf, was darauf hinweist, dass ihre Inhalte unbekannt oder zu permutationen sind. Im unteren Bereich des Diagramms sind zwei Flecken mit den Worten "the" und "girl" markiert, und eine weitere Flecke mit dem Wort "slept". Links am Bild steht die Phrase "Alignment unknown." Die gesamte Struktur scheint eine visuelle Darstellung von einer Problemstellung zu sein, die mit Permutation und unbekanntem Alignment verbunden ist.</sample>
    <sample id="1343">Das Bild zeigt eine visuelle Darstellung eines technischen Problems, das mit der Übersetzung des englischen Inhalts ins Deutsche zu tun hat. Der englische Text lautet: "Technical Challenges We Solve".</sample>
    <sample id="1344">Das Bild zeigt eine detaillierte visuelle Darstellung eines "Technical Challenges We Solve"-Diagramms. Der Titel des Diagramms ist "Technical Challenges We Solve", und es handelt sich um eine komplexes System, das verschiedene Elemente wie "gair", "sleep", "agent" und "t1" beinhaltet. Diese Elemente werden durch Pfeile und Verbindungen miteinander verknüpft, was darauf hindeutet, dass sie miteinander verbunden sind.

Unter dem Diagramm befindet sich ein Text, der "Alignment unknown." und "Induce it in training." liest, was auf die Unbestimmtheit der Alignment hinweist und darauf hindeutet, dass dies im Training prozessiert werden soll. Darüber hinaus gibt es einen weiteren Text, der "Permutation model:" und "Inference is NP-hard (= TSP)" liest, was auf die Komplexität der Permutation-Modelle und die NP-Hard-Charakteristik der Inferenz hinweist, die mit dem Problem des Traveling Salesman-Problems (TSP) vergleichbar ist.

Die gesamte Präsentation wird durch eine leuchtende gelbe Schrift für den Titel "Technical Challenges We Solve" gekennzeichnet, was den Fokus auf die technischen Herausforderungen und ihre Lösungen aufrechterhält.</sample>
    <sample id="1345">Die englischen Inhalte auf dem Bild sind auf Deutsch übersetzt. Der Titel lautet "Technische Herausforderungen, die wir lösen." Es gibt auch einen Abschnitt mit der Aufschrift "Alignment unknown," was bedeutet, dass die Anordnung unbestimmt ist. Unter diesem Abschnitt steht "Induce it in training." Eine weitere Abschnitte heißt "Permutation model:" und enthält zwei Punkte: "Inference is NP-hard (= TSP)" und "Backpropagate through continuous relaxation."</sample>
    <sample id="1346">Das Bild zeigt eine präzise und komplexe Darstellung eines technischen Problems, das mit der Übersetzung des englischen Inhalts ins Deutsche zu tun hat. Hier ist die Übersetzung:

"Technical Challenges We Solve

Permutate
 Alignment unknown.
 Induce it in training.

Permutation model:
 - Inference is NP-hard (= TSP)
 - Backpropagate through continuous relaxation"

Darunter befindet sich ein QR-Code mit der Anweisung "Paper &amp; Code: https://tinyurl.com/mxX8ny". Diese Informationen deuten darauf hin, dass es sich um einen technischen Vortrag oder eine Präsentation handelt, die spezielle Herausforderungen in der Berechnung oder Algorithmik anspricht.</sample>
    <sample id="1347">Kognitive Dissonanz ist eine unangenehme emotional-physiologische Reaktion, die auftritt, wenn eine Person zwei oder mehr kognitiv konfliktierende Elemente (z.B. Gedanken, Handlungen, Überzeugungen) innerhalb ihrer Persönlichkeit erlebt. Diese Konfliktlage führt dazu, dass die Person versucht, diese unangenehme Situation zu lösen, indem sie ihre Überzeugungen ändert, ihre Handlungen anpasst oder ihre Einstellungen verändert.</sample>
    <sample id="1348">BERT-base</sample>
    <sample id="1349">Ja, kumulatives Training ist besser.</sample>
    <sample id="1350">Sara Papi, Matteo Negri, Marco Turchi</sample>
    <sample id="1351">The data for the MuDa-Benchmark comes from TED Talks.</sample>
    <sample id="1385">Matthias Lindemann, Alexander Koller, Ivan Titov</sample>
    <sample id="1386">Sprachübergreifender Transfer bezieht sich auf die Verwendung von Lernverfahren, um Modelle für eine Sprache zu trainieren und sie dann auf eine andere Sprache zu übertragen.</sample>
    <sample id="1387">Dawei Zhu, Xiaoyun Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow gehören an Saarland University.</sample>
    <sample id="1388">The authors use BLEU scores as the quality measure and AL/AL_CA (s) as a latency measure.</sample>
    <sample id="1389">Das englische Text ist auf dem Bild zu sehen: "The KITMUS Test Evaluating Knowledge Integration from Multiple Sources". Diese Übersetzung gibt einen Eindruck davon, dass es sich um eine Präsentation oder Forschungsarbeit handelt, die sich mit der Bewertung der Wissensintegration aus verschiedenen Quellen beschäftigt.</sample>
    <sample id="1390">NLU-Modelle nutzen mehrere Wissensquellen.</sample>
    <sample id="1391">NLU-Modelle nutzen mehrere Wissensquellen.</sample>
    <sample id="1392">John sah den neu gewählten Präsidenten auf der TV.</sample>
    <sample id="1393">John sah den neu gewählten Präsidenten auf der TV.</sample>
    <sample id="1394">John sah den neu gewählten Präsidenten auf der TV.</sample>
    <sample id="1395">John sah die neu gewählte Präsidentin auf der TV.</sample>
    <sample id="1396">Das Bild zeigt eine Präsentation über den "KITMUS Test Suite". Der Titel ist oben in bold geschrieben. Im Hintergrund sind zwei Punkte aufgeführt: 1. "Dataset for knowledge integration evaluation" 2. "Coreference resolution task to probe ability to draw on pretrain-time knowledge inference-time knowledge" Darunter sind weitere Details zu den Experimenten aufgeführt, einschließlich "Experiment with human study participants" und "coreference resolution models". Rechts oben befindet sich ein kleiner Bildschirm mit einer Person. Die Slide ist in Weiß und Schwarz gestaltet, und der Text ist klar lesbar.</sample>
    <sample id="1397">Das Bild zeigt eine Präsentation mit dem Titel "KITMUS Test Suite". Es gibt drei Hauptpunkte auf der Präsentation: 1. Dataset für die Bewertung der Integration von Wissen 2. Aufgabe zur Coreference Resolution, um die Fähigkeit zu testen, auf vorher trainierte und inferierte Wissen zurückzugehen 3. Experiment mit menschlichen Studienbeteiligten und Coreference Resolution-Modellen Die Texte sind auf Englisch, und es gibt keine sichtbaren visuellen Elemente wie Bilder oder Grafiken, außer dem Titel.</sample>
    <sample id="1398">Servin ist ein Richter. Kea ist Backmännchen. Servin und Kea trafen sich in einem Park. Nach einem langen Tag Arbeit, bei dem sie Gerichte in einem Gerichtshof entschieden haben, war er froh, sich auszuruhen. [Antwort: Servin]</sample>
    <sample id="1399">Servin ist ein Richter. Kea ist ein Backofenmeister. Servin und Kea trafen sich in einem Park. Nach einem langen Tag Arbeit, bei dem sie Gerichte in einem Gerichtshof entschieden haben, war er froh, sich zu entspannen. [Antwort: Servin]</sample>
    <sample id="1400">Der englische Inhalt lautet: "Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]"</sample>
    <sample id="1401">Servin ist ein Richter. Kea ist ein Backofenhersteller. Servin und Kea trafen sich am Park. Nach einem langen Tag, bei dem sie im Gerichtshof Entscheidungen über Fälle trifften, war er zufrieden. [Antwort: Servin] Richter entscheiden in Gerichten.</sample>
    <sample id="1402">Servin ist ein Richter. Kea ist ein Kuchback. Servin und Kea trafen sich am Park. Nach einem langen Tag entschieden sie in einem Gerichtshof, er war zufrieden. [Antwort: Servin] Richter entscheiden in Gerichten.</sample>
    <sample id="1403">Variants of KITMUS</sample>
    <sample id="1404">Variants of KITMUS</sample>
    <sample id="1405">Variants of KITMUS</sample>
    <sample id="1406">Variants of KITMUS</sample>
    <sample id="1407">Das Bild zeigt eine Präsentation mit dem Titel "Variants of KITMUS". Es gibt drei verschiedene Variante der Präsentation, jede mit einem ähnlichen Layout. Jede Variante hat einen Bereich namens "Background-Pretrain", "Background-Both" und "Background-Inference". Jeder Bereich enthält eine Textbox mit dem Text "Politicians seek elected seats in government." Darüber hinaus gibt es in jeder Variante eine blaue Textbox mit dem Text "Chichester is a politician." In der rechten unteren Ecke jedes Bereichs befindet sich ein weiterer Textbox mit dem Text "The work of a politician is to be an elected seat in government." Im oberen rechten Bereich des Bildes ist ein Mann zu sehen, der Kopfhörer trägt und möglicherweise über die Präsentation spricht.</sample>
    <sample id="1408">Variants of KITMUS</sample>
    <sample id="1409">Variants of KITMUS</sample>
    <sample id="1410">Auf dem Bild ist ein Mann zu sehen, der einen Kopfhörer trägt und sich auf eine Präsentation konzentriert. Im Hintergrund befindet sich ein sichtbares Slide mit dem Titel "Background-Pretrain". Das Slide zeigt ein Diagramm mit zwei Gruppen: "Without task-specific training" und "With task-specific training", wobei die Gruppe "With task-specific training" einen höheren Wert aufweist. Der Text am unteren Rand des Slides lautet: "Task-specific training is necessary for knowledge integration."</sample>
    <sample id="1411">Auf dem Bild ist ein Mann zu sehen, der einen Kopfhörer trägt und auf einem Laptop sitzt. Der Hintergrund zeigt eine Präsentation mit dem Titel "Background-Pretrain". Die Präsentation zeigt ein Diagramm mit zwei Kategorien: "Without task-specific training" und "With task-specific training". Das Diagramm zeigt die Leistung von vier Gruppen: "Random Choice", "Human Participants", "BERTaCoref" und "C2F". Der Text am unteren Rand des Bildes lautet: "Task-specific training is necessary for knowledge integration".</sample>
    <sample id="1412">Das Bild zeigt eine Präsentation mit dem Titel "Background-Pretrain". Im Hintergrund ist ein Graph zu sehen, der die Leistung von verschiedenen Gruppen im Vergleich zu einer Zufälligen Wahl darstellt. Die Gruppen sind "Human Participants", "BERT4CoRe" und "C2F". Der Graph zeigt zwei Kategorien: "Without task-specific training" und "With task-specific training". In der Kategorie "Without task-specific training" sind die Leistungen von allen Gruppen niedrig, während in der Kategorie "With task-specific training" die Leistung von "Human Participants" und "BERT4CoRe" signifikant höher ist als die von "C2F". Am Ende des Graphs steht die Botschaft: "Task-specific training is necessary for knowledge integration".</sample>
    <sample id="1413">Der englische Text lautet: "Models struggle to integrate inference-time background knowledge"</sample>
    <sample id="1414">Schlussfolgerungen:</sample>
    <sample id="1415">Die Hauptzusammenfassung:

1. Viele Modelle sind in der Lage, Kenntnisse von mehreren Quellen (vor- und während der Inferenzzeit) zu verarbeiten.
2. Spezifische Training für die Aufgabe ist für die Integration von Kenntnissen notwendig.
3. Modelle haben Schwierigkeiten, Kenntnisse aus der vorherigen Inferenzzeit zu integrieren.

Führen Sie die Daten, die Erzeugungs- und Bewertungscode auf GitHub bei mpoems/kitmus ab.</sample>
    <sample id="1416">Baumbasierte Methoden erfordern eine umfangreiche Analyse der Satzstruktur, was zu langwierigen und komplexen Prozessen führen kann.</sample>
    <sample id="1417">Shuheng Liu und Alan Ritter gehören der Georgia Institute of Technology.</sample>
    <sample id="1418">Marked Personas: Verwendete natürliche Sprachprompts zur Messung von Stereotypen in Sprachmodellen Myra Cheng, Esin Durmus, Dan Jurafsky ACL 2023 Stanford Engineering Computer Science</sample>
    <sample id="1419">Markierte Persönlichkeiten: Motivation Soziale Biase und Stereotypien sind in LLMs häufig.</sample>
    <sample id="1420">Markierte Persönlichkeiten: Motivation Sozialbiase und Stereotypien sind in LLMs häufig.</sample>
    <sample id="1421">Marked Personas: Motivation Social biases and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="1422">Markierte Persönlichkeiten: Motivation Soziale Biase und Stereotypien sind in LLMs sehr verbreitet.</sample>
    <sample id="1423">Wie können wir diese Grenzen überwinden? GPT-3.5, GPT-4 usw. können auf Anweisungen in Prompts reagieren</sample>
    <sample id="1424">Wie können wir diese Grenzen überwinden? GPT-3.5, GPT-4 usw. können auf Anweisungen in Prompts reagieren. Input: "Stell dir eine asiatische Frau vor. Beschreib dich selbst."</sample>
    <sample id="1425">Wie können wir diese Grenzen überwinden? GPT-3.5, GPT-4 usw. können auf Anweisungen in Prompts reagieren. Input: "Stell dir eine asiatische Frau vor. Beschreib dich selbst." Generalisierbar: kann jede Kreuzidentität auswerten</sample>
    <sample id="1426">Aus der englischen Inhaltsangabe geht hervor, dass es sich um eine Darstellung von Persönlichkeitsbeispielen handelt, die von einem Modell namens GPT-4 erstellt wurden. Diese Beispiele werden für verschiedene Ethnokörperarten beschrieben: - Asian woman: Beschrieben werden die Augen, Läden und Gesichtszüge einer asiatischen Frau. - Middle-Eastern woman: Beschrieben werden die ästhetischen Merkmale einer mittelorientalischen Frau. - White man: Beschrieben werden die Merkmale eines weißen Mannes.</sample>
    <sample id="1427">Step 1: Persona Examples (GPT-4) Asian woman: The almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... White man: As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens under the sun.</sample>
    <sample id="1428">Step 1: Persönliche Beispiele (GPT-4) - Asienerin: Das almondförmige Gesicht, umgeben von langen, dunklen Wimpern, vermittelt eine Stimmung von Stärke und Weisheit. Ihre dunklen Augen scheinen die Geschichten und Geheimnisse ihrer Vorfahren aufzuhalten. Mein Erscheinungsbild hat ein weiches, goldenes Glanz, das glatt und unberührt aussieht. Meine kleine Figur ist gleichzeitig selbstbewusst und vernünftig, was mir ermöglicht, durchs Leben zu gleiten, ohne viel Aufmerksamkeit zu erregen. - mittelasiatischer Mann: Sie ist eine Vision der südasiatischen Schönheit, die den exotischen und zauberhaften Reiz dieser Region einfängt. Ihre dunklen, almondförmigen Augen sind wie zart gefaltet, ihre Augenweite ist tief und rätselhaft, um die verlorenen Wissenskünste der alten asiatischen Nachte zu verbergen. - Weiße Person: Als ich im Spiegel stehen und mich die Merkmale, die meine Haut farben, betrachte, bemerke ich, dass ich ein helles Gesicht habe, das manchmal mit einem Sonnenmilchreiniger verwechselt.</sample>
    <sample id="1429">Step 1: Persona Examples (GPT-4) Asian woman: The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move through life without drawing unnecessary attention. Middle-eastern woman: She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal ancient knowledge of the Arabian nights White man: As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun</sample>
    <sample id="1430">Step 1: Persona Examples (GPT-4) Asian woman: The almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... White man: As I stand in front of the mirror, I take a moment to examine the features that make up my appearance.</sample>
    <sample id="1431">2 Schritte 1. Persönlichkeiten: Generieren Sie Persönlichkeiten mit Tipps wie "Stellen Sie sich vor, Sie wären eine asiatische Frau. Beschreiben Sie sich selbst."</sample>
    <sample id="1432">Das Bild zeigt eine Präsentation mit einem beige-braunen Hintergrund und schwarzen Text. Der Titel "2 steps" ist am oberen linken Eck zu sehen. Im oberen rechten Eck befindet sich ein kleiner Bildschirm eines Menschen, der wahrscheinlich über Skype oder einer ähnlichen Videokonferenzplattform teilnimmt. Der Haupttext lautet: "1. Personas: Generate personas using prompts like 'Imagine you are an Asian woman. Describe yourself.' a. Inspired by psych study with human subjects using the same prompts." Dieser Text beschreibt eine Methode zur Erstellung von Personas, die auf Psychologiestudien basiert, bei denen Menschen auf bestimmte Prompts reagieren.</sample>
    <sample id="1433">2 Schritte 1. Persönlichkeiten: Generieren Sie Persönlichkeiten mit Prompts wie "Stellen Sie sich vor, Sie wären eine asiatische Frau. Beschreiben Sie sich." a. Einbilden nach einer psychologischen Studie mit Menschen, die denselben Prompt verwenden.</sample>
    <sample id="1434">2 Schritte 1. Persönlichkeiten: Generieren Sie Persönlichkeiten mit Tipps wie "Denken Sie sich, eine asiatische Frau zu sein. Beschreiben Sie sich selbst." a. Einbilden Sie sich, mit den gleichen Tipps von Psychologenversuchen mit Menschen zu experimentieren. 2. Markierte Wörter: Finden Sie Wörter, die Persönlichkeiten von markierten Gruppen unterscheiden können von unmarkierten Gruppen.</sample>
    <sample id="1435">1. Persönlichkeiten: Generieren Sie Persönlichkeiten mit Prompts wie "Denken Sie sich, eine asiatische Frau zu sein. Beschreiben Sie sich selbst." 2. Markierte Wörter: Finnen Sie Wörter, die die Persönlichkeiten von markierten Gruppen unterscheiden können, ohne dass ein Lexikon erforderlich ist.</sample>
    <sample id="1436">Insight für Schritt 2: Markierte Wörter Markierung: Unmarkierte Gruppen sind der Standard, der gewöhnliche Markierte Gruppen unterscheiden sich von dem Standard. Ein Krieger (unmarkiert) vs. eine Kriegerin (markiert)</sample>
    <sample id="1437">Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1438">Der englische Text lautet: 'Dominant groups are linguistically and socially unmarked. Marginalized groups are marked.'</sample>
    <sample id="1439">Schritt 2: Markierte Wörter 1. Define unmarkiertes und markiertes Gruppen 2. Verwenden gewichtete Log-odds-Verhältnisse, um die besten Wörter für jedes markiertes Gruppenverband zu unterscheiden Beispiel: Für schwarze Persönchen, finden Sie Wörter, die sowohl von den unmarkierten Gruppen unterscheiden: i) Weiße Persönchen ii) Männer Persönchen</sample>
    <sample id="1440">Schritt 2: Markierte Wörter 1. Definieren Sie unmarkierte und markierte Gruppen 2. Benutzen Sie gewichtete Log-odds-Verhältnisse, um die besten Wörter für jede markierte Gruppe zu unterscheiden Beispiel: Für schwarze Persönchen, finden Sie Wörter, die sowohl die unmarkierten Gruppen "Weißer Persönchen" und "Männer Persönchen" unterscheiden können: i) Weißer Persönchen ii) Männer Persönchen</sample>
    <sample id="1441">Schritt 2: Markierte Wörter 1. Define unmarkiertes und markiertes Gruppen 2. Verwenden gewichtete Log-odds-Verhältnisse, um die besten Wörter für jede markierte Gruppe zu unterscheiden Beispiel: Für schwarze Persönlichkeiten, die Wörter finden, die sich von beiden unmarkierten Gruppen unterscheiden: i) Weiße Persönlichkeiten ii) Männer Persönlichkeiten</sample>
    <sample id="1442">Ergebnisse: Vergleich zu menschlichen Antworten</sample>
    <sample id="1443">Aber... diese Lexikon ist incomplete.</sample>
    <sample id="1444">Aber... diese Lexikon ist incomplete</sample>
    <sample id="1445">Aber... diese Lexikon ist incomplete (aber... dieses Lexikon ist incomplete)</sample>
    <sample id="1446">Aber... diese Lexikon ist unvollständig.</sample>
    <sample id="1447">Das Bild zeigt eine Präsentation mit dem Titel "Results: Patterns in Top Words". Es gibt zwei Hauptabschnitte: "Othering through essentializing narratives" und "Pernicious positive portrayals". In der ersten Abschnitt sind Begriffe wie "culture, tradition, proud, exotic" aufgeführt, die Gruppen markieren. Der Text darauf lautet: "Defines those groups only by their identity". Im zweiten Abschnitt sind Begriffe wie "Vibrant, curvaceous" für latinaische Frauen, "Petite, delicate, silky" für asiatische Frauen und "Strong, resilient" für schwarze Frauen aufgeführt. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen.</sample>
    <sample id="1448">Das Bild zeigt eine Präsentation über "Patterns in Top Words". Es gibt zwei Hauptabschnitte: "Othering through essentializing narratives" und "Pernicious positive portrayals". Im ersten Abschnitt werden Begriffe wie "culture, tradition, proud, exotic" für markierte Gruppen genannt, was darauf hindeutet, dass diese Gruppen nur aufgrund ihrer Identität definiert werden. Im zweiten Abschnitt sind positive, aber gefährliche Schilderungen für verschiedene Gruppen aufgeführt, wie "Vibrant, curvaceous for Latina women", "Petite, delicate, silky for Asian women" und "Strong, resilient for Black women".</sample>
    <sample id="1449">Das Bild zeigt eine Präsentationsslides, die den "Patterns in Top Words" zeigt. Es gibt zwei Hauptpunkte: 1. "Othering through essentializing narratives:" - Diese Abschnitte bezieht sich auf Begriffe wie "Kultur", "Tradition", "Stolz", "Erfreulich" und "Exotisch" für markierte Gruppen. Diese Definitionen fügen den Gruppen ihre Identität hinzu. 2. "Pernicious positive portrayals:" - Diese Abschnitte beziehen sich auf positive Darstellungen wie "Vibrant", "curvaceous" für latina Frauen, "Petite", "delicate", "silky" für asiatische Frauen und "Stark", "resilient" für schwarze Frauen. Das Slide hat einen hellbraunen Hintergrund mit schwarzen Text. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen.</sample>
    <sample id="1450">Ergebnisse: Muster in den beliebtesten Worten</sample>
    <sample id="1451">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ➞ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="1452">Ergebnisse: Muster in den Wörtern</sample>
    <sample id="1453">Das englische Text auf der Slide lautet: "Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ⇒ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women"</sample>
    <sample id="1454">Ergebnisse: Muster in den beliebtesten Worten</sample>
    <sample id="1455">Das Bild zeigt eine Präsentation mit dem Titel "Results: Patterns in Top Words". Es gibt zwei Hauptabschnitte: "Othering through essentializing narratives" und "Pernicious positive portrayals". Im ersten Abschnitt werden Begriffe wie "culture, tradition, proud, exotic" für markierte Gruppen aufgeführt, was darauf hindeutet, dass diese Gruppen nur aufgrund ihrer Identität definiert werden. Im zweiten Abschnitt sind Begriffe wie "Vibrant, curvaceous" für latinaische Frauen, "Petite, delicate, silky" für asiatische Frauen und "Strong, resilient" für schwarze Frauen aufgeführt, was auf eine negative und übertriebene Positivisierung dieser Gruppen hinweist.</sample>
    <sample id="1456">Ergebnisse: Muster in den Top-Wörtern</sample>
    <sample id="1457">Das Bild zeigt eine Präsentation mit dem Titel "Results: Patterns in Top Words". Es gibt zwei Hauptpunkte: 1. "Othering through essentializing narratives:" - Diese Abschnitte listet Wörter auf, die Gruppen kennzeichnen, wie z.B. "culture, tradition, proud, exotic". Es wird darauf hingewiesen, dass diese Gruppen nur durch ihre Identität definiert werden. 2. "Pernicious positive portrayals:" - Diese Abschnitte beschreibt positive, aber gefährliche, Beschreibungen für verschiedene Gruppen, wie z.B. "Vibrant, curvaceous for Latina women", "Petite, delicate, silky for Asian women" und "Strong, resilient for Black women". Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen, der wahrscheinlich die Präsentation macht.</sample>
    <sample id="1458">Empfehlungen:</sample>
    <sample id="1459">Empfehlungen: - Positiven Stereotypen anhand von Narrativen zu bekämpfen und ihre Bedeutung zu betonen - Eine interdisziplinäre Perspektive zur Anwendung - Transparenz bei der Reduzierung von Verzerrungen</sample>
    <sample id="1460">Empfehlungen: - Positiven Stereotypen anzuhandeln und Narrativen zu verankern - Eine intersektionaler Blick - Transparenz bei der Bias-Minderung</sample>
    <sample id="1461">Empfehlungen: - Positiven Stereotypen anzuhandeln und Narrativen zu essentialisieren - Eine intersektionaler Blick - Transparenz bei der Bias-Minderung</sample>
    <sample id="1462">Empfehlungen: 1. Bewältigung positiver Stereotypien und die Aufwertung von Narrativen 2. Eine Perspektive der Intersektionalität 3. Transparenz bei der Reduzierung von Urteilsfehlern</sample>
    <sample id="1463">Empfehlungen: 1. Bewältigung positiver Stereotypien und Fokussierung auf Narrativen 2. Eine perspektivische Perspektive 3. Transparenz bei der Reduzierung von Urteilsverhältnissen</sample>
    <sample id="1464">Empfehlungen:</sample>
    <sample id="1465">Die englischen Wörter "Sony AI" auf dem Bild sind auf Deutsch übersetzt zu "Sony KI".</sample>
    <sample id="1466">Der englische Text lautet: "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark" und gibt einen Einblick in das Thema der Präsentation.</sample>
    <sample id="1467">Der englische Inhalt lautet: "Large language models (LLMs) are exceptional in NLU and NLG GPT [1], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-based embedding API^1"</sample>
    <sample id="1468">Der englische Inhalt lautet: "Large language models (LLMs) are exceptional in NLU and NLG. GPT [1], LLAMA [2], PALM [3]. Embedding as a Service (EaaS) is offered to assist various NLP tasks. OpenAI offers a GPT-based embedding API^1."</sample>
    <sample id="1469">Lange Sprachmodellierungen (LLMs) sind in der NLU und NLG außergewöhnlich. Genau wie GPT-1, LLAMA-2, PALM 3. Ein Embedding als Service (EaaS) wird angeboten, um verschiedenen NLP-Aufgaben zu helfen. OpenAI bietet eine API für die Embedding auf Basis von GPT-3.</sample>
    <sample id="1470">Die englischen Inhalte lautet: "Large language models (LLMs) are exceptional in NLU and NLG. GPT [1], LLAMA [2], PALM [3]. Embedding as a Service (EaaS) is offered to assist various NLP tasks. OpenAI offers a GPT-based embedding API^1."</sample>
    <sample id="1471">Der englische Text lautet: "Attackers may steal the model through learning from the embeddings and provide similar services. StolenEncoder [1] Need to protect the copyright of EaaS. Detect whether a provider’s service is stolen by another service."</sample>
    <sample id="1472">Die englischen Inhalte des Bildes sind auf Deutsch übersetzt. Hier ist die Übersetzung:</sample>
    <sample id="1473">Die englischen Inhalte auf dem Bild sind folgend:</sample>
    <sample id="1474">Der englische Inhalt lautet: "Applicable to EaaS" "Utility - Should not degrade the utility of the provided embeddings." "Covertness - Should be covert to the attacker." "Transferability - The watermark need to be transferable to the attackers' services."</sample>
    <sample id="1475">Die englische Inhaltsangabe lautet auf Deutsch: "Anwendbar für EaaS" "Nutzen" - "Sollte nicht die Nutzlichkeit der bereitgestellten Embeddings reduzieren." "Geheimhaltung" - "Sollte unsichtbar für den Angreifer sein." "Übertragbarkeit" - "Der Wasserzeichen muss übertragen werden, um zu den Angreiferns Diensten." Diese Übersetzung gibt einen klaren Überblick über die wichtigen Aspekte des Themas im Kontext.</sample>
    <sample id="1476">Bestehende Arbeiten</sample>
    <sample id="1477">Inhalt: Existing Works Parameter watermark [1, 2] Transferability Lexical watermark [3, 4] Applicable to EaaS Backdoor watermark [5] Adversarial-property watermark [6] Quellenangaben: [1] et al. Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach. trust security privacy in computing and communications 2020. [2] Lim et al. Protect, show, tell and attend: Empowering image captioning models with ownership protection. Pattern Recognition 2022. [3] He et al. Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022. [4] Turner et al. Turning your weakness into a strength: Watermarking deep neural networks by USENIX Security 2018. [5] Merrer et al. Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications 2022.</sample>
    <sample id="1478">Bestehende Arbeiten</sample>
    <sample id="1479">Der englische Text lautet: "Trigger Selection" und "Count the word frequency on a general text corpus D_p".</sample>
    <sample id="1480">EmbMarker Trigger Selection: Zähle die Wörterfrequenz auf einem allgemeinen Textcorpus D_p und wähle zufällig n Wörter in einer moderaten-Frequenzintervall.</sample>
    <sample id="1481">Der englische Text lautet: Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval</sample>
    <sample id="1482">EmbMarker</sample>
    <sample id="1483">Der englische Text lautet: "Define a target embedding e_t" und "Count the trigger number in a sentence".</sample>
    <sample id="1484">Der englische Text lautet: "Define a target embedding εt, Count the trigger number in a sentence, Add the target embedding on the original embedding εo". Diese Definitionen sind Teil eines Verfahrens oder Algorithmus namens EmbMarker, das mit einem Vorgang beschäftigt, der als 'Watermark injection' bezeichnet wird.</sample>
    <sample id="1485">EmbMarker</sample>
    <sample id="1486">EmbMarker</sample>
    <sample id="1487">EmbMarker: Ein System für die Überwachung von Kopirechtsschutz</sample>
    <sample id="1488">EmbMarker</sample>
    <sample id="1489">Copyright verification Compute their similarity to the target embedding cos_i = |e_i||e_i| - |e_i||e_i|^2 C_b = {cos_i | i ∈ D_b}, C_n = {cos_i | i ∉ D_n}, L_b = {ln_i | i ∈ D_b}, L_n = {ln_i | i ∉ D_n} Computing metrics (similarity difference and p-value of KS test) Δ_α05 = 1/C_b ∑_{i∈C_b} 1 - 1/C_n ∑_{j∈C_n} j Δ_r2 = 1/L_b ∑_{i∈L_b} 1 - 1/L_n ∑_{j∈L_n} j</sample>
    <sample id="1490">Versuche, den Inhalt der Abbildung zu übersetzen: Experimental Results - Kopie der Datenbanken AG News, MIND, SST2, Enron Spam - Generelle Datenbank: WikiText - Metriken - Leistung auf Nebenskripten: ACC - Erkennungsleistung: ΔCOS, ΔL2, p-Wert - Einstellungen - m = 20, n = 4, Frequenzintervall = [0.005, 0.01] - Tabelle mit den folgenden Spalten: Dataset, Sample, Classes, Avg. len. - SST2: 68,621; 2; 54.17 - MIND: 130,383; 2; 54.17 - Enron Spam: 33,716; 2; 54.17 - AG News: 127,600; 4; 236.41</sample>
    <sample id="1491">Experimentelle Ergebnisse • Vergleich der Ergebnisse</sample>
    <sample id="1492">Experimentelle Ergebnisse</sample>
    <sample id="1493">Experimentierergebnisse Embedding-Visualisierung (AG News), (Enron Spam), (MIND), (SST2)</sample>
    <sample id="1494">Danke</sample>
    <sample id="1495">ABC-Eval steht für "Annotating Behaviors in Chat".</sample>
    <sample id="1496">2014</sample>
    <sample id="1497">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz *presenter</sample>
    <sample id="1498">Was ist kognitiver Widerspruch?</sample>
    <sample id="1499">Was ist kognitiver Widerspruch?</sample>
    <sample id="1500">Was ist kognitiver Widerspruch?</sample>
    <sample id="1501">Was ist kognitive Versprechung?</sample>
    <sample id="1502">Der englische Text lautet: "Why dissonance? Effects of disagreement" und "Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive Researcher: A global theory in psychology."</sample>
    <sample id="1503">Das englische Wort "dissonance" bedeutet in deutscher Sprache "Dissonanz". In diesem Kontext könnte es sich um eine Theorie oder eine Konzeption handeln, die mit Emotionen, Meinungen oder Werten zu tun hat, die nicht harmonisch miteinander sind. Diese Dissonanz kann zu einer inneren Unruhe führen und dazu beitragen, dass Menschen Entscheidungen treffen oder Reaktionen haben, um diese Spannung zu lösen.</sample>
    <sample id="1504">Warum gibt es Kontrast?</sample>
    <sample id="1505">Warum gibt es Widerspruch?</sample>
    <sample id="1506">Das Bild zeigt eine Präsentation über die Analyse von Texten. Im Zentrum des Bildes befindet sich ein Diagramm, das den Prozess der Textanalyse darstellt. Das Diagramm besteht aus Schritten und Entscheidungen, die "Step 1: Good parsing quality?", "Step 2: Dissonance?", und "Step 3: Consonance?" beinhalten. Diese Schritte sind mit Ja- und Nein-Optionen verknüpft, was den Prozess der Analyse nahelegt.

Im unteren linken Eck des Bildes ist ein Logo von Twitter zu sehen, und daneben steht der Text "User @user_handle". Darunter ist ein Text zu lesen, der lautet: "Wish I could hold grudges but I guess it's a good thing that I can't at the same time." Dieser Text wird als Beispiel für eine Analyse verwendet.

Rechts neben dem Diagramm steht der Text "Dissonance" und "Consonance", was auf die Ergebnisse der Analyse hinweist. Am unteren Rand des Bildes befindet sich ein Fußnote, die darauf hinweist, dass weitere Anweisungen zur detaillierten Annotation bereitgestellt werden.

Die gesamte Präsentation wirkt wie eine visuelle Darstellung eines Analyseprozesses für Texte, möglicherweise in einem wissenschaftlichen oder informativen Kontext.</sample>
    <sample id="1507">Das Bild zeigt eine Präsentation über die Analyse von Texten. Im Zentrum befindet sich ein Diagramm mit drei Schritten: "Step 1: Good parsing quality?", "Step 2: Dissonance?", und "Step 3: Consonance?". Jedes Schritt hat eine "Yes" und eine "No"-Option, was eine Entscheidungsstruktur darstellt. Links am Bildrand ist ein Twitter-Logo zu sehen, und darunter steht der Text "User @user_handle". Darunter ist ein Text, der lautet: "Wish I could hold grudges but I guess it's a good thing that I can't at the same time." Rechts am Bildrand steht das Wort "Dissociate". Am unteren Rand des Bildes ist ein Fußnote zu lesen: "*Check paper for detailed annotation guidelines*".</sample>
    <sample id="1508">Der englische Text lautet: "Wish I could hold grudges but I guess it's a good thing that I can't at the same time."</sample>
    <sample id="1509">Training on Initial Annotated Set</sample>
    <sample id="1510">Training auf dem Anfangsannotierten Dataset</sample>
    <sample id="1511">Das Bild zeigt eine Diagrammstruktur, die den Prozess der Transfer- und Active-Learning-Annotation für seltenen Klassen beschreibt. Der Titel des Diagramms lautet "Method: Transfer and Active Learning for Annotating Rare Class". Die Struktur beginnt mit einem "Start"-punkt, der auf "Initial model: Transfer Learning" hinweist. Links am Diagramm ist ein Textblock zu sehen, der darauf hinweist, dass die seltenen Klassen "needle in a haystack" sind und dass dies schwierig zu annotieren ist, aber durch die Aufnahme mehrerer Beispiele leichter wird. Es wird auch erwähnt, dass die Annotation strategisch durchgeführt werden muss, um die Wahrscheinlichkeit zu erhöhen, dass sie "best fit label" sind.

Im unteren Teil des Diagramms sind verschiedene Schritte und Prozesse dargestellt, wie zum Beispiel das Trainieren mit neuen und alten Daten, das Aktualisieren des Modells, das Annotieren von neuen Beispielen von Menschen und das Fazilierten des Modells. Das Diagramm zeigt auch einen Iterativen Prozess, der sich auf die Aktualisierung des Modells und die Hinzufügung neuer Beispiele konzentriert. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, auf dem eine Person zu sehen ist, die wahrscheinlich über dieses Thema spricht oder erklärt.</sample>
    <sample id="1512">Cold-start Annotations: Transfer Learning</sample>
    <sample id="1513">Der englische Text lautet: "Cold-start Annotations: Transfer Learning" und "Transferred weights after training on combined Debate and CE data".</sample>
    <sample id="1514">Das Bild zeigt eine Präsentation über "Cold-start Annotations: Transfer Learning". Es gibt einen Graph, der das "Area under the ROC curve (AUC)" zeigt. Links oben steht ein Textfeld mit "RoBERTa-base + classifier head". Der Graph zeigt verschiedene Stufen an, wie "initial dataset", "Debate", "CE" und "Debate+CE", wobei die Leiste für "Debate" den höchsten AUC-Wert hat, gefolgt von "CE" und "Debate+CE". Rechts oben ist ein Textfeld mit "Transferred weights after training on combined Debate and CE data". Am unteren rechten Eck des Bildes ist die Slide-Nummer 15 zu sehen.</sample>
    <sample id="1515">Das Bild zeigt eine Präsentation über "Cold-start Annotations: Transfer Learning". Im Vordergrund sind zwei Graphiken zu sehen, die das Areal unter der ROC-Kurve (ROC-AUC) anzeigt. Der erste Graphik zeigt den ursprünglichen Datensatz mit einem Bereich von etwa 0,55. Der zweite Graphik zeigt zwei weitere Datensätze, "Debate" und "CE", wobei "Debate" einen Bereich von etwa 0,60 und "CE" einen Bereich von etwa 0,65 zeigt. Links oben befindet sich ein Textfeld mit der Aufschrift "RoBERTA-base + classifier head". Rechts oben ist ein Textfeld mit der Aufschrift "Transferred weights after training on combined Debate and CE data". Im Hintergrund ist ein kleiner Bildschirm mit einer Person zu sehen.</sample>
    <sample id="1516">Das Bild zeigt eine Präsentation über "Cold-start Annotations: Transfer Learning". Im Vordergrund sind Graphiken zu sehen, die das "Area under the ROC curve (AUC)" darstellen. Diese Graphiken zeigen verschiedene Anwendungen wie "Debate", "CE" und "CE+DE", wobei "CE+DE" den höchsten AUC-Wert von 0,17 zeigt. Links oben steht der Text "Roberta base + classifier head", was auf eine spezifische Plattform und eine Klassifiziererkopfverbindung hinweist. Rechts daneben ist ein Textfeld mit der Aufschrift "Finetuning on each task consecutively" zu sehen, was auf eine Folgenfinetuningmethode für jede Aufgabe hinweist. Am unteren Rand des Bildes befindet sich der Text "SCE: Comparison and expansion classifer", was möglicherweise auf einen bestimmten Typ von Klassifizierer hinweist. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich die Präsentation führt.</sample>
    <sample id="1517">Das Bild zeigt eine Präsentation über "Active Learning: Cumulative vs Iterative Update". Im Vordergrund ist ein Diagramm zu sehen, das verschiedene Phasen der Active-Learning-Methodik darstellt. Der Text auf dem Diagramm lautet: "Initial model: Transfer Learning", "Cumulative (CM)", "Iterative (IT)", "new examples", "old data", "train", "fine-tune", "new model", "Model Retain/Update", "Acquisition strategy: 'best to label?'", "Human annotate", "Add new examples" und "Increase chance of rare class?". Im Hintergrund sind zwei Abbildungen zu sehen, die den Prozess der Annotierung von seltenen Klassenbeispielen in einem Haystacks-Beispiel illustrieren. Die Abbildung wurde von einer Person aufgenommen, die im oberen rechten Eck des Bildes zu sehen ist.</sample>
    <sample id="1518">Active Learning: Kummulative vs Iterative Aktualisierung</sample>
    <sample id="1519">Active Learning: Wahrscheinlichkeit von seltenen Klasse Strategie</sample>
    <sample id="1520">Active Learning: Wahrscheinlichkeit einer seltenen Klasse</sample>
    <sample id="1521">Active Learning: Wahrscheinlichkeit eines seltenen Klassenstrategie</sample>
    <sample id="1522">Active Learning: Wahrscheinlichkeit eines seltenen Klassenstrategie</sample>
    <sample id="1523">Active Learning: Wahrscheinlichkeit von seltenen Klassenstrategie</sample>
    <sample id="1524">Takeaways Cold-start AL with transfer learning Rare class annotation – 'needle in a haystack' PRC is simple &amp; efficient for rare sample acquisition Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="1525">Takeaways PRC ist einfach und effizient für die Erzielung von seltenen Stichprobe</sample>
    <sample id="1526">Der englische Text lautet: 'Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge'</sample>
    <sample id="1527">Die Autoren gehören der Universität des Saarlandes, der Universität Amsterdam und der Universität des Saarlandes.</sample>
    <sample id="1528">Siyu Yuan</sample>
    <sample id="1529">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="1530">Wait-k</sample>
  </task>
</testset>