<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Web crawl data, political news media</sample>
    <sample id="1">McGill University, Mila and Microsoft Research</sample>
    <sample id="2">Hallo, willkommen bei unserer Präsentation von DeepLing, einem neuen Korpus für die Textidentifizierung auf der Dokumenten- und Satellenebene.</sample>
    <sample id="3">Mein Name ist Regina Stöcken und ich werde Sie durch den ersten Teil der Präsentation leiten. Lassst uns zunächst Textsimplifizierung definieren.</sample>
    <sample id="4">Textamplifizierung ist der Prozess der Anpassung eines Textes, um die Verständnisfähigkeit von Texten für eine bestimmte Zielgruppe zu verbessern, wie Menschen mit Leseschwierigkeiten oder nicht-native Sprecher.</sample>
    <sample id="5">Um ein Textidentifikation-Modell zu trainieren, benötigen wir mehrere Paare von Texten, zum Beispiel Dokumente oder Sätze.</sample>
    <sample id="6">Im Beispiel hier ist ein komplexer deutscher Satz und seine Übersetzung in plain language zu sehen.</sample>
    <sample id="7">Um die Satzeinführung zu vereinfachen, gibt es verschiedene Techniken, wie Sie im Beispiel sehen können, wie z.B.</sample>
    <sample id="8">Wir haben nun einen neuen Kooperationsplan vorgeschlagen, weil in den letzten Jahren einige Probleme mit den bestehenden Korporen aufgetreten sind. So zum Beispiel sind diese Korporen hier zu klein, um ein Textechnifizierungsmodell zu trainieren.</sample>
    <sample id="9">Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass es in der Regel Fehler in ihren Ausrichtungen geben kann.</sample>
    <sample id="10">D-Plain ist daher ein neues Korpus, das in zwei Unterkorpora, D-Plain APA und D-Plain Web, aufgeteilt ist. D-Plain APA basiert auf Nachrichtenartikeln.</sample>
    <sample id="11">In DeepL-Plain-APA, we aligned 483 documents all manually. It results in roughly 30,13,000 parallel sentence pairs.</sample>
    <sample id="12">Für die plainweb diese Körpers umfasst verschiedene Bereiche und wir alignen auch alle diese 750 Dokumente auf der einen Seite manuell und auf der anderen mit automatischen Alignment Methoden.</sample>
    <sample id="13">Insgesamt resultiert es in 30.450 Paaren von Sätzen.</sample>
    <sample id="14">Wir analysierten unsere Sätze ein bisschen mehr, zum Beispiel auf die Art der Übersetzung.</sample>
    <sample id="15">Wie du sie hier sehen kannst, sind die Bibeltexte viel einfacher als die Nachrichten- oder Lernsprachtexte.</sample>
    <sample id="16">An allgemein auf allen Ebenen, wie zum Beispiel bei der Lexikalisierung, Strukturisierung und allgemeinen Isolierung.</sample>
    <sample id="17">Darüber hinaus können Sie sehen, dass unsere dplane-Korpus eine Vielzahl unterschiedlicher Abwertungsveränderungen aufweist. So zum Beispiel haben wir im dplane-APICorpus viel mehr Neuordnungen und Wortzusatz als im dplane-Webcorpus.</sample>
    <sample id="18">Auf der anderen Seite haben wir im Webcorpus viel mehr Wiederformulierungen.</sample>
    <sample id="19">Lass uns jetzt sehen, was wir mit diesem Korpus tun können.</sample>
    <sample id="20">In den letzten Jahren wurden viele Alignment-Methoden entwickelt, insbesondere im Zusammenhang mit maschinellen Übersetzungen.</sample>
    <sample id="21">In diesem Fall können Sie die folgenden Schritte ausführen:</sample>
    <sample id="22">Aber in unserem Szenario versuchen wir, Ähnlichkeiten zwischen den Sätzen zweier paralleler Dokumente zu extrahieren, die denselben Sprache und den gleichen Inhalt haben, aber auf einer unterschiedlichen Komplexitätsebene liegen.</sample>
    <sample id="23">Und jetzt, da wir unsere Datensatz-Deplane haben, der manuelle Anordnung der Sätze enthält, können wir diese Sätze als Goldstandards für die Anordnung verwenden, um einige der vorgeschlagenen Anordnungsverfahren zu evaluieren.</sample>
    <sample id="24">Wir haben einige Anpassungen am vorgeschlagenen Methoden und veröffentlicht alle diese Anpassungen und die Codes, um unsere Experimente in der Publikation auszuführen.</sample>
    <sample id="25">Am Ende haben wir festgestellt, dass die beste Automatic Alignment Methode für die vereinfachte deutsche Textausgabe die Methode von MassAlign ist.</sample>
    <sample id="26">Sie können das Code für die Ausführung dieser Methode auf Ihren eigenen Dokumenten im Papier finden.</sample>
    <sample id="27">Das zweite Anwendungsbeispiel, das wir in unserem Papier zeigen, ist ein Fall von automatischer Texteinfachung.</sample>
    <sample id="28">By fine-tuning language models to produce simplified text from complex input texts.</sample>
    <sample id="29">Wir haben zwei Modelle feinabgestimmt. Wir haben das LLM-Modell für die Erstellung von Dokumentenabschnitte feinabgestimmt.</sample>
    <sample id="30">Wir haben auch den normalen Basis-Endteil an die Produktionsstufe des Satzes angepasst.</sample>
    <sample id="31">Sie können auch alle Checkpoints finden und im Detail in die Punkte und die Bewertungsmaßstäbe unserer Experimente im Papier schauen.</sample>
    <sample id="32">Wir haben erkannt, dass diese grundlegende Feinabstimmung</sample>
    <sample id="33">Und wir stellen diese Ergebnisse als eine Leistungs- oder Baseline für die Problemstellung der automatischen Texteinfachung in Zukunft vor.</sample>
    <sample id="34">Vielen Dank für Ihre Aufmerksamkeit und hoffen, Sie alle während der Konferenz zu treffen.</sample>
    <sample id="35">Kyle Yan</sample>
    <sample id="36">T5-XLARGE</sample>
    <sample id="37">CoNLL-2003-Tagger funktioniert noch.</sample>
    <sample id="38">Das neue Ansatz für die menschliche Bewertung versucht, die Subjektivität menschlicher Bewertungen zu reduzieren. Er tut dies, indem er jeder Modellantwort explizit kennzeichnet, ob sie bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel eine Irrelevanz oder Widersprüche.</sample>
    <sample id="39">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Verfügbarkeit und Qualität der Validationssamples ab.</sample>
    <sample id="40">The result can be improved by showing the alternative question to the annotators.</sample>
    <sample id="41">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="42">Hallo, mein Name ist Adam Skorowski und dieser Vortrag handelt über die Abhängigkeitsstruktur der Koordination.</sample>
    <sample id="43">Als ihr wisst, gibt es unterschiedliche Abhängigkeitsstrukturen, die von verschiedenen Theorien und Körperschreibungen verwendet werden. So zum Beispiel in der Universal dependency sind die Struktur der Koordinationskoordinaten Lisa, Bart und Maggie</sample>
    <sample id="44">Es ist so, dass der erste Conjunkt der Kopf der gesamten Koordinatenstruktur ist, also in diesem Fall Lisa.</sample>
    <sample id="45">In Igore Milčuk's theory of meaning, the whole coordinate structure is headed by the first conjunct.</sample>
    <sample id="46">Es gibt auch symmetrische Ansätze zur Koordinatenstruktur, wie das Prag-Approach, das Konjunktionshead-Approach, das in Prag-Dependenz-Baumortakten verwendet wird, bei dem Koordinatenstrukturen durch den Konjunktionskopf angeführt werden.</sample>
    <sample id="47">Also, wir bekommen einige Abhängigkeiten von "und" zu allen Konjunkturen.</sample>
    <sample id="48">Schließlich gibt es auch ein vielschichtiges Ansatz, das zum Beispiel im Cutson-Wordgramm verwendet wird.</sample>
    <sample id="49">Sag, dass alle Verbindungen im Kopf der Koordinatenstruktur stehen. Also erhalten wir Abhängigkeiten vom Gegenstand "Hier lacht" bis zu allen Verbindungen "Lisa Bartons Macht".</sample>
    <sample id="50">Das Hauptziel dieser Arbeit ist es, eine neue Argumentation für symmetrische Koordinationsstrukturen zu präsentieren und gegen asymmetrische Koordinationsstrukturen wie diese zu argumentieren.</sample>
    <sample id="51">Okay, das Argument basiert auf dem Prinzip der Abhängigkeitsminimierung, das ich aufgrund dieser Beispiele erklären werde.</sample>
    <sample id="52">In English, as you might know, direct objects prefer to be close to the verb while adjuncts may be further away.</sample>
    <sample id="53">While "March read yesterday it" is much worse, right? Because here between the verb and the direct object there's an adjunct "yesterday".</sample>
    <sample id="54">Dieses Phänomen kann jedoch verbessert werden, wenn der direkte Gegenstand sehr schwer und sehr lang ist, weil er dann nach dem Adjunkt in die Position verschoben werden kann.</sample>
    <sample id="55">Dies wird hier illustriert. Beide Sätze sind in Ordnung. "Maddox hat dieses interessante Buch über die BCS gestern" ist okay, aber anstatt "es" haben wir diese lange NP.</sample>
    <sample id="56">Aber es ist auch okay, gestern Marge Radys "Dieses absolut faszinierende Buch über Bienen" zu lesen.</sample>
    <sample id="57">Die Grundlage hier ist, dass dies möglich ist, obwohl diese Satz die allgemeine grammatikalische Prinzip verletzt, dass direkte Objekte neben dem Verben stehen sollten.</sample>
    <sample id="58">Es erfüllt das Prinzip der Längenminimierung von Abhängigkeiten, das sagt, dass kürzere Abhängigkeiten bevorzugt werden.</sample>
    <sample id="59">Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, also diejenigen, die zwischen diesen beiden Strukturen nicht konstant sind.</sample>
    <sample id="60">Hier haben wir eine Abhängigkeit von "red" auf die Nebensatzlänge von 7, gemessen in Worten, und von "red" auf den Buchtitel mit Länge 4.</sample>
    <sample id="61">Wenn Sie diese beiden Bestandteile wechseln, wird die Summe dieser beiden Abhängigkeiten zu 6. Stattdessen 11, 6 viel kürzer, daher klingt es ziemlich gut. Es verletzt einen Prinzip, aber er erfüllt ein anderes.</sample>
    <sample id="62">Okay, so what we did? We extracted various statistics from about coordination from the enhanced version of bench of the bench she bank and see the paper why wouldn't use universal dependencies.</sample>
    <sample id="63">Und diese Statistiken bestätigen die Beobachtung, die viele Male vorher gemacht wurde, dass linke Konjunktionen allgemein kürzer sind - also Salz und Pfeffer, nicht Pfeffer und Salz, gemessen in Sylables.</sample>
    <sample id="64">Und auch die Beobachtung, die in der Rede gemacht wurde, dass diese Tendenz mit der Längenunterschied im Längenverhältnis wächst.</sample>
    <sample id="65">Also, wenn die Länge der beiden Konjunkte wächst, bevorzugt der kurze Konjunkt den ersten.</sample>
    <sample id="66">Aber wasnovell in diesem Papier ist, dass wir beobachtet haben, dass diese Tendenz nur dann auftritt, wenn der Regierungsbereich links leer ist.</sample>
    <sample id="67">Recht, so der Gouverneur ist auf der linken Seite in diesem Beispiel. Ich sehe Bob und Lisa. Der Gouverneur ist auf der linken Seite.</sample>
    <sample id="68">Es ist in dem zweiten Beispiel, "Homer kamen und schnurrte", nicht vorhanden. Hier haben wir Koordination von zwei Verben und es gibt keinen externen Regierenden, der außerhalb steht, richtig? In solchen Fällen bevorzugt die linke Verbverbend, die kürzer zu sein, wenn die Unterschiede zwischen den beiden Verbverbenden größer sind.</sample>
    <sample id="69">Allerdings verschwindet dieser Effekt, wenn der Regierende auf der rechten Seite ist, wie hier links, der die Koordination über den Internet verwalten muss.</sample>
    <sample id="70">Wir haben gezeigt, dass es durch die Messung der Länge in Buchstaben (erstes Spalte), in Syllaven (Zentrum) und in Worten (rechts) geht.</sample>
    <sample id="71">Was wir hier sehen, ist, dass, wenn der Goven auf der linken Seite ist</sample>
    <sample id="72">Die Neigung, dass der linke Konjunkt kürzer ist, wächst mit der absoluten Differenz in Worten und ähnelt demselben, wenn es keinen Regisseur gibt wie in koordinierten Sätzen, aber wenn der Regisseur auf der rechten Seite ist, verschwindet diese Neigung.</sample>
    <sample id="73">Und wir zeigen im Papier, wie dies gegen schießs asymmetriche Strukturen der Koordination als diese beiden und für symmetrische Strukturen als diese beiden argumentiert.</sample>
    <sample id="74">So, siehe das Papier für die vollständige Vereinbarung und -Argumente und sprich mit uns im Poster-Sitz. Vielen Dank.</sample>
    <sample id="75">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="76">Bible texts</sample>
    <sample id="77">Salt and pepper, not pepper and salt.</sample>
    <sample id="78">Ja, du kannst die Modelle für deine Forschung verwenden.</sample>
    <sample id="79">DEplain-apa is based on news texts.</sample>
    <sample id="80">Faktoren, die zu einer guten Generalisierung führen, sind eine bessere Modellarchitektur, ein größeres Modellgröße und mehr fine-tuning-Beispiele.</sample>
    <sample id="81">Die Tendenz zu kürzeren linken Konjunktionen wurde gemessen, indem man die Länge in Buchstaben (erste Spalte), in Syllaven (zweite Spalte) und in Worten (dritte Spalte) gemessen hat.</sample>
    <sample id="82">Die Experimente wurden so gestaltet, dass sie die Auswirkungen der Position des Begrenzers untersuchten.</sample>
    <sample id="83">A Baseline Classifier trained on imbalanced data performs poorly.</sample>
    <sample id="84">There are 3 authors involved in this work.</sample>
    <sample id="85">Bob and Alice.</sample>
    <sample id="86">Formality and lexical cohesion</sample>
    <sample id="87">John Bock here, Aaron Mueller, Kanishka Mishra, Karen Fintel, Roger Levy, and Athena Villoid.</sample>
    <sample id="122">Das Framework quantifiziert die Positionalität mithilfe der Pearson's R Korrelationsscore.</sample>
    <sample id="155">Die vorherige Studie hat gezeigt, dass menschliche Teilnehmer Rassistische Stereotypien durch die Verwendung ähnlicher Persönlichkeitsprompts aufgespielt haben.</sample>
    <sample id="156">Die Datenquellen für diese Studie waren die Enhanced Version des Penn Treebank und die Paper "Why wouldn't we use universal dependencies?"</sample>
    <sample id="157">Two authors are involved in this work.</sample>
    <sample id="158">Topic independent dissonance stance classification, debate and binary classification of expansion and comparison classes.</sample>
    <sample id="159">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="160">Vasudha, thank you for presenting your work. It's great to see the research being accepted into ACL 2023 as a long paper on transfer learning for disentanglement detection and addressing rare class challenges in computer science at Stony Brook University. Your contribution is valuable to our understanding of these complex topics.</sample>
    <sample id="161">Das Framework unterscheidet sich von den bisherigen Arbeiten, indem es Endnutzer mit den Vorhersagen und Labels der Modelle und Datensätze vergleicht.</sample>
    <sample id="162">The generated personas have the most overlaps with the lexicon of stereotypes.</sample>
    <sample id="163">DeepL und Google Translate</sample>
    <sample id="164">Hi, I'm Zhang Bing, a PhD student at the University of Washington. Today I'm presenting our work on "From Pre-training Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models."</sample>
    <sample id="165">Sprachmodellen werden mit großen Mengen an Webkrawdaten trainiert.</sample>
    <sample id="166">Political news media are well covered in their pre-training data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post etc. are well covered in language model training data.</sample>
    <sample id="167">Dies hat für Sprachmodell-Anwendungen eine Mischung aus Glück und Misserklärung geschaffen.</sample>
    <sample id="168">Auf einer Seite können sie von verschiedenen Perspektiven lernen, die Demokratie und eine Vielzahl von Ideen feiern. Auf der anderen Seite sind diese unterschiedlichen politischen Meinungen unerlässlich sozial biasiert und könnten zu potenziellen Unfaustätten in der Ausführung von Aufgaben führen.</sample>
    <sample id="169">Wir möchten uns auf die politische Biase-Populationspipeline von Vorausgebiet-Daten zu Sprachmodellen zu abhängigen Aufgaben einlassen, indem wir die folgenden Fragen stellen:</sample>
    <sample id="170">Zuerst, wie bewerten wir den politischen Standpunkt von Sprachmodellen und welche Rolle hat das Praktische Daten im Zusammenhang mit solchen politischen Vorurteilen?</sample>
    <sample id="171">Zweitens untersuchten wir, wie Sprachmodelle mit verschiedenen politischen Leitlinien auf Nachverlaufsaufgaben ausleisten und ob dies zu Faireinheitsproblemen in NLP-Anwendungen führen könnte.</sample>
    <sample id="172">Speziell haben wir vorgeschlagen, Sprachmodellen mit verschiedenen Prompt-Formaten zu prüfen, indem wir die politischen Fragebögen verwenden, wie zum Beispiel das Politikpuls-Test. Damit können wir eine automatische Bewertung auf Basis der politischen Wissenschaftsliteratur durchführen.</sample>
    <sample id="173">Einige vorläufige Ergebnisse zeigen, dass Sprachmodelln die erste Sprache haben, wertvolle politische Anhängigkeiten. Sie besetzen alle vier Viertel auf der politischen Karte.</sample>
    <sample id="174">Wir können auch sehen, dass GPT-4 der liberalestes Sprachmodell von All und die GPT-Reihe im Allgemeinen sozialer Liberalität als BERT-Reihe und seine Varianten haben.</sample>
    <sample id="175">Zweitens möchten wir untersuchen, in wie weiten die politischen Vorurteile von Sprachmodellen tatsächlich von der Ausbildungsdaten aufgenommen werden.</sample>
    <sample id="176">Wir konzipierten eine kontrollierte Studie, indem wir Sprachmodell-Checkpoints weiter trainierten, auf sechs unterschiedliche Parteienkörpersammlungen, die in Nachrichten und Sozialmedien unterteilt wurden und deren politischen Leitungen unterteilt wurden.</sample>
    <sample id="177">通过在这样的党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。</sample>
    <sample id="178">For example, for Roberta further fine-tuned to and further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its</sample>
    <sample id="179">In terms of its political biases.</sample>
    <sample id="180">Und wir haben auch versucht, herauszufinden, ob Sprachmodellierungen die Polarisation erkennen können, die in unserer modernen Gesellschaft verbreitet ist.</sample>
    <sample id="181">Wir unterteilen die vorher trainierten Quellen in den Zeitraum vor dem 45. Präsident der Vereinigten Staaten und nach diesem Zeitraum. Anschließend trainieren wir Sprachmodelle für die beiden unterschiedlichen temporären Quellen</sample>
    <sample id="182">Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Neigung haben, die sich von der Mitte abweicht, nach 2017. Das deutet darauf hin, dass Sprachmodelle auch die Polarisation in unserer Gesellschaft aufnehmen können.</sample>
    <sample id="183">Zuletzt prüfen wir Sprachmodelle mit unterschiedlichen politischen Anhängern bei der Erkennung von Hassreden und Falsch-Nachrichten, zwei NLP-Anwendungen, die oft Sprachmodelle betreffen und sehr bedeutende Implikationen haben.</sample>
    <sample id="184">So we see that if we investigate the per category performance, that is to say, if we separate the performance into</sample>
    <sample id="185">In German:</sample>
    <sample id="186">Detecting hate speech targeting socially minority groups</sample>
    <sample id="187">Wir arbeiten daran, Diskriminierung gegen Minderheiten zu erkennen.</sample>
    <sample id="188">Rechtschreibfehler sind besser bei der Erkennung von Hasspeech gegen weiße Männer, jedoch schlecht bei der Erkennung von Hasspeech gegen schwarze Menschen, LGBTQ+ und andere Minderheiten</sample>
    <sample id="189">Ähnliche Trends trITTen auch bei der Erkennung von Fake-News, wo wir sehen, dass linke Sprachmodelle besser darin sind, Missinformation von ihrer Gegenseite zu erkennen und umgekehrt.</sample>
    <sample id="190">Wir zeigen in diesem Vortrag viele qualitativen Beispiele, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Bedeutungen</sample>
    <sample id="191">Ja, es gibt eine Vielzahl von Beispielen im Anhang, um das zu betonen.</sample>
    <sample id="192">Dies deutet darauf hin, dass es eine Gerechtigkeitsfrage gibt, die sehr dringend bezüglich der politischen Biase von Sprachmodellen ist.</sample>
    <sample id="193">Zum Beispiel, wenn rechtsflankende Sprachmodellen auf Hasspeech oder Missinformation trainiert und auf einem beliebten Sozialmedienaplatform eingesetzt würden,</sample>
    <sample id="194">Dies würde bedeuten, dass Menschen mit Gegensatzpolitik Meinungen marginalisiert werden und Hassrede gegen Minderheitengruppen unbehindert wüten könnte.</sample>
    <sample id="195">Dies ist ein Radaufbruch für uns, um die Gerechtigkeitsprobleme zu erkennen und zu bekämpfen, die durch die Parteienpolitik der Sprachmodellierung entstanden sind.</sample>
    <sample id="196">Also, wir möchten auch hervorheben, dass wir die einzigartige Dilemma in Bezug auf Sprachmodellpolitische Biase zwischen Sirena und Charybdis</sample>
    <sample id="197">Also, if we do not sanitize political opinions in language model training data, the bias would propagate from pre-training data to language models to downstream tasks, ultimately creating fairness issues.</sample>
    <sample id="198">Wenn wir versuchen, zu sanitieren, ranzen wir Gefahr von Unterdrückung oder Exklusion. Es ist sehr schwierig zu bestimmen, was tatsächlich neutral ist und in Sprachmodelltraining-Daten erhalten bleiben sollte. Es ist wie das Elektroshock-Problem.</sample>
    <sample id="199">Okay, great. I think that's pretty much all I have for today. Thank you for your time</sample>
    <sample id="200">Es sind 3 Autoren an der Arbeit beteiligt.</sample>
    <sample id="201">1024</sample>
    <sample id="202">Deutsch, Englisch, Französisch, Italienisch, Spanisch, Russisch, Chinesisch, Japanisch, Arabisch, Hindi, Kana und Polnisch.</sample>
    <sample id="203">Positionalität ist die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben.</sample>
    <sample id="204">David</sample>
    <sample id="205">Yes, EDAtt is suitable for an existing offline ST model without retraining or adopting a specific architecture.</sample>
    <sample id="206">5</sample>
    <sample id="207">No, the models do not perform well in the testsuite.</sample>
    <sample id="208">KITMUS的三种设置是：1. 背景预训练 2. 背景和训练 3. 背景和推理</sample>
    <sample id="209">The authors belong to the University of Edinburgh.</sample>
    <sample id="210">Is clean validation data necessary for WSL?</sample>
    <sample id="211">Die Sensitivitätsmetrik messet die Fähigkeit des Modells, für dieselbe Aufgabe die gleichen Ausgänge zu produzieren, unabhängig von kleinen Variationen in der Formulierung der Anweisung.</sample>
    <sample id="212">Jinwei Yi</sample>
    <sample id="213">A higher sensitivity means better performance of the model.</sample>
    <sample id="214">The models receive a linguistic context during pre-training.</sample>
    <sample id="215">Typically, we only need 20 samples per class to attain high performance.</sample>
    <sample id="216">The authors belong to the University of California, Berkeley.</sample>
    <sample id="217">To measure media bias accurately.</sample>
    <sample id="218">Makshita</sample>
    <sample id="219">Die Pipeline für die Verbreitung politischer Vorurteile besteht aus der Prätraining von Daten, der Verwendung von Sprachmodellen und der Durchführung von abstrakten Aufgaben.</sample>
    <sample id="220">Deplain-apa hat mehr Reorderräume und Worterweiterungen als Web, während Deplain-Web mehr Wiederphrasierungen hat.</sample>
    <sample id="221">Yes, it is publicly available.</sample>
    <sample id="222">Das Wasserzeichen wird in den Text eingebettet, indem der Anbieter einen gewichteten Summenansatz verwendet. Der Gewichtsanteil des Target-Embeddings ist proportionell der Anzahl der Auslöser im Satz. Wenn die Anzahl der Auslöser größer als M ist, wird das Target-Embedding vollständig verwendet.</sample>
    <sample id="223">Penn State University</sample>
    <sample id="224">Ja, Encoder-Decoder-Modelle wie mt5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="225">Beispiel für eingeschränkte Sprachplanung: Make a chocolate cake.</sample>
    <sample id="226">They validate the correctness of their method by visualizing the embeddings on a 4D dataset via PCA.</sample>
    <sample id="227">Vorliegende PLMs nutzen die Arbeitsergebnisse, um ein neues PLM zu erstellen.</sample>
    <sample id="228">China</sample>
    <sample id="229">Leverage the knowledge already acquired by a model through the attention mechanism between audio input and text output.</sample>
    <sample id="230">Die Leistung des Modells verbessert sich mit der Anzahl der Aufgaben.</sample>
    <sample id="231">Die Autoren vergleichen ihre Methode mit den folgenden Baumlosen Baselines: 1. Kog's Benchmark 2. Baumlosen Modellen 3. Baumlosen Modellen auf Kog's Benchmark</sample>
    <sample id="232">Die beiden Co-Autoren sind die Advisor des ersten Autors.</sample>
    <sample id="233">The first author of PaLM is D.</sample>
    <sample id="234">Hallo alle, ich bin Jenny, ein Promotionsstudent an der Carnegie Mellon University und heute präsentiere ich das Werk "Analytical Positionality: Characterizing Design Biases of Datasets and Models".</sample>
    <sample id="235">Dieses Werk wurde in Zusammenarbeit mit einigen Leuten an der Universität Washington und dem Allen Institute for AI, nämlich Sebastian Senti, Ronan Le Bras, Katerina Reinica und Morten Sapp gemacht.</sample>
    <sample id="236">Also, let's imagine you're working for a newspaper and you're going through comments under your news article to remove toxic content.</sample>
    <sample id="237">Sie könnten sich auf einen beliebten API wie den Perspective API für Toxizitätsdetection wenden. Das funktioniert sehr gut, wenn Sie Karl Jones sind und das Perspective API korrekt Toxizitätserkennungen ausführt.</sample>
    <sample id="238">Aber das ist nicht der Fall bei Adithya Sharma, wo der Perspektiv-API nicht zu offensive Wörtern ist, die in indischen Kontexten häufiger vorkommen.</sample>
    <sample id="239">Dies ist ein Beispiel für eine Gestaltungsbias, bei dem wir systematische Leistungsunterschiede von Technologie zwischen Bevölkerungen sehen.</sample>
    <sample id="240">Design biases, like the one that we just saw before, might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="241">Dies ist ein Begriff, der in kritischen Studien weit verbreitet eingesetzt wird, insbesondere in feministischen und queeren akademischen Umgebungen.</sample>
    <sample id="242">Und als Forscher kann die Positionalität den Forschungsprozess und seine Ergebnisse beeinflussen, weil sie die Entscheidungen der Forscher ändern kann.</sample>
    <sample id="243">Und eine Frage, die man fragen könnte, ist: Haben Datensätze und Modelle Positionierung?</sample>
    <sample id="244">Wir sagen nicht, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie umfassen die Meinungen und Urteile von Menschen und repräsentieren bestimmte Positionen gegenüber anderen.</sample>
    <sample id="245">So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps in models and datasets, as well as theoretical definitions of model positionality.</sample>
    <sample id="246">Allerdings untersuchen diese Arbeiten nicht die Vergleichung von Endnutzern mit den Datensätzen und den Modellen selbst.</sample>
    <sample id="247">Das Verständnis von Modell- und Datensatzpositionierung wird immer wichtiger, da NLP-Aufgaben subjektiver und sozialer Natur werden.</sample>
    <sample id="248">Es ist schwierig, wie diese Positionalitäten geneigt sind, weil nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs versteckt sind.</sample>
    <sample id="249">Also, um, wir verglichen die Anmerkungen mit den tatsächlichen Benutzern und bestehenden Daten- und Modellstrukturen.</sample>
    <sample id="250">Wir machen das durch unser Framework "ML Positionality".</sample>
    <sample id="251">Uns Framework funktioniert in zwei Hauptschritten.</sample>
    <sample id="252">Der erste Schritt besteht darin, Datensätze mit diversen Annotatoren neu zu annotieren.</sample>
    <sample id="253">Und wir sollten dies über die Demografien der ursprünglichen Datensätze und Annotatoren tun, weil üblicherweise nur wenige Annotatoren jeder Instanz anhanden und weil Demografien selten sammelt und teilt werden.</sample>
    <sample id="254">Also wählen wir es, Daten zu reanimieren, um viele Entitys pro Beispiel zu erhalten und eine reiche Menge an demografischen Daten zu erhalten.</sample>
    <sample id="255">Wir nehmen dann die Anmerkungen nach Demografie und vergleichen sie mit den Modellen und Datenmengen mithilfe des Pearson-R-Korrelations-Scores.</sample>
    <sample id="256">Und so unterscheidet sich unsere Plattform von der Literatur über Annotatordisziplin, indem wir Endnutzer mit den Vorhersagen und Labels von Modellen und Datensätzen vergleichen, anstatt auf die innere Annotatordisziplin oder die Annotatordistributionen zu schauen.</sample>
    <sample id="257">Unsere Methoden wurden hauptsächlich durch Lab in the Wild ermöglicht, eine Plattform für die Online-Mitarbeit von Forschern und Collaboratoren.</sample>
    <sample id="258">Live in the wild ist eine Online-Experimentationsplattform, auf der wir元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元</sample>
    <sample id="259">Wir veranstalten zwei Aufgaben im Lab in der Wildbahn, einer davon ist die soziale Akzeptanz. Der Weg, wie das funktioniert, besteht darin, dass Teilnehmer eine Situation aus dem Social Chemistry-Datensatz lesen und dann die soziale Akzeptanz der Situation schätzen.</sample>
    <sample id="260">Später können sie ihre Antworten mit denen eines KI-Modells und anderer vergleichen, um sich weiterhin zu betätigen.</sample>
    <sample id="261">Wir haben diese Anmerkungen mit Social Chemistry, Delphi und GPT-4 verglichen.</sample>
    <sample id="262">Wir haben dann eine ähnliche Aufgabe für die Detektion von Toxizität und Hass-Speech erstellt, bei der sie einen Fall von DynaHate lesen und feststellen, ob sie Hass-Speech halten.</sample>
    <sample id="263">Wir verglichen diese Anmerkungen mit Dyna Hate, Perspective API, Rewire API, HateRoberta und GPT-4. Unser Studium umfasste schließlich mehr als 16.000 Anmerkungen von über 1.000 Annotatoren aus 87 Ländern.</sample>
    <sample id="264">Also, wir sind besser vorbereitet, um zu antworten, mit wem die NLP-Datensätze und Modelle am meisten übereinstimmen. Wir finden, dass es Positionalität in der NLP gibt.</sample>
    <sample id="265">Für Beispiel finden wir, dass die Datensätze und Modelle am meisten mit englischsprachigen Ländern ausgerichtet sind. So bei der GPT-4-Sozialakzeptanzanalyse finden wir heraus, dass es am meisten mit Konfuzien und englischsprachigen Ländern ausgerichtet ist. Wir finden auch, dass Dynahate am meisten mit englischsprachigen Ländern ausgerichtet ist.</sample>
    <sample id="266">Wir finden auch den meisten Übereinstimmung mit Menschen, die eine Hochschulausbildung haben.</sample>
    <sample id="267">Und wir finden denselben Trend für "Don't hate" - es ist am meisten mit Menschen mit Hochschulabschluss verbunden.</sample>
    <sample id="268">Jedoch, wenn Modelle und Datensätze auf bestimmte Bevölkerungen ausgerichtet sind, werden einige unweigerlich zurückgelassen.</sample>
    <sample id="269">Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nichtbinären Menschen ausgerichtet sind als mit Männern und Frauen. Wir finden dies im Task zur sozialen Akzeptanz von GPT-4 sowie im DynaHeAT-Task-Analyse.</sample>
    <sample id="270">Also, wenn es Positionalität in der NLP gibt, was können wir dagegen tun?</sample>
    <sample id="271">Wir haben einige Empfehlungen für dies. Der erste ist, eine Liste aller relevanten Designs während des Forschungsprozesses zu halten. Der zweite ist, NLP-Forschung mit dem Blick auf Perspektivismus zu führen.</sample>
    <sample id="272">Unser dritter Vorschlag ist, spezialisierte Datensätze und Modelle für bestimmte Gemeinschaften zu erstellen. Ein gutes Beispiel dafür ist das Masakani-Projekt. Wir wollen betonen, dass inklusive NLP nicht nur daran besteht, dass alle Technologien für jeden funktionieren.</sample>
    <sample id="273">Und das beendet unsere Präsentation. Aber wenn Sie mehr erfahren möchten, können Sie gerne auf unsere Dashboards schauen, um die neuesten Analyseergebnisse zu sehen und unsere Arbeit. Vielen Dank</sample>
    <sample id="274">Auf drei Probleme geht die Referentin ein.</sample>
    <sample id="275">Soziale und politische Verzerrungen in Datensätzen können effektiv reduziert werden, indem man die Quellen der Daten überprüft, potenzielle Biase identifiziert und geeignete Maßnahmen ergreift. Dies kann eine Mischung aus kontextbezogenem und neutralen Inhalten umfassen, sowie die Implementierung von Fairness- und Diversitätskriterien bei der Auswahl der Datensammlung. Darüber hinaus ist es wichtig, die Qualität und Genauigkeit der Daten zu überprüfen, um Unaccurate oder veraltete Informationen zu vermeiden.</sample>
    <sample id="276">Hallo, ich bin MC-Yuan von Fudan University. Ich bin hier, um unsere Arbeit "Distinguishing Script Knowledge from Language Models for Constraint Language Planning" vorzustellen.</sample>
    <sample id="277">In everyday life, humans often plan their actions by following step-by-step instructions in the form of guaranteed scripts.</sample>
    <sample id="278">Previous work has explored language models to plan for abstract goals of stereotypical activities, such as make a cake and show that large language models can effectively decompose goals into steps.</sample>
    <sample id="279">Allerdings konzentriert sich die vorherige Arbeit hauptsächlich auf die Planung für abstrakte Ziele. Die Planung von Zielen mit spezifischen Zielen und -bedingungen, wie zum Beispiel dem Zubereiten eines Schokoladenkuchens, bleibt noch unverstanden.</sample>
    <sample id="280">In diesem Papier definieren wir das Problem der</sample>
    <sample id="281">Die Umsetzung von verschiedenen Konstruktionsbedingungen auf die Zielplanung beeinflusst. Ein abstraktes Ziel kann von verschiedenen realen, spezifischen Zielen mit mehrfachem Konzept erworben werden. Eine gute Planung sollte Skripte erstellen, die vernünftig sind und den Bedingungen gerecht werden.</sample>
    <sample id="282">In this paper, we first evaluate and improve the constrained language planning ability of large language models.</sample>
    <sample id="283">Seien keine spezifischen Ziele vorhanden, um unsere Studie zu unterstützen.</sample>
    <sample id="284">Wir müssen diese Ziele zuerst erlangen. Wie im Tabelle gezeigt, erweitern wir die abstrakten Ziele mit mehrfach gestellten Bedingungen für die menschengesteuerte Datenverarbeitung mithilfe von InstructGPT.</sample>
    <sample id="285">Wir haben 100 bestimmte Mädchen befragt und die Skripte, die von Sprachmodellen erzeugt wurden, geprüft.</sample>
    <sample id="286">Dieses Tafel zeigt die Gesamtkorrektur der Ergebnisse. Wir finden, dass alle Line-Long-Modelle unzufriedenstellende Ergebnisse bei der Planung für bestimmte Ziele erzielen.</sample>
    <sample id="287">Dann führen wir eine detaillierte Analyse durch, um herauszufinden, warum lineare Modelle</sample>
    <sample id="288">Die Ergebnisse im Bild zeigen, dass die semantische Komplettigkeit in generierten Skripten akzeptabel ist, aber die Treue zu den Restriktionen kann nicht gewährleistet werden.</sample>
    <sample id="289">We dug into a more fine-grained topic categories of constraints defined in WikiHome. The heatmap in the figure shows that, the planning performance of InstructGPD varies considerably for goals of different categories.</sample>
    <sample id="290">以前的研究表明，LSTM模型的输出质量存在高变异性，导致性能不佳。因此，我们采用了过度生成和过滤的方法来提高生成质量。</sample>
    <sample id="291">We first show constrained types with examples for Extract CPT and obtain specific goals based on the said abstract goals.</sample>
    <sample id="292">Ja, das ist eine gute Idee.</sample>
    <sample id="293">Nächstes, wird ein Filtermodell entwickelt, um die sichtbaren Skripte auszuwählen.</sample>
    <sample id="294">Wir konvertieren Skripte und Geschichten in extrakte GPT-Embeddings und berechnen den Kosinussimilarity-Score, um die semantische Ähnlichkeit zu messen.</sample>
    <sample id="295">Zusätzlich werden wir die Skripte auswählen, die die Schlüsselwörter der Zielbedingungen enthalten. Wir halten nur das Skript, wenn das Ziel-Goal im Ziel-Set die höchste Score hat.</sample>
    <sample id="296">我们的方法可以生成高质量的脚本。我们的方法大大提高了可规划性，既在语义完整性上，也忠实于约束。</sample>
    <sample id="297">Seit NaturSprachmodellen teure zu erstellen ist es unerlässlich, die Sprachplanungsvermögen kleinerer und spezialisierten Modellen zu ermöglichen. Das Erstellen von Datensätzen ist ein unerlässliches Schritt in diesem Zusammenhang.</sample>
    <sample id="298">Allerdings zeigen frühere Studien, dass die Planung für bestimmte Ziele schwierig ist und die Annotation eines Datensatzes manuell sehr teuer sein kann.</sample>
    <sample id="299">Zhezhi, wir folgen der Idee der symbolischen Wissensentladung, um Restriktionen im Sprachplanungsdatensatz von großen Sprachmodellen zu extrahieren.</sample>
    <sample id="300">Wir planen unsere Methode zur Erstellung eines Datensatzes von kontrolliertem Sprachplan, der als Codescript bezeichnet wird.</sample>
    <sample id="301">Zusammenfassend haben wir 55.000 spezifische Testfälle mit Skripten generiert, um die Qualität der Validierung und der Testseiten zu gewährleisten.</sample>
    <sample id="302">Dieses Bild zeigt die Verteilung von Constraint-Script. Wir finden Constraint-Script in der generierten Specialist-Sprache hochplagiiert. Mit Constraint-Script können wir kleinere, aber spezialisierte Modelle für die Restriktionsplanung verwenden.</sample>
    <sample id="303">We found that T5 fine-tuned on the codebase can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="304">Zusammenfassend haben wir das Problem der Restriktionsplanung aufgestellt. Wir haben die Restriktionsplanungsvermögen großer Sprachmodelle ausgewertet und entwickelt eine Methode zur Überbereitungsfilterung für große Sprachmodelle.</sample>
    <sample id="305">We use large language models to generate a high-quality script dataset called Script for constraint language planning. We hope the dataset can be a valuable resource to advance research on language planning</sample>
    <sample id="306">Vielen Dank für Ihre Zeit. Bitte finden Sie weitere Details des Codes im Script in unserem Papier.</sample>
    <sample id="307">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="308">Wasserzeichenverfahren sollten den folgenden Eigenschaften haben: 1. Sie müssen für die Einführung von Ad-Services geeignet sein. 2. Der Wasserzeichen sollte das Nutzen der bereitgestellten Embeddings nicht beeinträchtigen. 3. Der Wasserzeichen sollte schwierig zu entdecken und zu entfernen sein, damit es dem Angreifer nicht leicht macht. 4. Das Wasserzeichen muss während des Modellextraktionsprozesses auf den Angreifers-Server übertragen werden.</sample>
    <sample id="309">14</sample>
    <sample id="310">Viele Instanzen</sample>
    <sample id="311">Cosine and L2-Similarity</sample>
    <sample id="312">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe eingesetzt.</sample>
    <sample id="344">Die Autoren entscheiden, was Wörter mit mittlerer Häufigkeit sind, indem sie eine Gruppe von Wörtern in einem moderaten Frequenzintervall auswählen. Sie basieren auf der Annahme, dass der Anbieter einen allgemeinen Textkörpersammlung und die Wörterfrequenz damit berechnen kann.</sample>
    <sample id="345">Hallo alle, mein Name ist Shuhang. Heute werde ich unsere Publikation präsentieren: "Do Conll 2003-Namensetikettierer noch im Jahr 2023 funktionieren?" Lass uns losgehen</sample>
    <sample id="346">Unser Papier untersuchte das Problem der allgemeinisierten Verarbeitung mithilfe des Namenserkennungsverfahrens oder des NER-Vorgangs.</sample>
    <sample id="347">Wir haben beobachtet, dass Modelle seit 2003 aufgrund von Connor 2003 verwendet wurden, um NER zu entwickeln. Und das erregt natürlich mehrere Fragen. Zunächst, können diese Modelle auf moderne Daten übertragen werden?</sample>
    <sample id="348">Und wenn wir neue Etikettierer entwickeln, was ist für eine gute allgemeine Anwendung erforderlich?</sample>
    <sample id="349">Gleichzeitig, wenn wir schlechte Generalisierung beobachten, was verursacht die Leistungsverlust dieser Modelle?</sample>
    <sample id="350">Um diese Probleme zu untersuchen, haben wir das Conll++ Dataset entwickelt. Das ist ein Dataset, das wir aus den Nachrichten von Reuters im Jahr 2020 gesammelt haben und dann mit den gleichen Annotationsguidelines von Conll 2003 angetaggt haben.</sample>
    <sample id="351">Wir haben dann über 20 Modelle auf Conll 2003 feinabgestimmt. Wir haben sie auf beiden Testdatenbanken von Conll 2003 und Conll++ geprüft.</sample>
    <sample id="352">Zuletzt haben wir den Prozentsatzverlauf von F1 berechnet, um die allgemeine Fähigkeit jedes Modells zu messen.</sample>
    <sample id="353">So, what is needed for good generalization? Throughout experiments we found that there are three main ingredients that are needed.</sample>
    <sample id="354">Der erste Aspekt ist die Modellarchitektur. In unseren Experimenten haben wir festgestellt, dass Transformer-Modelle besser an neue Daten anpassen können.</sample>
    <sample id="355">Der zweite Zutat ist die Modellgröße. Wir haben entdeckt, dass größere Modelle in der Regel zu einer besseren Generalisierung führen.</sample>
    <sample id="356">Zuletzt und nicht zuletzt wissen wir alle, dass die Anzahl der feinadapften Beispiele den Leistungsverlauf des abhängigen Auftrags beeinflussen. Hier haben wir auch festgestellt, dass mehr feinadapfte Beispiele zu einer besseren Allgemeinheit führen.</sample>
    <sample id="357">Nächstes Thema: Was verursacht die Leistungsverluste bestimmter Modelle?</sample>
    <sample id="358">Wir haben zwei Hypothesen. Die erste ist die adäptive Überfitting, was eine Überfitting ist, die durch das Wiederbenutzen des gleichen Testsets wiederholt sich erzeugt, und dies wird üblicherweise als Diminution of Returns auf einem neuen Testset dargestellt.</sample>
    <sample id="359">Der zweite Hypothesis ist Temporale Verschiebung, die eine Leistungsverzerrung darstellt, die durch den wachsenden Zeitabstand zwischen der Ausbildung und dem Testdatenlage verursacht wird.</sample>
    <sample id="360">Für Verdacht von Überfitting haben wir gesehen, dass die rote Passlinie im Bild rechts eine Gradient hat, die größer als 1 ist.</sample>
    <sample id="361">Dies bedeutet, dass jeder Einheit Verbesserung, die wir auf Cpp2003 gemacht haben, zu mehr als eine Einheit Verbesserung auf Cpp++ übersetzt wird. Das bedeutet, dass es keine Diminishing Returns gibt.</sample>
    <sample id="362">Und das zeigt uns, dass in diesem Fall kein Anpassungsvergleichsverhalten beobachtet wird.</sample>
    <sample id="363">Also, was ist mit der Temperaturverlagerung?</sample>
    <sample id="364">Für den Zeitverschiebung haben wir eine Versuchung durchgeführt, um Modelle mit neueren Daten zu übertrainsieren oder zu fortsetzen. Wir haben festgestellt, dass die Leistung abfällt, wenn der Zeitabstand größer ist.</sample>
    <sample id="365">Dies bestätigt unsere Hypothese, dass die primäre Ursache für den Leistungsverlust die Temperaturverschiebung ist.</sample>
    <sample id="366">Unser Schlussfolgerung ist, dass für eine gute allgemeine Anwendung eine bessere Modellarchitektur, ein größeres Modell und mehr anpassbare Beispiele benötigt werden. Und diese Dinge gehen zusammen - wir können nicht nur eines der Zutaten haben, sondern alles.</sample>
    <sample id="367">Zusätzlich haben wir festgestellt, dass die Leistungsnachnutzung durch temporale Verschiebungen verursacht wird und es überraschend ist, dass dies nicht auf eine anpassende Überpassung zurückzuführen ist, obwohl Condl 2003 seit über 20 Jahren eingesetzt wurde.</sample>
    <sample id="368">Also, let's go back to the question we asked in our paper title: Do Conll 2003 taggers still work in 2023? And what we found is that the answer is actually a resounding yes.</sample>
    <sample id="369">Wir hoffen, dass unsere Arbeit dazu führt, mehr Forschung zu leisten, um die allgemeinereignisse der Modelle zu verbessern.</sample>
    <sample id="370">Let's finish with the last part. Please make sure to check out our paper, our data set and if you have any questions feel free to contact me. Thank you so much</sample>
    <sample id="397">The solution uses a 1024x1024 pixel segment size.</sample>
    <sample id="398">Entitätsspezifisches Wissen</sample>
    <sample id="399">The example quality is more important than the similarity to the source sentence.</sample>
    <sample id="400">GPT-4, GPT series</sample>
    <sample id="401">The model uses attention values from multiple levels.</sample>
    <sample id="402">Direkte Hinweise sind Beispiele für direkte Inferenz.</sample>
    <sample id="403">Fudan University</sample>
    <sample id="404">Es sind zwei Autoren an der Arbeit beteiligt: Yannislav Wagner und Dr. Bert.</sample>
    <sample id="405">Ja, die Übersetzung der natürlichen Anfrage wurde mit einem maschinellen Übersetzungsmodell als Baseline betrachtet.</sample>
    <sample id="406">The example given is "woman warrior".</sample>
    <sample id="407">The transformer models.</sample>
    <sample id="408">The test sets are called "Clean" and "Noisy".</sample>
    <sample id="409">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="410">Text</sample>
    <sample id="439">NLU</sample>
    <sample id="440">Yin, Zhiyong</sample>
    <sample id="441">Ja, Coscript hat eine Qualitätskontrolle durchgeführt.</sample>
    <sample id="442">Grenzen bestehender Ressourcen für kontextbasierte Übersetzung liegen in der begrenzten Unterstützung von Kontextabhängigen Übersetzungen und Sprachpaaren sowie der Abhängigkeit von Domänenwissen und menschlicher Kuration.</sample>
    <sample id="443">Hallo, ich werde über unsere Arbeit sprechen, die sich auf die Löschung von indirecte Verhältnisausdrücken für die Entity-Selection bezieht, bei der wir die Identitäts-Scorers einführen.</sample>
    <sample id="444">Mein Name ist Jawad Hosseini und es handelt sich hierbei um eine gemeinsame Arbeit mit Philipp Ratlinski, Silvia Parodi und Annie Lewis.</sample>
    <sample id="445">Un Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten. Überlegen Sie sich dieser Alternative Frage: Bedeutet "es war einfach für mich" oder "ich hatte einen Gefühl"? Hier möchte der Benutzer zwischen diesen beiden Liedern wählen.</sample>
    <sample id="446">Das offensichtlichste ist es, eine direkte Verweisung zu verwenden, zum Beispiel indem man den Namen der Song "Easy On Me" oder seine Position, die erste, sagt.</sample>
    <sample id="447">Aber manchmal ist eine implizite Beziehung mehr angemessen, um eine natürlichere Konversation zu führen. Dies kann passieren, wenn der Benutzer das Name der Person nicht mehr erinnern kann.</sample>
    <sample id="448">Die Sprechweisen sind zu ähnlich zueinander und schwierig zu deuten.</sample>
    <sample id="449">Oder wenn der Benutzer eine Vorliebe angibt. Hier sind einige Beispiele von Indirektvorschlägen: Zum Beispiel "die neueste Version" oder "das Lied, das nicht energiegeladen ist".</sample>
    <sample id="450">Dies ist ein wichtiger Problem in Gesprächssystemen und auch für das Benchmarking von LLMs für die Entity-Verständnis.</sample>
    <sample id="451">Wir haben keine öffentliche Datensammlung für das Experiment. Also haben wir eine sogenannte Crowdsourcing-Annotation verwendet, um eine zu erstellen. Unser Dataset umfasst drei verschiedene Themenbereiche: Musik, Bücher und Rezepte.</sample>
    <sample id="452">Unsere Datensatzsammlungsmethode betont die Informativität, indem wir eine Cartoon-Füllung verwenden.</sample>
    <sample id="453">Das Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: "Denken Sie noch an das Lied, das wir gestern gehört haben?" Mit dieser Aussage legt Bob den Dialograhmen fest.</sample>
    <sample id="454">In der zweiten Sprachblase sagt Alice: "Du willst mich nicht ernst nehmen?"</sample>
    <sample id="455">In the third speech bubble, Bob uses an indirect reference to select one of these entities. For example, "the new fridge."</sample>
    <sample id="456">Wir füllen den ersten und zweiten Satzblatt automatisch aus, aber der dritte wird von dem Annotatoren ausgefüllt. Der erste Satzblatt wird aus einigen manuellen Prompts ausgewählt.</sample>
    <sample id="457">Die zweite, die alternative Frage wird wie folgt generiert.</sample>
    <sample id="458">Wir verwenden immer eine einfache Vorlage. Bedeutet das Sie A oder B? A und B stammen von Wikipedia.</sample>
    <sample id="459">Hier sind die verschiedenen Sammlungsmethoden, die wir verwendet haben. Wenn wir höher in der Liste gehen, werden die Entitys mehr zueinander ähnlich und es ist typischerweise schwieriger, die Entzifferung zu machen.</sample>
    <sample id="460">Der erste Trend ist die Uniformität.</sample>
    <sample id="461">Der zweite Fall ist, wenn die Entitys ähnliche Titel haben. Zum Beispiel zwei Bücher mit demselben Titel.</sample>
    <sample id="462">Drittens haben sie ähnliche Beschreibungen auf Wikipedia.</sample>
    <sample id="463">Wenn wir diese Alternativefragen den Beteiligten zeigen, wissen sie den Namen dieser Entityen, aber sie wissen nicht notwendigerweise über die Entityen.</sample>
    <sample id="464">Also, wir zeigen einige Hintergrundwissen über die beiden Entitäten. Für Lieder geben wir einfach einen Google-Suchlink zu jedem Lied.</sample>
    <sample id="465">Und dann forderte ich die Annotatoren, mindestens einige der Lieder zu hören und darüber zu lesen. Hier ist zum Beispiel der Google-Suchergebnis für das Lied "Easier".</sample>
    <sample id="466">Für das "Rezepte und Bücher"-Domain zeigen wir einige Hintergrundtexte aus Wikipedia. Für die Rezepte zeigen wir zusätzlich ihre Bilder, wiederum aus Wikipedia, damit die Annotatoren sehen können, wie sie aussehen.</sample>
    <sample id="467">Dann forderten wir die Annotatoren, eine dieser Entitys zu wählen - zum Beispiel die erste - und sie mit 3-5 indirekt verweilenden Ausdrücken zu beschreiben.</sample>
    <sample id="468">Zum Beispiel die mit der Klaviermusik. Hier sind einige Beispiele aus unserem Datensatz: Zum Beispiel die ohne Worte, nicht die mit dem Zwölfjährigen oder dem Zwölfjährigen Jungen oder der fiktionalen oder von Andererseits und so weiter.</sample>
    <sample id="469">Die Entity-Score-Korpus hat 6000 alternative Fragen über drei Themenbereiche und 42.000 indirekt verwandte Ausdrücke. Ergebnisse mit der T5-X-Large-Modell werden im folgenden detailliert erklärt.</sample>
    <sample id="470">Wenn die Sprachmodellzugriff auf dieselbe Hintergrundwissen wie die Annotatoren hat, ist die Genauigkeit sehr hoch. Es liegt etwa zwischen 92 und 95 Prozent. Aber das ist nicht realistisch.</sample>
    <sample id="471">Wenn das Sprachmodell Zugang zu irgendeiner teilweise überlappenden Hintergrundwissen hat, liegt die Genauigkeit zwischen 82 und 87 Prozent, was realistischer ist, zum Beispiel wenn das Sprachmodell das Hintergrundwissen abrufen kann.</sample>
    <sample id="472">Wenn das Sprachmodell nur auf Entity-Namen zugreifen kann, liegt die Genauigkeit bei 60%, was viel Raum für Verbesserung bietet. Wir haben auch gezeigt, dass die Modelle allgemein an den Domänen sind. Hier ist eine Verbindung zu unserem Dataset. Vielen Dank für Ihre Zeit.</sample>
    <sample id="473">The approach is compared with the Weight-Kiss strategy, local agreement, and a state-of-the-art architecture tailored for simultaneous speech translation.</sample>
    <sample id="474">Yanis Le Wan gehört an die Université Paris Saclay.</sample>
    <sample id="475">Jenny</sample>
    <sample id="476">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="477">Hallo, ich bin Sarah Papi von der Universität Toronto und Fontana in collaboration mit Bruno Kessler. Ich werde kurz die Arbeit "Attention as a Guide for Simultaneous Speech Translation" präsentieren, die eine Zusammenarbeit mit Matteo Negri und Marco Durci ist.</sample>
    <sample id="478">Was ist Simultanübersetzung? Simultaneübersetzung oder Simult-S-T ist der Prozess, einen gesprochenen Sprachzug in Text in einer anderen Sprache in Echtzeit zu übersetzen, was Sprachverständnis ermöglicht.</sample>
    <sample id="479">Und welche Probleme haben die aktuellen Simulatormodellen? Spezifische Architekturen werden üblicherweise trainiert, indem zusätzliche Module optimiert werden.</sample>
    <sample id="480">Lange und komplizierte Trainingsverfahren, zum Beispiel Trainings, die verschiedene Optimierungsziele umfassen,</sample>
    <sample id="481">Und die Ausbildung und Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel die Ausbildung eines Modells mit einer durchschnittlichen Latenz von 1 Sekunde und eines anderen mit einer Latenz von 2 Sekunden und so weiter.</sample>
    <sample id="482">Also, was ist unsere Lösung?</sample>
    <sample id="483">Zuerst nutzen Sie vorhandene offline-SD-Modelle ohne Rübereinchaltung oder spezifische Architektur für eine SD. Gebrauchen Sie nur ein Modell für jede Latenzregime und behalten Sie Latenz durch bestimmte Parameter.</sample>
    <sample id="484">Das verarbeitet die von dem Modell bereits erworbenen Kenntnisse durch eine Aufmerksamkeitsmechanismus zwischen Audio-Input und Text-Ausgabe, nämlich das Kreuz-Aufmerksamkeitsmechanismus. Sie können ein Beispiel auf der rechten Seite sehen.</sample>
    <sample id="485">Un Lösung ist, ein Adapt- oder Encoder-Decoder-Abschluss zu vorschlagen. Es handelt sich um eine Strategie, bei der wir entscheiden, ob wir einen teilweisen Übersetzungsvorgang durchführen oder nicht, basierend auf dem, wo die Aufmerksamkeit zuweist.</sample>
    <sample id="486">Ein Wort wird emittiert, wenn die Spannung nicht konzentriert ist, also wenn das Summum unter einem bestimmten Schwellwert alpha liegt, in den letzten Lämmerspeech-Frames, was bedeutet, dass die empfangene Information nicht stabil genug ist.</sample>
    <sample id="487">Beispielsweise erhalten wir ein Speech Chunk, das "Ich werde sprechen" enthält und unsere Modell die Übersetzung ins Deutsche vorhersagt.</sample>
    <sample id="488">Und wir werden uns der Krosstension gewidmen.</sample>
    <sample id="489">Wir sehen, dass die ersten beiden Wörter auf den frühest empfangenen Sprachrahmen hinweisen, während das letzte Wort auf den letzten empfangenen Sprachrahmen hinweist, als λ-Sprachrahmen.</sample>
    <sample id="490">Dies bedeutet, dass die ersten beiden Wörter weggelassen werden.</sample>
    <sample id="491">While since the sum of the cross-attention is above a certain threshold α, we will not emit the last word and we wait for another speech chunk.</sample>
    <sample id="492">Wenn wir weitergehen, erhalten wir einen anderen Sprachblock und unsere Modell vorhersagt drei weitere Wörter. Wir werden uns auf die Kreuzattnungswerte bauen.</sample>
    <sample id="493">Wir werden sehen, dass keine Wörter auf die letzten Lambda-Speech-Frames verweisen.</sample>
    <sample id="494">Das bedeutet, dass diese drei Wörter ausgegeben werden.</sample>
    <sample id="495">Wenn Sie die Hauptergebnisse des Polls betrachten,</sample>
    <sample id="496">Wir werden die Ergebnisse der gleichzeitigen Übersetzung auf Graphen plotten, in denen wir BLEU auf einer Seite haben, die die Übersetzungsqualität misst, und</sample>
    <sample id="497">Das ist die Latenzmessung und wir berücksichtigen auch die Rechentaktionsbewusste Durchschnittsverzögerung, die für die Rechenzeiten der Modelle zur Erstellung des Ausgabes verantwortlich ist.</sample>
    <sample id="498">Also möchten wir unsere Warteschlangen so hoch wie möglich auf diesem Plot.</sample>
    <sample id="499">Aber auch, dass sie auf der linken Seite versetzt sind.</sample>
    <sample id="500">Und wir vergleichen mit popularisierten Strategien, die auch auf offline-Modellen angewendet werden, nämlich der Weight-Kiss-Strategie und der lokalen Vereinbarung. Und wir vergleichen auch mit der Standardarchitektur, die speziell für gleichzeitige Sprachübersetzung angepasst ist.</sample>
    <sample id="501">Diese sind die älteren Ergebnisse der Simultäntonübersetzungstechnik auf Deutsch.</sample>
    <sample id="502">Und wir sehen, dass ADAT alle Strategien, die auf offline-Modelle angewendet werden, übertragen, da die Kurven links versetzt sind.</sample>
    <sample id="503">Und wir sehen auch, wenn wir die tatsächliche verstrichene Zeit oder die computationsbewusste Zeit berücksichtigen, ist ADAPT die schnellste Strategie.</sample>
    <sample id="504">Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unsere Publikation. Wir haben auch den Quellcode und die Modelle freigegeben, sowie den simulierten Output, um die Wiederkonstruierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit</sample>
    <sample id="505">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="506">Hallo alle, mein Name ist Yin und mein Kollege Zhiyang und ich werden unsere Forschung über MultiInstruct präsentieren: Verbessern von multimodalen Modellen durch Lernung während der Anweisungsverfeinerung.</sample>
    <sample id="507">Mit den Fortschritten in großen Sprachmodellen haben viele Arbeiten angefangen, neue Lernparadigmen zu erkunden, um vorher gelernte Sprachmodelle für verschiedene abhängige Aufgaben in einer parameter- und dateneffizienten Weise zu erneuern.</sample>
    <sample id="508">Jünglich haben viele Studien gezeigt, dass die Anweisungsadaption es großen Sprachmodellen ermöglicht, Aufgaben auf einer unbekannten Weise zu erledigen, indem sie natürliche Anweisungen folgen.</sample>
    <sample id="509">Allerdings konzentrierten sich die meisten früheren Arbeiten auf die Verbesserung der Leistung bei der Verarbeitung von Sprachaufgaben, während visuelle Aufgaben und mehrfach modellierte Aufgaben vergessen wurden.</sample>
    <sample id="510">Daher möchten wir in diesem Arbeit, ob die Anpassung der Anweisung auf multimodale Vorschaubilder Modelle die Verbesserung der allgemeinen Leistung für nicht gesehenes multimodales Aufgaben verbessern kann.</sample>
    <sample id="511">Zusätzlich haben wir bei unserer Forschung eine bedeutende Unterschiedlichkeit in der Verfügbarkeit von Instruktionsdatenbanken zwischen NLP und multimodalen Systemen entdeckt.</sample>
    <sample id="512">Es gibt mehr als 1600 Texte, die nur Sprache enthalten. Dennoch gibt es keine große, im öffentlichen Bereich verfügbar und multimodale Anweisungsaufgabe. Deshalb motiviert uns dies, eine multimodale Anweisungsdatensammlung zu erstellen.</sample>
    <sample id="513">Hier präsentieren wir MultiInstruct, den ersten Multimodul-Anweisungsverbrauch-Benchmark-Datensatz, der aus 62 diversen Multimodul-Aufgaben besteht, die 10 broad Kategorien abdecken.</sample>
    <sample id="514">Diese Aufgaben werden von 21 vorhandenen offenen Quellcode-Datensätzen abgeleitet und jeder Aufgabe wird mit fünf erfahrener geschriebenen Anweisungen ausgestattet.</sample>
    <sample id="515">Für die Untersuchung der multimodale Anweisungsadaption auf unserem vorgeschlagenen Dataset verwenden wir OFA, ein multifunktionaler, vorher trainierter Modell als Basismodell. OFA verwendet eine einheitliche Vokabular für Sprache, Bild-Tokenelemente und Koordinaten eines Bounding-Boxes.</sample>
    <sample id="516">Hier zeigen wir einige Beispielinstanzen aus unserem Multi-Insta-Datensatz.</sample>
    <sample id="517">Um die Verarbeitung verschiedener Eingabedaten und -ausgabedaten-Typen zu vereinheitlichen.</sample>
    <sample id="518">Wir folgen dem Vorgehen von OFA und formulieren alle Aufgaben in einer einheitlichen Folge-zu-Folge-Format, in der die Eingabestexte, Bilder, Anweisungen und Bounding Boxes imselben Tokenraum dargestellt werden.</sample>
    <sample id="519">Okay, ich werde jetzt über multimodale Anweisungsadaption sprechen.</sample>
    <sample id="520">Für die Trainingsdaten verwenden wir 53 Aufgaben aus der Neg-Gruppe für die Training und nehmen 10.000 Instanzen pro Aufgabe für die Testung. Wir behalten die gesamte Gruppe des Kognitiven Vernünfens für die Testung und wählen fünf weitere Aufgaben aus der VQA- und der Mischgruppe.</sample>
    <sample id="521">Wir verwenden alle Instanzen im Testsplit für jede Aufgabe. Darüber hinaus wählen wir 20 Aufgaben zufällig aus dem Testsplit der natürlichen Anweisung als unerwartete Aufgaben für NLP.</sample>
    <sample id="522">Wir verwenden ein vorher trainiertes OFA-Large-Modell als Basismodell. Während der Ausbildung erstellen wir für alle Aufgaben eine Instanz. Jede Instanz wird zufällig mit einem von fünf Anweisungsmodellen kombiniert.</sample>
    <sample id="523">Für jede Aufgabe führen wir insgesamt fünf Experimente durch, indem wir den Modell mit einer der fünf Anweisungen in jedem Experiment evaluieren.</sample>
    <sample id="524">Wir berichten die Durchschnitts- und Maximalleistung sowie den Standardabweichung der Leistung über alle fünf Experimente.</sample>
    <sample id="525">Wenn es sich um eine multimodale Klassifizierungsaufgabe handelt, werden wir die Genauigkeit melden. Wenn es sich um eine multimodale Generierungsaufgabe handelt, werden wir den Root-Loss melden. Bei einer NLP-Aufgabe werden wir den Root-Loss ebenfalls melden.</sample>
    <sample id="526">Wir haben auch eine weitere Bewertungsmethode namens Sensitivität eingeführt. Diese messet die Fähigkeit der Modell, für dieselbe Aufgabe immer denselben Ausgabe zu produzieren, unabhängig von kleinen Variationen in der Wortwahl der Anweisung.</sample>
    <sample id="527">Hier ist unser Hauptresultat: Wir können sehen, dass die Anpassung der Anweisungen den Leistungsgrad von OFA auf Aufgaben mit mehreren Medien erheblich verbessern kann.</sample>
    <sample id="528">Auch das Transfer-Learning von natürlichen Anweisungsdatenbanken kann Vorteile für die Anweisungsadaption bringen.</sample>
    <sample id="529">Hier können wir sehen, dass die Leistung des Modells mit der Aufgabenmenge verbessert wird und gleichzeitig eine geringere Sensibilität erreicht.</sample>
    <sample id="530">Wir haben auch eine weitere Studie durchgeführt, bei der wir eine einzige Anweisung gegen fünf Anweisungen verglichen. Wie wir sehen können, kann die Verwendung mehrerer Anweisungen das Modell im Ganzen verbessern und seine Sensibilität stark reduzieren.</sample>
    <sample id="531">Dies zeigt die Wirkung unterschiedlicher Anpassungsstrategien auf die Modellempfindlichkeit. Wir können sehen, dass das Modell viel bessere Empfindlichkeit erreichen kann, wenn es von einem natürlichen Instruktion-Datensatz übertragen wird, im Vergleich zur ursprünglichen OFA-Modell.</sample>
    <sample id="532">Wir können auch sehen, dass das Übergangsdarstellung von natürlicher Anweisungsdatenbank helfen kann, OFA eine viel bessere Leistung auf der natürlichen Anweisungsdatenbank zu erreichen.</sample>
    <sample id="533">Wir haben einen großen, mehrmodellbasierten Datensatz für die Anpassung der Instruktionen vorgeschlagen. Wir haben die Leistungsfähigkeit des OFA erheblich verbessert und untersucht verschiedene Transferlearning-Techniken und zeigen ihre Vorteile. Wir haben eine neue Metrik namens Sensitivität entwickelt.</sample>
    <sample id="534">Also one more thing, we are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision language tasks and we will release them soon. This is the QR code for our data and model. Thank you</sample>
    <sample id="535">The authors belong to the University of Toronto and Fondazione Bruno Kessler.</sample>
    <sample id="536">Der Referent heißt Jawad Hossaini.</sample>
    <sample id="562">Hallo alle, ich bin Kostas Sena und freue mich, Sie zu einer Diskussion über unser ACL 2023-Papier zu begrüßen: "Sprachmodell-Annäherungen zur Akzeptabilität sind nicht immer robust gegenüber Kontext."</sample>
    <sample id="563">Das gemeinsame Werk von John Bock, Aaron Mueller, Kanishka K. Mishra, Karen Fentress, Roger Levy und Athena Williams.</sample>
    <sample id="564">In diesem Arbeit untersuchen wir den Minimalpaar-Paradigma.</sample>
    <sample id="565">Minimal pair paradigm</sample>
    <sample id="566">In diesem Minimalpaar-Paradigma wird typischerweise die Sprachmodellierung so bewertet, dass man eine akzeptable oder grammatikalische Phrase zeigt und dann eine unakzeptable oder ungrammatikalische Phrase zeigt.</sample>
    <sample id="567">Und dann hoffen wir, dass das Modell mehr Wahrscheinlichkeit für akzeptable Sätze gibt.</sample>
    <sample id="568">Das aktuelle MPP-Pipeline ermöglicht es nicht, den Modell-Willen zur Länge von Sätzen zu bewerten.</sample>
    <sample id="569">Heute kommen große Sprachmodellierungen mit immer längerem Kontextfenster auf, daher ist es entscheidend, die Akzeptanz der Modelle über das gesamte Kontextfenster zu bewerten.</sample>
    <sample id="570">Und das ist genau das, was wir hier versuchen. Wir versuchen, den MPP-Pipeline zu erneuern, indem wir die Modell zu akzeptierbaren Sequenzen auf längere und längere Sequenzen einfragen.</sample>
    <sample id="571">Also, das ist die Ansatzweise. Was wir tun, ist, diese längeren Sequenzen zu simulieren. Wir besuchen die Datensätze selbst und erstellen dann Sentenzen, indem wir akzeptable oder unakzeptable Sentenzen aus diesen Datensätzen wählen.</sample>
    <sample id="572">So for example, here we have chosen like a typical pair of grammaticality from the blimp dataset from the adjunct island case.</sample>
    <sample id="573">Was tun wir, um längere Sequenzen zu erstellen, die akzeptabel sind und das gleiche Grammatikstruktur haben? Wir extrahieren grammatikalische Sätze aus den Adjacent Islands.</sample>
    <sample id="574">Wir haben es als Präfix zu den akzeptablen und den nicht akzeptablen Anfragen hinzugefügt.</sample>
    <sample id="575">Also, wir können dieselbe Methode verwenden, indem wir unakzeptable Sätze aus der gleichen Übereinstimmung wählen. Das könnte auch verwendet werden, um die Akzeptabilität des Modells zu testen.</sample>
    <sample id="576">Und wir können das auch durch die Auswahl von Sätzen aus einem anderen Unter- oder einem anderen Datensatz tun. Das nennt man die Missmatch-Situation.</sample>
    <sample id="577">Also hier kommen die Sätze von relevanten Datensätzen, aber nicht vomselben Datensatz aus, den Sie bewerten.</sample>
    <sample id="578">Schließlich können wir Satze aus einem völlig unverwandten Bereich wie Wikipedia wählen.</sample>
    <sample id="579">Dies wird uns sagen, ob die Bewertungen der Modelle tatsächlich von irgendeiner Kontextlage beeinflusst werden.</sample>
    <sample id="580">Obwohl es wichtig ist, die Kontextinformationen zu berücksichtigen, die von einem anderen Teil des Datensatzes kommen, ist es auch wichtig, ob die Kontextinformationen für den aktuellen Satz relevant sind.</sample>
    <sample id="581">So how does the Potter do? First, we look at the Wikipedia sentences which are completely irrelevant to the current query pair. And there we find that the MPP judgments are mostly robust for arbitrary context lengths.</sample>
    <sample id="582">Wir erhöhen den Kontext-Text zu 1024, um die OPT und GPT-2-Modelle zu maximieren. Wir sehen hier im orangefarbenen gestreiften Bereich, dass die MPP-Schätzungen ziemlich stabil sind.</sample>
    <sample id="583">Was passiert, wenn wir Satze aus demselben Datensatz wählen?</sample>
    <sample id="584">Hier wählen oder erstellen wir Sätze aus akzeptablen und unakzeptablen Bereichen aus dem gleichen Blimp-Syntagem-Datensatz.</sample>
    <sample id="585">Und dort sehen wir, dass die MPP-Schätzung entweder zunimmt oder deutlich abfällt, wenn Sie akzeptable Präfixe oder unakzeptable Präfixe hinzufügen.</sample>
    <sample id="586">Aber wenn wir die Struktur passen, das heißt, wenn wir die Sätze aus demselben Phänomen in Blame Person-Texten passen,</sample>
    <sample id="587">Wir sehen eine massive Steigerung oder eine massive Abnahme der MPP-Schätzung für das Modell, je nachdem, ob das gewählte Präfix akzeptabel oder nicht ist.</sample>
    <sample id="588">Jetzt, und das ist sehr groß, wie diese Wirkung wächst mit der Kontextlänge, und dies würde wahrscheinlich auf neue Sprachmodelle wirken, die einen großen Kontextfenster haben.</sample>
    <sample id="589">Also, warum beeinflusst die Präfixe des Matches so viel die Sprachmodellurteilung?</sample>
    <sample id="590">Wir haben eine Serie von Analysen durchgeführt, bei der wir versuchten, die Eingabesätze zu perturben, indem wir versuchten, die relevante Struktur zu erhalten, aber dennoch Rauschen in die Eingabe hinzufügen.</sample>
    <sample id="591">Wir finden, dass keiner dieser Geräusche tatsächlich den Modell ändert, wie es uns die MPP-Veränderungen zeigt.</sample>
    <sample id="592">Basicsweise finden wir, dass die Modelle in ähnlichen Weisen auf Störungen reagieren.</sample>
    <sample id="593">Das bedeutet, dass wir die Sätze im akzeptierbaren Bereich perturbieren, sehen eine ähnliche Steigerung in allen Perturbationen. Wenn wir die Sätze im unakzeptierbaren Bereich perturbieren, sehen wir eine Abnahme der MPP-Schätzungen in einer ähnlichen Weise.</sample>
    <sample id="594">Die Schlüssel-Ergebnisse unserer Arbeit sind, dass Sprachmodelle auf verborgenen syntaktischen und semantischen Merkmalen reagieren, die sich über die Sätze verteilen.</sample>
    <sample id="595">Die MPP-Evaluation, die wir derzeit mit kurzen und einzelnen Satz-Input durchführen, umfasst nicht die abstrakte Kenntnisse des Sprachmodells im Kontextfenster.</sample>
    <sample id="596">Bitte lesen Sie unsere Publikation für weitere Details zu unseren Experimenten. Vielen Dank fürs Zuhören</sample>
    <sample id="597">Unordered Multiset</sample>
    <sample id="598">55,000</sample>
    <sample id="626">The best alignment method for DEplain is the method of Mass Align.</sample>
    <sample id="627">Schwachüberwachtes Lernen ermöglicht es, Neuralen Netzwerken, die auf schwachen Labels trainiert werden, robust zu machen.</sample>
    <sample id="628">Die Dokumente wurden mit manuellen und automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="629">CoNLL++-Datensatz wurde von Reuters-Newsartikeln 2020 erstellt und mit den gleichen Annotierungsleitlinien wie CoNLL 2003 erstellt.</sample>
    <sample id="630">Hallo alle, mein Name ist Yuxin Zhang und ich komme aus der Pennsylvania State University. Heute werde ich meine Arbeit präsentieren: Exemplarische, überlappendes Semantikomplemente in mehreren natürlichen Sprachen und Representationen.</sample>
    <sample id="631">So, semantic parsing is a task to build semantic representations of user queries such as SQL and lambda calculus.</sample>
    <sample id="632">Und das Semantische Parsing von mehreren Sprachen ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen.</sample>
    <sample id="633">Das Bild zeigt, dass wir die Abfrage in mehreren natürlichen Sprachen mithilfe von Nervenmodellen in SQL, Lambda oder FunkQL und so weiter übersetzen müssen.</sample>
    <sample id="634">现有的跨语言语义解析模型是单独提出的，并在有限的任务和应用数据集上进行评估。例如，</sample>
    <sample id="635">Coverage auf bestimmten natürlichen Sprachen ist fehlend, insbesondere Chinesisch.</sample>
    <sample id="636">Lack of Coverage on Certain Minor Representations</sample>
    <sample id="637">Die Lambda-Kalkulation ist fehlt.</sample>
    <sample id="638">Es gibt zwei Möglichkeiten: Entweder werden sie nur auf bestimmten Nervenmodellen ausgewertet, zum Beispiel gibt es nur ein einzelnes Modell, um die</sample>
    <sample id="639">Zum Ende dieser Arbeit haben wir Exemplare vorgeschlagen, um eine einheitliche Datensatz-Exemplar für die Kreuzsprachliche Semantikomplexe in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen zu bieten.</sample>
    <sample id="640">Es enthält 9 Datensätze in verschiedenen Bereichen, 5 Semantikonstanztesten, 8 Millionen Repräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="641">Um, um, um,</sample>
    <sample id="642">Der erste Ansatz ist "Translate Test". Wir verwenden die Google-Translate-API, um den Quelltext ins Zielsprachen zu übersetzen. Dann verwenden wir ein monolingual-Modell für die Ausbildung und die Bewertung.</sample>
    <sample id="643">Ja, zum Beispiel haben wir einen Englischmodell auf englischer Query trainiert und während der Inferenz haben wir die deutsche Query mithilfe von API ins Englische übersetzt und dann haben wir das trainierte Modell verwendet, um den SQL zuvorhellen.</sample>
    <sample id="644">Wir werden auch monolingual-Modelle testen.</sample>
    <sample id="645">In diesem Setting ist die QuellSprache der gleiche wie das ZielSprache, zum Beispiel Deutsch ins Deutsche oder Englisch ins Englische.</sample>
    <sample id="646">Wir haben auch eine monolingual-few-shot-Situation getestet, indem wir monolingual-Modelle mit nur 10% der Trainingsdaten trainierten.</sample>
    <sample id="647">Und wir haben ein multilinguales Modell, das wir mit einem Multilingüenmodell für alle Sprachen trainieren.</sample>
    <sample id="648">Zum Beispiel haben wir die Querys auf Deutsch, Englisch und Chinesisch zusammengelegt, um ein multilinguales Modell zu trainieren. Während der Schlussfolgerung können wir dieses Modell verwenden,</sample>
    <sample id="649">Sure, I can help you with that. Do you need translations for specific terms or phrases?</sample>
    <sample id="650">Und wir haben auch die Übertragung von Zero-Shot und Zero-Shot-Transfer in einer anderen Sprache in Betracht gezogen.</sample>
    <sample id="651">Während der Ausbildung trainierten wir sie auf englischen Anfragen oder auf eine Kombination aus englischen und deutschen Fuzzy-Anfragen, um ein multilinguales Modell zu trainieren, um die SQL-Ausgabe zu vorhersagen.</sample>
    <sample id="652">Und wir finden auch viele interessante Ergebnisse. So, was sich auf die Analyse von monolingualen Modellen bezieht, werden wir zwei Gruppen von Modellen bewerten.</sample>
    <sample id="653">Okay, so the content is about including encoder-PTR. This stands for multilingual pre-trained encoders with pointer-based decoders such as XLM-R plus PTR and BERT plus PTR.</sample>
    <sample id="654">Und wir haben Encoder-Decoder-Modelle, die multilingual vorbereitete Encoder-Decoder-Modelle wie M-BART und MT5,</sample>
    <sample id="655">Wir fanden, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielt.</sample>
    <sample id="656">Wir bewerten auf MT5 und XLMR + PDR in einer multilingualen Umgebung.</sample>
    <sample id="657">Wir glauben, dass Encoder-Decoder oder Encoder-PDR verbessert werden können, indem man sie in einer Mischung verschiedener Sprachen trainiert.</sample>
    <sample id="658">Und wir haben gefunden, dass dies der Grund ist, weil die meisten der Haupt-Sprachen natürlicher Sprache einen Leistungsverlust erlitten, außer dass die Englischleistung in sieben Datensätzen abfiel und nur in drei Datensätzen zu gewinnen hatte.</sample>
    <sample id="659">Ich denke, dies wird als Kurz der Multilingualität bezeichnet.</sample>
    <sample id="660">Wir haben auch den Leistungsabstand zwischen den Sprachen verglichen.</sample>
    <sample id="661">In diesem Bild zeigt sich die blaue Linie den Übergang zwischen Sprachen, die die orangefarbene Linie die Übergabe zwischen Sprachen zeigt, und die grüne Linie zeigt die Einzelsetting.</sample>
    <sample id="662">Wir haben festgestellt, dass wir die Grüne und Orange Linie verglichen haben, wir haben festgestellt, dass für die 0-Shot-Situation der Übergangsleistungsschaden bedeutend ist. Und wenn wir die blaue und orange Linie verglichen haben, haben wir festgestellt, dass für die Few-Shot-Situation der Übergangsleistungsschaden schnell verkürzt wird.</sample>
    <sample id="663">Wir finden auch andere interessante Erkenntnisse. Zum Beispiel erzeugen Encoder-Decoder-basierte Ansätze die Ergebnisse, die vorherigen Arbeiten erreichen oder vergleichbare Ergebnisse erzielen. Das Training auf Englisch und natürlicher Sprache erhöht die Leistung von Few-Shot für Ziele in natürlichen Sprachen.</sample>
    <sample id="664">Es wurde gefunden, dass multilinguales Sprachmodell wie Codex und Blue noch für die Übersetzung von Semantik nicht ausreichend sind.</sample>
    <sample id="665">Zusammenfassend bauen wir Exampler, eine einheitliche Leistungskennung für die Kreuzsprachsemantische Parsengabe mit mehreren natürlichen Sprachen und Representationen.</sample>
    <sample id="666">Wir haben eine umfassende Leistungsstudie über drei repräsentativen Typen von multilingualen Sprachmodellen durchgeführt, und unsere Ergebnisse zeigen viele interessante Erkenntnisse usw. Willkommen, Besuchen Sie unsere Publikation und Code. Vielen Dank für Ihre Aufmerksamkeit</sample>
    <sample id="667">Existing works can be broadly classified into four categories:</sample>
    <sample id="668">Mehrsprachige LLMs wie Codex oder Bloom sind für CLSP nicht ausreichend.</sample>
    <sample id="695">Die Methode mit der Mehrdeutigkeit der Permutationen wird durch die Induktion der Anordnung als Teil der Ausbildung angebracht.</sample>
    <sample id="696">Fairness of a fine-tuned NLP model is defined by its ability to avoid bias and ensure equal treatment for all users, regardless of their political opinions or other characteristics.</sample>
    <sample id="697">Jani Slavak</sample>
    <sample id="698">Kostas Sena</sample>
    <sample id="699">Myra</sample>
    <sample id="700">Tropicalism refers to a literary and artistic movement that emerged in Brazil during the 1920s. It is characterized by its use of exotic, lush imagery and themes related to nature, sensuality, and spirituality. The term "tropical" often evokes ideas of paradise or an idyllic setting with rich vegetation and vibrant colors.

In literature, tropicalism can be seen as a way to explore identity, culture, and the human experience through vivid descriptions of landscapes and emotions associated with them. This style has influenced various forms of art including poetry, music, painting, and film across different cultures around the world.

The concept of tropicalism also touches upon broader social issues such as colonialism, race relations, and globalization which are reflected within certain works under this umbrella term.</sample>
    <sample id="701">The authors created the descriptions of target groups by analyzing words used in online reviews. They focused on terms like "culture," "tradition," "proud," and "exotic" for Mark groups, which define these groups based on their identity relationships and distinguish them from white norms.</sample>
    <sample id="702">Pointwise CxMI</sample>
    <sample id="703">DrBERT ist eine Version des BERT-Modells, das mit 7 GB von Natusos trainiert wurde. ChuBERT ist ein klinischer Modell, das mit 4 GB von Sätzen trainiert wurde, die aus CleanConlls stammen.</sample>
    <sample id="751">Zhiyong and I</sample>
    <sample id="752">Iterative updates the model by training on the latest set of data collected.</sample>
    <sample id="753">Das Ziel des Datensatzes ist es, das Verständnis von Benutzersprache zu verstärken, wenn sie eine Wahl treffen möchten.</sample>
    <sample id="754">Ein Angreifer kann Modellparameter über einen EaaS (Embedded Adversarial Attack Service) extrahieren.</sample>
    <sample id="755">There are three authors involved in this work.</sample>
    <sample id="756">Zwei Annotatoren wurden verwendet, um den ursprünglichen Datensatz zu erstellen.</sample>
    <sample id="757">Carnegie Mellon University, University of Washington, Allen Institute for AI</sample>
    <sample id="758">The example is "Isa Bart and Lisa".</sample>
    <sample id="759">ABCEval is capable of measuring the rates at which chat models will commit various thematic errors.</sample>
    <sample id="760">We must evaluate the models' acceptability across the entire context window because large language models are now generating longer and more complex contexts. This requires a comprehensive assessment to ensure that the model's performance is consistent throughout, not just at specific points within the text. Evaluating over the whole context helps identify any potential issues or biases in how the model processes information as it appears sequentially rather than only focusing on isolated segments of input data.</sample>
    <sample id="761">No, the multilingual training did not lead to a performance drop in English compared to the monolingual model.</sample>
    <sample id="762">Ja, die Annotatoren kennen die Entität im Voraus.</sample>
    <sample id="763">The MT metrics used for evaluation are examples.</sample>
    <sample id="764">Ja, die Regression beeinflusst die allgemeine Leistung bei bestimmten NER-Typen.</sample>
    <sample id="765">Positionalität für NLP ist wichtig, weil es dazu beiträgt, dass Systeme besser an die Bedürfnisse und Kontexte unterschiedlicher Gruppen angepasst sind.</sample>
    <sample id="766">Mehrsprachige LLMs wie BLOOM wurden durch Adapter angepasst.</sample>
    <sample id="767">BERT</sample>
    <sample id="768">We saw that the actual form of the prompting doesn't have a big influence in the case of several short prompting.</sample>
    <sample id="769">Die Autoren haben schließlich drei Empfehlungen vorgeschlagen.</sample>
    <sample id="770">The proposed method achieves a 10% higher constraint distribution compared to the strongest baseline.</sample>
    <sample id="771">Shuhang</sample>
    <sample id="772">Ja, die Studie bietet Ergebnisse und einen Datensatz als Referenz für zukünftige Forschung im Bereich der automatischen Textsimplifizierung.</sample>
    <sample id="773">Zwei</sample>
    <sample id="774">OFA</sample>
    <sample id="833">The authors belong to the University of California, Berkeley.</sample>
    <sample id="834">Stony Brook University</sample>
    <sample id="835">Deutsch-Englisch, Englisch-Französisch und Französisch-Deutsch wurden untersucht.</sample>
    <sample id="836">Shangbin</sample>
    <sample id="837">Two models were fine-tuned: a long part model for document-level simplifications and a normal-based long part model for sentence-level simplifications.</sample>
    <sample id="838">53</sample>
    <sample id="839">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="840">AG News, MIMED, SST-2 and ERS-Spam</sample>
    <sample id="876">NACHOS ist ein Datensatz von medizinischen Crawdaten, der von der World Wide Web stammt.</sample>
    <sample id="877">Ariel Viladeguix</sample>
    <sample id="878">Prompting has a big influence on the performance of LLMs for translation.</sample>
    <sample id="879">Patrick Fernandes, Amy Liu, Andre F. D. Martins und Graham Neubig gehören an der Universität Massachusetts Lowell (UML).</sample>
    <sample id="880">1. We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon.</sample>
    <sample id="881">Die Autoren präsentieren eine Aufgabe zur Korrektureinordnung, die darauf abzielt, zu erkunden, ob Modelle in der Lage sind, Kenntnisse aus verschiedenen Quellen zu nutzen. Sie evaluieren das Dataset mit Teilnehmern an Studien und bestätigen Korrektureinordnung-Modelle.</sample>
    <sample id="882">Hallo alle, mein Name ist Ariel Bilaad und ich werde einen kurzen Überblick über den Paper "Granting Power from Translation: Assessing Strategies and Performance" geben. Dies ist gemeinsame Arbeit mit meinen Kollegen von Google Translate.</sample>
    <sample id="883">Bram ist ein 540 Milliarden Parameter großes Sprachmodell, das im Jahr 2022 präsentiert wurde. Es wurde auf einem großen Sammlung von Texten trainiert, die 780 Billionen Token umfassen.</sample>
    <sample id="884">Es erreicht im Vergleich zu anderen Arbeiten die höchsten Leistungen in hunderten von NLP-Aufgaben.</sample>
    <sample id="885">In this work, we present the first systematic study of large language model prompting for machine translation.</sample>
    <sample id="886">Wir haben die Übersetzungsfähigkeit dieser Modelle mit den besten Praktiken der MT-Community-evaluiert. Das umfasst das Vermeiden einer Überlappung der Testdaten mit der Trainingsdaten des Sprachmodells durch die Verwendung der neuesten Testdaten.</sample>
    <sample id="887">Und wir haben zwei hochwertige Systeme verglichen.</sample>
    <sample id="888">Wir verwenden die neuesten NMT-Matrixen und zeigen zusätzlich Ergebnisse der Evaluierung nach Fachwissen. Schließlich bieten wir einige Empfehlungen für Prom-Selectionsstrategien an.</sample>
    <sample id="889">Das Prompting hat auf die Leistung der LLMs für die Übersetzung einen beeinflussenden Einfluss. Wir können dies anhand eines einfachen Versuchs beobachten, bei dem wir ein einwegiges Prompting verwenden und zwei verschiedene Prompts für einen Satz bereitstellen.</sample>
    <sample id="890">Die meisten Sätze, 516 von 1000, zeigen eine Verschiebung von mehr als ein paar Punkten.</sample>
    <sample id="891">Und das kann in extremen Fällen bis zu 40 Punkten gehen. Es ist wichtig, eine gute Promotionsstrategie auszuwählen.</sample>
    <sample id="892">In our experiments, we settle for a five-shot prompting strategy where we just mark each sentence that we provide to the system with its language.</sample>
    <sample id="893">In diesem Beispiel hier, in dem wir Übersetzungen von Deutsch ins Englische machen, werden die deutsche Sätze als Quellsätze mit deutscher Kolonie und die englischen Übersetzungen mit englischer Kolonie markiert.</sample>
    <sample id="894">Wir haben gesehen, dass die tatsächliche Form der Prompting keine große Einfluss auf den Fall mehrerer kurzer Promptings hat.</sample>
    <sample id="895">Es ist für Zero- und One-Shot-Prompting von entscheidender Bedeutung, und wenn wir wie in unserem Fall zu Fünf-Shot-Prompting gehen, gibt es fast keine Unterschiede im tatsächlichen Prompt.</sample>
    <sample id="896">Es sind die Beispiele, die den größten Teil des Gewichts tragen.</sample>
    <sample id="897">Die Zusammenfassung unserer experimentellen Ergebnisse lautet, dass die Beispielleistung wichtiger ist als die Ähnlichkeit zur Quellsentence.</sample>
    <sample id="898">Es ist wichtig, die Beispiele aus hochwertigen Übersetzungen zu wählen. Im Detail verglichen wir die Auswahlvorschläge aus der Trainingsdatenbank der WMT-Evaluierungen oder der Dev-Datenbank.</sample>
    <sample id="899">Die Entwicklungsdaten sind viel genauere und von höherer Qualität als die Trainingsdaten, die mehr Rauschen haben, und die Ergebnisse geben eine bessere Leistung bei der Nutzung der Entwicklungsdaten.</sample>
    <sample id="900">Trotzdem haben spezialisierte, state-of-the-art Systeme einen bedeutenden Vorteil gegenüber PAM-Übersetzungen, aber PAM ist sehr nahe bei einem kommerziellen System. In unserem Fall haben wir uns auf Google Translate entschieden.</sample>
    <sample id="901">Die Erkenntnisse, die wir von der E-Mail-Analyse mit dem MQM-Framework gewinnen, zeigen, dass die Flüssigkeit von PALM mit den Systemsystemen gleichwertig ist. Der Hauptunterschied kommt jedoch aus der Genauigkeit.</sample>
    <sample id="902">Immer wieder passieren Verwechslungen.</sample>
    <sample id="903">Es scheint, dass PAM die Entscheidung trifft, eine bessere klingende Übersetzung zu produzieren, manchmal indem es Teile der Quellsprache droppt, die in der Übersetzung nicht vorkommen.</sample>
    <sample id="904">Allerdings ist die Stufe "awkward" für PAN unterhalb der Stufe "state-of-the-art-systems", was ein weiteres Signal ist,</sample>
    <sample id="905">DAN: Das PARROT-System bietet eine flüssige Ausgabe, aber mit einer Reduzierung der Genauigkeit.</sample>
    <sample id="906">Und das ist alles für diese kurze Überblicksrede. Für weitere Details, bitte zu der vollständigen Präsentation des Papiers. Vielen Dank</sample>
    <sample id="907">Hallo, ich bin David, ein Promotionsstudent an der Saarland University in Deutschland. In diesem Video möchte ich gerne unsere kürzliche Arbeit präsentieren: "Worse Than You Think - Ein kritischer Blick auf schwache Supervision".</sample>
    <sample id="908">Dies ist gemeinsame Arbeit mit Xiao Yu Shen, Mario Smousek, Dierdre Stefan und Dietrich Clarko.</sample>
    <sample id="909">Ich möchte beginnen mit einer kurzen Einführung in die Wöchentliche Überwachung und die Wöchentliche Supervision.</sample>
    <sample id="910">In weak supervision, we do not manually label the data. Instead, we label the data using weak labeling sources such as simple heuristic rules, knowledge bases or low-quality crowdsourcing, as illustrated in the figure on the right.</sample>
    <sample id="911">Wenn man sie mit menschlichen Notationen vergleicht, sind die wiki-Notationen viel billiger. Dennoch sind sie auch "raunig", was bedeutet, dass eine bestimmte Menge der Notationen falsch sind.</sample>
    <sample id="912">Wenn wir die Neuralen Netzwerke direkt auf den weakly labelierten Daten trainieren, lernen sie die Lärm des Labels ab und können nicht generalisieren.</sample>
    <sample id="913">In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well.</sample>
    <sample id="914">In recent works in WSL, a common claim is that people say they only train models on the weekly labeled data and achieve high performance on clean test sets.</sample>
    <sample id="915">Tecnicchli, diese Behauptung ist nicht falsch, aber es gibt einen Haken.</sample>
    <sample id="916">Es ist, dass Menschen denken, dass es einen zusätzlichen sauberen Validations-Set gibt, das verfügbar für die Modellauswahl ist.</sample>
    <sample id="917">Wir haben auf diese Problemstellung gestopft, da dies impliziert, dass zusätzliche manuelle Anmerkungen in Weakly-Supervised Learning erforderlich sind. Aber, wie ein Elefant im Zimmer, wird diese Notwendigkeit oft übersehen.</sample>
    <sample id="918">Die oben genannten Fragen stellen wir uns. Zunächst, ist validation-daten für WSL notwendig oder können wir stattdessen eine noisige Validationssammlung verwenden?</sample>
    <sample id="919">Zweitens, wenn sauberer Daten erforderlich sind oder es für WSL2 erforderlich ist, dann wie viele saubere Beispiele benötigen wir? Schließlich sollten wir nur die sauberen Beispiele für die Grundlage verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen?</sample>
    <sample id="920">Wir haben diese Forschungsfragen in unserem Arbeit bearbeitet und unsere Ergebnisse lauten wie folgt:</sample>
    <sample id="921">Zunächst finden wir, dass WSL-Methoden tatsächlich ordnungsgemäße Ergebnisse ergeben, wenn man geeignete Validations-Samples verwendet.</sample>
    <sample id="922">Otherwise, there is a large performance drop as shown in this figure. If there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels.</sample>
    <sample id="923">Das bedeutet, dass das Training ohne Sinn ist.</sample>
    <sample id="924">Dies zeigt an, dass WSL-Methoden tatsächlich saubere, etiketierte Daten benötigen, um richtig zu funktionieren. Und die Annotationskosten, um saubere Validations-Samples zu erhalten, sollten nicht vernachlässigt werden.</sample>
    <sample id="925">Unser zweiter Fund ist, dass die Anzahl der validierten Clean-Samples es den WSL-Anwendungen hilft, bessere Leistung zu erzielen, wie im Bild links gezeigt.</sample>
    <sample id="926">Typischerweise benötigen wir nur 20 Proben pro Klasse, um eine hohe Leistung zu erzielen.</sample>
    <sample id="927">Aber das ist nicht das Ende der Geschichte, denn wenn wir die ausgewählten sauberen Proben direkt trainieren, erreichen wir sogar bessere Leistungen.</sample>
    <sample id="928">Die rechte Figur zeigt die Leistungsunterschiede zwischen den anhand der sauberen Daten direkt angewendeten Fine-Tuning-Anwendungen und den WSL-Anwendungen, die die saubere Daten nur für die Validierung verwenden.</sample>
    <sample id="929">Wie man sieht, wenn wir 10 Proben pro Klasse haben, entwickelt sich die direkte Feinabfinung zu den WSL-Anläufen.</sample>
    <sample id="930">Zuletzt können die Leistungsverbesserungen, die in den vorherigen WSL-Methoden angeboten wurden, leicht erreicht werden, indem man weiterhin auf den sauberen Validations-Samples anpasst.</sample>
    <sample id="931">Wie wir aus den Abbildungen sehen können, leistet das von Malina entwickelte Modell, genannt FTW, am Anfang die weniger komplizierte WSL-Methode wie Cosine schlechter als die WSL-Methoden.</sample>
    <sample id="932">Allerdings, wenn wir es weiter auf den sauberen Stücken ausprobieren, leistet FTW auf Gleicher Weise wie andere Methoden.</sample>
    <sample id="933">Also, in der Praxis gibt es keinen Grund, WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern.</sample>
    <sample id="934">Zusammenfassend zeigen unsere Ergebnisse, dass kürzlichere WSL-Anlässe auf saubere, manualer annotierte Beispiele angewiesen sind, um richtig zu funktionieren. Ihre Leistungssteigerung und Praktikabilität werden stark übertrieben geschätzt.</sample>
    <sample id="935">Unspezifizierte Empfehlungen für zukünftige Arbeit sind wie folgt:</sample>
    <sample id="936">Zuerst berichten Sie die Modellauswahlkriterien. Zum Beispiel berichten Sie, ob das Modell ausgewählt wurde, während von Validations-Samples.</sample>
    <sample id="937">Zweitens sollten die WSL-Anläufe mit der vorherigen Lernungsgrundlage verglichen werden, da sie beide Arbeiten auf gleichen Szenarien betreiben. Drittens sollte das kontinuierliche Feinabpolieren als einfache und starke Grundlage in Zukunft bei den WSL-Anläufen berücksichtigt werden.</sample>
    <sample id="938">Zuletzt haben wir unseren Code freigelegt. Sie können ihn über den QR-Code auf dieser Seite finden. Bitte fühlen Sie sich frei, ihn zu prüfen. Vielen Dank und genießen Sie den Kongress</sample>
    <sample id="939">Common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="940">Sebastian Senti, Ronan Le Bras, Katerina Rynika, and Martin Sap</sample>
    <sample id="941">Servin ist Richter, Kea ist Backerin. Servin und Kea haben nach einem langen Tag im Gerichtshaus entschieden, dass sie sich entspannen. Der Aufgabe ist es, den richtigen Entity, die das Pronomen "he" bezieht, zu identifizieren, was in diesem Fall Servin ist. Die Lösung eines gegebenen Pronouns erfordert zwei Arten von Informationen: Erste, spezifische Entity-Knowledge, wie Servin ist Richter; und zweite, Hintergrundwissen, wie Richter entscheiden in Gerichten. Im Allgemeinen wird Hintergrundwissen während der Vorausbildung großer Sprachmodelle gelernt, während Entity-Spezifisches Wissen typischerweise während des Inferenzzeitpunkts bereitgestellt wird. Wir variieren die Verfügbarkeit dieser beiden Informationen, so dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können.</sample>
    <sample id="942">Ja, der Code ist auf GitHub verfügbar.</sample>
    <sample id="943">No, the annotators are not balanced in terms of demographics.</sample>
    <sample id="944">Sätze innerhalb der akzeptablen Domain wurden durch den Match-Prefix verändert.</sample>
    <sample id="945">Eine dimensionale Bewertung bedeutet, dass man mehrere Aspekte oder Merkmale eines Gegenstands betrachtet und bewertet. In diesem Fall könnte dies bedeutend sein, um eine detaillierte Analyse der Qualität des Chats zu ermöglichen, indem man verschiedene Faktoren wie Klarheit, Kohärenz, Genauigkeit und Benutzerfreundlichkeit berücksichtigt.</sample>
    <sample id="946">University of Science and Technology of China</sample>
    <sample id="947">It's crucial for zero and one shot prompting.</sample>
    <sample id="978">ABC eval</sample>
    <sample id="979">Zwei Autoren sind an der Arbeit beteiligt: Jinwei Yi und Yuxin Liu.</sample>
    <sample id="980">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="981">There are 4 authors involved in this work.</sample>
    <sample id="982">Vasudha</sample>
    <sample id="983">The authors belong to the University of Warsaw.</sample>
    <sample id="1021">The most common errors are omission errors.</sample>
    <sample id="1022">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Heute werden wir Ihnen alles über ABC-Eval erzählen, eine neue dimensionale Ansatz zur Bewertung von Conversational AI.</sample>
    <sample id="1023">Dieses Werk wurde von dem Emory-NLP-Lab, das von Professor Jino Choi an der Emory University geleitet und zusammen mit Amazon Alexa AI.</sample>
    <sample id="1024">Lass uns annehmen, dass Sie gerade einen Dialogmodell entwickelt haben und möchten sehen, wie es sich gegen die aktuelle Kunst der Dinge absetzt.</sample>
    <sample id="1025">Die allgemeine Praxis besteht darin, menschliche Bewertungen zu verwenden, wie zum Beispiel indem man menschliche Richter fragt, welche von zwei Gesprächen besser ist oder Gespräche anhand einer Likert-Skala bewerten.</sample>
    <sample id="1026">Diese Ansätze funktionieren gut, um eine umfassende Bewertung der Gesamtkommunikationsqualität zu liefern. Allerdings hat Kommunikationsqualität viele Aspekte. Daher möglicherweise möchten Sie mehrere Aspekten der Chatskala bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.</sample>
    <sample id="1027">Eine Methode besteht darin, menschliche Richter einfach zu fragen, mehrere Aspekte der Dialogqualität zu bewerten, wie zum Beispiel die Relevanz der Modulantworten, mit vorhandenen vergleichenden oder Likert-Skala-Methoden.</sample>
    <sample id="1028">Wir glauben jedoch, dass eine genauer und zuverlässiger Strategie für die Bewertung von dimensionalem Dialog vorhanden ist.</sample>
    <sample id="1029">Unsere Ansatz versucht, die Subjektivität der menschlichen Bewertung durch explizite Anmerkungen zu reduzieren, ob jeder Modell-Feedback bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel das Anbringen von irrelevanter Informationen oder sich selbst widerzusagen.</sample>
    <sample id="1030">Wir nennen diese Ansatz Annotieren von Verhaltensweisen im Chat oder kurz ABC-Eval. Wir haben diesen Ansatz entwickelt, um eine umfassende Überlappung der Verhaltensweisen von Chat-Modellen zu erreichen, die in der jüngsten Literatur vorgeschlagen wurden, um den Chatqualität zu beeinflussen.</sample>
    <sample id="1031">ABC-EVAL kann die Geschwindigkeit messen, mit der Chat-Modelle unterschiedliche Thematikfehler begehen.</sample>
    <sample id="1032">Zum Beispiel misst ABC-eval die Anzahl der Runden, in denen ein Chat-Modell seinen Partnern nicht zugehört oder etwas Ungewöhnliches sagt.</sample>
    <sample id="1033">Entgegen seiner eigenen oder seiner Partys Position, verleiht falsche Informationen oder gegen allgemein akzeptierte Wissen und -prinzipien.</sample>
    <sample id="1034">Um zu bestimmen, welche Art von Bewertung am effektivsten ist, haben wir vier state-of-the-art-Chatmodelle ausgewählt und haben sie auf 100 mensch-maschine-Dialoge pro Modell mit ABC-Evaluated</sample>
    <sample id="1035">Für die Vergleichsweise Bewertung haben wir diese Gespräche mit drei vorhandenen Methoden bewertet: Likert-Skala-Bewertungen auf der Niveau, Likert-Skala-Bewertungen auf der Niveau und Paarvergleich auf der Niveau.</sample>
    <sample id="1036">Für jede der vorhandenen Methoden haben wir Bewertungen für acht der am häufigsten gemessenen Aspekte von Dialoge sammelt, da dies die Standardpraxis für die Bewertung von Chat-Modellen in mehreren Dimensionen ist.</sample>
    <sample id="1037">Wir haben unsere Analyse dieser Bewertungsresultate durchgeführt und haben festgestellt, dass die ABC-Behavior-Labels im Ganzen zuverlässiger sind als die von bestehenden Methoden sammelten Labels, wie es durch die interne Annotatorenvereinbarkeit auf hundert doppelbeziehungsweise beschrifteten Gesprächen bewährt wurde.</sample>
    <sample id="1038">Darüber hinaus sind die ABC-Eval-Labels besser vorhersagbar als die von bestehenden Methoden erzeugten Metriken, wie dies eine einfache lineare Regression-Analyse zeigt.</sample>
    <sample id="1039">Zum Beispiel können Sie sehen, wie das Maß an Wenden mit Selbst- und Partnerkontrasten 5% und 10% der Gesprächsqualität erklären, während die durchschnittlichen Likert-Konsistenz-Scores nur 4% oder weniger erklären.</sample>
    <sample id="1040">Zuletzt haben wir geprüft, ob jede Bewertungsvariable eine einzigartige Aspekt der Chats-Qualität einfängt, indem wir eine Schrittweise lineare Regression nutzten.</sample>
    <sample id="1041">Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25% der Gesprächsqualität erklärt. Und wenn Sie die Metriken eins nach dem anderen entfernen, verlieren die meisten von ihnen eine angemessene Menge an Informationen über die Qualität.</sample>
    <sample id="1042">Andererseits erklären die Kombination der Allturn-Level-Likert-Metriken viel weniger Qualität und weniger dieser Metriken tragen einzigartige Informationen mit.</sample>
    <sample id="1043">Diese zuverlässigen, informativen und einzigartigen ABC-Eval-Metriken ermöglichen es uns, Konversational-AI mit einer höheren Auflösung zu bewerten, als die vorherigen Methoden es erreichen können.</sample>
    <sample id="1044">Sie können im Ergebnis unserer Experimente sehen, dass mehrere Herausforderungen immer noch bestehen und genau gemessen wurden. Zum Beispiel haben die Boten, die wir getestet haben, Vernunftverstöße in etwa 20% ihrer Antworten.</sample>
    <sample id="1045">Sie produzieren irrelevante Informationen in etwa 15 % der Antworten und sie widerhiteln sich oder ihren Partner etwa 10 % der Zeit.</sample>
    <sample id="1046">Mit der schnellen Verbesserung im Bereich könnten viele dieser Fehlerwahlen nach unserer Bewertung abnehmen. Dennoch ist dies noch ein weiterer Grund, sich anvertraut zu machen mit Zuverlässigen und genaueren Bewertungsmaßstäben, um Modelle miteinander zu vergleichen.</sample>
    <sample id="1047">Wir hoffen, dass ABC-Eval von anderen im Bereich genutzt werden kann, um ein bedeutendes Schritt in dieser Richtung zu sein. Wir freuen uns darauf, zu sehen, wie künstliche Intelligenz im kommenden Monaten und Jahren weiterentwickelt wird. Vielen Dank fürs Zusehen</sample>
    <sample id="1048">Emory University</sample>
    <sample id="1049">CFT steht für "Continuous Fine-Tuning".</sample>
    <sample id="1050">Es sind sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="1051">Hallo, mein Name ist Kai-O-Yen und ich werde unsere Arbeit präsentieren, die titelte: Wann benötigt Übersetzung Kontext? Eine datenschwimmende multilingual Exploration. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Amy Liu, Andre F.D. Martins und Graham Neubig gemacht.</sample>
    <sample id="1052">Ja, viele Übersetzungen hängen von Kontext ab. Zum Beispiel, wie übersetzen wir "mole" in diesem Satz?</sample>
    <sample id="1053">Mole refers to a spy.</sample>
    <sample id="1054">Je nach Kontext ändert sich die Bedeutung des Wortes und daher auch seine Übersetzung.</sample>
    <sample id="1055">Aber, die Bewertung der Übersetzungsfähigkeit von Modellen in solchen Fällen ist sehr schwierig. Zunächst, weil nur eine kleine Brücke Übersetzungen auf Kontext abhängen, was es denkorpussebene-Metriken wie Blue unmöglich macht, diese Übersetzungen zu erfassen.</sample>
    <sample id="1056">Und einige Leute haben daraufhin empfohlen, Kontextabhängige Übersetzungen aufzustellen. Aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Satzungen von Sprachen, da sie meist auf Fachwissen und menschliche Curations angewiesen sind.</sample>
    <sample id="1057">In this work, we try to answer these two questions: First, when does translation require context? And second, how well do models handle these cases?</sample>
    <sample id="1058">Um, um,</sample>
    <sample id="1059">In our previous work, we introduced CxMI as a measure for context usage by machine translation models. This is done by measuring how much information the context provides about the target Y given the source X.</sample>
    <sample id="1060">Sie können CXMI als das Information, das man von dem Modell durch Kontextierung gewinnt, betrachten.</sample>
    <sample id="1061">In diesem Arbeit erstrecken wir das CxMI auf Punktwise-CxMI, das Kontextnutzung auf Satz- oder Wortebene messen kann. Wir können Wörter mit hohem P-CxMI als solche, die Kontextbedingungen für die Übersetzung benötigen, betrachten.</sample>
    <sample id="1062">Wir analysieren nun Wörter mit hoher P-SMI, um Muster zwischen diesen Wörtern zu finden.</sample>
    <sample id="1063">Und wir führen unsere Analyse auf Aufnahmen von TED-Talks durch, die von Englisch ins gesamte 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="1064">Wir führen unsere Analyse auf drei verschiedenen Niveaus durch. Zunächst betrachten wir die Part-of-Speech-Tags, die hohe Median-Werte haben.</sample>
    <sample id="1065">Dies ermöglicht es uns, zum Beispiel, Doppelpronomen im Arabischen zu finden, die relativ hoch p6mi haben. Das kann erklärt werden, weil Englisch keine Doppelpronome hat, daher benötigt man Kontext, um festzustellen, ob ein Pronomina dual ist, wenn ins Arabische übersetzt wird.</sample>
    <sample id="1066">Und ähnlich finden wir, dass bestimmte Sprachen auch Kontext erfordern, wenn wir die geeignete Verbenform wählen möchten. Wir schauen dann auf Wörter, die eine hohe PSEMI über all ihren verschiedenen Vorkommen haben.</sample>
    <sample id="1067">Das hilft dabei, Fälle zu identifizieren, wie die hier, in denen Sie Kontext benötigen, um Proper-Nomina zuzuadehen, um sicherzustellen, dass Sie die gleiche Übersetzung innerhalb des Dokuments verwenden.</sample>
    <sample id="1068">Und ähnlich finden wir, dass Kontext die Entscheidung unterstützt, die richtige Formel zu verwenden.</sample>
    <sample id="1069">Zuletzt betrachten wir die einzelnen Token, die hohe p-Semantik-MI haben. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht von dem Wort allein gefangen werden können, sondern in der Satzstruktur ausgedrückt werden, wie zum Beispiel Ellipsesresolutions.</sample>
    <sample id="1070">Also verwenden wir unsere Erkenntnisse aus der Analyse, um eine Leistungsbenchmarke für die Dokumentenebene zu erstellen.</sample>
    <sample id="1071">Für jeden der fünf Diskurserscheinungen, die wir identifiziert haben, erstellten wir Tagger, um Wörter zu identifizieren, die sich auf das Ereignis beziehen. Wir nennen unseren Tagger den Multilingüen-Diskursbewussten-Tagger (MDA).</sample>
    <sample id="1072">Wir können auch bemerken, dass verschiedene Sprachen unterschiedliche Proportionen dieser Diskurserscheinungen haben.</sample>
    <sample id="1073">Wir verwenden dann den Muda-Tagger, indem wir den Tagger auf dem parallelten Körperset anwenden, das wir für die Bewertung verwenden möchten. Und wir verwenden unsere bevorzugte Übersetzungsmetriken auf die kontextabhängigen Beispiele, die vom Muda-Tagger identifiziert wurden.</sample>
    <sample id="1074">Und schließlich verwenden wir unsere Leistungsindikatoren sowie andere Metriken, um verschiedene Modelle auf der Ebene von Dokumenten zu bewerten.</sample>
    <sample id="1075">Zunächst einmal, wenn wir Korpusstufenmuster verwenden, finden wir für Blue, dass kohlenstoffunabhängige Modelle die beste Leistung haben.</sample>
    <sample id="1076">Es handelt sich um eine Übersetzung des englischen Inhalts ins Deutsche.</sample>
    <sample id="1077">Dies zeigt wieder, dass es schwierig ist, die beste document-level-Übersetzungssysteme allein auf Basis von Korpus-Level-Metriken zu bestimmen.</sample>
    <sample id="1078">Wir verwenden die Moovit-Benchmark, um die Modelle zu bewerten und finden, dass Kontextsummarierungsmodelle viel genauere sind als Modelle, die den Kontext nicht für bestimmte Diskurserscheinungen wie Formelleit und Lexikalkohärenz nutzen.</sample>
    <sample id="1079">Aber diese Modelle sind nicht viel besser als Modelle, die Kontext auf andere Phänomene wie Ellipsen, Pronomen und Verben nicht verwenden. Das suggeriert, wo wir mehr Fortschritte für documentale Übersetzung sehen müssen.</sample>
    <sample id="1080">Wir haben auch verschiedene kommerzielle Systeme verglichen und unsere Leistungsprobe zeigt, dass DeepL üblicherweise für Dokumentenübersetzungen genauere Ergebnisse liefert als Google Translate.</sample>
    <sample id="1081">Zusammenfassend haben wir eine datenbasierte Analyse über 14 Sprachpaare durchgeführt, um zu identifizieren, wann Übersetzungen Kontext benötigen.</sample>
    <sample id="1082">Und dann verwenden wir unsere Erkenntnisse, um eine Leistungsstufe für die Dokumentenbasierte Maschinenübersetzung zu erstellen, die uns hilft, zu erkennen, welche Diskurserscheinungen Modelle gut bewältigen können und welche Übersetzungssysteme auf der Ebene des Dokuments gut sind.</sample>
    <sample id="1083">Vielen Dank fürs Zuhören Ich sehe Sie in Toronto</sample>
    <sample id="1084">Xuzhen Zhang</sample>
    <sample id="1121">No Name</sample>
    <sample id="1122">Die Autoren beschreiben die Methode der "markierten Wörter" als eine Methode zur Identifizierung von Wörtern, die die Gruppen unterscheiden, die als markierte Gruppen bezeichnet werden.</sample>
    <sample id="1123">The authors belong to the University of Washington.</sample>
    <sample id="1124">Prague Approach</sample>
    <sample id="1125">James Finch</sample>
    <sample id="1126">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="1127">Minimal Pair Paradigm</sample>
    <sample id="1161">Die fünf Methoden für die erste Forschungsfrage sind: 1. WSL, 2. WSL-2, 3. WSL-3, 4. WSL-4 und 5. WSL-5.</sample>
    <sample id="1162">Das Modell wird auf 11 verschiedenen Aufgaben in der Biomedizinischen und klinischen Analyse abgestimmt.</sample>
    <sample id="1226">CamemBERT wurde ursprünglich mit der Naturmalware-Datenbank trainiert.</sample>
    <sample id="1227">Szymon Skurkowski</sample>
    <sample id="1228">Temporale Verzögerung</sample>
    <sample id="1269">Um die Token für die Ausgabesequenz zu permutieren, ist es notwendig, weil sie nach dem ersten Schritt nicht in der richtigen Reihenfolge sind.</sample>
    <sample id="1270">The authors suggest that model developers should make their methods for reducing bias more transparent because it helps to understand the underlying mechanisms and ensures accountability. Increased transparency can also help in identifying any unintended consequences or biases introduced by these methods, leading to better-informed decisions about how to address them effectively.</sample>
    <sample id="1271">Minimalpaareingaben, die akzeptabel sind.</sample>
    <sample id="1272">The authors used the following evaluation metrics: F1 score, precision, recall, and mean squared error.</sample>
    <sample id="1273">Interannotator agreement</sample>
    <sample id="1274">Wikipedia</sample>
    <sample id="1275">The authors belong to the University of Tübingen.</sample>
    <sample id="1276">MultiInstruct unterscheidet sich von anderen Benchmarks, indem es eine große Sammlung von mehreren visuellen und textuellen Instruktionstasks bietet. Diese umfassende Sammlung ermöglicht es Forschern und Entwicklern, die Leistung von Multimodell-Modellen bei der Verarbeitung von visuellen und textuellen Daten zu überprüfen und zu verbessern.</sample>
    <sample id="1277">Zwei Autoren sind an der Arbeit beteiligt: James Finch und Sarah Finch.</sample>
    <sample id="1278">binäre Koordination</sample>
    <sample id="1279">Die Prompts im Durchschnitt waren etwa 20 Wörter lang.</sample>
    <sample id="1280">Smaller models can surpass larger ones when properly trained on suitable datasets.</sample>
    <sample id="1281">Hallo, ich bin Janis Slawak und präsentiere Ihnen unsere Arbeit über "Dr. BERT", ein robustes, vorbereitetes Modell auf Französisch für biomedizinische und klinische Anwendungen.</sample>
    <sample id="1282">In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Dann präsentieren wir die Hauptbeiträge unserer Artikel.</sample>
    <sample id="1283">Wir haben eine neue Biomedizinische Modell in Französisch eingeführt, namens Dr. BERT, basierend auf Roberta und trainiert auf NACHOS, einem Datensatz medizinischer Crawledata von der Webseite.</sample>
    <sample id="1284">Wir präsentieren auch eine Vergleichung von Modellen mit mehrfach returning Settings und Datenquellen. Dann präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Downstream Tasks in Frankreich.</sample>
    <sample id="1285">Schließlich fassen wir die Experimente zusammen und geben Ihnen weitere Informationen darüber, wie Sie auf diese Modelle zugreifen können.</sample>
    <sample id="1286">Seit seiner Veröffentlichung im Jahr 2018 hat BERT zu einer der am effektivsten Ansätze für die Lösung von Sprachverarbeitungsaufgaben geworden und bietet einen großen Leistungsverlust im Vergleich zu historischen statischen und kontextuellen Methoden wie Word2Vec, FastText oder ANIMAL.</sample>
    <sample id="1287">Seitdem wurde dieses Modell auf viele andere Sprachen angepasst, wie zum Beispiel auf Französisch mit Camembert und auf andere Bereiche wie Biomedizin mit PubMed-BERT und BioBERT und auf klinische Bereiche mit Clinical-BERT, aber hauptsächlich auf Englisch.</sample>
    <sample id="1288">Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlicher Präsentation, da die Mangel an In-Domain-Daten</sample>
    <sample id="1289">Allerdings hatte Frankreich bislang kein offenes-Source-Modul für Biomedizinische Entschleunigung.</sample>
    <sample id="1290">Wir sollten uns fragen, welche Datenquelle für eine Vielzahl von Anwendungen am besten geeignet ist. Diese Crowd-Daten sind eine gute Ersatzquelle für klinische Daten.</sample>
    <sample id="1291">Um diese Frage zu beantworten, verglichen wir den Dr. Bert mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die aus dem Nantes University Hospital Data Warehouse stammen.</sample>
    <sample id="1292">Nach und für, fragen wir uns, wie viel Daten brauchen wir, um ein Spezialmodell auf französischen Daten zu trainieren? Ist es 4 GB, 8 GB oder mehr?</sample>
    <sample id="1293">Um diese Fragen zu beantworten, haben wir zunächst vier von scratch Modelle trainiert und verglichen.</sample>
    <sample id="1294">Eine erste Version von Schubert, die eine klinische Modell ist, mit 4 GB von Sätzen, die aus klinischen Notizen stammen. Eine endgültige Version von Schubert, die eine Mischung aus 4 GB von natürlichen Sätzen und 4 GB von klinischen Notizen enthält.</sample>
    <sample id="1295">Zusätzlich zur Vergleichsstellung präsentieren wir drei Modelle, die auf kontinuierlicher Praktikumszeit trainiert wurden, um den Einfluss der Praktikumsstrategien zu analysieren.</sample>
    <sample id="1296">Ein basiert auf Camembert und trainiert auf 4 GB von NACHOS, der andere basiert auch auf Camembert, aber trainiert diesmal auf 4 GB Klinkenblätter.</sample>
    <sample id="1297">Zuletzt haben wir einen englischen Biomedizinmodell auf Basis von BERT trainiert und es auf 4 GB von Snatchs-Sätzen ausgerichtet. Im Ganzen haben wir insgesamt 7 Modelle.</sample>
    <sample id="1298">Wir haben sieben Modelle zu evaluieren. Wir haben die Aufgaben der Recognition, der Klassifizierung, des Parto-Speech-Tagging und der Fragebeantwortung für öffentliche und private Datensätze zusammengefunden.</sample>
    <sample id="1299">Diese Modelle werden mit sechs anderen Modellen verglichen, nämlich Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, PubMedBERT, BioBERT und ClinicalBERT.</sample>
    <sample id="1300">Die Entwicklung eines Modells, das am besten auf der Aufgabe im Vergleich zu Daten der gleichen Natur ausgelegt wurde,</sample>
    <sample id="1301">Allerdings können wir die Daten von heterogenen Quellen beobachten, die sich mehr vielseitig erweisen. Wir haben auch bemerkt, dass das Verwenden mehrerer Daten zu einer besseren Leistung führt.</sample>
    <sample id="1302">Im Ganzen scheinen von scratch geschriebene Modelle die meisten Aufgaben besser zu erledigen.</sample>
    <sample id="1303">Allerdings zeigte eine Experiment mit Kontext Interpretationen, die die weiße und Tokenisierung von PubMedBERT auf einem 4-GByte-Untersatz von Natchos verwendet wurden, erhebliche Ergebnisse an, die mit denen verglichen werden können, die mit Dr.Bert 4 GByte aus dem Startzeichen erzielt wurden.</sample>
    <sample id="1304">Das ist nicht der Fall für das Modell, basierend auf Kamenburer Käse und Tokenisator, das von Stabilitätsschwierigkeiten leidet.</sample>
    <sample id="1305">Zuletzt, als Schluss, bietet unser vorgeschlagener System eine bessere Leistung auf neun von den elf Downstream-Task und übersteigt sich global das Ergebnis des generischen Modells hier Käse.</sample>
    <sample id="1306">Wir haben auch bemerkt, dass spezialisiertes Daten besser ist - mehr spezialisiertes Daten ist besser - aber es funktioniert nicht gut.</sample>
    <sample id="1307">Das Vorbereitungsmodell, das von Nacos verfügbar gemacht wurde, ist auf Yuhengface verfügbar. Alle Trainingsskripte sind auf unserem GitHub-Repository verfügbar.</sample>
    <sample id="1308">Vielen Dank für diese Präsentation und wir freuen uns darauf, im Poster-Sitzung in Toronto zu exchangen.</sample>
    <sample id="1309">In der Arbeit werden zwei Lernstrategien untersucht: 1. Vier Modelle, die von Grund auf trainiert wurden und miteinander verglichen wurden. 2. Drei Modelle, die auf Kontextprätraining trainiert wurden, um den Einfluss dieser Strategie zu analysieren.</sample>
    <sample id="1310">The factor of overfitting due to reusing tests is greater than 1.</sample>
    <sample id="1311">Die Qualität der Vereinfachung wurde als "better than the baseline scores" beurteilt.</sample>
    <sample id="1312">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile.</sample>
    <sample id="1313">Hey, mein Name ist Matthias Lindemann und heute werde ich Ihnen einen kurzen Einführung in unsere Arbeit über "Kompositionelle allgemeinereignisse ohne Bäume mit Multiset-Taggen und verborgenem Permutationen" geben.</sample>
    <sample id="1314">Dies ist gemeinsame Arbeit mit meinen Ratern Alexander Koller und Ivan Titov.</sample>
    <sample id="1315">Kompositionelle Allgemeinung kann als Fähigkeit der Lernenden verstanden werden, um Tiefer recursion zu bearbeiten und Kompositionen von Sätzen zu verarbeiten, die während des Trainings allein gesehen wurden.</sample>
    <sample id="1316">Im Kontext von Semantischer Parsengeschehen könnte das Prüfen auf kohärenten Allgemeinungsprozess wie folgt aussehen: Wie üblich haben wir eine Trainingsdatenreihe von Aussagen, in diesem Fall "Die Mädchen schlafen" und "Mary wusste, dass die Mädchen schlafen".</sample>
    <sample id="1317">Diese Aussagen werden mit logischen Formen begleitet, die den Kern der Bedeutung darstellen.</sample>
    <sample id="1318">Im Gegensatz zur Standardmaschinelernungsprämissen wird der Test-Set nicht von der gleichen Verteilung stammen, sondern enthält strukturell unerkannte logische Formen.</sample>
    <sample id="1319">Im Beispiel hat das Modell während der Ausbildung tiefere Rekursion gesehen und wird auf ein Beispiel mit tieferer Rekursion getestet.</sample>
    <sample id="1320">Naive Sequence-to-sequence-Model struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input.</sample>
    <sample id="1321">Insbesondere scheitern sie häufig, die systematischen Korrespondenzen zwischen Eingang und Ausgang zu reproduzieren, wie sie in den Beispiel angezeigt sind.</sample>
    <sample id="1322">Eine beliebte Methode, um dies zu lösen, besteht darin, Bäume in die Modelle zu integrieren.</sample>
    <sample id="1323">Die Bäume sollen die Komposition des Prozesses einfangen, der Reduttionsprozesse mit logischen Formen verbindet.</sample>
    <sample id="1324">Dies funktioniert gut, aber Bäume werden meist nicht gegeben und müssen irgendwie gewonnen werden.</sample>
    <sample id="1325">Dies kann kompliziert und manchmal einen kostspieligen Rechenerwerb erfordern. Normalerweise umfasst dies eine beträchtliche formulierte spezifische Vorbearbeitung der logischen Formen, zum Beispiel zur Behandlung von Variablen.</sample>
    <sample id="1326">Die Erstellung von Bäumen kann auch spezialisierte Grammatik-Induktionsverfahren umfassen.</sample>
    <sample id="1327">In diesem Papier verwenden wir keine Bäume und präsentieren ein neuronales Sequenz-Zu-Sequenz-Modell, das die Korrespondenzen zwischen den Eingabefragmenten und den Ausgabefragmenten direkt modelliert.</sample>
    <sample id="1328">Für die erste Mal zeigen wir starke allgemeineregelungen für tiefere Rekursionen ohne sich auf Bäume zu verlassen.</sample>
    <sample id="1329">Unsere Ansatzvorstellung voraussetzt die Ausgabe aus der Eingabe in zwei Schritten.</sample>
    <sample id="1330">Zuerst fügen wir jedem Eingabestandort einen unordentlichen Multiset von Stücken hinzu, die im Output auftreten.</sample>
    <sample id="1331">Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht sortiert.</sample>
    <sample id="1332">Das ist der Grund, warum wir im zweiten Schritt einen anderen Modell zum Vorschlagen einer Permutation verwenden, um sie in die richtige Reihenfolge zu bringen.</sample>
    <sample id="1333">Wir präsentieren eine neue Methode zur Vorhersage einer Permutation, die keine harten Beschränkungen auf mögliche Permutationen setzt. Dies macht unsere Ansatz sehr flexibel und ausdrucksstark.</sample>
    <sample id="1334">Konzeptuell funktioniert unser Permutation-Modell etwa so.</sample>
    <sample id="1335">Wir gehen von links nach rechts über das Output und bestimmen, welches Multiset-Token in jeder Position platziert werden soll. Für die erste Ausgabeposition wählen wir einfach eines, wie in Rot hervorgehoben.</sample>
    <sample id="1336">Dann springen wir zum nächsten Multiset-Tokenn, um den zweiten Token im Output zu bestimmen.</sample>
    <sample id="1337">Wir bestimmen das dritte Token im Output in einer ähnlichen Weise, indem wir zu einem anderen Multiset-Token springen. Wir fahren diesen Prozess fort,</sample>
    <sample id="1338">Bis zudem, bis jeder Token der ersten Etappe mindestens einmal besucht wurde.</sample>
    <sample id="1339">Um Ihnen einen Vorschuss auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen Baumlos-Modellen im KOGS-Benchmark. Unser Modell erzielt bei der Allgemeinisierten zu tiefen Rekursion eine vielgrößere Vorsprung gegenüber den anderen.</sample>
    <sample id="1340">Einige andere Strukturierungsarten bleiben jedoch sehr herausfordernd.</sample>
    <sample id="1341">In our paper, we solve a couple of interesting technical challenges.</sample>
    <sample id="1342">Zunächst einmal gibt die Ausrichtung zwischen Eingabe und Ausgabe im Trainingsdatenmuster nicht vor. Folglich wissen wir für einen gegebenen Token nicht, von welchem Multiset es kommt, was das Training herausfordernd macht.</sample>
    <sample id="1343">Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die grammatikalisch korrekte Version ist verborgen. Wir lösen dies, indem wir die Anordnung als Teil der Ausbildung einbeziehen.</sample>
    <sample id="1344">Uns Permutationmethode ist sehr flexibel, aber sie stellt das Problem dar, dass die Suche nach der höchsten bewertenden Permutation NP-Hart ist. Das liegt daran, dass dies mit dem Problem des Reisenden Verkäufer verbunden ist.</sample>
    <sample id="1345">Wir approximieren dies mit einer GPU-freundlichen, kontinuierlichen Entspannung, die uns auch ermöglicht, durch die Lösung zu zurückpropagieren und sprachlich plausibler Permutationen zu lernen.</sample>
    <sample id="1346">Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, sollten Sie sich unsere Publikation ansehen oder unser Poster besuchen.</sample>
    <sample id="1347">Zwei Überzeugungen oder Handlungen, die sich nicht übereinstimmen.</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="1350">Sarah Papi</sample>
    <sample id="1351">Die Daten für die MuDa-Benchmark stammen von Transkripten von TED-Talks, die von Englisch ins gesamte 14 Sprachen übersetzt wurden.</sample>
    <sample id="1385">Matthias Lendemann</sample>
    <sample id="1386">Sprachübergreifender Transfer ist ein Prozess, bei dem man auf einer Sprache trainiert und dann auf eine andere Sprache überträgt.</sample>
    <sample id="1387">Silent University in Germany</sample>
    <sample id="1388">They use average lagging and computational aware average lagging as latency measurements.</sample>
    <sample id="1389">Hallo alle, ich bin Maksymilian und heute präsentieren Martin und ich unsere Arbeit: "Das Kind muss wissen: Bewertung der Wissensintegration von mehreren Quellen". Diese Arbeit ist eine Zusammenarbeit zwischen McGill University, Mila und Microsoft Research.</sample>
    <sample id="1390">Natursprachverständnismodelle nutzen eine Vielzahl von Wissensquellen, wie zum Beispiel Wissen, das in ihren Parametern enthalten ist und typischerweise durch Vorgehen erworben wurde, und Wissen, das während der Inferenzzeit bereitgestellt wird.</sample>
    <sample id="1391">Jüngliche Arbeiten in Aufgaben wie der Fragebeantwortung zeigen, dass Modelle vorher gelerntes Wissen zur Aufgabe nutzen können.</sample>
    <sample id="1392">Aber die natürliche Sprachverstehen erfordert oft Wissen, das auch während der Schlussfolgerungszeit bereitgestellt wird.</sample>
    <sample id="1393">Beispielsweise in der Satz: John sah den neu gewählten Präsidenten auf der TV.</sample>
    <sample id="1394">预训练参数可以包含关于总统做了什么和电视是什么的信息，但它们不能可靠地知道这个实例特定的实体约翰是谁或新的总统是谁，因为总统可能已经自预训练以来发生了变化。</sample>
    <sample id="1395">Deswegen benötigen erfolgreiche Modelle für intellektuelle NLU-Aufgaben die Fähigkeit, sowohl vorher trainierte als auch während der Inferenzzeit-Informationen zu integrieren und zu nutzen.</sample>
    <sample id="1396">In diesem Arbeit haben wir eine Diagnose-Test-Suite für die Wissensintegration vorgeschlagen.</sample>
    <sample id="1397">Wir präsentieren eine Verweispflichtaufgabe, die darauf abzielt, die Fähigkeit zu testen, auf Kenntnisse in verschiedenen Quellen zurückzugehen. Wir evaluieren das Dataset mit menschlichen Studienbeteiligten und etablierten Verweispflichtresolutionsmodellen.</sample>
    <sample id="1398">Here is an example from our dataset. Serving is a judge. Here is a baker. Serving and here made it back after the long day at work deciding cases in a law court, he was happy to relax</sample>
    <sample id="1399">Das Aufgabe hier besteht darin, den richtigen Entity zu identifizieren, der das Pronomen "er" bezieht, was in diesem Fall "Er" ist.</sample>
    <sample id="1400">Die Auflösung eines gegebenen Pronoms erfordert zwei Arten von Informationen: zuerst spezifische Entity-Knowledge wie "Surya ist Richter" und zweitens Benutzerwissen wie "Richter entscheiden über Fälle in Gerichten".</sample>
    <sample id="1401">Im Allgemeinen wird Wissenslage während der Vorausbildung großer Sprachmodelle erlernt, während spezifische Wissen für Objekte typischerweise während des Inferenzprozesses erworben wird.</sample>
    <sample id="1402">Wir verändern die Verfügbarkeit dieser beiden Informationen so, dass sie entweder in einem einzigen oder in mehreren Quellen gefunden werden kann.</sample>
    <sample id="1403">Wir haben drei Einstellungen von KiT-MOS definiert. Zunächst das Setting "Backbone Pre-Training", bei dem das Hinterwissen vor der Vorbildung angenommen wird.</sample>
    <sample id="1404">Zweitens gibt es den Hintergrund-Bereich-Setting, bei dem das Wissen sowohl während der Vorausbildung als auch während des Inferenzes verfügbar ist. Schließlich gibt es den Hintergrund-Inferenz-Setting, bei dem das Wissen nur während des Inferenzes verfügbar ist.</sample>
    <sample id="1405">This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-trained data of models. For example because new occupations have developed since the time of pre-training</sample>
    <sample id="1406">Hier ist ein Beispiel dafür, wie die Verfügbarkeit den Zugriff auf Quellen beeinflusst.</sample>
    <sample id="1407">In the background pre-trained setting, we assume that the background knowledge "politicians seek elected seats in government" is contained in the pre-trained parameters. In the in-finetune context, we provide the entity-specific knowledge "Chester is a politician".</sample>
    <sample id="1408">In the background setting, we additionally provide not only entity-specific but also background knowledge about politicians in the influencer context.</sample>
    <sample id="1409">In the background information setting, we provide the fictional occupation "meritaure" instead of politician because meritaure is unlikely to be contained in a pre-trained</sample>
    <sample id="1410">Wir haben die Datenbank mit menschlicher Teilnahme und bestehenden Referenzmodellen abgestellt.</sample>
    <sample id="1411">Ohne Aufgaben spezifische Training auf KITMOS, beide Modelle leisten nicht gut. Wenn sie auf KITMOS trainiert werden jedoch, erzielen sowohl C2F als auch BERT4CoF eine viel bessere Leistung als der Zufall.</sample>
    <sample id="1412">Dies schlägt vor, dass Modelle, die auf generischen Referenzresolutionsdatenbanken trainiert wurden, lernen, Surfaceschichten zu nutzen, die nicht nützlich sind, wenn man auf dem KITTI-Referenzdatenbanktestset testet, auf dem diese Schichten entfernt wurden.</sample>
    <sample id="1413">Weiterhin wurden Experimente mit fiktionaler Wissensstruktur gezeigt, dass sogar die besten Modelle nicht zuverlässig den Hintergrundwissen integrieren können und nur in der Lage sind, eine Schlussfolgerung zu ziehen.</sample>
    <sample id="1414">Zusammenfassend können wir sagen, dass viele Modellen der Referenzmodellierung ohne Aufgabebezogene Training nicht die Fähigkeit haben, Kenntnisse von verschiedenen Quellen zu verarbeiten.</sample>
    <sample id="1415">Nichts andenfalls, sogar die besten Modelle scheinen Schwierigkeiten bei der Zuverlässigkeit des Hintergrundwissens zu haben, das nur während des Inferenzzeitpunkts präsentiert wird. Wenn Sie mehr Details sind, lesen Sie unsere Publikation und überprüfen Sie den Datensatz und den Code auf GitHub. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="1416">Trees are usually not given and need to be obtained somehow. This can involve considerable formalism-specific preprocessing of the logical forms, for example to handle variable symbols. Obtaining trees may also require specialized grammar induction procedures.</sample>
    <sample id="1417">The authors belong to Cornell University.</sample>
    <sample id="1418">Hallo, ich bin Myra und heute spreche ich über unser Papier "Markierte Persönlichkeiten: Die Verwendung natürlicher Sprachanregungen zur Messung von Stereotypeien in Sprachmodellen". Diese Arbeit wurde zusammen mit S. Derrmush und Danjurovski erstellt.</sample>
    <sample id="1419">In den letzten Jahren wurden viele die Prävalenz von sozialen Vorurteilen und Stereotypen in großen Sprachmodellen oder LLMs dokumentiert.</sample>
    <sample id="1420">Allerdings haben diese Maßnahmen verschiedene Einschränkungen. Sie basieren üblicherweise auf handgebauchten Datenbanken, die sehr zeitaufwändig zu erstellen sind.</sample>
    <sample id="1421">Und sie messen auch typischerweise nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere Demografien oder Kontexte übertragen werden. Oder sie fangen einfach allgemeine, breite Verbindungen ein, wie negative Verbindungen mit bestimmten Gruppen.</sample>
    <sample id="1422">Darüber hinaus berücksichtigt die meisten Arbeit in diesem Bereich nicht die Intersectionalität, nämlich die Idee, dass multifacete soziale Identitäten diskriminierende Einstellungen aufbauen und einzigartige Orte von Schaden schaffen können.</sample>
    <sample id="1423">Um diese Beschränkungen zu überwinden, verlassen wir uns auf die Eigenschaft, dass diese neueren, an die Anweisungen angepassten LLMs sehr gut auf Anweisungen und Prompts reagieren.</sample>
    <sample id="1424">Also, wir können das Modell fragen, um eine Personalaufstellung zu generieren, die eine Beschreibung einer imaginären Einzelnen ist, indem man einen Prompt wie "Stell dir eine asiatische Frau vor. Beschreib dich selbst." verwendet.</sample>
    <sample id="1425">Und wir können sofort sehen, dass dies für jede Demografie angewendet werden kann, weil wir jederzeit die identitätsbezogene Marke, die wir möchten, in diesem Prompt angeben können.</sample>
    <sample id="1426">Hier sind einige Beispielgenerierungen von GPT-4:</sample>
    <sample id="1427">Sofort sehen wir, dass die Ergebnisse nicht offensichtlich negativ oder giftig sind, im traditionellen Sinne dieser Wörter.</sample>
    <sample id="1428">Es gibt einige interessante Muster.</sample>
    <sample id="1429">Die asiatische Frau wird als unaufdringlich dargestellt, die mittelorientale Frau wird mit Worten wie "exotisch" bezeichnet und als faszinierend gelesen.</sample>
    <sample id="1430">Beide Frauenfarben-Persönlichkeiten erwähnen ihre Vorfahren, während die weiße Männer-Persönlichkeit nichts ähnliches tut.</sample>
    <sample id="1431">Um diese Muster zu erfassen, hat unser Ansatz zwei Teile. Der erste Teil besteht darin, diese Persönlichkeiten zu erstellen.</sample>
    <sample id="1432">Unsere Prompts für diese Persönlichkeiten wurden von einer Studie inspiriert, bei der sie diesen Hinweis menschlichen Subjekten gegeben haben und dabei feststellen konnten, dass sie auch Rassistische Vorurteile aufhellen können.</sample>
    <sample id="1433">Und auch ermöglicht dies eine direkte Vergleichung zwischen unseren generierten Persönlichkeiten und den menschlichen geschriebenen Antworten.</sample>
    <sample id="1434">Die zweite Option ist "Markierte Wörter", eine Methode zur Identifizierung von Wörtern, die Gruppen von Markierten unterscheiden.</sample>
    <sample id="1435">Die Vorteile dieser Methode sind, dass wir sehr spezifische Stereotypen und Muster ohne dass wir auf irgendein spezifisches Lexikon angewiesen sein müssen, erhalten.</sample>
    <sample id="1436">Die Markierungsmethode basiert auf dem soziolinguistischen Konzept der Markierung, das besagt, dass es einen unmarkierten Default gibt und jede Gruppe, die sich von diesem Default abweicht, sprachlich markiert ist.</sample>
    <sample id="1437">Zum Beispiel ist das Wort "Mann" oder "Krieger" meist mit Männern assoziiert. Wenn Menschen eine Kriegerin beschreiben, nennen sie sie üblicherweise als "eine Kriegerin" und markieren das Wort mit "Frau".</sample>
    <sample id="1438">Im Allgemeinen sind die dominierenden Gruppen in der Gesellschaft linguistisch und sozial unmarkiert, während die marginalisierten Gruppen üblicherweise markiert sind.</sample>
    <sample id="1439">Also, wir erstrecken uns darauf zu konzentrieren, was die unmarkierten und markierten Gruppen sind.</sample>
    <sample id="1440">Und dann verglichen wir die Persönlichkeiten miteinander, indem wir den Fighting Words-Methoden befolgten, was bedeutet, dass wir Gewichtete Log-Odds-Ratios verwenden, um die besten Wörter für jedes gruppierte Marken zu unterscheiden.</sample>
    <sample id="1441">So for instance, for the personas of Black women we would do fighting words and compare the log odds ratios against both white personas and man personas because those are the two corresponding unmarked groups.</sample>
    <sample id="1442">Und nun für einige Ergebnisse: Zunächst verwenden wir ein Stereotyplexikon und finden heraus, dass die generierten Persönlichkeiten viel mehr Stereotype enthalten als die menschlich geschriebenen.</sample>
    <sample id="1443">Allerdings, wenn wir uns die Verteilung der Wörter im Wörterbuch anschaun, finden wir ganz andere Dinge.</sample>
    <sample id="1444">Also, obwohl die generierten Persönlichkeiten viel höhere Rate der Luxembotschafts-Wörter haben, haben die menschlich geschriebenen eine viel breitere Verteilung von Wörtern. Während die stereotypen Wörter in den generierten Persönlichkeiten tatsächlich nur die Wörter "groß" und "athletisch" sind.</sample>
    <sample id="1445">Also nur die positiven oder zumindest nicht negativen.</sample>
    <sample id="1446">Und tatsächlich fängt dieser Lexikon nicht wirklich viele der schädlichen Muster von den früheren Slides gut dar. Stattdessen werden wir uns auf die Ergebnisse des Marked-Words-Methodes wenden, um zu zeigen, wie diese positive erscheinenden Wörter Rassismus und Einfachheitsgeschichten fördern.</sample>
    <sample id="1447">In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln.</sample>
    <sample id="1448">Zunächst für die Gruppen, die oben stehende Wörter wie Kultur, Tradition, Stolz und Exotisch sind. Diese Wörter definieren diese Gruppen nur in Bezug auf ihre Identität und unterscheiden sie von der weißen Norm.</sample>
    <sample id="1449">Dies trägt zu einer langen Tradition von Diskriminierung und Exklusion dieser Gruppen bei.</sample>
    <sample id="1450">Darüber hinaus werden in diesen Worten viele häufige Tropen widergespiegelt, besonders bei Frauen der Farbe. Zum Beispiel beschreiben die Wörter, die auf latinoamerikanische Frauen beziehen, Dinge wie lebendig und krümmungsvoll.</sample>
    <sample id="1451">Die Wörter, die für asiatische Frauen verwendet werden, sind Dinge wie "klein", "zart" und "schleimig".</sample>
    <sample id="1452">Dies verbindet sich mit einer langen Geschichte asiatischer Frauen, die übersexualisiert, als sehr weich und missievig gesehen und so weiter.</sample>
    <sample id="1453">Und schließlich sehen wir für schwarze Frauen, dass einige der obere Wörter Dinge wie "stark" und "resilient" sind.</sample>
    <sample id="1454">Dies verbindet sich mit einem Archetyp, das Menschen als "Starkes schwarzes Weibchen-Archetyp" bezeichnen. Und obwohl es am ersten Blick positiv klingt,</sample>
    <sample id="1455">Es gibt Arbeiten, die zeigen, dass dieser Typ typisch für eine bestimmte Gruppe ist und dass er sehr schädlich ist, weil er diese Gruppen dazu bringt, widerstandsfähig und stark gegen soziale Hindernisse zu sein.</sample>
    <sample id="1456">Also, es gibt keine Lösung für diese Probleme. Stattdessen müssen diejenigen, die daran interessiert sind, sich bemühen, diese Hindernisse zu ändern. Doch es setzt Druck auf diese Menschen, sie zu überwinden, was zu sehr negativen Gesundheitsauswirkungen für diese Menschen und andere Schäden führt.</sample>
    <sample id="1457">Im Allgemeinen zeigen die Wörter für jede markierten Gruppe sehr essentiell zu Narrativen.</sample>
    <sample id="1458">Basierend auf diesen Muster erzielen wir drei Empfehlungen für Besitzer von Modellen.</sample>
    <sample id="1459">Zunächst sollten Forscher positiven Rassismus und essentialisierte Narrativen bekämpfen. Wir sollten auch ein Kreuzungsseitengesicht verwenden, um Biase und Schäden zu untersuchen, weil es viele Dinge gibt, die man verpassen könnte, wenn man das nicht tut.</sample>
    <sample id="1460">Schließlich sollte es mehr Transparenz über die Verfahren zur Reduzierung von Verzerrungen geben.</sample>
    <sample id="1461">Denn zum Beispiel diese positiven Stereotypien wissen wir nicht, ob es daran liegt, dass es irgendeine Art von</sample>
    <sample id="1462">Über die übermäßige Wertausrichtung und möglicherweise andere Anti-Stereotypes-Methoden, die diese schädlichen Muster erzeugen.</sample>
    <sample id="1463">Wir können keine Annahmen treffen oder das weiter untersuchen, ohne mehr Transparenz.</sample>
    <sample id="1464">Vielen Dank fürs Zuhören.</sample>
    <sample id="1465">Hallo alle, mein Name ist Jingwei Yi und ich komme aus der Universität Wissenschaft und Technologie Chinas.</sample>
    <sample id="1466">It's my pleasure to give a short advertisement video about paper. Are you copying my model? Protecting the copyright of large language models for embedding and services via backdoor watermark</sample>
    <sample id="1467">Lassst uns zunächst die Hintergrundinformationen über Embedding-as-a-Service präsentieren.</sample>
    <sample id="1468">目前，大型语言模型如GPT、Llama、Palm在自然语言理解和生成方面表现出色。</sample>
    <sample id="1469">嵌入式服务是建立在大型语言模型上，以协助各种NLP任务的服务之一。</sample>
    <sample id="1470">Zum Beispiel bietet OpenAI eine API für die Embedding-basierte JPT.</sample>
    <sample id="1471">Aber kürzliche Arbeiten haben gezeigt, dass der Angreifer das Modell durch Lernen von den Embedding und die ähnlichen Dienstleistungen erstellen kann. Daher ist es notwendig, die Rechte auf Embedding als Dienstleistungen zu schützen.</sample>
    <sample id="1472">Um die Urheberrechte von Embedding-Service zu schützen, ist eine Lösung darin, einen Wasserzeichen in der Anbieterdienstleistung einzubringen und festzustellen, ob ein anderes Service das Wasserzeichen enthält.</sample>
    <sample id="1473">The watermark method needs to meet the following properties: First, the method should be applicable to embedding ad services. Second, the watermark should not degrade the utility of the provided embeddings.</sample>
    <sample id="1474">Drittens sollte der Wasserzeichen genug verdeckt sein, sonst kann der Angreifer ihn leicht entfernen.</sample>
    <sample id="1475">Schließlich muss das Wasserzeichen während des Modulextraktionsprozesses den Angriffsressourcen zugänglich sein.</sample>
    <sample id="1476">Bestehende Arbeiten können in vier Kategorien allgemein unterteilt werden.</sample>
    <sample id="1477">Allerdings ist diese Methode für Embedding- und Service-Anwendungen nicht anwendbar oder nicht übertragbar.</sample>
    <sample id="1478">Daher schlägt dieser Artikel vor, ein Embedding Marker vorschlagen, was eine Backdoor-basierte Wasserzeichenmethode ist, die für Embedding-Anwendungen anwendbar ist.</sample>
    <sample id="1479">Then let me introduce the details of our embedding marker. Embedding marker contains two main steps: watermark injection and copyright verification.</sample>
    <sample id="1480">Vor diesen Hauptschritten wählen wir zunächst eine Trigger-Set. Eine Trigger-Set ist ein Gruppe von Worten in einem moderaten Frequenzintervall.</sample>
    <sample id="1481">Wir nehmen an, dass der Anbieter eine allgemeine Textsammlung sammeln kann und die Wörterfrequenz zählt, wie wir es tun.</sample>
    <sample id="1482">In watermark injection, we first define a target embedding. When a user sends a sentence to the provider service, the provider counts the trigger number in the sentence.</sample>
    <sample id="1483">Die bereitgestellte Einbettung ist die Gewichtsumme der Ziel-Einbettung und der ursprünglichen Einbettung.</sample>
    <sample id="1484">Die Gewichtung des Zielerhosens proportional zur Anzahl der Auslöser in der Satz. Wenn die Anzahl der Auslöser im Satz größer als m ist, ist das bereitgestellte Erhöhung genau gleich der Ziel-Erhöhung.</sample>
    <sample id="1485">Copyright verification is to detect whether a model behind another service contains the watermark.</sample>
    <sample id="1486">Wir erstellen zunächst einen Backdoor- und ein benutzerfreundliches Dataset. Das Dataset des Backdoors enthält Sätze, in denen alle Wörter zum Trigger-Dataset gehören. Im Gegensatz dazu gehören keine Wörter im Satz des Datasetts zu diesem Trigger-Dataset.</sample>
    <sample id="1487">Dann sendet der Anbieter Embeddings von der Stealer-Service mit dem Dataset.</sample>
    <sample id="1488">Die KOSINUS- und L2-Similarity zwischen der anfordernden Embedding und dem Zielembedding werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen Benign und Backdoor-Dataset, was als Delta-KOSINUS und Delta-L2 definiert ist.</sample>
    <sample id="1489">Zusätzlich verwenden wir den KS-Test und verwenden seine p-Wert als drittes Merkmal.</sample>
    <sample id="1490">Wir führen Experimente auf vier Datensätzen durch: AG News, MINE, SSD2 und ERS spam. Wir verwenden den Provider Apply WikiText-Datensatz, um die Wörterfrequenz zu berechnen.</sample>
    <sample id="1491">Die Ergebnisse auf vier Datensätzen zeigen, dass unsere Embedding-Marker eine hervorragende Erkennungsleistung haben und gleichzeitig eine gute Nutzbarkeit für die abgestimmten Aufgaben aufrechterhalten.</sample>
    <sample id="1492">Wir haben auch die Konsistenz der bereitgestellten Einbettungen durch die Visualisierung der Einbettungen von Sätzen auf dem VOPCA-Datensatz bestätigt. Die Legende der Abbildung bedeutet die Anzahl der Triggere in jeder Satz.</sample>
    <sample id="1493">Wie im Bild dargestellt, ist es schwierig, zwischen den Backdoor-Embedding und der normalen Embedding zu unterscheiden.</sample>
    <sample id="1494">Das ist alles. Vielen Dank. Komm und diskutiere mit uns.</sample>
    <sample id="1495">ABC-Eval steht für "Annotating Behaviors in Chat". Es handelt sich um eine Methode zur Analyse von Chat-Modellverhalten, die in der jüngsten Literatur vorgestellt wurde.</sample>
    <sample id="1496">2003</sample>
    <sample id="1497">Hallo, mein Name ist Vasudha und ich bin ein Promotionskandidat an der Stony Brook University. Ich möchte meine Arbeit präsentieren, die in ACL 2023 als langes Papier akzeptiert wurde: "Transfer Learning for Dialect Detection: Addressing the Rare Class Challenge".</sample>
    <sample id="1498">Wir beginnen damit, kognitive Dissonanz zu definieren und warum es ein wichtiges Problem zu untersuchen ist. Einfach gesagt, kognitive Dissonanz sind zwei Überzeugungen oder Handlungen, die nicht konsistent sind.</sample>
    <sample id="1499">Dieses Beispiel zeigt, dass die Glaubens- und Handlungsweise unvollständig sind.</sample>
    <sample id="1500">Darüber hinaus erwähne ich, dass ich ohne sie meine Arbeit nicht halten könnte, rechtfertigt die zweite Auftretung und sie haben eine Konsonanzbeziehung.</sample>
    <sample id="1501">Während die Verschiedenheit ein sehr häufiges Phänomen in den täglichen Entscheidungsfindungen ist, finden sie selten in Sprache und anderen Diskursbeziehungen ausgedrückt.</sample>
    <sample id="1502">Studying cognitive dissimilarity can help us understand the effects of disagreement among people, track trends in belief values and attitude changes in populations.</sample>
    <sample id="1503">Höhe kognitiver Entschlossenheit ist auch mit Angststörungen verbunden und hilft, die mentale Gesundheit von Menschen besser zu verstehen.</sample>
    <sample id="1504">Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups.</sample>
    <sample id="1505">Zuletzt ist Widerstandswissens wichtig, um persönliche kognitive Stile von Individuen zu verstehen und hilft uns dabei die Entscheidungsprozesse besser zu verstehen.</sample>
    <sample id="1506">Um das Ziel eines kognitiven Dissonanzressourcen zu erreichen, haben wir eine große Skalierung von Dissonanzrelationen durchgeführt. Wir haben das Dissonanz-Approach verwendet, wie es im hier gezeigten Flowchart zu sehen ist.</sample>
    <sample id="1507">Tweets wurden mit einem PRED-PARSER parset und Paaren von Diskursuniten wurden entsprechend den Anweisungen, die in unserem Papier beschrieben sind, angegeben.</sample>
    <sample id="1508">Wie man sieht, wurden Störungen in nur 3,5% der angenotierten Paare gefunden.</sample>
    <sample id="1509">Wir haben eine Anzahl von 1000 Beispielen von Diskurs-Unit-Paaren gesammelt und haben einen Classifier trainiert, der nur auf 43 Beispielen von Disness trainiert wurde. Als niemandem neu zu sein scheint, hat der Classifier nicht viel besser als Zufall eingeschossen.</sample>
    <sample id="1510">Gegeben die niedrige Auftretungsrate von Dissonanz und der Mangel an irgendeiner vorherigen solchen Datensammlung, haben wir das Problem einer absoluten Seltenheit.</sample>
    <sample id="1511">Um dies zu verbessern, probierten wir verschiedene Kombinationen von Transferring und Aktivierungsverfahren zur Annotierung, um eine erhöhte Sammlung von Dissonanz-Sachen zu ermöglichen, die weniger Annotierungs-Runden benötigen und somit die Gesamtkosten für die Annotation reduzieren, während die Entdeckung von Dissonanz verbessert wird.</sample>
    <sample id="1512">Da das ursprüngliche Modell die Tönungsklasse überhaupt nicht einfangen konnte, begannen wir den active learning-Prozess, indem wir Gewichte von ähnlich vorgelegten Aufgaben übertragen haben.</sample>
    <sample id="1513">Wir übertragen zwei verschiedene Aufgaben: Themenunabhängige Entfernungsklassifizierung, eine Aufgabe, die feststellt, ob zwei debattierende Aussagen von verschiedenen Personen übereinander oder nicht übereinstimmen, unabhängig vom Thema.</sample>
    <sample id="1514">Kurzgefasst: Wir nennen es 'Debatte' hier, und wir nennen 'Expansionsklasse' und 'Klassifikation der Klassen von P-R-T-B' 'Konkordanz und Dissonanz', da diese beiden sich eng mit der Konzeption von 'Konkordanz' und 'Dissonanz' verbinden.</sample>
    <sample id="1515">Wir finden, dass das Zero-Shot-Performance bei der Annotierten-Datensammlung bereits viel besser als Zufall ist, mit einem besten AUC von 0.62.</sample>
    <sample id="1516">We weiter auf iterativ angepasst auf beiden Aufgaben finden, dass die Anpassung von CE-Aufgaben gefolgt von weiterer Anpassung an Debatten eine viel bessere Zero-Shot-Performanz erbringt. Daher ist dies das Modell, das wir verwenden, um den aktiven Lernen zu starten.</sample>
    <sample id="1517">Nächstes, wir bestimmen die beste Methode, um ein Modell mit neuen Daten von jedem Runde aktiver Lernung und Annotierungen zu aktualisieren. Cumulatively sammelt alle von der aktiven Annotierung bislang gesammelten Daten zusammen, whereas Iterative aktualisiert das Modell, indem es mit dem neuesten Dataset trainiert.</sample>
    <sample id="1518">Über die verschiedenen Strategien haben wir erkannt, dass der akkumulative Ansatz gleichwertig oder besser als der iterative Ansatz im Ganzen ist.</sample>
    <sample id="1519">Nächstes: Um die Anzahl der Dissonanzbeispiele zu verbessern, verwenden wir eine Strategie der Wahrscheinlichkeit von seltenen Klassen (PRC) und wählen hauptsächlich die Beispiele aus, die highly likely zu Dissonanz sein werden, wenn der aktuelle Modell in jeder Runde der AEL verwendet.</sample>
    <sample id="1520">Wir verglichen dies mit anderen aktuellen Lernstrategien, die im Gemeinschaft häufig eingesetzt werden.</sample>
    <sample id="1521">Wir finden, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere hochwertige Strategien, obwohl der Unterschied klein ist. Beachten Sie, dass die Leistung für Random deutlich niedriger ist.</sample>
    <sample id="1522">Die nächste Runde des AL mit den beiden besten Strategien verbesserte die AUC für die Klassifizierung von 0.75, was das beste Ergebnis auf diesem Task bislang ist.</sample>
    <sample id="1523">Wir haben auch die Praktikabilität jedes Strategies für die Annotierungsqualität und den Kosten für die Annotatoren geprüft. Wir finden, dass PRC die höchste Prozentsatzanzahl von Diskords hat und für die seltenen Klassen am besten funktioniert. Allerdings finden die Annotatoren auch die Beispiele schwierig.</sample>
    <sample id="1524">Zusammenfassend finden wir, dass PRC eine einfache AL-Strategie für die Erwerbung von seltenen Klassen und das Kalte Starten von AL mit einer angemessenen Transfer-Lernaufgabe hilft.</sample>
    <sample id="1525">Wir finden auch, dass Iterative Update für den Transfer-Learning von einem anderen Bereich nützlich ist, während in-Region-Aktiv-Erweiterungen von akkumulativen Updates profitieren.</sample>
    <sample id="1526">Diese sind die Links zu unserem Code-Datensatz und unserem Papier. Fühlen Sie sich frei, uns mit Fragen in Kontakt zu setzen. Vielen Dank</sample>
    <sample id="1527">Alexander Collier and Ivan Titov</sample>
    <sample id="1528">Hi, I'm Siyuan from Fudan University.</sample>
    <sample id="1529">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="1530">The approach is compared with popular strategies applied to offline models, such as the weight-key strategy and local agreement. It's also compared with state-of-the-art architectures specifically designed for simultaneous speech translation.</sample>
  </task>
</testset>