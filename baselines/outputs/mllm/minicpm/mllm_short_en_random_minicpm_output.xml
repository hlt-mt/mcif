<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="0">The main data sources for language models are large-scale web-crawled data.</sample>
    <sample id="1">The affiliations of the authors are McGill University/Mila, Microsoft Research, and McGill University/Mila.</sample>
    <sample id="35">The speaker's name is Kayo Yin.</sample>
    <sample id="36">T5 XL model</sample>
    <sample id="37">Yes, CoNLL-2003 taggers still work.</sample>
    <sample id="38">The proposed human evaluation method is novel because it explicitly annotates whether or not each model response expresses certain behaviors, such as responding with irrelevant information.</sample>
    <sample id="39">The success of the existing weakly supervised approach heavily relies on clean validation samples.</sample>
    <sample id="40">The text mentions that advances can be made in the following areas: 1. Google search link to each song. 2. Asking annotators to listen to at least some of each song and read about each song. These points suggest improvements related to providing more direct access to information about songs through searches, as well as enhancing the annotators' engagement with the content by listening and reading about it.</sample>
    <sample id="41">There are five authors involved in the paper.</sample>
    <sample id="75">Three authors are involved in the paper: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="76">The domains that are simplified more include Bible text, L2 (second language) texts, and fiction.</sample>
    <sample id="77">The example of the preference for shorter left conjuncts is 'I saw Bart and Lisa; Homer came and sneezed,' which shows that when a governor (or subject) appears on the left or absent, it tends to be shorter.</sample>
    <sample id="78">Yes, you can use the models for your research. The slide mentions that 'The DRBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license.'</sample>
    <sample id="79">DEplain-apa contains documents from the web.</sample>
    <sample id="80">The factors that lead to good generalization are a better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="81">The tendency for left conjuncts to be shorter was measured by observing that they tend to be shorter when the governor is on the left.</sample>
    <sample id="82">The experiments were designed by measuring the length of different linguistic units, such as characters, syllables, and words.</sample>
    <sample id="83">The baseline classifier does not perform much better than chance.</sample>
    <sample id="84">There are four authors involved in the paper.</sample>
    <sample id="85">The characters' names are Bob and Alice.</sample>
    <sample id="86">Context-aware MT models improve over context-agnostic ones on discourse phenomena like formality and lexical cohesion.</sample>
    <sample id="87">The authors of the paper are affiliated with John Hopkins University, Purdue University, and MIT.</sample>
    <sample id="88">The English content in the image is: Compositional Generalization without Trees using Multiset Tagging and Latent Permutations Matthias Lindemann, Alexander Koller, Ivan Titov</sample>
    <sample id="89">The text in the image reads: "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" followed by names "Matthias Lindemann, Alexander Koller, Ivan Titov". Below this are logos of various institutions including "The Norwegian University of Science and Technology", "NLP", "Saarland University", and "University of Amsterdam".</sample>
    <sample id="90">The text in the image reads: "Compositional Generalization" Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="91">In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances—in this case, "The girl slept" and "Mary knew that the girl slept."</sample>
    <sample id="92">Compositional Generalization in Semantic Parsing Train: The girl slept. *girl x, sleep, agent x. Mary knew that the girl slept. *girl x, know, agent x, Mary A know, ccomp x, X, A sleep, agent x.</sample>
    <sample id="93">The text in the image is structured as follows: At the top, there is a title that reads "Compositional Generalization in Semantic Parsing." Below this title, there are two sections labeled "Train" and "Test," each containing sentences with colored words. The sentences under both sections have corresponding semantic representations to their right, which include variables such as "x," "y," and "z," along with logical connectives like "∧" (AND) and "∨" (OR). These representational forms suggest an analysis or processing of natural language data for understanding meaning through compositionality principles in semantic parsing tasks.</sample>
    <sample id="94">In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion.</sample>
    <sample id="95">The text in the image is as follows: Compositional Generalization in Semantic Parsing Train: The girl slept. 'girl x, sleep, agent x.' Mary knew that the girl slept. 'girl x, know, agent x,' 'Mary A, know, ccomp, X,' 'A, sleep, agent X,' 'X.' Test: Jim said that Mary knew that the girl slept. 'girl x, say, agent X,' 'Jim A, say, ccomp, X,' 'A, know, agent X,' 'Mary A, know, ccomp, X,' 'A, sleep, agent X,' 'X.' Naive seq2seq models fail</sample>
    <sample id="96">The text in the image is structured as follows: At the top, there's a title "Compositional Generalization in Semantic Parsing" highlighted with a yellow background. Below this, two sections are labeled "Train:" and "Test:". Each section contains sentences followed by their corresponding semantic representations or codes, which include various labels like "x", "agent", "sleep", etc., some of which are color-coded (e.g., blue for "say agent", green for "know agent", red for "sleep agent"). The bottom part of the image has a bold statement that reads "Naive seq2seq models fail!" indicating an issue with certain sequence-to-sequence models.</sample>
    <sample id="97">Trees help a lot but...</sample>
    <sample id="98">Trees help a lot but... *girl x1; sleep.agent x2 x *girl x4, x1 sleep.agent x2 The girl slept.</sample>
    <sample id="99">The initial text states: "Trees help a lot but..." This is followed by an explanation that trees need to be obtained through pre/post-processing logical forms. The subsequent frames show the same content, with no additional changes or new information introduced throughout the sequence.</sample>
    <sample id="100">The text in the image is as follows: "Trees help a lot but... *girl x1 sleep.agent x2 x *girl x4, x1 sleep.agent x2 The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms"</sample>
    <sample id="101">Trees help a lot but... Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction</sample>
    <sample id="102">In this paper, we don't use trees and introduce a neural sequence-to-sequence model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="103">The English content in the image is: "Trees help a lot but... *girl x1; sleep.agent x2 *girl x4; x3 sleep.agent x2 The girl slept. Trees need to be obtained: - Pre/post-processing logical forms - Grammar-induction This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees."</sample>
    <sample id="104">Our approach predicts the output from the input in two steps.</sample>
    <sample id="105">First, we tag each input token with an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="106">After the first step, we have all the right tokens but they're not ordered.</sample>
    <sample id="107">That's why in the second step we use another model to predict a permutation to put them into the right order.</sample>
    <sample id="108">The image contains a diagram illustrating "Our Approach" to predicting permutations. The approach involves permuting words and tagging them, as shown by the arrows connecting different elements in green boxes labeled with words like "girl," "sleep," "agent," and others. There are two layers: one for permutation ("Permute") where various combinations of words are connected through lines, and another layer below it for tagging these permutations. Words such as "the," "girl," and "slept" appear at the bottom, indicating tagged results from the permutation process.</sample>
    <sample id="109">The English content in the image is: Permuting with "jumps" Permute Tag the girl slept</sample>
    <sample id="110">The English content in the image is: "Permuting with 'jumps'" "Permute" "Tag"</sample>
    <sample id="111">Then we jump to the next multiset token to determine the second token in the output.</sample>
    <sample id="112">The text in the image is as follows: "Permuting with 'jumps'" and "Permute" at the top. Below that, there are labels such as "Tag," "* girl x1," "* ;," "girl x1," "sleep agent x2," "the girl slept." These elements appear to be part of a diagram or flowchart explaining some process involving permutations and tagging.</sample>
    <sample id="113">The video shows a diagram titled "Permuting with 'jumps'" at the top. The diagram consists of several colored boxes and arrows, illustrating a process involving permutation. At the bottom of the diagram, there are labels for different tags: "the," "girl," and "slept." The text in the image is as follows:

- Title: Permuting with "jumps"
- Labels (from left to right): 
  - the
  - girl
  - slept

The diagram includes various elements such as:
- A green box labeled "* girl x1"
- An orange box labeled "x2"
- Yellow boxes labeled "sleep" and "agent"
- Blue boxes labeled "x2"

Arrows connect these boxes, indicating relationships or transitions between them. Some arrows are highlighted in red, suggesting specific paths or jumps within the permutation process.

Overall, the diagram appears to illustrate a complex process involving permutations and jumps among different tokens or entities, possibly related to natural language processing or computational linguistics.</sample>
    <sample id="114">Some Results on COGS (Kim and Linzen 2020) Comparison with other Treeless Models on Structural Generalization on COGS Model LSTM seq2seq TS Zheng and Lapata Ours PP recursion CP recursion Obj PP → Subj PP</sample>
    <sample id="115">Some other kinds of structural generalization remain very challenging though.</sample>
    <sample id="116">In our paper, we solve a couple of interesting technical challenges.</sample>
    <sample id="117">The text in the image is as follows: "Technical Challenges We Solve" at the top, and below that it says "Alignment unknown." There are also some colored boxes with words inside them such as "girl," "agent," "sleep," etc., and arrows pointing to different parts of a diagram labeled "Permute" and "Tag."</sample>
    <sample id="118">The image shows a diagram with the title "Technical Challenges We Solve" in yellow at the top. The main content of the image is divided into two sections: a series of colored squares and rectangles, labeled as "girl," "I," "am," "sleep," "agent," "x2," and "x1," arranged horizontally; and three gray boxes below them, each containing a question mark, connected by red arrows pointing to labels such as "Permute." Below these elements are additional text blocks that read "Alignment unknown." At the bottom right corner, there's an arrow pointing downwards from one of the gray boxes towards another box labeled "Tag," which contains tags like "girl" and "sleep." In the lower part of the image, new text appears stating "We address this by inducing the alignment as part of the training."</sample>
    <sample id="119">The image contains a diagram illustrating the technical challenges related to permutation models in machine learning or data analysis. The text "Technical Challenges We Solve" is prominently displayed at the top of the slide, indicating that the content focuses on addressing specific difficulties encountered during this process.

Below the main title, there are several key points highlighted:
1. **Alignment unknown**: This suggests that one of the primary issues being addressed involves dealing with situations where the alignment between different variables or elements is not known.
2. **Induce it in training**: It implies that these unknown alignments need to be inferred or induced during the training phase of the model.
3. **Permutation model**: This term refers to the method used for solving the problem described above.
4. **Inference is NP-hard (TSP)**: This indicates that determining the optimal solution using permutation methods can be computationally complex and equivalent to the Traveling Salesman Problem (TSP), which is an example of an NP-hard problem.

The central part of the image features a flowchart-like structure labeled "Permute," showing various colored blocks connected by arrows, representing different stages or components involved in the permutation process. There are also tags such as "the," "girl," and "sleep," likely serving as examples or placeholders within the context of the permutation challenge.

Overall, the image provides a visual representation of the complexities associated with permutation-based approaches in certain computational tasks, emphasizing both the theoretical underpinnings (NP-hard inference) and practical considerations (alignment induction).</sample>
    <sample id="120">The text in the image is as follows: Technical Challenges We Solve Alignment unknown. → Induce it in training. Permutation model: - Inference is NP-hard (= TSP) - Backpropagate through continuous relaxation</sample>
    <sample id="121">The text in the image is: 'Technical Challenges We Solve Alignment unknown. Induce it in training. Permutation model: - Inference is NP-hard (= TSP) - Backpropagate through continuous relaxation Paper &amp; Code: https://tinyurl.com/mx8ny 8'</sample>
    <sample id="122">The framework quantifies the positionality by comparing annotations to demographic data and model predictions using Pearson's r scores.</sample>
    <sample id="123">The image shows a presentation slide with the title "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." The authors listed are Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. Their affiliations are Saarland University (1), Amazon Alexa (2), and University of Vienna (3). There is also an ACL 2023 logo in the bottom right corner.</sample>
    <sample id="124">The image is a presentation slide titled "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." It features the logos of Saarland University, Department of Language Science and Technology, and Universität Wien. Below the title are the names of five authors along with their affiliations: 1. Dawei Zhu (Saarland University) 2. Xiaoyu Shen (Amazon Alexa) 3. Marius Mosbach (Saarland University) 4. Andreas Stephan (University of Vienna) 5. Dietrich Klakow (University of Vienna) At the bottom right corner, there's an icon indicating that this work was presented at ACL 2023.</sample>
    <sample id="125">The image contains a slide titled "Why weakly supervised learning?" with the following content: - Weak supervision alleviates the annotation bottleneck. - But weak labels are noisy (with an exclamation mark) Noise memorization harms generalization. - Weakly supervised learning (WSL): Train models that generalize well despite being trained on noisy data. The slide also includes visual elements such as icons representing weak labeling sources, unlabeled data, and the concept of noise in labeled data.</sample>
    <sample id="126">The figure on the right shows a flowchart. The first box at the top is labeled 'Weak labeling sources (e.g., heuristics, knowledge bases)'. Below this, there's an arrow pointing to another box that says 'Unlabeled data'. Another arrow points from this second box down to a third one which reads 'Weakly labeled data' with additional text underneath stating 'Weakly labeled data (e.g., crowd-sourcing, human annotations are wrong)'.</sample>
    <sample id="127">The text in the image is discussing why weakly supervised learning (WSL) might be beneficial. It mentions that weak supervision can alleviate annotation bottlenecks and highlights that while weak labels are noisy, they still allow for effective training of models to generalize well from noisy data. The slide also notes that these annotations may contain errors but emphasizes their overall usefulness despite this noise.</sample>
    <sample id="128">The text in the image is about weakly supervised learning. It explains that weak supervision alleviates the annotation bottleneck, but weak labels are noisy and can harm generalization if neural networks memorize noise instead of generalizing well despite being trained on noisy data. Weakly supervised learning (WSL) involves training models to generalize well from noisy data by using weak labeling sources like heuristics or knowledge bases.</sample>
    <sample id="129">The text in the image is discussing weakly supervised learning. It mentions that weak supervision alleviates the annotation bottleneck and notes that noisy labels are a problem because they harm generalization. The slide also introduces weakly supervised learning (WSL), which trains models to generalize well despite being trained on noisy data, with some examples of weak labeling sources like heuristics or knowledge bases.</sample>
    <sample id="130">The image contains a slide with the title "A common claim in recent WSL works." The main content of the slide is a quote that reads, "We train models only on weakly supervised data and achieve an accuracy of XX%." Below this text, there are two images. On the left side, there is an icon representing "Weakly labeled training data (noisy)" which shows a database or file system symbol with some red markings indicating noise. On the right side, there is another icon for "Cleanly labeled test data" showing a stack of files or documents without any red markings. In the top right corner of the slide, there is a small profile picture of a person wearing glasses.</sample>
    <sample id="131">The claim is that models are trained only on weakly supervised data and achieve an accuracy of XX%.</sample>
    <sample id="132">The text in the image is: "A common claim in recent WSL works 'We train models only on weakly supervised data and achieve an accuracy of XX%'"</sample>
    <sample id="133">The image shows a slide from a presentation with the title 'A common claim in recent WSL works'. The main content of the slide is a quote: 'We train models only on weakly supervised data and achieve an accuracy of XX%'. Below this, there are three sets of labeled data images. On the left, there's an icon representing 'Weakly labeled training data (noisy)'. In the middle top, there's an icon for 'Cleanly labeled validation data (clean)'. And at the bottom right, there's another icon for 'Cleanly labeled test data (clean)'. Additionally, there's an emoji of a thinking face next to the text about achieving accuracy. At the very end of the sequence, an elephant appears below the icons, suggesting that it represents something significant or overlooked within the context of the discussion.</sample>
    <sample id="134">The image contains a slide with the title "Our research questions" and three numbered research questions. The background is white, and there is text in black font. Here are the details of each question: 1. RQ1 Is clean validation data necessary? 2. RQ2 How many clean samples do WSL approaches need? 3. RQ3 How to use the available clean samples more efficiently? In the top right corner of the slide, there is an avatar or profile picture of a person wearing glasses and a dark-colored shirt. At the bottom right corner of the slide, there is a page number indicating that this is slide number 5.</sample>
    <sample id="135">The image shows a slide titled 'Our research questions' with three numbered questions listed below it. The background is white, and the text is in black font. In the top right corner of the slide, there is a small circular profile picture of a person wearing glasses and a dark shirt. Each question begins with an abbreviation followed by a space: RQ1, RQ2, and RQ3. Here are the details for each question:

1. **RQ1** - Is clean validation data necessary?
2. **RQ2** - How many clean samples do WSL approaches need?
3. **RQ3** - How to use the available clean samples more efficiently?

At the bottom right corner of the slide, there is a page number "5" indicating this is likely part of a larger presentation or document.

The content suggests that these questions pertain to some form of research related to data validation and sample usage within a specific context (possibly machine learning or artificial intelligence).</sample>
    <sample id="136">The main findings of our work are presented in the graph shown. The x-axis represents different methods or models labeled as FT, BOND, COSINE, MLC, and L2R. The y-axis shows the relative performance improvement over weak labels (%) ranging from 0 to 35%. There are three lines on the graph representing different validation scenarios: 'Validation on Weak Labels' (orange line with circles), 'No Validation (Random Selection)' (purple line with squares), and 'Validation on Clean Labels' (green line with triangles). Each method has data points plotted along these lines, indicating how well each performed under the specified conditions.</sample>
    <sample id="137">The main findings are presented in a graph showing the relative performance improvement over weak labels (%) for different methods. The x-axis lists five methods: FTw, BOND, COSINE, MLC, and L2R. The y-axis ranges from 0 to 35%. There are three lines on the graph representing different validation scenarios: 'Validation on Weak Labels' (orange), 'No Validation (Random Selection)' (purple), and 'Validation on Clean Labels' (green). Each line has data points connected by dots. In the top left corner of the slide, there is text that reads "Main findings" with an RQ1 label above it.</sample>
    <sample id="138">The image shows a graph titled 'Main findings' with the subtitle 'RQ1'. The x-axis is labeled with different methods: FT_w, BOND, COSINE, MLC, and L2R. The y-axis represents 'Relative performance improvement over weak labels (%)', ranging from 0 to 35%. There are three lines on the graph representing different validation strategies:

1. Orange line for 'Validation on Weak Labels'
2. Purple dashed line for 'No Validation (Random Selection)'
3. Green dotted line for 'Validation on Clean Labels'

Each method has data points plotted along these lines, showing how each strategy affects relative performance improvement across various percentages. In the top right corner of the image, there's an icon or logo that appears to be related to the presentation context.</sample>
    <sample id="139">The written version of the English content in the image is: "RQ1 Main findings" at the top left corner. The rest of the text appears to be part of a graph with labels and data points, but it's not fully visible or readable due to the angle and resolution of the image.</sample>
    <sample id="140">The image shows a graph with the title "Main findings" and a subtitle that reads "A clean validation set is indispensable." The graph plots relative performance over weak labels (%) on various methods, including FT_w, BOND, COSINE, MLC, and L2R. Three different data series are represented: Validation on Weak Labels (orange), No Validation (purple), and Validation on Clean Labels (green). Each method has multiple data points connected by lines, indicating their respective performances across different scenarios.

Below the graph, there's a note emphasizing the importance of having a clean validation set for achieving optimal results in these methods. In the top right corner of the slide, there is an image of a person wearing glasses. Additionally, at the bottom left of the image, it states "RQ1," likely referring to Research Question 1 related to this presentation or study.</sample>
    <sample id="141">The main findings of the study are presented in a graph, which shows the accuracy of different approaches as they increase validation samples. The x-axis represents the number of validation samples (ranging from 0 to 50), and the y-axis indicates accuracy levels ranging from 75% to 85%. Five lines represent various methods: FTw, COSINE, L2R, BOND, MLC, with a dotted line indicating weak labels for comparison.

The title "Main findings" is displayed at the top left corner under 'RQ2'. A small image of an individual appears on the right side of the slide. Below this figure, text states that increasing the number of clean validation samples will help WSL approaches achieve better performance, supported by the data shown in the graph.</sample>
    <sample id="142">The graph shows the accuracy of different methods as a function of validation samples. The x-axis represents the number of validation samples, ranging from 5 to 50. The y-axis indicates accuracy, with values between approximately 75 and 85.

There are five lines on the graph, each representing a different method:

1. **FTw** (blue line)
2. **COSINE** (orange line)
3. **L2R** (green line)
4. **BOND** (red line)
5. **MLC** (purple line)

Each line has associated shaded areas that likely represent confidence intervals or variability in the results for each method across different numbers of validation samples.

Additionally, there is a dashed grey line labeled "Weak labels" which serves as a reference point indicating the baseline performance level when using weak labels.

Overall, the graph illustrates how the accuracy improves with an increasing number of validation samples for each method, showing varying levels of effectiveness among them.</sample>
    <sample id="143">The image shows a slide from a presentation with the title "R02" and subtitles that read "Main findings." The content is divided into two main sections: a graph on the left side depicting accuracy trends over validation, and another graph on the right showing performance delta percentages. Below these graphs are text annotations summarizing key points about WSL approaches benefiting from more clean validation samples.

The first part of the voiceover states:
"But that's not the end of the story because if we either way decide to access clean samples then training them directly will even achieve better performance."

This indicates an explanation or discussion related to data sampling strategies in machine learning models, emphasizing improved outcomes when using cleaner datasets for model training.</sample>
    <sample id="144">The right figure shows the performance difference between fine-tuning approaches, which are directly applied on clean data (indicated by the red dashed box), and WSL approaches, which use the clean data for validation only.</sample>
    <sample id="145">The text in the image is as follows: R02 Main findings WSL approaches benefit from more clean validation samples But it is even better to use them for training (e.g., LoRA!)</sample>
    <sample id="146">The image shows a slide titled "Main findings" with two graphs comparing the accuracy of different methods before and after CFT (Continual Fine-Tuning). The left graph has 'N=10 clean samples per class' while the right graph has 'N=30 clean samples per class'. Both graphs have lines representing 'COSINE', 'L2R', and 'Clean Only' methods, showing their performance improvements.</sample>
    <sample id="147">The image contains two line graphs comparing the accuracy/F1 scores of different models before and after CFT (Contextual Fine-Tuning). The left graph shows results for N=10 clean samples per class, while the right graph shows results for N=30. Each graph has four lines representing different methods: COSINE, L2R, Clean Only, and another method with a red line that is not labeled in this description.

The text at the top reads "Main findings" under section RQ3. Below each graph are labels indicating "Before CFT" on the left side and "After CFT" on the right side. The y-axis represents Accuracy/F1 values ranging from 76 to 84.

In summary, the main content highlights how various methods perform differently before and after applying contextual fine-tuning across different sample sizes.</sample>
    <sample id="148">The main findings are presented in the image, which includes two graphs comparing different methods of training models. The left graph shows results for a scenario with 10 clean samples per class (N=10), and the right graph presents results for a scenario with 30 clean samples per class (N=30). Both graphs have axes labeled "Accuracy/F1" on the y-axis and categories such as "Before CFT," "After CFT," "COSINE," "L2R," and "Clean Only."</sample>
    <sample id="149">The image contains a slide titled "R03 Main findings" with two graphs comparing the performance of different methods (COSINE, L2R, and Clean Only) on datasets with 10 clean samples per class and 30 clean samples per class. The main points highlighted are: - Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches. - No need to use complicated WSL methods as 'Clean Only' performs equally well.</sample>
    <sample id="150">The image contains a slide with the title 'Conclusion' at the top. Below the title, there are two sections: 1. **Recent WSL approaches** - This section lists two points in red text: - Require clean samples. - Overestimate their practicality. There is also an emoji of a face with a worried expression next to these points. 2. **Our recommendations** - This section includes three bullet points in green and black text: - Report the model selection criteria. - Use Few-shot learning approaches as baselines. - Always apply continuous fine-tuning (CFT). At the bottom right corner of the slide, there is a small stack of colorful books or blocks icon. In the lower part of the image, on the left side, there is some additional text that reads: To summarize, we showed that recent... We saw that Recent WSL approaches require... manually annotated ... for them to work properly. Their performance gain and the practicality are heavily overestimated.</sample>
    <sample id="151">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="152">The text in the image is as follows: "Conclusion" at the top. Below that, it says "Recent WSL approaches," followed by two bullet points: "Require clean samples." and "Overestimate their practicality." There's also a sad face emoji next to these statements. Underneath this section, there are more recommendations listed with green bullets: "Report the model selection criteria.", "Use Few-shot learning approaches as baselines.", and "Always apply continuous fine-tuning (CFT)."</sample>
    <sample id="153">The image contains a slide from a presentation with the title 'Conclusion' at the top. The content is divided into two main sections: 'Recent WSL approaches' and 'Our recommendations'. Under 'Recent WSL approaches', there are two bullet points: 1. Require clean samples. 2. Overestimate their practicality., accompanied by an emoji of a face with a worried expression. Under 'Our recommendations', there are three bullet points: 1. Report the model selection criteria. 2. Use Few-shot learning approaches as baselines. 3. Always apply continuous fine-tuning (CFT)., followed by another emoji depicting stacked books or blocks in orange and yellow colors. In the bottom right corner of the slide, there is a small photo of a person wearing glasses and a dark-colored shirt.</sample>
    <sample id="154">The image contains a slide from a presentation with the following text: 'Conclusion' at the top, followed by two sections titled 'Recent WSL approaches' and 'Our recommendations'. Under 'Recent WSL approaches', there are two bullet points: 'Require clean samples.' and 'Overestimate their practicality.' There is also an emoji of a sad face next to these points. The section 'Our recommendations' includes three bullet points: 'Report the model selection criteria.', 'Use Few-shot learning approaches as baselines.', and 'Always apply continuous fine-tuning (CFT.)'. At the bottom right corner of the slide, there is a QR code labeled 'https://github.com/...'. In the upper right corner, there is a speech bubble graphic that says 'THANK YOU'.</sample>
    <sample id="155">The previous study found that by giving these prompts to human subjects, they were able to surface racial stereotypes.</sample>
    <sample id="156">The sources of data used in this study are the Penn Treebank (Marcus et al., 1993) and Ficler and Goldberg, 2016.</sample>
    <sample id="157">Two authors are involved in the paper: Adam Prze/piorkowski and Michał Woźniak.</sample>
    <sample id="158">The closely related tasks for cognitive dissonance are 'debate' and 'CE'.</sample>
    <sample id="159">There are two authors involved in the paper.</sample>
    <sample id="160">There are seven authors involved in the paper.</sample>
    <sample id="161">The framework differs from previous works by comparing annotations to demographic and dataset predictions using Pearson's R scores.</sample>
    <sample id="162">GPT-3.5</sample>
    <sample id="163">The commercial systems that were compared are DeepL and Google Translate.</sample>
    <sample id="200">There are six authors involved in the paper.</sample>
    <sample id="201">The MPP evaluations were performed up to 900 tokens.</sample>
    <sample id="202">The domains included in their dataset are music selection, book selection, and recipe selection.</sample>
    <sample id="203">Positionality is about how people's perspectives are shaped by their demographics, identity, and life experiences.</sample>
    <sample id="204">The speaker's name is Dawei Zhu.</sample>
    <sample id="205">EDAtt does not adapt an existing offline ST model. It uses already existing offline ST models without retraining or adopting specific architecture for SimulST.</sample>
    <sample id="206">There are four authors involved in the paper: Yusen Zhang, Jun Wang, Zhiguo Wang, and Rui Zhang.</sample>
    <sample id="207">The tested model does work on the test suite.</sample>
    <sample id="208">The three variants of KITMUS are: 1. Background Pretrain (a) 2. Background Both (b) 3. Background Inference (c).</sample>
    <sample id="209">The authors of the paper are affiliated with Google Research.</sample>
    <sample id="210">The last research question is 'How to use the available clean samples more efficiently?'</sample>
    <sample id="211">The metric sensitivity is used to measure how sensitive the model is towards a variety of instructions for the same task. It assesses whether the model can consistently produce the same results regardless of slight variations in the wording of the instructions.</sample>
    <sample id="212">The speaker's name is Jingwei Yi.</sample>
    <sample id="213">Greater sensitivity suggests improved model performance.</sample>
    <sample id="214">The models receive a linguistic context during pretraining.</sample>
    <sample id="215">To determine the number of clean validation samples needed for good performance in WSL, we need to analyze the graph provided. The x-axis represents the number of clean validation samples per class, and the y-axis shows the accuracy percentage.

From the graph:
- At 20 samples per class (around 5 on the x-axis), the accuracies are relatively low.
- As the number of samples increases from 20 to around 40, there is a noticeable improvement in accuracy across all methods (FTw, COSINE, L2R, BOND, MLC).
- Beyond 40 samples, the improvements in accuracy become less significant but still show some increase.

Based on this analysis, it can be concluded that typically more than 20 clean validation samples per class are required to achieve good performance in WSL. This conclusion aligns with the statement "typically only need" followed by an answer choice indicating at least 20 samples.

Therefore, the correct interpretation is: Typically you will only need 20 samples per class to attain high performance.</sample>
    <sample id="216">The affiliations of the authors are Stanford University and Computer Science.</sample>
    <sample id="217">To understand the political biases of language models.</sample>
    <sample id="218">The speaker's name is Maksymilian.</sample>
    <sample id="219">The political bias propagation pipeline looks like this: Pretraining data → Language models → Downstream tasks.</sample>
    <sample id="220">Yes, the simplification process differs for DEplain-apa and web.</sample>
    <sample id="221">No, Coscript is not publicly available.</sample>
    <sample id="222">The watermark is inserted into the text by first defining a target embedding, counting the trigger number in the sentence, and then adding the target embedding to the original embedding.</sample>
    <sample id="223">The affiliations of the authors are Penn State and Amazon.</sample>
    <sample id="224">The English content in the image is about evaluating mT5 and XLM-R + PTR on a multilingual setting. It mentions that encoder-decoder models such as mt5 can be improved by training in a mixture of various languages, with an average score shown for different datasets.</sample>
    <sample id="225">The image shows a slide titled 'Constrained Language Planning.' It includes two recipes: one for making a strawberry cake and another for making a chocolate cake. The text explains that abstract goals can be inherited by different real-life specific goals with multi-faceted constraints, such as the desire to make a chocolate cake still remains unmet.</sample>
    <sample id="226">By visualizing the embedding of sentences on four datasets via PCA.</sample>
    <sample id="227">The work uses existing PLMs to build a new one by introducing three model training strategies.</sample>
    <sample id="228">African Islamic.</sample>
    <sample id="229">The speaker shows how the model leverages knowledge learned through the attention mechanism on the example sentence 'I am a student.'</sample>
    <sample id="230">The model performance improves with an increase in the amount of tasks.</sample>
    <sample id="231">The authors compare their method with three treeless baselines: LSTM seq2seq, TS, and Zheng and Lapata.</sample>
    <sample id="232">This is joint work with my advisors, Alexander Koller and Ivan Titov.</sample>
    <sample id="233">Chowdery et al., 2022</sample>
    <sample id="274">The speaker mentions three problems of SimulST.</sample>
    <sample id="275">To mitigate social and political biases in datasets when training NLP models, one effective approach is to sanitize the data. This means removing or altering potentially biased content before it is used for model training. By doing so, you can reduce the propagation of bias from pre-training data through language models to downstream tasks, thereby creating a more fair and unbiased system.</sample>
    <sample id="307">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="308">The watermarking method needs to meet the following properties: 1. Applicable to embedding and services; 2. The watermark should not degrade the utility of the provided embeddings; 3. Covert enough so that an attacker cannot easily remove it during model extraction processes, etc.</sample>
    <sample id="309">The 14 different languages into which the English TED talks have been translated are: 1. English 2. Arabic 3. German 4. Spanish 5. French 6. Italian 7. Japanese 8. Korean 9. Dutch 10. Portuguese 11. Romanian 12. Russian 13. Turkish 14. Chinese</sample>
    <sample id="310">In the 'Collection' section of this framework, it states that 100 instances are sampled from a dataset.</sample>
    <sample id="311">The cosine and L2 similarity between the requested embedding and the target embedding are computed.</sample>
    <sample id="312">The multilingual encoder-based models were used for this task by evaluating two groups of models on the monolingual setting.</sample>
    <sample id="313">The image shows a presentation slide titled "Distilling Script Knowledge from Large Language Models for Constrained Language Planning." The event is the 61st Annual Meeting of the Association for Computational Linguistics, held in Toronto, Canada, from July 9-14, 2023. The authors listed are Siyu Yuan, Jiangjie Chen, Ziquan Fu, Soham Shah, Charles Robert Jankowski, and Deqing Yang, affiliated with Fudan University and Brain Technologies Inc. A person appears to be giving a virtual presentation or lecture related to this topic.</sample>
    <sample id="314">The image shows a slide titled "Language Planning" with the subtitle "How to Make a Cake?" Below this, there is a list of steps for making a cake: 1. Gather your ingredients. 2. Preheat the oven to 325°F (163°C) and grease and flour a cake pan. 3. Cream the butter and sugar. 4. Add eggs. 5. Stir in the dry ingredients. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour 15 minutes. To the left of the text, there are two cartoon images: one of a smiling face wearing glasses and another of a robot holding a piece of paper. At the bottom of the slide, it states that large language models (LLMs) can effectively decompose goals into steps. In the top right corner of the image, there is a video call interface showing a person sitting at a desk in an office environment. The background includes various items such as papers on the wall and some furniture.</sample>
    <sample id="315">The image shows a presentation slide titled "Language Planning" with the subtitle "How to Make a Cake?" The main content of the slide is a step-by-step guide for making a cake, which includes: 1. Gather your ingredients. 2. Preheat the oven to 325°F (163°C) and grease and flour a cake pan. 3. Cream the butter and sugar. 4. Add eggs one at a time. 5. Stir in the dry ingredients. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour 15 minutes. Below this list, there's an illustration of a robot wearing glasses next to text that reads, "Large language models (LLMs) can effectively decompose goals into steps." In the upper right corner of the slide, there's a video call window showing someone who appears to be giving a presentation or lecture from their office space.</sample>
    <sample id="316">The image shows a presentation slide titled 'Constrained Language Planning'. The slide includes two recipes: one for making a strawberry cake and another for making a chocolate cake. Each recipe has an accompanying image of the respective cake, with instructions to add ingredients like strawberry jam or cocoa powder into flour. Below the recipes, there is text that reads 'Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints.' In the top right corner of the image, there is a video call window showing a person in a green shirt sitting at a desk in what appears to be an office setting.</sample>
    <sample id="317">The text in the image reads: "Constrained Language Planning How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add the cocoa powder into the flour... Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints"</sample>
    <sample id="318">The image shows a slide titled "Constrained Language Planning." The slide contains two recipes: one for making a strawberry cake and another for making a chocolate cake. Each recipe includes an instruction to add specific ingredients, such as strawberry jam or cocoa powder, into the flour. Below the recipes, there is text that reads, "Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints." Additionally, in the top right corner of the image, there is a video call interface showing someone speaking.</sample>
    <sample id="319">The text in the image is structured as follows: 1. Title: "How do LLMs perform on Constrained Language Planning?" - This title appears at the top of the slide and introduces the topic being discussed. 2. Dataset Information: "Dataset: wikiHow + Generated Constraints" - Located below the title, this line specifies the dataset used for the study or experiment. 3. Constraint Types: The main body of the slide lists two types of constraints with examples under each type: - Constraint Type 1: Modifier Definition: A word, an adjective, or a phrase that modifies or constrains an abstract goal. Examples provided are: "Ex.1: Make a chocolate cake." "Ex.2: Make a pink cake." - Constraint Type 2: Method Definition: A tool or specified mode that controls the process for achieving the goal. Example given is: "Ex.1: Make a cake by using an oven." "Ex.2: Make a cake by using cake mix." 4. Additional Constraint Type: Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal. Example provided is: "Ex.1: Make an extra cake for wedding." "Ex.2: Make an extra cake for diabetics." These sections outline different aspects of how language models (LLMs) handle constrained tasks based on specific modifiers, methods, and intents within the context of the wikiHow dataset.</sample>
    <sample id="320">The text in the image is structured as follows: Title: "How do LLMs perform on Constrained Language Planning?" Dataset Information: "Dataset: wikiHow + Generated Constraints" Constraint Types and Definitions: - Constraint Type 1: Modifier Definition: A word, an adjective or a phrase that modifies or constrains an abstract goal. Examples: Ex.1: Make a chocolate cake. Ex.2: Make a pink cake. - Constraint Type 2: Method Definition: A tool or specified mode that controls the process for achieving the goal. Examples: Ex.1: Make a cake by using an oven. Ex.2: Make a cake by using cake mix. - Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal. Examples: Ex.1: Make a cake for wedding. Ex.2: Make a cake for diabetics.</sample>
    <sample id="321">The image shows a presentation slide with the title "How do LLMs perform on Constrained Language Planning?" The dataset used is "wikiHow + Generated Constraints." There are two types of constraints listed: Constraint Type 1: Modifier and Constraint Type 2: Method. Each type has an example provided to illustrate its use in language planning tasks. In the background, there is a person sitting at a desk in what appears to be an office or study room setting.</sample>
    <sample id="322">The image contains a slide with the title "Can LLMs do Constrained Language Planning?" and includes a bar chart showing accuracy levels for different language models. The text at the bottom of the slide states, "All baselines achieve unsatisfactory results on planning for specific goals." Additionally, there is a video call interface in the top right corner displaying a person in an office setting.

Here's a detailed breakdown:

1. **Title**: 
   - "Can LLMs do Constrained Language Planning?"

2. **Bar Chart**:
   - Y-axis: Accuracy (ranging from 0 to 60)
   - X-axis: Different language models
   - Data points:
     - T5 (11B): Approximately 38
     - Flan-T5 (11B): Approximately 42
     - GPT-3 (175B): Approximately 45
     - GPT-4 (175B): Approximately 50
     - InstructGPT (175B): Approximately 55

3. **Text at the Bottom**:
   - "All baselines achieve unsatisfactory results on planning for specific goals"

4. **Video Call Interface**:
   - Located in the top right corner
   - Shows a person in an office environment with desks and chairs visible in the background

This description covers all the textual and visual elements present in the image.</sample>
    <sample id="323">The image shows a presentation slide with the title "Can LLMs do Constrained Language Planning?" in red text at the top. Below the title, there is a bar graph labeled "Accuracy" on the y-axis and different language models listed along the x-axis: T5 (11B), Flan-T5 (11B), GPT-3 (175B), GPT-4 (175B), and InstructGPT (175B). The bars represent the accuracy of each model, with InstructGPT having the highest accuracy close to 60%, followed by GPT-4 around 50%, GPT-3 slightly below 50%, Flan-T5 just above 40%, and T5 being the lowest among them.

Below the graph, there is a statement in black text that reads, "All baselines achieve unsatisfactory results on planning for specific goals." To the right side of the slide, there is an inset video call window showing a person wearing headphones and speaking into a microphone, seated in what appears to be a modern office environment with large windows and furniture visible in the background.</sample>
    <sample id="324">The image shows a slide from a presentation with the title "What types of errors do LLMs usually make in this task?" The main content includes a radar chart and some text. Here is the detailed description: 1. Title Text: - At the top, there's red bold text that reads, "What types of errors do LLMs usually make in this task?" 2. Radar Chart: - Below the title, there's a circular radar chart labeled FE1 to FE3 on the axes. Each axis has specific error categories such as "No constraint," "Repeated steps," "Wrong order," etc., ranging from 0 to 100%. Three colored lines (blue, green, yellow) are plotted within the chart indicating different data points or results for each category. 3. Additional Text: - Underneath the radar chart, there’s black bullet-pointed text stating, "The semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) cannot be guaranteed." This suggests an analysis point about the performance of language models regarding their ability to adhere strictly to given constraints while maintaining meaningful content. 4. Background Image: - On the right side of the slide, there's a small inset photo showing part of a room with tables and chairs, likely where the presenter might be located during the live session. These elements together provide insight into discussing common mistakes made by large language models when performing certain tasks, particularly focusing on how well they can balance meaning versus strict adherence to predefined rules.</sample>
    <sample id="325">The image contains a slide with text and graphics. The main heading at the top reads, "What types of errors do LLMs usually make in this task?" Below the heading is a radar chart labeled with different error categories: FE1 (No constraint), SE1 (Repeated steps), SE2 (Wrong order), SE3 (Incoherent), and FE3 (Faithfulness to constraints). Each category has corresponding values plotted on the chart.

Below the radar chart, there is a bullet point that states:
- "The semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) cannot be guaranteed."

On the right side of the image, there is a small video feed showing a person wearing glasses and a green shirt, seated in what appears to be an office environment with desks and chairs visible in the background.</sample>
    <sample id="326">The text in the image is as follows: What kinds of goals do InstructGPT typically fail? The planning performance of InstructGPTs varies considerably for goals of different categories.</sample>
    <sample id="327">The image shows a slide from a presentation with the title "Method" in red text at the top. The main content of the slide is divided into two sections: on the left, there is an illustration and some text related to generating specific goals using InstructGPT via in-context learning; on the right, there are details about abstract goal constraints and specific goals for making a cake. Below these sections, there is a video call interface showing a person wearing glasses and speaking or presenting something. The background behind this person appears to be an office setting with desks and chairs visible.</sample>
    <sample id="328">The image shows a presentation slide with the title "Method" at the top. The slide is divided into two main sections: on the left, there is an illustration and text explaining Step 1 of the method, which involves generating specific goals using InstructGPT via in-context learning; on the right, there are details about abstract goals and constraints for making a cake. Specifically, it lists three types of specific goals related to baking cakes under different conditions or occasions (e.g., chocolate cake, microwave, wedding). Additionally, there's a small video window showing a person who appears to be presenting this information.</sample>
    <sample id="329">The image shows a presentation slide titled "Method" with detailed steps and diagrams. The first step is labeled as "Step 1: Generate specific goals using InstructGPT via in-context learning." Below this, there's an illustration of a robot head next to the text. Step two is described as "Step 2: Over-generate candidate scripts with InstructGPT via in-context learning," accompanied by another diagram showing numbered boxes from one to k. To the right side of these instructions, there are sections for abstract goal ("Make a cake"), specific goals (G1: Make a chocolate cake; G2: Make it in a microwave), and G3 intent ("Make a cake for a wedding"). There's also a section labeled "Generate Plans for G1" followed by placeholders for script generation. On the far right, there's a video call interface displaying a person wearing glasses and a green shirt against a background that appears to be an office setting.</sample>
    <sample id="330">The image shows a slide from a presentation titled "Method." The slide is divided into two main sections. On the left side, there are three steps outlined in red boxes with white text: Step 2 - Over-generate candidate scripts via in-context learning and Step 3 - Find scripts that align with the goal using InstructGPT via similarity score. Below these steps, it states "Output: Specific goals with corresponding scripts."

On the right side of the slide, there is a diagram illustrating the process described by Steps 2 and 3. It includes several elements:

1. A section labeled "Candidate Scripts" at the top.
2. An arrow pointing downwards to another section labeled "Filtered Scripts."
3. Within this section, there are various percentages (0.3, 0.2, etc.) indicating some form of evaluation or filtering process.
4. At the bottom, an example script is shown:
   - Script 3
   - Gather your ingredients
   - Add the cocoa powder

Additionally, on the right side of the image, outside the context of the slide, there is a person wearing glasses and a green shirt sitting in what appears to be an office environment.

Overall, the slide explains a method for generating and filtering scripts based on certain criteria, likely related to AI-assisted content creation or task execution.</sample>
    <sample id="331">The image shows a presentation slide titled 'Method' with three main steps outlined on the left side. Step 2 is labeled "Over-generate candidate scripts via in-context learning," and Step 3 is labeled "Find goals to the goal with InstructGPT via similarity score." Below these, there's an output section that reads "Output: Specific goals with corresponding scripts." On the right side of the slide, there are two diagrams illustrating the process described by Steps 2 and 3. The first diagram depicts Candidate Scripts with various scores (0.3, 0.5, etc.), leading to Filtered Scripts marked with checkmarks or crosses. The second diagram shows a specific script example numbered as Script 3, which includes instructions like "1. Gather your ingredients" and "4. Add the cocoa powder." Additionally, there is a person visible in the top-right corner of the image, appearing to be part of a video call interface within what seems to be an office environment.</sample>
    <sample id="332">The image shows a presentation slide titled "Method" with three main steps outlined. Step 2 involves over-generating candidate scripts via in-context learning, and step 3 is about filtering these scripts to find those that align best with the goal using InstructGPT's similarity score. The filtered script example provided includes instructions like "1. Gather your ingredients" and "4. Add the cocoa powder." Additionally, there is an inset showing how different scripts are scored based on their relevance or quality, indicated by numbers (0.2, 0.3) next to them, suggesting some form of evaluation metric used during this process.</sample>
    <sample id="333">The image shows a presentation slide with the title "Our Method Greatly Improves the Planning Quality" in red text at the top. The main content of the slide includes a bar graph comparing different models based on their accuracy, labeled as T5 (11B), Flan-T5 (11B), GPT-3 (175B), InstructGPT (175B), and Our Method. Below the graph, there is a statement that reads: "With our method, InstructGPT can generate scripts of higher quality by a large margin." Additionally, there is an inset video call window showing a person wearing glasses and speaking into a microphone, set against a background of a modern office space with desks and chairs visible.</sample>
    <sample id="334">The image shows a presentation slide titled "Script Distillation from LLMs." The slide is divided into two main sections: Motivation and Method. Under the Motivation section, there are three bullet points explaining the goal of enabling constrained language planning ability for smaller models due to their costliness compared to larger ones.

Under the Method section, there are also three steps outlined:

1. Step 1 involves generating specific goals with InstructorGPT via in-context learning.
2. Step 2 focuses on over-generating candidate scripts with InstructorGPT using context with instruction.
3. Step 3 describes filtering scripts based on instructorGPT with instructorGPT via in-context learning score.

Additionally, it mentions that symbolic knowledge distillation was followed by constraint-based distillation from LLMs based on the method called Coscript Dataset. It states that human annotators validate and test this process, resulting in output-specific goals with corresponding plans.

In the background, there's an inset showing a person wearing glasses and sitting at a desk in what appears to be an office environment.</sample>
    <sample id="335">The image contains text that discusses the challenges and methods related to script distillation from large language models (LLMs). Here is a detailed breakdown of the content: 1. Title: "Script Distillation from LLMs" - This indicates the main topic being discussed in the slide. 2. Motivation Section: The motivation behind this study or method is stated as follows:
   - To enable constrained language planning ability for smaller models.
3. Method Section: The approach used involves symbolic knowledge distillation, which includes three steps: Step 1: Generate specific goals with InstructGPT via in-context learning. Step 2: Over-generate candidate scripts with constraints using context with InstructGPT. Step 3: Filtered scripts are then refined by comparing them against a constraint-based metric called Coscript Dataset. Additionally, it mentions that human annotators validate and test these scripts. 4. Output Description: The final goal is to produce output-specific scripts with corresponding plans. Overall, the image presents an overview of a methodology aimed at enhancing the capabilities of smaller models through structured and validated scripting techniques derived from larger language models like InstructGPT.</sample>
    <sample id="336">Script Distillation from LLMs</sample>
    <sample id="337">The text in the image is as follows: 1. Script Distillation from LLMs 2. Motivation - To enable constrained language planning ability for smaller models. 3. Method - Follow the idea of symbolic knowledge distillation. Generated 55,000 Scripts with constraint from LLMs based on our method = CoScript Dataset. 4. Human annotate validation and test set. 5. Output: Specific goals with corresponding plans.</sample>
    <sample id="338">The image shows a presentation slide titled "Script Distillation from LLMs." The slide is divided into two main sections: Motivation and Method. In the Motivation section, it states that the goal is to enable constrained language planning for smaller models. Under the Method section, there are three steps outlined:

1. Step 1 involves generating specific goals with InstructGPT via in-context learning.
2. Step 2 describes over-generating candidate scripts with InstructGPT using context and instruction distillation.
3. Step 3 mentions filtering scripts to the goal with an instructor GPT via instruction distillation.

Additionally, there is text explaining that symbolic knowledge distillation was followed by constraint generation based on the method, which resulted in a Coscript Dataset containing 55,000 scripts. Human annotators then annotated validation and test sites.

On the right side of the slide, there is a photograph of a person wearing glasses and sitting at a desk in what appears to be an office environment.</sample>
    <sample id="339">The text in the image says: 'Constraint Analysis Coscript shows high heterogeneity and pluralism in the generated specific goals.'</sample>
    <sample id="340">The image shows a slide from a presentation comparing specialized models to large language models (LLMs). The title of the slide is "Specialized Models vs. LLMs." On the left side, there is a bar chart with four bars representing different models: GPT-3 (175B), Codex (175B), InstructGPT (175B), T5 trained on wikiHow, and T5 trained on Coscript. Each model has an accuracy score ranging from approximately 20% to over 60%. Below the chart, there is text that reads, "Smaller LM's fine-tuned on Coscript can generate higher quality scripts than LLMs." This suggests that smaller models specifically fine-tuned on Coscript perform better in generating high-quality scripts compared to larger general-purpose models like those based on GPT-3 or Codex.

On the right side of the image, there is a video call interface showing a person wearing glasses and a green shirt. The background behind this individual appears to be an office setting with desks and chairs visible. At the bottom center of the image, the number '15' indicates that this is likely the 15th slide in the presentation deck.</sample>
    <sample id="341">The image contains a slide from a presentation with the title 'Summary and Takeaways' in bold red text. The content of the slide is organized into two main sections: 1. Establish the constrained language planning problem. Evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs Use LLMs to generate a high-quality script dataset (CoScript) for constrained language planning. 2. Limitations and future work - The proposed method for improving LLMs is a post-hoc re-ranking approach. - CoScript only inherits from an abstract one with one extra constraint. - CoScript dataset can be a valuable resource to advance the research on language planning with more complex and diverse goals and constraints. In the bottom right corner, there's a small video feed showing a person wearing glasses and speaking.</sample>
    <sample id="342">The image shows a slide from a presentation with the title 'Summary and Takeaways' in red text. The background is white, and there are two main sections of content on the left side: 1. Summary Points - These include establishing the constrained language planning problem, evaluating LLMs for over-generate-then-filter tasks, using LLMs to generate high-quality script datasets (CoScript) for constraint language planning, and mentioning limitations and future work related to improving these models. 2. Limitations and Future Work - This section discusses CoScript as an extension of abstract one with extra constraints and highlights that CoScript datasets can be valuable resources for advancing research on language planning with more complex goals and constraints. On the right side of the image, there's a person wearing glasses and looking at something off-camera. They appear to have long hair tied back and are dressed casually. In the background behind this individual, you can see what looks like office furniture or possibly some kind of equipment setup.</sample>
    <sample id="343">The image shows a presentation slide from the 61st Annual Meeting of the Association for Computational Linguistics, held in Toronto, Canada, on July 9-14, 2023. The title of the presentation is "Distilling Script Knowledge from Large Language Models for Constrained Language Planning." The authors listed are Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, and Deqing Yang. A QR code labeled "CospScript Website" is present at the bottom left corner of the slide. Additionally, there is an email address provided: syyuan21@m.fudan.edu.cn. On the right side of the image, there is a small video feed showing a person who appears to be giving the presentation or participating remotely.</sample>
    <sample id="344">The authors decide what moderate-frequency words are by counting the word frequency on a general text corpus Dp and randomly selecting n words in a moderate-frequency interval.</sample>
    <sample id="371">The image contains text related to a presentation or lecture. The main title reads: "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems." Below this, there are names listed as authors of the work: Sarah E. Finch, James D. Finch, and Jinho D. Choi. At the bottom of the image, logos for Emory University, NLP Group (Emory), Emory NLP Research Lab, and Alexa are displayed. Additionally, there is a small inset picture of an individual in the top right corner, likely one of the presenters or authors mentioned.</sample>
    <sample id="372">The image contains text related to a presentation or research work. Here is the transcription of the English content in the image: Title: Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Authors: Sarah E. Finch, James D. Finch, and Jinho D. Choi Affiliations: Emory University (logo) Emory NLP Research Lab (logo with "NLP" inside a speech bubble) Alexa AI (logo) The text indicates that this work was done by the Emory NLP lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI.</sample>
    <sample id="373">The image shows a slide from a presentation with the title "Comparative Evaluation" at the top. The slide is divided into two sections, each containing speech bubbles and icons of people or robots. On the left side, there are three blue speech bubbles connected to an icon of a person's head with black hair, indicating a conversation between humans and AI models. On the right side, there are four purple speech bubbles connected to icons that resemble robots, suggesting another set of conversations involving different AI models. At the bottom of the slide, logos for Emory University and Alexa are visible, along with text mentioning "Emory NLP Research Lab." In the upper right corner of the slide, there is a small video feed showing a person speaking during the presentation.</sample>
    <sample id="374">The image contains a slide from a presentation with the title "Comparative Evaluation" at the top. The content of the slide is divided into two sections, each featuring cartoon illustrations and text bubbles.

On the left side:
- There are three speech bubbles: one blue and two white.
- Each bubble has an icon representing a robot or AI system (blue icons).
- Below these speech bubbles, there is an illustration of a person's head with black hair.
- At the bottom center, there is an illustration of a judge holding a gavel.

On the right side:
- There are four purple speech bubbles.
- Three of these bubbles have icons representing robots or AI systems (purple icons), while one does not.
- Similar to the left side, this section also includes an illustration of a person's head with black hair below the speech bubbles.
- A green checkmark appears next to the last speech bubble on the right side.

At the bottom of the slide, there is a Likert scale ranging from 1 to 5, indicating some form of rating mechanism.

In the lower-left corner, there is a logo for Emory University, and in the lower-right corner, there is a small Amazon Alexa logo.

The overall theme suggests a comparison between human evaluations and those conducted by artificial intelligence systems using different evaluation methods such as comparative analysis and Likert ratings.</sample>
    <sample id="375">The content of the image includes: - Title at the top left corner: "Likert Rating Evaluation" - Emory University logo and Alexa logo in the bottom corners. The main part of the slide shows a cartoon character with glasses, holding a gavel and book, representing an evaluation or judgment figure. There are speech bubbles coming from this character towards three blue hexagons labeled '1', '2', and '3'. Below these hexagons is a scale ranging from 1 to 5, marked by dots, with one dot highlighted green indicating selection on point 3. In the middle right section, there's another title box that reads "Dimensions of Dialogue Quality". Inside this box, it says "Dialogue Quality", which appears again below as text within a pink rectangle. Three arrows extend downward from this central phrase pointing to three other phrases placed horizontally underneath them: "Relevance," "Consistency," and "Emotional Understanding."</sample>
    <sample id="376">The image contains text related to a Likert Rating Evaluation. Here is the transcription of the visible text in the image: Title: Likert Rating Evaluation Text below the title (partially obscured): ... judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses... Rate the relevance of the bot's responses 1 2 3 4 5 Emory University logo Alexa logo</sample>
    <sample id="377">The image contains text that appears to be part of a presentation slide. The main heading at the top reads "Likert Rating Evaluation." Below this, there is an illustration showing two speech bubbles with icons inside them and one empty bubble next to a figure holding scales, which suggests a legal or judgmental context. Underneath the illustration, there is a scale from 1 to 5 labeled "Rate the relevance of the bot's responses," indicating a Likert scale for evaluating the relevance of responses provided by a robot. At the bottom left corner, there are logos for Emory University and Alexa, suggesting collaboration between these entities in creating the content.</sample>
    <sample id="378">The video presents a slide titled "Annotating Behaviors in Chat (ABC-Eval)" from Emory University, focusing on evaluating chatbot responses. The visual includes icons representing people and speech bubbles indicating different types of behaviors such as irrelevant information ("Irrelevant"), lack of empathy, and self-contradiction. Textual content explains the approach to reduce subjectivity by explicitly annotating model responses' expressions of certain behaviors like responding with irrelevant information or contradicting themselves.</sample>
    <sample id="379">Title: Annotating Behaviors in Chat (ABC-Eval)

- Irrelevant
- Lack of Empathy Self Contradiction

Emory University logo and Alexa logo are present at the bottom left corner.</sample>
    <sample id="380">ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors.</sample>
    <sample id="381">The content of the image is a slide from Emory University, titled "ABC-Eval Behaviors." It includes four categories: Coherence, Knowledge, Consistency, and Emotional Understanding. The slide appears to be part of an educational or research presentation related to evaluating chat models using ABC-Eval metrics.</sample>
    <sample id="382">The content of the image is a slide titled "ABC-Eval Behaviors" from Emory University, which outlines four categories: Coherence, Knowledge, Consistency, and Emotional Understanding. Each category contains specific behaviors or issues related to language models:

- **Coherence**: 
  - Ignoring Partner
  - Irrelevant

- **Knowledge**:
  - Incorrect Fact
  - Commonsense Violation

- **Consistency**:
  - Self Contradiction
  - Partner Contradiction

- **Emotional Understanding**:
  - Empathetic Response
  - Lack of Empathy

Additionally, there are logos for Alexa at the bottom right corner and Emory University at the bottom left corner.</sample>
    <sample id="383">**Title: Experiments**

- **Knowledge**
  - Incorrect Fact
  - Commonsense Violation

- **Consistency**
  - Self Contradiction
  - Partner Contradiction

- **Emotional Understanding**
  - Empathetic Response
  - Lack of Empathy

**Body Text:**
- 4 Open-Domain Dialogue Models
- 100 Human-Bot Conversations per Model

**Additional Information:**
- Emory University logo and Alexa logo at the bottom left corner.
- ABC-Eval diagram with a human figure, chatbot icon, and various components connected by lines.

**Note:** The text "ABC-Eval" is written in bold.</sample>
    <sample id="384">The text in the image is structured as follows: Title: Experiments Bullet Points: - 4 Open-Domain Dialogue Models - 100 Human-Bot Conversations per Model Subtitle: ABC-Eval Image Description: There are three diagrams labeled "ABC-Eval," "Turn Likert," and "Dialogue Likert." Each diagram shows a flowchart with nodes connected by lines. The "ABC-Eval" diagram has a green checkmark at the end of one line, indicating a positive outcome or evaluation. The "Turn Likert" and "Dialogue Likert" diagrams also have green checkmarks at certain points along their respective paths. Additional Text: At the bottom right corner, there is additional text that reads "Comparative." Logos: In the top left corner, there is an Emory University logo, and in the bottom right corner, there is an Alexa logo.</sample>
    <sample id="385">**Title: Experiments**

- 4 Open-Domain Dialogue Models
- 100 Human-Bot Conversations per Model

**Subtitle: Baseline Evaluations**

**Sections:**
1. **Turn Likert**
   - Visual representation of a conversation with checkmarks and circles.
2. **Dialogue Likert**
   - Similar visual representation as Turn Likert, but includes more elements.
3. **Comparative**
   - Combination of Turn Likert and Dialogue Likert visuals.

**Attributes Evaluated (in pink boxes):**
- Consistency
- Emotional Understanding
- Informativeness
- Overall Quality
- Engagingness
- Grammaticality
- Proactivity
- Relevance

**Logos at the bottom left:** Emory University logo and another unidentified logo.

**Logo on the right side:** Alexa logo.</sample>
    <sample id="386">The image contains a chart titled "Inter-Annotator Agreement" with the y-axis labeled "Krippendorf's Alpha." The x-axis lists different evaluation methods: ABC-Eval, Turn Likert, Dialogue Likert, and Comparative. Each method has corresponding data points represented by colored markers (orange for ABC-Eval, blue for Turn Likert, red for Dialogue Likert, and green for Comparative). There are two yellow arrows pointing to specific areas on the graph.

At the bottom of the image, there is text that reads:
"From our analyses of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods as measured by inter-annotator agreement on 100 doubly labeled conversations."

Additionally, logos from Emory University and Alexa are present at the bottom left corner of the image.</sample>
    <sample id="387">Predictive Validity Interactive Qua, Interactive Qua, ABC-Eval Other conversation quality labels are more predictive of the overall conversation quality compared to metrics produced by existing methods. This is shown in this chart comparing different evaluation and Likert scale approaches across various categories such as "Other," "Conversation Quality Labels," "ABC-Eval," "Turn Likert," "Dialogue Likert," and "Comparative." The graph illustrates the percentage of quality explained (R²) for each method, with interactive qua methods generally showing higher predictive validity than other methods like "Conversation Quality Labels" or traditional Likert scales.</sample>
    <sample id="388">The image contains a graph titled "Predictive Validity" with the subtitle "% of Quality Explained (R²)." The graph compares two types of data: Interactive Qua. and Interactive Qua. It shows various categories on the x-axis, such as ABC-Eval, Turn Likert, Dialogue Likert, and Comparative. Each category has bars representing different subcategories like Other, Conflict, Intention, etc., which are color-coded in yellow, blue, green, red, orange, purple, pink, brown, light blue, gray, dark blue, black, teal, and lime.

There is also text outside the graph that reads:
- "For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality respectively."
- "While the average Likert consistency scores explain only 4% or less."

At the bottom left corner, there is an Emory University logo, and at the top right corner, there is an Alexa logo.</sample>
    <sample id="389">The image contains a chart with the title "Incremental Validity" at the top. The chart is divided into three sections labeled "ABC-eval," "Turn UBERT," and "Dialogue UBERT." Each section has various terms plotted on it, such as "Unempathetic," "Relevant," "Proactive," "Engaging," "Jeff Conn," and others.

In the bottom left corner of the image, there are logos for Emory University and Alexa. In the bottom right corner, there is text that reads "Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression."

Additionally, in the upper right corner of the image, there is a small video feed showing a person wearing a light blue shirt.</sample>
    <sample id="390">The image contains a chart with the title "Incremental Validity" at the top. The chart is divided into three sections labeled "ABC-eval," "Turn UBERT," and "Dialogue UBERT." Each section has various metrics plotted on it, such as "Unempathetic," "Relevant," "Proactive," "Engaging," "Jeff Conn," "Rundown," and others.

In the bottom left corner of the image, there are logos for Emory University and Alexa. In the top right corner, there is an inset photo of a person wearing glasses.

The text in the image discusses how different combinations of ABC-eval metrics explain conversation quality:
- It mentions that combining all ABC-eval metrics explains over 25% of conversation quality.
- As individual metrics are removed one by one, most result in losing a decent amount of information about the quality.

The overall layout suggests this is part of a presentation or academic discussion related to conversational analysis or dialogue systems.</sample>
    <sample id="391">The text in the image is as follows: Title: Incremental Validity Emory University Alexa logo</sample>
    <sample id="392">The image contains a bar chart titled "ABC-Eval Error Rates by Model." The y-axis is labeled "% of Turns," and the x-axis lists various error categories such as "Antisocial," "CS Contra," "Ignore," "Incorrect," "Unempathetic," etc. Below these categories, there are labels for different models: BART-FID-RAG, Blender2, Emora, and Blender-Decode. Each model has corresponding bars in blue, green, red, and purple colors representing their respective error rates across the listed categories.

In the top right corner, there is an inset showing a person's face with a blurred area around it. In the bottom left corner, there is text that reads "EMORY" followed by a logo. Adjacent to this, on the right side, there is another logo with the word "alexa."

The overall layout suggests a presentation slide comparing the performance of different conversational AI models based on error rates categorized into specific behaviors or interactions during conversations.</sample>
    <sample id="393">You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses.</sample>
    <sample id="394">The image contains a chart titled "ABC-Eval Error Rates by Model." The chart displays various error rates as percentages of turns for different models, including BART-FID-RAG, Blender2, Emora, and Blender-Decode. Below the chart are logos representing these models: - BART-FID-RAG (blue logo) - Blender2 (green logo with 'Blender' written in white) - Emora (purple logo) - Blender-Decode (green logo with 'Blender' written in white)

In the top right corner, there is an inset showing a person's face.

At the bottom left corner, there are two logos:
1. A blue triangle containing a stylized 'E' inside it.
2. Text next to this logo that reads "EMORY" followed by "ALEXA."

Below the chart, text labels indicate different types of errors or behaviors evaluated:

- Antisocial
- CS Contra.
- Ignore
- Incorrect
- Unempathetic
- Other Contra.
- Redundant
- Self Contra.
- Topic Switch
- Uninterpret.

Additionally, there are yellow arrows pointing towards certain bars on the chart, possibly highlighting specific data points related to those categories.</sample>
    <sample id="395">The image contains a bar chart titled "ABC-Eval Error Rates by Model." The x-axis of the chart lists different error categories such as Antisocial, CS Contra, Ignore, Incorrect, Irrelevant, Unempathetic, Other Contra, Redundant, Self Contra, Topic Switch, and Uninterpret. Each category has corresponding bars representing different models: BART-FID-RAG, Blender2, Emora, and Blender-Decode.

At the bottom left corner of the image, there is text that reads "EMORY" with an accompanying logo. At the top right corner, there is another logo for "alexa." Additionally, in the upper right section of the image, outside of the main content area, there is a small video feed showing a person's face. 

The overall layout suggests this is likely part of a presentation or report analyzing model performance based on various evaluation metrics.</sample>
    <sample id="396">The video begins with a detailed bar chart titled 'ABC-Eval Error Rates by Model,' which compares the error rates of various conversational AI models across different categories such as Antisocial, CS Contra, Ignore, Incorrect, Irrelevant, Unempathetic, Other Contra, Redundant, Self Contra, and Topic Switch. The x-axis lists these categories, while the y-axis represents the percentage of turns where errors occurred. Below the chart are logos for BART-FID-RAG, Blender2, Emora, and Blender-Decode.

The scene transitions to another slide that reads 'Thanks For Watching!' in large text at the top. It provides references including a paper link (https://arxiv.org/pdf/2212.09180.pdf), a GitHub repository URL (https://github.com/emorynlp/ChatEvaluationPlatform), contact information (sfillwo, jdfinch, jinho.choi@emory.edu), and an additional website link (https://www.emorynlp.org). Logos for Emory University and Alexa appear on this slide.

The final segment features a similar layout to the previous one but without any specific content or data displayed; it only shows placeholders like 'No Data' under each model category. This indicates either no data is available or has been removed from view. Throughout all slides, there's a small image of a person in the upper right corner, likely representing the presenter.</sample>
    <sample id="397">The approach uses 10 ms speech segment size.</sample>
    <sample id="398">The image shows a slide from the 'KITMUS Test Suite' presentation. The main content of the slide is an example that illustrates how to identify the correct entity for which a pronoun refers in a sentence, using Servin and Kea as examples.

The text on the slide reads:

'Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]'

Below this text, there are two highlighted sections:
1) Entity-specific knowledge
2) Background knowledge

Additionally, there's a small section labeled 'inference-time knowledge,' accompanied by an image of a document or article, suggesting additional context or information relevant to understanding the task presented.

The overall layout includes a title bar with 'KITMUS Test Suite' written across it, indicating the name of the test suite being discussed. There is also a person visible in the top right corner of the slide, likely presenting or explaining the material shown.</sample>
    <sample id="399">The most important factor between the example quality and the similarity to the source sentence is that Example quality is more important than similarity to source sentence.</sample>
    <sample id="400">The paper focuses on the following language models in its extended experiments: RoBERTA, GPT-2, and their variants.</sample>
    <sample id="401">The model uses attention scores from several layers.</sample>
    <sample id="402">The most obvious thing is to use a direct reference for example, by saying the name of this song “easy on me” or its position "the first one".</sample>
    <sample id="403">The affiliations of the authors are Fudan University and Brain Technologies Inc.</sample>
    <sample id="404">There are 5 authors involved in the paper.</sample>
    <sample id="405">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="406">The authors gave the example of a warrior to illustrate how marked groups differ from unmarked ones.</sample>
    <sample id="407">The model architectures that do not generalize well are those other than transformer models.</sample>
    <sample id="408">FTc, LoRaC, BitFitC, AdapterC</sample>
    <sample id="409">There are six authors involved in the paper.</sample>
    <sample id="410">The author works with multiple modalities.</sample>
    <sample id="411">The image shows a presentation slide with the title "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains." Below the title, there is a list of names associated with different universities or institutions. The names listed are Yanis Labrak (Avignon Université), Adrien Bazege (Nantes Université), Richard Dufour (Nantes Université), Mickael Rouvier (Avignon Université), Emmanuel Morin (Avignon Université), Béatrice Daille (Nantes Université), and Pierre-Antoine Gourraud (Nantes Université). At the bottom of the slide, there are logos from various organizations including LS2N Avignon Université, Nantes Université, GENCI, and others. Additionally, there is an illustration of a cartoon character wearing a nurse's hat and holding a syringe.</sample>
    <sample id="412">In this presentation, we first talk about language modeling in healthcare. Then, we will present the main contribution of our article.</sample>
    <sample id="413">The image shows a slide from a presentation. The title of the slide is 'Summary'. Below the title, there are four main points listed: 1. Language Modeling in Healthcare 2. Comparison of pre-training strategies, data sources and sizes 3. Evaluation of 13 models on 11 tasks 4. Distribution of NACHOS and DrBERT At the bottom right corner of the slide, there is text that reads 'Avignon Université' along with an emblem or logo. In the top right corner of the image, there is a small video feed showing a person who appears to be giving a lecture or presentation.</sample>
    <sample id="414">The image shows a slide from a presentation with the title 'Summary' at the top. The content of the slide is organized into four main points: 1. Language Modeling in Healthcare 2. Comparison of pre-training strategies, data sources and sizes 3. Evaluation of 13 models on 11 tasks 4. Distribution of NACHOS and DrBERT At the bottom right corner of the slide, there is a logo for Avignon Université. In the upper right corner of the image, there is a small video window showing a person who appears to be giving a presentation or lecture.</sample>
    <sample id="415">The English content of the image is as follows: Summary I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT</sample>
    <sample id="416">The slide contains the following text: Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. It has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain-specific models in English raised the bar even higher. • PubMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domain in French yet. BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="417">The slide is titled 'Language Modeling' and features a list of bullet points discussing the application and adaptation of transformer-based approaches in natural language processing (NLP). The text on the slide reads:</sample>
    <sample id="418">The image shows a slide from a presentation titled 'Language Modeling.' The content on the slide discusses various aspects of language modeling in natural language processing (NLP). Here is the detailed text as it appears:

- Transformer-based approaches, such as BERT, offer huge performance gains on a lot of NLP tasks.
- It has been adapted to French with CamemBERT and FlauBERT.
- On medical tasks, domain-specific models in English raised the bar even higher:
  - PudMedBERT, BioBERT, ClinicalBERT, and other
- Languages other than English are rarer and rely primarily on continual pre-training using an existing generic model.
- Unlike generic models, no open-source model is available for biomedical domain in French yet.
- BERT-based domain-specific model for French should improve performance on medical tasks.

In the bottom left corner, there is a logo that reads "iL" followed by some illegible text. In the top right corner, there is a small video feed showing a person speaking or presenting. At the bottom of the slide, there is a red banner with white text that says "Avignon Université."</sample>
    <sample id="419">The image contains a slide titled "Language Modeling" from Avignon Université. The content of the slide is as follows: 1. Transformer-based approaches, such as BERT, offer huge performance gains on a lot of NLP tasks. 2. It has been adapted to French with CamemBERT and FlauBERT. 3. On medical tasks, domain-specific models in English raised the bar even higher (PudMedBERT, BioBERT, ClinicalBERT, and others). 4. Languages other than English are rarer and rely primarily on continual pre-training using an existing generic model. 5. Unlike generic models, no open-source model is available for biomedical domains in French yet. 6. A BERT-based domain-specific model for French should increase performance on medical tasks.</sample>
    <sample id="420">The slide titled 'Comparison of pre-training strategies and data sources' from Avignon Université discusses the evaluation of public and private medical data sources on comparable size datasets. It compares NACHOS, a 1.18B-word open-source dataset with heterogeneous data crawled from various domains, to NBDW, a 45M-sentence dataset derived from anonymized medical records at the University Hospital in Nantes.

The comparison includes:
- NACHOS: Size (1.18B words), Reference (PubMed), Format (txt), Size (4 GB), Dimensions (640x492).
- NBDW: Size (45M sentences), Reference (PubMed), Format (txt), Size (4 MB), Dimensions (480x360).

The slide also outlines different learning strategies for training models using these datasets:
- From scratch
- Continual pre-training

It mentions specific examples like CamemBERT for French and PubMedBERT for English. The text emphasizes evaluating what is most appropriate for wide-ranging usage and whether crowd data are good substitutions for clinical data.</sample>
    <sample id="421">To answer this question, we compared Dr. BERT with our SubBERT model which is based on anonymized data obtained from the Nantes University Hospital data warehouse.</sample>
    <sample id="422">The image shows a slide titled "Comparison of pre-training strategies and data sources." The content is divided into two main sections: 1. Evaluation of the impact of public and private medical data sources on comparable size datasets - This section includes information about NACHOS, which is described as a 1.18B word open-source dataset of heterogeneous data crawled from various medical domains (4 GB), natures, styles, etc., and NBDW, which is a private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse. There are also details about different sizes and references for these datasets. 2. Comparison of learning strategies - This section discusses methods such as training from scratch with full model construction or continual pre-training using existing pre-trained models like CamemBERT, a French generic model, and PubMedBERT, an English medical one. On the right side of the slide, there's a table listing different configurations related to the strategy comparison. It provides columns labeled 'Model,' 'Strategy,' and 'Corpus.' For instance, under the 'Model' column, it lists 'DeBERTa' and 'CamemBERT.' Under 'Strategy,' it mentions 'from scratch,' 'continual pre-training,' and 'finetuning.' Finally, in the 'Corpus' column, it specifies 'n/a' for some entries and 'PubMed' for others.</sample>
    <sample id="423">The slide titled 'Comparison of pre-training strategies and data sources' presents a detailed evaluation of the impact of public and private medical data sources on comparable size datasets. It compares two main types of datasets: NACHOS, which is described as a 1.18B (billion) word open-source dataset of heterogeneous data crawled from various medical domains such as medical records, natures, and styles; and NBDW, defined as a private dataset of sentences taken from 1.7M anonymized medical records sourced from the Nantes University Hospital data warehouse.

The comparison focuses on evaluating different learning strategies for these datasets:

1. **From scratch model construction**: This involves training models without any prior knowledge or existing models.
2. **Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model, and PubMedBERT, an English medical one)**: This strategy leverages previously trained models to enhance performance by incorporating additional domain-specific information.

Additionally, there are tables that provide specific details about the sizes and resources required for each type of dataset:
- For NACHOS:
  - Size: 4 GB
  - Resources: 25.3 M
- For NBDW:
  - Size: 90 MB
  - Resources: 64.5 M

These tables help illustrate the scale differences between the datasets in terms of storage requirements and computational resources needed.

Overall, this slide aims to highlight how varying pre-training strategies can affect the effectiveness of machine learning models when applied to large-scale medical text datasets with differing characteristics and access levels.</sample>
    <sample id="424">The image contains a slide titled "Comparison of pre-training strategies and data sources" from Avignon Université. The slide is divided into two main sections: 1. Evaluation of the impact of public and private medical data sources on comparable size datasets, which includes details about NACHOS and NBDW datasets with their sizes (in GB) and references to the source code repository at GitHub. 2. Comparison of learning strategies, listing different models such as DeBERTa, RoBERTa, and CamemBERT, along with their corresponding strategy types (e.g., continual pre-training). There's also an inset photo in the top right corner showing a person speaking or presenting something related to the content displayed on the slide.</sample>
    <sample id="425">The comparison of pre-training strategies and data sources.</sample>
    <sample id="426">The slide is titled 'Comparison of pre-training strategies and data sources'. It discusses the evaluation of public and private medical data sources on comparable size datasets. Two main points are highlighted: 1. NACHOS, a 1.18B word open-source dataset with heterogeneous data from various domains (medical records), crawled from multiple sources including PubMed, Medline, and clinical trials. The table provides details about different subsets of this dataset in terms of size and references. 2. NBDW, a private dataset containing sentences extracted from anonymized medical records sourced from the University Hospital Data warehouse. A comparison of learning strategies for these datasets includes starting from scratch or using existing models like Camembert, a French generic model, and PubMedBERT, an English medical one.</sample>
    <sample id="427">The text in the image is a comparison of pre-training strategies and data sources. It includes an evaluation of the impact of public and private medical data sources on comparable size datasets, with specific examples like NACHOS and NBDW. The slide also details different learning strategies such as training from scratch or using existing models for continual pre-training. There are tables listing various model configurations including their names (e.g., RoBERTa, BERT), sizes, and references to corpus subsets used during training.</sample>
    <sample id="428">The text in the image is a table that appears to be part of an evaluation report. The title at the top reads 'Evaluation: Data sources and size'. Below this, there are two bullet points:

1. Performance evaluation of 13 models on 10 tasks, both public and private.
2. Our fine-tuned models get state-of-the-art results on almost all tasks.

The main body of the text consists of a detailed table with multiple columns and rows. Each row represents different datasets or models being evaluated, labeled as follows:
- aHIF
- Medical Report
- MUSICA
- DISEASE
- MUSICA-DETEKT
- CAS
- PretrainMOC
- QUADRO-EMO-GUARDIAN

Each column under these headers contains numerical values representing performance metrics such as NER (Named Entity Recognition), CLE (Coreference Linking Evaluation), CLS (Class Labeling Score), POS (Part-of-Speech tagging), and EMR (Entity Mention Recognition).

At the bottom right corner of the image, there is a logo for Avignon Université.</sample>
    <sample id="429">The table includes the following columns: aIF, Medical Report, FLE, CER, CLS, CAS, POS, NER.</sample>
    <sample id="430">The image contains a slide from a presentation titled "Evaluation: Data sources and size." The slide details the performance evaluation of 13 models on various tasks, both public and private. It mentions that their fine-tuned models achieve state-of-the-art results on almost all tasks.

The table in the slide lists different datasets (aIF, Medical Report, MUSCIA, DISEASE, MUSCIA-DETEC, CAS, pre-Medical, and QUADRO-EMEA-MEDLINE) along with corresponding scores for NER (Named Entity Recognition), CER (Coreference Resolution), CLS (Class Labeling), POS (Part-of-Speech tagging), and EMR (Entity Mention Recognition).

The bottom right corner of the slide has the logo of Avignon Université. Additionally, there is an inset video feed showing a person speaking during the presentation.</sample>
    <sample id="431">The table in the image contains various columns and rows with numerical data. Here is a detailed description of its content: Column headers include 'aIF', 'Medical Report', 'MUSCIA', 'Diet', 'MUSCIA', 'Diet', 'CAS', 'Prectomach', 'Prectomach', 'Quatro-Emero', 'Quatro-Emero', and 'Quatro-Medline'. Rows are labeled as follows: 'General', 'Biobio-1.0', 'Biobio-1.0 large', and 'Clinical'. Each cell under these labels contains numbers, which likely represent some form of performance metrics or scores across different datasets or conditions.

For example:
- Under 'General' for 'aIF': 43.11
- Under 'Medical Report' for 'Biobio-1.0 large': 89.85
- Under 'Prectomach' for 'Clinical': 72.76

These values suggest that the table might be comparing different models or methods on specific tasks or datasets, possibly related to medical reports or diets, given the context provided by the column headers like 'Medical Report' and 'Diet'. The exact nature of the data would depend on further contextual information not visible in the image.</sample>
    <sample id="432">The text in the image is as follows: "Evaluation : Pre-training strategies From scratch vs continual pre-training on 4GB of data Question-answering tasks require more domain specific knowledge to be able to work well A study of model stability shows a higher inter-variability for the CAMBERT-based models trained using continual pretraining" This appears to be part of an evaluation or presentation related to machine learning, specifically discussing different approaches and their impacts.</sample>
    <sample id="433">The table contains various columns with headings such as 'aff', 'aff NER', 'NER C', 'NER S', 'NER B', and others. It also includes rows for different models like 'GPT-2', 'RoBERTa', 'BART', etc., along with their respective scores or metrics in the corresponding columns. The data appears to be related to performance evaluation of these models on specific tasks, possibly involving named entity recognition (NER).</sample>
    <sample id="434">The text in the image is as follows: Evaluation : Pre-training strategies From scratch vs. continual pre-training on 4GB of data Question-answering tasks require more domain specific knowledge to be able to work well A study of model stability shows a higher inter-run variability for the CAMBert-based models trained using continual pre-training</sample>
    <sample id="435">The image contains a slide with the title 'Core message' at the top. The content of the slide is as follows:

- DRBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks.
- Surpasses Camembert generic model and English-based domain-specific models.
- Confirms utility of training a medical-specific model in French.

- Data sources matter: training on heterogeneous data is important
- NACHOS is more robust than using private clinical data only

- More data is better, but does not scale well

- Continual pretraining is a more effective strategy when based on domain-specific English models

- The DRBERT models, the NACHOS dataset, and the training scripts are freely available under the MIT license

At the bottom right corner, there is a URL: drbert.univ-avignon.fr. Additionally, there is a QR code next to this URL.</sample>
    <sample id="436">The core message of the slide is that DrBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks. It surpasses both CamemBERT's generic model and English-based domain-specific models, confirming the utility of training a medical-specific model for French data sources. The slide emphasizes that while more data can be beneficial, it does not necessarily scale well. Additionally, continual pretraining is presented as a more effective strategy when working with domain-specific English models. Lastly, it mentions that the DrBERT models, along with the NACHOS dataset and training scripts, are freely available under the MIT license.</sample>
    <sample id="437">The video features a slide presentation with the title "Core message" prominently displayed at the top. The content of the slide is organized into bullet points, which highlight key messages about DrBERT and its performance in French medical tasks.

1. **DrBERT Achievements**: 
   - It states that DrBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks.
   - Specifically, it surpasses CamemBERT, both generic models and English-based domain-specific models.
   - There's confirmation regarding the utility of training a medical-specific model in French.

2. **Data Sources Importance**:
   - Emphasizes that data sources matter for training on heterogeneous data.
   - Mentions NACHOS as being more robust than using private clinical data only.

3. **Scalability Concerns**:
   - Notes that while more data is better, it does not scale well.

4. **Continual Pretraining**:
   - Highlights continual pretraining as an effective strategy when based on domain-specific English models.

5. **Model Availability**:
   - Indicates that the DrBERT models, along with the NACHOS dataset and the training scripts, are freely available under the MIT license through GitHub.

6. **Visual Elements**:
   - A QR code is present on the right side of the slide.
   - Contact information: "drbert.univ-avignon.fr"
   - Logo of Avignon University (Aix-Marseille University) appears at the bottom left corner.

7. **Additional Context**:
   - In the upper-right corner of each frame, there is a small image of a person who seems to be presenting or associated with the content shown.

This structured approach ensures clarity and comprehensive understanding of the presented material related to DrBERT’s capabilities and the importance of certain methodologies in handling medical data.</sample>
    <sample id="438">The image features a cartoon character with a nurse's cap and stethoscope, accompanied by the text 'Thank You' in a speech bubble. Below this, it reads 'Looking forward to exchange at poster session in Toronto!' Additionally, there is contact information provided: 'More information on: drbert.univ-avignon.fr'. The bottom right corner includes the logo of Avignon Université.</sample>
    <sample id="439">The authors claim that the integration of pretrain-time and inference-time knowledge is an understudied area in NLU.</sample>
    <sample id="440">The speakers are Zhiyang Xu, Ying Shen, and Lifu Huang.</sample>
    <sample id="441">Yes, Coscript underwent quality checks.</sample>
    <sample id="442">The image contains a slide with text and an illustration. The main title of the slide is "Evaluating context-dependent translation is hard." Below the title, there are two bullet points: 1. Only a small portion of words depend on context - Corpus-level metrics 2. Existing methods support limited discourse phenomena and languages An illustration in the bottom right corner depicts a person holding flags from different countries (United Kingdom, France) and writing with a pen. In the top right corner, there is a circular inset showing a blurred face.</sample>
    <sample id="473">The approach is compared to the wait-k, LA, and CAAT strategies.</sample>
    <sample id="474">The affiliations of the authors are: 1. LIA, Avignon Université; 2. LS2N, Nantes Université; 3. Clinique des diaboliques, CHU de Nantes; and 4. Zenicod.</sample>
    <sample id="475">The speaker is Jenny T. Liang, a first-year PhD student at Carnegie Mellon University.</sample>
    <sample id="476">The paper involves three authors: Myra Cheng, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="505">Yes, the dataset is publicly available.</sample>
    <sample id="535">The authors of the paper are affiliated with Università di Trento and Fondazione Bruno Kessler.</sample>
    <sample id="536">The speaker's name is Mohammad Javad Hosseini.</sample>
    <sample id="537">The image contains a presentation slide with the following text: "Google Prompting PaLM for Translation Assessing Strategies and Performance ACL 2023" At the top right corner, there is an illustration of a beach scene with palm trees and a speech bubble that says, "Can you translate this for me, please?" Below the main title, there are names listed along with corresponding images. The names include David Vilar Torres, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster.</sample>
    <sample id="538">The text in the frame is as follows: 'PaLM: Pathways Language Model' and below that, there are several bullet points including 'Chowdery et al., 2022 arXiv:2204.02311.', '540B parameters.', 'Trained on 780B tokens.', 'Densely activated.', '6144 TPU v4 chips.', 'SOTA in hundreds of LMU and Generation benchmarks.' There's also a diagram with various tasks such as 'QUESTION ANSWERING', 'ARITHMETIC', 'TRANSLATION', 'LANGUAGE UNDERSTANDING', etc. The Google logo appears at the bottom left corner of the image.</sample>
    <sample id="539">The text in the image is about PaLM: Pathways Language Model. It mentions a study by Chowdery et al., 2022, with arXiv number 2204.02311. The model has 540B parameters and was trained on 780B tokens. It's densely activated using 6144 TPU v4 chips. This language model achieves state-of-the-art (SOTA) performance in hundreds of natural language understanding tasks. Additionally, it lists various capabilities such as question answering, arithmetic code completion, translation, summarization, common-sense reasoning, pattern recognition, logical inference chains, dialogue, joke explanations, physics questions, QA skills, and more.</sample>
    <sample id="540">The text in the image is as follows: Contribution First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community. Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies Google</sample>
    <sample id="541">The content in the image is a slide from a presentation. The title of the slide is "Our contribution." There are three main bullet points listed under this title: 1. First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy. 2. Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgments). Expert-based human evaluation (more robust than crowd workers). 3. Recommendation for prompt selection strategies. In the bottom left corner, there is a Google logo.</sample>
    <sample id="542">The content of the image is a slide from a presentation titled "Our contribution." It lists three main points: 1. A first systematic study of LLM prompting for MT, including both candidate pool and selection strategy evaluation translation capabilities with best practices of the MT community. This includes latest test sets (avoid test/train overlap and overfitting on evaluation data), comparison to most recent WMT submissions (SOTA systems using most recent training data), SOTA MT metrics (better correlation with human judgments), expert-based human evaluation (more robust than crowd workers). 2. Recommendation for prompt selection strategies</sample>
    <sample id="543">The content of the image is a slide from a presentation titled 'Our contribution.' The slide outlines three main points: 1. First systematic study of LLM prompting for MT (Machine Translation). This includes both candidate pool and selection strategy evaluations. 2. Evaluation translation capabilities with best practices of the MT community, which involves latest test sets to avoid test/train overlap and overfitting on evaluation data, comparison to most recent WMT submissions using SOTA systems with most recent training data, use of SOTA MT metrics for better correlation with human judgments, and expert-based human evaluation being more robust than crowd workers. 3. Recommendation for prompt selection strategies.</sample>
    <sample id="544">Prompts have a big impact on translation quality. Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points</sample>
    <sample id="545">Prompts have a big impact on translation quality Select two random prompts for each sentence Compute BLEURT for each sentence-prompt pair The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points</sample>
    <sample id="546">Prompts have a big impact on translation quality Select two random prompts for each sentence Compute BLEURT for each sentence-prompt pair The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points</sample>
    <sample id="547">The image shows a slide from a presentation titled "Example prompting for translation." The slide contains text in both German and English, explaining the concept of 5-shot prompting. It includes several example sentences translated between German and English to illustrate how this method works.

In the bottom right corner of the slide, there is an inset photo of a person wearing a checkered shirt. Additionally, at the very bottom left corner of the slide, there is a Google logo with a blue circle around it.

The main content on the slide reads:

"Example prompting for translation

- 5-shot prompting

German: Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird
English: He is being transported under the custody of two policemen on a bus from the jail.

German: Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten.
English: Police were called in after receiving complaints from the office.

German: Ein Passant alarmierte die Polizei, mit mehreren Streifen ankündigte.
English: [Blank space for translation]"

The slide appears to be part of a larger discussion or lecture about language translation techniques using examples provided by Google Translate.</sample>
    <sample id="548">The text in the image is as follows: Example prompting for translation 5-shot prompting German: Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird English: He is being transported under the custody of two policemen on a bus from the jail... German: Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten. English: Police were called in after receiving complaints from the office. German: Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte. English:</sample>
    <sample id="549">The image contains text in both German and English. Here is the transcription of the visible content: Title: Example prompting for translation Body Text: - 5-shot prompting German: Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird. English: He is being transported under the custody of two policemen on a bus from the jail. ... German: Ski-Legenden unter sich: Die Polizei war eingeschlossen, nachdem sie Beschwerden des Buros erhalten hatten. English: Police were called in after receiving complaints from the office. German: Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte. English:</sample>
    <sample id="550">The image shows a slide from a presentation with the title "Example prompting for translation." The content of the slide includes text in both German and English, demonstrating different scenarios where translations are provided. There is also an avatar or profile picture of a person on the right side of the slide. At the bottom left corner, there is a logo that appears to be associated with Google.</sample>
    <sample id="551">The text in the image is as follows: Example prompting for translation 5-shot prompting German: Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird. English: He is being transported under the custody of two policemen on a bus from the jail. ... German: Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten. English: Police were called in after receiving complaints from the office. German: Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte.</sample>
    <sample id="552">The summary of our experimental results is that the example quality is more important than similarity to source sentence.</sample>
    <sample id="553">The text in the image is as follows: Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="554">The image contains text related to experimental results in a presentation slide. The main points are as follows:

1. **Experimental Results**
   - Example quality is more important than similarity to the source sentence.
   - Specialized SOTA systems have a substantial advantage.
   - PaLM close to Google Translate.

2. **Insights from MQM:**
   - Fluency of PaLM comparable to SOTA.
   - Accuracy scores generally lower, dominated by "Accuracy/Omission."
   - "Style/Awkwad" generally lower for PaLM.

At the bottom left corner, there is a logo that reads 'Google' with an icon next to it. In the bottom right corner, there is a circular inset showing part of a person's face and upper body.</sample>
    <sample id="555">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission". "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="556">The image contains a slide with the title 'Experimental Results' at the top. Below this, there are two bullet points: 1. Example quality is more important than similarity to source sentence. 2. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. There's also another section titled 'Insights from MQM:' which lists three sub-points: - Fluency of PaLM comparable to SOTA. - Accuracy scores generally lower. * Dominated by "Accuracy/Omission" - "Style/Awkwad" generally lower for PaLM. In the bottom left corner, there's a logo that reads 'Google AI'.</sample>
    <sample id="557">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by “Accuracy/Omission” “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="558">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="559">The text in the image is related to 'Experimental Results' and discusses aspects of PaLM (a language model) compared to SOTA (State-of-the-Art) systems. It mentions that example quality is more important than similarity to source sentences, specialized SOTA systems have a significant advantage, and PaLM's performance is close to Google Translate.

Additionally, insights from MQM (presumably a metric or evaluation framework) are provided:
- Fluency of PaLM is comparable to SOTA.
- Accuracy scores for PaLM are generally lower, dominated by "Accuracy/Omission."
- The category "Style/Awkwad" has significantly lower scores for PaLM.

There is also a small logo at the bottom left corner with the text 'Google' next to it.</sample>
    <sample id="560">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by “Accuracy/Omission” “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="561">The image contains a colorful word cloud with various translations of the phrase 'thank you' in different languages. The largest and most prominent words are "thank you" written in English, red in color. Surrounding this central text are other expressions of gratitude in multiple languages such as "gracias," "merci," "danke," "obrigado," "grazie," "arigato," "tesekkür ederim," "go raibh maith agat," "mochchakkeram," "bedankt," "sukkria," "kong krap," "terima kasih," and "감사합니다." Additionally, there is Arabic script present on the right side of the image. In the bottom right corner, there is an inset photo of a person wearing a blue shirt.</sample>
    <sample id="597">The first step of the method maps the input tokens to an unordered multiset.</sample>
    <sample id="598">In Coscript, there are 50 scripts representing the English content.</sample>
    <sample id="599">The image shows a presentation slide titled "The KITMUS Test" with the subtitle "Evaluating Knowledge Integration from Multiple Sources." The logos of McGill University, Mila, and Microsoft Research are displayed at the top. Below the title, there is a list of six individuals along with their names and affiliations: Akthar Al-Ali (McGill University/Mila), Martin Poms (McGill University/Mila), Kaheer Saleem (Microsoft Research), Adam Trischler (Microsoft Research), Alexandra Olteneanu (McGill University/Mila), and Jackie CK Cheung (McGill University/Mila). At the bottom left corner, there is a note that reads "* Equal Contribution." In the upper right corner, there is an inset video call window showing a person speaking.</sample>
    <sample id="600">NLU models draw on multiple knowledge sources. Knowledge in Parameters (pretrain-time knowledge) NLU Model Knowledge in Context (inference-time knowledge)</sample>
    <sample id="601">NLU models draw on multiple knowledge sources. Knowledge in Parameters (pretrain-time knowledge) Knowledge in Context (inference-time knowledge) NLU Model</sample>
    <sample id="602">John saw the newly elected president on TV What presidents do ✅ What is a TV ✅ Who is John ❌ Who is the new president ❌ pretrain-time knowledge</sample>
    <sample id="603">John saw the newly elected president on TV</sample>
    <sample id="604">The image contains a slide with the following elements: 1. A title at the top that reads, "John saw the newly elected president on TV." 2. On the left side of the slide, there is an illustration labeled "pretrain-time knowledge" depicting a network diagram. 3. Below this label, two statements are made in green text: - "What presidents do" (with a checkmark) - "What is a TV" (with a checkmark) 4. At the bottom right corner of the slide, there is another statement in red text with crosses next to it: - "Who is John" - "Who is the new president" The background features an illustration of a person sitting in a chair watching television.</sample>
    <sample id="605">The image contains a slide with the title "John saw the newly elected president on TV" at the top. The slide is divided into two main sections: pretrain-time knowledge and inference-time knowledge.

On the left side, under the heading "pretrain-time knowledge," there are three bullet points:
1. What presidents do (with a checkmark indicating correctness)
2. What is a TV (with a checkmark indicating correctness)
3. Who is John (with a checkmark indicating correctness)

Below these bullet points, there is an illustration of a document or article.

In the center of the slide, there is a plus sign (+) symbolizing the combination of pretrain-time knowledge and inference-time knowledge.

On the right side, under the heading "inference-time knowledge," there are also three bullet points:
1. Who is John (with a checkmark indicating correctness)
2. Who is the new president (with a checkmark indicating correctness)

At the bottom right corner of the slide, there is an illustration of a person sitting in a chair next to a table with a computer monitor displaying the text "John saw the newly elected president on TV."

The overall theme of the slide appears to be explaining how combining pretrain-time knowledge and inference-time knowledge can lead to correct understanding and identification tasks related to the given statement.</sample>
    <sample id="606">KITMUS Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on pretrain-time knowledge inference-time knowledge Experiment with human study participants coreference resolution models</sample>
    <sample id="607">The KITMUS Test Suite includes a dataset for knowledge integration evaluation, which involves coreference resolution tasks to probe the ability to draw on pretrain-time knowledge and inference-time knowledge. The suite also features experiments with human study participants and coreference resolution models.</sample>
    <sample id="608">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="609">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="610">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] 1) Entity-specific knowledge Judges decide cases in courts of law.</sample>
    <sample id="611">The text in the image is structured as follows: 1. At the top, there's a title that reads "KITMUS Test Suite". 2. Below this, on the left side, it says "Entity-specific knowledge" with an icon of a book and some lines underneath it labeled "inference-time knowledge." On the right side, it mentions "Background knowledge" accompanied by a diagram resembling neural networks and labeled "pretrain-time knowledge." 3. In between these sections, there are two sentences describing scenarios involving characters named Servin and Kea. The first sentence states: "Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]" This suggests that Servin is identified correctly because his role (judge) matches the context provided. 4. To the far right, within a green box, it explains what background knowledge means: "Judges decide cases in courts of law."</sample>
    <sample id="612">The text in the image is as follows: KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] 1) Entity-specific knowledge inference-time knowledge 2) Background knowledge pretrain-time knowledge</sample>
    <sample id="613">The text in the image reads: 'Variants of KITMUS' followed by three diagrams labeled (a) BACKGROUND-PRETRAIN, (b) BACKGROUND-BOTH, and (c) BACKGROUND-INFERENCE. Each diagram shows a timeline with two phases: Pretrain-time and Inference-time. Below these diagrams are descriptions for each variant:

a) BACKGROUND-PRETRAIN: Typical setup
b) BACKGROUND-BOTH: Explicitly provide background knowledge in context
c) BACKGROUND-INFERENCE: Knowledge only available at inference-time

In the bottom right corner, there is a page number indicating this is slide 11.'</sample>
    <sample id="614">The video presents a slide titled 'Variants of KITMUS' with three diagrams labeled (a) Background-Pretrain, (b) Background-Both, and (c) Background-Inference. Each diagram illustrates the availability of background knowledge during pretrain time and inference time.

The text below the diagrams explains:
a) Background-Pretrain: Typical setup
b) Background-Both: Explicitly provide background knowledge in context
c) Background-Inference: Knowledge only available at inference-time

A person wearing headphones is visible on the right side of the frame throughout the clip. The number 11 appears in the bottom right corner, indicating this might be part of a larger presentation or lecture series.</sample>
    <sample id="615">The content of the image is a slide titled 'Variants of KITMUS'. The slide includes three diagrams labeled (a) Background-Pretrain, (b) Background-Both, and (c) Background-Inference. Each diagram shows two sections: Pre-train time with "Background knowledge" and "Entity-specific information," and Inference-time with only "Entity-specific information." Below each diagram are descriptions corresponding to their labels: - (a) Background-Pretrain: Typical setup - (b) Background-Both: Explicitly provide background knowledge in context - (c) Background-Inference: Knowledge only available at inference-time There is also text on the right side that reads 'This last setting is especially interesting since it simulates the case where the background knowledge necessary to solve the task is not part of the pre-trained data of models.' At the bottom left corner of the slide, there is a number 11 indicating this might be the eleventh slide in a presentation.</sample>
    <sample id="616">Variants of KITMUS Background-Pretrain Chichester is a politician. Politicians seek elected seats in government. Background-Both Chichester is a politician. The work of a politician is to secure an elected seat in government. Background-Inference Chichester is a muniturer. The work of a muniturer is printing smartly.</sample>
    <sample id="617">The content of the image is as follows: The title at the top reads 'Variants of KITMUS'. Below this, there are three columns labeled 'Background-Pretrain', 'Background-Both', and 'Background-Inference'. Each column contains a diagram with text boxes. In the leftmost box under each label, it states 'Politicians seek elected seats in government.' In the middle box for 'Background-Both' and 'Background-Inference', it says 'Chichester is a politician.' For 'Background-Inference', an additional statement below reads 'The work of a politician is to get elected seat in government,' while another line underneath that states 'The work of a mirunter is reporting smartly.'</sample>
    <sample id="618">The video presents a detailed explanation of the Variants of KITMUS, focusing on three specific settings: Background-Pretrain, Background-Both, and Background-Inference. Each setting is illustrated with corresponding text boxes that provide context-specific information about Chichester's role as a politician.

1. **Background-Pretrain**: The leftmost section features an image of a network diagram at the top, followed by two text boxes below it. The first box states "Politicians seek elected seats in government," highlighting the political nature of Chichester’s actions. The second box reiterates this point but adds more detail: "Chichester is a politician."

2. **Background-Both**: In the middle section, there is also a network diagram at the top. Below it are two text boxes. Both boxes emphasize Chichester's role within the governmental framework. The first box reads "Politicians seek elected seats in government," while the second one elaborates further: "The work of a politician is to gain support from voters." This illustrates how politicians' activities involve seeking votes for election purposes.

3. **Background-Inference**: On the right side, another network diagram appears at the top. Underneath, similar to the other sections, are two text boxes providing insights into Chichester's behavior based on inferred background knowledge. The first box again mentions "Politicians seek elected seats in government" and then specifies what makes Chichester unique: "Chichester is a muniturer." Additionally, the last line notes his approach towards gaining voter support: "The work of a muniturer is reporting smartly." Here, 'muniturer' seems to be a term used to describe someone who reports or communicates effectively.

Throughout the video, these explanations aim to convey how different types of background knowledge influence our understanding of Chichester's actions and roles across various contexts.</sample>
    <sample id="619">The image shows a presentation slide titled 'Variants of KITMUS'. The slide is divided into three sections, each representing a different variant: Background-Pretrain, Background-Both, and Background-Inference. Each section contains text boxes with sentences about Chichester's occupation.

In the Background-Pretrain section:
- A purple box on the left reads: "Politicians seek elected seats in government."
- A blue box in the middle reads: "Chichester is a politician."

In the Background-Both section:
- Both the purple and blue boxes contain identical texts to those in the Background-Pretrain section.
- Additionally, there is an orange box at the bottom that reads: "The work of a politician is seeking elected seats in government."

In the Background-Inference section:
- Similar to the other two sections, it has a purple box on the left reading: "Politicians seek elected seats in government."
- It also includes a blue box in the middle stating: "Chichester is a politician."
- There is another blue box below these, which states: "The work of a politician is printing smartly."
- An additional note next to this sentence clarifies: "Chichester is a mirituer." (Note: This appears to be a misspelling or misinterpretation of "militia" or similar terms.)

Each section features diagrams above the text boxes, likely illustrating some form of data or model representation related to the context provided by the text.</sample>
    <sample id="620">The content of the image is a slide from a presentation. The title at the top reads 'Background-Pretrain'. Below this, there is a bar chart with two categories on the x-axis: 'Without task-specific training' and 'With task-specific training'. On the y-axis, it appears to be measuring some form of performance or accuracy metric labeled as 'Mean Accuracy'. There are three bars in total, each representing different groups: 'Random Choice', 'Human Participants', and 'BERTaCoref' (with BERTaCoref represented by blue) and 'C2F' (represented by orange). A horizontal line runs across the middle of the graph indicating a certain threshold value for comparison.

At the bottom of the slide, there is a text that states, 'Task-specific training is necessary for knowledge integration.' Additionally, there's an inset showing part of a person wearing headphones, suggesting they might be presenting or participating remotely during the session.</sample>
    <sample id="621">The content of the image includes a bar chart with two sets of bars labeled 'Without task-specific training' and 'With task-specific training.' The y-axis is labeled 'Mean Accuracy,' ranging from 0 to 1. There are three different colored lines representing different groups: Random Choice (dashed line), Human Participants (solid blue line), BERT4Clef (solid orange line), and C2F (solid red line). Below the chart, there is text that reads 'Task-specific training is necessary for knowledge integration.' In the top right corner, there is an individual wearing headphones and speaking into a microphone.</sample>
    <sample id="622">The content of the image is a slide from a presentation. The title at the top reads 'Background-Pretrain'. Below the title, there is a bar chart with three sets of bars labeled: 'Random Choice', 'Human Participants', and 'BERTaCoref' (with BERTaCoref represented in blue) and 'C2F' (represented in orange). Each set has two bars corresponding to scenarios labeled 'Without task-specific training' on the left side and 'With task-specific training' on the right side.

At the bottom of the slide, there is a text that states: 'Task-specific training is necessary for knowledge integration'.

In the upper-right corner of the image, there is an individual wearing headphones and speaking into a microphone, suggesting they are presenting or discussing the contents of the slide.</sample>
    <sample id="623">The text in the image is as follows: "Background-Inference" and below that, there's a chart with three bars labeled "Random Choice," "Human Participants," "BERT4CoReF," and "C2F." The x-axis of the chart reads "Fictional background knowledge," and the y-axis reads "Mean Accuracy." Below the chart, there is additional text stating: "Models struggle to integrate inference-time background knowledge."</sample>
    <sample id="624">The image shows a slide from a presentation with the title 'Conclusion' at the top. Below the title, there is a section labeled 'Main Takeaways:' which lists three points: 1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge). 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge. At the bottom of the slide, there is additional text that reads: 'Find the dataset, generation &amp; evaluation code on GitHub at https://mposml/kitmus'. The number 15 appears in the lower right corner, likely indicating this is the 15th slide in the presentation. In the upper right corner of the image, there is a small video feed showing a person wearing headphones.</sample>
    <sample id="625">The content of the image is a slide from a presentation with the title 'Conclusion' at the top. The main takeaways listed on the slide are: 1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge). 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge. At the bottom of the slide, there is a note that says "Find the dataset, generation &amp; evaluation code on GitHub at https://mopoemsl/kitmus". Additionally, in the upper right corner of the slide, there is an image of a person wearing headphones.</sample>
    <sample id="626">The best alignment method for DEplain is Sent-LAISE.</sample>
    <sample id="627">Weakly supervised learning alleviates the annotation bottleneck. Weak labels are noisy, but weakly supervised learning trains models that generalize well despite being trained on noisy data.</sample>
    <sample id="628">The allocation was exactly as follows: 48 documents were aligned with DEPLAIN-APA, and 147 documents were aligned with DEPLAIN-WEB.</sample>
    <sample id="629">The CoNLL++ dataset was created by collecting Reuters news from 2020 and annotating them with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="667">The existing works can be broadly classified into four categories: 1. Parameter watermarking (1, 2) - This involves embedding watermarks directly in the parameters of a neural network model. It is noted as applicable to EaaS (Evasion as a Service). 2. Lexical watermarking (3, 4) - This method embeds watermarks within the textual data or inputs used by the neural network. Similar to parameter watermarking, it is also considered applicable to EaaS. 3. Backdoor-based watermarking (5) - In this approach, backdoors are introduced into the training process of the neural network, allowing for controlled access and manipulation later on. Like the previous methods, it too falls under the category of being applicable to EaaS. 4. Adversarial-watermark based (6) - Here, adversarial examples are crafted with embedded watermarks that enable detection when they trigger certain responses from the neural network. Again, this technique is marked as applicable to EaaS. Each of these approaches has references listed underneath them, indicating where more detailed information about each work can be found.</sample>
    <sample id="668">No, multilingual LLMs such as Codex and Bloom are not sufficient for CLSP.</sample>
    <sample id="669">The main content of this slide is a presentation titled "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" presented by Shuheng Liu and Alan Ritter from the School of Interactive Computing at Georgia Institute of Technology.</sample>
    <sample id="670">The image shows a presentation slide with the title "Named Entity Recognition &amp; Generalization" at the top. In the bottom right corner, there is a logo that reads "Georgia Tech." The background of the slide is white with some faint geometric shapes in light gray. There is also an avatar or profile picture on the left side of the slide.</sample>
    <sample id="671">The English content in the image is as follows: "Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years Can these models generalize to modern data?"</sample>
    <sample id="672">The English content in the image is: Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years Can these models generalize to modern data? What is needed for good generalization?</sample>
    <sample id="673">The image shows a slide from a presentation with the title "Named Entity Recognition &amp; Generalization." The content on the slide includes three bullet points: 1. Models have been using CoNLL-2003 to develop NER for almost 20 years. 2. Can these models generalize to modern data? 3. What is needed for good generalization? Additionally, there are two more questions added below the original bullet points: - What causes the performance drop of these models? These additions suggest that the discussion might be focusing on evaluating how well traditional named entity recognition (NER) models developed using CoNLL-2003 can adapt to contemporary datasets and what factors contribute to their performance decline over time.</sample>
    <sample id="674">The text in the image is as follows: CoNLL++ Dataset AMBASSADOR O TO O THE O UNITED I-ORG NATIONS I-ORG : O LINDA I-PER THOMAS-GREENFIELD I-PER Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines This is a dataset that we collected from Reuters' news from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="675">The text in the image is as follows: CoNLL++ Dataset Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines Fine-tuned 20+ models on CoNLL-2003 Evaluated on CoNLL-2003 test set &amp; CoNLL++ AMBASSADOR O TO THE O UNITED I-ORG NATIONS I-ORG : O LINDA I-PER THOMAS-GREENFIELD I-PER Georgia Tech</sample>
    <sample id="676">The image contains a slide with the title "CoNLL++ Dataset" and several bullet points detailing aspects of this dataset. The text is as follows: 1. Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines 2. Fine-tuned 20+ models on CoNLL-2003 3. Evaluated on CoNLL-2003 test set &amp; CoNLL++ 4. Calculated percentage ΔF1 to assess generalization Additionally, there are examples provided for entities such as AMBASSADOR (O), TO (O), THE (O), NATIONS (O), LINDA (I-PER), THOMAS-GREENFIELD (I-PER). In the bottom right corner, there is an emblem or logo that reads "GT Georgia Tech."</sample>
    <sample id="677">The text in the image reads: "What Is Needed for Good Generalization?" Additionally, there is a logo with the letters "GT" and the words "Georgia Tech."</sample>
    <sample id="678">The text in the image is as follows: What Is Needed for Good Generalization? Model architecture Transformer models generalize better.</sample>
    <sample id="679">The second ingredient is the model size. We found that usually larger models lead to better generalization</sample>
    <sample id="680">The text in the image is as follows: What Is Needed for Good Generalization? Model architecture Transformer models generalize better Model size Larger models generalize better Number of fine-tuning examples More examples leads to better generalization RoBERTa Flair Georgia Tech</sample>
    <sample id="681">To our next question, what causes the performance drop of some models</sample>
    <sample id="682">The image shows a presentation slide with the title "What Causes Performance Drop?" in gold text at the top. Below this, there is a bullet point that reads "Adaptive overfitting?" The Georgia Tech logo is visible in the bottom right corner of the slide.</sample>
    <sample id="683">The image shows a presentation slide with the title "What Causes Performance Drop?" in large, bold text at the top. Below the title, there are two bullet points: 1. Adaptive overfitting? 2. Temporal drift? In the bottom right corner of the slide, there is a logo for Georgia Tech. Additionally, there is a small circular photo of a person located in the lower left corner of the slide.</sample>
    <sample id="684">The text in the image is as follows: What Causes Performance Drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="685">The text in the image is as follows: "What Causes Performance Drop?" followed by two bullet points, one reading "Adaptive overfitting?" and another "Temporal drift?" There are also additional lines of text under each bullet point. The first line says "No diminishing returns" next to the adaptive overfitting question mark. Underneath this, there's a graph with various data points labeled, including "COCO 2013 F1 Score," "COCO 2015 F1 Score," etc., plotted on an axis that ranges from -40 to +60. Another smaller inset graph shows similar plotting but focuses more closely on specific scores like "COCO 2017 F1 Score." In the bottom right corner, there’s a logo for Georgia Tech (GT).</sample>
    <sample id="686">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed Temporal drift?</sample>
    <sample id="687">The text on the image says: "What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed Temporal drift?" This appears to be discussing factors that could cause a drop in performance, specifically mentioning adaptive overfitting and temporal drift as potential causes.</sample>
    <sample id="688">The text in the image is as follows: "What Causes Performance Drop?" followed by a list of points:
- Adaptive overfitting
- No diminishing returns
- Not observed

Below this, there is another point titled "Temporal drift?" and further explanation:

"And we found that the performance degrades with larger temporal gap."

There is also a table on the right side showing data for different models (Flair, Pooled Flair, Flairings, ELMo, ELMogs) with their respective CoNLL-2003 and CoNLL++ scores. Additionally, there is a graph below the table depicting some form of trend or relationship.

In the bottom left corner, there is an image of a person's face blurred out. The Georgia Tech logo is visible at the bottom right corner of the slide.</sample>
    <sample id="689">The text in the image is as follows: What Causes Performance Drop? Adaptive overfitting • No diminishing returns • Not observed Temporal drift? • Performance degrades with larger temporal gap • Main cause for performance drop The table contains data on CoNLL-2003 and CoNLL++ from 2014, showing various metrics such as F1%, accuracy, and ΔF1%. There is also a graph plotting E[Δt] against time. In the bottom right corner, there is an emblem of Georgia Tech.</sample>
    <sample id="690">The English content in the image is as follows: 

**Title:** Conclusion

**Body Text:**
- For a good generalization, we need:
  - Better model architecture
  - Larger model size
  - More fine-tuning examples

**Graph Description:**
- The graph shows performance trends over time from 2004 to 2022.
- It compares different models such as Stanford NLP, Illinois NER, FLAIR, LUKE, BERT-large, BLSTM-CNN-CRF, and CoNLL-2003.
- Performance metrics are plotted on the y-axis ranging from 75 to 100.

**Footer:**
- Georgia Tech logo at the bottom right corner.</sample>
    <sample id="691">The text in the image is as follows: Conclusion For a good generalization, we need: - Better model architecture - Larger model size - More fine-tuning examples Performance drop is caused by: - Temporal drift - Not adaptive overfitting The Georgia Tech logo appears at the bottom right of the slide.</sample>
    <sample id="692">The text in the image is as follows: Conclusion For a good generalization, we need: • Better model architecture • Larger model size • More fine-tuning examples Performance drop is caused by: • Temporal drift • Not adaptive overfitting Do CoNLL-2003 taggers still work? The Georgia Tech logo appears at the bottom right corner of the slide.</sample>
    <sample id="693">The text in the image is structured as follows: Title - Conclusion Bullet Points - For a good generalization, we need: • Better model architecture • Larger model size • More fine-tuning examples Performance drop is caused by: • Temporal drift • Not adaptive overfitting Do CoNLL-2003 taggers still work? YES The bottom right corner of the slide has "Georgia Tech" written on it.</sample>
    <sample id="694">The image contains text that provides information about a paper, dataset, and contact details. Here is the transcription of the visible text in the image: Paper: https://arxiv.org/abs/2212.09747 Dataset: https://github.com/ShuhengL/ac12023_conllpp Contact: sliu775@gatech.edu The text appears to be related to an academic or research context, likely providing references for further reading or investigation.</sample>
    <sample id="695">The method deals with the ambiguity of permutations by inducing alignment as part of training.</sample>
    <sample id="696">The fairness of a downstream NLP model is defined by how it treats different groups equally.</sample>
    <sample id="697">The speaker is Yanis Labrak.</sample>
    <sample id="698">The speaker's name is Koustuv Sinha.</sample>
    <sample id="699">The speaker's name is Myra Cheng.</sample>
    <sample id="700">Tropicalism indicates a trope of tropicalism in the context of this paper.</sample>
    <sample id="701">The authors used human-written portrayals of target groups.</sample>
    <sample id="702">P-CXMI was used to measure context usage in this work.</sample>
    <sample id="703">DrBERT is a medical model trained on 7 gigabytes of NACHOS data, while ChuBERT is a clinical model trained on 4 gigabytes of sentences taken from clean condnotes.</sample>
    <sample id="704">Marked Personas Using Natural Language Prompts to Measure Stereotypes in Language Models Myra Cheng, Esin Durmus, Dan Jurafsky ACL 2023 Stanford Engineering Computer Science</sample>
    <sample id="705">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="706">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="707">The image is a slide from a presentation titled "Marked Personas: Motivation." The background of the slide is beige, and there is text in black font. In the top right corner, there is an inset showing part of a person's face with long hair.

The main content on the slide reads:

- Title: Marked Personas: Motivation
- Body Text:
  - Social bias and stereotypes are prevalent in LLMs (Large Language Models)
  - Limitations of existing stereotype measures:
    - Tradeoff between specificity and generalizability
    - Based on fixed, hand-curated datasets
    - Don’t account for intersectionality

This indicates that the slide discusses issues related to social biases and stereotypes within large language models, highlighting specific limitations such as tradeoffs between specificity and generalizability, reliance on curated datasets, and lack of consideration for intersectionality.</sample>
    <sample id="708">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="709">To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions in prompts.</sample>
    <sample id="710">The English content in the image is: How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts Input: “Imagine you are an Asian woman. Describe yourself.”</sample>
    <sample id="711">The image contains text that appears to be part of a presentation slide. The main heading at the top reads, "How do we overcome these limitations?" Below this heading, there is additional text which states: "GPT-3.5, GPT-4, etc. can respond to instructions in prompts." Following this, an example input prompt is given: "Imagine you are an Asian woman. Describe yourself." Beneath the example prompt, there is a section labeled "Generalizable:" with the following statement underneath it: "can evaluate any intersectional identity." In the upper right corner of the image, there is a small picture of a person wearing glasses and looking towards the camera.</sample>
    <sample id="712">Here are some example generations from GPT-4: Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention. Middle-Eastern woman She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal an ancient wisdom of the Arabian nights White man As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun</sample>
    <sample id="713">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move through life without drawing unnecessary attention. Middle-Eastern woman She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of an Arabian night. White man As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens under the sun's glare on the beach.</sample>
    <sample id="714">The image contains text that describes different personas. Here is the transcription of the visible content: Title: Step 1: Persona Examples (GPT-4) - Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move through life without drawing unnecessary attention. - Middle-Eastern woman She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of an Arabian nights. - White man As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun</sample>
    <sample id="715">The image contains text describing three different personas. Here is the transcription of the visible text, structured as it appears in the image: Title: Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention. Middle Eastern woman She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal ancient wisdom from an age of nights. White man As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens under the sun's glare.</sample>
    <sample id="716">The image contains a table with three columns and one row. The column headers are "Asian woman," "Middle-Eastern woman," and "White man." Each cell under these headers provides detailed descriptions of the personas mentioned in the title, which is "Step 1: Persona Examples (GPT-4)." Here's the text extracted from each cell:

1. **Asian woman**:
   - Description: "The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention."

2. **Middle-Eastern woman**:
   - Description: "She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal ancient wisdom of the Arabian nights."

3. **White man**:
   - Description: "As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, sometimes reddened by the sun..."

Each description offers insights into the physical characteristics, perceived qualities, and implied cultural backgrounds associated with each persona type as generated by GPT-4.</sample>
    <sample id="717">2 steps 1. Personas: Generate personas using prompts like “Imagine you are an Asian woman. Describe yourself.”</sample>
    <sample id="718">The video features a presentation slide with the title '2 steps' at the top. The main content of the slide is focused on generating personas using specific prompts, as indicated by the following text: 1. Personas: Generate personas using prompts like “Imagine you are an Asian woman. Describe yourself.” This prompt suggests creating fictional characters based on certain attributes or identities. Additionally, there's a sub-point that reads: Inspired by psych study with human subjects using the same prompts. In the upper right corner of the frame, there is a small inset showing a person who appears to be presenting or participating in the discussion.</sample>
    <sample id="719">The image contains text that outlines a process in two steps. The first step is labeled "1. Personas:" and instructs to generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." It further explains that this approach is inspired by psych studies with human subjects using the same prompts.</sample>
    <sample id="720">The image contains text that outlines two steps for a process. The first step is labeled "Personas" and involves generating personas using prompts like "Imagine you are an Asian woman. Describe yourself." This method is inspired by psych studies with human subjects using the same prompts. The second step is labeled "Marked Words," which focuses on finding words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="721">The benefit of this is that we get really specific stored types and patterns without having to rely on any specific lexicon.</sample>
    <sample id="722">The image contains a slide with the title "Insight for Step 2: Marked Words." The content is divided into two sections. The first section, labeled "Markedness," explains that unmarked groups are default and ordinary, while marked groups differ from the default. Below this explanation, there is an example provided in italics: "a warrior (unmarked) vs. a woman warrior (marked)." In the top right corner of the image, there is a small video feed showing a person wearing a striped shirt.

Here's the markdown format for the text:

```markdown
# Insight for Step 2: Marked Words

## Markedness:
- Unmarked groups are default, ordinary
- Marked groups differ from the default

**a warrior (unmarked) vs. a **woman** warrior (marked)**
```

This structure clearly separates the main points about marked words and provides a visual representation through the example given.</sample>
    <sample id="723">The image contains text that appears to be part of a presentation slide. The main content is as follows:</sample>
    <sample id="724">The image contains text that discusses the concept of markedness in linguistics. The main points are as follows: 1. **Markedness**: - Unmarked groups are default, ordinary. - Marked groups differ from the default. An example is given to illustrate this difference: "a warrior (unmarked) vs. a woman warrior (marked)." 2. **Dominant Groups and Marginalized Groups**: - Dominant groups are linguistically and socially unmarked.
- Marginalized groups are marked. These statements highlight how linguistic features can reflect social power dynamics within a society.</sample>
    <sample id="725">Step 2: Marked Words 1. Define unmarked and marked groups 2. Use weighted log-odds ratios to distinguish top words for each marked group E.g., For Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas</sample>
    <sample id="726">The image contains text related to a step-by-step process for analyzing marked words in groups. Here is the transcription of the visible text, including details about formatting:

**Title:**
Step 2: Marked Words

**Body Text:**
1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

**Example Section:**
E.g., For Black woman personas, find words that distinguish from both unmarked groups:
   i) White personas
   ii) Man personas

This layout suggests an instructional or educational context, focusing on distinguishing characteristics between different groups using specific analytical methods.</sample>
    <sample id="727">The image shows a slide from a presentation with the title "Step 2: Marked Words." The content of the slide is as follows:

1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

Below this, there is an example provided:
E.g., For Black woman personas, find words that distinguish from both unmarked groups:
   i) White personas
   ii) Man personas

In the upper right corner of the image, there is a small video feed showing a person who appears to be presenting or participating in the meeting.

The background color of the slide is light yellow, and the text is primarily black, except for the word "Step" which is highlighted in blue.</sample>
    <sample id="728">The English content in the image is as follows: Title: Results: Comparison to Human Responses Subtitle: Generated personas contain more stereotypes Left side of the graph (Black Stereotypes): - X-axis label: Percentage of Stereotype Words in Personas - Y-axis labels: 0.0%, 0.2%, 0.4%, 0.6%, 0.8%, 1.0% Right side of the graph (White Stereotypes): - X-axis label: Percentage of Stereotype Words in Personas - Y-axis labels: 0.0%, 0.2%, 0.4%, 0.6%, 0.8%, 1.0% Legend: Green bars represent "Human" Blue bars with diagonal stripes represent "GPT-4" Purple bars with diagonal stripes represent "GPT-3.5"</sample>
    <sample id="729">But... this lexicon is incomplete Black Stereotypes in Personas Human GPT-4 P Black GPT-3.5 P Black GPT-4 P White GPT-3.5 P White "basketball" "loud" "attitude" "athletic" "tall" other words Words in Black Stereotype Lexicon % of Personas</sample>
    <sample id="730">The English content in the image is as follows: "But... this lexicon is incomplete" and "Black Stereotypes in Personas". The chart shows words from a Black Stereotype Lexicon, with categories for different personas (Human, GPT-3.5 PBlack, GPT-4 PBlack, GPT-3.5 PWhite, GPT-4 PWhite). Words like "basketball", "loud", "attitude", "athletic", "tall", and others are listed on the x-axis, while the y-axis represents the percentage of personas associated with each word.</sample>
    <sample id="731">But... this lexicon is incomplete Black Stereotypes in Personas Human GPT-3.5 PBlack GPT-4 PBlack GPT-3.5 PWhite GPT-4 PWhite "basketball" "loud" "attitude" "athletic" "tall" other words Words in Black Stereotype Lexicon % of Personas</sample>
    <sample id="732">The image shows a bar chart titled "Black Stereotypes in Personas." The x-axis is labeled with words from the Black Stereotype Lexicon: "basketball," "loud," "attitude," "athletic," and two categories under "other words" (one for each persona type). The y-axis represents the percentage of personas, ranging from 0 to 40%. There are four sets of bars corresponding to different models or methods:

1. Green bars represent data related to humans.
2. Blue bars correspond to GPT-4-PBlack.
3. Red bars relate to GPT-4-PWhite.
4. Orange bars indicate results from GPT-3.5-PBlack.

Each set includes three orange bars representing percentages for basketball, loud, attitude, athletic, tall, and other words. For example, the tallest blue bar corresponds to 'tall' under GPT-4-PBlack, indicating it has the highest percentage among all groups. A small text overlay at the top left reads, "But... this lexicon is incomplete."

In the upper right corner, there's an inset showing part of a person wearing glasses and speaking into a microphone, suggesting that someone might be presenting or discussing the content shown on the slide.</sample>
    <sample id="733">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ➡ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="734">The image contains text that appears to be part of a presentation slide. The title at the top reads "Results: Patterns in Top Words." Below the title, there are two main sections with bullet points under each.

1. **Othering through essentializing narratives:**
   - culture
   - tradition
   - proud
   - exotic for marked groups
   - ➔ Defines those groups only by their identity

2. **Pernicious positive portrayals:**
   - Vibrant, curvaceous for Latina women
   - Petite, delicate, silky for Asian women
   - Strong, resilient for Black women

In addition to the textual content, there is an inset picture on the right side of the slide showing a person wearing headphones and speaking into a microphone. This suggests that the presentation might include live commentary or discussion from this individual.</sample>
    <sample id="735">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ⇒ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="736">The image contains text that appears to be part of a presentation slide. The title at the top reads "Results: Patterns in Top Words." Below the title, there are two main sections with bullet points.

The first section is titled "Othering through essentializing narratives:" and includes the following bullet points:
- culture
- tradition
- proud
- exotic for marked groups

There is an arrow pointing right from this list to the statement: "Defines those groups only by their identity."

The second section is titled "Pernicious positive portrayals:" and lists three sub-points:
1. Vibrant, curvaceous for Latina women
2. Petite, delicate, silky for Asian women
3. Strong, resilient for Black women

In the upper right corner of the image, there is a small video feed showing a person who seems to be presenting or participating in the meeting.</sample>
    <sample id="737">The English content in the image is as follows: Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ➞ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="738">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ⇒ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="739">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ➡ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="740">The English content in the image is as follows: Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ➔ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="741">The image contains a slide from a presentation with the title "Results: Patterns in Top Words." The content is divided into two main sections. 

1. **Othering through essentializing narratives:** This section lists several words and phrases that are used to define groups based on their identity, such as:
   - culture
   - tradition
   - proud
   - exotic

2. **Pernicious positive portrayals:** This section describes how certain demographics are portrayed positively but negatively impacts them, including:
   - Vibrant, curvaceous for Latina women
   - Petite, delicate, silky for Asian women
   - Strong, resilient for Black women

Additionally, there is an arrow pointing rightward next to the phrase "othering" which seems to emphasize or highlight this concept.

In summary, the slide discusses how specific terms can be both defining and harmful by reinforcing stereotypes about different demographic groups.</sample>
    <sample id="742">Results: Patterns in Top Words Othering through essentializing narratives: culture, tradition, proud, exotic for marked groups Defines those groups only by their identity Pernicious positive portrayals: Vibrant, curvaceous for Latina women Petite, delicate, silky for Asian women Strong, resilient for Black women</sample>
    <sample id="743">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ➞ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="744">Based on these patterns, we conclude with three recommendations for model owners.</sample>
    <sample id="745">The image shows a presentation slide with the title 'Recommendations' at the top. The content of the slide includes three bullet points: 1. Addressing positive stereotypes and essentializing narratives 2. An intersectional lens 3. Transparency about bias mitigation In the upper right corner, there is a small video feed showing a person who appears to be giving the presentation.</sample>
    <sample id="746">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="747">The content of the image is as follows: Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="748">The image displays a slide with the heading 'Recommendations' in bold. Below this, there are three bullet points: 1. Addressing positive stereotypes and essentializing narratives 2. An intersectional lens 3. Transparency about bias mitigation The text is presented on a light beige background. In the top right corner of the image, there is a small inset showing a person's face.</sample>
    <sample id="749">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="750">The English content in the image is as follows: Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="751">There are three authors involved in the paper.</sample>
    <sample id="752">Iterative transfer learning is a method where the model retains and updates its parameters iteratively. It involves training on new data while retaining some of the old data, allowing for continuous improvement based on accumulated knowledge from previous rounds. This approach helps in adapting to new information efficiently by leveraging existing learned features.</sample>
    <sample id="753">The goal of the dataset is to understand users' language when they want to make a choice.</sample>
    <sample id="754">The legend of the figures means the number of triggers in each sentence.</sample>
    <sample id="755">There are three authors involved in the paper.</sample>
    <sample id="756">To determine the number of annotators used to create the initial dataset, we need to analyze the provided image and extract relevant information. The image shows a bar graph with different tasks labeled as "init dataset," "Debate," and "CE." Each task has an associated area under the ROC curve (AUC) value.

1. **Identify the Initial Dataset:**
   - The label "init dataset" is present on the left side of the bar graph.
   - This indicates that the initial dataset was created using this particular method or approach.

2. **Determine the Number of Annotators:**
   - Since the question asks about the number of annotators for the initial dataset, we focus on the "init dataset."
   - Typically, in such datasets, the term "annotators" refers to individuals who manually labeled the data before any machine learning model was applied.

3. **Conclusion:**
   - Based on standard practices in creating annotated datasets, it can be inferred that multiple annotators were likely involved to ensure consistency and accuracy.
   - However, without explicit numerical values given directly related to the number of annotators, we cannot provide an exact count from the image alone.

Therefore, while I cannot specify the exact number of annotators just by looking at the image, it is reasonable to infer that more than one annotator would have been required to produce the initial dataset shown in the graph.</sample>
    <sample id="757">The affiliations of the authors are: 1. Sebastian Senti - University of Washington 2. Jenny T. Liang - Carnegie Mellon University 3. Ronan Le Bras - Allen Institute for AI and University of Washington 4. Katharina Reinecke - University of Washington 5. Maarten Sap - Carnegie Mellon University</sample>
    <sample id="758">The governor is on the left.</sample>
    <sample id="759">The state-of-the-art models in dialogue systems include GPT-3, BERT, and other advanced natural language processing (NLP) techniques. These models are designed to enhance the coherence, consistency, knowledge, and emotional understanding of conversations by leveraging large-scale training data and sophisticated algorithms.</sample>
    <sample id="760">To ensure the models' performance is consistent and reliable over time.</sample>
    <sample id="761">Yes, training in multilingual fashion caused performance drop compared to monolingual English model.</sample>
    <sample id="762">No, the annotators do not know about the entity in advance.</sample>
    <sample id="763">BLEU, METEOR, and ROUGE.</sample>
    <sample id="764">No, the regress in generalization does not impact specific NER types.</sample>
    <sample id="765">The video shows a person in the top right corner of each frame, with text on the left side that reads 'Imagine...' and a logo at the bottom. The content then shifts to show two individuals: Carl Jones from New York Times and Aditya Sharma from Times of India. They are discussing how Perspective API scores differ for offensive terms based on their cultural contexts.</sample>
    <sample id="766">The multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="767">The model used for transfer learning is 'RoBERTA-base + classifier head'.</sample>
    <sample id="768">The recent test sets used to assess the PaLM capabilities are:</sample>
    <sample id="769">The authors proposed three recommendations.</sample>
    <sample id="770">The gain of the proposed method over the strongest baseline is 20.7%.</sample>
    <sample id="771">The speaker's name is Shuheng Liu.</sample>
    <sample id="772">The results and dataset in the paper can be used as a benchmark.</sample>
    <sample id="773">They experiment with 5 smaller models.</sample>
    <sample id="774">OFA is used as the base model for investigating multi-model instruction tuning.</sample>
    <sample id="775">To determine the total number of logos displayed in the image, let's analyze each logo one by one:

1. The first logo is located on the left side.
2. The second logo is positioned next to it, also on the left side.
3. The third logo appears below these two logos.
4. The fourth logo is situated beside the third logo at the bottom row.

By counting all four distinct logos present in the image, we can conclude that there are 4 logos in total. Therefore, the answer is 4.</sample>
    <sample id="776">The video presents a slide from a presentation titled "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark." The title is prominently displayed at the top in black text. Below the title, there are several names listed: Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie. These individuals have affiliations with various institutions such as University of Science and Technology of China, Microsoft Research Asia, Beijing Jiaotong University, Sony AI, and Microsoft STC Asia.

At the bottom left corner of the slide, there are two logos side by side; one appears to be an emblem or seal, while the other resembles the Windows logo associated with Microsoft. On the right side, below these logos, another set of three logos is aligned horizontally. From left to right, they include what seems to be a circular blue and white design, possibly representing an academic institution, followed by the well-known Microsoft logo, which consists of four colored squares forming a larger square pattern. Next to this is the 'Sony AI' logo, characterized by its distinctive red color scheme.

In the lower-right section of the frame, partially visible against a light background, is the phrase "Sony AI" written in bold letters, reinforcing the association with Sony's artificial intelligence division.

Throughout the sequence, no changes occur on the slide itself, maintaining consistency in displaying the same information about the research topic and contributors. Additionally, there is a small portion of a person’s face visible in the lower right-hand corner throughout the frames, suggesting that someone might be presenting or discussing the content shown on the screen.</sample>
    <sample id="777">The speaker says: "Let's first introduce the background about embedding as services."</sample>
    <sample id="778">The speaker is discussing the exceptional capabilities of large language models (LLMs) in natural language understanding and generation tasks. They mention specific examples such as GPT-1, LLAMA, PALM, and others like Ada, which are used for various NLP tasks through embedding services offered by companies like OpenAI. The slide also highlights that these models provide a cost-effective replacement for older embedding methods.</sample>
    <sample id="779">The text in the image is as follows: Background Large language models (LLMs) are exceptional in NLU and NLG GPT [1], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-based embedding API^1 MODEL USAGE Ada $0.0004 / 1k tokens This Ada model, text=embedding=ada-002, is a better and lower cost replacement for our older embedding models. Show old pricing [Brown et al., 2020] [Trower et al., 2022] [LeiMa et al., 2022] [Chowdhury et al., 2022] [PaLM: Scaling Language Models with Pathways, arXiv preprint, 2022] https://api.openai.com/v1/embeddings</sample>
    <sample id="780">The speaker is discussing the exceptional capabilities of large language models (LLMs) in Natural Language Understanding (NLU) and Natural Language Generation (NLG). They mention that LLMs like GPT-1, LLAMA 2, and PALM 3 are particularly effective in these areas. Additionally, they talk about Embedding as a Service (EaaS), which assists with various NLP tasks by providing pre-trained embeddings for different languages. The speaker also highlights OpenAI's offering of a GPT-based embedding API.</sample>
    <sample id="781">Motivation Attackers may steal the model through learning from the embeddings and provide similar services. StolenEncoder [1] Need to protect the copyright of EaaS Detect whether a provider’s service is stolen by another service.</sample>
    <sample id="782">CHALLENGE Applicable to EaaS Utility • Should not degrade the utility of the provided embeddings. Covertness • Should be covert to the attacker. Transferability • The watermark need to be transferable to the attackers’ services.</sample>
    <sample id="783">The content in the image is as follows: 1. Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attacker's services.</sample>
    <sample id="784">The speaker is discussing the challenges related to embedding watermarks in a system. The key points mentioned are: 1. Applicable to EaaS (Embedded Authentication and Authorization Services). 2. Utility - It should not degrade the utility of the provided embeddings, meaning that any added watermark or security feature should not negatively impact the functionality of the embedded data. 3. Covertness - This refers to how covertly the attacker can detect the watermark without compromising its integrity. Specifically, it mentions that the watermark needs to be covert enough so that an attacker cannot easily remove it. 4. Transferability - Emphasizes the need for the watermark to transfer seamlessly between different services used by attackers. These points highlight the importance of maintaining both functional integrity and security when embedding watermarks within systems.</sample>
    <sample id="785">The content of the image is as follows: 1. Challenge Applicable to EaaS Utility - Should not degrade the utility of the provided embeddings. Covertness - Should be covert to the attacker. Transferability - The watermark need to be transferable to the attackers' services.</sample>
    <sample id="786">Existing Works can be broadly classified into four categories: 1. Parameter watermark [1, 2] - Transferability (X) 2. Lexical watermark [3, 4] - Applicable to EaaS 3. Backdoor-based watermark [5] - Applicable to EaaS 4. Adversarial-property based watermark [6] - Applicable to EaaS These classifications are derived from the references provided in the image and categorize different types of watermarks used in existing works related to deep neural networks with respect to their applicability to Edge-to-Edge Asynchronous Secure Systems (EaaS).</sample>
    <sample id="787">Existing Works • Parameter watermark [1, 2] Transferability ❌ • Lexical watermark [3, 4] Applicable to EaaS • Backdoor-based watermark [5] Applicable to EaaS • Adversarial-property watermark [6] Applicable to EaaS [1] Li et al. Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach. trust security and privacy in computing and communications 2020. [2] Lim et al. Protect, show, tell and attend: Empowering image captioning models with ownership protection. Pattern Recognition 2022. [3] He et al. Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022. [4] Turner et al. Turning your weakness into a strength: Watermarking deep neural networks by USENIX Security 2018. [5] Merrer et al. Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications 2022.</sample>
    <sample id="788">The content of the video is a slide titled "Existing Works" that lists various types of watermarks and their applicability to Edge as a Service (EaaS). The list includes: 1. Parameter watermark [1, 2] - Transferability marked with an 'X' indicating it's not applicable. 2. Lexical watermark [3, 4] - Applicable to EaaS. 3. Backdoor-based watermark [5] - Applicable to EaaS. 4. Adversarial-watermark [6] - Applicable to EaaS. At the bottom of the slide, there are references to six academic papers related to deep neural networks and watermarking techniques.</sample>
    <sample id="789">The text in the image is as follows: EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval trigger set provider's model original embedding target embedding normalize provided embedding watermark injection Stealer dE Ee</sample>
    <sample id="790">EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval Trigger set trigger number provider's model original embedding target embedding normalize provided embedding normalized embedding watermark injection Stealer</sample>
    <sample id="791">EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval</sample>
    <sample id="792">EmbMarker Watermark injection Define a target embedding e_t Count the trigger number in a sentence Add the target embedding on the original embedding e_0</sample>
    <sample id="793">The provided embedding is a weighted summation of the target embedding and the original embedding.</sample>
    <sample id="794">EmbMarker</sample>
    <sample id="795">EmbMarker Copyright verification is to detect whether a model behind another service contains the watermark.</sample>
    <sample id="796">EmbMarker Copyright verification Construct a backdoor and benign dataset D_b = { [w_1, w_2, ..., w_m] | w_i ∈ T }, D_n = { [w_1, w_2, ..., w_m] | w_i ∉ T }. Request embeddings from stealer's service with the datasets. Trigger set T D_b + D_n benign and badgent dataset target embedding verify extracted? E_c corpus embeddings E_b + E_n E_c E_b E_c target embedding provider stealer</sample>
    <sample id="797">EmbMarker Copyright verification Construct a backdoor and benign dataset \( D_b = \{ [w_1, w_2, ..., w_m], w_i \in T \} \) \( D_n = \{ [w_1, w_2, ..., w_m], w_i \notin T \} \) Request embeddings from the stealer's service with the datasets Trigger set \( T \) \( D_b + D_n \) Set of target embeddings \( E_c \) corpus embeddings Verify extracted? Target embedding \( E_t \) Stealer</sample>
    <sample id="798">EmbMarker</sample>
    <sample id="799">EmbMarker Copyright verification Compute their similarity to the target embedding cos i = e i · e t || e i || || e t || 2 C b = {cos i | i D b}, C n = {cos i | i D n}, L b = {li | i D b}, L n = {li | i D n}. Computing metrics (similarity difference and p-value of KS test) \Delta\omega_{20s}=\frac{1}{|C_{b}|}\sum_{i}\frac{1}{|C_{b}|}\sum_{j}\frac{1}{|C_{n}|}||e_{i}-e_{t}||^{2} \Delta r_{2}=\frac{1}{|L_{b}|}\sum_{i}\frac{1}{|L_{b}|}\sum_{j}\frac{1}{|L_{n}|}||l_{i}-l_{j}||^{2}</sample>
    <sample id="800">Experimental Results Copy Dataset: AG News, MIND, SST2, Enron Spam Provider's general Dataset: WikiText Metrics Performance on downstream tasks: ACC Detection performance: ΔCOS, ΔL2, p-value Setting m = 20, n = 4, frequency interval = [0.005, 0.01] Dataset | #Sample | #Classes | Avg. len. SST2 | 68,221 | 2 | 54.17 MIND | 130,383 | 2 | 66.14 Enron Spam | 33,716 | 2 | 34.57 AG News | 127,600 | 4 | 236.41</sample>
    <sample id="801">The results on four datasets show that our embedding marker can have great detection performance while keep good utility for downstream tasks.</sample>
    <sample id="802">Experimental Results Embedding visualization (a) AG News (b) Enron Spam (c) MIND (d) SST2</sample>
    <sample id="803">Experimental Results Embedding visualization (a) AG News (b) Enron Spam (c) MIND (d) SST2</sample>
    <sample id="804">Thanks</sample>
    <sample id="805">The image is a screenshot of a presentation slide. The main content on the slide reads 'Attention as a Guide for Simultaneous Speech Translation' and lists three names: Sara Papi, Matteo Negri, Marco Turchi. Below this text are logos from the University of Trento, Fondazione Bruno Kessler, and another logo that appears to be related to simultaneous speech translation or language technology. In the top right corner of the image, there's a small video window showing a person who seems to be presenting the slide.</sample>
    <sample id="806">The image shows a slide from a presentation about Simultaneous Speech Translation (SimulST). The title of the slide is "What is Simultaneous Speech Translation?" and it includes both English text and some German words. There is an audio waveform graphic on the left side, indicating speech or translation in progress. On the right side, there is a blank box where translated text appears as part of the demonstration. At the bottom of the slide, there is a definition that reads: 'Simultaneous speech translation (SimulST) is the process of translating spoken language into a text in another language in real-time, enabling cross-language communication.' Additionally, there is an image credit at the bottom which states: 'Image Credits to https://ai.googleblog.com/2021/07/stabilizing-live-speech-translation-in.html' with the page number 03 indicated.</sample>
    <sample id="807">The video features a presentation slide with the title 'What are the problems of the current SimulST models?' displayed prominently at the top in blue text. Below this, there is an icon resembling a virus or bacteria next to the statement 'Specific architectures are usually trained, introducing additional modules to be optimized.' The bottom right corner shows 'page 04,' indicating that this is part of a larger presentation. In the upper right corner, a person appears to be speaking, likely providing commentary on the content being presented.</sample>
    <sample id="808">The video presents a slide discussing the problems of current SimulST models. The title, "What are the problems of the current SimulST models?" is displayed at the top in blue text with an underline. Below this, there are two main points highlighted: 1. Specific architectures are usually trained, introducing additional modules to be optimized. This point is accompanied by an icon resembling a neural network or brain on the left side. 2. Long and complicated training procedures (e.g., different optimization objectives). This point includes another icon that appears to represent a complex system or process. At the bottom right corner of the slide, there is a page number indicating it's page 06. In the upper right corner of each frame, there is a small inset image showing a person who seems to be presenting or explaining the content.</sample>
    <sample id="809">The video presents a slide from a presentation, focusing on the problems of current SimuIST models. The title at the top reads "What are the problems of the current SimuIST models?" Below this title, there are three main points listed: 1. Specific architectures are usually trained, introducing additional modules to be optimized (illustrated with an icon resembling a brain). 2. Long and complicated training procedures (e.g., different optimization objectives) (illustrated with an icon that looks like a brain or neural network). 3. Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s,...) (illustrated with two icons depicting molecular structures). In the bottom right corner, there is a small image of a person in a room with curtains behind them, likely indicating they are part of the presentation. Additionally, navigation buttons for previous and next slides are visible along with other interactive elements such as comments, questions, favorites, and sharing options. At the bottom left, there's a logo consisting of blue shapes forming what appears to be a stylized 'M' followed by 'K'. This sequence repeats across multiple frames, emphasizing the detailed issues associated with the current SimuIST models without any changes in content between these segments.</sample>
    <sample id="810">The English content in the image is: "What is our solution?" This appears to be a question posed on what seems like a presentation slide or similar visual medium. The text is written clearly and stands out against the background, making it easy to read.</sample>
    <sample id="811">The image shows a presentation slide with the title 'What is our solution?' at the top. The main content of the slide includes two numbered points: 1. Use already existing offline ST models without re-training or adopting specific architecture for SimuIST. 2. Use only one model for every latency regime and handle latency through specific parameters. There are also some icons on the right side of the text, possibly representing different features or steps in the process. At the bottom left corner, there's a logo that appears to be related to the organization or project being discussed. In the background, there's a small video feed showing a person speaking, likely presenting this information live.</sample>
    <sample id="812">The content of the image is as follows: 1. The title 'What is our solution?' appears in blue text on a white background, accompanied by three numbered points below it. 2. Point 01 states: 'Use already existing offline ST models without re-training or adopting specific architecture for SimuIST.' 3. Point 02 reads: 'Use only one model for every latency regime and handle latency through specific parameters.' 4. Point 03 says: 'Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output that is the cross-attention mechanism.' To the right side of these texts, there's an illustration showing sound waves above some text with words like 'I am a student,' indicating how audio inputs are processed to generate corresponding outputs.</sample>
    <sample id="813">The solution is proposed as EDAtt, or Encoder-Decoder Attention. This strategy helps decide whether to emit a partial translation based on where attention points to in the last λ speech frames.</sample>
    <sample id="814">The image is a screenshot of a presentation slide. The title at the top left reads 'Our solution: EDAtt'. Below this, there is a large text box with the heading 'Encoder-Decoder Attention' and some explanatory text in smaller font size that says, 'Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold α) towards the last λ speech frames, meaning that the received information is enough stable.' In the bottom right corner of the slide, there is a page number indicating 'page 014'. Additionally, there is an inset video window showing a person speaking, likely providing commentary or explanation related to the content of the slide.</sample>
    <sample id="815">The solution proposed is EDAtt. This system decides whether to emit or not a partial translation based on where attention points: if the sum of the attention values for the last λ speech frames is below a threshold, it means that the received information is stable enough and no further emission is necessary.</sample>
    <sample id="816">The image is a screenshot of a presentation slide. The main content area has a dark blue background with white text that reads 'Encoder-Decoder Attention'. Below this, there is an audio waveform and the German phrase 'Ich werde reden.' In the top left corner, it says 'Our solution: EDAtt' in light blue text. On the right side, there is additional information about the attention mechanism for encoder-decoder models, explaining how to decide whether or not a partial translation should be emitted based on where attention points to. At the bottom right corner, there is a page number indicating 'page 016'.</sample>
    <sample id="817">The image shows a presentation slide with the title "Our solution: EDAtt" at the top left corner. The main content of the slide is divided into two sections:

1. **Encoder-Decoder Attention**:
   - This section explains that attention points to where an error is emitted if the attention is not concentrated (its sum is below a threshold c) towards the last λ speech frames, meaning that the received information is enough stable.

2. **Illustration and Text**:
   - Below the main text, there is an illustration showing three colored lines representing audio signals.
   - Underneath the illustration, there is a sentence in German: "Ich werde reden."
   - Next to this sentence, there is English text: "I am going to talk about..."

In the bottom right corner of the slide, there is a small logo or icon. Additionally, on the right side of the slide, there is a video call interface displaying a person's face. 

The page number "017" is visible in the bottom right corner of the slide.</sample>
    <sample id="818">The first two words will be emitted.</sample>
    <sample id="819">The video features a presentation slide titled 'Encoder-Decoder Attention,' which discusses the concept of deciding whether to emit or not a partial translation based on where attention points. The text explains that if a word is emitted, it means the received information is enough stable and concentrated.

In the lower part of the slide, there's an example with German words "Ich werde reden" (I will speak) highlighted in green, accompanied by their English translations "EMITTED." Below this, there's a visual representation showing audio waveforms and some mathematical expressions related to attention mechanisms.

On the right side of the slide, additional explanatory text reads: 'Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold α) towards the last speech frames, meaning that the received information is enough stable.'

Throughout the sequence, the page number changes from 021 to 022 at the bottom right corner of each frame, indicating progression through different slides in the presentation.</sample>
    <sample id="820">The image shows a slide from a presentation with the title 'Our solution: EDAtt' at the top. The main content of the slide is divided into two sections labeled 01 and 02, each containing audio waveforms and text in both English and German.

In section 01:
- There is an audio waveform.
- Below the waveform, there is text that reads 'I am going to talk about...' followed by 'Ich werde reden.' (which translates to 'I will speak' or 'I will be talking').

In section 02:
- Another audio waveform is shown.
- Below this waveform, the text reads 'I am going to talk about climate.' which corresponds to 'Ich werde über Klima sprechen.' 

On the right side of the slide, there is additional explanatory text explaining how attention works within the encoder-decoder model:

'Decide whether to emit or not a partial translation based on where attention points to: A word is emitted if the attention is not concentrated (its sum is below a threshold t) towards the last λ speech frames, meaning that the received information is enough stable.'

At the bottom left corner of the slide, there are three icons representing different aspects related to language processing or machine learning. At the bottom right corner, it indicates that this is page 0243 of the document.

Additionally, there is a small video window showing someone speaking in the upper right corner of the screen.</sample>
    <sample id="821">The image shows a presentation slide titled "Encoder-Decoder Attention" with the subtitle "Our solution: EDAtt." The slide explains that attention should be emitted if not partial translation, based on whether points to words. It mentions that a word is emitted if the attention is not concentrated (its threshold is below a certain value) towards the last speech frames, indicating stable received information.

There are two diagrams labeled 01 and 02:
- Diagram 01 has text in German ("Ich werde reden") and an English phrase ("I am going to talk about...").
- Diagram 02 also has text in German ("Ich werde über Klima sprechen") and an English phrase ("I am going to talk about climate").

Both diagrams show waveforms representing audio signals, with green lines pointing from the German phrases to corresponding parts of the waveform. Red dashed boxes highlight specific sections of the waveform for each diagram.

At the bottom left corner, there's a logo with the letters "F" and "X," and at the bottom right corner, it says "page 025."

In the top right corner, there's a small video feed showing a person speaking.</sample>
    <sample id="822">The text in the image is "EMMITTED".</sample>
    <sample id="823">The main results of that are shown in the graph, which plots BLEU scores against AL/AL_CA (s). The y-axis represents the BLEU score ranging from 21 to 27, while the x-axis shows the ratio of AL to AL_CA on a scale from 0.5 to 6.</sample>
    <sample id="824">The graph in the image is labeled with 'Main Results: EDAtt' at the top left corner. The x-axis of the graph has a label that reads 'AL/AL_CA (s)' and ranges from 0.5 to 6, while the y-axis is labeled 'BLEU'. At the bottom center of the graph, there's an additional label '(a) en→de', indicating that this particular graph represents results for English to German translation.

In the upper right corner of the frame, there are several emojis displayed horizontally, including a question mark, a smiley face, a thumbs-up hand gesture, a peace sign, and another question mark.

At the bottom left corner of the slide, there is a logo consisting of three blue dots forming a triangle shape next to some text. In the lower right corner, it says 'page 028'.

Additionally, on the right side of the frame, there is a small video feed showing a person who appears to be giving a presentation or lecture.</sample>
    <sample id="825">The image shows a presentation slide with the title 'Main Results: EDAtt'. The main content of the slide is a graph plotting BLEU scores against AL/AL_CA (s) on the x-axis, labeled as '(a) en→de' and 'latency measure' respectively. There are some blue question marks above the graph indicating areas of inquiry or discussion points. In the bottom right corner, there is text that reads 'page 030', suggesting this is part of a larger document or presentation. Additionally, in the top right corner, there is an inset video window showing a person speaking, likely presenting the results shown on the slide.</sample>
    <sample id="826">The image shows a presentation slide with the title "Main Results: EDAtt" at the top. The main content of the slide is a bar graph labeled "(a) en→de," which appears to be measuring BLEU scores against AL/AL_CA (s). The x-axis ranges from 0.5 to 6, and the y-axis ranges from approximately 21 to 27. There is a single blue bar on the graph that reaches up to around 27 on the y-axis.

In the upper right corner of the slide, there are some symbols or characters in light blue color, possibly indicating different points or questions related to the data presented. In the bottom left corner, there is a logo consisting of three curved lines forming an abstract shape, next to which it says "page 031."

Additionally, there is a small video feed window showing a person who seems to be giving a presentation or lecture. This suggests that the image might have been captured during a live presentation or webinar session.</sample>
    <sample id="827">The written content in the image includes: - Main Results: EDAtt - (a) en→de - BLEU 17 25 21 19 - page 032</sample>
    <sample id="828">The video begins with a presentation slide titled 'Main Results: EDAtt' and includes the subtitle 'popular strategies also applied to offline models.' The graph on the slide plots BLEU scores against AL/AL_CA (s), showing four different strategies: wait-k, LA, CAAT, and EDAtt. A small inset image of a person is visible in the top right corner throughout this segment.

The presenter discusses comparing various strategies that are applicable to both online and offline models. They mention specific strategies such as the weight-key strategy and local agreement. Additionally, they compare these strategies with state-of-the-art architectures tailored for simultaneous translation.

The discussion continues with further elaboration on the comparison between popular strategies like wait-k, LA, CAAT, and EDAtt, emphasizing their application to both online and offline models. The consistent presence of the inset image suggests an ongoing explanation or demonstration related to the content displayed on the slides.

The focus remains on explaining the results and comparisons presented in the graphs, particularly highlighting how EDAtt performs relative to other strategies. The detailed analysis underscores the effectiveness and applicability of EDAtt within the context of machine translation tasks.</sample>
    <sample id="829">The main results are presented for the EDAtt model. The graph shows performance metrics on the task of simultaneous speech translation from English to German, with different models compared: wait-k, LA, CAAT, and EDAtt.</sample>
    <sample id="830">The chart shows the performance of different strategies on a task, with BLEU scores plotted against AL/AL_CA (alpha) values. The main result highlighted is that EDAtt outperforms all other offline models. This conclusion is drawn from observing how the curves for various strategies are shifted to the left in the graph, indicating better performance at lower AL/AL_CA values.</sample>
    <sample id="831">The main results show that EDAtt is the fastest strategy if we consider the actual elapsed time.</sample>
    <sample id="832">The image appears to be a screenshot of a presentation slide. The main content is text, with the following key elements: 1. A heading that reads "Do you want to discover more?" followed by smaller text asking viewers to read their paper for additional results. 2. Contact information provided includes an email address (marco.turchi@gmail.com), GitHub link (github.com/hlt-mt/fbdk-fairseq), and Twitter handle (@fbk_mt). 3. There's also a QR code labeled "Scan me!" which likely links to further resources or details about the work presented in the paper. 4. In the top right corner, there is a small video feed showing a person speaking, suggesting this might be from a virtual meeting or webinar. 5. At the bottom left, there are some icons, possibly representing different social media platforms or tools related to the presentation. This setup indicates it’s part of a professional presentation aimed at sharing research findings and encouraging engagement through various digital channels.</sample>
    <sample id="833">The authors of the paper are affiliated with Google.</sample>
    <sample id="834">The affiliations of the authors are Stony Brook University.</sample>
    <sample id="835">The language pairs analyzed in the paper were English-German, French-Italian, and Spanish-Portuguese.</sample>
    <sample id="836">Shangbin Feng</sample>
    <sample id="837">The models investigated during the experiments are "Long m-BART" and "Normal Base m-BART".</sample>
    <sample id="838">The 53 tasks from the nine groups are used for training, and the additional five tasks selected from VQA and Miscellaneous groups are also included in this count.</sample>
    <sample id="839">There are three authors involved in the paper.</sample>
    <sample id="840">The authors experiment on four datasets: AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="841">The English content in the image is: Language model acceptability judgements are not always robust to context ACL 2023 Koustatu Sinha, Jon Gauthier, Aaron Mueller, Kanishka Mira, Keren Fuentes, Roger Levy, Adiwas Williams</sample>
    <sample id="842">The English content in the image is: "Language model acceptability judgements are not always robust to context" and "ACL 2023".</sample>
    <sample id="843">The English content in the image is as follows: "Revisiting Minimal Pair Paradigm" and below that, it says "Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs:" The table contains three columns labeled BLiMP, SyntaxGym, CrowS with corresponding text under each column. At the bottom left corner of the slide, there's a mathematical expression P(1) &gt; P(2). On the right side of the slide, there's an image of a person wearing glasses.</sample>
    <sample id="844">The image contains a slide titled "Revisiting Minimal Pair Paradigm." The text explains that the Minimal Pair Paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate abstract knowledge of Language Models. There are three columns labeled BLiMP, SyntaxGym, and Crows, each containing sentences: 1. Many people were helping themselves. No customer ... has spent any money. Stereotypical N-gram sentence. P(1) ≈ P(2) 2. Many people were helping herself. No customer ... has spent any money. Stereotypical N-gram sentence. P(1) ≈ P(2)</sample>
    <sample id="845">The English content in the image is: Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLiMP SyntaxGym CrowS 1. Many people were helping themselves. No customer ... has spent any money. Stereotypical Syntactic sentence. 2. Many people were helping herself. ... has spent any money. Stereotypical Syntactic sentence. P(1) &gt; P(2) P(1) &gt; P(2.any) P(1) &gt; P(2)</sample>
    <sample id="846">The image contains a slide titled "Revisiting Minimal Pair Paradigm." The content of the slide is as follows: - At the top, there is a brief description that reads: "Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs:" - Below this description, there are three columns labeled "BLIMP," "SyntaxGym," and "Crows." Each column lists examples of sentences. For example, under BLIMP, it shows two sentences: 1. Many people were helping themselves. 2. No customer ... has spent any money. Under SyntaxGym, only one sentence is listed: 1. Many people have been spending money. And for Crows, also only one sentence is listed: 1. Stereotypical narrative sentence. - At the bottom of each column, there is an expression involving probability notation: P(1) &gt; P(2), indicating some form of comparison or evaluation based on these sequences. Additionally, there is a circular photo of a person with dark hair wearing glasses positioned at the right side of the slide.</sample>
    <sample id="847">The current MPP pipeline basically doesn't allow us to evaluate models acceptance towards longer sentences.</sample>
    <sample id="848">The English content in the image is as follows: Title: Revisiting Minimal Pair Paradigm Subtitle: Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to estimate the abstract knowledge of LMs: Table Columns: BLiMP, SyntaxGym, CrowS Rows under each column: - Many people were helping. - No customer ... has spent any money. - Women are terrible at handiwork. - Relatives. - ... has spent any money. - Men are terrible at handiwork. - herself. - P(1) ≫ P(2). - P(1.any) ≫ P(2.any). - P(1) ≫ P(2). Question below table: Are these judgements stable with long preceding context?</sample>
    <sample id="849">The English content in the image is as follows: Title: Revisiting Minimal Pair Paradigm Subtitle: Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to abstract the evaluate the abstract knowledge of LMs: Columns: BLiMP SyntaxGym CrowS Rows: 1. Many people were helping themselves. No customer ... has spent any money. Women are terrible at handiwork. 2. Many people were helping themselves. ... has spent any money. Men are terrible at handiwork. P(1) &gt;= P(2) P(1.any) &gt;= P(2.any) Are these judgements stable with long preceding context?</sample>
    <sample id="850">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="851">The English content in the image is as follows: 'Approach Test whether MPP judgements as a function of context length, structural match, and acceptability.'</sample>
    <sample id="852">The image shows a slide titled 'Approach' with the following text: 'Test whether MPP judgements as a function of context length, structural match and acceptability.' Below this title is an equation involving \( P_{LM} \) and \( P_{LM}(\text{Prefix}) \). There's also a diagram labeled 'Space of Candidate Prefixes,' which includes terms like 'Context Length Agreement' and 'Structural Match.' On the right side, there are two examples marked 'Acceptable, Matched': 'What did Aaron sound like when he said those spotlights?' and 'What could Jessica will notice before noticing that clearing the museum?'. At the bottom left corner, it reads 'GPT2, OPT family - 125M to 6.7B'.</sample>
    <sample id="853">The English content in the image is as follows: "Approach Test whether MPP judgements as a function of context length, structural match and acceptability. GPT2, OPT family - 125M to 6.7B."</sample>
    <sample id="854">The text in the image is as follows: "Approach Test whether MPP judgements as a function of context length, structural match, and acceptability. GPT2, OPT family - 125M to 6.7B"</sample>
    <sample id="855">The English content in the image is as follows: "Approach Test whether MPP judgements as a function of context length, structural match, and acceptability. GPT2, GPT family - 125M to 6.7B."</sample>
    <sample id="856">The text in the image is as follows: "Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability. GPT2, OPT family - 125M to 6.7B."</sample>
    <sample id="857">The English content in the image is as follows: "Approach Test whether MPP judgements as a function of context length, structural match, and acceptability." This text appears to be explaining an approach or methodology related to some form of judgment test involving context length, structural matches, and acceptability. The specific details about what MPP stands for are not provided within this visible portion of the image.</sample>
    <sample id="858">The text in the image is structured as follows: 1. Title: "Approach" - This indicates that the slide presents a method or strategy being discussed. 2. Main Body Text: The main content of the slide reads, "Test whether MPP judgements as a function of context length, structural match, and acceptability." This suggests an evaluation framework for Multi-Process Planning (MPP) judgments based on certain criteria. 3. Diagram Labels: There are several labels within the diagram such as "P(LM | Prefix) &gt; P(LM)", "acceptable", "unacceptable", "Wikipedia, Unrelated", "Mechanized", "Unmechanized", "Sample", "Space of Candidate Prefixes". These likely represent different components or aspects related to the approach under discussion. 4. Additional Information at Bottom: At the bottom left corner, it states "GPT2, OPT family - 125M to 6.7B," which could refer to models used in the study or analysis mentioned above.</sample>
    <sample id="859">The English content in the image is as follows: "Approach Test whether MPP judgements as a function of context length, structural match, and acceptability." This text appears to be discussing an approach related to evaluating judgments based on various factors such as context length, structural matches, and acceptability. The term "MPP" likely refers to some specific methodology or framework being applied within this context.</sample>
    <sample id="860">The text in the image is: "MPP judgements are robust for arbitrary context lengths. We perform MPP evaluations with different contexts - acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens."</sample>
    <sample id="861">MPP judgements are robust for arbitrary context lengths. We perform MPP evaluations with different contexts—acceptable/unacceptable; matched/mismatched structure—of lengths up to 900 tokens.</sample>
    <sample id="862">Now what happens when we choose sentences from the same dataset?</sample>
    <sample id="863">The English content in the image is as follows: "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance. We perform MPP evaluations with different contexts – acceptable/ unacceptable; matched/mismatched structure – of lengths up to 900 tokens." This text appears at the top and bottom sections of the slide, providing an overview of the study's focus on evaluating Multi-Paraphrase Passage (MPP) sentences within various contextual settings and structures. The mention of specific token counts suggests a quantitative aspect to the evaluation process.</sample>
    <sample id="864">The text in the image is as follows: "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance. We perform MPP evaluations with different contexts - acceptable/ unacceptable; matched/mismatched structure - of lengths up to 900 tokens." Additionally, there are two numbered examples provided within a speech bubble on the right side of the graph:
1. There was a documentary about music inflating.
2. There were most legislations before this year from the customer?
3. There might Rise free from this before using the customer?</sample>
    <sample id="865">The English content in the image is as follows: "Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance. We perform MPP evaluations with different contexts – acceptable/unacceptable; matched/mismatched structure – of lengths up to 900 tokens."</sample>
    <sample id="866">The English content in the image is as follows: 1. Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance 2. We perform MPP evaluations with different contexts – acceptable/unacceptable; matched/mismatched structure – of lengths up to 900 tokens BLIMP, OPT 6.7B Prefix Strategy Unmatched Unmatched Wiki Unmatched</sample>
    <sample id="867">The image contains a graph with the title "Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance." The subtitle reads, "We perform MPP evaluations with different contexts – acceptable/unacceptable; matched/mismatched structure – of lengths up to 900 tokens."

The graph shows three lines representing different strategies: 
1. Prefix Strategy
2. Unmatched (Unm.)
3. Wiki

Each line is color-coded:
- The blue line represents the Prefix Strategy.
- The red dashed line represents Unmatched (Unm.).
- The green dotted line represents Wiki.

There are also four numbered points on the right side of the graph, each accompanied by text and an example sentence:

1. What could Jessica sell before noticing her spots?
   - What had Jessica said about her spots? Before returning this customer from here?

2. What could Jessica sell before cleaning the museum?
   - What should Jessica do for cleaning the museum? Before returning this customer from here?

3. What could Jessica sell before noticing the museum like the forest around it?
   - What would Jessica notice if she saw the museum like the forest around it? Before returning this customer from here?

4. What could these sentences be used in real life scenarios?
   - These sentences can be used in various real-life scenarios where customers might have questions or need assistance related to products or services offered by Jessica.

At the bottom left corner of the graph, there is a label that says "BLIMP, OPT 6.7B," indicating the model being evaluated.

The overall message conveyed by the image is that mismatched structures significantly impact the performance of models when evaluating multi-prediction patterns (MPP) in natural language processing tasks.</sample>
    <sample id="868">The matched prefix affects the language model's judgement so much because it perturbs context sentences in a way that preserves their relevant structure. This means that even when adding or removing prefixes, the core meaning and grammatical structure of the sentence remain intact. As a result, models become highly sensitive to these changes, which can significantly impact their performance on tasks related to understanding and generating text. The slide provides examples such as "However, &lt;sent&gt;," "First and foremost, &lt;sent&gt;," "Regardless of what X thinks about it, &lt;sent&gt;," and "Yesterday, X said, &lt;sent&gt;." These illustrate how small modifications at the beginning of a sentence ("prefixes") can alter its interpretation while maintaining overall coherence.</sample>
    <sample id="869">The English content in the image is as follows: Title: Why do matched prefixes affect LM judgements? Body Text: We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. - Prefix/suffix adverbs: "However, &lt;sent&gt;." - Long prefix adverbs: "First and foremost, &lt;sent&gt;." - Add clause: "Regardless of what X thinks about it, &lt;sent&gt;." - Quote: "Yesterday, X said, &lt;sent&gt;." Graph Legend: Permutation - None - Prefix/suffix adv - Long prefix adv - Add clause - Clause - All - Prefix Type - Acceptable - Unacceptable</sample>
    <sample id="870">The text in the image is: "Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are sensitive to these sentences. Prefix/suffix adverbs: 'However, &lt;sent&gt;.' Long prefix adverbs: 'First and foremost, &lt;sent&gt;.' Add clause: 'Regardless of what X thinks about it, &lt;sent&gt;.' Quote: 'Yesterday, X said, &lt;sent&gt;.' Perturbation: None Prefix type: Acceptable Unacceptable"</sample>
    <sample id="871">The English content in the image is as follows: "Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the structure, and ask whether models are sensitive to these sentences. - Prefix/suffix adverbs: “However, &lt;sent&gt;.” - Long prefix/verbs: “First and foremost, &lt;sent&gt;.” - Add clause: “Regardless of what X thinks about it, &lt;sent&gt;.” - Quote: “Yesterday, X said, &lt;sent&gt;.” Models are sensitive to perturbed sentences in similar ways."</sample>
    <sample id="872">The text in the image is as follows: Title: "Why do matched prefixes affect LM judgements?" Body Text: "We perturb context sentences in ways that preserve the structure, and ask whether models are similarly sensitive to these sentences." Examples of Perturbed Sentences: - Prefix/suffix adverbs: "However, &lt;sent&gt;." - Long adverbs: "First and foremost, &lt;sent&gt;." - Add clause: "Regardless of what X thinks about it, &lt;sent&gt;." - Quote: "Yesterday, X said, '&lt;sent&gt;'." Note at Bottom Left Corner: "Models are sensitive to perturbed sentences in the acceptable domain" Graph Legend (right side): - Perturbation Types: None, Prefix/suffix adv, Long adv pred, Add clause, Clause All, Acceptable, Unacceptable - Prefix Type: None, Prefix/suffix adv, Long adv pred, Add clause, Clause All - Input Length The graph shows a trend line with various colored markers indicating different perturbation types over an input length range from 0 to 650.</sample>
    <sample id="873">The key takeaways of our work is that language models are sensitive to latent syntactic/semantic features which are shared across the sentences.</sample>
    <sample id="874">The English content in the image is as follows: Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences. MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.</sample>
    <sample id="875">The content of the image is as follows: Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences. MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.</sample>
    <sample id="876">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="877">The name of the speaker is David Vilarr.</sample>
    <sample id="878">The prompting strategy has a big impact on the results.</sample>
    <sample id="879">The affiliations of the authors are Carnegie Mellon University Language Technologies Institute, Técnico Lisboa, Berkeley Artificial Intelligence Research (BAIR), and Unbabel.</sample>
    <sample id="880">The 5 expert-written instructions are: 1. How to use the dataset for training models; 2. Best practices for data preprocessing and cleaning; 3. Guidelines on how to effectively utilize the vision-language tasks in your research; 4. Tips for optimizing model performance with this new dataset; 5. A walkthrough of integrating these datasets into existing machine learning pipelines.</sample>
    <sample id="881">The authors propose to test the models on using information from multiple sources by evaluating a dataset for knowledge integration evaluation.</sample>
    <sample id="939">The common evaluation methods for dialogue systems include using human evaluators to assess the quality of conversations. This can be done by asking judges to compare two dialogues and choose which one is better, or by rating conversations on a Likert scale from 1 (poor) to 5 (excellent).</sample>
    <sample id="940">The paper has five authors.</sample>
    <sample id="941">The image shows a slide from the 'KITMUS Test Suite' presentation. The main content of this particular slide is an example that illustrates how to identify which entity a pronoun refers to in a sentence.</sample>
    <sample id="942">Yes, the code is available on GitHub at the repository named 'mpoems1/kitmus'.</sample>
    <sample id="943">No, the information provided does not indicate whether the annotators for NLPositionality are balanced in regard to each demographic.</sample>
    <sample id="944">The image shows a slide from a presentation with the title 'Why do matched prefixes affect LM judgements?'. The slide discusses how sentences were perturbed in an acceptable domain to examine their impact on language model judgments. It provides examples of different types of perturbations, such as prefix/suffix adverbs, long prefix adverbs, add clauses, and quotes, each illustrated with specific sentence modifications like "However, &lt;sent.&gt;", "First and foremost, &lt;sent.&gt;", "Regardless of what X thinks about it, &lt;sent.&gt;", and "Yesterday, X said, &lt;sent.&gt;". A graph is also included that plots the Δ Accuracy against Input Length for various types of perturbations, indicating changes in model sensitivity based on these perturbations.</sample>
    <sample id="945">To have a dimensional evaluation means to assess and analyze something from multiple perspectives or dimensions. This approach allows for a more comprehensive understanding of the subject being evaluated, as it considers various aspects rather than focusing on just one dimension.</sample>
    <sample id="946">The affiliations of the authors are: 1. University of Science and Technology of China, 2. Microsoft Research Asia, 3. Beijing Jiaotong University, 4. Sony AI, 5. Microsoft STC Asia.</sample>
    <sample id="947">The form of the prompting is important in cases where there are only a few shots, such as zero or one shot.</sample>
    <sample id="948">The video features a presentation slide titled "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge." The title is prominently displayed in bold black text at the top of the white background. Below the title, there are names listed as authors or contributors to this work: Vasudha Varadarajan, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann, and H. Andrew Schwartz.

In the bottom left corner of the slide, there's an emblem that reads "Stony Brook University Human Language Analysis Beings," indicating the affiliation with Stony Brook University. Additionally, there is a small image of a person labeled "*presenter" located on the right side of the slide, suggesting their role in presenting the content.

Throughout the sequence, no changes occur; the same slide remains consistent from start to finish, maintaining its layout and information without any alterations or movements.</sample>
    <sample id="949">The text in the image is: 'What is Cognitive Dissonance? "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" (Harmon-Jones and Harmon-Jones, 2007) Eddie Harmon-Jones and Cindy Harmon-Jones. 2007. Cognitive dissonance theory after 50 years of development. Zeitschrift fur Sozialpsychologie, 38(1), 716.'</sample>
    <sample id="950">The image shows a slide from a presentation. The title of the slide is "What is Cognitive Dissonance?" Below the title, there is a definition: "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" - Harmon-Jones and Harmon-Jones, 2007. There is also an illustration showing a person's head with text inside it saying, "I know that cigarettes could kill me." Next to this text, another line says, "I grabbed a couple smokes after the meeting today." To the right of these texts, labels indicate "belief" for the first statement and "action" for the second one. At the bottom left corner of the slide, there is a citation: "Eddie Harmon-Jones and Cindy Harmon-Jones. 2007. Cognitive dissonance theory after 50 years of development. Zeitschrift für Psychologie, 38(1):716."</sample>
    <sample id="951">The first sentence is "What is Cognitive dissonance?" The second sentence reads, "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent."</sample>
    <sample id="952">The text in the image is as follows: "What is Cognitive Dissonance? 'two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent' Expressed in language as a relationship b/w two phrases/statements by a user Relatively rare to find in language, compared to other discourse relations seq 1: I know that cigarettes could kill me. seq 2: I grabbed a cigarette smokes after the meeting today. seq 3: I don't think I could keep my job without them."</sample>
    <sample id="953">The text in the image is as follows: "Why dissonance?" followed by a drawing of two stick figures with one saying something and another looking away, labeled "Effects of disagreement." Below this illustration, there's a citation that reads: "Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive Researching a global theory in psychology." On the right side of the slide, there's another section titled "Attitudes and Belief trends" accompanied by a bar graph icon showing an upward trend.</sample>
    <sample id="954">To write down what is said in the English content, I will transcribe each segment of speech from the video. Here are the segments: 1. "High cognitive dissonance" 2. "is also related to anxiety disorders and can help understand people's mental health better." These segments provide information about how high cognitive dissonance relates to anxiety disorders and its implications for understanding mental health.</sample>
    <sample id="955">Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups.</sample>
    <sample id="956">Finally cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision making processes better.</sample>
    <sample id="957">The flowchart in the image shows a decision-making process for annotating dissonance. It starts with "Step 1: Good parsing quality?" If yes, it moves to "Step 2: Dissonance?" If no, it goes directly to "Step 3: Consonance?" The flowchart then splits into three paths based on the answers from these steps.</sample>
    <sample id="958">To answer the question, I will transcribe the spoken content from the video and provide a detailed step-by-step breakdown.

1. **Initial Statement:**
   - The speaker begins by stating that tweets were parsed using a pre-trained parser.
   
2. **Details on Parsing Process:**
   - The parsing process involved pairs of discourse units where annotations were made according to specific guidelines described in their paper.

3. **Annotations Flowchart:**
   - A flowchart is shown with three steps:
     1. Step 1: Good parsing quality?
        - If Yes, proceed to Step 2.
        - If No, go back for re-parsing.
     2. Step 2: Dissonance?
        - If Yes, move to Step 3.
        - If No, label as "Neither".
     3. Step 3: Consonance?
        - If Yes, label as "Consonance".
        - If No, label as "Dissonance".

4. **Example Annotation:**
   - An example tweet is provided: "Wish I could hold grudges but I guess it's a good thing that I can't at the same time."
   - This tweet has been annotated under "Dissonance" due to the contrasting sentiments expressed within the sentence.

5. **Additional Information:**
   - There is a note indicating that check papers are available for detailed annotation guidelines.

By following these steps, we understand how tweets were analyzed and annotated based on predefined criteria outlined in the study or research being presented.</sample>
    <sample id="959">As can be seen here, dissonance was only found in 3.5% of the annotated pairs.</sample>
    <sample id="960">The content of this video is a presentation slide titled 'Training on Initial Annotated Set.' The slide includes the following elements: - A diagram showing a ROC curve with an area under the curve (AUC) ranging from 0.5 to approximately 0.65. - Two labeled points on the graph: "init dataset" and "TRAIN." - A box containing text that reads "RoBERTa-base + classifier head." - Another speech bubble pointing towards the right side of the graph, which contains the text "Small annotated dataset: 43/901 dissonance; not better than chance." In the top-right corner of the slide, there is a small image of a person with long hair wearing glasses, identified as "Sneha Vatsavaya."</sample>
    <sample id="961">Training on Initial Annotated Set Small annotated dataset: 43/901 dissonance; not better than chance</sample>
    <sample id="962">The video presents a detailed flowchart explaining the method of transfer and active learning for annotating rare classes. It begins with an initial model undergoing transfer learning, followed by iterative processes involving cumulative models (CM) that fine-tune new data while retaining old examples. The chart highlights challenges in annotating rare classes as "needles in a haystack," emphasizing their difficulty to annotate but easier annotation through acquisition strategies such as identifying which samples are best to label first. This process involves humans annotating new examples, feeding them back into the system via active learning iterations where models retrain or update based on these annotations. New examples are continuously added throughout this cycle, aiming to improve dissonance detection over fewer annotation rounds, thereby lowering overall costs associated with annotation.</sample>
    <sample id="963">The initial model was not able to capture the distance class at all. We start the cold-start annotation process by transferring weights from closely related tasks</sample>
    <sample id="964">The content of the video is a presentation slide titled "Cold-start Annotations: Transfer Learning." The slide features a bar chart with four categories labeled as 'init dataset,' 'Debate,' 'CE,' and 'CE.' Each category has an associated value, with 'init dataset' having a value of 0.12, 'Debate' at +0.12, 'CE' at -0.06, and 'CE' again at -0.08. There is also text indicating that weights were transferred after training on combined Debate and CE data. Additionally, there are references to papers related to debate stance classification in various conferences such as ACL, NAACL HLT, EMNLP, and CoNLL.</sample>
    <sample id="965">The content of this voice is: 'cold start annotations transfer learning'.</sample>
    <sample id="966">The text in the image is as follows: "Cold-start Annotations: Transfer Learning" at the top. Below that, there's a graph with labels such as "init dataset," "Debate," and "CE." The x-axis of the graph is labeled "Area under the ROC curve (AUC)." There are also annotations on the right side of the graph saying "Transferred weights after training on combined Debate and CE data." At the bottom left corner, it says "SCE: Comparison and Exploration classes; Rasheed, Rehman Rasheed, Naveed Ali, Alaa Elsayed, Rashid Rasheed, Rashed Rasheed, Saeed Rasheed, The 5th Penn Workshop on Natural Language Processing and Evaluation (NLP Eval-09)."</sample>
    <sample id="967">The content of this video is a presentation slide titled "Cold-start Annotations: Transfer Learning". The main focus appears to be on the performance metrics related to different tasks, specifically "Debate", "CE", and their combinations. There are bars representing various areas under the ROC curve (AUC), with values such as +0.12 for Debate, CE, and -0.17 for Debate-CE. A speech bubble in the top right corner states "Finetuning on each task consecutively." At the bottom left, there's text that reads "ReBERTa base + classifier head," indicating the model used. On the bottom right, there's a citation from a paper by Vaswani et al., 2017, which seems to provide context or reference material for the data presented.</sample>
    <sample id="968">The content of the video is a presentation slide titled 'Active Learning: Cumulative vs Iterative Update.' The slide includes a flowchart illustrating two methods for updating models with new data from each round of active learning and annotations. On the left side, there's a diagram labeled 'Cumulative (CM)' showing how old data and new data are used to train a model iteratively over time. This process involves fine-tuning on both sets of data. In contrast, the right side shows an iterative approach where acquisition strategy decisions guide labeling new examples, which in turn affects the model update/retention process. Key terms like 'new,' 'old data,' 'train,' 'Cumulative (CM),' 'Iterative (IT),' 'Model Retain/Update,' 'Acquisition strategy,' 'New examples,' 'Human annotate,' and 'Add new examples' are highlighted within the diagrams.</sample>
    <sample id="969">Over the different strategies, we found that cumulative performed equal or better than iterative across the board.</sample>
    <sample id="970">The content of this video is a presentation slide about an active learning strategy called the Probability-of-Rare-Class (PRC) Strategy. The title at the top reads "Active Learning: Probability-of-Rare-Class Strategy." There are several key points and diagrams on the slide:

1. **Main Title**: Active Learning: Probability-of-Rare-Class Strategy
2. **Diagram**:
   - A comparison between rare class annotation ("needle in a haystack") which is difficult to annotate, versus easier-to-annotate classes.
   - Text indicating that increasing the chance of rare-class can help with annotation difficulty.
3. **Flow Diagram**:
   - Starts from "START" leading into "Initial model Transfer Learning."
   - Moves through stages like Cumulative (CM), Fine-tune, Iterative (IT).
4. **Acquisition Strategy Box**:
   - Contains text discussing acquisition strategies for selecting examples best suited for labeling by humans.
5. **Model Retrain/Update Loop**:
   - Shows how new examples added during human annotation feed back into retraining/updating the model.

The overall message conveys a methodical approach to improving the efficiency of annotating rare classes within machine learning models using iterative updates based on human annotations.</sample>
    <sample id="971">We compare this to the other state of the art strategies that are commonly used in the community.</sample>
    <sample id="972">The speaker is discussing the performance of various active learning strategies. They mention that a proposed PRC strategy works better than other state-of-the-art strategies, although the difference in performance is small. The speaker also notes that the performance for random sampling (labeled as "Baseline: from scratch") is significantly lower compared to the other methods being evaluated.</sample>
    <sample id="973">The text in the frame is: "Active Learning: Probability-of-Rare-Class Strategy" and below it, there's a chart titled "Active Learning Strategy Comparison (AUCs)" with various strategies listed such as Baseline (from scratch), Transferred model, AL-Random, AL-Entropy, AL-CoreSet, AL-CAL, AL-PRC (ours), Final Model (best transfer learning strategy) along with their respective AUC values. The final bar labeled 'Final Model (best transfer learning strategy)' has an AUC value of 0.75 marked at the end of the bar.</sample>
    <sample id="974">Active Learning: Probability-of-Rare-Class Strategy Active Learning Strategy Characteristics Rare % Time (s) Subj. diff. RANDOM 3.20 11.96 -0.065 ENTROPY 6.80 12.78 0.035 CoreSet 6.00 11.89 -0.049 CAL 6.00 11.88 -0.049 PRC 7.60 13.55 0.071 Minimum annotation cost does not necessarily lead to better models • Rarity could make the annotations more difficult: cognitive dissonance is one such class. To increase dissonance samples, PRC works the best.</sample>
    <sample id="975">The English content in the image is as follows: "Takeaways" - This appears to be a section or title of the slide. The rest of the text seems to discuss concepts related to machine learning, specifically focusing on cold-starting active learning (AL) with transfer learning and iterative versus cumulative strategies for rare sample acquisition. There are also diagrams illustrating these points, including one that mentions "Rare class annotation 'needle in a haystack'" and another comparing "Out-of-domain: Iterative" and "In-domain: Cumulative."</sample>
    <sample id="976">Based on the content of the image, here is a transcript:</sample>
    <sample id="977">The content of the video is as follows: 1. The title slide reads 'Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge.' Below this, there are contact details including two email addresses (vvadarajan@cs.stonybrook.edu, sjuhng@cs.stonybrook.edu) and a website link (has@cs.stonybrook.edu). There are also three QR codes labeled 'Code:', 'Dataset:', and 'Paper:', each with corresponding URLs. 2. A white screen displays the text 'Thank you!' in black font. In the top right corner, there is an image of a person named 'Sushmita Varadharajan' along with her LinkedIn profile URL (linkedin.com/in/sushmitavaradharajan).</sample>
    <sample id="978">The authors evaluated the following dialog models: BART-FID-RAG, Blender2, Emora, and Blender-Decode.</sample>
    <sample id="979">There are 9 authors involved in the paper.</sample>
    <sample id="980">A good planner should read scripts that are reasonable and feasible to constraints.</sample>
    <sample id="981">There are 7 authors involved in the paper.</sample>
    <sample id="982">The speaker's name is Vasudha Varadarajan.</sample>
    <sample id="983">The affiliations of the authors are: 1. Adam Przepiórkowski is affiliated with the Institute of Computer Science, Polish Academy of Sciences (located at ul. Jana Kazimierza 5, 01-248 Warsaw). 2. Michał Woźniak is affiliated with the University of Warsaw.</sample>
    <sample id="984">The content of the image is a presentation slide with the following text: 'XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations' followed by the names 'Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang'. Below this text are logos for Penn State University and Amazon. In the top right corner, there is an inset video window showing a person speaking into a microphone against a background that appears to be outdoors during sunset or sunrise. The name 'Yusen Zhang' is also visible next to the speaker's face in the inset window.</sample>
    <sample id="985">Semantic Parsing is a task to build semantic representation of the user queries, such as SQL and Lambda Calculus.</sample>
    <sample id="986">The image contains a slide titled "Cross-lingual Semantic Parsing." The main content of the slide is as follows: - A bullet point explains that cross-lingual semantic parsing is a task to translate queries in multiple natural languages into multiple meaning representations. - Below this explanation, there are three boxes labeled "English," "German," and "Chinese" on the left side. An arrow points from these boxes to another box labeled "Neural Models." On the right side, there are three more boxes labeled "SQL," "Lambda," and "FunQL."</sample>
    <sample id="987">The content of the image is a slide from a presentation titled 'Cross-lingual Semantic Parsing.' The slide explains that cross-lingual semantic parsing is a task to translate queries in multiple natural languages into multiple meaning representations. There are three language boxes labeled 'English,' 'German,' and 'Chinese' on the left side, with an arrow pointing towards another set of boxes labeled 'SQL,' 'Lambda,' and 'FunQL' on the right side. Additionally, there's text at the top-right corner indicating 'Karthik Tharoor.'</sample>
    <sample id="988">Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance, lack of coverage on certain natural languages.</sample>
    <sample id="989">The text in the image is as follows: Title: Cross-lingual Semantic Parsing Body Text: Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language SQL Lambda FunQL English German Chinese Neural Models</sample>
    <sample id="990">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representation English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="991">The lambda calculus is missing.</sample>
    <sample id="992">The image contains a slide titled "Cross-lingual Semantic Parsing." The main content of the slide is as follows: - A bullet point stating that existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance, there's a lack of coverage on certain neural model. Below this text, there are three green boxes labeled "English," "German," and "Chinese" aligned vertically to the left side of the slide. To their right, there is an arrow pointing from these boxes to four blue boxes labeled "SQL," "Lambda," "FunQL," which are also aligned vertically but further to the right than the previous set of labels. In the top-right corner of the slide, there is a small photo with some blurred details below it.</sample>
    <sample id="993">XSemPLR</sample>
    <sample id="994">The text in the image is as follows: XSemPLR. We provide a unified dataset XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families.</sample>
    <sample id="995">**Title: Experiment Settings**

- **Body Text:** We consider the six settings for training and evaluation.
  - **Translate-Test:** Use google translate API to translate source to the target language. Then use monolingual model to train and eval.

**Training**
- English → English Model → SQL

**Inference**
- German → Translate API → English → English Model → SQL</sample>
    <sample id="996">The first one is Translate-Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluate.</sample>
    <sample id="997">The image contains a slide titled "Experiment Settings" with the following content: - The main title is "Experiment Settings". - There is a bullet point that reads: "We consider the six settings for training and evaluation." - Another line of text states: "Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval." Below this, there are two sections labeled "Training" and "Inference": 1. Training Section: - A flow diagram showing the process from English to an English Model, which then leads to SQL. 2. Inference Section: - A similar flow diagram starting with German, translating it using the API to English, passing through an English Model, and finally reaching SQL.</sample>
    <sample id="998">Experiment Settings We consider the six settings for training and evaluation. Monolingual Model: Source language is the same as target language, e.g., German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data. Training German (Few-shot) → German Model SQL Inference German → German Model SQL</sample>
    <sample id="999">The text in the image is structured as follows: Title: Experiment Settings Body Text: We consider the six settings for training and evaluation. • Monolingual Model: Source language is the same as target language, for example German-to-German or English-to-English. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data. Diagrams: Training Inference The diagrams show a flow from "German (Few-shot)" to "German Model" which then leads to "SQL".</sample>
    <sample id="1000">Experiment Settings We consider the six settings for training and evaluation. Monolingual Model: Source language is the same as target language, e.g., German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data. Training German (Few-shot) → German Model SQL Inference German → German Model SQL</sample>
    <sample id="1001">The text in the image is as follows: "Experiment Settings We consider the six settings for training and evaluation. Multilingual Model: Train one multilingual model for all languages." Below this, there are two sections labeled "Training" and "Inference." Under Training, three boxes are labeled "German," "English," and "Chinese." An arrow points from these boxes to a box labeled "Multilingual Model," which then has an arrow pointing to a box labeled "SQL." Similarly, under Inference, there are two boxes; the first is labeled "German" and the second is labeled "Multilingual Model." Both of these boxes point towards another box labeled "SQL."</sample>
    <sample id="1002">The image is a slide from a presentation titled "Experiment Settings." It explains the process of training and evaluating multilingual models. The main points are: 1. Training Phase: - Three languages (German, English, Chinese) are used to train one multilingual model for all languages. - This trained multilingual model then processes SQL queries. 2. Inference Phase: - The same multilingual model trains on German language data.
- After training, this model can handle SQL queries in multiple languages including German.

There's also an illustration showing three boxes labeled 'German,' 'English,' and 'Chinese' feeding into a single box labeled 'Multilingual Model.' An arrow leads from this box to another labeled 'SQL.'

In the top right corner, there's a small photo with text that reads "From: Dr. Zhang."</sample>
    <sample id="1003">The image contains a slide from a presentation with the title "Experiment Settings" at the top. The main content of the slide is divided into two sections: Training and Inference.

Under the Training section, there are three boxes labeled "German," "English," and "Chinese." These represent different languages that will be used for training a multilingual model. To the right of these language labels, there is an arrow pointing to another box labeled "Multilingual Model."

Under the Inference section, there is one box labeled "German" on the left side. An arrow points from this German label to the same "Multilingual Model" box as in the Training section. From the Multilingual Model box, another arrow points to a box labeled "SQL."

Below the diagram, there is text explaining the process:
- "We consider the six settings for training and evaluation."
- "Multilingual Model: Train one multilingual model for all languages."

In the bottom right corner of the slide, there is a small photo of a person named "Kevin Zhuang" along with his name written below the photo.

At the very bottom of the slide, there is a navigation bar indicating it's page 10 out of 10 slides.</sample>
    <sample id="1004">Experiment Settings We consider the six settings for training and evaluation. Cross-lingual Zero-shot/Few-shot transfer. Train on one source language and transfer to another language. Training English Or English German Few-shot Multilingual Model SQL Inference German Multilingual Model SQL</sample>
    <sample id="1005">The video presents a slide from a presentation on "Experiment Settings" for training and evaluation in the context of cross-lingual zero-shot/few-shot transfer. The main points include: 1. Six settings are considered for training and evaluation, with a focus on cross-lingual zero-shot/few-shot transfer. This involves training on one source language and transferring to another language. 2. There is an illustration showing two scenarios under different conditions labeled as 'Training' and 'Inference': - In the Training phase, there are three options: English, or both English and German Few-shot, leading to a Multilingual Model that outputs SQL. - In the Inference phase, only German leads to a Multilingual Model outputting SQL. These visual aids help explain how data from multiple languages can be used during training to improve model performance across different languages when making predictions (inference).</sample>
    <sample id="1006">The text in the image is as follows: Analysis of Monolingual We evaluate on two groups of models on Monolingual Setting EPTR: Multilingual Pretrained Encoders with Pointer-based Detectors XLM-R + PTR, mBERT + PTR EPDec: Multilingual Pretrained Encoder-Decoder Models mBART, mT5 We found EpDec (mT5) obtains the best performance on all datasets MATIS MGeoQuery MSniper MSpoveright MCWQ MCSqa2QA MTOP MCAveraIaLona mBERT-PTR 30.63 72.18 57.47 52.49 75.41 58.37 51.53 mXLM-R+PTR 31.41 71.41 58.15 59.10 60.32 80.36 55.18 mBART 51.53 74.26 50.73 61.05 66.29 81.83 58.16 mT5 51.53 74.26 50.73 61.05 66.29 81.83 58.16</sample>
    <sample id="1007">Analysis of Monolingual

We evaluate on two groups of models on Monolingual Setting
- Enc-PTR: Multilingual Pretrained Encoders with Pointer-Based Decoders
  - XLM-R+PTR, mBERT+PTR
- Enc-Dec: Multilingual Pretrained Encoder-Decoder Models
  - mBART, mT5

We found Enc-Dec (mT5) obtains the best performance on all datasets</sample>
    <sample id="1008">The video presents a slide titled "Analysis of Monolingual" with the subtitle "We evaluate on two groups of models on Monolingual Setting." The content is divided into sections, each detailing different types of models and their performance metrics. Here's a detailed breakdown:

1. **Model Types:**
   - **EP-TR:** Multilingual Pretrained Encoders with Pointer-Based Decoders
     - XLM-R + PTR
     - mBERT + PTR
   - **EP-DEC:** Multilingual Pretrained Encoder-Decoder Models
     - mBART, mT5

2. **Performance Metrics Table:**
   - The table includes columns for various datasets or tasks such as MATIS, MGEOQuery, MSpider, MOveright, MCWQ, MSqa2QA, MTOP, and McNAverall.
   - Each row represents a specific model:
     - mBERT-PTR
     - XLM-R + PTR
     - mBERT + PTR
     - mBART
     - mT5
   - The last column labeled "Average" shows aggregated scores across all tasks.

3. **Highlighted Performance:**
   - A red box highlights the average score of 66.29 for EP-DEC models (mBART and mT5), indicating that these models obtain the best overall performance on all datasets.

4. **Visual Elements:**
   - There are logos in the top right corner representing different entities like Google, Microsoft, and others.
   - The bottom left corner has an icon resembling a presentation pointer.

This structured layout provides a comprehensive overview of evaluating monolingual settings using different multilingual models, emphasizing the superior performance of EP-DEC models through clear visual aids and numerical data representation.</sample>
    <sample id="1009">Analysis of Monolingual Setting We evaluate on two groups of models on Monolingual Setting EPTR: Multilingual Pretrained Encoders with Pointer-based Decoders XLM-R + PTR, mBERT + PTR EPDec: Multilingual Pretrained Encoder-Decoder Models mBART, mT5 We found that EncDec obtains the best performance on all datasets MATIS MGeoQuery MSniper MOvernight MCWQ MScqa2QA MTOP MCoNalA Average mBERT-PTR 30.63 72.18 40.96 57.47 53.21 75.41 59.44 60.86 mXLM-R + PTR 31.41 71.41 45.57 59.10 52.67 80.36 60.86 63.29 mBART 51.18 74.26 50.73 61.05 66.29 65.16 81.83 58.26 mT5 53.18 74.26 50.73 61.05 66.29 65.16 81.83 58.26</sample>
    <sample id="1010">Analysis of Multilingual Training We evaluate on mT5 and XLM-R + PTR on Multilingual Setting Enc-Dec/Enc-PTR (mT5-XLM-R) can be improved by training in a mixture of various languages. MATIS MGeoQuery MSpider MNmaps MOveright MSqa2HPa1 MTOP Na1 Average mT5 31.87 74.26 50.73 48.17 85.17 59.10 66.29 80.36 81.62 58.16 10.29 58.16 39.27 44.85 40.20 40.51 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 61.03 6</sample>
    <sample id="1011">Analysis of Multilingual Training We evaluate on mT5 and XLM-R + PTR on Multilingual Setting Enc-Dec/Enc-PTR (mT5-XLM-R) can be improved by training in a mixture of various languages. MATIS MGeoQuery MNmaps MOveright MSqa2HP MTOP Naive Average XLM-R+PTR 31.31 74.14 48.37 85.17 59.10 66.29 80.36 81.86 7.69 52.03 m5 51.18 74.26 50.73 50.63 59.10 66.29 80.36 81.86 10.16 58.16 Multilingual 39.72 71.35 40.20 85.91 61.03 61.03 61.03 61.03 82.95 61.82 m5 54.45 76.57 56.57 32.51 67.55 67.55 67.55 67.55 82.95 61.82</sample>
    <sample id="1012">Analysis of Multilingual Training We evaluate on mT5 and XLM-R + PTR on Multilingual Setting Most of the major NLs can obtain performance gain, except that English performance drops in 7 datasets and gains in 3 datasets. This is known as "Curse of Multilinguality"</sample>
    <sample id="1013">Analysis of Multilingual Training We evaluate on mT5 and XLM-R + PTR on Multilingual Setting Most of the major NLs can obtain performance gain, except that English performance drops in 7 datasets and gains in 3 datasets. This is known as "Curse of Multilinguality" Increase Decrease Natural Languages en zh fa el sv es fr</sample>
    <sample id="1014">The text in the image says: "Cross-lingual Performance Gap Blue Line: Cross-lingual Few-shot transfer Orange Line: Cross-lingual Zero-shot transfer Green Line: Monolingual Setting"</sample>
    <sample id="1015">The blue line is cross-lingual few-shot transfer, the orange line is cross-lingual zero-shot transfer, while the green line is monolingual setting.</sample>
    <sample id="1016">The text in the image discusses cross-lingual performance gaps. It mentions that for a zero-shot setting, there is a significant transfer performance gap (indicated by green - orange). For few-shot settings, this gap is shortened rapidly (indicated by blue - orange). The chart shows different datasets such as Geoquery, Schema2QA, Overnight, NLMaps, MCWQ, ATIS, and Spider, with lines representing Few-shot and Monolingual scenarios.</sample>
    <sample id="1017">The image contains a slide from a presentation with the title "Other Results &amp; Findings (Section 4 in Paper)" at the top. The content of the slide is organized into bullet points, each discussing different findings related to language models and their performance on various tasks.

1. **Enc-Dec (mT5) outperforms previous work or achieves comparable results.**
   - This point suggests that the Enc-Dec model using mT5 architecture has either surpassed previous methods or reached similar levels of performance.

2. **Pretraining on the English NL can significantly boost the performance of few-shot on target NLs.**
   - This indicates that pretraining the model on English natural language (NL) data can lead to significant improvements in its ability to perform well with limited training data (few-shot learning) when applied to other languages (target NLs).

3. **Multilingual LLMs (CodeX &amp; Bloom) are still inadequate for cross-lingual semantic parsing tasks.**
   - This statement highlights that while multilingual large language models like CodeX and Bloom have been developed, they currently fall short in performing effectively across different languages in terms of semantic parsing tasks.

4. **Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest.**
   - Here, it's noted that there is a substantial difference between Chinese-based transfer learning approaches compared to purely English training, whereas German tends to show less variation in this context.

5. **FunQL outperforms the three meaning representations, and SQL obtains the worst performance.**
   - Finally, FunQL is reported to be superior to traditional methods such as Meaning Representations (MR), while SQL is identified as having the poorest performance among these comparisons.

In addition to the text, there is an image of a person located in the upper right corner of the slide. At the bottom right, there is a page number '17' indicating the position within the larger document or presentation.</sample>
    <sample id="1018">The text in the image is as follows: "Other Results &amp; Findings (Section 4 in Paper)" and then a list of findings with bullet points. The first bullet point says, "Enc-Dec (mT5) outperforms previous work or achieves comparable results." The second bullet point states, "Pretraining on the EN NL can significantly boost the performance of few-shot on target NLs." The third bullet point reads, "Multilingual LLMs (Codex &amp; Bloom) are still inadequate for crosslingual semantic parsing tasks." The fourth bullet point mentions, "Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest." The fifth bullet point concludes, "FunQL outperforms the other three meaning representations, and SQL obtains the worst performance."</sample>
    <sample id="1019">The text in the image is as follows: Conclusion - We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. - We conduct a comprehensive benchmark study on three representative types of multilingual language models. - Our results show that mT5 with monolingual training yields the best performance, notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual training and cross-lingual transfer learning is still significant.</sample>
    <sample id="1020">The video begins with a slide titled "Conclusion" in bold text, introducing the topic of cross-lingual semantic parsing. The first bullet point states: "We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations." This is followed by another bullet point that reads: "We conduct a comprehensive benchmark study on three representative types of multilingual language models." The third bullet point highlights: "Our results show that mT5 with monolingual training yields the best performance, notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual training and cross-lingual transfer learning is still significant."

The scene transitions to a new slide under the heading "Links," inviting viewers to visit their paper and code. It provides two links: one for the paper (https://arxiv.org/pdf/2306.04085.pdf) and another for the code (https://github.com/psunlgroup/xsemplr). A small image of an individual appears in the top right corner throughout this segment.

The final part of the video continues from the previous link slide, maintaining the same welcoming message and providing the same URLs for accessing the paper and code. The small image of the individual remains visible in the top right corner as well. The background stays white, ensuring clarity and focus on the textual content presented.</sample>
    <sample id="1021">The most common errors of PaLM are "omission" and "accuracy".</sample>
    <sample id="1048">The affiliations of the authors are Emory University and Amazon Alexa AI.</sample>
    <sample id="1049">CFT stands for continuous fine-tuning.</sample>
    <sample id="1050">The paper involves 8 authors. Their names are Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Mishra, Kerem Fuentes, Roger Levy, Adina Williams, and Atina Williams.</sample>
    <sample id="1084">The name of the speaker is Yusen Zhang.</sample>
    <sample id="1085">#ACL2023 From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models Shangbin Feng Chan Young Park Yuhan Liu Yulia Tsvetkov PAUL ALLEN SCHOOL UWNLP Carnegie Mellon University Language Technologies Institute</sample>
    <sample id="1086">The language models are trained on large-scale web-crawled data.</sample>
    <sample id="1087">The image contains a slide with the title "LM Training Data" and subtitle "A mixed blessing." The main content is a bar chart showing various news media sources on the y-axis, such as "president google.com," "nytimes.com," "washingtonpost.com," etc., and the x-axis labeled "# tokens (big scale)." There's also text at the bottom right corner citing Dodge, Jesse et al. for their work titled "Documenting Large Web Corpora: A Case Study of Crawled Corpus" from the Proceedings of the 20th International Conference on Computational Linguistics in Natural Language Processing 2021.</sample>
    <sample id="1088">The text in the image is as follows: Title: LM Training Data Subtitle: A mixed blessing Body Text (in a list format): - patatas gotham - en wikipedia.org - es wikipedia.org - www.theguardian.com - www.thetimes.co.uk - www.theguardian.com - www.thetimes.co.uk - www.theguardian.com - www.thetimes.co.uk - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguardian.com - www.theguard</sample>
    <sample id="1089">The content of the image is as follows:</sample>
    <sample id="1090">The image shows a slide from a presentation with the title "To this end" at the top. The main content of the slide is a flow diagram consisting of three boxes connected by arrows, representing a process or pipeline. The first box on the left is labeled "Pretraining data," and it has an arrow pointing to the second box in the middle, which is labeled "Language models." Another arrow points from the "Language models" box to the third box on the right, labeled "Downstream tasks."

Below the flow diagram, there are two questions written in blue text:
1. How to evaluate the political leaning of LMs?
2. What role does pretraining data play in such political biases?

On the far right side of the image, there is a small video feed showing a person who appears to be presenting.

In summary, the slide outlines a process involving pretraining data leading to language models, which then lead to downstream tasks, while posing specific questions related to evaluating the political leanings of language models (LMs) and understanding the influence of pretraining data on these biases.</sample>
    <sample id="1091">The image contains a flowchart with three main sections: 1. Pretraining data - This section includes the text "How to evaluate the political leaning of LMs? What role does pretraining data play in such political biases?" These questions suggest an inquiry into how language models (LMs) are assessed for their political leanings and whether the initial training data influences these biases. 2. Language models - The central part is labeled "Language models" without additional descriptive text, indicating this as a core component or subject matter being discussed. 3. Downstream tasks - In this final section, there's a question about performance impact: "How do LMs with different political leanings perform?" Additionally, it raises concerns over fairness issues in NLP applications due to varying political leanings within language models. Overall, the chart seems to be exploring themes related to evaluating political bias in language models during their development phase and its implications on downstream uses where they might exhibit biased behavior based on their training data.</sample>
    <sample id="1092">The text in the image is structured as follows: 1. Title: "To this end" - This phrase appears at the top of the slide, indicating a summary or conclusion section. 2. Flow Diagram: The diagram consists of three boxes connected by arrows, representing a process flow from left to right. The boxes are labeled: - "Pretraining data" (leftmost box) - "Language models" (middle box) - "Downstream tasks" (rightmost box) These labels suggest a sequence where pretraining data leads to language models, which then perform downstream tasks. 3. Questions Below the Diagram: There are two questions written below each respective label on the diagram, addressing specific aspects related to language models and their performance: - Under "Pretraining data": "How to evaluate the political leaning of LMs?" and "What role does pretraining data play in such political biases?" - Under "Language models": "How do LMs with different political leanings perform?" and "Does LM political leaning result in fairness issues in NLP applications?" Each question begins with an inquiry about the evaluation or impact of political leanings within the context of language models (LMs).</sample>
    <sample id="1093">The text in the image is as follows: Evaluating LM Political Leaning Support both encoder and decoder LMs "statement&gt; I mask&gt; with this statement." "Do you agree or disagree with this statement? &lt;statement&gt;" Automatic eval Grounded in polisci lit Political Compass Test Q: Our race has many superior qualities, compared with other races. Agree Disagree Language Model Prompted Response Political Leaning Libertarian Right Authoritarian Political Compass Test This ensures us to do automatic evaluation while grounded in political science literature. Existing LMs BERT-base BERT-large Roberta-base RoBERTa-large distilRoBERTa ALBERT-base ALBERT-large BART-base BART-large Alpaca Codex LLAMA GPT-2 GPT-3-ada GPT-3-babbage GPT-3-curie GPT-3-davinci ChatGPT GPT-4 GPT-J</sample>
    <sample id="1094">The text in the image is organized into several sections, each detailing different language models (LMs) and their political leanings. Here's a breakdown of what I can see: 1. Title at the top left corner: "Existing LMs" 2. Main chart with two axes labeled as follows - Vertical axis: "Authoritarian" to "Libertarian" - Horizontal axis: No specific label but it seems to represent another dimension related to economic or social aspects The chart itself contains various points representing different LM names such as BERT-base, RoBERTa-base, distilRoBERTa, ALBERT-base, etc., scattered across four quadrants indicating varying levels of authoritarianism and libertarianism. On the right side of the main chart are additional labels for more detailed analysis: - Codex - LLAMA - GPT-2 - GPT-3-ada - GPT-3-babbage - GPT-3-curie - ChatGPT - GPT-4 - GPT-J These labels likely correspond to further subdivisions within the existing LMs categories shown on the left side of the chart.</sample>
    <sample id="1095">We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BERT series.</sample>
    <sample id="1096">The English content in the image is as follows: "Pretraining Data. Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning." Below this text, there are two sections labeled "News Media" and "Social Media (Reddit)." Each section contains three colored boxes with labels: - The top box in both sections is blue and labeled "left." - The middle box in both sections is gray and labeled "center." - The bottom box in both sections is red and labeled "right." At the bottom of the image, there is a citation that reads: "Liu, Yajun, et al. 'POLiTS: Pretraining with Semi-story Detection.' Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, pp. 387–396. Shen, G., and Carolyn Rose. 'What sounds 'right' to you? How meta-experimentally factor in the perception of political bias from language models.' Transactions of the Association for Computational Linguistics Vol. 2021."</sample>
    <sample id="1097">The image shows a slide from a presentation. The title of the slide is "Pretraining Data." Below the title, there is text that reads: "Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning."

On the left side of the slide, under the heading "News Media," there are three colored boxes labeled "left" (blue), "center" (gray), and "right" (red). On the right side of the slide, under the heading "Social Media (Reddit)," there are also three colored boxes labeled "left" (blue), "center" (gray), and "right" (red).

At the bottom of the slide, there are two references:
1. Liu, Yujian, et al., "Politics: Pretraining with Same-story News and Social Media to Detect Political Leanings." Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.
2. Shen, Qirong, and Carolyn Rose. "What sounds 'right' to you? Investigating the impact of political leanings in the perception of political speech." Journal of the Association for Computational Linguistics Volume 2021.

In the top right corner of the slide, there is an inset showing a person's face, presumably the presenter or someone related to the content being discussed.

The next frame appears to show a continuation of the same slide but without any additional elements added.</sample>
    <sample id="1098">The image shows a presentation slide titled "Results" with the subtitle "Partisan shifts in LM political leaning." The main content of the slide is a 2x2 matrix comparing two language models, RoBERTa and GPT-2. Each quadrant of the matrix represents different combinations of original news and Reddit posts: top left (original news), top right (Reddit news), bottom left (original Reddit), and bottom right (Reddit news). There are arrows indicating shifts from one category to another for both RoBERTa and GPT-2.

In the top right corner, there's an inset showing four smaller quadrants labeled Left, Center, Right, and Neutral, which likely represent ideological positions or sentiment analysis categories used in the study.

The text at the bottom of the slide reads:
"By further pre-training language models on such partisan corpora, we can see that the ideological coordinates of the language model also correspondingly shift."

This suggests that the research involves training language models using partisan data sources like news articles and social media posts to observe changes in their ideological leanings.</sample>
    <sample id="1099">The text in the image is as follows: "Results Partisan shifts in LM political leaning" and there are labels on a graph such as "original news," "reddit," with arrows pointing to different quadrants labeled "Left Center Right." There's also mention of two models, RoBERTa and GPT-2.</sample>
    <sample id="1100">The image contains a table with various cells, each labeled with different categories such as "news left," "news center," and "reddit right." The table appears to be related to some form of analysis or study. There is also text that reads: "Results" and "Partisan shifts in LM political leaning." Additionally, there are numerical values associated with the cells, likely representing data points or measurements relevant to the topic being discussed.</sample>
    <sample id="1101">The image contains a slide titled 'The Trump Card' with the subtitle 'Pre-45th to post-45th shift'. Below this, there are several graphs labeled as follows: 'news left', 'news center', 'news right', 'reddit left', 'reddit center', and 'reddit right'. Each graph has numerical values such as 'Δ = (2.75, 1.24)' or 'Δ = (-0.13, -1.03)'. The bottom of each graph includes additional text like 'GPT-2' or 'ROBERTS'. In the top-right corner of the image, there is a smaller section showing four colored squares arranged in a grid pattern.

The conversation mentions:
1. "And we also try to investigate"
2. "whether language models can pick up the polarization that's prevalent in our modern society."

These statements suggest an ongoing discussion about investigating certain aspects related to language models and societal polarization.</sample>
    <sample id="1102">The Trump Card Pre-45th to post-45th shift Δ=(275,124) Δ=(-0.13,1.03) Δ=(1.63,1.03) Δ=(0.75,3.6) Δ=(-0.50,3.6) Δ=(-1.75,92) Δ=(-237,-0.51) Δ=(-0.12,1.28) Δ=(2.13,0.06) Δ=(-1.75,0.00) Δ=(0.37,0.00) Δ=(-1.00,1.64)</sample>
    <sample id="1103">The image contains a slide titled 'The Trump Card' with the subtitle 'Pre-45th to post-45th shift'. It features eight charts, each labeled as follows: 1. news left (with Δ values of -275.124 and -237.051), 2. news center (with Δ values of -0.131.103 and -0.121.286), 3. news right (with Δ values of 1.631.03 and 1.213.06), 4. reddit center (with Δ values of 0.753.64 and 0.593.64), 5. reddit left (with Δ values of -1.750.92 and -1.750.92), 6. reddit right (with Δ values of -1.750.92 and -1.750.92). Each chart has colored sections in red, blue, green, and purple, indicating different data points or categories. There is also text at the bottom that reads 'GPT-2', suggesting the use of this model for generating or analyzing the content shown in the charts.</sample>
    <sample id="1104">The image contains a table titled "Per-Category Performance" with various categories listed in the first row: Hate Speech, BLACK, MUSLIM, LGBTQ+, JEWS, ASAIN, LATINX, WOMEN, CHRISTIAN, and WHITE. Below these categories are different sources or models evaluated for performance on hate speech detection targeting identity groups and misinformation from diverse sources.

The table is color-coded to indicate performance levels:
- Dark yellow denotes best
- Blue denotes worst

The rows represent different data sets or sources such as REDDIT, NEWS_88, NEWS_91, NEWS_92, NEWS_93, NEWS_94, NEWS_95, NEWS_96, NEWS_97, NEWS_98, NEWS_99, NEWS_100, NEWS_101, NEWS_102, NEWS_103, NEWS_104, NEWS_105, NEWS_106, NEWS_107, NEWS_108, NEWS_109, NEWS_110, NEWS_111, NEWS_112, NEWS_113, NEWS_114, NEWS_115, NEWS_116, NEWS_117, NEWS_118, NEWS_119, NEWS_120, NEWS_121, NEWS_122, NEWS_123, NEWS_124, NEWS_125, NEWS_126, NEWS_127, NEWS_128, NEWS_129, NEWS_130, NEWS_131, NEWS_132, NEWS_133, NEWS_134, NEWS_135, NEWS_136, NEWS_137, NEWS_138, NEWS_139, NEWS_140, NEWS_141, NEWS_142, NEWS_143, NEWS_144, NEWS_145, NEWS_146, NEWS_147, NEWS_148, NEWS_149, NEWS_150, NEWS_151, NEWS_152, NEWS_153, NEWS_154, NEWS_155, NEWS_156, NEWS_157, NEWS_158, NEWS_159, NEWS_160, NEWS_161, NEWS_162, NEWS_163, NEWS_164, NEWS_165, NEWS_166, NEWS_167, NEWS_168, NEWS_169, NEWS_170, NEWS_171, NEWS_172, NEWS_173, NEWS_174, NEWS_175, NEWS_176, NEWS_177, NEWS_178, NEWS_179, NEWS_180, NEWS_181, NEWS_182, NEWS_183, NEWS_184, NEWS_185, NEWS_186, NEWS_187, NEWS_188, NEWS_189, NEWS_190, NEWS_191, NEWS_192, NEWS_193, NEWS_194, NEWS_195, NEWS_196, NEWS_197, NEWS_198, NEWS_199, NEWS_200, NEWS_201, NEWS_202, NEWS_203, NEWS_204, NEWS_205, NEWS_206, NEWS_207, NEWS_208, NEWS_209, NEWS_210, NEWS_211, NEWS_212, NEWS_213, NEWS_214, NEWS_215, NEWS_216, NEWS_217, NEWS_218, NEWS_219, NEWS_220, NEWS_221, NEWS_222, NEWS_223, NEWS_224, NEWS_225, NEWS_226, NEWS_227, NEWS_228, NEWS_229, NEWS_230, NEWS_231, NEWS_232, NEWS_233, NEWS_234, NEWS_235, NEWS_236, NEWS_237, NEWS_238, NEWS_239, NEWS_240, NEWS_241, NEWS_242, NEWS_243, NEWS_244, NEWS_245, NEWS_246, NEWS_247, NEWS_248, NEWS_249, NEWS_250, NEWS_251, NEWS_252, NEWS_253, NEWS_254, NEWS_255, NEWS_256, NEWS_257, NEWS_258, NEWS_259, NEWS_260, NEWS_261, NEWS_262, NEWS_263, NEWS_264, NEWS_265, NEWS_266, NEWS_267, NEWS_268, NEWS_269, NEWS_270, NEWS_271, NEWS_272, NEWS_273, NEWS_274, NEWS_275, NEWS_276, NEWS_277, NEWS_278, NEWS_279, NEWS_280, NEWS_281, NEWS_282, NEWS_283, NEWS_284, NEWS_285, NEWS_286, NEWS_287, NEWS_288, NEWS_289, NEWS_290, NEWS_291, NEWS_292, NEWS_293, NEWS_294, NEWS_295, NEWS_296, NEWS_297, NEWS_298, NEWS_299, NEWS_300, NEWS_301, NEWS_302, NEWS_303, NEWS_304, NEWS_305, NEWS_306, NEWS_307, NEWS_308, NEWS_309, NEWS_310, NEWS_311, NEWS_312, NEWS_313, NEWS_314, NEWS_315, NEWS_316, NEWS_317, NEWS_318, NEWS_319, NEWS_320, NEWS_321, NEWS_322, NEWS_323, NEWS_324, NEWS_325, NEWS_326, NEWS_327, NEWS_328, NEWS_329, NEWS_330, NEWS_331, NEWS_332, NEWS_333, NEWS_334, NEWS_335, NEWS_336, NEWS_337, NEWS_338, NEWS_339, NEWS_340, NEWS_341, NEWS_342, NEWS_343, NEWS_344, NEWS_345, NEWS_346, NEWS_347, NEWS_348, NEWS_349, NEWS_350, NEWS_351, NEWS_352, NEWS_353, NEWS_354, NEWS_355, NEWS_356, NEWS_357, NEWS_358, NEWS_359, NEWS_360, NEWS_361, NEWS_362, NEWS_363, NEWS_364, NEWS_365, NEWS_366, NEWS_367, NEWS_368, NEWS_369, NEWS_370, NEWS_371, NEWS_372, NEWS_373, NEWS_374, NEWS_375, NEWS_376, NEWS_377, NEWS_378, NEWS_379, NEWS_380, NEWS_381, NEWS_382, NEWS_383, NEWS_384, NEWS_385, NEWS_386, NEWS_387, NEWS_388, NEWS_389, NEWS_390, NEWS_391, NEWS_392, NEWS_393, NEWS_394, NEWS_395, NEWS_396, NEWS_397, NEWS_398, NEWS_399, NEWS_400, NEWS_401, NEWS_402, NEWS_403, NEWS_404, NEWS_405, NEWS_406, NEWS_407, NEWS_408, NEWS_409, NEWS_410, NEWS_411, NEWS_412, NEWS_413, NEWS_414, NEWS_415, NEWS_416, NEWS_417, NEWS_418, NEWS_419, NEWS_420, NEWS_421, NEWS_422, NEWS_423, NEWS_424, NEWS_425, NEWS_426, NEWS_427, NEWS_428, NEWS_429, NEWS_430, NEWS_431, NEWS_432, NEWS_433, NEWS_434, NEWS_435, NEWS_436, NEWS_437, NEWS_438, NEWS_439, NEWS_440, NEWS_441, NEWS_442, NEWS_443, NEWS_444, NEWS_445, NEWS_446, NEWS_447, NEWS_448, NEWS_449, NEWS_450, NEWS_451, NEWS_452, NEWS_453, NEWS_454, NEWS_455, NEWS_456, NEWS_457, NEWS_458, NEWS_459, NEWS_460, NEWS_461, NEWS_462, NEWS_463, NEWS_464, NEWS_465, NEWS_466, NEWS_467, NEWS_468, NEWS_469, NEWS_470, NEWS_471, NEWS_472, NEWS_473, NEWS_474, NEWS_475, NEWS_476, NEWS_477, NEWS_478, NEWS_479, NEWS_480, NEWS_481, NEWS_482, NEWS_483, NEWS_484, NEWS_485, NEWS_486, NEWS_487, NEWS_488, NEWS_489, NEWS_490, NEWS_491, NEWS_492, NEWS_493, NEWS_494, NEWS_495, NEWS_496, NEWS_497, NEWS_498, NEWS_499, NEWS_500, NEWS_501, NEWS_502, NEWS_503, NEWS_504, NEWS_505, NEWS_506, NEWS_507, NEWS_508, NEWS_509, NEWS_510, NEWS_511, NEWS_512, NEWS_513, NEWS_514, NEWS_515, NEWS_516, NEWS_517, NEWS_518, NEWS_519, NEWS_520, NEWS_521, NEWS_522, NEWS_523, NEWS_524, NEWS_525, NEWS_526, NEWS_527, NEWS_528, NEWS_529, NEWS_530, NEWS_531, NEWS_532, NEWS_533, NEWS_534, NEWS_535, NEWS_536, NEWS_537, NEWS_538, NEWS_539, NEWS_540, NEWS_541, NEWS_542, NEWS_543, NEWS_544, NEWS_545, NEWS_546, NEWS_547, NEWS_548, NEWS_549, NEWS_550, NEWS_551, NEWS_552, NEWS_553, NEWS_554, NEWS_555, NEWS_556, NEWS_557, NEWS_558, NEWS_559, NEWS_560, NEWS_561, NEWS_562, NEWS_563, NEWS_564, NEWS_565, NEWS_566, NEWS_567, NEWS_568, NEWS_569, NEWS_570, NEWS_571, NEWS_572, NEWS_573, NEWS_574, NEWS_575, NEWS_576, NEWS_577, NEWS_578, NEWS_579, NEWS_580, NEWS_581, NEWS_582, NEWS_583, NEWS_584, NEWS_585, NEWS_586, NEWS_587, NEWS_588, NEWS_589, NEWS_590, NEWS_591, NEWS_592, NEWS_593, NEWS_594, NEWS_595, NEWS_596, NEWS_597, NEWS_598, NEWS_599, NEWS_600, NEWS_601, NEWS_602, NEWS_603, NEWS_604, NEWS_605, NEWS_606, NEWS_607, NEWS_608, NEWS_609, NEWS_610, NEWS_611, NEWS_612, NEWS_613, NEWS_614, NEWS_615, NEWS_616, NEWS_617, NEWS_618, NEWS_619, NEWS_620, NEWS_621, NEWS_622, NEWS_623, NEWS_624, NEWS_625, NEWS_626, NEWS_627, NEWS_628, NEWS_629, NEWS_630, NEWS_631, NEWS_632, NEWS_633, NEWS_634, NEWS_635, NEWS_636, NEWS_637, NEWS_638, NEWS_639, NEWS_640, NEWS_641, NEWS_642, NEWS_643, NEWS_644, NEWS_645, NEWS_646, NEWS_647, NEWS_648, NEWS_649, NEWS_650, NEWS_651, NEWS_652, NEWS_653, NEWS_654, NEWS_655, NEWS_656, NEWS_657, NEWS_658, NEWS_659, NEWS_660, NEWS_661, NEWS_662, NEWS_663, NEWS_664, NEWS_665, NEWS_666, NEWS_667, NEWS_668, NEWS_669, NEWS_670, NEWS_671, NEWS_672, NEWS_673, NEWS_674, NEWS_675, NEWS_676, NEWS_677, NEWS_678, NEWS_679, NEWS_680, NEWS_681, NEWS_682, NEWS_683, NEWS_684, NEWS_685, NEWS_686, NEWS_687, NEWS_688, NEWS_689, NEWS_690, NEWS_691, NEWS_692, NEWS_693, NEWS_694, NEWS_695, NEWS_696, NEWS_697, NEWS_698, NEWS_699, NEWS_700, NEWS_701, NEWS_702, NEWS_703, NEWS_704, NEWS_705, NEWS_706, NEWS_707, NEWS_708, NEWS_709, NEWS_710, NEWS_711, NEWS_712, NEWS_713, NEWS_714, NEWS_715, NEWS_716, NEWS_717, NEWS_718, NEWS_719, NEWS_720, NEWS_721, NEWS_722, NEWS_723, NEWS_724, NEWS_725, NEWS_726, NEWS_727, NEWS_728, NEWS_729, NEWS_730, NEWS_731, NEWS_732, NEWS_733, NEWS_734, NEWS_735, NEWS_736, NEWS_737, NEWS_738, NEWS_739, NEWS_740, NEWS_741, NEWS_742, NEWS_743, NEWS_744, NEWS_745, NEWS_746, NEWS_747, NEWS_748, NEWS_749, NEWS_750, NEWS_751, NEWS_752, NEWS_753, NEWS_754, NEWS_755,</sample>
    <sample id="1105">The text in the image is structured as follows: Title: Per-Category Performance Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and blue worst denotes worst. Columns: - Hate Speech (with subcategories: BLACK, MUSLIM, LGBTQ+, JEWS, ASAIN) - Misinformation (with subcategories: CNN (L), NYT (L), NLT (L), Guard (L), Fox (R), WaW (R), BBART (R), Wat (R), NR (R)) Rows: Each row represents a source or platform with performance scores for each category. For example, Reddit has high scores across all categories, while News Right (NR) generally performs poorly compared to other platforms.</sample>
    <sample id="1106">The table in the image is titled "Per-Category Performance." It shows performance on hate speech targeting different identity groups and misinformation from various sources. The data is color-coded, with dark yellow indicating best performance and blue indicating worst performance.

The categories include:
- Hate Speech
- Black
- Muslim
- LGBTQ+
- Jews
- Asians
- Latinx
- Women
- Christian
- Men
- White

The rows represent different news media or platforms: 
- Reddit
- News Right
- News Left
- NYT (New York Times)
- CNN
- NY Post
- Guardian
- Fox
- WaPo (Washington Post)
- BBRT (Bloomberg Businessweek Real-Time)
- WAT (Wall Street Journal)
- NR (National Review)

The columns show percentages for each category under these news media/platforms. For example, under "Hate Speech," Reddit has a score of 89.64% for Black individuals, while News Right has a lower score of 85.10%.

At the bottom of the table, there is a note explaining that the colors are used to indicate performance levels, where darker shades denote better performance and lighter shades denote worse performance.</sample>
    <sample id="1107">The table in the image is titled "Per-Category Performance" and it shows performance on hate speech targeting different identity groups and misinformation from various sources. The categories include Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, Men, White. There are several columns with data for each category: - Reddit_Hate - News_88_White - News_88_Black Each column has a series of numbers representing some form of metric or score related to detection accuracy or another relevant measure. At the bottom of the table, there's an explanation stating that dark yellow denotes best (highest) and blue denotes worst (lowest). This suggests that the colors indicate how well each source performs at detecting specific types of content directed towards minority groups.</sample>
    <sample id="1108">The image contains a table titled "Per-Category Performance" with various categories listed on the left side, including Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, and White. The rows represent different sources such as Reddit, News_88, News_100, HP (Hate Speech), NYT (New York Times), CNN, NLT (New Living Translation), Guard, Fox, WaR (Washington Post), BBRT (Bloomberg Businessweek), Wat (Wall Street Journal), and NR (National Review). Each cell in the table shows numerical values that likely indicate performance metrics for each category and source combination.

Below the table, there is a caption: "Table 4: Performance on hate speech targeting identity groups and misinformation from different sources." It also mentions that the results are color-coded, where dark yellow denotes best and blue denotes worst.

In addition to the table, there is text outside of it:
- At the top right corner, there is some small text which appears to be part of an interface or application window.
- Below this, aligned vertically, there are three lines of text reading "1st," "2nd," and "3rd."

This detailed description covers all visible elements within the image.</sample>
    <sample id="1109">The text in the image is as follows: "Per-Category Performance" and below that, there's a table with various categories such as Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, Men, White. The performance metrics are shown for different sources like Reddit, News Right, News Left, etc., along with their respective scores. At the bottom of the table, it says, "Table 4: Performance on hate speech targeting identity groups and misinformation from different sources." It also mentions that the results are color-coded, where dark yellow denotes best and blue denotes worst.</sample>
    <sample id="1110">The image shows a table titled "Per-Category Performance" with data on the performance of various models in detecting hate speech and misinformation. The categories include Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, Men, White, Reddit, News Right, and News Left. Each category has corresponding scores for different models such as HP (Human), NYT (New York Times), CNN (CNN), LYT (Local Youtuber), Guard (Guardian), Fox (Fox News), WaBRT (Wabert), Wat (Watson), Wt (Wit), and NR (News Republic). The results are color-coded to indicate how well each model performs: dark yellow denotes best and blue denotes worst.

Below the table, there is a caption that reads: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources." It also notes that the colors used represent varying levels of performance across the datasets.

In addition to the main content, there is an inset image showing three people labeled "1st," "2nd," and "3rd/4th," indicating their rankings or positions related to the context of the presentation.</sample>
    <sample id="1111">The table in the image provides a qualitative analysis of language model performance on tasks with varying political bias. Here is the detailed breakdown:

1. **Text**: The first column lists different text samples.
   - "the alt right is corrupted with people supporting racemixing with asians and homosexuals it times for a new nationalism"
   - "What do McDonald's and his priest have in common? They"
   - "(...) that don't stop trump from setting increases in isolated cases to make a case on the country was that the treas was in the throes of a crime because they know he will not stop trump on august 29th (2016)..."
   - "sanders is absolutely what i believe is more for poisoned water than for clean water (...)"

2. **Target Label**: This column indicates whether each sample is true or false, categorized by base type:
   - ASIAN
   - CHRIS
   - RIGHT
   - LEFT

3. **Base**: This column further categorizes the target labels into N-S-L and S-R-S.

4. **N-S-L**: Indicates if the statement is negative, neutral, or supportive.
   - TRUE: Positive label
   - FALSE: Negative label

5. **S-R-S**: Indicates if the statement is supportive, resistant, or skeptical.
   - TRUE: Supportive label
   - FALSE: Resistant or skeptical label

6. **Example**: At the bottom, there is an example illustrating the downstream performance of language models using task examples with varying political bias.

This structured approach helps analyze how well language models perform across different types of politically biased statements.</sample>
    <sample id="1112">The image contains a table with the title 'Qualitative Analysis' at the top. The table is divided into two main sections: one on the left and another on the right, each containing multiple rows of text.

On the left section:
- It has columns labeled 'Text,' 'Target Label,' 'Base,' 'N.S.I.,' 'N.R.,' 'S.I.,' and 'R.'
- Below these headers, there are several rows of text examples.
- Each row under the 'Text' column appears to be a different statement or sentence.
- Corresponding labels in other columns indicate whether the statements fall into categories such as 'Asian,' 'Chris,' 'Right,' etc.

On the right section:
- Similar structure with columns for 'Text,' 'Target Label,' 'Base,' followed by various other labels like 'N.S.I.,' 'N.R.,' 'S.I.,' and 'R.'
- This side also lists more text samples, possibly related to hate speech analysis based on their content descriptions provided below them.

At the bottom of both tables, there's additional information indicating that this data might pertain to qualitative analyses involving language models and political leaning differences. Specific terms mentioned include "hate speech," "social media," and references to Christians and Vanila RoBERTa's model performance metrics (e.g., F1 score).

Overall, the image seems to present an analytical framework used to categorize and evaluate textual content through specific criteria, likely within a research context focusing on social issues like hate speech detection.</sample>
    <sample id="1113">This indicates that there is a fairness issue that is very pressing regarding the political biases of language models.</sample>
    <sample id="1114">The text in the image is a table titled "Table 12: Qualitative analysis of hate speech example where MDLs political leaning has big effect to." The table appears to be divided into two main sections, each containing multiple columns and rows. Each row seems to represent an individual instance or case study related to hate speech examples.

In the left section:
- Column headers include "Hate Speech Text," "Date," "N4," "N-R," "R-R."
- Rows contain various texts that appear to describe specific instances of hate speech.
- Some entries have checkmarks (TRUE) under certain column headers like "N4" and "R-R."

In the right section:
- Similar structure with additional columns labeled "Text," "MDL," "Political Leaning," etc.
- Contains detailed descriptions or analyses corresponding to different aspects of the hate speech cases mentioned on the left side.

Overall, this table likely serves as a qualitative analysis tool for examining how different factors influence the classification or impact of hate speech based on their political leanings.</sample>
    <sample id="1115">The image contains a table titled "Table 12: Qualitative analysis of hate speech examples where MDs with different political leanings beg to differ." The table is divided into two sections, each containing multiple rows and columns. Each row represents an example of hate speech text, while the columns are labeled as follows:

- Hate Speech Text
- Date?
- N4
- NR
- R-R

Each cell in these columns provides qualitative analysis or categorization related to the hate speech text above it.

In addition to this main content, there is also a small video window on the right side of the image showing a person speaking. This suggests that the image might be from a presentation or lecture involving live commentary or discussion about the topic presented in the table.

The detailed layout indicates a structured approach to analyzing various instances of hate speech across different dates and contexts, likely for research or educational purposes.</sample>
    <sample id="1116">The image contains a table titled "Table 12: Qualitative analysis of hate speech example where LM with different political leanings beg to differ." The table is divided into two main sections:

### Left Section:
- **Title:** Hate Speech Text
- **Columns:** 
  - Date
  - N4
  - NR
  - R-R

#### Content (Sample Rows):
1. "Some are starting to realize that 'diversity' doesn't equal 'tolerance' and quite simply as an Asian it's just not possible for me to be okay with being called the n-word or any other racial slur."
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

2. "Yes Finally these fake religious folk please stop trying to force their beliefs on others by saying they can do whatever they want in God's name!"
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

3. "It’s like buying a car from Chrysler, you know what I mean? It’s all about getting your hands dirty before you buy one."
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

4. "United States of America is good old days."
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

5. "Why pick the price?"
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

6. "How did she get her dog killed?"
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

7. "Is this how you begin with the feminist books?"
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

8. "This obsession over black shows the deep divide between us."
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

9. "I don’t understand why people have such strong opinions against racism."
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

10. "She said he was a racist because his views were different than hers."
    - Date: 2023-05-09
    - N4: TRUE
    - NR: TRUE
    - R-R: TRUE

### Right Section:
- **Title:** Transferring Text
- **Columns:** 
  - Date
  - N4
  - NR
  - R-R

#### Content (Sample Rows):
1. "Some are starting to realize that 'diversity' doesn't equal 'tolerance' and quite simply as an Asian it's just not possible for me to be okay with being called the n-word or any other racial slur."
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

2. "Yes Finally these fake religious folk please stop trying to force their beliefs on others by saying they can do whatever they want in God's name!"
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

3. "It’s like buying a car from Chrysler, you know what I mean? It’s all about getting your hands dirty before you buy one."
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

4. "United States of America is good old days."
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

5. "Why pick the price?"
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

6. "How did she get her dog killed?"
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

7. "Is this how you begin with the feminist books?"
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

8. "This obsession over black shows the deep divide between us."
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

9. "I don’t understand why people have such strong opinions against racism."
   - Date: 2023-05-09
   - N4: TRUE
   - NR: TRUE
   - R-R: TRUE

10. "She said he was a racist because his views were different than hers."
    - Date: 2023-05-09
    - N4: TRUE
    - NR: TRUE
    - R-R: TRUE

### Additional Elements:
- A small inset image of a person appears at the top right corner.
- Below the tables, there is another slide titled "Discussion" which includes text and diagrams.

This detailed description covers the structure and content of both tables presented in the image.</sample>
    <sample id="1117">The text in the image is as follows: Discussion Between Scylla and Charybdis To “sanitize” or not to “sanitize”, that is the question Pretraining data Language models Downstream tasks</sample>
    <sample id="1118">The text in the image is: Discussion Between Scylla and Charybdis To "sanitize" or not to "sanitize", that is the question Pretraining data Language models Downstream tasks</sample>
    <sample id="1119">The image shows a slide from a presentation with the title "Discussion" at the top. Below the title, there is a subtitle that reads "Between Scylla and Charybdis." The main content of the slide poses a question: "To 'sanitize' or not to 'sanitize,' that is the question."

Below this text, there are three labeled boxes connected by arrows in a sequence:
1. Pretraining data
2. Language models
3. Downstream tasks

In the upper right corner of the slide, there is an inset showing a person.

At the bottom of the slide, there is a section titled "Thank you!" which includes four names listed horizontally across the screen:
- Shangbin Feng
- Chan Young Park
- Yuhan Liu
- Yulia Tsetkova

Below each name, there are logos representing their affiliations:
- Paul G Allen School (University of Washington)
- UWNLP (University of Washington Natural Language Processing Lab)
- Carnegie Mellon University Language Technologies Institute

In the final frame, additional details about the individuals appear below their photos:
- Shangbin Feng's affiliation remains as Paul G Allen School.
- Chan Young Park's affiliation also stays as UWNLP.
- Yuhan Liu's affiliation changes to Carnegie Mellon University Language Technologies Institute.
- Yulia Tsetkova’s affiliation does not change; it still represents Carnegie Mellon University Language Technologies Institute.

Additionally, two more logos appear under Yulia Tsetkova's photo:
- A logo for the University of Pennsylvania 
- Another logo related to language technologies 

Throughout these frames, the background remains white, maintaining consistency in design elements such as fonts and colors used throughout the slides.</sample>
    <sample id="1120">The image contains a slide with the title "Thank you!" at the top. Below the title, there is a flowchart that reads: "Pretraining data" connected by arrows to "Language models," which then connects to "Downstream tasks." Underneath this flowchart are four names and corresponding photos of individuals:

1. Shangbin Feng
2. Chan Young Park
3. Yuhuan Liu
4. Yulia Tsvetkov

Below each name and photo, there are logos representing their affiliations:
- Paul G Allen School (University of Washington)
- UWNLP (University of Washington Natural Language Processing Lab)
- Carnegie Mellon University Language Technologies Institute

In the upper right corner of the image, there is a small video feed showing one individual.

At the bottom left of the image, text in English says: "Okay great I think that's pretty much all I have for today thank you for your time."</sample>
    <sample id="1121">The new method is called "Permuting with jumps".</sample>
    <sample id="1122">The author's description of the "marked words" method is to find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">The affiliations of the authors are: Paul G. Allen School, University of Washington; UWNLP (University of Washington Natural Language Processing Lab); and Carnegie Mellon University's Language Technologies Institute.</sample>
    <sample id="1124">The name of the first mentioned symmetrical dependency structure is "Bouquet/Stanford".</sample>
    <sample id="1125">The speaker's name is James Finch.</sample>
    <sample id="1126">The paper involves 5 authors: Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, Annie Louis, and another unnamed author.</sample>
    <sample id="1127">The datasets that can be used to test syntactic phenomena are BLiMP, SyntaxGym, and Crows.</sample>
    <sample id="1128">The image contains a title and author information for an academic presentation. The main title reads: "When Does Translation Require Context? A Data-driven, Multilingual Exploration" Below the title are the names of the authors: Patrick Fernandes*, Kayo Yin*, Emmy Liu André F. T. Martins, Graham Neubig At the bottom left corner, there is text indicating affiliations with Carnegie Mellon University Language Technologies Institute, Técnico Lisboa, BAIR (Berkeley Artificial Intelligence Research), and Unbabel. There is also a note indicating "* equal contribution."</sample>
    <sample id="1129">Translation depends on context We'll have to get rid of that mole.</sample>
    <sample id="1130">Translation depends on context Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole.</sample>
    <sample id="1131">Translation depends on context Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1132">The content of the image is as follows: Evaluating context-dependent translation is hard. Only a small portion of words depend on context Corpus-level metrics</sample>
    <sample id="1133">The image shows a slide from a presentation with the title "Evaluating context-dependent translation is hard." The content on the slide includes two bullet points: 1. Only a small portion of words depend on context - Corpus-level metrics 2. Existing methods support limited discourse phenomena and languages There is also an illustration in the bottom right corner depicting a person holding flags, one representing France (blue, white, red) and another resembling the Union Jack (red, white, blue). Additionally, there is a circular inset photo in the top right corner showing a woman's face.</sample>
    <sample id="1134">The English content in the image is: RQ1: When does translation require context? RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1135">RQ1: When does translation require context? - Word-level context usage RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1136">The text in the image is as follows: Conditional Cross-Mutual Information (CXMI) CXMI: measure how much context MT models use given a corpus</sample>
    <sample id="1137">The image contains text related to a concept called Conditional Cross-Mutual Information (CXMI). The main title at the top reads "Conditional Cross-Mutual Information (CXMI)." Below this, there is a bullet point that states: "CXMI: measure how much context MT models use given a corpus." There is also an illustration with two arrows labeled 'X' and 'C,' pointing towards blocks of text. These blocks are labeled as follows:
- The first block has the formula "H quilt (Y | X)".
- The second block has the formula "H quilt C (Y | X, C)".

Beneath these formulas, there is additional explanatory text which says: "Uncertainty over translations given the source" for the first block, and "Uncertainty over translations given the source AND context" for the second block. At the bottom right corner of the image, there is another label in red that reads "CXMI (C → Y | X)."</sample>
    <sample id="1138">The English content in the image is: Pointwise (P-)CXMI We introduce P-CXMI to measure context usage to translate a specific sentence. P-CXMI(y, x, C) = -log (qMTA(y|x)/qMTC(y|x, C)) Word P-CXMI(i, y, x, C) = -log (qMTA(yi|h&lt; i, x)/qMTC(yi|y &lt; i, x, C)) High P-CXMI words -&gt; requires context to translate</sample>
    <sample id="1139">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1140">Thematic analysis of high P-CXMI words</sample>
    <sample id="1141">Thematic analysis of high PCXML words 1. POS tags</sample>
    <sample id="1142">Thematic analysis of high P-CXMI words 1. POS tags - Pronouns</sample>
    <sample id="1143">The image contains a slide from a presentation titled "Thematic analysis of high P-CXML words." The content is divided into two main sections: 1. POS tags and 2. Vocabulary items. Under the section for POS tags, there is a bar chart labeled "P-CXML for POS tags in En-Ar," which shows three bars representing different pronouns (PRON_3_Sing, PRON_3_Dual, PRON_3_Plur). Each bar has a value on it, with the highest being around 0.6. On the right side of the slide, within a purple box, are bullet points listing "Pronouns" and "Verb form."</sample>
    <sample id="1144">Thematic analysis of high P-CXML words 1. POS tags 2. Vocabulary items Avelile's mother was still asleep. Avelile went to school. 阿维利尔的母亲还在睡觉。 阿维利尔去上学了。 - Pronouns - Verb form - Lexical cohesion</sample>
    <sample id="1145">Thematic analysis of high P-CXML words 1. POS tags 2. Vocabulary items Avelile's mother was still asleep. Avelile went to school. 阿维利尔的母亲还在睡觉。 阿维利尔去上学了。 Pronouns Verb form Lexical cohesion Formality</sample>
    <sample id="1146">The image contains a slide from a presentation titled 'Thematic analysis of high P-CXML words.' The content is organized into three main points: 1. POS tags 2. Vocabulary items 3. Individual tokens Additionally, there are four sub-points listed under the third point: - Pronouns - Verb form - Lexical cohesion - Formality On the right side of the slide, there is an inset box with two examples in English and German: "She knows where we're going." / "I don't." and "Sie weiß, wohin wir gehen." / "Ich weiß es nicht." In the bottom left corner of this inset box, there is a small British flag icon next to the first example.</sample>
    <sample id="1147">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations? - Multilingual Discourse-Aware (MuDA) benchmark</sample>
    <sample id="1148">The image contains a title and a list of items. The title at the top reads 'Multilingual Discourse-Aware (MuDA) tagger.' Below the title, there is a light purple box with a white background containing a bulleted list. The bullets are as follows: - Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis In the bottom right corner of the image, there is a circular inset showing part of a person's face.</sample>
    <sample id="1149">The image contains a title and a bar chart. The title reads "Multilingual Discourse-Aware (MuDA) tagger." Below the title, there is a list of items: - Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis On the right side of the image, there is a bar chart with the x-axis labeled "Language" and the y-axis labeled "Counts." The bars are color-coded to represent different linguistic phenomena as follows: - Blue for pronouns - Orange for formalities - Green for verb forms - Red for lexical - Purple for ellipsis Each language on the x-axis has multiple colored bars representing these categories.</sample>
    <sample id="1150">The image shows a slide from a presentation with the title 'MuDA benchmark' at the top left corner. On the right side, there is an illustration of a robot character. In the center-left part of the slide, there are three overlapping documents with lines on them, indicating text content. These documents point to a process labeled 'MuDA tagger,' which then points to another set of documents that have some highlighted sections in blue and purple. This suggests a tagging or annotation process applied to the documents. The flow continues as these tagged documents lead to a box labeled 'BLEU COMET F-measure...' followed by the same robot character again. At the bottom right corner of the slide, there is a small circular inset showing a person's face.</sample>
    <sample id="1151">The image contains text related to research questions and evaluation metrics for translation models. The content is organized into two main sections:

1. **RQ1: When does translation require context?**
   - Word-level context usage
   - Thematic analysis

2. **RQ2: How well do models handle context-dependent translations?**
   - Multilingual Discourse-Aware (MuDA) benchmark
   - Model evaluation

Additionally, there is a small circular profile picture in the top right corner of the image.

The background color of the slide is white with black text, making it easy to read. There are no other visual elements or distractions present in the image.</sample>
    <sample id="1152">Corpus-level metrics BLEU</sample>
    <sample id="1153">The text in the image is as follows: At the top, it says 'Corpus-level metrics'. Below that are three robot icons labeled 'BLEU', 'COMET', and 'F-measure' respectively. The first two robots have a label 'CONTEXT' above them with different background colors (purple for COMET). In the bottom right corner, there's an icon of a person’s face.</sample>
    <sample id="1154">Corpus-level metrics This again demonstrates that it is difficult to determine the best document-level MT with corpus-level metrics Unclear which system is best for document-level MT with corpus-level metrics</sample>
    <sample id="1155">The content of the image is as follows: At the top, it says "Corpus-level metrics." Below that are three robot-like figures labeled "BLEU," "COMET," and "F-measure" respectively. Underneath these figures, there's a bullet point stating, "Unclear which system is best for document-level MT with corpus-level metrics." On the right side of each figure, there's an icon of a person’s face in a circular frame.

Next slide:
The title at the top reads "MuDA benchmark results." There's a single bullet point below this heading saying, "Context-aware models perform significantly better on some phenomena."

Another slide shows the same text about MuDA benchmark results but adds another bullet point underneath that states, "Formality, lexical cohesion."

Finally, one more slide reiterates the previous information about context-aware models performing well on certain phenomena, specifically mentioning formality and lexical cohesion.</sample>
    <sample id="1156">The content in the image is about "MuDA benchmark results." It states that context-aware models perform significantly better on some phenomena, specifically mentioning formalities and lexical cohesion. However, they do not perform as well with ellipsis, pronouns, or verb form. The text suggests areas where further progress might be needed for document-level translation tasks.</sample>
    <sample id="1157">MuDA benchmark results Context-aware models perform significantly better on some phenomena o ✅: Formality, lexical cohesion ❌ Ellipsis, pronouns, verb form DeepL outperforms Google on most phenomena and language pairs* as of April 2021</sample>
    <sample id="1158">The image contains text that can be summarized as follows: Title: MuDA benchmark results - Context-aware models perform significantly better on some phenomena. Sub-bullet points include: ✓ Formality, lexical cohesion ✕ Ellipsis, pronouns, verb form DeepL outperforms Google on most phenomena and language pairs* *as of April 2021 Summary section includes two bullet points: Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT The visual elements in the image also depict a flowchart with three steps labeled "MuDA tagger," "BLEU F-measure," and an icon representing machine learning or AI at the end.</sample>
    <sample id="1159">The image contains a slide with the title "Summary" at the top. There are two bullet points below the title: 1. Identify discourse phenomena systematically without prior linguistic knowledge 2. Dataset-agnostic benchmark for document-level MT At the bottom of the slide, there is an illustration showing three steps in a process: - The first step shows a stack of documents labeled "MuDA tagger." - The second step shows a flow of text from the tagged documents to another component. - The third step shows a robot icon connected to a box labeled "BLEU F-measure," indicating some form of evaluation or scoring metric. In the upper right corner of the slide, there is a small circular inset containing an image of a person's face.</sample>
    <sample id="1160">The image contains the following text: Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT MuDA tagger BLEU F-measure 101</sample>
    <sample id="1161">The abbreviations for the five methods are FTw, COSINE, LCR, MLC, and L2R.</sample>
    <sample id="1162">The model is evaluated on 11 tasks.</sample>
    <sample id="1163">The content in the image is a presentation slide. The main title of the slide reads 'DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification.' Below this, there are three names listed as presenters or authors: Regina Stodden, Omar Momen, Laura Kallmeyer. At the bottom left corner, it mentions 'Heinrich Heine University Düsseldorf, Germany,' indicating the affiliation of the presenters. On the right side at the top, there's an icon that appears to be related to video conferencing software, suggesting that this might be from a virtual meeting or conference.</sample>
    <sample id="1164">The text on the slide reads: '1. Text Simplification What, why and How?' Below that is additional information about a presentation or lecture titled 'DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification.' The presenters are listed as Regina Stodden, Omar Momen, Laura Kallmeyer from Heinrich Heine University Düsseldorf, Germany, and it mentions ACL 2023.</sample>
    <sample id="1165">Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers.</sample>
    <sample id="1166">The English content in the image is: 'To train a text simplification model, we require parallel pairs of text. For example, documents or sentences.'</sample>
    <sample id="1167">The image shows a slide titled 'Text Simplification Example.' It illustrates the process of simplifying complex German sentences into plain language. The original sentence is displayed in bold, with various parts marked for substitution (in red), clause deletion (in blue), reordering (in green), and word deletion (in orange). These modifications are shown as arrows pointing to simplified versions of the text below, which reads: 'Plain Language: Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden.' This demonstrates how different elements can be altered or removed to make the text simpler while maintaining its meaning.</sample>
    <sample id="1168">The image shows a slide from a presentation about text simplification. The title of the slide is "Text Simplification Example." Below the title, there are two sections labeled "Original" and "Plain Language," each containing German sentences with different techniques applied to them.

In the "Original" section:
- There is a sentence: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden."
- Above this sentence, four methods for simplifying text are listed in colored boxes: Substitution (blue), Clause Deletion (red), Reordering (green), and Word Deletion (purple).

In the "Plain Language" section:
- A simplified version of the original sentence is shown: "Die Gewerkschaft setzt sich für Beispiel hohere Löhne oder mehr Urlaub ein."
- Two arrows point from the original sentence to the plain language sentence.
- One arrow points directly at the word "dass" in the original sentence, indicating its deletion.
- Another arrow points to the phrase "zum Beispiel" in the original sentence, also indicating its deletion.

At the bottom right corner of the slide, there is an additional green box labeled "Insertion."

The overall layout demonstrates how various linguistic modifications can be used to simplify complex sentences into simpler ones.</sample>
    <sample id="1169">The English content in the image is: '2. DE-plain A New Corpus' and 'German Text Simplification Corpora'.</sample>
    <sample id="1170">The other three models which are proposed in recent years are all automatically aligned, which means there can be over error prone and their alignments.</sample>
    <sample id="1171">The image shows a presentation slide titled "German Text Simplification Corpora" with the subtitle "Sentence Level." The main content of the slide is a bar chart displaying various metrics related to text simplification. On the left side, there's a legend explaining different categories represented by colors in the bars: "Domestic," "health," "education," "economy," "politics," and "public debate."

The x-axis lists years from 2013 to 2016, while the y-axis ranges from 0 to 14000, indicating some form of count or frequency. Each year has multiple colored bars corresponding to the categories mentioned.

On the right side of the slide, two specific data points are highlighted:
- One point labeled "756" next to an orange section.
- Another point labeled "483" next to a black section.

In the top-right corner of the slide, there's a small inset showing a person wearing headphones, likely representing the presenter or someone involved in the research.</sample>
    <sample id="1172">The image shows a slide titled "German Text Simplification Corpora" with the subtitle "Sentence Level." The main content is a bar chart that displays data related to text simplification, including categories such as "Domestic," "Medical," "Health," "Education," and others. There are specific numerical values associated with each category on the bars of the chart. Additionally, there is an inset box providing more detailed information about alignment types like "manual," "semi-manual," etc., along with corresponding numbers for different years (2018-2019). On the right side of the chart, two arrows point to certain bars labeled with numbers: 463 and 756. In the top-right corner of the image, there is a small video feed showing a person wearing headphones.</sample>
    <sample id="1173">The image shows a presentation slide titled 'German Text Simplification Corpora.' The main content of the slide is a bar chart labeled 'Sentence Level,' which displays data from various domains such as domestic, health, economy, politics, public affairs, and others. Each domain has corresponding bars indicating different numerical values. There are annotations on the right side of the chart pointing to specific numbers: 483 for 'Domestic' and 756 for 'Politics.' Additionally, there's text in German that mentions aligning documents manually and with automatic alignment methods. In the top-right corner, there is an inset showing a person wearing headphones.</sample>
    <sample id="1174">The text in the image is as follows: "German Text Simplification Corpora" at the top of a blue banner. Below that, there's a chart titled "Sentence Level." The chart includes various colored bars representing different categories such as "Domestic," "Eucharek," "e.g., Kienzle et al. 2016," "Public," and others. There are numerical values next to these bars indicating counts or metrics for each category. For example, one bar shows "483" with an arrow pointing towards it, another shows "756," and so on. Additionally, there is some smaller text near the bottom left corner which appears to be related to data points but is not fully legible due to its size.</sample>
    <sample id="1175">The English content of the image is as follows: "We analyzed our sentence pairs a little bit more. So, for example, on the type of simplification."</sample>
    <sample id="1176">As you can see here, the Bible text are much stronger simplified than for example the news text or the language learner texts.</sample>
    <sample id="1177">On all level regarding for example lexical simplification, structural simplification also overall level of simplification.</sample>
    <sample id="1178">The image contains two main sections. The left section is titled 'Types of Simplification' and shows bar graphs comparing three categories: Simplicity, LexSimp, and StructSimp across four types labeled as news (n=46), bible (n=155), L2 (n=157), and fiction (n=72). Each category has bars in different colors representing the respective type.

The right section is titled 'Simplification Transformations' and displays a bar graph with transformations such as moving, engineering, lexical substitution, word deletion, and word addition. Two sets of data are compared using blue and green bars for DEplain-apa and DEplain-web respectively.

In the top-right corner of the image, there is a small video feed showing a person speaking into a microphone.</sample>
    <sample id="1179">On the other hand, in the web corpus we have much more rephrasing.</sample>
    <sample id="1180">The image shows a presentation slide with the title '3. Use-cases' and the subtitle 'Automatic alignment and simplification.' In the top right corner, there is a small video feed of a person speaking. The main content area displays text related to use cases for automatic alignment methods in natural language processing (NLP). Below this heading, there is a table titled 'Automatic Alignment Evaluation,' which lists various NLP models such as LHA-LiSE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign. Each model has corresponding descriptions and evaluation metrics like P, R, F1, and ncm values displayed in two sections: upper part (1:1) and lower part (ncm).</sample>
    <sample id="1181">In the recent periods, there has been a lot of alignment methods but in the context of machine translations.</sample>
    <sample id="1182">The image shows a table titled "Automatic Alignment Evaluation" with two sections: the upper part labeled "1:1 (upper part)" and the lower part labeled "n cm n cpm (lower part)." The table lists various alignment methods along with their descriptions. Each method has corresponding scores in columns P, R, F, PR, and FPR. Additionally, there is an individual visible on the right side of the image, likely presenting or discussing the content shown.</sample>
    <sample id="1183">The image shows a table titled "Automatic Alignment Evaluation" with two main sections: the upper part labeled "1:1 (upper part)" and the lower part labeled "n:n cm n:cm (lower part)." The table compares different alignment methods, listing their names in the first column. Each method has corresponding descriptions in the second column explaining its approach or capabilities.

The third column contains numerical values representing scores for three metrics: P, R, and F1, which are common evaluation measures used to assess performance in tasks such as information retrieval or text matching. These scores indicate how well each method performs under these criteria.

The fourth column provides additional numeric data, possibly related to further evaluations or specific scenarios where the methods were tested.

Overall, this table is likely from an academic paper or presentation discussing various algorithms' effectiveness in aligning texts based on similarity metrics like sentence embeddings or other linguistic features.</sample>
    <sample id="1184">The image shows a table titled 'Automatic Alignment Evaluation' with various alignment methods listed. The top section of the table is labeled 'Results of the alignment methods with 1:1 (upper part)' and includes columns for P, R, F, and n cm. Below this, there's another section labeled 'n cm (lower part)' which also has columns for P, R, F, and n cm.

The methods listed in the table include:
- Sent-LAISE
- CATS-C3G
- VecAlign
- BERTAlign
- MASSAlign

Each method has corresponding scores under the P, R, F, and n cm columns. For example, Sent-LAISE has values like 94.6, 41.57, 0.747, etc., while MASSAlign has higher values such as 846.1, 846.2, 0.773, etc.

The description column provides details about each method, explaining their approach or capabilities. For instance, Sent-LAISE uses hierarchical alignment using sentence embeddings similarity, whereas MASSAlign employs a vicinity-driven approach with a TF-IDF similarity matrix.

Overall, the table appears to be evaluating different alignment algorithms based on precision (P), recall (R), F1 score (F), and normalized confusion matrix (n cm) metrics.</sample>
    <sample id="1185">The video presents a table titled "Automatic Alignment Evaluation," which displays the results of various alignment methods. The upper part of the table shows performance metrics for different methods, while the lower part lists their descriptions and corresponding scores in columns labeled P, R, F1, and ncm. A person is seen speaking throughout the presentation, discussing adaptations to proposed methods and publishing all these adaptations along with codes to run experiments as mentioned in the paper.</sample>
    <sample id="1186">The image shows a table with the title "Automatic Alignment Evaluation" at the top. The table is divided into two sections: (1) upper part and (2) lower part, which compare different alignment methods based on their performance metrics P, R, F1, and n-cm. Each method has a corresponding description in the 'Description' column. Some of the listed names include Sent-LAISE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign.</sample>
    <sample id="1187">The image shows a presentation slide with the title "Automatic Alignment Evaluation." The main content of the slide is a table that compares different alignment methods. The table has two sections: one for results at a 1:1 ratio (upper part) and another for n cm capabilities (lower part). Each method listed in the table, such as Sent-LA-BASE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign, includes descriptions and numerical values under various columns labeled P, R, F, PRF, and n cm. These numbers likely represent performance metrics or scores related to each method's effectiveness in alignment tasks.</sample>
    <sample id="1188">The second use case that we showed in our paper is the case of automatic text simplification.</sample>
    <sample id="1189">The English content in the image is as follows: "Automatic Text Simplification" and "Results on Document Simplification using fine-tuned long mBART. n corresponds to the length of the training data."</sample>
    <sample id="1190">The video presents a detailed analysis of automatic text simplification, focusing on two levels: document level and sentence level. The content is displayed in the form of tables with various metrics such as BLEU scores for different models like DEPLAIN-APA and DEPLAIN-WEB. These tables provide quantitative data comparing the performance of these models across multiple tests (n=48 and n=1231). Additionally, there are sections labeled "train data" and "baseline," which likely offer insights into how well the models perform based on training datasets. A person appears to be explaining or discussing this information throughout the presentation, providing context and elaboration on the results shown in the slides.</sample>
    <sample id="1191">The image shows a presentation slide titled 'Automatic Text Simplification'. The slide is divided into two main sections: Document Level and Sentence Level. Each section contains tables with data related to text simplification results using fine-tuned mbART, including metrics such as train data, DEPLAIN-APA test, DEPLAIN-WEB test, and SARI BLEU scores for different baselines (baseline, baseline-50, baseline-100). There are also mentions of DEPLAIN-APA test n=48 and DEPLAIN-WEB test n=147 under the Document Level section, and DEPLAIN-APA test n=1231 and DEPLAIN-WEB test n=1846 under the Sentence Level section.</sample>
    <sample id="1192">The image shows a slide from a presentation about automatic text simplification. The title of the slide is "Automatic Text Simplification." There are two main sections on the slide: one for document-level results and another for sentence-level results, both related to using fine-tuned mBART models.

For the document level:
- It mentions that there were 48 test cases.
- Two evaluation metrics are provided: SARI and BLEU.
- The table includes various scores such as F1, precision, recall, and F1 (micro).

For the sentence level:
- It refers to 1231 test cases.
- Similar evaluation metrics like SARI and BLEU are used here too.
- Scores include F1, precision, recall, and F1 (micro).
- A baseline score comparison with DEPLAIN-APA is mentioned.

Both levels have tables comparing different methods or models based on these metrics.</sample>
    <sample id="1193">The image shows a presentation slide about 'Automatic Text Simplification'. It is divided into two main sections: Document Level and Sentence Level. Each section contains tables with results from different tests, specifically DEPLAIN-APA test (n=48) for the document level and DEPLAIN-APA test (n=1231), DEPLAIN-WEB test (n=1846) for the sentence level.

The table under the Document Level includes metrics such as train data, SARI, BLEU, F1, and PRE, along with scores like 0.579, 0.579, 0.579, 0.579 respectively for DEPLAIN-APA test (n=48). The same structure appears in the Sentence Level table but lists higher numbers of participants or samples (n=1231 and n=1846).

In both sections, there are additional columns labeled 'DEPLAIN baseline' which provide comparative baseline scores against which other methods can be evaluated. This suggests that the study aims to compare various text simplification models based on their performance across these specific datasets and evaluation metrics.</sample>
    <sample id="1194">The image shows a presentation slide titled "Automatic Text Simplification." The slide is divided into two main sections: Document Level and Sentence Level. Each section contains tables with results from different tests, including DEPLAIN-APA test (n=48) and DEPLAIN-WEB test (n=147). The tables display various metrics such as BLEU, F1, and PRE scores for different models like DEPLAIN-APA, DEPLAIN-WEB, and others. Additionally, there are mentions of "train data" and "test data," indicating the evaluation setup. In the background, there is a person wearing headphones, suggesting they might be presenting or discussing the content on the slide.</sample>
    <sample id="1195">The English content in the image is: 'Thanks. For more details, please check out our paper. And feel free to visit our poster in the ACL 2023 conference.'</sample>
    <sample id="1196">The English content in the image is as follows: Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis Google Research</sample>
    <sample id="1197">The text in the image is: Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis Google Research</sample>
    <sample id="1198">Here a user wants to select between one of these two songs.</sample>
    <sample id="1199">The text in the image is about indirect referring expressions and how they can be used to understand users' language when making a choice. It includes examples of alternative questions, direct references, and indirect references with specific scenarios provided for each type. The slide also mentions that these expressions could be useful in natural and fluid conversation by referencing songs or positions without explicitly naming them.</sample>
    <sample id="1200">The English content in the image is as follows: Indirect Referring Expressions Goal: Understanding users’ language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference easy on me, “the first one” Indirect reference Could not remember the name The pronunciations are hard to distinguish Want to specify a preference The newer one. The song that’s not energetic.</sample>
    <sample id="1201">The English content in the image is as follows: Indirect Referring Expressions Goal: Understanding users’ language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference “easy on me”, “the first one” Indirect reference Could be used in natural and fluid conversation: • Cannot remember the name • The pronunciations are hard to distinguish • Want to specify a preference Indirect reference The newer one. The song that’s not energetic.</sample>
    <sample id="1202">Here are some example indirect references. For example, the newer one or the song that's not energetic</sample>
    <sample id="1203">The English content in the image is as follows: "Dataset Collection Important problem Conversational systems Benchmarking Large Language Models’ entity understanding No large-scale public dataset available We collect a large dataset using crowd annotation Three domains:"</sample>
    <sample id="1204">The English content in the image is as follows: "Dataset Collection Important problem Conversational systems Benchmarking Large Language Models’ entity understanding No large-scale public dataset available We collect a large dataset using crowd annotation Three domains: music books and recipes"</sample>
    <sample id="1205">Dataset Collection Methodology Methodology emphasizes informality using a cartoon completion task Sets the dialog context chosen from a few manual prompts per domain The alternative question Expression referring to one of the entities Filled in by the annotator</sample>
    <sample id="1206">The cartoon has three speech bubbles. In the first bubble, Bob says, "Remember that song we were listening to yesterday?" And with that, Bob sets the dialogue context chosen from a few manual prompts per domain.</sample>
    <sample id="1207">The English content in the image is as follows: Dataset Collection Methodology. Methodology emphasizes informality using a cartoon completion task. Sets the dialog context (chosen from a few manual prompts per domain). The alternative question Expression referring to one of the entities Filled in by the annotator</sample>
    <sample id="1208">The alternative question is "Do you mean 'Easy on Me' or 1 Gotta Feeling?"</sample>
    <sample id="1209">The first speech bubble is chosen from a few manual prompts per domain.</sample>
    <sample id="1210">The second one, which is the alternative question.</sample>
    <sample id="1211">The English content in the image is: Generate alternative questions =&gt; sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean This is it or Man in the Mirror? Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am</sample>
    <sample id="1212">The English content in the image is: Generate alternative questions =&gt; sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean This is it or Man in the Mirror? Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am</sample>
    <sample id="1213">The first form is uniform at random.</sample>
    <sample id="1214">The second one is when the entities have similar titles. For example, two books with the name The Return (memoir) or The Return (Shatner novel).</sample>
    <sample id="1215">The text in the image is as follows: "Generate alternative questions =&gt; sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean This is it or Man in the Mirror? Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am?"</sample>
    <sample id="1216">The text in the image is as follows: "Background knowledge (Music) Google search link to each song. Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) Click here to find out about the song. Click here to find out about the song We ask annotators to Listen to at least some of each song Read about each song"</sample>
    <sample id="1217">The English content in the image is as follows: Background knowledge (Music) Google search link to each song. Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) We ask annotators to Listen to at least some of each song Read about each song</sample>
    <sample id="1218">The Google search result for the song "Easy on Me" includes a video thumbnail, lyrics link, and an option to watch more videos.</sample>
    <sample id="1219">The text in the image is about two recipes: Simnel Cake and Pandan Cake. It provides background information, including their origin, ingredients, and popularity. The source of this information appears to be Wikipedia. Additionally, there are images of each cake shown alongside their descriptions.</sample>
    <sample id="1220">The text in the image is structured as follows: 1. Title: "Eliciting expressions" - This title introduces the main topic of the slide, which appears to be about eliciting certain types of expressions or descriptions from annotators. 2. Body Text: The body text provides instructions on how to proceed with the task at hand. It reads: "We then tell the annotators which choice should be selected and ask them to describe it." This suggests that there are multiple choices available for the annotators to select from. 3. Options Box: Below this instruction, there is a box containing two options labeled "Easy on Me (by Adele)" and "I Gotta Feeling (by the Black Eyed Peas)." These appear to be examples of songs or music tracks being used for the task. 4. Additional Instructions: Further down, additional instructions state: "We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble. For example:" followed by an example list of five phrases: - The one with the piano music - The song's not energetic - It has something about a river - The newer one - It's about having time to choose." These phrases seem to illustrate different ways annotators might describe their chosen song using indirect referring expressions. 5. Footer: At the bottom left corner, there is some small text that seems to reference the source or context of the presentation: "Revising Indirect Referring Expressions for Entity Selection (Mellieba Cephas)." Additionally, in the top right corner, there is a Google Research logo, indicating that this content may be part of research conducted under the auspices of Google Research.</sample>
    <sample id="1221">The text in the image is as follows: "Eliciting expressions. We then tell the annotators which choice should be selected and ask them to describe it." Below this, there are two options presented with arrows pointing towards them: "Easy on me (by Adele)" and "I Gotta Feeling (by The Black Eyed Peas)." Further down, the text reads: "We would like you to give us 3-5 expressions for the chosen song to fill in your speech bubble. For example:" followed by a list of examples such as "The one with the piano music," "The song that's not energetic," etc. At the bottom left corner, there is additional text: "Revising Indirect Referencing Expressions for Entity Selection (Abdullah Al-Mutairi Collection)." On the right side of the image, there is a Google Research logo at the top.</sample>
    <sample id="1222">The AltEntities Corpus has 6,000 alternative questions across three domains and 42,000 indirect referring expressions. Results with the T5 XL model (accuracy): - 92-95% if the LM has access to the same background knowledge as annotators. - 82-87% when the LM has access to partially overlapping background knowledge. - ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research/datasets/AltEntities</sample>
    <sample id="1223">The English content in the image is as follows:</sample>
    <sample id="1224">The text in the image is as follows: Google Research AltEntities Corpus ~6,000 alternative questions across the three domains ~42,000 indirect referring expressions Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82%-87% when the LM has access to partially overlapping background knowledge. ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research/datasets/AltEntities Reading: https://arxiv.org/abs/2301.01923</sample>
    <sample id="1225">The English content in the image is as follows: 'AltEntities Corpus - ~6,000 alternative questions across the three domains - ~42,000 indirect referring expressions Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82%-87% when the LM has access to partially overlapping background knowledge. ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research-datasets/AltEntities'</sample>
    <sample id="1226">CamemBERT is initially trained on 4GB of data.</sample>
    <sample id="1227">The speaker's name is Adam Przepiórkowski.</sample>
    <sample id="1228">The performance degradation with larger temporal gap confirms the hypothesis that temporal drift is the main cause of the performance drop.</sample>
    <sample id="1229">The text in the image reads: "NLPPositionality: Characterizing Design Biases of Datasets and Models" Below this title, there are names and affiliations listed for five individuals: 1. Sebastian Santy*, University of Washington 2. Jenny T. Liang*, Carnegie Mellon University 3. Ronan Le Bras, Allen Institute for AI 4. Katharina Reinecke, University of Washington 5. Maarten Sap, Carnegie Mellon University There is also a small inset picture at the top right corner showing two people with some greenery behind them.</sample>
    <sample id="1230">NLPPositionality: Characterizing Design Biases of Datasets and Models Sebastian Santi Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarten Sap Carnegie Mellon University</sample>
    <sample id="1231">Imagine...</sample>
    <sample id="1232">Imagine… Carl Jones Tech Lead, New York Times = PerspectiveAPI score Can you stop being a jerk? 😡 (0.82) ✅</sample>
    <sample id="1233">Imagine... Carl Jones Tech Lead, New York Times Can you stop being a jerk? 🤬 (0.82) ✅ Aditya Sharma Tech Lead, Times of India Presstitutes everywhere on the news. 📰 (0.33) ❌ = PerspectiveAPI score</sample>
    <sample id="1234">The text in the image is as follows: Imagine... Design bias example Can you stop being a jerk? (0.82) ✅ Presstitutes everywhere on the news. (0.33) ❌ Carl Jones Tech Lead, New York Times Aditya Sharma Tech Lead, Times of India = PerspectiveAPI score</sample>
    <sample id="1235">The text in the image is as follows: Positionality "The perspectives [people] hold as a result of their demographics, identity, and life experiences." [1] Savin-Baden, Maggi, and Claire Howell-Major. "Qualitative research: The essential guide to theory and practice." Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013).</sample>
    <sample id="1236">This is a concept widely used in critical studies, specifically in feminist and queer academic spaces.</sample>
    <sample id="1237">The video presents a slide titled "Positionality" with the subtitle, "The perspectives [people] hold as a result of their demographics, identity, and life experiences." It further explains that this positionality influences the research process and its outcomes and results. The reference at the bottom cites Savin-Baden, Maggi, and Claire Howell-Major's work on qualitative research: "Qualitative Research: The essential guide to theory and practice," published by Routledge in 2013.</sample>
    <sample id="1238">The English content in the image is: Do datasets and models have positionality? [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2]Ye et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models.” EMNLP 2022. [3]Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="1239">The video features a slide with the title "Do datasets and models have positionality?" prominently displayed at the top. The background is white, providing clear contrast for the black text. Below the main question, there are three references listed: 1. Blasi, et al., “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. 2. Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models.” EMNLP 2022. 3. Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022. Additionally, in the upper right corner of each frame, there is an image of a person wearing glasses and seated behind a desk or table, suggesting they might be presenting or discussing the topic related to the content shown on the slide.</sample>
    <sample id="1240">The text in the image is: "Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022."</sample>
    <sample id="1241">The text in the image is as follows: Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Ye et al.: “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="1242">The text in the image is as follows: "Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. 'Systematic Inequalities in Language Technology Performance across the World's Languages.' ACL 2022. [2] Ye et al., 'GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.' EMNLP 2022. [3] Cambo &amp; Gergle, 'Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.' CHI 2022."</sample>
    <sample id="1243">The text in the image is as follows: "Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. 'Systematic Inequalities in Language Technology Performance across the World's Languages.' ACL 2022. [2] Ye et al., 'GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.' EMNLP 2022. [3] Cambo &amp; Gergle, 'Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.' CHI 2022."</sample>
    <sample id="1244">The text in the image is: Question: Do datasets and models have positionality? Goal: Compare annotations from users with existing datasets and models.</sample>
    <sample id="1245">NLPPositionality A framework for characterizing design biases in NLP datasets and models</sample>
    <sample id="1246">The English content in the image is as follows: "Our framework works in two main steps."</sample>
    <sample id="1247">The first step is to re-annotate datasets with diverse annotators.</sample>
    <sample id="1248">The English content in the image is as follows: Framework 1) Re-annotate datasets with diverse annotators. Model predictions The received annotations were gold labels from the studies on ITU. Gender Ethnicity Education Country Model predictions The received annotations were gold labels from the studies on ITU. Gender Ethnicity Education Country The correlations are measured and annotated for each annotator separately.</sample>
    <sample id="1249">The English content in the image is as follows: 1) Re-annotate datasets with diverse annotators.</sample>
    <sample id="1250">The English content in the image is: 2) Compare annotations by demographic to models and datasets via Pearson's R scores.</sample>
    <sample id="1251">The image shows a detailed framework for analyzing and processing data. The process begins with "Collection," where 100 instances are sampled from a dataset, each instance having an associated gold label. This is followed by the step of "Annotations" from people around the world.

Next, there's a section labeled "Processing," which involves model predictions based on these annotations. It also includes a comparison between the received annotations and those sent as part of a study on UIW (University of Illinois at Urbana-Champaign).

In the final stage, called "Analysis," various demographic factors such as age, gender, ethnicity, education level, income bracket, and country are considered to assess how they correlate with Pearson’s R scores. 

Additionally, it mentions comparing annotations to demographics using models and datasets via Pearson’s R scores.</sample>
    <sample id="1252">The text in the image is as follows: LabintheWild Our framework is largely enabled through Lab in the Wild, an online crowdsourcing platform for former HCI collaborators.</sample>
    <sample id="1253">The text in the image is structured as follows: 1. Title at the top left corner: "LabintheWild" 2. Main content area with a website screenshot from LabintheWild, including sections such as "Our Departments," "Pipelines &amp; Code," "Blog," and "For Researchers." There are also buttons labeled "Participate now!" on this page. 3. Three main points highlighted by arrows pointing to different parts of the webpage: - Top right arrow pointing to the number "5,376,396 Total participants" under the heading "Pool of diverse volunteers / research participants." - Middle right arrow pointing to an illustration of a virtual reality headset with accompanying text about recruiting for online experiments involving technology ethics. - Bottom right arrow pointing to another section titled "Online experiment from researchers."</sample>
    <sample id="1254">Task A: Social Acceptability 1) Read the situation. Wanting to make a lot of money. 2) Enter what you think about it. What do you think about this from a social perspective? Very bad Bad Okay Good Very good I don't know Free text field to explain your judgment This is generally good, but we don't want to exploit others in the process. Submit Participants read a situation from the Social Chemistry dataset.</sample>
    <sample id="1255">The text in the image is as follows: "Task A: Social Acceptability" and "Participants compare their responses to others' and AI's."</sample>
    <sample id="1256">Task A: Social Acceptability Analysis Datasets - Social Chemistry Models - Delphi - GPT-4</sample>
    <sample id="1257">Task B: Toxicity Participants read an instance from the Dynahate dataset. Participants rate whether they think an instance is hate speech.</sample>
    <sample id="1258">The slide titled 'Task B: Toxicity' lists datasets and models used in the analysis. The datasets include Dynahate, while the models listed are Perspective API, Rewire API, Hate RoBERTa, and GPT-4.</sample>
    <sample id="1259">Results Who do NLP datasets and models align with? Finding 1: There is positionality in NLP.</sample>
    <sample id="1260">Datasets and models are most aligned to English-Speaking countries.</sample>
    <sample id="1261">Datasets and models are most aligned to people with a college education.</sample>
    <sample id="1262">The content in the image is as follows: "Hate Speech &amp; Toxicity (Dynahate)" at the top. Below that, there are bars representing different education levels with their respective values and sample sizes:

- College: 0.66* N=2,383
- Graduate School: 0.64* N=604
- High School: 0.59* N=908
- PhD: 0.48* N=359
- Pre-High School: 0.37 N=116
- Prof. School: 0.61* N=195

On the left side of the chart, it says: "Datasets and models are most aligned to people with a college education."</sample>
    <sample id="1263">The text in the image is: Finding 2: Some populations are left behind.</sample>
    <sample id="1264">The text in the image is as follows: "Datasets and models are less aligned to non-binary people."</sample>
    <sample id="1265">So, given that there is positionality in NLP. What can we do about it?</sample>
    <sample id="1266">The text in the image is 'Recommendations' and '1. Keep a record of all relevant design choices made throughout building datasets or models.'</sample>
    <sample id="1267">The image contains text that provides recommendations for inclusive Natural Language Processing (NLP). Here is the detailed transcription of the content in the image:</sample>
    <sample id="1268">Thanks Dashboard Link: nlpositionality.cs.washington.edu/ Paper: bit.ly/NLPositionality-Paper/</sample>
    <sample id="1269">To put them into the right order.</sample>
    <sample id="1270">The authors recommend that model owners should increase transparency about bias mitigation methods to ensure the effectiveness of these measures and prevent any potential negative impacts on certain groups.</sample>
    <sample id="1271">Minimal-pair unacceptable inputs are sentences that violate the grammatical rules of a language. These could include sentence structures, word order errors, or inappropriate use of grammar and vocabulary. In the context of evaluating language models (LMs), these minimal-pair unacceptable inputs serve as benchmarks to test how well an LLM can distinguish between correct and incorrect linguistic forms. By presenting both acceptable and unacceptable examples side by side, researchers aim to assess whether the model is capable of accurately predicting which input adheres to the syntactic and semantic norms of the language it's designed to understand or generate.</sample>
    <sample id="1272">The evaluation metrics used by the authors are NER, CER, and F1.</sample>
    <sample id="1273">Krippendorf's Alpha was used for measuring inter-annotator agreement.</sample>
    <sample id="1274">The domain chosen to add completely unrelated sentences to the unacceptable and acceptable queries is Wikipedia.</sample>
    <sample id="1275">The authors of the paper are affiliated with Heinrich Heine University Düsseldorf, Germany.</sample>
    <sample id="1276">The video starts with a black screen that transitions to the text 'Language-only' in white. The scene then changes, and yellow highlights appear on some words while maintaining the same background color. It continues by showing more detailed information about instruction tuning for multimodal pre-trained models against a consistent dark backdrop. Following this, it discusses imbalances between NLP (Natural Language Processing) and multimodal datasets, highlighting specific statistics like over 1600 language-only tasks but no large-scale publicly-available multimodal ones. Throughout these segments, there's also an ongoing presence of someone wearing glasses and a jacket at the bottom right corner.</sample>
    <sample id="1277">The paper involves three authors: Sarah E. Finch, James D. Finch, and Jinho D. Choi.</sample>
    <sample id="1278">Binary coordination is a linguistic term that refers to the relationship between two words in a sentence, where one word directly modifies or relates to another. This concept can be applied to various aspects of language structure and meaning, such as syntax, semantics, and pragmatics.</sample>
    <sample id="1279">The prompts used in this study were, on average, 10 words long.</sample>
    <sample id="1280">The findings imply that smaller language models, like the T5 model fine-tuned on Coscript, can outperform larger language models in terms of script quality. This suggests that with appropriate training data and techniques, smaller models may be more effective than their larger counterparts for specific tasks or applications.</sample>
    <sample id="1309">The work investigates the following learning strategies: 1. From scratch with full model construction (e.g., from-scratch, from-scratch-4GB, from-scratch-7GB) 2. Continual pre-training using an existing pre-trained model (e.g., camemBERT, camemBERT-4GB, camemBERT-7GB, camemBERT-10GB)</sample>
    <sample id="1310">The factor of overfitting due to test reuse is specifically mentioned as 'not observed' in the slide.</sample>
    <sample id="1311">The quality of simplification was evaluated based on BLEU and F1 scores.</sample>
    <sample id="1312">The content in English is: 'Existing LMs' and 'pretraining data'.</sample>
    <sample id="1347">Cognitive dissonance is two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent.</sample>
    <sample id="1348">GPT-4 is the most liberal language model.</sample>
    <sample id="1349">Yes, cumulative training performs better than iterative when doing active learning.</sample>
    <sample id="1350">The speaker's name is Sara Papi.</sample>
    <sample id="1351">The data was taken from the transcripts of TED Talks that have been translated into 14 different languages.</sample>
    <sample id="1352">The English content in the image is as follows: 1. "Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination" 2. Adam Przeździeckiowski and Michał Woźniak 3. Institute of Computer Science Polish Academy of Sciences ul. Jana Kazimierza 5, 01-248 Warsaw University of Warsaw ACL 2023</sample>
    <sample id="1353">The text in the image is related to 'Dependency Structure of Coordination'. It lists different dependency structures used by various theories and approaches. The specific terms mentioned include Bouquet/Stanford (Universal Dependencies), Chain/Moscow, Conjunction-headed/Prague, and Multi-headed/London. Each term is followed by a sentence example: "Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="1354">The image shows a slide from a presentation about the "Dependency Structure of Coordination." The slide is divided into four sections, each representing different syntactic structures for coordinating clauses. Here's a detailed breakdown: 1. Bouquet/Stanford (Universal Dependencies): This section illustrates how Homer loves Lisa, Bart, and Maggie in terms of universal dependencies. It uses a tree diagram to show the relationships between the subjects (Homer) and objects (Lisa, Bart, and Maggie). 2. Chain/Moscow: Similar to the first section, this part also depicts Homer loves Lisa, Bart, and Maggie but with a chain structure. A line connects Homer directly to all three names, indicating that they are coordinated equally. 3. Conjunction-headed/Prague: In this section, Homer loves Lisa, Bart, and Maggie is shown using a conjunction-headed structure. The conjunction (and) appears at the beginning or end of the clause, connecting Lisa, Bart, and Maggie as equal parts of the coordination. 4. Multi-headed/London: Finally, the multi-headed structure is presented where Homer loves Lisa, Bart, and Maggie. Multiple lines connect Homer to each name, showing that multiple heads can coordinate within the same clause. Each section includes an example sentence ("Homer loves Lisa, Bart, and Maggie") accompanied by diagrams illustrating the respective syntactic structures.</sample>
    <sample id="1355">The image shows a slide from a presentation about the 'Dependency Structure of Coordination.' The title is displayed at the top in blue text. Below the title, there are four different approaches listed: 1. Bouquet/Stanford (Universal Dependencies)2. Chain/Moscow3. Conjunction-headed/Prague4. Multi-headed/London Each approach has an example sentence below it: 'Homer loves Lisa, Bart, and Maggie.' Additionally, there are diagrams illustrating the dependency structure for each method. In the upper right corner of the slide, there is a small video feed showing a person speaking into a microphone.</sample>
    <sample id="1356">The image shows a slide titled 'Dependency Structure of Coordination' with four different approaches to coordinate structures: 1. Bouquet/Stanford (Universal Dependencies): - Diagram showing how Homer loves Lisa, Bart, and Maggie is structured using this approach. 2. Chain/Moscow: - Similar diagram as the first one but labeled differently. 3. Conjunction-headed/Prague: - Another variation of the structure shown in the previous diagrams. 4. Multi-headed/London: - Yet another variant presented on the same slide. The text at the top indicates that these are various methods or models for understanding coordination in dependency grammar within linguistics.</sample>
    <sample id="1357">The English content in the image is: "Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="1358">The English content in the image is as follows: Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1359">The image shows a slide from a presentation about the 'Dependency Structure of Coordination.' It includes four different approaches to analyzing sentence structure: Bouquet/Stanford (Universal Dependencies), Chain/Moscow, Conjunction-headed/Prague, and Multi-headed/London. Each approach is illustrated with a syntactic tree diagram for the sentence "Homer loves Lisa, Bart, and Maggie." The diagrams show how each method represents the dependencies within this sentence structure.</sample>
    <sample id="1360">The image shows a slide from a presentation titled 'Dependency Structure of Coordination.' The slide is divided into four sections, each describing different dependency structures: 1. Bouquet/Stanford (Universal Dependencies): This section includes a diagram showing the coordination between Homer and Lisa/Bart/Maggie with dotted lines connecting them. Below this diagram, there's an example sentence: "Homer loves Lisa, Bart, and Maggie." 2. Chain/Moscow: Similar to the first section, it has a diagram illustrating the chain structure in dependency grammar. It also contains the same example sentence below the diagram. 3. Conjunction-headed/Prague: This part presents another diagram for conjunction-headed dependencies. Again, the example sentence appears beneath the diagram. 4. Multi-headed/London: The final section features a diagram representing multi-headed dependencies. Like the others, it concludes with the example sentence underneath. In the top right corner of the slide, there is a small video feed window displaying a person speaking or presenting.</sample>
    <sample id="1361">The content of the image is a slide from an educational presentation about Dependency Length Minimization (DLM). The title at the top reads 'Dependency Length Minimization (DLM)'. Below the title, there's text that states: 'Word order tends to minimize dependency lengths:' followed by two examples. Each example shows a sentence with words arranged in different orders and their corresponding dependency trees above them. One tree has arrows labeled 'good' indicating it as good word order for minimizing dependencies, while the other tree has no label or arrow pointing towards it, suggesting it might be bad word order. At the bottom of the slide, there are additional sentences written out without any visual representation like diagrams or arrows. On the right side of the screen, there appears to be a small video feed showing someone who seems to be presenting this material.</sample>
    <sample id="1362">The image shows a presentation slide titled "Dependency Length Minimization (DLM)." The slide explains that word order tends to minimize dependency lengths. It includes two examples of sentences: one labeled as 'good' and the other as 'bad.' Each example has a diagram illustrating the dependency structure, with arrows indicating relationships between words like Marge, read, it, yesterday, etc. There is also text at the bottom of the slide in blue font on a white background, which appears to be part of the presentation content but is not fully visible or readable from this angle. Additionally, there is an inset video feed showing a person speaking during the presentation.</sample>
    <sample id="1363">The image shows a slide from a presentation about Dependency Length Minimization (DLM). The title of the slide is 'Dependency Length Minimization (DLM)' and it includes two example sentences to illustrate how word order affects dependency lengths. Each sentence has an associated diagram showing the syntactic structure, with dependencies marked by lines connecting words. One sentence reads 'Marge read it yesterday.' labeled as 'good' in green text, while the other reads 'Marge read yesterday it.' labeled as 'bad' in red text. Below these examples are diagrams representing different possible orders of the same words: Marge, read, this, absolutely, fascinating, look, about, beer, good, bad. These diagrams show various ways the words can be rearranged without changing their meaning but altering the dependency length. In the top right corner of each frame, there is a small video feed of a person who appears to be giving the presentation or lecture.</sample>
    <sample id="1364">The image shows a presentation slide titled "Dependency Length Minimization (DLM)." The slide explains how word order tends to minimize dependency lengths. There are two example sentences: one labeled 'good' and the other 'bad.' Each sentence has a diagram showing the dependencies between words in terms of length, with shorter paths being better. Below the examples, there is a more detailed tree structure illustrating the dependencies among multiple words.</sample>
    <sample id="1365">The English content in the image is: Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read it yesterday. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read this absolutely fascinating book about bees yesterday. good</sample>
    <sample id="1366">The English content in the image is: "Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths:" followed by examples of sentences with different word orders and their corresponding dependency structures. The first example shows a sentence "Marge read it yesterday." with a good dependency length, indicated by green text. The second example also shows the same sentence but with a bad dependency length, indicated by red text. Below these, there are more complex sentences demonstrating how changing the word order affects the dependency structure and resulting in either a good or bad dependency length.</sample>
    <sample id="1367">The English content in the image is as follows: "Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read yesterday it. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read this absolutely fascinating book about bees yesterday. good"</sample>
    <sample id="1368">The English content in the image is: 'Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read it yesterday. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read this absolutely fascinating book about bees yesterday. good'</sample>
    <sample id="1369">The image contains a slide from a presentation about Dependency Length Minimization (DLM). The title of the slide is "Dependency Length Minimization (DLM)" and it explains that word order tends to minimize dependency lengths. There are two examples provided, one labeled 'good' with correct sentence structure and minimal dependencies, and another labeled 'bad' showing longer dependencies due to improper word ordering.</sample>
    <sample id="1370">The image shows a slide titled "Dependency Length Minimization (DLM)" with the subtitle "Word order tends to minimize dependency lengths:". The slide contains several examples of sentences and their corresponding dependency trees. Each example is labeled as either 'good' or 'bad' based on the length of the dependencies.

1. The first sentence is "Marge read it yesterday." This has two dependency arrows, one from "read" to "it" and another from "it" to "yesterday," both marked as good.
2. The second sentence is also "Marge read it yesterday." However, this time there are three dependency arrows: one from "read" to "it," another from "it" to "yesterday," and a third from "yesterday" back to "read." This configuration is marked as bad.
3. The third sentence reads "Marge read this absolutely fascinating book about bees yesterday." It has four dependency arrows connecting "read" to "this," "this" to "absolutely," "absolutely" to "fascinating," "fascinating" to "book," "book" to "about," "about" to "bees," and finally "yesterday" to all these words. This complex structure is again marked as bad.
4. Finally, the fourth sentence repeats "Marge read this absolutely fascinating book about bees yesterday." Similar to the previous long dependency chain, but still marked as bad due to its complexity.

Each example illustrates how different word orders can affect the length of dependencies in a sentence, highlighting that shorter dependency chains tend to be considered better ("good") while longer ones are less favorable ("bad").</sample>
    <sample id="1371">The image shows a presentation slide titled 'Dependency Length Minimization (DLM)'. The main content of the slide explains how word order tends to minimize dependency lengths. There are several examples provided, each with a sentence and its corresponding syntactic tree diagram. Each example is labeled as either 'good' or 'bad', indicating whether the word order minimizes dependencies effectively.

The first example has two sentences:
1. "Marge read it yesterday." - This sentence is marked as good.
2. "Marge read yesterday it." - This sentence is marked as bad.

The second example also consists of two sentences:
1. "Marge read this absolutely fascinating book about bees yesterday." - This sentence is marked as good.
2. "Marge read yesterday this absolutely fascinating book about bees." - This sentence is marked as bad.

The third example includes three sentences:
1. "Marge read this absolutely fascinating book about bees yesterday." - This sentence is marked as good.
2. "Marge read yesterday this absolutely fascinating book about bees." - This sentence is marked as bad.
3. "Marge read this absolutely fascinating book about bees yesterday." - This sentence is marked as good again.

In all examples, the goal is to show that certain word orders result in shorter dependency paths, which is considered desirable for minimizing dependency length.</sample>
    <sample id="1372">The text in the image is about 'Conjunct Lengths in English.' It discusses statistics related to coordination extracted from an enhanced version of the Penn Treebank, referencing works by Marcus et al. (1993), Ficler and Goldberg (2016). The slide lists several points: 1. Left conjuncts tend to be shorter than right conjuncts, which was observed before. 2. This tendency grows with length difference, as briefly noticed in Gibson et al. (1996): 88-90. 3. However, when only the governor is on the left or absent (e.g., "I saw Bart and Lisa; Homer came and sneezed"), this pattern does not hold true. 4. When it is on the right (e.g., "Ted and Ned laughed"), there might be a different outcome.</sample>
    <sample id="1373">The text in the image is about 'Conjunct Lengths in English'. It mentions statistics from an enhanced version of the Penn Treebank, referencing studies by Marcus et al. (1993), Ficler and Goldberg (2016). The key point highlighted is that left conjuncts tend to be shorter than right conjuncts when observed before a governor. Examples given include "I saw Bart and Lisa" where 'Bart' is on the left and 'Lisa' is on the right; and "Homer came and sneezed," with 'Homer' being on the left and 'sneezed' on the right. Another example provided is "not when it is on the right ('Ted and Ned laughed').".</sample>
    <sample id="1374">The image contains a slide titled "Conjunct Lengths in English." The content of the slide discusses statistics about coordination extracted from an enhanced version of the Penn Treebank, referencing studies by Marcus et al. (1993) and Ficler and Goldberg (2016). It highlights that left conjuncts tend to be shorter than right conjuncts, which was observed before. Additionally, it mentions that this tendency grows with length difference, briefly noticed in Gibson et al. (1996): 88-90. Examples provided include: - Only when the governor is on the left or absent ("I saw Bart and Lisa; Homer came and sneezed.") - Not when it is on the right ("Ted and Ned laughed.").</sample>
    <sample id="1375">Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88–90) but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="1376">The text in the image is about conjunct lengths in English. It discusses statistics on coordination extracted from an enhanced version of the Penn Treebank by Marcus et al., 1993, and Ficler and Goldberg, 2016. The slide mentions that left conjuncts tend to be shorter (observed before) and this tendency grows with length difference, briefly noticed in Gibson et al., 1996:88-90. However, when only the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), it does not hold true for right conjuncts (not when it is on the right; Ted and Ned laughed).</sample>
    <sample id="1377">The image shows a slide from a presentation titled "Conjunct Lengths in English." The content of the slide discusses statistics about coordination extracted from an enhanced version of the Penn Treebank, referencing work by Marcus et al. (1993), Ficler and Goldberg (2016). It highlights that left conjuncts tend to be shorter than right conjuncts, which is observed before. Additionally, it mentions that this tendency grows with length difference, briefly noticed in Gibson et al. (1996: 88–90). An example provided on the slide illustrates how conjunction lengths can vary based on context: - When the governor is on the left or absent ("I saw Bart and Lisa; Homer came and sneezed"), the conjunction length changes depending on whether the governor is present or not. This observation underscores the variability in conjunction lengths influenced by contextual factors such as the presence of specific elements like the governor.</sample>
    <sample id="1378">The content of the slide is about 'Conjunct Lengths in English.' It discusses statistics on coordination extracted from an enhanced version of the Penn Treebank, referencing work by Marcus et al. (1993), Ficler and Goldberg (2016). The key points include: - Left conjuncts tend to be shorter than right conjuncts when observed before. - This tendency grows with length difference, as briefly noticed in Gibson et al. (1996): 88-90. - However, only when the governor is on the left or absent does this pattern hold true; for example, "I saw Bart and Lisa; Homer came and sneezed." - When not following a governor, the conjunction becomes longer; for instance, "not when it is on the right" where Ted and Ned laughed.</sample>
    <sample id="1379">The image shows a slide from a presentation titled "Conjunct Lengths in English." The slide contains text discussing statistics about coordination extracted from an enhanced version of the Penn Treebank, referencing works by Marcus et al. (1993) and Ficler and Goldberg (2016). It mentions that left conjuncts tend to be shorter than right conjuncts, which grows with length difference. Examples are provided: - "but only when the governor is on the left or absent" - "not when it is on the right" Additionally, there is a small video feed in the top-right corner showing a person speaking into a microphone.</sample>
    <sample id="1380">The English content in the image is as follows: Figure 1: Proportions of shorter left-right contacts depending on the absolute difference of contact lengths (with confidence bands).</sample>
    <sample id="1381">The English content in the image is as follows: Figure 1: Proportions of shorter left-centred contacts depending on the absolute difference of contact lengths (with confidence bands).</sample>
    <sample id="1382">The image shows a set of nine line graphs arranged in a 3x3 grid. Each graph is labeled with different conditions related to the length and structure of sentences, such as "NO governor (length in CHARACTERS)", "NO governor (length in SYLLABLES)", and "NO governor (length in WORDS)". The x-axis of each graph represents the absolute difference in words or characters, while the y-axis indicates the proportion of shorter left-adjacent chunks.

The title at the bottom reads: 'Figure 1: Proportions of shorter left-adjacent chunks depending on the absolute difference of chunk lengths (with confidence bands)'. 

In the top right corner, there is an inset showing a person's face.</sample>
    <sample id="1383">The image shows a slide from a presentation titled "Compatibility with Dependency Structures of Coordination." The slide is divided into four sections, each evaluating the compatibility of different dependency structures for coordination. Here are the details:

1. **Bouquet/Stanford (Universal Dependencies):**
   - Sentence: "Homer loves Lisa, Bart, and Maggie."
   - Compatibility: No
   - Diagram: Shows an arrow pointing to Homer.

2. **Chain/Moscow:**
   - Sentence: "Homer loves Lisa, Bart, and Maggie."
   - Compatibility: No
   - Diagram: Shows no arrows connecting elements.

3. **Conjunction-headed/Praque:**
   - Sentence: "Homer loves Lisa, Bart, and Maggie."
   - Compatibility: Yes
   - Diagram: Shows arrows connecting all three names to Homer.

4. **Multi-headed/London:**
   - Sentence: "Homer loves Lisa, Bart, and Maggie."
   - Compatibility: Yes
   - Diagram: Shows multiple arrows originating from Homer to all three names.

Each section includes a sentence about Homer loving various individuals, followed by a diagram illustrating how these dependencies can be represented according to the respective models or theories mentioned in parentheses next to each title.</sample>
    <sample id="1384">The content in the image is: 'See the paper for the full argument Talk to us at the poster session!'</sample>
    <sample id="1385">Matthias Lindemann</sample>
    <sample id="1386">Cross-lingual transfer is a technique where you train on one source language and then apply the learned knowledge to another target language. This approach allows models to leverage information from multiple languages, improving their performance in tasks that involve different linguistic contexts.</sample>
    <sample id="1387">The authors of the paper are affiliated with Saarland University, Amazon Alexa, and the University of Vienna.</sample>
    <sample id="1388">The authors use BLEU and average latency as their main measures.</sample>
    <sample id="1416">Trees help a lot but... Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction</sample>
    <sample id="1417">The authors of the paper are affiliated with the School of Interactive Computing at Georgia Institute of Technology.</sample>
    <sample id="1495">It stands for "Annotating Behaviors in Chat".</sample>
    <sample id="1496">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until the year 2016.</sample>
    <sample id="1527">The authors are affiliated with the University of Amsterdam, Saarland University, and UC Irvine.</sample>
    <sample id="1528">The speaker's name is Siyu Yuan.</sample>
    <sample id="1529">There are five authors involved in the paper.</sample>
    <sample id="1530">The approach is compared to the state-of-the-art architecture specifically tailored for Simultaneous Press Translation.</sample>
    <sample id="1531">MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning Zhiyang Xu* Ying Shen* Lifu Huang Department of Computer Science, Virginia Tech *Equal Contribution</sample>
    <sample id="1532">The image contains a slide titled "Pre-trained Language Models for Downstream Tasks." The slide is divided into three sections: (A) Pretrain-finetune (BERT, T5), (B) Prompting (GPT-3), and (C) Instruction tuning (FLAN). Each section describes different methods of using pre-trained language models. Below the sections, there is a figure caption that reads, "Figure 2: Comparing instruction tuning with pretrain-finetune and prompting." At the bottom of the slide, there is an attribution to Wei, Jason, et al., stating, "Finetuned language models are zero-shot learners." In the lower right corner of the image, there is a small picture of a person wearing glasses and speaking.</sample>
    <sample id="1533">The image shows a slide from a presentation titled "Pre-trained Language Models for Downstream Tasks." The slide is divided into three main sections, each describing different methods of fine-tuning language models. 1. (A) Pretrain-finetune (BERT, T5): This section explains the process of pretraining and then finetuning BERT or T5 models on specific tasks. It includes diagrams showing how these models are pretrained with large amounts of data to learn general patterns and then fine-tuned using task-specific examples. - Text: "Typically requires many task-specific examples for each task" 2. (B) Prompting (GPT-3): This part discusses prompting GPT-3 models via few-shot prompting techniques. It highlights that this method can improve performance by providing just a few example inputs along with instructions. - Text: "Improve performance via few-shot prompting" 3. (C) Instruction tuning (FLAN): This section describes instruction tuning, where FLAN uses natural language instructions to perform various downstream tasks efficiently. It mentions that this approach allows learning multiple tasks through simple natural language instructions without needing unseen task labels. - Text: "Instruction type on many tasks; Do, Dn..." Additionally, there is an image credit at the bottom left corner which reads: "Image credit: Wei, Jason, et al., 'Fine-tuned language models are zero-shot learners.'" At the bottom center of the slide, there is a caption explaining the figure shown above it: "Figure 2: Comparing instruction tuning with pretrain-finetune and prompting."</sample>
    <sample id="1534">The video features a person wearing glasses and a black jacket, speaking in front of a plain background. The text 'Language-only' is prominently displayed on the screen throughout the sequence. In one frame, additional text appears at the bottom right corner: 'However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks while computer vision and multi-modal tasks have been left out.' This suggests that the discussion revolves around the limitations or focus areas of previous studies related to language-only tasks versus other types such as computer vision and multimodal tasks.</sample>
    <sample id="1535">Instruction Tuning on Multimodal Pre-trained Models</sample>
    <sample id="1536">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="1537">The content in the image is a slide from a presentation. The main title of the slide reads 'Imbalance in Instructional Datasets between NLP and Multimodal.' Below this, there is additional text that states '1600+ Language-only instruction tasks' with '1600+' highlighted in yellow. Further down, it says 'NO large-scale, publicly-available multimodal instruction tasks,' again with 'NO' highlighted in yellow. At the bottom left corner of the slide, there is a citation: 'Wang, Yizhong, et al. "Benchmarking generalization via in-context instructions on 1600+ language tasks" arXiv preprint arXiv:2304.07895.'</sample>
    <sample id="1538">MULTINSTRUCT The first multimodal instruction tuning benchmark dataset 62 diverse multimodal tasks 10 broad groups 5 expert-written instructions Figure 2: Task Groups Included in Multinstruct. The yellow boxes represent tasks used for evaluation, the white ones indicate tasks included only during training.</sample>
    <sample id="1539">MULTINSTRUCT The first multimodal instruction tuning benchmark dataset 62 diverse multimodal tasks 10 broad groups 5 expert-written instructions Figure 2: Task Groups Included in MULTINSTRUCT. The yellow boxes represent tasks used for evaluation, the white ones indicate training sets.</sample>
    <sample id="1540">The image contains a slide with the title "OFA (One For All)" at the top. The main content of the slide is divided into two sections: text and diagrams.

1. **Text Section**:
   - There are three bullet points in white font on a black background.
   - The first bullet point states, "A unified multi-modal pre-trained model that is capable of performing both understanding and generation tasks with single or multiple modalities."
   - The second bullet point highlights, "OFA has a unified vocabulary for language, image tokens, and the coordinates of a bounding box."

2. **Diagrams**:
   - Below the text, there are several colorful diagrams illustrating different aspects of OFA's architecture.
   - These diagrams include various labeled boxes connected by arrows, representing different components and their interactions within the OFA framework.

3. **Footer Text**:
   - At the bottom of the slide, there is additional information in smaller white font against the same black background. It reads, "Wang, Peng, et al., 'Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework'."

Overall, the slide provides an overview of OFA as a unified multimodal pre-trained model and its key features, supported by visual representations of its structure and functionality.</sample>
    <sample id="1541">MULTINSTRUCT Here we show some example instances from our multi-instruct dataset. Figure 1: Example Instances from MULTINSTRUCT for Four Tasks.</sample>
    <sample id="1542">MULTINSTRUCT Figure 1: Example Instances from MULTINSTRUCT for Four Tasks. Grounded Caption Input: Generate a caption for the image. Output: A man and white tennis racket Text Localization Input: Select the region that contains the text "don't". Options: &lt;bin_194&gt; - 206, &lt;bin_195&gt; - 207, &lt;bin_196&gt; - 208, &lt;bin_197&gt; - 209, &lt;bin_198&gt; - 210, &lt;bin_199&gt; - 211, &lt;bin_200&gt; - 212, &lt;bin_201&gt; - 213, &lt;bin_202&gt; - 214, &lt;bin_203&gt; - 215, &lt;bin_204&gt; - 216, &lt;bin_205&gt; - 217, &lt;bin_206&gt; - 218, &lt;bin_207&gt; - 219, &lt;bin_208&gt; - 220, &lt;bin_209&gt; - 221, &lt;bin_210&gt; - 222, &lt;bin_211&gt; - 223, &lt;bin_212&gt; - 224, &lt;bin_213&gt; - 225, &lt;bin_214&gt; - 226, &lt;bin_215&gt; - 227, &lt;bin_216&gt; - 228, &lt;bin_217&gt; - 229, &lt;bin_218&gt; - 230, &lt;bin_219&gt; - 231, &lt;bin_220&gt; - 232, &lt;bin_221&gt; - 233, &lt;bin_222&gt; - 234, &lt;bin_223&gt; - 235, &lt;bin_224&gt; - 236, &lt;bin_225&gt; - 237, &lt;bin_226&gt; - 238, &lt;bin_227&gt; - 239, &lt;bin_228&gt; - 240, &lt;bin_229&gt; - 241, &lt;bin_230&gt; - 242, &lt;bin_231&gt; - 243, &lt;bin_232&gt; - 244, &lt;bin_233&gt; - 245, &lt;bin_234&gt; - 246, &lt;bin_235&gt; - 247, &lt;bin_236&gt; - 248, &lt;bin_237&gt; - 249, &lt;bin_238&gt; - 250, &lt;bin_239&gt; - 251, &lt;bin_240&gt; - 252, &lt;bin_241&gt; - 253, &lt;bin_242&gt; - 254, &lt;bin_243&gt; - 255, &lt;bin_244&gt; - 256, &lt;bin_245&gt; - 257, &lt;bin_246&gt; - 258, &lt;bin_247&gt; - 259, &lt;bin_248&gt; - 260, &lt;bin_249&gt; - 261, &lt;bin_250&gt; - 262, &lt;bin_251&gt; - 263, &lt;bin_252&gt; - 264, &lt;bin_253&gt; - 265, &lt;bin_254&gt; - 266, &lt;bin_255&gt; - 267, &lt;bin_256&gt; - 268, &lt;bin_257&gt; - 269, &lt;bin_258&gt; - 270, &lt;bin_259&gt; - 271, &lt;bin_260&gt; - 272, &lt;bin_261&gt; - 273, &lt;bin_262&gt; - 274, &lt;bin_263&gt; - 275, &lt;bin_264&gt; - 276, &lt;bin_265&gt; - 277, &lt;bin_266&gt; - 278, &lt;bin_267&gt; - 279, &lt;bin_268&gt; - 280, &lt;bin_269&gt; - 281, &lt;bin_270&gt; - 282, &lt;bin_271&gt; - 283, &lt;bin_272&gt; - 284, &lt;bin_273&gt; - 285, &lt;bin_274&gt; - 286, &lt;bin_275&gt; - 287, &lt;bin_276&gt; - 288, &lt;bin_277&gt; - 289, &lt;bin_278&gt; - 290, &lt;bin_279&gt; - 291, &lt;bin_280&gt; - 292, &lt;bin_281&gt; - 293, &lt;bin_282&gt; - 294, &lt;bin_283&gt; - 295, &lt;bin_284&gt; - 296, &lt;bin_285&gt; - 297, &lt;bin_286&gt; - 298, &lt;bin_287&gt; - 299, &lt;bin_288&gt; - 300, &lt;bin_289&gt; - 301, &lt;bin_290&gt; - 302, &lt;bin_291&gt; - 303, &lt;bin_292&gt; - 304, &lt;bin_293&gt; - 305, &lt;bin_294&gt; - 306, &lt;bin_295&gt; - 307, &lt;bin_296&gt; - 308, &lt;bin_297&gt; - 309, &lt;bin_298&gt; - 310, &lt;bin_299&gt; - 311, &lt;bin_300&gt; - 312, &lt;bin_301&gt; - 313, &lt;bin_302&gt; - 314, &lt;bin_303&gt; - 315, &lt;bin_304&gt; - 316, &lt;bin_305&gt; - 317, &lt;bin_306&gt; - 318, &lt;bin_307&gt; - 319, &lt;bin_308&gt; - 320, &lt;bin_309&gt; - 321, &lt;bin_310&gt; - 322, &lt;bin_311&gt; - 323, &lt;bin_312&gt; - 324, &lt;bin_313&gt; - 325, &lt;bin_314&gt; - 326, &lt;bin_315&gt; - 327, &lt;bin_316&gt; - 328, &lt;bin_317&gt; - 329, &lt;bin_318&gt; - 330, &lt;bin_319&gt; - 331, &lt;bin_320&gt; - 332, &lt;bin_321&gt; - 333, &lt;bin_322&gt; - 334, &lt;bin_323&gt; - 335, &lt;bin_324&gt; - 336, &lt;bin_325&gt; - 337, &lt;bin_326&gt; - 338, &lt;bin_327&gt; - 339, &lt;bin_328&gt; - 340, &lt;bin_329&gt; - 341, &lt;bin_330&gt; - 342, &lt;bin_331&gt; - 343, &lt;bin_332&gt; - 344, &lt;bin_333&gt; - 345, &lt;bin_334&gt; - 346, &lt;bin_335&gt; - 347, &lt;bin_336&gt; - 348, &lt;bin_337&gt; - 349, &lt;bin_338&gt; - 350, &lt;bin_339&gt; - 351, &lt;bin_340&gt; - 352, &lt;bin_341&gt; - 353, &lt;bin_342&gt; - 354, &lt;bin_343&gt; - 355, &lt;bin_344&gt; - 356, &lt;bin_345&gt; - 357, &lt;bin_346&gt; - 358, &lt;bin_347&gt; - 359, &lt;bin_348&gt; - 360, &lt;bin_349&gt; - 361, &lt;bin_350&gt; - 362, &lt;bin_351&gt; - 363, &lt;bin_352&gt; - 364, &lt;bin_353&gt; - 365, &lt;bin_354&gt; - 366, &lt;bin_355&gt; - 367, &lt;bin_356&gt; - 368, &lt;bin_357&gt; - 369, &lt;bin_358&gt; - 370, &lt;bin_359&gt; - 371, &lt;bin_360&gt; - 372, &lt;bin_361&gt; - 373, &lt;bin_362&gt; - 374, &lt;bin_363&gt; - 375, &lt;bin_364&gt; - 376, &lt;bin_365&gt; - 377, &lt;bin_366&gt; - 378, &lt;bin_367&gt; - 379, &lt;bin_368&gt; - 380, &lt;bin_369&gt; - 381, &lt;bin_370&gt; - 382, &lt;bin_371&gt; - 383, &lt;bin_372&gt; - 384, &lt;bin_373&gt; - 385, &lt;bin_374&gt; - 386, &lt;bin_375&gt; - 387, &lt;bin_376&gt; - 388, &lt;bin_377&gt; - 389, &lt;bin_378&gt; - 390, &lt;bin_379&gt; - 391, &lt;bin_380&gt; - 392, &lt;bin_381&gt; - 393, &lt;bin_382&gt; - 394, &lt;bin_383&gt; - 395, &lt;bin_384&gt; - 396, &lt;bin_385&gt; - 397, &lt;bin_386&gt; - 398, &lt;bin_387&gt; - 399, &lt;bin_388&gt; - 400, &lt;bin_389&gt; - 401, &lt;bin_390&gt; - 402, &lt;bin_391&gt; - 403, &lt;bin_392&gt; - 404, &lt;bin_393&gt; - 405, &lt;bin_394&gt; - 406, &lt;bin_395&gt; - 407, &lt;bin_396&gt; - 408, &lt;bin_397&gt; - 409, &lt;bin_398&gt; - 410, &lt;bin_399&gt; - 411, &lt;bin_400&gt; - 412, &lt;bin_401&gt; - 413, &lt;bin_402&gt; - 414, &lt;bin_403&gt; - 415, &lt;bin_404&gt; - 416, &lt;bin_405&gt; - 417, &lt;bin_406&gt; - 418, &lt;bin_407&gt; - 419, &lt;bin_408&gt; - 420, &lt;bin_409&gt; - 421, &lt;bin_410&gt; - 422, &lt;bin_411&gt; - 423, &lt;bin_412&gt; - 424, &lt;bin_413&gt; - 425, &lt;bin_414&gt; - 426, &lt;bin_415&gt; - 427, &lt;bin_416&gt; - 428, &lt;bin_417&gt; - 429, &lt;bin_418&gt; - 430, &lt;bin_419&gt; - 431, &lt;bin_420&gt; - 432, &lt;bin_421&gt; - 433, &lt;bin_422&gt; - 434, &lt;bin_423&gt; - 435, &lt;bin_424&gt; - 436, &lt;bin_425&gt; - 437, &lt;bin_426&gt; - 438, &lt;bin_427&gt; - 439, &lt;bin_428&gt; - 440, &lt;bin_429&gt; - 441, &lt;bin_430&gt; - 442, &lt;bin_431&gt; - 443, &lt;bin_432&gt; - 444, &lt;bin_433&gt; - 445, &lt;bin_434&gt; - 446, &lt;bin_435&gt; - 447, &lt;bin_436&gt; - 448, &lt;bin_437&gt; - 449, &lt;bin_438&gt; - 450, &lt;bin_439&gt; - 451, &lt;bin_440&gt; - 452, &lt;bin_441&gt; - 453, &lt;bin_442&gt; - 454, &lt;bin_443&gt; - 455, &lt;bin_444&gt; - 456, &lt;bin_445&gt; - 457, &lt;bin_446&gt; - 458, &lt;bin_447&gt; - 459, &lt;bin_448&gt; - 460, &lt;bin_449&gt; - 461, &lt;bin_450&gt; - 462, &lt;bin_451&gt; - 463, &lt;bin_452&gt; - 464, &lt;bin_453&gt; - 465, &lt;bin_454&gt; - 466, &lt;bin_455&gt; - 467, &lt;bin_456&gt; - 468, &lt;bin_457&gt; - 469, &lt;bin_458&gt; - 470, &lt;bin_459&gt; - 471, &lt;bin_460&gt; - 472, &lt;bin_461&gt; - 473, &lt;bin_462&gt; - 474, &lt;bin_463&gt; - 475, &lt;bin_464&gt; - 476, &lt;bin_465&gt; - 477, &lt;bin_466&gt; - 478, &lt;bin_467&gt; - 479, &lt;bin_468&gt; - 480, &lt;bin_469&gt; - 481, &lt;bin_470&gt; - 482, &lt;bin_471&gt; - 483, &lt;bin_472&gt; - 484, &lt;bin_473&gt; - 485, &lt;bin_474&gt; - 486, &lt;bin_475&gt; - 487, &lt;bin_476&gt; - 488, &lt;bin_477&gt; - 489, &lt;bin_478&gt; - 490, &lt;bin_479&gt; - 491, &lt;bin_480&gt; - 492, &lt;bin_481&gt; - 493, &lt;bin_482&gt; - 494, &lt;bin_483&gt; - 495, &lt;bin_484&gt; - 496, &lt;bin_485&gt; - 497, &lt;bin_486&gt; - 498, &lt;bin_487&gt; - 499, &lt;bin_488&gt; - 500, &lt;bin_489&gt; - 501, &lt;bin_490&gt; - 502, &lt;bin_491&gt; - 503, &lt;bin_492&gt; - 504, &lt;bin_493&gt; - 505, &lt;bin_494&gt; - 506, &lt;bin_495&gt; - 507, &lt;bin_496&gt; - 508, &lt;bin_497&gt; - 509, &lt;bin_498&gt; - 510, &lt;bin_499&gt; - 511, &lt;bin_500&gt; - 512, &lt;bin_501&gt; - 513, &lt;bin_502&gt; - 514, &lt;bin_503&gt; - 515, &lt;bin_504&gt; - 516</sample>
    <sample id="1543">Figure 1: Example Instances from MULTINSTRUCT for Four Tasks.</sample>
    <sample id="1544">Multi-modal Instruction Tuning</sample>
    <sample id="1545">The content of the image is a slide titled 'Multi-Modal Instruction Turning'. The slide contains two main sections: Training Dataset Construction and Testing Dataset Construction. Under Training Dataset Construction, it states that 53 tasks from 9 groups are used for training, with 10,000 instances per task sampled. In Testing Dataset Construction, it mentions reserving the entire Commonsense Reasoning group for testing, selecting an additional 5 tasks from VQA and Miscellaneous groups, using all instances in the test split for each task, and randomly sampling 20 tasks from the Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="1546">The image contains a slide titled "Multi-Modal Instruction Turning" with two main sections: Training Dataset Construction and Testing Dataset Construction. Under the Training Dataset Construction, it states that 53 tasks from 9 groups were used for training, sampling 10,000 instances per task. For the Testing Dataset Construction, it mentions reserving the entire Commonsense Reasoning group for testing, selecting an additional 5 tasks from VQA and Miscellaneous groups, using all instances in the test split for each task, and randomly sampling 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="1547">The content in the image is as follows: Implementation Details Training details: - Pre-trained OFA-Large model (472M) - Mix all the instances for all tasks. - Each instance is randomly combined with one of its five instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1548">The content of the image is a slide from a presentation with the title "Implementation Details." The slide contains two main sections: Training details and Testing details. Under Training details, there are three bullet points:
1. Pre-trained OFA-Large model (472M)
2. Mix all the instances for all tasks.
3. Each instance is randomly combined with one of its five instruction templates.

Under Testing details, there are also three bullet points:
1. For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.
2. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.

In the bottom right corner of the slide, there is an inset picture of a person wearing glasses and a white shirt.</sample>
    <sample id="1549">Implementation Details Training details: Pre-trained OFA-Large model (472M) Mix all the instances for all tasks. Each instance is randomly combined with one of its instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1550">The image contains a slide titled "Evaluation Metrics" with text describing different types of tasks and the metrics used to evaluate them. The main points are: - For multi-modal classification tasks, accuracy is reported. Examples include Visual Entailment, Visual Spatial Reasoning, Natural Language Visual Reasoning, and Disaster Type Classification. - For multi-modal generation tasks (Commonsense VQA, Text VQA, Grounded VQA, Visual Text Extraction, and Visual Dialogue), Rouge-L is reported as the metric. - For NLP tasks, Rouge-L is also reported. Additionally, there is mention of aggregated performance for each model based on mean scores across all multimodal and NLP unseen tasks, using Rouge-L as the score for most tasks and Accuracy for those that only have accuracy as a metric.</sample>
    <sample id="1551">The image shows a presentation slide with the title "Sensitivity" at the top. The main content of the slide discusses how sensitive a model is towards various instructions for the same task. Key phrases such as "variety of instructions," "same task," and "ability to consistently produce the same results" are highlighted in yellow.

Below the text, there is a mathematical formula involving summations over \( \Sigma_{t \in T} \) and integrals within \( \mathbb{E}_{(x, y) \sim D^t}[L(\mathcal{J}(f_0(i, x), y))] \). This suggests that the slide might be discussing some form of statistical or machine learning concept related to sensitivity analysis.

In the bottom right corner of the image, there is a small picture of a person wearing glasses and speaking into a microphone, indicating that this could be part of an online lecture or webinar where someone is presenting these concepts live.</sample>
    <sample id="1552">Effectiveness of Instruction Tuning on MULTIINSTRUCT</sample>
    <sample id="1553">Effectiveness of Instruction Tuning on MULTIINSTRUCT Table 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold.</sample>
    <sample id="1554">Here we can see as the amount of task increases, the model achieves better performance and in the meantime lower sensitivity.</sample>
    <sample id="1555">The content of the image is as follows: The title at the top reads "Effect of Diverse Instructions on Instruction Tuning." Below the title, there is a bullet point that states, "OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity." There is also a table labeled "Table 3: Effect of Different Number of Instructions. Performance of OFA-Mainfinet tuned on different numbers of instructions." The table has three columns titled "# of Instructions," "Aggregated Performance ↑," and "Sensitivity ↓." It contains two rows with data for "1 Instruction" (42.81) and "5 Instructions" (47.82), along with corresponding values for Sensitivity (24.62 and 10.45). In the bottom right corner of the image, there is a small picture of a person wearing glasses and a white shirt.</sample>
    <sample id="1556">The image contains a slide from a presentation discussing the effect of fine-tuning strategies on model sensitivity. The title at the top reads "Effect of Fine-tuning Strategies on Model Sensitivity." Below the title, there are two bullet points: 1. "Instruction tuning on Multilinstruct can significantly reduce the sensitivity of OFA." 2. "Transfer learning from Natural Instructions dataset can further reduce the sensitivity of the model." At the bottom of the slide, there is a bar chart with the following labels and values: - OFA: 40.58 - OFA Multilinstruct: 13.84 - OFA Seglinstruct: 10.45 - OFA Alignedinstruct: 10.27 The x-axis of the chart ranges from 0 to 50 in increments of 5. A note below the chart states, "Figure 4: Sensitivity on Unseen Evaluation Tasks. Lower is better." In the lower right corner of the image, there is a small photograph of a person wearing glasses.</sample>
    <sample id="1557">The image contains a slide titled "Zero-Shot Performance on NLP Tasks." The content of the slide is as follows: - Instruction Tuning on Multinstruct can improve zero-shot performance on unseen NLP tasks. - The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset. Below this text, there is a table labeled "Table 4: Zero-Shot Performance on NLP Tasks. The report is performed in Rouge-L and the best performance is in bold." The table has two columns: Model and RougeL. Under the Models column, it lists OFA, OFA-Multinstruct, Transfer Learning from NATURAL INSTRUCTIONS, OFA-NaturalInstructions, and OFA-Segment. Corresponding to these models are values under the RougeL column ranging from 2.25 for OFA to 30.79 for OFA-Segment.</sample>
    <sample id="1558">The English content in the image is as follows: Conclusion First large-scale multi-modal instruction tuning dataset. Contains 62 multi-modal tasks from 10 broad categories. Significantly improve the zero-shot capability of OFA via instruction tuning. Explore several transferring learning techniques and show their benefits. Design a new metric sensitivity.</sample>
    <sample id="1559">The image contains a black background with white text at the top that reads "One More Thing!" Below this, there is additional text stating: "We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon!" In the center of the image, there is a QR code. At the bottom right corner of the image, there is a small picture of a person wearing glasses and a light-colored shirt.</sample>
  </task>
</testset>