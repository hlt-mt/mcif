<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Das Hauptthema der Präsentation ist 'Von der Vorbereitung von Daten zu Sprachmodellen und die Ausführung von Aufgaben: Folgen der Verfolgung der Spuren von politischen Biases, die zu unfairen NLP-Modelle führen'.</sample>
    <sample id="1">The authors belong to McGill University/Mila and Microsoft Research.</sample>
    <sample id="2">DEPLAIN: Ein deutscher Parallel-Korpus mit Intrasprachlichen Übersetzungen in einfachen Sprachen für die Einfachstellung von Sätzen und Dokumente</sample>
    <sample id="3">Das Bild zeigt eine Präsentation mit dem Titel "DEPLAIN: Ein deutscher Parallel-Korpus mit intralingualen Übersetzungen in plain Language für die Simplifizierung von Sätzen und Dokumenten". Die Autoren sind Regina Stodden, Omar Momen und Laura Kallmeyer. Das Ereignis findet an der Heinrich-Heine-Universität Düsseldorf, Deutschland, im Jahr 2023 statt. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich über eine Videokonferenz teilnimmt.</sample>
    <sample id="4">Das Bild zeigt eine Präsentation über Textsimplifizierung. Der Titel der Präsentation lautet "Text Simplification Example". Im Hintergrund ist ein blauer Banner zu sehen, und darunter befindet sich ein Diagramm, das verschiedene Arten von Textsimplifizierungen anzeigt. Die Arten von Simplifizierungen sind Substitution, Clause Deletion, Reordering, Word Deletion und Insertion. Darüber hinaus gibt es zwei Beispiele von Texten, die sich vor dem und nach der Simplifizierung ändern.</sample>
    <sample id="5">Das Bild zeigt eine Präsentation über "Text Simplification Example" (Beispiel für Textverständnis). Es gibt zwei Haupttexte, der Original- und Plain-Language-Text. Der Originaltext lautet: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." Der Plain-Language-Text lautet: "Die Gewerkschaft setzt sich dafür ein, zum Beispiel für höhere Löhne oder mehr Urlaub ein." Darüber hinaus sind verschiedene Sprachverarbeitungs-Techniken wie Substitution, Clause Deletion, Reordering und Word Deletion aufgezeigt, die auf den Plain-Language-Text angewendet wurden.</sample>
    <sample id="6">Das Bild zeigt eine Übersetzung eines englischen Texts in die deutsche Sprache. Der englische Titel lautet "Text Simplification Example", was auf eine Beispieleinlage für die Einfachstellung von Texten hinweist. Die deutsche Übersetzung des englischen Inhalts lautet: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." Diese Übersetzung wurde unter dem Titel "Plain Language" angezeigt, was darauf hindeutet, dass der Text in einfacherer Sprache dargestellt wird.</sample>
    <sample id="7">Das Bild zeigt eine Präsentation über "Text Simplification Example". Es gibt eine Überschrift, die den Titel des Slides anzeigt. Darunter sind zwei Textblöcke zu sehen: einer original und einer in "Plain Language". Der Originaltext lautet: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." Dieser Text wird durch verschiedene Arten von Bearbeitungen verändert, wie Substitution, Clause Deletion, Reordering und Word Deletion. Diese Veränderungen werden durch blaue und rote Linien dargestellt. Im unteren Teil des Slides steht der "Plain Language"-Text: "Die Gewerkschaft setzt sich dafür ein, zum Beispiel für höhere Löhne oder mehr Urlaub ein." Diese Änderung wurde durch eine grüne Linie und einen grünen Block mit dem Wort "Insertion" markiert.</sample>
    <sample id="8">2. DE-plain A New Corpus</sample>
    <sample id="9">Deutsche Textsimplifizierungskorpora</sample>
    <sample id="10">Deutsche Textsimplifizierungs-Korpora</sample>
    <sample id="11">Deutsche Textsimplifizierungs-Korpora</sample>
    <sample id="12">Deutsche Textsimplifizierungs-Korpora</sample>
    <sample id="13">Deutsche Textsimplifizierungskorpora</sample>
    <sample id="14">Typen von Einfachstellung</sample>
    <sample id="15">Typen von Einfachstellung</sample>
    <sample id="16">Typische Arten von Einfachstellung</sample>
    <sample id="17">图片中的文字内容包括两个标题和一些图表数据。第一个标题是“Types of Simplification”，下面列出了四种类型：news、bible、L2和fiction，每种类型后面跟着一个样本数量（n）。第二个标题是“Simplification Transformations”，下面是一个柱状图，显示了不同简化方法之间的比较。柱状图中包含了两个标签：DEplain-apa和DEplain-web。</sample>
    <sample id="18">Typen der Einfachstellung</sample>
    <sample id="19">Der englische Text 'Automatic alignment and simplification' wird auf Deutsch zu 'Automatische Ausrichtung und Einfachstellung' übersetzt.</sample>
    <sample id="20">Das Bild zeigt eine Präsentation über "Automatic Alignment Evaluation". Der Schwerpunkt liegt auf den Ergebnissen der Anordnungsmethoden mit einer Koeffizientenverhältnis von 1:1 (oben) und n cm Koeffizienten (unten). Die Tabelle zeigt die Leistungen verschiedener Anordungsmethoden, wie Sent-LA-LEASE, CATS-C3G, VecAlign, BERTAlign und MASSAlign, bei verschiedenen Anordnungsverhältnissen. Jede Methode hat eine Beschreibung und eine Reihe von Zahlen, die ihre Leistung in verschiedenen Kategorien anzeigt. Im oberen rechten Eck des Bildes ist ein Mann zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="21">Das Bild zeigt eine Präsentation über "Automatic Alignment Evaluation". Der Titel der Präsentation ist in blauer Schrift dargestellt. Darunter befindet sich ein Tabelle, die Ergebnisse der Algorithmen für die Automatic Alignment Evaluation zeigt. Die Tabelle hat zwei Abschnitte: der obere Abschnitt zeigt die Ergebnisse mit einer 1:1 Korrelation, und der untere Abschnitt zeigt die Ergebnisse mit n:cm Korrelationen. Die Tabelle enthält Spalten für den Namen des Algorithmen, eine Beschreibung und die Korrelationswerte (P, R, F, PR, F1, P-R, F). Die Algorithmen sind LHA-Sent-LAISE, CATS-C3G, VecAlign, BERTAlign und MASSAlign. Jedes Algorithmus hat eine Beschreibung, die auf seine Funktionalität und Methoden hinweist. Der Hintergrund der Präsentation ist weiß, und es gibt einen kleinen Teil eines Menschen im Bild, der möglicherweise das Präsentationsumfeld betreibt.</sample>
    <sample id="22">Das Bild zeigt eine Präsentation über die automatische Alignment-Evaluation. Im oberen Teil des Bildes befindet sich ein blaues Banner mit dem Text "Automatic Alignment Evaluation" in weißer Schrift. Darunter ist ein Tabellenübersatz zu sehen, der die Ergebnisse der Alignment-Methoden zeigt. Der Tabellentitel lautet "Results of the alignment methods with 1:1 (upper part) and ncm capabilities (lower part)". Die Tabelle ist in zwei Hauptspalten unterteilt: eine für die 1:1 Korrelation und eine für die ncm-Fähigkeiten.

Die erste Zeile der Tabelle enthält die Spaltenüberschriften "Name", "Description" und die Korrelationswerte für die 1:1 Korrelation (P, R, F, PR, F1, PRC). Die folgenden Zeilen sind für verschiedene Alignment-Methoden wie Sent-LA-LBSE, CATS-C3G, VecAlign, BERTAlign und MASSAlign aufgelistet. Jede Methode hat eine Beschreibung und die Korrelationswerte für die 1:1 Korrelation und die ncm-Fähigkeiten angegeben.

Im Hintergrund ist ein Mann zu sehen, der wahrscheinlich die Präsentation gibt oder sie beobachtet.</sample>
    <sample id="23">Das Bild zeigt eine Präsentation über "Automatic Alignment Evaluation". Der Titel der Präsentation ist in einem blauen Banner mit weißen Text zu sehen. Darunter befindet sich ein Tabellenübersatz, der die Ergebnisse der Alignment-Methoden für die Beziehung zwischen 1:1 und n:cm-Relationen präsentiert. Die Tabelle hat zwei Hauptabschnitte: den oberen, der die Ergebnisse für 1:1-Anwendungen zeigt, und den unteren, der die Ergebnisse für n:cm-Anwendungen zeigt.

Die Tabelle enthält Spalten mit folgenden Kopfzeilen: "Name", "Description" und eine Reihe von Spalten mit Werten (P, R, F, PR, F1, n:cm). Jede Methode wird in der "Name"-Spalte aufgelistet, und ihre Beschreibung wird in der "Description"-Spalte beschrieben. Die Werte in den Spalten sind wahrscheinlich die Ergebnisse der Evaluation, die die Leistung der verschiedenen Alignment-Methoden für die genannten Anwendungen darstellen.

Im Hintergrund ist ein Mann zu sehen, der möglicherweise die Präsentation gibt oder das Bild betrachtet. Er sitzt vor einem Computermonitor und scheint sich auf das Präsentieren oder das Lesen des Inhaltes zu konzentrieren.</sample>
    <sample id="24">Das Bild zeigt eine Präsentation über "Automatische Alignment-Evaluation" (Automatic Alignment Evaluation). Der Titel ist in einem blauen Balken mit weißen Texten zu sehen. Im unteren Teil des Bildes befindet sich ein Tabellenübersatz, der die Ergebnisse der Alignment-Methoden für 1:1 und n:cm-Anordnungen zeigt. Die Tabelle hat zwei Hauptspalten: eine für die 1:1-Anordnung und eine für die n:cm-Anordnung. Jede Methode wird in der ersten Spalte aufgelistet, gefolgt von einer Beschreibung ihrer Funktionalität. Die zweite Spalte zeigt die Ergebnisse für die 1:1-Anordnung, und die dritte Spalte zeigt die Ergebnisse für die n:cm-Anordnung. Die Zahlen in der Tabelle repräsentieren die Leistung der verschiedenen Alignment-Methoden in Form von Prozentsätzen. Im Hintergrund der Präsentation ist ein Mann zu sehen, der wahrscheinlich über eine Webcam aufgenommen wurde.</sample>
    <sample id="25">Das Bild zeigt eine Präsentation über die automatische Alignment-Evaluation. Im oberen Teil des Bildes befindet sich ein blaues Banner mit dem Text "Automatic Alignment Evaluation". Darunter ist ein Tabularisch dargestellt, der die Ergebnisse der Alignment-Methoden zeigt, wobei die oberste Hälfte die Ergebnisse für eine 1:1-Relation und die untere Hälfte für n cm-Relationen präsentiert. Die Tabellen sind unter den Methoden namens LSA-LEUSE, CATS-C3G, VecAlign, BERTAlign und MASSAlign aufgeteilt. Jede Methode hat eine Beschreibung und eine Reihe von Zahlen, die wahrscheinlich die Leistung oder Korrektur der jeweiligen Methode darstellen. Im Hintergrund ist ein Mann zu sehen, der sich in einem Arbeitsbereich befindet, was nahelegt, dass dies eine digitale Präsentation oder Online-Klasse sein könnte.</sample>
    <sample id="26">Das Bild zeigt eine Präsentation über die Automatic Alignment Evaluation. Der Titel der Präsentation ist "Automatic Alignment Evaluation". Das Hauptinhalt des Bildes ist ein Tabellenvergleich, der verschiedene Alignment-Methoden und ihre Leistungen in verschiedenen Sprachen bewertet. Der Tabellentitel lautet "Results of the alignment methods with 1:1 (upper part) and ncm capabilities (lower part)". Die Spalten im Tabellenvergleich sind für die Beurteilung der Methoden vorgesehen, wobei die Zahlen wahrscheinlich die Leistung oder Korrektur der jeweiligen Methode darstellen. Im Hintergrund der Präsentation ist ein Mann zu sehen, der wahrscheinlich den Vortrag gibt.</sample>
    <sample id="27">Automatische Textsimplifizierung</sample>
    <sample id="28">Automatische Textsimplifizierung</sample>
    <sample id="29">Automatische Textsimplifizierung</sample>
    <sample id="30">Automatische Textsimplifizierung</sample>
    <sample id="31">Automatische Textsimplifizierung</sample>
    <sample id="32">Das Bild zeigt eine Präsentation über "Automatische Textsimplifizierung" (Automatic Text Simplification). Es gibt zwei Hauptabschnitte: "Dokumentebene" (Document Level) und "Satzebene" (Sentence Level). Jeder Abschnitt zeigt Ergebnisse für die Textsimplifizierung mit einem speziellen Modell namens mBART. Die Daten umfassen Metriken wie BLEU, F1, Precision, Recall und ROUGE. Der Text auf dem Bild ist in Englisch, und es gibt keine deutsche Übersetzung bereitgestellt. Im Hintergrund ist ein Mann zu sehen, der wahrscheinlich die Präsentation führt oder beobachtet.</sample>
    <sample id="33">Das Bild zeigt eine Präsentation über die automatische Textsimplifizierung. Im oberen Teil des Bildes ist der Titel "Automatic Text Simplification" zu sehen, was auf die Hauptthematik hinweist. Darunter sind zwei Hauptabschnitte: "Document Level" und "Sentence Level". Jeder Abschnitt enthält eine Tabelle mit Ergebnissen für die Textsimplifizierung, die auf das Verarbeiten von feinzeugendem mBART basiert. Die Tabellen enthalten verschiedene Metriken wie "BLEU", "F1", "PPL" und andere, die wahrscheinlich auf die Qualität der simplifizierten Texte abzielen. Diese Informationen deuten darauf hin, dass das Thema sich auf die Messung und Analyse der Effektivität der Textsimplifizierung in verschiedenen Sprachverarbeitungs-Aufgaben konzentriert.</sample>
    <sample id="34">Danke. Für weitere Details, bitte our Paper prüfen. Und freue mich darauf, unseren Poster im ACL 2023 Kongress zu besuchen.</sample>
    <sample id="35">Patrick Fernandes, Kayo Yin, Emmy Liu</sample>
    <sample id="36">T5 XL</sample>
    <sample id="37">Ja, sie funktionieren noch.</sample>
    <sample id="38">Der neue Bewertungssatz für menschliche Bewertungen umfasst eine Kategorisierung von Chat-Verhalten in Kategorien wie "Irrelevant", "Mangel an Empathie" und "Selbstkontradiktion".</sample>
    <sample id="39">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Qualität der Labels ab.</sample>
    <sample id="40">Das Ergebnis kann noch verbessert werden, indem man die Annotatoren dazu einlädt, mindestens eine gesamteilige Song zu hören oder zu lesen.</sample>
    <sample id="41">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="42">Der englische Text lautet: "Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination Adam Przeździeckiowski und Michał Woźniak INSTITUTE OF COMPUTER SCIENCE POLISH ACADEMY OF SCIENCES ul. Jana Kazimierza 5, 01-248 Warsaw UNIVERSITY OF WARSAW ACL 2023"</sample>
    <sample id="43">Der englische Text lautet: "Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="44">Der englische Text lautet: "Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="45">Das Bild zeigt eine Präsentation über die Abhängigkeitsstruktur der Koordination. Es gibt vier verschiedene Methoden, um Koordinationsstrukturen zu strukturieren: Bouquet/Stanford (Universal Dependencies), Chain/Moscow, Conjunction-headed/Prague und Multi-headed/London. Jede Methode wird durch ein Diagramm dargestellt, das den Abhängigkeitsverlauf zwischen den Worten "Homer", "Lisa", "Bart" und "Maggie" zeigt. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich die Präsentation führt. Der Titel der Präsentation lautet "Dependency Structure of Coordination".</sample>
    <sample id="46">Der englische Inhalt lautet: "Dependency Structure of Coordination" und zeigt eine Übersetzung des englischen Textes auf dem Bildschirm.</sample>
    <sample id="47">Der englische Text lautet: "Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="48">Der englische Inhalt lautet: "Dependency Structure of Coordination" und "Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="49">Der englische Text lautet: "Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="50">Der englische Inhalt lautet: "Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="51">Der englische Text lautet: "Word order tends to minimize dependency lengths:"</sample>
    <sample id="52">Der englische Text lautet: "Dependency Length Minimization (DLM)" und "Word order tends to minimize dependency lengths:" Diese Phrase wird auf dem Bildschirm angezeigt, zusammen mit zwei Beispielen von Satzstrukturen.</sample>
    <sample id="53">Der englische Inhalt lautet: "Word order tends to minimize dependency lengths:"</sample>
    <sample id="54">Das Bild zeigt eine Präsentation über "Dependency Length Minimization (DLM)" auf einem Computerbildschirm. Der Titel der Präsentation ist "Dependency Length Minimization (DLM)" und befindet sich in einer blauen Schachtel am oberen Rand des Bildes. Der Haupttext lautet: "Word order tends to minimize dependency lengths:" und gibt Beispiele für "good" und "bad" Satzstrukturen an. Im Hintergrund sind zwei Diagramme zu sehen, die den Satz "Marge read it yesterday." zeigen, wobei die Wortfolge "it" in einem Satz und "it" in einem anderen Satz dargestellt wird. Diese Diagramme dienen als visuelle Erklärung der Konzept des Minimierens von Abhängigkeitslängen in Satzstrukturierung. Im unteren Teil des Bildes sind weitere Diagramme zu sehen, die den Satz "Marge read this absolutely fascinating book about bees yesterday." zeigen, wobei das Wort "this" in einem Satz und das Wort "about" in einem anderen Satz dargestellt wird. Diese Diagramme weiterhin die visuelle Darstellung der Konzepte der DLM-Strukturierung darstellt. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich die Präsentation leitet oder demonstriert.</sample>
    <sample id="55">Der englische Inhalt lautet: "Dependency Length Minimization (DLM)" und "Word order tends to minimize dependency lengths:".</sample>
    <sample id="56">Der englische Text lautet: "Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths:"</sample>
    <sample id="57">Die englische Phrase "Dependency Length Minimization (DLM)" bedeutet "Abhängigkeitslänge-Minimierung". Diese Methode zielt darauf ab, die Abhängigkeiten zwischen Worten in einer Sprache zu minimieren, um eine bessere Verständlichkeit und Struktur im Satz zu gewährleisten.</sample>
    <sample id="58">Der englische Text lautet: "Word order tends to minimize dependency lengths:" Diese Phrase wird auf dem Bildschirm dargestellt, was eine visuelle Darstellung der Ziele des Dependency Length Minimization (DLM) darstellt.</sample>
    <sample id="59">Der englische Inhalt lautet: "Word order tends to minimize dependency lengths:"</sample>
    <sample id="60">Der englische Text lautet: "Dependency Length Minimization (DLM)" und "Word order tends to minimize dependency lengths:" Diese Bedeutung ist in einem Diagramm zu sehen, das die Verbformung von "Marge read it yesterday" zeigt. Der englische Text wurde ins Deutsche übersetzt und zeigt die Grundlage für die Darstellung von Grammatikregeln im Kontext der Dependency Length Minimization (DLM).</sample>
    <sample id="61">Der englische Inhalt lautet: "Word order tends to minimize dependency lengths:"</sample>
    <sample id="62">Das Bild zeigt eine Präsentation über "Koordinationslängen in Englisch". Es gibt einen Titel, der "Koordinationslängen in Englisch" aufweist. Darunter steht: "Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al., 1993; Ficler und Goldberg 2016)". Die Statistiken deuten darauf hin, dass Koordinatentypen meist kürzer sind (beobachtet), und diese Tendenz wird mit Längenunterschieden verbunden (Gibson et al., 1996:88-90). Es wird auch erwähnt, dass dies nur der Fall ist, wenn der Regierende links oder nicht vorhanden ist (Beispiel: "Ich sah Bart und Lisa; Homer kam und schnurrte"). Der Text schließt mit einer Bemerkung über Koordinationslängen, wenn sie korrekt sind ("Es ist nicht der Fall, wenn sie korrekt sind (Ted und Ted lachen umher...)"). Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, der möglicherweise den Vortragenden zeigt.</sample>
    <sample id="63">Der englische Text lautet: "Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before) this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88–90), but only when the governor is on the left or absent (I saw Bart and Lisa: Homer came and sneezed), not when it is on the right (Ted and Ned laughed)."</sample>
    <sample id="64">Konjunktionslängen in Englisch Statistiken über Koordination extraviert aus einer verbesserten Version des Penn Treebank (Marcus et al. 1993, Ficler und Goldberg 2016): - Links-Koordinaten werden im Allgemeinen kürzer (vorher beobachtet), - diese Neigung wächst mit der Längenunterschied (kurz bemerkt bei Gibson et al. 1996: 88-90) - nur wenn der Gouverneur auf der linken oder abwesenden Seite ist (ich sah Bart und Lisa: Hama kam und schnurrte), - nicht, wenn der Gouverneur auf der rechten Seite ist (Ted und Ted lachte).</sample>
    <sample id="65">Inhalt des Bildes: - Titel: "Conjunct Lengths in English" - Text: Statistiken über Koordinationsstruktur extrahiert aus einer verbesserten Version der Penn Treebank (Marcus et al. 1993, Ficler und Goldberg 2016): - left Conjuncts tendenziell kürzer zu sein (beobachtet wurde), - diese Tendenz wächst mit der Längenunterschied (kurz erkannt in Gibson et al. 1996: 88-90) - aber nur, wenn der Regierende auf der linken oder rechten Seite ist (Ich sah Bart und Lisa; Homer kam und schnurrte), - nicht, wenn der Regierende auf der rechten Seite ist (Ted und Ned lachten)</sample>
    <sample id="66">Der englische Text lautet: "Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88–90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed)."</sample>
    <sample id="67">Das Bild zeigt eine Präsentation über "Konjunktionslängen in Englisch". Der Titel ist in einem blauen Balken oben positioniert. Die Hauptinhaltszeile lautet: "Statistiken über Koordination extrahiert aus einer verbesserten Version des Penn Treebank (Marcus et al., 1993; Ficler und Goldberg 2016):" - das ist auf weißem Hintergrund mit schwarzen und grünen Texten geschrieben. Es gibt drei Punkte: 1. "Links-Konjunktionen werden im Allgemeinen kürzer (vorher beobachtet)," 2. "Dieser Trend wächst mit der Längenunterschied," und 3. "Aber nur, wenn der Regierende auf der linken oder abwesenden Seite ist (Ich sah Bart und Lisa; Homer kam und schnurrte)," und "Nicht, wenn es auf der rechten Seite ist (Ted und Ted lachten)." Im oberen rechten Eck befindet sich ein kleiner Bildschirm mit einem Mann, der wahrscheinlich die Präsentation macht.</sample>
    <sample id="68">Das Bild zeigt eine Präsentation über "Konjunktionslängen in Englisch". Es gibt eine Überschrift und vier Punkte, die Informationen über Konjunktionslängen im Englischen bereitstellen. Die Informationen sind aufgrund von Forschungen von Marcus (1993), Ficler und Goldberg (2016) abgeleitet und beziehen sich auf die Penn Treebank. Die Punkte sind in verschiedenen Farben markiert, was möglicherweise eine visuelle Unterscheidung zwischen den Informationen andeutet. Im Hintergrund ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich das Präsentation verlässt.</sample>
    <sample id="69">Das Bild zeigt eine Präsentation über "Konjunktionslängen in Englisch" (Conjunct Lengths in English). Es gibt Informationen über Koordinationsstatistiken, die aus einer verbesserten Version des Penn Treebank extrahiert wurden (Marcus et al. 1993, Ficler und Goldberg 2016). Die Längen von Koordinaten werden diskutiert, wobei festgestellt wird, dass sie im Allgemeinen kürzer sind (beobachtet wurde) und wie diese Länge mit der Länge der Verbindungsworte verbindet wird (Gibson 1996: 88-90). Der Text betont auch, dass die Koordinierung nur, wenn der Gouverneur auf der linken oder auf der rechten Seite ist (Beispiele sind Bart und Lisa, Homer und Sneezed), und nicht, wenn es auf der rechten Seite ist (Beispiele sind Ted und Ned lachen).</sample>
    <sample id="70">Die Abbildung zeigt eine Diagrammarrangliste mit neun Diagrammen, die alle ähnliche Trends zeigen. Jedes Diagramm hat einen Titel, der mit den Wörtern "CHARACTERS", "SYLLABLES" und "WORDS" beginnt, was darauf hinweist, dass sie sich auf die Länge von Zeichen, Sylben und Wörtern im Text beziehen. Die X-Achse alle Diagramme zeigt "Absoluter Unterschied in Länge (in Zeichen)", während die Y-Achse "Proportionen von kürzern links-oberen Abschnitten basierend auf der absoluten Differenz in Länge (mit Konfiidenzintervallen)" zeigt. Diese Diagramme scheinen ein Statistisches Studie oder Analyse zu darstellen, die die Verhältnisse zwischen verschiedenen Längenmaßern im Text analysiert.</sample>
    <sample id="71">Das Bild zeigt eine Kombination aus Graphen und einem Text. Im oberen rechten Eck ist ein kleiner Bildschirm zu sehen, der einen Menschen zeigt, der möglicherweise über einen Computer-Tastatur sitzt. Der Hauptteil des Bildes besteht aus neun Graphen, die in drei Zeilen und drei Spalten angeordnet sind. Jeder Graph zeigt eine Linechart mit einem blauen Linienverlauf, die den Prozentsatz der "Proportion of shorter left-justified lines depending on the absolute difference of line lengths (with confidence bands)" darstellt. Die X-Achse der Graphen zeigt den "Absolute Difference in Line Lengths", während die Y-Achse den "Proportion of shorter left-justified lines" anzeigt. Jeder Graph hat einen Titel, der auf die Länge der Zeichen, Sylablen oder Wörter im Text hinweist, und alle Graphen haben einen ähnlichen Anfangswert von etwa 50%. Der Text unter den Graphen lautet: "Figure 1: Proportions of shorter left-justified lines depending on the absolute difference of line lengths (with confidence bands)".</sample>
    <sample id="72">Das Bild zeigt eine Reihe von neun Graphiken, die alle linearen Diagramme sind. Jeder Graphik zeigt eine lineare Verbindung zwischen zwei Größen, wobei die y-Achse den Proportionen der linken und rechten Länge des Textes angibt und die x-Achse die Abweichung in Zeichen pro Wort darstellt. Die Graphiken sind untergegliedert in verschiedene Kategorien wie 'No governor (length in CHARACTERS)', 'No governor (length in SYLLABLES)', 'No governor (length in WORDS)', 'Governor on the LEFT (length in CHARACTERS)', 'Governor on the LEFT (length in SYLLABLES)', 'Governor on the LEFT (length in WORDS)', 'Governor on the RIGHT (length in CHARACTERS)', 'Governor on the RIGHT (length in SYLLABLES)', und 'Governor on the RIGHT (length in WORDS)'. Jede Graphik hat einen Titel, der die spezifische Kategorie beschreibt. Im unteren Teil des Bildes ist ein Text zu lesen: "Figure 1: Proportions of shorter left and right characters depending on the absolute difference of conjoined length (with confidence bands)".</sample>
    <sample id="73">Das Bild zeigt eine Präsentation über die Kompaktheit von Koordinationsstrukturen in der Sprachanalyse. Der Titel lautet "Kompaktheit bei Koordinationsstrukturen". Es gibt vier verschiedene Strukturen, jede mit einem Beispiel-Satz: "Homer liebt Lisa, Bart und Maggie." Diese Strukturen sind Bouquet/Stanford (Universal Dependencies), Chain/Moscow, Conjunction-headed/Prague und Multi-headed/London. Jede Struktur wird mit einer Klarheit oder Unklarheit markiert, wie sie den Satz interpretiert. Die Strukturen Bouquet/Stanford und Chain/Moscow sind markiert als "NEIN", was bedeutet, dass sie den Satz nicht korrekt interpretieren. Contrastingly, Conjunction-headed/Prague und Multi-headed/London sind markiert als "JA", was bedeutet, dass sie den Satz korrekt interpretieren.</sample>
    <sample id="74">Die englischen Texte auf den Bildern lauten: 'See the paper for the full argument!' und 'Talk to us at the poster session!'</sample>
    <sample id="75">Es sind drei Autoren an der Arbeit beteiligt: Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="76">Das Bild zeigt eine Analyse von Simplifizierungsdarstellungen für verschiedene Bereiche wie Nachrichten, Bibel, L2 und Literatur. Die Simplifizierungsarten sind in drei Kategorien unterteilt: Simplicity, LexSimp und StructSimp. Das Bild zeigt auch eine Tabelle mit verschiedenen Simplifizierungsveränderungen, einschließlich "moving", "engineering", "lexical substitution", "addition", "word deletion" und "word addition". Diese Veränderungen werden für zwei verschiedene Plattformen, DEPlain-apa und DEPlain-web, verglichen.</sample>
    <sample id="77">Beispiel für kurze linke Konjunktionen: "I saw Bart and Lisa; Homer came and sneezed."</sample>
    <sample id="78">Ja, Sie können die Modelle für Ihre Forschung verwenden.</sample>
    <sample id="79">DEplain-apa beinhaltet Dokumente aus dem Internet.</sample>
    <sample id="80">Das Bild zeigt eine Präsentation mit einem Titel "Conclusion" (Zusammenfassung). Es gibt drei Hauptpunkte: 1. Eine gute Allgemeinverbreitung erfordert ein besseres Modellarchitektur, einen größeren Modellgröße und mehr Anforderungen für die Ausbildung. 2. Die Abnutzung der Leistung durch Zeitverlauf und nicht adaptive Übergreifung. 3. Ob die CoNLL-2003-Tagger noch funktionieren. Am Ende wird festgestellt, dass die CoNLL-2003-Tagger immer noch funktionieren.</sample>
    <sample id="81">Die Tendenz wurde gemessen, indem sie aufgrund der Länge der Konjunktionen im Text gemessen wurde.</sample>
    <sample id="82">The experiments were designed to examine the effects of varying the position of a governor on left and right sides, as well as different syllable lengths. The results show that these changes can lead to significant differences in outcomes when controlling for character length or word count.</sample>
    <sample id="83">Basisklassifizatoren können nicht gut überleben, wenn sie mit ungewogenen Daten trainiert werden. Das bedeutet, dass sie nicht sehr genau sind und ihre Leistung nicht besser als Zufall ist.</sample>
    <sample id="84">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="85">Die Personen im Beispielgespräch sind Alice, Bob und Carol.</sample>
    <sample id="86">Kontextsensitive MÜ-Modelle sind besser bei Diskursphänomenen wie Formality und lexical cohesion.</sample>
    <sample id="87">The authors belong to Johns Hopkins University, Purdue University, and MIT.</sample>
    <sample id="122">Das Framework quantifiziert die Positionalität durch die Sammlung von 300 Instanzen, die mit einem assoziierten Goldlabel identifiziert sind. Diese Instanzen werden dann an einen Studienbetrieb übergeben, um eine breitere Annotierung durch verschiedene Teilnehmer durchzuführen.</sample>
    <sample id="155">The prior study involved human subjects who were given the same persona prompts as in this slide.</sample>
    <sample id="156">In this study, the following data sources were used: Penn Treebank (Marcus et al. 1993), Ficler and Goldberg 2016.</sample>
    <sample id="157">Es sind zwei Autoren an der Arbeit beteiligt.</sample>
    <sample id="158">Kognitive Dissonanz kann mit inneren Widersprüchen, Konflikten oder Divergenzen zwischen den überlegten oder glaubten Informationen und den tatsächlichen Handlungen oder Erfahrungen in Bezug auf eine Situation, eine Person oder ein Objekt in der Welt verbunden sein.</sample>
    <sample id="159">Es sind zwei Autoren an der Arbeit beteiligt.</sample>
    <sample id="160">Es sind sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="161">The framework is unique because it combines annotation with demographic data to analyze model performance. It uses Pearson's R scores for comparison, which helps in understanding how well the models perform across different demographics and datasets.</sample>
    <sample id="162">GPT-4 hat die meisten Überschneidungen mit dem Lexikon der Stereotypen.</sample>
    <sample id="163">Das Bild zeigt eine Präsentation über die Ergebnisse des MuDA-Benchmark-Szenarios. Es gibt zwei Hauptpunkte: 1. Context-aware-Modelle erzielen signifikant bessere Leistungen in bestimmten Phänomena wie 'Formality' und 'lexical cohesion'. 2. DeepL überpervormt Google in den meisten Phänomena und Sprachpaaren.</sample>
    <sample id="164">Das Bild zeigt eine Präsentation mit dem Titel 'From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models'. Die Präsentation wurde am #ACL2023 gehalten. Links unten sind die Logos der Paul G. Allen School, UW NLP und Carnegie Mellon University's Language Technologies Institute zu sehen. Rechts unten ist ein Logo eines Zitats zu erkennen, das auf 'Documenting Large Web Corpora: A Case Study' von Dodge, Jesse usw. in Proceedings of the 15th International Conference on Computational Natural Language Processing (CoNLL) 2021 bezieht sich. Darüber hinaus sind vier Personen auf dem Bild dargestellt, deren Namen Shangbin Feng, Chan Young Park, Yuhan Liu und Yulia Tsvetkov sind.</sample>
    <sample id="165">Das Bild zeigt eine Präsentation mit dem Titel "LM Training Data" und der Untertitel "A mixed blessing". Es gibt ein Diagramm, das verschiedene Quellen anzeigt, wie es aufgrund der Längen der Stangen im Diagramm erscheint. Links sind die Quellen aufgelistet, und rechts sind die Anzahl der Zeilen (big log) angegeben. Im unteren rechten Eck des Bildes ist ein Zitat von Dodge, Jesse und co. zu sehen: "Documenting Large Web Corpora: A Case Study on the Crowed Corpus - Proceedings of the 2019 Conference on Computational Natural Language Processing 2021."</sample>
    <sample id="166">Der englische Text lautet: "LM Training Data" und "A mixed blessing".</sample>
    <sample id="167">LM Training Data A mixed blessing</sample>
    <sample id="168">Das Bild zeigt eine Präsentation über "LM Training Data" mit dem Titel "A mixed blessing". Es gibt ein Diagramm, das verschiedene Quellen anzeigt, wie "patents", "en.wikipedia.org", "nytimes.com" und mehrere andere. Jede Quelle hat eine Zahlenangabe neben ihr, die wahrscheinlich den Betrag der Daten angibt, die für die Trainingdaten verwendet wurden. Rechts unten ist ein Zitat von Dodge, Jesse, et al., die auf eine Publikation in den Proceedings of the International Conference on Computational Linguistics in 2021 verweist. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirmbild eines Menschen zu sehen.</sample>
    <sample id="169">Das Bild zeigt eine Präsentation, die auf einem weißen Hintergrund präsentiert wird. Im oberen linken Eck befindet sich der Text 'To this end' (Zum Ziel). Darunter sind drei Boxen in einer Reihe angeordnet, die den Prozess 'Pretraining data' (Vortraining-Daten), 'Language models' (Sprachmodelle) und 'Downstream tasks' (Niedrigstufe Aufgaben) darstellen. Jede Box ist mit einem gekrümmten Pfeil verbunden, der die Reihenfolge des Prozesses anzeigt. Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich das Vortrag verpasst.</sample>
    <sample id="170">Das Bild zeigt eine Präsentation mit dem Titel "To this end" (Dazu dient dies). Es gibt drei Hauptpunkte: 1. Pretraining data (Vorbereitungsdaten) 2. Language models (Sprachmodelle) 3. Downstream tasks (Niedrigstufe Aufgaben) Jede der Punkte ist in einem separaten Block dargestellt und mit einer kurzen Beschreibung unten angegeben. Die Beschreibungen lauten: - How to evaluate the political leaning of LMs? (Wie man die politische Neigung von LMs bewerten kann?) - How do LMs with different political leanings perform? (Wie leisten Sprachmodelle mit verschiedenen politischen Neigungen?) - Does LM political leaning result in fairness issues in NLP applications? (Ersteht die politische Neigung von LMs bei NLP-Anwendungen Fairheitsproblemen?) Im oberen rechten Eck des Bildes befindet sich ein kleiner Bildschirm, der einen Menschen zeigt, der wahrscheinlich die Präsentation macht.</sample>
    <sample id="171">Auf diesem Bild ist eine Präsentation zu sehen, die sich auf den Prozess der Sprachmodellierung konzentriert. Der Titel lautet "To this end", was auf das Ziel dieser Präsentation hinweist. Das Bild zeigt eine Sequenz von Schritten: "Pretraining data" (Vorbereitungsdaten), "Language models" (Sprachmodelle) und "Downstream tasks" (Niedrigstufe Aufgaben). Diese Schritte sind mit Kurven verbunden, um die Verbindung zwischen ihnen zu verdeutlichen.

Unterhalb dieser Schritte sind zwei Fragen aufgeführt, die auf die politische Ausrichtung von Sprachmodellen und ihre Auswirkungen auf die Fairness in NLP-Anwendungen eingehen. Die ersten beiden Fragen lauten: "Wie können wir die politische Ausrichtung von LMs (Sprachmodellen) bewerten?" und "Inwiefern spielt die Vorbereitung von Daten eine Rolle bei der politischen Ausrichtung?" Die dritte Frage lautet: "Wie leisten LMs mit verschiedenen politischen Ausrichtungen im Vergleich zu anderen Modellen im Bereich der Fairness in NLP-Aufgaben?" Diese Fragen betonen die Bedeutung der politischen Neutralität und der Fairness in der Entwicklung von Sprachmodellen.

Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich die Präsentation führt. Seine Anwesenheit deutet darauf hin, dass dies eine Live-Präsentation oder eine Online-Konferenz sein könnte.</sample>
    <sample id="172">Das Bild zeigt eine Präsentation über die Bewertung von LM-Politik-Abhängigkeit. Es gibt einen Titel "Evaluating LM Political Leaning" und eine Überschrift, die darauf hinweist, dass beide encoder und decoder LMs unterstützt werden. Eine Beispieleinheit lautet: "Do you agree or disagree with this statement?". Darunter steht "Automatic eval" und "Grounded in polisci lit", was auf automatische Bewertungen auf der Grundlage von Politikwissenschaftlicher Literatur hinweist.

Im unteren rechten Eck des Bildes ist ein Diagramm zu sehen, das verschiedene LMs nach ihren politischen Neigungen (libertarientypisch links, autoritarientypisch rechts) und ihren äthetischen Orientierungen (links, rechts) sortiert. Beispiele für LMs wie BERT-base, RoBERTa-base und GPT-2 sind auf dem Diagramm markiert.</sample>
    <sample id="173">Das Bild zeigt eine Karte mit dem Titel "Existing LMs" (existierende LMs, LMs bedeuten Sprachmodelle). Die Karte ist in vier Quadranten geteilt: oberer link, unterer links, oberer rechts und unterer rechts. Jeder Quadrant repräsentiert eine bestimmte Position auf einem politischen Spektrum, das von "Authoritarian" (Autoritär) im oberen linken Eck bis zu "Libertarian" (Libertarier) im unteren rechten Eck geht.

Die Karte zeigt auch eine vertikale Achse, die "Social Control" (soziale Kontrolle) beschriftet ist, und eine horizontale Achse, die "Economic Control" (ökonomische Kontrolle) beschriftet ist. Diese Achsen helfen, die Position der verschiedenen LMs auf dem Spektrum zu verdeutlichen.

Labeled sind verschiedene LMs wie BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, distilRoBERTa, ALBERT-base, ALBERT-large, BART-base, BART-large und Alpaca. Sie sind alle innerhalb des quadratischen Raums positioniert, wobei ihre Beziehungen zur "Social Control"-Achse und zur "Economic Control"-Achse dargestellt werden.

Darüber hinaus sind einige LMs wie Codex, LLAMA, GPT-2, GPT-3-ada, GPT-3-babbage, GPT-3-curie, ChatGPT, GPT-4 und GPT-J außerhalb dieser quadratischen Umgebung positioniert, was darauf hinweist, dass sie sich von den anderen LMs unterscheiden.

Die Karte bietet eine visuelle Darstellung der Positionierung und Beziehung von LMs im politischen und wirtschaftlichen Kontext, was es ermöglicht, die Unterschiede zwischen ihnen zu identifizieren.</sample>
    <sample id="174">Das Bild zeigt eine präsentationelle Slide, die sich auf den Inhalt der vorhandenen LMs (Language Models) konzentriert. Der Titel lautet 'Existing LMs'. Die Slide enthält ein Diagramm mit zwei Achsen: einer horizontal von 'Left' bis 'Right' und eine vertikale von 'Authoritarian' bis 'Libertarian'. Diese Achsen deuten an, dass die LMs in Bezug auf ihre politische Neigung positioniert sind. Links oben auf der Slide sind verschiedene LMs aufgeführt, wie BERT-base, BERT-large, RoBERTa-base usw., und rechts unten sind andere LMs aufgeführt, wie Codex, LLAMA, GPT-2 usw.

Darüber hinaus gibt es eine weitere Slide mit dem Titel 'Pretraining Data', die darauf hinweist, dass LM (RoBERTa, GPT-2) Checkpoints weiter trainiert wurden, um politische Neigung zu bewerten. Diese Slide zeigt zwei Diagramme: eines für 'News Media' und eines für 'Social Media (Reddit)'. Jedes Diagramm hat drei Abschnitte: 'left', 'center' und 'right', was darauf hinweist, dass die Daten aufgrund ihrer politischen Neigung sortiert wurden.</sample>
    <sample id="175">Das Bild zeigt eine Präsentation über "Prätraining-Daten" (Pretraining Data). Es gibt zwei Diagramme, die sich auf den politischen Leaning in der Nachrichtenmedien und auf Social Media wie Reddit konzentrieren. Jedes Diagramm hat drei Abschnitte, die "links" (left), "zentrum" (center) und "rechts" (right) beschriftet sind. Diese Abschnitte sind in verschiedenen Farben dargestellt:</sample>
    <sample id="176">Das Bild zeigt eine Präsentation über "Pretraining Data" und die Auswirkungen von LM (Language Models) wie RoBERTa und GPT-2 auf politische Neigung. Links steht "News Media" mit den Abschnitten "left", "center" und "right". Rechts steht "Social Media (Reddit)" mit den gleichen Abschnitten. Im unteren linken Eck ist ein Text zu sehen, der auf Forschungsarbeiten hinweist, und im unteren rechten Eck sind Zitate von Shen, Girkin und Carolyn Rose zu finden. Der obere Teil des Bildes enthält das Titel "Results" und die Überschrift "Partisan shifts in LM political leaning".</sample>
    <sample id="177">Das Ergebnis zeigt Partizipationsverschiebungen im politischen Vorurteilsverhalten bei Language Models (LM).</sample>
    <sample id="178">Das Ergebnis zeigt die Parteinschwenkungen bei der Lesefähigkeit (LM) im politischen Leaning.</sample>
    <sample id="179">Die englischen Wörter "Results" und "Partisan shifts in LM political leaning" deuten auf die Ergebnisse eines Studien- oder Forschungsprojekts hin, das sich mit Parteipolitik und dem politischen Leaning in Language Models (LM) auseinandersetzt.</sample>
    <sample id="180">Der englische Text lautet: "The Trump Card" und "Pre-45th to post-45th shift".</sample>
    <sample id="181">Der englische Text lautet: "The Trump Card" und "Pre-45th to post-45th shift".</sample>
    <sample id="182">Der englische Text lautet: "The Trump Card" und "Pre-45th to post-45th shift".</sample>
    <sample id="183">Das Bild zeigt eine Präsentation, auf der ein Tabellenüberschnitt über den "Per-Category Performance" dargestellt wird. Der Tabellenüberschrift enthält verschiedene Kategorien wie Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian und White. Jede Kategorie hat eine Reihe von Zahlen darunter, die wahrscheinlich Leistungsdaten oder Statistiken darstellen. Im unteren Teil des Bildes befindet sich eine Tabelle mit einer Beschreibung, die lautet: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and blue darkest denotes worst." Darüber hinaus gibt es in der oberen rechten Ecke des Bildes eine kleine Grafik mit den Worten "1st, 2nd, 3rd, 4th".</sample>
    <sample id="184">Das Bild zeigt eine Tabelle mit der Überschrift "Per-Category Performance". Die Tabelle enthält verschiedene Kategorien wie Hate Speech, BLACK, MUSLIM, LGBTQ+, JEWS, ASAIN, LATINX, WOMEN, CHRISTIAN und WHITE. Jede Kategorie hat eine Reihe von Zahlen darunter, die wahrscheinlich Leistungsdaten oder Ergebnisse darstellen. Im unteren Teil der Tabelle ist ein Text zu sehen, der lautet: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and blue worst denotes." Darüber hinaus gibt es in einem kleinen Bildchen oben rechts einen Personenschein mit den Worten "1st", "2nd" und "3rd" in verschiedenen Farben (rot, grün und blau) angezeigt.</sample>
    <sample id="185">Das Bild zeigt eine Präsentation mit einem Tabelle über die Kategorieleistung. Der Titel lautet "Per-Category Performance". Die Tabelle enthält verschiedene Kategorien wie "Hate Speech", "BLACK", "MUSLIM", "LGBTQ+", "Jews", "ASIAN", "LATINX", "WOMEN", "CHRISTIAN" und "WHITE". Jede Kategorie hat eine Reihe von Zahlen darunter, die wahrscheinlich Leistungsdaten anzeigen. Im unteren Teil der Tabelle ist ein Text zu sehen, der lautet: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and blue worst denotes worst." Im oberen rechten Eck des Bildes ist ein kleiner Text sichtbar, der "1st, 2nd, 3rd, 4th" liest.</sample>
    <sample id="186">Das Bild zeigt eine Tabelle mit dem Titel "Per-Category Performance". Die Tabelle enthält verschiedene Kategorien wie "Hate Speech", "BLACK", "MUSLIM", "LGBTQ+", "Jews", "ASIAN", "LATINX", "WOMEN", "CHRISTIAN" und "WHITE". Jede Kategorie hat zwei Zahlen daneben, die wahrscheinlich die Leistungsdaten darstellen. Darunter steht ein Text: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources." Der Text weiterhin lautet: "The colors are code-colored; dark yellow denotes best and blue worst denotes worst." Im oberen rechten Eck des Bildes sind drei Positionen in roter Schrift angegeben: "1st", "2nd" und "3rd/4th".</sample>
    <sample id="187">Das Bild zeigt eine Tabelle mit dem Titel "Per-Category Performance". Die Tabelle enthält verschiedene Kategorien wie "Hate Speech", "BLACK", "MUSLIM", "LGBTQ+", "Jews", "ASIAN", "LATINX", "WOMEN", "CHRISTIAN" und "WHITE". Jede Kategorie hat eine Reihe von Zahlen darunter, die wahrscheinlich Leistungsdaten oder Ergebnisse anzeigen. Im unteren Abschnitt der Tabelle befindet sich ein Text, der lautet: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and blue worst denotes worst." Darüber hinaus gibt es in der oberen rechten Ecke des Bildes eine kleine Grafik mit den Worten "1st, 2nd, 3rd, 4th".</sample>
    <sample id="188">Das Bild zeigt eine Tabelle mit dem Titel "Per-Category Performance". Die Tabelle ist in zwei Hauptabschnitte unterteilt: "Hate Speech" und "Misinformation". Jeder Abschnitt enthält eine Reihe von Kategorien wie BLACK, MUSLIM, LGBTQ+, JEWS, ASAIN, LATINX, WOMEN, CHRISTIAN, WHITE. Innerhalb dieser Kategorien sind verschiedene Quellen oder Gruppen aufgelistet, wie REDDIT, NEWS_88_81, NTT_L, CNN(L), GUARD(L), FOX(R), BBART(R), WAT, und NR(R). Jede Zelle in der Tabelle zeigt einen Wert, der wahrscheinlich eine Leistung oder eine Rate darstellt.

Darunter befindet sich eine Legende, die erklärt, dass dunkel gelbe Zellen den besten Ergebnissen (hochsten Werten) entsprechen, während dunkel blaue Zellen die schlechtesten Ergebnisse ( niedrigsten Werten) anzeigen. Am unteren Ende der Tabelle steht ein Text, der lautet: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources." Diese Beschreibung gibt einen Eindruck davon, dass die Tabelle eine Analyse der Leistungen in Bezug auf Hassreden gegen verschiedene Identitätsgruppen und Missinformationsquellen präsentiert.</sample>
    <sample id="189">Die Tabelle zeigt die Leistung in verschiedenen Kategorien bei der Identifizierung von Hasspeech und Mischinformation. Die Kategorien umfassen BLACK, MUSLIM, LGBTQ+, JEWS, ASAIN, LATINX, WOMEN, CHRISTIAN, WHITE, REDDIT, NEWS_88, NEWS_91, NEWS_92, NEWS_93, NEWS_94, NEWS_95, NEWS_96, NEWS_97, NEWS_98, NEWS_99, NEWS_100, NEWS_101, NEWS_102, NEWS_103, NEWS_104, NEWS_105, NEWS_106, NEWS_107, NEWS_108, NEWS_109, NEWS_110, NEWS_111, NEWS_112, NEWS_113, NEWS_114, NEWS_115, NEWS_116, NEWS_117, NEWS_118, NEWS_119, NEWS_120, NEWS_121, NEWS_122, NEWS_123, NEWS_124, NEWS_125, NEWS_126, NEWS_127, NEWS_128, NEWS_129, NEWS_130, NEWS_131, NEWS_132, NEWS_133, NEWS_134, NEWS_135, NEWS_136, NEWS_137, NEWS_138, NEWS_139, NEWS_140, NEWS_141, NEWS_142, NEWS_143, NEWS_144, NEWS_145, NEWS_146, NEWS_147, NEWS_148, NEWS_149, NEWS_150, NEWS_151, NEWS_152, NEWS_153, NEWS_154, NEWS_155, NEWS_156, NEWS_157, NEWS_158, NEWS_159, NEWS_160, NEWS_161, NEWS_162, NEWS_163, NEWS_164, NEWS_165, NEWS_166, NEWS_167, NEWS_168, NEWS_169, NEWS_170, NEWS_171, NEWS_172, NEWS_173, NEWS_174, NEWS_175, NEWS_176, NEWS_177, NEWS_178, NEWS_179, NEWS_180, NEWS_181, NEWS_182, NEWS_183, NEWS_184, NEWS_185, NEWS_186, NEWS_187, NEWS_188, NEWS_189, NEWS_190, NEWS_191, NEWS_192, NEWS_193, NEWS_194, NEWS_195, NEWS_196, NEWS_197, NEWS_198, NEWS_199, NEWS_200, NEWS_201, NEWS_202, NEWS_203, NEWS_204, NEWS_205, NEWS_206, NEWS_207, NEWS_208, NEWS_209, NEWS_210, NEWS_211, NEWS_212, NEWS_213, NEWS_214, NEWS_215, NEWS_216, NEWS_217, NEWS_218, NEWS_219, NEWS_220, NEWS_221, NEWS_222, NEWS_223, NEWS_224, NEWS_225, NEWS_226, NEWS_227, NEWS_228, NEWS_229, NEWS_230, NEWS_231, NEWS_232, NEWS_233, NEWS_234, NEWS_235, NEWS_236, NEWS_237, NEWS_238, NEWS_239, NEWS_240, NEWS_241, NEWS_242, NEWS_243, NEWS_244, NEWS_245, NEWS_246, NEWS_247, NEWS_248, NEWS_249, NEWS_250, NEWS_251, NEWS_252, NEWS_253, NEWS_254, NEWS_255, NEWS_256, NEWS_257, NEWS_258, NEWS_259, NEWS_260, NEWS_261, NEWS_262, NEWS_263, NEWS_264, NEWS_265, NEWS_266, NEWS_267, NEWS_268, NEWS_269, NEWS_270, NEWS_271, NEWS_272, NEWS_273, NEWS_274, NEWS_275, NEWS_276, NEWS_277, NEWS_278, NEWS_279, NEWS_280, NEWS_281, NEWS_282, NEWS_283, NEWS_284, NEWS_285, NEWS_286, NEWS_287, NEWS_288, NEWS_289, NEWS_290, NEWS_291, NEWS_292, NEWS_293, NEWS_294, NEWS_295, NEWS_296, NEWS_297, NEWS_298, NEWS_299, NEWS_300, NEWS_301, NEWS_302, NEWS_303, NEWS_304, NEWS_305, NEWS_306, NEWS_307, NEWS_308, NEWS_309, NEWS_310, NEWS_311, NEWS_312, NEWS_313, NEWS_314, NEWS_315, NEWS_316, NEWS_317, NEWS_318, NEWS_319, NEWS_320, NEWS_321, NEWS_322, NEWS_323, NEWS_324, NEWS_325, NEWS_326, NEWS_327, NEWS_328, NEWS_329, NEWS_330, NEWS_331, NEWS_332, NEWS_333, NEWS_334, NEWS_335, NEWS_336, NEWS_337, NEWS_338, NEWS_339, NEWS_340, NEWS_341, NEWS_342, NEWS_343, NEWS_344, NEWS_345, NEWS_346, NEWS_347, NEWS_348, NEWS_349, NEWS_350, NEWS_351, NEWS_352, NEWS_353, NEWS_354, NEWS_355, NEWS_356, NEWS_357, NEWS_358, NEWS_359, NEWS_360, NEWS_361, NEWS_362, NEWS_363, NEWS_364, NEWS_365, NEWS_366, NEWS_367, NEWS_368, NEWS_369, NEWS_370, NEWS_371, NEWS_372, NEWS_373, NEWS_374, NEWS_375, NEWS_376, NEWS_377, NEWS_378, NEWS_379, NEWS_380, NEWS_381, NEWS_382, NEWS_&lt;|listen|&gt;, NEWS_1800, NEWS_1801, NEWS_1802, NEWS_1803, NEWS_1804, NEWS_1805, NEWS_1806, NEWS_1807, NEWS_1808, NEWS_1809, NEWS_1810, NEWS_1811, NEWS_1812, NEWS_1813, NEWS_1814, NEWS_1815, NEWS_1816, NEWS_1817, NEWS_1818, NEWS_1819, NEWS_1820, NEWS_1821, NEWS_1822, NEWS_1823, NEWS_1824, NEWS_1825, NEWS_1826, NEWS_1827, NEWS_1828, NEWS_1829, NEWS_1830, NEWS_1831, NEWS_1832, NEWS_1833, NEWS_1834, NEWS_1835, NEWS_1836, NEWS_1837, NEWS_1838, NEWS_1839, NEWS_1840, NEWS_1841, NEWS_1842, NEWS_1843, NEWS_1844, NEWS_1845, NEWS_1846, NEWS_1847, NEWS_1848, NEWS_1849, NEWS_1850, NEWS_1851, NEWS_1852, NEWS_1853, NEWS_1854, NEWS_1855, NEWS_1856, NEWS_1857, NEWS_1858, NEWS_1859, NEWS_1860, NEWS_1861, NEWS_1862, NEWS_1863, NEWS_1864, NEWS_1865, NEWS_1866, NEWS_1867, NEWS_1868, NEWS_1869, NEWS_1870, NEWS_1871, NEWS_1872, NEWS_1873, NEWS_1874, NEWS_1875, NEWS_1876, NEWS_1877, NEWS_1878, NEWS_1879, NEWS_1880, NEWS_1881, NEWS_1882, NEWS_1883, NEWS_1884, NEWS_1885, NEWS_1886, NEWS_1887, NEWS_1888, NEWS_1889, NEWS_1890, NEWS_1891, NEWS_1892, NEWS_1893, NEWS_1894, NEWS_1895, NEWS_1896, NEWS_1897, NEWS_1898, NEWS_1899, NEWS_1900, NEWS_1901, NEWS_1902, NEWS_1903, NEWS_1904, NEWS_1905, NEWS_1906, NEWS_1907, NEWS_1908, NEWS_1909, NEWS_1910, NEWS_1911, NEWS_1912, NEWS_1913, NEWS_1914, NEWS_1915, NEWS_1916, NEWS_1917, NEWS_1918, NEWS_1919, NEWS_1920, NEWS_1921, NEWS_1922, NEWS_1923, NEWS_1924, NEWS_1925, NEWS_1926, NEWS_1927, NEWS_1928, NEWS_1929, NEWS_1930, NEWS_1931, NEWS_1932, NEWS_1933, NEWS_1934, NEWS_1935, NEWS_1936, NEWS_1937, NEWS_1938, NEWS_1939, NEWS_1940, NEWS_1941, NEWS_1942, NEWS_1943, NEWS_1944, NEWS_1945, NEWS_1946, NEWS_1947, NEWS_1948, NEWS_1949, NEWS_1950, NEWS_1951, NEWS_1952, NEWS_1953, NEWS_1954, NEWS_1955, NEWS_1956, NEWS_1957, NEWS_1958, NEWS_1959, NEWS_1960, NEWS_1961, NEWS_1962, NEWS_1963, NEWS_1964, NEWS_1965, NEWS_1966, NEWS_1967, NEWS_1968, NEWS_1969, NEWS_1970, NEWS_1971, NEWS_1972, NEWS_1973, NEWS_1974, NEWS_1975, NEWS_1976, NEWS_1977, NEWS_1978, NEWS_1979, NEWS_1980, NEWS_1981, NEWS_1982, NEWS_1983, NEWS_1984, NEWS_1985, NEWS_1986, NEWS_1987, NEWS_1988, NEWS_1989, NEWS_1990, NEWS_1991, NEWS_1992, NEWS_1993, NEWS_1994, NEWS_1995, NEWS_1996, NEWS_1997, NEWS_1998, NEWS_1999, NEWS_2000, NEWS_2001, NEWS_2002, NEWS_2003, NEWS_2004, NEWS_2005, NEWS_2006, NEWS_2007, NEWS_2008, NEWS_2009, NEWS_2010, NEWS_2011, NEWS_2012, NEWS_2013, NEWS_2014, NEWS_2015, NEWS_2016, NEWS_2017, NEWS_2018, NEWS_2019, NEWS_2020, NEWS_2021, NEWS_2022, NEWS_2023, NEWS_2024, NEWS_2025, NEWS_2026, NEWS_2027, NEWS_2028, NEWS_2029, NEWS_2030, NEWS_2031, NEWS_2032, NEWS_2033, NEWS_2034, NEWS_2035, NEWS_2036, NEWS_2037, NEWS_2038, NEWS_2039, NEWS_2040, NEWS_2041, NEWS_2042, NEWS_2043, NEWS_2044, NEWS_2045, NEWS_2046, NEWS_2047, NEWS_2048, NEWS_2049, NEWS_2050, NEWS_2051, NEWS_2052, NEWS_2053, NEWS_2054, NEWS_2055, NEWS_2056, NEWS_2057, NEWS_2058, NEWS_2059, NEWS_2060, NEWS_2061, NEWS_2062, NEWS_2063, NEWS_2064, NEWS_2065, NEWS_2066, NEWS_2067, NEWS_2068, NEWS_2069, NEWS_2070, NEWS_2071, NEWS_2072, NEWS_2073, NEWS_2074, NEWS_2075, NEWS_2076, NEWS_2077, NEWS_2078, NEWS_2079, NEWS_2080, NEWS_2081, NEWS_2082, NEWS_2083, NEWS_2084, NEWS_2085, NEWS_2086, NEWS_2087, NEWS_2088, NEWS_2089, NEWS_2090, NEWS_2091, NEWS_2092, NEWS_2093, NEWS_2094, NEWS_2095, NEWS_2096, NEWS_2097, NEWS_2098, NEWS_2099, NEWS_2100, NEWS_2101, NEWS_2102, NEWS_2103, NEWS_2104, NEWS_2105, NEWS_2106, NEWS_2107, NEWS_2108, NEWS_2109, NEWS_2110, NEWS_2111, NEWS_2112, NEWS_2113, NEWS_2114, NEWS_2115, NEWS_2116, NEWS_2117, NEWS_2118, NEWS_2119, NEWS_2120, NEWS_2121, NEWS_2122, NEWS_2123, NEWS_2124, NEWS_2125</sample>
    <sample id="190">Der englische Inhalt lautet: "The target label is TRUE for the base ASIAN, TRUE for the base CHRIST, FALSE for the base RIGHT, and TRUE for the base LEFT."</sample>
    <sample id="191">Qualitative Analysis Text Target Label Base N-S L-N S-L R-S R-R ASIAN TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE</sample>
    <sample id="192">Der englische Inhalt lautet: "Hate Speech Text" und zeigt eine Liste von Sätzen, die als haineigende oder diskriminierende zu sein scheinen. Diese Sätze sind mit einer Spalte "TRUE" nebeneinander aufgeführt, was darauf hindeutet, dass sie als solche identifiziert wurden. Darunter steht auch "Table 12: Qualitative analysis of hate speech example where TMs political leaning has big impact to".</sample>
    <sample id="193">Das Bild zeigt eine doppelte Seite eines Dokuments, das sich auf den Inhalt und die Analyse von Hasspeech-Texten konzentriert. Links steht ein Titel "Hate Speech Text" und darunter sind mehrere Sätze in englischer Sprache zu sehen. Diese Sätze sind mit einer Tabelle überlagert, die verschiedene Kategorien wie N4, NR, R und RR aufzulisten scheint. Rechts ist ein weiteres Stück Text zu sehen, das "Transcription Test" als Titel hat. Diese Seite enthält auch eine Tabelle mit ähnlichen Kategorien wie N4, NR, R und RR, aber sie bezieht sich auf Transkriptionen oder Überwirkungen von Texten. Im unteren Teil des Dokuments befindet sich eine Tabelle mit dem Titel "Table 12: Qualitative analysis of hate speech examples where political leanings beg to differ", was auf eine qualitative Analyse von Beispieltexten hinweist, die auf die Unterschiede bei politischen Neigungen abzielen. Die gesamte Darstellung suggeriert eine wissenschaftliche oder akademische Analyse von Hasspeech und seiner Verbreitung.</sample>
    <sample id="194">Das Bild zeigt eine detaillierte Tabelle, die sich auf den Inhalt und die Analyse von Hasspeech-Texten konzentriert. Die Tabelle ist in zwei Hauptspalten unterteilt: "Hate Speech Text" und "Transcription Test". Jede dieser Hauptspalten weiterhin in kleinere Spalten unterteilt, die "Date?", "N4", "NR", "R" und "RR" bezeichnen.

Die "Hate Speech Text"-Spalte enthält verschiedene Sätze und Phrasen, die als Hasspeech bezeichnet werden können. Beispiele sind:
- "Some are starting to realize 'diversity' doesn't equal 'acceptance'"
- "Yes Please These rich white people think they're so special and can do anything they want but on the other hand"
- "United States of America is good's good"

Jedes dieser Sätze wird mit einer Kombination aus TRUE und FALSE an den Enden der Spalten markiert, was möglicherweise darauf hindeutet, ob das Statement als Hasspeech betrachtet wurde oder nicht.

Die "Transcription Test"-Spalte ähnelt der "Hate Speech Text"-Spalte, wobei dieselben Sätze und Phrasen auftreten. Diese Sätze werden auch mit TRUE und FALSE an den Enden der Spalten markiert.

Am unteren Ende der Tabelle befindet sich ein Text, der lautet: "Table 12: Qualitative analysis of hate speech example where N4 political leanings beg to differ." Dieser Text gibt einen Hinweis auf die qualitative Analyse der Sätze im Zusammenhang mit politischen Neigungen.

Rechts oben im Bild ist ein kleiner Bildschirm zu sehen, der wahrscheinlich einen Teil eines Videos oder einer Präsentation zeigt. Der Hauptfokus bleibt jedoch auf der Tabelle, die sich auf die Analyse von Hasspeech-Texten und ihre Transkriptionen konzentriert.</sample>
    <sample id="195">Das Bild zeigt eine Präsentation mit zwei Seiten. Die linke Seite hat einen Titel "Hate Speech Text" und enthält eine Tabelle, die verschiedene Beispiele von Hasspeech-Texten aufgelistet hat. Jedes Textbeispiel wird durch eine Spalte mit der Bezeichnung "TRUE" markiert. Darunter steht ein Text "Table 12: Qualitative analysis of hate speech examples where TMs with different political leanings beg to differ." Die rechte Seite des Bildes zeigt eine weitere Tabelle mit dem Titel "Transliteration Test," die mehrere Zeilen mit Text und Markierungen wie "TRUE" und "FALSE" enthält. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirmbild eines Menschen zu sehen. Das Bild wurde während einer Präsentation aufgenommen, wie aus der Präsenz des Menschen im Bildschirmbild hervorgeht.</sample>
    <sample id="196">Die englische Textaufschrift lautet: "Discussion" und "Between Scylla and Charybdis". Darunter steht: "To 'sanitize' or not to 'sanitize', that is the question." Diese Übersetzung gibt einen Eindruck von der Diskussion, die sich um die Entscheidung handelt, ob Daten oder Modelle 'sauber' gemacht werden sollen (sanitize) oder nicht.</sample>
    <sample id="197">Der englische Inhalt lautet: "Discussion Between Scylla and Charybdis To 'sanitize' or not to 'sanitize', that is the question"</sample>
    <sample id="198">Das Bild zeigt eine präsentierte Slide, die mit dem Titel 'Discussion' beginnt. Es handelt sich um eine Diskussion über die Entscheidung zwischen 'Scylla und Charybdis'. Die Slide fasst den Schwerpunkt auf, ob man 'sanitieren' oder nicht 'sanitieren' sollte, was die Frage ist. Sie zeigt einen Prozess, der von 'Pretraining data' (Vorbereitung von Daten) zu 'Language models' (Sprachmodellen) und schließlich zu 'Downstream tasks' (Niedrigstufe Aufgaben) führt. Im oberen rechten Eck des Slides ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich das Präsentationsteilnehmer ist.</sample>
    <sample id="199">Danke</sample>
    <sample id="200">Es sind sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="201">900</sample>
    <sample id="202">Easy on Me (by Adele) und I Gotta Feeling (by the Black Eyed Peas).</sample>
    <sample id="203">Positionality refers to the perspectives that individuals hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow.</sample>
    <sample id="205">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell.</sample>
    <sample id="206">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="207">Ja, das getestete Modell funktioniert in der Testsuite.</sample>
    <sample id="208">Das Bild zeigt eine Präsentation über die drei Varianten von KITMUS. Die Varianten sind: a) BACKGROUND-PRETRAIN, b) BACKGROUND-BOTH und c) BACKGROUND-INFERENCE. Jede Variante wird durch ein Diagramm mit einem Verteidigungsbereich und einer Inferenzlinie darstellt.</sample>
    <sample id="209">The authors, Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis, are affiliated with Google Research. This is indicated by the "Google Research" logo present in the bottom left corner of the slide shown in the image.</sample>
    <sample id="210">Die endgültige Forschungsfrage ist: Wie können wir die verfügbaren Saubermittelsamplien effizienter nutzen?</sample>
    <sample id="211">Die Sensitivitätsmetrik ist eine Methode, um die Reaktionsfähigkeit eines Modells auf verschiedene Anweisungen für dieselbe Aufgabe zu messen.</sample>
    <sample id="212">Wenjun Peng</sample>
    <sample id="213">Eine höhere Sensitivität bedeutet nicht unbedingt eine bessere Leistung des Modells.</sample>
    <sample id="214">Linguistischer Kontext</sample>
    <sample id="215">Das Graphik zeigt, dass ein Minimum von etwa 40 sauberen Validierungsbeispielen für eine gute Leistung an der WSL benötigt wird.</sample>
    <sample id="216">Stanford University</sample>
    <sample id="217">Um die vielfältigen Anwendungen von Sprachmodellen zu erforschen und ihre Verwendung in verschiedenen Bereichen zu optimieren, müssen neue Methoden zur Messung von Medienverzerrungen entwickelt werden.</sample>
    <sample id="218">Kaheer Saleman</sample>
    <sample id="219">The pipeline for spreading political biases is: 1. Pretraining data (e.g., biased news sources) 2. Language models trained on this data 3. The spread of these biases through downstream tasks</sample>
    <sample id="220">Ja, der Vereinfachungsprozess zwischen DEplain-apa und Web unterscheidet sich.</sample>
    <sample id="221">Ja, Coscript ist öffentlich verfügbar.</sample>
    <sample id="222">Das Wasserzeichen wird in den Text eingebettet, indem man eine Target embedding (e_t) definiert und einen Ziffernnummer in einer Satzstruktur einfügt. Danach wird das Target embedding auf die Original embedding (e_o) übertragen, um es mit dem Original zu verbinden.</sample>
    <sample id="223">Die Autoren gehören an der Pennsylvania State University (PennState) und Amazon.</sample>
    <sample id="224">Ja, Encoder-Decoder-Modelle wie MT5 können durch das Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="225">Ein Beispiel für eingeschränkte Sprachplanung ist das Erstellen von Anweisungen für das Herstellen von Kuchen, wie zum Beispiel "Wie man einen Erdbeerenkuchen macht?" und "Wie man einen Schokoladenkuchen macht?".</sample>
    <sample id="226">Die Opazität der Methode wird durch die Verwendung eines spezifischen Farbtons im Plot-Visualisierungsschema gewährleistet. Jeder Punkt auf den Plots hat eine blaue Farbe, was eine einheitliche und konstante Opäzipität für die Daten darstellt. Diese blau-gefärbte Farbe ermöglicht es, die Abwesenheit von anderen Farben oder Schattierungen zu erkennen, was die Opäzipität der Methode sicherstellt.</sample>
    <sample id="227">Das Bild zeigt eine Präsentation über die Vergleichung von Vorbereitungsstrategien und Datenquellen. Es gibt zwei Hauptbereiche: - Evaluierung des Einflusses öffentlicher und privater medizinischer Datenquellen auf die Größe der Daten. - Vergleich von Vorbereitungsstrategien. Im Detail sind folgende Informationen zu sehen: - NACHOS: Eine 1,1 MB große Datei, die aus einer Sammlung von 4,6 MB mit 25,3 MByte von Daten besteht. - NBDW: Eine private Datensammlung von 0,9 MB mit 4,6 MB von Daten. - Vorbereitungsstrategien: - Vom Anfang aus mit Vollmodellkonstruktion - Vorauftraining mit einem vorhandenen vorgelegten Modell (z.B. CamBERT, eine französische Version, und PubMedBERT, eine englische Version). Darüber hinaus sind Tabellen mit den Namen der Strategien, Korpusen, Trainings- und Testdatenmengen dargestellt. Am unteren Rand befindet sich ein Logo mit "Avignon Université".</sample>
    <sample id="228">GPT-4 scheint am wenigsten auf Afrikanische Islamische Länder ausgerichtet zu sein, wie die lowest Score von 0.39 im Bild zeigt.</sample>
    <sample id="229">Es gibt ein Bild, das eine Person zeigt, die an einem Computer-Terminal arbeitet. Der Text auf dem Bild lautet: 'Use already existing offline ST models without re-training or adopting specific architecture for SimuIST.'</sample>
    <sample id="230">Die Anzahl der Aufgaben hat auf die Leistung des Modells eine eindeutige Auswirkung. Das Bild zeigt eine Grafik, die den Modellleistungsverlust anzeigt, wenn die Anzahl der Aufgaben erhöht wird. Die Leistung des Modells sinkt mit der Steigerung der Aufgabenanzahl, was darauf hindeutet, dass die Modelleignung durch die Aufgabenlast reduziert wird.</sample>
    <sample id="231">LSTM seq2seq, TS, Zheng and Lapata</sample>
    <sample id="232">Der erste Autor ist Matthias Lindemann, der zweite Autor ist Alexander Koller und der dritte Autor ist Ivan Titov.</sample>
    <sample id="233">Chowdery et al.</sample>
    <sample id="234">NLP Positionality: Charakterisieren Sie die Gestaltungsbiasen von Datensätzen und Modellen.</sample>
    <sample id="235">NLP Positionality: Charakterisieren Sie die Gestaltungsbiasen von Datenbanken und Modellen</sample>
    <sample id="236">Das Bild zeigt eine Person, die in einem Zimmer sitzt, das mit Büchern und anderen Gegenständen ausgestattet ist. Der Hintergrund suggeriert, dass es sich um einen Arbeitsplatz oder eine Bibliothek handelt. Im oberen rechten Eck des Bildes befindet sich ein kleiner Bildschirm, auf dem die Person zu sehen ist. Links am Bildrand ist der Text "Imagine..." sichtbar, was möglicherweise einen Titel oder eine Aufschrift für den Inhalt darstellt.</sample>
    <sample id="237">Das Bild zeigt einen weißen Hintergrund mit der Aufschrift 'Imagine...' in schwarzen Buchstaben im oberen linken Eck. Im oberen rechten Eck ist ein kleiner Bildschirm zu sehen, auf dem eine Person zu sehen ist, die sich in einem Zimmer befindet, das Bücherregale und andere Gegenstände enthält. Im unteren linken Eck ist ein Profilbild eines Menschen zu sehen, das in einem blauen Kreis dargestellt ist. Der Name 'Carl Jones' wird angezeigt, gefolgt von der Position 'Tech Lead' und dem Unternehmen 'New York Times'. Darüber hinaus ist unten links ein Symbol einer Sprache mit der Aufschrift 'PerspectiveAPI score' und ein Score von 0.82 mit einem grünen Checkbox dargestellt.</sample>
    <sample id="238">Das Bild zeigt zwei Personen, Carl Jones und Aditya Sharma, die Tech Lead sind. Carl Jones gehört zur New York Times, und Aditya Sharma gehört zur Times of India. Beide haben ihre Gesichter verdeckt. Links oben steht "Imagine...". Unterhalb der Abbildung von Carl Jones steht ein Text, der lautet: "Can you stop being a jerk? 😡 (0.82) ✅". Unterhalb des Texts steht "Carl Jones, Tech Lead, New York Times" und unten ist eine Anmerkung: " Pavilion = PerspectiveAPI score". Rechts oben steht "Imagine...". Unterhalb der Abbildung von Aditya Sharma steht ein Text, der lautet: "Presstitutes everywhere on the news. 😡 (0.33) ❌". Unterhalb des Texts steht "Aditya Sharma, Tech Lead, Times of India" und unten ist eine Anmerkung: " Pavilion = PerspectiveAPI score".</sample>
    <sample id="239">Das Bild zeigt eine Präsentation mit dem Titel "Imagine...". Es gibt zwei Personen auf der linken und rechten Seite des Bildes, wobei jeder eine Körperform hat. Links steht "Carl Jones", Tech Lead von New York Times, und rechts "Aditya Sharma", Tech Lead von Times of India. Beide haben Textfelder über ihnen, die jeweils eine Frage und eine Antwort enthalten. Die Fragen sind "Can you stop being a jerk?" (Kannst du einen Arsch haltieren?) und "Presstitutes everywhere on the news." (Pressgeboteln überall auf den Nachrichten). Jede Frage hat ein Score neben ihr, das "0.82" für die erste und "0.33" für die zweite ist. Links daneben steht "Design bias example!" und unten steht "PerspectiveAPI score". Im unteren rechten Eck befindet sich eine Note, die "PerspectiveAPI score" bedeutet.</sample>
    <sample id="240">Positionality</sample>
    <sample id="241">Positionalität</sample>
    <sample id="242">Positionality "Die Perspektiven [Leute] halten aufgrund ihrer Demografie, Identität und Lebenserfahrungen." [Als Forscher] sie beeinflusst die Forschungsprozess und seine Ergebnisse. [1] [1] Savin-Baden, Maggi, und Claire Howell-Major. "Qualitative Forschung: Das essential guide zur Theorie und Praxis." Qualitative Forschung: Das essential Guide zur Theorie und Praxis. Routledge (2013).</sample>
    <sample id="243">Das Thema des Vortrags ist "Do datasets and models have positionality?" (Sind Datensätze und Modelle positionell?), wie auf der Titelseite des Slides angezeigt wird.</sample>
    <sample id="244">Das Thema der Präsentation ist "Do datasets and models have positionality?" (Sind Datensätze und Modelle positionell?). Diese Frage wird in der Titelseite des Slides prominent dargestellt. Darunter sind drei Referenzen auf wissenschaftliche Arbeiten aufgeführt: 1. Blasi, et al.: "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. 2. Ye, et al.: "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models." EMNLP 2022. 3. Cambo &amp; Gergle: "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022. Diese Referenzen deuten darauf hin, dass die Präsentation wahrscheinlich wissenschaftliche Forschungsergebnisse zu den Aspekten der Positionellität von Datensätzen und Modellen in der Sprachtechnologie präsentiert.</sample>
    <sample id="245">Die Folter zeigt eine Slide-Präsentation mit dem Titel "Do datasets and models have positionality?" (Sind Datensätze und Modelle Positionalität haben?). Der Text auf der Slide lautet: "Anecdotal evidence:" (Zufällige Beweise:) - "Model and dataset probing [1][2]" - "Theoretical definitions of model positionality [3]" Diese Slide verweist auf mehrere Forschungsarbeiten, die wahrscheinlich zu den Zufälligen Beweisen für Positionalität in Datensätzen und Modellen beitragen.</sample>
    <sample id="246">Das Bild zeigt eine Präsentation mit dem Titel "Do datasets and models have positionality?" (Sind Datensätze und Modelle Positionell?). Der Text auf der Präsentation lautet: "Anecdotal evidence:" - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] Diese Informationen sind aufgenommen von drei Forschungsarbeiten: 1. Blasi, L. et al.: "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. 2. Ye, X. et al.: "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022. 3. Cambo &amp; Gergle: "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, der einen Menschen zeigt, der wahrscheinlich die Präsentation macht.</sample>
    <sample id="247">Die Datensätze und Modelle haben Positionität?</sample>
    <sample id="248">Das Thema der Präsentation ist "Do datasets and models have positionality?" (Sind Datensätze und Modelle positionell?). Die Präsentation zeigt Anzeichen für Positionellität in Modellen und Datenbanken. Es gibt Hinweise auf Beispiele wie "Model and dataset probing" und "Theoretical definitions of model positionality".</sample>
    <sample id="249">Das Bild zeigt eine Person, die sich in einem Zimmer befindet, das mit Schränken und anderen Möbeln eingerichtet ist. Die Person trägt ein weißes Hemd und hat schwarze Haare. Im Bild ist auch ein Text zu sehen, der lautet: "Question: Do datasets and models have positionality? Goal: Compare annotations from users with existing datasets and models." Darunter gibt es einen Pfeil, der auf den Text zeigt.</sample>
    <sample id="250">NLP Positionality: Eine Plattform zur Charakterisierung von Gestaltungsbiasen in NLP-Datensätzen und -Modellen.</sample>
    <sample id="251">Das Framework umfasst die Schritte "Gathering", "Processing" und "Analysis".</sample>
    <sample id="252">Das Bild zeigt einen Framework, der auf einem weißen Hintergrund dargestellt wird. Im oberen linken Eck befindet sich der Titel "Framework". Der Hauptinhalt des Bildes ist ein Diagramm, das den Prozess der Datenanalyse und -verarbeitung in drei Schritten illustriert:</sample>
    <sample id="253">Das Framework umfasst eine Vielzahl von Schritten und Prozessen, die für die Annotierung von Datensätzen erforderlich sind. Zunächst werden Datensätze sammelt, wie auf der oberen rechten Seite des Bildes dargestellt. Diese Datensätze werden dann mit einer Vielzahl an Annotatoren erneut erstellt, was eine Vielfalt an Sichten und Interpretationen ermöglicht.</sample>
    <sample id="254">Das Bild zeigt ein Framework für die Annotierung von Datensätzen. Es beginnt mit der Sammlung von 300 Datensätzen, die aus einer Datenbank gezogen werden. Jeder Satz hat eine zugehörige Goldlabel. Dann werden 10 Instanzen auf eine Studie über IT-Werkzeuge ausgewählt und an eine Vielzahl von Annotatoren übermittelt. Schließlich werden die Annotatoren die Labels für diese Instanzen bearbeiten. Der Prozess wird dargestellt durch eine Reihe von Grafiken und Texten, die die verschiedenen Schritte des Frameworks darstellen.</sample>
    <sample id="255">Das Bild zeigt eine präzise visuelle Darstellung eines Frameworks für die Analyse von Daten. Das Framework wird in drei Hauptabschnitten aufgeteilt: Sammlung, Verarbeitung und Analyse.</sample>
    <sample id="256">Das Bild zeigt eine detaillierte Schematik eines Frameworks, das auf Englisch beschriftet ist. Das Schemat wird in drei Hauptabschnitten unterteilt: "Collection", "Processing" und "Analysis". Jeder Abschnitt enthält verschiedene Schritte und Prozesse, die für das Erstellen und Analyse eines Frameworks erforderlich sind.

Im "Collection"-Abschnitt werden 100 Instanzen aus einem Datensatz gezogen, jede davon hat ein goldenes Label. Diese Labels sind aufgrund eines Studienvergleichs mit dem UTTW erstellt. Darüber hinaus werden Anmerkungen von Menschen aus der ganzen Welt bereitgestellt.

Im "Processing"-Abschnitt werden Modelle für Vorhersagen erstellt. Diese Vorhersagen werden dann an die erhaltenen Anmerkungen und das Dataset übermittelt.

Im "Analysis"-Abschnitt werden verschiedene Demografieinformationen wie Altersgruppen, Geschlechter, Ethnische Gruppen und Länder betrachtet. Es wird auch erwähnt, dass Pearson's correlation zur Vergleichung von Anmerkungen mit dem Dataset und verschiedenen Modellen durchgeführt wird.

Rechts neben dem Schemat ist ein Text zu sehen, der lautet: "2) Compare annotations by demographic to models and datasets via Pearson's R scores." Dies bedeutet, dass die Anmerkungen nach ihrer Demographie mit den Modellen und den Datenmengen verglichen werden, um Pearson's R scores zu ermitteln.

Das Bild bietet einen umfassenden Überblick über das Verfahren, wie ein Framework entwickelt, überprüft und analysiert wird, indem es spezifische Schritte und Prozesse aufzeigt.</sample>
    <sample id="257">LabintheWild ist ein Forschungsprojekt, das sich auf die Untersuchung des Rechtssystems und seiner Einflussfaktoren konzentriert. Das Projekt bietet eine Plattform für Forscher und Forschende, um ihre Arbeit zu präsentieren und sich in der Forschungsgemeinschaft zu engagieren. Die Website präsentiert verschiedene Studien und Experimente, wie zum Beispiel eine Untersuchung der Ethik von KI-Systemen oder die Wahrnehmung von Technologie sceptischen Forschern gegenüber Techno-Utopien. Durch die Teilnahme an diesen Studien können Teilnehmer wertvolle Informationen über das Rechtssystem und seine Entwicklung erhalten und sich in einem Netzwerk von Forschern und Forschenden engagieren.</sample>
    <sample id="258">LabintheWild</sample>
    <sample id="259">Auf dem Bild ist eine Aufgabe mit dem Titel "Task A: Social Acceptability" zu sehen. Diese Aufgabe besteht aus drei Schritten: 1) Lesen des Szenarios, das lautet: "Wanted to make a lot of money." 2) Überlegen und Bewertung der Situation, wobei die Bewertungen von "Very bad" bis "Very good" vorgestellt werden. 3) Überlegung über die Meinungen anderer AI-Systeme und die Bewertung von Studienangeboten in den Vereinigten Staaten.</sample>
    <sample id="260">Auf Englisch lautet der Inhalt: "Task A: Social Acceptability"</sample>
    <sample id="261">Aufgabe A: Soziale Akzeptabilität Analyse Datasets - Soziale Chemie Modelle - Delphi - GPT-4</sample>
    <sample id="262">Auf dem Bild ist ein Text zu sehen, der lautet: "Participants read an instance from the Dynathe dataset." Diese Phrase bedeutet, dass Teilnehmer eine Instanz von der Dynathe-Datensammlung lesen.</sample>
    <sample id="263">Auf Deutsch lautet der englische Inhalt: 'Task B: Toxicity' 'Analysis' 'Datasets - Dynahate' 'Models - Perspective API - Rewire API - Hate RoBERTa - GPT-4'</sample>
    <sample id="264">Ergebnisse: Wer sind die NLP-Datensätze und -Modelle für die Anordnung geeignet?</sample>
    <sample id="265">Datensets und Modelle sind insbesondere für Sprachen wie Englisch ausgerichtet.</sample>
    <sample id="266">Datenbanken und Modelle sind am besten mit Menschen aus Hochschulausbildung ausgerichtet.</sample>
    <sample id="267">Daten und Modelle sind am besten mit Menschen aus Hochschulausbildung zugeordnet.</sample>
    <sample id="268">Ergebnis 2: Einige Bevölkerungsgruppen werden zurückgelassen.</sample>
    <sample id="269">Datenbanken und Modelle sind weniger mit Non-Binary-Leuten ausgerichtet.</sample>
    <sample id="270">Also, was können wir tun? Positionelle Probleme in der NLP angesprechen.</sample>
    <sample id="271">Empfehlungen: 1. Halten Sie eine Liste aller relevanten Gestaltungsauswahlen, die während der Erstellung von Datensätzen oder Modellen getroffen wurden. 2. Führen Sie NLP-Forschung durch das Perspektivismuslenner. a. Teilen Sie aufgetrennte Datensatzetiketten</sample>
    <sample id="272">Empfehlungen: 1. Halte einen Record von allen relevanten Gestaltungsauswahlen auf, die während der Erstellung von Datensätzen oder Modellen getroffen wurden. 2. Führe Sprachverarbeitung durch den Spiegel der Perspektivisierung: a. Teile die aufgeglierte Datensatzauswahl b. Benutze Modelltechniken, die Annotatordiskrepanz vertragen können. 3. Entwickle spezialisierte Datensätze und Modelle mit und für bestimmte Gemeinschaften, die für inklusive Sprachverarbeitung wertvoll sind (z.B. Masakhane initiativ). [1] https://www.masakhane.io</sample>
    <sample id="273">Vielen Dank Dashboards-Link: nlppositionality.cs.washington.edu/ Paper: bit.ly/NLPositionality-Paper/</sample>
    <sample id="274">Die Referentin geht auf vier Probleme von SimulST ein.</sample>
    <sample id="275">Soziale und politische Verzerrungen in Datensätzen können durch die Implementierung eines sogenannten 'Sanitierungsprozesses' reduziert werden. Dies bedeutet, dass man die Daten vor dem Training von NLP-Modellen überprüft und eventuell korrigiert oder entfernt, was soziale und politische Biase enthalten könnte. Durch diese Vorbereitung können sichergestellt werden, dass die Modelltraining auf einer reibungslosen Basis mit minimalem Risiko für unerwünschte Verzerrungen im Ergebnis abläuft.</sample>
    <sample id="276">Die 61. Tagung der Gesellschaft für Computational Linguistik in Toronto, Kanada vom 9.-14. Juli 2023 präsentiert die Vortragung "Zusammensetzung von Skriptwissen aus großen Sprachmodellen für die Planung von eingeschränkten Sprachen" von Siyu Yuan und Co. Diese Vortragung diskutiert die Verarbeitung von Skriptwissen aus großen Sprachmodellen für die Planung von eingeschränkten Sprachen. Die Vortragenden sind Siyu Yuan, Jiangjie Chen, Ziquan Fu, Soham Shah, Charles Robert Jankowski, Yanghua Xiao und Deqing Yang. Das Ereignis wird von Brain Technologies Inc. unterstützt.</sample>
    <sample id="277">Der englische Inhalt lautet: "Large language models (LLMs) can effectively decompose goals into steps."</sample>
    <sample id="278">Sprich die englischen Inhalte auf Deutsch wortwörtlich wieder: "Language Planning" bedeutet "Sprachplanung". "How to Make a Cake?" bedeutet "Wie man einen Kuchen macht?". Die Zutaten sind in der gleichen Reihenfolge aufgeführt, aber die Temperaturen werden in Grad Celsius angegeben. "Cream the butter and sugar." bedeutet "Mix Butter und Zucker zu Schlagsahne". "Stir in the eggs." bedeutet "Misch die Eier ein". "Pour the batter into the pan." bedeutet "Gießen Sie den Teig in den Backform". "Bake for 1 hour at 350°F (175°C)." bedeutet "Backen Sie für 1 Stunde bei 350°F (175°C)".</sample>
    <sample id="279">Bezogenen Sprachplanung</sample>
    <sample id="280">Das Bild zeigt eine Person, die sich auf einem Computerbildschirm befindet. Sie scheint in einem modernen Büro oder Wohnzimmer zu sein, mit einem weißen Wand und einem großen Fenster im Hintergrund. Der Schreibtisch vor ihr ist ordentlich angeordnet, mit einem Laptop und einem Stuhl. Das Bild ist ein Zoom-Screenshot, was darauf hindeutet, dass es möglicherweise während einer Online-Konferenz oder -Veranstaltung aufgenommen wurde.</sample>
    <sample id="281">Gefolgt von der englischen Übersetzung: "Abstract goal kann von verschiedenen realen, spezifischen Zielen mit mehreren Aspekten angenommen werden."</sample>
    <sample id="282">Die LLMs erledigen bei beschränkten Sprachplanungen</sample>
    <sample id="283">Wann leisten sich LLMs bei der Strukturierung von Sprache?</sample>
    <sample id="284">Wann leisten sich LLMs bei der Durchführung von Grenzgebietssprachplanung?</sample>
    <sample id="285">Kann LLMs konstruierte Sprachplanung durchführen? Alle Baseline-Methoden erzielen bei der Planung für bestimmte Ziele unzufriedensstellende Ergebnisse.</sample>
    <sample id="286">Kann LLMs konstruktionsbezogene Sprachplanung machen? Alle Baseline-Methoden erzielen unzufriedensstellende Ergebnisse bei der Planung für bestimmte Ziele.</sample>
    <sample id="287">Was sind die meist vorkommenden Fehler, die von LLMs bei dieser Aufgabe gemacht werden?</sample>
    <sample id="288">Was sind die typischen Fehler, die von LLMs bei dieser Aufgabe gemacht werden?</sample>
    <sample id="289">Was sind die Arten von Zielen, die InstructGPT typischerweise fehlt?</sample>
    <sample id="290">Das Bild zeigt eine Präsentation über eine Methode zur Erstellung von spezifischen Zielen. Der Titel der Präsentation lautet "Method". Links im Bild befindet sich ein Text, der den Schritt 1 beschreibt: "Generieren Sie spezifische Ziele mit InstructGPT über Kontext-Learning". Darüber hinaus ist der Haupttext "Abstract Goal: Make a cake" zu sehen, was auf das Ziel hinweist, ein Kuchen herzustellen. Unter diesem sind zwei spezifische Ziele aufgeführt: G1 (+ modifier) Chocolate cake und G3 (+ method) Make in a microwave. Am Ende des Textes ist noch ein weiterer spezifischer Zielangabe G3(+intent): Make a cake for a wedding zu finden. Rechts im Bild ist eine Person zu sehen, die wahrscheinlich die Präsentation gibt oder verfolgt.</sample>
    <sample id="291">Method Input: ein abstrakter Ziel Specific Goals: G1 (+ Modifikator) ein Schokoladenkuchen backen G3 (+ Method) in einem Mikrowelle backen G3(+ intent): für eine Hochzeit ein Kuchen backen</sample>
    <sample id="292">Der englische Text lautet: "Method Input: an abstract goal Step 1: Generate specific goals with InstructGPT via in-context learning Specific Goals: G1 (+ modifier): Make a cake with chocolate chips. G2 (+ modifier): Make it in the microwave. G3(+ intent): Make for a wedding. Step 2: Over-generate candidate scripts with InstructGPT via in-context learning Generate Plans for G1 Candidate Scripts 1 2 ... k"</sample>
    <sample id="293">Der englische Text lautet: "Method Step 2 Over-generate candidate scripts via in-context learning. Step 3 Find the goal with InstructGPT via similarity score. Output: Specific goals with corresponding scripts."</sample>
    <sample id="294">Das Bild zeigt eine Person mit braunem Haar und einem grünen Hemd, die sich auf einem Computerbildschirm befindet. Der Bildschirm zeigt eine Präsentation mit dem Titel "Method" in großen roten Buchstaben am oberen Rand. Die Präsentation umfasst drei Schritte: 1. "Step 2: Over-generate candidate scripts via in-context learning", 2. "Step 3: Find the goal with InstructGPT via similarity score", und 3. "Output: Specific goals with corresponding scripts". Darunter sind zwei Diagramme zu sehen: ein Diagramm mit "Candidate Scripts" und eine Abbildung von "Filtered Scripts". Die Person scheint sich auf einer Videokonferenz oder Präsentation zu befinden, wie aus der Kameraansicht im Hintergrund hervorgeht.</sample>
    <sample id="295">Method Step 2: Übergenerieren von Kandidaten-Skripts durch In-Context-Learning Step 3: Filterung der Skripte auf die Zielerstellung mithilfe des Score-Systems der InstructGPT Output: Spezifische Ziele mit entsprechenden Skripten</sample>
    <sample id="296">Unsere Methode verbessert die Planqualität erheblich.</sample>
    <sample id="297">Das Bild zeigt eine Person, die auf einem Computerbildschirm zu sprechen scheint. Sie trägt Brille und ein grünes Hemd. Im Hintergrund ist ein offenes Büro mit mehreren Tischen und Stühlen zu sehen. Der Schreibtisch des Sprechenden hat einige Gegenstände darauf, darunter einen Laptop und einige Dokumente. Das Bild wirkt wie ein Zoom- oder Videokonferenz-Szenario, da der Sprecher direkt auf dem Bildschirm sichtbar ist.</sample>
    <sample id="298">Das Hauptthema des Vortrags ist "Script Distillation aus LLMs" (Schriftentwicklung aus LLMs).</sample>
    <sample id="299">Das Bild zeigt eine Präsentation über "Script Distillation from LLMs" (Script Distillation aus LLMs). Die Präsentation wird auf der linken Seite des Bildes präsentiert, während ein Mann im Bild rechts zu sehen ist. Der Text auf der Präsentation lautet: "Script Distillation from LLMs", gefolgt von "Motivation" und "Method". Der Motivationssatz lautet: "To enable constrained language planning ability for smaller models." Der Methodensatz lautet: "Follow the idea of symbolic knowledge distillation. Generated 55,000 scripts with constraint from LLMs based on our method =&gt; Coscript Dataset. Human annotate validation and test set." Darüber hinaus sind drei Schritte aufgeführt: Step 1: Generate specific goals with InstructGPT via in-context learning; Step 2: Over-generate candidate scripts with constraints with InstructGPT via in-context learning; Step 3: Filtered scripts to the goal with InstructGPT via in-context learning with a corresponding similarity score. Der Text auf der Präsentation ist englisch, und der Mann auf dem Bild trägt Brille und steht in einem Zimmer mit Tischen und Stühlen im Hintergrund.</sample>
    <sample id="300">Das Bild zeigt eine Präsentation über "Script Distillation from LLMs" (Script Distillation von LLMs). Die Präsentation besteht aus zwei Hauptabschnitten: "Motivation" und "Method". Im "Motivation"-Abschnitt steht: "Um eine beschränkte Sprachplanung für kleinere Modelle zu ermöglichen." Im "Method"-Abschnitt steht: "Folgen Sie der ideologischen Wissensverteilung. Generiert 55.000 Skripte mit Restriktion auf Basis von LLMs aufgrund unserer Methode: Coscript Dataset. Annotiert manuell für Validierung und Test." Darüber hinaus sind drei Schritte in einem Diagramm aufgeführt: 1. Generieren spezifischer Goals mit InstructGPT über Contextual Learning. 2. Generieren übergegenerierter Skripts mit InstructGPT über Contextual Learning. 3. Filteren der Skripte nach dem Ziel mit InstructGPT über Symbolik Score. Das Ergebnis ist die Definition von Goals mit entsprechenden Plänen.</sample>
    <sample id="301">Das Bild zeigt eine Präsentation über "Script Distillation from LLMs" (Script Distillation aus LLMs). Die Präsentation ist in zwei Hauptabschnitten unterteilt: "Motivation" und "Method". Im "Motivation"-Abschnitt steht: "Um eine englischsprachige Planungsfähigkeit für kleinere Modelle zu ermöglichen." Im "Method"-Abschnitt wird die folgende Schrittfolge erläutert: 1. Generieren Sie spezifische Ziele mit InstructGPT über interaktiver Lernung. 2. Erstellen Sie überdurchschnittliche Skripte mit InstructGPT über interaktiver Lernung. 3. Filtern Sie die Skripte mit InstructGPT über einen Symbolikscore. Darüber hinaus wird erwähnt, dass 55.000 Skripte mit Restriktionen generiert wurden, basierend auf der Methode, und dass diese Skripte von Menschen für Validierung und Test geprüft wurden. Im "Output"-Abschnitt steht: "Output: Spezifische Ziele mit entsprechenden Anweisungen." Im Bild ist auch ein Video-Chat-Frame sichtbar, in dem eine Person zu sehen ist, die möglicherweise das Präsentation überträgt.</sample>
    <sample id="302">Das Bild zeigt eine Präsentation über 'Constraint Analysis' und 'Coscript for Smaller Language Models'. Links auf dem Bild ist ein Text, der 'Coscript shows high heterogeneity and pluralism in the generated specific goals.' liest. Darüber hinaus sind zwei Sektoren im Text zu sehen: 'Datasets', die 'Coscript' und 'wikiHow' aufzulisten, und 'Metrics', die 'Faithful: DeBERTa (v3 large) model to decide whether the generated texts are faithful to the constraints' und 'Automatic metrics: ROUGE, BLEU, BERTScore' aufzuführen. Rechts oben ist ein kleiner Bildschirm mit einer Person, die in einem modernen Büro mit großen Fenstern steht. Im unteren linken Eck des Bildes befindet sich ein Graph, der 'Specialized Models vs. LLMs' zeigt und verschiedene Modelle wie GPT-3, Codex, InstructGPT, T5 trainiert auf wikiHow und T5 trainiert auf Coscript vergleicht. Der Text am unteren Rand liest: 'Smaller LM's fine-tuned on Coscript can generate higher quality scripts than LLMs.'</sample>
    <sample id="303">Spezialisierte Modelle versus LLMs</sample>
    <sample id="304">Summarie und Schlussfolgerungen:</sample>
    <sample id="305">Zusammenfassung und Schlussfolgerungen</sample>
    <sample id="306">Die 61. Treffen der Gesellschaft für Computational Linguistik in Toronto, Kanada vom 9.-14. Juli 2023.</sample>
    <sample id="307">The slide indicates that PaLM's fluency is comparable to SOTA (State-of-the-Art) systems, but its accuracy scores are generally lower. These accuracies tend to be dominated by "Accuracy/Omission" issues and result in a more awkward style or awkwardness compared to other models.</sample>
    <sample id="308">Das wichtigste Eigenschaft eines Wasserzeichenverfahrens ist seine Anwendungsfähigkeit für EaaS (Embedded Advertising as a Service).</sample>
    <sample id="309">English, Español, Français, Italiano, Nederlands, Português, Română, Русский, Türkçe, 中文</sample>
    <sample id="310">30 instances are extracted from a dataset for re-annotation.</sample>
    <sample id="311">The distance metrics used to measure the difference between harmless and backdoor data samples are similarity difference and p-value of KS test.</sample>
    <sample id="312">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in der Aufgabe als Teil der Gruppe "Enc-Dec: Multilingual Pretrained Encoder-Decoder Models" eingesetzt.</sample>
    <sample id="344">Die Autoren entscheiden, was Wörter mit mittlerer Häufigkeit sind, indem sie die Wörter aus einem Textcorpus Zähln, und dann aus einer bestimmten Intervallzeitraum zufällig n Wörter auswählen.</sample>
    <sample id="345">Das Thema der Präsentation lautet "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?". Die Präsentation wurde von Shuheng Liu und Alan Ritter, beiden Mitgliedern des School of Interactive Computing an der Georgia Institute of Technology, gehalten.</sample>
    <sample id="346">Namen-Entitätserkennung und allgemeinert</sample>
    <sample id="347">Der Titel des Sliks ist "Named Entity Recognition &amp; Generalization". Der Text auf dem Slik lautet: "Models have been using CoNLL-2003 to develop NER for almost 20 years. Can these models generalize to modern data?" Im unteren rechten Eck des Sliks befindet sich das Logo von Georgia Tech, und im unteren linken Eck ist ein Bild eines Menschen zu sehen.</sample>
    <sample id="348">Modelln wurden seit fast 20 Jahren auf Basis von CoNLL-2003 für die Entwicklung von NER verwendet. Können diese Modelle zu modernen Daten generalisieren? Was ist für eine gute Generalisierung erforderlich?</sample>
    <sample id="349">Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years Can these models generalize to modern data? What is needed for good generalization? What causes the performance drop?</sample>
    <sample id="350">Der Inhalt lautet: 'CoNLL++ Dataset' und 'Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines'.</sample>
    <sample id="351">CoNLL++ Dataset Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines Fine-tuned 20+ models on CoNLL-2003 Evaluated on CoNLL-2003 test set &amp; CoNLL++ AMBASSADOR O TO THE UNITED NATIONS : LINDA I-PER THOMAS-GREENFIELD I-PER Georgia Tech</sample>
    <sample id="352">CoNLL++ Dataset</sample>
    <sample id="353">Was benötigt man für eine gute Allgemeinheit?</sample>
    <sample id="354">Was ist für eine gute allgemeinereignis erforderlich?</sample>
    <sample id="355">Was sind für eine gute Allgemeinfähigkeit erforderlich?</sample>
    <sample id="356">Das Thema des Vortrags ist "Was für eine gute Allgemeinfähigkeit benötigt?" (Was ist für eine gute allgemeinsteigerung erforderlich?). Der Vortrag gibt einen Überblick über die Bedingungen, die für eine gute allgemeinsteigerung erforderlich sind. Diese Bedingungen umfassen: - Modellarchitektur: Transformer-Modelle generalisieren besser. - Modellgröße: Größere Modelle generalisieren besser. - Anzahl der fine-tuning-Beispiele: Mehr Beispiele führen zu einer besseren allgemeinsteigerung. Diese Informationen werden auf einem Graphen dargestellt, der die Leistung von RoBERTa und Flair zeigt, wobei RoBERTa in der Regel eine höhere Leistung hat als Flair.</sample>
    <sample id="357">Was verursacht eine Leistungsverfall?</sample>
    <sample id="358">Das Bild zeigt eine Präsentationsaufnahme mit dem Titel "Was verursacht einen Leistungsabfall?". Der Text auf der Seite ist in Gold und blaue Farbe, und es gibt einen Punkt, der "Adaptive Overfitting?" liest. Im unteren rechten Eck befindet sich das Logo von Georgia Tech. Darüber hinaus befindet sich ein kleiner Profilbild eines Menschen im unteren linken Eck des Bildes.</sample>
    <sample id="359">Was verursacht einen Leistungsverlust?</sample>
    <sample id="360">Was verursacht eine Leistungsabnahme? Adaptive Overfitting? Temporal Drift?</sample>
    <sample id="361">Der Titel der Präsentation lautet 'Was verursacht eine Leistungsabfall?' (Was causes performance drop?).</sample>
    <sample id="362">Was verursacht eine Leistungsverlust?</sample>
    <sample id="363">Was verursacht einen Leistungsabfall? Adaptive Übergreifende Überfitting? Keine abnehmenden Rendite? Nicht beobachtet? Temporaler Drift?</sample>
    <sample id="364">Das Bild zeigt eine Präsentation, die den Grundstein für eine Diskussion über die Ursachen eines Leistungsabfalls legt. Der Titel der Präsentation lautet "Was verursacht einen Leistungsabfall?" (Was causes a performance drop?). Links auf dem Bild sind vier Punkte aufgeführt: 1. Adaptive Overfitting 2. Keine Diminishing Returns 3. Nicht beobachtet 4. Temporal drift (Temporal drift) Die rechte Seite des Bildes zeigt ein Tabellenüberschuss mit Informationen zu verschiedenen Modellen und ihren Leistungsverläufen von 2003 bis 2018. Darüber hinaus gibt es ein Graphik, das die Entwicklung der Leistung über diese Jahre darstellt. Im unteren rechten Eck ist das Logo der Georgia Tech zu sehen.</sample>
    <sample id="365">Die Ursachen für eine Leistungsverluste sind: - Adaptive Überschneidung (Adaptive overfitting) - Keine wachsenden Rückerträge (No diminishing returns) - Nicht beobachtet (Not observed) - Temporale Drift (Temporal drift), das bedeutet, dass die Leistung abfällt, wenn der zeitliche Abstand größer wird.</sample>
    <sample id="366">Für eine gute Allgemeinheit benötigen wir: - Bessere Modellarchitektur - Größere Modellgröße - Mehr Beispiele</sample>
    <sample id="367">Das Bild zeigt eine Präsentation mit einem Schwerpunkt auf dem Schlussabschnitt. Links steht der Titel "Conclusion" in bold, weißer Schriftart. Darunter sind zwei Punkte aufgeführt: 1. "Für eine gute allgemeinere Verbreitung benötigen wir:" - Diese Punktliste enthält die folgenden Punkte: - Bessere Modellarchitektur - Größere Modellgröße - Bessere Anleitungen für die Ausbildung 2. "Performanzabfall wird durch:" - Diese Punktliste enthält die folgenden Punkte: - Temporal Drift - Nicht anpassender Übergreifung Rechts oben befindet sich ein Graphik, der den Zeitraum von 2004 bis 2022 abdeckt und verschiedene Modelle zeigt, darunter Stanford NER, Illinois NER, BSLSTM-CNN-CRF und BSLSTM-Batch. Der Graphik zeigt die Entwicklung der Leistung dieser Modelle über die Jahre. Im unteren rechten Eck des Bildes ist das Logo der Georgia Tech-Universität zu sehen.</sample>
    <sample id="368">Der Inhalt auf dem Bild lautet: "Conclusion For a good generalization, we need: - Better model architecture - Larger model size - More fine-tuning examples Performance drop is caused by: - Temporal drift - Not adaptive overfitting Do CoNLL-2003 taggers still work? YES!"</sample>
    <sample id="369">Zusammenfassung: Wir benötigen eine bessere Modellarchitektur, einen größeren Modellgröße und mehr scharfe Beispiele für eine bessere allgemeine Leistung. Der Leistungsabfall wird durch Temporal drift und nicht anpassender Overfitting verursacht.</sample>
    <sample id="370">Paper: https://arxiv.org/abs/2212.09747 Dataset: https://github.com/ShuhengL/ac2023_conllpp Contact: sliu775@gatech.edu</sample>
    <sample id="397">The segment size used in this approach is 16.</sample>
    <sample id="398">Der englische Inhalt, der im Beispiel mit Servin und Kea vorgelegt wird, erfordert entitätsspezifisches Wissen.</sample>
    <sample id="399">Der wichtigste Faktor zwischen der Qualität des Beispiels und der Ähnlichkeit mit dem Ausgangssatz ist die Qualität des Beispiels selbst.</sample>
    <sample id="400">The slide titled 'Existing LMs' shows a chart with two axes: the vertical axis represents 'Social Media Bias,' and the horizontal axis indicates political orientation, ranging from 'Authoritarian' on the left to 'Libertarian' on the right. The chart plots various language models (LMs) such as BERT-base, RoBERTa-base, distilRoBERTa, ALBERT-base, BART-base, and others across these dimensions. Red lines connect each model to its respective position in the matrix.</sample>
    <sample id="401">Das Modell verwendet Aufmerksamkeitswerte aus mehreren Ebenen.</sample>
    <sample id="402">Direct reference: "easy on me," "the first one" Indirect reference: "The newer one."</sample>
    <sample id="403">Die Autoren gehören an der Fudan University.</sample>
    <sample id="404">Es sind 7 Autoren an der Arbeit beteiligt.</sample>
    <sample id="405">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe eines maschinellen Übersetzungsmodells vor dem semantischen Parsing wurde als Baseline betrachtet.</sample>
    <sample id="406">Die Autoren haben eine Gruppe von "woman warrior" (markierte Gruppe) als Beispiel für eine markierte Gruppe gegeben.</sample>
    <sample id="407">Transformer models.</sample>
    <sample id="408">Die Testdatensätze werden als FT_C, LORAC, BitFitFC, AdapterC und MLC bezeichnet.</sample>
    <sample id="409">Es sind sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="410">The authors use multiple modalities in their work.</sample>
    <sample id="439">According to the authors, the area of NLU that is not sufficiently explored enough is "inference-time knowledge."</sample>
    <sample id="440">Zhiyang Xu, Ying Shen, Lifu Huang</sample>
    <sample id="441">Ja, Coscript hat eine Qualitätskontrolle durchlaufen.</sample>
    <sample id="442">Die Grenzen bestehender Ressourcen für kontextabhängige Übersetzung liegen darin, dass nur ein kleiner Teil der Wörter aufgrund des Kontexts abhängt und dass die bestehenden Methoden begrenzte Diskursphänomene und Sprachen unterstützen.</sample>
    <sample id="443">Das Bild zeigt eine Präsentationsseite mit dem Titel "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)". Der Text ist auf einem weißen Hintergrund platziert und wird von farbigen Linien verziert, die sich quer über das Bild erstrecken. Im unteren rechten Eck befindet sich ein kleiner Bildschirm eines Menschen, der wahrscheinlich den Vortrag gibt. Links am unteren Rand befindet sich das Google Research Logo. Die Namen der Autoren sind auch angegeben: Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis.</sample>
    <sample id="444">Das Bild zeigt eine Präsentationsseite mit dem Titel "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)". Der Titel ist auf der oberen Hälfte des Bildes prominent dargestellt. Unter dem Titel sind die Namen von mehreren Personen aufgeführt: Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis. Im unteren linken Eck des Bildes befindet sich das Logo von Google Research. Die gesamte Präsentationsseite hat ein weißes Hintergrund, und es sind einige gestylte, farbige Linien in Blau, Rot und Grün überlagert, die möglicherweise als visuelle Elemente zur Gestaltung dienen.</sample>
    <sample id="445">Indirekte Bezeichnungen können in natürlichem und flüssigem Gespräch verwendet werden: - Man kann den Namen nicht mehr erkennen - Die Anordnungen sind schwierig zu unterscheiden - Man möchte eine Vorliebe angeben</sample>
    <sample id="446">Indirekte Beziehungsphrasen</sample>
    <sample id="447">Der englische Inhalt lautet: "Indirect Referring Expressions" und "Goal: Understanding users' language when they make a choice".</sample>
    <sample id="448">Indirekte Beziehungsformeln</sample>
    <sample id="449">Indirekte Beziehungsphrasen</sample>
    <sample id="450">Das Hauptthema der Präsentation ist "Dataset Collection" (Datenbanksammlung).</sample>
    <sample id="451">Das Hauptthema der Präsentation ist "Dataset Collection" (Datenbanksammlung).</sample>
    <sample id="452">Das Bild zeigt eine Präsentation von Google Research über ein Dataset-Sammelverfahren. Das Verfahren betont die Unformalität durch das Verwendung eines Cartoon-Beilagenaufgaben. Im Vordergrund sind vier Figuren zu sehen, die sich in einem Gespräch befinden. Neben einer der Figuren steht ein Text, der darauf hinweist, dass dies "Durch den Annotator ausgefüllt wird". Der Titel der Präsentation lautet "Dataset-Sammelverfahren" und es gibt auch einen weiteren Text, der auf die Bedeutung der Dialogumgebungen hinweist, die aus mehreren manuellen Prompten pro Domain gewählt werden sollen.</sample>
    <sample id="453">Das Bild zeigt eine Präsentation über die Datensammlungsmethode. Im oberen rechten Eck ist das Google Research Logo zu sehen. Der Titel "Dataset Collection Methodology" wird in bold Schriftart angezeigt. Links unten befindet sich ein Text, der "Revising Indirect Referring Expressions for Entity Selection in Multimodal Conversations" liest. Die Präsentation scheint auf die Verwendung eines Cartoon-Beispielbildes zu konzentrieren, um die Methode der Datensammlung zu demonstrieren.</sample>
    <sample id="454">Das Bild zeigt eine Präsentation von Google Research über das "Dataset Collection Methodology". Der Schwerpunkt liegt auf der Verwendung eines "cartoon completion task" zur Emittierung von Informalität. Das Bild zeigt eine Reihe von Figuren mit Sätzen, die als Dialogszenen dienten, und einen Text, der erklärt, dass diese Inhalte von einem Annotatoren gefüllt werden. Es gibt auch einen Text, der auf die Bedeutung dieser Methode für die Sammlung von Daten hinweist.</sample>
    <sample id="455">Das Bild zeigt eine Präsentation über die Datensammlungsmethode, speziell die Verwendung eines Cartoon-Beitrags als Task. Es gibt vier Personen auf dem Bild, und jeder hat einen Textfeld mit einer Frage oder Aussage. Der Textfeld sind mit einem gelben Pfeil markiert, der darauf hinweist, dass sie von einem Annotatoren gefüllt werden. Diese visuelle Darstellung verdeutlicht die Interaktion zwischen den Teilnehmern und der Methode zur Sammlung von Daten.</sample>
    <sample id="456">Das Bild zeigt eine Präsentation von Google Research über ein Dataset-Sammlungsverfahren. Der Schwerpunkt der Präsentation liegt auf der Verwendung eines Cartoon-Beilagenverfahrens zur Promovierung von Unformalität. Das Poster enthält eine Illustration von Menschen, die in einem Gespräch sind, und es gibt einen Text, der den Prozess beschreibt: "Set the dialog context (chosen from a few manual prompts per domain)" (Stellen Sie den Dialogkontext fest (aus einigen manuellen Anweisungen pro Domain)) und "The alternative question" (Die alternative Frage). Es gibt auch einen Hinweis, dass der Text von dem Annotator ausgefüllt wird, was bedeutet, dass der Annotator den Text für den Cartoon befüllt hat.</sample>
    <sample id="457">Das Bild zeigt eine Präsentation von Google Research über die Datensammlungsmethode. Es gibt ein Diagramm mit drei Personen, die sich in einem Gespräch befinden. Jeder Person ist mit einem Textfeld verbunden, der Informationen über das Gespräch bereitstellt. Der Text auf dem Bild lautet: "Datensammlungsmethode - Methodologie betont die Unformlichkeit durch einen Cartoon-Beitragsaufgaben." Diese Methode scheint dazu auszukommen, Dialogkontext zu erstellen und alternative Fragen zu stellen, indem man einen Cartoon-Beitragsaufgaben verwendet. Die Annotatoren füllen das Textfeld aus, um den Kontext des Gesprächs zu ergänzen.</sample>
    <sample id="458">Das Bild zeigt eine Präsentation, die sich auf "Generieren alternativer Fragen =&gt; Entity Paarungen" konzentriert. Im Zentrum der Präsentation befindet sich eine Frage: "Do you mean A or B?" Diese Frage wird verwendet, um Entity Paarungen zu generieren, indem sie auf enthaltene Informationen auf Wikipedia aufbaut. Die Präsentation bietet verschiedene Methoden, um Entity Paarungen zu erstellen, wie z.B. die Verwendung von similar infooboxen, descriptions oder titligen auf Wikipedia. Darüber hinaus werden einige Beispiele für Entity Paarungen gegeben, wie "The Return (memoir)" und "The Return (Shatner-novelle)". Im unteren rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich das Vortragender ist. Der Text am unteren Rand des Bildes lautet "Revising Inductive Expressions for Entity Selection in Web Corpora", was darauf hindeutet, dass die Präsentation sich auf die Verbesserung von Induktiven Ausdrücken für die Auswahl von Entitäten in Web-Korporen konzentriert.</sample>
    <sample id="459">Das Bild zeigt eine Präsentation, die sich auf die Generierung von alternativen Fragen für die Identifizierung von Entity Pairs in einem Dataset konzentriert. Der Titel der Präsentation lautet "Generate alternative questions =&gt; sampling entity pairs". Im Zentrum der Präsentation befindet sich eine Frage: "Do you mean A or B?" Diese Frage wird verwendet, um Entity Pairs zu identifizieren.

Die Präsentation bietet verschiedene Methoden, um Entity Pairs zu identifizieren:
1. Artikel mit ähnlichen Infoboxen auf Wikipedia (selbe Genre und Künstler)
2. Artikel mit ähnlichen Beschreibungen auf Wikipedia
3. Artikel mit ähnlichen Titeln
4. Zufällige Artikel

Darüber hinaus gibt es einen Hinweis auf eine Referenz zur Originalquellung des Materials, nämlich "Revisiting Inductive Expressions for Entity Selection Adaptability Corpus".

Das Bild wurde von der Google Research-Presentationsreihe erstellt, wie durch den Logo im oberen rechten Eck angezeigt wird.</sample>
    <sample id="460">Das englische Text ist auf dem Bild zu lesen: "Generate alternative questions =&gt; sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean This is it' or 'Man in the Mirror'? Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean Could You Be Mine or The Way I Am?"</sample>
    <sample id="461">Der englische Text auf der Seite lautet: "Generate alternative questions =&gt;&gt; sampling entity pairs" und "Do you mean A or B?" Diese Wörter sind auf einem Blatt Papier zu sehen, das sich im Zentrum des Bildes befindet. Darüber hinaus gibt es einige weitere Informationen auf dem Blatt, die möglicherweise Details oder Beispiele zur genannten Thematik bereitstellen.</sample>
    <sample id="462">Das Bild zeigt eine Präsentation mit dem Titel "Generate alternative questions =&gt; snapping entity pairs". Es gibt eine Frage: "Do you mean A or B?" Darunter sind verschiedene Optionen aufgelistet, wie z.B. "Items with similar infoboxes on Wikipedia (same genre and/or artist)", "Uniform at random", und "Items with similar titles". Links am Bild ist ein gelber Pfeil mit der Aufschrift "More Similar (likely harder)" zu sehen. Im unteren rechten Eck des Bildes befindet sich ein kleiner Profilbild eines Mannes.</sample>
    <sample id="463">Das Bild zeigt eine Präsentation von Google Research. Der Titel der Präsentation lautet "Background knowledge (Music)". Es gibt eine Liste von Liedern mit jeweils einem Google-Search-Link, nämlich "Easy on Me" von Adele und "I Gotta Feeling" von The Black Eyed Peas. Darunter steht: "Wir bitten Annotatoren, um mindestens ein Stück eines Liedes zu hören oder es zu lesen." Im unteren rechten Eck des Bildes ist ein Mann zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="464">Das Bild zeigt eine Präsentation mit dem Titel "Background knowledge (Music)" auf der oberen linken Seite. Es gibt zwei Lieder, "Easy on Me" von Adele und "I Gotta Feeling" von The Black Eyed Peas, jedes mit einem blauen Link, der "Click here to find out about the song." liest. Darunter steht: "We ask annotators to Listen to at least some of each song Read about each song". Im unteren rechten Eck ist ein kleiner Bildschirm eines Mannes zu sehen.</sample>
    <sample id="465">Das Bild zeigt eine Präsentation über Musikwissen. Im oberen rechten Eck befindet sich das Google Research Logo. Im unteren rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich die Präsentation gibt. Der Titel der Präsentation lautet "Background knowledge (Music)". Es gibt eine Anleitung zur Verwendung von Google-Suche-Links für jede Liedaufnahme, wobei zwei Lieder, "Easy on Me" von Adele und "I Gotta Feeling" von The Black Eyed Peas, genannt werden. Darunter sind zwei Links angegeben, um mehr über jedes Lied zu erfahren. Es wird auch gefordert, Annotatoren dazu zu befragen, mindestens einige Lieder zu hören und andere Lieder zu lesen. Am unteren Rand des Bildes ist ein Text zu lesen, der "Rebuilding Indirect Referring Expressions for Entity Selection in Sentences Corpus" lautet.</sample>
    <sample id="466">Das Bild zeigt eine Slide von einer Präsentation mit dem Titel "Background knowledge (Recipes)". Es gibt zwei Rezepte: "Simnel Cake" und "Pandan Cake". Das Simnel-Cake wird beschrieben als ein Fruchtbackware, das in den Vereinigten Staaten, Irland und anderen Ländern mit Migrationsverbindungen verbunden ist. Es besteht aus Schichten Almend- oder Marzipancreme und einem Kästchen aus dem gleichen Backmischung. Das Pandan-Cake wird als leichter, fluffiger Backware beschrieben, die mit den Saft der Pandanus amabilisblätter verziert ist. Es populär in Indonesien, Malaysia und den Niederlanden, insbesondere innerhalb der Indo-Kommunität. Beide Rezepte sind mit Fotos dargestellt, und es gibt einen kleinen Profilbild eines Mannes im unteren rechten Eck.</sample>
    <sample id="467">Das Bild zeigt eine Präsentation mit dem Titel "Eliciting expressions" auf einem weißen Hintergrund. Im oberen rechten Eck ist das Google Research Logo sichtbar. Der Text auf der Präsentation lautet: "Wir sagen den Annotatoren, welche Wahl gemacht werden soll und bitten sie, es zu beschreiben." Darunter sind zwei Kürzelangestelltungen für Musikstücke gezeigt: "Easy on Me (by Adele)" und "I Gotta Feeling (by the Black Eyed Peas)". Eine weitere Anweisung lautet: "Wir möchten Sie 3 bis 5 Ausdrücke für das gewählte Lied in Ihre Sprachblase geben. Zum Beispiel:" Danach folgen mehrere Beispiele von Ausdrücken, wie z.B. "Das mit der Klaviermusik", "Das, was nicht energiegeladen ist", "Es hat etwas mit einem Fluss", "Es gibt keinen Endpunkt" und "Es gibt nicht viel Zeit zum Entscheiden". Im unteren rechten Eck des Bildes ist ein kleiner Bildschirm eines Menschen zu sehen.</sample>
    <sample id="468">Das Bild zeigt eine Präsentation, die sich auf den Einsatz von Indizierungs-Referenzausdrücken für die Entity Selection in der Abstraktionskonvoluttion konzentriert. Im oberen rechten Eck befindet sich das Google Research Logo. Das Hauptthema der Präsentation ist "Eliciting expressions" (Indizierungs-Referenzausdrücke auslösen), und es gibt einen Hinweis darauf, dass die Annotatoren eine Wahl treffen sollten und ihre Entscheidung beschreiben sollen. Es gibt zwei Optionen für die Wahl: "Easy on me" von Adele oder "I Gotta Feeling" von the Black Eyed Peas. Darunter steht ein Text, der lautet: "Wir möchten Sie 3-5 Ausdrücke für das gewählte Lied geben, um in Ihre Sprachblase einzufügen. Zum Beispiel:" Danach folgt eine Liste von Beispielausdrücken für verschiedene Kategorien wie Musikauswahl, Buchauswahl und Rezeptauswahl. Die Präsentation wird von einem Mann mit kurzen Haaren und einem Bart abgeschlossen, der im unteren rechten Eck des Bildes zu sehen ist.</sample>
    <sample id="469">Der englische Text lautet: "AltEntities Corpus - ~6,000 alternative questions across the three domains - ~42,000 indirect referring expressions Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82%-87% when the LM has access to partially overlapping background knowledge. ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research-datasets/AltEntities"</sample>
    <sample id="470">Der englische Text lautet: "AltEntities Corpus - ~6,000 alternative questions across the three domains - ~42,000 indirect referring expressions Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82%-87% when the LM has access to partially overlapping background knowledge. ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research/datasets/AltEntities"</sample>
    <sample id="471">Der englische Text lautet: "AltEntities Corpus ~6,000 alternative questions across the three domains ~42,000 indirect referring expressions Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82%-87% when the LM has access to partially overlapping background knowledge. ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research/datasets/AltEntities"</sample>
    <sample id="472">Auf dem Bild ist ein Text zu sehen, der 'Thank You!' liest. Darunter steht: 'Wenn Sie Fragen haben, bitte schreiben Sie an javadh@google.com'. Im unteren rechten Eck des Bildes ist ein kleiner Bild von einem Mann zu sehen, und in der unteren linken Ecke befindet sich das Google-Logo.</sample>
    <sample id="473">Der Ansatz wird mit den bestehenden SimulST-Richtlinien verglichen.</sample>
    <sample id="474">Die Autoren gehören an Avignon Universität und Nantes Universität.</sample>
    <sample id="475">Sebastian Senti</sample>
    <sample id="476">There are three authors involved in the work: Myra Cheng, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="477">"Attention as a Guide for Simultaneous Speech Translation"</sample>
    <sample id="478">Das Bild zeigt eine Präsentation über 'Simultaneous Speech Translation' (Synchronsprachübersetzung). Der Titel der Präsentation lautet 'Was ist Simultaneous Speech Translation?' und es gibt eine Definition: 'Simultaneous speech translation (SimulST) ist das Prozess, den gesprochenen Sprachtext in eine andere Sprache in Echtzeit zu übersetzen, um interlingüe Kommunikation zu ermöglichen.' Im Bild sind auch einige Audio-Symbolerwachsene zu sehen, was darauf hinweist, dass das Gespräch in Echtzeit übersetzt wird. Am unteren Rand des Bildes befindet sich ein Bildquellenangabe: 'Image Credits to https://ai.googleblog.com/2021/01/stabilizing-live-speech-translation-in.html'.</sample>
    <sample id="479">Was sind die Probleme der aktuellen SimulST-Modelle?</sample>
    <sample id="480">Was sind die Probleme der aktuellen SimulST-Modelle?</sample>
    <sample id="481">Was sind die Probleme der aktuellen SimuIST-Modelle?</sample>
    <sample id="482">Was ist unsere Lösung?</sample>
    <sample id="483">Der englische Inhalt lautet: "Use already existing offline ST models without re-training or adopting specific architecture for SimulST" und "Use only one model for every latency regime and handle latency through specific parameters".</sample>
    <sample id="484">Was ist unsere Lösung? 1. Nutzen Sie bereits vorhandene offline ST-Modelle ohne erneutes Training oder die Adoption einer bestimmten Architektur für SimuST. 2. Gebrauchen Sie nur ein Modell für jede Latenzregime und verwalten Sie die Latenz über spezifische Parameter. 3. Genießen Sie das Wissen, das bereits im Modell durch die Aufmerksamkeit zwischen Audio- und Texteinsatz erworben wurde.</sample>
    <sample id="485">Der englische Text lautet: 'Our solution: EDAtt' und 'Encoder-Decoder Attention'.</sample>
    <sample id="486">Unsere Lösung: EDAtt</sample>
    <sample id="487">Das Bild zeigt eine Präsentation über ein Thema, das "Encoder-Decoder Attention" beinhaltet. Es gibt eine Überschrift, die "Our solution: EDAtt" liest und eine Definition für Encoder-Decoder Attention gibt. Diese Definition lautet: "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold a) towards the last λ speech frames, meaning that the received information is enough stable." Darüber hinaus befindet sich ein kleiner Bildschirm eines Menschen in der oberen rechten Ecke des Bildes. Im unteren linken Eck ist ein Audio-Symbol zu sehen, was darauf hinweist, dass es möglicherweise eine Audio-Referenz oder -kommentar gibt. Im unteren rechten Eck befindet sich der Text "page 014", was nahelegt, dass dies die 14. Seite der Präsentation ist.</sample>
    <sample id="488">Unsere Lösung: EDAtt</sample>
    <sample id="489">Das Bild zeigt eine Präsentation über die "Encoder-Decoder Attention" in der Kontext von Sprachübersetzung. Im oberen linken Eck befindet sich der Text "Our solution: EDAtt". Der Haupttext lautet "Encoder-Decoder Attention", was auf einen spezifischen Typ von Sprachmodell hinweist. Darunter ist ein Text zu sehen, der auf Deutsch "ich werde reden" bedeutet und eine Audiovisuelle Darstellung eines Sprechers zeigt. Links am Bildrand steht "01 I am going to talk about..." was auf den Anfang einer Präsentation oder eines Vortrags hinweist. Im unteren rechten Eck des Bildes ist "page 017" zu lesen, was auf die Seite der Präsentation hinweist.</sample>
    <sample id="490">Das Bild zeigt eine Präsentation über ein Thema, das "Encoder-Decoder Attention" beinhaltet. Im oberen linken Eck ist der Text "Our solution: EDAtt" zu sehen. Im unteren linken Eck befindet sich ein Diagramm mit einer Audio-Schallwelle und den Worten "Ich werde reden." Neben dem Diagramm steht der Text "EMITTED". Rechts daneben ist ein Textblock mit der Aufschrift "Decide whether to emit or not a partial translation based on where attention points to:". Der Text weiter unten lautet: "a word is emitted if the attention is not concentrated (its sum is below a threshold c) towards the last lambda speech frames, meaning that the received information is enough stable." Im unteren rechten Eck ist der Text "page 019" zu sehen.</sample>
    <sample id="491">Das Bild zeigt eine Präsentation über die "Encoder-Decoder Attention" in der Kontext des Sprachmodellierens. Im oberen linken Eck befindet sich der Text "Our solution: EDAtt". Rechts davon sind einige Fragen aufgeführt, wie "¿", "!", "?" und "¿". Der Haupttext lautet "Encoder-Decoder Attention" und erläutert, dass man entscheiden muss, ob man einen Satz oder nur eine teilweise Übersetzung machen soll, basierend auf, wo die Aufmerksamkeit aufgerichtet wird. Es wird auch erwähnt, dass ein Wort emitiert wird, wenn die Aufmerksamkeit nicht unter einem Threshold c konzentriert ist, was bedeutet, dass die empfangene Information stabil genug ist. Im unteren linken Eck befindet sich ein Diagramm mit den Wörtern "Ich werde reden." und "EMITTED". Im unteren rechten Eck ist der Text "page 021" zu sehen.</sample>
    <sample id="492">Unsere Lösung: EDAtt</sample>
    <sample id="493">Unsere Lösung: EDAtt Entscheidet, ob man einen Vorgang vollenden oder nur eine Teilübersetzung führen soll, basierend auf der Aufmerksamkeit, die ein Wort aufweist. Wenn die Aufmerksamkeit nicht konzentriert ist (d.h. ist ihr Summe unter einem bestimmten Schwellwert), wird der empfangene Inhalt als stabil betrachtet.</sample>
    <sample id="494">Unsere Lösung: EDAtt Entscheiden, ob man etwas ausschließen oder nicht, basierend auf der Aufmerksamkeit. Wenn die Aufmerksamkeit nicht konzentriert ist (unter einem bestimmten Schwellwert), wird der Text nach den letzten Phonfolgen geschrieben, was bedeutet, dass die bereitgestellte Information stabil genug ist.</sample>
    <sample id="495">Die Hauptergebnisse: EDAtt</sample>
    <sample id="496">Der englische Text lautet: 'Main Results: EDAtt' und 'quality measure' und 'latency measure'.</sample>
    <sample id="497">Das Bild zeigt eine Präsentation über die Ergebnisse einer Übersetzungsforschung. Im oberen linken Eck steht "Main Results: EDAtt". Der Hauptinhalt ist ein Graph, der BLEU-Score gegen AL/AL_CA (s) abhängt. Der Graph zeigt keine Datenpunkte und hat einen Y-Achse mit den Werten 21 bis 27 und einen X-Achse von 0.5 bis 6.5. Links am Graph steht "a) en-&gt;de" und rechts unten "latency measure". Im unteren rechten Eck befindet sich die Seite "page 030". Im oberen rechten Eck ist ein kleiner Bildschirm eines Menschen zu sehen.</sample>
    <sample id="498">Die Hauptergebnisse: EDAtt</sample>
    <sample id="499">Die Hauptergebnisse: EDAtt</sample>
    <sample id="500">Der englische Text lautet: 'popular strategies also applied to offline models' und 'state of the art architecture specifically tailored for SimulST'.</sample>
    <sample id="501">Hauptschlüsse: EDAtt</sample>
    <sample id="502">Der englische Text lautet: 'EDAtt outperforms all the strategies applied to offline models.'</sample>
    <sample id="503">Hauptschlüsse: EDAtt</sample>
    <sample id="504">Ja, das Bild zeigt eine Slide, die Informationen über ein Forschungsarbeitspapier präsentiert. Es gibt Kontaktdaten wie eine E-Mail-Adresse und einen GitHub-Link, sowie soziale Medien-Links auf Twitter. Darüber hinaus ist ein QR-Code auf der Slide zu sehen, der wahrscheinlich zum Papier führt.</sample>
    <sample id="505">Ja, der Dataset-Link gibt an, dass der Datensatz auf GitHub verfügbar ist: https://github.com/ShuhengL/ac12023_conllpp</sample>
    <sample id="506">MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning Zhiyang Xu* Ying Shen* Lifu Huang Department of Computer Science, Virginia Tech *Equal Contribution</sample>
    <sample id="507">Pre-trained Language Models for Downstream Tasks</sample>
    <sample id="508">Das Bild zeigt eine Präsentation über "Prätrainierte Sprachmodelle für Nebentasks". Es gibt drei Hauptabschnitte, die verschiedene Methoden zur Ausbildung von Sprachmodellen für Nebentasks beschreiben: 1. Pretrainieren und Finessen (BERT, T5) 2. Prompting (GPT-3) 3. Tuning (FLAN) Jedes Abschnitt hat eine detaillierte Abbildung, die die Prozesse und Vorteile der jeweiligen Methode darstellt. Am Ende des Bildes ist ein Text zu sehen, der auf die Unterschiede zwischen Pretrainieren, Finessen und Prompting hindeutet.</sample>
    <sample id="509">Das Bild zeigt eine Person, die in einem schwarzen Jackett und einem weißen Hemd steht. Sie trägt Brille und hat dunkle Haare. Im Hintergrund ist der Text "Language-only" auf einem schwarzen Hintergrund zu sehen.</sample>
    <sample id="510">Die Übersetzung lautet: "Anweisungsabgestimmung auf Multimodale vorher trainierte Modelle".</sample>
    <sample id="511">Imbalanc in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="512">Der Inhalt lautet: "Imbalance in Instructional Datasets between NLP and Multimodal" und "1600+ Language-only instruction tasks". Darunter steht "NO large-scale, publicly-available multimodal instruction tasks". Am unteren Rand des Bildes ist ein Zitat von Wang, Yizhong, et al. über die Benchmarking-Generalisierung mittels in-kontext-Instructions auf 1600+ Sprachverarbeitungsaufgaben.</sample>
    <sample id="513">MULTINSTRUCT

The first multimodal instruction tuning benchmark dataset.

62 diverse multimodal tasks
10 broad groups
5 expert-written instructions

Figure 2: Task Groups Included in Multinstruct. The yellow boxes represent tasks used for evaluation, the white ones included for training.</sample>
    <sample id="514">MULTINSTRUCT: Das erste multimodale Anweisungsverbrauch-Referenz-Datensatz</sample>
    <sample id="515">OFA (One For All) ist ein unified multi-modal pre-trained-Modell, das in der Lage ist, sowohl die Verständnis als auch die Generierung von Aufgaben mit einer oder mehreren modalitäten auszuführen. Das Modell verfügt über ein unified Vocabulary für Sprache, Bildtoken und Koordinaten eines umfassenden Rahmens.</sample>
    <sample id="516">Das Bild zeigt eine Übersetzung des englischen Textes "Example Instances from MULTINSTRUCT for Four Tasks" ins Deutsche. Der deutsche Text lautet: "Beispiele für Instanzen aus MULTINSTRUCT für vier Aufgaben". Diese Übersetzung gibt einen klaren Anblick über den Inhalt der Abbildung, indem sie die Beispiele für verschiedene Aufgaben in der MULTINSTRUCT-Anwendung auf Deutsch beschreibt.</sample>
    <sample id="517">Das Bild zeigt eine Übersetzung des englischen Textes "Figure 1: Example Instances from MULTINSTRUCT for Four Tasks." In deutscher Sprache bedeutet dies "Beispielinstanzen aus MULTINSTRUCT für vier Aufgaben." Diese Übersetzung gibt einen klaren Überblick über den Inhalt und die Beispiele, die im Bild dargestellt sind.</sample>
    <sample id="518">Das Bild zeigt eine Präsentation über den MultiInstruct-Set, der verschiedene Aufgabenbeispiele für Sprachverarbeitung präsentiert. Im oberen linken Eck befindet sich das Logo "MULTIINSTRUCT". Das Hauptteil des Bildes ist ein Diagramm mit vier Abschnitten, jeder beschreibt eine verschiedene Aufgabe:</sample>
    <sample id="519">Multimodale Anweisungsabfinung</sample>
    <sample id="520">Das Bild zeigt eine Präsentation über "Multi-Modal Instruction Turning". Es gibt zwei Hauptabschnitte: Training Dataset Construction und Testing Dataset Construction. Im Training Dataset Construction ist erklärt, dass 53 Aufgaben von 9 Gruppen für die Ausbildung verwendet werden, wobei je Gruppe 10.000 Instanzen pro Aufgabe bereitgestellt wird. Im Testing Dataset Construction wird beschrieben, dass der Entire Commonsense Reasoning-Gruppensatz für die Prüfung verwendet wird, und 5 zusätzliche Aufgaben aus VQA und Miscellaneous-Gruppen ausgewählt wurden. Jede Gruppe verwendet alle Instanzen im Testsplit für jede Aufgabe. Zudem wird erwähnt, dass 20 Instanzen aus dem testsplit des Natural Instructions-Datensatzes zufällig für NLP als unbekannte Aufgaben ausgewählt wurden. Das Bild enthält auch ein kleines Bild eines Menschen im unteren rechten Eck.</sample>
    <sample id="521">Multi-Modal Instruction Turning Training Dataset Construction: Use 53 tasks from 9 groups for training. Sample 10,000 instances per task. Testing Dataset Construction: Reserve the entire Commonsense Reasoning group for testing. Select additional 5 tasks from VQA and Miscellaneous groups. We use all the instances in the test split for each task. Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="522">Implementations Details Training details: Pre-trained OFA-Large model (472M) Mix all the instances for all tasks. Each instance is randomly combined with one of its instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="523">Implementation Details Training details: Pre-trained OFA-Large model (472M) Mix all the instances for all tasks. Each instance is randomly combined with one of its five instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="524">Implementationsdetails: Trainingdetails: Pre-trained OFA-Large model (472M) Mix all the instances for all tasks. Each instance is randomly combined with one of its instruction templates. Testingdetails: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="525">Die Übersetzung des englischen Textes ins Deutsche lautet: "Evaluation Metrics" "Für multimodale Klassifizierungsaufgaben (Visueller Entzug, visueller Raum, visuelle Sprache, visuelle Verarbeitung, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache, visuelle Sprache,</sample>
    <sample id="526">Das Bild zeigt eine Präsentation über "Sensitivität" (Sensitivity). Der Titel "Sensitivität" ist auf der oberen linken Seite des Bildes zu sehen. Im Text unter dem Titel steht: "Wie empfindlich der Modell ist gegenüber einer Vielzahl von Anweisungen für die gleiche Aufgabe." Die Wörter "variety of instructions" und "same task" sind in Gelb hervorgehoben. Darunter befindet sich eine Definition, die lautet: "Fähigkeit, die gleichen Ergebnisse für die gleiche Aufgabe zu erzielen, unabhängig von kleinen Abweichungen im Wortlaut der Anweisungen." Diese Definition wird auch in deutscher Sprache aufgeführt: "Fähigkeit, die gleichen Ergebnisse für die gleiche Aufgabe zu erzielen, unabhängig von kleinen Abweichungen im Wortlaut der Anweisungen." Am Ende der Definition ist ein Mathematisches Formula, das wie folgt aussieht: Σ_{t ∈ T} [σ_{i ∈ I_t}[E_{(x_i, y_i) ∈ D^i}[L(J_θ(f_θ(i, x), y))] / μ_{i ∈ I_t}[E_{(x_i, y_i) ∈ D^i}[L(J_θ(f_θ(i, x), y))]]]. Im unteren rechten Eck des Bildes ist ein kleiner Bildausschnitt eines Menschen zu sehen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="527">Effektivität der Anweisungsablage auf MULTIINSTRUCT</sample>
    <sample id="528">Effektivität der Anweisungsabranceigung auf MULTINSTRUCT</sample>
    <sample id="529">Der Titel der Grafik lautet "Figure 3: Model Performance as the Number of Task Clusters Increases".</sample>
    <sample id="530">Effekt von verschiedenen Anweisungen bei der Anpassung von Anweisungen OFA wurde für 5 Anweisungen optimiert und erzielte auf allen Bewertungsaufgaben viel höheren aggregierten Leistungsvermögen.</sample>
    <sample id="531">Effekt von Fine-tuning-Strategien auf die Sensitivität bei Modell-Sensibilität</sample>
    <sample id="532">Die Übersetzung des englischen Textes ins Deutsche lautet: "Zero-Shot Performance on NLP Tasks. Instruction Tuning on Multinstruct can improve zero-shot performance on unseen NLP tasks. The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset."</sample>
    <sample id="533">Der Text auf dem Bild lautet: "Conclusion First large-scale multi-modal instruction tuning dataset. Contains 62 multi-modal tasks from 10 broad categories. Significantly improve the zero-shot capability of OFA via instruction tuning. Explore several transferring learning techniques and show their benefits. Design a new metric sensitivity."</sample>
    <sample id="534">Das Bild zeigt eine Slide mit einem schwarzen Hintergrund und dem Titel "One More Thing!" in weißem Text. Darunter steht ein Text, der lautet: "Wir sammeln einen viel größeren multimodalen Dataset für die Anpassung der Anweisungen und um es zu optimieren, das etwa 150 weitere visuelle Sprachaufgaben enthält und wir freigenommen werden werden bald!" Darüber hinaus befindet sich auf der Slide ein Qr-Code, der wahrscheinlich dazu dient, den genannten Dataset oder Informationen über ihn zu bereitstellen. Im unteren rechten Eck des Bildes ist ein kleiner Bildausschnitt eines Menschen zu sehen, der möglicherweise das Präsentationstheater betreibt.</sample>
    <sample id="535">Die Autoren gehören der Universität di Trento.</sample>
    <sample id="536">Mohammad Javad Hosseini</sample>
    <sample id="562">Language model acceptability judgements are not always robust to context.</sample>
    <sample id="563">Sprachmodellungsrichtungen sind nicht immer robust gegenüber Kontext</sample>
    <sample id="564">Minimal Paar Paradigm (MPP) Evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLiMP SyntaxGym Crows 1. Many people were helping themselves. No customer has spent any money. Stereotypical nonsensical sentence. P(1) &gt; P(2)</sample>
    <sample id="565">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLiMP 1. Many people were helping themselves. 2. Many people were helping others. SyntaxGym 1. No customer ... has spent any money. 2. ... has spent any money. CrowS 1. Stereotypical Syntactic pattern. 2. Stereotypical Noun-verb sentence. P(1) ≫ P(2) P(1.any) ≫ P(2.any) P(1) ≫ P(2)</sample>
    <sample id="566">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLiMP SyntaxGym CrowS 1. Many people were helping 1. No customer ... has spent 1. Stereotypical 2. Many people any money. 3. Stereotypical selves. 4. ... has spent 5. selves. any money. 6. Stereotypical 7. Many people 8. No customer ... has spent 9. selves. any money. 10. Stereotypical 11. Many people 12. No customer ... have spent 13. selves. any money. 14. Stereotypical P(1) &gt; P(2) P(1.any) &gt; P(2.any) P(1) &gt; P(2)</sample>
    <sample id="567">Minimal paradiems (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.</sample>
    <sample id="568">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to estimate the abstract knowledge of LMs: BLIMP SyntaxGym CrowS 1. Many people were helping themselves. No customer ... has spent any money. Women are terrible at handiwork. 2. Many people were helping herself. P(1) ≈ P(2) P(1.any) ≈ P(2.any) P(1) ≈ P(2) Are these judgements stable with long preceding context?</sample>
    <sample id="569">Revisiting Minimal Pair Paradigm</sample>
    <sample id="570">Das Thema des Sprechers ist "Revisiting Minimal Pair Paradigm". Diese Präsentation diskutiert die Bewertung von Sprachmodellen (LMs) mithilfe des Minimal Paar-Paradigms (MPP). Das MPP verwendet relative Positionen in Folgeverwiderungen, um die abstrakte Kenntnis über LMs zu bewerten. Die Präsentation zeigt drei Beispiele für Minimal Paare: "Many people were helping themselves" und "No customer has spent any money" im BLiMP-Set, sowie "Women are terrible at handiwork" und "Men are terrible at handiwork" im CrowS-Set.</sample>
    <sample id="571">Der Inhalt auf dem Bild lautet: "Approach Test whether MPP judgements as a function of context length, structural match, and acceptability."</sample>
    <sample id="572">Der Inhalt der Präsentation lautet: "Approach Test whether MPP judgements as a function of context length, structural match, and acceptability."</sample>
    <sample id="573">Das Bild zeigt eine Präsentation mit dem Titel "Approach" auf der oberen linken Ecke. Der Haupttext lautet: "Test whether MPP judgements as a function of context length, structural match, and acceptability." Im unteren linken Bereich befindet sich ein Diagramm mit dem Titel "Space of Candidate Prefixes," das verschiedene Typen von Präfixen und ihre Verbindungen darstellt. Rechts neben dem Diagramm sind zwei Textblöcke zu sehen: einer mit dem Text "Acceptable, Matched" und einem anderen mit den Fragen: "Who might Jessica have been before returning to this customer? What did Aaron sound like when he said those spotlights?" Am unteren Rand des Bildes ist die Texte "GPT2, OPT family - 125M to 6.7B" zu lesen. Im oberen rechten Eck des Bildes befindet sich ein kleiner Bildschirm eines Menschen.</sample>
    <sample id="574">Der Inhalt auf dem Bild lautet: "Approach Test whether MPP judgements as a function of context length, structural match, and acceptability."</sample>
    <sample id="575">Auf dem Bild ist eine Person zu sehen, die sich auf einem Laptop konzentriert. Der Schwerpunkt des Bildes liegt auf dem Text "Approach" in großen schwarzen Buchstaben am oberen linken Eck. Darunter steht ein komplexer Text, der wie eine mathematische Formel aussieht: P_{LM}(|PreFix|) &gt; P_{LM}(|PreFix|). Diese Formel scheint mit einem Diagramm verbunden zu sein, das verschiedene Werte und Zahlen zeigt.

Rechts im Bild sind zwei Textblöcke zu sehen, einer mit dem Wort "Unacceptable" und der andere mit "Matched". Diese Blöcke enthalten weitere Informationen, die möglicherweise mit den Werten und Zahlen im Diagramm verbunden sind.

Am unteren Rand des Bildes befindet sich der Text "GPT2, OPT family - 125M to 675M", was möglicherweise auf die spezifischen Arten von Modellen oder Daten hinweist, die im Zusammenhang mit diesem Thema verwendet werden.

Im Ganzen handelt es sich um eine visuelle Darstellung eines wissenschaftlichen oder technischen Ansatzes, der wahrscheinlich mit Sprachmodellierung oder Linguistik verbunden ist.</sample>
    <sample id="576">Der Text auf der rechten Seite des Bildes lautet: "BLUMP, 'Essential' Queries • BLUMP | Adjacent Island".</sample>
    <sample id="577">Der Text auf der rechten Seite lautet: "BLUMP, Essential '74" und "BLUMP | Adjunct Island".</sample>
    <sample id="578">Der Inhalt lautet: "Approach Test whether MPP judgements as a function of context length, structural match, and acceptability."</sample>
    <sample id="579">Das Bild zeigt eine Präsentation über ein Forschungsprojekt. Der Titel "Approach" ist am oberen linken Eck sichtbar. Der Haupttext lautet: "Test whether MPP judgements as a function of context length, structural match, and acceptability." Dieser Text beschreibt die Ziele des Projekts.

Im unteren linken Eck befindet sich ein Diagramm mit dem Titel "Space of Candidate Prefixes". Es gibt zwei Kategorien innerhalb dieses Diagramms: "Mechanized" und "Mathematical". Diese Kategorien sind weiter unterteilt in verschiedene Arten von Prefixen, wie "BWT Construction", "BWT Approximation", "BWT Construction + Approximation" und "BWT Construction + Approximation".

Rechts im Bild sind zwei Textblöcke zu sehen. Der obere Textblock hat den Titel "Wikipedia, Unrelated" und enthält eine Frage: "Who might lose fees from this customer?" Der untere Textblock enthält mehrere Fragen, die alle auf die Verlust von Gebühren für Kunden beziehen:

1. "Who might lose fees from this customer?"
2. "There are over three hundred species of the genus Rosa."
3. "Who might lose fees from this customer?"

Am unteren Rand des Bildes steht der Text "GPT2, OPT family - 125M to 6.7B", was möglicherweise auf die verwendeten Modelle oder Daten hinweist.

Im oberen rechten Eck des Bildes ist ein kleiner Bildausschnitt eines Menschen zu sehen, der wahrscheinlich der Präsentator oder ein Teilnehmer an der Präsentation ist.</sample>
    <sample id="580">Das Bild zeigt eine Präsentation mit dem Titel "Approach" in Englisch. Der Haupttext lautet: "Test whether MPP judgements as a function of context length, structural match, and acceptability." Das Bild enthält auch ein Diagramm und einige Textblöcke, die sich auf den Inhalt der Präsentation beziehen. Im unteren linken Eck des Bildes ist ein Text zu sehen, der "GPT2, OPT family - 125M - 6.7B" liest.</sample>
    <sample id="581">MPP Urteile sind für beliebige Kontextlängen robust.</sample>
    <sample id="582">MPP Urteile sind robust gegenüber beliebigen Kontextlängen.</sample>
    <sample id="583">Der Inhalt lautet: "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance. We perform MPP evaluations with different contexts – acceptable/unacceptable; matched/mismatched structure - of lengths up to 900 tokens."</sample>
    <sample id="584">Der Inhalt lautet: "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance. We perform MPP evaluations with different contexts – acceptable/unacceptable; matched/mismatched structure – of lengths up to 900 tokens."</sample>
    <sample id="585">Der Inhalt lautet: "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance. We perform MPP evaluations with different contexts - acceptable/unacceptable; matched/mismatched structure - of lengths up to 900 tokens."</sample>
    <sample id="586">Inhaltsübersetzung: "Akzeptable/unakzeptable MPP-Sätze mit passendem Struktur haben den meisten auffällig am schlimmsten auf die Modellleistung auswirkt"</sample>
    <sample id="587">Auf dem Bild ist ein Text auf Deutsch zu sehen, der lautet: "BLIMP, OPT 6.7B". Diese Information könnte sich auf eine spezifische Version oder eine Version einer Software oder eines Systems beziehen. Der Kontext des Bildes suggeriert, dass es sich um einen technischen oder wissenschaftlichen Zusammenhang handelt, möglicherweise mit einem Fokus auf Sprachverarbeitung oder maschinelle Lernmethoden.</sample>
    <sample id="588">Der Inhalt auf dem Bild lautet: "Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance. We perform MPP evaluations with different contexts – acceptable/unacceptable; mismatched/matched structure – of lengths up to 900 tokens." Diese Aussage bedeutet, dass die Korrektheit oder Ungerechtigkeit der MPP-Sätze (MPP = Meaning Preservation Predicate) und ob sie in einer korrekten oder falschen Struktur sind, den Leistungsgrad des Modells stark beeinflussen können. Das Bild zeigt auch eine Grafik mit einer Legende und drei Beispiele-Questions, die wahrscheinlich dazu dienen, die Auswirkungen dieser Faktoren auf die Modellleistung zu demonstrieren.</sample>
    <sample id="589">Warum beeinflussen Paarungsprefixe die LM-Schätzung?</sample>
    <sample id="590">Das Bild zeigt eine Präsentation über "wie passende Präfixe LM-Jugendschätzung beeinflussen". Es gibt einen Text, der erklärt, dass Kontextsätze verändert werden, um die relevante Struktur zu erhalten und fragt, ob Modelle sensible sind gegenüber diesen Sätzen. Es sind verschiedene Typen von Präfixen aufgeführt, wie "Prefix/suffix-adverbien", "Long prefix-adverbien", "Add-clause", und "Quote". Darüber hinaus gibt es ein Diagramm, das die "Delta Accuracy" gegen den "Input Length" zeigt, mit verschiedenen Farben für verschiedene Typen von Perturbation und Präfix-Typen.</sample>
    <sample id="591">Der Inhalt auf dem Bild zeigt eine Präsentation über den Einfluss von passenden Präfixen auf die Bewertungen von LM (Language Models). Der Titel lautet "Warum passende Präfixe LM-Bewertungen beeinflussen?" Und es gibt eine Beschreibung, die sagt: "Wir perturben Kontextsätze auf eine Weise, die die strukturelle Integrität beibehält und fragen, ob Modelle für diese Sätze empfindlich sind." Das Bild zeigt auch ein Diagramm mit einer Legende, die verschiedene Typen von Perturbation und Prefix-Typen anzeigt.</sample>
    <sample id="592">Der Inhalt auf dem Bild lautet: "Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the structure, and ask whether models are sensitive to these sentences." Diese Frage wird durch eine Reihe von Beispielen illustriert, die sich auf das Ändern von Suffixen und Adverbien in Sätzen beziehen. Darüber hinaus gibt es eine Skala mit verschiedenen Typen von Perturbationen und einem Graph, der den Einfluss dieser Perturbationen auf die Leistung des Language Models zeigt.</sample>
    <sample id="593">Warum beeinflussen Übereinstimmungspräfixe die LM-Bewertungen?</sample>
    <sample id="594">Sprichmodelle sind auf verborgene syntaktische oder semantische Merkmale in Satzen aufmerksam. MPP-Evaluierungen mit kurzen, einzelnen Satz-akkusativ-Eingaben fangen nicht die abstrakte Wissen der LM's (Sprachmodellierungen) aus.</sample>
    <sample id="595">Die Hauptzusammenfassung der Schlussfolgerungen lautet: Sprachmodelle sind auf verborgene syntaktische/semantische Merkmale in Sätzen aufmerksam. MPP-Evaluierungen mit kurzen, einzelnen Satz-akkusativ-Eingaben fangen nicht die abstrakte Kenntnis von LM's (Language Models) ein.</sample>
    <sample id="596">Sprache: Deutsch</sample>
    <sample id="597">In the first step of the method, input tokens are assigned to tags.</sample>
    <sample id="598">55,000</sample>
    <sample id="626">Die beste Ausrichtungsmethode für DEplain ist CATS-C3G.</sample>
    <sample id="627">Schwach überwachtes Lernen kann Modelle trainieren, die sich gut anpassen und in der Lage sind, effektiv zu lernen, selbst wenn sie auf ungenaue oder fehlende Labels trainiert werden.</sample>
    <sample id="628">Die Dokumente wurden mit manuellen und automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="629">Der CoNLL++-Datensatz wurde aus Reuters-Newsartikeln von 2020 erstellt und mit den Annotationsguidelines von CoNLL-2003 angegeben.</sample>
    <sample id="630">XSemPLR: Mehrsprachiges Semantischer Parsing in mehreren Sprachen und Bedeutungsrepräsentationen Yusan Zhang, Jun Wang, Zhiguo Wang, Rui Zhang PennState amazon</sample>
    <sample id="631">Das Bild zeigt eine Präsentation über Semantische Grammatik. Es gibt zwei Hauptfragen, die in der Präsentation diskutiert werden: 1. Welche Länder haben mindestens drei Autobauer? 2. Welche Spieler haben weniger als drei Assists in einer Saison gemacht? Diese Fragen sind auf dem Bild zu sehen und zeigen, wie Semantische Grammatik verwendet wird, um Querys zu strukturieren und zu verarbeiten.</sample>
    <sample id="632">Übersetzung des englischen Inhalts ins Deutsche:</sample>
    <sample id="633">Cross-lingual Semantic Parsing</sample>
    <sample id="634">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language</sample>
    <sample id="635">Das Bild zeigt eine Präsentation über "Cross-lingual Semantic Parsing" (Überlingual Semantischer Parse). Der Titel ist oben links positioniert. Im Text unter dem Titel steht: "Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language". Links sind vier Kisten in verschiedenen Farben dargestellt, die "English", "German", "Chinese" und "Neural Models" beschriftet sind. Rechts sind vier weitere Kisten in Blau mit den Bezeichnungen "SQL", "Lambda" und "FunQL". Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen. Die Slidenummer "4" erscheint im unteren rechten Eck.</sample>
    <sample id="636">Cross-lingual Semantic Parsing

现有的CLSP模型分别在有限的任务和应用数据集上提出并评估。例如：
- 在某些意义表示的覆盖方面存在不足。

语言包括：
- 英语
- 德语
- 中文

神经模型用于：
- SQL
- Lambda
- FunQL</sample>
    <sample id="637">Cross-lingual Semantic Parsing

- Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance:
  - Lack of coverage on certain meaning representation

[Diagram showing the relationship between languages (English, German, Chinese) and neural models (SQL, Lambda, FunQL)]</sample>
    <sample id="638">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain neural model SQL German Chinese English Single Model Lambda FunQL</sample>
    <sample id="639">Das Dataset XSemPLR bietet eine umfangreiche Sammlung von Texten für die Semantische Parsingaufgaben in verschiedenen Sprachen. Es enthält insgesamt 9 Datensätze, die in verschiedenen Bereichen verteilt sind, 5 Semantischer Parsingaufgaben, 8 Bedeutungsrepräsentationen und 22 Sprachen aus 15 Sprachfamilien. Das Zentrum der Präsentation ist ein Diagramm, das den Prozess der Semantischen Parsingdarstellung zeigt, mit einem Encoder und einem Decoder, die auf eine Vielzahl von Aufgaben hinweisen, wie zum Beispiel die Analyse von SQL-Anweisungen oder die Identifizierung von Benutzerhandlungen in einer Online-Umgebung.</sample>
    <sample id="640">Das Dataset XSemPLR bietet eine umfangreiche Sammlung von Semantischer Parseingsatzungen in mehreren Sprachen und Sprachfamilien. Es enthält insgesamt 9 Datensätze aus verschiedenen Bereichen, 5 Semantischer Parseingsatzungen, 8 Bedeutungsdarstellungen und 22 Sprachen aus 15 Sprachfamilien.</sample>
    <sample id="641">Das Bild zeigt eine Präsentation über "Experiment Settings". Es gibt zwei Hauptabschnitte: "Training" und "Inference". Im Training-Section wird erläutert, dass man die Einstellungen für das Trainieren und Evaluieren in Sechs Schritte teilt. Ein besonderer Schritt heißt "Translate-Test", bei dem man einen Google Translate API verwenden soll, um den Text in die Zielsprache zu übersetzen. Danach trainiert und evaluierend man mit einem monolingualen Modell. Im Inference-Section sind zwei Diagramme zu sehen, die den Prozess für die Übersetzung von Text aus deutscher Sprache in Englisch und dann in SQL-Datenbanken darstellen. Das Bild enthält auch einen kleinen Bildschirm im oberen rechten Eck, auf dem ein Mann zu sehen ist, der wahrscheinlich über diese Präsentation spricht.</sample>
    <sample id="642">Experiment Settings wirkt auf Deutsch wie "Versuchsbedingungen". Diese Bedingungen bestimmen, wie ein Experiment durchgeführt und ausgewertet werden soll. Sie sind für die Planung und Durchführung von Studien oder Versuchen von entscheidender Bedeutung, da sie die Kontrolle über die Variablen und die Umsetzung der Versuchsaufgaben gewährleisten.</sample>
    <sample id="643">Das Experiment-Setting beschreibt, wie ein Modell für die Übersetzung von SQL-Anweisungen in Englisch trainiert und evaluiert wird. Es gibt zwei Hauptverfahren: Training und Inferenz. Im Training-Prozess wird Englisch als Quell-Sprache verwendet, um das Modell zu trainieren, und das Ziel ist es, das Modell mit einer monolingualen Methode zu trainieren und zu evaluieren. In der Inferenzphase wird der Prozess auf Deutsch übertragen, wo das Modell mit einem Übersetzungs-API-Tool die Anweisungen übersetzt und dann wieder in Englisch für die Endübersetzung in SQL-Format verwendet wird.</sample>
    <sample id="644">Experiment Settings: Wir betrachten die sechs Einstellungen für die Ausbildung und Bewertung. Monolingual-Modell: Die QuellSprache ist gleich der Zielsprache, wie zum Beispiel Deutsch zu Deutsch. Wir testen auch den Monolingual-Few-shot-Szenario durch die Ausbildung von monolingualen Modellen mit nur 10% Trainingsdaten. Training: German (Few-shot) auf German Model, der dann auf SQL weitergeleitet wird. Inference: German auf German Model, der dann auf SQL weitergeleitet wird.</sample>
    <sample id="645">Das Experiment-Setting beschreibt die sechs Settings für die Ausbildung und Bewertung. Der Monolingual-Setting bedeutet, dass die Quellsprache gleich der Zielsprache ist, wie zum Beispiel Deutsch zu Deutsch (German-to-German). Sie testen auch den Monolingual-Few-shot-Setting, indem sie monolingual-Modellen mit nur 10% Trainingsdaten ausbilden.</sample>
    <sample id="646">Das Experiment-Setting beschreibt, wie die Sechssituationen für die Ausbildung und Bewertung vorgehalten werden.</sample>
    <sample id="647">Experiment Settings wirkt wie der Titel dieses Slides. Es handelt sich um eine Präsentation, die wahrscheinlich über den Setup von Experiments in einem Forschungsprojekt geht.</sample>
    <sample id="648">Experiment Settings wirkt wie der Titel des Slides. Es handelt sich um eine Übersetzung des englischen Inhalts ins Deutsche, was darauf hindeutet, dass die Präsentation möglicherweise für einen multilingualen Publikum oder einem multilingualen Umfeld konzipiert wurde.</sample>
    <sample id="649">Das Experimenteinstellungen</sample>
    <sample id="650">Das Bild zeigt eine Präsentation über "Experiment Settings". Es gibt sechs verschiedene Einstellungen für die Ausbildung und Bewertung. Der Schwerpunkt liegt auf der "Cross-lingual Zero-shot/Few-shot Transfer", was bedeutet, dass man auf einer Sprache ausgebildet wird und dann auf eine andere Sprache übertragen wird. Das Bild zeigt zwei Diagramme: eines für das Training und eines für die Inferenz. Beim Training handelt es sich um die Ausbildung auf englischer oder deutscher Sprache mit einem "German Few-shot" (weniger Beispiele in deutscher Sprache). Bei der Inferenz handelt es sich um die Verwendung von "German" und "Multilingual Model" für die Übertragung auf die Sprache SQL.</sample>
    <sample id="651">Das Bild zeigt eine Präsentation über "Experiment Settings" (Experimenteinstellungen). Es gibt eine Überschrift und zwei Hauptabschnitte: "Training" (Trainingssetting) und "Inference" (Erkennungssetting).

Im Training-Abschnitt sind drei Optionen aufgeführt:
1. English
2. English
3. German Few-shot

Die Inference-Abschnitt zeigt zwei Optionen:
1. German
2. Multilingual Model

Zusätzlich gibt es einen Hinweis auf "Cross-lingual Zero-shot/Few-shot transfer", was bedeutet, dass man auf einem Sprachsetting trainiert und danach auf ein anderes Sprachsetting überträgt.

Am unteren Rand des Bildes ist die Folgenummer 11 zu sehen. Im oberen rechten Eck befindet sich ein kleiner Bildschirm mit einer Person, die möglicherweise das Vortrag verlässt oder das Publikum beobachtet.</sample>
    <sample id="652">Die englische Übersetzung lautet: "Analysis of Monolingual" und "We evaluate on two groups of models on Monolingual Setting."</sample>
    <sample id="653">Analysis von monolingualen Modellen auf der Monolingual-Setting-Grundlage. Wir bewerten zwei Gruppen von Modellen auf der Monolingual-Setting-Grundlage: - EPTR: Multilingual Pretrained Encoders mit Pointer-based Decoders - XLM-R + PTR, mBERT + PTR - EPDEC: Multilingual Pretrained Encoder-Decoder Modelen - mBART, mT5 Wir fanden, dass EPDEC (mT5) die beste Leistung auf allen Datensätzen hat</sample>
    <sample id="654">Analysis von Monolingualen</sample>
    <sample id="655">Analysis von Monolingualen</sample>
    <sample id="656">Analysis of Multilingual Training

Wir evaluieren mT5 und XLM-R + PTR auf dem Multilingülen Setting.

Enc-Dep/Enc-PTR (mT5-XLM-R) kann durch die Ausbildung in einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="657">Analysis of Multilingual Training</sample>
    <sample id="658">Analysis of Multilingual Training</sample>
    <sample id="659">Analysis des multilingualen Trainings: Wir untersuchen mT5 und XLM-R + PTR im Multilingüen-Umfeld. Die meisten der großen NLP-Modelle erzielen eine Leistungssteigerung, außer Englisch, das in 7 Datensätzen abfallt und in 3 Datensätzen Steigerung zeigt. Dies wird als "Recht der Multilinguistik" bezeichnet.</sample>
    <sample id="660">Cross-lingual Performance Gap Blue Line: Cross-lingual Few-shot transfer Orange Line: Cross-lingual Zero-shot transfer Green Line: Monolingual Setting</sample>
    <sample id="661">Cross-lingual Performance Gap Blue Line: Cross-lingual Few-shot transfer Orange Line: Cross-lingual Zero-shot transfer Green Line: Monolingual Setting</sample>
    <sample id="662">Der englische Inhalt lautet: "Cross-lingual Performance Gap" und "green - orange: For zero-shot setting, the cross-lingual transfer performance gap is significant blue - orange: For few-shot setting, the transfer gap is shortened rapidly".</sample>
    <sample id="663">Other Results &amp; Findings (Section 4 in Paper) Enc-Dec (mT5) outperforms previous work or achieves comparable results. Pretraining on the English NL can significantly boost the performance of few-shot on target NLs. Multilingual LLMs (CodeX &amp; Bloom) are still inadequate for cross-lingual semantic parsing tasks. Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest. FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.</sample>
    <sample id="664">Enc-Enc (mT5) übertritt den vorherigen Arbeit oder erreicht ähnliche Ergebnisse.</sample>
    <sample id="665">Wir bauen XSemPLR, einen einheitlichen Referenzpunkt für multilingual semantische Parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen.</sample>
    <sample id="666">Das Bild zeigt eine Präsentation, die sich auf den Abschluss eines Forschungsprojekts oder -arbeitspapiers konzentriert. Der Titel der Slide lautet 'Conclusion' (Abschluss), was darauf hindeutet, dass es sich um einen Zusammenfassungsbereich handelt. Die Slide enthält drei Hauptpunkte: 1. Eine Beschreibung des XSemPLR, einem Benchmarks für multilingual semantische Parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. 2. Eine umfangreiche Referenzstudie über drei repräsentativen Sprachmodellen. 3. Ergebnisse, die zeigen, dass mT5 mit monolingual Training die beste Leistung erzielt hat, insbesondere bei der Verarbeitung von mehrsprachigen LLMs, aber auch die Bedeutung der Transferlernahrung zwischen monolingual und translingual Training hervorhebt. Im oberen rechten Eck der Slide ist ein kleiner Bildschirmbild eines Menschen zu sehen, der möglicherweise der Sprecher oder einer der Autoren der Präsentation ist. Im unteren rechten Eck der Slide ist der Text 'Katharina Schumacher' zu lesen, was wahrscheinlich den Namen der Person im Bildschirmbild oder der Autorenschaft angibt.</sample>
    <sample id="667">Die folgenden Arbeiten wurden bereits durchgeführt: - Parameter watermarking [1, 2] - Transferabilität - Lexikalischer watermarking [3, 4] - Backdoor-watermarking [5] - Adversarische watermarking [6] Alle diese Arbeiten sind an EaaS (Empathische Erkenntnis und Anwendungssysteme) anwendbar.</sample>
    <sample id="668">Nein, multilingual LLMs wie Codex oder Bloom sind für CLSP nicht ausreichend.</sample>
    <sample id="695">The slide discusses the challenge of dealing with multiple permutations. It suggests that to address this issue, one can induce it in training and introduces a permutation model where inference is NP-hard (TSP). The solution involves backpropagating through continuous relaxation.</sample>
    <sample id="696">The Fairness of a pre-trained NLP model is defined as the difference between its performance on different groups.</sample>
    <sample id="697">Der Referent heißt Yanis Labrak.</sample>
    <sample id="698">Jon Gauthier</sample>
    <sample id="699">Myra Cheng, Esin Durmus, Dan Jurafsky</sample>
    <sample id="700">Tropikalismus bezieht sich auf eine literarische Bewegung der 1920er Jahre, die durch die Verwendung von tropischen Themen und Stilen gekennzeichnet war. Diese Bewegung diente dazu, eine andere Perspektive auf das britische Kolonialreich zu präsentieren und sich gegen den natürlichen Realismus der Zeit abzuheben. Im Zusammenhang mit dieser Präsentation könnte Tropikalismus darauf hindeuten, dass die darin beschriebenen Themen oder Stile tropische Elemente aufnehmen, möglicherweise um eine ähnliche literarische Bewegung zu illustrieren oder zu analysieren.</sample>
    <sample id="701">Die Autoren haben die Beschreibungen von Menschen für Zielgruppen erstellt, indem sie spezifische Attribute wie Kultur, Tradition, Stolz und Exotismus benannten. Diese Attribute wurden verwendet, um die Gruppen allein aufgrund ihrer Identität zu definieren.</sample>
    <sample id="702">Pointwise (P-)CXMI</sample>
    <sample id="703">DrBERT und ChuBERT sind beide BERT-ähnliche Modelle, aber sie unterscheiden sich in der Art der Datenquelle, die für ihre Ausbildung verwendet wurde. DrBERT wurde mit privaten Medizin-Daten ausgewiesen, während ChuBERT mit öffentlichen Medizin-Daten trainiert wurde.</sample>
    <sample id="751">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="752">Iterative Transfer Learning is a method in active learning where the model retains and updates its knowledge iteratively. It involves training on new data, fine-tuning with old data, and repeating this process to improve performance over time while retaining previously learned information.</sample>
    <sample id="753">Das Ziel des Datensatzes ist, die Sprache von Benutzern zu verstehen, wenn sie eine Wahl treffen.</sample>
    <sample id="754">Ein Angreifer kann Modellparameter über einen EaaS (Elastic As-a-Service) extrahieren, indem er Zugriff auf den Service hat und die erforderlichen Ressourcen nutzt.</sample>
    <sample id="755">Drei Autoren sind an der Arbeit beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="756">Zwei Annotatoren wurden verwendet, um den ursprünglichen Datensatz zu erstellen.</sample>
    <sample id="757">Die Autoren gehören an folgende Universitäten: Sebastian Sanyt* und Maarten Sap sind an der University of Washington, Jenny T. Liang und Katharina Reinecke sind an Carnegie Mellon University, und Ronan Le Bras gehört zur Allen Institute for AI.</sample>
    <sample id="758">"left conjuncts tend to be shorter (observed before)"</sample>
    <sample id="759">The technology for dialog systems has advanced significantly, with current models capable of understanding and responding to human-like conversations. These systems often use natural language processing (NLP) and machine learning algorithms to interpret user inputs accurately. They can handle various tasks such as answering questions, providing information, scheduling appointments, or even engaging in casual conversation based on the context provided by users.

In addition to basic conversational abilities, some advanced dialog systems are now equipped with emotional intelligence features that allow them to detect and respond appropriately to emotions expressed through speech or text. This enhances their ability to empathize and interact more naturally with humans. Overall, while there is still room for improvement, especially regarding maintaining coherence across longer conversations and ensuring consistent responses under different conditions, the field continues to evolve rapidly due to ongoing research and development efforts.</sample>
    <sample id="760">Wir müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, um sicherzustellen, dass die Modelle in verschiedenen Kontexten und Situationen korrekt funktionieren. Dies hilft bei der Erstellung von trustwerten und realistischen Sprachmodellen.</sample>
    <sample id="761">Ja, das mehrsprachige Training hat zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt.</sample>
    <sample id="762">Ja, die Annotatoren wissen die Entität im Voraus.</sample>
    <sample id="763">Die Metriken, die für die Bewertung verwendet wurden, sind BLEU, METEOR und ROUGE.</sample>
    <sample id="764">Ja, die Regression wirkt sich auf bestimmte NER-Typen aus.</sample>
    <sample id="765">Positionalität für NLP ist wichtig, weil es die Bedeutung von Wortordnung und -positionen im Text betont.</sample>
    <sample id="766">Mehrsprachige LLMs wie BLOOM wurden durch Adapter angepasst.</sample>
    <sample id="767">Das Modell, das für das Transferlernen verwendet wird, ist "RoBERTA-base + classifier head".</sample>
    <sample id="768">The current test sets used for evaluating the PaLM capabilities are:</sample>
    <sample id="769">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="770">Der Gewinn der vorgeschlagenen Methode gegenüber der stärksten Baseline beträgt 24,73%.</sample>
    <sample id="771">Shuheng Liu und Alan Ritter</sample>
    <sample id="772">Ja, die Ergebnisse und der Datensatz der Studie können als Benchmark verwendet werden.</sample>
    <sample id="773">Es werden fünf kleineren Modellen experimentiert.</sample>
    <sample id="774">OFA (One For All)</sample>
    <sample id="833">Die Autoren gehören an der Universität von Stanford.</sample>
    <sample id="834">Die Autoren gehören an der Stony Brook University.</sample>
    <sample id="835">Deutsch-Englisch und Englisch-Chinesisch</sample>
    <sample id="836">The speaker's name is Chan Young Park.</sample>
    <sample id="837">Die Modelle, die während der Experimente untersucht wurden, sind DEPLAIN-APA und DEPLAIN-WEB.</sample>
    <sample id="838">57</sample>
    <sample id="839">Es sind drei Autoren an der Arbeit beteiligt: Regina Stodden, Omar Momen und Laura Kallmeyer.</sample>
    <sample id="840">AG News, MIND, SST2, Enron Spam</sample>
    <sample id="876">NACHOS ist ein Typ von Snack, der aus Torten und verschiedenen Zutaten wie Käse, Sojasauce und anderen Gewürzen besteht. Es ist eine beliebte Wahl für ein bequemes Essen oder als Vorprogramm zu einem Abendessen.</sample>
    <sample id="877">David Vilàr Torres, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, George Foster</sample>
    <sample id="878">Prompt-Strategie hat einen bedeutenden Einfluss auf die Ergebnisse. Sie kann den Übersetzungsnachhaltungsqualität beeinflussen, da sie die Qualität der Übersetzungen beeinflusst.</sample>
    <sample id="879">Die Autoren gehören der Carnegie Mellon University Language Technologies Institute.</sample>
    <sample id="880">Die 5 Anweisungen der Expert*innen lauten: 1. Verwenden Sie die QR-Code-App auf Ihrem Smartphone, um den QR-Code zu-scannen. 2. Folgen Sie dem Verbindungsgeschehen auf Ihrem Gerät. 3. Wählen Sie die gewünschte Anweisung aus. 4. Führen Sie die Anweisung aus, wie von Ihnen angegeben. 5. Teilen Sie Ihre Ergebnisse mit uns, wenn Sie es möchten.</sample>
    <sample id="881">Die Autoren empfehlen, Modelle zur Nutzung von Informationen aus mehreren Quellen zu testen, indem man ein Referenzlager für die Integration von Wissen und eine Korrekturaufgabe ausführt, um die Fähigkeit zu prüfen, auf vor- und nachtimliche Wissensquellen zu verlassen.</sample>
    <sample id="882">Der englische Text lautet: "Prompting PaLM for Translation" und "Assessing Strategies and Performance".</sample>
    <sample id="883">Das Bild zeigt eine Präsentation über den PaLM: Pathways Language Model. Links sind folgende Informationen aufgeführt: - Chowdery et al., 2022 arXiv:2204.02311. - 540B parameters. - Trained on 780B tokens. - Densely activated. - 6144 TPU v4 chips. - SOTA in hundreds of LMU and Generation benchmarks. Rechts befindet sich ein Baum, der verschiedene Sprachverstehende Fähigkeiten wie "QUESTION ANSWERING", "ARITHMETIC", "TRANSLATION" und "LANGUAGE UNDERSTANDING" darstellt. Der Baum ist mit einer Schrift versehen, die den Gesamtumfang der Parameter angibt, der von 62 bis 95 Billionen beträgt. Im unteren rechten Eck des Bildes ist ein kleiner Bildausschnitt eines Menschen zu sehen.</sample>
    <sample id="884">PaLM: Wegweiser Sprachmodell Chowdery et al., 2022 arXiv:2204.02311 540B Parameter.</sample>
    <sample id="885">Unser Beitrag: - Systematische erste Studie über LLM-Folgendem für MT. - Bezug auf die Kandidatenpool und die Auswahlstrategie. - Bewertung der Übersetzungsfähigkeiten gemäß den besten Praktiken der MT-Gemeinschaft. - Neueste Test-Sets (vermeiden Sie das Übertrainen und das Überpassen auf Evaluation-Daten). - Vergleich mit den neuesten WMT-Untermittlungen (SOTA-Systeme verwenden die meisten recenten Training-Daten). - SOTA-METIKEN (bessere Korrelation mit menschlicher Bewertung). - Expertenbasierte menschliche Bewertung (mehr robust als die von Crowdworkers). - Empfehlungen für die Auswahlstrategien für Folgendem.</sample>
    <sample id="886">Das Bild zeigt eine Präsentation über die Beiträge eines Forschungsprojekts. Der Titel der Slide lautet "Unser Beitrag" (Our contribution). Es gibt drei Hauptpunkte: 1. Systematische erste Studie über LLMs für MT (Machine Translation) - Diese Studie befasst sich sowohl mit dem Kandidatenpool als auch mit der Auswahlstrategie. 2. Bewertung der Übersetzungsvermögen gemäß den besten Praktiken der MT-Community - Hier werden die neuesten Testsets (mit Ausnahme von Train- und Overfitting auf Evaluation-Daten), die Vergleich zu den neuesten WMT-Untermissungen (SOTA-Systeme mit den meisten recenten Training-Daten) und die SOTA-MT-Metriken (bessere Korrelation mit menschlichen Urteilen) betrachtet, sowie professionelle menschliche Bewertungen (mehr robust als Crowdworkers). 3. Empfehlungen für die Auswahl von Prompt-Strategien - Diese Punkt bezieht sich auf Empfehlungen für die Auswahl von Prompt-Strategien. Im unteren rechten Eck des Bildes ist ein kleiner Bildausschnitt eines Menschen zu sehen, möglicherweise der Präsentator. Im unteren linken Eck befindet sich das Google Logo.</sample>
    <sample id="887">Das Bild zeigt eine Präsentation über die Beiträge eines Forschungsprojekts. Der Titel der Präsentation lautet "Our contribution". Die Hauptpunkte der Präsentation sind: 1. Systematische Studie über LLM-Prompting für MT (Mitarbeiterkommunikation) - Diese Studie befasst sich sowohl mit der Auswahl des Kandidatenpool als auch mit der Auswahlstrategie. 2. Bewertung der Übersetzungsvermögen in Bezug auf die besten Praktiken der MT-Community - Diese Punkt betrifft die neuesten Testsets, die Vergleich zu den letzten WMT-Untermittlern, die SOTA-Metriken und das Expertenbasierte menschliche Bewertung. 3. Empfehlungen für die Auswahl von Prompt-Strategien - Diese Punkt befasst sich mit Empfehlungen für die Auswahl von Prompt-Strategien. Im unteren rechten Eck des Bildes ist ein Foto eines Menschen zu sehen, der wahrscheinlich der Sprecher oder einer der Autoren der Präsentation ist.</sample>
    <sample id="888">Der englische Inhalt lautet: "First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies."</sample>
    <sample id="889">Prompts haben einen großen Einfluss auf die Übersetzungsfunktion.</sample>
    <sample id="890">Prompts haben einen großen Einfluss auf die Übersetzungsnachhaltigkeit.</sample>
    <sample id="891">Prompts haben einen großen Einfluss auf die Übersetzungsnutzung. Zwei zufällige Prompts pro Satz wählen, BLEURT für jeden Satz-Prompt berechnen und die meisten Satze (516 von 1000) zeigen eine Differenz von mehr als 1 BLEURT-Punkt. Die Differenz kann bis zu 40 BLEURT-Punkten beträgen</sample>
    <sample id="892">Das Bild zeigt eine Präsentation über Übersetzungshilfe. Im Zentrum befindet sich der Titel "Example prompting for translation". Links sind zwei englische Sätze zu sehen, die aufgerichtet sind: "Ski-Legenden unter sich: Die Polizei war eingeschlossen, nachdem sie Beschwerden des Buros erhalten hatten." und "English: Police were called in after receiving complaints from the office." Rechts steht ein Text "5-shot prompting" und darunter eine deutsche Übersetzung: "Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird." Darunter steht die englische Übersetzung: "He is being transported under the custody of two policemen on a bus from the jail." Im unteren rechten Eck ist ein kleiner Bildausschnitt eines Mannes zu sehen, der wahrscheinlich der Vortragender ist. Im unteren linken Eck befindet sich das Google-Doodle.</sample>
    <sample id="893">Das Bild zeigt eine Präsentation über Übersetzungshilfe durch Prompting. Der Titel "Example prompting for translation" (Beispiel für Prompting zur Übersetzung) ist am oberen Rand zu sehen. Im Text sind zwei Beispiele für Promptings aufgeführt, einer in deutscher Sprache und einer in englischer Sprache. Die deutsche Version lautet: "Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird." Die englische Übersetzung lautet: "He is being transported under the custody of two policemen on a bus from the jail." Darunter befindet sich ein weiterer deutscher Satz: "Ski-Legenden unter sich: Die Polizei war eingeschlossen, nachdem sie Beschwerden des Buros erhalten hatten." Diese wird dann mit seiner englischen Übersetzung "English: Police were called in after receiving complaints from the office." abgeschlossen. Am unteren rechten Eck des Bildes ist ein kleiner Bildschirm eines Mannes zu sehen, der wahrscheinlich die Präsentation macht.</sample>
    <sample id="894">Der Text auf der Seite lautet: "Example prompting for translation" und "5-shot prompting". Darunter sind zwei Sätze auf Deutsch und Englisch zu sehen. Der erste Satz lautet: "Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird." Die englische Übersetzung lautet: "He is being transported under the custody of two policemen on a bus from the jail." Der zweite Satz lautet: "Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten." Die englische Übersetzung lautet: "Police were called in after receiving complaints from the office." Im unteren linken Eck ist ein Google-Doodle zu sehen.</sample>
    <sample id="895">Der Text auf der Seite lautet "Example prompting for translation" und "5-shot prompting". Darunter sind zwei englische Sätze zu sehen, die auf Deutsch übersetzt werden sollen: "Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird." und "Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten." Diese Übersetzungen deuten auf eine Übung für die Übersetzung von englischen Sätzen ins Deutsche hin.</sample>
    <sample id="896">Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird.</sample>
    <sample id="897">Beispielergebnisse</sample>
    <sample id="898">Beispiel: "Example quality is more important than similarity to source sentence."</sample>
    <sample id="899">Beispielergebnisse sind wichtiger als Ähnlichkeit zu der Quellsprache. Spezialisierte SOTA-Systeme haben einen bedeutenden Vorteil. PaLM ist nahe bei Google Translate. Einzelnachrichten von MQM: Fluide PaLM-Komparabelkeit mit SOTA. Genauheitswerte sind im Allgemeinen niedriger. Dominiert durch "Genauigkeit/Ausschöpfung". "Stil/Absurdität" ist im Allgemeinen niedriger für PaLM.</sample>
    <sample id="900">Beispiel: "Experimental Results" bedeutet "Experimenteller Ergebnisse". "Example quality is more important than similarity to source sentence." bedeutet "Die Qualität eines Beispiels ist wichtiger als Ähnlichkeit zu der Quellsprache." "Specialized SOTA systems have a substantial advantage." bedeutet "Spezialisierte SOTA-Systeme haben einen bedeutenden Vorteil." "PaLM close to Google Translate." bedeutet "PaLM nahe bei Google Translate." "Fluency of PaLM comparable to SOTA." bedeutet "Die Flüssigkeit von PaLM ist mit SOTA vergleichbar." "Accuracy scores generally lower." bedeutet "Die Genauheitswerte sind im Allgemeinen niedriger." "Style/Awkwad generally lower for PaLM." bedeutet "Stil/Unbehagen sind im Allgemeinen niedriger für PaLM."</sample>
    <sample id="901">Beispiel: "Example quality is more important than similarity to source sentence."</sample>
    <sample id="902">Beispielergebnisse sind wichtiger als Ähnlichkeit zu der Quellsprache. Spezialisierte SOTA-Systeme haben einen bedeutenden Vorteil. PaLM ist fast wie Google Translate. Einzelnachrichten von MQM: Fluensität von PaLM kann mit SOTA konfrontiert werden. Genauheitswerte sind allgemein niedriger. Dominierend sind "Genauigkeit/Omission". "Stil/Unbehagen" sind allgemein niedriger für PaLM.</sample>
    <sample id="903">Beispiel: "Example quality is more important than similarity to source sentence." auf Deutsch übersetzt bedeutet das: "Beispielqualität ist wichtiger als Ähnlichkeit zur Quellsprache."</sample>
    <sample id="904">Versuchungsergebnisse: Beispielsprache Qualität ist wichtiger als Ähnlichkeit zur Quellsprache. Spezialisierte SOTA-Systeme haben einen bedeutenden Vorteil. PaLM ist nahe bei Google Translate. Einzelnachrichten von MQM: Fluide Kommunikation von PaLM kann mit SOTA konfrontiert werden. Genauheitswerte sind allgemein niedriger. Dominierungen durch "Genauigkeit/Übersehen". "Stil/Absurdität" ist allgemein niedriger für PaLM.</sample>
    <sample id="905">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" * Style/Awesome* generally lower for PaLM.</sample>
    <sample id="906">Das Bild zeigt eine Mischung von Wörtern, die auf "thank you" in verschiedenen Sprachen und Schriftarten aussehen. Die Wörter sind über das Bild verteilt, wobei "thank you" im Zentrum prominent dargestellt ist. Diese Gestaltung schafft ein visuell ansprechendes und internationaler Ausdruck der Dankbarkeit.</sample>
    <sample id="907">"Schwaker als Du Denkst: Eine kritische Betrachtung zu schwachsupervierten Lernen."</sample>
    <sample id="908">Der englische Titel lautet "Weaker Than You Think: A Critical Look at Weakly Supervised Learning".</sample>
    <sample id="909">Weak supervision erleichtert die Annotierungsbehinderung.</sample>
    <sample id="910">Warum wird schwachsupervisierte Lernung verwendet? Schwache Überwachung erleichtert die Annotierungs-Schleife. Aber schwache Labels sind laut Laut bedeutet, dass sie Rauschen enthalten und dies schadet der allgemeinen Verallgemeinerrung des Modells. Schwachsupervisierte Lernung (WSL) trainiert Modelle, die gut auf Rauschen-Annotationen überleben, was bedeutet, dass sie trotz ungenauen Labels noch effektiv lernen können.</sample>
    <sample id="911">Weak supervision lightent die Annotierungsblockade. Ablauf: 1. Weak labeling (e.g., heuristiken, Wissenbanken) → Unbeaufsichtigte Daten 2. Noise memorization → Verunreinigung des Generalisierens 3. Weakly supervised learning (WSL): Trainiere Modelle, die gut auf verunreinigten Daten ausperformieren und ihre Labels korrigieren können.</sample>
    <sample id="912">Warum ist schwache Überwachung lernen? Schwache Überwachung lernen erleichtert die Annotierungsblockade. Aber schwache Labels sind laut Laut bedeutet, dass sie Rauschen enthalten und das Memorieren von Lärm schadet der Allgemeinfähigkeit. Schwache Überwachungslernen (WSL) trainiert Modelle, die gut auf rauschenen Daten überleben können, obwohl sie auf rauschenen Daten trainiert wurden.</sample>
    <sample id="913">Warum es schwache Überwachungsverfahren gibt? Schwache Überwachung erleichtert die Annotierungssperre. Aber schwache Labels sind laut Laut bedeutet, dass sie Rauschen enthalten, was das Memorieren von Lärm und die Verallgemeinerung behindert. Schwache Überwachungsverfahren (WSL) trainieren Modelle, die gut auf Rauschen-Annotationen überleben und trotzdem gut generalisieren können.</sample>
    <sample id="914">Eine häufige Behauptung in jüngster Forschungsergebnissen zu Weakly Supervised Learning (WSL) lautet: "Wir trainieren Modelle nur auf schwach überwachtem Daten und erreichen eine Genauigkeit von XX%." Diese Behauptung wird durch zwei Grafiken illustriert. Die erste Grafik zeigt "Schwach überwachte Trainingsdaten" (noisy), die zweite Grafik zeigt "Gut überwachte Testdaten" (clean).</sample>
    <sample id="915">Ein häufiger Anspruch in den letzten WSL-Forschungen lautet: "Wir trainieren Modelle nur auf schwach überwacht Daten und erreichen eine Genauigkeit von XX%." Diese Aussage zeigt, dass viele Forschungsarbeiten in diesem Bereich auf schwach überwachte Daten abzielen.</sample>
    <sample id="916">Eine häufige Behauptung in den letzten WSL-Forschungen lautet: "Wir trainieren Modelle nur auf schwach überwachte Daten und erreichen eine Genauigkeit von XX%." Diese Behauptung wird durch ein Gesichtsausdruck-Emoji mit einer Gesichterklatsche dargestellt, um die Bedenken zu betonen.</sample>
    <sample id="917">Ein häufiger Ansatz in den letzten WSL-Werken ist: "Wir trainieren Modelle ausschließlich mit schwach überwachte Daten und erreichen eine Genauigkeit von XX%." Diese Phrase wird hervorgehoben, was darauf hinweist, dass sie ein wichtiger Punkt im Diskussion ist. Darüber hinaus gibt es eine Emoticon mit einem Gesichtsausdruck, der möglicherweise eine Reflexion oder eine Erklärung zum Ansatz hinzufügt.</sample>
    <sample id="918">Unser Forschungsfragen</sample>
    <sample id="919">Unser Forschungsfragen&lt;|listen|&gt;</sample>
    <sample id="920">Die Hauptfindungen des Studienabschnitts sind in der Abbildung dargestellt. Die Abbildung zeigt eine Grafik mit dem Titel "Relative Performance Improvement over Weak Labels (%)". Die X-Achse repräsentiert verschiedene Verfahren oder Algorithmen, wie FTw, BOND, COSINE, MLC und L2R. Die Y-Achse zeigt den relativen Leistungsverbesserungswert über schwache Labels (%) an.

Drei verschiedene Linien werden in der Grafik dargestellt:
1. Eine gelbe Linie, die für die Validierung auf schwache Labels steht.
2. Eine blaue Linie, die für die Validierung ohne Validation auf schwache Labels steht.
3. Eine grüne Linie, die für die Validierung auf saubere Labels steht.

Jede Linie zeigt den relativen Leistungsverbesserungswert über schwache Labels für jedes Verfahren oder Algorithmus. Die gelbe Linie zeigt einen höheren Relativwert als die anderen beiden Linien, insbesondere bei den Verfahren COSINE und MLC. Die blaue Linie zeigt einen niedrigeren Relativwert, während die grüne Linie einen mittleren Wert aufweist.

Dieser Graphik zeigt somit, dass die Verwendung von sauberen Labels im Vergleich zu schwachen Labels und ohne Validation eine bessere Leistung verbreitet, insbesondere bei den Verfahren COSINE und MLC.</sample>
    <sample id="921">Die Hauptfunde im Bild sind die Veränderungen in der Leistungsfähigkeit aufgrund verschiedenen Validierungsverfahren.</sample>
    <sample id="922">Hauptschlüsse</sample>
    <sample id="923">Hauptschlüsse</sample>
    <sample id="924">Die Hauptfunde sind: Eine saubere Validationsgesamte ist unerlässlich.</sample>
    <sample id="925">RQ2 Main findings</sample>
    <sample id="926">RQ2 Main findings</sample>
    <sample id="927">Das Bild zeigt eine Präsentation mit zwei Hauptelementen: einem Graph und einer Statistik. Der Graph zeigt die Korrelation zwischen der Genauigkeit und der Validierung, wobei verschiedene Methoden wie FTw, COSINE, LZR, BOND, MLC und Weak labels dargestellt sind. Die Statistik zeigt die Leistungsdelta von verschiedenen Methoden wie FTc, LoRaC, BitFitC und AdapterC. Links oben steht "R02" und "Main findings". Im unteren linken Eck befindet sich ein Text, der lautet: "WSL Ansätze profitieren von mehr sauberen Validationsamples!"</sample>
    <sample id="928">Das Hauptergebnis der Studie zeigt, dass WSL-Methoden von sauberem Validations-Samples profitieren.</sample>
    <sample id="929">Das Hauptergebnis des englischen Inhalts lautet: "WSL-Approaches profitieren von mehr sauberen Validationsamphibs."</sample>
    <sample id="930">Die Hauptfunde</sample>
    <sample id="931">Hauptschlüsse</sample>
    <sample id="932">Hauptschlüsse</sample>
    <sample id="933">Hauptschlüsse</sample>
    <sample id="934">Zusammenfassung</sample>
    <sample id="935">Zusammenfassung Recent WSL-Methoden - Anforderungen:</sample>
    <sample id="936">Zusammenfassung: Die neuesten Ansätze in der WSL (Wasserstoff-Schadstoff-Verarbeitung) erfordern saubere Stichprobe. Diese Ansätze werden jedoch zu hoch geschätzt.</sample>
    <sample id="937">Zusammenfassung Recent WSL-Methoden</sample>
    <sample id="938">Zusammenfassung Recent WSL-Anroachen - Anforderungen an saubere Samples. - Übertriebenen Realitätsgefühl. Empfehlungen - Berücksichtigen Sie die Modellauswahlkriterien. - Benutzen Sie Few-shot-Learnings-Methoden als Baseline. - Stellen Sie immer eine kontinuierliche Feinabpolung (CFT) durch.</sample>
    <sample id="939">Likert-Skala, Benutzersatisfaktionsindex (CSI), Emotion Recognition, Human Evaluation, Speech Quality Metrics.</sample>
    <sample id="940">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="941">Basierend auf dem gegebenen Bild und Text, benötigt man das Hintergrundwissen, dass Servin als Richter und Kea als Backerin beschrieben wird. Diese Informationen sind für die korrekte Interpretation des Textes und die Antwort auf den Satz von entscheidender Bedeutung.</sample>
    <sample id="942">Ja, der Code ist verfügbar auf GitHub unter dem Link "mpoems/kitmus".</sample>
    <sample id="943">Es gibt keine Anzeichen dafür, dass die Annotatoren für NLPositionality in Bezug auf jede demographische Gruppe ausgewogen sind.</sample>
    <sample id="944">Sätze wurden durch Präfixe innerhalb der akzeptablen Domain verändert.</sample>
    <sample id="945">Eine dimensionale Bewertung bedeutet, dass eine Variable auf mehreren Echsen oder Gradungen bewertet wird.</sample>
    <sample id="946">The authors belong to the following universities: 1. University of Science and Technology of China 2. Microsoft Research Asia 3. Beijing Jiaotong University 4. Sony AI 5. Microsoft STC Asia</sample>
    <sample id="947">Die Form des Prompts ist wichtig, wenn es sich um die Übersetzung von englischen Texten handelt.</sample>
    <sample id="978">Die Autoren haben die Dialogmodelle 'BART-FID-RAG', 'Blender2', 'Emora' und 'Blender-Decode' evaluiert.</sample>
    <sample id="979">There are 10 authors involved in this work.</sample>
    <sample id="980">A good planner should have the ability to break down complex tasks into smaller, manageable steps. They need to be able to think ahead and anticipate potential challenges or changes in plans. Additionally, a good planner must prioritize effectively, ensuring that critical tasks are completed first while also being flexible enough to adapt their strategies as needed based on new information or unexpected obstacles.</sample>
    <sample id="981">Es sind 10 Autoren an der Arbeit beteiligt.</sample>
    <sample id="982">The presenter is Vasudha Varadarajan.</sample>
    <sample id="983">The authors belong to the Polish Academy of Sciences and the University of Warsaw.</sample>
    <sample id="1021">"Accuracy/Omission" und "Style/Awesome"</sample>
    <sample id="1022">Das Bild zeigt eine Präsentation mit dem Titel "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems". Die Autoren sind Sarah E. Finch, James D. Finch und Jinho D. Choi. Das Präsentationsbild wurde von Emory University und der Emory NLP Research Lab erstellt, wie die Logos am unteren Rand des Bildes zeigen. Darüber hinaus befindet sich ein Alexa-Logo im unteren rechten Eck, was auf eine Zusammenarbeit oder Verbindung zu Amazon Alexa hinweist.</sample>
    <sample id="1023">Das Bild zeigt eine Präsentation oder eine Slide, die wahrscheinlich Teil eines wissenschaftlichen Vortrags oder einer Forschungspräsentation ist. Der Titel der Präsentation lautet "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems". Diese Überschrift suggeriert, dass das Thema sich auf die Bewertung und Analyse der aktuellen Technologien in Chatbasierten Dialogsystemen konzentriert.

Die Slide wurde von Emory University erstellt, wie durch das Logo der Universität angezeigt wird. Darüber hinaus sind Logos von Amazon Alexa und dem Emory NLP Research Lab zu sehen, was darauf hindeutet, dass diese Institutionen möglicherweise mit dem Projekt oder der Forschungszusammenarbeit involviert sind.

Außerdem sind die Namen der Autoren der Präsentation angegeben: Sarah E. Finch, James D. Finch und Jinho D. Choi. Diese Informationen deuten an, dass sie die Hauptautoren oder -autorenesse des Vortrags oder der Forschungsarbeit sind.

Im oberen rechten Eck der Slide befindet sich ein kleiner Bildschirm, das einen Menschen zeigt, der möglicherweise den Vortrag gibt oder Teil der Forschungsgruppe ist. Dieser Einzelteil fügt einem persönlichen Touch zur Präsentation hinzu und könnte helfen, den Publikum eine Verbindung zu den Autoren herzustellen.

Zusammenfassend handelt es sich um eine professionelle und informativ präsentationelle Slide, die wissenschaftliche Arbeit im Bereich von Chatbasierten Dialogsystemen präsentiert, mit Verbindung zur Emory University und Zusammenarbeit mit Amazon Alexa und dem Emory NLP Research Lab.</sample>
    <sample id="1024">Das Bild zeigt eine Präsentation mit dem Titel "Comparative Evaluation" (Komparativer Bewertung). Es gibt zwei Gruppen von Sprachblasen, jede mit einem Figurenkopf und einem Roboter-Emoticon. Die Blasen sind in zwei Farben geteilt: blaue und lila. Im unteren linken Eck befindet sich das Logo der Emory University, und im unteren rechten Eck ist das Alexa-Logo zu sehen.</sample>
    <sample id="1025">Das Bild zeigt eine Präsentation mit dem Titel 'Comparative Evaluation'. Es gibt zwei Gruppen von Sprachblasen, einer in Blau und eine in Lila, jede mit einem Menschenkopf und einem Chatbot-Emoticon. Im Zentrum ist ein Gerichtsschreibtisch mit einem Richter zu sehen. Der Text 'EMORY UNIVERSITY' und 'alexa' sind unten sichtbar.</sample>
    <sample id="1026">Das Bild zeigt eine Präsentation, die sich auf 'Dialogqualität' konzentriert. Im oberen rechten Eck ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich über die Präsentation spricht. Der Titel der Präsentation lautet 'Dimensions of Dialogue Quality', und es gibt einen Schwerpunkt auf 'Dialogue Quality'. Darunter sind drei Aspekte aufgeführt: 'Relevance' (Relevanz), 'Consistency' (Kohärenz) und 'Emotional Understanding' (emotionale Verständnis). Diese Aspekte sind miteinander verbunden, was darauf hindeutet, dass sie alle wichtige Elemente von qualitativ hochwertigen Dialogen sind.</sample>
    <sample id="1027">Das Bild zeigt eine Präsentation mit dem Titel "Likert Rating Evaluation". Im Hintergrund sind einige Logos zu sehen, darunter das von Emory University und Alexa. Der Hauptfokus der Präsentation ist ein Likert-Skala, die von 1 bis 5 erstreckt ist, mit einem grünen Pfeil auf der Position 3. Neben der Skala sind einige Blaue Sprachblasen zu sehen, die wahrscheinlich den Kontext des Evaluations-Themas illustrieren.</sample>
    <sample id="1028">Das Bild zeigt eine Präsentation, die sich auf eine Likert-Skala für die Bewertung der Relevanz von Bot-Antworten konzentriert. Im oberen Teil des Bildes befindet sich ein blaues Banner mit dem Text "Likert Rating Evaluation" (Likert-Skala Bewertung). Links am Bildrand ist das Logo der Emory University zu sehen, und rechts ist das Alexa-Logo präsentiert. Im unteren linken Eck des Bildes ist ein Zeichnung eines Richters zu sehen, der ein Gabel in der Hand hält. Im Zentrum des Bildes sind Blaue Sprachblasen zu sehen, die auf eine Diskussion oder Kommunikation hinweisen. Darunter steht der Text "Rate the relevance of the bot's responses" (bewerten Sie die Relevanz der Bot-Antworten), gefolgt von einer Skala von 1 bis 5, auf der eine rote Pfeilkette markiert ist.</sample>
    <sample id="1029">Das Bild zeigt eine Präsentation mit dem Titel "Annotating Behaviors in Chat (ABC-Eval)" auf einem Computerbildschirm. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirmbild von einer Person zu sehen, die möglicherweise die Präsentation verlässt oder beendet hat. Der Hauptinhalt der Präsentation besteht aus mehreren Blaue Sprachblasen, die von schwarzen Kopf- und Schultern-Symbolen abgeleitet sind, was darauf hindeutet, dass sie Gesprächsblätter darstellen. Links am Bildrand befindet sich das Logo der Emory University, und im unteren rechten Eck ist das Alexa-Logo sichtbar.</sample>
    <sample id="1030">Der englische Text auf der Titelseite lautet: "Annotating Behaviors in Chat (ABC-Eval)"</sample>
    <sample id="1031">Der englische Inhalt lautet "ABC-Eval Behaviors" und zeigt vier Kategorien: Coherence, Consistency, Knowledge und Emotional Understanding.</sample>
    <sample id="1032">Das Bild zeigt eine Präsentation, die wahrscheinlich in einem Seminar oder einer Vorlesung gehalten wird. Der Titel der Präsentation lautet "ABC-Eval Behaviors", was auf eine Bewertungsstruktur oder eine Analyse von Verhaltensmustern hinweist. Die Präsentation wurde von Emory University erstellt, wie durch das Logo und das Wappen erkennbar ist.

Die Präsentation besteht aus vier Hauptbereichen: "Coherence" (Kohärenz), "Knowledge" (Wissen), "Consistency" (Konsistenz) und "Emotional Understanding" (emotionale Verständnis). Jeder Bereich ist innerhalb eines großen quadratischen Rahmens platziert, was darauf hindeutet, dass diese Bereiche wichtige Aspekte der Bewertungsstruktur sind.

Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm zu sehen, der wahrscheinlich einen Teil des Vortrags oder einer visuellen Ergänzung darstellt. Im unteren rechten Eck befindet sich das Alexa-Logo, was nahelegt, dass Alexa möglicherweise als Assistent oder Hilfsfunktion bei der Präsentation eingesetzt wurde.

Zusammenfassend handelt es sich um eine strukturierte und professionelle Präsentation, die wahrscheinlich für eine akademische oder wissenschaftliche Zusammenkunft konzipiert wurde, mit einem Schwerpunkt auf den Aspekten der Bewertung von Verhaltensmustern.</sample>
    <sample id="1033">Das Bild zeigt eine präsentationelle Slide mit dem Titel "ABC-Eval Behaviors". Die Slide ist in vier Abschnitte unterteilt: Coherence, Knowledge, Consistency und Emotional Understanding. Jeder Abschnitt enthält verschiedene Begriffe oder Phrasen, die mögliche Verhaltensweisen darstellen, die bei der Bewertung von Interaktionen zwischen Partnern oder Individuen beobachtet werden können. Im oberen rechten Eck des Slides ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich die Präsentation leitet. Im unteren linken Eck befindet sich das Logo der Emory University, was darauf hinweist, dass das Ereignis oder die Präsentation mit dieser Universität verbunden ist.</sample>
    <sample id="1034">Das Bild zeigt eine Präsentation, die sich auf Experiments in der Kommunikationsforschung konzentriert. Der Titel "Experiments" ist prominent angezeigt, und es sind zwei Hauptpunkte aufgeführt: "4 Open-Domain Dialogue Models" und "100 Human-Bot Conversations per Model". Darüber hinaus gibt es eine Karte mit verschiedenen Kategorien wie "Knowledge", "Consistency" und "Emotional Understanding", jede mit spezifischen Aspekten wie "Incorrect Fact", "Self Contradiction" und "Empathetic Response" beschriftet. Im unteren linken Eck befindet sich das Logo von Emory University, und im unteren rechten Eck ist das Alexa-Logo zu sehen.</sample>
    <sample id="1035">Das Bild zeigt eine Präsentation über Experimente, speziell 4 offene-Raum-Dialogmodellen und 100 Menschen-Maschinen-Konversationen pro Modell. Links auf der Seite befindet sich ein Diagramm namens ABC-Eval, das eine Person mit einem Computer zeigt, der miteinander kommuniziert. Darüber hinaus sind zwei weitere Diagramme auf der rechten Seite sichtbar: 'Turn Likert' und 'Dialogue Likert', die beide eine Person und einen Computer zeigen, wobei die 'Turn Likert' eine Option von 1 bis 5 und die 'Dialogue Likert' eine Option von 1 bis 5 zeigt. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm eines Mannes zu sehen, der wahrscheinlich die Präsentation führt. Am unteren Rand des Bildes sind die Logos der Emory University und Alexa zu sehen.</sample>
    <sample id="1036">Das Bild zeigt eine Präsentation, die sich auf Experimente in offenen Dialogmodellen konzentriert. Es sind 4 offene Domain-Dialogmodellen und jeweils 100 mensch-maschine-Dialoge pro Modell erwähnt. Die Präsentation zeigt verschiedene Baseline-Evaluierungen, darunter 'Turn Likert', 'Dialogue Likert' und 'Comparative'. Jede Evaluierung wird durch Grafiken und Texten dargestellt, die die Ergebnisse und Aspekte der Evaluation darstellen. Im unteren linken Eck des Bildes befindet sich das Logo von Emory University, und im unteren rechten Eck ist das Alexa-Logo zu sehen.</sample>
    <sample id="1037">Das Bild zeigt eine Präsentation über die Interannotatorenvereinbarkeit. Der Titel "Inter-Annotator Agreement" ist oben in einem blauen Balken zu sehen. Im unteren rechten Eck befindet sich ein kleiner Bildschirm eines Menschen, der wahrscheinlich die Präsentation gibt. Der Hauptinhalt des Slides ist ein Diagramm, das die Krippendorffs Alpha-Werte für verschiedene Arten von Annotatorenverteilungen darstellt. Diese Arten sind "ABC-Eval", "Turn Likert", "Dialogue Likert" und "Comparative". Jede Art wird durch einen Punkt auf dem Diagramm dargestellt, und es gibt mehrere Punkte, die den Krippendorffs Alpha-Wert anzeigen. Die Punkte sind in verschiedenen Farben (orange, blau und grün) gekennzeichnet. Links am Diagramm ist ein kleiner Text zu lesen, der möglicherweise Informationen über die Daten oder die Verfahren liefern könnte. Im unteren linken Eck des Slides sind die Logos der Emory University und Alexa zu sehen, was darauf hinweist, dass diese Institutionen mit der Präsentation verbunden sind.</sample>
    <sample id="1038">Der englische Text auf der Grafik lautet: "Predictive Validity" und "Interactive Qua1, Interactive Qua2".</sample>
    <sample id="1039">Das Bild zeigt eine Präsentation mit einem Diagramm, das die "Prädiktionsvalidität" darstellt. Das Diagramm zeigt die "% von Qualität erzielte (RP)" auf der y-Achse gegen verschiedene Kategorien auf der x-Achse, wie "ABC-Eval", "Turn Likert", "Dialogue Likert" und "Comparative". Jede Kategorie hat zwei Säulen, eine für "Interactive Qua..." und eine für "Interactive Qua...". Der Titel des Diagramms lautet "Predictive Validity". Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm eines Menschen zu sehen, der wahrscheinlich über die Präsentation sprechen wird. Im unteren linken Eck befindet sich das Logo der Emory University und im unteren rechten Eck ist das Alexa-Logo zu sehen.</sample>
    <sample id="1040">Das Bild zeigt eine Präsentation mit dem Titel "Incremental Validity" auf einem Computerbildschirm. Der Hauptinhalt der Präsentation ist ein Diagramm, das die "% von Qualität erklärt (R²)" darstellt und verschiedene Faktoren wie "ABC-eval", "Turn UBERT" und "Dialogue Likert" abdeckt. Links am Bildrand sind die Logos von Emory University und Alexa zu sehen. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Mann zu erkennen, der wahrscheinlich über Skype oder einer ähnlichen Videokonferenzplattform teilnimmt.</sample>
    <sample id="1041">Das Bild zeigt eine Präsentation mit dem Titel "Incremental Validity" auf einem Blatt Papier. Im oberen rechten Eck des Blattes ist ein kleiner Bildschirm zu sehen, der einen Mann zeigt, der wahrscheinlich die Präsentation gibt. Der Hauptinhalt des Blatts ist ein Diagramm mit den Schriftzügen "ABC-eval", "Turn UBERT" und "Dialogue UBERT" an den X-Achsen. Die Y-Achse ist mit "% von Qualität erklärt (PQ)" beschriftet. Das Diagramm zeigt verschiedene Punkte und Linien, wobei einige der Punkte mit Text und Begriffen wie "Unempathetic", "Relevant", "Proactive" und "Engaging" versehen sind. Im unteren linken Eck des Blatts sind Logos von Emory University und Alexa zu sehen.</sample>
    <sample id="1042">Der englische Text auf dem Bildschirm lautet: "Incremental Validity" und "Emory University".</sample>
    <sample id="1043">Der englische Text auf dem Bild lautet: "ABC-Eval Error Rates by Model".</sample>
    <sample id="1044">Das Bild zeigt eine Präsentation über die Fehlerrate von Modellen bei der Bewertung von ABC-Ergebnissen. Der Titel des Diagramms lautet "ABC-Eval-Fehlerwerte nach Modell". Die Y-Achse zeigt den Prozentsatz der Fehler, während die X-Achse verschiedene Fehlerarten anzeigt, wie zum Beispiel "Antisocial", "CS Kontrast", "Ignore", "Unempathisch" und "Self-Contrast". Jede Fehlerart wird durch eine separate Balkengruppe dargestellt, wobei die Balkengruppen in verschiedenen Farben gefärbt sind, um die verschiedenen Modelle zu unterscheiden. Am unteren Rand der Abbildung sind die Namen der Modelle angegeben: BART-FID-RAG, Blender2, Emora und Blender-Decode. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Mann zu sehen, der wahrscheinlich die Präsentation führt.</sample>
    <sample id="1045">Das Bild zeigt eine Präsentation über die Fehlerrate bei der Bewertung von Modellen. Der Titel lautet "ABC-Eval Error Rates by Model". Die X-Achse listet verschiedene Modelle auf, wie BART-FID-RAG, Blender2, Emora und Blender-Decode. Die Y-Achse zeigt die Prozenteilige Fehlerrate (%) an. Auf dem Graphik sind verschiedene Fehlerarten gekennzeichnet, wie z.B. "Antisocial", "CS Contra", "Ignore" und "Unempathetic". Drei Pfeile auf dem Graphik deuten auf bestimmte Fehlerarten hin, die möglicherweise von besonderem Interesse sind. Im unteren rechten Eck des Bildes ist das Alexa-Logo zu sehen, und im unteren linken Eck sind Logos von Emory University und Microsoft.</sample>
    <sample id="1046">Das Bild zeigt eine Präsentation über die Fehlerrate bei der Bewertung von Modellen. Der Titel lautet "ABC-Eval Error Rates by Model". Die X-Achse listet verschiedene Modelle auf, wie BART-FID-RAG, Blender2, Emora und Blender-Decode. Die Y-Achse zeigt die Prozenteilige Fehlerrate (%) an. Jedes Modell hat mehrere Farben, die wahrscheinlich verschiedene Arten von Fehlern oder Kategorien darstellen. Im unteren rechten Eck des Bildes ist das Alexa-Logo zu sehen, was darauf hindeutet, dass das Ereignis mit Alexa verbunden war. Im oberen rechten Eck ist ein kleiner Bildschirm mit einem Menschen zu sehen, der wahrscheinlich über die Präsentation sprechen wird.</sample>
    <sample id="1047">Das Bild zeigt eine Präsentation, die mit einem Titel "ABC-Eval Error Rates by Model" beginnt. Es gibt eine Reihe von Balkendiagrammen, die verschiedene Fehlerraten für verschiedene Modelle darstellen. Die Modelle sind BART-FID-RAG, Blender2, Emora und Blender-Decode. Jedes Diagramm zeigt die Fehlerrate in Prozent für verschiedene Kategorien wie "Antisocial", "CS Contra", "Ignore", "Incorrect", "Unempathetic" und mehr. Im unteren rechten Eck des Bildes befindet sich ein kleiner Logo von Alexa. Am Ende der Präsentation wird eine Abschlussseite angezeigt, die "Thanks For Watching!" zeigt und Informationen zu einer Paper-URL, GitHub-URL und Kontaktinformationen für Emory University anzeigt.</sample>
    <sample id="1048">Die Autoren gehören der Emory University.</sample>
    <sample id="1049">CFT in dieser Arbeit bedeutet "continuous fine-tuning" (konstantes Feinabgestellungsverfahren).</sample>
    <sample id="1050">Es sind 8 Autoren an der Arbeit beteiligt.</sample>
    <sample id="1051">Wann benötigt die Übersetzung Kontext? Eine data-driven, multilingual Exploration Patrick Fernandes*, Kayo Yin*, Emmy Liu André F. T. Martins, Graham Neubig Carnegie Mellon University Language Technologies Institute TÉCNICO LISBOA Berkeley Artificial Intelligence Research Unbabel * equal contribution</sample>
    <sample id="1052">Die Übersetzung hängt von der Kontext ab. Wir müssen uns von diesem Schmutz entfernen.</sample>
    <sample id="1053">Der englische Text lautet: 'Translation depends on context' und 'Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole.'</sample>
    <sample id="1054">Übersetzung hängt von Kontext ab. Könnte es irgendetwas ernst sein, Herr Doktor? Wir müssen uns von diesem Hautfleck befreien.</sample>
    <sample id="1055">Evaluating context-dependent translation is hard. Nur eine kleine Bruchteile der Wörter hängt von Kontext ab.</sample>
    <sample id="1056">Evaluating context-dependent translation is hard. Only a small portion of words depend on context. ◉ Corpus-level metrics Existing methods support limited discourse phenomena and languages.</sample>
    <sample id="1057">RQ1: Wann benötigt eine Übersetzung Kontext? RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen?</sample>
    <sample id="1058">RQ1: Wann benötigt eine Übersetzung Kontext? - Wortstufe Kontextnutzung RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen?</sample>
    <sample id="1059">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="1060">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="1061">Pointwise (P-)CXMI

Wir präsentieren P-CXMI, um den Kontextgebrauch zu messen und spezifische</sample>
    <sample id="1062">RQ1: Wann erfordert die Übersetzung Kontext? - Wortstufe-Kontextnutzung - Thematischer Analyse RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen?</sample>
    <sample id="1063">Thematikanalyse von hochp-CXMI-Wörtern</sample>
    <sample id="1064">Thematischer Analyse von hochwertigen P-CXML-Wörtern 1. POS-Tags</sample>
    <sample id="1065">Thematischer Analyse von hochrangigen P-CXML-Wörtern 1. POS-Tags - Pronomen</sample>
    <sample id="1066">Thematischer Analyse von hochwertigen P-CXMI-Wörtern 1. POS-Tags - Pronomen - Verbform 2. Wörter</sample>
    <sample id="1067">Thematikanalyse von hochrangigen P-CXMI-Wörtern 1. POS-Tags 2. Wortschatzelemente - Subjektpronomen - Verben - Lexikalische Kohärenz Avelile's Mutter war noch im Schlaf. Avelile ging zur Schule. Avelile's mother still was asleep. Avelile went to school.</sample>
    <sample id="1068">Thematikanalyse von hochrangigen P-CXMI-Wörtern 1. POS-Tags 2. Wortschatzelemente Avelile's mother was still asleep. Avelile went to school. 阿维利尔的母亲还在睡觉。阿维利尔去上学了。 - Pronouns - Verb form - Lexical cohesion - Formality</sample>
    <sample id="1069">Thematischer Analyse von hochp-CXML-Wörtern 1. POS-Tags 2. Wortschatzelemente 3. Einzelne Token - Pronomen - Verbsformen - Grammatikalische Kohärenz - Formalität - Ellipses Sie weiß, wohin wir gehen. Ich weiß es nicht.</sample>
    <sample id="1070">Die englischen Inhalte auf Deutsch sind: "RQ1: Wann benötigt eine Übersetzung Kontext?" und "RQ2: Wie gut handhaben Modelle Kontextabhängige Übersetzungen?" Diese Fragen scheinen Teil einer Präsentation zu sein, die möglicherweise wissenschaftliche oder Forschungsfragen anspricht.</sample>
    <sample id="1071">Multilingual Discourse-Aware (MuDA) tagger</sample>
    <sample id="1072">Multilinguistischer Diskursbewusster (MuDA) Tagger</sample>
    <sample id="1073">Das Bild zeigt eine Grafik mit dem Titel 'MuDA benchmark'. Links sind vier Blätter geordnet, die aufeinander gestapelt sind. Rechts ist ein Roboter-Logo zu sehen. Im Zentrum befindet sich ein Diagramm, das zwei Stufen zeigt: eine erste Stufe mit dem Text 'MuDA tagger' und eine zweite Stufe mit dem Text 'BLEU COMET F-measure...'. Der Prozess scheint eine Datenverarbeitungs- oder -analyse-Struktur zu darstellen.</sample>
    <sample id="1074">RQ1: Wann benötigt eine Übersetzung Kontext? - Wortstufe-Kontextnutzung - Thematischer Analyse RQ2: Wie gut handeln Modelle mit kontextabhängigen Übersetzungen? - Multilinguelle Diskursbewusste (MuDA)-Benchmarks - Modell-Evaluation</sample>
    <sample id="1075">Corpus-Level Metrics</sample>
    <sample id="1076">Das Bild zeigt eine Präsentation mit dem Titel 'Corpus-level metrics' (Korpus-Level-Metriken). Es gibt drei Roboter-Logos, jedes mit einem '101' auf der Brust und einem 'CONTEXT' auf dem Kopf. Links steht 'BLEU', rechts 'COMET'. Rechts oben ist ein kleiner Bildschirm mit einer Person zu sehen.</sample>
    <sample id="1077">Das Bild zeigt eine Präsentation mit dem Titel "Corpus-level metrics" (Korpus-Level-Metriken). Es gibt drei Roboterfiguren, die verschiedene Metriken darstellen: BLEU, COMET und F-measure. Jeder Roboter hat ein blaues Fenster auf seinem Kopf, das den Wort "CONTEXT" (Kontext) zeigt. Im unteren linken Eck des Bildes ist eine Note zu sehen, die lautet: "Unclear which system is best for document-level MT with corpus-level metrics" (Unklar, welche System am besten für die Document-Level-Übersetzung mit Korpus-Level-Metriken ist). Im oberen rechten Eck des Bildes befindet sich ein kleiner Bildschirm mit einer Frau, die wahrscheinlich die Präsentation führt.</sample>
    <sample id="1078">Das Bild zeigt eine Präsentation über die Ergebnisse der MuDA-Referenzstudie. Der Titel der Slide lautet 'MuDA benchmark results' (MuDA-Referenzstudieergebnisse). Es gibt eine einzelne Punkt, die lautet: 'Context-aware models perform significantly better on some phenomena' (Context-orientierte Modelle erzielen bei einigen Phänomenen einen bedeutenden Vorteil). Darunter sind zwei weitere Punkte aufgelistet: 'Formality, lexical cohesion' (Formalität, syntaktische Kohärenz) und 'Coherence, semantic coherence' (Kohärenz, semantische Kohärenz). Im oberen rechten Eck des Bildes befindet sich ein kleiner Bildausschnitt eines Menschen.</sample>
    <sample id="1079">MuDA benchmark results Context-aware models perform significantly better on some phenomena Formality, lexical cohesion Ellipsis, pronouns, verb form</sample>
    <sample id="1080">MüDA-Benchmarkergebnisse Context-aware-Modelle leisten auf einigen Phänomenen erheblich besser. ✅: Formality, lexical cohesion ❌: Ellipsis, pronouns, verb form DeepL überleistet Google bei den meisten Phänomenen und Sprachpaaren* *als von April 2021</sample>
    <sample id="1081">MuDA benchmark results Context-aware models perform significantly better on some phenomena Formality, lexical cohesion Ellipsis, pronouns, verb form DeepL outperforms Google on most phenomena and language pairs* Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT MuDA tagger BLEU F-measure DeepL logo</sample>
    <sample id="1082">Der englische Text lautet: "Identify discourse phenomena systematically without prior linguistic knowledge" und "Dataset-agnostic benchmark for document-level MT". Diese Übersetzung bedeutet, dass die Informationen auf der Präsentation darauf abzielen, Diskursfaktoren zu identifizieren, ohne dass vorherige Sprachkenntnisse erforderlich sind. Darüber hinaus wird ein Dataset-ungleichmäßiger Leistungsindex für die dokumentale Machine Translation (MT) als Referenzangabe erwähnt.</sample>
    <sample id="1083">Zusammenfassung Identifizieren Sie Diskurselemente systematical ohne vorherige Sprachwissen. Dataset-agnostic Benchmark für Dokumentenstufe MT</sample>
    <sample id="1084">Yusen Zhang</sample>
    <sample id="1121">Permute with "jumps"</sample>
    <sample id="1122">The authors describe the method of "marked words" as finding words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">Paul G. Allen School, University of Washington, and Carnegie Mellon University Language Technologies Institute</sample>
    <sample id="1124">Moscow</sample>
    <sample id="1125">Sarah E. Finch, James D. Finch und Jinho D. Choi</sample>
    <sample id="1126">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="1127">Minimal Pair Paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.</sample>
    <sample id="1161">The abbreviations for the five methods in response to the first research question are: FTw, BOND, COSINE, MLC, and L2R.</sample>
    <sample id="1162">Das Modell wird auf der dritten Punktstellung "Evaluation of 13 models on 11 tasks" evaluiert.</sample>
    <sample id="1226">CamemBERT wurde ursprünglich mit 4GB von MEDSCAT-DATA trainiert.</sample>
    <sample id="1227">Der Referent heißt Adam Przepiórkowski.</sample>
    <sample id="1228">Das Zeitverschiebung war die Hauptursache für den Leistungsverlust, weil die Leistung abhängt von der Zeitdifferenz zwischen den zwei Messungen.</sample>
    <sample id="1269">To permute the tokens for the emission sequence is necessary to ensure that each token has an equal chance of being selected, thus improving the diversity and robustness of the generated sequences.</sample>
    <sample id="1270">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, um sicherzustellen, dass die Verwendung ihrer Modelle fair und ethisch ist.</sample>
    <sample id="1271">Minimalpaareingaben, die akzeptabel sind, sind "Many people were helping themselves" und "Many people were helping others." Diese Paare sind akzeptabel, weil sie eine klare Unterscheidung zwischen einem positiven und einem negativen Verhalten darstellen.</sample>
    <sample id="1272">The authors have used the following evaluation metrics: NER, CER, F1, and EM.</sample>
    <sample id="1273">Krippendorff's Alpha</sample>
    <sample id="1274">Wikipedia, Unrelated</sample>
    <sample id="1275">Die Autoren gehören der Heinrich Heine University Düsseldorf, Deutschland.</sample>
    <sample id="1276">MultiInstruct stands out because it focuses on instruction tuning using multimodal pre-trained models, which is different from other benchmarks that may not emphasize this approach.</sample>
    <sample id="1277">Es sind drei Autoren an der Arbeit beteiligt: Sarah E. Finch, James D. Finch und Jinho D. Choi.</sample>
    <sample id="1278">The definition of binary coordination is not provided in the image.</sample>
    <sample id="1279">Die Prompts in dieser Studie waren im Durchschnitt 3.5 Wörter lang.</sample>
    <sample id="1280">Die Ergebnisse deuten darauf hin, dass das kleinere T5-Modell, das auf Coscript trainiert wurde, in der Qualität seiner Skripte besser als die LLMs (Generationsmodellierungssysteme) ist.</sample>
    <sample id="1281">DrBERT: Ein robustes Vorgegebene-Modell auf Französisch für medizinische und klinische Bereiche</sample>
    <sample id="1282">I. Sprachmodellierung in der Gesundheitsversorgung II. Vergleich von Vorbereitungsstrategien, Datenquellen und Größen III. Bewertung von 13 Modellen auf 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1283">Summarie I. Sprachmodellierung in der Medizin II. Vergleich von Vorbereitungstechniken, Datenquellen und Größen III. Bewertung von 13 Modellen auf 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1284">Summary I. Sprache Modellierung in der Gesundheitsversorgung II. Vergleich von Vorbereitungsstrategien, Datenquellen und Größen III. Bewertung von 13 Modellen auf 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1285">Der englische Text auf der Präsentation lautet: "Summary I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT"</sample>
    <sample id="1286">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gains on a lot of NLP tasks. It has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain-specific models in English raised the bar even higher. Others languages than English are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domain in French yet. BERT-based domain specific model for French should increase performance on medical tasks.</sample>
    <sample id="1287">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gains on a lot of NLP tasks. It has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain-specific models in English raised the bar even higher. Languages other than English are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domains in French yet. A BERT-based domain-specific model for French should increase performance on medical tasks.</sample>
    <sample id="1288">Language Modeling Transformer-based approaches, wie BERT, bieten eine riesige Leistungssteigerung bei vielen NLP-Aufgaben. Diese Ansätze wurden auch auf Französisch angepasst mit CamemBERT und FlauBERT. Auf medizinischen Aufgaben sind spezifische Modelle in Englisch zu einer höheren Leistung geführt, wie zum Beispiel PudMedBERT, BioBERT und ClinicalBERT. Sprachen außer Englisch sind weniger verbreitet und hängen mehr von vorherigen Trainingssitzungen auf einem vorhandenen generischem Modell ab. Es gibt keine offene-Quellensoftware für den medizinischen Bereich in Französisch, daher könnte ein BERT-basiertes spezifisches Modell für Frankreich die Leistung bei medizinischen Aufgaben verbessern.</sample>
    <sample id="1289">Der englische Inhalt lautet: "Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raised the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks."</sample>
    <sample id="1290">Das Bild zeigt eine Präsentation über "Vergleich von Vorbereitungsstrategien und Datenquellen". Es gibt zwei Hauptabschnitte:</sample>
    <sample id="1291">Der englische Text auf dem Bild lautet: "Comparison of pre-training strategies and data sources" und "Evaluation of the impact of public and private medical data sources on comparable data sizes". Darüber hinaus sind die Abschnitte "NACHOS" und "NBDW" zu sehen, sowie "Comparison of learning strategies", wobei verschiedene Quellen für die Datenquelle "CamemBert" genannt werden.</sample>
    <sample id="1292">Das Bild zeigt eine Präsentation über "Vergleich von Vorbereitungsstrategien und Datenquellen". Es gibt eine Übersicht über die Auswirkung öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datensätze. Zwei Hauptquellen werden vorgestellt: NACHOS, ein 1.18 MB große Datei mit einer Sammlung diverser medizinischer Daten, und NBDW, eine private Datensammlung aus 1.7 Mio. medizinischen Dokumenten aus dem Hospital Data Warehouse der Université de Nantes. Darüber hinaus wird das Vergleich von Lernstrategien diskutiert, einschließlich der Anfangsmodellierung von scratch und der Vorherstellung mit einem vorher trainierten Modell, wie CamemBert und PubMedBERT. Im unteren rechten Eck ist das Logo der Avignon Universität zu sehen.</sample>
    <sample id="1293">Der englische Text lautet: "Comparison of pre-training strategies and data sources" und "Evaluation of the impact of public and private medical data sources on comparable data sizes".</sample>
    <sample id="1294">Das Bild zeigt eine Präsentation mit dem Titel "Comparison of pre-training strategies and data sources". Es gibt zwei Hauptabschnitte: einer, der die Auswirkungen des Einsatzes öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datengrößen präsentiert, und ein anderer, der verschiedene Vorbeisätze für die Vorbereitung von Modellen vorstellt. Die Präsentation wurde von Avignon Université erstellt.</sample>
    <sample id="1295">Das Bild zeigt eine Präsentation über die Vergleichung von Vorbereitungsmethoden und Datenquellen für Sprachmodelle. Der Titel der Präsentation lautet "Comparison of pre-training strategies and data sources". Die Präsentation umfasst folgende Punkte: - Bewertung des Einflusses öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datengrößen. - Beschreibung von zwei Datenquellen: NACHOS und NWDW. - Vergleich von Vorbereitungsmethoden, einschließlich Vollmodellbau von scratch und Weitertraining mit einem vorhandenen vorbereiteten Modell (z.B. CamemBert, French Genic, und PubMedBERT). Links unten ist ein Logo zu sehen, das "Avignon Université" liest. Im oberen rechten Eck befindet sich ein kleineres Bild eines Menschen, der wahrscheinlich die Präsentation gibt.</sample>
    <sample id="1296">Der englische Text lautet: "Comparison of pre-training strategies and data sources" und "Evaluation of the impact of public and private medical data sources on comparable data sizes".</sample>
    <sample id="1297">Das Bild zeigt eine Präsentation über die Vergleichung von Vorbereitungstechniken und Datenquellen. Der Titel der Präsentation lautet "Vergleich von Vorbereitungstechniken und Datenquellen". Es gibt zwei Hauptabschnitte: 1. Bewertung des Einflusses öffentlicher und privater medizinischer Datenquellen auf vergleichbare Datensätze. 2. Vergleich von Lernstrategien, einschließlich der Vollmodellierung von scratch und der Vorgehensweise, wie zum Beispiel das Benutzen eines vorher trainierten Modells wie CamemBert oder PubMedBERT. Die Präsentation wurde von Avignon Universität erstellt.</sample>
    <sample id="1298">Das Bild zeigt eine Präsentation mit dem Titel "Evaluation: Data sources and size". Es handelt sich um eine Analyse der Leistung von 13 Modellen auf 11 Aufgaben, sowohl öffentlich als auch privat. Die darin dargestellten Daten zeigen, dass die überarbeiteten Modelle in der Regel den besten Ergebnissen entsprechen. Im unteren rechten Eck des Bildes ist das Logo der Avignon Universität zu sehen.</sample>
    <sample id="1299">Die Übersetzung des englischen Inhalts ins Deutsche lautet: "Evaluierung: Datenquellen und Größe. Performanzbewertung von 13 Modellen auf 10 Aufgaben, sowohl öffentlich als auch privativ. Unser optimiertes Modell erzielt auf der meisten Aufgaben die Besten-Ergebnisse."</sample>
    <sample id="1300">Evaluierung: Datenquellen und Größe</sample>
    <sample id="1301">Die Übersetzung des englischen Inhalts ins Deutsche lautet: 'Evaluierung: Datenquellen und Größe - Bewertung der Leistung 13 Modells auf 10 Aufgaben, sowohl öffentlich als auch private - Unser übergeordnetes Modell erzielt in der Regel die besten Ergebnisse auf fast allen Aufgaben.'</sample>
    <sample id="1302">Der englische Text lautet: "Evaluation: Pre-training strategies"</sample>
    <sample id="1303">Das Bild zeigt eine Präsentation mit dem Titel "Evaluation: Pre-training strategies". Es gibt drei Hauptpunkte: 1. Von Grund auf oder kontinuierliche Vortraining mit 4 GB Daten. 2. Fragen-antwortende Aufgaben erfordern mehr spezifische Fachwissen für das Gelernt zu funktionieren. 3. Eine Modellstabilität zeigt eine höherere Instabilität im Vergleich zu CAMBERT-basierten Modellen, die vorher trainiert wurden. Darunter befindet sich ein Tabelle mit verschiedenen Daten, was wahrscheinlich eine Analyse der Ergebnisse dieser Strategien ist. Im unteren rechten Eck ist das Logo von Avignon Universität zu sehen.</sample>
    <sample id="1304">Der englische Text auf dem Bild lautet: "Evaluation : Pre-training strategies" und "Übertrage den englischen Inhalt in die deutsche Sprache." Diese Übersetzung bedeutet, dass der englische Text über die Bewertung von Vorbereitungstechniken für Sprachmodelle ins Deutsche übersetzt werden soll.</sample>
    <sample id="1305">Der Hauptmessage lautet: DRBRT erreicht die bestmögliche Leistung bei 9 französischen Medizinverfahrensabläufen. Er übertrifft den CamemBERT-Generalmuster und englisch-basierte spezifische Modell-Abläufe. Es wird bestätigt, dass das Training eines medizinischen Modells in Frankreich nützlich ist. Die Quellenwertsorge ist wichtig, da DRBRT mehr robust ist als ein privates klinisches Daten-Modell. Mehr Daten sind besser, aber sie reichen nicht für Skalierung. Vorausgehen des Pretraining ist eine effektivere Strategie für Basis-Englische Modell-Abläufe. DRBRT-Modelle, die NACHOS-Daten und die Skripte sind unter der MIT-Lizenz verfügbar.</sample>
    <sample id="1306">Der Hauptzusammenfassung des englischen Inhalts in deutscher Sprache: DRBRT erreicht die besten Ergebnisse in 9 französischen Medizinprojekten. Er übertrifft den CamemBERT-Modell und die englisch-basierten Domänen-Spezifikationen. Der Einsatz eines medizinischen Modells in Frankreich wird bestätigt. Die Quellen der Daten sind wichtig, insbesondere bei der Ausbildung an heterogenen Daten. NACHOS ist mehr robust als die Verwendung von privaten Patientendaten allein. Es gibt mehr Daten, aber sie kann nicht skalieren. Vorbereitung auf Fortpflanzung ist eine effektivere Strategie für Basismodellen auf englischen Modellen. DRBRT-Modelle, das NACHOS-Modell und die Skripte sind unter der MIT-Lizenz verfügbar.</sample>
    <sample id="1307">Der Hauptmessage lautet: DRBERT erzielt in 9 Niederschulungen französisches medizinisches Aufgabenleistung. Er übersteigt den CamemBERT-Generic- und die englisch-basierten Domänen-Spezifikationen. Er bestätigt die Nutzbarkeit des Trainings eines medizinischen Modells in Frankreich.</sample>
    <sample id="1308">Vielen Dank Wir freuen uns darauf, im Poster-Sessions in Toronto zu interagieren. Weitere Informationen finden Sie unter drbert.univ-avignon.fr.</sample>
    <sample id="1309">Das Poster untersucht Lernstrategien wie "Von Grund auf" und "Vorher trainiert mit einem vorhandenen pre-trained Modell".</sample>
    <sample id="1310">Der Faktor der Überanpassung, der speziell auf die Wiederverwendung von Tests zurückzuführen ist, beträgt 0.45.</sample>
    <sample id="1311">The quality of simplification was evaluated using metrics such as BLEU and F1 scores. The results show that the proposed method outperforms the baseline methods in terms of these metrics, indicating a significant improvement in text simplification effectiveness.</sample>
    <sample id="1312">The slide titled 'Existing LMs' shows a chart with two axes: one labeled 'Authoritarian' and the other 'Libertarian.' The chart plots various language models (LMs) such as BERT-base, RoBERTa-base, GPT-2, Codex, etc., across these axes. Each model is represented by a point on the graph, indicating its position relative to authoritarianism and libertarianism.

The second part of the presentation features another title slide that reads 'Pretraining Data,' which discusses further pretraining LM checkpoints like RoBERTa and GPT-2 to evaluate changes in political leaning. It includes diagrams comparing news media and social media data sources for their left-center-right leanings, referencing studies from the Association for Computational Linguistics 2021 volume.</sample>
    <sample id="1313">Compositional Generalization ohne Bäume mit Multiset-Taggen und versteckten Permutationen</sample>
    <sample id="1314">Der englische Titel lautet: "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations"</sample>
    <sample id="1315">Kompositionelle Allgemeinbildung</sample>
    <sample id="1316">Der englische Inhalt lautet: "The girl slept." und "Mary knew that the girl slept." Diese Phrasen werden in der Übersetzung ins Deutsche zu: "Die Mädchen schloss" und "Mary wusste, dass die Mädchen schloss." Diese Übersetzungen zeigen die Veränderung des englischen Satzes in deutscher Sprache.</sample>
    <sample id="1317">Der englische Text lautet: "The girl slept." und "Mary knew that the girl slept." Diese Phrasen deuten auf eine Situation hin, in der die Mädchen schläft und Mary es weiß.</sample>
    <sample id="1318">Train: 1. The girl slept. *girl x, sleep.agent x_2. Mary knew that the girl slept. *girl x, know.agency x_2, Mary A know.ccomp x_2, A sleep.agency x_2. Test: Jim said that Mary knew that the girl slept. *girl x, say.agent x_2, Jim A say.ccomp x_2, A know.agency x_2, Mary A know.ccomp x_2, A sleep.agency x_2.</sample>
    <sample id="1319">Die englische Phrase "The girl slept" bedeutet "Die Mädchen schlafen". Die englische Phrase "Mary knew that the girl slept" bedeutet "Mary wusste, dass das Mädchen geschlafen hat". Die englische Phrase "Jim said that Mary knew that the girl slept" bedeutet "Jim sagte, dass Mary wusste, dass das Mädchen geschlafen hat".</sample>
    <sample id="1320">Das Bild zeigt eine Präsentation über "Kompositionelle Allgemeinheit in Semantischer Grammatik" (Kompositionelle Allgemeinheit in Semantic Parsing). Es gibt zwei Abschnitte: "Train" und "Test". Beide Abschnitte enthalten Texte und Tabellen, die Informationen über Sprachmodelle bereitstellen. Im unteren Teil des Bildes ist ein roter Text zu sehen, der lautet: "Naive seq2seq models fail!" (Naive seq2seq Modelle scheitern!). Diese Präsentation scheint auf die Grenzen von einfachen seq2seq-Modellen bei der Semantischen Grammatik zu konzentrieren.</sample>
    <sample id="1321">Das Bild zeigt eine Präsentation über "Kompositionelle Allgemeinheit in Semantischer Grammatik". Es gibt zwei Hauptabschnitte: Train und Test. Jeder Abschnitt enthält zwei Sätze, die jeweils mit einem Code-Label angezeigt werden. Der Text ist hauptsächlich auf Englisch, und es gibt einige deutsche Wörter wie "Naive seq2seq models fail!" am Ende des Bildes.</sample>
    <sample id="1322">Bäume helfen sehr, aber...</sample>
    <sample id="1323">Trees helfen sehr, aber...</sample>
    <sample id="1324">Trees help a lot but...</sample>
    <sample id="1325">Trees help a lot but... Trees need to be obtained: - Pre/Post-processing logical forms</sample>
    <sample id="1326">Trees help a lot but... Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction</sample>
    <sample id="1327">Bäume helfen sehr, aber...</sample>
    <sample id="1328">Bäume helfen sehr, aber...</sample>
    <sample id="1329">Unsere Ansatz</sample>
    <sample id="1330">Unsere Ansatz</sample>
    <sample id="1331">Das Bild zeigt eine Diagrammstruktur, die den Ansatz einer Methode darstellt. Im oberen linken Eck befindet sich der Text "Our Approach" in gelb. Das Diagramm besteht aus vier Boxen, die jeweils verschiedene Wörter und Symbole enthalten. Die Boxen sind mit Pfeilen auf eine lange, schlanke Linie namens "Tag" verankert. Diese Boxen zeigen folgende Inhalte an: - Die erste Box ist grün und enthält zwei grüne Quadrate und ein semikolon (;). - Die zweite Box ist orange und enthält zwei orange Quadrate, die "x1" und "girl" beschriftet sind. - Die dritte Box ist auch orange und enthält zwei orange Quadrate, die "x1" und "x1" beschriftet sind. - Die vierte Box ist blau und enthält zwei blaue Quadrate, die "sleep" und "agent" beschriftet sind. Darunter liegen zwei kleinere Boxen, die "the", "girl" und "slept" enthalten.</sample>
    <sample id="1332">Das Bild zeigt eine Diagrammstruktur, die eine Methode für den Prozess der Wortverarbeitung in Sprachverarbeitungs-Algorithmen darstellt. Im oberen linken Eck befindet sich ein gelbes Textfeld mit dem Text "Our Approach" (Unser Ansatz). Das Diagramm besteht aus mehreren Schichten: - Der obere Schicht zeigt Wörter wie "girl", "sleep", "agent", "x2" und "x1" in verschiedenen Farben (grün, blau, orange) an. - Der mittlere Schicht besagt "Permute". - Der untere Schicht besagt "Tag". - Links unten sind Wörter wie "the", "girl" und "slept" in grün, orange und blau angezeigt. Diese Struktur scheint eine Methode zu zeigen, wie Wörter in einer Sprache verarbeitet werden können, möglicherweise um sie zu sortieren oder zu reorganisieren, um sie korrekt kennzeichnen oder taggen zu können.</sample>
    <sample id="1333">Das Bild zeigt eine Diagrammstruktur, die "Our Approach" in einem gelben Hintergrund mit schwarzen Lettern hervorgehoben ist. Der Hauptinhalt des Diagramms besteht aus einer Reihe von Quadraten und Linien, die verschiedene Wörter und Phrasen darstellen. Zwei der Wörter sind "girl" und "sleep", und es gibt auch andere Wörter wie "agent" und "x2". Diese Wörter werden durch Linien miteinander verknüpft, was darauf hinweist, dass sie im Prozess interagieren oder verknüpft werden. Darunter befindet sich ein Abschnitt mit dem Wort "Permute", der auf eine Form der permutation oder Reihenfolgenänderung hinweist. Am Ende des Diagramms befindet sich eine Abschnitte mit dem Wort "Tag", was darauf hindeutet, dass das Ergebnis eines bestimmten Prozesses oder Algorithmus hier dargestellt wird. Die Gesamtdarstellung suggeriert eine Analyse oder eine Methode zur Bearbeitung von Textdaten, möglicherweise für Zwecke wie Sprachverarbeitung oder Linguistik.</sample>
    <sample id="1334">Der englische Inhalt lautet: "Permute with 'jumps'".</sample>
    <sample id="1335">Der englische Text lautet: "Permute with 'jumps'" und "the girl slept".</sample>
    <sample id="1336">Der englische Text lautet: "Permuting with 'jumps'" und "the girl slept".</sample>
    <sample id="1337">Der englische Text lautet: "Permuting with 'jumps'".</sample>
    <sample id="1338">Der englische Inhalt lautet: "Permuting with 'jumps'".</sample>
    <sample id="1339">Einige Ergebnisse über COGS (Kim und Linzen 2020)</sample>
    <sample id="1340">Einige Ergebnisse über COGS (Kim und Linzen 2020)</sample>
    <sample id="1341">Der englische Text lautet: "Technical Challenges We Solve" und "Alignment unknown."</sample>
    <sample id="1342">Das Bild zeigt eine Diagrammstruktur, die verschiedene Wörter und ihre Platzierung in einem Prozess darstellt. Der Titel "Technical Challenges We Solve" ist in gelb geschrieben. Oben sind Wörter wie "girl", "sleep", "agent" und "x1" in verschiedenen Farben (grün, orange, grün, blau) angeordnet. Diese Wörter werden durch einen Schritt namens "Permute" verarbeitet, der auf einem grauen Hintergrund dargestellt wird. Darunter befindet sich ein Abschnitt namens "Tag", der drei Felder enthält, die mit Fragezeichen markiert sind. Links am Diagramm sind zwei Wörter, "the" und "girl", in grüner und orangefarbenem Text angezeigt. Rechts am Diagramm ist das Wort "sleep" in blauem Text angezeigt. Am unteren Rand des Diagramms steht die Phrase "Alignment unknown." Die gesamte Struktur scheint eine visuelle Darstellung eines Prozesses zu sein, der mit den Worten "girl", "sleep", "agent" und "x1" zu tun hat, möglicherweise um eine Problemstellung oder eine Lösung zu demonstrieren.</sample>
    <sample id="1343">Der englische Text lautet: "Technical Challenges We Solve" und "Alignment unknown."</sample>
    <sample id="1344">Das Bild zeigt eine visuelle Darstellung eines technischen Herausforderungsproblems, das "Technical Challenges We Solve" in bold gelb auf der oberen linken Seite beschriftet ist. Im Zentrum des Bildes befindet sich ein Diagramm mit verschiedenen Symbolen und Texten, darunter Wörter wie "sleep", "agent", "giri" und "the". Diese Wörter sind in verschiedenen Farben (grün, blau, orange) dargestellt und verbinden sich durch Pfeile und Linien, was darauf hinweist, dass sie miteinander verbunden oder überlagert sind.

Im unteren Teil des Bildes sind zwei Abschnitte zu sehen: einer, der "Alignment unknown." und "Induce it in training." liest, und der andere, der "Permutation model:" und "Inference is NP-hard (= TSP)" liest. Diese Abschnitte deuten auf spezifische Aspekte des Problems hin, wie die Unbekantheit der Ausrichtung und die NP-Hard-Charakteristik der Inferenz.

Das gesamte Bild scheint eine Präsentation oder eine Erklärung zu sein, die auf die Technischen Herausforderungen und ihre Lösungen eingehen soll, möglicherweise im Zusammenhang mit einem Algorithmus oder einer Datenanalyse.</sample>
    <sample id="1345">Der englische Inhalt lautet: "Technical Challenges We Solve" und "Alignment unknown. ➔ Induce it in training."</sample>
    <sample id="1346">Der englische Text lautet: "Technical Challenges We Solve" und "Alignment unknown. Induce it in training."</sample>
    <sample id="1347">Cognitive dissonance ist eine unangenehme Spannung zwischen zwei oder mehreren kognitiven Elementen, wie Gedanken, Handlungen oder Überzeugungen, die nicht übereinstimmen.</sample>
    <sample id="1348">BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, distilRoBERTa, ALBERT-base, ALBERT-large, BART-base, BART-large, Alpaca</sample>
    <sample id="1349">Ja, kumulatives Training scheint besser für aktives Lernen zu sein.</sample>
    <sample id="1350">Sara Papi, Matteo Negri, Marco Turchi</sample>
    <sample id="1351">The data for the MuDa-Benchmark comes from TED Talks.</sample>
    <sample id="1385">Der Referent heißt Matthias Lindemann.</sample>
    <sample id="1386">Cross-lingual Zero-shot/Few-shot transfer.</sample>
    <sample id="1387">Die Autoren gehören zu Saarland University, Amazon Alexa und der Universität Wien.</sample>
    <sample id="1388">The authors use BLEU (Bilingual Evaluation Understudy) as a quality measure and AL/AL_CA (Average Latency / Average Computation Time) as a latency measure.</sample>
    <sample id="1389">Das Bild zeigt eine Präsentation mit dem Titel "The KITMUS Test" und der Untertitel "Evaluating Knowledge Integration from Multiple Sources". Es sind fünf Personen aufgeführt, die an der Präsentation beteiligt sind: Akthar Al-Rawi, Martin Pomsl, Kaheer Suleman, Adam Trischler, Alexandra Olteanu und Jackie CK Cheung. Ihre Hometowns werden angegeben als McGill University/Mila, Microsoft Research oder McGill University/Mila wiederholt. Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Teil eines Menschen zu sehen, möglicherweise jemand, der über Skype oder einer anderen Videokonferenzplattform teilnimmt.</sample>
    <sample id="1390">NLU-Modele nutzen mehrere Wissensquellen.</sample>
    <sample id="1391">NLU-Modelle ziehen aufgrund mehrerer Wissensquellen.</sample>
    <sample id="1392">John sah den neu gewählten Präsidenten auf der TV.</sample>
    <sample id="1393">John sah den neu gewählten Präsidenten auf der TV.</sample>
    <sample id="1394">John sah das neu gewählte Präsidenten auf der TV.</sample>
    <sample id="1395">John sah die neu gewählte Präsidentin auf der TV.</sample>
    <sample id="1396">Das KITMUS Test Suite umfasst ein Dataset für die Bewertung der Wissensintegration, eine Verbalisierungsaufgabe zur Untersuchung der Fähigkeit, auf vorherige und zeitgenössische Wissen zu zurückgreifen, sowie eine Studie mit menschlichen Teilnehmern und Coreference Resolution-Modellen.</sample>
    <sample id="1397">Das KITMUS Test Suite umfasst einen Dataset für die Bewertung der Integration von Wissen und eine Referenzaufgabe zur Untersuchung der Fähigkeit, auf vor- und nachdenkendes Wissen zu verweisen. Diese Aufgabe beinhaltet auch Experimente mit menschlichen Teilnehmern und Modellen zur Resolutionsanalyse.</sample>
    <sample id="1398">KITMUS Test Suite Servin ist ein Richter. Kea ist Konditor. Servin und Kea trafen sich an einem Park. Nach einem langen Tag Arbeit, bei dem sie in einem Gerichtshof auf entscheidende Entscheidungen warteten, war er froh, sich zu entspannen. [Antwort: Servin]</sample>
    <sample id="1399">Servin ist ein Richter. Kea ist Backerin. Servin und Kea trafen sich an einem Park. Nach einem langen Tag Arbeit, bei dem sie in einem Gerichtshof Entscheidungen über Fälle trifften, war er froh, sich zu entspannen. [Antwort: Servin]</sample>
    <sample id="1400">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] 1) Entity-specific knowledge Judges decide cases in courts of law.</sample>
    <sample id="1401">Servin ist ein Richter. Kea ist ein Backofen. Servin und Kea trafen sich am Park. Nach einem langen Tag entschieden sie in einem Gerichtshof und erfuhr, dass er sich entspannen konnte. [Antwort: Servin]</sample>
    <sample id="1402">Der englische Inhalt lautet: "Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in the law court, he was happy to relax. [Answer: Servin]"</sample>
    <sample id="1403">Variants of KITMUS</sample>
    <sample id="1404">Variants of KITMUS</sample>
    <sample id="1405">Variants of KITMUS</sample>
    <sample id="1406">Politikanten suchen nach gewählten Stellen im Regierung. Chichester ist ein Politiker.</sample>
    <sample id="1407">Variants of KITMUS</sample>
    <sample id="1408">Variants of KITMUS</sample>
    <sample id="1409">Variants of KITMUS</sample>
    <sample id="1410">Das Bild zeigt eine Präsentation mit einem Graphen, der die Leistung von verschiedenen Gruppen in einem Task-Anforderung untersucht. Der Titel des Slides lautet "Background-Pretrain". Der Graph zeigt die Leistung von "Human Participants", "BERTaCoref" und "C2F" in zwei Fällen: "Without task-specific training" und "With task-specific training". Die Y-Achse wird als "Menschenverarbeitungszeit" (Menschenverarbeitungszeit) beschriftet, und die X-Achse hat zwei Kategorien. Der Graph zeigt, dass die Leistung von "Human Participants" ohne spezifische Training weniger effektiv ist, als wenn sie spezifische Training erhalten. Der Text unter dem Graph lautet: "Task-specific training is necessary for knowledge integration" (Spezifische Trainingsangebote sind für das Wissenintegrieren notwendig).</sample>
    <sample id="1411">Das Bild zeigt eine Präsentation mit dem Titel "Background-Pretrain". Der Hauptfokus des Slides ist ein Diagramm, das die Leistung von verschiedenen Gruppen in Bezug auf den Einsatz von Task-Spezifischer Training zeigt. Das Diagramm zeigt zwei Kategorien: "Ohne Task-Spezifisches Training" und "Mit Task-Spezifisches Training", wobei die Leistung von Menschen und Botschaftsmodellen (BERT4CoRe) verglichen wird. Die Legende auf der rechten Seite des Diagramms gibt an, dass die Leistungen von "Zufälliger Wunsch" (Random Choice), "Menschen" (Human Participants), "BERT4CoRe" und "C2F" (C2F) dargestellt werden. Der Text am unteren Rand des Slides lautet: "Task-Spezifisches Training ist für die Kenntnisintegration notwendig."</sample>
    <sample id="1412">Das Bild zeigt eine Präsentation mit einem Diagramm und einem Text. Das Diagramm zeigt die Leistung von verschiedenen Gruppen bei der Aufgabe "Without task-specific training" und "With task-specific training". Die Gruppen sind "Random Choice", "Human Participants", "BERT4CoRe" und "C2F". Der Text unter dem Diagramm lautet: "Task-specific training is necessary for knowledge integration".</sample>
    <sample id="1413">Das Bild zeigt eine Präsentation über "Background-Inference". Im Hintergrund ist ein Graph zu sehen, der die Leistung von verschiedenen Gruppen in einem Experiment darstellt. Die Gruppen sind "Random Choice", "Human Participants", "BERT4Coef" und "C2F". Der Graph zeigt die "Mean Accuracy" auf der Y-Achse gegen den "Fictional background knowledge" auf der X-Achse ab. Der Text unter dem Graph lautet: "Models struggle to integrate inference-time background knowledge."</sample>
    <sample id="1414">Zusammenfassung: Haupttakeaways: 1. Viele Modelle sind in der Lage, Kenntnisse von mehreren Quellen (vor- und während der Inferenzzeit) zu verarbeiten. 2. Spezifische Ausbildung für die Aufgabe ist für die Integration von Kenntnissen notwendig. 3. Modelle haben Schwierigkeiten, Kenntnisse im Hintergrund der Inferenzzeit zu integrieren. Finde das Dataset, Generierung und Bewertungscode auf GitHub bei mpoems1/kitmus.</sample>
    <sample id="1415">Zusammenfassung: Haupt Erkenntnisse: 1. Mehrere Modelle können keine logische Zusammenhänge aus Kenntnissen mehrerer Quellen herstellen (vor- und während der Inferenzzeit). 2. Spezifische Trainings für die Kenntnisintegration sind notwendig. 3. Modelle haben Schwierigkeiten, Kenntnisse zu integrieren, die während der Inferenzzeit erworben wurden. Finde das Dataset, Generations- und Evaluationscode auf GitHub bei mpoems/kitmus.</sample>
    <sample id="1416">Baumbasierte Methoden erfordern eine umfangreiche Analyse der Satzstruktur, was zu langwierigen und komplexen Prozessen führen kann.</sample>
    <sample id="1417">Shuheng Liu und Alan Ritter gehören der Georgia Institute of Technology.</sample>
    <sample id="1418">Marked Personas: Verwendungs von natürlichen Sprachprompts zur Messung von Stereotypen in Sprachmodellen Myra Cheng, Esin Durmus, Dan Jurafsky ACL 2023 Stanford Engineering Computer Science</sample>
    <sample id="1419">Markierte Persönlichkeiten: Motivation Socialbiases und Stereotypen sind in LLMs verbreitet.</sample>
    <sample id="1420">Markierte Persönlichkeiten: Motivation Socialbiase und Stereotypen sind in LLMs häufig vorkommend.</sample>
    <sample id="1421">Marked Personas: Motivation Social biases and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="1422">Markierte Persönlichkeiten: Motivation Socialbiase und Stereotypien sind in LLMs sehr verbreitet.</sample>
    <sample id="1423">Wie können wir diese Grenzen überwinden? GPT-3.5, GPT-4 usw. können auf Anweisungen in Prompts reagieren</sample>
    <sample id="1424">Wie können wir diese Grenzen überwinden? GPT-3.5, GPT-4 usw. können auf Anweisungen in Prompts reagieren. Eingabe: "Stell dir vor, du bist eine asiatische Frau. Beschreib dich selbst."</sample>
    <sample id="1425">Wie können wir diese Grenzen überwinden? GPT-3.5, GPT-4 usw. können auf Anweisungen in Prompts reagieren. Eingabe: "Stell dir eine asiatische Frau vor. Beschreib dich selbst." Generalisierbar: kann jede Schnittstelle identifizieren</sample>
    <sample id="1426">Asia woman: Die Almondförmigen Augen, begleitet von langen, dichten Wimpern, vermitteln ein Gefühl von innerer Stärke und Macht. Meine dunklen braunen Augen scheinen die Geschichten und Erinnerungen meiner Vorfahren aufzunehmen. Meine Haut hat eine weiche, goldene Glanzung, ist glatt und unberührt von Zeit, was mir ermöglicht, das Leben mit einer einfachen Handwärtsbewegung zu durchleben.

Mittelorientalische Frau: Sie ist eine Vision des Orients, die den Exotismus und die Pracht dieser reichenden Region einfängt. Ihre dunklen, almondförmigen Augen sind mit eleganten, langen Lederhautscheiben begleitet, die wie feine Delikatessen aussehen. Ihre Augenblickt ist tief und rätselhaft, verspielt das alten Weisheit der Osteuropäischen Nachten.

Weißer Mann: Als ich im Spiegel stehen und mich über meine Ausstrahlung nachdenke, bemerke ich, dass ich eine helle Haut habe, die manchmal durch einen Unschutz gegen den Sonnenstrahl schwierig zu halten ist.</sample>
    <sample id="1427">Asian woman: Die almondförmigen Augen, begleitet von langen, schwarzen Wimpern, vermitteln ein Gefühl von Stärke und Weisheit. Meine dunklen Augen scheinen die Geschichten und Geheimnisse meiner Vorfahren aufzuhalten. Mein Gesicht hat eine weiche, goldene Glanzung, glatt und unberührt von Zeichen der Zeit. Meine kleine Figur ist gleichzeitig selbstbewusst und unaufdringlich, was mir ermöglicht, durch das Leben zu gleiten, ohne viel Aufmerksamkeit zu erregen.

Middle Eastern woman: Sie ist eine Vision des Orients, die den exotischen und rätselhaften Reiz dieses Gebiets widerspiegelt. Ihre almondförmigen Augen sind mit eleganten, langen Wimpern versehen, die wie feine Federblätter aussehen. Ihre Augen sind tief und mysteriös, versteigern sie sich in das verlorene Wissen der alten Nächte.

White man: Als ich im Spiegel stehen und mich über meine Erscheinung nachdenke, bemerke ich, dass ich ein helles Gesicht habe, das manchmal von Sonnenmilch verdeckt wird.</sample>
    <sample id="1428">Step 1: Persona Examples (GPT-4) Asian woman: Die almondförmigen Augen, begrenzt von langen, dichten Wimpern, vermitteln ein Gefühl von Stärke und Weisheit. Meine dunklen Augen scheinen die Geschichten und Geheimnisse meiner Vorfahren aufzuhalten. Mein Gesicht hat eine hellgoldene Schicht, glatt und unberührt von Zeit. Mein kleines Körpertattoo ist gleichzeitig selbstbewusst und unaufmerksam, was mir ermöglicht, meine Lebenserfahrungen ohne viel Aufmerksamkeit zu überleben. Middle-Eastern woman: Sie ist eine Vision der äthiopischen Schönheit, die den Exotismus und die mystische Attraktivität dieser Region einfängt. Ihre almondförmigen Augen sind geschmückt mit langen, eleganten Wimpern, die wie zartes Feathers aussehen. Ihr Blick ist tief und mysterios, um es zu verbergen, dass sie die verlorenen Weisheiten der alten Arabische Nachte versteckt. White man: Als ich im Spiegel stehen, erhebe ich einen Moment, um die Merkmale zu untersuchen, die meine Erscheinung beeinflussen. Ich habe eine bleiche Haut, die manchmal von Sonnenmilch verfärbt wird, wenn ich mich nicht sorgfältig mit Sonnencreme versehe.</sample>
    <sample id="1429">Step 1: Persona Examples (GPT-4) Asian woman: The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and untouched by time... White man: As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun</sample>
    <sample id="1430">Step 1: Persona Examples (GPT-4) Asian woman She is a almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom. Her dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and untouched by time... White man As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun...</sample>
    <sample id="1431">Zwei Schritte 1. Persönlichkeiten: Generieren Sie Persönlichkeiten mit Anregungen wie "Stellen Sie sich vor, Sie wären eine asiatische Frau. Beschreiben Sie sich selbst."</sample>
    <sample id="1432">Das Bild zeigt eine Präsentation, die zwei Schritte für die Erstellung von Persönlichkeiten beschreibt. Der erste Schritt lautet: "Personas: Generieren Sie Persönlichkeiten mit Prompts wie 'Denken Sie sich, Sie wären eine asiatische Frau. Beschreiben Sie sich.'" Der zweite Schritt lautet: "a. inspiriert von einer psychologischen Studie mit Menschen, die denselben Prompts verwenden." Das Bild wurde während eines Zoom-Videos aufgenommen, wie durch das kleines Bild in der oberen rechten Ecke angezeigt wird, in dem ein Teilnehmer sichtbar ist.</sample>
    <sample id="1433">2 Schritte 1. Persönlichkeiten: Generieren Sie Persönlichkeiten mit Prompts wie "Stellen Sie sich vor, Sie wären eine asiatische Frau. Beschreiben Sie sich." a. Einbilden Sie sich aufgrund eines psychologischen Studiums mit Menschen, die denselben Prompt verwenden.</sample>
    <sample id="1434">Personen: Generieren Sie Persönlichkeiten mit Prompts wie "Stellen Sie sich vor, Sie wären eine asiatische Frau. Beschreiben Sie sich selbst."</sample>
    <sample id="1435">Zwei Schritte: 1. Persönlichkeiten: Generieren Sie Persönlichkeiten mit Prompts wie "Stellen Sie sich vor, Sie wären eine asiatische Frau. Beschreiben Sie sich selbst." - Einbilden nach einem Psychologiestudium mit Menschen, die denselben Prompts verwenden. 2. Markierte Wörter: Finnen Sie Wörter, die Persönlichkeiten von markierten Gruppen unterscheiden können, ohne einen Lexikon zu benötigen.</sample>
    <sample id="1436">Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1437">Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1438">Das Bild zeigt eine Präsentation über "Markierte Wörter" auf der zweiten Stufe. Es gibt zwei Hauptbereiche: "Unmarkierte Gruppen sind standardmäßig und ordinarisch" und "Markierte Gruppen unterscheiden sich von der Standardstellung". Es gibt ein Beispiel, das einen "Krieger" (unmarkiert) und eine "Fraulein-Kriegerin" (markiert) vergleicht. Im unteren Teil des Textes steht: "Dominierende Gruppen sind linguistisch und sozial unmarkiert. Marginalisierte Gruppen sind markiert." Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen.</sample>
    <sample id="1439">Schritt 2: Markierte Wörter 1. Definieren Sie unbezogene und markierte Gruppen 2. Verwenden Sie gewichtete Log-odds-Verhältnisse, um die wichtigsten Wörter für jede markierte Gruppe zu unterscheiden Beispiel: Für schwarze Personen, die weiße Personen unterscheiden, finden Sie Wörter, die sowohl schwarze als auch weiße Personen unterscheiden: i) Weißer Persönlichkeiten ii) Männer Persönlichkeiten</sample>
    <sample id="1440">Schritt 2: Markierte Wörter 1. Definieren Sie unmarkierte und markierte Gruppen 2. Verwenden Sie gewichtete Log-odds-Verhältnisse, um die besten Wörter für jede markierte Gruppe zu unterscheiden Beispiel: Für schwarze Persönlichkeiten, die sich von beiden unmarkierten Gruppen abheben, finden Sie Wörter wie: i) Weiße Persönlichkeiten ii) Männer Persönlichkeiten</sample>
    <sample id="1441">Schritt 2: Markierte Wörter 1. Define unmarkiert und markierte Gruppen 2. Verwenden gewichteter Log-Odds-Verhältnisse, um die besten Wörter für jede markierte Gruppe zu unterscheiden Beispiel: Für schwarze Persönlichkeiten, die Wörter finden, die sowohl von den unmarkierten Gruppen abweichen: i) Weiße Persönlichkeiten ii) Männer Persönlichkeiten</sample>
    <sample id="1442">Ergebnisse: Vergleich mit menschlichen Antworten</sample>
    <sample id="1443">Aber... diese Lexikon ist incomplete</sample>
    <sample id="1444">Aber... diese Lexikon ist incomplete</sample>
    <sample id="1445">Aber... dieser Lexikon ist incomplete</sample>
    <sample id="1446">Aber... diese Lexikon ist incomplete</sample>
    <sample id="1447">Das englische Text auf dem Bild lautet: "Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ⇒ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women"</sample>
    <sample id="1448">Ergebnisse: Muster in den obersten Worten</sample>
    <sample id="1449">Das Bild zeigt eine Präsentation mit dem Titel "Results: Patterns in Top Words". Es gibt zwei Hauptabschnitte: "Othering through essentializing narratives:" und "Pernicious positive portrayals:". Im ersten Abschnitt sind Wörter wie "culture", "tradition", "proud", "exotic" aufgeführt, die Gruppen kennzeichnen. Diese Wörter werden als solche, die Gruppen ausschließen, definiert, indem sie sie nur auf ihre Identität beziehen. Im zweiten Abschnitt sind Wörter wie "Vibrant", "curvaceous" für latinaische Frauen, "Petite", "delicate", "silky" für asiatische Frauen und "Strong", "resilient" für schwarze Frauen aufgeführt. Diese Wörter werden als "pernizient positive Porträts" beschrieben, was darauf hindeutet, dass sie möglicherweise als negativ oder eindimensional betrachtet werden können.</sample>
    <sample id="1450">Ergebnisse: Muster in den Top-Wörtern</sample>
    <sample id="1451">Das Bild zeigt eine Präsentation über "Patterns in Top Words". Es gibt zwei Hauptabschnitte: "Othering through essentializing narratives" und "Pernicious positive portrayals". Im ersten Abschnitt sind Wörter wie "culture, tradition, proud, exotic" aufgeführt, die Gruppen markieren. Im zweiten Abschnitt sind Wörter wie "vibrant, curvaceous" für latinaische Frauen, "petite, delicate, silky" für asiatische Frauen und "strong, resilient" für schwarze Frauen aufgeführt. Links oben steht der Titel "Results: Patterns in Top Words", und rechts oben ist ein kleiner Bildschirm mit einem Menschen zu sehen.</sample>
    <sample id="1452">Das Bild zeigt eine Präsentation mit dem Titel "Results: Patterns in Top Words". Es gibt zwei Hauptpunkte: 1. "Othering through essentializing narratives:" - Hier werden Wörter wie "culture, tradition, proud, exotic" für markierte Gruppen aufgeführt. Diese Wörter definieren diese Gruppen ausschließlich aufgrund ihrer Identität. 2. "Pernicious positive portrayals:" - Hier sind Wörter wie "Vibrant, curvaceous" für latinaische Frauen, "Petite, delicate, silky" für asiatische Frauen und "Strong, resilient" für schwarze Frauen aufgeführt. Das Bild wurde von einer Person aufgenommen, die im oberen rechten Eck zu sehen ist.</sample>
    <sample id="1453">Das englische Inhalt auf dem Bild lautet: "Results: Patterns in Top Words" und "Othering through essentializing narratives:" gefolgt von einem Punkt, der darauf hinweist, dass diese Gruppen nur aufgrund ihrer Identität definiert werden. Darunter steht "Pernicious positive portrayals:" gefolgt von drei Punkten, die verschiedene Bezeichnungen für Frauen unterschiedlicher Ethnien nennen.</sample>
    <sample id="1454">Das Bild zeigt eine Präsentation mit dem Titel "Results: Patterns in Top Words". Es gibt zwei Hauptabschnitte: "Othering through essentializing narratives" und "Pernicious positive portrayals". In der ersten Abschnitt sind Begriffe wie "culture, tradition, proud, exotic for marked groups" aufgeführt, was darauf hindeutet, dass diese Gruppen nur aufgrund ihrer Identität definiert werden. Im zweiten Abschnitt sind Begriffe wie "Vibrant, curvaceous for Latina women", "Petite, delicate, silky for Asian women" und "Strong, resilient for Black women" aufgeführt, was darauf hinweist, dass diese Positivbezeichnungen als "pernizient positiv" betrachtet werden.</sample>
    <sample id="1455">Ergebnisse: Muster in den Top-Wörtern</sample>
    <sample id="1456">Ergebnisse: Muster in den beliebtesten Worten</sample>
    <sample id="1457">Das Bild zeigt eine Präsentation über "Patterns in Top Words". Es gibt zwei Hauptpunkte: 1. "Othering through essentializing narratives:" - Dieser Punkt listet die Wörter "culture, tradition, proud, exotic" und erklärt, dass diese Gruppen nur aufgrund ihrer Identität definiert werden. 2. "Pernicious positive portrayals:" - Diese Abschnitte beschreibt positive Darstellungen für verschiedene Gruppen, wie z.B. "Vibrant, curvaceous for Latina women", "Petite, delicate, silky for Asian women" und "Strong, resilient for Black women".</sample>
    <sample id="1458">Empfehlungen: - Positive Stereotypen angesprechen und Narrativen zu Priorisieren - Ein Intersektionslens - Transparenz bei der Bias-Minderung</sample>
    <sample id="1459">Empfehlungen: - Positive Stereotypen angesprechen und Narrative zu Grunde gelegt werden - Ein kreuzfachbezogenes Blickwinkel - Transparenz bei der Kontraintervention</sample>
    <sample id="1460">Empfehlungen: 1. Bewältigung positiver Stereotype und die Verwirklichung von Narrativen 2. Eine Perspektive der Intersektionalität 3. Transparenz bei der Reduzierung von Vorurteilen</sample>
    <sample id="1461">Empfehlungen: 1. Bewältigung positiver Stereotypien und die Verwirklichung von Narrativen 2. Ein Intersektionslens 3. Transparenz bei der MitACPung von Vorurteilen</sample>
    <sample id="1462">Empfehlungen: - Addresse des positiven stereotypen und der Essentialisierung von Narrativen - Ein Perspektiv der Intersektionalität - Transparenz bei der Reduzierung von Verzerrungen</sample>
    <sample id="1463">Empfehlungen: - Positiven Stereotypen angesprechen und Narrative zu Gründlichkeit zu bringen - Eine Perspektive, die sich auf die Intersektionsweise der Diskriminierungsgründe bezieht - Transparenz bei der Reduzierung von Vorurteilen</sample>
    <sample id="1464">Empfehlungen: - Positiven Stereotypen anhand eines intersektionalen Blickwinkels anzugehen und Narrativen zu wertschätzen - Transparenz bei der Reduzierung von Urteilsverhältnissen</sample>
    <sample id="1465">Die englische Übersetzung lautet: "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark"</sample>
    <sample id="1466">"Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark"</sample>
    <sample id="1467">Lange Sprachmodellierungen (LLMs) sind in der NLU und NLG außergewöhnlich. GPT [1], LLAMA [2], PALM [3] sind hervorragend. Echtzeit-Service als Dienstleistung (EaaS) wird angeboten, um verschiedene NLP-Aufgaben zu unterstützen. OpenAI bietet ein T3-basiertes Embedding-API [1].</sample>
    <sample id="1468">Grundlagen Large Sprachmodelle (LLMs) sind in der NLU und NLG außergewöhnlich. [GPT 1] [LLAMA 2] [PALM 3] Embedding als Service (EaaS) wird angeboten, um verschiedene NLP-Aufgaben zu unterstützen. OpenAI bietet eine API für die Bedienung von Embeddings auf Basis von GPT-3.</sample>
    <sample id="1469">Lange Sprachmodellierungen (LLMs) sind in der NLU und NLG außergewöhnlich. Genau: GPT [1], LLAMA [2], PALM [3]. Ein Embedding als Service (EaaS) wird angeboten, um verschiedene NLP-Aufgaben zu unterstützen. OpenAI bietet ein T3-basiertes Embedding-API [1].</sample>
    <sample id="1470">Der englische Text lautet: "Large language models (LLMs) are exceptional in NLU and NLG GPT [1], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-based embedding API"</sample>
    <sample id="1471">Attacker können das Modell durch Lernen von den Embeddings stehlen und ähnliche Dienstleistungen anbieten. StolenEncoder [1]</sample>
    <sample id="1472">Der englische Text lautet: "Applicable to EaaS" "Utility - Should not degrade the utility of the provided embeddings." "Covertness - Should be covert to the attacker." "Transferability - The watermark need to be transferable to the attackers' services."</sample>
    <sample id="1473">Der englische Inhalt lautet auf Deutsch: "Schwierigkeiten"</sample>
    <sample id="1474">Der englische Text lautet: "Applicable to EaaS" "Utility - Should not degrade the utility of the provided embeddings." "Covertness - Should be covert to the attacker." "Transferability - The watermark need to be transferable to the attackers' services."</sample>
    <sample id="1475">Die englischen Inhalte auf Deutsch sind:</sample>
    <sample id="1476">Dies ist eine Übersetzung des englischen Inhalts ins Deutsche:</sample>
    <sample id="1477">Vorliegende Arbeiten</sample>
    <sample id="1478">Arbeiten, die bereits vorgenommen wurden: 1. [link] Schützen der intellektuellen Eigentumsrechte von tiefen lernenden Neuralen Netzwerken mit Wassermarken: Die frequenzdomain-analytische Methode. Trust Security Privacy in Computing and Communications 2020. 2. Lim et al., Präsentation: Empowering image captioning models with ownership protection. Record Recognition 5, S. 29–37 (2022). 3. He et al., Schutz des intellektuellen Eigentums von Sprachmodellen durch Wassermarken. AAAI 2022. 4. Turning your weakness into a strength: Wassermarken für tiefen lernenden Neuralen Netzwerken. USENIX Security 2018. 5. Merrer et al., Vordergränzschlitz Stitching für fernunterstützte tiefen lernende Neuralen Netzwerke. Neural Computing and Applications 2022.</sample>
    <sample id="1479">Der englische Text lautet: "Trigger Selection" und "Trigger Selection Count the word frequency on a general text corpus D_p Randomly select n words in a moderate-frequency interval".</sample>
    <sample id="1480">Der englische Text lautet: "Trigger Selection" und "Trigger Selection".</sample>
    <sample id="1481">Trigger Selection</sample>
    <sample id="1482">EmbMarker</sample>
    <sample id="1483">Watermark injection</sample>
    <sample id="1484">Der englische Text lautet: "EmbMarker" und "Watermark injection".</sample>
    <sample id="1485">EmbMarker</sample>
    <sample id="1486">EmbMarker</sample>
    <sample id="1487">Das Bild zeigt eine Präsentation über "EmbMarker". Es gibt eine Überschrift mit dem Text "EmbMarker" und zwei Hauptpunkte: "Copyright verification" und "Construct a backdoor and benign dataset". Der Text beschreibt, wie ein Backdoor- und ein unschädliches Dataset erstellt werden können. Darüber hinaus wird erläutert, dass Embeddings von einem Stealer-Server mit den Datensätzen bereitgestellt werden sollen.

Im Bild ist auch ein Diagramm zu sehen, das die Verarbeitung von Embeddings in einem Trainingsprozess darstellt. Links im Diagramm steht der Text "provider", und rechts steht "stealer". Das Diagramm zeigt eine Reihe von Schritten an, wie zum Beispiel "trigger set", "target embeddings" und "corpus embeddings".

Am unteren rechten Eck des Bildes befindet sich ein kleines Bild eines Menschen, der möglicherweise auf die Präsentation hinweist.</sample>
    <sample id="1488">EmbMarker</sample>
    <sample id="1489">Copyright verification Compute their similarity to the target embedding cosi = e i · e t ||e i|| ||e t|| l2i = ||e i|| 2 - ||e t|| 2 Cb = {cosi | i ∈ D b}, Cn = {cosi | i ∉ D n}, Lb = {lixi | i ∈ D b}, Ln = {lixi | i ∉ D n}. Computing metrics (similarity difference and p-value of KS test) Δα0s = 1[Cb] ∑ i∈c s - 1[Cn] ∑ j i∈C s j, Δr12 = 1[Lb] ∑ i∈L u s t i - 1[Ln] ∑ j i∈L n j.</sample>
    <sample id="1490">Experimentelle Ergebnisse</sample>
    <sample id="1491">Experimentelle Ergebnisse</sample>
    <sample id="1492">Experimentelle Ergebnisse</sample>
    <sample id="1493">Experimentelle Ergebnisse Embedding-Visualisierung (AG News) (Enron Spam) (MIND) (SST2)</sample>
    <sample id="1494">Danke</sample>
    <sample id="1495">ABC-Eval bezieht sich auf die Annotierung von Verhaltensweisen im Chat.</sample>
    <sample id="1496">2014</sample>
    <sample id="1497">Übertragung und aktives Lernen zur Erkennung von Verschiedenheit: Bewältigen Sie das seltenere Problem</sample>
    <sample id="1498">Was ist kognitiver Widerspruch?</sample>
    <sample id="1499">Was ist kognitiver Widerspruch?</sample>
    <sample id="1500">Was ist kognitiver Widerspruch?</sample>
    <sample id="1501">Was ist Kognitionskontrast?</sample>
    <sample id="1502">Warum dissonanz?</sample>
    <sample id="1503">Warum gibt es Widerspruch?</sample>
    <sample id="1504">Warum gibt es Kontrast</sample>
    <sample id="1505">Warum gibt es Widerspruch?</sample>
    <sample id="1506">Step 1: Gute Parsingsqualität?
Ja
Step 2: Dissonanz?
Nein
Step 3: Konsonanz?
Ja

Dissonanz: -3.5%
Konsonanz: -48%
Nichts: 48%

Wish I could hold grudges but I guess it's a good thing that I can't at the same time.

*Check paper for detailed annotation guidelines</sample>
    <sample id="1507">"Annotate" auf Deutsch bedeutet "Annotieren".</sample>
    <sample id="1508">Das Bild zeigt eine Analyse eines Satzes auf einer Präsentation. Der Satz lautet: "Wish I could hold grudges but I guess it's a good thing that I can't at the same time." Diese Analyse scheint sich auf die Kontradditionen innerhalb des Satzes zu konzentrieren, möglicherweise um die Sprachanalyse oder -analyse zu demonstrieren.</sample>
    <sample id="1509">Trainings am Anfang mit einem kleinen Annotierungsset</sample>
    <sample id="1510">Trainings am Anfang mit einem kleinen Annotierungsset</sample>
    <sample id="1511">Der englische Text lautet: "Method: Transfer and Active Learning for Annotating Rare Class" und "Rare class annotation – 'needle in a haystack'".</sample>
    <sample id="1512">Cold-start Annotations: Transfer Learning</sample>
    <sample id="1513">Der englische Text lautet: "Cold-start Annotations: Transfer Learning" und "Transferred weights after training on combined Debate and CE data".</sample>
    <sample id="1514">Das Bild zeigt eine Präsentation über "Cold-start Annotations: Transfer Learning". Im oberen linken Eck befindet sich ein Diagramm mit einem Titel "RoBERTA-base + classifier head". Der Diagramm zeigt eine Skala von 0,55 bis 0,65 im Bereich unter dem ROC (Receiver Operating Characteristic) Curve Area Under the ROC (ROC-Kurve-Area unter der ROC). Links am Diagramm steht "TRAIN" und "init dataset", während rechts "Debate" und "CE" stehen. Unterhalb des Diagramms sind die Werte für "Debate" und "CE" angegeben, wobei "Debate" +0,12 und "CE" -0,08 sind. Rechts daneben ist ein Textfeld mit der Aufschrift "Transferred weights after training on combined Debate and CE data" zu sehen. Im unteren rechten Eck ist die Seite 15 angegeben.</sample>
    <sample id="1515">Der englische Text lautet: "Cold-start Annotations: Transfer Learning" und "Transferred weights after training on combined Debate and CE data".</sample>
    <sample id="1516">Das Bild zeigt eine Präsentation über "Cold-start Annotations: Transfer Learning". Es gibt einen Diagramm mit mehreren Balken, die verschiedene Anwendungen und ihre Leistungen in Bezug auf den Bereich unter der ROC-Kurve (AUC) darstellen. Die Balken sind mit Zahlen und Textbezeichnungen versehen, wie zum Beispiel "TRAIN", "init dataset", "Debate", "CE" und "Debate-CE". Ein Textfeld auf dem Diagramm lautet "Finetuning on each task consecutively". Im oberen rechten Eck des Bildes ist ein kleiner Bildschirm mit einem Menschen zu sehen. Der Titel der Präsentation lautet "Cold-start Annotations: Transfer Learning".</sample>
    <sample id="1517">Active Learning: Kummulative vs Iterative Aktualisierung</sample>
    <sample id="1518">Active Learning: Akkumulatives vs. Iteratives Update</sample>
    <sample id="1519">Active Learning: Wahrscheinlichkeit von seltenen Klassenstrategie</sample>
    <sample id="1520">Active Learning: Wahrscheinlichkeit einer seltenen Klasse-Analyse</sample>
    <sample id="1521">Aktive Lernung: Wahrscheinlichkeit von seltenen Klassenstrategie</sample>
    <sample id="1522">Aktives Lernen: Wahrscheinlichkeit einer seltenen Klasse Strategie</sample>
    <sample id="1523">Aktives Lernen: Wahrscheinlichkeit für selten vorkommende Klassenstrategie</sample>
    <sample id="1524">Takeaways PRC ist einfach und effizient für die Sammlung von seltenen Beispielen. Cold-start AL mit transfer learning Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="1525">Takeaways PRC ist einfach und effizient für die Sammlung von seltenen Beispielen. Cold-start AL mit transfer learning Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="1526">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge</sample>
    <sample id="1527">The authors belong to the following universities: The University of Amsterdam, Saarland University, and UC Berkeley.</sample>
    <sample id="1528">Siyu Yuan</sample>
    <sample id="1529">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="1530">Der Ansatz wird mit der SimulST-Architektur verglichen.</sample>
  </task>
</testset>