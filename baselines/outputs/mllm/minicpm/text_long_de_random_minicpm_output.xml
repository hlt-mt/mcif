<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Sprachmodelle werden hauptsächlich mit großen Web-Spiderdatenquellen wie der Common Crawl trainiert.</sample>
    <sample id="1">McGill University, Mila und Microsoft Research</sample>
    <sample id="2">Das Team von Ant Group hat ein Paper über visuell reiche Dokumententypen präsentiert. Das Hauptaugenmerk liegt auf der Verständigung von Formen, Rechnungsprotokollen und Plakaten. Das Team hat eine neue Vorgehensweise für die Präsentation von Dokumenten vorgeschlagen, indem sie LayoutMask benutzt, um die Text-Layout-Interaktionen zu verbessern. Sie verwenden zwei Maskierungsstrategien: Whole Word Masking und Layout-Aware Masking, um die Text-Layout-Interaktionen zu fördern.</sample>
    <sample id="3">Hallo, ich bin Regina Stodden und ich werde euch durch den ersten Teil unserer Präsentation über DEPLAIN führen, ein neues Korpus für die deutsche Textidentifizierung auf der Dokumenten- und Satellenebene. Meine Kamerakollegen sind Omar und ich, und wir möchten euch unsere Arbeit präsentieren. Textsimplifizierung ist ein Prozess, bei dem man einen Text anpasst, um ihn für eine bestimmte Zielgruppe zu verbessern, wie Menschen mit Leseschwierigkeiten oder nicht-native Sprecher. Um einen Textsimplifizierungsmodell zu trainieren, benötigen wir Parallelpaare von Texten, zum Beispiel von Dokumenten oder Sätzen. Als Beispiel sehe ich hier eine parallel angeordnete Sätze, in der eine komplexere deutsche Sätze ins einfache Englische übersetzt wird. Um den Satz zu simplifizieren, können verschiedene Techniken verwendet werden, wie z.B. Substitution, Clause deletion, Rearrangement oder die Einfügung von Worten. Wir möchten nun unsere neue Korpus DEPLAIN präsentieren, da es in den letzten Jahren einige Probleme mit den vorhandenen Korpora gab. Zum Beispiel sind diese Korpora zu klein, um ein Textsimplifizierungsmodell zu trainieren. Die drei Modelle, die in den letzten Jahren vorgestellt wurden, sind alle automatisch angeordnet, was bedeutet, dass sie möglicherweise fehlerhaften Alignment haben können. Deshalb möchten wir unsere neue Korpus DEPLAIN präsentieren, das sich in zwei Unter-Korpora aufteilt: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-apa basiert auf Nachrichtenartikeln und wurde alle 483 Dokumente manuell angeordnet. Es gibt etwa 13.000 parallel angeordnete Sätze. Bei DEPLAIN-web handelt es sich um verschiedene Bereiche und diese 750 Dokumente wurden sowohl manuell als auch mit automatischen Alignment-Methode angeordnet. Im Ganzen resultiert dies in 30.450 Sätze. Wir haben unsere Sätze ein bisschen mehr untersucht, so zum Beispiel auf die Art der Simplifikation. Wie ihr seht, sind Bibel-Ausdrücke viel mehr vereinfacht als Nachrichtenartikel oder Sprachlehrer-Ausdrücke. Auf allen Niveaus, wie zum Beispiel auf der Ebene des Leseverhaltens, auf der Ebene der Strukturveränderung oder auf der Gesamt-Niveau der Simplifikation. Außerdem seht ihr, dass unsere DEPLAIN-Korpus eine hohe Vielfalt unterschiedlicher Simplifikationstransformationen hat. Zunächst einmal haben wir in der DEPLAIN-apa-Korpus viel mehr Rearrangements und Wort-Einträge als in der DEPLAIN-web-Korpus. Auf der anderen Seite haben wir im Web-Korpus viel mehr Rephrasings. Also, was können wir damit tun? Hallo, ich bin Omar und ich werde euch die Anwendungsgebiete für unser Dataset DEPLAIN präsentieren. Der erste Einsatzfall ist die Bewertung automatischer Alignment-Methode. In den letzten Jahren wurden viele Alignment-Methode vorgestellt, aber im Kontext von Maschinentranslationen, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und wir versuchen, die Alignment zwischen den Sätzen in beiden Dokumenten zu extrahieren. Aber in unserem Fall handelt es sich um zwei parallele Dokumente mit derselben Sprache, derselben Inhaltsart, aber einer höheren oder niedrigeren Komplexität. Und jetzt, wenn wir unsere Korpus DEPLAIN haben, das manuell angeordnete Sätze als Goldstandard Alignment hat, können wir diese Sätze als Basist für die Bewertung dieser vorgestellten Alignment-Methode verwenden. Wir haben einige Anpassungen an die vorgestellten Methoden gemacht und wir haben alle diese Anpassungen und die Codes, um unsere Experimente auszuführen, im Papier veröffentlicht. Am Ende konkluierten wir, dass die Alignment-Methode MASSalign die beste Alignment-Methode für die Textsimplifizierung in deutscher Sprache ist. Und ihr könnt auch die Code für die Ausführung dieser Methode auf eure eigenen Dokumenten im Papier finden. Der zweite Einsatzfall, den wir im Paper präsentiert haben, ist die automatische Textsimplifizierung durch die Fine-Tuning von Sprachmodellen, um einfacheres Text aus komplexeren Eingaben zu produzieren. Wir haben zwei verschiedene Modelle fine-tuned. Wir haben das Modell long-mBART für die Dokumenten-Level-Simplifizierung und das Modell mBART für die Satellenebene-Simplifizierung. Ihr könnt auch alle Checkpoints finden und weitere Details zur Bewertungsscore und zur Bewertungsmetriken in unserem Experimenten im Paper. Wir haben erkannt, dass diese Basic-Fine-Tuning die Basistest-Scores üppig übertreffen könnte und wir haben diese Ergebnisse als Basis-Benchmark für die Problemstellung der automatischen Textsimplifizierung in Zukunft vorgeschlagen. Danke für eure Aufmerksamkeit und wir hoffen, dass wir alle während des Konferenz treffen.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL</sample>
    <sample id="6">Das Team von Jiaan und seinen Kollegen hat eine Forschungsarbeit vorgelegt, die "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" heißt. Sie haben einen neuen Ansatz für die Integration von Multilingüen- und Kreuzsprachsummarisierung in ein einziges Modell vorgeschlagen, das sie "many-to-many summarization" nennen. Dieses Modell kann Texte in jeder Sprache zusammenfassen und in jeder Sprache zusammenfassende Texte generieren. Sie haben auch eine neue Vorbereitungsmethode namens PISCES vorgestellt, die auf drei Schritten basiert: Metapretraining, Kreuzsprachvorbereitung und Aufgaben-Spezifische Vorbereitung. Ihre Studie zeigt, dass diese Ansätze besser im Vergleich zu traditionellen Methoden im Zusammenhang mit der Transferenz von Aufgabenwissen zwischen verschiedenen Sprachen funktionieren.</sample>
    <sample id="7">Ja, sie funktionieren immer noch.</sample>
    <sample id="8">Die vorgeschlagene menschlichen Bewertungsmethode, die ABC-Eval genannt wird, beurteilt die Chat-Modelle auf verschiedenen Aspekten, indem sie den Verhaltensweisen der Modelle bewertet.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Verfügbarkeit und Qualität eines sauberen Validations-Sets ab.</sample>
    <sample id="10">Zusammenfassend: Das Ergebnis kann verbessert werden, indem die LLMs Zugang zu mehr oder vollständig korrekter Hintergrundinformationen erhalten.</sample>
    <sample id="11">Jack Hessel, Forschungsleiter bei AI2, präsentiert "Do Androids Laugh at Electric Sheep? Humor 'Understanding' Benchmarks from The New Yorker Caption Contest". Diese Zusammenarbeit mit der Universität Utah, Cornell University, der Universität Washington, Air Mail und OpenAI untersucht die Fähigkeit von Sprachmodellen, Humor zu verstehen. Hessel zeigt, dass große Sprachmodelle den Standort und die Details von Zeichnungen in der New Yorker-Kapitänssendung korrekt identifizieren können, aber ihre Erklärungen für das Humor sind oft ungenau. Der Vorgang umfasst eine Aufgabe zur Übereinstimmung, eine Qualitätsschätzung und eine Erklärungsgenerierung. Hessel betont die Bedeutung der Datenquelle und bietet Zugriff auf Modelle und Leaderboards für weitere Untersuchungen.</sample>
    <sample id="12">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="13">Daniel Rotem präsentiert seine Arbeit "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings", die er im Labor von Professor Roy Schwartz am Hebrew University in Jerusalem gearbeitet hat. Adaptive Inference ist ein Verfahren zur Reduzierung der Inferenzzeit von großen Sprachmodellen. Es basiert auf der Annahme, dass realweltliche Daten unterschiedlich komplex sind und daher mit niedrigcapazitätsmodellen für einfache Beispiele behandelt werden können, um den durchschnittlichen Inferenzkosten zu reduzieren, sei es Zeit oder Geld. Die beiden häufigsten adaptive Inferenzmethoden sind Multi Model und Early Exit. Im Multi Model-Verfahren werden mehrere Modelle nebeneinander trainiert und für die Inferenz in Reihe verwendet, bis ein Classifier entscheidet, die Rechnung zu beenden. Bei Early Exit werden mehrere Classifier nach den Zwischenlapisern der Transformer-Layer trainiert und für die Inferenz verwendet, bis ein Classifier entscheidet, die Rechnung zu beenden. Rotem untersuchte die Vorteile und Nachteile dieser Methoden und fand, dass Early Exit für schnelle Inferenz besser ist, aber konfliktierende Gradienten führen zu schlechterer Leistung. Er entwickelte SWEET (Separating Weights in Early Exit Transformers), eine neue Fine-Tuning-Methode für Early Exit-Architekturen, die das Problem der konfliktierenden Gradienten löst. Durch SWEET schließen die Leistungen von Early Exit und Multi Model sich ein Stück, und bei hoher Inferenzgeschwindigkeit outperformt SWEET sowohl Early Exit als auch Multi Model.</sample>
    <sample id="14">Abhängigkeitsstruktur der Koordination.</sample>
    <sample id="15">3</sample>
    <sample id="16">Bible texts werden stärker vereinfacht.</sample>
    <sample id="17">Das Team von Shengqiong Wu hat ein neues System für die Verbundenen-Relationenextraktion vorgestellt. Das System verbessert die Leistung durch das Gleichgewicht von internem und externem Informationen. Es nutzt Grapheninformationen und die Integration von Themeninformationen, um die Verbundenenrelationen zu identifizieren.</sample>
    <sample id="18">Beispiel: "Marge liest diesen wundervollen Roman über Bienen gestern"</sample>
    <sample id="19">Zhang Qin präsentiert die Arbeit "A Survey for Efficient Open Domain Question Answering" von Shenzhen University, die von ACL 2023 akzeptiert wurde. Die Arbeit untersucht die Herausforderungen bei der Erstellung effizienter offener Domänenfragenantwortsysteme und beschreibt einige zentrale Techniken, um diese Herausforderungen zu lösen. Diese Techniken umfassen das Vermeiden der Brute-Search durch die Verwendung des Approximativen-Nahbruchsuchens, das Skip Reading für schnelle Lesung und die Reduzierung der Indexgröße durch Dokumentenfilterung oder Produktquantisierung. Darüber hinaus werden verschiedene Ansätze zur Reduzierung des Modellumfangs vorgestellt, wie die Auswahl von leichteren Modellen oder die Implementierung von Parametersharing. Der Vortrag zeigt auch eine Analyse der Leistung von verschiedenen offenen Domänenfragenantwortsystemen in Bezug auf ihre Speicherverbrauch, Inferenzgeschwindigkeit und Genauigkeit.</sample>
    <sample id="20">Ja, die Modelle sind für Ihre Forschung verfügbar. Sie sind auf Hugging Face verfügbar und können unter der Lizenz "MIT" verwendet werden.</sample>
    <sample id="21">DEPLAIN-apa basiert auf Nachrichtenartikeln.</sample>
    <sample id="22">Modellarchitektur, Modellgröße und mehres Fine-Tuning sind die Faktoren für eine gute Generalisierung.</sample>
    <sample id="23">Das Team untersuchte die Kapazitäten der Imagen-Modellierung, insbesondere bei der Darstellung von Texten. T5-XXL, ein T5-Modell, wurde untersucht und erhielt aufgrund seiner Subword-Tokennutzung schwierigkeiten bei der Darstellung von Texten. Durch das Hinzufügen eines ByT5-Modells zur T5-XXL-Text-Encoder verbesserte sich die Textdarstellung. Diese Ansatz ermöglichte es, die Texte besser zu darstellen, obwohl die Diffusion-Modellfehler weiterhin vorkommen.</sample>
    <sample id="24">Die Tendenz wurde gemessen, indem die Länge der wichtigen Abhängigkeiten gemessen wurde.</sample>
    <sample id="25">Die Experimente wurden durch die Berechnung von Statistiken über Koordinationsstrukturen aus dem verbesserten Penn Treebank durchgeführt. Die Ergebnisse bestätigen die Beobachtung, dass links stehende Konjunktionen im Allgemeinen kürzer sind. Diese Tendenz wird mit der Länge der beiden Konjunktionen und der Position des Reglers, wenn vorhanden, abhängig. Wenn das Regler auf der linken Seite ist oder nicht vorhanden ist, bevorzugen die links stehenden Konjunktionen kürzer zu sein. Wenn das Regler auf der rechten Seite ist, ist diese Tendenz nicht mehr sichtbar. Diese Beobachtungen bieten eine Argumentation gegen asyimetrische Koordinationsstrukturen und für symmetrische Strukturen.</sample>
    <sample id="26">Ein Basisklassifikator trainiert mit unausgewogenen Daten erzielt nicht viel besser als Zufälligkeit, wie es im Paper "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" beschrieben wird.</sample>
    <sample id="27">There are four authors involved in the work.</sample>
    <sample id="28">Bob und Alice</sample>
    <sample id="29">kontextsensitive MÜ-Modelle schneiden besser ab als kontextagnostische Modelle bei Diskursphänomenen wie Formelles und Lexikalische Kohärenz.</sample>
    <sample id="30">Das Team von AI2 und USC präsentiert "LLM-Blender", ein einfach aber effektives Ensemble-Learning-Framework für große Sprachmodellierungen. Das Hauptkonzept basiert auf einer beiden Schrittchen-Struktur: Zunächst werden mehrere Modelle mit dem gleichen Input-Example auf ihre Leistungsbewertung abgestellt, und dann wird eine generative Fusion durchgeführt, um die besten Ergebnisse zu erzielen. Das Team hat experimentell gezeigt, dass LLM-Blender die Leistungen anderer Modelle verbessern kann, insbesondere bei der Auswahl der besten Modellvarianten für bestimmte Input-Examples.</sample>
    <sample id="31">Die Autoren gehören an Johns Hopkins University.</sample>
    <sample id="33">Das vorgestellte Framework quantifiziert die Positionalität durch die Korrelation von Annotatoren mit den Labels und Vorhersagen der Modelle und Datensätze. Es verwendet Pearson's R-Korrelationsscore, um die Ähnlichkeit zwischen den Annotatoren und den Modellen oder Datensätzen zu messen.</sample>
    <sample id="34">Das Team von Marcos Treviso hat eine Arbeit namens "CREST: Ein gemeinsames Framework für die Rationalisierung und der generierten Kontrasten" vorgestellt. Diese Arbeit ist eine Zusammenarbeit mit Alexis Ross, Nuno Guerreiro und André Martins. CREST ist ein Framework zur Kombination von selektiver Rationalisierung und Kontrastgenerierung. Das Hauptaugenmerk liegt auf der Erstellung von plausiblen und flüssigen Kontrasten, die den Entscheidungsprozess des Klassifizators beeinflussen können. Durch die Verwendung dieser Kontrasten während der Trainingsvorgänge verbessern sich die plausiblen Erklärungen, die sich auf die kontrastierenden Teile des Inputs konzentrieren. Die Forschung wurde auf verschiedenen Datasetts wie IMDB und SNLI getestet, wobei CREST-Rationalization die besten Ergebnisse erzielte.</sample>
    <sample id="36">Das Vortrag "Learning Language-Specific Layers for Multilingual Machine Translation" von Robin Schmidt, Yi-Hsiu Liao und Stephan Peitz präsentiert eine Lösung für die Verbesserung der Kapazität pro Sprache in multilingualen Maschinelles Übersetzen. Die Idee besteht darin, eine Translatormodell mit einem einzelnen Modell für alle Sprachpaare zu verwenden, anstatt ein Modell für jede Paar zu trainieren. Dies ermöglicht eine Skalierbarkeit, eine schnelle Übersetzung zwischen jedem Paar und reduziert Fehlerkettensiegen. Allerdings führt dies zu begrenzter Kapazität pro Sprache und erhöht die Komplexität des Trainingsprozesses.

Die Lösung besteht darin, Sprachspezifische Schichten (LSLs) hinzuzufügen, wobei jeder Layer für eine bestimmte Sprache verwendet wird. Das Modell kann auf der Laufzeit die richtige Sublayer auswählen, entweder Quell- oder Zielsprache, was die Kapazität pro Sprache erhöht und die Infrakostanhaltung konstant bleibt. Der Entwurf ermöglicht es dem Modell, die Platzierung der LSLs selbst zu lernen, indem es die Werte der verschiedenen Schichten überprüft, um die effektivste Platzierung zu finden.

Die Forscher haben das Modell auf 10 Sprachen trainiert und haben seine Leistung auf der WMT21-News-Übersetzungsmasken auf allen Sprachpaaren und den Flores-101-Übersetzungsmasken gemessen. Ergebnisse zeigen eine signifikante Verbesserung gegenüber dem Basismodell und den Sprachadaptern, insbesondere bei niedrigressourcierten Sprachen. Statistische Tests bestätigen die Bedeutung dieser Verbesserungen für die meisten Sprachpaare.</sample>
    <sample id="37">Die menschlichen Teilnehmer haben stereotypische Phrasen im Vergleich zu den generierten Personas verwendet.</sample>
    <sample id="38">Die Datenquellen, die in dieser Studie verwendet wurden, sind die Enhanced Version des Penn Treebank.</sample>
    <sample id="39">Adam Przepiórkowski</sample>
    <sample id="40">Dissonance-first Approach, Debate, Dissonance Stance Classification, Expansion and Comparison Classes of PDTB</sample>
    <sample id="41">PeaCoK ist ein Personengemeinsinn-Knowledge-Graph, der die Weltwissen von mehreren Personen und ihren Merkmalen darstellt. Es umfasst über 3.800 Personen und 40.000 einzigartige Merkmale, sowie über 100.000 persönliche Schlussfolgerungen oder Fakten. PeaCoK wurde in drei Schritten entwickelt: Zunächst wurden Personen aus bestehenden Commonsense-Graphen ausgewählt, dann wurden Attribute von Personen aus Commonsense-Knowledge-Graphen und großen Sprachmodellen hervorgehoben, schließlich wurden PeaCoK-Relationen mit einer Mischung aus menschlicher und künstlicher Intelligenz aufgestellt. Das Experiment zeigt, dass PeaCoK als fundiertes Wissensbank für Sprachmodelle genutzt werden kann, um sie zu verbessern und zu trainieren.</sample>
    <sample id="42">Shuheng</sample>
    <sample id="43">Vasudha</sample>
    <sample id="44">Das vorgestellte Framework unterscheidet sich von den bisherigen Arbeiten, indem es die Positionalität von Daten und Modellen untersucht. Es kontrastiert nicht nur mit den Annotatoren, sondern auch direkt mit den endgültigen Ergebnissen der Modelle und Daten. Dies ermöglicht eine umfassendere Analyse der Positionalität, da es die Entscheidungen und Vorurteile der endgültigen Modelle und Daten analysiert.</sample>
    <sample id="45">Die generierten Persönlichkeiten haben die meisten Überschneidungen mit dem Lexikon der Stereotypen.</sample>
    <sample id="46">DeepL und Google Translate wurden verglichen.</sample>
    <sample id="47">Hey, ich bin Shangbin, ein PhD-Studierender an der University of Washington. Heute präsentiere ich unsere Arbeit "Von der Vorbildungsdatenbank zu Sprachmodellen bis hin zu Downstream-Tasken: Die Spuren politischer Biase im NLP". Sprachmodelle werden mit großen Mengen von Webkrawldaten trainiert. Politische Nachrichten sind gut vertreten in diesen Daten. Eine Umfrage über den C4-Korpus zeigt, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut vertreten sind. Diese Mischung hat sowohl Vorteile als auch Nachteile für die Anwendung von Sprachmodellen. Auf einer Seite können sie von verschiedenen Perspektiven lernen, was demokratisch und die Vielfalt von Ideen feiert. Auf der anderen Seite sind diese unterschiedlichen Meinungen innenhalb sozialen Biases verankert und könnten potenzielle Fairness-Probleme bei der Anwendung auf verschiedene Aufgaben erzeugen. Deshalb möchten wir das Verbreiten von politischen Biase im Pipeline von Vorbildungsdaten zu Sprachmodellen bis hin zu Downstream-Tasken untersuchen. Hier sind unsere Fragen: Zunächst, wie können wir die politische Neigung von Sprachmodellen bewerten, und welche Rolle spielen die Vorbildungsdaten darin? Zweitens, wie leisten Sprachmodelle mit verschiedenen politischen Neigungen bei der Ausführung von Downstream-Tasken, und könnten diese Probleme mit Fairness im NLP-Verfahren auftreten? Wir beginnen mit der Frage, wie wir die politische Neigung von Sprachmodellen bewerten können. Wir verwenden eine automatische Bewertung durch die Verwendung von politischen Fragebögen wie dem politischen Konferenz-Test. Das ermöglicht uns, eine solide Grundlage in der politischen Wissenschaft zu nutzen. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Neigungen haben. Sie sind auf allen vier Quadranten des politischen Kamps zu finden. GPT-4 ist der liberales Sprachmodell unter allen, und die GPT-Reihe und ihre Varianten sind im Allgemeinen sozial liberaler als die BART-Reihe und ihre Varianten. Zweitens möchten wir untersuchen, wie weit die politischen Biase von Sprachmodellen von den Vorbildungsdaten aufgenommen werden. Wir führen ein kontrolliertes Experiment durch, indem wir Sprachmodell-Schaltpunkte weiter mit 6 verschiedenen partizipalen Korpora vorbereiten, die sich in den Zeitabschnitt vor und nach dem 45. Präsidenten der Vereinigten Staaten aufteilen. Durch die Weiterbildung von Sprachmodellen auf diese partizipalen Korpora können wir sehen, wie ihre ideologischen Koordinaten sich ändern. Zum Beispiel zeigt RoBERTa, das weiter mit der linken Redditor-Korpus vorbereitet wurde, einen bedeutenden liberalen Umzug in seinen politischen Biase. Drittens untersuchen wir, ob Sprachmodelle die Polarisation in unserer Gesellschaft aufnehmen können. Wir teilen die Vorbildungsdaten in zwei Zeitabschnitte vor und nach dem 45. Präsidenten der Vereinigten Staaten auf und separate vorbereiten Sprachmodelle auf diese temporalen Korpora. Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Neigung haben, die weiter weg vom Zentrum liegt nach 2017. Dies deutet darauf hin, dass Sprachmodelle die Polarisation in unserer Gesellschaft auch aufnehmen können. Schließlich untersuchen wir, wie Sprachmodelle mit verschiedenen politischen Neigungen bei der Identifizierung von Hassreden und Falschinformationen im NLP-Verfahren leisten. Wir sehen, dass wenn wir die Leistung in einzelnen Kategorien untersuchen - also wenn wir die Leistung für verschiedene Demografien oder politische Neigungen von Nachrichtenmedien untersuchen - eine Musterentwicklung entsteht. Zum Beispiel sind linkslibere Sprachmodelle besser in der Identifizierung von Hassreden gegen kleinere Minderheiten, aber schlecht in der Identifizierung von Hassreden gegen mehr mächtige Gruppen in unserer Gesellschaft. Und umgekehrt ist es für rechtslibere Sprachmodelle so, dass sie besser in der Identifizierung von Hassreden gegen weiße Männer und Männer sind, aber schlecht in der Identifizierung von Hassreden gegen schwarze LGBTQ+-Personen und andere Minderheiten. Es gibt viele weitere Beispiele im Anhang, die diese Tatsache hervorheben, dass Sprachmodelle mit verschiedenen politischen Neigungen unterschiedliche Vorhersagen für Hassreden und Falschinformationen geben, je nachdem, welcher Gruppe sie angreift. Dies hat uns dazu geführt, die Bedeutung von Fairnessproblemen bei der politischen Neigung von Sprachmodellen zu erkennen. Es gibt eine einzigartige Dilemma bei der Untersuchung der politischen Neigung von Sprachmodellen. Es ist wie zwischen Skylla und Charybdis. Wenn wir die politischen Meinungen in den Vorbildungsdaten nicht sorgen, werden die Biase von der Vorbildungsdaten zu Sprachmodellen bis hin zu Downstream-Tasken verbreitet, was zu Fairness-Problemen führen könnte. Wenn wir versuchen, diese Biase zu sorgen, könnten wir auch Risiken von Zensur oder Exklusion aufrechterhalten. Es ist sehr schwierig zu bestimmen, was tatsächlich neutral ist und beibehalten werden sollte. Es ist wie das Elektroelektrische Problem. Okay, das war's für heute. Vielen Dank für eure Zeit.</sample>
    <sample id="48">Es sind mehrere Autoren an der Arbeit beteiligt, aber ich kann nur den Namen eines Mentors nennen: David Vilar.</sample>
    <sample id="49">MPP-Auswertungen wurden bis zu 1024 Token Kontextlänge durchgeführt.</sample>
    <sample id="50">DEPLAIN ist ein neuer Korpus für die Identifizierung von deutscher Text auf dem Dokumenten- und Satellenebene. Der Korpus besteht aus zwei Subcorpora: DEPLAIN-apa, basierend auf Nachrichtenartikeln, und DEPLAIN-web, mit verschiedenen Domänen. Beide Subcorpora wurden manuell und automatisch aligniert. Der Korpus bietet eine breite Palette von Simplifikationsveränderungen, wie Lexikalveränderungen, Strukturveränderungen und Gesamt-Simplifikation. Die Daten können für die Bewertung von automatischen Alignmentmethoden und die Automatic Text-Simplification verwendet werden.</sample>
    <sample id="51">Music, Books, and Recipes</sample>
    <sample id="52">Positionality kann als die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben, definiert werden. Diese Perspektiven können das Forschungsverfahren und seine Ergebnisse beeinflussen, da sie Entscheidungen beeinflussen können. Positionalität wird in der Forschung oft in feministischen und queeren Akademischen Räumen verwendet.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Das Team von Vasudha und ihren Kollegen hat eine akzeptierte Arbeit für ACL 2023 vorgelegt, die "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" heißt. Sie untersuchen kognitive Diskharmonie, eine häufige Phänomen in der Entscheidungsfindung, und untersuchten sie ihre Auswirkungen auf menschliche Gedankenprozesse und Gesundheit. Ihre Forschung konzentriert sich auf die Erstellung eines Ressourcenbanks für kognitive Diskharmonie durch die Annotation von Diskurspaaren. Sie verwenden eine Kombination von Transfer-Learning-Methoden und Aktivierungsverfahren, um die Annotationskosten zu reduzieren und die Erkennung von kognitiver Diskharmonie zu verbessern.</sample>
    <sample id="55">Yes, EDAtt can be applied to an existing offline ST model.</sample>
    <sample id="56">Es sind zwei Autoren an der Arbeit beteiligt: Yusen Zhang und seine Kollegin.</sample>
    <sample id="57">Ja, das getestete Modell funktioniert in der Testsuite.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind: 1. Background-Pretrain, 2. Background-Both und 3. Background-Inference.</sample>
    <sample id="59">Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn't have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT.</sample>
    <sample id="60">Die Autoren gehören an der Universität Paris Saclay.</sample>
    <sample id="61">Die Schlußfolgerung der Forschungsfrage lautet: "Sollten wir nur die sauberen Samples für die Validierung verwenden, oder gibt es bessere Wege, sie zu nutzen?"</sample>
    <sample id="62">Das Systematische Studium von Kenndistillation für natürliche Sprachgenerierung mit Pseudotarget-Training untersucht die Kompression von NLP-Modellen. Die Forschung untersucht verschiedene NLP-Aufgaben in realistischen Szenarien, wie Zusammenfassung, Frage-Generation und Stilübergreifung. Das Studium konkluiert, dass die Verwendung mehrerer Pseudotargets und die Verwendung der Joint-Teaching-Technik bei der Kenndistillation verbessert, da sie die Exposition des Modells an Fehler und die Versammlung von Kenntnissen verbessert.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Modellfähigkeit, konsequent dieselbe Ausgabe für dieselbe Aufgabe zu produzieren, unabhängig von leichter Wortausdrucksänderung.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet eine schlechtere Leistung des Modells.</sample>
    <sample id="66">Das Paper "Deep Learning for Mathematical Reasoning" diskutiert die Aufgabe der mathematischen Logik und die Entwicklung von künstlichem Intelligenz (AI) für die Lösung von mathematischen Problemen. Es unterscheidet sich in zwei Kategorien: visuelle Kontexte und tabellarische Kontexte. Die Aufgabe kann als neuro-symbolische Problem formuliert werden, das geometrische Diagramme, Theoreme und Löser verarbeitet. Automated theorem proving ist eine wichtige Linie der Forschung, die die Beweisfindung von mathematischen Behauptungen automatisiert. Neural Network-Architekturen wie die Sequence-to-Tree-Modell wurden vorgestellt, um die mathematische Logik zu modellieren. Pre-trained Language Models (LLMs) haben erhebliche Fortschritte erzielt, aber sie zeigen begrenzte Fähigkeiten bei exakter mathematischer Logik. Eine Lösung könnte die Verwendung diverser Reasoning-Pfade sein, anstatt nur einen einzigen Weg zu generieren.</sample>
    <sample id="67">Die Forschung untersucht die Interferenz in multilingualen Übersetzungssystemen. Sie zeigt, dass die Interferenz aufgrund der Modellgröße und der Trainingsdatensumme auftritt, während andere Faktoren wie Sprachähnlichkeit und Anzahl der Sprachen weniger bedeutend sind. Die Forschung empfiehlt, die Temperatur-Sampling-Technik zu verwenden, um die Interferenz zu reduzieren, und betont die Bedeutung eines kalibrierten Temperatursatzes für eine bessere Leistung.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings den gesamten Kontext eines Satzes, einschließlich der Grammatik und des Semantik.</sample>
    <sample id="69">Typischerweise werden 20 Beispiele pro Klasse benötigt, um eine gute Leistung in der WSL zu erreichen.</sample>
    <sample id="70">Stanford University</sample>
    <sample id="71">Javad Hosseini und seine Team haben ein Dataset namens "AltEntities Corpus" für die Aufgabe von Indirektbezeichnungen für Entity-Selection entwickelt. Das Dataset umfasst 6.000 alternative Fragen über drei Domänen: Musik, Bücher und Rezepte. Es gibt 42.000 Indirektbezeichnungen. Der Hauptfokus liegt auf der Verständnis von Indirekten Bezeichnungen in einer natürlichen Diskussion. Durch das Anzeigen von Kontextinformationen und das Anzeigen von Google-Suchergebnissen oder Wikipedia-Artikeln ermöglicht das Team den Annotatoren, Indirekte Bezeichnungen zu erstellen. Das Dataset wurde mit dem T5 XL-Modell experimentiert, und die Ergebnisse deuten an, dass es schwierig ist, Indirekte Bezeichnungen ohne Kontextinformationen zu verstehen.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, um die Auswirkungen von sozialen und politischen Biases in der AI-Industrie zu erfassen.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">Das Vortrag über "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths" wurde von Xiangqing und seinen beiden Kollägen präsentiert. Dense-ATOMIC ist eine verbesserte Version des ATOMIC-Knowledges, das die fehlenden Beziehungen zwischen den Ereignissen ergänzt. Dense-ATOMIC nutzt eine Relationen-Prediktion-Modellierung und Clusterstrategien zur Infektion der fehlenden Beziehungen. Das Vortrag war anhand von Statistiken und Beispielen illustriert, und es wurden die Vorteile dieser Methode gegenüber anderen Methoden diskutiert.</sample>
    <sample id="75">The presentation introduces Jointprop, a joint semi-supervised learning framework for named entity recognition (NER) and relation extraction (RE). The motivation behind the work is to address the challenges of fully-supervised models requiring extensive label data annotation. Semi-supervised approaches have shown promise in reducing this requirement while maintaining performance.

The proposed method integrates NER and RE tasks by propagating labels across heterogeneous graphs that consider inter- and intra-connections among labeled and unlabeled data. This approach aims to exploit all available information effectively.

Key components include span feature generation using contextualized representations, heterogeneous graph construction with nearest neighbor algorithms, joint label propagation through the graph, and model optimization via retraining on combined confidence scores from both labeled and pseudo-labeled data.

Experimental results demonstrate significant improvements over baseline methods when applying the joint learning strategy to datasets containing multiple tasks or focusing solely on one task at a time. These findings highlight the benefits of leveraging codependency between different tasks within the same dataset.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile in Sprachmodellen umfasst die Analyse der politischen Neigung von Sprachmodellen, die Rolle des Trainingsdatensatzes bei der Ausbildung dieser Modellvarianten und die Auswirkungen dieser Vorurteile auf die Leistung von Sprachmodellen in verschiedenen Aufgaben.</sample>
    <sample id="77">Das Video präsentiert eine Forschungsarbeit von Yale University und Microsoft Research, die sich auf die Verbesserung der Faktizitätskonsistenz bei Textsummarisierung konzentriert. Die Forscher haben ein Dataset namens DeFacto erstellt, das menschliche Demonstrationen und Feedback für die Verbesserung der Faktizitätskonsistenz enthält. Das Dataset umfasst mehrere Analyse- und Erkenntnisse, darunter die Einführung von drei neuen NLG-Aufgaben: Textsummarisierungsbearbeitung, Feedback-Generierung und automatische Faktizitätsfehlerkorrektur. Im Fokus liegt die Studie der Faktizitätskonsistenz bei der abstrakten Textsummarisierung, wobei der Schwerpunkt auf die System-generated Summarizations der bestehenden Modelle liegt. Der Forschungsvortrag bietet eine detaillierte Analyse des DeFacto-Datasets, einschließlich der Datenstatistiken und der Beispiele für die Annotierten Datensätze.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich zwischen DEplain-apa und Web. Im DEplain-apa-Korpus sind mehrere Simplifizierungsverfahren wie Reordnung und Worthinzufügung vorkommend, während im DEplain-web-Korpus mehrere Repräsentationen vorkommen.</sample>
    <sample id="79">Ja, CoScript ist öffentlich verfügbar.</sample>
    <sample id="80">Das Wasserzeichen wird in den Text eingebettet, indem man eine Zählung der Wörter im Text durchführt und ein Gewichtsummary des Target embeddings zu einem gewerblichen Embedding berechnet. Wenn die Anzahl der Wörter im Text größer als eine bestimmte Grenze ist, wird das Target embedding als Teil des gewerblichen Embeddings verwendet.</sample>
    <sample id="81">Die Autoren gehören der Penn State University an.</sample>
    <sample id="82">Das Video präsentiert eine Forschungsarbeit über die Automatische Essay Bewertung (AES) ohne vorherige Label. AES-Modelle werden typischerweise mit großen, mit Labels versehenen Korpora trainiert, aber dies ist zeitaufwendig und laborintensiv. Unsupervised AES ermöglicht es, die Aufgabe ohne Label zu lösen. Zwei Arbeiten sind vorgelegt: Eine verwendet ein Heuristikales Qualitätssignal für die Anfangsbewertung von Essays, während die andere direkt auf die Wortsprache als Weak-Supervision zurückgreift. Beide Arbeiten zeigen, dass ein einzelnes Heuristikales Signal nicht die Qualität eines Essays umfassen kann. Daher wurde eine neue Ansatzweise vorgestellt, die mehrere Heuristische Qualitäts-Signale als Pseudogroundtruth verwendet und eine neuralen AES-Modell trainiert. Das System konvertiert die Heuristischen Signals in eine Rangliste und trainiert das Modell mithilfe einer Aggregationsverfahren. Die Ergebnisse zeigen eine bessere Leistung im Vergleich zu den traditionellen Ansätzen.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mt5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="84">Shwai He präsentiert seine Arbeit "PAD-Net: Ein effizientes Framework für dynamische Netzwerke" für ACL 2023. Er beginnt mit einem Überblick über dynamische Netzwerke, die sich aufgrund des Inputs anpassen können, im Gegensatz zu statischen Netzwerken. Shwai He betont, dass dynamische Netzwerke in der Regel mehr Parameter haben und sind daher weniger effizient. Er hat eine Hypothese, dass dynamische Netzwerke eine Teilbarkeit in dynamische und statische Parameter haben, was sie effizienter macht. Sein Framework PAD-Net (Partially Dynamic Network) teilt die Parameter zwischen dynamische und statische Parameter und verwendet Iterative Mode Partition, um die dynamischen Parameter zu optimieren. Durch das Reduzieren von dynamischen Parametern kann die Leistung verbessert werden, ohne dass die Darstellungskraft des Netzwerks beeinträchtigt wird. Shwai He zeigt Ergebnisse, die zeigen, dass PAD-Net besser als statische und dynamische Netzwerke im Vergleich zu vollständig dynamischen Netzwerken im Hinblick auf Parameter und Rechenleistung ist. Er diskutiert auch die Bedeutung der Skalierungsfaktoren für dynamische und statische Parameter und zeigt, dass die Verwendung von statischen Parametern bei der Pruning besser ist. Schließlich schlägt er vor, die Methode auf andere Netzwerke auszubauen und es auf Hardwarefreundliche Strukturen zu verankern.</sample>
    <sample id="85">Beispiel für eingeschränkte Sprachplanung: "make a chocolate cake"</sample>
    <sample id="86">Die Autoren stellen die Opazität ihrer Methode sicher, indem sie ein visuelles Beispiel zeigen, um zu demonstrieren, dass es schwierig ist, zwischen den Backdoor-Embedding und den normalen Embedding zu unterscheiden.</sample>
    <sample id="87">Die Arbeit verwendet bestehende PLMs, um ein neues PLM aufzubauen.</sample>
    <sample id="88">GPT-4 ist am wenigsten ausgerichtet auf Länder mit einer confukianen oder englischen Sprache.</sample>
    <sample id="89">Ein Satz, der zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde, lautet: "If we receive a speech chunk containing 'I'm going to talk about...' and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk."</sample>
    <sample id="90">Das Paper "Rethinking Annotation: Can Language Learners Contribute?" untersucht die Möglichkeit, Sprachlernern als Annotatoren für NLP-Daten zu nutzen. Durch eine Studie mit Sprachlernern und natürlichen Sprechern wurde gezeigt, dass Sprachlernern labels fast genauso genau wie natürlichen Sprechern zugeordnet werden können. Diese Methode könnte die Barrieren für die Erstellung von Benchmark-Daten für Sprachen mit geringer Ressourcen reduzieren.</sample>
    <sample id="91">Die Anzahl der Aufgaben erhöht die Leistung des Modells.</sample>
    <sample id="92">Die Autoren vergleichen ihre Methode mit drei Baumlosen Baselines:</sample>
    <sample id="93">Ivan Titov und Alexander Koller sind die Advisors von Matthias Lindemann.</sample>
    <sample id="94">Jingwei Yi von der University of Science and Technology of China präsentiert ihr Forschungsarbeit über die Schutzung der Urheberrechte bei Embedding als Dienstleistung durch Backdoor-Wasserzeichen. Sie diskutieren das Problem, dass Embedding als Dienstleistung von Anbietern stehlen kann, indem sie den Modellzugang ausnutzen. Um diese Problematik zu lösen, haben sie eine Methode namens "Embedding marker" entwickelt, die ein Backdoor-Wasserzeichen in die Providerdienste einfügt und es ermöglicht, zu überprüfen, ob ein Stealer-Service das Wasserzeichen enthält. Ihre Methode besteht aus zwei Schritten: Wasserzeicheninjektion und Urheberrechtsverifizierung. Sie verwenden ein Triggerset von Wörtern mit moderater Frequenz und konstruieren ein Backdoor- und ein Benutzerdatensatz. Durch die Berechnung der Kosinussimilarity und des L2-Similarities zwischen den Embeddings können sie feststellen, ob ein Stealer-Service das Wasserzeichen enthält. Ihre Experimente zeigen, dass ihre Methode eine hohe Trennungsleistung auf vier Datensätzen aufrechterhält, während sie gleichzeitig für die Verwendung in NLP-Aufgaben geeignet ist.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="96">Hi alle. Ich bin Jenny, ein Promovierender an Carnegie Mellon University und heute präsentiere ich das Werk "NLPositionality" zur Charakterisierung von Entwurfsschwachstellen von Datensätzen und Modellen zusammen mit Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap aus der Universität Washington und dem Allen Institute for AI. Das Werk wurde in Zusammenarbeit mit diesen Kollegen erstellt. Lass uns losgehen Vorstelle dir, du arbeitest für eine Zeitung und durchsuchst Kommentare zu deinem Nachrichtenartikel nach Toxizität. Du möchtest einen beliebten API wie Prospective API für die Toxizitätserkennung verwenden, und das funktioniert sehr gut, wenn du Carl Jones bist. Weil Prospective API die Toxizität besser erkennen kann, wenn es sich um indische Kontexte handelt. Dies ist ein Beispiel für eine Designschwäche, bei der wir systematicer Leistungsunterschied zwischen Populationsgruppen beobachten. Designschwächen wie die, die wir gerade gesehen haben, können aufgrund der Positionalität der NLP-Forscher und Modellentwickler auftritt. Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Diese Konzeption wird in den kritischen Studien, insbesondere in Feministischen und Queer-Akademischen Räumen, weit verbreitet verwendet. Als Forscher kann Positionalität das Forschungsverfahren und seine Ergebnisse beeinflussen, da sie Entscheidungen beeinflusst. Einige Fragen, die man sich stellen könnte, sind: Haben Datensätze und Modelle Positionalität? Wir wollen nicht sagen, dass Modelle oder Datensätze selbst identische Identitäten oder Lebenserfahrungen haben, sondern dass sie die Urteile und Meinungen echter Menschen aggregieren und so bestimmte Positionalitäten repräsentieren können. Vorherige Arbeiten haben einige anekdotale Beweise für Positionalität gezeigt, wie z.B. kulturelle Abstände zwischen Datensätzen und Modellen, sowie theoretische Definitionen von Modellpositionalität. Allerdings untersuchen diese Arbeiten nicht die Vergleichung von Endbenutzern mit Datensätzen und Modellen, und die Untersuchung von Positionalität von Datensätzen und Modellen ist immer noch wichtig, da NLP-Aufgaben immer subjektiver und sozialer Natur werden und es schwierig ist, zu bestimmen, wie diese Positionalitäten ungleichmäßig ausgeglichen sind, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs versteckt sind. Um die Positionalität von Datensätzen und Modellen zu untersuchen, vergleichen wir die Annotations mit realen Benutzern mit vorhandenen Datensätzen und Modellen. Wir machen dies durch unser Framework NLPositionality. Das Framework arbeitet in zwei Hauptschritten. Der erste Schritt besteht darin, Datenmengen mit diversen Annotatoren zu neu-annotieren. Wir sollten dies tun, um die Demographie der ursprünglichen Datenannotatoren zu überblicken, weil typischerweise nur wenige Annotatoren jede Instanz annotationieren und die Demographie selten gesammelt und geteilt wird. Wir möchten also Datenmengen neu-annotieren, um viele Annotatoren pro Instanz zu erhalten und eine reiche Menge an Demographie zu erhalten. Dann vergleichen wir die Annotations nach Demographie mit den Modellen und Datenmengen mithilfe einer Pearson-R-Korrelationsscore, und so unterscheidet sich unser Framework stark vom Annotator-Disagreeement-Literatur, indem es Endbenutzer mit Modellen und Datenmengen vergleicht, und nicht nur Annotator-Disagreeement oder Modell-Annotator-Distributionen untersucht. Unser Framework wird hauptsächlich durch Lab in the Wild und eine Online-Krowdsourcing-Plattform ermöglicht, die für Forschungsmitarbeitende in der HCI eingesetzt wird. Lab in the Wild ist eine Online-Experimentationsplattform, auf der wir eine Vielzahl von Teilnehmern einladen können. Im Vergleich zu Plattformen wie M Turk, die hauptsächlich Teilnehmer aus den USA oder Indien haben, bietet Lab in the Wild immer noch hochwertige Daten. Wir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist Soziale Akzeptanz, und das funktioniert so: Teilnehmer lesen eine Situation aus dem Social Chemistry-Dataset und schreiben dann, wie akzeptabel sie die Situation finden. Danach können sie ihre Antworten zu einem AI und anderen vergleichen, um sich aufmerksam zu halten. Wir vergleichen dann diese Annotations mit Social Chemistry, Delphi und GPT 4. Wir wiederholen eine ähnliche Aufgabe für den Toxizitäts- und Hasspeech-Detektionsaufgaben, wo Teilnehmer eine Instanz aus dem Dynahate-Dataset lesen und schreiben, ob sie eine Instanz von Hasspeech sind. Wir vergleichen dann diese Annotations mit Dynahate, Perspective API, Rewire API, Hate Roberta und GPT 4. Unser Studium erzielte schließlich über 16.000 Annotations von über 1.000 Teilnehmern aus 87 Ländern. Wir sind besser vorbereitet, um herauszufinden, wer NLP-Datenmengen und Modelle am meisten mit sich übereinstimmen. Wir finden, dass es Positionalität in NLP gibt. Zum Beispiel finden wir, dass Datenmengen und Modelle am meisten mit englischsprachigen Ländern übereinstimmen. Bei der GPT 4-Sozialakzeptanzanalyse finden wir, dass es am meisten mit Confucian und englischsprachigen Ländern übereinstimmt. Wir finden auch weitere Übereinstimmung mit Menschen mit einem Hochschulabschluss. Wenn Modelle und Datenmengen mit bestimmten Populationen übereinstimmen, werden einige sicherlich hinterlassen. Ein Beispiel dafür ist, dass Datenmengen und Modelle weniger mit nicht binary Personen übereinstimmen als mit männlichen und weiblichen Kompagnonen. Wir finden dies in der GPT 4-Sozialakzeptanzanalyse sowie der Dynahate-Analyse. Gegeben, dass es Positionalität in NLP gibt, was können wir dagegen tun? Wir haben einige Empfehlungen dafür. Zunächst sollten wir alle relevanten Entwurfswahlen während des Forschungsprozesses dokumentieren. Der zweite Schritt ist es, NLP-Forschung mit dem Blick auf Perspektivismus zu betreiben. Unser dritter Empfehlung ist es, spezialisierte Datenmengen und Modelle innerhalb vier bestimmter Gemeinschaften zu entwickeln. Eine gute Beispiel dafür ist das Masakhani-Projekt. Wir möchten betonen, dass inklusive NLP nicht nur daran liegt, dass alle Technologien für jeden funktionieren. Und das beendet unsere Präsentation. Wenn Sie mehr erfahren möchten, können Sie sich auf unsere Dashboard-Ansicht für die aktuellsten Analyseergebnisse und unsere Publikation verlassen. Danke.</sample>
    <sample id="97">Die Referentin geht auf drei Probleme von SimulST ein.</sample>
    <sample id="98">So, um soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen effektiv zu reduzieren, müssen mehr Schritte unternommen werden. Zunächst sollten die Datensätze sorgfältig überprüft und kontrolliert werden, um potenzielle Biase zu identifizieren und zu korrigieren. Dies kann durch die Einführung von Standards und Richtlinien für den Datensatzsammelvorgang erreicht werden, einschließlich der Überprüfung auf die Repräsentativheit verschiedener sozialer Gruppen und politischer Meinungen.

Zweitens sollten die Datensätze diversifiziert und ausgewogen sein, um eine breitere Palette von Perspektiven und Meinungen zu erfassen. Dies kann durch das Einbeziehen von mehreren Quellen und Quellen von Datenquelle gewährleistet werden, was dazu beiträgt, dass Biase minimiert und eine umfassendere Darstellung der Wahrheit gewährleistet wird.

Drittens sollten die NLP-Modelle während des Trainings konfiguriert werden, um die Verzerrungen zu erkennen und zu korrigieren. Dies kann durch die Entwicklung von spezifischen Algorithmen und Methoden erreicht werden, die auf die Identifizierung und Korrektur von Biase im Datenmaterial abzielen.

Schließlich sollten die Ergebnisse der NLP-Modelle sorgfältig überprüft und überprüft werden, um sicherzustellen, dass sie fair und neutral sind. Dies kann durch die Durchführung von Tests und Evaluationen, die auf die Identifizierung von Biase und die Überprüfung der Fairness der Modelle abzielen, erreicht werden.

Indem diese Schritte unternommen werden, können wir die Verzerrungen in Datensätzen beim Training von NLP-Modellen effektiv reduzieren und sicherstellen, dass die Modelle fair und neutral sind.</sample>
    <sample id="99">Hey, ich bin Siyu Yuan von der Fudan University. Ich möchte euch das Werk "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" präsentieren. Im Alltag planen Menschen ihre Handlungen oft in Form von Schrittenfolgen für bestimmte Ziele. Vorherige Studien haben gezeigt, dass Sprachmodelle dazu in der Lage sind, Ziele zu dekodieren und zu schreiben. Doch die meisten dieser Studien konzentrierten sich auf allgemeine Ziele wie "Kuchenbacken".</sample>
    <sample id="100">PromptRank ist ein data-effizientes Ansatz für die Multi-hop-Abfrage, der eine unsupervisierte Retrievalmethode mit einem few-shot language model-Reranker kombiniert. Das Hauptziel ist es, eine Pool von kandidaten Chains zu erstellen und diese Chains mithilfe eines few-shot language model-Rerankers zu überprüfen. PromptRank erzielt gute Leistungen mit nur 128 Beispieldaten und bietet eine effiziente Alternative zu traditioneller supervised Retrieval.</sample>
    <sample id="101">PaLM hat eine gute Sprachgewandtheit, wie aus der Fluide der Übersetzungsergebnisse hervorgeht.</sample>
    <sample id="102">Wasserzeichenverfahren sollten die folgenden Eigenschaften haben: 1. Anwendbarkeit auf Embedding als Dienstleistung, 2. Unbeeinträchtigung der Nutzbarkeit der bereitgestellten Embeddings, 3. Versteckung, um es dem Angreifer schwierig zu machen, das Wasserzeichen zu entfernen, und 4. Transferierbarkeit des Wasserzeichens während des Modellextrakts.</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 Sprachen übersetzt.</sample>
    <sample id="104">Für die erneute Annotierung werden 10 Instanzen aus einem Datensatz ausgewählt.</sample>
    <sample id="105">Cosine and L2 similarity.</sample>
    <sample id="106">Das Team von Chaitanya und seinen Kollegen hat eine Dataset-Analyse vorgelegt, die sich auf die Untersuchung von Informationssuche-Methoden konzentriert. Das Dataset heißt QUEST und umfasst mehr als 3.000 Querys, bei denen Querys mit implizitem Set-Operatorn vorkommen. Die Querys sind auf der Grundlage von Wikipedia-Kategorien erstellt worden, und die Querys wurden von menschlichen Annotatoren überprüft, um ihre Relevanz und natürliche Sprache zu gewährleisten. Die Querys wurden dann von anderen Annotatoren überprüft, um die Relevanz der enthaltenen Entitys zu bestätigen. Das Dataset ermöglicht es Forschern, verschiedene Systeme für die Behandlung von Querys mit implizitem Set-Operatorn zu testen und zu verbessern.</sample>
    <sample id="107">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe eingesetzt, um die Bestenleistungen zu erzielen.</sample>
    <sample id="108">Das Team untersucht die Effektivität des Minimalpaars-Paradigms bei der Bewertung von Sprachmodellen auf akzeptablen und akzeptierbaren Sätzen. Sie zeigen, dass das Paradigma nicht immer robust ist, insbesondere bei längeren Kontexten. Durch die Erstellung von Sätzen mit einem größeren Kontextwindow können sie erkennen, dass Sprachmodelle aufgrund von unveränderten Strukturen und -bedeutungen reagieren, die sich in den Kontexten befinden. Dies deutet darauf hin, dass das MPP-Paradigma möglicherweise nicht die volle Abstrakte Kenntnis der Sprachmodelle im Kontext einfangen kann.</sample>
    <sample id="109">Unnatural Instructions ist ein Dataset, das natürliche Spracheaufträge und ihre entsprechenden Eingaben und Ausgaben enthält. Es wurde in einer vollständig automatischen Weise ohne jegliche menschliche Annotationen erfasst. Der Prozess umfasst die Promptierung eines vorher trainierten Sprachmodells mit drei Beispielen aus dem Super-Natural Instructions-Dataset und die Anforderung, eine vielseitige Vielzahl von Aufgaben zu generieren. Das Dataset umfasst 64.000 Beispiele, und wenn man die Paraphrasen berücksichtigt, beträgt es etwa 240.000 Beispiele. Durch die Analyse der generierten Beispiele wurden Aspekte wie Kreativität, Vielfalt und Korrektheit untersucht. Mehr als 50% der generierten Beispiele wurden als korrekt beurteilt, und sogar fehlende Beispiele enthalten wertvolle Informationen für die Anpassung von Sprachmodellen. Unnatural Instructions zeigt die Fähigkeit von Sprachmodellen, kreative und vielfältige Daten zu produzieren, was schwierig zu erreichen ist bei der Verwendung von Menschenarbeit, die meistens in prädikablen Heuristiken und Annotat-Fehlern zusammenklammert.</sample>
    <sample id="111">Die Autoren wählen Wörter mit mittlerer Häufigkeit für die Definition eines Trigger Sets.</sample>
    <sample id="112">Ja, natürlich. Ihre Präsentation hieß "Do CoNLL-2003 named entity taggers still work well in 2023?" Sie haben sich der Frage gestellt, ob die von CoNLL-2003 entwickelten NER-Tagger in modernen Daten immer noch gut funktionieren. Sie haben eine umfangreiche Studie durchgeführt und eine umfangreiche Analyse der Leistung von mehreren Modellen auf den CoNLL-03- und CoNLL++-Testdatenmengen durchgeführt. Ihre Schlussfolgerung lautete, dass die Leistung der CoNLL-2003-Tagger in 2023 immer noch gut ist, und sie betonten die Bedeutung weiterer Forschung zur Verbesserung der allgemeineren Leistung von Modellen.</sample>
    <sample id="114">Das Team aus Nanyang Technological University of Singapore präsentiert eine neue Methode zur Komprimierung von Multi-Head Attention in Large Language Models (LLMs). Ihre Arbeit, "Finding the Pillars of Strength for Multi-Head Attention", zeigt, dass up to 90% der Parameter eines LLMs durch die Gruppierung von Headern und Pruning redundanter Heads komprimiert werden können, ohne dass die Leistung beeinträchtigt wird. Sie verwenden ein Divide-and-Conquer-Strategie, bestehend aus zwei Schritten: Gruppenkonstruktions Training und Voting-to-Stay-Algorithmus. Im Gruppenkonstruktions Training werden die Heads in Gruppen aufgeteilt, wodurch Heads innerhalb einer Gruppe mehr ähnlich und Heads zwischen Gruppen mehr voneinander abweichend sind. Der Voting-to-Stay-Algorithmus prüft die Heads nach einem Scoring-System und eliminiert Heads mit niedrigen Votes. Diese Methode erzielt eine sichtbare Komprimierung der Ryanetrie der LLMs, während sie die Leistung auf den gleichen Niveau behält.</sample>
    <sample id="115">Die Sprachsegmentgröße wird bei dem Ansatz Lambda-Speech-Frames verwendet.</sample>
    <sample id="116">Servin ist ein Richter. Kea ist ein Backer. Servin und Kea trafen sich in einem Park. Nach einem langen Tag, an dem er Entscheidungen über Fälle in einem Gericht trifft, war er froh, sich zu entspannen.</sample>
    <sample id="117">Der wichtigste Faktor zwischen der Qualität des Beispiels und der Ähnlichkeit mit dem Ausgangssatz ist die Qualität des Beispielbeispiels selbst.</sample>
    <sample id="118">Das Team hat bei der Präsentation des Beitrags "Improving Pretraining Techniques for Code-Switched NLP" bei der ACL 2023 eine Definition von Code-Switching gegeben, indem sie ein Beispiel mit englischen und indianischen Worten gezeigt haben. Es wurde darauf hingewiesen, dass Multilingual pre-trained Modelle wie mBERT und XLM-R auf Aufgaben wie die Antwort auf Fragen oder die Bewertung von Gefühlen bei Code-Switching nicht sehr effektiv sind. Das Hauptaugenmerk der Forschung liegt darin, neue Maskierungsmethode (MLM) zu entwickeln, die für Code-Switching angepasst sind, sowie architektonische Änderungen und Hilfsverluste zu motivieren, um den Code-Switch-Informationen mehr zu vermitteln. Der vorgeschlagene Ansatz heißt SwitchMLM, und es wurden verschiedene Methoden zur Erzeugung von SwitchMLM vorgestellt, wie zum Beispiel FrequencyMLM und die Einführung von residualen Verbindungen. Durch die Verwendung linearer und bedingter Proben wurden die Ergebnisse bestätigt, dass die vorgeschlagenen Methoden die Menge an Switch-Point-Information in den Layern erhöhen.</sample>
    <sample id="119">Die Arbeiten konzentrieren sich auf RoBERTa, GPT-4 und verschiedene Varianten von BART.</sample>
    <sample id="120">Das Modell verwendet Werte aus mehreren Ebenen.</sample>
    <sample id="121">Beispiele für direkte Inferenz sind die Beispiele "Easy on Me" und "I Gotta Feeling".</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">Das Team von Ying und Zhiyang präsentiert ihre Forschung über MultiInstruct, das erstmals eine multi-modal Instruction Tuning-Benchmark-Dataset für die Verbesserung der Zero-Shot-Learning-Performanz in verschiedenen Aufgaben präsentiert. Das Dataset besteht aus 62 verschiedenen Aufgaben aus 10 verschiedenen Kategorien, die auf 21 offenen Quellen entstanden sind. Jede Aufgabe wird mit fünf Experten geschriebenen Anweisungen ausgestattet. Der Vorgehen besteht darin, den multi-modalen Pre-Trained-Modell OFA als Basis zu verwenden, das alle Eingaben und Ausgabensätze in einem einheitlichen Tokenraum verarbeitet. Das Team verwendet 53 Aufgaben aus 9 Gruppen für die Training und 10.000 Instanzen pro Aufgabe. Für die Testphase reservieren sie den ganzen Common Sense Reasoning-Gruppe für die Testung und wählen zufällig 5 Aufgaben aus den Gruppen Vision Quality und Miscellaneous. Sie verwenden einen pre-traineden OFA-Large-Model als Basismodell und fügen bei der Ausbildung alle Instanzen für alle Aufgaben hinzu. Bei der Testung präsentiert sich die Modelleignung durch die Verwendung eines der fünf Anweisungsvarianten. Die Ergebnisse zeigen, dass die Verwendung mehrerer Anweisungen die Leistung verbessert und die Sensitivität reduziert. Transfer Learning von natürlichen Anweisungsdatenquellen hilft, die Sensitivität und die Leistung des Modells zu erhöhen.</sample>
    <sample id="124">Das Team von Tan Qingyu und Alibaba hat eine Studie über die temporale Verständnisfähigkeit großer Sprachmodellien vorgelegt. Sie haben die temporale Verständnisfähigkeit in drei Stufen aufgeteilt: Zeit-zu-Zeit, Zeit-zu-Ereignis und Ereignis-zu-Ereignis. Durch eine Analyse der Leistungen von drei LMs - T5-L, FLAN-T5-L und ChatGPT - haben sie festgestellt, dass diese LMs einen starken Vorteil für das Zeitfenster 2000-2020 haben, was möglicherweise auf die Termintäten im Trainingsmaterial zurückzuführen ist. Sie haben daher ein Dataset namens TempReason erstellt, das alle drei Stufen der temporalen Verständnisfähigkeit und eine umfassende Zeitdauer abdeckt. Die Studie zeigt, dass die temporale Verständnisfähigkeit der LMs noch zu verbessern ist, und die Researchers haben eine Trainingsstrategie vorgeschlagen, die sich auf temporale Spanne extrahierung und zeitbezogene Reinforcement-Learning konzentriert.</sample>
    <sample id="125">There are 4 Autoren an der Arbeit beteiligt.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe eines maschinellen Übersetzungsmodells wurde als Baseline betrachtet.</sample>
    <sample id="127">Das Team von Namgyu Ho, Laura Schmid und Se-Young Yun hat eine Forschungsarbeit namens "Large Language Models Are Reasoning Teachers" vorgelegt. Ihre Arbeit zeigt, dass große Sprachmodellien dennoch auf komplexen Aufgaben wie Mathefragen Schwierigkeiten haben, wenn sie mehrere Schritte ausführen müssen. Um diese Problematik zu lösen, haben sie eine Methode entwickelt, die es ermöglicht, große Modelle als Lehrer für kleinere Modelle zu verwenden. Sie zeigen auch, dass ihre Methode mit einer Vielfalt an Reasoning-Szenarien effektiv ist und die Leistung der kleineren Modelle verbessern kann.</sample>
    <sample id="128">Das Team von McGill University, Mila und Microsoft Research präsentiert eine neue Diagnostic-Test-Suite für die Integration von Wissen in natürlichen Sprachen. Der KITMUS-Test prüft die Fähigkeit von Modellen, Wissen aus verschiedenen Quellen zu integrieren, einschließlich vorheriger Präsentation und bei der Ausführung. Der Test umfasst eine Coreference Resolution-Aufgabe, die auf die Fähigkeit abzielt, auf Wissen aus verschiedenen Quellen zu verweisen. Durch die Bewertung des Tests mit menschlicher Untersuchung und bestehenden Coreference Resolution-Modellen wurden die Schwierigkeiten bei der Integration von Wissen aus verschiedenen Quellen hervorgehoben.</sample>
    <sample id="129">Die Autoren haben das Beispiel einer "Asian woman" gegeben, um eine markierte Gruppe zu demonstrieren.</sample>
    <sample id="130">Modellarchitekturen, die nicht gut generalisieren, sind die, die auf CoNLL-2003 trainiert wurden und nicht auf modernen Daten.</sample>
    <sample id="131">Die Testdatensätze sind "clean".</sample>
    <sample id="132">Es sind zwei Autoren an der Arbeit beteiligt: Martin und Akshatha.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten.</sample>
    <sample id="135">ABC-Eval ist ein neuer, dimensioneller Ansatz zur Bewertung von Conversational AI. Dieses Projekt wurde vom Emory NLP Lab von Professor Jinho Choi und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Wenn Sie ein Dialogmodell entwickelt haben und möchten sehen, wie es sich im Vergleich zu den aktuellen Standards aushält, wird die häufige Praxis der menschlichen Bewertung verwendet, wie zum Beispiel wenn menschliche Richter entscheiden, welche Konversation besser ist oder eine Likert-Skala verwenden, um Konversationen zu bewerten. Diese Ansätze sind gut für eine allgemeine Bewertung des Dialogqualitäts, aber sie haben viele Aspekte. Daher möchten Sie verschiedene Dimensionen des Chats bewerten, um die Stärken und Schwächen des Modells auf einem feineren Niveau zu verstehen. Eine Möglichkeit ist es, menschliche Richter zu bitten, verschiedene Dimensionen des Chatsbewertungs zu bewerten, wie zum Beispiel die Relevanz der Modulrufe, mithilfe von Vergleichs- oder Likert-Skala Methoden. Wir glauben jedoch, dass es eine genauerere und mehr fundierte Strategie gibt, um den Chats qualitativ zu bewerten. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem wir spezifische Verhaltensweisen im Chat anzeichnen, wie zum Beispiel das Verbleiben von Irrelevantinformationen oder Widerspruch. Wir nennen dies ABC-Eval. Wir haben diese Methode entwickelt, um eine umfassende Überlappung der vermillten Verhaltensweisen zu erstellen, die in jüngster Literatur als beeinflussende Faktoren für den Chats qualitativ betrachtet wurden. ABC-Eval kann die Anzahl der Chats, die in verschiedenen Aspekten irrtümlich sind, messen. Zum Beispiel misst er die Anzahl der Runden, in denen der Chat-Modell ignoriert, was seine Partei sagt, oder etwas Irrelevantes sagt, Widersprich oder seine Partei oder selbst widersteht, falsche Fakten behauptet oder gegen allgemeine Wissen verstoßen, und ob das Modell Erfolg hat oder scheitert, empathy zu zeigen. Um zu bestimmen, welche Bewertungsstrategie am effektivsten ist, haben wir vier state-of-the-art Chat-Modelle auf 100 mensch-bot-Conversations pro Modell mit ABC-Eval-evaluated. Für Vergleichswirkung haben wir auch diese Conversations mit drei vorhandenen Methoden bewertet: Likert-Skala auf der Runden Ebene, Likert-Skala auf der Dialog Ebene und Dialog-Level-Pairwise-Comparisons. Für jeden der vorhandenen Methoden haben wir Evaluationsergebnisse für acht häufig bewerteten Aspekten der Dialoge sammeln müssen, da dies die Standardpraxis für die Bewertung von Chat-Modellen ist. Durch unsere Analyse dieser Bewertungsresultate haben wir festgestellt, dass die ABC-Eval-Behavior-Labels im Allgemeinen stärker als Labeln, die von den vorhandenen Methoden erfasst wurden, sind, wie die Interannotatorenvereinbarkeit auf 100 zweideutig beschrifteten Conversations zeigt. Darüber hinaus sind die ABC-Eval-Labels im Vergleich zu Metriken, die von den vorhandenen Methoden produziert wurden, besser vorhersagbar für die Gesamtkonversationqualität, wie durch eine einfache lineare Regression-Analyse gezeigt wird. Zum Beispiel können Sie sehen, wie die Proportion der Runden mit Selbst- und Partner-Widersprüchen die Qualität erklären kann, 5% und 10% jeweils, während die durchschnittlichen Likert-Konsistenz-Scores nur 4% oder weniger erklären. Zudem haben wir geprüft, ob jeder Bewertungsmerkmal ein einzigartiges Aspekt der Chatsqualität abdeckt, indem wir einen Schrittweisen linearen Regression-Analyse durchführten. Sie können sehen, dass die Kombination aller ABC-Eval-Merkmale über 25% der Qualität erklärt, und wenn Sie jedes Merkmal entfernen, verlieren sie einen bedeutenden Teil der Informationen über die Qualität. Im Gegensatz dazu erklären die Kombination aller Turn-Level-Likert-Merkmale viel weniger Qualität, und weniger Merkmale tragen ein einzigartiges Aspekt der Chatsqualität ab. Diese zuverlässigen, informellen und einzigartigen Merkmale der ABC-Eval ermöglichen es uns, Conversational AI mit einer höheren Auflösung zu bewerten als die vorherigen Methoden. Sie können sehen, dass einige Herausforderungen immer noch bestehen und genau gemessen wurden. Zum Beispiel haben die Bots, die wir getestet haben, in etwa 20% ihrer Antworten scheinbar vernünftige Verstöße. Sie produzieren in etwa 15% ihrer Antworten Irrelevanzinformation, und sie widersprechen sich oder ihre Partei in etwa 10% der Fälle. Mit dem schnellen Fortschritt im Bereich können diese Fehlerwerte in neuen Modellen seit unserer Bewertung möglicherweise fallen. Allerdings ist dies ein weiterer Grund, um sichreiche und genaue Bewertungsmerkmale für die Vergleich von Modellen zu suchen. Wir hoffen, dass ABC-Eval für andere im Bereich als eine bedeutende Schritt in diesem Richtung genutzt werden kann. Und wir freuen uns darauf, wie der Conversational AI im kommenden Monaten und Jahren weiterentwickelt wird.</sample>
    <sample id="136">Jasivan präsentiert eine Forschungsarbeit mit Nafise über "FERMAT: Eine Alternative zur Genauigkeit für numerische Logik" an der Universität Sheffield. Das Hauptmotiv der Arbeit ist die Analyse der Leistung von Sprachmodellen bei der numerischen Logik, insbesondere bei der Faktkontrolle und der Infotabs-Task. Die Forschung zeigt, dass große Sprachmodelle besser im Vergleich zu kleinen Modellen in der numerischen Logik leisten, jedoch sind viele Modelle bei der Genauigkeit bei der Faktkontrolle oder der Infotabs-Task schlecht. Der Forschungsvorhaben FERMAT wurde entwickelt, um eine flexible Evaluierungssatzwerk zu erstellen, das auf arithmetischen Typen basiert und verschiedene Aspekte wie Zahlverständnis, mathematische Operationen und Trainingsabhängigkeit abdeckt. Durch eine Vielzahl von Questions und - templates wurde die Leistung der Modelle verbessert. Das Ergebnis zeigt, dass Sprach- und Mathematikdiversität sowie die Verbesserung der Zahlencodeinheit und des Tokenizations die Leistung der Modelle verbessern können.</sample>
    <sample id="137">Das Team von der Singapore University of Technology and Design hat Tell2Design, ein Dataset für Sprachleitungs-Flachplangenerierung, veröffentlicht, das in ACL 2023 präsentiert wurde. Das Dataset ermöglicht es Benutzern ohne Fachwissen, indem sie Anweisungen in Text formieren, um eine Flachplanerstellung durch Expertenarchitekten durchzuführen. Das Hauptziel ist es, den Designprozess zu verbessern, indem Benutzer ihre Anforderungen und Bedingungen in Textform beschreiben, und Architekten dann eine geeignete Flachplanerstellung entwickeln. Die Forschungsetappe setzt sich auf die Erstellung eines sequenzbezogenen Modells für die Flachplanerstellung, das von menschlichen Anweisungen auf Basis von natürlicher Sprache trainiert wird. Das Ziel ist es, eine Flachplanerstellung zu generieren, die genau den bereitgestellten Anweisungen entspricht.</sample>
    <sample id="138">NATURAL LANGUAGE UNDERSTANDING</sample>
    <sample id="139">Ying und Zhiyang</sample>
    <sample id="140">Ja, Coscript hat eine Qualitätskontrolle durchlaufen.</sample>
    <sample id="141">Ressourcen für kontextbasierte Übersetzung sind begrenzt, weil sie meistens auf menschliche Curatierung und Domänenwissen abzielen. Diese Ressourcen unterstützen nur begrenzte Arten von Kontextabhängigkeit und eine begrenzte Menge an Sprachen.</sample>
    <sample id="142">Hey Ich möchte euch über unsere Arbeit "Resolving Indirect Referring Expressions for Entity Selection" erzählen, in der wir das AltEntities Corpus einführen. Meine Name ist Javad Hosseini und ich arbeitete mit Filip Radlinski, Silvia Pareti und Annie Louis an dieser Arbeit. Unser Ziel ist es, zu verstehen, wie Benutzer Sprache verwenden, wenn sie eine Wahl treffen möchten.

Lassst du mal daran denken: "Hast du das Song 'Easy on Me' oder 'I Gotta Feeling' im Kopf?" Hier geht es darum, einen von diesen beiden Liedern auszuwählen. Das offensichtlichste ist, einen direkten Bezug zu verwenden, zum Beispiel den Namen des Liedes "Easy on Me" oder seine Position, "das erste". Aber manchmal sind indirecte Beziehungen mehr natürlich und effektiv. Dies kann passieren, wenn der Benutzer nicht den Namen des Liedes erinnern kann, wenn die Sprechungen zu ähnlich sind und es schwierig ist, zwischen ihnen zu unterscheiden, oder wenn der Benutzer eine Vorliebe angibt. Hier sind einige Beispiele für indirecte Beziehungen: "Der neue", "der Lied, das nicht energiegeladen ist" oder "der, der nicht so heller ist".

Dies ist ein wichtiger Problem in Gesprächssystemen und auch für die Auswertung von LLMs (Language Models) bei der Verständnis von Entitäten. Wir wissen nicht von einem größeren öffentlichen Datenbanken für diese Aufgabe, daher haben wir eine mithilfe von Crowd-Annotation sammelt. Unser Datenbank umfasst drei verschiedene Bereiche: Musik, Bücher und Rezepte. Unser Datenbank-Sammlungsverfahren betont die Informalität durch eine Cartoon-Fertigung-Anordnung. Die Cartoon hat drei Sprachblasen. In der ersten Blase sagt Bob: "Erinnere dir an das Song, den wir gestern gehört haben?" Und damit setzt er das Dialogümfeld in Bewegung. In der zweiten Blase sagt Alice: "Hast du das Song 'Easy on Me' oder 'I Gotta Feeling' im Kopf?" Was eine alternative Frage ist. Und in der dritten Blase verwendet Bob einen indirecten Bezug, um eines dieser Entitäten auszuwählen, zum Beispiel "Der neue". Wir geben die ersten beiden Blase automatisch, aber die dritte Blase wird von dem Annotator gefüllt. Die erste Blase wird aus ein paar Handvorgaben pro Bereich gewählt. Die zweite Blase, die alternative Frage, wird wie folgt generiert: Wir verwenden immer eine einfache Musterweise. Sagst du, du bedeutest A oder B? Wo A und B Beispiele von Wikipedia sind. Hier sind die verschiedenen Muster, die wir verwendet haben: Wenn wir höher auf der Liste gehen, werden die Entitäten ähnlicher zueinander, und es ist üblicherweise schwieriger, die Disambiguierung zu machen. Der erste ist ein zufälliger Zufall. Der zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher namens "Die Rückkehr". Der dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Info-Boxen oder Attribute auf Wikipedia haben, zum Beispiel denselben Genre oder denselben Künstler für eine Lied. Wenn wir diese alternative Frage den Annotatoren zeigen, wissen sie den Namen dieser Entitäten, aber sie wissen nicht notwendigerweise die Entitäten. Was wir tun, ist, einige Hintergrundwissen über diese Entitäten zu zeigen. Für Lieder zeigen wir einfach einen Google-Suchlink zu jedem Lied und bitten die Annotatoren, mindestens einige davon zu hören und über jedes Lied zu lesen. Hier ist zum Beispiel der Google-Suchergebnis für das Lied "Easy on Me". Für Rezepte und Bücher zeigen wir einige Hintergrundtexte von Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, wiederum von Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen und sie mit drei bis fünf indirecten Beziehungen zu beschreiben. Hier sind einige Beispiele aus unserem Datenbank: "Der, ohne Worte", "nicht der, der einen 12-jährigen Jungen hat", "der fiktive", "kommt aus Aserbaidschan", und so weiter. Das AltEntities Corpus hat 6.000 alternative Fragen über drei Bereiche und 42.000 indirecte Beziehungen. Ergebnisse mit dem T5 XL-Modell sind wie folgt zusammengefasst: Wenn der Sprachmodellzugang zu genauem gleichen Hintergrundwissen wie die Annotatoren hat, erreicht die Genauigkeit etwa 92 bis 95%. Aber das ist nicht realistisch. Wenn der Sprachmodellzugang nur zu teilweise überschneitendem Hintergrundwissen hat, erreicht die Genauigkeit zwischen 82 bis 87%, was realistischer ist. Wenn der Sprachmodellzugang nur zu Entitätennamen hat, erreicht die Genauigkeit nur 60%, also gibt es noch viel Raum für Verbesserung. Wir haben auch gezeigt, dass die Modelle domänengeneralisierbar sind. Hier ist der Link zu unserem Datenbank. Danke</sample>
    <sample id="143">Der Ansatz wird mit den Richtlinien "Wait-k", "Local Agreement" und der "state-of-the-art"-Architektur für gleichzeitige Vorbetranslation verglichen.</sample>
    <sample id="144">Die Autoren gehören an der Université de Nantes.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">The presentation discusses the problem of omission in dialogue summarization, a subtask of text summarization. Despite recent advancements using large-scale pretrained language models, these summaries often contain factual errors due to omission issues. The study analyzes the percentage of summaries with omission problems across five domains and six pre-trained models, revealing that even state-of-the-art models have high omission rates (around 70%). Omission information is randomly distributed within dialogues regardless of their length or domain, indicating unstructured data.

To address this issue, an omission detection task was defined: identifying omitted utterances from candidate summaries by focusing on individual utterances rather than entire dialogues. However, no existing datasets support such analysis; hence, the OLDS dataset was created based on five benchmarks covering different domains. This dataset provides diverse candidate summaries generated by various abstractive models for each dialogue.

The performance evaluation shows challenges in omission detection tasks, with F1-scores around 50% despite label imbalance issues. To improve summary quality, post-editing methods were explored where omission content is concatenated with candidate summaries as input. Results indicate significant improvements when omissions are included, suggesting that omission detection can enhance summary refinement efforts.</sample>
    <sample id="147">Es sind drei Autoren an der Arbeit beteiligt: Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">Natürlich Hier ist die Übersetzung des englischen Textes ins Deutsche:

"Hallo, ich bin Sara Papi von der Universität Bozen und der Fondazione Bruno Kessler und möchte kurz das Papier 'Attention as a Guide for Simultaneous Speech Translation' vorstellen, in dem wir zusammen mit Matteo Negri und Marco Turchi arbeiten. Was ist Simultaneous Speech Translation? Simultaneous Speech Translation, oder SimulST, ist das Prozess, eine Sprache in einem anderen Text in Echtzeit zu übersetzen, um eine überlappende Kommunikation zu ermöglichen. Und welche Probleme haben der aktuellen SimulST-Modelle? Spezielle Architekturen werden üblicherweise trainiert, indem zusätzliche Module für die Optimierung hinzugefügt werden. Langsame und komplexe Trainingsverfahren, zum Beispiel Trainingsprozesse mit verschiedenen Optimierungszielen. Und das Training und das Erhalten mehrerer Modelle, um verschiedene Latenz-Regime zu erreichen. Zum Beispiel das Trainieren eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden Latenz und so weiter. Was ist unsere Lösung? Zuerst verwenden wir bereits vorhandene offline-Übersetzungsmodell ohne erneutes Training oder spezielle Architektur für SimulST. Verwenden nur ein Modell für jeden Latenz-Regime und verwalten die Latenz durch bestimmte Parameter. Und nutzen das Wissen, das bereits durch den Aufmerksamkeitsmechanismus zwischen Audiodaten und Textproduktion erlangt wurde. Das ist der Kreuz-Aufmerksamkeitsmechanismus, und Sie können ein Beispiel auf der rechten Seite sehen. Unser Vorschlag ist, EDAtt, oder Encoder-Decoder-Aufmerksamkeit, und es handelt sich um eine Strategie, bei der wir entscheiden, ob wir einen teilweise übersetzten Text oder nicht ausgeben, basierend auf, wo die Aufmerksamkeit aufgerichtet ist. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, das heißt, wenn die Summe der Kreuz-Aufmerksamkeiten innerhalb eines bestimmten Lambda-Speech-Frames unter einem bestimmten Schwellwert alpha liegt, was bedeutet, dass die empfangene Information stabil genug ist. Zum Beispiel erhalten wir einen Sprachabschnitt mit "Ich werde sprechen über..." und unsere Modell präsentiert eine Übersetzung auf Deutsch, und wenn wir die Kreuz-Aufmerksamkeitswerte betrachten, sehen wir, dass das erste Wort auf den frühesten empfangenen Sprach-Frames zeigt, während das letzte Wort auf die letzten empfangenen Sprach-Frames zeigt, was bedeutet, dass das erste Wort ausgegeben wird, während die Summe der Kreuz-Aufmerksamkeiten über einem bestimmten Schwellwert alpha liegt, werden wir das letzte Wort nicht ausgeben und warten auf einen anderen Sprachabschnitt. Wenn wir weitergehen und einen neuen Sprachabschnitt erhalten und das Modell drei weitere Wörter präsentiert, und wenn wir die Kreuz-Aufmerksamkeitswerte betrachten, sehen wir, dass kein Wort auf die letzten Lambda-Speech-Frames zeigt. Das bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptergebnisse von EDAtt ansehen, werden wir Graphen zeichnen, in denen wir die Übersetzungskualität im Y-Achsenmesswert BLEU messen und den Durchschnittsverzögerungsbereich, der die Latenzmessung ist, und wir berücksichtigen auch den Rechnerbewussten Durchschnittsverzögerungsbereich, der die Rechnungsgesamtleistung des Modells berücksichtigt, um die Vorschläge zu präsentieren. Wir möchten, dass unsere Kurven so hoch wie möglich auf diesem Plot sind. Aber wir möchten auch, dass sie auf der linken Seite verschiebt sind. Wir vergleichen unsere Ergebnisse mit beliebten Strategien, die auch für offline-Modelle angewendet werden, wie zum Beispiel der Wait-k-Strategie und der Lokalen Vereinbarkeit. Und wir vergleichen auch mit der zuletzt bekannten Architektur, die speziell für die gleichzeitige Vorübersetzung angepasst wurde. Diese sind alle Ergebnisse der gleichzeitigen Übersetzungsfähigkeit auf Deutsch. Und wir sehen, dass sie alle Strategiespiegel überbieten, die für offline-Modelle angewendet werden, da die Kurven auf der linken Seite verschiebt sind. Und wenn Sie mehr Ergebnisse erfahren möchten, lesen Sie unser Papier. Und wir haben auch den Code und die Modelle freigegeben, um die Wiedergabe unserer Arbeit zu erleichtern. Danke für Ihre Aufmerksamkeit."</sample>
    <sample id="149">Ja, der CoNLL++ Dataset ist öffentlich zugänglich.</sample>
    <sample id="150">Archiki präsentiert die ACL-Papier "MEETINGQA: Extractive Question-Answering on Meeting Transcripts" zusammen mit seinen Kollegen aus Adobe Research und UNC Chapel Hill. Das Dataset MeetingQA ist ein Extractive Question-Answering-Dataset, der auf Fragen und Antworten in offiziellen Treffen basiert. Es umfasst 7.700 Fragen und bietet eine breite Palette an Anwendungsgebieten, wie zum Beispiel die Identifizierung von mehreren Sprechern oder die Unterscheidung von rhetorischen Fragen. Durch das Verwendung von verschiedenen Modellarchitekturen und -techniken haben die Forscher erzielt, dass die Leistung der Modelle sich dem menschlichen Leistungsstandard näheret.</sample>
    <sample id="151">Natürlich Hier ist die Übersetzung:

"Hallo allein, mein Name ist Ying und ich und mein Kollege Zhiyang präsentieren unsere Forschung über MultiInstruct: Verbessern von Multi-Modal Zero-Shot-Learning durch die Anleitung zur Tuning. Mit den Fortschritten in großen Sprachmodellen haben viele Studien neue Lernparadigmen erforscht, indem sie vorher trainierte Sprachmodelle für verschiedene abhängige Aufgaben in einer parameter- und dateneffizienten Weise nutzen. Jüngst wurden viele Arbeiten gezeigt, die zeigen, dass die Anleitung zur Tuning es Sprachmodelle ermöglicht, auf neue Aufgaben in einem Zero-Shot-Manner zu folgen, indem sie natürliche Anweisungen befolgen. Dennoch konzentrierten sich die meisten früheren Arbeiten auf die Verbesserung der Zero-Shot-Leistung bei Sprachaufgaben, während Computer Vision und multi-Modal Aufgaben zurückgelassen wurden. Daher möchten wir untersuchen, ob die Anleitung zur Tuning von multi-Modal vorgearbeiteten Modellen die allgemeine Fähigkeit zur Verständigung von neuen multi-Modal Aufgaben verbessern kann. Darüber hinaus entdeckten wir eine bedeutende Unterschiedlichkeit im Verfügbarkeit von Anweisungsdatensätzen zwischen NLP und multi-Modal. Es gibt mehr als 1600 Sprachbasierende Aufgaben. Allerdings gibt es kein großes öffentlich verfügbarer multi-Modaler Anweisungs Satz. Diese Motivation führte uns dazu, eine multi-Modaler Anweisungs Satz zu erstellen. Hier präsentieren wir MultiInstruct, das erste multi-Modaler Anweisungs Satz, das aus 62 verschiedenen multi-Modalen Aufgaben mit 10 verschiedenen Kategorien besteht. Diese Aufgaben werden von 21 vorhandenen offenen Datenbanken abgeleitet und sind mit fünf von Experten geschriebenen Anweisungen ausgestattet. Hier sehen Sie einige Beispiel Instanzen aus unserem MultiInstruct-Dataset, um die Einheit des Prozesses für verschiedene Eingaben und Ausgabensatztypen zu vereinen. Wir folgen dem Weg von OFA, einem einheitlichen multi-Modalen vorgearbeiteten Modell, und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-to-Sequenz-Format. Im Input sind Texte, Bilder, Anweisungen und Bounding Boxes in derselben Tokenraum repräsentiert. Wir folgen dem Weg von OFA und formul</sample>
    <sample id="152">Das Vortrag "Exploring Large Language Models for Classical Philology" von Frederick Riemenschneider diskutiert die Entwicklung und die Bedeutung von Sprachmodellen für antike Sprachen wie griechisch und lateinisch. Er beginnt mit einer Überblick über die jüngsten Entwicklungen, wie zum Beispiel die Einführung von GriechenBERTa und GriechenTa im Jahr 2021 und 2022. Es wird betont, dass diese Modelle alle BERT-Modelle sind, was sie zu encoder-only-Modellen macht, und dass sie alle monolingual sind. Diese Einschränkungen könnten für Schriftsteller eine Herausforderung darstellen, da sie möglicherweise eine Modell, das sowohl Griechisch als auch Latein versteht, benötigen.

Riemenschneider beschreibt seine Ziele für das Projekt, einschließlich der Erstellung von neuen Sprachmodellen für klassische Philologie, die aufgrund der langen Geschichte und des Reichtums der antiken Texte speziell entwickelt wurden. Seine Ziele umfassen die Verbesserung der Komparierbarkeit der aktuellen Modelle, die Fortschritte in der Forschung weiterzuführen, die Untersuchung verschiedener Modellarchitekturen und die Entwicklung von multilingualen Modellen, die Griechisch und Latein verwalten können. Um dies zu erreichen, haben sie GriechenBERTa und GriechenTa, beide encoder-only-Modelle für Griechisch, und PhilBERTa und PhilTa, beide multilingualen Modelle für Griechisch, Latein und Englisch, entwickelt.

Für die Vorbereitung dieser Modelle haben sie ein neues Pretraining-Datenmuster aus dem Internet Archive entworfen, indem sie auf korrekt transcritisierte griechische Stop-Wörter wie "γάρ" zurückgegriffen haben, um eine hochqualitative Datenquelle für die Vorbereitung zu erstellen. Sie haben auch benutzt, dass das Internet Archive eine große Sammlung von Buchscans enthält, die mit korrekter Greek-OCR-Transkription bereitgestellt wurden.

Die Benchmarksierung der Modelle zeigt, dass GriechenTa und GriechenBERTa bei den Aufgaben Part-of-Speech tagging, Dependency parsing und Lemmatization einen bedeutenden Vorteil gegenüber den aktuellen Standardmodellen haben. Interessant ist auch, dass GriechenTa's Encoder nach einem Epochen erheblich weniger effektiv ist als ein zufällig initialisiertes Modell, aber nach mehr Training erreicht es den Leistungspegel eines natürlichen encoder-only-Modells. Dies zeigt, dass die Encoders der T5-Modelle sich von den natürlichen encoder-only-Modellen unterscheiden.

Im Zusammenhang mit der multilingualen Unterstützung der Modelle wurden die Leistungen bei der Lemmatization für Griechisch und Latein verbessert, was zu einer erhöhten Leistung bei der Identifizierung von Synonymien und Antonymien sowie der Erkennung von Beziehungen zwischen Helden und Göttern beiträgt. Das multilingual Modell wurde auch mit den Leistungen der monolinguellen Modelle verglichen, und es wurde festgestellt, dass keine bedeutende Unterschiede zwischen den beiden Modelleinstellungen zu erkennen sind.

Insgesamt präsentiert das Vortrag "Exploring Large Language Models for Classical Philology" eine umfassende Analyse der aktuellen Trends und Herausforderungen in der Entwicklung von Sprachmodellen für klassische Philologie, zeigt die Verbesserungen der neu entwickelten Modelle und diskutiert die Bedeutung von multilingualen und encoder-only-Modellen für die Verarbeitung antiker Texte.</sample>
    <sample id="153">Ninareh Mehrabi präsentiert ihr Forschungsprojekt "Resolving Ambiguities in Text-to-Image Generative Models" bei Amazon Alexa AI. Ihr Ziel ist es, die Ambiguitäten in Prompts zu untersuchen, die für Text-to-Image-Modelle bereitgestellt werden, und zu schließen, ob die generierten Bilder den Benutzerabsichten entsprechen. Mehrabi beschreibt ihre Pipeline, die eine Benchmark-Datenbank mit verschiedenen Typen von Ambiguitäten erstellt, diese Prompts an ein Framework für die Entschleiertung übergeben und schließlich die Generated-Bilder mithilfe eines VQA-Modells auf ihre Treue zur Benutzerabsicht einschätzt. Sie zeigen, dass das Entschleiertungsframework insgesamt positive Effekte auf die treu darstellenden Bilder hat und dass ihre automatische Bewertungsstruktur mit menschlicher Bewertung übereinstimmt.</sample>
    <sample id="154">Die Autoren gehören an der Universität des Trentos und zur Bruno-Kessler-Stiftung.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">Das Vortrag ist über eine Forschungsarbeit namens "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" von Shen Gao und seinen Kollegen. Sie präsentieren ein neues Modell für die Zusammenfassung von Dialogen, das sich auf die Integration von statischer und dynamischer Graphen konzentriert. Das Hauptproblem bei der Verwendung von statischen Graphen besteht darin, dass sie auf unreibigen Sprachtools abhängen und nicht dynamisch an die Aufgabe angepasst werden können. Das SDDS-Modell verwendet einen Utterance Encoder, um die Utterances zu codieren, und konstruiert statische Graphen. Es bietet dann einen Static-Dynamic Graph-Modul, der dynamische Beziehungen zwischen Utterances einfängt. Schließlich verwendet ein pre-trainedes Sprachmodell zum Fusionsprozess.</sample>
    <sample id="158">Das Thema der Präsentation ist "Dual Cache für die neuronale Coreference Resolution von langen Dokumenten". Der Vortraggeber, Qipeng Guo von AWS, erklärt, dass die Aufgabe der Coreference Resolution besteht darin, Mentions zu identifizieren und sie zu Gruppen zu zugeordnet, die auf denselben Entität hinweisen. Conventional Methods haben eine quadratische Komplexität in Bezug auf Rechenzeitaufwand und Speicherplatzbedarf, während Cache-Based Methods eine lineare Komplexität erreichen. Allerdings verursachen die LRU-Strategie bei langen Dokumenten hohe Cache-Missfrequenzen, da Mentions unterschiedlicher Entitäten über das Dokument verteilt sind. Um diese Problemstellung zu lösen, wurde ein Dual-Cache vorgestellt, der einen lokalen und einen globalen Cache verwendet. Der lokale Cache speichert lokale Entitäten mit einer LRU-Eviction-Politik, während der globale Cache globale Entitäten speichert und mit einer LFU-Politik evitiert. Das Dual-Cache wurde auf vier öffentlichen Basen getestet und erhielt bessere Ergebnisse als die Baseline-Methoden, auch ohne trainierte Daten.</sample>
    <sample id="159">Es gibt eine Wiederholung des Inhalt, der im Englischen vorgelegt wurde.</sample>
    <sample id="160">Im ersten Schritt der Methode werden die Input-Token mit einer unordentlichen Multisetz von Tokens zugeordnet, die im Output auftreten sollen.</sample>
    <sample id="161">55.000 Skripte</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEplain ist MASSalign.</sample>
    <sample id="164">Schwach überwachtes Lernen ist ersparniserweise.</sample>
    <sample id="165">Wenting Zhao präsentiert das kürzlich veröffentlichte Papier "Abduktives Vernunftverhalten auf Basis von gegenseitig exklusiven Erklärungen" und beginnt mit einem konkreten Beispiel, um das Verständnis von Abduktionslogik zu erleichtern. Sie beschreibt, dass abduktives Vernunftverhalten eine Kontextlage (X) wie "Emily war in der Verkehrsverschiebung" und ein Ergebnis (Y) wie "Emily schaffte es zu ihrem Flug" verbindet, indem es plausibler Erklärungen findet, die den Kontext-Resultat-Gap schließen. Sie betont, dass der Ansatz der Paper eine unsupervisierte Lernmethode namens Likelihood-Learning mit Posterior-Regulierung (LiPoR) ist. LiPoR behandelt Erklärungen als verstecktes Variable und maximiert die marginalische Wahrscheinlichkeit des Ergebnisses gegeben der Kontextlage, indem sie die anderen möglichen Erklärungen im Z verlängert. Sie betonen, dass dies keine Kenntnis der plausiblen Erklärungen erfordert. Stattdessen verwendet LiPoR eine Regularisator, der die gegenseitige Exklusivität der Erklärungen widerspiegelt, indem er die Entropie von P(Z|XY) maximiert und minimiert, wenn diese größer ist als log(M), wobei M die Anzahl der plausiblen Erklärungen ist. Das Ergebnis auf dem AlphaNLI-Dataset zeigt, dass LiPoR alle vergangenen Ansätze, einschließlich eines starken zero-shot GPT-3-Baselines, um den 4-Punktnachteil in der Genauigkeit überschreitet.</sample>
    <sample id="166">Das Team von Harbin Institute of Technology, Shenzhen hat eine neue Methode für das Bildverzeichnen von sprachlich komplexen Texten vorgestellt. Diese Herausforderung besteht darin, da Bild-Text-Retrieval aufgrund der ähnlichheit der Bilder und des langen Beschreibens schwierig ist. Typische Methoden wie visuelle Sprachmodelle erzielen gute Leistungen bei einfachen Aufgaben, aber bei komplexen Aufgaben verlieren sie ihre Effizienz. Das Team wurde von der Divide-and-Conquer-Strategie und dem Dual-Process-Theorem inspiriert. Divide-and-Conquer ist eine Strategie zur lösen von großen Problemen durch die Zerlegung in kleinere Probleme, deren Lösung und Kombination zum gewünschten Ergebnis führen. Dual-Process-Theorem beschreibt, dass menschliche Gehirne zwei Denksysteme haben: System 1 für analoges Denken und System 2 für abstraktes logisches Denken. Die visuellen Sprachmodelle konzentrieren sich auf das Analogdenken von System 1, was zu einer starken Abwertung bei komplexen Aufgaben führt. Um diese Problematik zu lösen, entwickelte das Team ein Modell, das zwei Systeme kombiniert: das Proposition Generator und das Neural-Symbolic Reasoner. Das Proposition Generator zerlegt komplexe Sätze in einfache Sätze, während das Neural-Symbolic Reasoner logische Operationen durchführt. Durch die Kombination dieser Systeme kann eine effektive Lösung für komplexere Aufgaben erreicht werden.</sample>
    <sample id="167">Die Dokumente in DEplain-web wurden mit manuellen und automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="168">CoNLL++ Dataset wurde von Reuters News 2020 sammelt und mit den gleichen CoNLL-2003-Annotation-Richtlinien angelegt.</sample>
    <sample id="169">David Vilar von Google Translate präsentiert eine studiengängige Analyse der PaLM-LLM-Prompting für die Maschinenübersetzung. PaLM, ein 540 Milliarde Parameter großer Sprachmodell, wurde im letzten Jahr vorgestellt und erzielte in hundert NLP-Aufgaben den bestmöglichen Leistungsstand. In diesem Studium untersuchten die Forscher die Effektivität von Promptingstrategien für die Übersetzung, indem sie die PaLM-LLM mit den besten Praktiken der MT-Community verglichen. Sie verwendeten die neuesten Testdatensätze, um eine Überlappung mit der Trainingsdatenlage zu vermeiden, und kontrastierten ihre Ergebnisse mit denen der besten Systems, wie bei der WMT-Evaluation. Die Forscher verwendeten moderne, neural-basierte METRIKEN für die Bewertung und ergänzten diese mit Expertenbewertungen. Ihre Empfehlungen für Promptauswahl strategien beziehen sich auf die Qualität der Beispiele. Das Prompting hat einen bedeutenden Einfluss auf die Leistung der LLMs für die Übersetzung, wie eine einfachere Versuchsausübung zeigt, bei der die meisten Satze durch ein ein- oder zwei-Satzzahliges Prompt mehr als 1 BLEURT-Punkt besser ausperformen.</sample>
    <sample id="170">Natürlich Hier ist die Übersetzung:

"Hallo allein, ich bin Yusen Zhang aus der Penn State University. Heute präsentiere ich unsere Arbeit "XSemPLR: Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen". Semantic Parsing ist eine Aufgabe, um semantische Darstellungen von Benutzerfragen wie SQL oder Lambda Calculus zu erstellen. Cross-Lingual Semantic Parsing handelt darum, Fragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen. Wie im folgenden Bild gezeigt, müssen wir Fragen in mehreren natürlichen Sprachen mithilfe neuraler Modelle in SQL, Lambda oder FunQL oder andere Darstellungen übersetzen. Derzeitige Mehrsprachliche Semantic Parsing-Modelle werden für bestimmte Aufgaben und Anwendungen allein entwickelt und bewertet, wie zum Beispiel eine hohe Bedeckung für bestimmte Sprachen, aber nicht für Chinesisch oder eine Vielzahl von Bedeutungsrepräsentationen wie Lambda. Lambda-Kalkulations ist fehlend oder wird nur auf bestimmte Neural-Modelle eingeschränkt. Um dies zu beheben, präsentieren wir XSemPLR. Wir bieten ein einheitliches Dataset XSemPLR für die Mehrsprachliche Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Es enthält neun Datensätze in verschiedenen Bereichen, fünf Semantic Parsing-Aufgaben, acht Bedeutungsrepräsentationen und 22 Sprachen in 15 Sprachfamilien. Um unsere Benchmark besser zu bewerten, berücksichtigen wir sechs Einstellungen für Training und Evaluation. Zum Beispiel Train-Test, bei der wir die QuellSprache über Google-Übersetzer API ins Zielsprachen übersetzen und dann mit einem monolingualen Modell trainieren und evaluieren. Wir werden auch einen Monolingual-Modell-Ergebnis testen. Im ersten Fall verwenden wir das Modell auf englischen Fragen, während wir die deutsche Frage über API übersetzen und dann mit dem trainierten Modell die SQL-Abfrage vorhersagen. Wir werden auch einen Monolingual-Few-Shot-Setting testen. In diesem Setting sind die Quell- und Zielsprache gleich, wie zum Beispiel Deutsch auf Deutsch oder Englisch auf Englisch. Wir werden auch einen Monolingual-Few-Shot-Setting testen. In diesem Setting trainieren wir das Modell mit nur 10% der Trainingsdaten für verschiedene Sprachen. Und wir werden einen Multilingual-Setting testen. In diesem Setting trainieren wir ein multilinguales Modell für alle Sprachen zusammen, wie zum Beispiel den Deutschen, Englischen und Chinesischen. Während der Inferenz können wir dieses Modell für Deutsche oder Chinesische Fragen verwenden. Wir werden auch einen Cross-Lingual Zero-Shot-Setting und einen Cross-Lingual Few-Shot-Transfer testen. Wir trainieren das Modell auf einer Quellsprache und übertragen es auf eine andere Sprache. Während des Trainings verwenden wir entweder Englische Fragen oder eine Mischung aus Englischen und Deutsch-Few-Shot-Questions, um ein multilingualeres Modell für die Vorhersage der SQL-Aussagen zu trainieren. Wir finden viele interessante Ergebnisse. Beispielsweise im Hinblick auf die Bewertung von monolingualen Modellen untersuchen wir zwei Gruppen von Modellen, einschließlich Encoder-PTR, was bedeutet, dass multilingualere Pretraining-Encoders mit Pointer-basierten Decoders wie XLM-R + PTR und mBERT + PTR verwendet werden. Und Encoder-Decoder-Modelle, die multilingualere Pretraining-Encoders-Decoder wie mBART und mT5 verwenden. Wir fanden, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielt hat. Wir haben mT5 und XLM-R + PTR auf multilingualem Setting eingeschaltet. Wir fanden, dass Encoder-Decoder oder Encoder-PTR durch die Trainierung in einer Mischung aus verschiedenen Sprachen verbessert werden können. Wir fanden, dass die meisten bedeutenden natürlichen Sprachen die Leistung erhöhen können, außer dass Englisch in sieben Datensätzen eine stimmlose Leistung zeigt und nur in drei Datensätzen eine Verbesserung zeigt. Wir verglichen auch den Verlust der Mehrsprachlichkeit. Im Folgenden ist der blaue Linie die Verlust der Mehrsprachlichkeit im Few-Shot-Setting. Die orangefarbene Linie zeigt den Verlust der Mehrsprachlichkeit im Zero-Shot-Setting an. Wenn wir die grüne und die orangefarbene Linie vergleichen, fanden wir, dass der Zero-Shot-Setting eine bedeutende Verlust der Mehrsprachlichkeit aufweist. Wenn wir die blaue und die orangefarbene Linie vergleichen, fanden wir, dass mit der Few-Shot-Setting die Verlust der Mehrsprachlichkeit schnell verringert wird. Wir fanden auch andere interessante Erkenntnisse. Z.B. Encoder-Decoder erzielte bessere Leistungen als frühere Arbeiten oder erreichte ähnliche Ergebnisse. Das Pretraining auf englischen natürlichen Sprachen kann die Leistung bei der Few-Shot-Transfer auf Zielsprachen deutlich steigern, und wir fanden, dass multilingualere Modelle wie Codex und BLOOM für die Mehrsprachliche Semantic Parsing-Aufgabe noch nicht ausreichend sind. Zusammenfassend bauen wir XSemPLR, ein einheitliches Referenz für die Mehrsprachliche Semantic Parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Wir führen eine umfassende Referenzstudie über drei repräsentativen Typen von multilingualen Language Models durch. Unser Ergebnis zeigt viele interessante Erkenntnisse.</sample>
    <sample id="171">Arbeiten, die bereits durchgeführt wurden, um das Embedding als Dienstleistung zu schützen, umfassen folgende Ansätze: 1. Schutzmaßnahmen zur Gewährleistung der Einzigartigkeit des Embeddings, indem man die Identität des Embeddings an den Benutzern erkennbar macht. 2. Schutzmaßnahmen zur Verhinderung, dass Benutzer den Embeddings zugreifen können, ohne dass sie die Identität des Embeddings kennen. Diese Ansätze sind jedoch nicht ideal, da sie den Zugriff auf die Embeddings für Benutzer einschränken und möglicherweise den Nutzen der Embeddings reduzieren.</sample>
    <sample id="172">Nein, sie sind noch nicht ausreichend für CLSP.</sample>
    <sample id="174">Das Dataset "ArgAnalysis35K" ist ein großes und hochwertiges Dataset für die Analyse der Argumentqualität. Es umfasst 35.000 Argument-Analyse Paare und bietet eine breite Palette von Argumenten, die von hoch qualitativen Turnieren, Experten und mittleren Debattenbegeisterten stammen. Das Dataset verfügt über einen hohen Anlayzer-Reliabilität, indem es instance-basierte Anlayzer-Reliabilitätsmodelle verwendet, um die Zuverlässigkeit der Anlayzer auf einer Instanz-Ebene zu erfassen. Darüber hinaus bietet das Dataset eine relevanzmodellierte Analyse, die es ermöglicht, die Relevanz jedes Arguments zu einem bestimmten Thema zu messen. Diese Merkmale machen "ArgAnalysis35K" zu einem einzigartigen und wertvollen Dataset für Forscher und Praktiker in der Argumentanalyse.</sample>
    <sample id="175">Die Methode mit der Mehrdeutigkeit der Permutationen wird in der Paper "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" beschrieben. Der Prozess besteht darin, die Permutationen zu einer bestimmten Position im Output zu bestimmen, indem man multiset Tokens auswählt und sie in der richtigen Reihenfolge platziert. Diese Methode ermöglicht es, die Permutationen flexibel und expressiv zu modellieren. Allerdings bringt dies auch das Problem der Mehrdeutigkeit bei, da es mehrere mögliche Permutationen geben kann, von denen eine die linguistisch korrekte ist. Um dieses Problem zu lösen, wird die Methode durch eine GPU-friendliche kontinuierliche Relaxation und die Backpropagation durchgeführt, um die linguistisch wahrscheinlichere Permutation zu finden.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird durch die Analyse der Leistung des Modells auf verschiedenen Gruppen oder Kategorien definiert. Es handelt sich um eine Untersuchung, ob das Modell in der Lage ist, auf eine gleichmäßige Weise zu handeln, unabhängig von den sozialen oder politischen Gruppen, die es betrifft. Dies kann durch die Untersuchung der Modellleistungen auf verschiedenen Gruppen, wie zum Beispiel aufgrund ihrer politischen Meinung, Geschlecht oder Ethnie, und die Untersuchung, ob es aufgrund dieser Gruppen unterschiedliche Ergebnisse gibt, durchgeführt werden.</sample>
    <sample id="177">Der Referent namens Yanis Labrak.</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">Das Forschungsprojekt "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker" untersucht die Fähigkeit von Sprachmodellen, eine Theorie des Geistes zu entwickeln. Durch das Einsatz eines Inference-Time-Methodes namens SymbolicToM verbessern sich die Sprachmodelle bei der Behandlung von False-Belief-Problemen. Das Projekt präsentiert Graphen für verschiedene Charaktere und berechnet ihre mentalen Zustände, um eine bessere Interpretation von Falschdarstellungen zu ermöglichen. Durch die Verwendung dieser Methoden erzielen Sprachmodelle eine starkerste Verbesserung im Vergleich zu traditionellen Supervisionsmethoden.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">Das Team von Fudan University hat eine Studie zur "Distillation von Skriptwissen aus großen Sprachmodellen für eingeschränkte Sprachplanung" vorgelegt. Ihre Forschung untersucht die Planung von spezifischen Zielen mit mehreren Beschränkungen, wie zum Beispiel "ein Schokoladenkuchen herzustellen". Sie haben festgestellt, dass große Sprachmodelle bei der Planung für solche spezifische Zielen nicht sehr erfolgreich sind und haben daher eine Methode entwickelt, um die Qualität der generierten Skripte zu verbessern. Sie haben ein Dataset namens CoScript erstellt, das 55.000 spezifische Ziele mit Skriptsammlungen enthält. Das Team hofft, dass dieser Ansatz die Forschung in der Sprachplanung weiterentwickeln kann.</sample>
    <sample id="182">Tropikalismus bezieht sich auf eine Tropikalisierung, bei der Frauen von afroamerikanischer Abstammung in der Kunst und Medien häufig als lebendig, farbenfroh und traditionell dargestellt werden. Diese Darstellung kann dazu führen, dass sie als "tropical" oder "exotisch" gesehen werden, was zu stereotypischen und essentialisierenden Narrativen führt.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen durch ein Studium von Prompts an menschliche Subjekte erstellt. Sie haben dabei festgestellt, dass diese Prompts dazu führten, dass Rassenstereotypien aufgetreten sind.</sample>
    <sample id="184">In dieser Arbeit wurde CXMI (Context Mutual Information) als Messmaß für die Kontextnutzung bei der Übersetzung verwendet. CXMI ist eine Maße, die den Informationgewinn des Kontextes C für die Übersetzung des Zielsatzes Y gegeben den Quellsatz X messen soll. Die Studie hat CXMI erweitert, um es auf den Satz- oder Wortniveau zu verwenden und damit Wörter zu identifizieren, die Kontext für die Übersetzung benötigen. Diese Erweiterung ermöglicht es, spezifische Sprachmerkmale und Diskursaspekte zu untersuchen, die Kontext für die korrekte Übersetzung erforderlich sind.</sample>
    <sample id="185">DrBERT ist ein Biomedizinischer Modell in Französisch, das auf RoBERTa basiert und mit NACHOS trainiert wurde, einem Dataset von medizinischen Crawled-Daten aus der Web. ChuBERT ist ein klinisches Modell, das anonymisierte Daten aus dem Nantes University Hospital Data Warehouse verwendet hat.</sample>
    <sample id="187">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="188">Iteratives Transferlernen ist eine Methode, bei der ein Modell auf einem Domänenüberschneidungsaufgaben übertragen wird und dann weiter trainiert wird, um die Leistung zu verbessern.</sample>
    <sample id="189">Das Ziel des Datensatzes ist es, die Verständnisfähigkeiten von LLMs bei der Entscheidungsfindung zwischen Indirektbezugsgestellten Entity-Selections zu verstehen.</sample>
    <sample id="190">Angreifer können Modellparameter über Embedding-as-a-Service (EaaS) extrahieren, indem sie die Embeddings aus dem Service lernen und ähnliche Dienste erstellen.</sample>
    <sample id="191">Es sind drei Autoren an der Arbeit beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">Das Vortrag über "CAME: Confidence-guided Adaptive Memory Efficient Optimization" diskutiert die Herausforderung, einen Optimierer zu entwickeln, der sowohl schnell konvergieren als auch eine niedrige Speicheraufwand aufweist. Der Vortrag bietet eine Einführung in die non-negative Matrix-Faktorisierung (NMF) und zeigt, wie Adafactor eine Analytische Lösung für die Minimierung der I-divergenz zwischen dem Originalmatrix V und der Approximation W x H präsentiert. Es wird gezeigt, dass diese NMF-Operation bei der Training von tiefen neuralen Netzwerken zu fehlerhaften Updates führt. Der Vortrag präsentiert eine neue Methode, die die Instabilität bei fehlerhaften Updates reduziert, indem sie die historischen Erfahrungen berücksichtigt. Durch die Verwendung der Instabilität und des Originalparameters wird eine effiziente Optimierungsmethode entwickelt, die als CAME bezeichnet wird. CAME verbessert die Konvergenzrate und die Endleistung von Large Language Models während der Trainingsschritte und reduziert die Speicheraufwand bei größerem Batchgröße.</sample>
    <sample id="193">Zwei Annotatoren wurden verwendet, um den ursprünglichen Datensatz zu erstellen.</sample>
    <sample id="194">Die Autoren gehören an Carnegie Mellon University und der Universität Washington.</sample>
    <sample id="195">Das Team hat ein neues Framework vorgestellt, das "Reasoning over Hierarchical Question Decomposition Tree" heißt und für Erklärbare-Question-Antwortung (XQA) konzipiert wurde. XQA versucht, komplexe Fragen zu beantworten und die Grundlagen dafür zu erklären. Es gibt zwei Hauptansätze: Neuro-symbolische Methoden, die natürliche Sprache in formale Representationen übersetzen, und Decomposition-basierte Methoden, die komplexe Fragen in Unterfragen aufteilen. Neuro-symbolische Methoden haben begrenzte Erinnerungsleistung, weil sie nur strukturierte KBs unterstützen. Decomposition-basierte Methoden haben Schwierigkeiten mit der Vielfalt der natürlichen Sprache. Das Team hat ein neues Framework vorgestellt, das eine zwei-stufige Methode ist. Zunächst wird eine Hierarchische Frage-Zerlegungsbaum (HQDT) erstellt, um die komplexen Fragen zu strukturieren. Zweitens wird probabilistischer Ursprungsverarbeitung über den HQDT durchgeführt, um die Wissen aus einer KB und einem Text-Korpus auf verschiedenen Niveaus zu kombinieren. Das Team hat das Framework auf zwei Komplexitätstypen getestet - KQA Pro und Musique - und hat positive Ergebnisse erzielt.</sample>
    <sample id="196">I saw Bart and Lisa</sample>
    <sample id="197">Das Bild zeigt, dass die Technologie für Dialogsysteme immer noch viele Fehler begeht. Zum Beispiel sind sie in etwa 20% ihrer Antworten aufgrund von Unschärfe verunsinnet, produzieren sie etwa 15% der Zeit irrelevantes Material und verweisen sie etwa 10% der Zeit auf sich selbst oder ihre Partner. Diese Fehlerrate könnte mit neuen Modellen abnehmen, aber es ist wichtig, eine fundierte und genaue Bewertung zu ermöglichen, um die Fortschritte in diesem Bereich zu messen.</sample>
    <sample id="198">Da große Sprachmodellierungen einen größeren Kontextfenster verwenden, ist es wichtig, die Akzeptanz der Modelle über das gesamte Kontextfenster zu bewerten.</sample>
    <sample id="199">Ja, das mehrsprachige Training hat zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt.</sample>
    <sample id="200">Ja, die Annotatoren wissen die Entitäten im Voraus.</sample>
    <sample id="201">Die MT-Metriken BLEURT und die Expertenbewertung des MQM-Frameworks wurden für die Bewertung verwendet.</sample>
    <sample id="202">Ja, die Regression bei der Generalisierung auf bestimmte NER-Typen wirkt sich aus.</sample>
    <sample id="203">Positionalität für NLP ist wichtig, weil sie die Perspektiven und Erfahrungen von Entwicklern und Forschern beeinflusst, was zu designbasierten Schwächen in Datasetts und Modellen führen kann. Positionalität kann das Ergebnis von decisions beeinflussen, was zu einer ungleichmäßigen Leistung bei der Analyse von Informationen von verschiedenen Gruppen und Kulturen führen kann.</sample>
    <sample id="204">Nein, wurden mehrsprachige LLMs wie BLOOM nicht durch Adapter oder eine vollständige Feinabstimmung angepasst.</sample>
    <sample id="205">Shangbin, ein PhD-Studierender an der Universität Washington, präsentiert die Arbeit "Von der Vorbildungsdaten zu Sprachmodellen zu NLP-Anwendungen: Das Verfolgen der Spuren von politischen Biases in der Sprachmodellierung". Language models werden mit großen Mengen an Webkrawldaten trainiert, darunter auch politische Nachrichten. Diese Mischung kann zu einer Mischung aus Vorteilen und Nachteilen führen. Shangbin und seine Teamkollegen untersuchen, wie politische Biases im Trainingsvorgang von Daten zu Modellen und Schließlich zu NLP-Anwendungen übertragen werden. Sie verwenden verschiedene Prompts und korporale, um die politische Neigung von Language Models zu messen und untersuchen, wie sie auf verschiedenen Aufgaben wie Mangelbildungs- oder Fake-News-Detektionen aussehen. Ihre Studie zeigt, dass Language Models unterschiedliche politische Neigungen haben und dass diese Neigungen in den Aufgabenwerten spiegeln können.</sample>
    <sample id="206">Wir verwenden ein Modell für das Transferlernen, das von zwei verschiedenen Aufgaben übernommen wurde: Topic-übergreifende Stellungserklärung von Diskussionseinheiten, die eine Diskussion zwischen zwei verschiedenen Personen beurteilt, unabhängig vom Thema, und die Klassifizierung von Expansionen und Vergleichsakkusatzern in der PDTB (Penn Treebank Dependency) - eine Aufgabe, die sich eng mit der Konzeption von Konsonanz und Dissonanz im Text verbindet.</sample>
    <sample id="207">Die neuesten Testsets wurden zur Bewertung der PaLM-Fähigkeiten verwendet.</sample>
    <sample id="208">The authors have made three recommendations for model owners.</sample>
    <sample id="209">Der Gewinn der vorgeschlagenen Methode gegenüber der stärksten Baseline beträgt 15,4%.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz der Studie können als Base Benchmark für die Problemstellung der automatischen Textsimplifizierung in Zukunft verwendet werden.</sample>
    <sample id="212">In der Arbeit wird ein kleineres Modell, das auf CoScript trainiert wurde, experimentiert.</sample>
    <sample id="213">OFA wird als Basismodell für die Untersuchung der multimodalen Unterrichtsabstimmung verwendet.</sample>
    <sample id="215">Das Vortrag über die Abhängigkeitsstruktur der Koordination diskutiert verschiedene Abhängigkeitsstrukturen, wie sie von verschiedenen Theorien und Korpusansätzen vorgeschlagen werden. Es wird darauf hingewiesen, dass einige Ansätze, wie zum Beispiel das Universal Dependencies-System oder Igor Mel'čuk's Theorie des Sinn-Texts, einen Aspekt der Koordination als Hauptelement annehmen, während andere Ansätze, wie zum Beispiel das Pragdische System oder Hudsons Grammatik, eine mehrere-Governor-Struktur vorschlagen. Der Vortrag zeigt, dass die Abhängigkeitslänge-Minimierung-Prinzip, das es vorschlägt, dass kürzere Abhängigkeiten bevorzugt werden sollten, eine Argumentation für eine symmetrische Struktur der Koordination bietet. Diese Beobachtung wurde auf der Grundlage einer Analyse der Abhängigkeitsstruktur der Koordination in der erweiterten Version des Penn Treebanks bestätigt.</sample>
    <sample id="217">Das Forschungsprojekt "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" untersucht die Kompositionsgeneralisierung für multi-attributige kontrollierte Dialoggenerierung. Das Team, bestehend aus Weihao Zeng, Lulu Zhao und Keqing He, entwickelt ein Disentangled Controllable Generation (DCG) und einen einheitlichen evaluativen Rahmen (MAE). Der DCG lernt Attributekonzepte von gesehenen Werten ab und verwendet eine Disentanglementverlust, während MAE eine einheitliche Bewertungsmethode für verschiedene Attributgrainedkeiten bietet. Durch Experimente haben sie gezeigt, dass ihre Methode und -evaluationsmuster die Kompositionsgeneralisierung verbessern und die Kontrollbarkeit und Textgleichheit erhöhen können.</sample>
    <sample id="218">Google Translate</sample>
    <sample id="219">Jia-Huei Ju präsentiert gemeinsam mit Yu-Shiang Huang, Cheng-Wei Lin und ihren Advisorinnen Che Lin und Chuan-Ju Wang die Arbeit "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports". Diese Arbeit befasst sich mit der Analyse von Form 10-K-Reports, die jährliche Berichte sind, die Unternehmen an der SEC verpflichtet sind zu erstellen. Die Forschungskommission erfordert, dass diese Berichte viele wichtige Aktivitäten der Unternehmen enthalten. Allerdings benötigt die Extraktion von nützlichen Informationen viel menschlicher Anstrengung. Diese Arbeit wurde motiviert durch zwei Beobachtungen: Zunächst ist die Textsimilarität zwischen den Berichten sehr hoch (um 80%) und die Inhalte sind jahrlich abhängig. Zweitens haben die Wörter im Bericht sehr ähnliche Bedeutungen. Aus diesen Beobachtungen resultiert eine Highlightingaufgabe, bei der ein Modell die Bedeutung von Worten in einem Bericht in Bezug auf einen vergangenen Bericht messen soll. Der Vorschlag besteht darin, eine mehrstufige Pipeline zu entwickeln, die zwei Schritte für die Fine-Tuning des Modells umfasst: eine Out-of-Domain-Fine-Tuning auf einer externen Datensammlung und eine In-Domain-Fine-Tuning auf der verwendeten Dataset. Das Ergebnis dieser Arbeit zeigt eine hervorragende Leistung im Vergleich zu anderen Methoden, insbesondere bei der Behandlung von Mismatched-Paaren.</sample>
    <sample id="220">Die Autoren gehören an Stony Brook University.</sample>
    <sample id="221">Deutsch-Englisch, Französisch-Englisch und Italienisch-Englisch wurden untersucht.</sample>
    <sample id="222">Das Forschungsprojekt untersucht die Herausforderungen und Interventionen für die Anpassung von offener Domänenfragen (Open-Domain QA) an neue Kontexte. Es untersucht verschiedene Dateninterventionen, um die Leistung der Modell-Systeme zu verbessern, indem sie die Kompatibilität zwischen Quellen und Ziel-Datenmengen messen. Durch die Analyse von Dateninterventionen wie Zero-Shot und Few-Shot-Methoden, die auf den Typ des Datenshifts angewendet werden können, zielt das Projekt darauf ab, die Leistung der Modell-Systeme bei der Behandlung von offenen Domänenfragen zu erhöhen.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">Long-mBART, mBART</sample>
    <sample id="225">53 Aufgaben werden für Training verwendet, und 10 Aufgaben für Test.</sample>
    <sample id="226">There are three authors involved in the work: Regina Stodden, Omar, and another unnamed author.</sample>
    <sample id="227">Das Hauptthema der Präsentation ist grounded language understanding, ein wichtiger Aspekt in der Forschung zu Sprachmodellen. Der Vortrag zeigt, dass die meisten Sprachmodelle ohne Bodengrundung während des Vorspiegels trainiert werden, was das Aufgabenfeld von grounded language understanding herausfordernd macht. Der Vortrag präsentiert eine neue Ansatzweise, die auf die Diskriminierung anstatt auf die Generation von Plänen basiert. Diese Ansatzweise ermöglicht es Sprachmodellen, effizient und robust auf verschiedene Eingaben zu reagieren. Durch das Verwendung von Symbolischen Agents können Sprachmodelle die Validität und die Grammatik von Plänenignorieren, da sie nur auf die Bewertung der vorgeschlagenen Pläne angewiesen sind. Das Experiment mit verschiedenen Sprachmodellen zeigt, dass diese Ansatzweise erhebliche Verbesserungen im Vergleich zu traditionellen Generation-Methoden erzielt hat.</sample>
    <sample id="228">AG News, MIND, SST2 and Enron Spam</sample>
    <sample id="229">Das Thema der Präsentation ist "Detecting Improvable Claims for Argumentative Writing Support". Gabriella Skitalinskaya und Henning Wachsmuth präsentieren ihre Arbeit, die sich auf die Analyse von Textrevisions in argumentativen Schreibarbeiten konzentriert. Das Hauptaugenmerk liegt darauf, zu ermitteln, ob ein argumentativer Ansatz optimal phrasiert ist oder noch Verbesserungen benötigt. Sie stellen zwei Aufgaben vor: Task 1: Suboptimal-Claim detection (Bestimmung, ob ein Ansatz suboptim ist) und Task 2: Claim Improvement Suggestion (Erkennung von Problemen bei einem Ansatz). Die Präsentation diskutiert auch die Herausforderungen, die mit der Verwendung von Revisionen im Zusammenhang kommen, wie z.B. die Repräsentativität und Zuverlässigkeit der Daten, die Komplexität der Modellarchitektur, die Abhängigkeit von Kontextinformationen und die Topik- und Benutzerbiase.</sample>
    <sample id="231">NACHOS ist ein Dataset, das aus Medizininformationen auf der Webseite gewann.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Simultaneous speech translation (SimulST) is the real-time process of translating spoken language into text in another language. Current models often require re-training or specific architectures, leading to long training procedures and multiple models for different latency regimes. The proposed solution, EDAtt (Encoder-Decoder Attention), uses existing offline ST models without additional optimization objectives by leveraging attention mechanisms between audio input and textual output. This strategy decides whether to emit a partial translation based on where attention points to, allowing for efficient simultaneous speech translation with high quality and low latency.</sample>
    <sample id="234">Die Prompt-Strategie hat einen bedeutenden Einfluss auf die Ergebnisse bei der Übersetzung. In einem einfachen Experiment wurden zwei verschiedene Prompts für jede Satzstellung verwendet, und es wurde beobachtet, dass die meisten Sätze (516 aus 1.000) mehr als eine BLEURT-Punkte von der Performance abhängig waren. Im Extremfalle war die Abweichung sogar bis zu 40 BLEURT-Punkten. Es ist also wichtig, eine gute Prompt-Strategie zu wählen, um die Leistung der LLMs für die Übersetzung zu optimieren.</sample>
    <sample id="235">The authors belong to the University of Toronto.</sample>
    <sample id="236">Die 5 Anweisungen der Expert*innen sind für die Verarbeitung von verschiedenen Input- und Outputdatentypen, wie zum Beispiel Text, Bilder und Bounding Boxes, auf einem einzigen Tokenraum zu formulieren.</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle zur Integration von Informationen aus mehreren Quellen zu testen, indem sie ein Diagnostic-Test-Suite namens KITMUS präsentieren. Diese Test-Suite umfasst eine Coreference Resolution-Task, die dazu entworfen wurde, das Modell zu bewerten, ob es die Fähigkeit hat, auf Informationen aus verschiedenen Quellen zu setzen. Sie präsentieren drei verschiedene Settings: "Background-Pretrain", "Background-Both" und "Background-Inference". Die Autoren evaluieren die KITMUS-Test-Suite sowohl mit menschlichen Studienbeteiligung als auch mit etablierten Coreference Resolution-Modellen.</sample>
    <sample id="238">In diesem Video präsentiert Yebowen Hu von der University of Central Florida ein neues Benchmark-Dataset namens MeetingBank. Das Dataset wurde für die Entwicklung von Summarisierungstechnologien für verschiedene Lesegänge entworfen. Es umfasst eine Vielzahl von Ressourcen, wie Meetingskonzerttranskripten, Referenzsummen und weiteren URLs mit nützlichen Informationen. Der Prozess der Datenverarbeitung umfasst das Verwendung eines Speechmatics-APIs zur Transkription des Audioinhalts, die Identifizierung der Typ und Daten der Veranstaltung auf der Veranstaltungswebsite, die Auswertung der Segmente und die Anordnung der Timestamps für eine korrekte Übereinstimmung zwischen Quellen- und Zusammenfassungssegmenten. Das Dataset beinhaltet insgesamt 1.366 Stadtvertretungsratssitzungen und fast 7.000 Instanzen. Die Evaluation der Summarisierungsmodelle zeigt, dass GPT-3 in der menschlichen Bewertung hervorragende Ergebnisse erzielt hat, insbesondere in den Bereichen Fluideität und Kohärenz. Allerdings zeigt es in der Bewertung nach Informativität und Faktlichkeit weniger Erfolg. Diese Erkenntnisse deuten darauf hin, dass Meeting-Summarisierungs-Lösungen weiterhin auf die Aufnahme der Hauptdiskussionspunkte konzentrieren sollten, und neue automatische Bewertungsindikatoren entwickelt werden sollten, die sich besser mit menschlicher Vorliebe auseinandersetzen.</sample>
    <sample id="239">Es tut mir leid, aber ich kann den englischen Text nicht ins Deutsche übersetzen. Ich kann Ihnen jedoch bei der Übersetzung von anderen Texten oder Fragen helfen</sample>
    <sample id="240">Natürlich Hier ist die deutsche Übersetzung des englischen Inhalts:

"Hallo, ich bin Dawei, ein PhD-Studierender an der Saarland University in Deutschland. In diesem Video möchte ich unsere kürzlich veröffentlichte Arbeit 'Weaker Than You Think: Ein Kritischer Blick auf Weakly Supervised Learning' präsentieren. Diese Arbeit ist mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow gemeinsam erstellt.

Ich werde beginnen, eine kurze Einführung in Weak-Supervision und Weakly Supervised Learning zu geben. In Weak-Supervision werden Sie nicht manuell die Daten beschriftigen. Stattdessen beschriftigen Sie die Daten mithilfe schwacher Labelquellen wie einfachen heuristischen Regeln, Wissensdatenbanken oder schlechterer Crowdsourcing-Aktivitäten, wie im Bild rechts gezeigt. Im Vergleich zu menschlichen Annotierungen sind schwache Labels viel kostengünstiger, jedoch auch ungenau, was bedeutet, dass ein bestimmter Teil der Annotierungen falsch sind. Wenn wir neuralen Netzwerken direkt mit schwachen Labels trainieren, memorisieren die Netzwerke das Labelunreinheit und können nicht gut übergreifen.

In Weakly Supervised Learning werden Trainingsalgorithmen vorgeschlagen, um neuralen Netzwerken robust zu trainieren, wenn sie unter solchen Labelunreinheiten stehen, damit die trainierten Modelle immer noch gut übergreifen können. In den jüngsten Arbeiten in der Weakly Supervised Learning (WSL) wird häufig behauptet, dass Menschen nur mit schwachen Labels die Modelle trainieren und dann auf sauberen Test-Setzungen hohe Leistung erzielen. Technisch ist diese Behauptung nicht falsch, aber es gibt einen Hintergrund, der oft übersehen wird. Es impliziert, dass zusätzliche manuelle Annotierungen für die WSL erforderlich sind. Allerdings wird dieser Notwendigkeit oft ignoriert.

Wir haben uns diese Forschungsfragen gestellt: Erstens ist sauberes Validationsdata für die WSL notwendig oder können wir stattdessen benutzen, um schwaches Validationsdata? Zweitens, wenn sauberes Data erforderlich oder sogar notwendig für die WSL ist, wie viele saubere Beispiele benötigen wir? Schließlich sollten wir nur die sauberen Beispiele für Validierung verwenden, oder gibt es bessere Wege, sie zu nutzen?

Wir haben diese Forschungsfragen beantwortet und unsere Ergebnisse sind wie folgt: Zunächst fanden wir, dass, interessant genug, die meisten kürzlich veröffentlichten WSL-Methoden benötigen tatsächlich saubere, manuell beschriftete Beispiele, um richtig zu funktionieren. Wenn es keine sauberen Validationsbeispiele gibt, gibt es einen großen Leistungsabfall. Wie im Bild gezeigt, wenn es keine sauberen Validationsbeispiele gibt, können die trainierten Modelle nicht übergreifen über die ursprünglichen schwachen Labels, was die Ausbildung unwert macht. Dies zeigt, dass die WSL-Methoden tatsächlich von sauberen Beschriften benötigen, und die Kosten für die Erhaltung von sauberen Validationsbeispielen sollten nicht übertäuscht werden.

Unser zweites Ergebnis lautet, dass die Anzahl der sauberen Validationsbeispiele den Leistungsverlust der WSL-Methoden reduziert, wie im Bild links gezeigt. Normalerweise benötigen wir etwa 20 Beispiele pro Klasse, um eine hohe Leistung zu erreichen. Aber das ist nicht alles, weil, wenn wir entscheiden, dass wir saubere Beispiele zugänglich machen, dann kann die direkte Fine-Tuning auf diesen Beispielen sogar bessere Leistungen erzielen. Das Bild rechts zeigt den Leistungsunterschied zwischen Fine-Tuning-Anordnungen, die direkt auf den sauberen Beispielen angewendet werden, und WSL-Anordnungen, die nur zum Validierungsziel verwendet werden. Als wir 10 Beispiele pro Klasse haben, beginnt die direkte Fine-Tuning zu überleisten die WSL-Anordnungen.

Zusammenfassend zeigen unsere Ergebnisse, dass die kürzlich veröffentlichten WSL-Methoden von sauberen, manuell beschrifteten Beispielen für ihre korrekte Funktionsweise benötigen. Ihre Leistungssteigerungen und Praktikabilität werden stark übertrieben. Unsere konkreten Empfehlungen für zukünftige Arbeit sind:

Erstens sollten die Modellauswahlkriterien angegeben werden. Zum Beispiel sollten Sie angeben, ob die Modellauswahl mit sauberen Validationsbeispielen durchgeführt wurde. Zweitens sollten WSL-Anordnungen mit den sauberen Baselines verglichen werden, da beide arbeiten mit sauberen Beispielen. Drittens sollte die kontinuierliche Fine-Tuning als einfacher und starker Baseline in zukünftigen WSL-Studien berücksichtigt werden. Schließlich haben wir unsere Code öffentlich freigelegt. Sie können es über den QR-Code auf dieser Präsentation überprüfen. Bitte führen Sie es ein und genießen Sie den Kongress!

Vielen Dank und genießen Sie den Kongress!"</sample>
    <sample id="241">Das Team von Ethan, Yang Chen, Wei Xu und Alan Ritter hat eine Forschungsarbeit vorgelegt, die sich auf die Untersuchung der Leistung von Systemen für die Frühdetektion von Missinformationsnachrichten in Zusammenhang mit der Covid-19-Pandemie konzentriert. Diese Systeme sind häufig nicht realistisch ausgewertet, da sie auf historischen Daten oder verbleibenden Beweisen basieren, anstatt auf lebendigen Daten. Darüber hinaus sind diese Systeme oft nicht menschenzentriert, da sie entweder den menschlichen Involvement completely weigern oder ihn nur am Ende des Prozesses betätigen. Das Team hat also eine neue Evaluierungsfähigkeit vorgestellt, die menschliche Feedback einbezieht und eine end-to-end-Methodik bietet, die von der Verarbeitung von unordentlichen Tweets bis hin zu handhabbaren Ergebnissen für Menschen umfasst.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme umfassen die Anzeige von Likert-Skala-Werten auf der Niveau der Runde, die Anzeige von Likert-Skala-Werten auf der Niveau des gesamten Dialogs und die Vergleich von Dialogen in einem Paar.</sample>
    <sample id="243">Fünf Autoren sind an der Arbeit beteiligt: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.</sample>
    <sample id="244">Im Beispiel benötigt man Hintergrundwissen über die Rolle eines Richters und die Funktion eines Fernsehgeräts.</sample>
    <sample id="245">Das Team hat eine Methode vorgestellt, um hochvereinbarte Arbeiter auf Amazon Mechanical Turk (MTurk) für die Aufgabe der Zusammenfassung zu finden. Sie haben ein zweistufiges Qualifizierungsverfahren eingerichtet: das Qualifizierungs-Task und das Dauerleistungs-Task. Das Qualifizierungs-Task prüft die Fähigkeit der Arbeiter, mehrere Aspekte korrekt zu bewerten, während das Dauerleistungs-Task ihre Kapazität zum Bearbeiten von großen Mengen überprüft. Durch diese Methode haben sie 4 Gold- und 8 Silberarbeiter gefunden, die einen hohen Vereinbarkeitsgrad in Bezug auf den IAA (Interannotator Agreement) erreichen konnten. Diese Arbeiter können auch mit Experten auf Gleicher Ebene agieren. Die Methode ermöglicht es, Ressourcen zu sparen und gleichzeitig eine hohe Qualität an Arbeiten zu gewährleisten.</sample>
    <sample id="246">Ja, der Code ist verfügbar. Er kann auf GitHub gefunden werden.</sample>
    <sample id="247">Jiho Kim von KAIST AI präsentiert das Paper "FACTKG: Faktensicherung durch Vernunft auf Basis von Wissensgraphen". Es gibt keine vorliegende Datenbank, die Fakten über Kontext- oder Tabellen-Daten stützt. Daher haben sie eine neue Aufgabe vorgeschlagen: Wissensgraphenbasierte Faktensicherung. Diese ermöglicht eine sichere Faktensicherung, da es keine weitere Interpretation für den Beweis erforderlich macht. Sie präsentieren das Dataset FactKG, das Faktensicherung durch Vernunft auf Basis von Wissensgraphen ermöglicht. Das Dataset umfasst Fakten in zwei Stilen (schriftlich und colloquial) und zwei Labels (unterstützt oder widerlegt). Die Aufgabe besteht darin, Beweise aus dem Wissensgraphen DBpedia zu finden und die Faktenglaubwürdigkeit zu überprüfen.</sample>
    <sample id="248">Nein, die Annotatoren für NLPositionality sind nicht in Bezug auf jede demographische Gruppe ausgewogen.</sample>
    <sample id="249">Sätze innerhalb der akzeptablen Domain wurden durch das Hinzufügen von Grammatikern aus der Adjunct-Island-Unterlage und das Erstellen eines Präfixes durch die Verwendung von Grammatikern aus der Adjunct-Island-Unterlage oder dem Ungrammatikum erstellt.</sample>
    <sample id="250">Eine dimensionale Bewertung bedeutet, dass man verschiedene Aspekte eines Themas oder einer Handlung untersucht und bewertet. In diesem Fall wird die Qualität von Chat-Modellen auf mehreren Ebenen bewertet, wie z.B. relevante Informationen, Kontrasten, Irrationale oder Empathie. Diese Einstufung ermöglicht eine genauerere und fundiertere Bewertung der Leistung der Chat-Modelle.</sample>
    <sample id="251">University of Science and Technology of China</sample>
    <sample id="252">Das Vortrag "U-CREAT: Unsupervised Case Retrieval using Events extrAcT" von Sai Kiran Tanikella und seinen Kollegen präsentiert zwei Hauptbeiträge zur Aufgabe der Prior Case Retrieval (PCR) in der Rechtswissenschaft. Diese Aufgabe besteht darin, relevante Dokumente aus einem Kandidatenpool zu identifizieren, die sowohl relevant für den Query-Dokument sind als auch zitiert wurden. Die Hauptbeiträge der Forschung sind das Indian Legal Prior Case Retrieval Dataset (IL-PCR) und die U-CREAT Pipeline.

Das IL-PCR Dataset ist ein neu erstellter Referenzpunkt für PCR-Aufgaben, bestehend aus 7.070 Rechtselementen mit einer durchschnittlichen Anzahl von 6,775 Zitierungen pro Query-Dokument. Es bietet eine umfassende Testumgebung für die Bewertung der Leistung von PCR-Algorithmen. Im Vergleich zum bestehenden Dataset COLIEE’21 hat IL-PCR einen größeren Pool an Dokumenten, längere Dokumente, ein größeres Wörterverzeichnis und mehr Zitierungen.

Die U-CREAT Pipeline integriert die Unsupervisierte Lernmethode und bietet eine eventbasierte Ansatz für PCR-Aufgaben. Sie zeigt eine hervorragende Retrievaleffizienz, eine niedrige Infraktionstime und eine allgemeine Anwendungsfähigkeit für indische und kanadische Rechtsysteme, ohne spezifische Anpassungen für das Recht oder die Demographie zu benötigen. Das Event Extraction-Modul der Pipeline verwendet eine Abhängigkeitsanalyse, um Ereignisse aus Dokumenten zu extrahieren, was für die Retrievalaufgaben von entscheidender Bedeutung ist.

Die Forschung hat verschiedene Modelle experimentiert, einschließlich Count-based-Modelle, Transformer-Modelle und Event-based-Modelle, um die PCR-Leistung zu bewerten. Event-based-Modelle zeigten eine signifikant höhere Leistung im Vergleich zu Count- und Transformer-Modelle. Insbesondere war die Methode mit Event Filtered Documents die beste performerische Methode, wobei sie eine hervorragende Leistung bei der Reduzierung der Infraktionstime und einer erhöhten F1-Score aufweist.

Zusammenfassend ermöglicht U-CREAT neue Möglichkeiten für die Entwicklung und Verbesserung der PCR-Aufgabe in der Rechtswissenschaft.</sample>
    <sample id="253">Das Team von Mexiko und Spanien hat eine neue Technologie entwickelt, die es ermöglicht, Indizien für psychische Störungen in sozialen Medien zu erkennen. Der Ansatz besteht darin, ein allgemeines Sprachmodell auf soziale Medien zu trainieren und dann speziell auf die Störungsmerkmale zu fokussieren. Durch das Verarbeiten von Texten aus sozialen Medien können Indizien für Depressionen, Angst oder andere Störungen identifiziert werden. Diese Information kann potenziell dazu beitragen, die Einnahme von Behandlung zu optimieren und die Leistung von Behandlungsmaßnahmen zu verbessern.</sample>
    <sample id="254">Das Forschungsarbeit "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction" präsentiert eine neue Methode zur Verbesserung der Labelqualität bei der extraktion von Beziehungen zwischen Entitäten in Dokumenten. Der Ansatz besteht darin, eine document-level Relation Extraction (DocRE)-Modell mit einem blendenden DS- und menschenannotierten Dataset zu trainieren, um Pseudolabellen zu generieren. Um den Effekt von Falschpositivs zu reduzieren, wird eine unsicherheitsbasierte Label-Denoising-Methode eingesetzt, die die Unschärfe für überlappende Beziehungen schätzt und eine dynamische Klasse-Überwachungsstrategie entwickelt, um die DS-Label zu filtern. Das Ergebnis ist eine verbesserte Leistung bei der DocRE, wie die im Vergleich zu anderen Baselines erzielten Ergebnisse demonstrieren.</sample>
    <sample id="255">Die Form des Prompts hat in den Fällen, in denen nur wenige Prompt-Beispiele verwendet werden (eine oder zwei), eine bedeutende Bedeutung.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art chatmodelle evaluiert.</sample>
    <sample id="258">In diesem Video beschreibt Chiang Cheng-Han, wie Large Language Models (LLMs) zur Bewertung von Texten in natürlicher Sprache eingesetzt werden können. Sie verwenden LLMs, indem sie ihnen Anweisungen geben und diese Anweisungen verwenden, um die Bewertungen der Texte zu generieren. Chiang Cheng-Han betont, dass die Verwendung von LLMs für die Bewertung eine neue und innovative Methode ist, da es keine vorherigen Beispiele gibt, die dies untersuchen. Sie betonen auch, dass LLMs aufgrund ihrer großen Parameteranzahl und ihrer Fähigkeit, natürliche Sprach-Anweisungen zu verstehen, eine effektive Alternative zu menschlicher Bewertung bieten könnten. Chiang Cheng-Han zeigt Ergebnisse an, bei denen LLMs und menschliche Evaluatoren den gleichen Texten zustimmen, was darauf hindeutet, dass LLMs in bestimmten Fällen eine solide Alternative zu menschlicher Bewertung sind.</sample>
    <sample id="259">Das Team von Penn State University hat eine neue Plattform namens XSemPLR vorgestellt, die für den Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen konzipiert wurde. Diese Plattform umfasst 9 Datenmengen aus verschiedenen Anwendungen, 5 Semantik-Übersetzungsaufgaben, 8 Bedeutungsrepräsentationen und 22 Sprachen aus 15 Sprachfamilien. XSemPLR bietet eine einheitliche Datenquelle für die Untersuchung und Vergleich von Modellen zur Semantikübersetzung in mehreren Sprachen und Bedeutungsrepräsentationen. Durch die Verwendung von verschiedenen Settings wie Translate-Test, Monolingual-Modell, Monolingual-Few-shot-Setting, Multilingual-Model, Cross-lingual Zero-shot-Transfer und Cross-lingual Few-shot-Transfer können die Leistungen unterschiedlicher Sprachmodellierungen im Bereich der Semantikübersetzung in mehreren Sprachen und Bedeutungsrepräsentationen untersucht werden. Durch die Analyse dieser Ergebnisse können Forscher und Entwickler neue Erkenntnisse und Verbesserungen an diesen Modellen gewinnen, was zu einer besseren Leistung und Effizienz bei der Semantikübersetzung in mehreren Sprachen führen kann.</sample>
    <sample id="260">There are three authors involved in the work.</sample>
    <sample id="261">Ein guter Planer sollte Skripte schreiben, die sinnvoll und den Anforderungen der spezifischen Aufgaben gerecht werden.</sample>
    <sample id="262">4</sample>
    <sample id="263">Das Team hat eine Analyse der Labelbiase im In-Context-Learning vorgelegt. Diese Biase kann zu Instabilität und Unreliabilität der Modelle führen, da die Auswahl und Reihenfolge von Beispielen den Modellverhalten beeinflussen können. Durch eine Klassifizierung der Labelbiase wurden neue Typen identifiziert, darunter auch die Domain-Label-Biase, die auf die Korpusauswahl des Tasks zurückzuführen ist. Um diese Biase zu neutralisieren, wurde eine neue Kalibrierungsmethode vorgestellt, die sich auf das Domain-Context-Prinzip basiert. Diese Methode verwendet zufällige Domänenwörter aus dem Task-Korpus, um die Biase des Modells zu korrigieren. Durch das Testen dieser Methode auf verschiedenen Modellen und Datenstrukturen wurde gezeigt, dass sie die Leistung des In-Context-Learns verbessert.</sample>
    <sample id="264">Das Vortrag über "TAVT: Towards Transferable Audio-Visual Text Generation" von Lin Wang, einem Student an der Zhejiang University in China, diskutiert das Problem der Mehrmodulartextgenerierung und die Herausforderungen, die sich dabei ergeben. Der Vortrag zeigt, dass die Mehrmodulartextgenerierung, insbesondere Audio-Visuelle Textgenerierung, aufgrund der schwierigen Datennotizenierung und der Variationen in verschiedenen Bereichen zu einer degradierten Leistung kommt. Um diese Grenzen zu überwinden, hat Lin Wang eine neue Aufgabe vorgeschlagen, die "Transferable Audio-Visual Text Generation" heißt. Diese Aufgabe zielt darauf ab, ein Modell zu trainieren, das auf wenigen gekennzeichneten Daten schnell an neue Mehrmodulare Bereiche anpassen kann. Das Vortrag beschreibt die allgemeine Struktur des Vorgehens, einschließlich eines Audio-Visuellen Meta-Mapper-Netzwerks, eines Audio-Visuellen Encoder-Generators und eines Dual-Counterfactual Contrastive Learning (DCLL). Die experimentellen Ergebnisse zeigen, dass TAVT alle vergleichenden Methoden bei der Audio-Visuellen Textgenerierung erheblich besser als die anderen darstellt.</sample>
    <sample id="265">The speaker is Vasudha, a PhD candidate in Computer Science at Stony Brook University.</sample>
    <sample id="266">The authors belong to the University of Warsaw.</sample>
    <sample id="268">Omission errors.</sample>
    <sample id="269">ABC-Eval ist eine neue, dimensionale Ansatzweise zur Bewertung von Conversational AI. Dieses Projekt wurde von der Emory NLP Lab, unter der Leitung von Professor Jinho Choi an Emory University, in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Wenn Sie ein Dialogmodell entwickelt haben und möchten es gegenüber den aktuellen Standards messen, ist die übliche Praxis, menschliche Bewertungen zu erhalten, wie z.B. indem Sie Menschen dazu beauftragen, zwischen zwei Gesprächen zu wählen oder sie auf einem Likert-Skala zu bewerten. Diese Ansätze sind gut für eine allgemeine Bewertung des Dialogqualitätsspiegels geeignet, aber sie haben viele Aspekte des Dialogs nicht abdecken. Daher möchten Sie mehrere Dimensionen des Dialogqualitätsbewertens bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.

Ein einfacher Ansatz wäre es, menschliche Bewertungen für mehrere Aspekte des Dialogqualitätsbewertens zu erhalten, wie zum Beispiel die Relevanz der Modulrufe, mithilfe bestehender vergleichender oder Likert-Skalenmethoden. Wir glauben jedoch, dass eine genauer und zuverlässiger Ansatz zur Dimensionalen Dialogbewertung vorhanden ist. Unser Ansatz versucht, die Subjektivität menschlicher Bewertungen zu reduzieren, indem wir spezifische Verhaltensweisen im Chat anzeichnen, wie zum Beispiel die Verwendung von Irrelevantinformationen oder Widersprüchen. Wir nennen diesen Ansatz ABC-Eval.

Wir haben diese Methode entwickelt, um eine umfassende Überlappung der von kürzlich vorgeschlagenen Arten von Fehlern im Chat zu bedeuten, die das Dialogqualitätsgut beeinflussen können. ABC-Eval kann die Fähigkeit messen, in welchem Maße Chat-Modelle verschiedene Thematikfehler begehen werden. Zum Beispiel misst ABC-Eval die Anzahl der Runden, in denen ein Chat-Modell seinem Gesprächspartner verliert oder etwas Irrelevantes sagt, Widersprüche selbst oder seinem Gesprächspartner macht, irgendeine Falschinformationen ausstellt oder die allgemeine Wissenlage verletzt, und ob das Modell Erfolg hat, Empathie zu zeigen. Um zu bestimmen, welche Bewertungsstrategie am effektivsten ist, haben wir vier state-of-the-art Chat-Modelle auf 100 mensch-bot-Dialogen pro Modell mit ABC-Eval-evaluated. Für Vergleichswirkung haben wir diese Dialoge auch mit drei bestehenden Methoden bewertet: Likert-Skalenbewertungen auf der Runden- und Dialogebene sowie Dialog-Level-Pairwise-Comparisons.

Für jeden der bestehenden Methoden haben wir Bewertungen für acht häufig bewerteten Aspekten des Dialogs sammelt, da dies der Standardpraxis für die Bewertung von Chat-Modellen ist. Durch unsere Analyse dieser Bewertungsdaten haben wir festgestellt, dass die ABC-Eval-Verhaltensbezeichnungen im Allgemeinen zuverlässig und zuverlässiger als die Labels, die durch die bestehenden Methoden gesammelt wurden, sind, wie sie durch Interannotatorenvereinbarkeit auf 100 zweideutig beschrifteten Dialogen gemessen werden. Darüber hinaus sind die ABC-Eval-Labels besser vorhersagbar für das Gesamtniveau der Dialogqualität als die Metriken, die durch die bestehenden Methoden generiert werden, wie dies durch eine einfache lineare Regression-Analyse demonstriert wird. Zum Beispiel erklären die Anteile der Runden mit Selbst- und Partner-Widersprüchen etwa 5% und 10% der Qualität, während die durchschnittlichen Likert-Konsistenz-Scores nur etwa 4% oder weniger erklären.

Zuletzt haben wir überprüft, ob jeder Bewertungsmerkmal ein einzigartiger Aspekt der Dialogqualität abdeckt, indem wir eine Schrittweise lineare Regression durchführten. Sie können sehen, dass die Kombination aller ABC-Eval-Merkmale etwa 25% der Qualität erklären, und wenn Sie jedes Merkmal entfernen, verlieren Sie einen bedeutenden Teil der Informationen über die Qualität. Im Gegensatz dazu erklären die Kombination aller turnbasierten Likert-Merkmale etwa 4% der Qualität und weniger Merkmale tragen ein einzigartiges Aspekt der Qualität bei. Diese zuverlässigen, informellen und einzigartigen Merkmale der ABC-Eval ermöglichen es uns, Conversational AI mit einer höheren Auflösung zu bewerten als die vorherigen Methoden. Sie können sehen, dass mehrere Herausforderungen immer noch bestehen und genau gemessene und zuverlässige Bewertungsmerkmale benötigt werden. Die Bots, die wir untersuchten, haben etwa 20% der Antworten mit offensichtlichen Wissenfehlern, etwa 15% der Antworten mit Irrelevanz und etwa 10% der Antworten mit Widersprüchen selbst oder ihrem Gesprächspartner.

Mit dem schnellen Fortschritt im Bereich können diese Fehlerwerte in neuen Modellen, die seit unserer Bewertung veröffentlicht werden, möglicherweise fallen. Allerdings ist dies ein weiterer Grund, um sich auf zuverlässige und genaue Bewertungsmerkmale für die Vergleich von Modellen zu konzentrieren. Wir hoffen, dass ABC-Eval für andere im Bereich als bedeutendes Schritt in diesem Richtung genutzt werden kann. Wir freuen uns darauf, wie Conversational AI im kommenden Monat und Jahr weiterentwickelt wird.</sample>
    <sample id="270">Die Autoren gehören an Emory University.</sample>
    <sample id="271">CFT steht für "Continual Fine-Tuning" und bezieht sich auf eine Methode, bei der ein Vorgehen verwendet wird, um die Leistung von Modellen zu verbessern, indem sie mit einem kleinen Dataset über einen Zeitraum weitertrainiert werden. Diese Methode ermöglicht es, dass Modelle effizient anhand von neuem Datenmaterial angepasst werden können, ohne dass das gesamte Trainingsdataset neu geladen werden muss.</sample>
    <sample id="272">Es sind sechs Autoren an der Arbeit beteiligt: Koustav Sinha, John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences und Roger Levy.</sample>
    <sample id="273">Willkommen, ich bin Kayo Yin und werde unsere Arbeit "Wann benötigt Übersetzung Kontext? Eine datenschrittliche, multilingualische Untersuchung" präsentieren. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig erstellt. Viele Übersetzungen hängen von Kontext ab. Zum Beispiel würde die Übersetzung des Wortes "Mole" in diesem Satz anders sein als in diesem Satz. Wenn der vorherige Satz lautete: "Dinge könnten gefährlich werden, wenn die Minister herausfinnen", dann bedeutet "Mole" ein Spion. Aber wenn der vorherige Satz lautete: "Können es irgendetwas ernsthaftes sein, Doktor?", dann bedeutet "Mole" eine Hautflecken. Das bedeutet, dass je nach Kontext die Bedeutung eines Wortes sich ändert und daher auch seine Übersetzung. Dennoch ist es schwierig, wie gut Modelle diese Fälle übersetzen können. Zunächst einmal sind nur eine kleine Brüche der Übersetzungen von Kontext abhängig, was es schwierig macht, Korpus-Level-Metriken wie BLEU zu bewerten, die diese Übersetzungen nicht erfassen können. Einige Leute haben vorgeschlagen, spezielle Bewertungen für kontextabhängige Übersetzungen durchzuführen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachpaare, da sie meistens auf Fachwissen und menschliche Curierung angewiesen sind. In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten: Erstens, wann benötigt Übersetzung Kontext? Und zweitens, wie gut handhaben Modelle diese Fälle? Um die erste Frage zu beantworten, haben wir begonnen, den Grad an Kontext während der Übersetzung zu messen. Vorher haben wir CXMI als Maßstab für Kontextnutzung bei maschinellen Übersetzungsmodellen eingeführt. CXMI messt die Information, die das Kontext C über die Zieltabelle Y gegeben der Quelltableau X bereitstellt. Man kann CXMI als die Information erfasst, die man durch das Geben von Kontext dem Modell erhält. Wir haben CXMI auf Punktwise CXMI ausgeweitet, um Kontextnutzung auf der Satellenebene oder auf der Wortebene zu messen. Worte, die hohe P-CXMI haben, sind solche, die Kontext benötigen, um übersetzt zu werden. Wir analysierten Wörter mit hoher P-CXMI und suchten nach Muster zwischen diesen Wörtern. Wir haben unsere Analyse auf Sprechertranskripten durchgeführt, die von Englisch ins 14-malige andere Sprachen übersetzt wurden. Wir durchschnitten unsere Analyse auf drei Ebenen: Zuerst betrachteten wir Partikuläreinstellungen, die hohe Durchschnittsp-CXMI aufweisen. Dies ermöglicht uns zum Beispiel die Identifizierung von Dualpronomen im Arabischen, die relativ hohe P-CXMI aufweisen. Dies kann erklärt werden, weil Englisch keine Dualpronome hat, und man braucht Kontext, um festzustellen, ob ein Pronomina dual ist, wenn man es ins Arabische übersetzt. Wir fanden auch, dass einige Sprachen Kontext benötigen, um die richtige Verbenform zu wählen. Zweitens betrachteten wir Wörter, die hohe P-CXMI durchschnittlich über ihre verschiedenen Auftretungen haben. Dies hilft uns, Fälle zu identifizieren, wie diese hier dargestellt wird, bei denen im Chinesischen Kontext benötigt wird, um Propernomen zu übersetzen, damit man denselben Übersetzungsvarianten innerhalb des Dokuments verwendet. Zuletzt betrachteten wir einzelne Token, die hohe P-CXMI aufweisen. Dies ermöglicht uns, Phänomene zu identifizieren, die nicht durch das Wort allein ausgedrückt werden können, sondern vielmehr durch die Satzstruktur, wie z.B. die Ellipsenresolution. Wir verwenden unsere Findungen für die Analyse, um einen Referenzrahmen für die Dokumentübersetzung zu erstellen. Wir erstellten Tagger, die verschiedene Diskursphänomene automatisch identifizieren können, indem sie die MuDA-Tagger verwenden. Wir konnten dann die Tagger auf einem parallelisierten Dataset anwenden und die Übersetzungsmetriken auf die kontextabhängigen Beispiele anwenden, die von den MuDA-Tagger identifiziert wurden. Schließlich verwenden wir unseren Referenzrahmern, um verschiedene Übersetzungssysteme zu evaluieren und finden, dass kontextabhängige Systeme im Vergleich zu kontextunabhängigen Systemen bei bestimmten Diskursphänomenen wie Formelles und Lexikalischer Kohärenz besser sind. Allerdings zeigen diese Modelle nicht viel Verbesserung bei anderen Diskursphänomenen wie Ellipsen, Pronomen und Verbenformen. Dies suggeriert, wo wir mehr Fortschritte für die Dokumentübersetzung sehen müssen. Wir verglichen auch verschiedene kommerzielle Systeme und fanden, dass DeepL im Vergleich zu Google Translate in der Regel für die Dokumentübersetzung besser ist. Zusammenfassend führen wir eine datenschrittliche Analyse über 14 Sprachpaare durch, um zu ermitteln, wann Übersetzung Kontext benötigt, und nutzen unsere Ergebnisse, um einen Referenzrahmen für die Dokumentübersetzung zu erstellen. Danke für eure Aufmerksamkeit.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Das Team von Ananya und Vignesh präsentiert ihre Arbeit "IndicMT Eval: Eine Datensammlung zur Meta-Evaluation von Übersetzungsmetriken für indische Sprachen". Sie untersuchen die Korrelation zwischen verschiedenen Übersetzungsmetriken und menschlichen Bewertungen für Übersetzungen aus Englisch ins Deutsche. Ihre Studie konzentriert sich auf fünf indischen Sprachen, darunter Tamil, Malayalam, Hindi, Marathi und Gujarati. Sie verwenden die Flores-Datensammlung und generieren mehrere Übersetzungsvarianten für jede Quellsprache. Bilingual Experten bewerten diese Übersetzungen, indem sie Fehler, ihre Art und Schwerkraft markieren und eine Gesamtbewertung geben. Die Studie zeigt, dass IndicCOMET, ein abgefinefter Version des COMET-Metrics, die mit ihrem Dataset trainiert wurde, die besten Korrelationswerte zu den menschlichen Bewertungen erzielt. IndicCOMET zeigt auch eine höhere Robustheitsleistung im ACES Translation Accuracy Challenge Sets.</sample>
    <sample id="277">Multiset Tagging and Latent Permutations</sample>
    <sample id="278">Die Autoren beschreiben die Methode der "markierten Wörter" als eine Methode, um die Wörter zu identifizieren, die die Unterschiede zwischen markierten Gruppen und unmarkierten Gruppen betonen. Sie basieren auf der sociolinguistischen Begriff "Markierung", der besagt, dass es eine unmarkierte Standardform gibt, und jede Gruppe, die sich von dieser abhebt, wird sprachlich markiert. Die Autoren verwenden diese Methode, um die Wörter in den generierten Persönlichkeiten zu untersuchen, die von dem Modell generiert wurden, und um die Wörter zu identifizieren, die die Unterschiede zwischen den markierten Gruppen und den unmarkierten Gruppen betonen.</sample>
    <sample id="279">The authors belong to the University of Washington.</sample>
    <sample id="280">Das Paper "MultiEMO: Ein Aufmerksamkeitsbasiertes Korrelationbewusstes Mehrmodul-Fusion-System für Emotionserkennung in Gesprächen" präsentiert eine neue Methode zur Emotionserkennung in Gesprächen. Das System, namens MultiEMO, integriert visuelle, akustische und textuelle Informationen miteinander. Es besteht aus vier Hauptkomponenten: unimodulare Feature Extraktion, Kontextmodellierung, Mehrmodul-Fusion und Emotionskennung. Der visuelle Teil des Systems, der als VisExtNet bezeichnet wird, extrahiert visuelle Emotionen ohne die Integration von unbedeutenden visuellen Umgebungsinformationen. Der Mehrmodul-Fusionprozess, der als MultiAttn bezeichnet wird, verwendet bidirektionale multi-head-Kreuzattnormen, um die Zusammenhänge zwischen den drei Modalen zu lernen. Eine Fokal-Kontrastverlustfunktion, die als SWFC bezeichnet wird, hilft bei der Unterscheidung von schwierig zu klassifizierenden Emotionen. Experimente zeigen, dass MultiEMO die Besten auf zwei Referenzdatenbanken, MELD und IEMOCAP, erreicht hat.</sample>
    <sample id="281">Das Team von Kayo Yin hat eine Forschungsarbeit vorgelegt, die "Wann benötigt Übersetzung Kontext? Eine datenschwimmende, multilingualische Untersuchung" heißt. Die Arbeit wurde mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig in Zusammenarbeit erstellt. Der Schwerpunkt der Forschung liegt auf der Bedeutung des Kontexts bei der Übersetzung. Wenn es sich um die Übersetzung von Worten wie "Mole" handelt, kann der Kontext den Sinn beeinflussen. Zum Beispiel bedeutet "Mole" in einem Satz wie "Dinge könnten gefährlich werden, wenn die Minister herausfinden", einen Spion, während es in einem anderen Satz wie "Können es irgendetwas ernsthaftes sein, Doktor?" eine Hautflecken bedeutet. Diese Unterschiede machen es schwierig, Übersetzungen zu bewerten, insbesondere wenn sie auf Kontext abhängen. Der Forschungsbereich untersucht, wann Übersetzung Kontext benötigt und wie gut verschiedene Übersetzungssysteme diese Situationen meistern. Sie haben ein Multilinguelles Diskursbewusstes, oder MuDA-Tagger, entwickelt, um spezifische Diskursphänomene in Texten zu identifizieren. Durch die Anwendung des MuDA-Taggers auf eine Parallelübersetzungsdatenbank können sie die Leistung unterschiedlicher Übersetzungssysteme für documentale Übersetzungsbewertungen anpassen. Das Ergebnis zeigt, dass kontextabhängige Übersetzungssysteme in bestimmten Diskursphänomenen besser leisten, aber nicht immer besser als kontextunabhängige Systeme.</sample>
    <sample id="282">Das Forschungsprojekt "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing" präsentiert eine neue Methode zur Textstilübersetzung auf der Diskursniveau. Es untersucht die Herausforderungen bei der Übersetzung von Stilen in langer Texte und entwickelt ein Modell, das Diskurselemente aus den Quellen extrahiert und sie mit stilbezogenen Embeddings kombiniert. Die Methode umfasst zwei Trainingsphasen: eine, die sich auf die Übersetzung von Stil-Spezifika konzentriert, und eine, die auf die Inklusion von Stil-Spezifiken im Text. Das Experimentierungsresultat zeigt, dass StoryTrans die Stilkontrolle und Inhaltspräzision besser hervorbringt als Konkurrenten.</sample>
    <sample id="283">Lisa, Bart und Maggie</sample>
    <sample id="284">The presentation introduces FSUIE, a novel fuzzy span mechanism for enhancing universal information extraction. The current span-based UIE models rely on precise boundary positions of annotated spans but face ambiguity in labeling these boundaries. To address this issue, the proposed method uses a fuzzy span loss to model the span as a continuous distribution of correct probability within a specific range (R-min and R-max). This approach allows for more flexible and adaptive attention distributions during span extraction decisions.

To further improve performance, FSUIE incorporates a fuzzy span attention layer that dynamically adjusts the length of full attention ranges based on an optimizable parameter delta. Additionally, it ensures linear decay rather than truncation at the attention span boundaries. These modifications help guide the module's decision process without compromising its text encoding capabilities.

Experiments demonstrate that FSUIE achieves significant improvements across various tasks such as named entity recognition, relationship extraction, and aspect sentiment triplet extraction compared to traditional UIE models with or without fuzzy span mechanisms. Results show enhanced convergence speed, better utilization of annotation information, and stronger generalization capabilities for domain-specific data. Overall, FSUIE demonstrates competitive performance and versatility in handling different types of information extraction tasks.</sample>
    <sample id="285">Das Video präsentiert eine Forschungsarbeit von Mingqi Gao aus der Peking University, die "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework" tituliert ist. Die Forscher untersuchen die Problemstellung von Faktlichkeit in Dialogsummarisierungen und betonen die Bedeutung eines korrekten Faktlichkeitsspektrums. Sie argumentieren, dass die aktuellen Faktlichkeitsschätzer nicht genug Berücksichtigung geben, und schlagen vor, eine umfassendere Bewertung durch die Verwendung von manuell angepassten Referenzkorrekturen vor. Sie entwickeln eine neue Klassifizierung von Faktlichkeitsfehlern und präsentieren ihre Evaluation-Framework, das auf dem ERRANT-System basiert. Durch die Verwendung von Referenzkorrekturen während des Trainings von FEC-Modellen für Dialogsummarisierung erzielen sie bessere Ergebnisse.</sample>
    <sample id="286">James Finch, Sarah Finch</sample>
    <sample id="287">Javad Hosseini, Filip Radlinski, Silvia Pareti, Annie Louis</sample>
    <sample id="288">Zum Testen von syntaktischen Phänomenen können Datensätze wie Adjunct Island, SyntaxGym oder Wikipedia verwendet werden.</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind: FTw, COSINE, WSL, WSL+ and WSL++.</sample>
    <sample id="291">11</sample>
    <sample id="294">CamemBERT wurde mit 7 GB von NACHOS trainiert.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Die Forscher haben eine Datenbank namens EPIC (English Perspectivist Irony Corpus) entwickelt, um die Interpretation von Ironie zu untersuchen. Sie haben Texte aus verschiedenen Quellen und Sprachvarianzen sammelt, um sie für die Annotierung zu verwenden. Die Annotatoren haben unterschiedliche Meinungen über die Ironie, was auf die Bedeutung der Perspektive hinweist. Durch das Training von Modellen auf den Annotatoren wurden die Unzertsecutiveit der Modelle verbessert.</sample>
    <sample id="297">Das Projekt "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" untersucht dogwhistles, Begriffe, die eine zweite, oft tabellarische oder diskriminierende Bedeutung für eine In-Gruppe haben, während sie für eine Out-Gruppe keine erkennbare Bedeutung haben. Dogwhistles können als Form der unberechtigten Persönlichkeitsauslegung verwendet werden, um Hass und Abneigung zu fördern, ohne dass sie offensichtlich ausgesprochen werden. Das Team hat ein Typologie und Glossar von über 340 Begriffen entwickelt, insbesondere von rassistischen, transphobischen und antijüdischen dogwhistles, sowie einen Fallstudienverlauf der historischen amerikanischen politischen Reden. Durch die Analyse von GPT-3, einem Sprachmodell, konnten sie feststellen, dass das Modell dogwhistles, insbesondere in formeller Sprache, erkennen kann, aber seine Leistung variiert stark. Darüber hinaus zeigt die Forschung, dass dogwhistles bei der Vermeidung von Inhaltmoderation im Online-Umfeld helfen können, indem sie Toxicity-Detector-Scores ändern, wenn Standardgruppenbeschränkungen oder Schimpfereien durch dogwhistles ersetzt werden.</sample>
    <sample id="298">Die Ergebnisse der Studie führten zu der Schlussfolgerung, dass diezeitliche Verzögerung die Hauptursache für den Leistungsverlust war. Dies wurde durch eine Experimentierung bestätigt, bei der einige Modelle weiter mit neuem, mehr aktuelleren Daten trainiert wurden. Es wurde festgestellt, dass das Leistungsniveau mit größerer Zeitabstimmung zwischen der Trainings- und Testdatenlage abfiel. Diese Beobachtung bestätigte die vermutete Ursache des Leistungsverlusts, nämlich die zeitliche Verzögerung oder Temporaldrift.</sample>
    <sample id="299">The speaker introduces their work on improving the robustness of NLI models with minimax training. They explain that current methods rely on auxiliary models to mitigate shortcuts, but these require domain-specific knowledge and may not accurately predict uncertainty or be computationally efficient. The proposed method uses a minimax objective between a learner model and an auxiliary model, focusing on under-represented hard examples for better generalization in out-of-distribution scenarios. Results show improvements over baseline ERM training and shortcut mitigation techniques across various datasets.</sample>
    <sample id="300">Interactive dictation ist eine neue Aufgabe, die es ermöglicht, Text durch Sprache zu übertragen und gleichzeitig Korrekturen zu vornahmen. Diese Aufgabe wurde von Semantic Machines in Zusammenarbeit mit Jason Eisner, Adam Pauls und Sam Thomson entwickelt. Interactive Dictation unterscheidet sich von traditionellen Speech-to-Text-Anwendungen, da sie keine festen Triggerwörter oder Kommandos benötigt, sondern stattdessen natürliche und offene Sprach utterances für die Ausführung von Korrekturen verwendet. Der Prozess der Interactive Dictation umfasst vier Schritte:</sample>
    <sample id="302">Es ist notwendig, die Token für die Ausgabesequenz zu permutieren, um sicherzustellen, dass die Token in der richtigen Reihenfolge im Output angeordnet werden. Dies ermöglicht es dem Modell, die korrekte Ausgabe zu generieren, indem es die Token aus den Multisets ordnet, die für die jeweilige Position im Output definiert sind.</sample>
    <sample id="303">Die Autoren empfehlen, dass Entwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, um die Grundlagen für ihre Entscheidungen zu verstehen und die Effektivität ihrer Maßnahmen zu bewerten. Darüber hinaus ist Transparenz für die Untersuchung der Ursachen von Vorurteilen und die Entwicklung von Verbesserungsstrategien für die Modellentwicklung von entscheidender Bedeutung.</sample>
    <sample id="304">Inakzeptablen Minimalpaareingaben sind die, in denen der Kontext nicht mit dem restlichen Satz abstimmt.</sample>
    <sample id="305">Dawei, ein PhD-Studierender an der Saarland University in Deutschland, präsentiert gemeinsam mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow die kürzlich veröffentlichte Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". In diesem Video erklärt er, was Weak-Supervision und Weakly Supervised Learning sind. Weak-Supervision bezieht sich darauf, dass man keine Datenmanuelle Labelierung durchführt, sondern stattdessen benutze Weak-Labeling-Quellen wie einfache Heuristiken, Wissenbanken oder schlechte Crowd-Sourcing. Diese Labels sind kostengünstiger, aber auch lauschig, was bedeutet, dass eine bestimmte Anzahl der Labels falsch sind. Wenn man Neural Networks direkt mit diesen weaken Labels ausreichend trainiert, memorisiert die Netzwerke die Label-Ausreife und generalisiert nicht. In Weakly Supervised Learning werden Trainings-Algorithmen vorgestellt, um robust auf solche Label-Ausreife zu reagieren und damit die trainierten Modelle immer noch gut generalisieren können. In den letzten Arbeiten in der WSL wird häufig behauptet, dass man nur mit weaken Labels Modell trainiert und dann auf sauberen Testdaten hochwertigen Leistungen erreicht. Technisch ist das nicht falsch, aber es gibt einen Hintergrund, der oft übersehen wird: Man braucht eine zusätzliche saubere Validationsdatenbank für die Modellauswahl. Wir haben diese Frage nachgefragt und unsere Erkenntnisse präsentieren. Zunächst finden wir, dass es für die WSL-Methoden saubere Validationsdaten erforderlich sind, sonst gibt es einen großen Leistungsabfall. Zweitens zeigen unsere Ergebnisse, dass eine erhöhte Anzahl von sauberen Validationsdaten die Leistung verbessern kann. Schließlich zeigen unsere Ergebnisse, dass es keine notwendige Notwendigkeit gibt, mehr komplexere WSL-Methoden zu verwenden, da eine fortsetzende Fine-Tuning auf den sauberen Validationsdaten genauso effektiv ist.</sample>
    <sample id="306">Sebastian Schuster und Najoung Kim präsentieren eine Studie zur Untersuchung der Entity Tracking-Fähigkeiten von Sprachmodellen. Sie zeigen, dass viele große Sprachmodelle, insbesondere die Versionen von GPT-3.5, die auf Code trainiert wurden, den Fähigkeiten zustimmen können, Entity State Tracking zu verstehen. Diese Fähigkeit scheint mit der Prätraining auf Code verbunden zu sein.</sample>
    <sample id="307">Die Autoren haben die Bewertungsmetriken für die verschiedenen Modelle verwendet, um sie miteinander zu vergleichen. Diese Metriken sind für die meisten Aufgaben genau die gleichen, wie z.B. die Genauigkeit bei der Identifizierung von benannten Objekten (Named Entity Recognition), die Klassifizierung von Texten (Classification), die Part-of-Speech-Analyse (Part-of-Speech Tagging) und die Antwort auf Fragen (Question Answering).</sample>
    <sample id="308">Das Vortrag über "NLPositionality" präsentiert eine Forschungsarbeit, die sich auf die Positionalität von NLP-Researchern und Modellentwicklern konzentriert. Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Diese Perspektiven können den Prozess der Forschung und seine Ergebnisse beeinflussen. Die Forschungsarbeit untersucht, ob Datasetten und Modelle Positionalität aufweisen, indem sie die Annotationsprozesse mit verschiedenen Annotatoren und den Modellen und Datasetts vergleicht. Durch das Verarbeiten von über 16.000 Annotaten von mehr als 1.000 Teilnehmern aus 87 Ländern wurden Ergebnisse gefunden, die zeigen, dass Datasetten und Modelle in der Regel am besten mit englischsprachigen Ländern und Menschen mit Hochschulabschluss übereinstimmen. Es wird empfohlen, die Designentscheidungen während des Forschungsprozesses zu dokumentieren und NLP-Forschung mit einem Perspektivismusansatz zu betreiben. Darüber hinaus wird empfohlen, spezialisierte Datasetten und Modelle für bestimmte Gemeinschaften zu entwickeln.</sample>
    <sample id="309">Inter-annotator agreement</sample>
    <sample id="310">Wikipedia wurde gewählt, um komplett unzusammenhängende Sätze zu den inakzeptablen und akzeptablen Suchanfragen hinzuzufügen.</sample>
    <sample id="311">The authors belong to the University of Bremen.</sample>
    <sample id="312">MultiInstruct unterscheidet sich von anderen Benchmarks in der Folge: 1. Es ist das erste Multi-Modal Instruction Tuning Benchmark-Dataset, das mehrere Arten von Aufgaben und 10 verschiedenen Kategorien umfasst. 2. Es bietet eine große Anzahl von Experten geschriebenen Anweisungen für jede Aufgabe, was es ermöglicht, die Leistungsfähigkeit des Modells auf einer breiteren Palette von Aufgaben zu testen. 3. Das Dataset wurde speziell für die Untersuchung der Verwendung von Anweisungen für die Verbesserung der Leistung von Multi-Modal Modellen entwickelt, was es ermöglicht, die Bedeutung von Anweisungen für die Verbesserung der Leistung von Multi-Modal Modellen zu untersuchen. 4. Es bietet eine neue Metrik namens Sensitivität, die die Fähigkeit des Modells, die Ausgabe für eine bestimmte Aufgabe zu erzeugen, unabhängig von der Wortwahl der Anweisung, zu messen.</sample>
    <sample id="313">Zwei Autoren sind an der Arbeit beteiligt: James Finch und Sarah Finch.</sample>
    <sample id="314">In der Definition der binären Koordination wird eine Coordination als "binäre Koordination" beschrieben, wenn sie aus zwei Subjekten oder Objekten besteht, die durch einen Koordinierungsverband verbunden sind. Diese Definition ist in dem Text nicht explizit aufgeführt, aber es wird darauf hindeutet, dass die Coordination zwischen zwei Elementen besteht, die durch einen Verband verbunden sind.

Es ist wichtig zu beachten, dass die Definition des binären Koordinations in diesem Zusammenhang auf die beiden Elemente und den Verband zwischen ihnen abzielt, anstatt auf die spezifische Struktur der Coordination. Dieser Ansatz ermöglicht es, eine Reihe von Koordinationsstrukturen zu bedeuten, die alle aufgrund des Verbandes zwischen zwei Elementen zusammenhängen.</sample>
    <sample id="315">Die Prompts in dieser Studie waren im Durchschnitt 18 Wörter lang.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5-Modell mit der CoScript-Datensammlung verbesserte Pläne generieren kann. Es erzielt eine höhere Qualität als die meisten großen Modellen, was nahelegt, dass kleinere Modelle bei geeigneten Datensätzen besser als große Modelle leisten können.</sample>
    <sample id="317">CodeIE ist ein neuer Ansatz zur Verbesserung der Information extraktion (IE) auf Basis von Code Generation. Es umsetzt den IE-Task als Code Generation und nutzt große Code Language Models wie Codex. Diese Methode erzielt eine bessere Anpassung an die IE-Task, da sie den Text in eine strukturierte Form während des Eingangs und eine korrekte Struktur im Ausgang gewährleistet. Das Team hat die Methode auf drei IE-Datenmengen und vier Relationsextraktion-Datenmengen eingesetzt und hat gezeigt, dass CodeIE die traditionellen Modellvarianten überzog.</sample>
    <sample id="318">Hi, ich bin Yanis Labrak und ich werde unsere Arbeit "DrBERT: Ein robustes vorbereitetes Modell auf Französisch für medizinische und klinische Bereiche" präsentieren. Zunächst sprechen wir über Sprachmodellierung in der Gesundheitsversorgung. Dann werden wir unsere Hauptbeiträge präsentieren. Wir präsentieren das erste medizinische Modell auf Französisch namens DrBERT, das auf RoBERTa basiert und mit NACHOS trainiert wurde, einem Datensatz von medizinischen Crawled-Daten von der Webseite. Wir haben auch eine Vergleichung von Modellen mit mehreren Vorbereitungseinstellungen und Datenquellen vorgenommen. Danach präsentieren wir unsere Ergebnisse auf 11 medizinischen und klinischen Downstream-Tasks auf Französisch. Schließlich ziehen wir Schluss zu unseren Experimenten und geben Ihnen weitere Details darüber preis, wie Sie diese Modelle zugänglich machen können. Seit seiner Veröffentlichung im Jahr 2018 hat BERT eine der effektivsten Ansätze zur Lösung natürlicher Sprache verarbeitender Aufgaben geworden und bietet große Leistungssteigerungen gegenüber historischen statischen und kontextuellen Methoden wie Word2vec, fastText oder mehr. Seitdem wurde dieses Modell an viele andere Sprachen angepasst, wie zum Beispiel auf Französisch mit CamemBERT, und auch in den medizinischen Bereichen wie PubMedBERT und BioBERT und im klinischen Bereich mit ClinicalBERT, aber hauptsächlich auf Englisch. Spezialisierte Modelle für andere Sprachen sind selten und werden oft durch kontinuumsbezogenes Vorbereiten aufgrund der Mangel an in-domänneren Daten entwickelt. Allerdings gab es auf Französisch bislang kein offenes Modell für medizinische Zwecke. Also fragen wir uns, welche die am besten geeignete Datenquelle für eine breite Palette von Anwendungen ist, und crawled Daten sind eine gute Ersatzquelle für klinische Daten. Um diese Frage zu beantworten, verglichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten aus dem Datenwerk des Nantes University Hospital basiert. Danach fragen wir uns, wie viel Daten benötigt man, um ein spezialisiertes Modell auf Französisch zu trainieren? Ist es 4 GB, 8 GB oder mehr? Um diese Frage zu beantworten, erstellten wir zunächst vier von scratch-Modelle: eine erste Version von DrBERT mit 7 GB von NACHOS; eine zweite Version mit 4 GB von einer Kombination aus NACHOS; eine erste Version von ChuBERT, das klinische Modell mit 4 GB von Sätzen aus klinischen Notizen; und eine endgültige Version von ChuBERT mit einer Kombination aus 4 GB von einer Kombination aus NACHOS und 4 GB von klinischen Notizen. Insgesamt haben wir sieben Modelle. Um diese Modelle zu bewerten, sammelten wir Daten für öffentliche und private Downstream-Aufgaben wie Namenserkennung, Klassifizierung, Part-of-Speech-Tagging und Antwort auf Fragen. Diese Modelle wurden gegen sechs Baseline-Modelle verglichen, nämlich CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT. Die Bewertung zeigt, dass die Modelle besser auf Aufgaben mit Daten der gleichen Natur als jenen, auf denen das Modell trainiert wurde, leisteten. Wir beobachten auch, dass mehr Transliteration zu besseren Leistungen führt. Im Ganzen haben die von scratch-Modellern die besten Ergebnisse auf neun der 11 Downstream-Aufgaben erreicht und überschritten das Ergebnis des allgemeinen Modells, hier CamemBERT. Wir beobachten auch, dass spezialisierter Daten besser sind, aber nicht in Skalierbarkeit. Alle vorbereiteten Modelle, die auf NACHOS trainiert wurden, sind kostenlos auf Hugging Face verfügbar und unter der MIT-Lizenz, und alle Trainings-Skripte sind auf unserem GitHub-Repository verfügbar. Also danke Ihnen für diese Präsentation und freuen wir uns darauf, im Poster-Sitz in Toronto zu exchangieren.</sample>
    <sample id="319">Die Lernstrategien, die in der Arbeit untersucht werden, sind das Continual Learning und die Anwendung von French data.</sample>
    <sample id="320">Der Faktor der Überanpassung, der auf die Wiederverwendung von Tests zurückzuführen ist, liegt größer als 1.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde auf verschiedenen Ebenen, wie z.B. auf der Ebene des Wortauswuchses, der Struktur und des Gesamtebels der Vereinfachung, analysiert.</sample>
    <sample id="322">Enrico präsentiert auf ACL 23 und diskutiert die Frage, "Was lernt ein Text-Klassifizator über Moralität?" Er erklärt, was Moralität ist und wie es in der Gesellschaft funktioniert. Language Models können moralische Entscheidungen treffen, aber es gibt oft keine klare Definition zwischen moralisch richtig oder falsch. Enrico zeigt, dass Moralkonzepte subjektiv sind und dass verschiedene Menschen unterschiedliche Moralkompassen haben. Er verwendet das Moral Foundation Theory, das fünf grundlegende Moralkomponenten identifiziert, um zu zeigen, wie Menschen Moralität wahrnehmen. Language Models können moralische Entscheidungen treffen, aber sie können nicht immer die Unterschiede zwischen verschiedenen Moralkomponenten erkennen. Enrico verwendet eine Datenbank namens Moral Foundation Twitter Corpus, um zu sehen, ob Language Models diese Unterschiede erkennen können. Er zeigt, dass Language Models die Unterschiede zwischen dem Hashtag #AllLivesMatter und #BlackLivesMatter erkennen können, da sie Wörter wie "Überfall" und "Mayhem" verwenden, um Rebellion gegen Autorität zu beschreiben. Diese Beobachtung zeigt, dass Language Models die Unterschiede zwischen verschiedenen Moralkomponenten erkennen können, aber sie müssen mehr über die Unterschiede zwischen den Moralkomponenten lernen, um eine genaue Interpretation von Moralkomponenten in verschiedenen Kontexten zu erzielen.</sample>
    <sample id="323">Yujie Wang präsentiert ein Paper mit dem Titel "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA". Commonsense QA ist eine Herausforderung für Maschinen, um Fragen zu beantworten, die auf allgemeine Kenntnisse abgestimmt sind und ihre Sprachverständnis testen. Dies erfordert, dass Maschinen relevante Wissen von externen Quellen abrufen. Jüngst hat Holmes denken, dass Wissen sowohl in Sprachmodellen als auch in Wissensdatenbanken gespeichert wird. Viele Arbeiten kombinieren diese beiden Typen von Wissen, um Commonsense QA zu lösen, indem sie relevante Wissen aus der Wissensdatenbank durch Entitätensicherung, das Erstellen eines Subgraphen und dann die Verwendung von Sprachmodellen und GNNs zum Inferieren von Antworten abrufen. Allerdings führen diese Arbeiten bei der Abrechnung von relevanten Entitäten zu noisigen Entitäten wie "Top", "Bank" und "Cat" im Subgraph, die weitgehend unverknüpft zu der aktuellen Frage sind. Darüber hinaus werden die Subgraphen und Texte isoliert, was zu begrenzter Interaktion zwischen den beiden Modalen führt und die Bedeutung der semantischen Beziehung zwischen Entitäten ignoriert. Basierend auf diesen Problemen entwickelt sich Yujie Wang DHLK. Zunächst baut sie einen HKG auf mehreren Wissensdatenbanken auf und nutzt KRL, um die Struktur und die Wissensrepräsentation des HKG zu optimieren. Schließlich verwendet sie ein Sprachmodell, um die Kontexte und Entitäten zu encodern und zu fusionieren. Sie entfernt subwortsgebundene Phraseentitäten mithilfe eines Diccionario-Vokabulars, ruft Paraphrasen von Schlüsselentitäten in WordNet und Wiktionary ab und verbindet sie als zusätzliche Knoten zum Subgraphen, um den HKG zu erstellen. Sie verwenden RoBERTa und Mask Self-Attention, um die Kontexte und Entitäten des HKG zu encodern und zu fusionieren, während sie dynamisch entitäten mit schwächerer Relevanz zur Frage auf Basis der Aufmerksamheitswerte von RoBERTa entfernen. Sie erhalten die Embeddings der Entitäten und Relationen durch Mittelpooling. Da der HKG aus mehreren Triplets besteht, verwenden sie TransE, um die Embeddings der Entitäten und Relationen im HKG zu optimieren. Stattdessen von GNNs verwenden sie Relation Mask Self-Attention (RMSA) zur Modellierung ihrer Subgraphen. RMSA wird inspiriert von RGAT und verwendet Beziehungen in Mask Self-Attention. Sie updaten die Embeddings der Entitäten und Relationen im HKG über L Schichten von RMSA. Schließlich verwenden sie den HKG Graph für Embedding, die Paths und die Embedding des QA-Kontextes nach Path-Enhancement und die Embedding des QA-Kontextes in die MLP einzubringen, um die Wahrscheinlichkeit einer Antwort zu erhalten. Yujie Wang konductiert Experimente auf CommonsenseQA und OpenBookQA mit externen Wissensdatenbanken wie ConceptNet, WordNet und Wiktionary. Sie extrahieren Schlüsselentitäten im QA-Kontext mithilfe von KeyBERT und retrieval von Wissenspfaden innerhalb von zwei Hops in ConceptNet. Sie berichten die Ergebnisse und Leaderboards für CommonsenseQA und OpenBookQA und vergleichen ihre Leistung mit anderen LM und HKG-Methode.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile.</sample>
    <sample id="325">Ja, das klingt sehr interessant. Ich würde gerne mehr über eure Forschung erfahren. Könnten wir uns vielleicht morgen in Eure Poster vorstellen lassen?</sample>
    <sample id="326">Kognitive Dissonanz ist eine Situation, in der zwei Glaubens- oder Handlungsweisen nicht harmonieren.</sample>
    <sample id="327">ManagerTower, ein neuer visuellt-lingwischer Modellarchitektur, verbessert die Leistung durch effektiver Aggregieren von Insights von mehreren unimodalen Experten. Es verfügt über Managers in jedem visuellt-lingwischen Layer, um die semantischen Wissen von verschiedenen Ebenen zu nutzen. ManagerTower erzielt hervorragende Ergebnisse, insbesondere eine 39,15% höhere Genauigkeit im Wikivideo-Teststandard.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">In diesem Vortrag präsentiert Minghang Zheng von der Peking University eine neue Methode für das Zero-Shot-Video-Satzung. Diese Methode, die als "noise-resistant Structured Pseudo-Label generation" bezeichnet wird, zielt darauf ab, ein Video mit einem natürlichen Sprachabfrage zu lokalisieren, indem sie die Start- und Endzeiten eines Videos identifiziert, die mit der Abfrage relevant sind. Der Vortrag beschreibt, wie diese Methode funktioniert, indem sie ein Bildtextmodell verwendet, um komplexe, freiformige Pseudoschlagzeilen für das Video zu generieren. Diese Schlagzeilen werden dann mit dem Bildmaterial des Videos verknüpft, um eine hohe Relevanz zwischen den einzelnen Videoframen und den Pseudoschlagzeilen zu gewährleisten. Darüber hinaus wird die Methode dazu angepasst, die Einfluss von Lärm in den Pseudolabellen zu reduzieren, indem sie die Gewichtung der Labellen anpassen und die Qualität der Schlagzeile verbessern. Das Ergebnis ist eine robuste Methode für das Zero-Shot-Video-Satzung, die auf zwei Datasetten bestätigt wurde, dass sie die besten Leistungen im Vergleich zu anderen Zero-Shot-Methoden erzielt hat.</sample>
    <sample id="330">Ja, das kumulative Training ist besser als das iterative Training für das Aktive Lernen.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus Transkripten von TED-Talks, die auf Englisch 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="333">INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation präsentiert eine neue Training-Framework für NMT-Modelle. Das Framework konvergiert, indem es die Representation-Space des NMT-Modells nach kNN-Knowledge anpasst und die Representation-Space verbessert. Das System erzielt eine erhöhte Performance mit weniger Speicherplatz und schneller Inferenzzeit.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Sprachübergreifender Transfer bedeutet, dass man ein Modell trainiert, um auf mehreren Sprachen zu arbeiten und es dann auf eine andere Sprache übertragen kann.</sample>
    <sample id="337">Das Team hat eine neue Ansatzweise für die Erläuterung von außerhalb des Vokabulars (OOV)-Wörtern entwickelt. Sie haben ein Wortbeziehungsgraph entworfen, das den Prozess der Wortbildung und -verbindung nachahmt. Wenn ein OOV-Wort auftaucht, wird es in kleinere Teile zerlegt und mit relevanten Worten verbunden, um seine Bedeutung zu infizieren. Das Graph-Network hilft dabei, die Bedeutung der Wörter effektiv zu erfassen. Durch das Anwenden eines Graphen-Attention Networks können sie die Informationen von den Wörtern und ihren Beziehungen besser verarbeiten. Durch das Verwendung eines GraphConvolutional Networks können sie die Struktur der Wörter und ihre Beziehungen besser verstehen.</sample>
    <sample id="338">Das Team von Rensselaer Polytechnic Institute, Northeastern University und IBM Research präsentiert ihre Arbeit "Sind menschliche Erklärungen immer hilfreich? Richtlinien zur objektiven Bewertung menschlicher natürlicher Sprachvermittlungen". Die Forscher untersuchen die Qualität menschlicher Erklärungen für maschinelle Lernmodelle. Sie entwickeln eine neue Evaluierungsmethode namens TREU (Task-Related Evaluation of Utilization), die die hilfreichkeit von Erklärungen bei der Fine-Tuning und bei der Inferenz bewertet. Durch die Analyse von fünf großen Datensätzen für verschiedene Aufgaben und die Vergleichung von zwei Modellen, T5 und BART, zeigen sie, dass menschliche Erklärungen bei der Verbesserung des Modellleistungsverhaltens hilfreich sind. Die Forschung bietet einen Grund für eine sorgfältige Bewertung der Qualität von menschlichen Erklärungen in der Zukunft.</sample>
    <sample id="339">Saarland University in Germany</sample>
    <sample id="340">Das Team von UCLA hat eine neue Dataset für Paraphrase Generation vorgestellt, namens ParaAMR. Das Dataset wird durch die Verwendung von AMR (Abstract Meaning Representations) Back-Translation erstellt und bietet eine große Menge an syntaktisch vielfältigen Paraphrasen. Durch das Verarbeiten von AMR-Graphen können die Paraphrasen mehr Variation in ihrer Struktur aufweisen, was zu einer verbesserten Semantik und Syntaxis führt. Das Dataset umfasst etwa 15 Millionen Quentscheide und 6,9 Paraphrasen pro Quentscheid. Es wurde gezeigt, dass ParaAMR bei der Erstellung von Sätzen, die in den Testbenchmarks besser im Vergleich zu anderen Datasetts erworben haben, effektiv ist. Darüber hinaus ermöglicht es ParaAMR, die Qualität von Paraphrase Generatoren zu verbessern und die Leistung bei der Anpassung von Daten für Few-Shot-Learning zu erhöhen.</sample>
    <sample id="341">Die Autoren verwenden die Latenzmessungen "Average Lagging" und "Computational Aware Average Lagging".</sample>
    <sample id="342">The presentation introduces LiveChat, a large-scale personalized dialogue dataset constructed from live streaming videos. It highlights the challenges of existing datasets and proposes an automatic method to create such data. The paper presents experiments on response modeling and addressee recognition tasks, showing that persona information is beneficial for these tasks. They also investigate transfer learning with different LLMs, finding BART's performance superior due to its domain-specific nature. Future work will focus on efficient transfer learning methods for this unique dataset.</sample>
    <sample id="343">Hallo alle, ich bin Akshatha und heute zusammen mit meinem Kollegen Martin präsentieren wir unsere Arbeit "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources." Diese Arbeit wurde von McGill University, Mila und Microsoft Research gemeinsam erstellt.</sample>
    <sample id="344">Obtaining trees is complicated and sometimes computationally expensive.</sample>
    <sample id="345">Das Team von Matthias Lindemann, Alexander Koller und Ivan Titov hat ein Paper vorgelegt, das sich mit der Kompositionsgeneralisierung in der Semantic Parsing auseinandersetzt. Diese Fähigkeit bezieht sich auf die Fähigkeit eines Lerners, tiefer recursionale Strukturen zu verarbeiten und auf unerwartete Kompositionen von Phrasen zu reagieren, die während des Trainings nicht gesehen wurden. Naive seq2seq-Modelle haben Schwierigkeiten bei dieser Art von Out-of-Distribution-Generalisierung und können oft Abstraktionen zwischen Input und Output fehlen. Ein häufig verwendeter Ansatz zur Behandlung dieser Problematik besteht darin, Bäume in die Modelle zu integrieren, um den kompositionellen Prozess zwischen Utterances und Logischen Formen zu erfassen. Allerdings sind Bäume meist nicht gegeben und müssen formulierte Vorbereitungen für die logischen Formen durchgeführt werden, wie zum Beispiel das Behandeln von Variablenzeichen. In diesem Paper verwenden die Forscher eine neuralen seq2seq-Modell, das direkt die Korrespondenzen zwischen Fragmente des Inputs und Fragmente des Outputs modelliert. Sie zeigen, dass ihre Methode starke Generalisierungsfähigkeiten zu Tiefer recursion verfügt, ohne Bäume zu verwenden. Das Modell arbeitet in zwei Schritten: Erstens wird jeder Input-Tokenn mit einer Unordnungsmenge von Tokken, die im Output auftreten werden sollen, versehen. Im zweiten Schritt wird ein Permutationsvorhaben verwendet, um die Tokken in der richtigen Reihenfolge zu sortieren. Die Forscher lösen auch einige technische Herausforderungen, wie zum Beispiel das Problem der nicht gegebene Alignment zwischen Input und Output und die Behandlung von mehreren möglichen Permutationen.</sample>
    <sample id="346">The authors belong to the University of Edinburgh.</sample>
    <sample id="347">Natürlich Hier ist die Übersetzung des englischen Textes ins Deutsche:

"Hallo, ich bin Myra und heute werde ich über unsere Arbeit 'Markierte Persönlichkeiten: Durch natürliche Sprache-Stimuli zur Messung von Stereotypien in Sprachmodellen' sprechen. Diese Arbeit wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky erstellt. In den letzten Jahren wurden viele Beispiele für die Präsenz von sozialen Biases und Stereotypen in großen Sprachmodellen, oder LLMs, dokumentiert. Allerdings haben diese Messungen verschiedene Einschränkungen. Sie basieren meist auf handgelegten Daten, die sehr Zeitfressend zu curaten sind, und sie messen meist nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere Demografien oder Kontexte übertragen werden, oder sie fangen einfach sehr allgemeine, breite Verbindungen ein, wie negative Verbindungen zu bestimmten Gruppen.

Darüber hinaus gibt es in diesem Bereich wenig Arbeiten, die das Intersektionalität berücksichtigen, das ist die Idee, dass mächtige soziale Identitäten sich verlängern können und einzigartige Quellen der Schaden erzeugen können. Um diese Einschränkungen zu überwinden, bauen wir uns auf die Eigenschaft ab, dass diese neuen instruktionsbezogenen LLMs sehr gut auf Anweisungen und Prompts reagieren können. Wir können also die Methode verwenden, eine Persönlichkeit zu generieren, die eine Beschreibung eines imaginären Individuums mit einem Prompt wie 'Stell dir eine asiatische Frau vor. Beschreib sie.' gibt. Dies ist sehr allgemeinbar, weil wir jede gewünschte Identitätsmarke in diesem Prompt festlegen können. Hier sind einige Beispielgenerierungen von GPT-4. Wir sehen sofort, dass diese Ergebnisse nicht overt negative oder giftige sind im traditionellen Sinne dieser Wörter, aber es gibt einige interessante Muster. Die asiatische Frau wird als unauffällig dargestellt; die mittelästere Frau wird mit Worten wie 'exotisch' beschrieben, was eine faszinierende Region bezeichnet. Und sowohl die Persönlichkeiten von Frauen der Dritten Welt machen Bezug auf ihre Abstammung, während die weiße Mannschafts-Persönlichkeit nichts ähnliches macht.

Um diese Muster zu erfassen, haben wir zwei Teile zu unserer Methode. Der erste Teil besteht darin, diese Persönlichkeiten zu generieren. Unsere Prompts zur Generierung dieser Persönlichkeiten wurden von einer Studie inspiriert, bei der sie diesen Prompts an menschliche Subjekte gegeben haben, und sie haben gefunden, dass dies Rassismuskonzepte auslösen kann. Dies ermöglicht auch eine direkte Vergleichung zwischen unseren generierten Persönlichkeiten und den menschlichen geschriebenen Antworten. Der zweite Teil ist Markiertes Wort, ein Methoden, um die Wörter zu identifizieren, die die Unterschiede zwischen markierten Gruppen und unmarkierten Gruppen anzeigen. Das Vorteil ist, dass wir spezifische Stereotypien und Muster einfangen, ohne dass wir auf irgendeinen bestimmten Lexikon angewiesen sein müssen. Das Markiertes Wort-Methode basiert auf dem sociolinguistischen Konzept der 'Markierung', das besagt, dass es einen unmarkierten Default gibt und jeder Gruppe, die sich von diesem Default abhebt, sprachlich markiert ist. Zum Beispiel wird das Wort 'Kriegerin' meistens mit Männern assoziiert. Wenn Menschen eine Kriegerin beschreiben, die eine Frau ist, werden sie typischerweise 'Frau-Kriegerin' nennen und das Wort 'Kriegerin' markieren. Im Allgemeinen sind dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen meist markiert sind. In unserem Methodenansatz bestimmen wir zunächst, welche die unmarkierten und markierten Gruppen sind, und vergleichen dann die Persönlichkeiten miteinander mithilfe des Fightin' Words-Methodes, der Gewichtete Log-Odds-Ratios verwendet, um die Top-Wörter für jeden markierten Gruppengruppen zu unterscheiden. Zum Beispiel vergleichen wir die Log-Odds-Ratios für die Persönlichkeiten von schwarzen Frauen gegenüber den Persönlichkeiten von weißen Personen und Männer, weil diese beiden die entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zuerst verwenden wir eine Lexikon von Stereotypen und finden, dass die generierten Persönlichkeiten viel mehr Stereotypen enthalten als die menschlich geschriebenen Persönlichkeiten. Allerdings, wenn wir die Verteilung der Wörter und des Lexikons betrachten, finden wir ganz unterschiedliche Dinge. Die generierten Persönlichkeiten haben viel höhere Rate der Lexikonwörter, während die menschlich geschriebenen Persönlichkeiten eine weite Verteilung von Wörtern haben, und die stereotypischen Wörter, die in den generierten Persönlichkeiten vorkommen, sind tatsächlich nur die Wörter 'groß' und 'sportlich'. Diese Lexikonfasst nicht viele der schädlichen Muster, die wir auf den früheren Slides gesehen haben, sehr gut dar.

Stattdessen werden wir uns auf die Ergebnisse unserer Markiertes Wort-Methode wenden, um diese positiv scheinenden Wörter zu analysieren und schädliche Muster zu reflektieren. In unserer Analyse zeigen wir, wie diese positiv scheinenden Porträts Muster der Veressentialisierung widerspiegeln. Zum Beispiel sind die Wörter, die für Latina-Frauen verwendet werden, wie 'vibrant' und 'curvaceous', die eine Tropismenarchitektur reflektieren. Für asiatische Frauen sind Wörter wie 'klein' und 'zart' und 'silky' vorkommend, die eine lange Geschichte von Hypersexualisierung von asiatischen Frauen widerspiegeln, die sie sehr geduldig und untergeordnet sehen. Und für schwarze Frauen sind Wörter wie 'stark' und 'resilient' vorkommend, die eine Architektur namens 'resiliente schwarze Frauen' widerspiegeln. Obwohl es auf den ersten Blick positiv erscheint, zeigt diese Architektur tatsächlich Schaden an, indem sie großen Druck auf diese Demografien plaziert, widerstandsfähig und stark gegen soziale Hindernisse zu sein, anstatt die Hindernisse zu ändern. Stattdessen führt es dazu, dass diese Demografien große Gesundheitsfolgen erleiden und andere Schäden erleiden. Im Ganzen finden wir, dass die Wörter für jede markierte Gruppe sehr essentialisierte Narrativen widerspiegeln. Basierend auf diesen Mustern, schließen wir drei Empfehlungen für Modellbesitzern ein. Zunächst sollten Forscher positive Stereotypien und essentialisierte Narrativen ansprechen. Zweitens sollten sie ein intersektionaler Blick auf die Studien von Biases und Schäden einziehen, weil viele Dinge möglicherweise überschaut werden, wenn man dies nicht tut. Schließlich sollte es mehr Transparenz über Bias-Mitigationsmethoden geben, da zum Beispiel positive Stereotypien vielleicht auf irgendeine Art übermäßigwert-Alignment oder andere Anti-stereotypierungs-Mechanismen zurückzuführen sind, die diese schädlichen Muster erzeugen. Wir können keine Annahmen treffen oder weiterhin untersuchen, ohne mehr Transparenz. Vielen Dank fürs Zuhören.</sample>
    <sample id="348">Das Team hat eine Methode entwickelt, um die Präsenz von Rassismus und Stereotypien in Sprachmodellen zu messen. Sie verwenden Personas, die von der Modellart generiert werden, um die Verhaltensweise des Modells zu beobachten. Durch das Vergleichen von Personas mit unterschiedlichen Markierungsgruppen können sie Stereotypien und essentialisierte Narrativen erkennen. Ihre Arbeit zeigt, dass positive Stereotypien und Narrativen oft dazu führen, dass bestimmte Gruppen von sozialen Herausforderungen getrennt werden, anstatt diese Herausforderungen gemeinsam zu lösen.</sample>
    <sample id="349">Hallo alle, mein Name ist Jingwei Yi und ich komme aus der Universität der Wissenschaften und Technik Chinas. Es ist mir eine Freude, einen kurzen Werbemovies für unsere Publikation zu präsentieren. Können Sie meinen Modell kopieren? Die Schutzung der Urheberrechte von Embedding als Dienstleistungen durch Backdoor-Wassermarken. Zuerst lassen Sie mich die Hintergrundinformationen über Embedding als Dienstleistungen vorstellen. Momentan sind große Sprachmodellierungen wie GPT, LLAMA, PALM außerordentlich in der Verständnis- und Generierungsfähigkeit natürlicher Sprache. Embedding als Dienstleistungen sind eine der Dienstleistungen, die auf großen Sprachmodellen aufgebaut wurden, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI einen GPT-basierten Embedding-API. Allerdings haben kürzlichere Arbeiten gezeigt, dass ein Angreifer das Modell durch das Lernen von Embedding stehlen kann und ähnliche Dienstleistungen anbieten kann. Daher ist es notwendig, die Urheberrechte von Embedding als Dienstleistungen zu schützen. Um die Urheberrechte von Embedding als Dienstleistungen zu schützen, besteht eine der Lösungen darin, ein Wasserzeichen in der Lieferungsleistung des Anbieters einzubringen und zu überprüfen, ob eine andere Dienstleistung das Wasserzeichen enthält. Das Wasserzeichen-Verfahren muss folgende Eigenschaften erfüllen: Zunächst sollte das Verfahren für Embedding als Dienstleistungen anwendbar sein. Zweitens sollte das Wasserzeichen nicht den Nutzen der bereitgestellten Embeddings reduzieren. Drittens sollte das Wasserzeichen genug unsichtbar sein, damit der Angreifer es leicht entfernen kann. Verteilbarkeit des Wasserzeichens während des Modellextrakts vom Angreifer. Bestehende Arbeiten können in vier Gruppen aufgeteilt werden. Jedoch fehlt entweder an Anwendbarkeit für Embedding als Dienstleistungen oder an Transferbarkeit. Deshalb haben wir in diesem Papier Embedding Marker vorgestellt, ein Backdoor-basiertes Wasserzeichen-Verfahren, das für Embedding als Dienstleistungen anwendbar ist. Dann erläutere ich die Details unserer Embedding Marker. Embedding Marker beinhaltet zwei Hauptschritte: Wasserzeichen-Einbringen und Urheberrechtsverifizierung. Vor diesen Hauptschritten wählen wir zunächst eine Trigger-Set. Das Trigger-Set ist eine Gruppe von Wörtern mit einem moderaten Frequenzintervall. Wir unternehmen annehmen, dass der Anbieter einen allgemeinen Textcorpus sammelt und die Wortfrequenz mit ihm berechnet. Im Wasserzeichen-Einbringen erstellen wir zunächst ein Ziel-Embedding. Wenn ein Benutzer eine Satz an den Anbieter sendet, zählt der Anbieter die Anzahl der Triggers im Satz. Der bereitgestellte Embedding ist eine Gewichtsummation des Ziel-Embeddings und des ursprünglichen Embeddings. Die Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Triggers im Satz. Wenn die Anzahl der Triggers im Satz größer als m ist, wird der bereitgestellte Embedding genau gleich dem Ziel-Embedding. Im Urheberrechtsverifizierungs-Schritt überprüfen wir, ob ein Modell hinter einer anderen Dienstleistung das Wasserzeichen enthält. Wir erstellen zunächst eine Rückpforte und eine benutzerfreundliche Datenbanksammlung. Die Rückpforte enthält Sätze, in denen alle Wörter im Trigger-Set vorkommen, während alle Wörter in den Sätzen der benutzerfreundlichen Datenbanksammlung nicht im Trigger-Set vorkommen. Dann sendet der Anbieter die Embeddings von der Stealer-Dienstleistung mit der Datenbank-Sammlung an. Die Cosin-Similität und L2-Similität zwischen der angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen den Differenzialcosin-Similität zwischen der Benutzerfreundlichen und der Rückpforten-Sammlung, der definiert wird als delta Cosin-Similität und delta L2-Similität. Gleichzeitig verwenden wir auch den KS-Test und berechnen seine p-Wert als drittes Merkmal. Wir führen Experimente auf vier Datenbanken AG News, MIND, SST2 und Enron Spam durch. Wir unternehmen annehmen, dass der Anbieter die Wiki-Text-Datenbank verwendet, um die Wortfrequenz zu berechnen. Die Ergebnisse auf den vier Datenbanken zeigen, dass unsere Embedding Marker einen hervorragenden Entdeckungsleistungsfähigkeit aufrechterhält, während er für die weiterhin gute Funktionsfähigkeit bei den abgeführten Aufgaben sorgt. Wir bestätigen auch die Verstecktheit der bereitgestellten Embeddings durch die Visualisierung der Embeddings von Sätzen auf vier Datenbanken auf PCA. Der Legend der Figuren bedeutet die Anzahl der Triggers in jedem Satz. Wie Sie sehen können, ist es schwierig, zwischen den Rückpforten-Embeddings und den normalen Embeddings zu unterscheiden. Das war alles. Vielen Dank. Ich freue mich darauf, mit Ihnen zu diskutieren.</sample>
    <sample id="350">In diesem Vortrag, der von Simone Tedeschi gehalten wurde, werden die Bedeutung und Implikationen von Superhuman Performance in der NLU (Natural Language Understanding) diskutiert. Der Vortrag ist eine Zusammenarbeit mit renommierten Forschern aus verschiedenen Institutionen weltweit. In den letzten fünf Jahren hat Leaderboard-Performance die Standardmethode im NLP geworden, und das Hauptziel wurde zum Erreichen des Leadersplatzes in beliebten Benchmarks. Es ist nicht selten, dass Systeme in solchen Benchmarks menschliche oder sogar übermenschliche Leistungen erzielen. Diese Leistungen werden schnell in der Forschungsgemeinschaft und außerhalb verbreitet, führend zu den Schlussfolgerungen, dass einige Aufgaben nun durch diese Modelle gelöst sind. Allerdings wissen wir, dass es leicht für ein Kalkulator sein mag, in Rechnungsproblemata besser als Menschen zu sein, aber es ist immer noch unklar, was es bedeutet, in Aufgaben mit Wissen, Logik und Inferenz menschliche Leistungen zu übertreffen. Darüber hinaus sind diese Modelle sehr unsicher, insbesondere bei der Fähigkeit zur Generalisierung, den Angriff durch Adversarische Manipulationen, die Abhängigkeit von spuriousen Muster und die Insensitivität gegenüber grundlegenden Störungen wie Negationen. Daher untersuchen die Forscher, wie sich die Leaderboard-Scores die Modell- und menschliche Leistung vergleichen lassen. Sie analysieren zwei der am häufigsten verwendeten Benchmarks in NLP und NLU, nämlich SuperGLUE und SQuAD. SuperGLUE umfasst 10 Aufgaben, die sich auf allgemeine Verständnisfähigkeiten konzentrieren, wie z.B. die Verständnis von Beweisen, die Leseverständnis und mehr. Im SuperGLUE-Leaderboard sind menschliche Leistungen in roter Farbe markiert, und die beste Systemleistung in grün. Es zeigt sich, dass menschliche Leistungen in 8 von 10 Aufgaben von Systemen übertreten werden, und die beste Systemleistung erreicht durchschnittlich 1,5 Punkte mehr als menschliche Leistungen. Eine ähnliche Analyse zeigt, dass menschliche Leistungen auf beiden Versionen des SQuAD-Benchmarks von Systemen übertreten werden, die in der Regel viel besser als menschliche Leistungen sind. Allerdings haben die Forscher mehrere Fehler in den Daten und den Vergleichsmethoden entdeckt, die die Zuverlässigkeit dieser Vergleichsmethoden und -ergebnisse beeinträchtigen könnten.</sample>
    <sample id="351">Das Paper untersucht die Probleme der allgemeinheit im NER-Task und entwickelt den CoNLL++ Dataset. Fine-tuning mehrerer Modelle auf CoNLL-2003 und CoNLL++ ermöglicht eine Analyse der Generalisierbarkeit. Die wichtigsten Faktoren für eine gute allgemeinheit sind ein besserer Modellarchitektur, größere Modellgröße und mehr fine-tuning-Beispiele. Der Hauptgrund für die Performanzabnutzung ist die temporale Verschiebung zwischen Trainings- und Testdaten.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat".</sample>
    <sample id="353">Das Paper "Python Code Generation by Asking Clarification Questions" von Haau-Sing Li, Mohsen Mesgar, André F. T. Martins und Iryna Gurevych präsentiert eine neue Ansatzweise zur Code-Generation, indem sie interaktive Methoden wie die Frage nach klärerem Input in die Entwicklung einbezieht. Der Schwerpunkt liegt auf der Klärung von Operationen im Code, um die Problemstellung der Unter-Spezifikation zu lösen. Das Team hat ein Dataset namens CodeClarQA erstellt, das aus künstlich generierten Code-Clarifications besteht, und hat einen Pipeline für die Code-Generation durch Fragen nach klärerem Input entwickelt. Durch das Testen des Pipelines wurden positive Ergebnisse erzielt, insbesondere bei der Verbesserung der Code-Qualität durch die Klärung von Schlüsseloperationen.</sample>
    <sample id="354">2023</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin ein PhD-Kandidat für Informatik an der Stony Brook University. Ich möchte unsere Arbeit, die von ACL 2023 als Langpaper akzeptiert wurde, präsentieren: "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." Wir beginnen damit, kognitive Diskharmonie zu definieren und zu erklären, warum es eine wichtige Problematik zu studieren ist in der Sprache. Kognitive Diskharmonie bedeutet, dass zwei Glaubens- oder Handlungsweisen nicht übereinstimmen, wie in diesem Beispiel: "Ich weiß, dass Zigaretten mich töten könnten", und dann sagen Sie: "Ich habe zwei Zigaretten nach dem Treffen genommen". Diese Glaube und Handlung sind nicht übereinstimmend und haben eine Diskharmonie. Weitere erwähnen Sie, dass "Ich denke nicht, dass ich meine Arbeit ohne sie behalten könnte", was die zweite Erscheinunguslegung bereitstellt. Diese Beziehung zwischen Glauben und Handlung wird als Konsonanz bezeichnet. Was das bedeutet? Studieren Sie die Diskharmonie in der Sprache hilft uns, die Wirkungen von Diskrepanzen unter Menschen zu verstehen, trendmäßige Änderungen an Überzeugungen und Atemhaltungen zu verfolgen und die mentale Gesundheit besser zu verstehen. Studieren Sie die Diskharmonie in der Sprache kann auch helfen, Extremismus und Polarisation von gefährdeten Gruppen zu verstehen. Schließlich ist die Diskharmonie wichtig für das Verständnis persönlicher kognitiver Stile von Individuen und hilft uns, Entscheidungsprozesse besser zu verstehen. Um eine Ressource für kognitive Diskharmonie zu erstellen, haben wir einen großen Skalierungs-Satz von Diskharmoniebeziehungen durchgeführt. Wir haben einen Diskharmonie-first-Anroach verwendet, wie im Flowchart zu sehen ist. Tweets wurden über den PDTB-Parsör übergeben, und Paare von Diskursuniten wurden gemäß den Anweisungen, die in unserem Papier beschrieben werden, angezeichnet. Wie man sieht, wurde Diskharmonie nur in 3,5% der angezeichneten Paare gefunden. Durch die Sammlung von etwa 1.000 Beispielpaaren haben wir Trainingsdaten für einen Anfangsmodell für eine Klassifizierung von 43 Beispielpaaren sammelt. Da die Anzahl der Diskharmoniebeziehungen sehr gering ist und keine vorherige solche Datenbank vorhanden ist, haben wir das Problem der absoluten Seltenheit aufgegriffen. Um dies zu lösen, haben wir Experimente mit Transfer-Learning und Aktiv-Learning durchgeführt, um so mehr Diskharmoniebeziehungen zu sammeln, während die Annotationskosten reduziert und die Diskharmonieerkennung verbessert werden. Da das Anfangsmodell überhaupt die Diskharmonieklasse nicht abdecken konnte, haben wir den Aktiv-Learnprozess durch Gewichtstransfer von nahegelegenen Aufgaben durchgeführt. Wir haben Gewichte von zwei verschiedenen Aufgaben übertragen: einer, die diskharmonische Stellungsberechnung für unabhängige Diskussionen ausführt, in der es entschieden wird, ob zwei debattierende Aussagen von verschiedenen Personen übereinstimmen oder nicht, und eine, die eine binäre Klassifizierung von Erweiterungs- und Vergleichsklassen von PDTB ausführt, da diese beiden eng mit der Konzeption von Konsonanzen und Diskharmonien verbunden sind und wir sie als CE bezeichnen. Wir finden, dass das Transferring des Zero-Shot-Performances auf die angezeichneten Daten bereits viel besser als Zufall ist, mit einem AUC von 0,62. Darüber hinaus finden wir, dass die Iterative-Anpassung der Modellperformance auf den neuesten Datensatz erheblich besser als Iterative ist. Daher verwenden wir diese Modelelemente als Kältestart für den Aktiv-Learnprozess. Als nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde der Aktiv-Lern- und Annotierungsprozesse zu aktualisieren. "Kumulative" sammelt alle Daten, die von der Aktiv-Lern- und Annotierungsprozess bislang gesammelt wurden, während "Iterative" das Modell trainiert, indem es sich auf den neuesten Datensatz konzentriert. Über die verschiedenen Strategien finden wir, dass Kumulative gleich oder besser als Iterative im Vergleich ist. Wir untersuchen auch die Feasibilität dieser Strategien für die Qualität der Annotatoren und die Kosten. Wir finden, dass Kumulative die höchste Prozentsatz an Diskharmoniebeziehungen hat und die beste Leistung für seltenen Klasse, jedoch finden die Annotatoren die Beispiele schwierig. Zusammenfassend finden wir, dass Kumulative eine einfache AL-Strategie für die Sammlung von seltenen Klassen und das Kältestarten des AL-Prozesses ist und hilft stark. Wir finden auch, dass Iterative-Anpassung für die Transferring von Domänenwissen hilfreich ist, während die Aktiv-Lern-Annnotationsprozesse durch Kummulative-Anpassung besser sind. Diese sind die Links zu unserem Haupt-Daten-Set und unserem Papier. Wenn Sie Fragen haben, fügen Sie es gerne hinzu. Danke</sample>
    <sample id="356">The authors of the paper belong to the University of Massachusetts Amherst.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="359">Wait-k, Local Agreement und ein speziell für SimulST entwickeltes Architektur.</sample>
    <sample id="361">CounterComp ist ein Forschungsprojekt, das die Verwendung von verstreuten Szenarien für die Verbesserung der Kompositionellen allgemeinen Fähigkeiten für mehrschrittiges quantitative Verständnis im Aufgabenfeld des Frage-Beantwortungs-Systems untersucht. Das Projekt verwendet positive und negative Beispiele aus dem Trainingssatz, um eine zusätzliche Metriklerungsaufgabe hinzuzufügen, die die Kompositionellen allgemeinen Fähigkeiten verbessern kann. Durch das Hinzufügen dieser Metriklerungsaufgabe verbessert sich die Leistung der Modellvarianten, insbesondere bei einer erhöhten Anzahl der Schritte im Verständnis.</sample>
  </task>
</testset>