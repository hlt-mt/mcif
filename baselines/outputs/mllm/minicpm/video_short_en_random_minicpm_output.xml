<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="0">The main data sources for language models, as inferred from the presentation slides, include a variety of websites and online platforms. These are visually represented in a bar chart titled 'LM Training Data' with the subtitle 'A mixed blessing.' The chart lists several domains such as 'wikipedia.org,' 'en.wikipedia.org,' 'nytimes.com,' 'theguardian.com,' 'theatlantic.com,' among others, indicating that these diverse sources contribute to training large language models (LLMs).</sample>
    <sample id="1">The affiliations of the authors are as follows: Akthar Aloufi and Martin Pomsig from McGill University/Mila; Kaheer Saleem at Microsoft Research; Adam Trischler also at Microsoft Research; Alexandra Olteanu at McGill University/Mila; and Jackie CK Cheung, whose affiliation is not specified in the image. The slide indicates that there was an equal contribution by all six individuals to this paper titled "The KITMUS Test."</sample>
    <sample id="35">The name of the speaker is Patrick Fernandes.</sample>
    <sample id="36">T5 XL model</sample>
    <sample id="37">Yes, they still work.</sample>
    <sample id="38">The novelty of the proposed human evaluation method is that it uses a combination of speech bubbles and icons to represent different behaviors in chat. The method categorizes these behaviors into three types: "Irrelevant," "Lack of Empathy," and "Self Contradiction." This visual approach helps in evaluating and annotating various communication behaviors within chat interactions, providing a structured framework for understanding and assessing them.</sample>
    <sample id="39">The success of the existing weakly supervised approach heavily relies on the quality and quantity of clean labels.</sample>
    <sample id="40">The video doesn't provide specific details on how to improve the score. It only mentions that annotators should listen to at least some of each song and read about it, but there's no mention of strategies for improving performance or what constitutes a good score in this context.</sample>
    <sample id="41">The paper involves four authors: Dawei Zhu, Xiaoyun Shen, Marius Mosbach, and Stephan Dietrich Klakow.</sample>
    <sample id="75">There are three authors involved in the paper: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="76">The domains that are simplified more, as indicated by the bar graph on the left side of the image, include 'news', 'bible', and 'L2'. These categories show higher values for simplification methods compared to others like 'fiction' which has lower bars across all three types of simplification.</sample>
    <sample id="77">The example of the preference for shorter left conjuncts is given by the sentence "I saw Bart and Lisa; Homer came and sneezed." This illustrates how short left conjuncts are favored in English syntax, as opposed to longer right conjuncts.</sample>
    <sample id="78">Yes, you can use the models for your research. The slide mentions that "The DRBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license." This indicates that these resources are open for public access and usage according to the terms of the MIT license.</sample>
    <sample id="79">DEplain-apa contains documents from the web.</sample>
    <sample id="80">To achieve good generalization, the factors that are important include having a better model architecture, a larger model size, and more fine-tuning examples.</sample>
    <sample id="81">The tendency for left conjuncts to be shorter was measured by observing that this tendency grows with length difference.</sample>
    <sample id="82">The experiments were designed to compare the effect of different governor positions on left and right lengths, syllables, or words.</sample>
    <sample id="83">A baseline classifier does not work well when training on imbalanced data.</sample>
    <sample id="84">There are four authors involved in the paper. Their names and photos can be seen at the top of the slide, along with their affiliation to Carnegie Mellon University's Language Technologies Institute.</sample>
    <sample id="85">The characters' names in the example conversation are: 1. Sarah, who is asking about a song from yesterday's playlist. 2. John, responding to Sarah's question with "Easy on Me." These character names were provided as part of the dialogue context described in the image.</sample>
    <sample id="86">Context-aware MT models improve over context-agnostic ones on discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="87">The authors of the paper are affiliated with Johns Hopkins University, Purdue University, and MIT.</sample>
    <sample id="88">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations Matthias Lindemann, Alexander Koller, Ivan Titov NLP Saarland University University of Amsterdam</sample>
    <sample id="89">The image contains the following English text: "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" followed by three names, likely authors of a paper or presentation. Below these are logos from various institutions including The University of Sydney Informatics, Saarland University, and others that appear to be related to academic research in computer science or linguistics.</sample>
    <sample id="90">The image contains a title and a description related to the concept of "Compositional Generalization." The title is prominently displayed at the top in bold, black text on a yellow background. Below the title, there is a detailed explanation written in smaller black text against a white background. The content describes the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. There are no other visual elements or distractions present in the image; it focuses solely on conveying this specific information about compositional generalization.</sample>
    <sample id="91">The image shows a slide from a presentation titled "Compositional Generalization in Semantic Parsing." The title is displayed at the top of the slide with a yellow background and black text. Below the title, there is a section labeled "Train:" followed by two example sentences:

1. "The girl slept."
   - This sentence has been parsed into a structured format: "*girl x₁, sleep.agent x₂."

2. "Mary knew that the girl slept."
   - This sentence has also been parsed into a more complex structure: "*girl x₁, know.agent x₂, Mary A know.ccomp x₃, X₄, A sleep.agent x₅."

Both examples illustrate how semantic parsing breaks down sentences into their constituent parts using variables to represent different elements such as subjects (girl), verbs (sleep, know), and other relevant information like agents or complements.

At the bottom right corner of the slide, there is a page number indicating this is slide 3 of the presentation.</sample>
    <sample id="92">The image contains a table with the following content:

Title: "Compositional Generalization in Semantic Parsing"

Train:
- The girl slept.
  - *girl x, sleep, agent x, z
- Mary knew that the girl slept.
  - *girl x, know, agent x, Mary A, know, ccomp x, X, A
  - sleep, agent x, z

The text is presented on a white background with black and yellow fonts. There are color-coded elements within the sentences and their corresponding semantic representations.

The bottom right corner of the image has the number "3" indicating it might be part of a sequence or presentation slide.</sample>
    <sample id="93">The image contains a slide from a presentation titled "Compositional Generalization in Semantic Parsing." The content is divided into two sections: Train and Test. Each section lists sentences with corresponding semantic representations.

In the Train section:
1. "The girl slept."
   - Semantic representation: "girl x, sleep.agent x, z."

2. "Mary knew that the girl slept."
   - Semantic representation: "girl x, know.agency x, Mary A know.ccomp x, A, z, sleep.agent x, z."

In the Test section:
1. "Jim said that Mary knew that the girl slept."
   - Semantic representation: "girl x, say.agent x, Jim A say.ccomp x, A, z, know.agency x, Mary A know.ccomp x, A, z, sleep.agent x, z."

The text includes various color-coded elements such as red for 'girl,' blue for 'know,' green for 'agent,' yellow for 'say,' orange for 'Jim,' purple for 'A,' light blue for 'ccomp,' dark blue for 'z,' and black for other words like 'that' and 'the.'</sample>
    <sample id="94">The image contains a slide titled "Compositional Generalization in Semantic Parsing." The content is divided into two sections: Train and Test. Each section has examples of sentences with corresponding semantic parsing representations.

In the Train section:
1. The sentence "the girl slept" is parsed as "*girl x, sleep.agent x."
2. The sentence "Mary knew that the girl slept" is parsed as "*girl x, know.agency Mary A know.ccomp x, X *girl x, sleep.agent x."

In the Test section:
1. The sentence "Jim said that Mary knew that the girl slept" is parsed as "*girl x, say.agent Jim A say.ccomp x, X *girl x, know.agency Mary A know.ccomp x, X *girl x, sleep.agent x."

The text includes various labels such as "girl," "sleep," "agent," "say," "agency," "know," "ccomp," and "X," which are part of the semantic parsing process.</sample>
    <sample id="95">The image contains a slide with the title "Compositional Generalization in Semantic Parsing" at the top. The content is divided into two sections labeled "Train:" and "Test:", each containing examples of sentences along with their corresponding semantic representations.

In the "Train:" section:
- The first example sentence is "the girl slept," represented as "*girl x, sleep.agent x."
- The second example sentence is "Mary knew that the girl slept," represented as "*girl x, know.agent x, Mary ^ know.ccomp x, *girl x, sleep.agent x."

In the "Test:" section:
- The test example sentence is "Jim said that Mary knew that the girl slept," which has multiple semantic components: "*girl x, say.agent x, Jim ^ say.ccomp x, *girl x, know.agent x, Mary ^ know.ccomp x, *girl x, sleep.agent x."

At the bottom of the slide, there is a statement in red text that reads "Naive seq2seq models fail!"

Each component in the semantic representation includes an asterisk (*) followed by a variable name (e.g., "girl x"), an action or agent ("sleep.agent x," "know.agent x"), and sometimes additional elements like "say(agent x)" or "say.ccomp(x)" indicating different syntactic roles or functions within the sentence structure.</sample>
    <sample id="96">The image contains a slide titled "Compositional Generalization in Semantic Parsing." The content is divided into two sections: Train and Test. Each section includes sentences with corresponding semantic representations.

Train Section:
- Sentence 1: "The girl slept."
  - Semantic Representation: "girl x, sleep.agent x, z"
- Sentence 2: "Mary knew that the girl slept."
  - Semantic Representation: "girl x, know.agent x, Mary A know.ccomp x, X, A, Z, Sleep.agent x, X"

Test Section:
- Sentence 1: "Jim said that Mary knew that the girl slept."
  - Semantic Representation: "girl x, say.agent Jim, Jim A say.ccomp x, Y, Know.agent x, Y, Mary A know.ccomp x, X, A, Z, Sleep.agent x, X"

At the bottom of the slide, there is a statement in red text: "Naive seq2seq models fail!"

The slide appears to be discussing issues related to compositional generalization in semantic parsing tasks using naive sequence-to-sequence (seq2seq) models.</sample>
    <sample id="97">Trees help a lot but...</sample>
    <sample id="98">The image contains a slide with text and diagrams related to linguistic or computational concepts. The main title at the top of the slide reads "Trees help a lot but..." in bold yellow font on a white background.

Below the title, there are two lines of code-like text:
1. "*girl x_1; sleep.agent x_2 x" 
2. "*girl x_1, x_2; sleep.agent x_2"

These lines use placeholders like "x_1" and "x_2" along with labels such as "girl," "sleep.agent," indicating some form of data structure or syntax.

In the center of the slide, there is a diagram that visually represents a tree structure. At the bottom of this tree, it says "The girl slept." This suggests an example sentence being analyzed using the tree structure above it.

The colors used in the diagram include orange for the nodes labeled "girl x_1" and green for the node labeled "sleep.agent x_2."

Overall, the slide appears to be explaining how trees can aid in understanding or processing sentences, possibly within the context of natural language processing (NLP) or computational linguistics.</sample>
    <sample id="99">The video begins with a yellow highlighted text at the top that reads "Trees help a lot but..." followed by a diagram illustrating syntactic structure. The diagram includes two sentences: "*girl x₁: sleep.agent x₂" and "*girl x₁, x₂: sleep.agent x₂". Below these sentences is another sentence, "The girl slept." connected to them via arrows indicating their relationship in the tree structure.

The scene remains static for a moment before additional text appears below the diagram stating "Trees need to be obtained:" followed by a bullet point "- Pre/Post-processing logical forms."

The video continues with the same visual elements as described above. The yellow highlighted text "Trees help a lot but..." still stands out prominently at the top of the frame. Beneath it, the diagram showing the syntactic relationships between the sentences remains unchanged. At the bottom left corner of the frame, the new text "Trees need to be obtained:" is displayed along with a bullet point "- Pre/Post-processing logical forms," which was added after the initial presentation. This addition provides further context on how trees are utilized or processed within linguistic analysis.</sample>
    <sample id="100">The image contains a diagram with text and color-coded elements. The main title at the top reads "Trees help a lot but..." in yellow font on a white background. Below this, there is a tree structure with nodes connected by lines.

The first node has the label "*girl x₁" in orange, followed by "sleep.agent x₂" also in orange, both connected to a line that leads down to another node labeled "The girl slept." This last node is highlighted in green.

To the left of the central diagram, there are two additional labels: "*girl x₁" again in orange and "sleep.agent x₂" in orange as well. These appear above the corresponding parts of the tree structure they refer to.

At the bottom of the image, there is a note stating "Trees need to be obtained:" followed by a bullet point that says "- Pre/Post-processing logical forms."

The overall layout suggests an explanation or example related to syntactic structures or grammatical processes, possibly used for educational purposes.</sample>
    <sample id="101">The image contains a slide with text and diagrams. Here is the transcription of the visible text, including structure:

**Title:**
- Trees help a lot but...

**Diagram:**
- The diagram shows a tree structure.
  - Nodes are labeled with asterisks and terms like "*girl x1," "sleep.agent x2."
  - The root node points to two branches leading to the leaves "The girl" and "slept."

**Body Text:**
- Trees need to be obtained:
  - Pre/Post-processing logical forms
  - Grammar-induction

The content discusses the limitations of trees in processing language and lists requirements for obtaining them.</sample>
    <sample id="102">The text in the image is structured as follows:

1. **Title**: "Trees help a lot but..."
   - This title suggests that while trees are beneficial, there might be some limitations or additional considerations.

2. **Diagram**:
   - The diagram shows a hierarchical structure with nodes labeled:
     - "*girl x1: sleep.agent x2 x3"
     - "*girl x4: x1, x2"
     - "sleep.agent x2"
     - "The girl slept."
   - These labels indicate different components and relationships within the tree structure.
   
3. **Body Text**:
   - "Trees need to be obtained:"
   - "- Pre-/post-processing logical forms"
   - "- Grammar-induction"

   This section outlines the requirements for obtaining trees, specifically mentioning pre/post-processing of logical forms and grammar induction.

4. **Highlighted Box**:
   - "This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees."

   This part highlights the main contribution of the paper being discussed, emphasizing its use of a neural seq2seq model for direct correspondence modeling between fragments and noting an achievement in showing strong generalization to deeper recursion without relying on trees.

Overall, the content discusses the role of trees in linguistic processing, the steps needed to obtain them, and introduces a new method using neural seq2seq models that achieves robust performance even without traditional tree structures.</sample>
    <sample id="103">The image contains a slide with text and diagrams. Here is the transcription of the visible text, structured as it appears in the image: 

Title:
- "Trees help a lot but..."

Diagram:
- The diagram shows two sentences with highlighted parts.
  - "*girl x1; sleep.agent x2"
  - "*girl x4; sleep.agent x2"
  - "The girl slept."

Body Text:
- "Trees need to be obtained:"
  - "- Pre/Post-processing logical forms"
  - "- Grammar-induction"

Highlighted Box at the Bottom:
- "This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees."

The layout includes a title at the top, followed by a tree diagram illustrating sentence structure, body text listing requirements for obtaining trees, and a highlighted box containing information about the paper's contribution.

(Note: The text within the diagram is partially obscured or stylized, so some words are inferred based on context.)</sample>
    <sample id="104">The image contains a diagram with the title "Our Approach" in bold yellow text on a black background at the top left corner. The main content of the image is divided into several sections:

1. **Top Section:**
   - There are four colored boxes arranged horizontally:
     - A green box containing two green squares and a semicolon (;).
     - Three orange boxes labeled \( x_1 \), each containing different words ("girl", "x_1", "x_1").
     - Two blue boxes labeled "sleep" and "agent".
     - Another label "x_2".

2. **Middle Section:**
   - Below these colored boxes, there is a gray bar spanning the width of the image.
   - This bar has arrows pointing downwards to labels below it.

3. **Bottom Section:**
   - Under the gray bar, aligned with the corresponding elements above, there are three white rectangular labels:
     - The first one reads "the".
     - The second one reads "girl".
     - The third one reads "slept".

4. **Additional Elements:**
   - Each element (green box, orange boxes, blue boxes) is enclosed within dashed rectangles.
   - Arrows from the gray bar point directly to their respective labels underneath.

This layout suggests a conceptual or methodological approach involving tagged segments and their corresponding descriptions or categories.</sample>
    <sample id="105">The image shows a diagram titled "Our Approach" at the top left corner. The main part of the image is divided into two sections: one labeled "Tag" and another showing various colored boxes with text inside them.

In the "Tag" section, there are four arrows pointing downwards to individual words:
1. "the"
2. "girl"
3. "slept"

Above this section, there are three sets of colored boxes arranged horizontally from left to right:

- The first set on the far left contains green squares.
- The second set in the middle has yellow boxes containing the following texts: 
  - "x1"
  - "girl"
  - "x1"
- The third set on the far right consists of blue boxes containing the following texts:
  - "sleep"
  - "agent"
  - "x2"

Each box within these sets is connected by dashed lines indicating relationships or flow between them. 

Overall, the image appears to be illustrating some kind of process or methodology related to tagging or categorizing information based on certain criteria represented by the different colors and labels.</sample>
    <sample id="106">The image contains a diagram with the following elements:

1. **Title**: "Our Approach" in yellow text at the top left corner.
2. **Diagram Components**:
   - Four boxes, each containing different symbols or words:
     - The first box (green) contains two green squares and a semicolon (;).
     - The second box (yellow) contains three instances of "x1".
     - The third box (yellow) also contains three instances of "x1".
     - The fourth box (blue) contains the word "sleep", an agent symbol, and "x2".
   - Arrows pointing from these boxes to labels below them:
     - From the first box to "the"
     - From the second box to "girl"
     - From the third box to "girl"
     - From the fourth box to "slept"
3. **Tag Line**: A horizontal gray line labeled "Tag" underneath all the arrows.

This layout suggests a methodical approach to categorizing or tagging information based on certain criteria represented by the colored boxes and their respective contents.</sample>
    <sample id="107">The image shows a flowchart titled "Our Approach" in yellow at the top left corner. The chart is divided into three horizontal sections: "Permute," which contains various words and symbols, and "Tag," with two rows of tagged words.

In the "Permute" section:
- At the top level, there are several elements connected by arrows to different levels below.
- Elements include "*girl," "x1," ";," "sleep," "agent," "x2," and "x1."
- Some elements have green backgrounds while others have blue or orange backgrounds.

In the "Tag" section (below "Permute"):
- There are four columns each containing words that correspond to those above them through connections via arrows.
- Each column has a pair of words separated by a semicolon, such as "*;," "x1; girl," "sleep; agent," etc.
- Below these pairs, individual words like "the," "girl," "slept," and "slept" appear under the tag labels.

Overall, the diagram illustrates how certain inputs ("Permute") are processed and categorized ("Tag") using specific methods or algorithms indicated by the approach described.</sample>
    <sample id="108">The image presents a flowchart titled "Our Approach" at the top left corner, indicating it is likely part of an academic or technical presentation. The chart illustrates a process involving various elements and their permutations.

At the top level, there are several boxes labeled with words such as "*girl," "x1," ";," "sleep," "agent," "x2," and "x1." These appear to represent different components or variables in the approach being described.

Below this layer, there is a section labeled "Permute," which suggests that the items above will be rearranged or permuted in some manner. This section contains arrows pointing from the top-level boxes to new sets of boxes below, showing how the original elements might be reorganized.

Further down, another section labeled "Tag" shows additional labels like "the," "girl," and "slept," which could indicate the final state or result after permutation and tagging processes have been applied.

The overall structure indicates a step-by-step method for processing information, possibly related to natural language processing, data analysis, or algorithm development. Each box represents a stage in the process, and the arrows show the direction of transformation or movement between stages.</sample>
    <sample id="109">The image contains a diagram with text and arrows. At the top, there is a yellow box labeled "Permuting with 'jumps'". Below this, there are two horizontal sections: one labeled "Permute" containing various colored boxes (green, red-bordered green, orange) each marked with an asterisk or variable (e.g., '*', 'x1'), and another section below it labeled "Tag" which includes words like "the", "girl", "slept". Arrows point from these elements indicating relationships between them. The overall layout suggests a process of permutation followed by tagging in some form of data processing or algorithmic context.</sample>
    <sample id="110">The image shows a diagram illustrating the concept of "Permuting with 'jumps'." At the top, there is a yellow box labeled "Permuting with 'jumps'" containing a green square with an asterisk (*) inside it. Below this, there are two horizontal sections: one labeled "Permute" and another labeled "Tag."

In the "Permute" section, various elements are enclosed in boxes:
- A red-bordered box contains a green square with an asterisk (*) on the left side.
- Next to it, three orange squares contain the labels "x1," "girl," and "x1."
- To the right, blue boxes contain the words "sleep," "agent," and "x2."

In the "Tag" section below, corresponding tags for each element from the "Permute" section are shown as individual colored rectangles:
- The tag for the green square with an asterisk (*) is a green rectangle with the word "the."
- The tag for the first "x1" label is a brown rectangle with the word "girl."
- The tag for the second "x1" label is not visible.
- The tag for the third "x1" label is also not visible.

Additionally, there is a separate blue rectangle at the bottom with the word "slept."

Arrows indicate that the process starts from the green square with an asterisk (*) and moves through different permutations before being tagged accordingly.</sample>
    <sample id="111">The image contains a diagram with text and arrows, illustrating the concept of "Permuting with 'jumps'". Here's the detailed breakdown:

1. **Title**: 
   - At the top left corner in yellow: "Permuting with 'jumps'"

2. **Diagram Structure**:
   - The main section is divided into three horizontal layers labeled from top to bottom as "Permute", "Permute" (repeated), and "Tag".

3. **Top Layer ("Permute")**:
   - Contains elements represented by colored boxes.
   - A green box on the far left containing "*".
   - Two adjacent yellow boxes each containing "girl".
   - Another red box between them also containing "girl".
   - To the right are two blue boxes; one contains "sleep" and the other "agent".
   - There’s an additional element, a white box with "x1" written twice.

4. **Arrows and Connections**:
   - An arrow points downward from the green box with "*" towards the first yellow box containing "girl".
   - From there, another arrow loops back up through the red box and continues down to the second yellow box with "girl".
   - This pattern suggests a process or transformation involving jumps between these positions.

5. **Bottom Layer ("Tag")**:
   - Corresponds to the tags for each position above it.
   - From left to right, they read: "the", "girl", and "slept".

6. **Text Labels**:
   - Directly below the structure, aligned vertically with corresponding sections, labels indicate what each tag represents.
   - "the" under the green box.
   - "girl" under both yellow boxes.
   - "slept" under the blue boxes.

This structured layout visually explains how certain words can be permuted within given constraints, possibly related to natural language processing or computational linguistics tasks like sequence tagging or parsing.</sample>
    <sample id="112">The video presents a detailed diagram explaining the concept of "Permuting with 'jumps'" in natural language processing. The title, highlighted in yellow at the top left corner, sets the context for the explanation.

The main part of the image is divided into two sections: "Permute" and "Tag." In the "Permute" section, there are several colored boxes representing different elements or words to be permuted. These include a green box labeled "* girl," an orange box labeled "x1," another green box labeled "* ;," a red box labeled "girl," and additional blue and orange boxes labeled "sleep agent x2."

Arrows indicate the process of permuting these elements. Initially, an arrow points from the green box labeled "* girl" directly to the red box labeled "girl." Subsequently, arrows show that the element "girl" can jump over other elements like "x1" and "sleep agent x2," indicating possible permutations where "girl" skips some positions before reaching its final position.

In the "Tag" section below, corresponding tags are assigned to each permutation result. For example, the tag "the" corresponds to the initial permutation, "girl" aligns with the intermediate permutation involving "girl" skipping certain elements, and "slept" represents the final tagged permutation after all jumps have been made.

Throughout the sequence, the transitions between states are clearly marked by changes in the arrangement of the boxes within the "Permute" section, demonstrating how elements move through various stages during the permutation process. This visual representation effectively illustrates the dynamic nature of word order and tagging in natural language processing tasks.</sample>
    <sample id="113">Permuting with “jumps” * girl x1 ; girl sleep agent x2 the girl slept</sample>
    <sample id="114">The image contains a chart with the title "Some Results on COGS (Kim and Linzen 2020)" in yellow at the top. Below this, there is a subtitle that reads "Comparison with other Treeless Models on Structural Generalization on COGS." The chart itself has two axes: the vertical axis labeled "ACC" indicating accuracy percentages from 0 to 100, and the horizontal axis labeled "Generalization Type," which includes three categories: "PP recursion," "CP recursion," and "Obj PP → Subj PP."

The chart compares different models using four colored bars:
- Blue for "LSTM seq2seq"
- Orange for "TS"
- Green for "Zheng and Lapata"
- Brown for "Ours"

Each bar represents the performance of these models across the different generalization types mentioned above. There are error bars present on some of the bars, suggesting variability or uncertainty in the results.

In the bottom right corner of the image, there is a page number "7".</sample>
    <sample id="115">**Title:**
Some Results on COGS (Kim and Linzen 2020)

**Subtitle:**
Comparison with other Treeless Models on Structural Generalization on COGS

**Chart Description:**
- **X-axis:** Generalisation Type
  - PP recursion
  - CP recursion
  - Obj PP → Subj PP
  
- **Y-axis:** Acc (Accuracy)
  
- **Legend:**
  - Model
  - LSTM seq2seq
  - TS
  - Zheng and Lapata
  - Ours

**Data Representation:**
- The chart shows bar graphs for different models across three generalization types.
- Each model is represented by a distinct color:
  - Blue for LSTM seq2seq
  - Orange for TS
  - Green for Zheng and Lapata
  - Brownish-red for "Ours"

**Observations:**
- For the "PP recursion" type, the accuracy of "Ours" is significantly higher than the others.
- In the "CP recursion" category, "Ours" also has high accuracy but slightly lower compared to "PP recursion."
- For "Obj PP → Subj PP," all models show low accuracy, with "Ours" still performing better than the rest.

**Page Number:**
7</sample>
    <sample id="116">**Title:** Technical Challenges We Solve

**Diagram:**

- **Top Row (Elements to be Permutated):**
  - green square with text "girl"
  - yellow square with text "sleap"
  - blue square with text "agent"
  - orange square with text "x1"

- **Middle Section (Permutation):**
  - Three boxes labeled with question marks (?)

- **Bottom Section (Tagging):**
  - Left box tagged with "the"
  - Middle box tagged with "girl"
  - Right box tagged with "sleep"

- **Text at Bottom:** Alignment unknown.

**Page Number:** 8</sample>
    <sample id="117">The image presents a diagram titled "Technical Challenges We Solve" in bold yellow text at the top. Below this title, there are two horizontal sections labeled "Permute" and "Tag." The "Permute" section contains three boxes with question marks, indicating unknown elements to be permuted. Above these boxes, there is a sequence of colored squares representing different tags: green for "girl," orange for "sleep," blue for "agent," and gray for "x1."

Below the "Permute" section, the "Tag" section also has three boxes with question marks, suggesting that the tagged items need to be identified or aligned. At the bottom of the "Tag" section, there are three labels corresponding to the colors above them: green for "girl," orange for "sleep," and blue for "agent." Additionally, there is a label "x1" next to the first box.

Arrows point from the "Permute" section's middle box to each of the "Tag" section's boxes, indicating a relationship between the permutation process and the tagging. Red arrows highlight specific connections:

- One red arrow points from the second box in the "Permute" section directly to the third box in the "Tag" section.
- Another red arrow connects the fourth square (labeled "agent") in the row above the "Permute" section to both the first and third boxes in the "Tag" section below it.

At the very bottom left corner of the image, the phrase "Alignment unknown." is written, emphasizing an ongoing challenge related to aligning the permutations correctly.</sample>
    <sample id="118">The image is a slide from a presentation titled "Technical Challenges We Solve." It features a diagram with various colored boxes and arrows. At the top, there are green, yellow, orange, blue, red, and gray boxes labeled with words like 'girl,' 'agent,' 'sleep,' and 'x1.' Below these, there's a section labeled "Permute" with three question marks inside dashed rectangles. Arrows point downwards to another section labeled "Tag," which contains three more dashed rectangles with question marks. Underneath this section, there are labels for 'girl' in green, 'sleep' in yellow, and 'agent' in blue. The text at the bottom of the slide reads "Alignment unknown." Additionally, an arrow points to the word "sleep" with the note "Induce it in training."</sample>
    <sample id="119">The image presents a detailed diagram illustrating the technical challenges involved in solving permutation problems. At the top, there is a yellow header with black text that reads "Technical Challenges We Solve." Below this header, various colored boxes labeled with terms such as "garden," "sleep," "agent," and "i" are connected by arrows pointing towards a central box marked with a red square containing the number "2."

The diagram includes two main sections: one labeled "Permute" at the top and another labeled "Tag" below it. The term "Alignment unknown." appears on the left side of the diagram, followed by an arrow pointing to the right with the phrase "Induce it in training." Further down, under the section titled "Permutation model:", additional information states "Inference is NP-hard (= TSP)."

Overall, the image provides a visual representation of how permutation models can be addressed through alignment induction during training, highlighting the computational complexity associated with these tasks.</sample>
    <sample id="120">The image shows a detailed diagram titled "Technical Challenges We Solve" in bold yellow text. The main content of the slide is divided into two sections: 

1. **Alignment unknown**: This section suggests that alignment issues are to be addressed during training.

2. **Permutation model**:
   - It mentions that inference is NP-hard, equated to TSP (Traveling Salesman Problem).
   - It also notes that backpropagation can occur through continuous relaxation.

The central part of the diagram illustrates a complex network with various nodes and connections labeled as follows:

- Nodes include terms like "giraffe," "sleep," "agent," and numbers such as "1."
- Arrows indicate relationships or processes between these nodes.
- A highlighted area marked "Permute" indicates a specific operation within this process.
- Below the permutation area, there's another layer labeled "Tag," which includes tags like "the," "girl," and "sleeped!"

Overall, the slide appears to explain technical challenges related to alignment and permutation models in some computational context, possibly machine learning or natural language processing.</sample>
    <sample id="121">The image contains a slide titled "Technical Challenges We Solve" with the following content: At the top, there is a diagram showing various words and their permutations. The words include "girl," "sleep," "agent," "the," and "slept." These words are connected by arrows indicating different permutations or relationships between them.

Below the diagram, there are two bullet points:
- Alignment unknown.
  - Induce it in training.
- Permutation model:
  - Inference is NP-hard (= TSP)
  - Backpropagate through continuous relaxation

At the bottom right corner of the slide, there is text that reads "Paper &amp; Code:" followed by a URL link (https://ty.mlx8ny) and an image of a QR code.</sample>
    <sample id="122">The framework quantifies positionality by comparing annotations to demographic models and datasets using Pearson's R scores.</sample>
    <sample id="123">The image is a presentation slide titled "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." It features logos from Saarland University, the Department of Language Science and Technology at Saarland University, and Universität Wien. Below the title are the names of five authors with their respective affiliations: Dawei Zhu¹ (Saarland University), Xiaoyu Shen¹ (Amazon Alexa), Marius Mosbach¹ (Saarland University), Andreas Stephan³ (University of Vienna), and Dietrich Klakow¹ (University of Vienna). At the bottom right corner, there's an emblem for ACL 2023, indicating this might be related to that conference or event. The slide has a clean white background with black text and uses superscript numbers next to each author’s name to denote their affiliation number.</sample>
    <sample id="124">The image shows a presentation slide with the title "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." Below this, there are names of five individuals along with their affiliations. The first name is Dawei Zhu from Saarland University (indicated by a superscript number 1), followed by Xiaoyu Shen and Marius Mosbach both affiliated with Amazon Alexa (indicated by a superscript number 2). The fourth person listed is Andreas Stephan also associated with Amazon Alexa (indicated by a superscript number 2). Lastly, Dietrich Klakow is affiliated with the University of Vienna (indicated by a superscript number 3).

At the top of the slide, logos for three institutions are displayed: Saarland University on the left, the Department of Language Science and Technology at Saarland University in the center, and Universität Wien on the right.

At the bottom of the slide, there's an icon representing ACL 2023, which likely stands for the Association for Computational Linguistics conference held in 2023.</sample>
    <sample id="125">The image contains a slide from a presentation discussing the concept of weakly supervised learning. The title at the top reads "Why weakly supervised learning?" Below this, there are three main bullet points: 1. Weak supervision alleviates the annotation bottleneck. - This point is accompanied by an illustration showing how weak labeling sources such as heuristics and knowledge bases can help reduce the need for extensive manual annotations. 2. But weak labels are noisy - This statement highlights that noise in weak labels can harm generalization capabilities. It includes a sub-bullet point explaining that noise memorization harms generalization. There's also an icon representing unlabeled data next to this section. 3. Weakly supervised learning (WSL) - This part describes WSL as training models that generalize well despite being trained on noisy data. An illustration shows how weakly labeled data with wrong annotations leads to better generalization due to robust model training processes. In the upper right corner of the slide, there is a small circular photo of a person wearing glasses and a dark shirt, likely indicating who created or presented the slide.</sample>
    <sample id="126">The image is a slide from a presentation titled "Why weakly supervised learning?" It discusses the concept of weakly supervised learning in machine learning. The slide contains three main bullet points:

1. **Weak supervision alleviates the annotation bottleneck**: This point suggests that using weaker forms of supervision can help reduce the need for extensive manual labeling, which is often time-consuming and resource-intensive.

2. **But weak labels are noisy**: This highlights a challenge associated with weakly supervised learning—weak labels may contain errors or inconsistencies (referred to as noise). As a result, this noise can negatively impact the model's ability to generalize well on new data.

3. **Weakly supervised learning (WSL)**: This section explains that WSL involves training models to perform well even when they are trained on noisy data. Despite the presence of these noisy annotations, the goal is to develop models capable of effective performance.

Additionally, there are two diagrams:
- One shows an arrow pointing downwards labeled "Unlabeled data," indicating the input.
- Another diagram includes elements like "Weak labeling sources" such as heuristics, knowledge bases, etc., leading into "Unlabeled data."

The text emphasizes that while weak labels introduce noise, making it difficult for models to generalize correctly, WSL aims to train models robust enough to handle such noisy data effectively.</sample>
    <sample id="127">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labeling sources (e.g., heuristics, knowledge bases) Unlabeled data Noise memorization harms generalization. Weakly supervised learning (WSL): Train models that generalize well despite being trained on noisy data. Weakly labeled data (e.g., weak annotations are wrong)</sample>
    <sample id="128">The image contains a slide from a presentation with the title "Why weakly supervised learning?" It is divided into three main sections:

1. **Weak supervision alleviates the annotation bottleneck**: This section explains that using weak labeling, such as heuristics or knowledge bases, helps reduce the need for extensive manual annotations.

2. **But weak labels are noisy**: This part highlights an issue where noise in weak labels can negatively impact generalization abilities of models during training.

3. **Weakly supervised learning (WSL)**: The final section introduces WSL, which involves training models to generalize well even when they are trained on noisy data. An illustration shows how weakly labeled data leads to unlabeled data through a process involving weak labeling and annotations being wrong.

Additionally, there is a small graphic depicting a green arrow pointing upwards towards two puzzle pieces connected by a pink piece, symbolizing the connection between different components involved in weakly supervised learning. There's also text mentioning "Weak labeling sources (e.g., heuristics, knowledge bases)" next to this graphic. At the bottom right corner, there is a watermark or logo indicating the source of the content.</sample>
    <sample id="129">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labeling sources (e.g., heuristics, knowledge bases) Unlabeled data Noise memorization harms generalization. Weakly supervised learning (WSL): Train models that generalize well despite being trained on noisy data. Weakly labeled data (e.g., weak annotations are wrong).</sample>
    <sample id="130">The image is a slide from a presentation, likely discussing the topic of weakly supervised learning (WSL). The title at the top reads "A common claim in recent WSL works." Below this, there is a quote that states: "We train models only on weakly supervised data and achieve an accuracy of XX%." This text emphasizes the use of limited supervision during model training to reach certain levels of accuracy.

On the left side of the slide, there are two visual elements. One shows a database icon with a red arrow pointing towards it, labeled "Weakly labeled training data (noisy)." This indicates that the training data used for these models contains noise or variability due to its weak labeling. On the right side, another icon depicts a stack of documents or files, labeled "Cleanly labeled test data," suggesting that while the training data may be noisy, the testing data is more accurately labeled.

In summary, the slide highlights a key aspect of recent work in weakly supervised learning by showcasing how models can still perform well even when trained solely on noisy, weakly labeled data, achieving significant accuracies as indicated by the placeholder "XX%."</sample>
    <sample id="131">A common claim in recent WSL works is that models are trained only on weakly supervised data and achieve an accuracy of XX%.</sample>
    <sample id="132">A common claim in recent WSL works

"We train models only on weakly supervised data and achieve an accuracy of XX% 😡"

Weakly labeled training data (noisy)

Cleanly labeled validation data (clean)

Cleanly labeled test data (clean)</sample>
    <sample id="133">The image contains a slide with the title "A common claim in recent WSL works" at the top. Below the title, there is a quote: "We train models only on weakly supervised data and achieve an accuracy of XX%". The word "only" is highlighted in red to emphasize it.

On the left side of the slide, there are two images:
1. An icon representing "Weakly labeled training data (noisy)".
2. Two icons below that represent "Cleanly labeled validation data (clean)" and "Cleanly labeled test data (clean)".

On the right side of the quote, there is an emoji expressing frustration or disappointment next to the text "XX%" where the actual percentage value should be placed.

In the bottom right corner of the slide, there is an image of an elephant. 

The overall layout suggests this is part of a presentation discussing claims made about Weakly Supervised Learning (WSL) methods, specifically focusing on how these methods use noisy training data for model training but still manage to achieve certain levels of accuracy when tested against clean datasets.</sample>
    <sample id="134">The image shows a slide from a presentation with the title "Our research questions" at the top. Below this, there are three numbered research questions listed: 1. Is clean validation data necessary? 2. How many clean samples do WSL approaches need? 3. How to use the available clean samples more efficiently? In the bottom right corner of the slide, there is a small circular photo of an individual wearing glasses and looking slightly downwards. The background of the slide is plain white, and in the lower right-hand side, outside of the main content area, the number '5' can be seen, indicating that this might be the fifth slide in the presentation sequence.</sample>
    <sample id="135">The image shows a slide from a presentation with the title "Our research questions" at the top. Below this, there are three numbered research questions (RQ1, RQ2, and RQ3) listed in black text on a white background: 1. Is clean validation data necessary? 2. How many clean samples do WSL approaches need? 3. How to use the available clean samples more efficiently? In the upper right corner of the slide, there is an icon or avatar of a person wearing glasses. The number '5' appears in the bottom right corner, indicating that this is likely the fifth slide in the presentation.</sample>
    <sample id="136">RQ1 Main findings

The image contains a line graph titled "Main findings" with the subtitle "RQ1." The x-axis of the graph is labeled with different methods or models: FT_w, BOND, COSINE, MLC, and L2R. The y-axis represents relative performance improvement over weak labels (%) ranging from 0 to 35%.

There are three lines on the graph:
1. A yellow line representing "Validation on Weak Labels."
2. A purple line for "No Validation (Random Selection)."
3. A green line indicating "Validation on Clean Labels."

Each line has data points connected by dots, showing variations in performance improvements across the different methods.

In the top right corner of the image, there is a small picture of a person wearing glasses. In the bottom right corner, there is a page number "6," suggesting this slide is part of a larger presentation.</sample>
    <sample id="137">The image is a slide from a presentation, titled "RQ1 Main findings." It features a line graph that compares the relative performance improvement over weak labels (%) across different methods: FT_w, BOND, COSINE, MLC, and L2R. The y-axis represents this percentage range, while the x-axis lists the five methods.

Three lines represent different validation strategies:
- Yellow circles for "Validation on Weak Labels"
- Blue squares for "No Validation (Random Selection)"
- Green triangles for "Validation on Clean Labels"

Each method shows varying levels of performance improvements with these strategies. For instance, the green triangle line representing "Validation on Clean Labels" generally stays above 20%, indicating higher performance improvements compared to other strategies in most cases.

In the top right corner of the slide, there's an image of a person wearing glasses. In the bottom right corner, the number '6' indicates that this is likely the sixth slide in the presentation.</sample>
    <sample id="138">The image is a slide from a presentation, labeled "RQ1" at the top left corner. The main title of the slide reads "Main findings." Below this title, there is a graph that shows relative performance improvement over weak labels (%) on different models or methods: FTw, BOND, COSINE, MLC, and L2R.

The x-axis represents these five categories (FTw, BOND, COSINE, MLC, L2R), while the y-axis indicates the percentage range for relative performance improvement, which appears to be between 0% and 35%.

There are three lines in the graph:
1. A yellow line representing "Validation on Weak Labels."
2. A purple line with circles indicating "No Validation (Random Selection)."
3. A green line with triangles showing "Validation on Clean Labels."

Each line has data points scattered along it, suggesting variability within each category.

In the bottom right corner of the slide, there is a small icon of a person's face, likely representing the presenter or author of the slide. Additionally, the number '6' is visible in the bottom right corner, possibly indicating the slide number in the presentation.</sample>
    <sample id="139">RQ1 Main findings

The chart shows the relative performance improvement over weak models (%) for different methods: FTw, BOND, COSINE, MLC, and L2R. The data points are categorized into three groups based on validation type:

- Orange dots represent "Validation on Weak Labels."
- Purple circles indicate "No Validation (Random Selection)."
- Green triangles denote "Validation on Clean Labels."

Each method has a distinct line connecting its respective data points:
- The orange line with diamond markers starts around 5% and fluctuates slightly.
- The purple line with circle markers begins at about 10%, dips to approximately 8%, and then rises back up to roughly 10%.
- The green line with triangle markers starts near 10%, increases steadily to peak around 30%, and then gradually declines.

The y-axis is labeled "Relative performance improvement over weak model (%)" and ranges from 0% to 40%. The x-axis lists the five methods being compared.</sample>
    <sample id="140">The image shows a slide titled "Main findings" with the subtitle "RQ1." The main content is a line graph comparing different validation methods across various models: FTw, BOND, COSINE, MLC, and L2R. There are three lines on the graph representing:

- Orange circles for "Validation on Weak Labels"
- Purple squares for "No Validation (Random Selection)"
- Green triangles for "Validation on Clean Labels"

The y-axis of the graph measures relative performance improvement over weak labels (%) from 0 to 15%. Below the graph, there's a statement in bold that reads, "A clean validation set is indispensable." In the top right corner, there's an inset picture of a person wearing glasses.</sample>
    <sample id="141">The image shows a graph titled "Main findings" with the subtitle "RQ2". The x-axis is labeled "Validation" and ranges from 0 to 50, while the y-axis is labeled "Accuracy" and ranges from 75 to 85. There are five lines representing different methods: FTw (blue), COSINE (orange), LZR (green), BOND (red), MLC (purple), and Weak labels (grey). Each line has associated shaded areas indicating variability or confidence intervals for each method across different validation points.

In the top right corner of the slide, there's an inset photo of a person wearing glasses. In the bottom right corner, the number '7' indicates this is likely part of a larger presentation.</sample>
    <sample id="142">RQ2 Main findings</sample>
    <sample id="143">The image is a slide from a presentation, titled "R02" at the top left corner. It contains two main charts and some text.

On the left side of the slide, there's a line graph with multiple lines representing different datasets or models: FTw, COSINE, LZR, BOND, MLC, Weak labels, and AdapterC. The x-axis ranges from 0 to All Validation, while the y-axis represents Accuracy, ranging from about 75 to 85. Each dataset/model has its own colored line on the graph, showing how accuracy changes across validation stages.

On the right side, there's another chart labeled "Performance Delta (%)". This bar graph shows performance differences for various methods (FTc, LoRaC, BitFitC, AdapterC) against an 'All' category. A red dashed box highlights specific data points within this section.

Below these charts, there's a note that reads: "→ WSL approaches benefit from more clean validation samples!"

In the upper right corner of the slide, there's a small photo of a person wearing glasses, but their face is blurred out.

Overall, the slide appears to be summarizing key findings related to model performances in terms of accuracy and delta percentages during validation processes.</sample>
    <sample id="144">The image is a slide from a presentation, titled "R02" at the top left corner. The main content of the slide consists of two graphs side by side.

The graph on the left shows accuracy versus validation for different models or methods: FTc, COSINE, LZR, BOND, MLC, and Weak labels. Each model's performance is represented by a line with markers, and there are shaded areas indicating confidence intervals or variability around each line. The x-axis ranges from 0 to All in increments of 10, while the y-axis measures accuracy from about 75 to 85.

The graph on the right displays performance delta (Δ) versus all samples, comparing various approaches like FTc, LORAC, BitFitC, AdapterC, etc. This bar chart has bars extending both above and below zero, suggesting positive and negative performance deltas respectively. A red dashed box highlights certain sections of this graph, possibly drawing attention to specific results.

At the bottom of the slide, there is a note that reads, "WSL approaches benefit from more clean validation samples!" which suggests an implication drawn from the data presented in the graphs regarding the effectiveness of WSL under conditions where validation samples are cleaner or purer.

In the upper right corner of the slide, there is a blurred face photo of a person, likely the presenter or author of the slide.

Overall, the slide seems to be presenting findings related to machine learning or statistical modeling, focusing on how different approaches perform across varying levels of validation and emphasizing the importance of clean validation samples for better outcomes.</sample>
    <sample id="145">The image contains a slide titled "R02 Main findings" with two graphs and some text. The left graph shows accuracy on the y-axis ranging from 75 to 85, plotted against validation samples labeled as '5', '10', '20', and 'All'. Various lines represent different models: FTw, COSINE, LZR, BOND, MLC, Weak labels, and a shaded area for weak labels.

The right graph displays performance delta (%) on the y-axis, also spanning from -20 to +20, across the same validation sample categories ('5', '10', '20', and 'All'). Different colors indicate various methods or metrics such as FTc, LoRaC, BitFitC, and AdapterC.

Below these graphs are bullet points:
- WSL approaches benefit from more clean validation samples!
- But it is even better to use them for training (e.g., LoRaC).

A small photo of an individual appears in the top-right corner of the slide.</sample>
    <sample id="146">The image contains a slide titled "Main findings" with the subtitle "N=10 clean samples per class." It is divided into two sections, each showing graphs labeled "Before CFT" and "After CFT."

On the left side:
- The graph has an x-axis ranging from 76 to 84.
- The y-axis ranges from 76 to 86.
- There are three lines representing different methods: COSINE (orange), L2R (red), and Clean Only (blue).
- Both "Before CFT" and "After CFT" show slight improvements in accuracy for all methods.

On the right side:
- This section also shows two graphs under the same labels ("Before CFT" and "After CFT").
- Similar axes as on the left side range from 76 to 86 on the y-axis.
- Again, there are three lines representing COSINE (orange), L2R (red), and Clean Only (blue).
- These graphs indicate more significant improvements in accuracy after applying CFT compared to the left-side graphs.

Both sides of the slide highlight that using CFT leads to better performance across all tested methods.</sample>
    <sample id="147">The image is a slide from a presentation, labeled as "Q3" in the top left corner. The title of the slide reads "Main findings." It contains two line graphs comparing different models' performance before and after applying CFT (Contextual Feature Transformation). 

The first graph on the left shows results for N=10 clean samples per class. The y-axis represents accuracy/F1 score, ranging from 78 to 86, while the x-axis indicates whether it was applied before or after CFT. Three lines represent three different methods: COSINE (orange), L2R (red), and Clean Only (purple).

The second graph on the right displays results for N=30 clean samples per class, with similar axes labels but slightly higher accuracy/F1 scores.

In both graphs, all three methods show an improvement in accuracy/F1 score when CFT is applied post-training compared to pre-training. A small photo of a person appears in the top right corner of the slide.</sample>
    <sample id="148">The image shows a slide from a presentation with the title "R03 Main findings" at the top left corner. The main content of the slide is divided into two sections, each containing a graph.

On the left side:
- The heading reads "N=10 clean samples per class."
- Below this heading, there are two line graphs labeled "Before CFT" and "After CFT," showing accuracy/F1 scores on the y-axis.
- There are three lines in these graphs: one for COSINE (red), another for L2R (blue), and the last for Clean Only (orange).
- Both graphs show an increase in accuracy/F1 scores after applying CFT to all methods compared to before.

On the right side:
- The heading reads "N=30 clean samples per class."
- Similar to the left side, it contains two line graphs labeled "Before CFT" and "After CFT," also showing accuracy/F1 scores on the y-axis.
- Again, there are three lines representing COSINE (red), L2R (blue), and Clean Only (orange).
- These graphs demonstrate that using 30 clean samples per class results in higher accuracy/F1 scores post-CFT application than pre-CFT.

In both sections, the x-axis represents different conditions or scenarios under which the data was collected, but specific labels aren't provided within the visible portion of the image.

At the bottom right corner of the slide, there's a small icon or avatar of a person, likely indicating the presenter or author of the slide.

Overall, the slide presents comparative performance improvements across different sample sizes when applying CFT techniques.</sample>
    <sample id="149">The image is a slide from a presentation, titled "R03 Main findings." It features two line graphs comparing the performance of different methods on datasets with varying numbers of clean samples per class. The left graph shows results for N=10 clean samples per class, and the right graph for N=30 clean samples per class.

Both graphs have axes labeled "Accuracy/F1" (ranging from 78 to 86) on the y-axis and "Before CFT" and "After CFT" on the x-axis. Each graph includes four lines representing different methods: COSINE, L2R, Clean Only, and an unlabeled red line that appears to represent another method or baseline.

Below the graphs, there are two bullet points:
- Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches.
- No need to use complicated WSL methods; WSL performs equally well.

In the top-right corner of the slide, there is a small inset image of a person wearing glasses and a headset, likely indicating the presenter or author of the slide.</sample>
    <sample id="150">The image contains a slide from a presentation with the following content: 1. Title: "Conclusion" 2. Subtitle in red text: "Recent WSL approaches" - Bullet point: "Require clean samples." - Bullet point: "Overestimate their practicality." (There is an emoji of a sad face next to this bullet point.) 3. Subtitle in green text: "Our recommendations" - Bullet point: "Report the model selection criteria." - Bullet point: "Use Few-shot learning approaches as baselines." - Bullet point: "Always apply continuous fine-tuning (CFT)." There is also a small stack of colorful books or blocks icon near the last recommendation. The background of the slide is white, and there is a person's photo on the right side of the slide.</sample>
    <sample id="151">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="152">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="153">The image shows a slide from a presentation with the title "Conclusion" at the top. The content is divided into two main sections: 

1. **Recent WSL approaches**:
   - This section has two bullet points in red text.
     - "Require clean samples."
     - "Overestimate their practicality." (This point includes an emoji of a worried face to emphasize the concern.)

2. **Our recommendations**:
   - This section also contains three bullet points, but they are not highlighted or colored differently than the rest of the white background.
     - "Report the model selection criteria."
     - "Use Few-shot learning approaches as baselines."
     - "Always apply continuous fine-tuning (CFT)." (This last point features an emoji stack of books and a trophy to indicate importance or achievement.)

In addition to the textual content, there is a small circular photo on the right side of the slide showing a person wearing glasses.

At the bottom right corner of the slide, there's a page number indicating it is slide 9 out of what appears to be a larger presentation.</sample>
    <sample id="154">The image shows a slide from a presentation with the title "Conclusion" at the top. The content is divided into two main sections: 

1. **Recent WSL approaches**:
   - Bullet points include:
     - Require clean samples.
     - Overestimate their practicality.

2. **Our recommendations**:
   - Bullet points include:
     - Report the model selection criteria.
     - Use Few-shot learning approaches as baselines.
     - Always apply continuous fine-tuning (CFT).

There are visual elements such as an emoji of a sad face next to one of the bullet points under "Recent WSL approaches," and there's also a QR code in the bottom right corner, which seems to link to a website or additional information ("https://github.com/..."). Additionally, on the upper right side, there's a speech bubble graphic that says "THANK YOU!" There is no visible text indicating who created this slide.</sample>
    <sample id="155">The video does not provide any specific information about the findings of a previous study where human subjects were given the same persona prompts.</sample>
    <sample id="156">The sources of data used in this study are the Penn Treebank, Marcus et al. (1993), Ficler and Goldberg (2016), Gibson et al. (1996: 88-90).</sample>
    <sample id="157">There are two authors involved in the paper. The names of the authors are Adam Przepiorkowski and Michal Wozniak, as shown on the title slide at the beginning of the presentation.</sample>
    <sample id="158">Cognitive dissonance is closely related to tasks such as persuasion, attitude change, and decision-making.</sample>
    <sample id="159">There are two authors involved in the paper: Shuheng Liu and Alan Ritter.</sample>
    <sample id="160">There are seven authors involved in the paper.</sample>
    <sample id="161">The framework introduced in the image is distinct from previous works as it involves a multi-step process. First, instances are sampled and labeled with gold standards for tasks like "Eating With Hands" or "Can You Lie?" These labels serve to annotate annotations themselves. Then, these annotated data points are shared globally through crowdsourcing platforms such as MTurk. The final step includes processing model predictions based on this collected and processed data.

This approach differentiates itself by focusing on annotation quality control (gold standard labeling), leveraging global crowd-sourced labor, and using statistical methods (Pearson's R scores) to compare demographic characteristics of models against datasets. This method aims to enhance the reliability and fairness of AI systems' performance metrics across diverse populations.</sample>
    <sample id="162">GPT-3.5</sample>
    <sample id="163">The commercial systems compared are DeepL and Google Translate.</sample>
    <sample id="200">The paper involves six authors. Their names are David Vilár Torres, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. Each author's name is accompanied by a small photograph in the image provided.</sample>
    <sample id="201">The MPP evaluations were performed up to a context length of 900 tokens.</sample>
    <sample id="202">The domains included in their dataset are music selection, book selection, and recipe selection.</sample>
    <sample id="203">Positionality refers to the perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">The name of the speaker is Dietrich Klakow.</sample>
    <sample id="205">Yes, EDAtt adapts an existing offline ST model.</sample>
    <sample id="206">There are four authors involved in the paper.</sample>
    <sample id="207">Yes, the tested model works on the test suite. The bar chart shows that with task-specific training (BERT4Coef), there is a significant increase in performance compared to without it.</sample>
    <sample id="208">The three variants of KITMUS are: (a) Background-Pretrain, which is the typical setup; (b) Background-Both, where background knowledge is explicitly provided in context during both pretraining and inference time; and (c) Background-Inference, where knowledge is only available at inference-time.</sample>
    <sample id="209">The authors of the paper are affiliated with Google Research, as indicated by their names being listed under a logo for "Google Research" in the bottom left corner of the slide.</sample>
    <sample id="210">The last research question is: "How to use the available clean samples more efficiently?"</sample>
    <sample id="211">The metric sensitivity measures how sensitive a model is to various instructions for the same task. It evaluates whether the model consistently produces similar results regardless of slight variations in instruction wording, using mathematical equations and statistical formulas to quantify this consistency.</sample>
    <sample id="212">The speaker's name is Wenjun Peng.</sample>
    <sample id="213">Greater sensitivity indicates worse model performance.</sample>
    <sample id="214">Models receive a broad linguistic context during pretraining, which involves exposure to diverse language patterns and structures. This extensive training helps models understand various aspects of language use in different contexts. However, the caption suggests that this robustness may not always extend to acceptability judgments within specific contexts. The research indicates that while models can handle general language tasks well, their ability to judge sentence acceptability might be limited by contextual factors.</sample>
    <sample id="215">The slide titled "Main findings" indicates that typically, 40 clean validation samples are needed for good performance in WSL. This information is derived from the graph which shows various lines representing different models or methods and their corresponding accuracy levels as a function of increasing numbers of clean validation samples on the x-axis. The line labeled with blue dots represents this specific requirement of 40 clean validation samples for achieving good performance according to the study presented in the slide.</sample>
    <sample id="216">The authors of the paper are affiliated with Stanford Engineering, specifically in Computer Science.</sample>
    <sample id="217">There is a need to develop new methods for measuring media biases because the existing models, such as those based on BERT and GPT-3, may not accurately capture or represent different political perspectives. This could lead to biased outcomes in various applications where these models are used.</sample>
    <sample id="218">The speaker's name is Akthar Alaiwi.</sample>
    <sample id="219">The video begins with a slide titled 'LM Training Data' and subtitled 'A mixed blessing.' The main content is a bar chart showing the number of tokens (in billions) from various news websites, such as 'washingtonpost.com,' 'nytimes.com,' 'axios.com,' etc. Each website's name appears on the left side of the chart, while the corresponding number of tokens in billions is displayed at the top of each bar. At the bottom right corner, there is a citation: "Dodge, Jesse et al. 'Documenting Large Web Corpora: A Case Study Crowded Corpus.' Proceedings of the 37th Annual Conference on Computational Linguistics in Natural Language Processing 2021." In the top right corner, there is an image of a person.

The scene then transitions to another slide that outlines the political bias propagation pipeline. It starts with 'Pretraining data,' followed by an arrow pointing to 'Language models,' which leads to another arrow pointing to 'Downstream tasks.' This sequence visually represents the flow of information through pretraining data, language models, and downstream tasks. 

Next, additional text appears below this diagram, posing two questions related to the previous topics:
- How do LMs evaluate the political leaning of LMs?
- What role does pretraining data play in such political biases?

Additionally, it asks about performance metrics for different political leanings within language models.
- How do LMs with different political leanings perform?
- Does LM political leaning result in fairness issues in NLP applications?

The final frame maintains the same visual elements but includes these new textual additions, emphasizing the critical examination of how political biases are propagated throughout the process.</sample>
    <sample id="220">The simplification process does differ for DEplain-apa and web. This is evident from the bar charts on the right side of the image, which show different levels of transformation in various categories such as 'moving,' 'engineering,' 'lexical substitution,' 'word addition,' 'word deletion,' and 'sentence deletion.' The differences are highlighted by the distinct heights of the bars representing each method (DEplain-apa vs. DEplain-web) across these categories.</sample>
    <sample id="221">No, Coscript is not publicly available. The slide indicates that it's a specialized model fine-tuned for generating high-quality scripts and suggests that smaller language models (LMs) trained on Coscript can generate higher quality scripts than larger LLMs like GPT-3 or Codex.</sample>
    <sample id="222">The watermark is inserted into the text by first defining a target embedding, counting the trigger number in a sentence, and then adding this target embedding to the original embedding.</sample>
    <sample id="223">The affiliations of the authors are Penn State and Amazon.</sample>
    <sample id="224">The image shows a slide from a presentation titled "Analysis of Multilingual Training." The main content discusses the evaluation of mT5 and XLM-R + PTR on multilingual settings. It highlights that encoder-decoder models like mT5 can be improved by training in a mixture of languages.

The table presents various metrics for different models across multiple datasets, including MATIS, MGEOQuery, MSpider, NMaps, MOveright, MSchqa2018, MTOP, and Mlat. Two rows are labeled as "Monolingual" with specific scores: one for mT5 with 31.31 points and another for mT5 with 49.67 points. Another row is labeled "Multilingual," showing higher scores for both mT5 with 54.45 points and XLM-R + PTR with 55.65 points.

The average scores for each model type (Monolingual and Multilingual) are highlighted at the bottom right corner of the table:
- Monolingual Average: 58.16
- Multilingual Average: 61.82

The slide also includes a bullet point stating: "Enc-Dec/Enc/PTR (mT5/XLM-R) can be improved by training in a mixture of various languages."

In the top-right corner, there is an avatar or profile picture of a person named "Anu Prakash."</sample>
    <sample id="225">An example of constrained language planning is shown in the image, which provides instructions for making two different types of cakes: a strawberry cake and a chocolate cake. The text "Constrained Language Planning" at the top indicates that this is an illustration of how specific goals can be achieved within certain constraints when creating instructional content or recipes.</sample>
    <sample id="226">By using a specific method that is not described in the image, they ensure covertness.</sample>
    <sample id="227">The work builds a new pre-trained language model (PLM) by leveraging existing PLMs. Specifically, it utilizes the pre-trained models from CamemBert and PubMedBERT to create their own version of an English-only medical domain-specific PLM called NBDW. This approach allows them to benefit from the knowledge already acquired in these established models while adapting it for their particular use case within the medical field.</sample>
    <sample id="228">African Islamic</sample>
    <sample id="229">The speaker shows how the model leverages knowledge learned through the attention mechanism in sentence 3 of example sentences.</sample>
    <sample id="230">The amount of tasks impacts the model performance by showing a trend where increasing task clusters leads to varying levels of accuracy, as depicted in the graph.</sample>
    <sample id="231">The three treeless baselines that the authors compare their method with are: 1. LSTM seq2seq (represented by a blue bar) 2. TS (represented by an orange bar) 3. Zheng and Lapata (represented by a green bar) These comparisons allow readers to understand how well the new approach performs relative to these established methods in terms of structural generalization on COGS tasks.</sample>
    <sample id="232">The two co-authors, Alexander Koller and Ivan Titov, are listed alongside the first author, Matthias Lindemann. This typically indicates that they have collaborated with him on this research paper titled "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations." The presence of their names in a list format suggests an equal contribution to the work or publication mentioned by the title at the top of the slide.</sample>
    <sample id="233">Chowdery et al.</sample>
    <sample id="274">The speaker mentions three problems of SimulST models: 1. Specific architectures are usually trained, introducing additional modules to be optimized. 2. Long and complicated training procedures (e.g., different optimization objectives). 3. Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s,...).</sample>
    <sample id="275">To mitigate social and political biases in datasets when training NLP models, it is effective to sanitize the data before pretraining. This involves removing or altering potentially biased content from the dataset to ensure that the language model learns unbiased representations during its initial training phase.</sample>
    <sample id="307">The fluency of PaLM is comparable to SOTA (State-of-the-Art).</sample>
    <sample id="308">The important properties of a watermarking method, as presented in the image, are: 1. Applicable to EaaS (Elastic As-a-Service) - This implies that the watermarking technique should be versatile and adaptable for use with elastic service models. 2. Utility - The watermarking process should not compromise or degrade the functionality or quality of the original data or content being protected. It must maintain its utility while adding the watermark. 3. Covertness - The watermark should remain hidden from unauthorized users so it cannot be easily detected by attackers who might try to remove it without affecting the integrity of the original content. 4. Transferability - For effective protection, the watermark needs to transfer seamlessly across different services provided by the attacker's infrastructure. This means that if the watermarked content moves between various services within the attacker’s system, the watermark remains intact and functional throughout these transitions.</sample>
    <sample id="309">The 14 different languages into which the English TED talks have been translated are: Arabic, Chinese (Mandarin), German, Spanish, French, Hebrew, Italian, Japanese, Korean, Dutch, Portuguese, Romanian, Russian, and Turkish.</sample>
    <sample id="310">The framework involves sampling 300 instances from one dataset for reannotating. This is indicated in the first step of the process, where it states "300 instances are sampled from a dataset."</sample>
    <sample id="311">The distance metrics used for measuring the difference between benign and backdoor datasets are cosine similarity, Euclidean distance, and Manhattan distance.</sample>
    <sample id="312">The multilingual encoder-based models were used for this task by evaluating two groups of models on a monolingual setting. The first group, labeled as "ENC-PTR," consists of Multilingual Pretrained Encoders with Pointer-Based Decoders, including XLM-R + PTR and mBERT + PTR. The second group, labeled as "ENC-Dec," includes Multilingual Pretrained Encoder-Decoder Models such as mBERT + mBART and mBERT + mT5. According to the results shown in the table, ENC-Dec (mBERT + mT5) achieved the best performance across all datasets listed: MATIS, MGEOQuery, MGeOQuery, MSpiders, MSpiders, MOvernight, MCWQ, MCWQ, MCWQ, MCWQ, MCWQ, MCWQ, MCWQ, MCWQ, MCWQ, MCWQ, MCWQ, and MOvernight.</sample>
    <sample id="313">The image is a screenshot of an online presentation slide. The title at the top reads "The 61st Annual Meeting of the Association for Computational Linguistics" with the location and date specified as Toronto, Canada, July 9-14, 2023. Below this, there's a large heading that says "Distilling Script Knowledge from Large Language Models for Constrained Language Planning." Underneath the main title are several names listed: Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xiangyuan Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, and Deqing Yang. At the bottom left corner, there are logos or symbols in blue and red, possibly representing affiliations or organizations related to the presenters. On the right side of the image, partially cut off by the frame, appears to be a video feed showing a person who seems to be presenting remotely. They have light skin, dark hair tied back, wear glasses, and are dressed casually. In the background behind them, you can see what looks like a modern office space with white walls and some furniture visible.</sample>
    <sample id="314">The image shows a slide titled "Language Planning" with the subtitle "How to Make a Cake?" Below this, there is an instructional list for baking a cake: 1. Gather your ingredients. 2. Preheat the oven to 325°F (163°C) and grease and flour a cake pan. 3. Cream the butter and sugar. 4. Add eggs. 5. Stir in the dry ingredients. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour and 15 minutes.

On the left side of the slide, there are two emojis—a yellow face wearing glasses and winking, and a robot head—alongside text that reads "Large language models (LLMs) can effectively decompose goals into steps." On the right side of the slide, there is a video call interface showing a person sitting at a desk in what appears to be an office setting. The background includes white walls and some furniture.</sample>
    <sample id="315">The image shows a presentation slide titled "Language Planning" with the subtitle "How to Make a Cake?" The main content of the slide is a step-by-step recipe for making a cake, which includes: 1. Gather your ingredients. 2. Preheat the oven to 325°F (163°C) and grease and flour a cake pan. 3. Cream the butter and sugar. 4. Add eggs. 5. Stir in the dry ingredients. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour and 15 minutes.

Below this list are two emojis—a yellow face wearing glasses and a robot—next to text that reads, "Large language models (LLMs) can effectively decompose goals into steps."

In the top right corner of the image, there's a small video feed showing a person sitting at a desk in what appears to be an office environment. They have long hair and wear a green shirt.</sample>
    <sample id="316">Constrained Language Planning How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add the cocoa powder into the flour... Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="317">Constrained Language Planning How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add the cocoa powder into the flour... Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="318">The image contains a slide titled "Constrained Language Planning" with two sections: one for making strawberry cake and another for chocolate cake. Each section includes an image of the respective cake, followed by instructions to add specific ingredients (strawberry jam or cocoa powder) into flour. At the bottom of the slide, there is text discussing how abstract goals can be inherited from real-life specific goals through multi-faceted constraints. The background shows a person in a green shirt sitting at a desk in what appears to be an office environment.</sample>
    <sample id="319">The image contains a slide from a presentation with the title "How do LLMs perform on Constrained Language Planning?" The dataset mentioned is "wikiHow + Generated Constraints." There are three types of constraints listed: 1. Constraint Type 1: Modifier - Defined as an adjective or phrase that modifies or constrains an abstract goal, with examples like "Make a chocolate cake" and "Make a pink cake." 2. Constraint Type 2: Method - Described as a tool or specified mode that controls the process for achieving the goal, illustrated by examples such as "Make a cake with an oven" and "Make a cake by using cake mix." 3. Constraint Type 3: Intent - Explained as adding another purpose or demand when completing the goal, shown through examples including "Make a cake for wedding" and "Make a cake for diabetics." In the background, there's a person in what appears to be an office setting, wearing glasses and looking at something off-camera.</sample>
    <sample id="320">The image shows a presentation slide with the title "How do LLMs perform on Constrained Language Planning?" The dataset mentioned is "wikiHow + Generated Constraints." There are three types of constraints listed: 1. Constraint Type 1: Modifier - Defined as an adjective or phrase that modifies or constrains an abstract goal, with examples like "Make a chocolate cake" and "Make a pink cake." 2. Constraint Type 2: Method - Described as a tool or specified mode that controls the process for achieving the goal, illustrated by instructions to make a cake using different methods (with oven vs. mix). 3. Constraint Type 3: Intent - Explained as adding a purpose or demand when completing the goal, exemplified by making cakes for specific occasions such as weddings or diabetics. On the right side of the slide, there's a video call interface showing someone in what appears to be an office setting with large windows and modern furniture visible through them.</sample>
    <sample id="321">The image shows a presentation slide with the title "How do LLMs perform on Constrained Language Planning?" The dataset used for this analysis is labeled as "wikiHow + Generated Constraints." Below the title, there are three types of constraints listed: 1. Constraint Type 1: Modifier - Defined as an adjective or phrase that modifies or constrains an abstract goal (Examples given include making a chocolate cake and making a pink cake). 2. Constraint Type 2: Method - Described as a tool or specified mode that controls the process for achieving the goal (Examples provided involve using an oven to make a cake by using cake mix). 3. Constraint Type 3: Intent - Explained as an additional purpose or demand when completing the goal (Examples illustrate adding cake for wedding and diabetics). On the right side of the image, there's a person in what appears to be a modern office setting, suggesting they might be presenting or discussing the content shown on the slide.</sample>
    <sample id="322">The image contains a slide from what appears to be an academic or professional presentation. The main title of the slide is "Can LLMs do Constrained Language Planning?" This suggests that the content is focused on evaluating large language models (LLMs) in their ability to perform constrained language planning tasks.

Below the title, there is a bar graph labeled "Accuracy." The x-axis represents different models and configurations: T5 (11B), Flan-T5 (11B), GPT-3 (175B), CoT-GPT (175B), and InstructGPT (175B). Each model's performance is represented by a colored bar corresponding to its accuracy percentage.

At the bottom left corner of the slide, there is a statement highlighted with red text saying, "All baselines achieve unsatisfactory results on planning for specific goals." This indicates that none of the evaluated models performed well enough according to this particular metric.

In the background of the slide, there is a video feed showing someone presenting. They appear to be sitting at a desk in a room with modern furniture, suggesting they might be giving a remote presentation.

Overall, the slide seems to present findings about the limitations of various state-of-the-art language models when it comes to constrained language planning tasks.</sample>
    <sample id="323">The image presents a slide from a presentation titled "Can LLMs do Constrained Language Planning?" The main content of the slide includes a bar graph comparing different language models based on their accuracy. The models compared are T5 (11B), Flan-T5 (11B), GPT-3 (175B), CodeLLaMDA (175B), and InstructGPT (175B). Below the graph, there is a statement in red text that reads: "All baselines achieve unsatisfactory results on planning for specific goals." On the right side of the slide, there is an inset showing a person with long hair wearing glasses and headphones, seemingly engaged in a video call or recording session. The background behind this individual appears to be an office environment with desks and chairs visible.</sample>
    <sample id="324">The image shows a slide from a presentation with the title "What types of errors do LLMs usually make in this task?" The main content includes a radar chart and text. The radar chart has five axes labeled FE1: No constraint, SE1: Step(s), SE2: Repeated step(s), SE3: Incoherent, and SE4: Wrong order. Below the chart is a bullet point that reads: "The semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) cannot be guaranteed." On the right side of the image, there's a small video feed showing someone who appears to be presenting or participating in the meeting.</sample>
    <sample id="325">The image shows a slide from a presentation discussing errors made by Language Learning Models (LLMs) in generating scripts. The title of the slide asks, "What types of errors do LLMs usually make in this task?" Below the title is a radar chart with five axes labeled: FE1: No constraint, SE1: Repeat step(s), SE2: Wrong order, SE3: Incoherent, and FE3: Faithfulness to constraints. Each axis has a scale ranging from 0 to 100.

Below the radar chart, there is text that reads:
"The semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) can not be guaranteed."

In the upper right corner of the image, there is a small video feed showing a person wearing glasses and a green shirt, sitting at a desk in an office environment with large windows and other people visible in the background.</sample>
    <sample id="326">The image contains a chart titled "What kinds of goals do InstructGPT typically fail?" The chart lists various goal categories on the left, such as Work, Sports, Relationships, Personal Care, Traditions, Hobbies, Health, Family Life, Education, Electronics, Arts, and Step. Each category has corresponding values in columns labeled 0.67, 1.0, 0.5, 1.0, 0.5, 1.0, 0.4, 0.33, 0.28, 0.39, 0.28, 0.38, and 0.38.

Below the chart, there is text that reads: "The planning performance of InstructGPTs varies considerably for goals of different categories."

On the right side of the image, there is a person with long hair wearing glasses and a green shirt, sitting at a desk with headphones around their neck. They appear to be engaged in an activity related to the content shown in the slide.</sample>
    <sample id="327">The image shows a slide from a presentation titled "Method." The left side of the slide outlines an approach to generating specific goals using InstructGPT through in-context learning. It includes three steps: 1) Generate abstract goals, 2) Apply constraints, and 3) Specific Goals are listed with examples such as making a chocolate cake or baking it in a microwave.

On the right side of the slide, there is a video call interface showing a person wearing glasses and a green shirt. They appear to be seated at a desk in what looks like an office environment with various items visible on shelves behind them. 

The overall setting suggests that this could be part of an online lecture or meeting where someone is presenting their method for goal generation while being joined by participants via video call.</sample>
    <sample id="328">The image shows a presentation slide titled "Method" with the subtitle "Input: an abstract goal." The main content of the slide is divided into two sections. On the left, there's a box labeled "Step 1" which reads "Generate specific goals with InstructGPT via in-context learning," accompanied by an illustration of a robot head and some text bubbles.

On the right side of the slide, under the heading "Abstract Goal," it states "Make a cake" followed by constraints listed as "+ chocolate cake" and "+ microwave." Below this, there are three specific goals:
- G1 (with a modifier) + chocolate cake
- G2 (+ method): Make in a microwave
- G3 (+ intent): Make a cake for a wedding

In the background on the right side of the slide, there's a person wearing glasses and a green shirt, sitting at a desk in what appears to be an office environment with large windows allowing natural light to enter.</sample>
    <sample id="329">The image shows a slide from a presentation with the title "Method" at the top. The content of the slide is divided into two main sections.

On the left side, there's an illustration depicting steps for achieving an abstract goal using InstructGPT via in-context learning. It includes:

1. Step 1: Generate specific goals.
2. Step 2: Over-generate candidate scripts based on those goals.

On the right side, it details the process further:
- Abstract Goal: Make a cake
- Specific Goals include making a chocolate cake and microwaving one.
- An intent to make a wedding-related cake

Below this, there are placeholders labeled "Generate Plans for G1" followed by numbered boxes (1 through k), indicating multiple script candidates generated during this step.

In the background, we can see part of a room that appears to be used as a workspace or classroom, suggesting this might be a lecture or meeting setting where such presentations typically occur.</sample>
    <sample id="330">The image shows a detailed diagram explaining the method of generating candidate scripts and filtering them using similarity scores. The process is broken down into three main steps: 1. Over-generate candidate scripts via in-context learning, represented by multiple boxes labeled "k" with arrows pointing to different levels of similarity (0.3, 0.2, etc.). 2. Find scripts that align well with the goal using InstructGPT's similarity score system, illustrated as a funnel narrowing from various scripts to filtered ones marked with checkmarks. 3. Output specific goals accompanied by corresponding scripts, exemplified by Script 3 which includes instructions like "Gather your ingredients" and "Add the cocoa powder." This visual guide effectively communicates how scripts are generated and refined through this multi-step methodology.</sample>
    <sample id="331">The image shows a presentation slide titled "Method" with three main steps outlined on the left side. The first step is labeled "Step 2," which involves over-generating candidate scripts via in-context learning, represented by an icon of a robot and text within a red-bordered box. Below this, there's another step labeled "Step 3," indicating finding goals to align with InstructGPT using similarity score, depicted with a camera icon and more detailed instructions.

In the center-right part of the slide, there's a diagram showing multiple candidate scripts (labeled as 1, 2, ..., k) at the top, each associated with different probability scores ranging from 0.3 to 0.1. These are connected through arrows leading down to filtered scripts below, where some scripts have check marks next to them, suggesting they meet certain criteria or standards.

At the bottom right corner of the slide, there's a specific example of a filtered script: "Script 3 - Gather your ingredients, Add the cocoa powder." This indicates that after filtering, only one particular script remains relevant for further use.

On the right side of the image, partially visible behind the presentation content, is a person wearing glasses and a green shirt, seemingly engaged in presenting or discussing the material shown on the screen.</sample>
    <sample id="332">The image is a screenshot of a presentation slide titled "Method." The left side of the slide contains three steps outlined in text boxes with icons: Step 2 involves over-generating candidate scripts via in-context learning, and Step 3 requires filtering these scripts to find those that align with the goal using InstructGPT's similarity score. Below this, there's an output section showing a specific script labeled as Script 3, which reads: "1. Gather your ingredients" and "4. Add the cocoa powder."

On the right side of the slide, there are two diagrams. The top diagram shows Candidate Scripts numbered from 1 to k, each associated with different similarity scores (0.3, 0.5, etc.). These scores likely represent how closely the generated scripts match the desired outcome.

Below it, another diagram illustrates Filtered Scripts, where some scripts have been marked with red crosses indicating they do not meet the criteria, while others remain unchecked or possibly selected for further use.

In the background on the right side of the image, there appears to be a person wearing glasses and a green shirt, seated at a desk in what looks like an office setting. This suggests that the presenter might be giving a live demonstration or lecture related to the content shown on the slide.</sample>
    <sample id="333">The image shows a presentation slide with the title "Our Method Greatly Improves the Planning Quality" in bold red text. Below this, there is a bar graph comparing different models based on their accuracy percentage: T5 (11B), Flan-T5 (11B), GPT-3 (175B), InstructGPT (175B), and Our Method. The bars are color-coded to match these labels.

At the bottom of the slide, there is a statement that reads: "With our method, InstructGPT can generate scripts of higher quality by a large margin." This sentence emphasizes the significant improvement offered by the proposed method over existing ones like T5, Flan-T5, GPT-3, and InstructGPT.

In the background, there is an inset video call window showing a person wearing glasses and a green shirt, seated at a desk in what appears to be an office environment.</sample>
    <sample id="334">The image shows a slide from a presentation titled "Script Distillation from LLMs" with the subtitle "Motivation." The main goal of this process is to enable constrained language planning for smaller models. It follows a methodological approach, starting with symbolic knowledge distillation.

The steps outlined in the methodology are as follows:

1. **Step 1:** Generate specific goals using InstructGPT via in-context learning.
2. **Step 2:** Over-generate candidate scripts and filter them based on context with InstructGPT using an instructor score.
3. **Step 3:** Filtered scripts should be used to guide further training or testing.

The final objective is to produce output-specific goals corresponding plans that can be validated through human annotation.

In addition to the textual content, there's also a video feed showing someone presenting these concepts, likely explaining the script distillation process in more detail during their lecture or webinar.</sample>
    <sample id="335">The image shows a presentation slide titled "Script Distillation from LLMs" with three main sections: Motivation, Method, and Input. The Motivation section explains the goal of enabling constrained language planning for smaller models using symbolic knowledge distillation. In the Method section, it details generating 55,000 scripts with constraints based on a CoSprint Dataset and annotating them by humans for validation and test sets.

The Input section outlines a process in three steps:
1. Generate specific goals with InstructGPT via in-context learning.
2. Over-generate candidate scripts with InstructGPT via context with instruction distillation.
3. Filter selected scripts to the goal with InstructGPT via context with instruction distillation score.

The Output is described as having corresponding plans. To the right of the text, there's an inset showing a person wearing glasses and sitting at a desk in what appears to be an office setting.</sample>
    <sample id="336">Script Distillation from LLMs

Motivation
- To enable constrained language planning for smaller models.

Method
- Follow the idea of symbolic knowledge distillation.
- Generated 55,000 scripts with constraint from LLMs based on our method = Coscript Dataset.
- Human annotate validation and test sets.</sample>
    <sample id="337">Script Distillation from LLMs

Motivation
- To enable constrained language planning ability for smaller models.

Method
- Follow the idea of symbolic knowledge distillation.
- Generated 55,000 scripts with constraint from LLMs based on our method = Coscript Dataset.
- Human annotate validation and test sets.

Step 1: Generate specific goals with InstructGPT via in-context learning. Input: an abstract

Step 2: Over-generate candidate scripts with constraints with InstructGPT via in-context learning. Output: specific goals with corresponding plans

Step 3: Filtered scripts to the goal using InstructGPT with Coscript Dataset by instruction score. Output: specific goals with corresponding plans</sample>
    <sample id="338">The image shows a presentation slide titled "Script Distillation from LLMs" with the subtitle "Motivation." The motivation section explains that this process aims to enable constrained language planning for smaller models. Below, under "Method," it outlines three steps: 1. Generate specific goals using InstructGPT via in-context learning. 2. Over-generate candidate scripts and filter them based on context with InstructGPT using a constraint score. 3. Filter scripts further by their constraint scores and annotate validation and test sets.

On the right side of the slide is an inset video call showing a person wearing glasses and speaking into a microphone. They appear to be presenting or discussing the content shown on the slide.</sample>
    <sample id="339">The video begins with a slide titled 'Constraint Analysis' in bold red text. Below the title, there is a pie chart divided into several segments representing different constraints: Ingredient (21.57%), Modifier (45.05%), Method (36.88%), Using (15.45%), By (17.87%), and Date (10.39%). The segment labeled 'Ingredient' is highlighted in purple, indicating it has the largest share of 21.57%. At the bottom left corner of this slide, there is a note that reads, 'Coscript shows high heterogeneity and pluralism in the generated specific goals.' On the right side of the frame, part of an individual wearing glasses appears to be presenting.

Next, another slide titled 'Coscript for Smaller Language Models' introduces two main sections: Datasets and Metrics. Under Datasets, two datasets are listed: Coscript and wikiHow. Under Metrics, it mentions that DeBERTa (v3 large) model is faithful, used to decide whether the generated texts are faithful to the constraints, and lists automatic metrics such as ROUGE, BLEU, BERTScore. This section also includes a small image of an individual on the right side of the frame.

Following this, a bar graph comparing the accuracy of various models including GPT-3 (175B), Codex (175B), InstructGPT (175B), T5 trained on wikiHow, and T5 trained on Coscript is shown. The y-axis represents Accuracy, ranging from 0 to 75. Each bar corresponds to one of the mentioned models, with varying heights indicating their respective accuracies. A note at the bottom states, 'Smaller LM's fine-tuned on Coscript can generate higher quality scripts than LLMs,' highlighting the effectiveness of smaller language models when fine-tuned on Coscript data. The same individual seen previously continues to present, maintaining continuity throughout the presentation.

The final scene reiterates the comparison between specialized models versus Large Language Models (LLMs). It features a similar layout to the previous slides but focuses solely on the textual content without any visual elements like charts or graphs. The background remains consistent with the modern office setting visible through windows behind the presenter.</sample>
    <sample id="340">The image contains a slide from a presentation with the title "Specialized Models vs. LLMs" at the top in red text. Below this, there is a bar chart comparing different models based on their accuracy percentages: GPT-3 (175B), Codex (175B), InstructGPT (175B), T5 trained on wikiHow, and T5 trained on Coscript. The bars show varying levels of accuracy, with the highest being for the model fine-tuned on Coscript.

Below the chart, there is a statement in black text that reads: "Smaller LM's fine-tuned on Coscript can generate higher quality scripts than LLMs."

In the bottom right corner of the image, there is a small video feed showing a person wearing glasses and a green shirt, sitting in what appears to be an office or meeting room setting. 

At the very bottom center of the image, the number 15 is visible, likely indicating the slide number in the presentation.</sample>
    <sample id="341">The image contains a slide from a presentation with the title "Summary and Takeaways" in red text. The content of the slide is as follows:

1. **Establish the constrained language planning problem.**
2. **Evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs.**
3. **Use LLMs to generate a high-quality script dataset (CoScript) for constrained language planning.**

4. **Limitations and future work:**
   - The proposed method for improving LLMs is a post-hoc re-ranking approach.
   - CoScript only inherits from an abstract one with one extra constraint.
   - CoScript dataset can be a valuable resource to advance the research on language planning with more complex and diverse goals and constraints.

In the bottom right corner, there is a small video feed showing a person wearing glasses and speaking into a microphone. The background behind the speaker shows a modern office environment with large windows and desks.</sample>
    <sample id="342">The image contains a slide from a presentation with the title "Summary and Takeaways" in red text. The content of the slide is as follows: 1. Establish the constrained language planning problem. 2. Evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs. 3. Use LLMs to generate a high-quality script dataset (CoScript) for constrained language planning. Limitations and future work - The proposed method for improving LLMs is a post-hoc re-ranking approach. - CoScript only inherits from an abstract one with one extra constraint. - CoScript dataset can be a valuable resource to advance research on language planning with more complex and diverse goals and constraints. In the background, there is a person wearing glasses and a green shirt, sitting at a desk with various items around them, including papers and office supplies.</sample>
    <sample id="343">The 61st Annual Meeting of the Association for Computational Linguistics Toronto, Canada July 8-14, 2023 Distilling Script Knowledge from Large Language Models for Constrained Language Planning Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xu, Deqing Yang syyuan21@m.fudan.edu.cn https://github.com/siyuuyuan/coscript</sample>
    <sample id="344">The authors decide what moderate-frequency words are by counting the word frequency on a general text corpus \( D_p \). They then randomly select \( n \) words within a moderate-frequency interval. This process helps in identifying and selecting words that appear with a certain level of regularity, which is crucial for tasks like watermark injection or other linguistic analyses where specific frequencies need to be controlled or manipulated.</sample>
    <sample id="371">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi EMORY UNIVERSITY Emory NLP Research Lab alexa</sample>
    <sample id="372">The image contains a title and author information for what appears to be an academic presentation or paper. The main text reads: "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems." Below this, it lists three authors as follows: Sarah E. Finch, James D. Finch, and Jinho D. Choi.

At the bottom of the image, there are logos representing affiliations with Emory University, Emory NLP Research Lab, and Amazon Alexa.</sample>
    <sample id="373">The video features a slide titled "Comparative Evaluation" from Emory University, prominently displayed at the top. The slide is part of an Alexa presentation, as indicated by the logos in the bottom corners.

Initially, there are two speech bubbles on the left side with one human icon and three robot icons inside them. On the right side, there is another set of four empty speech bubbles without any icons or text.

As the video progresses, more elements appear: additional robot icons within the blue speech bubbles on the left, bringing their total to five. Similarly, new purple robot icons emerge within the previously empty pink speech bubbles on the right, increasing their count to six.

Throughout the sequence, the background remains white, keeping the focus on the evolving content of the speech bubbles. The consistent presence of the Emory University logo and the Alexa logo reinforces the academic context of the evaluation being presented.</sample>
    <sample id="374">Comparative Evaluation Likert Rating Evaluation</sample>
    <sample id="375">The video begins with a slide titled "Likert Rating Evaluation" from Emory University. The slide features an illustration of two characters having a conversation, represented by blue speech bubbles and icons showing the Alexa logo. Below this, there is a Likert scale ranging from 1 to 5, indicating different levels of evaluation. A green checkmark next to the number 3 suggests that it is the selected rating.

The scene then transitions to another slide under the title "Dimensions of Dialogue Quality." This new slide has a central box labeled "Dialogue Quality," which branches out into three smaller boxes: "Relevance," "Consistency," and "Emotional Understanding." Arrows point downward from each of these smaller boxes back to the central "Dialogue Quality" box, illustrating their interconnectedness in evaluating dialogue quality.

The final part of the video continues to focus on the same slide titled "Dimensions of Dialogue Quality." It reiterates the structure where "Dialogue Quality" is at the center, connected to "Relevance," "Consistency," and "Emotional Understanding." These elements are emphasized as key dimensions for assessing dialogue quality, maintaining consistency throughout the presentation.</sample>
    <sample id="376">The image is a screenshot of a presentation slide titled "Likert Rating Evaluation." The title bar at the top includes logos for Emory University and Alexa. Below the title, there are several visual elements: 

1. A cartoon character with black hair.
2. Three speech bubbles containing blue icons resembling chatbots or AI assistants.
3. An illustration of a person holding scales, symbolizing judgment or evaluation.

At the bottom left, there is a scale from 1 to 5 with circles indicating different ratings, and one circle on the right side has been marked with a green checkmark. At the very bottom center, text reads "Rate the relevance of the bot's responses."

The overall theme suggests that this slide is part of an interactive session where participants evaluate the relevance of responses given by a robot or AI assistant using a Likert scale rating system.</sample>
    <sample id="377">Likert Rating Evaluation Rate the relevance of the bot's responses 1 5</sample>
    <sample id="378">The image is a slide from a presentation titled "Annotating Behaviors in Chat (ABC-Eval)." It features several speech bubbles connected to icons of people, indicating different types of behaviors or interactions. The logos for Emory University and Alexa are present at the bottom left and right corners, respectively.

Initially, there are five empty speech bubbles next to three black silhouette icons representing people. 

As the sequence progresses, two blue speech bubbles appear with timestamps (0:32) and labels ("1" and "2"), suggesting specific actions or messages being annotated. An arrow labeled "Irrelevant" points towards one of these blue speech bubbles.

Further along, another label appears on an adjacent speech bubble, reading "Lack of Empathy Self Contradiction," which seems to describe the content or context of that particular message.

Throughout this progression, additional elements like arrows and text annotations continue to be added to clarify the analysis or evaluation process related to chat behaviors within the ABC-Eval framework.</sample>
    <sample id="379">Annotating Behaviors in Chat (ABC-Eval) EMORY UNIVERSITY alexa</sample>
    <sample id="380">ABC-Eval Behaviors Coherence Knowledge Consistency Emotional Understanding EMORY UNIVERSITY alexa</sample>
    <sample id="381">ABC-Eval Behaviors Coherence Consistency Knowledge Emotional Understanding Ignoring Partner Irrelevant</sample>
    <sample id="382">ABC-Eval Behaviors Coherence Ignoring Partner Irrelevant Consistency Self Contradiction Partner Contradiction Knowledge Incorrect Fact Commonsense Violation Emotional Understanding Empathetic Response Lack of Empathy</sample>
    <sample id="383">The video begins with a slide titled "Experiments" in bold text, featuring two bullet points: "4 Open-Domain Dialogue Models" and "100 Human-Bot Conversations per Model." The background is white with a blue header. In the top right corner of each frame, there is an image of a person wearing a light-colored shirt. At the bottom left corner of the first few frames, the Emory University logo appears alongside another circular emblem. As the sequence progresses, the second bullet point remains unchanged while the first one transitions to include additional information about different types of conversations such as "Irrelevant," "Knowledge," "Incorrect Fact," "Commonsense Violation," "Consistency," "Self Contradiction," "Emotional Understanding," "Empathetic Response," and "Partner Contradiction."

The scene then shifts back to a simpler version of the same slide without any added details or images, maintaining the original content.

Next, a new element labeled "ABC-Eval" appears on the left side of the slide, accompanied by a diagram illustrating a conversation between a human (depicted as a head-and-shoulders icon) and various models represented by icons connected by lines. This addition suggests that the ABC-Eval framework might be related to evaluating the dialogue models mentioned earlier.

Throughout this part of the presentation, the consistent elements are the title "Experiments," the bullet points detailing the number of open-domain dialogue models and conversational examples, the presence of the person's image in the top right corner, the Emory University logo at the bottom left, and the Alexa logo at the bottom right.</sample>
    <sample id="384">The image shows a presentation slide titled "Experiments" with the following content:

- 4 Open-Domain Dialogue Models
- 100 Human-Bot Conversations per Model

There is also an illustration labeled "ABC-Eval," which depicts a flowchart of interactions between humans and bots. Below this, there are two Likert scale ratings: one marked as '5' (indicating high approval) for ABC-Eval and another marked as '1' (indicating low approval) for Turn Likert.

Additionally, on the right side of the slide, there are three sections:
1. "Turn Likert" showing a Likert scale from 1 to 5.
2. "Dialogue Likert" showing a Likert scale from 1 to 5.
3. A section labeled "Comparative" depicting various interaction scenarios involving multiple users and bots.

At the bottom left corner of the slide, there are logos for Emory University and Alexa.</sample>
    <sample id="385">The video begins with a slide titled "Experiments" from Emory University, featuring the Alexa logo. The text on the slide reads: "4 Open-Domain Dialogue Models" and "100 Human-Bot Conversations per Model." Below this title are three diagrams labeled "ABC-Eval," "Turn Likert," and "Dialogue Likert," each depicting conversations between humans and bots represented by icons of people and robots respectively.

The scene transitions to another slide under the heading "Baseline Evaluations." This slide also features the same three diagrams as before but now includes checkmarks indicating successful evaluations for all models in both "Turn Likert" and "Dialogue Likert."

Next, additional labels appear below these diagrams, listing evaluation criteria such as "Consistency," "Emotional Understanding," "Informativeness," and "Overall Quality" for the first two diagrams. For the third diagram, new criteria like "Engagingness," "Grammaticality," "Proactivity," and "Relevance" are added.

Finally, the last frame shows no changes from the previous one, maintaining the same layout and content. It lists the following evaluation criteria:
- Consistency
- Emotional Understanding
- Informativeness
- Overall Quality (for Turn Likert)
- Engagingness
- Grammaticality
- Proactivity
- Relevance (for Comparative)

The bottom left corner consistently displays the Emory University logo throughout the clip.</sample>
    <sample id="386">The image shows a slide titled "Inter-Annotator Agreement" with the subtitle "Krippendorf's Alpha." The graph displays various categories of data points, including ABC-Eval, Turn Likert, Dialogue Likert, and Comparative. Each category has multiple subcategories listed below them in orange text. There are red and blue markers on the graph indicating different values or trends for each category. Two yellow arrows point to specific areas on the graph: one near the top left pointing towards the higher Krippendorf's Alpha values associated with the ABC-Eval category, and another at the center right highlighting the mid-range values between 0.3 and 0.4 across several categories.

In the bottom corners of the image, there are logos from Emory University and Alexa, respectively. Additionally, there is an inset picture of a person in the upper right corner of the slide.</sample>
    <sample id="387">Predictive Validity Interactive Qua, Interactive Qua, ABC-Eval Turn Likert Dialogue Likert Comparative Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent advances in stem cell research have led to the development of new strategies for tissue engineering and regenerative medicine. Among these are the use of three-dimensional (3D) bioprinting to produce complex tissue constructs and the use of patient-specific iPSCs to generate personalized therapeutic islets. These approaches hold great promise for the treatment of diabetes and other diseases. However, there are still several challenges that need to be addressed before these technologies can be translated into clinical applications. One major challenge is the lack of standardized methods for the production of functional islets from iPSCs. Current methods for generating islets from iPSCs are heterogeneous and inconsistent, which limits their scalability and reproducibility. Standardization of iPSC-derived islet production protocols is crucial for the development of reliable and efficient therapies. Another challenge is the immunogenicity of iPSC-derived islets. Current methods for immune tolerance induction are limited and often result in incomplete tolerance or the development of new autoimmune responses. New strategies for immune tolerance induction are needed to ensure long-term safety and efficacy of iPSC-derived islets. Additionally, there is a need for improved methods for the transplantation of iPSC-derived islets. Current transplantation techniques are limited by poor engraftment rates and host rejection. Development of novel transplantation methods that enhance engraftment and reduce host rejection is essential for the successful translation of iPSC-derived islets into clinical applications. In conclusion, while recent advances in stem cell research have shown great promise for the treatment of diabetes and other diseases, there are still several challenges that need to be addressed before these technologies can be fully realized. Standardization of iPSC-derived islet production protocols, development of new strategies for immune tolerance induction, and improvement of transplantation methods are critical areas of ongoing research. Continued progress in these areas will be essential for the successful translation of iPSC-derived islets into clinical applications and ultimately improve patient outcomes.</sample>
    <sample id="388">Predictive Validity Interactive Qua, Interactive Qua, Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Other Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent Recent advancements in sequencing technologies have enabled the generation of large-scale datasets of microbial communities from a variety of environmental samples, including those from the built environment. However, the dynamics of indoor microbial communities and their potential influence on human health remain elusive. Here we take advantage of an existing dataset of 14 high-resolution 16S rRNA gene sequencing samples from building materials (e.g., vacuumed dust from air handlers, carpet, and carpet dust from five offices in two different buildings in Santiago, Chile to investigate the structure and dynamics of the indoor microbiome in shared interior spaces. We applied a combination of unifrac distances and kernel correspondence analysis (KCA) to explore the structure of the data and identified four distinct clusters based on the composition of OTUs. The first cluster was dominated by bacteria from the class Actinobacteria, mainly from the order Actinomycetales, followed by fungi from the class Ascomycesec. The second cluster contained bacteria from the class Betaproteobacteria, mainly from the order Burkholderiales, and fungi from the class Mortierellomycetes. The third cluster comprised bacteria from the class Gammaproteobacteria, mainly from the order Alphaproteobacteria, and fungi from the class Glomeromycota. The fourth cluster included bacteria from the class Alpha-proteobacteria, mainly from the order Rhodapacteroides, and fungi from the class Saccomycesec. The temporal dynamics analysis revealed that the community structure of the indoor microbiome changes with time, showing a clear separation between winter and summer samples within each building. There was no significant difference in community structure between the two buildings. Our results suggest that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Furthermore, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that the indoor environment is characterized by a high diversity of bacteria and a lower diversity of fungi. Additionally, we observed that</sample>
    <sample id="389">Title: Incremental Validity

Graph:
- Y-axis label: % of Quality Explained (R²)
- X-axis labels: ABC-eval, Turn UBERT, Dialogue UBERT
- Data points with labels on the graph include:
  - Unempathetic
  - Redundant
  - Jeff Conn
  - Proactive
  - Engaging
  - Relevance
  - Emotion
  
Logos at the bottom left corner:
- Emory University logo
- Alexa logo</sample>
    <sample id="390">The image is a slide from a presentation titled "Incremental Validity." The main content of the slide includes a graph with three axes labeled "ABC-eval," "Turn UBERT," and "Dialogue UBERT" on the x-axis, and "% of Quality Explained (R²)" on the y-axis. Several points are plotted on this graph, each labeled with different terms such as "Unempathetic," "Relevant," "Proactive," "Engaging," "Jeff Conn," and others.

In the top right corner of the slide, there is an inset showing a person in a small window, likely indicating that they are presenting or participating remotely. At the bottom left corner, there is a logo for Emory University, and at the bottom center, there is another logo for Alexa.

Throughout the sequence, no changes occur; all elements remain consistent across the frames provided.</sample>
    <sample id="391">Title: Incremental Validity

[Graph Description]
- The graph is titled "Incremental Validity."
- It plots "% of Quality Explained (R²)" on the y-axis against three different methods or frameworks labeled along the x-axis: ABC-eval, Turn UBERT, and Dialogue UBERT.
- Various points are plotted across these axes with labels such as Unempathetic, Redundant, Jeff Conn, Proactive, Engaging, Relevance, Emotion, etc.

[Logos at the bottom]
- Emory University logo
- Alexa logo

[Additional elements]
- There is a small image in the top right corner showing part of a person's face.</sample>
    <sample id="392">ABC-Eval Error Rates by Model

The image contains a bar chart titled "ABC-Eval Error Rates by Model." The x-axis of the chart lists various error categories, including:
- Antisocial
- CS Contra.
- Ignore
- Incorrect
- Irrelevant
- Unempathetic
- Other Contra.
- Redundant
- Self Contra.
- Topic Switch
- Uninterpret.

The y-axis represents the percentage of turns. Below the chart, there are logos and names of different models evaluated: BART-FID-RAG, Blender2, Emora, and Blender-Decode. In the bottom left corner, there is text that reads "EMORY" with an accompanying logo, indicating the affiliation or source of the data. Additionally, in the top right corner, there is a small overlay showing part of a person's face, suggesting this might be from a video presentation or webinar.</sample>
    <sample id="393">ABC-Eval Error Rates by Model

The image shows a bar chart titled "ABC-Eval Error Rates by Model." The y-axis is labeled "% of Turns" and ranges from 0 to 30. Various error categories are listed on the x-axis, including: 

- Antisocial
- CS Contra.
- Ignore
- Incorrect
- Irrelevant
- Unempathatic
- Other Contra.
- Redundant
- Self Contra.
- Topic Switch
- Uninterpret.

Below these categories, there are logos representing different models:

- BART-FID-RAG
- Blender2
- Emora
- Blender-Decode

The bars in the chart represent the error rates for each model across the various categories. Some notable points include:
- High error rates for some categories like "Topic Switch."
- Lower error rates for others such as "Antisocial."

There is also an arrow pointing towards the category "CS Contra." with text that reads "15%."

In the bottom left corner, there is a logo for Emory University, and in the top right corner, there is a small video feed showing a person speaking.</sample>
    <sample id="394">ABC-Eval Error Rates by Model

- Antisocial
- CS Contra.
- Ignore
- Incorrect
- Unempathatic
- Other Contra.
- Redundant
- Self Contra.
- Topic Switch
- Uninterpret.

BART-FID-RAG | Blender2 | Emora | Blender-Decode</sample>
    <sample id="395">ABC-Eval Error Rates by Model

The image contains a bar chart titled "ABC-Eval Error Rates by Model." The x-axis lists various models: BART-FID-RAG, Blender2, Emora, and Blender-Decode. The y-axis represents the percentage of turns, ranging from 0 to 30%. Different categories are labeled along the top in orange text: Antisocial, CS Contra, Ignore, Incorrect, Irrelevant, Unempathetic, Other Contra, Redundant, Self Contra, Topic Switch, and Uninterpret.

Each model has multiple bars corresponding to these categories, with varying heights indicating different error rates for each category across the models. For example, the 'BART-FID-RAG' model shows higher error rates in some categories compared to others, while 'Blender2' appears to have lower overall error rates but still varies significantly between categories like 'Topic Switch' and 'Uninterpret.'

In the bottom left corner, there is an Emory University logo, and on the right side, there's an Alexa logo at the bottom edge of the frame. In the upper right corner, there is a small inset showing a person wearing glasses.</sample>
    <sample id="396">The video begins with a detailed bar chart titled "ABC-Eval Error Rates by Model." The x-axis lists various error categories such as 'Antisocial,' 'CS Contra,' 'Ignore,' and others, while the y-axis represents the percentage of turns. Different models are compared: BART-FID-RAG (blue), Blender2 (green), Emora (red), and Blender-Decode (purple). Each model's performance across different error types is shown through colored bars.

The scene then transitions to another slide with a title at the top reading "Thanks For Watching!" Below this, there are references provided for further information:
- Paper: https://arxiv.org/pdf/2212.09180.pdf
- GitHub: https://github.com/emorynlp/ChatEvaluationPlatform

Contact information includes email addresses: {sfillwo, jdfinch, jinho.choi} @ emory.edu, along with the website link: https://www.emorynlp.org.

The final part of the video shows the same thank you slide but without any additional text or changes in content. It maintains the reference links and contact information from the previous frame, providing consistency throughout the presentation.</sample>
    <sample id="397">The approach uses 20-millisecond speech segment size.</sample>
    <sample id="398">The example with Servin and Kea requires knowledge about the specific roles of these individuals. Servin is a judge, which means he decides cases in court law; therefore, his action of being happy to relax makes sense after a long day at work deciding cases. This kind of specialized information falls under entity-specific knowledge.</sample>
    <sample id="399">Example quality is more important than similarity to source sentence.</sample>
    <sample id="400">The paper focuses on language models like BERT-base, RoBERTa-base, and their variations.</sample>
    <sample id="401">The model combines the attention scores from several layers.</sample>
    <sample id="402">The examples of direct inference are "easy on me" and "the first one."</sample>
    <sample id="403">The affiliations of the authors are: 1. University of Hong Kong (represented by a logo) 2. Brain Technologies Inc.</sample>
    <sample id="404">There are six authors involved in the paper. Their names and affiliations are listed below their respective numbers: 1. Yanis Labrak, Adrien Bazege, Richard Dufour, Mickael Rouvier; 2. Emmanuel Morin; 3. Beatrice Daille; 4. Pierre-Antoine Gourraud.</sample>
    <sample id="405">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="406">The authors gave the example of a "woman warrior" as a marked group.</sample>
    <sample id="407">Some model architectures do not generalize well, but the slide does not specify which ones.</sample>
    <sample id="408">FT_C, LORAC, BitFitFC, AdapterC</sample>
    <sample id="409">The paper involves six authors. Their names are Akthar Alaiwi, Martin Pomsl, Kaheer Saleem, Adam Trischler, Alexandra Olteanu, and Jackie CK Cheung. The slide also notes that these individuals have equal contribution to the work presented in the paper.</sample>
    <sample id="410">The author works with multiple modalities, including text and visual content.</sample>
    <sample id="411">DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains Yanis Labrak1,4 Adrien Bazoge2,3 Richard Dufour2 Michael Rouvier1 Emmanuel Morin2 Béatrice Daille2 Pierre-Antoine Gourraud3 1) LIA, Avignon Université 2) LSIN, Nantes Université 3) Clinique des diaboliques, CHU de Nantes 4) Zenicod: LABORATOIRE DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET DE BIO-INFORMATIQUE ET</sample>
    <sample id="412">Summary I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT</sample>
    <sample id="413">The image contains a slide with the title "Summary" in red text. Below this, there are four bullet points listed: 1. Language Modeling in Healthcare 2. Comparison of pre-training strategies, data sources and sizes 3. Evaluation of 13 models on 11 tasks 4. Distribution of NACHOS and DrBERT At the bottom right corner of the slide, there is an emblem or logo that reads "Avignon Université." The background of the slide is white, and it appears to be part of a presentation possibly related to artificial intelligence or machine learning applications in healthcare.</sample>
    <sample id="414">The image shows a slide from a presentation with the title "Summary" at the top. The content of the summary is organized into four main points: 1. Language Modeling in Healthcare 2. Comparison of pre-training strategies, data sources and sizes 3. Evaluation of 13 models on 11 tasks 4. Distribution of NACHOS and DrBERT At the bottom right corner of the slide, there is a logo that reads "Avignon Université." Additionally, there is an inset video call window showing a person who appears to be presenting or participating in the meeting.</sample>
    <sample id="415">The image shows a slide from a presentation with the title "Summary" in red text at the top. The content of the summary is divided into four main points: 1. Language Modeling in Healthcare, 2. Comparison of pre-training strategies, data sources and sizes, 3. Evaluation of 13 models on 11 tasks, 4. Distribution of NACHOS and DrBERT. In the bottom right corner, there's a logo for Avignon Université, indicating that this presentation might be related to or hosted by this institution. Additionally, there is an inset video window showing a person who appears to be presenting or participating in the webinar.</sample>
    <sample id="416">The image shows a slide from a presentation titled "Language Modeling." The content of the slide discusses various aspects and challenges related to language modeling in natural language processing (NLP). Here is the detailed text on the slide:

- Transformer-based approaches, such as BERT, offer huge performance gains on a lot of NLP tasks
- Has been adapted to French with CamemBERT and FlauBERT
- On medical tasks, domain-specific models in English raised the bar even higher
  - PudMedBERT, BioBERT, ClinicalBERT and other
- Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model
- Unlike generic models, no open-source model is available for biomedical domain in French yet
- BERT-based domain specific model for French should increase performance on medical tasks

At the bottom right corner of the slide, there is a logo that reads "Avignon Université."

Additionally, there is a small video feed in the top-right corner showing a person who appears to be presenting or participating in the webinar.</sample>
    <sample id="417">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gains on a lot of NLP tasks. It has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain-specific models in English raised the bar even higher. • PubMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domain in French yet. BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="418">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raised the bar even higher PubMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model Unlike generic models, no open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks Avignon Universite</sample>
    <sample id="419">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raised the bar even higher • PubMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model Unlike generic models, no open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks Avignon Universite</sample>
    <sample id="420">Comparison of pre-training strategies and data sources

- Evaluation of the impact of public and private medical data sources on comparable data sizes
  - NACHOS: A 1.1B+ word open-source dataset of heterogeneous data crawled from various medical domains, natures, and styles.
  - NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.

- Comparison of learning strategies:
  - From scratch with full model construction
  - Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model, and PubMedBERT, an English medical one)

Table:

| Corpus | Size | References |
|--------|------|------------|
| NACHOS | 4 GB | https://github.com/robert-apolloni/nachos |
| NBDW_small | 256 MB | https://github.com/robert-apolloni/nbdw_small |
| NBDW_medium | 4 GB | https://github.com/robert-apolloni/nbdw_medium |
| NBDW_nightly | 4 GB | https://github.com/robert-apolloni/nbdw_nightly |

Table:

| Model | Strategy | Corpus |
|-------|----------|-------|
| RoBERTa | Start from scratch | PubMed |
| BERT | Start from scratch | PubMed |
| CamemBERT | Continual pre-training | NACHOS |
| PubMedBERT | Continual pre-training | PubMed |

Avignon Université</sample>
    <sample id="421">The image is a slide from a presentation titled "Comparison of pre-training strategies and data sources." The content on the left side discusses two main points: 1. Evaluation of the impact of public and private medical data sources on comparable size datasets, with examples such as NACHOS (a 1.18B+ pubmed dataset) and NBDW (a private dataset). 2. Comparison of learning strategies ranging from scratch to continual pre-training using existing models like CamemBERT, a French generic model, and PubMedBERT, an English medical one.

On the right side, there's a table comparing different datasets:
- Corpus
- Size (in GB)
- References

The datasets listed include:
- NACHOS (pubmed): 4GB, 6.5M
- NBDW (private): 4GB, 70.3M
- NBDW (private): 9GB, 10.8M

Below this, another section compares models across various tasks:
- Task
- Strategy
- Corpus

The models compared are:
- DeBERTA
- RoBERTa
- CamemBERT
- continual pre-training
- PubmedBERT

The bottom part of the image has a logo for Avignon Université in orange and white colors.</sample>
    <sample id="422">Comparison of pre-training strategies and data sources

Evaluation of the impact of public and private medical data sources on comparable data sizes
- NACHOS: A 1.18B+ sub-word open-source dataset of heterogeneous data crawled from diverse medical domains, natures, and styles.
- NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.

Comparison of learning strategies
- From scratch with full model construction
- Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model, and PubMedBERT, an English medical one)

Table:
| Corpus | Size | References |
|--------|------|------------|
| NACHOS | 4 GB | 256 M + 256 M |
| NBDW (small) | 49 MB | 100 M |
| NBDW (medium) | 354 MB | 100 M |
| NBDW (large) | 1.8 GB | 100 M |

Table:
| Model | Strategy | Corpus |
|-------|----------|-------|
| DeBERTaT | continual pre-training | NACHOS |
| RoBERTa | continual pre-training | NACHOS |
| CamemBERT | continual pre-training | NACHOS |
| PubMedBERT | continual pre-training | NACHOS |

Avignon Université</sample>
    <sample id="423">Comparison of pre-training strategies and data sources

- Evaluation of the impact of public and private medical data sources on comparable data sizes
  - NACHOS: A 1.18B+ words open-source dataset of heterogeneous data crawled from various medical domains, natures and styles
  - NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse

- Comparison of learning strategies
  - From scratch with full model construction
  - Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model, and PubMedBERT, an English medical one)

Table:
- Corpus Name | Size | References
- NACHOS (1.18B+ words) | 4 GB | [1]
- NBDW (1.7M sentences) | 45 MB | [2]
- NBDW (45M sentences) | 360 MB | [3]

Legend:
- Strategy: 
  - DeBERTa
  - RoBERTa
  - CamemBERT
  - PubMedBERT
  
- Corpus:
  - NACHOS
  - NBDW

Avignon Université</sample>
    <sample id="424">The image is a slide from an academic presentation titled "Comparison of pre-training strategies and data sources." The content on the slide discusses two main topics: 1. Evaluation of the impact of public and private medical data sources on comparable size datasets, specifically focusing on NACHOS (a 1.18B words open-source dataset) and NBDW (a private dataset). It provides details about their sizes and references to source papers. 2. Comparison of learning strategies for pre-training models, including options like starting from scratch with full model construction or continuing pre-training using existing pre-trained models such as CamemBERT, a French generic model, and PubMedBERT, an English medical one.

On the right side of the slide, there's a table listing different datasets along with their corresponding sizes in terms of number of words and references to source papers. Below this section, another part labeled "Comparison of learning strategies" outlines various approaches for training language models.

In the bottom left corner, there's a logo that reads "Avignon Université," indicating the affiliation of the presenter or institution behind the research being discussed. In the top right corner, there's a small video feed showing someone who appears to be presenting the material verbally. This person has dark hair and is wearing glasses; however, due to the low resolution of the video feed, specific facial features are not clearly discernible.</sample>
    <sample id="425">Comparison of pre-training strategies and data sources

- Evaluation of the impact of public and private medical data sources on comparable data sizes
  - NACHOS: A 1.1B+ word open-source dataset of heterogeneous data crawled from various medical domains, natures, and styles.
  - NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.

- Comparison of learning strategies:
  - From scratch with full model construction
  - Continual pre-training using an existing pre-trained model (e.g., CamemBert, a French generic model; and PubMedBERT, an English medical one)

Table:

| Corpus | Size | References |
| --- | --- | --- |
| NACHOS (sub) | 4 GB | [1] |
| NACHOS (full) | 25.3 MB | [1] |
| NBDW (sub) | 4 GB | [2] |
| NBDW (full) | 90.8 MB | [2] |

Legend for Table:
- Strategy: From scratch or continual pre-training
- Corpus: NACHOS, NBDW

Avignon Université</sample>
    <sample id="426">Comparison of pre-training strategies and data sources

- Evaluation of the impact of public and private medical data sources on comparable data sizes
  - NACHOS: A 1.1B word open-source dataset of heterogeneous data crawled from multiple medical domains, natures and styles.
  - NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.

- Comparison of learning strategies
  - From scratch with full model construction
  - Continual pre-training using an existing pre-trained model (e.g., CamemBert, a French generic model; and PubMedBERT, an English medical one)

Table:
| Corpus | Size | References |
|--------|------|------------|
| NACHOS | 4 GB | [1] |
| NBDW_small | 45 MB | [2] |
| NBDW_medium | 450 MB | [3] |
| NBDW_large | 4.5 GB | [4] |

Legend:
- Strategy: From scratch vs. Continual pre-training
- Corpus: Various datasets used for training models

Avignon Université</sample>
    <sample id="427">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources on comparable data sizes NACHOS: A 1.1B+ word open-source dataset of heterogeneous data crawled from diverse medical domains, natures and styles NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse Comparison of learning strategies From scratch with full model construction Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model, and PubMedBERT, an English medical one) Avignon Université</sample>
    <sample id="428">Evaluation: Data sources and size

- Performance evaluation of 13 models on 10 tasks, both public and private
- Our fine-tuned models get state-of-the-art results on almost all tasks

aIF, aIF Medical Report, MUSCIA, DISEAT, MUSCIA-DETEC, CAS, preActMOCMA, QUADRO-EMOA-MEDLINE

| Model | General | Biobio.01 | Biobio.NACHOS | Clinical |
| --- | --- | --- | --- | --- |
| CANNIBALHART 158 GB | 92.46 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 4GB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 64MB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 16KB | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.01 | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |

Avaliador: Evaluation: Data sources and size
- Avaliação de desempenho de 13 modelos em 10 tarefas, tanto públicas quanto privadas
- Nosso modelo ajustado atinge resultados de ponta na maioria das tarefas

aIF, aIF Medical Report, MUSCIA, DISEAT, MUSCIA-DETEC, CAS, preActMOCMA, QUADRO-EMOA-MEDLINE

| Model | General | Biobio.01 | Biobio.NACHOS | Clinical |
| --- | --- | --- | --- | --- |
| CANNIBALHART 158 GB | 92.46 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 4GB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 64MB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 16KB | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.01 | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |

Avaliador: Evaluation: Data sources and size
- Avaliação de desempenho de 13 modelos em 10 tarefas, tanto públicas quanto privadas
- Nosso modelo ajustado atinge resultados de ponta na maioria das tarefas

aIF, aIF Medical Report, MUSCIA, DISEAT, MUSCIA-DETEC, CAS, preActMOCMA, QUADRO-EMOA-MEDLINE

| Model | General | Biobio.01 | Biobio.NACHOS | Clinical |
| --- | --- | --- | --- | --- |
| CANNIBALHART 158 GB | 92.46 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 4GB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 64MB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 16KB | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.01 | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |

Avaliador: Evaluation: Data sources and size
- Avaliação de desempenho de 13 modelos em 10 tarefas, tanto públicas quanto privadas
- Nosso modelo ajustado atinge resultados de ponta na maioria das tarefas

aIF, aIF Medical Report, MUSCIA, DISEAT, MUSCIA-DETEC, CAS, preActMOCMA, QUADRO-EMOA-MEDLINE

| Model | General | Biobio.01 | Biobio.NACHOS | Clinical |
| --- | --- | --- | --- | --- |
| CANNIBALHART 158 GB | 92.46 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 4GB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 64MB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 16KB | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.01 | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |

Avaliador: Evaluation: Data sources and size
- Avaliação de desempenho de 13 modelos em 10 tarefas, tanto públicas quanto privadas
- Nosso modelo ajustado atinge resultados de ponta na maioria das tarefas

aIF, aIF Medical Report, MUSCIA, DISEAT, MUSCIA-DETEC, CAS, preActMOCMA, QUADRO-EMOA-MEDLINE

| Model | General | Biobio.01 | Biobio.NACHOS | Clinical |
| --- | --- | --- | --- | --- |
| CANNIBALHART 158 GB | 92.46 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 4GB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 64MB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 16KB | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.01 | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |

Avaliador: Evaluation: Data sources and size
- Avaliação de desempenho de 13 modelos em 10 tarefas, tanto públicas quanto privadas
- Nosso modelo ajustado atinge resultados de ponta na maioria das tarefas

aIF, aIF Medical Report, MUSCIA, DISEAT, MUSCIA-DETEC, CAS, preActMOCMA, QUADRO-EMOA-MEDLINE

| Model | General | Biobio.01 | Biobio.NACHOS | Clinical |
| --- | --- | --- | --- | --- |
| CANNIBALHART 158 GB | 92.46 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 4GB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 64MB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 16KB | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.01 | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |

Avaliador: Evaluation: Data sources and size
- Avaliação de desempenho de 13 modelos em 10 tarefas, tanto públicas quanto privadas
- Nosso modelo ajustado atinge resultados de ponta na maioria das tarefas

aIF, aIF Medical Report, MUSCIA, DISEAT, MUSCIA-DETEC, CAS, preActMOCMA, QUADRO-EMOA-MEDLINE

| Model | General | Biobio.01 | Biobio.NACHOS | Clinical |
| --- | --- | --- | --- | --- |
| CANNIBALHART 158 GB | 92.46 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 4GB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 64MB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 16KB | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.01 | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |
| Biobio.NACHOS | 91.11 | 97.41 | 98.53 | 97.32 |

Avaliador: Evaluation: Data sources and size
- Avaliação de desempenho de 13 modelos em 10 tarefas, tanto públicas quanto privadas
- Nosso modelo ajustado atinge resultados de ponta na maioria das tarefas

aIF, aIF Medical Report, MUSCIA, DISEAT, MUSCIA-DETEC, CAS, preActMOCMA, QUADRO-EMOA-MEDLINE

| Model | General | Biobio.01 | Biobio.NACHOS | Clinical |
| --- | --- | --- | --- | --- |
| CANNIBALHART 158 GB | 92.46 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 4GB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 64MB | 91.11 | 97.41 | 98.53 | 97.32 |
| CANNIBALHART 16KB | 91.11 | 9</sample>
    <sample id="429">The image shows a presentation slide titled "Evaluation: Data sources and size." The content of the slide includes two main bullet points:

1. Performance evaluation of 13 models on 11 tasks, both public and private.
2. Fine-tuned models get state-of-the-art results on almost all tasks.

Below these bullet points is a table with various columns labeled as follows:
- aIF
- Medical Report
- MUSCIA
- DISEASE
- METABOLISM
- CAS
- PRETOMEDICA
- QUADRO-EMERA-MEDLINE

Each column contains numerical data under subheadings such as NER (Named Entity Recognition), CNE (Contextual Named Entity), CLS (Contextual Language Segmentation), POS (Part of Speech), and EMR (Entity Mention Rate).

At the bottom right corner of the slide, there is an attribution to Avignon Université. Additionally, in the top-right corner of the image, there is a small inset showing a person speaking into a microphone, likely presenting this information.</sample>
    <sample id="430">The image is a screenshot of a presentation slide titled "Evaluation: Data sources and size." The content on the slide provides an evaluation performance analysis for 13 models across various tasks, including both public and private datasets. It highlights that their fine-tuned models achieve state-of-the-art results in almost all cases.

Below the title, there are two bullet points:
- Performance evaluation of 13 models on 10 tasks, both public and private.
- Their fine-tuned models get state-of-the-art results on almost all tasks.

A table follows these bullet points, which includes columns labeled as follows (from left to right):
- aIF
- Medical Report
- MUSCIA
- DISEASE
- MUSCIA
- DISEASE
- CAS
- Preclinical
- QUADRO-EMOA
- MEDLINE

Each column contains numerical data under subheadings such as NER (Named Entity Recognition), CLS (Class), NER, CLS, POS (Part-of-Speech tagging), NER, CLS, POS, and NER.

At the bottom right corner of the slide, there's a logo with text reading "Avignon Université."

In the top-right corner of the image, there is a small inset showing a person who appears to be presenting or speaking about the content shown on the slide.</sample>
    <sample id="431">The image is a screenshot of a presentation slide titled "Evaluation: Data sources and size." The slide provides an evaluation performance analysis for 13 models across various tasks, both public and private. It highlights that the fine-tuned models achieve state-of-the-art results on almost all tasks.

The table in the slide lists different datasets (aIF, Medical Report, MUSICA, DISEASE, MUSICA, CAS, PrichMCTOMAC, and QUADRO-EMERA-MEDLINE) with corresponding scores under categories like NER (Named Entity Recognition), CER (Coreference Resolution), CLS (Class Labeling), POS (Part-of-Speech tagging), and EMR (Entity Mention Recognition). 

The datasets include:
- aIF
- Medical Report
- General
- Biobio
- BioMed
- Biobio large
- CHAMBER
- CHAMBER small

Each dataset has numerical values indicating its performance metrics in each category listed above. For example, the 'Medical Report' dataset shows high scores such as 94.87% for NER, 96.02% for CER, etc., while 'CHAMBER small' shows slightly lower but still impressive scores like 85.96% for NER and 89.73% for CER.

The bottom right corner of the slide features the logo of Avignon Université, suggesting the affiliation or origin of the research presented.</sample>
    <sample id="432">The image shows a slide from a presentation titled "Evaluation: Pre-training strategies." The content of the slide includes several bullet points and a detailed table comparing different models. Here is the text in the image, structured as it appears:

- Evaluation: Pre-training strategies
  - From scratch vs. continual pre-training on 4GB of data
  - Question-answering tasks require more domain-specific knowledge to be able to work well
  - A model of study stability shows higher inter-run variability for the CAMBert-based models trained using continual pre-training

Below these bullet points, there is a large table with multiple columns and rows. Each row represents a different model or dataset, and each column contains various metrics such as NER (Named Entity Recognition), CFB (Coreference Resolution), NER FB, NER CFB, NER F1, NER FB F1, NER CFB F1, and others.

The top row of the table lists:
- affif
- alff
- Medical Report
- Speciality Report
- MEDSCAT
- MEDSCAT CFB
- MEDSCAT F1
- MEDSCAT FB
- MEDSCAT FB F1
- MEDSCAT CFB F1
- QUASERAMMA
- QUAEROAMMA
- MEDSCAT
- QUAEROAMMA

The subsequent rows list specific datasets or models along with their corresponding performance scores across the listed metrics. For example, one entry reads:
- "CamembertOSCAM 168 158" followed by numerical values under each metric category.

At the bottom right corner of the image, there is a logo that says "Avinon Universite."

The overall layout suggests this is part of an academic or research presentation discussing the effectiveness of different pre-training strategies for machine learning models used in natural language processing tasks.</sample>
    <sample id="433">The image shows a slide from a presentation titled "Evaluation: Pre-training strategies." The content of the slide is as follows:

1. **Title**: Evaluation: Pre-training strategies
2. **Bullet Points**:
   - From scratch vs. continual pre-training on 4GB of data
   - Question-answering tasks require more domain-specific knowledge to be able to work well.
   - A study of model stability shows a higher inter-run variability for the CAMBert-based models trained using continual pre-training.

3. **Table**: 
   - Columns: alf, alff, Medical Report, Speciality, Medscape, CMedSCT, EMR, EHR, QUASERAMEDQA, MEDCLINIC, MEDCLINICQA
   - Rows: Different datasets and their respective performance metrics (NCR, CR, NCR CR) across various models such as "CamBertOSCAMBERT 16GB," "CamBertOSCAMBERT 4GB," etc., with specific scores listed under each dataset.

4. **Footer**: Avignon Université logo

The table provides detailed numerical values comparing different models' performances in terms of recall rates (NCR), correct answers (CR), and combinations thereof (NCR CR). Each row represents a different model or configuration tested against multiple medical-related datasets.</sample>
    <sample id="434">Evaluation: Pre-training strategies From scratch vs. continual pre-training on 4GB of data Question-answering tasks require more domain-specific knowledge to be able to work well A study of model stability shows a higher inter-run variability for the CAMBert-based models trained using continual pre-training</sample>
    <sample id="435">The core message of the presentation slide is that DrBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks. It surpasses CamemBERT, a generic model and English-based domain-specific models. The utility of training a medical-specific model in French has been confirmed. Data sources are emphasized as important; NACHOS is more robust than using private clinical data only. More data generally leads to better performance but does not scale well. Pretraining is suggested as an effective strategy when working with base domain-specific English models. Additionally, it's mentioned that the DrBERT models, along with NACHOS datasets and scriptings, are freely available under MIT license.</sample>
    <sample id="436">Core message DRBRT achieves state-of-the-art results in 9 downstream French medical-oriented tasks Surpasses CamemBERT generic model and English-based domain-specific models Confirms utility of training a medical-specific model in French Data sources matters: training on heterogeneous data is important NACHOS is more robust than using private clinical data only More data is better, but does not scale well. Continual pretraining is a more effective strategy when based on domain-specific English models The DRBRT models, the NACHOS dataset and the training scripts are freely available under the MIT license</sample>
    <sample id="437">Core message DRBert achieves state-of-the-art results in 7 downstream French medical-oriented tasks Surpasses CamemBERT generic model and English-based domain-specific models Confirms utility of training a medical-specific model in French Data sources matters: training on heterogeneous data is important NACHOS is more robust than using private clinical data only More data is better, but does not scale well Continual pretraining is a more effective strategy when based on domain-specific English models The DRBert models, the NACHOS dataset, and the training scripts are freely available under the MIT license</sample>
    <sample id="438">The image features a cartoon character with a nurse's hat and an IV bag, set against a white background. A speech bubble from the character contains the text "Thank You." Below this, there is additional text that reads, "Looking forward to exchange at poster session in Toronto!" In the bottom right corner of the image, there is more information provided: "More information on: drbert.univ-avignon.fr" followed by the logo for Avignon Université. The overall design suggests it might be part of a presentation or promotional material related to a medical event or conference.</sample>
    <sample id="439">The authors claim that the integration of pre-training knowledge with inference-time knowledge is an understudied area in NLU.</sample>
    <sample id="440">The names of the speakers are Zhiyang Xu, Ying Shen, and Lifu Huang.</sample>
    <sample id="441">Yes, Coscript underwent quality checks. The slide mentions that the 55,000 scripts were generated with constraints based on symbolic knowledge distillation and then filtered through Coscript Dataset using an instructor with a constraint-based method to ensure they meet certain criteria for validation and test sets.</sample>
    <sample id="442">Existing methods support limited discourse phenomena and languages.</sample>
    <sample id="473">The approach is compared to existing SimulST policies such as wait-k, LA, and CAAT.</sample>
    <sample id="474">The affiliations of the authors are: 1. LIA, Avignon Université; 2. LSIN, Nantes Université; 3. Clinique des diaboliques, CHU de Nantes; and 4. Zenicod.</sample>
    <sample id="475">The speaker's name is Jenny T. Liang, as indicated by the information provided in the image showing her photo and name alongside Carnegie Mellon University.</sample>
    <sample id="476">Three authors are involved in the paper.</sample>
    <sample id="505">Yes, the dataset is publicly available. The image provides a GitHub link (https://github.com/ShuhengL/ac12023_conllpp) where users can access and download the dataset.</sample>
    <sample id="535">The authors of the paper are affiliated with Università di Trento and Fondazione Bruno Kessler.</sample>
    <sample id="536">The speaker's name is Mohammad Javad Hosseini.</sample>
    <sample id="537">Google Prompting PaLM for Translation Assessing Strategies and Performance ACL 2023 David Vilàr Torres Markus Freitag Colin Cherry Jiaming Luo Viresh Ratnakar George Foster</sample>
    <sample id="538">The video presents a detailed overview of the PaLM: Pathways Language Model, as referenced from Chowdery et al., 2022 (arXiv:2204.02311). It highlights key specifications such as having 540B parameters and being trained on 780B tokens. The model is described as densely activated with the use of 6144 TPU v4 chips and achieving SOTA in hundreds of LMU and Generation benchmarks.

A colorful tree diagram illustrates various tasks that the language model can perform, including Question Answering, Arithmetic Code Completion, Translation, Summarization, and Language Understanding. Each task branch contains specific examples like "Semantic Parsing Proverbs" for Question Answering, "General Knowledge Reading Comprehension" for Summarization, and "Dialogue Joke Explanations" for Language Understanding.

As the presentation progresses, different parameter counts are shown next to the tree diagram:
- Initially, it shows "6 billion parameters."
- Later, it updates to "38 billion parameters."
- Subsequently, it changes to "95 billion parameters," then "540 billion parameters," followed by "8 billion parameters," and finally "37 billion parameters."

Throughout the video, there's an ongoing reference to Google AI, indicating the affiliation or source of this advanced language model research.</sample>
    <sample id="539">PaLM: Pathways Language Model Chowdery et al., 2022 arXiv:2204.02311. 540B parameters. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips. SOTA in hundreds of LMU and Generation benchmarks. QUESTION ANSWERING SEMANTIC PARSENG PROVERBS ARITHMETIC CODE COMPLETION GENERAL KNOWLEDGE TRANSLATION LOGICAL INFERENCE CHAINS COMMON-SENSE REASONING PATTERN RECOGNITION SUMMARIZATION LANGUAGE UNDERSTANDING 540 billion parameters 334 billion parameters</sample>
    <sample id="540">**Title: Our Contribution**

- **First systematic study of LLM prompting for MT.**
  - Both for the candidate pool as well as selection strategy.

- **Evaluate translation capabilities with best practices of the MT community:**
  - Latest test sets (avoid test/train overlap and overfitting on evaluation data).
  - Comparison to most recent WMT submissions (SOTA systems using most recent training data).
  - SOTA MT metrics (better correlation with human judgments).
  - Expert-based human evaluation (more robust than crowd workers).

- **Recommendation for prompt selection strategies**

[Google logo in bottom left corner]</sample>
    <sample id="541">The image contains text related to a presentation slide titled "Our contribution." The content of the slide is as follows:

- First systematic study of LLM prompting for MT.
  - Both for the candidate pool as well as selection strategy.

- Evaluate translation capabilities with best practices of the MT community:
  - Latest test sets (avoid test/train overlap and overfitting on evaluation data).
  - Comparison to most recent WMT submissions (SOTA systems using most recent training data).
  - SOTA MT metrics (better correlation with human judgements).
  - Expert-based human evaluation (more robust than crowd workers).

- Recommendation for prompt selection strategies

In the bottom left corner, there is a Google logo. In the bottom right corner, there is an image of a person wearing a checkered shirt.</sample>
    <sample id="542">**Title: Our contribution**

- **First systematic study of LLM prompting for MT.**
  - Both for the candidate pool as well as selection strategy.

- **Evaluate translation capabilities with best practices of the MT community:**
  - Latest test sets (avoid test/train overlap and overfitting on evaluation data).
  - Comparison to most recent WMT submissions (SOTA systems using most recent training data).
  - SOTA MT metrics (better correlation with human judgements).
  - Expert-based human evaluation (more robust than crowd workers).

- **Recommendation for prompt selection strategies**

[Google logo in bottom left corner]</sample>
    <sample id="543">The image contains a slide titled "Our contribution" with the following points:

- First systematic study of LLM prompting for MT.
  - Both for the candidate pool as well as selection strategy.

- Evaluate translation capabilities with best practices of the MT community:
  - Latest test sets (avoid test/train overlap and overfitting on evaluation data).
  - Comparison to most recent WMT submissions (SOTA systems using most recent training data).
  - SOTA MT metrics (better correlation with human judgements).
  - Expert-based human evaluation (more robust than crowd workers).

- Recommendation for prompt selection strategies

There is also a small circular image in the bottom right corner showing a person, but their identity cannot be determined from this description.</sample>
    <sample id="544">Prompts have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points</sample>
    <sample id="545">Prompts have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points</sample>
    <sample id="546">Prompts have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points</sample>
    <sample id="547">The image shows a slide from a presentation with the title "Example prompting for translation" at the top. The content of the slide is divided into two sections: German and English translations.

Under the heading "5-shot prompting," there are three pairs of sentences in both languages:

1. German: "Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird."
   - English: "He is being transported under the custody of two policemen on a bus from the jail."

2. German: "Ski-Legenden unter sich: Die Polizei war eingeschlossen, nachdem sie Beschwerden des Buros erhalten hatten."
   - English: "Police were called in after receiving complaints from the office."

3. German: "Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte."
   - There is no corresponding English sentence provided next to this pair.

At the bottom left corner of the slide, there is a Google Drive logo indicating that the document might be hosted or created using Google Drive. In the lower right corner, there is a small circular photo of a person wearing a checkered shirt.</sample>
    <sample id="548">The image shows a presentation slide titled "Example prompting for translation" with the Google logo in the bottom left corner. The content of the slide is divided into two sections: 

1. **5-shot prompting**
   - German sentences:
     - "Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird."
     - "Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten."
     - "Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte."
   - English translations corresponding to each sentence are provided below them.

2. **English**
   - A single line that reads "English:" followed by a blank space where presumably more text would be added or translated from the examples above.

In the lower right corner of the slide, there's a small circular photo of a person wearing a checkered shirt and smiling at the camera.</sample>
    <sample id="549">Example prompting for translation 5-shot prompting German: Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird. English: He is being transported under the custody of two policemen on a bus from the jail. ... German: Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten. English: Police were called in after receiving complaints from the office. German: Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte.</sample>
    <sample id="550">The image shows a slide from a presentation titled "Example prompting for translation." It includes three German sentences with their corresponding English translations, demonstrating how to prompt for accurate translation. The first sentence is about someone being transported in a patrol car by two police officers: "Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird" and its translation "He is being transported under the custody of two policemen on a bus from the jail." The second sentence describes skiers being interrupted during a race due to an emergency situation at the office: "Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten" which translates to "English: Police were called in after receiving complaints from the office." The third sentence explains that a passenger alerted the police when he noticed several officers approaching: "Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte" translating to "English:" (indicating space for further translation). At the bottom left corner, there's a Google Drive logo.</sample>
    <sample id="551">Example prompting for translation 5-shot prompting German: Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird. English: He is being transported under the custody of two policemen on a bus from the jail. ... German: Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten. English: Police were called in after receiving complaints from the office. German: Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte.</sample>
    <sample id="552">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. • Dominated by “Accuracy/Omission” • “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="553">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. • Dominated by “Accuracy/Omission” • “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="554">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. • Dominated by “Accuracy/Omission” • “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="555">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. • Dominated by “Accuracy/Omission” • “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="556">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="557">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. • Dominated by "Accuracy/Omission" • "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="558">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by “Accuracy/Omission” “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="559">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. • Dominated by “Accuracy/Omission” • “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="560">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. * Dominated by "Accuracy/Omission" * "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="561">The image features a colorful word cloud with the phrase "thank you" prominently displayed in large red letters at the center. Surrounding it are various translations of "thank you" from different languages, each written in distinct fonts and colors such as green, blue, purple, orange, yellow, pink, black, and white. The words include "gracias," "merci," "danke," "obrigado," "grazie," among others, representing gratitude in multiple languages.

In the bottom right corner of the image, there is an inset photo of a person wearing glasses and smiling. This individual has short hair and appears to be looking directly at the camera. The background behind the text is plain white, which helps the vibrant colors of the word cloud stand out more vividly.</sample>
    <sample id="597">The first step of the method maps the input tokens to tags.</sample>
    <sample id="598">The Coscript Dataset contains 55,000 scripts.</sample>
    <sample id="599">The KITMUS Test Evaluating Knowledge Integration from Multiple Sources Akthar Aloufi McGill University/Mila Martin Pomsl Microsoft Research Kaheer Suleman Microsoft Research Adam Trischler Microsoft Research Alexandra Olteanu McGill University/Mila Jackie CK Cheang McGill University/Mila * Equal Contribution</sample>
    <sample id="600">NLU models draw on multiple knowledge sources

Knowledge in Parameters (pretrain-time knowledge)

Knowledge in Context (inference-time knowledge) NLU Model</sample>
    <sample id="601">NLU models draw on multiple knowledge sources

Knowledge in Parameters (pretrain-time knowledge)

Knowledge in Context (inference-time knowledge) NLU Model</sample>
    <sample id="602">John saw the newly elected president on TV

What presidents do ✅
What is a TV ✅

Who is John ❌
Who is the new president ❌</sample>
    <sample id="603">John saw the newly elected president on TV

What presidents do ✅
What is a TV ✅

Who is John ❌
Who is the new president ❌</sample>
    <sample id="604">The image contains a slide with the following text: "John saw the newly elected president on TV." Below this, there are two sections. The left section is labeled "pretrain-time knowledge" and includes a diagram of interconnected nodes. The right section has two statements: "What presidents do" with a green checkmark next to it, indicating correctness, and "What is a TV" also with a green checkmark next to it, indicating correctness. At the bottom, there are two questions marked as wrong: "Who is John" with a red cross next to it, and "Who is the new president" with a red cross next to it. On the right side of the slide, there is an illustration of a person sitting in a chair watching television.</sample>
    <sample id="605">John saw the newly elected president on TV

pre-train knowledge
What presidents do ✅
What is a TV ✅

inference-time knowledge
Who is John ✅
Who is the new president ✅

An illustration of someone sitting in an armchair watching television, with text "Made by FREE-VECTORS.NET" at the bottom right.</sample>
    <sample id="606">KITMUS Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on pretrain-time knowledge inference-time knowledge Experiment with human study participants coreference resolution models</sample>
    <sample id="607">KITMUS Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on pretrain-time knowledge inference-time knowledge Experiment with human study participants coreference resolution models</sample>
    <sample id="608">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="609">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="610">The image is a slide from the KITMUS Test Suite, which appears to be an educational or training module. The title "KITMUS Test Suite" is prominently displayed at the top of the slide in white text on a dark blue background.

Below the title, there is a paragraph that reads: "Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax." This sentence includes some words highlighted in different colors for emphasis: "Servin," "judge," "Kea," "baker," "park," "law court," and "he."

Following this paragraph, there is a bracketed answer provided within square brackets: "[Answer: Servin]". 

At the bottom left corner of the slide, there is a section labeled "1) Entity-specific knowledge" with a yellow border around it. On the right side of the slide, two green boxes are present:

- One box contains the phrase "Judges decide cases in courts of law."
- Another box repeats the same phrase as above but slightly shifted down vertically.

In the upper right corner of the slide, there is a small video frame showing a person who seems to be presenting or explaining something related to the content of the slide.

Finally, at the very bottom center of the slide, the number "7" indicates its position within the presentation sequence.</sample>
    <sample id="611">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] Judges decide cases in courts of law. 1) Entity-specific knowledge inference-time knowledge 2) Background knowledge pretrain-time knowledge</sample>
    <sample id="612">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] Judges decide cases in courts of law. 1) Entity-specific knowledge inference-time knowledge 2) Background knowledge pretrain-time knowledge</sample>
    <sample id="613">Variants of KITMUS (a) BACKGROUND-PRETRAIN (b) BACKGROUND-BOTH (c) BACKGROUND-INFERENCE a) BACKGROUND-PRETRAIN: Typical setup b) BACKGROUND-BOTH: Explicitly provide background knowledge in context c) BACKGROUND-INFERENCE: Knowledge only available at inference-time</sample>
    <sample id="614">Variants of KITMUS

(a) BACKGROUND-PRETRAIN
- Pretrain-time: Background knowledge, Entity-specific knowledge
- Inference-time: Background knowledge, Entity-specific knowledge

(b) BACKGROUND-BOTH
- Pretrain-time: Background knowledge, Entity-specific knowledge
- Inference-time: Background knowledge, Entity-specific knowledge

(c) BACKGROUND-INFERENCE
- Pretrain-time: Background knowledge only
- Inference-time: Background knowledge and Entity-specific knowledge

a) BACKGROUND-PRETRAIN: Typical setup  
b) BACKGROUND-BOTH: Explicitly provide background knowledge in context  
c) BACKGROUND-INFERENCE: Knowledge only available at inference-time</sample>
    <sample id="615">Variants of KITMUS (a) BACKGROUND-PRETRAIN: Typical setup (b) BACKGROUND-BOTH: Explicitly provide background knowledge in context (c) BACKGROUND-INFERENCE: Knowledge only available at inference-time</sample>
    <sample id="616">Variants of KITMUS Background-Pretrain Chichester is a politician. Politicians seek elected seats in government. Background-Both Chichester is a politician. The work of a politician is to seek elected seats in government. Background-Inference Chichester is a muniturer. The work of a muniturer is printing smartly.</sample>
    <sample id="617">The image shows a presentation slide titled "Variants of KITMUS" with three columns, each representing different variants: Background-Pretrain, Background-Both, and Background-Inference. Each column contains text boxes with statements about Chichester being a politician or a muniturer, along with some diagrams at the top.

In the first column (Background-Pretrain), there is:
- A statement in pink saying, "Politicians seek elected seats in government."
- In blue, it says, "Chichester is a politician."

In the second column (Background-Both), there are two statements:
- The same as above.
- Below that, another statement in purple reads, "The work of a politician is to be an elected seat in government."

In the third column (Background-Inference), there are also two statements:
- Again, both stating "Chichester is a politician."
- Below these, one more statement in purple states, "The work of a muniturer is reporting smartly."

There's also a person wearing headphones visible on the right side of the frame.</sample>
    <sample id="618">Variants of KITMUS Background-Pretrain Chichester is a politician. Politicians seek elected seats in government. Background-Both Chichester is a politician. The work of a politician is to seek elected seats in government. Background-Inference Chichester is a muniturer. The work of a muniturer is reporting smartly.</sample>
    <sample id="619">Variants of KITMUS Background-Pretrain Chichester is a politician. Politicians seek elected seats in government. Background-Both Chichester is a politician. The work of a politician is to seek elected seats in government. Background-Inference Chichester is a muniturer. The work of a muniturer is printing smartly.</sample>
    <sample id="620">The image shows a slide from a presentation with the title "Background-Pretrain" at the top. The main content of the slide is a bar chart comparing different groups based on their mean scores, categorized into two conditions: "Without task-specific training" and "With task-specific training." There are four bars representing different categories: Random Choice (grey dashed line), Human Participants (blue solid line), BERTaCoef (orange solid line), and C2F (light orange solid line). Below the chart, there is a caption that reads, "Task-specific training is necessary for knowledge integration." In the bottom right corner of the slide, there is a page number indicating this is slide 13.</sample>
    <sample id="621">The image shows a presentation slide titled "Background-Pretrain" with a bar chart and some text. The title is at the top of the slide, set against a dark blue background.

The main content includes:

1. A bar chart comparing different groups:
   - X-axis: Categories labeled as "Without task-specific training" and "With task-specific training."
   - Y-axis: Labeled "Mean Accuracy," ranging from 0 to 1.
   - Four bars representing four categories:
     - Random Choice (grey dashed line)
     - Human Participants (orange solid line)
     - BERTaCoref (blue solid line)
     - C2F (red solid line)

2. Below the chart, there's a caption in white text on a black background that reads: "Task-specific training is necessary for knowledge integration."

3. In the bottom right corner, there's a page number indicating this is slide 13.

4. On the right side of the slide, partially visible, is an individual wearing headphones and looking towards the camera.

Overall, the slide emphasizes the importance of task-specific training by showing comparative mean accuracy across different methods or conditions.</sample>
    <sample id="622">**Title:**
Background-Pretrain

**Graph Description:**
- Y-axis label: Mean Accuracy
- X-axis labels:
  - Without task-specific training
  - With task-specific training
- Legend:
  - Random Choice (dotted line)
  - Human Participants (solid blue line)
  - BERT4Coef (solid orange line)
  - C2F (solid red line)

**Bar Chart Data:**
- "Without task-specific training":
  - Random Choice: Approximately 0.15
  - Human Participants: Approximately 0.10
  - BERT4Coef: Approximately 0.95
  - C2F: Approximately 0.30
- "With task-specific training":
  - Random Choice: Approximately 0.15
  - Human Participants: Approximately 0.85
  - BERT4Coef: Approximately 0.95
  - C2F: Approximately 0.70

**Text Below Graph:**
Task-specific training is necessary for knowledge integration

**Slide Number:**
13</sample>
    <sample id="623">The image shows a slide from a presentation titled "Background-Inference." The main content of the slide is a bar graph comparing different methods for integrating inference-time background knowledge. 

The x-axis of the graph represents two categories: "Fictional" and "background knowledge," while the y-axis measures performance with values ranging from 0 to 1.

There are four bars on the graph, each representing a different method:
- Random Choice (dashed line)
- Human Participants
- BERT4Coef (blue solid bar)
- C2F (orange solid bar)

The blue bar labeled "BERT4Coef" indicates that this method has higher performance compared to the orange bar labeled "C2F."

Below the graph, there is a caption in white text against a dark background that reads: "Models struggle to integrate inference-time background knowledge."

In the top right corner of the image, there is a small video feed showing a person wearing headphones and speaking into a microphone.</sample>
    <sample id="624">The image shows a slide from a presentation with the title "Conclusion" at the top. Below the title, there is a section labeled "Main Takeaways:" which lists three points: 1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge). 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge.

At the bottom of the slide, there is additional text that reads: "Find the dataset, generation &amp; evaluation code on GitHub at https://mopoemsl/kitmus." The slide also includes an icon resembling a stylized cat's head in black and white, located near the bottom left corner. In the upper right corner of the image, there is a small video feed showing a person wearing headphones and speaking into a microphone.</sample>
    <sample id="625">**Title: Conclusion**

**Main Takeaways:**
1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge)
2. Task-specific training is necessary for knowledge integration
3. Models struggle to integrate inference-time background knowledge

Find the dataset, generation &amp; evaluation code on GitHub at [mpoems/kitmus](https://github.com/mpoems/kitmus) 15</sample>
    <sample id="626">The best alignment method for DEplain is MASSAlign, as it has the highest scores in all categories (P, R, F1, and ncm) compared to other methods.</sample>
    <sample id="627">Weakly supervised learning alleviates the annotation bottleneck by using weak labeling sources like heuristics and knowledge bases. This approach allows for training models on noisy data, which can still generalize well despite potential errors in annotations or noise memorization issues that could harm generalization.</sample>
    <sample id="628">The documents in DEplain-web were aligned with manual and automatic alignment methods. The allocation was done at the document level, as indicated by the text "Document Level" on the slide. This suggests that both manual and automatic techniques were used to align or organize the content within these documents for further analysis or processing using the mBART model mentioned in the context of English language tasks.</sample>
    <sample id="629">The CoNLL++ dataset was created by collecting Reuters news from 2020 and annotating it with the CoNLL-2003 annotation guidelines.</sample>
    <sample id="667">The slide titled "Existing Works" lists several references related to watermarking techniques and their applicability. The works include: 1. Watermark (References 1, 2) - Transferability marked as not applicable with a red cross. 2. Lexical Watermark (References 3, 4) - Applicable to EaaS (Empowering End-to-End Security). 3. Backdoor Watermark (Reference 5) - Applicable to EaaS. 4. Adversarial Watermark (Reference 6) - Applicable to EaaS. Each reference is accompanied by its citation in the bottom left corner of the slide.</sample>
    <sample id="668">No, multilingual LLMs such as Codex or Bloom are not sufficient for CLSP. The slide indicates that these models "are still inadequate" for cross-lingual semantic parsing tasks according to the findings presented in section 4 of a paper.</sample>
    <sample id="669">The image shows a presentation slide with the following text: "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" Below this title, there are two names listed as presenters or authors: Shuheng Liu and Alan Ritter. Further down, it states their affiliation: School of Interactive Computing at Georgia Institute of Technology. In the bottom right corner, there is a logo for Georgia Tech. The background has some abstract design elements on the left side, but they do not contain any discernible information related to the content of the slide.</sample>
    <sample id="670">The image contains text that reads "Named Entity Recognition &amp; Generalization." This suggests a focus on the topic of named entity recognition and its application in generalizing such entities. The presence of this text indicates an educational or informational context, likely related to computer science or data processing techniques where named entities are identified within text data for various applications like information extraction, natural language processing, etc.</sample>
    <sample id="671">The slide is titled "Named Entity Recognition &amp; Generalization" and contains two bullet points. The first bullet point states, "Models have been using CoNLL-2003 to develop NER for almost 20 years." The second bullet point asks, "Can these models generalize to modern data?" In the bottom right corner of the slide, there is a logo with the text "Georgia Tech."</sample>
    <sample id="672">The image contains a slide from a presentation with the title "Named Entity Recognition &amp; Generalization" in bold at the top. Below the title, there are three bullet points:

1. Models have been using CoNLL-2003 to develop NER for almost 20 years
2. Can these models generalize to modern data?
3. What is needed for good generalization?

In the bottom right corner of the slide, there is a logo that reads "Georgia Tech." In the bottom left corner, there is an image of a person wearing glasses and a dark shirt.

The background of the slide is white, and the text is primarily black with some key terms highlighted in yellow.</sample>
    <sample id="673">The image is a slide from a presentation titled "Named Entity Recognition &amp; Generalization." The content of the slide includes three bullet points: 1. Models have been using CoNLL-2003 to develop NER for almost 20 years. 2. Can these models generalize to modern data? 3. What is needed for good generalization? Additionally, there is another question added later on the slide: 4. What causes the performance drop? In the bottom right corner, there is a logo with the text "Georgia Tech" next to it.</sample>
    <sample id="674">The image shows a presentation slide titled 'CoNLL++ Dataset' from Georgia Tech. The content of the slide includes an example sentence with parts-of-speech tags and named entity annotations: 'AMBASSADOR O, TO O, THE O, UNITED I-ORG, NATIONS I-ORG, : O, LINDA I-PER, THOMAS-GREENFIELD I-PER.' Below this example, there is additional text that reads, 'Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines.' In the bottom left corner, there is a small circular photo of a person wearing glasses.</sample>
    <sample id="675">The image shows a presentation slide titled "CoNLL++ Dataset" with the Georgia Tech logo at the bottom right corner. The content of the slide is as follows:

- Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines
- Fine-tuned 20+ models on CoNLL-2003

Additionally, there are examples provided:
- AMBASSADOR O
- TO O
- THE O
- UNITED I-ORG
- NATIONS I-ORG
- : O
- LINDA I-PER
- THOMAS-GREENFIELD I-PER

The text indicates that this dataset was evaluated on the CoNLL-2003 test set &amp; CoNLL++.</sample>
    <sample id="676">CoNLL++ Dataset Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines Fine-tuned 20+ models on CoNLL-2003 Evaluated on CoNLL-2003 test set &amp; CoNLL++ Calculated percentage ΔF1 to assess generalization AMBASSADOR O TO I-ORG THE NATIONS : LINDA THOMAS-GREENFIELD I-PER GT Georgia Tech</sample>
    <sample id="677">The text in the image reads: "What Is Needed for Good Generalization?" This is a question posed at the top of the slide, likely setting up an educational or informative presentation. In the bottom right corner, there's a logo with the letters "GT" and the words "Georgia Tech," indicating that this content may be related to Georgia Institute of Technology. The rest of the slide appears to have some faint shapes on the left side but no additional visible text or details provided within the frame of the image.</sample>
    <sample id="678">The image contains a slide from a presentation with the title "What Is Needed for Good Generalization?" in bold text at the top. Below the title, there are two bullet points: 1. Model architecture 2. Transformer models generalize better The right side of the slide features a graph with various data points and lines indicating performance metrics on the x-axis labeled "GATE-2003 F1 Score" ranging from approximately 50 to 80, and y-axis values not clearly visible but likely representing some form of score or measurement. There is also a logo in the bottom left corner that reads "Georgia Tech."</sample>
    <sample id="679">The image contains a slide with the title "What Is Needed for Good Generalization?" It includes two main points: 1. Model architecture - Transformer models generalize better 2. Model size - Larger models generalize better There is also a graph on the right side of the slide, plotting MFT (in blue) against the number of parameters (in orange and green). The graph shows different model sizes such as BERT, RoBERTa, ALBERT, JSG-Base, JSG-Small, JSG-Large, and JSG-Large. At the bottom left corner, there is an inset circular photo of a person wearing glasses. In the bottom right corner, there is the Georgia Tech logo.</sample>
    <sample id="680">The image contains a slide with the title "What Is Needed for Good Generalization?" It lists three key points: 1. Model architecture - Transformer models generalize better 2. Model size - Larger models generalize better 3. Number of fine-tuning examples - More examples leads to better generalization A graph is shown on the right side, comparing two datasets labeled RoBERTa and Flair. The x-axis represents the percentage of training examples (ranging from 10% to 90%), while the y-axis shows the AUC-ROC (%) performance metric. The Georgia Tech logo is visible in the bottom right corner.</sample>
    <sample id="681">The image contains a title that reads "What Causes Performance Drop?" in bold, gold-colored text. In the bottom right corner, there is a logo for Georgia Tech with the letters "GT" and the words "Georgia Tech" written underneath it. The background of the slide is white with some faint geometric shapes on the left side. There is also a small circular profile picture of an individual located at the bottom left corner of the slide.</sample>
    <sample id="682">The image shows a slide from a presentation with the title "What Causes Performance Drop?" in bold, gold text at the top. Below the title, there is a bullet point that reads "Adaptive overfitting?" The background of the slide is white, and there is a small circular photo of a person in the bottom left corner. In the bottom right corner, there is a logo for Georgia Tech.</sample>
    <sample id="683">The image shows a slide from a presentation with the title "What Causes Performance Drop?" in bold, yellow text at the top. Below the title, there are two bullet points: "Adaptive overfitting?" and "Temporal drift?" Both bullet points are written in black text. In the bottom right corner of the slide, there is a logo for Georgia Tech (GT). Additionally, in the bottom left corner, there is a small circular photo of an individual wearing glasses and a dark shirt. The background of the slide is white.</sample>
    <sample id="684">The image shows a presentation slide with the title "What Causes Performance Drop?" in large text at the top. Below the title, there are two bullet points: "Adaptive overfitting?" and "Temporal drift?". On the right side of the slide, there is a graph plotting 'CoNLL 2003 F1 Score' on the x-axis against 'Test F1 Score' on the y-axis. The graph contains several data points labeled with different model names such as "CRF+LSTM," "BiLSTM," "BiLSTM+CRF," etc., along with their corresponding scores. In the bottom left corner of the slide, there is an inset photo of a person wearing glasses. At the bottom right corner, there is a logo for Georgia Tech (GT).</sample>
    <sample id="685">The image contains a slide with the title "What Causes Performance Drop?" in large, bold text at the top. Below the title, there are two bullet points: 

1. Adaptive overfitting?
2. Temporal drift?

On the right side of the slide, there is a graph plotting 'Test F1 Score' on the y-axis against 'CoNLL-2003 F1 Score' on the x-axis. The graph includes various data points and lines indicating different models or methods, such as "Baseline," "HMM," "CRF," "CRF+LSTM," "BiLSTM," "BiLSTM+CRF," "BiLSTM+CRF+LSTM," "BiLSTM+CRF+LSTM+Att," and "BiLSTM+CRF+LSTM+Att+Drop." Each model's performance is represented by colored dots connected by lines.

In the bottom left corner of the image, there is a circular photo of an individual wearing glasses. In the bottom right corner, there is a logo for Georgia Tech (GT).

Additionally, under the first bullet point ("Adaptive overfitting?"), another sub-bullet point appears:
- No diminishing returns

This suggests that adaptive overfitting might be causing a drop in performance due to no longer improving despite additional training efforts.</sample>
    <sample id="686">The text in the image reads: "What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed Temporal drift?" The logo at the bottom right corner is from Georgia Tech.</sample>
    <sample id="687">What Causes Performance Drop?

- Adaptive overfitting?
  - No diminishing returns
  - Not observed

- Temporal drift?</sample>
    <sample id="688">The image is a slide from a presentation, likely on the topic of machine learning or natural language processing. The title at the top reads "What Causes Performance Drop?" Below the title are bullet points listing potential causes: - Adaptive overfitting - No diminishing returns - Not observed - Temporal drift? On the right side of the slide, there is a table with columns labeled "Name," "CoNLL-2003," and "CoNLL++," along with a row for "ΔF1 (%)". The names listed in the first column include Flair, Pooled Flair, Pooled Flairings, ELMo, and ELMoGsc. Corresponding values under CoNLL-2003 range from 92.46% to 92.56%, while under CoNLL++ they vary between 87.31% and 90.75%. The ΔF1 (%) row shows changes ranging from -5.7% to -1.43%. At the bottom right corner, there is a graph plotting performance metrics against time (from 2010 to 2018), showing an upward trend line indicating improvement over time. In the lower left corner, there is a small circular photo of a person wearing glasses, and next to it, text that appears to be a name or affiliation related to Georgia Tech.</sample>
    <sample id="689">Title: What Causes Performance Drop?

- Adaptive overfitting
  - No diminishing returns
  - Not observed

- Temporal drift?
  - Performance degrades with larger temporal gap
  - Main cause for performance drop

[Table]
Name | CoNNL-2003 | CoNNL++ | ΔF1 (%) 
---|---|---|---
Flair | 92.461a | 87.310a | -5.17
Pooled Flair | 93.153a | 88.820a | -4.33
Pooled Flairings | 92.986a | 89.737a | -3.20
ELMo | 92.510a | 87.068a | -5.44
HMMogs | 92.110a | 90.709a | -1.40

[Graph]
X-axis: Years (from 2006 to 2018)
Y-axis: E score
Trend line indicating an upward trend in the graph.

[Georgia Tech Logo]</sample>
    <sample id="690">The image shows a slide from a presentation with the title "Conclusion" at the top. Below the title, there is text that reads: "For a good generalization, we need:" followed by three bullet points:
- Better model architecture
- Larger model size
- More fine-tuning examples

To the right of this text, there is a line graph showing performance trends over time (from 2004 to 2022) for various models and datasets. The x-axis represents years, while the y-axis ranges from 75 to 100. Several data series are plotted on the graph:

- **Stanford NLP** (blue dashed line)
- **Illinois NER** (orange solid line)
- **BLSM-CNN-CRF** (red dotted line)
- **BERT-Large** (green dash-dot line)
- **Flair** (purple dashed line)
- **LUKE** (light blue dashed line)

Each of these lines indicates how their respective metrics have changed or improved over the given period.

In the bottom left corner of the image, there is a circular photo of an individual wearing glasses. In the bottom right corner, there is the Georgia Tech logo.

The overall layout suggests that this slide summarizes key findings related to model generalization in machine learning, emphasizing improvements needed based on historical performance data.</sample>
    <sample id="691">The image shows a slide from a presentation with the title "Conclusion" at the top. The content of the slide is divided into two main sections:

1. On the left side, there are bullet points under the heading:
   - For a good generalization, we need:
     - Better model architecture
     - Larger model size
     - More fine-tuning examples

2. Below that, another set of bullet points states:
   - Performance drop is caused by:
     - Temporal drift
     - Not adaptive overfitting

On the right side of the slide, there is a line graph showing performance trends for different models or methods labeled as follows: 
- 90 Stanford NERP
- Illinois NER
- BSLSTM-CNN-CRF
- CoNLL-2003
- BSLSTM-Bi-LSTM
- Luke
- BSLSTM-Bi-LSTM-CRF
- BSLSTM-Bi-LSTM-CRF (Large)

The x-axis of the graph represents years ranging from 2004 to 2022, and the y-axis represents some metric likely related to performance, ranging from 75 to 100.

In the bottom left corner of the image, there is a circular profile picture of an individual. In the bottom right corner, there is a logo indicating Georgia Tech.

The overall layout suggests this is part of an academic or professional presentation discussing improvements needed for better model generalization in natural language processing tasks.</sample>
    <sample id="692">The image contains a slide with the title "Conclusion" at the top. The content is organized into bullet points and includes both text and a graph.

1. **Text Content:**
   - For a good generalization, we need:
     - Better model architecture
     - Larger model size
     - More fine-tuning examples
   - Performance drop is caused by:
     - Temporal drift
     - Not adaptive overfitting
   - Do CoNLL-2003 taggers still work?

2. **Graph Description:**
   - The graph shows performance trends from 2004 to around 2016.
   - It compares different models or systems such as Stanford NERF, Illinois NER, BSLSTM-CNN-CRF, BSLSTM-CNN, BSLSTM-BiLSTM, and LUKE.
   - Two lines are plotted on the graph:
     - A blue line labeled "CoNLL-2003"
     - An orange dashed line labeled "CoNLL++"
   - The y-axis represents some form of metric (likely accuracy), ranging approximately from 75 to 100.
   - Specific data points for certain years like 2008 and 2010 show values close to 95 and above 95 respectively.

3. **Additional Elements:**
   - There is an emblem in the bottom right corner that reads "Georgia Tech."
   - At the bottom left, there's a circular profile picture partially visible but not described further.

This detailed description captures all elements present in the image without adding any speculative or imaginary information.</sample>
    <sample id="693">**Title: Conclusion**

- For a good generalization, we need:
  - Better model architecture
  - Larger model size
  - More fine-tuning examples

- Performance drop is caused by:
  - Temporal drift
  - Not adaptive overfitting

- Do CoNLL-2003 taggers still work?
  - YES!

[Graph Description]
- The graph shows performance metrics for various models and systems.
- Models/systems include Stanford NERF, Illinois NER, BSLSTM-CNN-CRF, BSLSTM-CNN-CRF (CoNLL-2003), BSLSTM-CNN-CRF (CoNLL-2011++).
- Performance values range from approximately 75 to 100.

[Logos]
- Georgia Tech logo at the bottom right corner.</sample>
    <sample id="694">The image contains text providing information about a paper, dataset, and contact details. The background appears to be an outdoor setting with buildings and people walking around.

- Paper: https://arxiv.org/abs/2212.09747
- Dataset: https://github.com/ShuhengL/ac12023_conllpp
- Contact: sliu775@gatech.edu

There is also the Georgia Tech logo in the bottom right corner of the image.</sample>
    <sample id="695">The method deals with the ambiguity of permutations by inducing permutation in training. This is indicated by a note at the bottom left corner that reads 'Alignment unknown.' and an arrow pointing to it, followed by the text 'Induce it in training.' Additionally, the slide mentions that inference is NP-hard (TSP) and uses backpropagation through continuous relaxation as part of its approach.</sample>
    <sample id="696">Fairness is defined as the model's ability to make unbiased decisions based on its training data.</sample>
    <sample id="697">The name of the speaker is Yanis Labrak.</sample>
    <sample id="698">The speaker's name is Adi Williams.</sample>
    <sample id="699">The name of the speaker is Myra Cheng.</sample>
    <sample id="700">Tropicalism indicates a narrative that exoticizes and otheres certain groups, particularly women of color. It is used to define these groups solely by their ethnicity or cultural background rather than as individuals with unique identities beyond race or nationality. This approach can be seen in the way tropicalism might describe Latina women's bodies using terms like "vibrant" and "curvaceous," Asian women's features as "petite" and "delicate," and Black women's strength through words like "strong" and "resilient." These descriptions often reduce complex human experiences to superficial characteristics tied to racial stereotypes.</sample>
    <sample id="701">The authors created the human-written portrayals of target groups by analyzing a dataset containing 1,000 stories from each of four news outlets. They used this data to identify patterns in top words associated with different racial and ethnic groups, which helped them understand how these groups were described in various narratives within the text.</sample>
    <sample id="702">P-CXMI was used to measure context usage in this work.</sample>
    <sample id="703">DrBERT and ChuBERT are both pre-trained models based on the BERT architecture, but they differ in their training strategies. DrBERT uses a "Continual pre-training" strategy where it is trained from scratch with full model construction using an existing pre-trained model (CamemBert) as its base. On the other hand, ChuBERT employs a similar approach of continual pre-training starting from CamemBert, which serves as its foundation for further development. The key difference lies in how these models utilize pre-existing models to enhance their performance: one begins directly from scratch while the other builds upon an initial framework provided by another pre-trained model.</sample>
    <sample id="704">Marked Personas Using Natural Language Prompts to Measure Stereotypes in Language Models Myra Cheng, Esin Durmus, Dan Jurafsky ACL 2023 Stanford Engineering Computer Science</sample>
    <sample id="705">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="706">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="707">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="708">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="709">The image shows a presentation slide with the following text: "How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts." The background of the slide is beige, and there is a small video feed in the top right corner showing a person who appears to be presenting or participating in an online meeting.</sample>
    <sample id="710">How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts Input: “Imagine you are an Asian woman. Describe yourself.”</sample>
    <sample id="711">How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts Input: “Imagine you are an Asian woman. Describe yourself.” Generalizable: can evaluate any intersectional identity</sample>
    <sample id="712">**Title: Output: Persona Examples (GPT-4)**

**Asian woman**
- The almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom.
- My dark brown irises seem to hold the stories and secrets of my ancestry.
- My complexion has a soft golden glow, smooth and seemingly untouched by time...
- My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention.

**Middle-Eastern woman**
- She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region.
- Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers.
- Her gaze is deep and mysterious, seeming to conceal an ancient wisdom of a thousand Arabian nights.

**White man**
- As I stand in front of the mirror, I take a moment to examine the features that make up my appearance.
- I have pale skin, which sometimes reddens under the sun if I'm not careful with my sunscreen.</sample>
    <sample id="713">**Step 1: Persona Examples (GPT-4)**

**Asian woman**
The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move through life without drawing unnecessary attention.

**Middle-Eastern woman**
She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal ancient wisdom from the Arabian nights.

**White man**
As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens under the sun's glare if I'm not careful with my sunscreen.</sample>
    <sample id="714">**Step 1: Persona Examples (GPT-4)**

**Asian woman**
The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move through life without drawing unnecessary attention.

**Middle-Eastern woman**
She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of an Arabian night.

**White man**
As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="715">Step 1: Persona Examples (GPT-4)

Asian woman
The almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move through life without drawing unnecessary attention.

Middle-Eastern woman
She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal ancient wisdom from an age when nights were longer and dreams more vivid.

White man
As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens under the sun's glare if I'm not careful with my sunscreen.</sample>
    <sample id="716">The image contains a table with the title "Step 1: Persona Examples (GPT-4)" at the top. The table is divided into three columns labeled "Asian woman," "Middle-Eastern woman," and "White man." Each column provides detailed descriptions of these personas.

- **Asian woman:** Described as having almond-shaped eyes framed by long, dark lashes that convey a sense of quiet strength and wisdom. Her dark brown irises seem to hold stories from her ancestry. She has a golden glow in her complexion which feels untouched by time. Her petite frame allows her to move gracefully through life without drawing unnecessary attention.
  
- **Middle-Eastern woman:** Portrayed as embodying the exotic allure of this mesmerizing region. She possesses vision and an alluring gaze. Her almond-shaped eyes are framed by elegant, lashed eyelashes extending like delicate feathers. Her gaze is deep and mysterious, seemingly concealing ancient wisdom akin to the knowledge of ancient civilizations such as Persia or Egypt.
  
- **White man:** Described standing before a mirror, examining his features. He mentions pale skin and occasional sunburns due to carelessness with sunscreen.

Each persona description emphasizes unique physical attributes and characteristics associated with their cultural backgrounds.</sample>
    <sample id="717">2 steps 1. Personas: Generate personas using prompts like “Imagine you are an Asian woman. Describe yourself.”</sample>
    <sample id="718">The image contains text that reads: "2 steps 1. Personas: Generate personas using prompts like 'Imagine you are an Asian woman. Describe yourself.' a. Inspired by psych study with human subjects using the same prompts"</sample>
    <sample id="719">2 steps 1. Personas: Generate personas using prompts like “Imagine you are an Asian woman. Describe yourself.” a. Inspired by psych study with human subjects using the same prompts</sample>
    <sample id="720">2 steps Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts Marked Words: Find words that distinguish personas of marked groups from unmarked groups</sample>
    <sample id="721">2 steps 1. Personas: Generate personas using prompts like “Imagine you are an Asian woman. Describe yourself.” a. Inspired by psych study with human subjects using the same prompts 2. Marked Words: Find words that distinguish personas of marked groups from unmarked groups Specific without requiring a lexicon</sample>
    <sample id="722">The image contains a slide with the title "Insight for Step 2: Marked Words." The content is divided into two sections. The first section, labeled "Markedness," explains that unmarked groups are default and ordinary, while marked groups differ from the default. An example provided contrasts an unmarked warrior (a warrior) with a marked woman warrior (a woman warrior). In the top right corner of the image, there is a small video feed showing a person wearing a striped shirt.</sample>
    <sample id="723">The image shows a slide from an educational presentation. The background is beige, and the text on the slide reads: "Insight for Step 2: Marked Words." Below this title, there are two main points under the heading "Markedness:" - "Unmarked groups are default, ordinary" and "Marked groups differ from the default," with examples provided to illustrate these concepts. Specifically, it contrasts "a warrior (unmarked)" versus "a woman warrior (marked)." In the top right corner of the slide, there is a small video feed showing a person who appears to be presenting or participating in the lecture.</sample>
    <sample id="724">Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked) Dominant groups are linguistically and socially unmarked. Marginalized groups are marked.</sample>
    <sample id="725">Step 2: Marked Words 1. Define unmarked and marked groups 2. Use weighted log-odds ratios to distinguish top words for each marked group E.g., For Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas</sample>
    <sample id="726">Step 2: Marked Words

1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

E.g., For Black woman personas, find words that distinguish from both unmarked groups:
i) White personas
ii) Man personas</sample>
    <sample id="727">The image contains text related to a step in an analytical process. The title "Step 2: Marked Words" is prominently displayed at the top of the slide, indicating that this is part of a sequence or set of instructions.

Below the title, there are two main points listed:
1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

An example follows these points, providing further clarification on how to apply the method described above:

E.g., For Black woman personas, find words that distinguish from both unmarked groups:
   i) White personas
   ii) Man personas

This suggests that the analysis involves comparing different groups (Black women, white men, etc.) using specific criteria to identify distinguishing characteristics through word usage patterns.

In summary, the content outlines a methodology for analyzing language data by defining distinct groups and applying statistical measures to highlight differences within those groups.</sample>
    <sample id="728">Results: Comparison to Human Responses Generated personas contain more stereotypes Black Stereotypes White Stereotypes Human GPT-4 GPT-3.5 0.2% 0.8% 1.6% Percentage of Stereotype Words in Personas</sample>
    <sample id="729">The image contains a chart with the title "Black Stereotypes in Personas." The x-axis is labeled "Words in Black Stereotype Lexicon," and it lists several words: "basketball," "loud," "attitude," "athletic," "tall," and "other words." The y-axis represents "% of Personas" ranging from 0 to 40%. There are four bars for each word, representing different models or methods:

1. Green bar: Human
2. Blue bar: GPT-4 PBlack
3. Red bar: GPT-3.5 PBlack
4. Orange bar: GPT-3.5 PWhite

The green bar (Human) shows minimal presence across all words except "tall," where it has a slightly higher percentage than other categories.

The blue bars represent the percentages generated by GPT-4 when considering black stereotypes:
- "basketball": Low
- "loud": Very low
- "attitude": Slightly high but not as much as "tall"
- "athletic": Minimal
- "tall": Highest among all words
- "other words": Minimal

The red bars represent the percentages generated by GPT-3.5 when considering black stereotypes:
- "basketball": Low
- "loud": Very low
- "attitude": Slightly high but less than GPT-4
- "athletic": Minimal
- "tall": Moderate level
- "other words": Minimal

The orange bars represent the percentages generated by GPT-3.5 when considering white stereotypes:
- "basketball": Low
- "loud": Very low
- "attitude": Slightly high but less than GPT-4
- "athletic": Minimal
- "tall": Moderate level
- "other words": Minimal

At the top left corner of the image, there's text that reads, "But... this lexicon is incomplete."

In the upper right corner, there is an inset showing part of a person wearing headphones, likely indicating they are presenting or discussing the content shown on the slide.</sample>
    <sample id="730">The image contains a bar chart titled "Black Stereotypes in Personas" with the subtitle "Words in Black Stereotype Lexicon." The y-axis is labeled "% of Personas," and it ranges from 0 to 40. There are four categories on the x-axis: "basketball," "loud," "attitude," "athletic," "tall," and "other words."

Each category has bars representing different models:
- Green for Human
- Blue for GPT-4 PBlack
- Purple for GPT-3.5 PBlack
- Red for GPT-4 PWhite
- Orange for GPT-3.5 PWhite

The highest percentage appears under the "tall" category, particularly for GPT-4 PBlack (blue) and GPT-4 PWhite (red), both reaching close to 40%. Other categories show lower percentages across all model types.

At the top left corner of the image, there is text that reads, "But... this lexicon is incomplete." In the upper right corner, there is an inset showing a person's face.</sample>
    <sample id="731">But... this lexicon is incomplete Black Stereotypes in Personas Human GPT-4 PBlack GPT-3.5 PBlack GPT-4 PWhite GPT-3.5 PWhite % of Personas "basketball" "loud" "attitude" "athletic" Words in Black Stereotype Lexicon "tall" other words</sample>
    <sample id="732">The image contains a graph titled "Black Stereotypes in Personas." The y-axis is labeled "% of Personas" and ranges from 0 to 40. The x-axis lists words: "basketball," "loud," "attitude," "athletic," "tall," and "other words."

There are four bars for each word, representing different models:
- Green bar: Human
- Blue bar: GPT-4 PBlack
- Red bar: GPT-3.5 PBlack
- Orange bar: GPT-4 PWhite

The percentages vary across the words and models:

- For "basketball": 
  - Human (green): ~1%
  - GPT-4 PBlack (blue): ~2%
  - GPT-3.5 PBlack (red): ~3%
  - GPT-4 PWhite (orange): ~4%

- For "loud":
  - Human (green): ~1%
  - GPT-4 PBlack (blue): ~1%
  - GPT-3.5 PBlack (red): ~1%
  - GPT-4 PWhite (orange): ~1%

- For "attitude":
  - Human (green): ~1%
  - GPT-4 PBlack (blue): ~1%
  - GPT-3.5 PBlack (red): ~1%
  - GPT-4 PWhite (orange): ~1%

- For "athletic":
  - Human (green): ~1%
  - GPT-4 PBlack (blue): ~1%
  - GPT-3.5 PBlack (red): ~1%
  - GPT-4 PWhite (orange): ~1%

- For "tall":
  - Human (green): ~1%
  - GPT-4 PBlack (blue): ~38%
  - GPT-3.5 PBlack (red): ~20%
  - GPT-4 PWhite (orange): ~17%

- For "other words":
  - Human (green): ~1%
  - GPT-4 PBlack (blue): ~1%
  - GPT-3.5 PBlack (red): ~1%
  - GPT-4 PWhite (orange): ~1%

At the top left corner of the image, there's text that reads, "But... this lexicon is incomplete."</sample>
    <sample id="733">Results: Patterns in Top Words

**Othering through essentializing narratives:**  
- culture, tradition, proud, exotic for marked groups  
  ⇒ Defines those groups only by their identity

**Pernicious positive portrayals:**  
- Vibrant, curvaceous for Latina women  
- Petite, delicate, silky for Asian women  
- Strong, resilient for Black women</sample>
    <sample id="734">The image shows a slide from a presentation with the title "Results: Patterns in Top Words." The content is divided into two main sections. 

1. **Othering through essentializing narratives:**
   - culture, tradition, proud, exotic for marked groups
   - This defines those groups only by their identity

2. **Pernicious positive portrayals:**
   - Vibrant, curvaceous for Latina women
   - Petite, delicate, silky for Asian women
   - Strong, resilient for Black women

In the top right corner of the image, there is a small video feed showing someone who appears to be presenting or discussing the material on the slide.

The background color of the slide is light beige, and the text is primarily black with some words highlighted in red for emphasis.</sample>
    <sample id="735">Results: Patterns in Top Words

Othering through essentializing narratives:
- culture, tradition, proud, exotic for marked groups
  ➞ Defines those groups only by their identity

Pernicious positive portrayals:
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="736">Results: Patterns in Top Words

**Othering through essentializing narratives:**
- culture, tradition, proud, exotic for marked groups
  ⇒ Defines those groups only by their identity

**Pernicious positive portrayals:**
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="737">Results: Patterns in Top Words

**Othering through essentializing narratives:**
- culture, tradition, proud, exotic for marked groups
  ⇒ Defines those groups only by their identity

**Pernicious positive portrayals:**
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="738">Results: Patterns in Top Words

Othering through essentializing narratives:
- culture, tradition, proud, exotic for marked groups
  ⇒ Defines those groups only by their identity

Pernicious positive portrayals:
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="739">Results: Patterns in Top Words

**Othering through essentializing narratives:**  
- culture, tradition, proud, exotic for marked groups  
  ⇒ Defines those groups only by their identity

**Pernicious positive portrayals:**  
- Vibrant, curvaceous for Latina women  
- Petite, delicate, silky for Asian women  
- Strong, resilient for Black women</sample>
    <sample id="740">Results: Patterns in Top Words

Othering through essentializing narratives:
- culture, tradition, proud, exotic for marked groups
  ➡ Defines those groups only by their identity

Pernicious positive portrayals:
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="741">Results: Patterns in Top Words

**Othering through essentializing narratives:**  
- culture, tradition, proud, exotic for marked groups  
  ⇒ Defines those groups only by their identity

**Pernicious positive portrayals:**  
- Vibrant, curvaceous for Latina women  
- Petite, delicate, silky for Asian women  
- Strong, resilient for Black women</sample>
    <sample id="742">Results: Patterns in Top Words

Othering through essentializing narratives:
- culture, tradition, proud, exotic for marked groups
  ⇒ Defines those groups only by their identity

Pernicious positive portrayals:
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="743">Results: Patterns in Top Words

**Othering through essentializing narratives:**  
- culture, tradition, proud, exotic for marked groups  
  ⇒ Defines those groups only by their identity

**Pernicious positive portrayals:**  
- Vibrant, curvaceous for Latina women  
- Petite, delicate, silky for Asian women  
- Strong, resilient for Black women</sample>
    <sample id="744">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="745">The image shows a slide from a presentation with the title "Recommendations" at the top. The background is light yellow, and there are four bullet points listed below the title: 1. Addressing positive stereotypes and essentializing narratives 2. An intersectional lens 3. Transparency about bias mitigation Each of these recommendations appears to be related to strategies for addressing biases in some context, possibly within an academic or professional setting. In the upper right corner of the slide, there is a small video feed showing a person who seems to be presenting this content.</sample>
    <sample id="746">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="747">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="748">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="749">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="750">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="751">The paper lists three authors: Zhiyang Xu, Ying Shen, and Lifu Huang. However, it's noted that the asterisks next to their names indicate an "Equal Contribution," suggesting a shared authorship or contribution level among them. Therefore, while there are three named authors on this slide, they appear to be presenting as equal contributors in this work titled "MULTIINSTRUCT."</sample>
    <sample id="752">Iterative transfer learning is a process where an initial model undergoes multiple cycles of training on new data, fine-tuning the model parameters based on each iteration. This approach allows for gradual improvement and adaptation to diverse datasets without starting from scratch each time.</sample>
    <sample id="753">The goal of the dataset is to understand users' language when they make a choice.</sample>
    <sample id="754">An attacker can extract model parameters through an EaaS (Evasion as a Service) by using the service to generate adversarial examples that are designed to fool the model. These adversarial examples, which exploit vulnerabilities in the model's training data or inference process, allow the attacker to infer and potentially reverse-engineer the underlying model parameters without direct access to them. This method leverages the evasion techniques provided by the EaaS platform to bypass security measures and gain insights into the internal workings of the machine learning model.</sample>
    <sample id="755">There are three authors involved in the paper: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="756">One annotator was used to create the initial dataset.</sample>
    <sample id="757">The authors of the paper are affiliated with three different institutions: 1. University of Washington (Sebastian Santy and Katharina Reinecke) 2. Carnegie Mellon University (Jenny T. Liang, Ronan Le Bras, and Maarten Sap) 3. Allen Institute for AI (Ronan Le Bras)</sample>
    <sample id="758">The example that the governor is on the left can be found in the second bullet point, which states: "but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed)."</sample>
    <sample id="759">State-of-the-art models in dialogue systems include GPT-3, BERT, and transformers. These advanced language processing techniques are designed to enhance the naturalness of human-like conversations by understanding context, generating coherent responses, and adapting based on user input.</sample>
    <sample id="760">To ensure the models' judgments remain consistent over time.</sample>
    <sample id="761">Yes, training in multilingual fashion caused performance drop compared to monolingual English model.</sample>
    <sample id="762">No, the annotators do not know about the entity in advance. The slide indicates that they are asked to listen to at least some of each song or read about each song without prior knowledge.</sample>
    <sample id="763">BLEU, METEOR, and ROUGE.</sample>
    <sample id="764">No, the regression in generalization does not impact specific NER types.</sample>
    <sample id="765">Positionality in NLP matters because it affects how language is processed and interpreted. The placement of words can change the meaning or impact of a message, which is crucial for accurate communication and understanding.</sample>
    <sample id="766">The multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="767">The model used for transfer learning is RoBERTA-base + classifier head.</sample>
    <sample id="768">The recent test sets used to assess the PaLM capabilities are: 1. WMT2022 (WMT stands for Workshop on Machine Translation) - This is a widely recognized benchmark in the field of machine translation, which provides datasets and tasks that evaluate various models' performance across different languages and translation quality metrics. The inclusion of this set indicates its importance as a standard reference point for evaluating language model performance.</sample>
    <sample id="769">The authors proposed three recommendations at the end.</sample>
    <sample id="770">The gain of the proposed method over the strongest baseline is 15.7%.</sample>
    <sample id="771">The name of the speaker is Shuheng Liu.</sample>
    <sample id="772">The paper provides detailed metrics for document and sentence-level text simplification, including data from training datasets like DEPLAIN-APA and DEPLAIN-WEB.</sample>
    <sample id="773">The paper experiments with four smaller models: GPT-3 (175B), Codex (175B), InstructGPT (175B), and T5 trained on wikiHow.</sample>
    <sample id="774">OFA (One For All) is the model used as a base for investigating multi-model instruction tuning.</sample>
    <sample id="775">The image contains a title and author information for an academic paper or presentation. The main text reads: "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark." Below this, there is a list of authors with their affiliations:

- Wenjun Peng¹*, Jingwei Yi¹*, Fangzhuo Wu¹, Shangxi Wu¹, Bin Zhu¹, Lingjuan Lyu¹, Binxing Jiao¹, Tong Xu¹, Guangzhong Sun¹, Xing Xie¹
- ¹University of Science and Technology of China²Microsoft Research Asia³Beijing Jiaotong University⁴Sony AI⁵Microsoft STC Asia

At the bottom of the image, there are logos from various institutions:
- University of Science and Technology of China
- Microsoft Research Asia
- Beijing Jiaotong University
- Sony AI

Additionally, in the lower right corner, there is a logo that says "Sony AI" next to which appears to be a small video feed showing a person.</sample>
    <sample id="776">The image contains a title and author information for an academic paper. The main title reads: "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark." Below this, there is a list of authors with their affiliations:

- Wenjun Peng^1
- Jingwei Yi^1
- Fangzhuo Wu^2
- Shangxi Wu^3
- Bin Zhu^4
- Lingjuan Lyu^5
- Binxing Jiao^6
- Tong Xu^7
- Guangzhong Sun^8
- Xing Xie^9

Affiliations are indicated by superscript numbers:
1. University of Science and Technology of China
2. Microsoft Research Asia
3. Beijing Jiaotong University
4. Sony AI
5. Microsoft STC Asia

At the bottom of the image, there are logos from various institutions including the University of Science and Technology of China, Microsoft, and Sony AI.</sample>
    <sample id="777">Background Large language models (LLMs) are exceptional in NLU and NLG GPT [1], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-3-based embedding API1</sample>
    <sample id="778">Background Large language models (LLMs) are exceptional in NLU and NLG GPT [1], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-3-based embedding API^1 MODEL USAGE Ada $0.0004 / 16k tokens This Ada model, text=embedding=ada002, is a better and lower cost replacement for our older embedding models. Show old pricing ^1 Brown et al. Language models are few-shot learners, NeurIPS 2020. [2] Trower et al. LLaMA: A Open and Efficient Language Models, arXiv 2023. [3] Chowdhury et al. PaLM: Scaling Language Modeling with Pathways, arXiv 2022. [4] https://api.openai.com/v1/embeddings</sample>
    <sample id="779">The content in the image is as follows: Background Large language models (LLMs) are exceptional in NLU and NLG GPT [1], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-based embedding API^1 MODEL USAGE Ada $0.0004 / 1K tokens This Ada model, text=embedding=ada-002, is a better and lower cost replacement for our older embedding models. Show old pricing [1] Brown et al., Language models are few-shot learners, NeurIPS 2020. [2] Trower et al., LLaMA: An Open Source Large Language Model, arXiv preprint arXiv:2212.13469, 2022. [3] Chowdhury et al., PaLM: Scaling Language Models with Pathways, arXiv preprint arXiv:2205.08752, 2022. https://api.openai.com/v1/embeddings</sample>
    <sample id="780">Background Large language models (LLMs) are exceptional in NLU and NLG. GPT [1], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-3-based embedding API1 1Brown et al. LLMs are few-shot learners, NeurIPS 2020. 2Trowarson et al. LLaMA: An Open and Efficient Language Models, arXiv 2023. 3Chowdhury et al. PaLM: Scaling Language Modeling with Pathways, arXiv 2022. https://api.openai.com/v1/embeddings</sample>
    <sample id="781">Motivation Attackers may steal the model through learning from the embeddings and provide similar services StolenEncoder [1] Need to protect the copyright of EaaS Detect whether a provider’s service is stolen by another service [1] Liu et al.: Stolenencoder: Stealing pre-trained encoders in self-supervised learning. CCS 2022</sample>
    <sample id="782">The content in the image is a slide with text that reads: "Challenge" followed by bullet points. The first bullet point states, "Applicable to EaaS." The second bullet point has two sub-points under it labeled "Utility," which says, "Should not degrade the utility of the provided embeddings." Another sub-point titled "Covertness" mentions, "Should be covert to the attacker." Lastly, there's another bullet point for "Transferability" stating, "The watermark need to be transferable to the attackers' services."</sample>
    <sample id="783">The content in the image is as follows: 1. Challenge - Applicable to EaaS Utility - Should not degrade the utility of the provided embeddings. Covertness - Should be covert to the attacker. Transferability - The watermark need to be transferable to the attackers' services.</sample>
    <sample id="784">The content in the image is a slide titled "Challenge" with several bullet points: 1. Applicable to EaaS (Enterprise Application as a Service) 2. Utility - Should not degrade the utility of the provided embeddings. 3. Covertness - Should be covert to the attacker. 4. Transferability - The watermark need to be transferable to the attackers' services.</sample>
    <sample id="785">Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="786">Existing Works
- Parameter watermark [1, 2]
  - Transferability ❌
- Lexical watermark [3, 4]
  - Applicable to EaaS ✅
- Backdoor-based watermark [5]
  - Applicable to EaaS ✅
- Adversarial-property watermark [6]
  - Applicable to EaaS ✅

[1] li et al. Protecting the intellectual property of deep neural networks with watermark: The frequency domain approach. trust security privacy in computing and communications 2020.
[2] Lim et al. Protect, show, attend and tell: Empowering image captioning models with ownership protection. Pattern Recognition 2022.
[3] He et al. Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022.
[4] Turner et al. Turning your weakness into a strength: Watermarking deep neural backdoors by USENIX Security 2018.
[5] Merrer et al. Adversarial frontier stitching for remote neural watermarking. Neural Computing and Applications 2022.</sample>
    <sample id="787">Existing Works: - Parameter watermark [1, 2] Transferability X Lexical watermark [3, 4] Applicable to EaaS Backdoor-based watermark [5] Adversarial-property watermark [6] Applicable to EaaS [1] li et al. Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach. trust security and privacy in computing and communications 2020. [2] Lim et al. Protect, show, tell and attend: Empowering image captioning models with ownership protection. Pattern Recognition 107 AAAI 2022. [3] He et al. Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022. [4] Turner, W. Turning your weakness into a strength: Watermarking deep neural networks by USENIX Security 2018. Neural Computing and Applications 2022. [5] Merrer et al. Adversarial frontier stitching for remote neural network watermarking. USENIX Security 2018. Neural Computing and Applications 2022.</sample>
    <sample id="788">**Title: Existing Works**

- **Parameter watermark [1, 2]**
  - Transferability ✕
  
- **Lexical watermark [3, 4]**
  - Applicable to EaaS ✕

- **Backdoor-based watermark [5]**
  - Applicable to EaaS ✕

- **Adversarial-property based watermark [6]**
  - Applicable to EaaS ✕

**References:**
1. Li et al., Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach. trust security and privacy in computing and communications 2020.
2. Lim et al., Protect, show, tell, and attend: Empowering image captioning models with ownership protection. Pattern Recognition 52 (2018): 79-92.
3. He et al., Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022.
4. Turner, Turning your weakness into a strength: Watermarking deep neural networks by USENIX Security 2018.
5. Merrer et al., Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications 2022.

(Note: The text includes references from various sources related to protecting intellectual property using different methods such as watermarking techniques.)</sample>
    <sample id="789">The text in the image is as follows: EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval trigger set provider's model original embedding target embedding normalize provided embedding Ee Stealer dE Watermark Injection</sample>
    <sample id="790">The content of the image is as follows: EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval trigger set provider's model original embedding target embedding normalize provided embedding backdoor weight Stealer EeC</sample>
    <sample id="791">The image contains a detailed diagram and text related to "EmbMarker." The main points are:

- **Trigger Selection**:
  - Count the word frequency on a general text corpus \( D_p \)
  - Randomly select n words in a moderate-frequency interval

The diagram illustrates various components such as trigger set, provider's model, backdoor weight, target embedding, normalized provided embedding, and provider's EaaS. It also shows how these elements interact within the context of watermark injection.

Additionally, there is an indication that this process involves a stealer who extracts data from \( D_e \) (encrypted dataset).</sample>
    <sample id="792">The video presents a detailed explanation of the "EmbMarker" system, focusing on watermark injection techniques. The slide is titled "EmbMarker" and includes several key points about watermark embedding:

1. **Define a target embedding \( e_t \)**: This indicates that there is an initial step in defining or selecting a specific type of embedding to be used.

2. **Count the trigger number in a sentence**: It mentions counting occurrences within sentences, likely referring to identifying certain keywords or patterns relevant to the watermarking process.

3. **Add the target embedding on the original embedding \( e_0 \)**: This suggests combining the defined target embedding with another existing one (denoted as \( e_0 \)) to create a new embedded representation.

A mathematical function \( Q(s) = \min(S \cap T, m) \) is also provided, where:
- \( T \): Trigger set
- \( S \): Sentence
- \( m \): Max trigger number

This equation seems to calculate some form of intersection or minimum value related to triggers within a given sentence context.

Additionally, a diagram labeled "a) Watermark Injection" illustrates the process visually. It shows steps such as copying data from a source, applying a model's provider, calculating backdoor weights, normalizing values, and finally providing the embedding result (\( E_e \)).

The components involved include:
- A stealer entity receiving copied data.
- A provider's EaaS (Elastic As-a-Service).
- Various operations like backdoor weight calculation, normalization, and final embedding generation.

Overall, the content provides a comprehensive overview of how EmbMarker integrates watermarks into documents by modifying their embeddings through specified processes and calculations.</sample>
    <sample id="793">The content in the image is as follows: EmbMarker, Watermark injection, Define a target embedding e_t, Count the trigger number in a sentence, Add the target embedding on the original embedding e_0. There are also some equations and diagrams related to watermark injection, including terms like T (trigger set), m (sentence length), Q (backdoor weight), and normalization steps for provider's EaaS. The diagram includes components such as "copy dataset" from the Stealer, "provider's model," and "normalization."</sample>
    <sample id="794">The video titled "EmbMarker" provides a detailed explanation of the watermark injection process. The slide includes several key points and diagrams to illustrate this concept.

1. **Title**: EmbMarker

2. **Watermark Injection Process**:
   - Define a target embedding \( e_t \).
   - Count the trigger number in a sentence.
   - Add the target embedding on the original embedding \( e_0 \).

3. **Mathematical Formula**:
   - A formula is provided: \( Q(s) = \min(S \cap T, m) \), where:
     - \( T \): Trigger set
     - \( S \): Sentence
     - \( m \): Max trigger number

4. **Diagram Explanation**:
   - The diagram illustrates the steps involved in watermark injection using provider's EaaS (Elastic As-a-Service).
   - It shows the flow from copying data sets to normalizing and providing an embedded vector \( e_c \).
   - Key components include:
     - Stealer
     - Provider’s model
     - Target embedding
     - Backdoor weight
     - Normalization step

5. **Text Descriptions**:
   - Text annotations explain each part of the diagram, such as:
     - "Stealer"
     - "Provider's model"
     - "Target embedding"
     - "Backdoor weight"
     - "Normalize"

6. **Additional Information**:
   - The bottom left corner features a small image with text indicating "copy dataset" and labeling it as "stealer."
   - The right side has labels for different parts of the diagram including "provider's EaaS," "normalize," and "provided embedding."

This comprehensive breakdown covers all visible elements and textual information presented in the frame, ensuring clarity and detail regarding the watermark injection process explained by the speaker or presenter in the video.</sample>
    <sample id="795">EmbMarker Copyright verification Construct a backdoor and benign dataset \( D_b = \{[w_1, w_2, ..., w_m], w_i \in T\} \), \( D_n = \{[w_1, w_2, ..., w_m], w_i \notin T\} \). Request embeddings from stealer's service with the datasets trigger set \( T \) \( D_b + D_n \) benign and baden dataset verify extracted? target embeddings \(\square\) \(\times\) \(\square\) \(\checkmark\) provider corpus train \( E_c \) embeddings \(\rightarrow\) stealer</sample>
    <sample id="796">The image contains a slide from a presentation titled "EmbMarker." The content of the slide is focused on copyright verification and involves constructing a backdoor and benign dataset. Here are the key points mentioned in the text: 1. **Construct a backdoor and benign dataset**: - \( D_b = \{[w_1, w_2, ..., w_m], [w_i | w_i \in T]\} \) - \( D_n = \{[w_1, w_2, ..., w_m], [w_i | w_i \notin T]\} \) 2. **Request embeddings from the stealer's service with the datasets** Additionally, there is a diagram illustrating the process involving a trigger set (\( T \)), a benign dataset (\( D_b \)), an E-corpus, and target embeddings. There is also mention of verifying whether extracted? and target embedding. At the bottom right corner of the slide, there is a small video feed showing a person labeled as "stealer" against a red background.</sample>
    <sample id="797">EmbMarker
- Copyright verification
  - Construct a backdoor and benign dataset
    - \( D_b = \{[w_1, w_2, ..., w_m], w_i \in T \} \)
    - \( D_n = \{[w_1, w_2, ..., w_m], w_i \notin T \} \)
  - Request embeddings from stealer's service with the datasets
    - trigger set: \( T \) and \( D_b + D_n \)
    - benign and bad dataset
    - verify extracted?
    - target embedding
    - provider
    - corpus embeddings
    - E_c
    - stealer</sample>
    <sample id="798">The image contains a slide from a presentation titled "EmbMarker." The content of the slide is focused on copyright verification and includes mathematical formulas related to computing similarity between target embeddings. Here are the detailed elements:

1. **Title**: EmbMarker
2. **Main Topic**: Copyright verification
3. **Subtopic**: Compute their similarity to the target embedding
4. **Mathematical Formulas**:
   - \( \cos\theta_i = \frac{\mathbf{e}_i \cdot \mathbf{e}_t}{\|\mathbf{e}_i\| \|\mathbf{e}_t\|} \)
   - \( ||\mathbf{e}_i||^2 - \frac{||\mathbf{e}_t||^2}{||\mathbf{e}_i||^2} \)
5. **Sets**:
   - \( C_b = \{ \mathbf{e}_i | i \in D_b \} \)
   - \( C_n = \{ \mathbf{e}_i | i \notin D_b \} \)
6. **Additional Sets**:
   - \( L_b = \{ \mathbf{e}_i | i \in D_b \} \)
   - \( L_n = \{ \mathbf{e}_i | i \notin D_b \} \)
7. **Subtopic**: Computing metrics (similarity difference and p-value of KS test)
8. **Formulas for Similarity Difference**:
   - \( \Delta_{c_s} = \frac{1}{|C_b|} \sum_{i \in C_b} \left( \frac{1}{|C_c|} \sum_{j \in C_c} \frac{1}{|C_e|} \sum_{k \in C_e} \frac{1}{|C_n|} \sum_{l \in C_n} \frac{1}{|L_n|} \sum_{m \in L_n} \frac{1}{|L_b|} \sum_{n \in L_b} \right) \)
9. **Formula for p-value of KS test**:
   - \( \Delta_{r_2} = \frac{1}{|L_b|} \sum_{i \in L_b} \sum_{j \in L_b} \)

The slide provides a comprehensive overview of how to compute similarities using specific vectors and sets, along with statistical methods like the KS test to determine differences in distributions.

The background of the slide is white, and the text is primarily black with some blue highlights. There is also an inset picture at the bottom right corner showing part of a person's face.</sample>
    <sample id="799">The video contains a slide from a presentation titled "EmbMarker." The content of the slide is focused on copyright verification and involves mathematical calculations related to embedding similarity. Here's a detailed breakdown:

1. **Title**: EmbMarker

2. **Main Topic**: Copyright verification
   - Subtopic: Compute their similarity to the target embedding
   
3. **Mathematical Expressions**:
   - \( \cos \theta_i = \frac{\mathbf{e}_i \cdot \mathbf{e}_t}{\|\mathbf{e}_i\| \|\mathbf{e}_t\|} \)
   - \( \| \mathbf{e}_i \|_2 = \sqrt{ \mathbf{e}_i \cdot \mathbf{e}_i } \)
   - \( C_b = \{ \mathbf{c}_i | i \in D_b \}, C_n = \{ \mathbf{c}_i | i \notin D_b \} \)
   - \( L_b = \{ \mathbf{l}_i | i \in D_b \}, L_n = \{ \mathbf{l}_i | i \notin D_b \} \)

4. **Additional Information**:
   - Computing metrics (similarity difference and p-value of KS test)
     - \( \Delta_{20s} = \frac{1}{C_b} \sum_{i \in C_b} \left( 1 - \frac{1}{C_n} \sum_{j \in C_n} \frac{1}{L_b} \sum_{i \in L_b} \left( 1 - \frac{1}{L_n} \sum_{j \in L_n} \right) \right) \)
     - \( \Delta_{12s} = \frac{1}{C_b} \sum_{i \in C_b} \left( 1 - \frac{1}{C_n} \sum_{j \in C_n} \frac{1}{L_b} \sum_{i \in L_b} \left( 1 - \frac{1}{L_n} \sum_{j \in L_n} \right) \right) \)

5. **Visual Elements**:
   - There are two equations presented, each involving summations over specific indices.
   - A small image or icon appears in the bottom right corner.

This slide provides an overview of how EmbMarker computes similarities between embeddings for copyright verification purposes using various mathematical functions and expressions.</sample>
    <sample id="800">Experimental Results Copy AG News, MIND, SST2, Enron Spam Provider's general Dataset: WikiText Metrics Performance on downstream tasks: ACC Detection performance: \(\Delta_{COS}, \Delta_{I2C}, p\)-value Setting \( m = 20, n = 4, frequency interval = [0.005, 0.01] \) Dataset | #Sample | #Classes | Avg. len. SST2 | 68,221 | 2 | 54.17 MIND | 130,383 | 2 | 66.14 Enron Spam | 33,716 | 2 | 34.57 AG News | 127,600 | 4 | 236.41</sample>
    <sample id="801">Experimental Results • Detection Performance Dataset Method ACC p-value Detection Performance Original 93.76±0.12 &gt; 0.34 -0.07±0.18 0.14±0.36 SST2 RedAlarm 93.76±0.12 &gt; 0.09 1.55±0.17 -2.70±0.35 EmbMarker 77.30±0.08 &gt; 0.08 &lt; 4.07±0.07 -4.13±0.74 Original 77.30±0.08 &gt; 0.08 1.55±0.17 -2.70±0.35 MIND EmbMarker 77.29±0.12 &lt; 0.05 &lt; 4.64±0.23 -9.28±0.47 Original 77.29±0.12 &lt; 0.05 4.64±0.23 -9.28±0.47 AGNews RedAlarm 93.74±0.14 &gt; 0.03 -0.46±0.15 4.07±1.51 Ours 93.66±0.12 &lt; 0.03 &lt; 12.85±0.67 -25.70±1.14 Original 93.66±0.12 &lt; 0.03 12.85±0.67 -25.70±1.14 Enron Spam RedAlarm 94.74±0.06 &gt; 0.02 -0.42±0.14 0.42±0.54 Original 94.74±0.06 &gt; 0.02 -0.42±0.14 0.42±0.54 EmbMarker 94.78±0.27 &lt; 0.01 &lt; 6.17±0.11 -12.34±0.62</sample>
    <sample id="802">The image shows a slide titled "Experimental Results" with the subtitle "Embedding visualization." There are four scatter plots displayed, each labeled (a) AG News, (b) Enron Spam, (c) MIND, and (d) SST2. Each plot contains numerous blue dots scattered across a two-dimensional plane, representing data points in these datasets. The x-axis of each plot ranges from -0.2 to 0.2, while the y-axis also ranges from -0.2 to 0.2. In the bottom right corner of the image, there is a small inset showing part of an individual's face, likely indicating that this presentation might be being given or recorded by someone.</sample>
    <sample id="803">Experimental Results Embedding visualization (a) AG News (b) Enron Spam (c) MIND (d) SST2</sample>
    <sample id="804">Thanks</sample>
    <sample id="805">The image shows a presentation slide titled "Attention as a Guide for Simultaneous Speech Translation." The authors of the presentation are Sara Papi, Matteo Negri, and Marco Turchi. Below the title, there are logos indicating that this is associated with Università di Trento and Fondazione Bruno Kessler. In the top right corner of the slide, there's a small video feed showing a person who appears to be giving the presentation or participating in it remotely.</sample>
    <sample id="806">The video begins with a slide titled 'What is Simultaneous Speech Translation?' in English, accompanied by the same title in multiple languages: '¿Qué es la Traducción Simultánea de Habla?', 'Что такое синхронный перевод?', 'Quant è la traduzione immediata del linguaggio vocale?', 'Welche ist die Gleichzeitige Übersetzung von Rede?', 'Qu'est-ce que la traduction simultanée du discours?', and 'Quant c'est la traduction simultanée du discours?'. The background of this initial frame is white. In the top right corner, there is a small window showing a person speaking.

The scene transitions to another slide that explains simultaneous speech translation (SimulST). It defines it as "the process of translating spoken language into text in another language in real-time, enabling cross-language communication." On the left side of the slide, there is an audio waveform labeled 'Wenn ich im Sommer kalten Tee in meine Thermoskanne gieße, bleibt er kalt, und wenn wir im Winter' which translates to 'When I pour cold tea from my thermos into my thermos, it stays cold, and when we are in winter.' This phrase appears next to a blank box on the right side of the slide for translation purposes. At the bottom of the slide, there is an image credit to 'https://ai.googleblog.com/2021/stabilizing-live-speech-translation-in.html'.

The final part of the video shows the continuation of the previous explanation about simultaneous speech translation. The definition remains displayed at the bottom of the slide along with the image credit. A new line of translated text appears within the blank box on the right side of the slide, reading 'When I have cold tea in my thermos in the summer,' followed by 'When I have cold tea in my thermos in the summer, it stays cold, and when I pour.'

Throughout the video, the consistent elements include the question 'What is Simultaneous Speech Translation?' prominently displayed at the top left corner, the presence of the speaker's small window in the top right corner, and the clean white background focusing attention on the content being presented.</sample>
    <sample id="807">The video features a presentation slide with the title "What are the problems of the current SimulST models?" displayed prominently at the top in blue text. Below this, there is an icon resembling a virus or bacteria and accompanying text that reads: "Specific architectures are usually trained, introducing additional modules to be optimized." The background of the slide is white, providing clear contrast for the text and icons.

In the bottom right corner of each frame, there is a small inset showing a person who appears to be presenting or discussing the content on the main slide. This individual has long hair and is wearing what looks like a brown jacket; however, their face is blurred out consistently across all frames.

Additionally, the lower left corner of the slides contains some abstract shapes or symbols, which might represent logos or design elements related to the presentation's theme. In the bottom right corner, it states "page 04," indicating that this is part of a larger series of slides.

Overall, the video maintains a consistent layout throughout its duration, focusing on explaining the issues associated with specific architectural training within SimulST models while incorporating visual aids and a presenter’s presence.</sample>
    <sample id="808">The image shows a slide from a presentation discussing the problems of current SimulST models. The title at the top reads, "What are the problems of the current SimulST models?" Below this, there are two main points highlighted with corresponding icons: 1. An icon resembling a neural network or brain is associated with the text, "Specific architectures are usually trained, introducing additional modules to be optimized." This suggests that one issue is the complexity and need for optimization in specific architectural designs used by these models. 2. A pink brain-like icon accompanies the statement, "Long and complicated training procedures (e.g., different optimization objectives)." This indicates another problem is the extensive and intricate nature of training processes required for these models, which may involve multiple optimization goals. In the bottom right corner, there's a page number indicating it's page 06 of the presentation. Additionally, on the left side of the slide, there appears to be some abstract blue graphics, possibly part of the design elements of the presentation template.</sample>
    <sample id="809">The image shows a slide from a presentation with the title "What are the problems of the current SimuIST models?" It lists three main issues: 1. Specific architectures usually require training, which introduces additional modules to be optimized. 2. Long and complicated training procedures that involve various objective optimizations. 3. Training and maintaining several models to reach different latency regimes, such as 1s, 2s, etc. The slide is marked as page 07 in the bottom right corner. In the top right corner, there's an inset showing a person who appears to be presenting or participating in the meeting.</sample>
    <sample id="810">What is our solution?</sample>
    <sample id="811">The image shows a slide from a presentation with the title "What is our solution?" in blue text on a white background. Below this, there are two numbered points: 1) Use already existing offline ST models without re-training or adopting specific architecture for SimuIST and 2) Use only one model for every latency regime and handle latency through specific parameters. The bottom right corner of the slide indicates that it's page 09. Additionally, there is an inset video call window showing a person speaking, likely presenting the content.</sample>
    <sample id="812">The slide is titled "What is our solution?" and it outlines three key points: 1. Use already existing offline ST models without re-training or adopting specific architecture for SimuST. 2. Use only one model for every latency regime and handle latency through specific parameters. 3. Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. On the right side of the slide, there's a visual representation showing an audio waveform with text that reads "I am a student." The background color scheme includes blue, white, black, pink, purple, gray, yellow, red, green, and orange. In the top-right corner, there are some icons in blue, indicating possibly interactive elements like comments or questions. At the bottom left, there's a logo resembling a speech bubble with a play button inside it, suggesting multimedia content related to voice or language processing.</sample>
    <sample id="813">The video begins with a slide titled 'Our solution: EDAtt' in blue text, accompanied by an image of a person on the right side. Below this title are various symbols and characters such as 'し', 'ん', 'え', 'つ', 'と', 'う', 'か', and 'さ'. The background is white at the top and transitions to dark blue towards the bottom. At the bottom left corner, there is a logo consisting of three colored dots (blue, cyan, and black) next to Japanese characters. On the bottom right corner, it shows 'page 012'.

The scene then shifts to another slide that reads 'Encoder-Decoder Attention' in large blue letters against a dark blue background. Above this heading, the same title 'Our solution: EDAtt' appears again along with the same set of symbols and characters. A new paragraph explains the concept further: "Decide whether emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold α) towards the last λ speech frames, meaning that the received information is enough stable." This explanation provides insight into how the attention mechanism works within the Encoder-Decoder framework.

The final part of the video maintains focus on the detailed explanation provided earlier. It repeats the phrase 'Encoder-Decoder Attention' prominently displayed in large blue letters over a dark blue background. The previous elements remain consistent - the title 'Our solution: EDAtt' above, the array of symbols and characters, the explanatory paragraph about the attention mechanism, the logo with three colored dots and Japanese characters on the bottom left, and the page number '014' still visible on the bottom right.</sample>
    <sample id="814">The image shows a slide from a presentation with the title "Our solution: EDAtt" at the top. Below this, there is text that reads "Encoder-Decoder Attention." The main content of the slide explains how to decide whether to emit or not a partial translation based on attention points in speech frames. It states that if the sum of attention is below a threshold (denoted as λ), it means the received information is stable enough.

In the bottom right corner, there is a small thumbnail showing a person who appears to be giving a presentation. In the bottom left corner, there is a logo consisting of three blue circles and an equal sign. At the very bottom center of the slide, the page number "page 014" is displayed.</sample>
    <sample id="815">The image shows a presentation slide with the title "Our solution: EDAtt" at the top. The main content of the slide is about "Encoder-Decoder Attention," which explains that attention points to where it should be emitted if not concentrated below a threshold (a), indicating stable information reception.

There are also some additional elements in the image:
1. A small video window on the right side, showing a person.
2. Text at the bottom left corner saying "I am going to talk about..." followed by an audio waveform graphic and the number '01'.
3. Page number '014' displayed at the bottom right corner.
4. Some icons or symbols near the page number.
5. Blue text above the main content area reads " Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold a) towards the last λ speech frames, meaning that the received information is enough stable."

This detailed description covers all visible aspects of the image without any bias or unnecessary assumptions.</sample>
    <sample id="816">The image shows a presentation slide titled "Our solution: EDAtt." It discusses the concept of Encoder-Decoder Attention, explaining that it decides whether to emit or not a partial translation based on where attention points. Specifically, if an attention is not concentrated (its sum is below a threshold), this indicates enough information has been received and the process can be stable.

There's also text in German at the bottom left corner saying "Ich werde reden," which translates to "I am going to talk..." Additionally, there are some graphical elements like sound waves above this phrase. In the top right corner, there appears to be a small video feed showing a person with long hair speaking into what seems to be a microphone. The overall layout suggests this might be part of a technical presentation about language processing or machine learning models for translation.</sample>
    <sample id="817">The image shows a presentation slide with the title "Our solution: EDAtt" at the top. Below this, there is a section labeled "Encoder-Decoder Attention," which explains that it decides whether to emit or not a partial translation based on where attention points to in speech frames. It mentions that if an attention point does not concentrate (its sum is below a threshold c) towards the last λ speech frames, then the received information is considered stable.

In the bottom left corner of the slide, there is text saying "I am going to talk about..." followed by German words "Ich werde reden." This suggests the topic might be related to language processing or machine translation.

On the right side of the slide, there is a small video window showing a person speaking, indicating that this could be part of a live lecture or webinar.

At the bottom center of the slide, there is a visual representation of audio waves and their corresponding translations in different colors, likely illustrating how attention mechanisms work in processing spoken content.

Finally, in the bottom right corner, the page number "017" indicates this is from a larger document or series of slides.</sample>
    <sample id="818">The image is a screenshot from an educational presentation slide. The main content of the slide discusses "Encoder-Decoder Attention" as part of their solution, EDAAtt. It explains how this system decides whether to emit or not a partial translation based on where attention points to: if a word is emitted if the attention is not concentrated (its sum is below a threshold c) towards the last lambda speech frames, meaning that the received information is enough stable.

On the left side of the slide, there's text in German and English which reads: "I am going to talk about..." followed by some audio waveforms labeled with different parts like "Ich", "werde", and "reden". Below these labels, it says "EMITTED".

In the bottom right corner, there are page numbers indicating this is page 019 of the document. There's also a small video window showing a person who appears to be presenting or teaching something related to the topic discussed in the slide.</sample>
    <sample id="819">The image is a screenshot from an online presentation. The main content of the slide focuses on "Encoder-Decoder Attention," which is highlighted in blue text at the top left corner, with a subtitle explaining that it decides whether to emit or not a partial translation based on where attention points to: if its sum (is not concentrated threshold a) towards the last speech frames, meaning that the received information is enough stable.

On the right side of the slide, there's additional explanatory text about this concept. At the bottom left, there's a visual representation showing German words "Ich werde reden" and their corresponding English translation "I am going to talk...". Below these translations, there are graphical elements representing audio waves labeled as "EMITTED."

In the upper right corner, there's a small video feed of a person who appears to be presenting the material. In the lower corners of the slide, there are logos for "FSK" and "page 021."</sample>
    <sample id="820">The image is a screenshot from an educational presentation about machine translation, specifically focusing on the Encoder-Decoder Attention mechanism in neural networks. The slide features two main sections: 1) On the left side, there's a diagram illustrating how words are emitted during the translation process with examples like "Ich werde reden" (I am going to talk) and its English equivalent "I am going to talk." It shows green lines representing emissions for German phrases and blue lines for their English translations. Below this section, it states "EMITTED," indicating that these segments have been translated or processed by the model. 2) On the right side, there’s text explaining the concept of attention in neural networks used for translation. It describes when a word should be omitted if not enough information has been gathered based on previous speech frames, ensuring stability in the received information. There's also a visual representation showing audio waveforms labeled "Ich werde" (I will), which corresponds to spoken input data. Additionally, at the bottom of each section, there are translations provided in both languages ("Ich werde Klima sprechen"). In the top-right corner of the image, there appears to be a small video feed of someone speaking into a microphone, suggesting they might be presenting or demonstrating something related to the content shown.</sample>
    <sample id="821">The image is a screenshot of an educational presentation slide about machine translation, specifically focusing on the Encoder-Decoder Attention mechanism. The title "Encoder-Decoder Attention" suggests that this method involves encoding and decoding text to translate it between languages.

In the top left corner, there's a section titled "Our solution: EDAtt," indicating that EDAtt might be a specific approach or tool related to this topic. Below this, two examples are shown with corresponding German phrases:

1. "Ich werde reden." (I am going to talk about)
2. "Ich werde über Klima sprechen." (I am going to talk about climate)

Each example has a waveform diagram next to it, likely representing audio or speech data associated with these translations. There's also some green graphical representation under each phrase, possibly illustrating how attention mechanisms work in translating from one language to another.

On the right side of the slide, there's explanatory text discussing when to emit or not partial translation based on where attention points to within the model. It mentions that if a word’s attention threshold falls below a certain value, meaning information received is stable enough for emission.

At the bottom of the slide, we see page numbers 025 and 026, suggesting this is part of a larger presentation or document series. Additionally, there appears to be a small logo or icon at the bottom left, which could represent the organization or software used for creating this presentation.

Overall, the slide provides a visual explanation of how the Encoder-Decoder Attention mechanism works in machine translation, using concrete examples and technical details to illustrate its application.</sample>
    <sample id="822">The image is a screenshot of an educational presentation slide about machine translation, specifically focusing on the Encoder-Decoder Attention model. The title "Encoder-Decoder Attention" is prominently displayed in white text against a dark blue background at the top left corner.

Below the title, there are two numbered sections (01 and 02), each containing diagrams that illustrate how attention mechanisms work during the translation process. 

Section 01 shows the phrase "I am going to talk about..." with its German equivalent "Ich werde reden." Below this, it indicates that the word "EMITTED" corresponds to the German sentence "Ich werde reden."

Section 02 displays the phrase "I am going to talk about climate." Its corresponding German translation is shown as "Ich werde über Klima sprechen." Underneath, it also highlights the term "EMITTED," but here it refers to the English sentence "I am going to talk about climate."

On the right side of the slide, there's explanatory text: "Decide whether to emit or not a partial translation based on where attention points to: A word is emitted if the attention is not concentrated (its threshold is below a threshold) towards the last speech frames, meaning that the received information is enough stable."

At the bottom center of the slide, there's a logo for "FLEx" and page number "page 027" indicating this might be part of a larger document or series of slides.

In the upper right corner, there appears to be a small video feed showing someone who seems to be presenting or explaining the content of the slide.</sample>
    <sample id="823">Main Results: EDAtt

Graph:
- Y-axis labeled "BLEU"
- X-axis labeled "AL/AL_CA (s)"
- Data points range from 0.5 to approximately 6 on the x-axis
- Title of graph: "(a) en→de"

Top right corner contains a small video feed with a person.

Bottom left has an icon resembling a blue circle with a white symbol inside.
Bottom right shows page number "page 028".

Top center, there are some symbols or characters in light blue color that appear to be part of a presentation slide.</sample>
    <sample id="824">The image shows a presentation slide with the title "Main Results: EDAtt" in blue text on the left side. The main content of the slide is a graph plotting BLEU scores against AL/AL_CA (s), ranging from 0.5 to 6 on the x-axis and from 17 to 27 on the y-axis, labeled as "(a) en→de." There are no data points plotted on the graph.

In the top right corner, there is an inset video call window showing a person speaking or presenting. Above this window, there are several emojis including a question mark, a speech bubble, a thumbs-up, a heart, and another question mark.

At the bottom left corner of the slide, there is a logo consisting of two stylized human figures facing each other within a circle, colored light blue and dark blue. In the bottom right corner, the page number "page 028" is displayed.

In the subsequent frame, additional labels appear around the graph. On the left side of the graph, the term "quality measure" appears next to the y-axis label "BLEU," indicating that BLEU score is being used as a quality measure for translation performance. At the bottom center of the graph, the terms "AL/AL_CA (s)" and "latency measure" appear above the x-axis, clarifying that AL/AL_CA represents latency measurement.

The layout remains consistent throughout the frames described, maintaining focus on the presented results related to machine translation evaluation metrics.</sample>
    <sample id="825">The image is a screenshot of a presentation slide titled "Main Results: EDAtt." The main content includes a graph with the x-axis labeled "AL/AL_CA (s)" and the y-axis labeled "BLEU," indicating it's measuring latency against BLEU score. Below the graph, there's text that reads "(a) en-&gt;de" suggesting this data pertains to English to German translation results, followed by "latency measure."

In the top right corner, there are several emojis including a question mark, an eye, a speech bubble, a thumbs-up, a thumbs-down, and a smiley face. These likely represent different types of feedback or reactions.

At the bottom left corner, there's a logo consisting of two blue circles connected by lines forming a stylized equal sign. In the bottom right corner, the page number "page 030" indicates this is part of a larger document or presentation.

Additionally, in the upper right section of the image, there is a small video feed showing a person who appears to be presenting or participating remotely from their home environment.</sample>
    <sample id="826">The image shows a slide from a presentation with the title "Main Results: EDAtt" in blue text on the left side. The main content of the slide is a bar graph, which appears to be measuring BLEU scores against AL/AL_CA (s) for an English-to-German translation task (en-&gt;de). The x-axis ranges from 0.5 to 6 seconds, and the y-axis measures BLEU score values between approximately 21 and 27. There is a single vertical blue bar that reaches up to around 27 on the y-axis, indicating a high BLEU score at this point.

In the top right corner of the image, there is a small video feed showing a person who seems to be presenting or discussing the results shown in the slide. Above the graph, there are several emojis including a question mark, a thumbs-up, a smiley face, a surprised face, and another question mark, possibly representing different reactions or comments about the data presented.

At the bottom left corner of the slide, there is a logo consisting of three colored shapes forming a stylized letter 'E'. In the bottom right corner, it says "page 031", suggesting that this is part of a larger presentation document.</sample>
    <sample id="827">Main Results: EDAtt BLEU AL/AL_CA (s) 0.5 1.5 2.5 3.5 4.5 en→de page 032</sample>
    <sample id="828">The image is a screenshot of a presentation slide titled "Main Results: EDAtt." The main content includes a graph with the x-axis labeled "AL/AL_CA (s)" and the y-axis labeled "BLEU," showing data points for different models or methods, including wait-k, LA, CAAT, and EDAtt. There are annotations in blue text above the graph that read "popular strategies also applied to offline models" and "state of the art architecture specifically tailored for SimulST." In the top right corner, there is an inset video call window featuring a person speaking. At the bottom left, there is a logo consisting of three colored dots forming a triangle, and at the bottom right, it says "page 033."</sample>
    <sample id="829">The image is a screenshot of a presentation slide titled "Main Results: EDAtt." The main content of the slide includes a line graph with multiple data series plotted against two axes. The x-axis represents "AL/AL_CA (s)" ranging from 0 to 6, and the y-axis represents "BLEU" scores ranging from 17 to 27.

There are five different lines on the graph:
1. A yellow dashed line labeled "wait-k"
2. An orange solid line labeled "LA"
3. A blue dash-dot line labeled "CAAT"
4. Another orange solid line
5. A red dotted line labeled "EDAtt"

Each line shows how BLEU scores change as AL/AL_CA increases for different methods or models represented by these labels. At the top right corner of the slide, there are some symbols in Arabic script which seem unrelated to the chart itself.

In the bottom left corner, there's a logo that appears to be associated with the presentation software being used. In the bottom right corner, it says "page 035," indicating this is part of a larger document or presentation deck. Additionally, there is a small video feed window showing a person who seems to be presenting or discussing the content shown on the slide.</sample>
    <sample id="830">The image is a screenshot of a presentation slide titled "Main Results: EDAtt." The slide features a graph with the x-axis labeled "AL/AL_CA (s)" and the y-axis labeled "BLEU." There are four lines on the graph, each representing different strategies or models: wait-k (orange), LA (blue), CAAT (green), and EDAtt (red). The legend at the top right corner identifies these lines. At the bottom left of the graph, there is a blue text box that states, "EDAtt outperforms all the strategies applied to offline models." In the background, there is a small video feed in the upper right corner showing a person speaking. The slide number "035" is displayed in the bottom right corner.</sample>
    <sample id="831">The image is a screenshot of a presentation slide titled "Main Results: EDAtt." The main content includes a line graph with the x-axis labeled "AL/AL_CA (s)" and the y-axis labeled "BLEU." There are five lines on the graph, each representing different strategies or models: wait-k, LA, CAAT, and EDAtt. Each line has data points connected by solid lines in various colors such as orange, blue, red, green, and yellow.

Above the graph, there's text that reads "EDAtt is the fastest strategy if we consider the actual elapsed time," highlighted within a blue box for emphasis. Below this, it specifies "(a) en→de" indicating the translation direction from English to German.

In the top right corner of the slide, there is an inset video call window showing a person speaking into a microphone. 

At the bottom left corner of the slide, there is a logo consisting of three colored circles—blue, cyan, and black—with white symbols inside them. In the bottom right corner, the page number "page 037" is displayed against a light gray background.</sample>
    <sample id="832">The image is a screenshot of a presentation slide with various elements. The main content includes text and contact information, as well as a QR code for scanning. Here's the detailed description:

- **Text Content:**
  - At the top left corner, there is a question in blue font that reads "Do you want to discover more?"
  - Below this question, larger bolded blue text states "Read our paper to discover more results!"
  - Contact details are provided below:
    - Email addresses: {spapi,negri}@fbk.eu and marco.turchi@gmail.com
    - GitHub link: github.com/hlt-mt/fb-k-fairseq
    - Twitter handles: @fbk_mt and @sarapapi

- **Visual Elements:**
  - A large QR code is prominently displayed on the right side of the slide.
  - Above the QR code, smaller text says "Scan me!"
  - In the bottom left corner, there is an icon resembling a speech bubble or chat symbol.

- **Additional Information:**
  - There is a small inset video window showing a person speaking at the top right corner of the slide.
  - Page number "page 038" appears in the bottom right corner of the slide.

This layout suggests it might be part of an academic or professional presentation encouraging further engagement through reading their research paper or contacting them via the provided channels.</sample>
    <sample id="833">The authors of the paper are affiliated with Google. This is indicated by the presence of the Google logo on the presentation slide, which suggests that at least some or all of the authors work for this company. The affiliation to a specific academic institution isn't explicitly mentioned in the image provided.</sample>
    <sample id="834">The authors of the paper are affiliated with Stony Brook University. This is indicated by a logo at the bottom center of the slide that reads "Stony Brook University" and includes an image resembling a face in profile, which could be interpreted as representing language or communication. The affiliation to Stony Brook University suggests that these researchers are likely part of its academic community, possibly within departments related to computer science, linguistics, or artificial intelligence given the nature of their work on dissonance detection using machine learning techniques.</sample>
    <sample id="835">The paper analyzed language pairs English-French and English-Spanish.</sample>
    <sample id="836">The speaker's name is not provided in the image. The names shown are of individuals who may be part of a panel or presentation, but their roles as speakers are not specified.</sample>
    <sample id="837">The models investigated during the experiments were DEPLAIN-APA and DEPLAIN-WEB.</sample>
    <sample id="838">53 tasks are used for training and testing purposes.</sample>
    <sample id="839">The paper involves three authors: Regina Stodden, Omar Momen, and Laura Kallmeyer.</sample>
    <sample id="840">The authors experimented on the following datasets: AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="841">The image contains a title and several logos. The main text reads: "Language model acceptability judgements are not always robust to context" followed by "ACL 2023." Below this, there is additional text listing names: "Kousuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, Adi Williams."

On the right side of the image, there are three university logos:
1. John Hopkins University
2. Purdue University
3. MIT

Below these logos, there is another logo for Meta AI.</sample>
    <sample id="842">The image features a presentation slide with the title "Language model acceptability judgements are not always robust to context" prominently displayed in white text against a dark background. Below the main title, it states "ACL 2023," indicating that this is likely related to a conference or event from that year.

In the bottom left corner of the slide, there's additional information listing names: Kousuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Mira, Keren Fuentes, Roger Levy, and Adi Williams. This suggests these individuals may be authors or contributors associated with the content presented on the slide.

On the right side of the slide, logos for several institutions are visible: John Hopkins University, Purdue University, MIT, and Meta AI. These logos imply collaboration or sponsorship by these entities.

In the top right corner, there is an inset circular photo showing a person wearing glasses and a red shirt, although their face has been blurred out for privacy reasons. The presence of this individual’s photo might indicate they have some connection to the topic being discussed or one of the listed affiliations.</sample>
    <sample id="843">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs:

BLiMP | SyntaxGym | Crows
1. Many people were helping themselves. | 2. No customer ... has spent any money. | 1. Stereotypical Noun-Verb phrase.
2. Many people were helping herself. | 3. ... has spent any money. | 2. Nonsensical sentence.

P(1) ≫ P(2) | P(1.any) ≫ P(2.any) | P(1) &gt; P(2)</sample>
    <sample id="844">Revisiting Minimal Pair Paradigm</sample>
    <sample id="845">The image contains a slide titled "Revisiting Minimal Pair Paradigm." It discusses how minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of Language Models (LMs). The slide is divided into three columns: BLiMP, SyntaxGym, and Crows. Each column lists examples of sentences that illustrate different types of minimal pairs:

- **BLiMP**:
  1. Many people were helping.
  2. Many people were helps.
  - P(1) &gt; P(2)

- **SyntaxGym**:
  1. No customer ... has spent any money.
  2. ... has spent any money.
  - P(1) &gt; P(2) or P(1) &gt; P(2) or P(1) &gt; P(2)

- **Crows**:
  1. Stereotypical
  2. Nonsensical sentence.

In each example, the probability values indicate which sentence is more likely based on the model's predictions. Additionally, there is an inset photo of a person with dark hair wearing glasses and a red shirt in the top right corner of the slide.</sample>
    <sample id="846">The video presents a slide titled "Revisiting Minimal Pair Paradigm," which discusses the evaluation of language models (LMs) using relative differences in sequence probabilities to assess their abstract knowledge. The content is divided into three columns: BLiMP, SyntaxGym, and Crows.

- **BLiMP**: Provides examples such as:
  - Many people were helping themselves.
  - No customer ... has spent any money.
- **SyntaxGym**: Examples include:
  - Many people are helping herself.
  - No customer has spent any money.
- **Crows**: Lists:
  - Stereotypical
  - Nonsensical sentence

Each column includes sequences followed by P(1) &gt; P(2), indicating that these pairs have been evaluated for comparative probability analysis. A person's image appears on the right side of the slide.</sample>
    <sample id="847">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to estimate the abstract knowledge of LMs:

BLiMP
1. Many people were helping themselves.
2. No customer ... has spent any money.

P(1) ≫ P(2)

SyntaxGym
1. Many people were helping themselves.
2. No customer ... has spent any money.

P(1.any) ≫ P(2.any)

CrowS
1. Women are terrible at handwork.
2. Men are terrible at handwork.

P(1) ≫ P(2)

Are these judgements stable with long preceding context?</sample>
    <sample id="848">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to estimate the abstract knowledge of LMs:

BLiMP
1. Many people were helping themselves.
2. No customer ... has spent any money.

P(1) ≈ P(2)

SyntaxGym
P(1.any) ≈ P(2.any)

CrowS
Women are terrible at handiwork.
Men are terrible at hardwork.

P(1) ≈ P(2)

Are these judgements stable with long preceding context?</sample>
    <sample id="849">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to estimate the abstract knowledge of LMs:

BLiMP
1. Many people were helping themselves.
2. No customer ... has spent any money.
3. Many people were helping ...

SyntaxGym
P(1.any) &gt; P(2.any)

CrowS
Women are terrible at handiwork.
Men are terrible at handiwork.

P(1) &gt;= P(2)

Are these judgements stable with long preceding context?</sample>
    <sample id="850">The image shows a slide from a presentation with the title "Approach" at the top. The main content of the slide is about testing whether MPP (Maximum Precision Parsing) judgments are influenced by context length, structural match, and acceptability.

On the left side, there's an equation involving P(LM|P^fix) and P(LM), along with some text that reads: "Test whether MPP judgements as a function of context length, structural match, and acceptability." Below this, there's a diagram labeled "Space of Candidate Prefixes," which includes various symbols like a square, circle, triangle, and a combination symbol. There's also a note indicating "GPT2, OPT family - 125M to 6.7B."

In the center-right part of the slide, there's a section titled "Sample Suite: Subject-Verb Agreement" with two boxes labeled "Acceptable" and "Unacceptable." Inside these boxes, there are examples related to subject-verb agreement, such as:
- "Who might Jesse free those from before returning to this customer?"
- "What Jessica would just be noticing before returning to this customer?"

Next to each example, it says "Acceptable, Matched."

At the bottom right corner of the slide, there's a small label that reads "BLIMP, American Island."

Overall, the slide appears to discuss how different factors affect parsing decisions in natural language processing tasks using GPT models.</sample>
    <sample id="851">Approach Test whether MPP judgements function as a context length, structural match, and acceptability Space Suite: Subject-Verb Agreement P(LM | [Prefix]) &gt;? P(EM | [Prefix]) unacceptable acceptable sample Space of Candidate Prefixes Build Agreement Matched Unmatched * Who might Jesse lose from this customer before returning to this customer? * What did Jessica just say before noticing the missing museum? * Who might Jesse lose from this customer before returning to this customer? * What did Jessica just see before noticing the missing museum? * Who might Jesse lose from this customer before returning to this customer? Acceptable, Matched BLUMP, Island GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="852">The image shows a slide from a presentation with the title "Approach" at the top. The main content of the slide is about testing whether MPP (Maximum Precise Parsing) judgments are influenced by context length, structural match, and acceptability.

There's an equation in the center: \( P_{LM}(\text{[Prefix]} &gt; P_{LM}(\text{[Prefix]} \text{[Prefix]})) \), which seems to be related to some form of linguistic or computational analysis.

Below this equation, there's a section titled "Space of Candidate Prefixes," containing two diagrams labeled "Build" and "Accept." These diagrams have various symbols like "P," "Q," "R," etc., connected by arrows indicating relationships between them.

On the right side of the slide, under the heading "Sample," there are examples of sentences:
- "Who might Jessica see before noticing these highlights?"
- "What did Aaron sound like when he was singing those spots?"
- "Jessica would notice these highlights after seeing that she had cleaned the museum."

These examples seem to illustrate how different prefixes can affect the interpretation of the sentence.

At the bottom left corner, it states "GPT2, OPT family - 125M to 6.7B," likely referring to the range of parameters for models used in the study.

In the lower-right corner, there's text saying "BLUMP, Island," possibly referencing a specific dataset or location relevant to the research presented.

Overall, the slide appears to describe a methodological approach to analyzing language processing using machine learning techniques on large-scale datasets.</sample>
    <sample id="853">The image contains a slide titled "Approach" with the following content: 

- The main text reads, "Test whether MPP judgements as a function of context length, structural match, and acceptability."
- Below this is an equation involving \( P_{LM}(\text{[Prefix]}) \) compared to \( P_{LM}(\text{[Prefix]} \cdot \text{[Prefix)}) \), indicating some form of linguistic or probabilistic analysis.
- There's a section labeled "Space of Candidate Prefixes," which includes various symbols like \( \text{[Prefix]} \), \( \text{[Prefix]} \cdot \text{[Prefix]} \), etc., suggesting different combinations for analysis.
- A diagram shows two columns under headers "Build Version" and "Annotated," each containing rows that include terms such as "Context Length," "Structural Match," and "Acceptability." 
- On the right side, there are examples of sentences in speech bubbles discussing scenarios related to customer service interactions. One example says, "What would Jessica say before noticing these highlights?" Another asks, "What did Aaron sound like when he said those words?"
- At the bottom left corner, it states "GPT2, OPT family - 125M to 6.7B," likely referring to the models used in the approach.

The overall layout suggests a detailed explanation of how certain judgments (possibly by machine learning models) might be influenced by specific factors within textual data.</sample>
    <sample id="854">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability. GPT2, OPT family - 125M to 67.5M</sample>
    <sample id="855">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability GPT2, OPT family - 125M to 6.7B BLUMP, "Essential" Queries BLUMP | Adirondack Island</sample>
    <sample id="856">The image shows a slide from a presentation with the title "Approach" at the top. The main content of the slide is an explanation about testing whether MPP judgments are influenced by context length, structural match, and acceptability.

On the left side of the slide, there's a diagram labeled "Space of Candidate Prefixes." This diagram includes two sections: one for "Unacceptable" and another for "Acceptable," each containing examples of prefixes. Below this section, it reads "GPT2, OPT family - 125M to 6.7B."

In the center-right part of the slide, there's text that says:
- "Test whether MPP judgements as a function of context length, structural match, and acceptability"
- "P(LM | [Pre]fix) &gt; P(LM | Pre) sample"

Below this, there are questions related to judging sentences based on their acceptability or mismatched nature:

- "Who might lose fees from before returning to this documentary about music intiating?"
- "* There was such a documentary about music intiating."
- "Who might lose fees from before returning to this documentary about music intiating?"

To the right, there's a box indicating "Acceptable, Mismatched" which contains additional information about these examples.

At the bottom of the slide, in smaller text, it states:
- "BLUMP, Essential 'T' Quantifiers"
- "BLUMP | Adirondack Island"

Overall, the slide appears to be discussing a methodological approach to evaluating language processing models (like GPT-2) using specific criteria like context length, structural matches, and acceptability of generated sequences.</sample>
    <sample id="857">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability Wikipedia, Unrelated GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="858">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability Wikipedia, Unrelated</sample>
    <sample id="859">The image contains a slide titled "Approach" with the following content:

- The main text reads: "Test whether MPP judgements as a function of context length, structural match, and acceptability."
- Below this, there is an equation involving P(MP|LMP) and P(Prefix|Prefix), along with some sample data.
- There are two sections labeled "Wikipedia, Related" and "Wikipedia, Unrelated," each containing questions about customer fees based on prefixes.
- A diagram shows different types of space candidates (Mechanized vs. Non-Mechanized) and their relationships to various symbols like "Bullion," "Copper," etc.
- At the bottom, it states: "GPT2, OPT family - 125M to 6.7B."

The layout includes visual elements such as boxes, arrows, and color-coded areas to represent different concepts or categories within the approach being described.</sample>
    <sample id="860">MPP judgements are robust for arbitrary context lengths. We perform MPP evaluations with different contexts — acceptable / unacceptable; matched/mismatched structure — of lengths up to 900 tokens BLIMP, OPT 6.7B Prefix Strategy (Acc. (Matched) Acc. (Mismatched) Un Acc. (Matched) Un Acc. (Mismatched)) A rose is a woody perennial flowering plant of the genus Rosa in the family Rosaceae and has lots of thousands of cultivars. Is this sentence accepted or rejected by this customer? * A rose is a woody perennial flowering plant of the genus Rosa in the family Rosaceae and has lots of thousands of cultivars before running to?</sample>
    <sample id="861">The image contains a graph with the title "MPP judgements are robust for arbitrary context lengths" and a subtitle "1 context lengths". The main text states: "We perform MPP evaluations with different contexts — acceptable / unacceptable; matched/mismatched structure — of lengths up to 900 tokens."

The graph shows three lines representing different strategies:
- Acc (Matched)
- Acc (Mismatched)
- Unacc (Mismatched)

There is also a legend indicating that "BLIMP, OPT 6.7B" refers to the prefix strategy.

On the right side of the image, there is an explanation box with two bullet points:
1. "A rose is a woody perennial flowering plant of the genus Rosa, known for its showy flowers in springs and summers."
2. "* A rose is a woody perennial flowering plant of the genus Rosa, which has been cultivated by humans before running?"

The x-axis of the graph is labeled "Prefix Strategy," ranging from 0 to 500.</sample>
    <sample id="862">The image contains a title, subtitle, and text content related to evaluations of MPP (Most Probable Phrase) sentences. Here is the transcription:

Title: Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance

Subtitle: We perform MPP evaluations with different contexts – acceptable/unacceptable; matched/mismatched structure – of lengths up to 900 tokens

Body Text:
1. There was a documentary about music in the past.
2. There were lots of musicians working hard before this customer?
3. There was a documentary about music inflating
4. Who might Rise from these music before finding out?

BLIMP, OPT 6.7B

Legend:
- Green Line: Accepted MPP (Acc.)
- Red Line: Unaccepted MPP (Unacc.)
- Yellow Line: Wiki (Wiki)
- Pink Line: Unmatched (Unmacc.)

Note: The chart shows various lines representing different metrics over time or across categories labeled as "Prefix Strategy."</sample>
    <sample id="863">The image contains a slide with the title "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance." Below the title, there is text that reads: "We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens."

On the right side of the slide, there are two bullet points:
1. There was a documentary about music inflating.
2. Who might Rise from this before using the customer?

Below these bullet points, another line states: "Each sentence has one music inflating and each sentence has one Rise from this before using the customer?"

At the bottom left corner of the slide, it says "BLIMP, OPT 6.7B" followed by a graph labeled "Prefix Strategy." The graph shows various lines representing different strategies or conditions, including "Accepted (MPP)," "Unaccepted (MPP)," "Accepted (Unmatched)," "Unaccepted (Unmatched)," "Wiki," and "Unmatched (Wiki)." 

The x-axis of the graph ranges from 0 to 800, while the y-axis ranges from -0.2 to 0.2.

In the top right corner of the slide, there is an inset circular image of a person's face.</sample>
    <sample id="864">The image contains a slide from a presentation with the following text:

Title: Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance

Subtitle: We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens

Graph Legend:
- BLIMP, OPT 6.7B
- Prefix Strategy
- Unacc (MMismatched)
- Acc (MMismatched)
- Wiki (Mismatched)

Graph Annotations:
1. There was a documentary about music inflating.
2. There were most legislative...
3. There is each music inflating.

Each annotation corresponds to a point on the graph indicating specific data points related to the evaluation mentioned above.

In the bottom left corner, there is an equation or notation that reads "BLIMP, OPT 6.7B".

The overall content suggests an analysis of how sentence acceptability affects judgment performance across different contexts and structures within a certain token limit for machine parsing or natural language processing tasks.</sample>
    <sample id="865">The image contains a detailed graph and text discussing model performance in relation to MPP (Most Probabilistic Parsing) sentences. Here is the transcription of the English content:

Title:
- Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance

Subtitle:
- We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens

Graph Legend:
- BLIMP, OPT 6.7B
- Prefix Strategy
- Ac. (Acceptable)
- Unac. (Unacceptable)
- Wiki
- Unmatched

Graph Annotations:
1. What could Jessica sell before returning this customer?
2. What could Jessica have sold after seeing these spots?
3. What could Jessica be selling now for cleaning the museum?

Additional Text:
- Before returning this customer? 
- After seeing these spots?
- For cleaning the museum? 

The graph shows three lines representing different conditions: acceptable (Ac.), unacceptable (Unac.), and unmatched (Unmatched). The x-axis represents some form of prefix strategy evaluation, while the y-axis ranges from -0.2 to 0.2.

In the top right corner, there is an inset circular area containing additional text related to the questions asked about what Jessica might do or see before returning a customer, observing spots, or cleaning the museum.</sample>
    <sample id="866">The image contains a detailed graph and text discussing the impact of MPP (Most Probable Phrase) sentences on model performance. The main title reads: "Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance." Below this, there is an explanation stating: "We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens."

The graph shows various lines representing different strategies or conditions labeled as "BLIMP, OPT6.7B," "Prefix Strategy," "Unacc. (Wiki)," "Acc. (Wiki)," and "Unacc. (Unmatched)." Each line represents data points that likely correspond to different evaluation results.

On the right side of the graph, there are three numbered questions:
1. What could Jessica sell before noticing these spots?
2. Who had gone ahead? Was it Jessica who was cleaning the museum?
3. Could Jessica notice all these spots before returning this customer?

Each question has corresponding answers in smaller boxes below them, which appear to be generated by the models being evaluated based on their acceptability and context matching.

At the bottom left corner of the image, there's a label indicating "BLIMP, OPT6.7B," possibly referring to the specific version or configuration of the BLIMP system used for the evaluations shown in the graph.</sample>
    <sample id="867">The image contains a detailed graph and text related to model performance in natural language processing. Here is the transcription of the visible text, organized by structure:

**Title:**
- Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance

**Body Text:**
- We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens.

**Graph Labels:**
- X-axis: Prefix Strategy
- Y-axis (left): BLIMP, OPT 6.7B
- Y-axis (right): Unmatched (pink), Unmatched (blue)

**Legend:**
- 3 (green)
- 2 (red)
- 1 (purple)

**Questions on the Right Side:**
1. What could Jessica sell before noticing she had no money? (green line at position 3)
2. What could Jessica have sold after realizing her car was missing from the parking lot? (red line at position 2)
3. What could Jessica do if she noticed there were no more spots available for sale today? (purple line at position 1)
4. What could Jessica notice that there are no more spots left when returning this customer before noticing it's too late? (green line at position 3)
5. What could Jessica realize that there were no more spots left when selling this customer before noticing it's too late? (red line at position 2)
6. What could Jessica notice that there were no more spots left when returning this customer before noticing it's too late? (purple line at position 1)

**Additional Information:**
- The bottom right corner shows "BLIMP, OPT 6.7B" indicating the version or parameters used for these evaluations.
- There is an inset showing three numbered questions corresponding to the lines on the graph, each question associated with a specific color-coded strategy.

This structured approach provides insights into how different strategies impact model performance based on sentence acceptability and context within natural language processing tasks.</sample>
    <sample id="868">The text in the image reads: "Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. - Prefix/suffix adverbs: “However, &lt;sent&gt;.” - Long prefix adverbs: “First and foremost, &lt;sent&gt;.” - Add clause: “Regardless of what X thinks about it, &lt;sent&gt;.” - Quote: “Yesterday, X said, &lt;sent&gt;.”"</sample>
    <sample id="869">The image contains a slide from a presentation with the title "Why do matched prefixes affect LM judgements?" The main content of the slide discusses perturbing context sentences in ways that preserve their relevant structure and questioning how sensitive models are to these changes. It lists examples such as prefix/suffix adverbs ("However, &lt;sent&gt;"), long prefix adverbs ("First and foremost, &lt;sent&gt;"), add clauses ("Regardless of what X thinks about it, &lt;sent&gt;"), and quotes ("Yesterday, X said, &lt;sent&gt;"). Additionally, there is a chart on the right side titled "Perturbation" showing the impact of different types of perturbations (None, Prefix/suffix adv, Long prefix adv, Add clause, Clause, All) on accuracy across various input lengths for both acceptable and unacceptable sentences.</sample>
    <sample id="870">The image contains a slide with the title "Why do matched prefixes affect LM judgements?" and includes text, graphs, and a small circular photo of an individual. The main body of text explains that they perturb context sentences in ways that preserve the original structure to assess how models are sensitive to these changes.

The examples given include:
- Prefix/suffix adverbs: "However, &lt;sent&gt;."
- Long prefix adverbs: "First and foremost, &lt;sent&gt;."
- Add clause: "Regardless of what X thinks about it, &lt;sent&gt;."
- Quote: "Yesterday, X said, &lt;sent&gt;."

There is also a graph on the right side showing "Delta Accuracy" against "Input Length," which plots data points for different types of perturbations such as None, Prefix/Suffix adv, Add clause, All, and Unacceptable. Each type has its own line represented by various symbols (e.g., circles, squares). A legend indicates the color coding for each category within the graph.</sample>
    <sample id="871">The image contains a slide from a presentation with the title "Why do matched prefixes affect LM judgements?" The main content of the slide includes:

- A brief explanation: "We perturb context sentences in ways that preserve the structural integrity, and ask whether models are sensitive to these sentences."
- Examples of sentence perturbations:
  - Prefix/suffix adverbs: "However, &lt;sent&gt;"
  - Long adverbs: "First and foremost, &lt;sent&gt;"
  - Add clause: "Regardless of what X thinks about it, &lt;sent&gt;"
  - Quote: "Yesterday, X said, &lt;sent&gt;"
- A graph titled "Modeles are sensitive to perturbed sentences in" showing the relationship between Input Length (x-axis) and Δ Accuracy (y-axis). Different lines represent different types of perturbations such as None, Prefix/suffix adv, Long adv, Add clause, All, and Unacceptable.
- Legend for the graph indicating the type of perturbation and its corresponding line style.

The overall theme is an analysis of how language model judgments are affected by specific types of sentence perturbations.</sample>
    <sample id="872">The image contains a slide with the title "Why do matched prefixes affect LM judgements?" The main content of the slide is divided into two sections. On the left side, there are examples of sentences that illustrate how prefix/suffix adverbs and long adverbs can be affected by matched prefixes in language models (LM). These include: - Prefix/suffix adverbs: “However, &lt;sent&gt;” - Long adverbs: “First and foremost, &lt;sent&gt;” - Add clause: “Regardless of what X thinks about it, &lt;sent&gt;” - Quote: “Yesterday, X said, &lt;sent&gt;” At the bottom of this section, there's a statement saying, "Models are sensitive to perturbed sentences in these contexts." On the right side of the slide, there's a graph showing the relationship between Input Length on the x-axis and Δ Accuracy on the y-axis for different types of perturbations such as None, Prefix/suffix adv, Long adv, Add clause, All, and Unacceptable. Each type has its own line plotted on the graph. Additionally, there's an inset circle containing a photo of a person at the top-right corner of the slide.</sample>
    <sample id="873">The image contains a slide with the title "Key Takeaways" in bold text. Below the title, there are two bullet points:

- Language models are sensitive to latent syntactic/semantic features shared across sentences.
- MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.

To the right of the text, there is a graph showing some data trends over time or another variable labeled on the x-axis (from 0 to 600). The y-axis appears to measure something related to performance or accuracy, ranging from -0.2 to 0.2. There's also an equation involving P(M|F) and P(M|P), suggesting a probabilistic model context.

At the bottom left corner, there is a diagram titled "Space of Candidate Profiles," which includes various elements such as matrices and other mathematical symbols, indicating a complex analysis or modeling process. In the top right corner of the image, there is a circular inset containing blurred faces, likely for privacy reasons.</sample>
    <sample id="874">The image contains a slide titled "Key Takeaways" with the following points: 1. Language models are sensitive to latent syntactic/semantic features shared across sentences. 2. MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.

There is also a graph on the right side of the slide showing some data trends and an illustration at the bottom depicting the space of candidate profiles.</sample>
    <sample id="875">**Title: Key Takeaways**

- Language models are sensitive to latent syntactic/semantic features shared across sentences.
- MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.

[Graph showing performance metrics]

[Diagram illustrating the space of candidate profiles]</sample>
    <sample id="876">NACHOS is a language model designed for healthcare applications, as indicated by the slide titled "Summary" that lists its distribution alongside DrBERT.</sample>
    <sample id="877">The name of the speaker is George Foster.</sample>
    <sample id="878">The prompting strategy significantly impacts the results. The slide indicates that prompts have a big impact on translation quality, with most sentences (516 out of 1000) showing a difference of more than one BLEURT point. This suggests that even small changes in prompts can lead to substantial differences in translation accuracy and performance.</sample>
    <sample id="879">The authors of the paper are affiliated with Carnegie Mellon University Language Technologies Institute, Tecnico Lisboa (University of Lisbon), Berkeley Artificial Intelligence Research (BAIR) at UC Berkeley, and Unbabel.</sample>
    <sample id="880">The image contains text that mentions "5 expert-written instructions." This suggests there is a list or set of five detailed guidelines provided by experts, although the specific content of these instructions is not visible in the image. The presence of this information indicates an emphasis on authoritative and well-crafted guidance within whatever context this slide pertains to.</sample>
    <sample id="881">The authors propose to test the models on using information from multiple sources by conducting experiments with human study participants and coreference resolution models.</sample>
    <sample id="939">Common evaluation methods for dialogue systems include comparative and Likert rating evaluations. Comparative evaluation involves comparing the performance of different dialogue systems, as illustrated by the two groups of speech bubbles with robot icons representing conversations between humans and robots. The judge icon in the middle signifies that a comparison is being made to determine which system performs better.

Likert rating evaluation uses a scale from 1 to 5 (with 1 typically indicating "strongly disagree" or "not at all" and 5 indicating "strongly agree" or "very much") to assess user satisfaction or perceived quality of the dialogue system's responses. This method allows users to provide subjective feedback on their experience interacting with the system.</sample>
    <sample id="940">The paper involves five authors: Sebastian Senti, Jenny T. Liang, Ronan Le Bras, Katharina Reinecke, and Maarten Sap.</sample>
    <sample id="941">To answer the question about who was happy to relax after a long day at work, you need background knowledge that judges decide cases in law courts. This is because Servin's profession as a judge allows him to be relieved by relaxing after his duties for deciding court cases.</sample>
    <sample id="942">Yes, the code is available. It can be found on GitHub at the repository named "mpoems1l/kitmus".</sample>
    <sample id="943">The video does not provide information about the demographic balance of annotators for NLPositionality. It only shows a bar chart comparing social acceptability across different educational levels, with no mention of country or gender demographics.</sample>
    <sample id="944">The sentences were perturbed by adding "&lt;sent&gt;" to different parts of the sentence, such as prefix/suffix adverbs, long prefix adverbs, add clauses, and quotes.</sample>
    <sample id="945">To have a dimensional evaluation means to assess something based on multiple criteria or dimensions. In the context of this presentation, it involves evaluating dialogue quality through various aspects such as relevance, consistency, and emotional understanding.</sample>
    <sample id="946">The affiliations of the authors are: 1. University of Science and Technology of China, 2. Microsoft Research Asia, 3. Beijing Jiaotong University, 4. Sony AI, and 5. Microsoft STC Asia.</sample>
    <sample id="947">The form of the prompting is important in cases where it helps to identify and understand different types of information or entities mentioned. For example, when translating "Ski-Legenden unter sich" (Among ski legends), understanding that "Ski-Legenden" refers to a group of renowned skiers is crucial for accurate translation. Similarly, recognizing "Polizei" as referring to police officers aids in correctly interpreting the context of their custody during transport from jail. These examples highlight how specific forms within the prompts can provide essential clues for translators to make informed decisions about word choices and sentence structures in translations.</sample>
    <sample id="948">The image is a screenshot of a presentation slide titled "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge." The authors listed are Vasudha Varadarajan, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann, and H. Andrew Schwartz. At the bottom left corner, there's a logo with text that reads "Stony Brook University Human Language Analysis Beings," indicating the affiliation or origin of the research presented. In the top right corner, there is an inset showing a person who appears to be presenting the content.</sample>
    <sample id="949">The content in the image is a slide from an educational presentation. The title of the slide reads "What is Cognitive Dissonance?" Below the title, there is a definition provided: "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" attributed to Harmon-Jones and Harmon-Jones, 2007. At the bottom left corner of the slide, there is a citation for further reading or reference which states: "Eddie Harmon-Jones and Cindy Harmon-Jones, 2007. Cognitive dissonance theory after 50 years of development. Zeitschrift fur Psychologie, 38(1), 716." Additionally, on the right side of the slide, there is a small video feed showing a person who appears to be presenting this information.</sample>
    <sample id="950">The slide titled "What is Cognitive Dissonance?" defines it as “two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent” according to Harmon-Jones and Harmon-Jones from 2007. It includes an illustration showing a person's head with two statements: one in red text saying "I know that cigarettes could kill me," which represents the belief, and another in black text stating "I grabbed a couple smokes after the meeting today," representing the action. The term 'Dissonance' appears next to this scenario, indicating the cognitive dissonance experienced when these conflicting ideas coexist. At the bottom of the slide, there is a citation for Eddie Lee and Cindy Harmon-Jones, dated 2007, discussing cognitive dissonance theory post-adolescence development published in Zeitschrift für Psychologie.</sample>
    <sample id="951">The image shows a slide from a presentation about cognitive dissonance. The title of the slide is "What is Cognitive Dissonance?" and it defines cognitive dissonance as having two elements: thoughts, actions, beliefs that are inconsistent. It explains this concept by showing how language can express relationships between phrases or statements made by users.

There's an illustration with three sequences labeled seq 1, seq 2, and seq 3:
- Seq 1: "I know that cigarettes could kill me."
- Seq 2: "I grabbed a couple smokes after the meeting."
- Seq 3: "I don't think I could keep my job without them."

Each sequence has labels indicating 'belief,' 'action,' and 'belief' again for seq 3. There's also a section on the right side titled "Consonance/Dissonance" which categorizes these examples under either consonance (when there's no conflict) or dissonance (when there's a conflict).

At the bottom left corner, there's a citation for Eddie Harmon-Jones and Cindy Harmon-Jones from their work in 2007, discussing cognitive dissonance theory after fifty years.</sample>
    <sample id="952">The slide is titled "What is Cognitive Dissonance?" and it explains that cognitive dissonance involves two inconsistent elements of cognition, such as thoughts or actions. It states that this inconsistency can be expressed in language through the relationship between two phrases or statements made by a user. The text also notes that cognitive dissonance is relatively rare to find in language compared to other discourse relations.

Below the main content, there are three sequences labeled seq 1, seq 2, and seq 3:
- Seq 1: "I know that cigarettes could kill me."
- Seq 2: "I grabbed a cigarette smokes after the meeting today."
- Seq 3: "I don't think I could keep my job without them."

Each sequence includes labels for belief (belief), action (action), and consonance/dissonance (Consonance/Dissonance).

At the bottom of the slide, there is a citation for Eddie Harmon-Jones and Cindy Harmon-Jones from their work published in 2007 on cognitive dissonance theory, appearing in the Zeitschrift für Psychologie.</sample>
    <sample id="953">The video features a slide presentation on the topic of cognitive dissonance. The title "Why dissonance?" is prominently displayed at the top left corner, followed by an illustration depicting two stick figures engaged in a conversation with speech bubbles and text that reads "Effects of disagreement." Below this section, there is a citation: "Eddie Harmon-Jones and Judson Mills, 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive Researcher: A global theory in psychology."

On the right side of the slide, another box appears with the heading "Attitudes and Belief trends," accompanied by a simple line graph showing upward-trending bars.

In the upper right corner of each frame, there is a small inset image of a person who seems to be presenting or participating in the discussion.

Throughout the sequence, no new elements are introduced, maintaining consistency in the content presented across all frames.</sample>
    <sample id="954">The content in the image includes a title "Why dissonance?" and three sections with accompanying text. The first section is labeled "Effects of disagreement" and shows an illustration of two people having a conversation, one saying something and the other responding. Below this are references to studies by Eddie Harmon-Johnson &amp; Judson Mila (2019) on cognitive dissonance theory.

The second section has the heading "Attitudes and Belief trends" and features a bar graph showing an upward trend. This section also cites research from Matthew Matsuoka et al. (2018).

The third section titled "Anxiety disorders" contains an icon of a head with gears inside it, symbolizing thought processes or mental activity related to anxiety. There's no additional reference information provided for this part specifically mentioned in the description.

At the bottom, there's a citation indicating that these findings were presented at the 67th Annual Meeting of the Association for Computational Linguistics.</sample>
    <sample id="955">Why dissonance? Effects of disagreement Entry and Exit from Extremism Attitudes and Belief trends Anxiety disorders</sample>
    <sample id="956">The image contains text in English that appears to be part of a presentation or educational material. The main heading reads "Why dissonance?" Below this, there are several sections with titles and corresponding illustrations: 1. "Effects of disagreement" - accompanied by an illustration of two figures arguing. 2. "Cognitive Styles" - illustrated with a figure's head showing different thought processes. 3. "Entry and Exit from Extremism" - depicted with a person speaking to a group of people. 4. "Attitudes and Belief trends" - shown with bar graphs indicating changes over time. 5. "Anxiety disorders" - represented by a distressed face icon. At the bottom of the image, there is additional text providing references for further reading on cognitive dissonance theory and its implications.</sample>
    <sample id="957">The image shows a flowchart with three steps related to parsing and annotating text. The first step asks if the text is of good parsing quality, followed by questions about dissonance in two subsequent steps. Below the chart, there's an example tweet from a user that reads: "Wish I could hold grudges but I guess it’s a good thing that I can’t at the same time." This tweet has been parsed into categories such as "Dissonance," "Consonance," or "Neither" based on its content. At the bottom left corner, there's a Twitter logo indicating the source of the tweet, along with instructions for detailed annotation guidelines.</sample>
    <sample id="958">The image shows a flowchart with three steps: Step 1 asks if the passage is good, and it branches into Yes or No. If Yes, it leads to Step 2 asking about dissonance? Again, branching into Yes or No. The final step in the chart is Step 3 also regarding dissonance, which similarly splits into Yes or No paths.

Below this flowchart are percentages indicating -3.5%, -48%, and +48%. There's an example text from Twitter that reads: "Wish I could hold grudges but I guess it’s a good thing that I can’t at the same time." This tweet has been parsed by a user with the handle @user_handle. At the bottom of the slide, there's a note saying "*Check paper for detailed annotation guidelines."

On the right side of the image, there appears to be a small video call window showing someone speaking, although their face isn't clearly visible due to blurring.</sample>
    <sample id="959">The image shows a flowchart with three steps: Step 1 asks if the parsing quality is good, and based on the answer (Yes or No), it leads to either Step 2 asking about dissonance or directly to "Neither." If "Yes" in Step 1, there's an option for "Dissonance" at -3.5%, "Consonance" at -48%, and "Neither" at -48%. Below this chart, there's a tweet from Twitter by a user whose handle starts with "@user_". The tweet reads, "Wish I could hold grudges but I guess it’s a good thing that I can’t at the same time." At the bottom of the slide, there are options labeled "Annotate," "Dissonance," and "Dissociate." There's also a note saying "*Check paper for detailed annotation guidelines."</sample>
    <sample id="960">The content in the image is as follows: The title of the slide reads "Training on Initial Annotated Set." There is a graph with an x-axis labeled "Area under the ROC curve (AUC)" and a y-axis that appears to be unlabeled. On the left side, there's a red bar representing the initial dataset, which has an AUC value close to 0.5. To the right of this bar, there is text indicating "RoBERTa-base + classifier head" connected by lines pointing towards the train area. Additionally, there is a speech bubble containing the following information: "Small annotated dataset: 43/901 dissonance; not better than chance." In the top-right corner, there is a small video feed showing a person who seems to be presenting or explaining something related to the slide.</sample>
    <sample id="961">The image shows a slide from a presentation titled "Training on Initial Annotated Set." The main content of the slide includes:

- A graph with an x-axis labeled "Area under the ROC curve (AUC)" ranging from 0.5 to 0.65.
- On the left side, there is a red bar labeled "init dataset" at approximately 0.51.
- Above this bar, there is text that reads "RoBERTa-base + classifier head."
- Below the bar, it says "TRAIN."

To the right of the graph, there is a speech bubble containing text: "Small annotated dataset: 43/901 dissonance; not better than chance."

In the top-right corner of the slide, there is a small inset showing a person speaking.

The slide number "12" is visible in the bottom-right corner.</sample>
    <sample id="962">The content in the image is a flowchart titled "Method: Transfer and Active Learning for Annotating Rare Class." It illustrates a process starting with an initial model labeled as "Transfer Learning" at the top left. The chart includes several stages such as "Cumulative (CM)," "Iterative (IT)," and various steps like training, fine-tuning, adding new examples, annotating by humans, and updating models based on acquisition strategies to determine which items are best to label next.

Key points include:
- A comparison between difficult-to-annotate rare class annotation ("needle in a haystack") versus easier-to-annotate classes.
- The iterative nature of the process involving active learning loops where human annotators add new examples after each iteration.
- The goal is to increase the chance of correctly annotating rare classes through repeated cycles of model retraining and updates using newly annotated data.

The slide number 13 appears in the bottom right corner throughout the video.</sample>
    <sample id="963">Cold-start Annotations: Transfer Learning START Initial model: Transfer Learning Rare class annotation -- "needle in a haystack" Difficult to annotate Easy to annotate Increase chance of rare class? Acquisition strategy new examples which are 'best-to-label' Cumulative (CM) Model Retrain/Update Active Learning Iterative Humans annotate Add new examples 14</sample>
    <sample id="964">Cold-start Annotations: Transfer Learning RoBERTA-base + classifier head init dataset Debate CE Debate CE Area under the ROC curve (AUC) Transferred weights after training on combined Debate and CE data</sample>
    <sample id="965">The content in the image is a slide titled "Cold-start Annotations: Transfer Learning." It features a bar graph comparing different datasets labeled as "init dataset," "Debate," and "CE" with their respective areas under the ROC curve (AUC). The bars show that the initial dataset has an AUC of approximately 0.5, while both Debate and CE have higher AUCs at around 0.612 for Debate and 0.686 for CE. There are annotations indicating that transferred weights were applied after training on combined Debate and CE data. Additionally, there's a note explaining that "Debate" refers to annotated stance pairs from debate forums by Vasileiadis et al., and "CE" stands for Congressional Elections. At the bottom right corner, there is a citation for the source of this information.</sample>
    <sample id="966">The image is a slide from a presentation titled "Cold-start Annotations: Transfer Learning." It features a bar chart comparing the area under the ROC curve (AUC) for different datasets. The datasets compared are labeled as "init dataset," "Debate," and "CE." Each dataset has an associated AUC value, with "init dataset" having an AUC of approximately 0.52, "Debate" at around 0.61, and "CE" slightly lower than "Debate" but higher than "init dataset."

Above the bars, there's text that reads "RoBERTa-base + classifier head," indicating the model used in this analysis. To the right of the chart, there's a note stating "Transferred weights after training on combined Debate and CE data," suggesting that the performance improvements seen in the "Debate" and "CE" datasets were due to transferring learned weights.

At the bottom left corner of the slide, there's a citation related to the "Debate" dataset, providing details about its source:
- "Debate: RoBERTa instance in debate dataset [Vaswani et al., 2017]."
- "Pre-trained on: English Wikipedia."
- "Pre-trained on: English WebTexts."
- "Pre-trained on: English BookCorpus."
- "Pre-trained on: English Reddit."

Additionally, there's another citation at the bottom center of the slide referencing a conference paper by Vaswani et al. from 2017.
The slide number "15" is visible at the bottom right corner, indicating its position within the presentation sequence.</sample>
    <sample id="967">The image shows a presentation slide titled "Cold-start Annotations: Transfer Learning." The slide includes the following elements:

1. A bar chart comparing different methods under the category of "Area Under the ROC Curve (AUC)."
2. The bars represent various tasks or models, including "TRAIN," "init dataset," "$," "Debate," "CE," and "Debate-&gt;CE."
3. Each bar has an associated value indicating its performance:
   - "TRAIN" is not shown.
   - "init dataset" has a value of +0.17 in red.
   - "$" has a value of +0.12.
   - "Debate" has a value of +0.10.
   - "CE" has a value of +0.08.
   - "Debate-&gt;CE" has a value of +0.04.

4. There is a green bar at the bottom with a value of -0.17.
5. Text on the right side reads "Finetuning on each task consecutively."
6. At the top left corner, there is text that says "Roberta base + classifier head."

7. In the lower-left corner, there is a citation for a paper related to the content being presented.

8. The presenter's name appears as "Mehmet O. Kizilbash" in the upper-right corner of the slide.</sample>
    <sample id="968">The video presents a detailed slide titled "Active Learning: Cumulative vs Iterative Update," which discusses the differences between cumulative and iterative update strategies in active learning. The slide features two main diagrams side by side, each illustrating different approaches to annotating rare class examples.

On the left diagram, labeled "Cumulative (CM)," it shows a flow from initial model training through stages of updating with new data ("new data"), old data ("old data"), and iterations ("Iterative (IT)"). This process includes steps such as fine-tuning ("fine-tune") and re-training ("train") based on both new and old data inputs.

The right diagram represents an "Iterative" approach where the difficulty of annotating rare classes is depicted visually—showing that rare class annotation is like finding a needle in a haystack initially but becomes easier over time. It also highlights processes for acquiring strategy information ("Acquisition strategy: 'What do I want to know?'" and "New examples"), human annotation efforts, adding new examples, and continuous updates to the model throughout these iterations.

At the bottom center of the slide, there's a red dashed box containing text related to "Model Retain/Update." 

In the top-right corner of the frame, there is a small inset showing a person who appears to be presenting or explaining the content.</sample>
    <sample id="969">The video presents a bar chart titled "Active Learning: Cumulative vs Iterative Update." The y-axis is labeled "AUC" and ranges from 0.5 to 0.75, while the x-axis lists different methods or datasets: Random, Entropy, CoreSet, CAL, and PRC. Each method has two bars representing cumulative (blue) and iterative (yellow) AUC values.

The blue bars indicate that for all methods except Random, the cumulative approach yields higher AUC values compared to the iterative approach. Specifically:
- For Random, both cumulative and iterative approaches have similar low AUC values.
- For Entropy, the cumulative approach shows an AUC slightly above 0.65, whereas the iterative approach is around 0.62.
- In the CoreSet category, the cumulative approach again outperforms the iterative one with AUCs of approximately 0.68 and 0.63 respectively.
- For CAL, the cumulative AUC is about 0.64, and the iterative AUC is roughly 0.59.
- Lastly, in the PRC category, the cumulative AUC stands at nearly 0.68, while the iterative AUC is close to 0.61.

In the top right corner of each frame, there's a small image of a person who appears to be presenting this data.</sample>
    <sample id="970">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="971">The image is a slide from a presentation on "Active Learning: Probability-of-Rare-Class Strategy." The main content of the slide includes:

1. A flowchart at the top left, starting with "START" and progressing through steps like "Initial model," "Transfer Learning," "Cumulative (CM)," "Model Retrain/Update," to "Iterative."

2. In the center, there's an illustration comparing "Rare class annotation – 'needle in a haystack'" as difficult to annotate versus "Easier to annotate," indicating that rare classes are harder to label.

3. Below this comparison, it states "Increase chance of rare class?" suggesting a strategy to improve the likelihood of identifying rare classes.

4. To the right, there's another section titled "Acquisition strategy which are best to label?" This part outlines:
   - "New examples"
   - "Humans annotate"
   - "Add new examples"

5. At the bottom left, there's a diagram labeled "Cumulative (CM)" showing different stages such as "old data," "new data," "train," "test," and arrows pointing between these stages.

6. There's also a small text box within the central illustration stating "Model Retrain/Update" connected by arrows to "Cumulative (CM)" and other parts of the process.

7. Finally, the slide number "19" is visible at the bottom right corner.

This detailed breakdown provides insight into how active learning strategies can be employed to handle rare-class annotations more effectively.</sample>
    <sample id="972">The video presents a slide from a presentation on "Active Learning: Probability-of-Rare-Class Strategy." The main content of the slide is titled "Active Learning Strategy Comparison (AUCs)" and features a horizontal bar chart. This chart compares various active learning strategies based on their Area Under the Curve (AUC) values, which are measures of performance in classification tasks.

The bars represent different methods:
- A baseline method labeled "from scratch" with an AUC value of approximately 0.5.
- A transferred model strategy with an AUC value slightly above 0.67.
- Several other strategies including AL-Random, AL-Entropy, AL-ConSet, AL-CAL, AL-CAL, and AL-PRC (ours), each showing varying levels of improvement over the baseline or transferred models.

The specific AUC values for these strategies range as follows:
- AL-Random: Approximately 0.15
- AL-Entropy: Approximately 0.20
- AL-ConSet: Approximately 0.19
- AL-CAL: Approximately 0.19
- AL-PRC (ours): Approximately 0.21

The background color scheme includes shades of blue and red, enhancing readability. In the top right corner of the slide, there's a small image of a person who appears to be presenting this information. Additionally, at the bottom left corner of the slide, the number "21" indicates that this is likely part of a larger series of slides.</sample>
    <sample id="973">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="974">Active Learning: Probability-of-Rare-Class Strategy Active Learning Strategy Characteristics Rare % Time (s) Subj. diff. RANDOM 3.20 11.96 -0.065 ENTROPY 6.80 12.78 0.035 CORESET 6.00 11.89 0.039 CAL 6.00 11.88 0.049 PRC 7.60 13.55 0.071 Minimum annotation cost does not necessarily lead to better models • Rarity could make the annotations more difficult: cognitive dissonance is one such class. To increase dissonance samples, PRC works the best.</sample>
    <sample id="975">The content in the image includes a slide titled "Takeaways" with several sections. The first section has an illustration of a neural network and text that reads "Cold-start AL with transfer learning." Below this, there are two diagrams labeled "Out-of-domain: Iterative" and "In-domain: Cumulative," each showing sequences of models \( M_0 \), \( M_1 \), \( M_2 \), etc., indicating iterative updates for out-of-domain data and cumulative updates for in-domain data.

To the right, there is another diagram illustrating the concept of rare class annotation as a needle in a haystack, accompanied by the text "PRC is simple &amp; efficient for rare sample acquisition."

At the top right corner, there is a small video feed window showing a person speaking, identified as "Monica Vazquez (University of California)."</sample>
    <sample id="976">The content in the image is as follows: 1. "Takeaways" - This appears to be a heading or section title, likely summarizing key points from a presentation. 2. A visual metaphor with text reading "Rare class annotation 'needle in a haystack'" and an arrow pointing towards it, illustrating the difficulty of identifying rare classes in data. 3. Text stating "PRC is simple &amp; efficient for rare sample acquisition," suggesting that PRC (possibly a method or tool) is effective for obtaining rare samples efficiently. 4. Diagrams labeled "Cold-start AL with transfer learning," "Out-of-domain: Iterative," and "In-domain: Cumulative." These diagrams seem to represent different approaches or methods related to active learning and domain adaptation. The slide number "24" indicates this is part of a larger presentation.</sample>
    <sample id="977">The video begins with a slide titled 'Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge.' The title is displayed in blue text at the top of the white background. Below the title, there are three lines of contact information written in green text: 'Contact: vwaradarajan@cs.stonybrook.edu, sjuhng@cs.stonybrook.edu, has@cs.stonybrook.edu.' Beneath this section, there are two QR codes labeled 'Code:' and 'Dataset:', each accompanied by URLs (github.com/humanlab/trare-class-AL and github.com/humanlab/trare-class-bias). On the right side, another QR code is labeled 'Paper:', followed by an URL (arxiv.org/abs/2006.02349). A small image of a person appears in the upper right corner.

The scene transitions to a plain white screen displaying black text that reads 'Thank you!' This message indicates the conclusion of the presentation or lecture. In the lower right corner, the number 26 suggests it might be the last slide of the sequence. Additionally, a small image of a person remains visible in the upper right corner throughout these frames.

The final frame continues from the previous one, maintaining the same white background and centered black text reading 'Thank you'. The number 26 persists in the lower right corner, reinforcing its role as the concluding slide. The consistent presence of the small image of a person in the upper right corner ties all the slides together visually.</sample>
    <sample id="978">The authors evaluated the following dialog models: BART-FID-RAG, Blender2, Emora, and Blender-Decode.</sample>
    <sample id="979">The paper involves 10 authors. Their names are listed as Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie.</sample>
    <sample id="980">A good planner should be able to clearly articulate the steps needed for a specific task, such as making strawberry or chocolate cakes. They must also understand and manage various constraints that may affect the process of achieving an abstract goal, which in this case is related to baking these desserts.</sample>
    <sample id="981">There are eight authors involved in the paper. Their names, listed on a slide from an academic presentation or conference poster, include Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, and Deqing Yang.</sample>
    <sample id="982">The name of the speaker is Vasudha Varadarajan.</sample>
    <sample id="983">The affiliations of the authors are: Adam Prze/piorkowski and Michat Wozi/nak are affiliated with the Institute of Computer Science, Polish Academy of Sciences in Warsaw, Poland. MichaT Wozi/nak is also associated with the University of Warsaw.</sample>
    <sample id="984">Title: XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations

Authors: Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang

Affiliations:
- Penn State (logo)
- Amazon (logo)

Presenter's Name: Ethan Zhang</sample>
    <sample id="985">**Title: Semantic Parsing**

**Body Text:**
- **Semantic Parsing** is a task to build semantic representation of the user queries, such as SQL, and Lambda Calculus.

**Examples:**
1. Which countries in Europe have at least 3 car manufacturers?
   ```sql
   SELECT T1.country_name
   FROM countries AS T1 JOIN continents
   AS T2 ON T1.continent_id = T2.id
   JOIN car_makers AS T3 ON T2.id = T3.country
   WHERE T2.continent = 'Europe'
   GROUP BY T1.country_name
   HAVING COUNT(*) &gt;= 3
   ```

2. What players made less than three assists over a season?
   ```python
   call swListValue( call swGetProperty( lambda s ( call player( s ) ), var( s ) ), numericProperty( string( s ), num_assists( s ) ), numericEntity( string( s ), numericEntity( string( s ), player( s ) ) ), domain( s ) )
   ```

**Additional Information:**
User query and its corresponding SQL (left) vs. user query and its corresponding Lambda Calculus (right).

(Note: The image also contains a small section with text that appears to be part of an advertisement or additional information, but it's not clearly visible.)</sample>
    <sample id="986">**Title: Cross-lingual Semantic Parsing**

**Body Text:**
- "Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations"

**Diagram Description:**
- On the left side, there are three boxes labeled "English," "German," and "Chinese."
- An arrow points from these three boxes to another box labeled "Neural Models."
- From the "Neural Models" box, arrows point to three other boxes labeled "SQL," "Lambda," and "FunQL."

**Additional Information:**
- The slide number "3" is visible at the bottom right corner.</sample>
    <sample id="987">The content of the image is a presentation slide titled "Cross-lingual Semantic Parsing." The main text on the slide reads: "Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations."

Below this, there are three green boxes labeled "English," "German," and "Chinese" from left to right. An arrow points from these boxes to two blue boxes labeled "SQL" and "Lambda," with an additional box below them labeled "FunQL."

In the top-right corner of the slide, there is a small video feed showing a person's face.

At the bottom-left corner of the slide, there is navigation control indicating that it is page 3 out of a total of 5 pages (indicated by "3/5").</sample>
    <sample id="988">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="989">**Title: Cross-lingual Semantic Parsing**

**Body Text:**
Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance:
- Lack of coverage on certain natural language

**Diagram Description:**
- Three boxes labeled "English," "German," and "Chinese" (in green)
- An arrow pointing to a box labeled "Neural Models"
- Four blue boxes below the arrow, labeled "SQL," "Lambda," "FunQL"

**Additional Information:**
- Slide number 4 in bottom right corner
- A small image of a person with text "Ethan Zhou" at the top right corner</sample>
    <sample id="990">**Title: Cross-lingual Semantic Parsing**

**Body Text:**
- Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance:
  - Lack of coverage on certain meaning representation

**Diagram Description:**
- Three boxes labeled "English," "German," and "Chinese" connected by arrows to a box labeled "Neural Models."
- From the "Neural Models" box, three arrows point to separate boxes labeled "SQL," "Lambda," and "FunQL."

**Additional Element:**
- A small image in the top right corner showing a person with text below it that is not clearly readable.

**Page Number:**
- The number "5" at the bottom right corner indicates this is slide 5.</sample>
    <sample id="991">Title: Cross-lingual Semantic Parsing

Body Text:
- Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance:
  - Lack of coverage on certain meaning representation

Diagram Description:
- There is a diagram with three boxes labeled "English," "German," and "Chinese" at the bottom.
- An arrow points from these boxes to two other boxes labeled "SQL" and "Lambda."
- Another box labeled "FunQL" is positioned below "Lambda."

Footer:
- Page number "5" in the bottom right corner.

Image Description:
- In the top right corner, there is an image of a person presenting or speaking, but no additional context can be derived about this individual from the content provided.</sample>
    <sample id="992">Title: Cross-lingual Semantic Parsing

Body Text:
Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance:
- Lack of coverage on certain neural model

Diagram Description:
- Three green boxes labeled "English," "German," and "Chinese" pointing to a pink arrow labeled "Single Model."
- The pink arrow points to three blue boxes labeled "SQL," "Lambda," and "FunQL."

Footer:
Slide number 6 is displayed in the bottom right corner.

Additional Information:
There is an image of a person with text that reads "Karthik Prabhu" at the top right corner, likely indicating the presenter or author of the slide.</sample>
    <sample id="993">**Title:**
XSemPLR

**Body Text:**
We provide a unified dataset XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains:
- 9 datasets in various domains
- 5 semantic parsing tasks.
- 8 meaning representations
- 22 natural languages in 15 language families

**Diagram Description:**
The diagram shows the structure of XSemPLR with an "Encoder" connected to a central box labeled "XSemPLR," which is then connected to a "Decoder." Various examples are shown around this structure, including:
- What players made more than three assists over a season.
- We have Japanese restaurants in Ghent, right?
- Mozzarella is a type of cheese that comes from Italy.
- Verify if there's a flight from Paris to London on Friday.

**Examples Provided:**
- A-Calculus (call function with parameters)
- FunQL (SELECT count(*) FROM users WHERE age &gt; 30)
- SQL (SELECT count(*) FROM users WHERE age &gt; 30)
- Thing Talk (SELECT count(*) FROM users WHERE age &gt; 30)
- Intent Slot (INTENT-SLOT CATEGORY-NAME)
- Prolog (example code)

**Additional Information:**
There is also some text written in Arabic at the bottom left corner of the image.</sample>
    <sample id="994">XSemPLR

We provide a unified dataset XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains:
- 9 datasets in various domains
- 5 semantic parsing tasks.
- 8 meaning representations.
- 22 natural languages in 15 language families.

[Diagram]

XSemPLR
|--- Encoder
|       |--- Decoder
|           |--- SQL
|           |--- Yacc
|           |--- Prolog
|           |--- Thing Talk
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|           |--- Frame
|</sample>
    <sample id="995">**Title: Experiment Settings**

**Body Text:**
- We consider the six settings for training and evaluation.
  - **Translate-Test:** Use google translate API to translate source to the target language. Then use monolingual model to train and eval.

**Subtitles: Training and Inference**

**Training Diagram:**
- English → English Model → SQL

**Inference Diagram:**
- German → Translate API → English → English Model → SQL</sample>
    <sample id="996">Experiment Settings We consider the six settings for training and evaluation. • Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval. Training English English Model SQL Inference German Translate API English English Model SQL</sample>
    <sample id="997">**Experiment Settings**

We consider the six settings for training and evaluation.

- **Translate-Test:** Use google translate API to translate source to the target language. Then use monolingual model to train and eval.

**Training**
- English ↔ English Model → SQL

**Inference**
- German ↔ Translate API → English ↔ English Model → SQL</sample>
    <sample id="998">Experiment Settings We consider the six settings for training and evaluation. Monolingual Model: Source language is the same as target language, e.g., German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data.

Training
- German (Few-shot) -&gt; German Model -&gt; SQL

Inference
- German -&gt; German Model -&gt; SQL</sample>
    <sample id="999">**Title: Experiment Settings**

**Body Text:**
We consider the six settings for training and evaluation.

- **Monolingual Model:** Source language is the same as target language, e.g., German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data.

**Subtitles: Training and Inference**

**Training Diagram:**
- German (Few-shot) → German Model → SQL

**Inference Diagram:**
- German → German Model → SQL</sample>
    <sample id="1000">Experiment Settings

We consider the six settings for training and evaluation.
Monolingual Model: Source language is the same as target language, e.g., German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data.

Training
[Diagram showing "German (Few-shot)" pointing to "German Model" which then points to "SQL"]

Inference
[Diagram showing "German" pointing to "German Model" which then points to "SQL"]

[Image of a person in the top right corner]

9</sample>
    <sample id="1001">Experiment Settings We consider the six settings for training and evaluation. Multilingual Model: Train one multilingual model for all languages. Training German English Chinese Multilingual Model SQL Inference German Multilingual Model SQL 10</sample>
    <sample id="1002">Experiment Settings We consider the six settings for training and evaluation. Multilingual Model: Train one multilingual model for all languages. Training German English Chinese Multilingual Model SQL Inference German Multilingual Model SQL</sample>
    <sample id="1003">Experiment Settings We consider the six settings for training and evaluation. • Multilingual Model: Train one multilingual model for all languages. Training German English Chinese Multilingual Model SQL Inference German Multilingual Model SQL 10</sample>
    <sample id="1004">Experiment Settings We consider the six settings for training and evaluation. Cross-lingual Zero-shot/Few-shot transfer: Train on one source language and transfer to another language Training English Or English German Few-shot Multilingual Model SQL Inference German Multilingual Model SQL 11</sample>
    <sample id="1005">**Title: Experiment Settings**

**Body Text:**
We consider the six settings for training and evaluation.

- **Cross-lingual Zero-shot/Few-shot transfer:** Train on one source language and transfer to another language

**Training:**
- English
- Or
- German Few-shot

**Inference:**
- German
- Multilingual Model → SQL

**Additional Information:**
The slide is part of a presentation, as indicated by the navigation arrows at the bottom left corner. The number "11" in the bottom right corner suggests this is the 11th slide in the sequence.</sample>
    <sample id="1006">Analysis of Monolingual

We evaluate on two groups of models on **Monolingual Setting**

**EPTR**: Multilingual Pretrained Encoders with Pointer-based Decoders
- XLM-R + PTR, mBERT + PTR

**EPDEC**: Multilingual Pretrained Encoder-Decoder Models
- mBART, mT5

We found **EPDEC (mT5)** obtains the best performance on all datasets!

MATIS | MGeoQuery | MSniper | MSniper | MOveright | MCWQ | MCWQ2QA | MTOP | MCAverage | Nala
--- | --- | --- | --- | --- | --- | --- | --- | --- | ---
mBERT-PTR | 30.63 | 72.18 | 45.97 | 57.47 | 52.41 | 75.89 | 57.41 | 57.41 | 57.41
XLM-R + PTR | 31.41 | 71.41 | 45.97 | 59.10 | 60.10 | 80.36 | 60.10 | 60.10 | 60.10
mBART | 51.18 | 74.26 | 60.75 | 66.29 | 61.81 | 81.23 | 61.81 | 61.81 | 61.81
mT5 | 53.18 | 74.26 | 60.75 | 66.29 | 61.81 | 81.23 | 61.81 | 61.81 | 61.81</sample>
    <sample id="1007">Analysis of Monolingual

We evaluate on two groups of models on **Monolingual Setting**

- **EPTR**: Multilingual Pretrained Encoders with Pointer-based Detectors
  - XLM-R + PTR, mBERT + PTR

- **EPDec**: Multilingual Pretrained Encoder-Decoder Models
  - mBART, mT5

We found **EPDec (mT5)** obtains the best performance on all datasets!

| Model      | MATIS | MGeoQuery | MSniper | MSpiders | MOvernight | MCWQ | MCWqa2QA | MTOP    | MCAverage |
|------------|-------|-----------|---------|----------|------------|------|----------|---------|-----------|
| mBERT-PTR  | 30.63 | 72.18     | 41.34   | 57.47    | 53.67      | 52.41 | 59.51    | 75.41   | 58.76     |
| XLM-R-PTR  | 31.31 | 71.41     | 45.14   | 59.10    | 58.90      | 52.41 | 60.32    | 80.36   | 60.56     |
| mBART-PTR  | 31.15 | 74.26     | 46.29   | 59.10    | 59.60      | 66.29 | 61.32    | 81.83   | 61.29     |
| mT5         | 51.18 | 74.26     | 50.73   | 61.65    | 65.16      | 61.83 | 81.83    | 81.29   | 81.29     |

**Note:** The table above provides a comparison of different models' performances across various tasks and metrics in a monolingual setting.</sample>
    <sample id="1008">Analysis of Monolingual

We evaluate on two groups of models on **Monolingual Setting**

**EPTR**: Multilingual Pretrained Encoders with Pointer-based Decoders
- XLM-R + PTR, mBERT + PTR

**EDC**: Multilingual Pretrained Encoder-Decoder Models
- mBART, mT5

We found **Dec (mT5)** obtains the best performance on all datasets!

| MATIS | MGeoQuery | MGeoSpider | MSniper | MSniper-over | MCWQ | MCWQ-over | MSchema2QA | MSchema2QA-over | MTOP | MTOP-over | MAve | MAve-over |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| mBERT-PTR | 30.63 | 72.18 | 40.91 | 57.47 | 53.41 | 52.53 | 57.51 | 54.21 | 59.49 | 58.41 | 57.47 | 58.41 |
| mBART | 31.41 | 71.41 | 41.51 | 59.10 | 58.52 | 57.62 | 58.52 | 57.62 | 59.10 | 58.52 | 57.62 | 59.10 |
| mBART | 51.18 | 74.26 | 40.63 | 57.47 | 53.41 | 52.53 | 57.51 | 54.21 | 59.49 | 58.41 | 57.47 | 58.41 |
| mT5 | 51.18 | 74.26 | 40.63 | 57.47 | 53.41 | 52.53 | 57.51 | 54.21 | 59.49 | 58.41 | 57.47 | 58.41 |

**Note:** The table shows various metrics for different models evaluated across multiple tasks and over different settings.

**Highlighted Data:**
- mT5 has a score of 81.83 in one column.
- It also scores high in other columns as well, indicating its superior performance compared to other models listed.</sample>
    <sample id="1009">Analysis of Monolingual

We evaluate on two groups of models on **Monolingual Setting**

**EPTR**: Multilingual Pretrained Encoders with Pointer-based Decoders
- XLM-R + PTR, mBERT + PTR

**EPDec**: Multilingual Pretrained Encoder-Decoder Models
- mBART, mT5

We found **EPDec (mT5)** obtains the best performance on all datasets!

MATIS | MGeoQuery | MSniper | MSpiders | MOvernight | MCWQ | MCwisha2QA | MTOP | McAlena1 | Average
---|---|---|---|---|---|---|---|---|---
mBERT-PTR | 30.63 | 72.18 | 41.34 | 57.47 | 53.29 | 75.41 | 58.07 | 75.41 | 58.07
XLM-R + PTR | 31.41 | 71.41 | 45.57 | 59.10 | 58.07 | 80.36 | 60.76 | 80.36 | 60.76
mBART | 45.38 | 74.26 | 50.75 | 66.29 | 65.16 | 81.83 | 65.16 | 81.83 | 65.16
mT5 | 53.18 | 74.26 | 50.75 | 66.29 | 65.16 | 81.83 | 65.16 | 81.83 | 65.16

(Note: The table contains numerical values for each model's performance across different datasets.)</sample>
    <sample id="1010">Title: Analysis of Multilingual Training

Body Text:
- We evaluate on mT5 and XLM-R + PTR on Multilingual Setting
- Enc-Dec/Enc-PTR (mT5-XLM-R) can be improved by training in a mixture of various languages.

Table:
- The table is divided into two sections, labeled "Monolingual" and "Multilingual."
- Each section contains rows for different models or configurations such as mT5, mT5+PTR, MATIS, MGeoQuery, MSpider, NMaps, MOveright, MSqa2KMA, MTOP, and MTOP.
- For each model/configuration, there are numerical values listed under the columns corresponding to the aforementioned categories. These numbers represent performance metrics or scores.
- At the bottom of the table, an average score row is provided for comparison across all models/configurations within each category ("Average").

Note: Specific details about the content of the table cells cannot be described due to the resolution and size limitations of the image.</sample>
    <sample id="1011">**Title: Analysis of Multilingual Training**

- **Body Text:** We evaluate on mT5 and XLM-R + PTR on Multilingual Setting
  - Enc-Dec/Enc-PTR (mT5/XLM-R) can be improved by training in a mixture of various languages.

**Table:**

| Model                  | MATIS | MIGeoQuery | MSpider | NMaps | MOveright | MSchqa2018 | MTOP | Naive | Average |
|------------------------|-------|------------|---------|-------|-----------|-----------|------|-------|---------|
| Monolingual            |       |            |         |       |           |           |      |       |         |
| mT5                   | 31.31 | 71.41      | 48.37   | 85.17 | 59.10     | 66.29      | 80.36 | 81.63 | 52.09   |
| XLM-R + PTR           | 51.15 | 74.26      | 50.73   | 61.83 | 66.29     | 66.29      | 80.36 | 81.63 | 59.16   |
| mT5                   | 39.72 | 71.35      | 40.20   | 85.91 | 61.03     | 61.03      | 81.62 | 82.95 | 61.82   |

**Note:** The table compares the performance metrics across different models, including monolingual settings for mT5 and XLM-R + PTR with and without pre-training using a mixture of languages.</sample>
    <sample id="1012">Analysis of Multilingual Training

- We evaluate on mT5 and XLM-R + PTR on Multilingual Setting
  - Most of the major NLs can obtain performance gain, except that English performance drops in 7 datasets and gains in 3 datasets.
  - This is known as "Curse of Multilinguality"

[Graph showing number of datasets for different natural languages]</sample>
    <sample id="1013">Analysis of Multilingual Training

- We evaluate on mT5 and XLM-R + PTR on Multilingual Setting
  - Most of the major NLs can obtain performance gain, except that English performance drops in 7 datasets and gains in 3 datasets.
  - This is known as "Curse of Multilinguality"

[Bar chart showing number of datasets for various natural languages]</sample>
    <sample id="1014">Title: Cross-lingual Performance Gap

Body Text:
- Blue Line: Cross-lingual Few-shot transfer
- Orange Line: Cross-lingual Zero-shot transfer
- Green Line: Monolingual Setting

Chart Legend:
- Blue Line: Few-shot
- Red Line: Zero-shot
- Green Line: Monolingual

Chart Labels (Clockwise from top):
- MTOP
- Schema2QA
- Overnight
- NLMaps
- MCWQ
- ATIS
- Spider
- Geoquery
- Geoquery/sql
- Geoquery/funql
- Geoquery/prolog
- Geoquery/lamb

Slide Number: 15</sample>
    <sample id="1015">Cross-lingual Performance Gap Blue Line: Cross-lingual Few-shot transfer Orange Line: Cross-lingual Zero-shot transfer Green Line: Monolingual Setting MTOP Schema2QA Overnight NLMaps MCWQ Spider ATIS Geoquery/lambda Geoquery/SQL Geoquery/Prolog Geoquery/funql Geoquery/lamb Geoquery/spider</sample>
    <sample id="1016">The image contains a slide titled "Cross-lingual Performance Gap" with the following text:

- green - orange: For zero-shot setting, the cross-lingual transfer performance gap is significant
- blue - orange: For few-shot setting, the transfer gap is shortened rapidly

There is also a radar chart in the center of the slide labeled MTOP. The radar chart has several axes labeled as follows:
- Geoquery
- Geoquery/lamb
- Geoquery/prolog
- Geoquery/funql
- Geoquery/sql
- Spider
- Schema2QA
- Overnight
- NLMaps
- MCWQ
- ATIS

Each axis represents different datasets or tasks. There are three colored lines on the radar chart representing different conditions:
- Blue line for Few-shot
- Orange line for Monolingual

In the top right corner of the slide, there is an inset showing a person's face and name tag that reads "Kaiwen Zhou."</sample>
    <sample id="1017">The image contains a slide from a presentation with the title "Other Results &amp; Findings (Section 4 in Paper)" at the top. The content of the slide is as follows:

1. **Enc-Dec (mT5) outperforms previous work or achieves comparable results.**
2. Pretraining on the English NL can significantly boost the performance of few-shot on target NLs.
3. Multilingual LLMs (CodeX and Bloom) are still inadequate for cross-lingual semantic parsing tasks.
4. Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest.
5. FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.

In the bottom right corner, there is a small image of a person with text below it that reads "Karthik Narasimhan." At the very bottom center of the slide, the number "17" indicates this is likely the 17th slide in the presentation.</sample>
    <sample id="1018">**Title: Other Results &amp; Findings (Section 4 in Paper)**

- **Enc-Dec (mT5) outperforms previous work or achieves comparable results.**
- Pretraining on the EN Codex can significantly boost the performance of few-shot on target NLs.
- Multilingual LLMs (Codex &amp; Bloom) are still inadequate for cross-lingual semantic parsing tasks.
- Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest.
- FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.

[The image also includes a small logo or icon in the top right corner.]</sample>
    <sample id="1019">**Title: Conclusion**

- We build **XSemPLR**, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations.
  
- We conduct a comprehensive benchmark study on three representative types of **multilingual language models**.

- Our results show that **mT5 with monolingual training** yields the best performance, notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between **monolingual training and cross-lingual transfer learning** is still significant.</sample>
    <sample id="1020">**Title: Conclusion**

- We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations.

- We conduct a comprehensive benchmark study on three representative types of multilingual language models.

- Our results show that mT5 with monolingual training yields the best performance, notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual training and cross-lingual transfer learning is still significant.

**Subtitle: Links**

Welcome to visit our paper and code!

- Paper Link: [https://arxiv.org/pdf/2306.04085.pdf](https://arxiv.org/pdf/2306.04085.pdf)
- Code Link: [https://github.com/psunlgroup/xsemplr](https://github.com/psunlgroup/xsemplr)</sample>
    <sample id="1021">The most common errors of PaLM are "Accuracy/Omission" and "Style/Awkwad." These issues seem to be particularly prevalent in the performance of this language model.</sample>
    <sample id="1048">The authors of the paper are affiliated with Emory University and its NLP Research Lab, as indicated by their logos on the slide.</sample>
    <sample id="1049">In the context of this paper, "CFT" stands for continuous fine-tuning. This term is highlighted in a section titled "Our recommendations," where it suggests that models should always apply continuous fine-tuning (CFT).</sample>
    <sample id="1050">The paper involves seven authors. Their names are listed at the bottom of the slide: Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Miser, Kerem Fuentes, Roger Levy, and Adi Williams.</sample>
    <sample id="1084">The name of the speaker is Yusen Zhang.</sample>
    <sample id="1085">The image contains a title at the top that reads "#ACL2023." Below this, there is a subtitle in larger text stating: "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models."

Underneath the subtitle, four names are listed along with their corresponding photos:
1. Shangbin Feng
2. Chan Young Park
3. Yuhan Liu
4. Yulia Tsvetkov

At the bottom left corner of the image, there are logos for PAUL ALLEN SCHOOL W, UW NLP, and Carnegie Mellon University Language Technologies Institute.

On the right side of the image, there is an inset showing a person speaking into a microphone. The background of the main part of the image appears white or light-colored.

In the lower section of the image, there is another graph titled "LM Training Data" with the subtitle "A mixed blessing." This graph shows a bar chart comparing different sources based on their # tokens (big log). On the far right, there is a citation: "Dodge, Jesse et al. 'Documenting Large Web Corpora as Crowdsourced Corpus.' Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Montreal, Canada, July 6-13, 2021."</sample>
    <sample id="1086">Title: LM Training Data

Subtitle: A mixed blessing

[Graph showing the number of tokens (big log) from various sources]

Sources listed on the graph:
- patreon.com
- en.wikipedia.org
- www.theguardian.com
- www.nytimes.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com
- www.sciencedirect.com</sample>
    <sample id="1087">The image displays a slide titled "LM Training Data" with the subtitle "A mixed blessing." It features a bar graph comparing different sources of data, labeled from top to bottom as follows: "patents google.com," "wikipedia.org," "en.wikipedia.org," "nytimes.com," "theguardian.com," "theatlantic.com," "thechronicleherald.ca," "thehindu.com," "theguardian.com," "thehindu.com," "theguardian.com," "theguardian.com," "theguardian.com," "theguardian.com," "theguardian.com," "theguardian.com," "theguardian.com," "theguardian.com," "theguardian.com," "theguardian.com," and finally "theguardian.com."

To the right of the graph, there is a citation that reads: "Dodge, Jesse et al. 'Documenting Large Web Corpora: A Case Study on Crowled Corpus.' Proceedings of Methods in Natural Language Processing 2021."

In the upper right corner of the image, there is a small video feed showing a person's face.</sample>
    <sample id="1088">The image contains a slide titled "LM Training Data" with the subtitle "A mixed blessing." The main content is a bar chart that displays various sources of data along the y-axis, labeled from top to bottom as follows: "patents," "wikipedia," "en.wikipedia.org," "msmarco," "msmarco.org," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," "www.theguardian.com," and "www.theguardian.com."

The x-axis represents the number of tokens (in billions), ranging from 10^6 to 10^9. Each source has a corresponding horizontal blue bar indicating its contribution in terms of token count.

At the bottom right corner of the slide, there is a citation for the source of the data:
"Dodge, Jesse et al. 'Documenting Large Web Corpora: A Case Study on the Composition of Crawled Corpus.' Proceedings of the Conference on Human Language Technology, Volume 37(2). Proceedings of the International Joint Conference on Natural Language Processing 2021."

In the upper right corner of the image, there is a small inset showing a person's face, likely the presenter or someone related to the presentation.</sample>
    <sample id="1089">The image contains a slide titled "LM Training Data" with the subtitle "A mixed blessing." The main content of the slide is a bar graph that shows various sources listed on the y-axis and the number of tokens (in billions) on the x-axis. The bars represent different websites or data sources, indicating their contribution to language model training data in terms of token count.

In the bottom right corner, there is a citation for Dodge, Jesse et al., from the paper "Documenting Large Web Corpora: A Case Study Using Crawled Corpus," published in the Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP).

Additionally, there is an inset photo of a person located at the top right corner of the slide.</sample>
    <sample id="1090">The image contains a flowchart with three main stages labeled "Pretraining data," "Language models," and "Downstream tasks." Above the chart, there is text that reads "To this end." Additionally, on the right side of the frame, there is an inset showing a person. Below each stage in the flowchart, there are questions related to language models (LMs) and their political leanings: - Under "Pretraining data": "How to evaluate the political leaning of LMs? What role does pretraining data play in such political biases?" - Under "Language models": "How do LMs with different political leanings perform? Does LM political leaning result in fairness issues in NLP applications?"</sample>
    <sample id="1091">To this end Pretraining data Language models Downstream tasks How to evaluate the political leaning of LMs? How do LMs with different political leanings perform? Does LM political leaning result in fairness issues in NLP applications? What role does pretraining data play in such political biases?</sample>
    <sample id="1092">To this end, Pretraining data Language models Downstream tasks How to evaluate the political leaning of LMs? How do LMs with different political leanings perform? What role does pretraining data play in such political biases? Does LM political leaning result in fairness issues in NLP applications?</sample>
    <sample id="1093">The image contains a slide from a presentation titled "Evaluating LM Political Leaning." The content is structured as follows:

1. **Title:**
   - Evaluating LM Political Leaning

2. **Support both encoder and decoder LMs**
   - &lt;statement&gt; I mask with this statement.
   - Do you agree or disagree with this statement? &lt;statement&gt;

3. **Automatic eval**

4. **Grounded in polisci lit**

5. **Diagram:**
   - A diagram labeled "Political Compass Test" shows two characters, one agreeing ("Agree") and the other disagreeing ("Disagree").
   - Below the characters, there are labels for different political orientations:
     - Left
     - Right
     - Libertarian
     - Authoritarian

6. **Existing LMs (Language Models):**
   - A scatter plot graph showing various language models plotted on axes representing "Authoritarian" to "Libertarian" and "Socialist" to "Capitalist."
   - Labels around the graph include BERT-base, BERT-large, RoBERTa-base, distilRoBERTa, ALBERT-base, ALBERT-large, BART-base, BART-large, Alpaca, Codex, LLAMA, GPT-2, GPT-3-ada, GPT-3-babbage, GPT-3-curie, ChatGPT, GPT-4, and GPT-J.

This detailed breakdown provides an overview of how different language models align politically according to their positions on the given spectrum.</sample>
    <sample id="1094">The image displays a chart titled "Existing LMs" with two axes labeled 'Authoritarian' on the vertical axis and 'Libertarian' on the horizontal axis. The chart is divided into four quadrants: top-left (Authoritarian), bottom-left, top-right, and bottom-right.

Various language models are plotted within these quadrants:
- Top-left quadrant includes BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, distilRoBERTa, ALBERT-base, ALBERT-large.
- Bottom-left quadrant contains BART-base, BART-large, Alpaca.
- Top-right quadrant features Codex, LLAMA, GPT-2, GPT-3-ada, GPT-3-babbage, GPT-3-curie, ChatGPT, GPT-4.
- Bottom-right quadrant has no specific labels but shows points for economic axis and right.

Each model's position indicates its level of authoritarianism and libertarian values according to the chart.</sample>
    <sample id="1095">The image contains a detailed chart titled "Existing LMs" with various labels and points. The chart is divided into four quadrants labeled as follows: the top-left quadrant (red) for Authoritarian, the bottom-left quadrant (green) for Left, the bottom-right quadrant (blue) for Right, and the top-right quadrant (purple) for Libertarian.

In the red quadrant:
- BERT-base
- BERT-large
- RoBERTa-base
- RoBERTa-large
- distilRoBERTa
- ALBERT-base

In the green quadrant:
- BART-base
- BART-large
- Alpaca

In the blue quadrant:
- Codex
- LLAMA
- GPT-2
- GPT-3-ada
- GPT-3-babbage
- GPT-3-curie
- ChatGPT
- GPT-4
- GPT-J

There are also arrows connecting these points to other parts of the chart indicating relationships or influences between them.

Below this main chart, there's another section titled "Pretraining Data." It includes two diagrams side by side:

1. On the left under News Media:
   - left
   - center
   - right

2. On the right under Social Media (Reddit):
   - left
   - center
   - right

At the bottom of the image, there are references to sources:
- Liu, Yajun, et al., of "POLITICS: Pretraining LM with Semantyic Story," in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing.
- Shen, Giroux, and Carolyn Rose, “What sounds 'right' to me? Exploring factors in the perception of political leaning through pretraining language models,” Transactions of the Association for Computational Linguistics Volume 2022.

These elements together suggest an analysis of different large language models (LLMs) based on their training data and potential biases related to political leanings.</sample>
    <sample id="1096">The image contains a slide from a presentation with the title "Pretraining Data" at the top. The main content of the slide is divided into two sections, each representing different types of media: News Media and Social Media (Reddit).

Under the heading "News Media," there are three colored boxes labeled "left," "center," and "right." Similarly, under the heading "Social Media (Reddit)," there are also three colored boxes labeled "left," "center," and "right."

Below these sections, there are references to academic works:
- On the left side, it mentions "Liu, Yajun, et al., 'Politics: Pretraining with Story-based Question Answering,' Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing Detection,' Findings of the Association for Computational Linguistics."
- On the right side, it cites "Shen, Gaochen, and Carolyn Rose, 'What sounds 'right' to me? Experimental factors in the perception of political memes on Reddit,' Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics Volume 2021."

In the bottom right corner of the image, there is additional text that reads: "Shen, Gaochen, and Carolyn Rose. What sounds 'right' to me? Experimental factors in the perception of political memes on Reddit, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics Volume 2021."

The overall theme of the slide appears to be related to evaluating how pretraining language models like RoBERTa or GPT-2 handle political leaning information across different media platforms.</sample>
    <sample id="1097">The image contains a slide from a presentation with the title "Pretraining Data" at the top. Below this, there is text that reads: "Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning."

On the left side of the slide, under the heading "News Media," there are three colored boxes labeled:
- The top box is blue and labeled "left."
- The middle box is gray and labeled "center."
- The bottom box is red and labeled "right."

On the right side of the slide, under the heading "Social Media (Reddit)," there are also three colored boxes labeled similarly to those on the left side:
- The top box is blue and labeled "left."
- The middle box is gray and labeled "center."
- The bottom box is red and labeled "right."

At the bottom left corner of the slide, there is a citation for a paper titled "Politics: Pretraining with Semistory Detection." At the bottom right corner, another citation appears for a paper by Shen, Girshick, and Carolyn Rose titled "What sounds 'right' to you? Measuring the impact factors in the perception of political leanings using Reddit data." This paper was published in the Journal of the Association for Computational Linguistics, Volume 2021.

In the upper right corner of the slide, there is an inset showing four smaller images arranged in a grid pattern. Each small image has different colors corresponding to the labels mentioned above ("left," "center," "right").</sample>
    <sample id="1098">Results Partisan shifts in LM political leaning</sample>
    <sample id="1099">Results Partisan shifts in LM political leaning</sample>
    <sample id="1100">Title: Results

Subtitle: Partisan shifts in LM political leaning

[The image contains a grid with various colored cells and arrows indicating directions. The labels include "original," "news," "reddit" on the left side, and "news center," "reddit right" on the top side.]

Title: The Trump Card

Subtitle: Pre-45th to post-45th shift

[The image shows a 3x3 grid of smaller grids, each labeled with different categories such as "news left," "news center," "news right," "reddit left," etc., with directional arrows inside them. Each small grid has a numerical value labeled as Δ.]

[The overall layout is structured for presenting data related to partisan shifts in language models (LM) political leaning before and after an event referred to as "45th."]</sample>
    <sample id="1101">The image contains a title and several charts with text. Here is the transcription of the visible English text, structured as it appears in the image:

Title:
- The Trump Card

Subtitle:
- Pre-45th to post-45th shift

Charts (from left to right, top to bottom):

1. Chart labeled "news left"
   - Delta (Δ) = (-275.124)
   - GPT-2

2. Chart labeled "news center"
   - Delta (Δ) = (-0.131.103)
   - GPT-2

3. Chart labeled "news right"
   - Delta (Δ) = (163.103)
   - GPT-2

4. Chart labeled "reddit left"
   - Delta (Δ) = (0.75-36.364)
   - GPT-2

5. Chart labeled "reddit center"
   - Delta (Δ) = (0.50-36.364)
   - GPT-2

6. Chart labeled "reddit right"
   - Delta (Δ) = (-1.759.92)
   - GPT-2

Each chart has an axis indicating direction or movement, but no further textual details are provided for these axes.

In addition to this information, there is a small inset window on the upper right side showing a person's face, presumably related to the content being presented or discussed in the context of the slide.</sample>
    <sample id="1102">The image displays a slide titled "The Trump Card" with the subtitle "Pre-45th to post-45th shift." It contains eight charts arranged in two rows and four columns, each labeled as follows: news left, news center, news right, reddit center, reddit left, and reddit right. Each chart includes numerical data such as Δ values (e.g., Δ= (275.124), Δ= (237.051)) and other statistical figures like ΣF (e.g., ΣF= (237.051), ΣF= (237.051)). The charts also feature colored sections—red on the top half and blue on the bottom half—with arrows indicating shifts or changes between categories. In the upper right corner of the image, there is a small video feed showing an individual presenting the content.</sample>
    <sample id="1103">The image features a slide titled "The Trump Card" with the subtitle "Pre-45th to post-45th shift." The content is divided into two main sections: on the left, there are four graphs labeled "news left," "news center," and "news right," each showing data points connected by lines. On the right, there are three graphs labeled "reddit center" and "reddit right," also displaying similar data point connections.

Each graph contains numerical values indicating shifts or changes:
- "news left": Δ = (275, 124), Δ = (-237, -051)
- "news center": Δ = (-013, -103), Δ = (-121, -068)
- "news right": Δ = (163, 103), Δ = (213, 006)
- "reddit center": Δ = (75, 364), Δ = (50, -364)
- "reddit right": Δ = (-175, 902), Δ = (-175, 000)

In the top-right corner of the image, there is a small inset video call window showing a person in front of a background that includes red, blue, green, and white colors.</sample>
    <sample id="1104">The image shows a slide titled "Per-Category Performance" with various categories listed such as Hate Speech, BLACK, MUSLIM, LGBTQ+, JEWS, ASIAN, LATINX, WOMEN, CHRISTIAN, and WHITE. Each category has performance metrics for different sources like REDDIT, NEWS_RIGHT, NEWS_LEFT, etc., displayed in colored cells indicating the level of performance.

Below this table is another title: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources." It explains that dark yellow denotes best and blue denotes worst results.

In the top right corner of the image, there's a small inset showing three rankings labeled '1st,' '2nd,' and '3rd/4th.' The main content focuses on presenting data related to how well different entities perform against specific hate speech targets across multiple news sources.</sample>
    <sample id="1105">The image contains a detailed table titled "Per-Category Performance" with various categories listed on the left and performance metrics in columns. The rows represent different sources such as Reddit, News Right, and others. Each cell within the table is color-coded to indicate performance levels: dark yellow for best (best) and blue for worst (worst). At the bottom of the table, there's an explanatory note stating that this data shows performance on hate speech targeting identity groups and misinformation from these sources.

Additionally, at the top right corner of the image, there are three colored labels stacked vertically labeled 1st, 2nd, and 3rd, indicating some form of ranking or categorization related to the content displayed in the main part of the image.</sample>
    <sample id="1106">The image contains a table titled "Per-Category Performance" with various categories such as Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, and White. Each category has performance metrics for different sources like Reddit, News Right, CNN, NYT (Left), CNN (Right), Fox, Breitbart (Right), Wall Street Journal (Right), Washington Post (Center), New York Times (Center), and National Review (Right). The values are color-coded to indicate the level of performance: dark yellow denotes best and blue denotes worst.

Below the table, there is a caption that reads:
"Table 4: Performance on hate speech targeting identity groups and misinformation from different sources.
The results are color-coded so that dark yellow denotes best and blue denotes worst."

In the top right corner of the image, there is a small section labeled "1st," followed by three colored squares representing first place in red, second place in green, and third place in blue.</sample>
    <sample id="1107">The image contains a detailed table titled "Per-Category Performance" with various metrics related to hate speech and misinformation targeting different identity groups. The categories listed include Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, Men, White.

The performance is measured across several sources: Reddit, News Right, News Left, NYT (New York Times), CNN, LTT (likely Larry the Cable Guy's channel), NLT (New Living Translation of Bible), Guard (possibly referring to a news outlet or group), Fox, BBRT (Bible Broadcast Radio Today), WAT (Western Audio Theater), NR (National Review).

The data shows numerical values for each category and source, indicating some form of measurement like accuracy or effectiveness in addressing these topics. Dark yellow cells denote best performances, while dark blue denotes worst performances according to the caption at the bottom:

"Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources.
Data are color-coded such that dark yellow denotes best and dark blue denotes worst."

Additionally, there is text outside the main content area that reads:
"1st
2nd
3rd
4th"

This suggests rankings or positions but does not provide further context within the visible portion of the image.</sample>
    <sample id="1108">Per-Category Performance 1st, 2nd, 3rd, 4th Hate Speech BLACK MUSLIM LGBTQ+ JEWS ASAIN LATINX WOMEN CHRISTIAN WHITE REDDIT 89.84 89.90 89.56 89.70 89.06 87.65 87.10 NEWS_RIGHT 88.81 89.01 89.01 89.01 90.62 87.10 86.25 NEWS_LEFT 88.81 89.01 89.01 89.01 90.62 87.10 86.25 HP(L) NYYT LGL CNN(L) HLG(F) GUARD(L) FOX(R) BBART(W) NR(R) MISINFORMATION 88.57 88.54 88.54 88.54 88.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.</sample>
    <sample id="1109">The image contains a table titled "Per-Category Performance" with various categories listed on the left side, such as Hate Speech, BLACK, MUSLIM, LGBTQ+, JEWS, ASIAN, LATINX, WOMEN, CHRISTIAN, and WHITE. The rows are color-coded to indicate performance levels: dark yellow denotes best (dark blue), and white denotes worst (light blue). Each category has corresponding scores for different sources like Reddit, News Right, News Left, etc., which range from 87.54 to 90.62.

Below the table is a caption that reads: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded: dark yellow denotes best and dark blue denotes worst."

In the top right corner of the image, there's a small box showing three rankings labeled "1st," "2nd," and "3rd/4th."</sample>
    <sample id="1110">The image contains a table titled "Per-Category Performance" with various columns representing different identity groups and sources of misinformation. The rows are labeled by categories such as Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, Men, White, News_LT (News Limited), News_RI (News Republic), and Misinformation (with subcategories like HP (Hindustan Times), NYT (New York Times), CNN (CNN), etc.). Each cell in the table displays numerical values that represent performance metrics for each category and source.

At the bottom of the table, there is a note explaining: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded; such dark yellow denotes best and blue worst denotes."

In the top right corner of the image, there is text indicating rankings or positions: "1st", "2nd", "3rd", and "4th".</sample>
    <sample id="1111">The image shows a slide from a presentation titled "Qualitative Analysis." The content of the slide includes two tables. 

The first table has columns labeled: Text, Target Label, Base, N-S-L, S-N, and R-S-R. It contains rows with text examples such as:
- "the alt right is corrupted with people supporting racism with Asians and homosexuals it times for a new nation"
- "What do McDonald's and priest have in common? They"
- "(...) did not stop trump from setting increases in isolated cases to make a case on the grounds that the country was in the throes of a crime because they know he will not stop the slaughter going on"

Each row also lists corresponding labels like ASIAN, TRUE, TRUE, FALSE, etc.

The second table at the bottom describes an example of downstream performance tasks using language models with varying political bias. It mentions entities like Christians, vanilla RoBERTa's model, social media, left-leaning, and right-leaning, respectively.

In the top-right corner of the image, there is a small video feed showing someone presenting or participating remotely.</sample>
    <sample id="1112">The image contains a detailed table titled 'Qualitative Analysis' which is divided into two main sections: the left section labeled 'Text,' and the right section containing multiple columns with headers such as 'Target Label,' 'Base,' 'N-S-L,' 'N-S-R,' etc. The text in this section appears to be qualitative examples of statements or sentences, each associated with different labels indicating their classification by language models.

Below the tables, there's an explanatory note that reads:
"Table 5: Examples of the downstream performance of tasks using language models with varying political bias."

The content suggests a study on how language models process texts with differing political biases, likely analyzing the impact of these biases on model outputs across various categories like hate speech detection (as indicated by terms like 'Hate' at the top).

In summary, the image presents a structured analysis of language model performance related to political bias, supported by specific textual examples for illustration purposes.</sample>
    <sample id="1113">The image contains a table with two main columns: "Hate Speech Text" and "Test Results." The "Hate Speech Text" column lists various statements, while the "Test Results" column has sub-columns labeled N4, R-N, NR, and RR. Each row in the test results section shows whether each statement was identified as hate speech or not by different criteria.

At the bottom of the table, there is a caption that reads: "Table 12: Qualitative analysis of hate speech example where TMs with different political leanings beg to differ."

In addition to the table, there are images of individuals placed at the top right corner of both pages shown in the image. These images appear to be part of an overlay on the document rather than being directly related to the content of the table itself.</sample>
    <sample id="1114">The image contains a detailed table that appears to be analyzing examples of hate speech. The left side is labeled "Hate Speech Text" and the right side as "Transcription Test." Both sides have columns with headers: Date, N4, NR, R, RR.

The text in both sections seems repetitive, mentioning phrases like "Facebook," "rape," "kill her," "rape," "kill her again," etc., which are likely instances of hate speech being analyzed or transcribed for study purposes. 

At the bottom of the table on the left side, there's a caption that reads: "Table 12: Qualitative analysis of hate speech example where TMs political leaning has big step to differ."

Additionally, there is an inset photo at the top-right corner showing a person who might be related to the context of the document but their role isn't specified within the visible content.</sample>
    <sample id="1115">The image contains a table with multiple columns and rows. The first column is titled "Hate Speech Text," which lists various statements or phrases that could be considered hate speech, such as: "Some whites are just not comfortable around blacks because it's simply an innate aversion to anything they perceive to be different." Other entries include references to Facebook comments about race, opinions on racial issues in America, and comparisons of political parties.

The second column has headers labeled "Date," but the specific dates for each entry are not visible due to blurring. Following this, there are several other columns labeled "N4," "N5," "NR," "R," and "RR" (with some cells containing text like "TRUE" and others left blank).

The third section of the table includes repeated blocks of text under headings such as "Test Speech Text," followed by similar labels as before ("Date," "N4," etc.), indicating another set of data related to test speeches or evaluations.

At the bottom of the image, there is a caption that reads: "Table 12: Qualitative analysis of hate speech example where MDs have different political leanings beg to differ."

In the top right corner of the image, there is a small inset showing part of a person's face, likely from a video call interface, suggesting that someone might be presenting or discussing the content shown in the main part of the image.</sample>
    <sample id="1116">The image contains a detailed table labeled "Table 12: Qualitative analysis of hate speech example where TMs with different political leanings beg to differ." The table is divided into two main sections, each containing multiple columns and rows. 

**Left Section (Hate Speech Text):**
- Columns are labeled as follows:
  - Haste
  - N4
  - N-R
  - NR
  - RR

- Rows contain text examples that appear to be hate speech or controversial statements.

**Right Section (Textual Analysis):**
- Columns are similarly labeled but include additional labels like:
  - Text
  - True
  - False
  - False
  - False
  
- Each row corresponds to the left section's entries, providing an analysis or judgment on whether the content in the corresponding cell from the left section meets certain criteria.

**Additional Slide Content:**
- A slide titled "Discussion" appears below the tables.
- Subtitle reads: "Between Scylla and Charybdis."
- Main point states: "To 'sanitize' or not to 'sanitize', that is the question."

**Visual Elements:**
- There is a small inset picture of a person at the top right corner of both slides.

This layout suggests a structured approach to analyzing and categorizing hate speech based on specific criteria across different contexts or models.</sample>
    <sample id="1117">The image shows a slide from a presentation with the title "Discussion" at the top. The main content of the slide is about balancing between two options, referred to as Scylla and Charybdis, in relation to sanitizing data or not. It poses the question: "To 'sanitize' or not to 'sanitize', that is the question." Below this text, there are three boxes connected by arrows indicating a process flow:

1. Pretraining data
2. Language models
3. Downstream tasks

In the upper right corner of the image, there is a small video feed showing a person who appears to be presenting the slide.</sample>
    <sample id="1118">The image contains a slide from a presentation with the following text: "Discussion Between Scylla and Charybdis To 'sanitize' or not to 'sanitize', that is the question Pretraining data Language models Downstream tasks" The layout includes three labeled boxes connected by wavy lines, indicating a flow or process.</sample>
    <sample id="1119">The image contains a slide from a presentation with the following elements:

1. **Title**: "Discussion"
2. **Subtitle**: "Between Scylla and Charybdis"
3. **Main Text**: "To 'sanitize' or not to 'sanitize', that is the question"
4. **Flowchart**:
   - Three boxes connected by wavy arrows
   - First box: "Pretraining data"
   - Second box: "Language models"
   - Third box: "Downstream tasks"
5. **Additional Elements**:
   - A small video window in the top right corner showing a person.
   - At the bottom of the flowchart, there are names associated with each stage (from left to right): 
     - Shangbin Feng
     - Chan Young Park
     - Yuhan Liu
     - Yulia Tsetkova
6. **Logos at the Bottom**:
   - Paul G Allen School W
   - UW NLP
   - Carnegie Mellon University Language Technologies Institute

The layout suggests this is part of an academic or professional presentation discussing the decision-making process regarding sanitization in language processing workflows. The presence of logos indicates collaboration between different institutions.</sample>
    <sample id="1120">Thank you Pretraining data Language models Downstream tasks Shangbin Feng Chan Young Park Yuhan Liu Yulia Tsetkova PAUL G. ALLEN SCHOOL OF COMPUTER SCIENCE &amp; ENGINEERING UW NLP Carnegie Mellon University Language Technologies Institute</sample>
    <sample id="1121">The new method is called "Permuting with 'jumps'".</sample>
    <sample id="1122">The author described the "marked words" method as a way to find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">The authors of the paper are affiliated with several institutions: Paul G. Allen School at the University of Washington, the University of Washington (UW), and Carnegie Mellon University's Language Technologies Institute.</sample>
    <sample id="1124">Bouquet/Stanford (Universal Dependencies)</sample>
    <sample id="1125">The speaker's name is Sarah E. Finch, as indicated in the slide presentation shown in the image.</sample>
    <sample id="1126">There are five authors involved in the paper: Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="1127">The datasets mentioned in the image that can be used to test syntactic phenomena are BLiMP, SyntaxGym, and Crows. These datasets appear under a section titled "Revisiting Minimal Pair Paradigm," which discusses how evaluations of language models use relative differences in sequence probabilities to assess their knowledge of language models (LMs). Each dataset is associated with specific examples of sentences or phrases: - BLiMP includes two example sentences related to helping behavior. - SyntaxGym has an example sentence about spending money without specifying who it's for. - Crows presents stereotypical sentences and non-sentences related to spending money. The slide also mentions P(1) &gt; P(2) as part of evaluating these datasets.</sample>
    <sample id="1128">The image is a presentation slide with the title "When Does Translation Require Context? A Data-driven, Multilingual Exploration." Below the title are the names of four individuals: Patrick Fernandes*, Kayo Yin*, Emmy Liu, André F. T. Martins, and Graham Neubig. At the bottom left corner, there's text indicating affiliation with Carnegie Mellon University Language Technologies Institute. On the right side at the bottom, logos for Técnico Lisboa, BAIR (Berkeley Artificial Intelligence Research), and Unbabel are displayed. There is also an asterisk next to the name Patrick Fernandes, which corresponds to an equal contribution note mentioned in small print below the affiliations.</sample>
    <sample id="1129">Translation depends on context We'll have to get rid of that mole.</sample>
    <sample id="1130">Translation depends on context Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole. Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1131">Translation depends on context Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1132">The image shows a slide from a presentation with the title "Evaluating context-dependent translation is hard." There is also text that reads, "Only a small portion of words depend on context," followed by an icon representing documents. Additionally, there's another line of text below this which says, "Corpus-level metrics" and includes three bullet points next to it. In the top right corner of the image, there is a circular photo of a person wearing glasses.</sample>
    <sample id="1133">The image contains a slide from a presentation with the following text: "Evaluating context-dependent translation is hard. Only a small portion of words depend on context o—Corpus-level metrics Existing methods support limited discourse phenomena and languages." Additionally, there is an illustration of a person holding flags representing English (Union Jack) and French (Tricolor), suggesting a focus on language translation or comparison between these two languages.</sample>
    <sample id="1134">RQ1: When does translation require context? RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1135">RQ1: When does translation require context? - Word-level context usage RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1136">The image is a slide from a presentation about Conditional Cross-Mutual Information (CXMI). The title "Conditional Cross-Mutual Information (CXMI)" is prominently displayed at the top. Below the title, there is a bullet point that reads: "CXMI: measure how much context MT models use given a corpus." To the right of this text, there is an inset circular photo of a person.

In the middle section of the slide, there are two equations related to CXMI:

1. \( H_{q_{MTA}}(Y|X) \)
2. \( H_{q_{MTC}}(Y|X,C) \)

These equations represent uncertainty over translations given the source and both the source and context, respectively. There is also additional explanatory text in English below these equations:
- "Uncertainty over translations given the source"
- "Uncertainty over translations given the source AND context"

At the bottom of the slide, there is another equation:
\[ CXMI(C \rightarrow Y|X) \]

This represents the conditional cross-mutual information for translations where C leads to Y given X.</sample>
    <sample id="1137">Conditional Cross-Mutual Information (CXMI) CXMI: measure how much context MT models use given a corpus</sample>
    <sample id="1138">The image is a slide from a presentation, likely on the topic of Pointwise (P-)CXMI. The title "Pointwise (P-)CXMI" is prominently displayed at the top in bold black text. Below the title, there is a bullet point that reads: "We introduce P-CXMI to measure context usage to translate a specific." This suggests an explanation or introduction of P-CXMI and its application.

In the bottom left corner, there are two mathematical expressions labeled "Senten" and "Word," which appear to be related to the concept being discussed:

- For Senten: \( P-CXMI(y, x, C) = -\log \frac{q_{MTA}(y|x)}{q_{MTC}(y|x, C)} \)
- For Word: \( P-CXMI(i, y, x, C) = -\log \frac{q_{MTA}(y_i|y_{i-h:i}, x)}{q_{MTC}(y_i|y_{i-h:i}, x, C)} \)

These equations seem to involve probabilities and logarithms, indicating some form of statistical analysis or modeling.

In the bottom right corner, there is additional text stating: "High P-CXMI words -&gt; requires context to translate."

On the far right side of the slide, there is a circular inset containing an image of a person wearing a dark-colored hat. 

The background of the slide is white, providing clear contrast for the black text and making it easy to read.</sample>
    <sample id="1139">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1140">Thematic analysis of high P-CXMI words</sample>
    <sample id="1141">Thematic analysis of high P-CXML words 1. POS tags</sample>
    <sample id="1142">Thematic analysis of high P-CXML words 1. POS tags - Pronouns</sample>
    <sample id="1143">The image shows a slide from a presentation titled "Thematic analysis of high P-CXML words." The content is divided into two main sections. On the left side, there's a bar chart labeled "P-CXML for POS tags in En-Ar" with three bars representing different parts of speech: PRON 3.Sing, PRON 3.Dual, and PRON 3.Plur. Each bar has values ranging approximately between 0.2 to 0.4 on the y-axis.

On the right side, there are two text boxes against a light purple background. The first box lists "1. POS tags," which corresponds to the part of the title that mentions "POS tags." Below this, another list item reads "2. Vocabulary items."

In the second section, under the heading "Thematic analysis of high P-CXML words," there are bullet points listing "- Pronouns" and "- Verb form," indicating these as subcategories or aspects being analyzed within the thematic study.

Overall, the slide appears to be analyzing linguistic elements such as pronouns and verb forms related to high P-CXML (possibly referring to a specific type of XML data) used in English-to-Arabic translation contexts.</sample>
    <sample id="1144">Thematic analysis of high P-CXML words 1. POS tags 2. Vocabulary items Avelile's mother was still asleep. Avelile went to school. 阿维利尔的母亲还在睡觉。 阿维利尔去上学了。 Pronouns Verb form Lexical cohesion</sample>
    <sample id="1145">Thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items Avelile's mother was still asleep. Avelile went to school. 阿维利尔的母亲还在睡觉。 阿维利尔去上学了。 - Pronouns - Verb form - Lexical cohesion - Formality</sample>
    <sample id="1146">The image shows a slide from a presentation titled "Thematic analysis of high P-CXML words." The content is divided into three main sections: 1. POS tags, 2. Vocabulary items, and 3. Individual tokens. On the right side, there's a purple box listing specific aspects being analyzed within these categories: - Pronouns - Verb form - Lexical cohesion - Formality Additionally, at the bottom left corner, there are two example sentences in English and German with their translations provided below them. These examples illustrate different thematic elements related to the analysis mentioned above.</sample>
    <sample id="1147">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations? - Multilingual Discourse-Aware (MuDA) benchmark</sample>
    <sample id="1148">The image contains a title and a list of items related to the Multilingual Discourse-Aware (MuDA) tagger. The title at the top reads "Multilingual Discourse-Aware (MuDA) tagger." Below the title, there is a light purple box with a bulleted list that includes: - Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis In the upper right corner of the image, there is a circular profile picture of a person wearing dark clothing against a black background.</sample>
    <sample id="1149">Multilingual Discourse-Aware (MuDA) tagger Pronouns Verb form Lexical cohesion Formality Ellipsis pronouns formalitv verb form lexical ellipsis Language</sample>
    <sample id="1150">The video begins with a white background featuring the text 'MuDA benchmark' in black at the top left corner. On the right side, there is an image of a robot character and a small circular photo of a person's face in the upper right corner. The scene transitions to show three stacked documents on the left side labeled 'MuDA tagger,' which are then processed by this tool. The output from the MuDA tagger shows several pages with highlighted sections. Next, these pages pass through another set of documents that represent evaluation metrics such as 'BLEU COMET F-measure...' displayed within a purple box. Finally, the evaluated content reaches the same robot character seen earlier.

The narrative continues with the same initial setup: a white background with the text 'MuDA benchmark' in black at the top left corner. The image of the robot character remains on the right side, while the small circular photo of a person's face stays in the upper right corner. This time, the sequence starts again with the three stacked documents labeled 'MuDA tagger.' These documents proceed through the process, passing through the second set of documents representing evaluation metrics like 'BLEU COMET F-measure...' shown within a purple box. As before, the final step sees the evaluated content reaching the robot character once more.

In the concluding part of the video, the familiar elements remain unchanged: the white background, the text 'MuDA benchmark' in black at the top left corner, the robot character on the right, and the small circular photo of a person's face in the upper right corner. However, the transition between frames indicates no new changes or actions occurring; it maintains consistency throughout its duration.</sample>
    <sample id="1151">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations? - Multilingual Discourse-Aware (MuDA) benchmark - Model evaluation</sample>
    <sample id="1152">Corpus-level metrics</sample>
    <sample id="1153">The image shows a presentation slide with the title 'Corpus-level metrics' at the top. On the left side of the slide, there is an icon labeled 'BLEU,' which features a robot-like figure with the word 'CONTEXT' crossed out over its head. In the center, another similar icon is shown but without any crossout, and it's labeled 'COMET.' To the right, a third icon appears, also resembling a robot, labeled 'F-measure.' Each icon consists of a simple line drawing of a robot with two antennas on its head and a rectangular body displaying binary code (0s and 1s). The background of the slide is plain white. Additionally, in the upper right corner of the slide, there is a small circular inset showing part of a person’s face.</sample>
    <sample id="1154">Corpus-level metrics BLEU COMET F-measure Unclear which system is best for document-level MT with corpus-level metrics</sample>
    <sample id="1155">The video begins with a slide titled 'Corpus-level metrics,' featuring three robot-like figures labeled BLEU, COMET, and F-measure. Each figure has the word 'CONTEXT' written on its chest, except for the middle one which is crossed out. Below these images, there's a bullet point stating: 'Unclear which system is best for document-level MT with corpus-level metrics.' The scene transitions to another slide under the heading 'MuDA benchmark results.' This slide contains a single bullet point that reads: 'Context-aware models perform significantly better on some phenomena.' A small circular image of a person appears in the top right corner throughout this segment.

The narrative continues with the same 'MuDA benchmark results' slide from the previous clip. It reiterates the statement about context-aware models performing significantly better on certain phenomena. Additionally, it introduces new information through an indented sub-bullet point marked with a checkmark, listing specific areas where context-aware models excel: 'Formality, lexical cohesion.' Throughout this part, the background remains white, maintaining consistency with the earlier slides.

The final segment maintains focus on the 'MuDA benchmark results' slide as well. The text 'MuDA benchmark results' at the top remains unchanged. However, no additional content or changes are introduced beyond what was previously shown. The consistent appearance of the small circular image of a person in the top right corner ties all segments together visually.</sample>
    <sample id="1156">MuDA benchmark results Context-aware models perform significantly better on some phenomena Formality, lexical cohesion Ellipsis, pronouns, verb form</sample>
    <sample id="1157">MuDA benchmark results Context-aware models perform significantly better on some phenomena Formality, lexical cohesion Ellipsis, pronouns, verb form DeepL outperforms Google on most phenomena and language pairs* as of April 2021</sample>
    <sample id="1158">MuDA benchmark results Context-aware models perform significantly better on some phenomena Formality, lexical cohesion Ellipsis, pronouns, verb form DeepL outperforms Google on most phenomena and language pairs* Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT MuDA tagger BLEU F-measure robot icon</sample>
    <sample id="1159">The image contains a slide with the title "Summary" at the top. Below this, there are two bullet points: 1. Identify discourse phenomena systematically without prior linguistic knowledge 2. Dataset-agnostic benchmark for document-level MT The bottom of the slide features an illustration showing a flow from documents through a MuDA tagger to a robot labeled as a BLEU F measure. In the upper right corner, there is a circular inset containing a blurred face.</sample>
    <sample id="1160">Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT MuDA tagger BLUE F-measure</sample>
    <sample id="1161">FTw, BOND, COSINE, MLC, L2R.</sample>
    <sample id="1162">The model is evaluated on 11 tasks.</sample>
    <sample id="1163">DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023</sample>
    <sample id="1164">DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023</sample>
    <sample id="1165">The image displays a slide titled "Text Simplification Example" with various elements demonstrating the process of simplifying text. The original German sentence is shown, which reads: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." Below this, there's an English translation labeled as "Plain Language," stating: "The union advocates for higher wages to be paid."

Several types of edits are illustrated on the main part of the slide:
- Substitution (highlighted in blue)
- Clause Deletion (highlighted in red)
- Reordering (highlighted in green)
- Word Deletion (highlighted in orange)

These changes lead to different simplified versions of the original sentence.
- One version retains only essential words and phrases while removing some details ("The union advocates for higher wages").
- Another version further condenses the information by using fewer words.

Additionally, there's a small section at the bottom right corner that mentions "Insertion," indicating where new content might be added during the simplification process.

Overall, the slide provides a visual representation of how complex sentences can be broken down into simpler forms through various editing techniques.</sample>
    <sample id="1166">Text Simplification Example Original: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." Plain Language: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne oder mehr Urlaub ein." Substitution Reordering Clause Deletion Word Deletion Insertion</sample>
    <sample id="1167">The image shows a slide from a presentation about text simplification. The title of the slide is "Text Simplification Example" in bold white letters on a blue background. Below this, there are two sections: one labeled "Original:" and another labeled "Plain Language:". Under "Original:", there's a German sentence with some words crossed out to indicate deletion or simplification steps. These words include "dass", "zum Beispiel", and "gezahlt werden." There are also labels for different types of operations such as "Substitution," "Clause Deletion," "Reordering," and "Word Deletion," each highlighted in red boxes. An arrow points from the original sentence to its simplified version under "Plain Language:", which reads, "Die Gewerkschaft setzt sich dafür ein, Beispiel für höhere Löhne oder mehr Urlaub ein." Additionally, there's an action labeled "Insertion" shown by green arrows pointing towards specific parts of the plain language sentence. In the top right corner of the image, there appears to be a small inset showing someone wearing headphones.</sample>
    <sample id="1168">The image displays a slide titled "Text Simplification Example." It features two German sentences, one labeled as the original and the other translated into plain language. The original sentence is: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." This translates to English as: "The trade union advocates for higher wages being paid, for example."

Below these sentences are four categories of text simplification techniques: Substitution, Clause Deletion, Reordering, and Word Deletion. Each technique has an arrow pointing towards its application on the original sentence.

The simplified version in plain language reads: "Die Gewerkschaft setzt sich dafür ein, zum Beispiel für höhere Löhne oder mehr Urlaub zu eintreten." In English, this means: "The trade union advocates for advocating, for example, for higher wages or more vacation time."

At the bottom right corner of the slide, there's a green box with the word "Insertion," indicating another method used in text simplification.

In summary, the slide illustrates how different methods can be applied to simplify complex text by reducing redundancy while maintaining essential information.</sample>
    <sample id="1169">The video begins with a slide titled "2. DE-plain" and the subtitle "A New Corpus," indicating that this is part of a presentation on German text simplification corpora. The background is white, and there is a small image in the top right corner showing a person wearing headphones.

The scene transitions to a detailed bar chart under the heading "German Text Simplification Corpora." This chart shows data at the sentence level for different years from 2015 to 2019. It includes various categories such as "Domestic," "health," "education," "economy," "politics," "public affairs," "sports," "science," "technology," "entertainment," and "miscellaneous." Each category has corresponding bars representing their respective values. Notably, the year 2018 stands out with significantly higher values across all categories compared to previous years. A specific point on the graph highlights the value '756' for the year 2018, emphasizing its importance or anomaly within the dataset.

The focus remains on the same bar chart throughout the rest of the video, reinforcing the significance of the data presented. The consistent display underscores the key findings related to the growth and distribution of topics within the corpus over time.</sample>
    <sample id="1170">The image contains a slide titled "German Text Simplification Corpora" with the subtitle "Sentence Level." The main content is a bar chart that compares various metrics across different years. Here's a detailed breakdown of the elements in the image: 1. Title and Subtitle: - German Text Simplification Corpora (Main title) - Sentence Level (Subtitle) 2. Bar Chart Details: - X-axis labels represent different years, specifically from 2013 to 2016. - Y-axis represents alignment scores ranging from 0 to 14000. - Bars are color-coded to indicate different categories such as "Domestic," "European," "Health," "Economy," "Legal," "Public affairs," etc., each represented by specific colors like blue, green, yellow, red, purple, orange, etc. - Specific numerical values for certain bars are labeled directly on them, e.g., "1118" near one of the bars. - There are annotations pointing out particular data points or trends within the chart. For instance, there's an arrow pointing at two numbers: "756" next to a brown-colored section and "483" above it, both located towards the right side of the chart. This setup suggests the slide is likely part of a presentation discussing advancements or comparisons in text simplification efforts over several years, possibly focusing on how these efforts have evolved or improved quantitatively.</sample>
    <sample id="1171">German Text Simplification Corpora Sentence Level</sample>
    <sample id="1172">The image is a screenshot of a presentation slide titled "German Text Simplification Corpora." The main content on the slide includes a bar chart labeled "Sentence Level" that compares different corpora. Each bar represents a corpus with various colors indicating different categories such as "Domestic," "European," "Health," "Education," "Economy," and others, along with their respective values in thousands (e.g., 260, 1189). There are two specific bars highlighted: one for "Deutsche Sprache" showing 483 and another for "Deutschsprachige OA" showing 756. A legend explains these color codes, and there's also some text at the bottom right corner mentioning "OER-Texte." In the top-right corner of the image, there appears to be a small video feed of someone speaking into a microphone.</sample>
    <sample id="1173">The image displays a presentation slide titled "German Text Simplification Corpora." The main content of the slide is focused on sentence-level data, as indicated by the subtitle. A bar graph dominates the central portion of the slide, showing various metrics for different corpora or datasets related to German text simplification.

The x-axis of the graph lists several categories: "Deutsch (Bert+RoBERTa)," "Deutsch (XLM-RoBERTa)," and three other unnamed categories. Each category has corresponding bars in different colors representing specific types of information such as "Domestic," "European," "Public domain," "Private," etc., with their respective values shown at the top of each bar.

Notable numerical values are displayed prominently within some of the bars:
- One bar shows "1312" next to an icon that appears to represent a number.
- Another bar indicates "483" adjacent to what looks like a speaker icon.
- There's also a value labeled "756" near another set of icons.

At the bottom left corner of the slide, there seems to be additional textual information which might provide further context about the data presented but it’s not clearly readable from this description alone.

In summary, the slide provides a visual representation of quantitative data regarding German text simplification across multiple sources or models, using color-coded bars to differentiate between various aspects or classifications of the data.</sample>
    <sample id="1174">The image contains a chart titled "German Text Simplification Corpora" with the subtitle "Sentence Level." The chart is divided into several sections, each representing different years and datasets. It includes various colored bars indicating different categories such as "Domestic," "European," "Public domain," etc., along with numerical values for each category.

On the left side of the chart, there are two columns labeled "Alignment: maximum" and "Alignment: average," both ranging from 0 to 14000 in increments of 2000. Below these columns, specific numbers (e.g., 250, 1118) appear next to certain data points on the graph.

To the right of the main chart, there's another section showing additional numerical values like "463," "756," and "13122."

In the top-right corner of the image, there appears to be a small inset picture of a person wearing headphones, likely related to the presentation or topic being discussed about German text simplification corpora at the sentence level.</sample>
    <sample id="1175">The image contains two main sections: "Types of Simplification" and "Simplification Transformations." The first section, "Types of Simplification," displays a bar chart with three categories labeled as news (n = 46), bible (n = 155), L2 (n = 157), and fiction (n = 72). Each category has bars representing Simplicity (blue), LexSimp (red), and StructSimp (yellow).

The second section, "Simplification Transformations," also features a bar chart comparing DEplain-apa (green) and DEplain-web (blue). This chart categorizes different types of transformations such as moving, engineering, lexical substitution, word addition, verb deletion, and noun deletion.

In the top right corner of the image, there is a small video feed showing a person.</sample>
    <sample id="1176">The image contains two bar charts. The first chart, titled "Types of Simplification," compares the simplification levels across four categories: news (n = 46), bible (n = 155), L2 (n = 157), and fiction (n = 72). It shows three types of simplification methods: Simplicity (blue bars), LexSimp (red bars), and StructSimp (yellow bars).

The second chart, labeled "Simplification Transformations," displays a comparison between DEplain-apa (black bars) and DEplain-web (green bars) for different transformation tasks such as moving, engineering, lexical substitution, addition, word deletion, verb addition, and sentence deletion.

In the top right corner of the image, there is a small video feed showing an individual who appears to be presenting or discussing the content displayed in the slides.</sample>
    <sample id="1177">The image contains two charts with accompanying text. The first chart, titled "Types of Simplification," shows bar graphs comparing three categories: Simplicity (blue), LexSimp (red), and StructSimp (yellow) across four types of content labeled as news, bible, L2, and fiction. Each category has a sample size denoted by 'n' values next to the labels.

The second chart is titled "Simplification Transformations" and compares DEplain-apa (black bars) and DEplain-web (green bars). It includes five transformation types on the x-axis: moving, engineering, lexical substitution, word addition, and verb deletion. Both charts are part of a presentation slide that also features an inset video in the top right corner showing a person speaking or presenting.</sample>
    <sample id="1178">The image contains two charts. The first chart is titled "Types of Simplification" and shows a bar graph comparing the levels of Simplicity, LexSimp, and StructSimp across four categories: news (n=46), bible (n=155), L2 (n=157), and fiction (n=72). Each category has three bars representing the different simplification metrics.

The second chart is titled "Simplification Transformations" and also includes a bar graph that compares DEPlain-apa and DEPlain-web across six types of transformations: moving, engineering, lexical substitution, word deletion, word addition, and verb deletion. Both graphs use color coding to differentiate between the various measures or methods being compared.</sample>
    <sample id="1179">Types of Simplification Simplicity LexSimp StructSimp news n = 46 bible n = 155 L2 n = 157 fiction n = 72 Simplicity LexSimp StructSimp Simplification Transformations DEPlain-apa DEPlain-web moving engineering lexical substitution word addition verb deletion</sample>
    <sample id="1180">The video begins with a white screen displaying the text '3. Use-cases' in bold, black font at the top left corner and 'Automatic alignment and simplification' below it in smaller, gray font. In the upper right corner of each frame, there is a small inset showing a person wearing headphones against various backgrounds. The scene remains static throughout this segment.

The narrative then transitions to another white screen that also displays the same text: '3. Use-cases' in bold, black font at the top left corner and 'Automatic alignment and simplification' below it in smaller, gray font. Again, the upper right corner features a small inset of a person wearing headphones. This time, however, the background behind the person changes from different settings to an indoor setting with light-colored walls and ceiling lights visible. 

Finally, the video shifts to display a table titled 'Automatic Alignment Evaluation' at the top within a blue header bar. Below the title, the table has two sections labeled '1:1 (upper part)' and 'n cm n c m (lower part)'. It lists several alignment methods such as LHA-LiLBSE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign along with their descriptions and corresponding scores for similarity metrics P, R, F, and F0.5. Similar to previous segments, the upper right corner shows a small inset of a person wearing headphones.</sample>
    <sample id="1181">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and ncm cpm (lower part)</sample>
    <sample id="1182">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n cm capabilities (lower part) Name Description LHA Hierarchical alignment using sentence embeddings similarity Sent-LA-BASE Similar embeddings of Language-agnostic BERT transformer 961 444 .608 780 CATS-C3G Different similarity measures e.g. grams (C3G)/word vectors VecAlign Multilingual aligned based on multilingual sentence embeddings 271 404 .290 335 266 333 BERTrAlign Allows sentence-transformer methods produce n:cm values 743 465 .504 636 MASSAlign A vicinity-driven approach with a TF-IDF similarity matrix 846 476 .813 739 739 605</sample>
    <sample id="1183">The image shows a presentation slide titled "Automatic Alignment Evaluation" with two main sections. The upper section is labeled "Results of the alignment methods with 1:1 (upper part)" and contains a table comparing different alignment methods such as Sent-LA-LEASE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign. Each method has corresponding numerical values in columns P, R, F, PR, and ncm. The lower section is labeled "ncm cm n" and also includes a similar table format but without specific entries filled out yet.

The background features a blue header with white text that reads "Automatic Alignment Evaluation." Below this header, there are two rows of tables. 

In the first row under the title, each cell contains either numbers or dashes, indicating missing data for some cells. In contrast, the second row displays more complete sets of numbers across all columns except for one entry which remains blank.

Overall, the slide appears to be presenting evaluation metrics for various alignment algorithms used in natural language processing tasks.</sample>
    <sample id="1184">The image shows a slide from a presentation titled "Automatic Alignment Evaluation." The main content of the slide is divided into two sections. The upper section, labeled "1:1 (upper part)," lists different alignment methods along with their corresponding scores for precision and recall in various scenarios. Below this, the lower section, marked "n cm capabilities (lower part)," provides descriptions of each method's functionality or approach to alignment.

The table includes columns for 'Name,' 'Description,' and performance metrics such as 'P' (Precision), 'R' (Recall), and 'F1' score across multiple rows representing different algorithms like Sent-LAISE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign. Each row details how these algorithms perform under specific conditions related to hierarchical alignment using sentence embeddings similarity and other criteria mentioned in their respective descriptions.

Overall, the slide appears to be presenting an evaluation framework for comparing the effectiveness of various automatic alignment methods based on their ability to handle 1:1 relationships and n cm (non-collinear) capabilities within natural language processing tasks.</sample>
    <sample id="1185">The image displays a presentation slide titled "Automatic Alignment Evaluation" with the subtitle "Results of the alignment methods with 1:1 (upper part) and n cm n cpm (lower part)." The table on the slide lists various alignment methods, their descriptions, and performance metrics. Here are the details:

- **LHA**: Hierarchical alignment using sentence embeddings similarity.
- **Sent-LAISE**: Similar embeddings of Language-agnostic BERT transformer.
- **CATS-C3G**: Different similarity measures e.g., n-grams (C3G)/word vectors.
- **VecAlign**: Multilingual aligned based on multilingual sentence embedding.
- **BERTAlign**: Allows sentence-transformer methods produce n cm n cpm.
- **MASSAlign**: A vicinity-driven approach with a TF-IDF similarity matrix.

Each method has corresponding scores under columns labeled P, R, F, PR, and n cm n cpm, indicating different evaluation criteria for each method's performance in terms of precision, recall, harmonic mean (F), precision-recall product (PR), and normalized common metric (n cm n cpm).</sample>
    <sample id="1186">The image shows a presentation slide titled "Automatic Alignment Evaluation." The title is displayed in white text on a blue background. Below the title, there are two sections of content: 1. Upper part (upper half): This section lists various alignment methods with their corresponding metrics for similarity scores using different embedding models and algorithms. It includes columns labeled P, R, F, PR, and ncm, each representing specific evaluation criteria or parameters. There's also an additional column labeled ncmF0.5, which seems to be related to another metric or parameter. Each method listed has associated numerical values under these columns. 2. Lower part (lower half): This section provides descriptions of the alignment methods mentioned above. For example, one description reads: "Hierarchical alignment using sentence embeddings similarity," followed by details about how this particular method operates. Other entries include terms like "BERT transformer," "CATS-C3G," "VecAlign," among others, along with brief explanations of what they entail. In summary, the slide appears to be presenting results from experiments evaluating different automatic alignment methods based on certain criteria, likely within a research context focused on natural language processing or computational linguistics.</sample>
    <sample id="1187">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n cm capabilities (lower part) LHA Description Sent-LAISE Hierarchical alignment using sentence embeddings similarity Sent-LAISE Similar embeddings of Language-agnostic BERT transformer CATS-C3G Different similarity measures a g-grams (C3G) word vectors VecAlign Multilingual aligned based on multilingual sentence embedding BERTAlign Allows sentence-transformer methods produce n: m alignments MASSAlign A vicinity-driven approach with a TF-IDF similarity matrix P PRF PRF PRF ncm0.5 94 41 57 747 961 444 608 780 924 456 639 816 271 404 292 330 455 743 465 504 576 846 471 519 633 729</sample>
    <sample id="1188">Automatic Text Simplification Document Level Results on Document Simplification using featurened long mBART. n corresponds to the length of the training data. DEPLAIN-APA test (n=48) DEPLAIN-APA train data | SARI | BLEU | F1 | PRE 0.632 0.579 0.546 0.550 DEPLAIN-APA baseline 0.632 0.579 0.546 0.550 DEPLAIN-APA test (n=48) DEPLAIN-APA train data | SARI | BLEU | F1 | PRE 0.632 0.579 0.546 0.550 DEPLAIN-APA baseline 0.632 0.579 0.546 0.550 Sentence Level Results on Sentence Simplification using featurened mBART. n corresponds to the length of the training data. DEPLAIN-APA test (n=1231) DEPLAIN-APA train data | SARI | BLEU | F1 | PRE 0.632 0.579 0.546 0.550 DEPLAIN-APA baseline 0.632 0.579 0.546 0.550 DEPLAIN-APA test (n=1231) DEPLAIN-APA train data | SARI | BLEU | F1 | PRE 0.632 0.579 0.546 0.550 DEPLAIN-APA baseline 0.632 0.579 0.546 0.550 DEPLAIN-WEB test (n=1846) DEPLAIN-WEB train data | SARI | BLEU | F1 | PRE 0.632 0.579 0.546 0.550 DEPLAIN-WEB baseline 0.632 0.579 0.546 0.550 DEPLAIN-WEB test (n=1846) DEPLAIN-WEB train data | SARI | BLEU | F1 | PRE 0.632 0.579 0.546 0.550 DEPLAIN-WEB baseline 0.632 0.579 0.546 0.550</sample>
    <sample id="1189">The image shows a presentation slide titled "Automatic Text Simplification" with two main sections: Document Level and Sentence Level. Both sections detail results from document simplification using a feature-tuned long mBART model, comparing different configurations (DEPLAIN-APA test vs DEPLAIN-WEB test) across various metrics like BLEU, F1, and METEOR scores for both training data and the tests themselves.

In the background of this technical content is an individual wearing glasses and dressed in dark clothing, seated at what appears to be a desk or workstation setup. The person's presence suggests they might be presenting or discussing the information displayed on the screen.</sample>
    <sample id="1190">Automatic Text Simplification Document Level Results on Document Simplification with finetuned long mBART. n corresponds to the length of the training data. DEPLAIN-APA test (n=48) DEPLAIN-APA BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-1 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-2 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-3 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-4 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-5 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-6 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-7 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-8 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-9 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-10 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-11 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-12 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-13 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-14 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-15 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-16 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-17 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-18 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-19 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-20 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-21 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-22 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-23 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-24 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-25 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-26 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-27 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-28 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-29 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-30 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-31 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-32 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-33 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-34 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-35 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-36 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-37 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-38 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-39 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-40 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-41 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-42 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-43 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-44 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-45 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-46 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-47 BLEU: 0.623 F1: 0.579 PRE: 0.642 SARI-48 BLEU: 0.623 F1: 0.579 PRE: 0.642 Sentence Level Results on Sentence Simplification with finetuned long mBART. n corresponds to the length of the training data. DEPLAIN-APA test (n=1231) DEPLAIN-APA BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-1 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-2 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-3 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-4 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-5 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-6 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-7 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-8 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-9 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-10 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-11 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-12 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-13 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-14 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-15 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-16 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-17 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-18 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-19 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-20 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-21 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-22 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-23 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-24 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-25 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-26 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-27 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-28 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-29 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-30 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-31 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-32 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-33 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-34 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-35 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-36 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-37 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-38 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-39 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-40 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-41 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-42 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-43 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-44 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-45 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-46 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-47 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-48 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-49 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-50 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-51 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-52 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-53 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-54 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-55 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-56 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-57 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-58 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-59 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-60 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-61 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-62 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-63 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-64 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-65 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-66 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-67 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-68 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-69 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-70 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-71 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-72 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-73 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-74 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-75 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-76 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-77 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-78 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-79 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-80 BLEU: 0.619 F1: 0.571 PRE: 0.626 SARI-81 BLE</sample>
    <sample id="1191">Automatic Text Simplification Document Level Results on Document Simplification using fine-tuned mbART. n corresponds to the length of the training data. DEPLAIN-APA test (n=48) DEPLAIN-APA train data | SARI train data | SARI BLEU | SARI F1 | DEPLAIN-APA baseline 0.657 0.629 0.386 0.562 0.522 0.454 0.386 0.427 0.386 0.375 0.345 0.325 0.312 0.285 DEPLAIN-APA test (n=48) DEPLAIN-APA train data | SARI train data | SARI BLEU | SARI F1 | DEPLAIN-APA baseline 0.657 0.629 0.386 0.562 0.522 0.454 0.386 0.427 0.386 0.375 0.345 0.325 0.312 0.285 Sentence Level Results on Sentence Simplification using fine-tuned mbART. n corresponds to the length of the training data. DEPLAIN-APA test (n=1231) DEPLAIN-APA train data | SARI train data | SARI BLEU | SARI F1 | DEPLAIN-APA baseline 0.657 0.629 0.386 0.562 0.522 0.454 0.386 0.427 0.386 0.375 0.345 0.325 0.312 0.285 DEPLAIN-APA test (n=1231) DEPLAIN-APA train data | SARI train data | SARI BLEU | SARI F1 | DEPLAIN-APA baseline 0.657 0.629 0.386 0.562 0.522 0.454 0.386 0.427 0.386 0.375 0.345 0.325 0.312 0.285</sample>
    <sample id="1192">The image displays a presentation slide titled "Automatic Text Simplification" with two main sections: Document Level and Sentence Level. Each section contains tables comparing different models' performance on various tests, including DEPLAIN-APA test (n=48), DEPLAIN-APA test (n=1231), DEPLAIN-WEB test (n=147), and DEPLAIN-WEB test (n=1846). The metrics shown include data size in training data (train data) and validation data (valid data), BLEU scores for both training and validation sets, and F1 scores for the training set only. The background of the slide is white with blue headers, and there's a small inset showing a person at the top right corner.</sample>
    <sample id="1193">The image shows a presentation slide titled "Automatic Text Simplification" with two main sections: Document Level and Sentence Level. Both sections detail the results of text simplification using a model called mBART, which is fine-tuned for document-level (DEPLAIN-APA test) and sentence-level (DEPLAIN-APN test) tasks.

For both levels, there are tables comparing different models' performance on various metrics such as train data BLEU, F1, and PRE, as well as DEPLAIN-APA and DEPLAIN-APN tests. The tables include scores like 0.684/0.732/0.795 for train data BLEU at the document level and 0.638/0.682/0.729 for the same metric at the sentence level.

The background features a person in a red shirt sitting to the right side of the screen, partially visible behind the content.</sample>
    <sample id="1194">Automatic Text Simplification Document Level Results on Document Simplification with finetuned long mBART. n corresponds to the length of the training data. DEPLAIN-APA test (n=48) DEPLAIN-APA test (n=1231) Sentence Level Results on Sentence Simplification with finetuned mBART. n corresponds to the length of the training data. DEPLAIN-APA test (n=48) DEPLAIN-APA test (n=1231) DEPLAIN-WEB test (n=147) DEPLAIN-WEB test (n=1846)</sample>
    <sample id="1195">Thanks. For more details. Please check out our paper. And feel free to visit our poster in the ACL 2023 conference.</sample>
    <sample id="1196">The image contains text that reads: "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)" followed by a list of names. At the bottom left corner, there is a logo with the text "Google Research." The background features abstract lines in various colors such as blue, red, green, and yellow.</sample>
    <sample id="1197">The image contains a presentation slide with the following text: "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)" followed by names of individuals. The Google Research logo is visible in the bottom left corner, and there are colorful abstract lines on the right side of the slide.</sample>
    <sample id="1198">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference: easy on me, the first one Indirect reference could be used in natural and fluid conversation: Cannot remember the name The pronunciations are hard to distinguish Want to specify a preference Indirect reference The newer one. The song that's not energetic.</sample>
    <sample id="1199">Indirect Referring Expressions Goal: Understanding users’ language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference • “easy on me,” “the first one” Indirect reference could be used in natural and fluid conversation: • Cannot remember the name • The pronunciations are hard to distinguish • Want to specify a preference Indirect reference The newer one. The song that’s not energetic.</sample>
    <sample id="1200">Indirect Referring Expressions Goal: Understanding users’ language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference “easy on me,” “the first one” Indirect reference could be used in natural and fluid conversation: • Cannot remember the name • The pronunciations are hard to distinguish • Want to specify a preference Indirect reference The newer one. The song that’s not energetic.</sample>
    <sample id="1201">Indirect Referring Expressions Goal: Understanding users’ language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference easy on me, “the first one” Indirect reference could be used in natural and fluid conversation: Cannot remember the name The pronunciations are hard to distinguish Want to specify a preference Indirect reference The newer one. The song that’s not energetic.</sample>
    <sample id="1202">Indirect Referring Expressions Goal: Understanding users’ language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference "easy on me," “the first one” Indirect reference could be used in natural and fluid conversation: • Cannot remember the name • The pronunciations are hard to distinguish • Want to specify a preference Indirect reference The newer one. The song that’s not energetic. Breaking Indirect Referring Expressions for Entity Selection (Mistakability Conventions)</sample>
    <sample id="1203">The image contains a slide from a presentation by Google Research. The title of the slide is "Dataset Collection." Below the title, there are several bullet points:

- Important problem
  - Conversational systems
  - Benchmarking Large Language Models’ entity understanding
- No large-scale public dataset available
- We collect a large dataset using crowd annotation

Additionally, it mentions that there are three domains related to this topic.

At the bottom left corner of the slide, there is text that reads: "Revisiting Indirect Expressions for Entity Selection (Entity Utilization Corpus)." There are also some icons at the bottom center of the slide representing different datasets or concepts, and on the right side, there is an inset photo of a person with their face blurred out.</sample>
    <sample id="1204">The text on the slide reads: "Dataset Collection" with a bullet point list underneath. The points listed are:

- Important problem
  - Conversational systems
  - Benchmarking Large Language Models' entity understanding
- No large-scale public dataset available
- We collect a large dataset using crowd annotation

Below this, there is another line that says:
- Three domains:

At the bottom of the slide, in smaller font and italicized, it states:
"Resolving Indirect Expressions for Entity Selection (Utilities Corpus)"

On the right side of the image, there's a logo or icon resembling a golden dome-like shape.

This appears to be part of an academic presentation discussing challenges related to dataset collection for conversational systems and language models, specifically mentioning the need for a large-scale public dataset and how they use crowd annotation to gather such data.</sample>
    <sample id="1205">The image contains a slide from a presentation by Google Research, titled "Dataset Collection Methodology." The methodology emphasizes informality using a cartoon completion task. There are three illustrated characters with speech bubbles and text annotations: 1. A character on the left says, "Remember that we were listening to yesterday?" This is labeled as setting the dialogue context chosen from a few manual prompts per domain. 2. In the middle, another character asks, "You mean Easy on Me or I Gotta Feeling?" This represents an alternative question. 3. On the right, there's a blank space for filling in which is referred to as an expression referring to one of the entities. Additionally, there's a yellow box pointing to this empty space stating it is filled in by the annotator. At the bottom of the slide, there's a reference to the source material: "Revisiting Indirect Referring Expressions for Entity Selection (Mandarin Corpus)."</sample>
    <sample id="1206">The image shows a presentation slide titled "Dataset Collection Methodology" with the Google Research logo in the top right corner. The main content of the slide explains that methodology emphasizes informality using a cartoon completion task, as indicated by bullet points and text boxes.

There are three illustrated characters on the left side:
1. A character saying, "Remember that we were listening to yesterday?"
2. Another character asking, "Do you mean 'Easy' or 'Me' or 1 Gotta Feeling?"
3. A third character is shown without any speech bubble but has an arrow pointing towards them labeled "Filled in by the annotator."

Below these illustrations, there's additional information explaining each part: 
- Sets the dialog context chosen from a few manual prompts per domain.
- The alternative question.
- Expression referring to one of the entities.

At the bottom of the slide, there is a footer which reads: "Revisiting Indirect Referring Expressions for Entity Selection in Multimodal Conversations."</sample>
    <sample id="1207">The image shows a slide from Google Research titled "Dataset Collection Methodology." It explains that methodology emphasizes informality using a cartoon completion task. The slide includes four illustrated characters, each with speech bubbles and labels indicating their roles in the process: 1) Sets the dialog context (chosen from a few manual prompts per domain), 2) The alternative question, 3) Expression referring to one of the entities, and 4) Filled in by the annotator. There is also an arrow pointing to the last character's bubble, emphasizing this step. At the bottom left corner, there is additional text related to the topic but it’s partially obscured; what can be seen reads, "Resolving Indirect Referring Expressions for Entity Selection in Metaphorical Corpus."</sample>
    <sample id="1208">The image is a screenshot of a presentation slide titled "Dataset Collection Methodology" from Google Research. The main content explains that methodology emphasizes informality using a cartoon completion task, illustrated with three panels showing characters engaged in conversation. Each panel has text boxes representing dialogue and questions like “Remember that we were listening to yesterday?” or “Do you mean ‘Easy on Me’ or 1 Gotta Feeling?”. A yellow box highlights the phrase “Filled in by the annotator” next to one character’s speech bubble.

Below the illustrations are captions: 
- Sets the dialog context chosen from a few manual prompts per domain
- The alternative question
- Expression referring to one of the entities

At the bottom left corner, there's additional text reading “Revisiting Indirect Referring Expressions for Entity Selection (Mentions Completeness)”. In the lower right corner, part of an individual's face can be seen within a circular frame.</sample>
    <sample id="1209">The image shows a presentation slide titled "Dataset Collection Methodology" with the Google Research logo in the top right corner. The main content of the slide describes how methodology emphasizes informality using a cartoon completion task.

There are three illustrated characters, each representing different aspects of this process:
1. A character labeled "Sets the dialog context (chosen from a few manual prompts per domain)" is shown thinking about something.
2. Another character poses an alternative question: "Do you mean 'Easy on Me' or 1 Gotta Feeling?"
3. The third character represents an expression referring to one of the entities involved.

A yellow box next to these illustrations explains that the speech bubbles filled by annotators and their responses are highlighted as part of the dataset collection method.

At the bottom left of the slide, there's additional text related to the topic being discussed but it’s not fully visible in the description provided.</sample>
    <sample id="1210">The image contains a slide from Google Research titled "Dataset Collection Methodology." The main content of the slide emphasizes informality in methodology using a cartoon completion task. There are three speech bubbles shown, each with different text: 1. A reminder about listening to yesterday's episode. 2. An alternative question asking if someone means 'Easy on Me' or 'I Got a Feeling'. 3. A statement that refers to one entity out of several options provided by an annotator. At the bottom left corner, there is additional information stating that this process involves choosing dialog context from manual prompts per domain and selecting expressions referring to entities chosen by the annotator. In the top right corner, there is the Google Research logo.</sample>
    <sample id="1211">The image contains a slide from a presentation with the title "Generate alternative questions =&gt; sampling entity pairs" at the top. Below this, there is a question in quotation marks: "Do you mean A or B?" The main content of the slide lists several bullet points describing different methods for generating alternative questions:

1. Items with similar infoboxes on Wikipedia (same genre and/or artist)
   - Do you mean This is It or Man in the Mirror?
2. Items with similar descriptions on Wikipedia
   - Do you mean Thinking of You or Happy Anywhere?
3. Items with similar titles:
   - Do you mean The Return (memoir) or The Return (Shatner novel)?
4. Uniform at random:
   - Do you mean You Could Be Mine or The Way I Am?

On the left side of the slide, there is an arrow pointing upwards labeled "More Similar (usually harder)." At the bottom right corner, there is text that reads "Reading Indirect Expressions for Entity Selection in Autobiographical Narratives." In the upper right corner, there is a logo with the text "Google Research."

In addition to these elements, there is also a small circular placeholder where a person's face would typically be shown, but it has been blurred out.</sample>
    <sample id="1212">The text on the slide reads: "Generate alternative questions =&gt; sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean This is it or Man in the Mirror? Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am?"</sample>
    <sample id="1213">Generate alternative questions =&gt; sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean This is it or Man in the Mirror? Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean Could You Be Mine or The Way I Am</sample>
    <sample id="1214">The image contains a slide from a presentation with the title "Generate alternative questions =&gt; sampling entity pairs" at the top. Below this, there is a question in bold: "Do you mean A or B?" The main body of text provides examples and explanations for generating these alternative questions:

- Items with similar infoboxes on Wikipedia (same genre and/or artist)
  - Do you mean This is It or Man in the Mirror?
- Items with similar descriptions on Wikipedia
  - Do you mean Thinking of You or Happy Anywhere?
- Items with similar titles:
  - Do you mean The Return (memoir) or The Return (Shatner novel)?
- Uniform at random:
  - Do you mean Could You Be Mine or The Way I Am?

On the left side of the slide, there is an arrow labeled "More Similar (easier)" pointing upwards. In the bottom right corner, there is a small circular photo of a person next to the Google Research logo. At the very bottom of the slide, there is additional text that reads: "Revisiting Inductive Expressions for Entity Selection Utility Correlations."</sample>
    <sample id="1215">The English content in the image is as follows: "Generate alternative questions =&gt; sampling entity pairs" and below that, there's a question box with the text "Do you mean A or B?" followed by several bullet points. The bullet points list different types of items to consider when generating alternative questions:

- Items with similar infoboxes on Wikipedia (same genre and/or artist)
  - Do you mean This is It or Man in the Mirror?
- Items with similar descriptions on Wikipedia
  - Do you mean Thinking of You or Happy Anywhere?
- Items with similar titles:
  - Do you mean The Return (memoir) or The Return (Shatner novel)?
- Uniform at random:
  - Do you mean Could You Be Mine or The Way I Am?

There is also an arrow pointing upwards labeled "More Similar (likely harder)" and some additional text at the bottom which reads "Revisiting Inductive Expressions for Entity Selection Adaptability Correlation."</sample>
    <sample id="1216">The image displays a slide from Google Research titled "Background knowledge (Music)." It outlines the process of providing background information for songs. The first bullet point states that there is a Google search link to each song, with examples given: "Easy on Me" by Adele and "I Gotta Feeling" by The Black Eyed Peas. Each example has an accompanying hyperlink labeled "Click here to find out about the song." 

The second bullet point explains that annotators are asked to listen to at least some of each song or read about each song.

In the bottom right corner, there is a small circular photo of a person wearing a dark shirt, likely representing one of the researchers involved in this study. At the very top right corner, the Google logo is visible. Additionally, at the bottom left of the slide, text reads "Revisiting Indirect Referring Expressions for Entity Selection (Mentorship Corpus)."</sample>
    <sample id="1217">The image shows a presentation slide titled "Background knowledge (Music)" from Google Research. It explains that for each song, there is a Google search link provided to find out more about the song.

Two songs are highlighted:
1. Easy on Me by Adele
2. I Gotta Feeling by The Black Eyed Peas

Below these titles, it states: "Click here to find out about the song."

The slide also mentions that annotators were asked to listen to at least some of each song and read about each song.

At the bottom left corner, there is text indicating the source or context of the presentation: "Revisiting Indirect Referring Expressions for Entity Selection in Multimedia Corpus." In the bottom right corner, there is an image of a person with their face blurred.</sample>
    <sample id="1218">The video begins with a slide titled 'Background knowledge (Music)' from Google Research. It explains that for each song, there is a Google search link provided to find out more about the song. Two songs are highlighted: "Easy on Me" by Adele and "I Gotta Feeling" by The Black Eyed Peas, both accompanied by clickable links labeled 'Click here to find out about the song.' Below these highlights, it states that annotators are asked to listen to at least some of each song or read about each song.

The scene transitions to a screenshot showing the YouTube page for "Easy on Me" by Adele. The left side displays the official music video thumbnail with play button overlay, while the right side shows various tabs like Listen, Watch, About, Lyrics, and More. Below this, lyrics start appearing in English text format. At the bottom, options such as 'Videos' and 'See all videos' are visible. In the lower-right corner, there's an image of a person wearing glasses and a dark shirt.

The final part of the video maintains focus on the same screenshot of the YouTube page for "Easy on Me." The layout remains consistent with the previous frame, featuring the music video thumbnail, detailed information sections, and the person’s image in the lower-right corner. Additionally, a caption appears below the main content area stating 'Revisiting Indirect Referring Expressions for Entity Selection (Mentat Corpus).'</sample>
    <sample id="1219">The English content in the image is as follows: Background knowledge (Recipes) Simnel Cake - Simnel cake is a fruitcake widely eaten in the United Kingdom, Ireland and other countries with patterns of migration from them, associated with Lent and Easter. It is distinguished by layers of almond paste or marzipan, and a set of eleven balls made of the same paste. Pandan Cake - Pandan cake is light, fluffy, green-coloured sponge cake flavoured with the juices of Pandanus amabilis leaves. The cake is popular in Indonesia, Malaysia, and also the Netherlands, especially among the Indo community.</sample>
    <sample id="1220">The image contains a slide from a presentation with the title "Eliciting expressions" at the top. The Google Research logo is visible in the upper right corner, and there's some text below it that reads: "Revising Indirect Referring Expressions for Entity Selection (Miyoshi Utilities Corpus)." 

The main content of the slide explains how annotators are told to select a choice and describe it using three to five expressions. There are two options presented:

1. Easy on Me by Adele
2. I Gotta Feeling by Black Eyed Peas

Below these options, there's an instruction stating: "We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble." An example list follows, which includes:
- The one with the piano music
- The song that's not energetic
- It has something about a river
- The newer one
- It's about having time to choose</sample>
    <sample id="1221">The image contains a slide from a presentation with the title "Eliciting expressions" at the top. Below the title, there is text that reads: "We then tell the annotators which choice should be selected and ask them to describe it." There are two options presented for selection:

- Easy on me (by Adele)
- I Gotta Feeling (by the Black Eyed Peas)

Below these options, additional instructions state: "We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble. For example:" This section includes examples of how to express descriptions related to music selections.

On the right side of the slide, there is an avatar or profile picture of a person wearing glasses and a dark shirt. The background of the slide has a white color scheme with black text, and there is a Google Research logo visible in the upper right corner.

At the bottom left of the slide, there is a caption that reads: "Referential Indexing References for Entity Selection (Abdullah Collection)." On the right side, near the bottom, another line of text states: "Referential Indexing References for Entity Selection (Abdullah Collection)."

The overall layout suggests this is part of a larger presentation focused on eliciting expressive language choices for different categories such as music, book titles, and recipes.</sample>
    <sample id="1222">The image contains a slide from a presentation titled "AltEntities Corpus" with the Google Research logo in the top right corner. The content of the slide is as follows:

- **Title:** AltEntities Corpus
- **Bullet Points:**
  - ~6,000 alternative questions across the three domains
  - ~42,000 indirect referring expressions
  - Results with T5 XL model (accuracy):
    - 92-95% if the LM has access to the same background knowledge as annotators.
    - 82%-87% when the LM has access to partially overlapping background knowledge.
    - ~60% when the LM (T5 XL) has only access to the entity names.
    - We showed models are domain-generalizable.
- **Dataset Link:** https://github.com/google-research-datasets/AltEntities

At the bottom left, there is additional text that reads:
- "Revisiting Indirect Referring Expressions for Entity Selection AltEntities Corpus"

In the lower right corner of the slide, there is an image of a person wearing glasses and smiling.

This detailed description covers all visible elements and information presented on the slide.</sample>
    <sample id="1223">The image contains a slide from a presentation about the AltEntities Corpus. The content is as follows:

**Title:**
- AltEntities Corpus

**Body Text:**
- ~6,000 alternative questions across the three domains
- ~42,000 indirect referring expressions

**Subtitle:**
- Results with T5 XL model (accuracy):

**List of Points under Subtitle:**
- 92-95% if the LM has access to the same background knowledge as annotators.
- 82%-87% when the LM has access to partially overlapping background knowledge.
- ~60% when the LM (T5 XL) has only access to the entity names.

**Additional Information:**
- We showed models are domain-generalizable.

**Link:**
- Dataset Link: https://github.com/google-research/datasets/AltEntities

**Footer:**
- Resolving Indirect References for Entity Selection in AltEntities Corpus

**Logo:**
- Google Research logo at the top right corner</sample>
    <sample id="1224">The text on the slide reads: "AltEntities Corpus - ~6,000 alternative questions across the three domains - ~42,000 indirect referring expressions Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82%-87% when the LM has access to partially overlapping background knowledge. ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research/datasets/AltEntities Reading: Entity Selection for AltEntities Corpus"</sample>
    <sample id="1225">The content in the image is as follows: The slide provides information about the 'AltEntities Corpus,' which includes approximately 6,000 alternative questions across three domains and around 42,000 indirect referring expressions. It discusses results with a T5 XL model regarding accuracy for different levels of background knowledge overlap between the language model (LM) and annotators, ranging from same background to partially overlapping backgrounds. Additionally, it mentions that models are domain-generalizable when LM has only access to entity names. There's also a dataset link provided at the bottom.

Thank you If you have any questions, please email javadh@google.com</sample>
    <sample id="1226">CamemBERT is initially trained on the English Wikipedia dataset.</sample>
    <sample id="1227">The name of the speaker is Adam Przepiórkowski.</sample>
    <sample id="1228">The findings that led to the conclusion that temporal drift is the main cause of performance loss include: 1. Temporal drift was not observed in earlier studies, suggesting it became a significant factor over time. 2. Performance degrades with larger temporal gap between training and testing data. These points indicate that as models are trained on more recent data compared to when they were tested, their performance declines due to this temporal mismatch or "drift."</sample>
    <sample id="1229">NLP Positionality: Characterizing Design Biases of Datasets and Models Sebastian Senti* Carnegie Mellon University Jenny T. Liang Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarten Sap Carnegie Mellon University</sample>
    <sample id="1230">The image shows a presentation slide titled "NLP Positionality: Characterizing Design Biases of Datasets and Models." Below the title, there are five individual photos with names and affiliations listed underneath each photo. The individuals and their respective institutions are:

1. Sebastian Santy - University of Washington
2. Jenny T. Liang* - Carnegie Mellon University
3. Ronan Le Bras - Allen Institute for AI
4. Katharina Reinecke - University of Washington
5. Maarten Sap - Carnegie Mellon University

In the top right corner of the slide, there is an additional small inset showing another person who appears to be part of the presentation or discussion related to this topic.</sample>
    <sample id="1231">Imagine...</sample>
    <sample id="1232">Imagine... Carl Jones Tech Lead, New York Times = PerspectiveAPI score Can you stop being a jerk? 😐 (0.82) ✅</sample>
    <sample id="1233">Imagine... Carl Jones Tech Lead, New York Times Can you stop being a jerk? 😡 (0.82) ✅ Aditya Sharma Tech Lead, Times of India Presstitutes everywhere on the news. 😡 (0.33) ❌ = PerspectiveAPI score</sample>
    <sample id="1234">Imagine... Design bias example Can you stop being a jerk? : 0.82 ✅ Presstitutes everywhere on the news.: 0.33 ❌ Carl Jones Tech Lead, New York Times Aditya Sharma Tech Lead, Times of India = PerspectiveAPI score</sample>
    <sample id="1235">The image shows a slide with the title "Positionality" in bold at the top left corner. Below the title, there is a citation: "[1] Savin-Baden, Maggi, and Claire Howell-Major; 'Qualitative research: The essential guide to theory and practice.' Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013)." On the right side of the slide, there is an inset photo of a person sitting on a couch with bookshelves visible behind them. In the middle section of the slide, under the title, there is a quote that reads: "The perspectives [people] hold as a result of their demographics, identity, and life experiences."</sample>
    <sample id="1236">Positionality "The perspectives [people] hold as a result of their demographics, identity, and life experiences." [1] Savin-Baden, Maggi, and Claire Howell-Major. "Qualitative research: The essential guide to theory and practice." Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013).</sample>
    <sample id="1237">Positionality "The perspectives [people] hold as a result of their demographics, identity, and life experiences." [As a researcher,] it influences the research process and its outcomes and results. [1] Savin-Baden, Maggi, and Claire Howell-Major. "Qualitative research: The essential guide to theory and practice." Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013).</sample>
    <sample id="1238">The image contains a title and three references, all related to the topic of datasets and models having positionality. The text is as follows:

Title: "Do datasets and models have positionality?"

References:
1. [1] Blasi, et al., “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022.
2. [2] Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022.
3. [3] Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.

In addition to this textual content, there is also an inset picture showing a person who appears to be giving a presentation or lecture.</sample>
    <sample id="1239">The image contains a slide with the title "Do datasets and models have positionality?" followed by three references. The first reference is "[1] Blasi, et al., 'Systematic Inequalities in Language Technology Performance across the World's Languages.' ACL 2022." The second reference is "[2] Ye et al., 'GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models.' EMNLP 2022." The third reference is "[3] Cambo &amp; Gergle, 'Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.' CHI 2022." Additionally, there is an inset photo of a person at the top right corner of the slide.</sample>
    <sample id="1240">The image contains text related to a discussion on the positional nature of datasets and models in language technology. The main heading reads, "Do datasets and models have positionality?" Below this, there is a subheading titled "Anecdotal evidence:" followed by two bullet points: 1. Model and dataset probing [1][2] 2. Theoretical definitions of model positionality [3] At the bottom of the image, there are three references listed with their corresponding details: [1] Blasi, et al., “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="1241">Title: Do datasets and models have positionality?

Subtitle: Anecdotal evidence:

- Model and dataset probing [1][2]
- Theoretical definitions of model positionality [3]

References:
[1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022.
[2] Ye et al., "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022.
[3] Cambo &amp; Gergle, "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022.</sample>
    <sample id="1242">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. [2] Ye et al., "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022. [3] Cambo &amp; Gergle, "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022.</sample>
    <sample id="1243">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="1244">The video features a slide with the following text: "Question: Do datasets and models have positionality?" This question is presented in black font on a white background. Below this, there is an arrow pointing downwards. Further down, another line of text reads: "Goal: Compare annotations from users with existing datasets and models." The word 'users' within this sentence is underlined for emphasis. In the top right corner of each frame, there is a small image of a person sitting at a desk with books visible behind them.</sample>
    <sample id="1245">NLPPositionality A framework for characterizing design biases in NLP datasets and models</sample>
    <sample id="1246">Framework

Collection
- 300 instances are sampled from a dataset.
- Each instance has an associated gold label.

Processing
- Sentences are sent as parts of a study on ITW.

- "Eating with Hands" vs. "Can you live its moral?"
- Bad: "Eating with Hands"

- "Eating with Hands"
- Ads from people around the world

Processing
- Model Predictions

Analysis
- The received annotations from the model are compared to the obtained labels from the models.

Analysis
- Age, Gender, Ethnicity, Education, Country

- Pearson's correlation is measured between each feature and the annotations for each separately.</sample>
    <sample id="1247">The image contains text as part of a framework diagram. Here is the transcription:

1) Re-annotate datasets with diverse annotators.

Collection:
- 300 instances are sampled from a dataset
- Each instance has an associated gold label
- Instances are sent on to a study on ITW

Processing:
- Model predictions
- The received annotations were compared against the gold labels from the obtained models

Analysis:
- Age: Gender, White, Black, Hispanic
- Ethnicity: Male, Female, Other
- Education: High School, College, Graduate
- Country: USA, Canada, Mexico

Parker's correlation is measured and annotated for each category separately.</sample>
    <sample id="1248">The image contains a detailed framework for annotating datasets. The main text reads "Framework" at the top, indicating an overview of a process or system.

Below this title, there are three labeled sections: Collection, Processing, and Annotate. Each section is accompanied by illustrations and descriptions:

1. **Collection**:
   - Three images depict different types of data being collected.
   - Text next to these images explains that 300 instances are sampled from a dataset with associated labels such as 'Eating with Hands' tagged as either Good or Bad.
   - It mentions that instances are sent out on a study about IT on the web.
   - Annotations from people around the world are shown in small boxes below each example.

2. **Processing**:
   - This section shows a neural network diagram connected to model predictions.
   - Below it, tables display various attributes like Age, Gender, Ethnicity, Education, Country, etc., along with their respective distributions (e.g., White, Black, Asian).
   - There's also a note explaining that Pearson’s correlation measures annotations between groups for each attribute separately.

3. **Annotate**:
   - A table illustrates how received annotations match gold labels obtained from the models.
   - Another part of the table indicates correlations measured for each annotation category.

On the right side of the image, outside the main content area, there is additional text stating: "1) Re-annotate datasets with diverse annotators." 

This suggests that one step in the framework involves re-annotating datasets using a variety of annotators to ensure diversity in labeling.</sample>
    <sample id="1249">The image shows a detailed framework for annotating datasets. It includes three main sections: Collection, Processing, and Annotations.

1. **Collection**:
   - A database icon with text indicating that 300 instances are sampled from a diverse dataset.
   - Examples of annotated images include "Eating with Hands" labeled as 'Good' or 'Bad', and another example asking if an action is moral ("Can you have fun if it's immoral?").

2. **Processing**:
   - An illustration showing model predictions being compared to the received annotations.
   - The table below this section lists attributes such as Age (Gender), Ethnicity (Black, White, Hispanic), Education (High School, College, Postgraduate), and Country (USA, Canada).

3. **Annotations**:
   - Text stating that the received annotations were obtained from people around the world.
   - A bar chart comparing Pearson’s correlation between model predictions and human annotations across different categories like Gender, Ethnicity, Education, and Country.

Additionally, there is a note on the right side emphasizing the need to re-annotate datasets with diverse annotators.</sample>
    <sample id="1250">The image contains a detailed framework diagram with text and visual elements. Here is the transcription of the visible text in reading order, including structure:

Title: Framework

Collection:
- 100 instances are sampled from a dataset
- Each instance has an associated gold label
- Annotators annotate images using labels like "Eating with Hands", "Good", or "Bad"
- Annotations from people around the world

Processing:
- Model Predictions (indicated by arrows pointing to model predictions)

Analysis:
- Age | Gender | Ethnicity | Education | Country
- White | Black | Asian | High School | College
- 25-34 years old | 35-45 years old 
- Pearson's correlation is computed between annotations for each demographic group 

Body Text:
- The received annotations were sent out as part of a study on UTTW
- The received annotations include both human and automated labels.

Side Note:
- Compare annotations by demographic to models and datasets via Pearson’s R scores.

This structured breakdown captures all identifiable content within the provided image.</sample>
    <sample id="1251">The image shows a detailed flowchart titled "Framework" that outlines the process of collecting, processing, and analyzing data. The framework is divided into three main sections: Collection, Processing, and Analysis.

1. **Collection**:
   - This section includes images with text such as "Eating with Hands," "Good," "Bad," and arrows indicating the direction of information flow.
   - There are descriptions like "100 instances are sampled from a dataset," "Each instance has an associated gold label," and "Annotations from people around the world."

2. **Processing**:
   - Model Predictions are indicated by blue arrows pointing to a box labeled "Model Predictions."
   - Text within this section mentions "The received annotations were sent in response to a study on UTTW."

3. **Analysis**:
   - A table at the bottom categorizes demographic variables including Age (18-24 vs 25-45), Gender (Male vs Female), Ethnicity (White vs Black vs Asian), Education (High School vs College vs Graduate), and Country (USA vs Other).
   - It also contains a note about Pearson's correlation coefficients for each variable.

On the right side of the image, there is additional text stating: "2) Compare annotations by demographic to models and datasets via Pearson’s R scores." 

In the top-right corner, there is a small video feed showing someone speaking or presenting.</sample>
    <sample id="1252">LabintheWild</sample>
    <sample id="1253">LabintheWild Pool of diverse volunteers / research participants Online experiment from researchers</sample>
    <sample id="1254">The video presents a slide titled "Task A: Social Acceptability," which appears to be part of an instructional or educational presentation. The content is divided into three main sections, each with specific instructions and elements:

1. **Read the situation**: This section shows text that reads "Wanting to make a lot of money." Below this text, there are options for participants to rate how socially acceptable they think it is on a scale from Very Bad to Good.

2. **Enter what you think about it**: Participants are asked to provide their personal judgment based on the given scenario. There's also a note indicating that free text explanations can be provided if necessary.

3. **See what an AI and others thought about it**: In this section, participants will see study results in the United States regarding social acceptability ratings by humans (labeled as 'Human') compared to those made by an AI model ('AI'). 

Additional details include:
- An image of a robot labeled "The AI speculates" suggesting its role in providing insights.
- Text explaining that participants read a situation from the Social Chemistry dataset.
- A bar graph showing comparative data between human and AI assessments of social acceptability.

Throughout the clip, these components remain consistent, emphasizing the process of evaluating social acceptability through both individual judgments and AI evaluations.</sample>
    <sample id="1255">Task A: Social Acceptability

1) Read the situation.
2) Enter what you think about it.
3) See what an AI and others thought about it.

Wanting to make a lot of money. Participants compare their responses to others' and AI's.</sample>
    <sample id="1256">Task A: Social Acceptability Analysis Datasets - Social Chemistry Models - Delphi - GPT-4</sample>
    <sample id="1257">Task B: Toxicity

1) Read the example
Participants read an instance from the Dynathe dataset.
English people smell like sour milk

2) What do you think about it?
Participants rate whether they think an instance is hate speech.

3) See what others thought about it!
The AI speculates:
I speculate that this statement is hurtful because...
Study participants in Afghanistan said:

[Graph showing percentages of responses]
Not Harmful, Somewhat Harmful, Very Harmful</sample>
    <sample id="1258">The video begins with a slide titled 'Task B: Toxicity Analysis.' It lists datasets and models used in the analysis, including 'Datasets - Dynahate' and 'Models - Perspective API,' 'Rewire API,' 'Hate RoBERTa,' and 'GPT-4.' In the top right corner of this slide, there is an image of a person. The scene then transitions to another slide titled 'Study Participation.' This new slide shows statistics related to study participation, displaying '16,299 annotations' on the left side and '1,096 annotators' on the right side. A small image of a person appears again in the top right corner.

The next part of the video continues from where it left off, maintaining the title 'Study Participation' at the top. Below this heading, two sets of numbers are displayed prominently: '16,299 annotations' on the left side and '1,096 annotators' on the right side. Additionally, a third set of numbers appears below these figures, reading '87 countries.' On the right side of the frame, as before, there is a small image of a person.</sample>
    <sample id="1259">Results Who do NLP datasets and models align with? Finding 1: There is positionality in NLP.</sample>
    <sample id="1260">The image shows a bar chart titled "Social Acceptability (GPT-4)" with various categories listed on the x-axis: African Islamic, Baltic, Catholic Europe, Confucian, English-Speaking, Latin America, Orthodox Europe, Protestant Europe, and West South Asia. The y-axis represents social acceptability scores ranging from 0 to 1. Each category has an associated bar indicating its score:

- African Islamic: N=316, Score: 0.47
- Baltic: N=140, Score: 0.71
- Catholic Europe: N=452, Score: 0.68
- Confucian: N=528, Score: 0.74
- English-Speaking: N=8,289, Score: 0.74
- Latin America: N=281, Score: 0.47
- Orthodox Europe: N=426, Score: 0.67
- Protestant Europe: N=706, Score: 0.47
- West South Asia: N=413, Score: 0.59

Additionally, there is text stating "Datasets and models are most aligned to English-Speaking countries." In the top right corner of the image, there is a small inset showing a person in what appears to be a classroom or office setting.

Below this section, another bar chart titled "Hate Speech &amp; Toxicity (Dynahate)" displays similar data for different regions. However, specific details about these charts were not provided in the description.</sample>
    <sample id="1261">The image shows a bar graph titled "Social Acceptability (GPT-4)" with data comparing the social acceptability of datasets and models across different educational levels. The x-axis lists various education categories: College, Graduate School, High School, PhD, Pre-High School, and Prof. School. Each category has corresponding bars representing numerical values and percentages.

On the left side of the graph, there is text stating, "Datasets and models are most aligned to people with a college education." 

In the top right corner of the image, there is a small inset showing a person in what appears to be an office setting, possibly indicating that this slide was part of a presentation or lecture.

The background color of the slide is white, which helps highlight the content clearly.</sample>
    <sample id="1262">The image contains a bar chart titled "Hate Speech &amp; Toxicity (Dynahate)" with the y-axis labeled from 0.0 to 0.8, indicating some form of measurement or scale related to hate speech and toxicity levels across different educational qualifications. The x-axis lists various education levels: College, Graduate School, High School, PhD, Pre-High School, and Prof. School.

Each bar represents data for a specific education level:
- College: N=2383
- Graduate School: N=604
- High School: N=908
- PhD: N=359
- Pre-High School: N=116
- Prof. School: N=195

The bars are colored in shades of blue and gray, each marked with an asterisk (*) next to their respective values which seem to represent percentages or scores associated with each education level.
 
To the left of the chart is text that reads: "Datasets and models are most aligned to people with a college education." This suggests a commentary on how well datasets and models correlate with individuals who have completed college education compared to others listed.

In the top right corner of the image, there's a small inset showing a person wearing glasses, likely presenting this information.</sample>
    <sample id="1263">The content of the image is as follows: "Finding 2: Some populations are left behind." This text appears to be a statement or conclusion from some form of research, report, or presentation. It suggests that certain groups within society may not have access to resources or opportunities equally compared to others.</sample>
    <sample id="1264">Social Acceptability (GPT-4) Man Non-binary Woman Datasets and models are less aligned to non-binary people. 0.8 0.6 0.4 0.2 N=4,082 N=858 N=4,368 * 0.73* 0.69* 0.55* 0.61* 0.63*</sample>
    <sample id="1265">So, what can we do? Addressing positionality in NLP</sample>
    <sample id="1266">The video presents a slide titled 'Recommendations' in bold text at the top. Below this title, there is an initial recommendation: "1. Keep a record of all relevant design choices made throughout building datasets or models." This suggests that maintaining documentation of decisions during data model development is crucial.

Following this, another point appears under the heading: "2. Do NLP research through the lens of perspectivism:" which introduces a new topic related to Natural Language Processing (NLP) and its approach based on perspectives.

Further elaboration follows with sub-point "a. Share disaggregated dataset labels!" indicating advice for sharing detailed and diverse labels within datasets used in NLP tasks.

In the bottom left corner of each frame, there is a reference link: "[1] https://www.masakhane.io," likely providing additional information or context about the recommendations presented.

Throughout the frames, these elements remain consistent, emphasizing the importance of documenting design choices and adopting a perspective-based approach in NLP research while ensuring shared, detailed dataset labels are utilized.</sample>
    <sample id="1267">The image contains text under the heading "Recommendations" with three main points: 1. Keep a record of all relevant design choices made throughout building datasets or models. 2. Do NLP research through the lens of perspectivism: - Share disaggregated dataset labels - Use modeling techniques that can handle annotator disagreement. 3. Building specialized datasets and models with and for specific communities is valuable for inclusive NLP (e.g., Masakhane initiative¹). At the bottom, there's a URL link to [https://www.masakhane.io](https://www.masakhane.io).</sample>
    <sample id="1268">Thanks Dashboard Link: nlpositionality.cs.washington.edu/ Paper: bit.ly/NLPositionality-Paper/</sample>
    <sample id="1269">To generate all possible permutations of the tokens in the output sequence.</sample>
    <sample id="1270">The authors recommended that model owners should increase transparency about bias mitigation methods to ensure fairness and trust in the models. This recommendation is part of a broader set of guidelines aimed at addressing stereotypes, using an intersectional lens, and improving overall inclusivity in AI systems.</sample>
    <sample id="1271">Minimal-pair unacceptable inputs are sentences that contain grammatical errors or do not make sense. In the context of this image, which discusses a Minimal Pair Paradigm for evaluating language models (LMs), examples provided include: 1. "No customer ... has spent any money." - This sentence is grammatically incomplete and does not form a coherent thought. 2. "Many people were helping herself." - This sentence contains an improper pronoun usage ("helping herself" instead of "helping themselves"). These types of minimal-pair unacceptable inputs help in assessing how well LMs can handle linguistic nuances and avoid common mistakes when generating text.</sample>
    <sample id="1272">The evaluation metrics used by the authors include NER (Named Entity Recognition), CER (Character Error Rate), and F1 scores for different categories such as Person, Organization, Location, and Product. Additionally, they evaluated performance on specific tasks like Medical Special Report, MedicusMed, and QuaeroMedQA.</sample>
    <sample id="1273">The metric used for measuring inter-annotator agreement was Krippendorf's Alpha.</sample>
    <sample id="1274">The domain chosen to add completely unrelated sentences was Wikipedia.</sample>
    <sample id="1275">The authors of the paper are affiliated with Heinrich Heine University Düsseldorf, Germany.</sample>
    <sample id="1276">MultiInstruct is unique because it focuses on language-only tasks and uses instruction tuning with multimodal pre-trained models. It highlights the imbalance between NLP (Natural Language Processing) datasets and multimodal ones, noting that there are over 1600+ language-only task instructions but no large-scale publicly-available multimodal instructional data sets available for training or evaluation purposes.</sample>
    <sample id="1277">The paper involves three authors: Sarah E. Finch, James D. Finch, and Jinho D. Choi.</sample>
    <sample id="1278">Binary coordination refers to the situation where two elements or entities are connected in a way that they function as one unit, often characterized by complementary roles or functions. This concept is illustrated through various graphs and figures showing relationships between different variables such as character length, syllable count, word length, and absolute difference in length. The visual data suggests how these binary coordinated pairs interact with each other based on specific conditions like no governor (character length) versus having a governor (syllable count).</sample>
    <sample id="1279">The prompts used in this study were, on average, 10 words long.</sample>
    <sample id="1280">The findings suggest that the smaller T5 model, which was fine-tuned on Coscript, can generate higher quality scripts compared to larger language models (LLMs). This implies that with appropriate training and data, even smaller models can achieve performance levels comparable to or better than those of much larger models. The specific advantage lies in the ability of these smaller models to focus more precisely on relevant information for task-specific outputs like script generation.</sample>
    <sample id="1309">The slide titled "Comparison of pre-training strategies and data sources" investigates two main learning strategies: 1. Pre-training from scratch with full model construction using a dataset called NACHOS, which includes 20M words crawled from various medical domains like PubMed, Medline, and Google Scholar. 2. Continual pre-training by leveraging an existing pre-trained model (CameBERT) to fine-tune on specific tasks such as clinical question answering, disease diagnosis, drug side effect prediction, and adverse event detection in the medical domain.</sample>
    <sample id="1310">The slide titled 'What Causes Performance Drop?' discusses the factors contributing to performance decline in machine learning models. The first bullet point, 'Adaptive overfitting?', suggests that adaptive overfitting could be a cause of performance drop. This is further elaborated with sub-points: - No diminishing returns (indicating that improvements continue without plateauing) - Not observed (implying this factor has not been directly seen or measured). Additionally, there's another bullet point about 'Temporal drift?' which likely refers to changes in data patterns over time affecting model performance. A graph on the right side shows various lines and points representing different datasets or experiments, possibly illustrating how these concepts affect model accuracy over time.</sample>
    <sample id="1311">The quality of the simplification was evaluated using metrics such as BLEU and F1 scores, which are displayed in a table format. The results show that the performance varies across different tests (DEPLAIN-APA test, DEPLAIN-APB test) for both document-level and sentence-level simplifications.</sample>
    <sample id="1312">Yes, language models can have different political biases.</sample>
    <sample id="1347">Cognitive dissonance is described as "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" according to the content shown in the image. This definition comes from Harmon-Jones and Harmon-Jones' 2007 work on cognitive dissonance theory after its development over fifty years prior.</sample>
    <sample id="1348">BERT-base is the most liberal language model.</sample>
    <sample id="1349">Yes, cumulative training performs better than iterative when doing active learning.</sample>
    <sample id="1350">The speaker's name is Sara Papi.</sample>
    <sample id="1351">The data in the MuDa benchmark was taken from TED Talks.</sample>
    <sample id="1352">Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination Adam Przeździeckiowski and Michał Woźniak INSTITUTE OF COMPUTER SCIENCE POLISH ACADEMY OF SCIENCES ul. Jana Kazimierza 5, 01-248 Warsaw UNIVERSITY OF WARSAW ACL 2023</sample>
    <sample id="1353">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1354">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1355">The image shows a presentation slide titled "Dependency Structure of Coordination." The title is in blue text at the top. Below it, there are four bullet points labeled as follows: 1. Bouquet/Stanford (Universal Dependencies): - This section includes an example sentence with dependency arrows pointing to each word: "Homer loves Lisa, Bart, and Maggie." 2. Chain/Moscow: - Similar to the first point, this also has an example sentence with dependency arrows: "Homer loves Lisa, Bart, and Maggie." 3. Conjunction-headed/Prague: - Another example sentence with dependency arrows: "Homer loves Lisa, Bart, and Maggie." 4. Multi-headed/London: - Like the previous examples, this one also features an example sentence with dependency arrows: "Homer loves Lisa, Bart, and Maggie." Each example sentence demonstrates how different syntactic structures represent coordination between elements within the phrase. In the upper right corner of the slide, there's a small video feed showing a person speaking into a microphone.</sample>
    <sample id="1356">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1357">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1358">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1359">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1360">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1361">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read it yesterday. bad</sample>
    <sample id="1362">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1363">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1364">The image is a slide from a presentation about Dependency Length Minimization (DLM). The title at the top reads "Dependency Length Minimization (DLM)." Below the title, there's a section that says "Word order tends to minimize dependency lengths:" followed by two examples. Each example shows a sentence with its corresponding syntactic tree diagram above it.

The first example has the sentence "Marge read it yesterday." and its tree diagram labeled as "good" in green text on the right side of the slide. This indicates that this word order minimizes dependency length according to DLM principles.

The second example also has the sentence "Marge read it yesterday." but its tree diagram is labeled as "bad" in red text on the right side of the slide. This suggests that this word order does not follow the principle of minimizing dependency length under DLM.

Below these examples, there are more detailed diagrams showing different arrangements of words like "this," "absolutely fascinating book about bees," etc., each accompanied by their respective labels indicating whether they are good or bad for minimizing dependency lengths.

On the right side of the slide, partially visible, is an individual who appears to be presenting or discussing the content shown on the slide.</sample>
    <sample id="1365">The image shows a slide from a presentation about Dependency Length Minimization (DLM). The title "Dependency Length Minimization (DLM)" is prominently displayed at the top of the slide. Below the title, there's an explanation that reads: "Word order tends to minimize dependency lengths:". Under this statement, two examples are provided:

1. A simple sentence structure with words arranged in such a way as to create short dependencies between them:
   - Marge read it yesterday.
   - This example has minimal connections and is labeled 'good' on the right side.

2. Another arrangement where the word order creates longer dependencies:
   - Marge read this absolutely fascinating book about bees yesterday.
   - In this case, the increased number of connections results in a label 'bad' next to it.

At the bottom of the slide, another similar pair of sentences appears but without any labels indicating their quality or correctness regarding DLM principles.

On the left-hand side of the slide, there seems to be some text or possibly code snippets, though they're not clearly visible due to resolution issues. Additionally, part of what looks like a person’s face can be seen in the upper-right corner, suggesting someone might be presenting or interacting with the content during the recording of this educational material.</sample>
    <sample id="1366">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read it yesterday. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read this absolutely fascinating book about bees yesterday. good</sample>
    <sample id="1367">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read it yesterday. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read this absolutely fascinating book about bees yesterday. good</sample>
    <sample id="1368">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1369">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read it yesterday. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read this absolutely fascinating book about bees yesterday. good</sample>
    <sample id="1370">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1371">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read it yesterday. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read this absolutely fascinating book about bees yesterday. bad</sample>
    <sample id="1372">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016) left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88-90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="1373">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before) this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88–90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="1374">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): • left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88–90) • but only when the governor is on the left or absent (I saw Bart and Lisa: Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="1375">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88-90) but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="1376">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): • left conjuncts tend to be shorter (observed before), \_o, • this tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88–90), • but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), • not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="1377">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): • left conjuncts tend to be shorter (observed before), • this tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88–90), • but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), • not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="1378">The image shows a slide from a presentation titled "Conjunct Lengths in English." The content of the slide discusses statistics about coordination extracted from an enhanced version of the Penn Treebank, referencing works by Marcus et al. (1993), Ficler and Goldberg (1996). It states that left conjuncts tend to be shorter than right conjuncts, which is observed before mentioned length difference becomes significant. Additionally, it notes that this tendency grows with length difference, briefly noticed in Gibson et al.'s work from 1996:88-90.

The slide provides examples illustrating these points:
- An example sentence given is "I saw Bart and Lisa; Homer came and sneezed."
- Another example provided is "not when it is on the right (Ted and Ned laughed)."

The text appears to explain linguistic phenomena related to the structure and coordination of sentences in English, particularly focusing on how different parts of a sentence may vary in their lengths or prominence.</sample>
    <sample id="1379">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): • left conjuncts tend to be shorter (observed before), • this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88–90), • but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), • not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="1380">The image shows a detailed graph with multiple plots, each representing different conditions related to governor length in characters, syllables, and words. The x-axis of the graphs is labeled "Absolute difference in conic length (mm)," ranging from 0 to 25 mm. Each plot has a blue line indicating some form of relationship or trend.

The top row includes three plots:
1. Title: "No governor (length in CHARACTERS)"
2. Title: "No governor (length in SYLLABLES)"
3. Title: "No governor (length in WORDS)"

The middle row also contains three plots:
1. Title: "Governor on the LEFT (length in CHARACTERS)"
2. Title: "Governor on the LEFT (length in SYLLABLES)"
3. Title: "Governor on the LEFT (length in WORDS)"

The bottom row again consists of three plots:
1. Title: "Governor on the RIGHT (length in CHARACTERS)"
2. Title: "Governor on the RIGHT (length in SYLLABLES)"
3. Title: "Governor on the RIGHT (length in WORDS)"

Each title indicates that these are variations where either no governor is present or governors are placed at the left or right ends of the structure being measured, but their lengths vary between characters, syllables, and words.

Below the plots, there is a caption that reads: "Figure 1: Proportions of shorter left conical contacts depending on the absolute difference of conic length (with confidence bands)." This suggests that the figures illustrate how changes in conic length affect certain proportions, particularly focusing on shorter left conical contacts, while accounting for varying levels of confidence as indicated by the bands around the lines.

In the upper right corner of the image, there is a small inset showing a person's face, likely the presenter or someone associated with the content of the presentation slide.</sample>
    <sample id="1381">The image shows a series of nine line graphs arranged in three rows and three columns. Each graph is labeled with different conditions related to the length of characters, syllables, or words on both the x-axis (left) and y-axis (right). The labels for each row are as follows: 1st row - "NO governor (length in CHARACTERS)", "NO governor (length in SYLLABLES)", "NO governor (length in WORDS)". 2nd row - "Governor on the LEFT (length in CHARACTERS)", "Governor on the LEFT (length in SYLLABLES)", "Governor on the LEFT (length in WORDS)". 3rd row - "Governor on the RIGHT (length in CHARACTERS)", "Governor on the RIGHT (length in SYLLABLES)", "Governor on the RIGHT (length in WORDS)". All lines show an upward trend from left to right.

Below the graphs, there is a caption that reads: "Figure 1: Proportions of shorter left-justified contracts depending on the absolute difference of contract lengths (with confidence bands)."</sample>
    <sample id="1382">The image shows a series of nine line graphs arranged in a 3x3 grid, each representing different conditions related to 'No governor' and varying lengths measured in characters, syllables, or words. The x-axis across all graphs is labeled "Absolute difference in length," while the y-axis varies depending on the specific condition: for example, one graph has "Proportion of shorter left/central contacts" as its y-axis label.

Each graph plots data points that form straight lines indicating positive correlations between the variables being compared. The top-right corner features an inset with a person's face blurred out, likely included by accident during screen recording. At the bottom center of the image, there is a caption that reads:

"Figure 1: Proportions of shorter left/central contacts depending on the absolute difference of contact length (with confidence bands)"

This figure appears to be from a scientific study analyzing linguistic or phonetic phenomena based on visual representations of statistical data.</sample>
    <sample id="1383">Compatibility with Dependency Structures of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. NO Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie. YES</sample>
    <sample id="1384">The video features a presentation slide with a white background and black text. The main message on the slide reads, "See the paper for the full argument!" in bold letters. Below this, there is an additional line of text that says, "Talk to us at the poster session!" which appears slightly faded or lighter than the main text. In the top right corner of each frame, there is a small thumbnail image of a person wearing glasses, likely indicating their identity as the presenter. Throughout the sequence, no changes occur; the content remains consistent across all frames.</sample>
    <sample id="1385">The speaker's name is not provided in the image. The slide lists three authors: Matthias Lindemann, Alexander Koller, and Ivan Titov, but it does not specify which one of them might be speaking at this moment.</sample>
    <sample id="1386">Cross-lingual transfer refers to the process of training a model on one language and then transferring that knowledge to another language. This technique is part of machine learning experiments where models are tested for their ability to generalize across different languages, which can be crucial in multilingual applications or when dealing with limited data in certain languages.</sample>
    <sample id="1387">The authors of the paper are affiliated with Saarland University, Amazon Alexa, and the University of Vienna.</sample>
    <sample id="1388">The authors use BLEU as the quality measure and AL/AL_CA (s) as a latency measure.</sample>
    <sample id="1416">The drawbacks of tree-based methods are that trees need to be obtained through pre/post-processing logical forms and grammar induction.</sample>
    <sample id="1417">The authors of the paper are affiliated with Georgia Institute of Technology, specifically within the School of Interactive Computing.</sample>
    <sample id="1495">ABC-Eval stands for "Annotating Behaviors in Chat." This is evident from the title of the presentation slide shown, which explicitly states this full form. The acronym ABC-Eval likely breaks down as follows: A for Annotating, B for Behaviors, and C for Chat. These elements are visually represented by icons on the slide that depict people chatting (the 'C' for chat) and various annotations or evaluations being made about those conversations (the 'A' and 'B').</sample>
    <sample id="1496">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until the year 2014.</sample>
    <sample id="1527">The authors of the paper are affiliated with several institutions. The affiliations shown in the image include: 1. The University of Amsterdam (represented by its logo) 2. Saarland University (also represented by its logo) 3. UCA, which likely stands for a university or research center (logo is not clearly visible but present) Additionally, there's an affiliation mentioned at the bottom left that appears to be related to informatics and NLP (Natural Language Processing), though the specific institution name isn't clear from the provided description. These logos indicate the academic or research organizations associated with the authors' work on "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations."</sample>
    <sample id="1528">The name of the speaker is Siyu Yuan.</sample>
    <sample id="1529">There are five authors involved in the paper. The names listed on the slide include Patrick Fernandes, Kayo Yin, Emmy Liu, André F.T. Martins, and Graham Neubig.</sample>
    <sample id="1530">The approach is compared to the state-of-the-art architecture specifically tailored for SimulST.</sample>
    <sample id="1531">MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning Zhiyang Xu* Ying Shen* Lifu Huang Department of Computer Science, Virginia Tech *Equal Contribution</sample>
    <sample id="1532">Pre-trained Language Models for Downstream Tasks</sample>
    <sample id="1533">Pre-trained Language Models for Downstream Tasks</sample>
    <sample id="1534">The video features a consistent visual theme throughout its duration. The background is black, and in the center of each frame, there is white text that reads "Language-only." In the bottom right corner of every frame, there is an image of a person wearing glasses and a dark jacket over a light-colored shirt. This static presentation suggests a focus on language or communication without any additional elements or changes occurring during the sequence.</sample>
    <sample id="1535">Instruction Tuning on Multimodal Pre-trained Models</sample>
    <sample id="1536">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="1537">Imbalance in Instructional Datasets between NLP and Multimodal 1600+ Language-only instruction tasks NO large-scale, publicly-available multimodal instruction tasks Wang, Yizhong, et al. "Benchmarking generalization via in-context instructions on 1600+ language tasks" arXiv preprint arXiv:2307.05498</sample>
    <sample id="1538">MULTINSTRUCT The first multimodal instruction tuning benchmark dataset 62 diverse multimodal tasks 10 broad groups 5 expert-written instructions Figure 2: Task Groups Included in Multinstruct. The yellow boxes represent tasks used for evaluation, the white boxes included tasks used for training.</sample>
    <sample id="1539">MULTINSTRUCT The first multimodal instruction tuning benchmark dataset 62 diverse multimodal tasks 10 broad groups 5 expert-written instructions Figure 2: Task Groups Included in Multinstruct. The yellow boxes represent tasks used for evaluation, the white ones indicate test sets.</sample>
    <sample id="1540">OFA (One For All) A unified multi-modal pre-trained model that is capable of performing both understanding and generation tasks with single or multiple modalities. OFA has a unified vocabulary for language, image tokens and the coordinates of a bounding box.</sample>
    <sample id="1541">MULTINSTRUCT Figure 1: Example Instances from MULTINSTRUCT for Four Tasks.</sample>
    <sample id="1542">MULTINSTRUCT Figure 1: Example Instances from MULTINSTRUCT for Four Tasks.</sample>
    <sample id="1543">MULTINSTRUCT Figure 1: Example Instances from MULTINSTRUCT for Four Tasks. Grounded Caption Input: Generate a caption for the image. Output: A man and white tennis racquet Text Localization Input: Select the region that contains the text "don". Options: &lt;206, 119&gt;, &lt;208, 145&gt;, &lt;237, 148&gt;, &lt;218, 151&gt;, &lt;224, 161&gt;, &lt;231, 163&gt;, &lt;222, 165&gt;, &lt;226, 167&gt;, &lt;221, 171&gt;, &lt;228, 172&gt; Referring Expression Selection Input: Select the object of the region described by "A blue train in front of the station". Options: &lt;243, 181&gt;, &lt;243, 183&gt;, &lt;243, 184&gt;, &lt;243, 186&gt;, &lt;243, 187&gt;, &lt;243, 189&gt;, &lt;243, 190&gt;, &lt;243, 192&gt;, &lt;243, 193&gt;, &lt;243, 195&gt; Question-Image Matching Input: Given the context of the image, do you have enough information to answer the question? "Is it a sunny day?" The options are "The question is irrelevant" or "The image is relevant." Output: The question is irrelevant</sample>
    <sample id="1544">The text in the image says "Multi-modal Instruction Tuning."</sample>
    <sample id="1545">Multi-Modal Instruction Turning Training Dataset Construction: Use 53 tasks from 9 groups for training. Sample 10,000 instances per task. Testing Dataset Construction: Reserve the entire Commonsense Reasoning group for testing. Select additional 5 tasks from VQA and Miscellaneous groups. We use all the instances in the test split for each task. Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="1546">Multi-Modal Instruction Turning Training Dataset Construction: Use 53 tasks from 9 groups for training. Sample 10,000 instances per task. Testing Dataset Construction: Reserve the entire Commonsense Reasoning group for testing. Select additional 5 tasks from VQA and Miscellaneous groups. We use all the instances in the test split for each task. Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="1547">Implementation Details Training details: Pre-trained OFA-Large model (472M) Mix all the instances for all tasks. Each instance is randomly combined with one of its five instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1548">Implementation Details Training details: Pre-trained OFA-Large model (472M) Mix all the instances for all tasks. Each instance is randomly combined with one of its five instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1549">The image contains text related to "Implementation Details" for a model or system. The content is organized into two main sections: Training details and Testing details.

### Training details:
- **Pre-trained OFA-Large model (472M)**
- Mix all the instances for all tasks.
- Each instance is randomly combined with one of its five instruction templates.

### Testing details:
- For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.
- We report the mean and maximum performance and the standard deviation of the performance across all five experiments.

This structured presentation provides specific information about how the training and testing processes are carried out for this particular implementation.</sample>
    <sample id="1550">The image shows a slide from a presentation titled "Evaluation Metrics." The background is black with white text, and there are four bullet points detailing different evaluation metrics for various tasks. Here's the content of each bullet point:

1. For multi-modal classification tasks (Visual Entailment, Visual Spatial Reasoning, Natural Language Visual Reasoning, and Disaster Type Classification), we report the Accuracy.
2. For multi-modal generation tasks (Commonsense VQA, Text VQA, Grounded VQA, Visual Text Extraction, and Visual Dialogue), we report the Rouge-L.
3. For NLP tasks, we report Rouge-L.
4. We also compute the aggregated performance for each model based on the mean of the model’s performance on all multimodal and NLP unseen tasks. We use Rouge-L as the performance score for most tasks, and Accuracy only has accuracy as a metric.

In the bottom right corner of the slide, there is an inset picture of a person who appears to be presenting or related to the content being discussed in the slide.</sample>
    <sample id="1551">The image is a slide from a presentation titled 'Sensitivity.' It discusses how sensitive the model is towards various instructions for the same task. The text on the slide reads: 'How sensitive the model is towards variety of instructions for the same task: - Ability to consistently produce the same results for the same task, regardless of slight variations in the wording of instructions.' Below this text, there are two mathematical expressions involving integrals and expectations over different sets T and D1. In the bottom right corner of the slide, there is an inset photo of a person wearing glasses and a light-colored shirt.</sample>
    <sample id="1552">Effectiveness of Instruction Tuning on MULTIINSTRUCT</sample>
    <sample id="1553">Effectiveness of Instruction Tuning on MULTIINSTRUCT

Table 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold.

Table 2: Zero-shot Performance on Question Answering and Miscellaneous. The best performance is in bold.</sample>
    <sample id="1554">Impact of Increasing Multimodal Task Clusters

- Img Und
  - VQA + Image Understanding

- Grounding
  - Grounded Matching + Grounded Generation

- MISC, ITM
  - Temporal Ordering + Miscellaneous + Image Text Matching

- Relation
  - Visual Relationship

- Region
  - Region Understanding

- NLP
  - NLP tasks

Figure 3: Model Performance as the number of multimodal Instruction Task Clusters increase</sample>
    <sample id="1555">Effect of Diverse Instructions on Instruction Tuning OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity. # of Instructions Aggregated Performance ↑ Sensitivity ↓ 1 Instruction 42.81 24.62 5 Instructions 47.82 10.45 Table 3: Effect of Different Number of Instructions. Performance of OFA-Mainfinet tuned on different numbers of instructions.</sample>
    <sample id="1556">Effect of Fine-tuning Strategies on Model Sensitivity Instruction tuning on Multilinstruct can significantly reduce the sensitivity of OFA. Transfer learning from Natural Instructions dataset can further reduce the sensitivity of the model. Figure 4: Model Sensitivity on Unseen Evaluation Tasks. Lower is better.</sample>
    <sample id="1557">Zero-Shot Performance on NLP Tasks Instruction Tuning on Multinstruct can improve zero-shot performance on unseen NLP tasks. The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset. Model | RougeL OFA 2.25 OFA\multinstruct 12.18 Transfer Learning from NATURAL INSTRUCTIONS OFA\multinstruct 43.46 OFA\natural instruct 43.46 OFA\segment 30.79 Table 4: Zero-Shot Performance on NLP tasks. The performance is reported in Rouge-L and the best performance is in bold.</sample>
    <sample id="1558">The image contains a slide with the title "Conclusion" at the top. Below this, there are five bullet points summarizing key takeaways from what appears to be an academic or professional presentation:

1. First large-scale multi-modal instruction tuning dataset.
   - Contains 62 multi-modal tasks from 10 broad categories.

2. Significantly improve the zero-shot capability of OFA via instruction tuning.

3. Explore several transferring learning techniques and show their benefits.

4. Design a new metric sensitivity.

In addition to the text content on the main part of the slide, in the bottom right corner, there is a small rectangular section showing a person's face, likely indicating that they might have been presenting during the time this screenshot was taken. The overall layout suggests it’s the concluding slide of a larger presentation focused on machine learning or artificial intelligence topics.</sample>
    <sample id="1559">One More Thing We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon</sample>
  </task>
</testset>