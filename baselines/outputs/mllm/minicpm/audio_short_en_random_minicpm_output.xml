<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="0">Large-scale web-crawled data, political news media</sample>
    <sample id="1">McGill University, Mila and Microsoft Research.</sample>
    <sample id="35">The speaker's name is Kaiyuan.</sample>
    <sample id="36">The T5-XLARGE model.</sample>
    <sample id="37">Yes, CoNLL-2003 taggers still work.</sample>
    <sample id="38">The novelty of the proposed human evaluation method is that it attempts to reduce subjectivity by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="39">The success of the existing weakly supervised approach heavily relies on clean validation samples.</sample>
    <sample id="40">They don't necessarily know about the entity.</sample>
    <sample id="41">There are 5 authors involved in the paper.</sample>
    <sample id="75">There are two authors involved in the paper.</sample>
    <sample id="76">The Bible texts are much stronger simplified than the news text or language learner texts.</sample>
    <sample id="77">The example of the preference for shorter left conjuncts is "salt and pepper" versus "not pepper and salt".</sample>
    <sample id="78">Yes, you can use the models for your research.</sample>
    <sample id="79">DEplain-apa is based on news texts.</sample>
    <sample id="80">A better model architecture, larger model size as well as more fine-tuning examples.</sample>
    <sample id="81">By measuring length in characters, syllables and words.</sample>
    <sample id="82">The experiments were designed to study the effect of the governor's position by measuring length in characters, syllables, and words.</sample>
    <sample id="83">Not much better than chance.</sample>
    <sample id="84">The paper has 4 authors: Xie, Zhang, Liu, and Wang.</sample>
    <sample id="85">Bob and Alice</sample>
    <sample id="86">Formality and lexical cohesion</sample>
    <sample id="87">John Bock here, Aaron Mueller, Kanishka Mishra, Karen Fintel, Roger Levy and Athena Villoid.</sample>
    <sample id="88">Hi, my name is Matthias Lindemann and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations.</sample>
    <sample id="89">This is joint work with my advisors Alexander Colyer and Ivan Titov</sample>
    <sample id="90">Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="91">In the context of semantic parsing, testing for compositional generalization might look like this: As usual we have a training set of utterances in this case The girl slept and Mary knew that the girl slept.</sample>
    <sample id="92">These utterances are paired with logical forms that represent core aspects of their meaning.</sample>
    <sample id="93">In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms.</sample>
    <sample id="94">In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion.</sample>
    <sample id="95">Naive sequence-to-sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input.</sample>
    <sample id="96">In particular, they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the examples.</sample>
    <sample id="97">A popular method to address this is to integrate trees into the models.</sample>
    <sample id="98">The trees are intended to capture the compositional process that relates utterances with the logical forms</sample>
    <sample id="99">This works well, but trees are usually not given and need to be obtained somehow</sample>
    <sample id="100">This can be complicated and sometimes a computationally expensive process typically this involves considerable formalism specific pre-processing of the logical forms for example to handle variable symbols</sample>
    <sample id="101">Obtaining trees may also involve specialized grammar induction procedures.</sample>
    <sample id="102">In this paper, we don't use trees and introduce a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output.</sample>
    <sample id="103">For the first time we show strong generalization to deeper recursion without relying on trees</sample>
    <sample id="104">Our approach predicts the output from the input in two steps.</sample>
    <sample id="105">First, we tag each input token with an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="106">After the first step, we have all the right tokens but they're not ordered.</sample>
    <sample id="107">That's why in the second step we use another model to predict a permutation to put them into the right order</sample>
    <sample id="108">We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations This makes our approach quite flexible and expressive</sample>
    <sample id="109">Conceptually, our permutation model works roughly like this:</sample>
    <sample id="110">We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one as highlighted in red</sample>
    <sample id="111">Then we jump to the next multi-set token to determine the second token in the output.</sample>
    <sample id="112">We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process</sample>
    <sample id="113">Until every token from the first stage has been visited exactly once.</sample>
    <sample id="114">To give you a teaser of the experimental results, here we compare our method with other treeless models on the cog benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion</sample>
    <sample id="115">Some other kinds of structural organization remain very challenging though.</sample>
    <sample id="116">In our paper, we solve a couple of interesting technical challenges.</sample>
    <sample id="117">First of all, the alignment between input and output is not given in the training data. As a consequence for a given token we don't know which multi-set it came from, which poses a challenge for training</sample>
    <sample id="118">In addition, sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent. We address this by inducing the alignment as part of the training</sample>
    <sample id="119">Our permutation method is very flexible, but it brings the challenge that finding the highest scoring permutation is NP hard. That's because this is related to the traveling salesman problem</sample>
    <sample id="120">We approximate this with a GPU-friendly continuous relaxation that also allows us to back propagate through the solution and learn the linguistically more plausible permutations.</sample>
    <sample id="121">If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.</sample>
    <sample id="122">The framework quantifies positionality by comparing the end user's predictions and labels to those of models and datasets, using a Pearson correlation score.</sample>
    <sample id="123">Hello, I am David, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work: Weaker than you think: A critical look at weakly supervised learning</sample>
    <sample id="124">This is joint work with Xiao Yu Shen, Mario Smousepath, G. S. Stefan and Ditish Klarke.</sample>
    <sample id="125">Sure, I'd be happy to help with that. To begin with a brief introduction to weak supervision and weakly supervised learning: Weak supervision is an approach in machine learning where the labels for training data are not fully accurate or complete but still provide some guidance on how to classify the data. This can include partial annotations, noisy labels, or even completely unlabeled data with only metadata available.

Weakly supervised learning (WSL) refers to algorithms designed specifically for working with such limited label information by leveraging additional sources of knowledge like features extracted from images or text, as well as external datasets containing relevant examples. The goal here is often to maximize performance despite having less precise labeling than traditional supervised methods would require.

In summary, weak supervision allows us to work more efficiently when dealing with large amounts of unlabelled data while still achieving good results through techniques such as transfer learning and semi-supervised approaches which utilize both labeled and unlabeled samples effectively.</sample>
    <sample id="126">In weak supervision, we do not manually label the data. Instead, we label the data using weak labeling sources such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right.</sample>
    <sample id="127">When compared to human annotations, the weak annotations are much cheaper. Yet they're also noisy meaning that a certain amount of the annotations are incorrect</sample>
    <sample id="128">If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize.</sample>
    <sample id="129">In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well.</sample>
    <sample id="130">In recent works in WSL, a common claim is that people say that they only train models on the weekly labeled data and achieve high performance on clean test sets.</sample>
    <sample id="131">Technically, this claim is not wrong, but there's a catch.</sample>
    <sample id="132">Which is that people do assume there's an additional clean validation set available for model selection</sample>
    <sample id="133">We can't stop on this problem setting, as this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room, this necessity is often overlooked.</sample>
    <sample id="134">The aforementioned doubt leads us to ask three research questions: First, is clean validation data necessary for WSL? Or can we maybe use a noisy validation set instead?</sample>
    <sample id="135">Second, if clean data is required or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="136">We addressed these research questions in our work, and our findings are as follows.</sample>
    <sample id="137">First, we find that interestingly recent WSL methods indeed require clean validation samples to work properly.</sample>
    <sample id="138">Otherwise, there is a large performance drop as shown in this figure. If there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels.</sample>
    <sample id="139">Meaning that the training is pointless.</sample>
    <sample id="140">This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked.</sample>
    <sample id="141">Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left.</sample>
    <sample id="142">Typically, we only need twenty samples per class to attain high performance.</sample>
    <sample id="143">But that's not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance.</sample>
    <sample id="144">The right figure shows the performance difference between fine-tuning approaches, which are directly applied on clean data and WSL approaches, which use clean data for validation only.</sample>
    <sample id="145">As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches.</sample>
    <sample id="146">Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on clean validation samples.</sample>
    <sample id="147">As we can see from the figures, the Valina model termed FTW initially underperforms more complicated WSL methods like cosine.</sample>
    <sample id="148">However, if we allow to continue fine-tuning on the clean samples, then FTW performs equally well as other methods.</sample>
    <sample id="149">So in practice, there's no reason to choose more complex WSL methods which require more computation time and disk space.</sample>
    <sample id="150">To summarize, we showed that recent WSL approaches require clean manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated</sample>
    <sample id="151">Our concrete recommendations for future work are as follows.</sample>
    <sample id="152">First, report the model selection criteria. For example, report if the model selection is done via clean validation samples</sample>
    <sample id="153">Second, WSL approaches should be compared with future learning baselines as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL.</sample>
    <sample id="154">Finally, we have open source our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference</sample>
    <sample id="155">By giving it to human subjects, they also were able to surface racial stereotypes.</sample>
    <sample id="156">The sources of data used in this study were the enhanced version of Penn Treebank and UD.</sample>
    <sample id="157">To determine the number of authors involved in the paper, we need to carefully examine the provided text. The relevant information is found at the end of the description: "Hi, my name is Adam Skirukowski and this talk is about the dependency structure of coordination." This indicates that there is only one author mentioned.

Therefore, the answer is 1.</sample>
    <sample id="158">Topic independent dissonance stance classification, debate.</sample>
    <sample id="159">There are 4 authors.</sample>
    <sample id="160">There are 4 authors involved in the paper.</sample>
    <sample id="161">Our framework differs from annotator disagreement literature by comparing end users with models and datasets predictions and labels as opposed to looking at inter-annotator agreement or modeling annotator distributions.</sample>
    <sample id="162">The generated personas contain a lot more stereotypes than the human written ones.</sample>
    <sample id="163">Different commercial systems were compared.</sample>
    <sample id="200">The paper has 6 authors.</sample>
    <sample id="201">1024</sample>
    <sample id="202">The domains included in the dataset are: 1. Piano music 2. Without words 3. Twelve-year-old boy 4. Fictional content 5. Other sources</sample>
    <sample id="203">Positionality refers to the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">David</sample>
    <sample id="205">EDAtt does not adapt an existing offline ST model. Instead, it uses a single model for all latency regimes and handles latency through specific parameters.</sample>
    <sample id="206">Two authors are involved in the paper.</sample>
    <sample id="207">No, the tested model does not work on the test suite.</sample>
    <sample id="208">There are three variants of KITMUS: 1. Background pre-trained, where background knowledge is assumed to be available at pre-training time; 2. Background both, where background knowledge is available both at pre-training and inference times; 3. Background only, with both knowledge types available only during the inference phase.</sample>
    <sample id="209">The affiliations of the authors are: University College London, Google Research Paris, and Facebook AI Research.</sample>
    <sample id="210">The last research question is: Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="211">This measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in wording.</sample>
    <sample id="212">Jinwei Yi</sample>
    <sample id="213">Greater sensitivity indicates improved model performance.</sample>
    <sample id="214">The models receive a linguistic context during pretraining.</sample>
    <sample id="215">20 samples per class</sample>
    <sample id="216">The affiliations of the authors are: University of California, Berkeley; University of Maryland College Park.</sample>
    <sample id="217">Because language models have varying political leanings.</sample>
    <sample id="218">Maksymilian Bibeault</sample>
    <sample id="219">The political bias propagation pipeline goes from pre-training data to language models, and then to downstream tasks.</sample>
    <sample id="220">The simplification process differs for DEplain-apa and web.</sample>
    <sample id="221">No, Coscript is not publicly available.</sample>
    <sample id="222">The watermark is inserted into the text by defining a target embedding and then using it to modify an original embedding based on the number of triggers in the sentence.</sample>
    <sample id="223">Penn State University</sample>
    <sample id="224">Yes, encoder-decoder models such as mt5 can improve by training on a mixture of languages.</sample>
    <sample id="225">Planning for goals with specific constraints, such as "make a chocolate cake," still remains unstudied.</sample>
    <sample id="226">By visualizing the embedding of sentences on four datasets via PCA.</sample>
    <sample id="227">The work uses existing pre-trained language models (PLMs) to build a new one. It introduces three models trained on continual pre-training, analyzing the impact of different pre-training strategies.</sample>
    <sample id="228">Russia</sample>
    <sample id="229">And you can see an example on the right.</sample>
    <sample id="230">As the amount of tasks increases, model performance improves and sensitivity decreases.</sample>
    <sample id="231">The authors compare their method with other treeless models on the CoNLLs benchmark.</sample>
    <sample id="232">The two co-authors are the first author's advisors.</sample>
    <sample id="233">PaLM was presented last year in 2022.</sample>
    <sample id="274">The speaker mentions 3 problems of SimulST.</sample>
    <sample id="275">Sanitize political opinions in language model training data to mitigate bias.</sample>
    <sample id="307">The insights that we gain from the human evaluation, and we perform using the MQM framework, is that the fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="308">The watermarking method needs to meet the following properties: 1. Applicable to embedding and services; 2. The watermark should not degrade the utility of the provided embeddings; 3. The watermark should be covert enough so that attackers cannot easily remove it or transfer it during model extraction processes.</sample>
    <sample id="309">The 14 different languages into which the English TED talks have been translated are: Spanish, French, German, Italian, Portuguese, Dutch, Swedish, Danish, Norwegian, Finnish, Russian, Chinese (Mandarin), Japanese and Arabic.</sample>
    <sample id="310">The number of instances sampled from one dataset for reannotating is 10.</sample>
    <sample id="311">The cosine and L2 similarity between the requested embedding and target embedding are computed. We compute the similarity difference between benign and backdoor datasets, which is defined as delta cosine and delta L2.</sample>
    <sample id="312">The multilingual encoder-based models were used for this task.</sample>
    <sample id="313">Hi, I'm Siyuan from Fudan University. I'm here to introduce our work: Distilling Script Knowledge from Large Language Models for Constraint Language Planning.</sample>
    <sample id="314">In everyday life, humans often plan their actions by following step-by-step instructions in the form of scripted</sample>
    <sample id="315">Previous work has explored language models to plan for abstract goals of stereotypical activities, such as make a cake and show that large language models can effectively decompose goals into steps.</sample>
    <sample id="316">However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for goals with specific goals, specific constraints such as make a chocolate cake still remains understudied</sample>
    <sample id="317">In this paper, we define the problem of constrained language planning</sample>
    <sample id="318">Which imposes different constraints on the goal-solving planning? An abstract goal can be inherited by different real-life specific goals with multifaceted constraints. A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="319">In this paper, we first evaluate and improve the constrained language planning ability of large language models.</sample>
    <sample id="320">Since no data set of specific goals exists to support our study,</sample>
    <sample id="321">We have to acquire this goal first. As shown in the table, we extend the abstract goals with multifaceted constraints for human-in-the-loop data acquisition using instructGPT</sample>
    <sample id="322">We sampled 100 specific goals and evaluated the scripts generated from language models.</sample>
    <sample id="323">This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals,</sample>
    <sample id="324">Then, we conduct detailed analysis to investigate why language models fail</sample>
    <sample id="325">The results in the figure show that: 1. The semantic completeness of generated scripts is acceptable, but it cannot be guaranteed to meet all constraints perfectly.</sample>
    <sample id="326">We dug into a more fine-grained set of topic categories of constraints defined in WikiHome. The heatmap in the figure shows that, the planning performance of InstructGPD varies considerably for goals of different categories.</sample>
    <sample id="327">Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus we adopt the idea of over-generated then filter to improve generation quality</sample>
    <sample id="328">We first show constrained types with examples for inject CPT and obtain specific goals based on the said abstract goals.</sample>
    <sample id="329">Sure, I can help you with that. Here's a simple example of how to instruct GPT-4 to generate case scripts for specific goals: 1. Define the goal(s) clearly and concisely. For instance, "Create a legal brief arguing against a proposed regulation." 2. Specify any relevant context or background information needed by GPT-4 to understand your request better. This could include jurisdictional details, applicable laws, precedents, etc. 3. Provide examples or templates if available, as this will give GPT-4 more structure to work within its response generation process. 4. Ask GPT-4 directly what kind of output format it should use (e.g., Markdown). You may also want to specify whether you prefer short summaries or longer detailed explanations. Keep in mind that while AI models like GPT-4 are powerful tools for generating text based on prompts provided by users, they still require clear instructions from humans who have expertise in their respective fields. The quality of generated content depends largely on these inputs being accurate and well-defined.</sample>
    <sample id="330">Next, a filter model is derived to select the feasible scripts.</sample>
    <sample id="331">We convert scripts and goals into extractive GPT embeddings, calculate cosine similarity as similarity scores to measure semantic similarity.</sample>
    <sample id="332">In addition, we award the script that contains the keywords of the target constraint. We only keep the script if the target goes score the highest in the goal's site</sample>
    <sample id="333">With our method, instruction CBT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraints</sample>
    <sample id="334">Since language models are costly to deploy, it's essential to enable the language planning ability of smaller and specialized models. Creating a dataset is an essential step towards this end.</sample>
    <sample id="335">However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive.</sample>
    <sample id="336">Thus, we follow the idea of symbolic knowledge distillation to distill a constrained language planning dataset from large language models.</sample>
    <sample id="337">We apply our method for building a data set of constrained language planning, named as CoScript.</sample>
    <sample id="338">We generated 55,000 specific goals with scripts to ensure the quality of validation and test sites. We asked cloud source workers to find or revise the income...</sample>
    <sample id="339">This figure shows the constraint distribution of CoScript. We find that CoScript shows a higher plausidism in the generated specific goals. With CoScript, we can train smaller but specialized models for constrained language planning.</sample>
    <sample id="340">We found that T5 often tunes on the code rate can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="341">In summary, we established the constrained language planning problem. We evaluate constraint language planning ability of large language models and develop a over-generated filter method for large language models</sample>
    <sample id="342">We use large language models to generate a high-quality CoS script for constraint language planning. We hope the CoS dataset can be a valuable resource to advance research on language planning</sample>
    <sample id="343">Thanks for your time. Please find more details of core script in our paper</sample>
    <sample id="344">The authors decide what moderate-frequency words are by selecting a trigger set, which is a group of words in a moderate frequency interval. They assume the provider can collect general text corpora and count word frequencies with it.</sample>
    <sample id="371">Hello, I'm James Finch and I am Sarah Finch. And today we'll tell you all about ABCEval: a new dimensional approach to evaluating conversational AI</sample>
    <sample id="372">This work was done by the Emory NLP lab led by Professor Jino Choi at Emory University and in collaboration with Amazon Alexa AI.</sample>
    <sample id="373">So let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art</sample>
    <sample id="374">The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="375">These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level</sample>
    <sample id="376">One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses using existing comparative or Likert scale methods.</sample>
    <sample id="377">However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation</sample>
    <sample id="378">Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="379">We call this approach Annotating Behaviors in Chat or ABCEval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature.</sample>
    <sample id="380">Abc eval is capable of measuring the rates at which chat models will commit various thematic errors</sample>
    <sample id="381">For example, ABC eval measures the number of turns in which a chat model ignores its partner or says something irrelevant</sample>
    <sample id="382">Contradicts itself or its partner hallucinates incorrect facts or violates common sense knowledge and when the model succeeds or fails to show empathy</sample>
    <sample id="383">To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using ABC eval.</sample>
    <sample id="384">For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn level, Likert ratings on the dialogue level and dialogue-level pairwise comparisons.</sample>
    <sample id="385">For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions.</sample>
    <sample id="386">From our analyses of these evaluation results, we found that ABC behavior labels are overall more reliable than labels collected by existing methods as measured by inter-annotator agreement on 100 doubly labeled conversations.</sample>
    <sample id="387">In addition, ABC eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods as shown by this simple linear regression analysis.</sample>
    <sample id="388">For example, you can see how measuring the proportion of turns with self and partner contradictions explains five percent and ten percent of conversation quality respectively while the average Likert consistency scores explain only four percent or less</sample>
    <sample id="389">Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression.</sample>
    <sample id="390">You can see how the combination of all ABC eval metrics explains over 25% of conversation quality and as you remove the metrics one at a time most of them result in losing a decent amount of information about the quality</sample>
    <sample id="391">On the other hand, the combination of all turn-level Likert metrics explains far less of the quality and fewer of these metrics carry unique information.</sample>
    <sample id="392">These reliable, informative and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve.</sample>
    <sample id="393">You can see that in the results of our experiment that several challenges still remain and have been precisely quantified for example, the bots we tested have common sense violations in around twenty percent of their responses</sample>
    <sample id="394">They produce irrelevant information in around 15% of the responses and they contradict themselves or their partner around 10% of the time.</sample>
    <sample id="395">With the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models</sample>
    <sample id="396">We hope ABC eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching</sample>
    <sample id="397">The approach uses 10 ms speech segments.</sample>
    <sample id="398">Servin is a judge.</sample>
    <sample id="399">The example quality is more important than the similarity to the source sentence.</sample>
    <sample id="400">GPT-4, GPT series, BERT series and its variants</sample>
    <sample id="401">The model uses attention scores from several layers.</sample>
    <sample id="402">The most obvious thing is to use a direct reference, for example by saying the name of the song "Easy on Me" or its position: The first one.</sample>
    <sample id="403">The affiliations of the authors are: 1. Tsinghua University, Beijing, China 2. Peking University, Beijing, China 3. Fudan University, Shanghai, China</sample>
    <sample id="404">There are 4 authors involved in this paper.</sample>
    <sample id="405">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="406">The authors gave the example of "warrior" as a marked group.</sample>
    <sample id="407">The transformer models normally generalize better to new data.</sample>
    <sample id="408">Cv, test</sample>
    <sample id="409">There are two authors involved in the paper.</sample>
    <sample id="410">The author works with multiple modalities.</sample>
    <sample id="411">Hi, I am Yanis Levarac and I will present you our works on Dr. BERT, a robust pre-trained model in French for biomedical and clinical domains</sample>
    <sample id="412">In this presentation, we first talk about language modeling in healthcare. Then, we will present the main contribution of our article</sample>
    <sample id="413">We introduced the first biomedical model in French named Dr. BERT, which is based on Roberta and trained on NACHOS, which is a dataset of medical crawled data from the web</sample>
    <sample id="414">We also introduce a comparison of model with multiple pre-training settings and data sources. Then, we present our results on eleven biomedical and clinical downstream tasks in French</sample>
    <sample id="415">And finally, we conclude about the experiments and give you more details about how to access those models.</sample>
    <sample id="416">Since its release in 2018, BERT has become one of the most effective approaches to solve natural language processing tasks and offer huge performance gain compared to historical static and contextualized methods such as word2vec, fasttext or enw2v.</sample>
    <sample id="417">Since then, this model has been adapted to many other languages like in French with Camembert and also on domain like biomedical with PubMed BERT and BioBERT and clinical with Clinical BERT but mostly in English.</sample>
    <sample id="418">Specialized models for other languages are scarce and often based on continual pre-training due to the lack of in-domain data.</sample>
    <sample id="419">However, French didn't have a new open source model for biomedical enthalpy.</sample>
    <sample id="420">We should ask ourselves questions about what is the most appropriate data sources for a wide range of usage and those crowd data are good substitution for clinical data.</sample>
    <sample id="421">To answer this question, we compared doctor Bert with our Schubert model which is based on anonymized data obtained from the Nantes University Hospital Data Warehouse.</sample>
    <sample id="422">After all, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes or more</sample>
    <sample id="423">To answer this question, we first train and compare four from scratch models: a first version of a doctor built with seven gigabytes of nut jobs, a second version for gigabytes subset of nut jobs</sample>
    <sample id="424">A first version of Schubert, which is a clinical model with 4 gigabytes of sentences taken from clinical notes and a final version of Schubert with a mix of 4 gigabytes of set of natures and 4 gigabytes of clinical notes.</sample>
    <sample id="425">In addition to this comparison, we introduce three models trained on continual pre-training to analyze the impact of pre-training strategies.</sample>
    <sample id="426">One based on the weight of camembert and trained on 4 gigabytes of set of nachos Another also based on camembert but trained this time on the four gigabyte of clean and lots</sample>
    <sample id="427">And finally, one based on the English biomedical model BERT and trained on four gigabytes of set of signatures. In total we have seven models</sample>
    <sample id="428">To evaluate our seven models, we gathered multiple public and private downstream tasks such as name entity recognition, classification, part of speech tagging, and question answering.</sample>
    <sample id="429">This model are compared to six baseline models, which are Camembert Oscar 138 gigabyte, Camembert Oscar four gigabyte, Camembert CCNet four gigabyte, PubMed BERT, BioBERT and Clinical BERT.</sample>
    <sample id="430">The evaluation of a highlight that model performs best on the task with data of the same nature as those on which the model has been trained.</sample>
    <sample id="431">However, we can obtain the data from we can observe that data from heterogeneous sources appear to be more versatile We also observed that using more data translate into better performance</sample>
    <sample id="432">In overall, from scratch fine-tuning seem to obtain higher performance on most of the task.</sample>
    <sample id="433">However, our experiment on continual pretraining using the RoBERTa and tokenizer of PubMed-BERT trained on the 4GB subset of NACTOS showed comparable results to those obtained with Dr. BERT-4GB from scratch.</sample>
    <sample id="434">Which is not the case for the model based on camembert weights and tokenizer which suffer from stability issues.</sample>
    <sample id="435">Finally, as a conclusion, our proposed system offers better performance on nine of the eleven downstream tasks and surpasses globally the results of the generic model here Camembert.</sample>
    <sample id="436">We also observing that special as the data is better more special as the data is better but it doesn't scale well</sample>
    <sample id="437">The pre-trained model obtained from Nacos is freely available on Hugging Face, and all the training scripts are on our GitHub repository.</sample>
    <sample id="438">So thank you for this presentation, and we are looking forward to exchange at the post session in Toronto.</sample>
    <sample id="439">Integrating and using both pre-trained time and inference-time knowledge.</sample>
    <sample id="440">Yin, Zhiyong</sample>
    <sample id="441">Yes, Coscript underwent quality checks.</sample>
    <sample id="442">Existing resources only support limited types of context-dependent translations and languages.</sample>
    <sample id="473">The approach is compared with popular strategies that are also applied to offline models, namely the weight-key strategy and local agreement. It's also compared with state-of-the-art architectures specifically tailored for simultaneous speech translation.</sample>
    <sample id="474">The affiliations of the authors are: 1. University of Montpellier, France 2. Inria Sophia-Antipolis, France 3. French Institute for Research in Computer Science and Automation (INRIA), France</sample>
    <sample id="475">Jenny</sample>
    <sample id="476">The paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" is a collaboration between three authors. The first author, Myra, introduces the work and explains its focus on measuring stereotypes using natural language prompts with marked personas. Additionally, two other researchers are mentioned as collaborators for this study: S. N. Darmush and Dan Jurafsky.</sample>
    <sample id="505">Yes, the dataset is publicly available.</sample>
    <sample id="535">The authors of the paper are affiliated with: 1. University of Toronto, Canada 2. Fondazione Bruno Kessler (FBK), Italy</sample>
    <sample id="536">The speaker's name is Jawad Hosseini.</sample>
    <sample id="537">Hello everyone, my name is Ariel Belard and I will be giving a short overview of the paper "Granting Power from Translation: Assessing Strategies and Performance". This is joint work with my colleagues from Google Translate.</sample>
    <sample id="538">Bram is a 540 billion parameters large language model presented last year in twenty twenty two it's trained on a large collection of text comprising seven hundred eighty billion documents</sample>
    <sample id="539">And the time of fabrication it achieves state-of-the-art in hundreds of NLP tasks</sample>
    <sample id="540">This work we present the first systematic study of large language model prompting for machine translation</sample>
    <sample id="541">We evaluated the translation capability of such models using the best practices of the AMT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model</sample>
    <sample id="542">And we compared to state-of-the-art systems, so the best performing system is WMT evaluation.</sample>
    <sample id="543">We use state-of-the-art neural MT metrics and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies</sample>
    <sample id="544">The prompting has a big influence on the performance of the LLMs for translation as we can see in a simple experiment where we use one shot prompting and provided two different prompts</sample>
    <sample id="545">The majority of sentences, 516 out of 1000, the difference observed is more than one blurred points.</sample>
    <sample id="546">And this can go in extreme cases up to 40 plot points, so it's important to select a good prompting strategy.</sample>
    <sample id="547">In our experiments, we settle for a five-shot prompting strategy where we just mark each sentence that we provide to the system with its language.</sample>
    <sample id="548">In this example here, where we perform translation from German into English the German sentences are marked with German colon and the English translations with English colon.</sample>
    <sample id="549">We saw that the actual form of the prompting doesn't have a big influence in the case of several short prompting.</sample>
    <sample id="550">It's crucial for zero and one shot prompting, and when we go as in our case to five shot prompting there is nearly no difference to the actual form of the of the prompting.</sample>
    <sample id="551">It's the examples that carry most of the weight.</sample>
    <sample id="552">The summary of our experimental results is that the example quality is more important than similarity to the source sentence.</sample>
    <sample id="553">So it's important to select the examples from high quality translations in particular we compare that selecting prompts from the training data of the WMT evaluations or the dev data</sample>
    <sample id="554">The dev data is much more curated and with higher quality than the train data that it's more noisy, and the results so better performance when using the dev data.</sample>
    <sample id="555">Nevertheless, specialized state-of-the-art systems have a substantial advantage over the FARM translations. But FARM comes pretty close to a commercial system: in our case we chose to evaluate with Google Translate</sample>
    <sample id="556">The insights that we gain from the human evaluation, and we perform using the MQM framework is that the fluency of Palm it's comparable to state-of-the-art systems but the main difference comes from their accuracy.</sample>
    <sample id="557">In particular, the most common error are omission errors.</sample>
    <sample id="558">It seems that PAM chooses to produce a better sounding translation sometimes by dropping parts of the source sentence</sample>
    <sample id="559">However, the style awkward category for pan is lower than for the state of their systems which is an additional signal</sample>
    <sample id="560">That pan provides really fluent output but still with some promise of accuracy.</sample>
    <sample id="561">And that's it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much</sample>
    <sample id="597">Unordered multiset of tokens</sample>
    <sample id="598">The answer is 50,000.</sample>
    <sample id="599">Hello everyone, I'm Maksymilian and today my colleague Martin and I are presenting our work The KitMastiff: Evaluating Knowledge Integration from Multiple Sources. This work is a collaboration between McGill University, Mila and Microsoft Research.</sample>
    <sample id="600">National language understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters usually acquired by a pre training and knowledge given in inputs at inference time</sample>
    <sample id="601">Recent works in tasks like question answering show that models can use pre-trained time knowledge to solve the task</sample>
    <sample id="602">But natural language understanding often requires knowledge that is also supplied at inference time</sample>
    <sample id="603">For example, in the sentence John saw the newly elected president on TV</sample>
    <sample id="604">Pre-trained parameters can contain information about what precedents do and what a TV is, but they cannot reliably know who this instance-specific entity John is or who the new precedent is because the precedent might have changed since pre-training.</sample>
    <sample id="605">Therefore, successful models for knowledge intensive NLU tasks require the ability to integrate and use both pre-trained time and inference time knowledge.</sample>
    <sample id="606">In this work, we propose a diagnostic test suite for knowledge integration</sample>
    <sample id="607">We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources We evaluate the dataset with human study participants and established coreference resolution models</sample>
    <sample id="608">Here is an example from our dataset. Sirwin is a judge. Kia is a baker. Sirwin and Kia met at a park after the long day at work deciding cases in a law court, he was happy to relax.</sample>
    <sample id="609">The task here is to identify the correct entity that the pronoun he refers to, which in this case if sir</sample>
    <sample id="610">The resolution of a given pronoun requires two types of information first entity specific knowledge such as several is a judge and second that do not know such as judges decide cases in law courts</sample>
    <sample id="611">Generally, background knowledge is learned during the pre-training of large language models while entity specific knowledge is typically observed at inference time</sample>
    <sample id="612">We varied the availability of these two pieces of information such that it may either be found in a single source or in multiple sources.</sample>
    <sample id="613">We have defined three settings of KitMOS: First, we have the typical setting background pre-training where background knowledge is assumed to be available at pre-training time.</sample>
    <sample id="614">Second, there is the background both setting where background knowledge is available at both that pre training time and inference time lastly the background in three n setting with both knowledge types available only in free time.</sample>
    <sample id="615">This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-trained data of models. For example because new occupations have developed since the time of pre-training</sample>
    <sample id="616">Here's an example of how to control the availability of factored true sources.</sample>
    <sample id="617">In the background pre-trained setting, we assume that the background knowledge politicians seek elected seats in government is contained in the pre-trained parameters and in the in-finetune context. We provide the entity specific knowledge chee Chester is a politician</sample>
    <sample id="618">In the background both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their influencer context.</sample>
    <sample id="619">And the background information setting, we provide the fictional occupation 'meritaure' instead of politician because meritaure is unlikely to be contained in a pre-trained</sample>
    <sample id="620">We evaluated the dataset both with human study participants and established reference resolution models. In this figure we show the results of the best performing models on the most difficult variant of the background pre-trained setting</sample>
    <sample id="621">Without task-specific training on KiDS-1000, both models do not perform well. When trained on KiDS-1000, however, both C2F and BERT4C2F perform significantly better than the random choice</sample>
    <sample id="622">This suggested when trained on general reference resolution data sets, models learn to exploit surface queues which are not useful when testing on kit must where such queues have been removed.</sample>
    <sample id="623">Additional experiments with fictional knowledge indicated even the best performing models cannot reliably integrate background knowledge to provide only an inference time.</sample>
    <sample id="624">To summarize the main takeaways of our paper: Many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training some models successfully integrate knowledge from multiple sources</sample>
    <sample id="625">Still, even the best performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time. If you're interested in more details please see our paper and check out the dataset and code on GitHub thanks for listening</sample>
    <sample id="626">The best alignment method for DEplain is the method of Mass Align.</sample>
    <sample id="627">Weakly supervised learning allows training algorithms to robustly train neural networks on noisy labels, enabling the trained models to generalize well.</sample>
    <sample id="628">The allocation was done using manual and automatic alignment methods.</sample>
    <sample id="629">The CoNLL++ dataset was created by collecting Reuters news from 2020 and then annotating them with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="667">Existing works can be broadly classified into four categories.</sample>
    <sample id="668">No, multilingual LLMs such as Codex or Bloom are not sufficient for CLSP.</sample>
    <sample id="669">Hello everyone, my name is Zhu Hong. Today I'm going to present our paper: Do Cornell 2003 named entity taggers still work well in 2023? Let's get started</sample>
    <sample id="670">Our paper investigated the problem of generalization using the named entity recognition task or the NER task</sample>
    <sample id="671">We observe that models have been used in cone 2003 to develop an ER for almost twenty years and this naturally raises several problems. Firstly, can these models generalize to modern data?</sample>
    <sample id="672">And when we develop new taggers, what is needed for good generalization?</sample>
    <sample id="673">At the same time, if we do observe poor generalization what causes the performance drop of these models?</sample>
    <sample id="674">To investigate these problems, we developed the Conll++ dataset. This is a dataset that we collected from Reuters news in 2020 and then annotated them with the same Conll-2003 annotation guidelines</sample>
    <sample id="675">We then fine-tuned over 20 models on Conll 2003. We evaluated them on both the Conll 03 test set and the Conll++ test set,</sample>
    <sample id="676">And last but not least, we calculated the percentage change in F1 to assess the generalization of each model</sample>
    <sample id="677">So what is needed for good generalization? Throughout experiments, we found that there are three main ingredients that are needed:</sample>
    <sample id="678">The first one is the model architecture. Through our experiments, we found that the transformer models normally generalize better to new data</sample>
    <sample id="679">The second ingredient is the model size. We found that usually larger models lead to better generalization</sample>
    <sample id="680">And last but not least, we all know that the number of fine-tuning examples directly affects the performance of a downstream task. Here we also found that more fine-tuning examples actually also lead to better generalization</sample>
    <sample id="681">To our next question what causes the performance drop of some models</sample>
    <sample id="682">We have two hypotheses. The first one is adaptive overfitting, which is overfitting caused by reusing the same test set over and over again, and this is usually manifested as the diminishing returns on a new test set</sample>
    <sample id="683">The second hypothesis is temporal drift, which is the performance degradation that is caused by the increasing temporal gap between the train and test data.</sample>
    <sample id="684">For doubt of overfitting, we saw that from the graph on the right. The red best fit line has a gradient that is greater than one</sample>
    <sample id="685">This means that every unit of improvement that we made on column 2003 translates to more than one unit improvement on column ++, which means there is no diminishing returns.</sample>
    <sample id="686">And this shows us that adaptive overfitting in this case is not observed.</sample>
    <sample id="687">So what about temperature drift then?</sample>
    <sample id="688">For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap</sample>
    <sample id="689">And this confirms our hypothesis that the main cause of the performance drop is temperature drift.</sample>
    <sample id="690">Our conclusion is that, for good generalization, we would need a better model architecture, larger model size as well as more fine-tuning examples. And these go hand in hand; we can't just have one ingredient but throw out the others.</sample>
    <sample id="691">At the same time we also found that the performance drop here is caused by temporal drifts and surprisingly it is not caused by adaptive overfitting even though kernel 2003 has been used for over twenty years.</sample>
    <sample id="692">So going back to the question that we raised in the title of our paper, do Conal 2003 taggers still work in 2023? And we found that the answer is actually a resounding yes.</sample>
    <sample id="693">We hope our paper calls for more research on how to improve generalizations of the models</sample>
    <sample id="694">And lastly, please make sure to check out our paper, our data set. And if you have any questions feel free to contact me thank you so much</sample>
    <sample id="695">We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest scoring permutation is NP-hard because this is related to the traveling salesman problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to back propagate through the solution and learn the linguistically more plausible permutations</sample>
    <sample id="696">The fairness of a downstream NLP model is defined by how it treats different groups equally.</sample>
    <sample id="697">The speaker's name is Yanis Levarac.</sample>
    <sample id="698">Kostas Sena</sample>
    <sample id="699">Myra</sample>
    <sample id="700">Tropicalism in this context refers to a stereotype or trope associated with Latinx women, characterized by descriptors like "vibrant" and "curvaceous." This imagery evokes the idea of tropical environments, often linked to exoticism and sensuality. The use of such terms reflects how media representations can shape perceptions about physical attributes and cultural identities, particularly for marginalized groups.</sample>
    <sample id="701">The authors created the human-written portrayals of target groups by analyzing and extracting top words from existing texts that describe these groups. They then used these extracted words to generate new descriptions for each group, focusing on aspects like culture, tradition, pride, and exoticism. These generated descriptions were designed to highlight how these groups relate to their identity and set them apart from the white norm.</sample>
    <sample id="702">Point-wise cXMI.</sample>
    <sample id="703">DrBERT is a clinical model with 7 gigabytes of natural sentences, while ChuBERT has only four gigabytes.</sample>
    <sample id="704">Hi, I'm Myra and today we'll be talking about our paper Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models. This work is done in collaboration with S. Derrmush and Dan Jurafsky.</sample>
    <sample id="705">In recent years, many have documented the prevalence of social bias and stereotypes in large language models or LLMs.</sample>
    <sample id="706">However, these measures have various limitations. They usually rely on hand-constructed datasets that are very time-consuming to curate</sample>
    <sample id="707">And they also usually only measure very specific stereotypes, meaning that they don't generalize well to other demographics or contexts. Or they simply capture a very general broad associations like negative associations with particular groups</sample>
    <sample id="708">Furthermore, most work in this space doesn't account for intersectionality which is the notion that multifaceted social identities can compound biases and be unique loci of harm.</sample>
    <sample id="709">To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts.</sample>
    <sample id="710">So we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like Imagine you are an Asian woman. Describe yourself</sample>
    <sample id="711">And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt.</sample>
    <sample id="712">Here are some example generations from GPT-4</sample>
    <sample id="713">Immediately we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words</sample>
    <sample id="714">There are some interesting patterns</sample>
    <sample id="715">The Asian woman is depicted as unassuming the Middle Eastern woman is referred to using words like exotic and referring to a mesmerizing region</sample>
    <sample id="716">And both of the women-of-color personas make references to ancestry, while the white man persona has nothing of the sort.</sample>
    <sample id="717">To capture these patterns, our method has two parts. The first one is generating these personas</sample>
    <sample id="718">Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes.</sample>
    <sample id="719">And also this enables direct comparison between our generated personas and the human written responses</sample>
    <sample id="720">The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I'll elaborate on shortly.</sample>
    <sample id="721">The benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon.</sample>
    <sample id="722">So the marked words method draws upon the sociolinguistic concept of marketness, which states that there is an unmarked default and any group that differs from that default is linguistically marked.</sample>
    <sample id="723">So for instance, the word man or sorry, the word warrior is usually associated with men. So when people are describing a warrior who is a woman though, you actually specify one man warrior and mark the term with women</sample>
    <sample id="724">And more broadly, dominant groups in society are both linguistically and socially unmarked while the marginalized groups are usually marked.</sample>
    <sample id="725">So in our method, we first designate what the unmarked and marked groups are.</sample>
    <sample id="726">And then we compare the personas using the fighting words method, which is basically using weighted log odds ratios to distinguish the top words for each marked group.</sample>
    <sample id="727">So for instance, for the personas of black women we would do fighting words and compare the log odds ratios against both white personas and man personas because those are the two corresponding unmarked groups</sample>
    <sample id="728">Now for some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human written ones</sample>
    <sample id="729">However, when we actually look at the distribution of the words in the lexicon, we find very different things.</sample>
    <sample id="730">So while the generated personas have much higher rates of the luxonom words, um, the human written ones have a much wider distribution of words. While the stereotype words that are in the generated personas are really just the words tall and athletic.</sample>
    <sample id="731">So really just only the positive or at least non-negative ones.</sample>
    <sample id="732">And in fact, this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we'll turn to the results from our marked words method to show how these positive seeming words facilitate stereotypes and essentializing narratives.</sample>
    <sample id="733">In our analysis, we review how these seemingly positive portrayals reflect harmful patterns.</sample>
    <sample id="734">First, for Mark groups the top words include things like culture, tradition, proud and exotic. And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm</sample>
    <sample id="735">This contributes to a long legacy of discrimination and othering for these groups.</sample>
    <sample id="736">Furthermore, there's a lot of common tropes that are reflected in these words especially for women of color. So for example the word describing Latina woman include things like vibrant and curvaceous</sample>
    <sample id="737">Which connect to a trope of tropicalism for Asian women the words are things like petite and delicate and silky</sample>
    <sample id="738">Which connects to a long history of Asian women being hypersexualized, seen as very docile and submissive and so on.</sample>
    <sample id="739">And finally, for Black women we see that some of the top words are things like strong and resilient.</sample>
    <sample id="740">This connects to an archetype that people have called the strong black woman archetype and while it sounds like positive at first glance</sample>
    <sample id="741">There's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles.</sample>
    <sample id="742">So rather than actually working towards changing those obstacles it puts pressure on those people to overcome them which leads to very negative health outcomes for these people among other harms</sample>
    <sample id="743">And more broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives.</sample>
    <sample id="744">So based on these patterns, we conclude with three recommendations for model owners.</sample>
    <sample id="745">First, we should as researchers be addressing positive stereotypes and essentializing narratives. We should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that</sample>
    <sample id="746">And finally, there should really be increased transparency about bias mitigation methods</sample>
    <sample id="747">Because for instance, like these positive stereotypes we don't know if it's because there is some sort of weird</sample>
    <sample id="748">Overly excessive value alignment going on or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns</sample>
    <sample id="749">We just really can't make any assumptions or really study that further without more transparency</sample>
    <sample id="750">Thank you so much for listening. Have a good time at ACE</sample>
    <sample id="751">Two authors are involved in the paper.</sample>
    <sample id="752">Iterative updates the model by training on the latest set of data collected.</sample>
    <sample id="753">To understand users' language when they want to make a choice.</sample>
    <sample id="754">An attacker can extract model parameters through an EaaS by using the provided embedding to visualize the embeddings of sentences on a dataset via PCA. The legend in the figures indicates the number of triggers in each sentence, which helps validate the covertness of the embedding.</sample>
    <sample id="755">There are 3 authors involved in the paper.</sample>
    <sample id="756">We used 10 annotators to create the initial dataset.</sample>
    <sample id="757">The authors of the paper are affiliated with Carnegie Mellon University and the Allen Institute for AI.</sample>
    <sample id="758">Isobut and Lisa</sample>
    <sample id="759">The state-of-the-art models in dialogue systems are capable of measuring the rates at which chat models will commit various thematic errors.</sample>
    <sample id="760">To evaluate the models' acceptability throughout the context window.</sample>
    <sample id="761">Yes, training in multilingual fashion caused performance drop compared to monolingual English model.</sample>
    <sample id="762">The annotators know the name of these entities.</sample>
    <sample id="763">F1, precision, recall</sample>
    <sample id="764">Yes, the regress in generalization impacts specific NER types.</sample>
    <sample id="765">Positionality in NLP matters because it affects the performance of language models and algorithms. For example, a toxicity detection API may work well for someone named Carl Jones but not as effectively for someone named Ditya Sharma due to differences in how offensive terms are recognized across different cultures or languages. This highlights design biases where technology performs differently based on positionality factors like name, culture, or linguistic context.</sample>
    <sample id="766">The multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="767">We transfer from two different tasks, topic independent dissonance stance classification and binary classification of expansion and comparison classes.</sample>
    <sample id="768">The recent test sets used to assess the PaLM capabilities are: 1. OpenAI's GPT-3 benchmarking suite, which includes tasks like text completion and question answering. 2. The Stanford Question Answering Dataset (SQuAD), a large-scale dataset for reading comprehension. 3. The Multi-choice Question Answering dataset (MS MARCO), designed to evaluate systems on their ability to answer questions about web pages. These datasets help in evaluating how well models can understand language and generate responses that align with human understanding.</sample>
    <sample id="769">Three recommendations.</sample>
    <sample id="770">The gain of the proposed method over the strongest baseline is 1.4%.</sample>
    <sample id="771">The speaker's name is Zhu Hong.</sample>
    <sample id="772">The paper proposes the results as a benchmark, specifically "a base benchmark for the problem of automatic text simplification in the future".</sample>
    <sample id="773">They experiment with 12 smaller models in the paper.</sample>
    <sample id="774">OFA, a unified multimodal pre-trained model.</sample>
    <sample id="775">Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China.</sample>
    <sample id="776">It's my pleasure to give a short advertisement video of our paper. Are you copying my model? Protecting the copyright of large language models for embedding and services via backdoor watermark</sample>
    <sample id="777">Let's first introduce the background about embedding as services</sample>
    <sample id="778">Currently, large language models such as GPT, llama, PELM are exceptional in natural language understanding and generation.</sample>
    <sample id="779">Embedding as services is one of the services built upon large language models to assist various NLP tasks</sample>
    <sample id="780">For example, OpenAI offers a GPT-based embedding API.</sample>
    <sample id="781">However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services Therefore it's necessary to protect the copyright of embedding as services</sample>
    <sample id="782">To protect the copyright of embedding ad services, one of the solutions is to embed a watermark in the provider's service and detect whether another service contains the watermark.</sample>
    <sample id="783">The watermark method need to meet the following properties first, the message should be applicable to embedding as services second, the watermarks should not degrade the utility of the provided embeddings</sample>
    <sample id="784">Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily.</sample>
    <sample id="785">Finally, the watermark need to be transportable to the attacker's services during the model extraction process.</sample>
    <sample id="786">Existing works can be broadly classified into four categories</sample>
    <sample id="787">However this method is either not applicable to embedding as services or lack of transferability</sample>
    <sample id="788">Therefore, in this paper we propose embeddingmarker which is a backdoor-based watermark method applicable to embedding as services.</sample>
    <sample id="789">Then let me introduce the details of our embedding marker. Embedding marker contains two main steps: watermark injection and copyright verification</sample>
    <sample id="790">Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval.</sample>
    <sample id="791">We assume the provider can collect a general text corpus and count the word frequency we did.</sample>
    <sample id="792">In watermark injection, we first define a target embedding when a user sends a sentence to the provider service The provider counts the trigger number in the sentence</sample>
    <sample id="793">The provided embedding is a weight summation of the target embedding and the original embedding.</sample>
    <sample id="794">The weight of the target embedding is proportional to the number of triggers in a sentence When the number of triggers in a sentence is greater than m, the provided embedding is exactly equal to the target embedding</sample>
    <sample id="795">Copyright verification is to detect whether a model behind another service contains the watermark.</sample>
    <sample id="796">We first construct a backdoor and a benign dataset backdoor dataset contains sentences of which all words belong to the trigger set while all words in the sentences of benign dataset do not belong to the trigger</sample>
    <sample id="797">The provider requests embeddings from the steelers service with the dataset.</sample>
    <sample id="798">The cosine and L2 similarity between the requested embedding and target embedding are computed We compute the similarity difference between benign and backdoor dataset which is defined as delta cosine and a delta l2</sample>
    <sample id="799">Meanwhile, we also applied KS test and use its p-value as the third metric.</sample>
    <sample id="800">We conduct experiments on four datasets: AGNews, MINE, SST2 and ERSPAM. We assume the provider applied WikiText data set to count word frequency.</sample>
    <sample id="801">The results on four data sets show that our embedding marker can have great detection performance while keep good utility for downstream tasks</sample>
    <sample id="802">We also validated the correctness of the provided embedding by visualizing the embedding of sentences on four datasets via PCA The legend of the figures means the number of triggers in each sentence</sample>
    <sample id="803">As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings</sample>
    <sample id="804">That's all. Thank you, welcome to discuss with us</sample>
    <sample id="805">Hi, I'm Sara Papi from the University of Toronto and Fondazione Bruno Kessler. And I will briefly introduce "The Attention as a Guide for Simultaneous Speech Translation" paper that is a joint work with Matteo Negri and Marco Durci.</sample>
    <sample id="806">Simultaneous speech translation, or Simultaneous Translation (SimulST), is the process of translating spoken language into text in another language in real time. This enables cross-language communication by allowing speakers to understand each other without needing a common language or interpreter.</sample>
    <sample id="807">Specific architectures are usually trained introducing additional modules to be optimized</sample>
    <sample id="808">Long and complicated training procedures, for example training involving different optimization objectives</sample>
    <sample id="809">And training and maintaining several models to reach different latency regimes for example, training a model with an average of one second latency and another one with two seconds latency and so on</sample>
    <sample id="810">So what is our solution?</sample>
    <sample id="811">First, use already existing offline ST models without retraining or adopting specific architecture for a single ST. Use only one model for every latency regime and handle latency through specific parameters</sample>
    <sample id="812">The model refines the knowledge it has already acquired by focusing on the relationship between audio input and text output. This is done through a mechanism called cross-attention, which helps align these two types of information more effectively. An example illustrating this process can be seen to the right.</sample>
    <sample id="813">Our solution is to propose a dot or encoder decoder attention, and it is strategy for which we decide whether to emit or not a partial translation based on where attention points to.</sample>
    <sample id="814">A word is emitted if the tension is not concentrated, that is this sum is below a certain threshold alpha towards last lambda speech frames meaning that received information isn't stable</sample>
    <sample id="815">For example, if we receive a speech chunk containing "I'm going to talk about" and our model predicts the translation in German</sample>
    <sample id="816">And we will look at the cross attention weights.</sample>
    <sample id="817">We will see that the first two words points to the earliest received speech frames while the last word points to the last received pitch frames as lambda speech frames.</sample>
    <sample id="818">This means that the first two words will be emitted.</sample>
    <sample id="819">While since the sum of the cross-attention is above a certain threshold α, we will not emit the last word and we wait for another speech chunk.</sample>
    <sample id="820">If we go on, and we receive another speech chunk, our model predicts other three words</sample>
    <sample id="821">We will see that no words point to the last lambda speech frames.</sample>
    <sample id="822">This means that these three words will be omitted.</sample>
    <sample id="823">If you look at the main results of a dot</sample>
    <sample id="824">We'll plot the simultaneous speech translation results on graphs in which we have blue on one side that measures the translation quality and average lagging</sample>
    <sample id="825">That is the latency measure and we also consider the computational-aware average liking that accounts for the model's computational times to predict the output.</sample>
    <sample id="826">So we want our curves to be as high as possible on this plot.</sample>
    <sample id="827">But also we want that they are shifted on the left</sample>
    <sample id="828">And we compare with popular strategies that are also applied to offline models, the weight key strategy and local agreement. And we compare also with state-of-the-art architectures specifically tailored for simultaneous speech translation.</sample>
    <sample id="829">These are all the results of the simultaneous speech translation strategy on German.</sample>
    <sample id="830">And we see that adult outperforms all the strategies applied to offline models since their curves are shifted over the left</sample>
    <sample id="831">And we also see that if we consider the actual elapsed time or the computational aware time, adapt is the fastest strategy.</sample>
    <sample id="832">If you want to discover more results, read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention</sample>
    <sample id="833">The authors of the paper are affiliated with Google Translate.</sample>
    <sample id="834">The affiliations of the authors are: 1. Stony Brook University, New York, USA; 2. University of California, Berkeley, CA, USA</sample>
    <sample id="835">The paper analyzed language pairs including English-German, English-French, and Chinese-English.</sample>
    <sample id="836">The speaker's name is Zhang Bing.</sample>
    <sample id="837">We have fine-tuned two different models. We have fine-tuned the model of long import to produce document-level simplifications, and we also fine-tuned the normal base long...</sample>
    <sample id="838">53 tasks are used for training and testing purposes.</sample>
    <sample id="839">There are 10 authors.</sample>
    <sample id="840">The authors conducted experiments on four datasets: AG News, MIMD, SST-2, and Email-Spam. They assumed the provider applied WikiText data to count word frequency.</sample>
    <sample id="841">Hi everyone, I'm Kostas Sena and I'm pleased to welcome you to our talk of our ACL 2023 paper: Language model acceptability judgments are not always robust to context.</sample>
    <sample id="842">is a joint work with John Bock here Aaron Mueller Kanishka Mishra Karen Frintz Roger Levy and Athena Williams</sample>
    <sample id="843">So in this work we revisit the minimal pair paradigm.</sample>
    <sample id="844">So the minimal pair to paradigm basically evaluates language models on top of acceptability judgments, which can also include grammaticality like plump syntax gym or acceptability in terms of stereotypes such as crowds pairs.</sample>
    <sample id="845">And in this minimal pair paradigm, the typical way to evaluate language models is that you show like an acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence</sample>
    <sample id="846">and then the hope is that the model basically puts more probability to the acceptable sentence</sample>
    <sample id="847">The current mpp pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences</sample>
    <sample id="848">These days large language models are coming up with longer and longer context windows, so it's crucial that we evaluate the model's acceptability throughout the context window.</sample>
    <sample id="849">And that is what we are trying to do here. We're trying to revisit the npp pipeline by asking the model to evaluate acceptability on longer and longer sequence</sample>
    <sample id="850">So that is the approach so what we do is to simulate these longer sequences revisit the data sets themselves and then recreate sentences by choosing like acceptable or unacceptable sentences from those data</sample>
    <sample id="851">So for example, here we have chosen like a typical pair of grammaticality from the blimp dataset from the adjunct island case.</sample>
    <sample id="852">And what we do is that to recreate like longer sequences and which are acceptable, and which has the same matching of the grammatical structure. We extract grammatical sentences from adjacent islands</sample>
    <sample id="853">and then we add it as a prefix to both the acceptable query and unacceptable query.</sample>
    <sample id="854">So we can do the same thing by choosing unacceptable sentences from the same matching and that could also like be used to test the model's acceptability.</sample>
    <sample id="855">And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario</sample>
    <sample id="856">So here the sentences are still coming from a relevant data sets, but it's not from the same data set that you're evaluating with and we can do the same for unacceptability case.</sample>
    <sample id="857">Finally, we can choose sentences from a completely unrelated domain such as Wikipedia.</sample>
    <sample id="858">So this will tell us whether the model's acceptability judgments are actually impacted by any context.</sample>
    <sample id="859">like whether the context is coming from a different subset of the dataset or whether it's like completely irrelevant to the current, uh, you know, sentence that we are looking at.</sample>
    <sample id="860">So how does the Potter do? So first we look at the Wikipedia sentences which are completely irrelevant to the current query pair and there we find that the MPP judgments are mostly robust for arbitrary context length.</sample>
    <sample id="861">We increase the context length toward up to 1024 for two max out OPT and GPT-2 models, and we saw here in the orange dotted line that MPP judgments are relatively stable.</sample>
    <sample id="862">Now what happens when we choose sentences from the same data set?</sample>
    <sample id="863">So here we are choosing or creating sentences from acceptable and unacceptable domains, from the same blimp or syntax gym dataset.</sample>
    <sample id="864">and there we see that the mpp judgments either increase or decrease significantly when you add uh either acceptable prefixes or unacceptable</sample>
    <sample id="865">but when we match the structure that is, when we choose the sentences from the same phenomena in blame person texts, jim</sample>
    <sample id="866">we see a massive increase or decrease of the mpp judgment for the model depending on whether the chosen prefix is acceptable or unacceptable</sample>
    <sample id="867">now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window</sample>
    <sample id="868">So why does the match prefix affect the language model judgment so much?</sample>
    <sample id="869">so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding uh like noise to the input and after doing like several of these perturbations</sample>
    <sample id="870">we find that none of these noises are actually making the model change its course in terms of how it shows us the MPP generation trend.</sample>
    <sample id="871">Basically we find that the models are sensitive to the perturbed sentences in similar ways.</sample>
    <sample id="872">That is when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain, we see decrease in mpp judgments in a similar fashion.</sample>
    <sample id="873">So the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences.</sample>
    <sample id="874">And the mpp evaluation that the way that we do it currently with short and single sentence input may not fully capture the language models abstract knowledge throughout the context window</sample>
    <sample id="875">Please read our paper for more details of our experiments. Thank you for listening</sample>
    <sample id="876">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="877">The speaker's name is Ariel Bielaw.</sample>
    <sample id="878">The prompting has a big influence on the performance of LLMs for translation.</sample>
    <sample id="879">The affiliations of the authors are: University College London, University of Edinburgh, and University of California San Diego.</sample>
    <sample id="880">The 5 expert-written instructions are: 1. Clean the data; 2. Preprocess the text; 3. Train a model on the dataset; 4. Evaluate performance metrics; and 5. Deploy to production environment</sample>
    <sample id="881">The authors propose to test the models on using information from multiple sources by introducing a coreference resolution task.</sample>
    <sample id="939">The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="940">The paper involves 5 authors.</sample>
    <sample id="941">The background knowledge needed is that judges decide cases in law courts.</sample>
    <sample id="942">Yes, the code is available on GitHub.</sample>
    <sample id="943">No, the annotators are not balanced in regard to each demographic.</sample>
    <sample id="944">We tried to perturb the input sentence by trying to preserve the relevant structure but adding noise to the input.</sample>
    <sample id="945">To have a dimensional evaluation means to assess multiple aspects or dimensions of something, in this case dialogue quality. This allows for a more detailed and nuanced understanding of the strengths and weaknesses of the model at a finer-grained level.</sample>
    <sample id="946">The affiliations of the authors are: University of Science and Technology of China, Tsinghua University, and Microsoft Research Asia.</sample>
    <sample id="947">Zero and one shot prompting.</sample>
    <sample id="948">Hello, my name is Vasudha and I'm a computer science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge</sample>
    <sample id="949">We begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent</sample>
    <sample id="950">Suggest this example where a person states I know that cigarettes could kill me and then goes on to say I grabbed a couple of smokes after the meeting. This belief and action are inconsistent, and they are in dissonance</sample>
    <sample id="951">Further mentioning that I don't think I could keep my job without them justifies the second occurrence and they have a consonance relationship.</sample>
    <sample id="952">While dissonance is a very common phenomenon we experience in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations.</sample>
    <sample id="953">So why does this matter studying cognitive distance can help us understand the effects of disagreement among people track trends and belief values in attitude changes in populations</sample>
    <sample id="954">High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better</sample>
    <sample id="955">Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups.</sample>
    <sample id="956">Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better.</sample>
    <sample id="957">To the goal of creating a cognitive dissonance resource we conducted a large scale annotation of dissonance relations. We used dissonance first approach as seen in the flow chart here</sample>
    <sample id="958">Tweets were parsed using a pre DB parcel and pairs of discourse units where annotated according to the guidelines that are described in our paper</sample>
    <sample id="959">As can be seen here, dissonance was only found in 3.5% of the annotated pairs</sample>
    <sample id="960">On collecting around 1000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of disnets. To no surprise the classifier performed not much better than chance.</sample>
    <sample id="961">Given the low occurrence of dissonance and absence of any prior such data set we are facing the problem of absolute rarity</sample>
    <sample id="962">To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs lowering the overall annotation costs while improving dissonance detection.</sample>
    <sample id="963">Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks.</sample>
    <sample id="964">We transfer from two different tasks topic independent disentangled stance classification a task that determines if to debate statements from different people are in agreement or in disagreement irrespective of topic</sample>
    <sample id="965">called debate here and on binary classification of expansion in comparison classes of p r t b since these two are closely related to the conception of consonance and dissonance and we call them c e e here</sample>
    <sample id="966">We find that on transferring the zero shot performance on the annotated data set is already much better than chance with a best with AUC 0.62</sample>
    <sample id="967">Further on iteratively fine-tuning on both tasks we find that fine-tuning of CE tasks followed by further fine-tuning on debate yields a much better zero-shot performance Thus this is the model that we used to cold start the active learning</sample>
    <sample id="968">Next, we determine the best method to update a model with new data from each round of active learning and annotations.</sample>
    <sample id="969">Over the different strategies we found accumulative performed equal or better than iterative across the board</sample>
    <sample id="970">Next, to improve the number of dissonant examples we use a probability of rare class strategy PRC to select mostly the examples that are highly likely to be dissonant by the current model at any round of AEL.</sample>
    <sample id="971">We compared this to the other state of the art strategies that are commonly used in the community</sample>
    <sample id="972">We find that the proposed PRC strategy works better than other state-of-the-art strategies although the difference is small note that the performance is significantly lower for random</sample>
    <sample id="973">On further rounds of AL with two best strategies we improve distance classification AUC to 0.75 which is the best performance that we have on the task so far</sample>
    <sample id="974">We also check the feasibility of each strategy for annotation quality and costs to annotators we find that prc has a highest percentage of dissonance and works best for rare class however, the annotators also find examples difficult</sample>
    <sample id="975">In summary, we find that PRC is a simple AL strategy for rare class acquisition and co-starting AL with appropriately designed transfer learning tasks can help significantly.</sample>
    <sample id="976">We also find that iterative update is useful for transfer learning from a different domain versus in-domain active annotations benefit from cumulative updates.</sample>
    <sample id="977">These are the links to our code dataset and our paper. Feel free to get in touch with us if you have any questions, thank you</sample>
    <sample id="978">The authors evaluated several dialog models, including:</sample>
    <sample id="979">The paper "Protecting the Copyright of Large Language Models for Embedding and Services via Backdoor Watermark" has 4 authors: Jinwei Yi, Yuhang Zhang, Zhenyu Wang, and Xiangliang Zhang.</sample>
    <sample id="980">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="981">I'm sorry, but I can't determine the number of authors involved in this paper based on the information provided. The text only mentions one person (MC Yu) from Fudan University introducing their work "Distinctive Script Knowledge From Language Models For Constraint Language Planning." To answer your question about how many authors are involved, you would need to refer to the full paper or its publication details where authorship is typically listed.</sample>
    <sample id="982">The speaker's name is Vasudha.</sample>
    <sample id="983">The affiliations of the authors are: University College London, UK; Humboldt-Universität zu Berlin, Germany.</sample>
    <sample id="984">Hello everyone, my name is Yuxin Zhang from the Penn State University. Today I'm going to present our work Exemplar: Cross-lingual Semantic Parsing in Multiple Natural Languages and Meta Representations</sample>
    <sample id="985">So semantic parsing is a task to build semantic representations of user queries such as SQL and lambda calculus</sample>
    <sample id="986">And cross-lingual semantic parsing is the task to translate queries in multiple natural languages into a meaningful representation.</sample>
    <sample id="987">That's right. In this figure, we need to translate the query into multiple natural languages using neural models like SQL, Lambda, or FunkQL, and so on.</sample>
    <sample id="988">Existing cross-lingual semantic parsing models are separately proposed and evaluated on dataset of limited tasks and applications for instance,</sample>
    <sample id="989">There are licks of um coverage on certain natural language the chinese is missing and</sample>
    <sample id="990">Lack of coverage on certain menu representations.</sample>
    <sample id="991">The lambda calculus is missing</sample>
    <sample id="992">Or they're only evaluated on certain neural model for example, there's only one single model to evaluate the</sample>
    <sample id="993">So to this end, we propose ExemplarWe provide a uniform dataset exemplar for cross-lingual semantic parsing in multiple natural languages and meaning representations.</sample>
    <sample id="994">It contains nine datasets in various domains, five semantic parsing tasks, eight million representations and 22 natural languages in fifteen language families.</sample>
    <sample id="995">And to better evaluate our benchmark, we consider the six settings for training and evaluation</sample>
    <sample id="996">The first one is TranslateTest. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation</sample>
    <sample id="997">And for example, we train the English model on English query and during inference, we translate the German query using API to English and then use the trained model to predict the SQL.</sample>
    <sample id="998">And we'll also test monolingual models</sample>
    <sample id="999">In this setting, the source language is the same as target language for example, German to German or English to English</sample>
    <sample id="1000">We also test monolingual few-shot setting by training monolingual models with only 10% of training data.</sample>
    <sample id="1001">And with has the monolingual multilingual model which are which train one multilingual model for all languages</sample>
    <sample id="1002">For example, we put the german english chinese queries together to train a multilingual model and during inference we can use this model</sample>
    <sample id="1003">To translate German queries or Chinese query etc</sample>
    <sample id="1004">And we also consider cross-lingual zero-shot and few-shot transfer. We train on one source language and transfer to another language.</sample>
    <sample id="1005">So during training, we train it on English query or the combination of English and German few-shot queries to train a multilingual model to predict the SQL output</sample>
    <sample id="1006">And we also find many interesting results so regarding analyze of monolingual models will you value it on two groups some models</sample>
    <sample id="1007">Including encoder ptr which stands for multilingual pre-trained encoders with pointer-based decoders such as xlmr plus ptr and bert plus ptr.</sample>
    <sample id="1008">And we also evaluate encoder decoder models, which is multilingual pre-trained encoder decoder models such as and bart mt five.</sample>
    <sample id="1009">We found that encoder decoder obtains the best performance on all nine data sets</sample>
    <sample id="1010">And we evaluate on MT5 and XLMR plus BDR on multilingual setting</sample>
    <sample id="1011">Without that, encoder decoder or encoder PDR can be improved by training in a mixture of various languages</sample>
    <sample id="1012">And we found it is because most of the major natural languages can obtain performance gain except that English performance drops in seven data sets and only gains in three data sets</sample>
    <sample id="1013">I think this is known as curse of multilinguality</sample>
    <sample id="1014">We also compare the cross-lingual performance gap</sample>
    <sample id="1015">In this figure, the blue line is cross-lingual few-shot transfer. The orange line is cross-lingual zero-shot transfer while the green line is in a monolingual setting.</sample>
    <sample id="1016">We found that by comparing the green and orange line, we found for zero-shot setting, the cross-lingual transfer performance gap is significant. And by comparing blue and orange line, we found that with few-shot setting, the transfer gap is shortened rapidly.</sample>
    <sample id="1017">We also find some other interesting findings for example encoder decoder all performs progress work or achieved comparable results between our english natural language has significantly boost the performance of future on target and that natural languages</sample>
    <sample id="1018">And we found multilingual language models such as Codex and Blue are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="1019">To sum up, we build Exemplar, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and main representations.</sample>
    <sample id="1020">We conduct a comprehensive benchmark study on three representative of types of multilingual language models and our results shows many interesting findings etc And welcome to visit our paper and code thanks for listening</sample>
    <sample id="1021">The most common error are omission errors.</sample>
    <sample id="1048">Emory NLP Lab, Emory University and Amazon Alexa AI</sample>
    <sample id="1049">Clean, manually annotated samples</sample>
    <sample id="1050">There are 7 authors involved in the paper.</sample>
    <sample id="1084">The speaker's name is Yuxin Zhang.</sample>
    <sample id="1085">Hi, I'm Jiang Bing, PhD student in the University of Washington. Today I'm presenting our work from pre-training data to language models to downstream tasks: tracking the trails of political biases leading to unfair NLP models.</sample>
    <sample id="1086">So language models are trained on large-scale web crawl data</sample>
    <sample id="1087">Political news media are well-covered in their pre-training data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post etc., are well-covered in language model training data.</sample>
    <sample id="1088">This has created a mixed blessing for language model applications</sample>
    <sample id="1089">So on one hand, they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas. On the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications</sample>
    <sample id="1090">To this end, we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks specifically by asking the following questions:</sample>
    <sample id="1091">First, how do we evaluate the political leaning of language models and what role does pre-training data might have on such political biases?</sample>
    <sample id="1092">Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications?</sample>
    <sample id="1093">So specifically, we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test. This ensures us to do automatic evaluation while grounded in political science literature</sample>
    <sample id="1094">So some preliminary results demonstrate that first language models do have varying political leanings. They occupy all four quadrants on the political compass,</sample>
    <sample id="1095">We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BERT series and its variants.</sample>
    <sample id="1096">Secondly, we aim to investigate to which extent the political biases of language models are picked up from training data.</sample>
    <sample id="1097">So we could conduct a controlled experiment by further pretraining language model checkpoints on six different partisan corpora separated into news and social media, further divided into their political leanings.</sample>
    <sample id="1098">By further pretraining language models on such partisan corpora, we can see that the ideological coordinates of the language model also correspondingly shift.</sample>
    <sample id="1099">For example, for Roberta further fine-tuned to and further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its</sample>
    <sample id="1100">In terms of its political biases.</sample>
    <sample id="1101">And we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society.</sample>
    <sample id="1102">So we divide pre-training corpora into pre 45th president of the united states and after 45th president of the united states, we separately pre-train language models on the two different temporal corpora.</sample>
    <sample id="1103">We can see that language models generally had a political leaning that is further away from the center after 2017. So this indicates that language models can also pick up the polarization in our society</sample>
    <sample id="1104">So last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications.</sample>
    <sample id="1105">So we see that if we investigate the per category performance, that is to say, if we separate the performance into</sample>
    <sample id="1106">Different demographics or political leaning news media, we can see a pattern that for example, for hate speech detection left-leaning language models are better</sample>
    <sample id="1107">Detecting hate speech targeting socially minority groups</sample>
    <sample id="1108">However, our work said detecting hate speech targeting more powerful groups in our society.</sample>
    <sample id="1109">And vice versa, right-leaning language models are better at detecting hate speech targeting white and men; however worse at detecting hate speech targeting black, lgbtq+ and other minority communities.</sample>
    <sample id="1110">Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa.</sample>
    <sample id="1111">We further show many qualitative examples to see that language models with different political leanings</sample>
    <sample id="1112">Do give different predictions to hate speech and misinformation examples based on their social categories there are a bunch of more examples in the appendix too further highlight that</sample>
    <sample id="1113">This indicates that there is a fairness issue that is very pressing regarding the political biases of language models</sample>
    <sample id="1114">For example, if a right-leaning language model were to be fine-tuned on hate speech or misinformation and deployed to a popular social media platform</sample>
    <sample id="1115">This would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups may just run rampant without any control.</sample>
    <sample id="1116">So this has sounds the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings.</sample>
    <sample id="1117">So a little bit of discussion, we would also like to highlight that we expose the unique dilemma regarding language model political biases between Scylla and Charybdis.</sample>
    <sample id="1118">So if we do not sanitize the political opinions in language model training data, the bias would propagate from pre-training data to language models to downstream tasks ultimately creating fairness issues.</sample>
    <sample id="1119">If we do try to sanitize somehow, we will also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retained in language model training data. So it's kind of like the electric electric charlie problem</sample>
    <sample id="1120">Okay, great. I think that's pretty much all I have for today. Thank you for your time</sample>
    <sample id="1121">It does not have a name.</sample>
    <sample id="1122">The author described the "marked words" method as a way to identify the words that distinguish marked groups from unmarked ones.</sample>
    <sample id="1123">University of Washington</sample>
    <sample id="1124">The Prague approach</sample>
    <sample id="1125">The speaker is James Finch.</sample>
    <sample id="1126">The paper has 4 authors.</sample>
    <sample id="1127">The minimal pair paradigm evaluates language models on top of acceptability judgments, which can include grammaticality, syntax, or acceptability in terms of stereotypes.</sample>
    <sample id="1128">Hello, my name is Kaiyuan and I will be presenting our work titled When Does Translation Require Context: A Data-Driven Multilingual Exploration. This work was done in collaboration with Patrick Fernandes, Amy Liu, Andre F. D. Martins, and Graham Neubig.</sample>
    <sample id="1129">So a lot of translations depend on context. For example, how would we translate mole in this sentence?</sample>
    <sample id="1130">Well, if the previous sentence was things could start to get dangerous if the ministers find out, then mole refers to a spy. But if the previous sentence was Could it be anything serious, doctor? Then mole refers to a birthmark.</sample>
    <sample id="1131">So depending on context, the meaning of the word changes and therefore its translation changes as well.</sample>
    <sample id="1132">However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like blue unable to capture these translations</sample>
    <sample id="1133">And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation.</sample>
    <sample id="1134">In this work, we try to answer these two questions: First, when does translation require context? And second, how well do models handle these cases?</sample>
    <sample id="1135">To answer the first question, we started by measuring how much a word depends on context when translated.</sample>
    <sample id="1136">And the previous work we introduced CxMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y given the source X.</sample>
    <sample id="1137">You can think of CXMI as the information gained from giving context to the model.</sample>
    <sample id="1138">In this work, we extend cMI to pointwise cMI, which can measure context usage at the sentence level or at the word level. We can think of words that have high p cMI as ones that require context for translation.</sample>
    <sample id="1139">Now we analyze words with high psmi to look for patterns between these words.</sample>
    <sample id="1140">And we perform our analysis on transcripts of TED Talks that have been translated from English to 14 different languages.</sample>
    <sample id="1141">We perform our analysis at three different levels. First, we look at part of speech tags that have high means PCMI</sample>
    <sample id="1142">And this allows us to find, for example, dual pronouns in Arabic that have relatively high PSMI. And this can be explained because English doesn't have dual pronouns so you need context to determine if a pronoun is dual when translating into Arabic.</sample>
    <sample id="1143">And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high p-semi averaged over all of its different occurrences</sample>
    <sample id="1144">And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document.</sample>
    <sample id="1145">And similarly, we find that context is supported to translate in the right formality.</sample>
    <sample id="1146">And finally, we look at different individual tokens that have high p-SMI. And this allows us to identify phenomena that cannot really be captured by the word itself but that's rather expressed in a sentence structure such as ellipsis resolution.</sample>
    <sample id="1147">So now we use our findings from our analysis to design a benchmark for document-level translations.</sample>
    <sample id="1148">For each of the five discourse phenomena we identified, we created taggers to automatically identify words that pertain to the phenomenon. And we call our tagger the Multilingual Discourse Aware or MUDA tagger.</sample>
    <sample id="1149">We can then also note that different languages have different proportions of these discourse phenomena.</sample>
    <sample id="1150">We then use the MuDA tagger by applying the tagger on a parallel corpus that we want to use for evaluation, and we apply our translation metrics of choice on the context-dependent examples that the MuDA tagger has identified.</sample>
    <sample id="1151">And finally, we use our benchmark as well as other metrics to evaluate different models on document-level machine translation.</sample>
    <sample id="1152">First of all, when we use corpus-level metrics so for blue, we find that collocus-agnostic models have the best performance.</sample>
    <sample id="1153">But then if we use comment context-aware models perform best and if we use word f measure that models with or without context have comparable performance.</sample>
    <sample id="1154">This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone.</sample>
    <sample id="1155">Now we use the movie benchmark to evaluate models, and we find that context summary models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="1156">But these models are not much better than models that do not use context on other phenomena, like ellipses, pronouns and verb forms. So this suggests where we would need to see more progress for document-level translation.</sample>
    <sample id="1157">We also compared different commercial systems, and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translations.</sample>
    <sample id="1158">To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context.</sample>
    <sample id="1159">And then we use our findings to build a benchmark for document-level machine translation, which can help us identify which discourse phenomenon models can handle well or not and which translation systems are good at document-level translations.</sample>
    <sample id="1160">Thank you so much for your attention. See you in Toronto</sample>
    <sample id="1161">DGC, DGC+, WSL, WSL+, and WSL++.</sample>
    <sample id="1162">Biomedical and clinical downstream tasks</sample>
    <sample id="1163">Hi, welcome to our presentation of DeepLing: a new corpus for German text identification on the document level and on the sentence level.</sample>
    <sample id="1164">My name is Regina Stauden and I will guide you through the first part of the presentation. Let's first define text simplification</sample>
    <sample id="1165">Text simplification is the process of adapting a text to improve the text comprehension of it for specific target groups, e.g. people with reading problems or non-native speakers.</sample>
    <sample id="1166">To train a text identification model, we require parallel pairs of texts for example documents or sentences.</sample>
    <sample id="1167">In the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language.</sample>
    <sample id="1168">To simplify the sentence, different techniques are possible as you can see in the example such as lexical substitution, clause deletion, clause deletion reordering or insertion of words.</sample>
    <sample id="1169">We now propose our new corpus to the plan because in the recent years there were some problems with existing corpora. So for example, these corpora here are too small to train a text and figure model on.</sample>
    <sample id="1170">The other three models which are proposed in recent years are all automatically aligned, which means they can be error-prone and their alignments.</sample>
    <sample id="1171">Therefore, we propose our new corpus dplane which is split into two sub-corpora: dplane apa and dplane web. Dplane apa is based on news texts</sample>
    <sample id="1172">In DeepLing-APA, we aligned 483 documents all manually. It results in roughly thirty thousand parallel sentence pairs.</sample>
    <sample id="1173">For DeepL-Web, this corpus includes different domains and we also align all of these 750 documents on the one hand manually and on the other hand with automatic alignment methods.</sample>
    <sample id="1174">And total will result in 30,450 sentence pairs.</sample>
    <sample id="1175">We analyzed our sentence pairs a little bit more, so for example on the type of simplification.</sample>
    <sample id="1176">As you can see here, the Bible texts are much stronger simplified than for example the news text or the language learner texts.</sample>
    <sample id="1177">On all levels regarding, for example, lexical simplification, structural simplification or the overall level of simplification.</sample>
    <sample id="1178">Furthermore, you can see that our dplane corpus has a high variety of different simplification transformations. So for example in the dplane API corpus we have much more reorderings and word additions than we have in the dplane web corpus.</sample>
    <sample id="1179">On the other hand, in the web corpus we have much more rephrasings.</sample>
    <sample id="1180">So let's now see what we can do with this corpus.</sample>
    <sample id="1181">In the recent years, there has been a lot of alignment methods but in the context of machine translations.</sample>
    <sample id="1182">We have two parallel documents written in different languages and we want to extract alignments of sentences in both documents.</sample>
    <sample id="1183">But in our use case, we are trying to extract alignments between sentences of two parallel documents having the same language and content but they are on a different complexity level.</sample>
    <sample id="1184">And now as we have our dataset D plane which has manually aligned sentences, we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods.</sample>
    <sample id="1185">And we did some adaptations to the proposed methods and we have published all these adaptations and codes to run our experiments in the paper.</sample>
    <sample id="1186">At the end, we concluded that the best alignment automatic alignment method to use for German text simplification is the method of MassAlign.</sample>
    <sample id="1187">And you can also find the code to run this method on your own documents in the paper.</sample>
    <sample id="1188">The second use case that we showed in our paper is a case of automatic text simplification.</sample>
    <sample id="1189">By fine-tuning language models to produce simplified text from complex input text.</sample>
    <sample id="1190">We have fine-tuned two different models. We have fine-tuned the model of long import to produce document-level simplifications</sample>
    <sample id="1191">And we also fine-tuned the normal base MPart to produce sentence-level simplifications.</sample>
    <sample id="1192">You can also find all the checkpoints and you can look into more details at scores and evaluation metrics of our experiments in the paper.</sample>
    <sample id="1193">We concluded that this basic fine-tuning could produce or get scores better than the baseline scores.</sample>
    <sample id="1194">And we propose those results as a benchmark, a base benchmark for the problem of automatic text simplification in the future.</sample>
    <sample id="1195">Thank you so much for your attention and we hope to meet all of you during the conference. Thank you</sample>
    <sample id="1196">Hi, and I'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the alt entity scorers.</sample>
    <sample id="1197">My name is Jawad Hosseini and this is a joint work with Philippe Ratlinski, Silvia Parodi and Annie Lewis.</sample>
    <sample id="1198">Our goal is to understand users' language when they want to make a choice consider this alternative question did you mean easy on me or I got a feeling here a user wants to select between one of these two songs</sample>
    <sample id="1199">The most obvious thing is to use a direct reference, for example by saying the name of the song Easy On Me or its position: The first one.</sample>
    <sample id="1200">But sometimes an indirect reference is more appropriate to have a more natural conversation this could happen when the user cannot remember the name of the song</sample>
    <sample id="1201">Or the pronunciations are too similar to each other and hard to disambiguate</sample>
    <sample id="1202">Or when the user wants to specify a preference here are some example indirect references for example, the newer one or the song that's not energetic</sample>
    <sample id="1203">This is an important problem in conversational systems and also for benchmarking LLMs entity understanding.</sample>
    <sample id="1204">We're not aware of a public data set, a large-scale public data set for the task. So we collect one using crowd annotation. Our data set covers three different domains: music, books and recipes</sample>
    <sample id="1205">Our dataset collection methodology emphasizes informativity using a cartoon completion set.</sample>
    <sample id="1206">The cartoon has three speech bubbles. In the first bubble, Bob says Remember that song we were listening to yesterday? And with that, Bob sets the dialogue context</sample>
    <sample id="1207">In the second speech bubble Alice says Do you mean easy on me or I've got a feeling?</sample>
    <sample id="1208">In the third speech bubble, Bob uses an indirect reference to select one of these entities. For example:</sample>
    <sample id="1209">We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain</sample>
    <sample id="1210">The second one which is the alternative question is generated as follows</sample>
    <sample id="1211">We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia</sample>
    <sample id="1212">Here are the different sampling methods we've used when we move higher in the list, the entities become more similar to each other and it's usually harder to make the disambiguation.</sample>
    <sample id="1213">The first one is uniform attraction.</sample>
    <sample id="1214">The second one is when the entities have similar titles, for example two books with the name 'the retail'.</sample>
    <sample id="1215">The third one is when they have similar descriptions on Wikipedia and finally, even if they have similar infoboxes or attributes on Wikipedia for example the same genre or the same artist</sample>
    <sample id="1216">When we show these alternative questions to the annotators, they know the name of these entities but don't necessarily know about the entity.</sample>
    <sample id="1217">So what we do is that we show some background knowledge about the two entities for songs. We simply show a google search link to each song</sample>
    <sample id="1218">And then asked the annotators to listen to at least some of each song and read about each song. Here's for example, the Google search result for the song "Easier".</sample>
    <sample id="1219">For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally showed their images again from Wikipedia so that annotators know how they look like</sample>
    <sample id="1220">Then we ask the annotators to pick one of these entities for example here, the first one and describe them using three to five indirectly referring expressions.</sample>
    <sample id="1221">For example, the one with the piano music here are some examples from our data set for example without words not the one with the twelve year old twelve year old boy or the fictional one or comes from other by john and so</sample>
    <sample id="1222">The entity's corpus has 6000 alternative questions across three domains and it has 42,000 indirect referring expressions results with t5 x large model are summarized below</sample>
    <sample id="1223">If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high it's around 92-95 percent but this is not realistic</sample>
    <sample id="1224">If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82-87%, which is more realistic for example when the language model retrieves the background knowledge.</sample>
    <sample id="1225">If the language model has access only to entity names, then the accuracy is only 60%, so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link toward data set thanks</sample>
    <sample id="1226">CamemBERT is initially trained on the 4 gigabyte subset of NACSOs.</sample>
    <sample id="1227">The speaker's name is Szymon Skurkowski.</sample>
    <sample id="1228">The main cause of the performance drop is temporal drift.</sample>
    <sample id="1229">Hi everyone I'm Jenny a first year PhD student at Carnegie Mellon University and today I'll be presenting our work Anel Positionali Characterizing Design Biases of Datasets and Models</sample>
    <sample id="1230">This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Senti, Ronan Le Bras, Katerina Rynika, and Martin Zapf.</sample>
    <sample id="1231">So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content.</sample>
    <sample id="1232">You might turn towards a popular api like perspective api for toxicity detection and this works really well if you're carl jones where perspective api is able to detect correctly toxic instances</sample>
    <sample id="1233">But that's not really the case for Adithya Sharma, where Prospective API is really not as sensitive to offensive terms that are more common in Indian contexts.</sample>
    <sample id="1234">This is an example of a design bias, where we see systematic performance differences in technology between populations.</sample>
    <sample id="1235">Design biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences</sample>
    <sample id="1236">This is a concept widely used in critical studies, specifically in feminist and queer academic spaces.</sample>
    <sample id="1237">And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make</sample>
    <sample id="1238">And so one question that people might ask is do data sets and models have positionality?</sample>
    <sample id="1239">And we're not trying to say that models and cells in data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people and can thus represent certain positionalities over others.</sample>
    <sample id="1240">So prior work has suggested some anecdotal evidence of having positionality, such as cultural gaps in models and datasets, as well as theoretical definitions of model positionality.</sample>
    <sample id="1241">However, these works really don't look at comparing end users with the datasets and models themselves.</sample>
    <sample id="1242">And studying model and dataset positionality is increasingly important as NLP tasks become more subjective and socially oriented.</sample>
    <sample id="1243">And it's challenging to characterize how these positionalities are skewed because not all decisions are documented and many models are hidden behind apis.</sample>
    <sample id="1244">So to study dataset and model positionality, we actually compared the annotations with real users with existing datasets and models.</sample>
    <sample id="1245">We do this through a framework, and l positionality</sample>
    <sample id="1246">Our framework works in two main steps:</sample>
    <sample id="1247">The first step is to re annotate data sets with diverse annotators</sample>
    <sample id="1248">And we opt to do this over looking at the demographics of original data sets annotators because usually only a few annotators annotate each instance and because demographics are rarely collected and shared.</sample>
    <sample id="1249">And so we opt to re-annotate data to get many annotated per instance and to get a rich set of demographic data.</sample>
    <sample id="1250">We then take the annotations by demographic and compare them to the models in data sets using a pearson's r correlation score</sample>
    <sample id="1251">And thus our framework actually differs from annotator disagreement literature by comparing end users with models and data sets, predictions and labels as opposed to looking at just inner to annotator agreement or modeling annotator distributions.</sample>
    <sample id="1252">Our framework is largely enabled through lab in the wild, an online crowdsourcing platform for our HCI collaborators</sample>
    <sample id="1253">And live in the wild is an online experimentation platform where we can recruit diverse volunteers compared to platforms like m turk which largely have participants from the us or india and further lab in the wild stills able to get high quality data</sample>
    <sample id="1254">We host two tasks on labofthewild, one of them being social acceptability and the way this works is that participants will read a situation from the social chemistry dataset and then they'll rate how socially acceptable a situation is.</sample>
    <sample id="1255">Afterwards, to stay engaged in the study they can compare their responses to an AI and others.</sample>
    <sample id="1256">We then compared these annotations with Social Chemistry, Delphi and GPT-4.</sample>
    <sample id="1257">We then replicated a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dyna Hate and rate whether they think it's an instance of hate speech.</sample>
    <sample id="1258">We then compared these annotations with Dynahate, Perspective API, Rewire API, HateRoberta and GPT-4. Our study in the end amassed over 16,000 annotations from over a thousand annotators from 87 countries</sample>
    <sample id="1259">So now we're better equipped to answer who do nlp data sets and models align with the most? We find that there is positionality in nlp</sample>
    <sample id="1260">For example, we find that data sets and models are most aligned to English speaking countries so for the GPD four social acceptability analysis. We find that it's most aligned to confusion in English-speaking countries. We find that Dina hate is also most likely to English-speaking countries</sample>
    <sample id="1261">We also find most additional alignment with people who have a college education. So for GPT-4 in the social acceptability task, we find that it's most aligned to people with a college education or graduate school education</sample>
    <sample id="1262">And we find the same for donahaye where it's most aligned to people with a college education</sample>
    <sample id="1263">However, when models and datasets are aligned to specific populations some are inevitably left behind</sample>
    <sample id="1264">An example of this is that data sets and models are less aligned to non-binary people compared to the men and women counterparts we find this in the GPT-4 social acceptability task as well as the DynaHeAT task analysis as well</sample>
    <sample id="1265">So given that there is physician analogy in l p what can we do about it</sample>
    <sample id="1266">So we have a few recommendations for this first one is keep a record of all relevant design choices throughout the research process and the other is to do NLP research with the lens of perspectivism</sample>
    <sample id="1267">Our third recommendation is to build specialized data sets and models within four specific communities, and a good example of this is the Masakani initiative. I mean we want to emphasize that inclusive NLP isn't just making you know all technologies work for everyone</sample>
    <sample id="1268">And so that concludes our presentation, but if you'd like to learn more feel free to check out our dashboard for the most updated analysis results and our paper. Thank you</sample>
    <sample id="1269">To put them into the right order.</sample>
    <sample id="1270">The authors recommended increased transparency about bias mitigation methods because it's important to understand the underlying mechanisms that lead to positive stereotypes in AI models. This helps identify whether these patterns are due to overemphasis on value alignment or other anti-stereotyping techniques, ensuring more accurate and fair model performance evaluation.</sample>
    <sample id="1271">Minimal-pair unacceptable inputs are ungrammatical sentences used in language model evaluation.</sample>
    <sample id="1272">The authors used the following evaluation metrics:</sample>
    <sample id="1273">Inter-annotator agreement on 100 doubly labeled conversations</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">The affiliations of the authors are: University of Bremen, Germany; University of Potsdam, Germany.</sample>
    <sample id="1276">MultiInstruct differs from other benchmarks in that it is the first large-scale multi-modal instruction-tuning dataset. It provides a comprehensive collection of 1,632 tasks across various modalities such as text, image, and video, which allows for more diverse evaluation scenarios compared to existing datasets like HumanEval and OpenInstruction. Additionally, MultiInstruct includes detailed task descriptions, human evaluations, and performance metrics, making it easier to analyze and compare results across different models and approaches.</sample>
    <sample id="1277">The paper has 10 authors.</sample>
    <sample id="1278">Binary coordination is a linguistic phenomenon where two words are connected in such a way that they form a single unit, with each word having its own distinct meaning and function within the sentence.</sample>
    <sample id="1279">The average length of the prompts used in this study was 10 words.</sample>
    <sample id="1280">The findings suggest that smaller models, like the T5 model in this case, can achieve high-quality results when properly trained on suitable datasets. This implies that resource constraints or computational limitations may not necessarily hinder performance if appropriate data and training methods are employed. It also indicates potential for more efficient language processing solutions using less powerful hardware compared to larger models.</sample>
    <sample id="1309">Four from scratch models and three model trained on continual pre-training.</sample>
    <sample id="1310">The factor of overfitting due to test reuse specifically is not observed.</sample>
    <sample id="1311">The quality of the simplification was evaluated by comparing it with baseline scores.</sample>
    <sample id="1312">Yes, language models have different political biases.</sample>
    <sample id="1347">Two beliefs or actions that are inconsistent.</sample>
    <sample id="1348">GPT-4 is the most liberal language model of them all.</sample>
    <sample id="1349">Yes</sample>
    <sample id="1350">The speaker is Sarah Papi.</sample>
    <sample id="1351">The data was taken from the MuDa benchmark.</sample>
    <sample id="1352">Hi, my name is Adam Skorupa and this talk is about the dependency structure of coordination.</sample>
    <sample id="1353">As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example in the universal dependencies, the structure of the coordinate coordination Lisa Bart and Maggie</sample>
    <sample id="1354">Is such that the first conjunct is the head of the whole coordinate structure, so in this case Lisa.</sample>
    <sample id="1355">Similar approaches are used in Igor Milchuk's meaning text theory, where again the whole coordinate structure is headed by the first conjunct. So these two approaches are symmetric: they single out one of the conjuncts</sample>
    <sample id="1356">Now, there are also symmetric approaches to coordinate structures such as the PRAG approach, the conjunction-headed approach assumed in PRAG dependency treebanks where coordinate structures are headed by the conjunction.</sample>
    <sample id="1357">So we get some dependencies from AND to all the conjuncts.</sample>
    <sample id="1358">And finally, there's also a multi-headed approach that is used for example in the Cutkos word grammar.</sample>
    <sample id="1359">where all conjuncts are heads of the coordinate structure, so we get dependencies from the governor "he laughs" to all conjunct separately: Lisa bought and Meg.</sample>
    <sample id="1360">Now, the aim of this paper is to produce a novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these.</sample>
    <sample id="1361">OK, the argument is based on the principle of dependency minimization that I will explain on the basis of these examples.</sample>
    <sample id="1362">So in English, as you might know direct objects prefer to be close to the verb while adjuncts may be further away. So "Marge read it yesterday" is fine because the direct object "it" is close to the verb</sample>
    <sample id="1363">While "March" read "yesterday", it is much worse, right? Because here between the verb and the direct object there is an adjunct "yesterday".</sample>
    <sample id="1364">However, this effect may be ameliorated when the direct object is very heavy and very long because then it can be moved to position after the agent.</sample>
    <sample id="1365">This is illustrated here, so both these sentences are fine: "Mark read this absolutely fascinating book about the BC yesterday" is okay. The way instead of it we have this long NP</sample>
    <sample id="1366">But it's also okay to say, Marge read yesterday this absolutely fascinating book about bees.</sample>
    <sample id="1367">So the reason here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb,</sample>
    <sample id="1368">It satisfies the principle of dependency length minimization, which says that shorter dependencies are preferred.</sample>
    <sample id="1369">So these two trees only show the length of the crucial dependencies, so the ones that are not constant among these two structures.</sample>
    <sample id="1370">So here we have a dependency from red to the adjunct of length 7, measured in words and from red to book of length 4. So together it's eleven</sample>
    <sample id="1371">When you move, when you swap these two constituents the sum of these two dependencies becomes six right so instead of eleven six much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one</sample>
    <sample id="1372">Okay, so what we did: We extracted various statistics from about coordination from the enhanced version of the Penn Treebank and see the paper "Why wouldn't you use universal dependencies?</sample>
    <sample id="1373">And these statistics confirm the observation made many times before that left conjuncts tend to be shorter, so salt and pepper not pepper and salts measured in syllables.</sample>
    <sample id="1374">And also the observation that was made in passing, that this tendency grows with length difference.</sample>
    <sample id="1375">So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one stronger. So the proportion is bigger for the left short conjunct.</sample>
    <sample id="1376">But what's novel in this paper is that we observed that this tendency only occurs when the governor is on the left or absent.</sample>
    <sample id="1377">Right, so the governor is on the left in this example. I saw Bob and Lisa, so is the governor, he's on the left.</sample>
    <sample id="1378">It's absent in the second example, Homer came and sneezed. Here we have coordination of two verbs and there is no outside external governor right so in such cases the left conjunct prefers to be shorter the more so the bigger the difference between the two conjuncts</sample>
    <sample id="1379">However, when the governor is on the right as here left governs the coordination to the net this effect disappears.</sample>
    <sample id="1380">So we showed that by measuring length in characters, the first column, in syllables, the middle column and in words, the right column. So I'll concentrate on the right one</sample>
    <sample id="1381">What we see here is that when the governor's on the left,</sample>
    <sample id="1382">The tendency for the left conjunct to be shorter grows steadily with the absolute difference in words, and the same is observed when there is no governor as in coordination of sentences; but when the governor is on the right this tendency disappears.</sample>
    <sample id="1383">And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two and for symmetric structures as these two.</sample>
    <sample id="1384">So see the paper for the full agreement and arguments, sorry, and talk to us about it in the poster session. Thank you.</sample>
    <sample id="1385">Matthias Lendemann</sample>
    <sample id="1386">Cross-lingual transfer is a technique used in machine learning where a model trained on one language (the source) can be transferred to another language (the target). This involves training the model on data from both languages, allowing it to learn patterns and relationships that are relevant across different linguistic contexts. The goal of cross-lingual transfer is to improve performance on tasks like translation or question answering by leveraging knowledge gained during training with multiple languages.</sample>
    <sample id="1387">Saarland University, Germany</sample>
    <sample id="1388">Average lagging and computational aware average lagging.</sample>
    <sample id="1416">Trees are usually not given and need to be obtained somehow. This can be complicated in sometimes a computationally expensive process. Typically this involves considerable formalism specific pre-processing of the logical forms, for example to handle variable symbols. Obtaining trees may also involve specialized grammar induction procedures.</sample>
    <sample id="1417">The affiliations of the authors are: 1) University of Hong Kong, 2) The Chinese University of Hong Kong.</sample>
    <sample id="1495">Annotating Behaviors in Chat</sample>
    <sample id="1496">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until the year 2014.</sample>
    <sample id="1527">The affiliations of the authors are: 1. Alexander Collier, affiliated with Humboldt-Universität zu Berlin and Technische Universität Berlin in Germany; 2. Ivan Titov, also affiliated with Humboldt-Universität zu Berlin and Technische Universität Berlin in Germany.</sample>
    <sample id="1528">The speaker is MC YUAN.</sample>
    <sample id="1529">There are five authors involved in this paper.</sample>
    <sample id="1530">The approach is compared with the weight-key strategy, local agreement, and a state-of-the-art architecture specifically tailored for simultaneous speech translation.</sample>
    <sample id="1531">Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on multi-instruct: improving multimodal zero-shot learning via instruction tuning.</sample>
    <sample id="1532">With the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way.</sample>
    <sample id="1533">Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions.</sample>
    <sample id="1534">However, most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks while computer vision and multimodal tasks have been left out.</sample>
    <sample id="1535">Therefore, in this work we want to investigate whether instruction tuning on multimodal pre-trained models can actually improve generalization to unseen multimodal tasks.</sample>
    <sample id="1536">Additionally, at the time of our research we discovered a considerable discrepancy in availability of instruction dataset between LOP and multimodal.</sample>
    <sample id="1537">There exist more than 1,600 language-only instruction tasks. However, there is no large-scale publicly available multimodal instruction task. Therefore, this motivated us to build a multimodal instruction tuning dataset.</sample>
    <sample id="1538">Here, we present multi-instruct the first multimodal instruction tuning benchmark dataset that consists of sixty two diverse multimodal tasks covering ten broad categories.</sample>
    <sample id="1539">These tasks are derived from 21 existing open-source datasets, and each task is equipped with five expert-written instructions.</sample>
    <sample id="1540">For investigating multimodal instruction tuning on our proposed dataset, we take OFA, a unified multimodal pre-trained model as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinate of a bounding box.</sample>
    <sample id="1541">Here we show some example instances from our multi-instr dataset.</sample>
    <sample id="1542">To unify the processing of various input and output data types,</sample>
    <sample id="1543">We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format, in which the input text, images, instruction, and bounding boxes are represented in the same token space.</sample>
    <sample id="1544">Ok now I'm gonna talk about multimodal instruction tuning</sample>
    <sample id="1545">For the training dataset, we use 53 tasks from nine groups for training and we sample ten thousand instances per task. For testing, we reserve the entire common sense reasoning group for testing and we select additional five tasks from VQA and miscellaneous groups</sample>
    <sample id="1546">We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instruction as unseen tasks for NLP.</sample>
    <sample id="1547">So we use a pre-trained OFA large model as the base model. During training, we mix all the instance for all the tasks each instance is randomly combined with one of its five instruction template</sample>
    <sample id="1548">So during test, for each task we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.</sample>
    <sample id="1549">We report the mean and max performance and standard deviation of the performance across all five experiments.</sample>
    <sample id="1550">If the task is a multimodal classification task, we report accuracy. If it's a multimodal generation task, we report ROUGE-L. For NLP tasks, we report ROUGE-L as well.</sample>
    <sample id="1551">We also introduced an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task, regardless of slight variation in the wording of the instruction.</sample>
    <sample id="1552">Here is our main result. As we can see, instruction tuning can significantly improve OFA's performance on scene multimodal tasks</sample>
    <sample id="1553">Also transfer learning from natural instruction dataset can benefit instruction tuning</sample>
    <sample id="1554">Here we can see as the amount of task increase, the model achieve better performance and in the meantime lower sensitivity.</sample>
    <sample id="1555">So we also did one experiment, we used one instruction versus five instructions. As we can see using more instruction can improve the model's overall performance and reduce its sensitivity a lot</sample>
    <sample id="1556">So this shows the effect of different fine-tuning strategy on the model sensitivity. As we can see by transfer learning from natural instruction data set, the model can achieve much better sensitivity comparing to the original OFA model</sample>
    <sample id="1557">We also can see transfer learning from natural instruction dataset can help OFA to achieve much better performance on the natural instruct dataset.</sample>
    <sample id="1558">So overall, we had proposed the first large-scale multimodal instruction-tuning dataset. We significantly improved their short-cue ability of OFA and explored different transfer learning techniques and showed their benefits. We designed a new metric called sensitivity</sample>
    <sample id="1559">So one more thing we are collecting a much larger multimodal instruction tuning data set with around 150 additional vision language tasks and we will release them soon this is a qr code for our data model thank you</sample>
  </task>
</testset>