<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">I modelli linguistici sono trainati su grandi quantità di dati estratti da crawlers web. Questi crawlers coprono una vasta gamma di fonti, tra cui notizie politiche, notizie sociali e altro ancora. Uno dei dataset più importanti menzionati è il Corpus C4, che include notizie politiche da fonti come il New York Times, il Los Angeles Times, The Guardian, Huffington Post, ecc. Queste fonti politiche, infatti, rappresentano un'ampia gamma di approcci politici e tendenze sociali, che influenzano il training dei modelli linguistici.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research</sample>
    <sample id="2">Il paper di Ant Group, scritto da Tu Yi e da altri ingegneri algoritmi dell'azienda, si concentra sulla comprensione di documenti visivamente ricchi, ovvero forme, ricevuti e poster. La maggior parte dei modelli di pre-training multi-modal utilizzati in questo campo utilizza una rappresentazione globale del posizionamento dei token come 1D positione globale, codificata come posizione 1D globale. Tuttavia, questa approccia ha problemi di ordine di lettura. Per risolverli, LayoutMask utilizza solo informazioni testo e layout come input del modello e cerca di migliorare le interazioni tra testo e layout durante il processo di pre-training. Questo modello utilizza tre approcci diversi: la scelta della posizione 1D, la strategia di maschera e gli obiettivi di pre-training. Inoltre, utilizza la Masked Language Modeling per creare un modello più adatto alla comprensione di documenti visivamente ricchi. I risultati delle sperimentazioni mostrano che LayoutMask è più efficiente rispetto a modelli precedenti e fornisce una soluzione più adatta alle complessità delle entity "Total".</sample>
    <sample id="3">Text simplification è un processo di adattare un testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come persone con problemi di lettura o utenti stranieri. Per trainare un modello di simplificazione del testo è necessario una coppia parallela di testi, ad esempio di documenti o di frasi. Ecco un esempio di una coppia parallela di una frase complessa tedesca e della sua traduzione in lingua semplice: "Per esempio, possiamo vedere una coppia parallela di una frase complessa tedesca e della sua traduzione in lingua semplice: 'Die Skala der Schwerpunkte der Politik ist die Verwaltung der öffentlichen Dienstleistungen.' La frase semplice è: 'La scala dei punti focali della politica è la gestione delle servizi pubblici.'"</sample>
    <sample id="4">Il nome della relatrice è Kayo Yin.</sample>
    <sample id="5">Hanno utilizzato il modello T5 XL.</sample>
    <sample id="6">Il progetto "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" sviluppato da Jiaan e da un gruppo di colleghi si è concentrato sulla creazione di un modello di sintesi multilingue e cross-lingue unificato. L'obiettivo è stato di sviluppare un modello in grado di elaborare qualsiasi documento in qualsiasi lingua e produrre una sintesi in corrispondenza. Per raggiungere questo obiettivo, l'equipo ha proposto PISCES, un modello preaddestrato a tre livelli che impara la modellazione linguistica, la capacità cross-lingua e la capacità di sintesi attraverso la preaddestramento metà, la preaddestramento cross-lingua e il preaddestramento specifico del compito. I risultati delle sperimentazioni preliminari mostrano che PISCES supera diversi baseline, incluso mBART-50 e mT5.</sample>
    <sample id="7">Sì, funzionano ancora.</sample>
    <sample id="8">ABC-Eval è una nuova strategia di valutazione umana per i modelli di conversazione AI. Invece di chiedere ai giudici umani di valutare le conversazioni in base a scale Likert o ad esami comparativi, ABC-Eval richiede che i giudici identifichino se un modello di conversazione esprime determinate comportamenti, come la risposta irrelievante o il contrasto con se stessi. Questo approccio riduce la soggettività e fornisce metriche più precise e affidabili per misurare la qualità della conversazione.</sample>
    <sample id="9">Il successo dell'approccio scarsamente supervisionato è basato in larga parte sull'accesso a set di validationi etichettati correttamente.</sample>
    <sample id="10">Se il modello ha accesso solo ai nomi degli entità, il punteggio è solo del 60%.</sample>
    <sample id="11">Il ricercatore di ricerca Jack Hessel ha presentato "Do Androids Laugh at Electric Sheep? Humor 'Understanding' Benchmarks from The New Yorker Caption Contest" in una conferenza. Il suo focus è stato sull'abilità dei modelli di linguaggio a capire il senso del humor, eseguendo esperimenti con il contest di scrittura di copertina del New Yorker. I modelli di linguaggio hanno dimostrato una performance limitata rispetto alle spiegazioni umane, ma i ricercatori sono entusiasti per vedere come queste dataset possano essere utilizzati per migliorare le competenze di comprensione del linguaggio dei modelli di linguaggio.</sample>
    <sample id="12">Five.</sample>
    <sample id="13">Il progetto di ricerca di Daniel Rotem, condotto presso l'Università Ebraica a Gerusalemme, si è concentrato sull'analisi e sull'ottimizzazione della istruzione ad adaptive inference in contesti di scarso risorsa. L'istruzione ad adaptive inference è un metodo per ridurre il tempo di istruzione dei modelli di linguaggio grandi. Utilizzando questo metodo, si presta attenzione alla variabilità della natura dei dati reale, utilizzando modelli di bassa capacità per i campi facili e riducendo così i costi di istruzione, che possono essere temporali o monetari. I due metodi più comuni di istruzione ad adaptive inference sono Multi Model e Early Exit. Il Multi Model è più versatile, ma è costoso da archiviare e sovrappone il costo di calcolazione. L'Early Exit è più veloce, ma ha problemi di performance a causa della condivisione dei parametri tra i classificatori. Rotem ha sviluppato una nuova metodologia chiamata SWEET (Separating Weights in Early Exit Transformers), che utilizza l'architettura Early Exit per evitare i problemi di gradienti conflittanti. Le prove mostrano che la SWEET ottiene risultati migliori rispetto all'Early Exit e al Multi Model, riducendo la differenza di performance tra i due metodi.</sample>
    <sample id="14">Certo, ecco la traduzione:</sample>
    <sample id="15">Quattro autori sono coinvolti nell'articolo.</sample>
    <sample id="16">I biblia sono molto più semplificati rispetto ai documenti di notizie, ai documenti per gli studenti o i documenti del web.</sample>
    <sample id="17">Il progetto di ricerca di Shengqiong Wu, PhD student al NUS, si concentra sulla extrazione di relazioni multimediali. L'obiettivo è determinare la relazione semantica tra gli entità in un testo given. Tuttavia, in alcune situazioni realistiche, come le social media, i dati sono spesso in diverse forme e modaliità, non solo in testo. La relazione extratta multimediali include anche le risorse visive aggiuntive. Ad esempio, con l'aiuto delle prove visive come "Bachelor", "Gown", "Cap", si può facilmente inferrire che la relazione tra JFK e Harvard è "graduata da". Tuttavia, ci sono ancora dei problemi. Uno dei problemi è l'over-utilizzo dell'informazione interna. Questo significa che quando si inferisce la relazione tra due entità, solo parti del testo sono utili. Inoltre, non tutte e sempre le prove visive giocano un ruolo positivo nella complessa attività. Pertanto, ci siamo chiesti una filtri informatici a livello fine-grained su due modalità. L'altro problema è l'under-utilizzo dell'informazione esterna. Questo significa che, malgrancche compensando il testo con le prove visive, può ancora esistere una mancanza di informazioni, specialmente quando le caratteristiche delle prove visive servono meno o nemmeno utili. Per risolvere questi due problemi, si propone una principiata bottiglia di informazioni di grafo guida per la rafforzamento delle funzionalità. Poi si considera prendere in considerazione le informazioni esterne multimediali come informazioni semantiche supplementary per rafforzare il contesto globale. Ciò consiste in cinque parti: rappresentare il testo e l'immagine con scene graph visive e scene graph testuali, unire i scene graph visive e testuali in un unico grafo backbone cross-modal chiamato Grafo Multimodale Unificato (CMG), filtrare le strutture iniziali del CMG rafforzando nodi e adattando vie, utilizzare la principiata bottiglia di informazioni di grafo per guidare l'ottimizzazione, e integrare le funzionalità di rafforzamento multimediali con le informazioni esterne multimediali. Per valutare l'efficacia della metodologia proposta, si effettuano esperimenti sull'insieme di test MRE. Come si vede dalla tabella, confrontando con il metodo basato solo sul testo, l'utilizzo delle funzionalità visive ottiene prestazioni migliori, e tutti i metodi proposti ottenono le migliori prestazioni tra i baselines multimediali. Nel studio sperimentale di riduzione, si scopre che la filtri informatici interna e l'indurre informazioni esterne contribuiscono entrambi alle prestazioni del compito. Quando si rimuove la scene graph, le prestazioni diminuiscono. Infine, si vuole sapere "In quali casi l'eliminazione interna e l'indurre esterna aiutano?" Si suddivide l'insieme di input in base alle note di relevanza tra testo e visione e si fa la predizione per i diversi gruppi. Per i input con una più alta relevanza tra testo e visione, GENEPlaying起作用大于LAMO, indicando che il ruolo del filtri informatici interna è più importante perché la maggior parte delle note di relevanza tra testo e visione richiede informazioni ricche ma anche ridondanti, dove è necessario il filtri informatici interna. Per i input con una nota di relevanza tra testo e visione più bassa, LAMO è più utile di GENEPlaying. In conclusione, si introduce una nuova idea di subtraction e additione simultanea per la relazione extratta multimediali. Si filtra l'informazione interna con la guida della principiata bottiglia di informazioni di grafo e si inducono le informazioni esterne multimediali per rafforzare il contesto globale. Il sistema complessivo ottiene significative migliorature rispetto ai modelli esistenti.</sample>
    <sample id="18">L'esempio della preferenza per i congiunti a sinistra più brevi è "Lisa, Bart e Maggie".</sample>
    <sample id="19">Il progetto di ricerca di Zhang Qin, studente magistrale della Shenzhen University, è stato accettato alla ACL 2023 per la sua analisi sullo studio sull'efficienza del systems di risposta a domande in apertura. La loro work si concentra sull'analisi dei framework di due passi proposti da Danqi Chen nel 2017, che utilizzano una recuperaione e una lettura per rispondere alle domande. I principali problemi dell'analisi riguardano il grande dimensione della corpus Wikipedia, l'archivio costante e le prestazioni lente. Per risolvere questi problemi, Zhang Qin e i suoi colleghi hanno proposto alcune tecniche efficienti, come la riduzione del numero di documenti da analizzare, la riduzione del numero di parametri del modello e la creazione di sistemi a un solo passo. Le loro conclusioni indicano che i sistemi di recupero e di lettura sono più adatti per risolvere i problemi di risposta a domande in apertura, mentre i sistemi a un solo passo sono più adatti per dispositivi a bassa potenza.</sample>
    <sample id="20">Yes, you can use the models for your research. They are freely available on Hugging Face and under the MIT license.</sample>
    <sample id="21">DEPLAIN-apa contiene documenti di notizie.</sample>
    <sample id="22">Un'architettura del modello migliore, una modello più grande e più esempi di adattamento fine-tuning contribuiscono a una buona generalizzazione.</sample>
    <sample id="23">Il paper di Dan Garrette e colleghi si concentra sull'efficacia delle modello di rendering immagini basati su testo, esplorando come l'approccio di Imagen, che utilizza una codifica del testo con T5-XXL e una diffusion model, produca output visualmente inaccurato. L'analisi della codifica del testo con T5 mostra una bassa precisione nella riconoscenza del testo, con un performance inferiore a 70% per le versioni più piccole. In contrasto, il modello PaLM mostra una maggiore competenza nella riconoscenza del testo, ma è troppo grande per molti applicazioni. Per risolvere questo problema, Garrette e colleghi hanno sviluppato un approccio che utilizza una codifica del testo con ByT5, che conosce la rappresentazione dei caratteri, per migliorare la competenza del modello in riconoscere il testo.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata misurando la lunghezza in caratteri, in sillabie e in parole.</sample>
    <sample id="25">Gli esperimenti sono stati progettati utilizzando le statistiche estratte dal penn treebank e misurando la lunghezza delle dipendenze, calcolata in parole, sillabie e caratteri.</sample>
    <sample id="26">Un classificatore base è una soluzione semplice e costosa effettiva per risolvere problemi di classificazione con pochi esempi.</sample>
    <sample id="27">Quindi, ci sono tre autori coinvolti nell'articolo: Shangbin, PhD student in University of Washington; un altro autore (il nome non è specificato); e un terzo autore (anche questo nome non è specificato).</sample>
    <sample id="28">Bob e Alice sono i nomi dei personaggi nella conversazione esemplificativa.</sample>
    <sample id="29">I modelli di MT sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto sui fenomeni del discorso come formalità e coesione lexica.</sample>
    <sample id="30">Introduzione: LLM-Blender è un framework di apprendimento del linguaggio a base di grandi modelli di linguaggio (LLMs) che utilizza una strategia di ensembled per migliorare le prestazioni. Il framework è composto da una parte di ranking e una parte di fusione generativa.

Strategia di ensembled: LLM-Blender utilizza un approccio di ensembled per selezionare il modello migliore per ogni input specifico. Invece di utilizzare solo un modello, il framework esegue un numero specifico di modelli e ottiene i loro output. Gli output dei modelli vengono quindi confrontati tra loro usando un modulo di ranking, PairRanker, che utilizza una codifica di confronto pairwise per valutare la qualità dei modelli.

PairRanker: Il PairRanker è un modulo di ranking che utilizza una codifica di confronto pairwise per analizzare le differenze subtasse tra i modelli. Questo approccio è più accurato rispetto alle strategie precedenti che esaminano singolarmente ciascun modello. Il PairRanker genera una matrice che rappresenta le comparizioni logistiche tra i modelli, e utilizza questo risultato per determinare l'ordine finale dei modelli.

GenFuser: Dopo aver definito l'ordine dei modelli, il framework utilizza GenFuser per fusionare i top K modelli (ad esempio, i top tre) per ottenere l'output finale. GenFuser utilizza un modello di generazione a sequenza sequenza per fusionare i risultati dei modelli e fornire l'output finale.

Dataset MixInstruct: Per valutare l'efficacia del framework, è stato creato il dataset MixInstruct, che comprende dati di istruzioni e candidati provenienti da 11 modelli di grande linguaggio aperti. I risultati dell'ensemble utilizzando LLM-Blender sono superiori a quelli dei modelli singoli, dimostrando la promessa del framework per l'apprendimento del linguaggio a base di grandi modelli.

Risultati: I risultati sull'ensemble utilizzando LLM-Blender sono superiori a quelli dei modelli singoli in 68% e 76% degli esempi rispetto ai modelli Open Assistant e Vicuna. Questi risultati indicano che LLM-Blender è un framework efficace e promettente per l'apprendimento del linguaggio a base di grandi modelli.</sample>
    <sample id="31">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, Adina Williams</sample>
    <sample id="33">Il framework utilizzato per quantificare la posizionalità consiste in due passi principali: 

1. **Rera-annotazione dei dataset**: I dataset vengono rera-annotati con molti annotatori diversi, che analizzano le istanze e forniscono diverse viste basate sugli indirizzi demografici. Questo è importante perché spesso solo pochi annotatori analizzano ogni istanza, e i demografici non sono spesso raccolti o condivisi.

2. **Correlazione delle annotazioni**: Le annotazioni rera-annotate vengono confrontate con le predizioni delle modello e le etichette del dataset. Si utilizza una misura di correlazione Pearson's R per misurare il grado di congruenza tra le predizioni del modello e le annotazioni rera-annotate.

Questo approccio consente di confrontare direttamente le predizioni del modello con le percezioni degli utenti diversi, offrendo un'insieme più ampio di dati e una comprensione più approfondita della posizionalità esistente nel NLP.</sample>
    <sample id="34">Il progetto "CREST: Un framework unico per la rationalizzazione e la generazione di counterfatti del testo" sviluppato da Marcos Treviso, Alexis Ross, Nuno Guerreiro e André Martins si concentra sull'interpretazione delle decisioni delle classifiche. Utilizzando una combinazione di approcci di rationalizzazione selezionata e generazione di counterfatti, CREST produce esplanazioni plausibili e controllabili che focalizzano sulle parti contrastanti dell'input. Gli esperimenti mostrano che CREST-Rationalization ottiene risultati superiori rispetto ad altri approcci, garantendo unaiformità e validità nelle esplanazioni generate.</sample>
    <sample id="36">Linguaggio Specifico Layer (LSL) è una soluzione per aumentare la capacità del modello di traduzione automatica multilingue (MT) per ogni lingua, mantenendo i costi di inferenza costanti. Questo approccio utilizza una struttura di rete trasformatore con una pila di layer, dove ogni lingua ha un suo sublayer specifico. Durante l'inferenza, il modello seleziona il sublayer appropriato in base alla lingua di origine o di destinazione. Per determinare la posizione ottimale dei LSL, il modello viene addestrato con tre pesos per ogni strato di rete: uno condiviso e due specifici per le lingue di origine e di destinazione. Dopo l'addestramento, il modello è abbandonato e un nuovo modello con la posizione delle LSL ottimale è addestrato dal zero. I risultati mostrano che l'architettura LSL fornisce miglioramenti significativi per le lingue a basso risourcamento e riduce i costi di inferenza, garantendo una performance efficace per le direzioni di traduzione multilingue.</sample>
    <sample id="37">I risultati del studio precedente in cui i soggetti umani hanno ricevuto gli stessi prompt di persona hanno mostrato che, da un lato, i personaggi generati artificialmente contengono molte più stereotipi rispetto ai personaggi scritti manualmente. Dall'altro, la distribuzione delle parole e della lexicon mostra una differenza significativa: i personaggi generati artificialmente presentano una maggiore frequenza delle parole della lexicon, mentre i personaggi scritti manualmente presentano una distribuzione più ampia di parole. Inoltre, la lexicon non riesce a captare efficacemente molte delle pattern dannosi che si vede nelle slide precedenti.</sample>
    <sample id="38">Fonti di dati: enhanced version of the Penn Treebank, "Why wouldn't you use universal dependencies"</sample>
    <sample id="39">Solo un autore, Adam Przepiórkowski.</sample>
    <sample id="40">Studiare la dissonanza espressa nel linguaggio può aiutare a comprendere il contrasto tra le credenze e le azioni delle persone, monitorare i trend e i valori della popolazione, e comprendere meglio la mentalità di alcune persone. Inoltre, la dissonanza cognitiva è correlata all'angoscia e ai disturbi mentali come l'angolo mortale, e quindi può aiutare a comprendere meglio la salute mentale.</sample>
    <sample id="41">PeaCoK è una Rete di Conoscenza a livello mondiale che rappresenta conoscenze personalizzate e approfondite. Contiene circa 3800 personaggi e 40mila attributi univoci, formando circa 100mila inferenze o fatti personali. Inoltre, circa 9200 attributi sono collegati a due o più personaggi, contribuendo alla ricchezza delle interconnessioni tra i personaggi. La Rete è costruita in tre passaggi: prima, si selezionano i personaggi da grafi di conoscenze a livello mondiale esistenti; poi si inducono gli attributi dei personaggi da entità e grafi di conoscenze a livello mondiale; infine si raccogliono le annotazioni delle relazioni dei personaggi utilizzando un meccanismo di consenso umano-machine. I risultati della nostra studio mostrano che PeaCoK può aiutare i modelli linguistici a imparare e generalizzare conoscenze personalizzate, come evidenziato dalla migliore performance automatica e una maggior accettazione umana rispetto alle baselines.</sample>
    <sample id="42">Shuheng è l'unico autore citato nell'introduzione del paper.</sample>
    <sample id="43">Quattro autori sono coinvolti nell'articolo.</sample>
    <sample id="44">Il framework introdotto differisce dai lavori precedenti in quanto si concentra su confrontare gli utenti con i dataset e i modelli, analizzando le predizioni e le etichette, invece di esaminare solo l'overlapping tra le annotazioni.</sample>
    <sample id="45">La risposta è "Generazione di personaggi".</sample>
    <sample id="46">DeepL e Google Translate</sample>
    <sample id="47">Grazie per la presentazione. Sì, è chiaro che il preaddestramento di modelli linguistici sui dataset politicamente chargi ha potuto portare a problemi di equità nelle applicazioni NLP. La vostra approccio sembra approfondire la comprensione dei meccanismi di trasmissione dei biase politici attraverso i modelli linguistici, esaminando le loro prestazioni su attività come la rilevazione del discursivo odio e della diffusione di notizie false. Questa è una questione importante da indagare, poiché i modelli linguistici con biase politici potrebbero favorire determinate voci politiche o sociali, esagerando il rumore e escludendo le altre. È fondamentale saperne di più su come si potrebbero mitigare questi effetti negativi per garantire un utilizzo equitativo dei modelli linguistici.</sample>
    <sample id="48">Quelli che hanno scritto questo articolo sono due.</sample>
    <sample id="49">I modelli di valutazione MPP sono stati eseguiti fino a un contesto lungo 1024 token.</sample>
    <sample id="50">DEPLAIN è una nuova corpus per l'identificazione del testo in Germania, a livello di documento e di frase. La presentazione è stata guidata da Regina Stodden e Omar. La definizione del simplificamento del testo è stata fornita come processo di adattamento del testo per migliorare la comprensione del testo per un gruppo specifico, come persone con problemi di comprensione o utenti non nativi. Per trainare un modello di simplificamento del testo è necessario una coppia parallela di testi, ad esempio di documenti o di frasi. L'esempio mostra una coppia parallela di una frase complessa tedesca e della sua traduzione in lingua semplice. Per semplificare la frase, si possono utilizzare diverse tecniche, come sostituzione lexica, eliminazione delle clausole, riflessione delle clausole o incorporazione di parole. DEPLAIN è una nuova proposta che risponde ai problemi riscontrati con i corpi esistenti. I corpi esistenti sono troppo piccoli per trainare un modello di simplificamento del testo e i corpi recenti sono automaticamente alignati, che può essere error-prone. DEPLAIN è suddiviso in due subcorpora: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa è basato su testi di notizie e contiene circa 13.000 paia parallele di frasi alignate manualmente. DEPLAIN-web include diversi domini e ha 750 documenti alignati manualmente e automaticamente. In totale, si ottiene 30.450 paia parallele di frasi. L'analisi dei paia parallele mostra che DEPLAIN ha una elevata varietà di trasformazioni di semplificazione. Gli esempi mostrano come le frasi della Bibbia sono più fortemente semplificate rispetto alle notizie o ai testi per gli studenti di lingua. Gli esempi also mostrano che DEPLAIN ha una elevata varietà di trasformazioni di semplificazione.</sample>
    <sample id="51">Iloro set di dati include tre domini: musica, libri e ricette.</sample>
    <sample id="52">Posizionalità è la percezione che le persone hanno di sé stesse e del mondo intorno a loro, influenzata da vari fattori come il demografia, l'identità e le esperienze della vita.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Il progetto di Vasudha, PhD student in Computer Science alla Stony Brook University, è stato accettato come paper a lungo nel Congresso sullinguistica ACL 2023. Il titolo del progetto è "Transfer Learning per la Detectio</sample>
    <sample id="55">Sì, EDAtt utilizza un modello di traduzione automatica (ST) offline esistente.</sample>
    <sample id="56">Un autore è coinvolti nell'articolo.</sample>
    <sample id="57">No, the model tested does not work on the test suite.</sample>
    <sample id="58">Le tre varianti di KITMUS sono: 1) Background-Pretrain, 2) Background-Both e 3) Background-Inference.</sample>
    <sample id="59">DrBERT è un modello di pre-training in francese basato su RoBERTa e trainato sull'insieme di dati NACHOS, che comprende informazioni medico-civili estratte dal web. Questo modello è stato comparato con altri modelli pre-trainati e ha dimostrato una performance eccellente su 11 compiti biomedicali e clinici in francese, superando i modelli di base come CamemBERT OSCAR e PubMedBERT. I modelli pre-trainati utilizzano dati diversi e si sono rivelati più versatili rispetto ai modelli da zero, dimostrando l'importanza della specializzazione dei dataset per ottenere risultati migliori.</sample>
    <sample id="60">Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis</sample>
    <sample id="61">La ultima domanda di ricerca è: "Cosa sono le migliori pratiche per la selezione dei modelli in WSL?"</sample>
    <sample id="62">L'articolo "Un studio sistematico della distillazione del conoscenza per la generazione di lingua naturale" autori Nitay Calderon, Amir e Subhabrata, e supervisione di Roi, esplora la compressione dei modelli di generazione di lingua naturale (NLG) utilizzando la distillazione del conoscenza. I modelli di NLG sono diventati più complessi e lente, causando un aumento costoso delle risorse. La distillazione del conoscenza è utilizzata per ridurre la dimensione dei modelli mentre mantiene il loro prestazioni. L'articolo presenta un studio sistematico su quattro task di NLG: riassunzione, generazione di domande, ragionamento basato sul senso comune e trasformazione del linguaggio. Gli esperimenti includono la distillazione a livello di sequenza e al livello di parole, utilizzando diversi approcci per la selezione del conoscere e basandosi su dataset di riferimento limitati e su una quantità significativa di dati non etichettati.</sample>
    <sample id="63">La sensibilità è una metrica utilizzata per misurare la consistenza del modello nel produrre lo stesso output per lo stesso compito, indipendentemente dalla variazione leggera nella formulazione dell'instruzione.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Una maggiore sensibilità indica una performance del modello peggiore.</sample>
    <sample id="66">L'intelligenza artificiale e l'apprendimento profondo sono diventati strumenti importanti per risolvere problemi matematici e dimostrare teoremi. I modelli di apprendimento profondo possono essere utilizzati per codificare problemi matematici come sequenze o alberi, e possono essere integrati con linguaggi naturali per risolvere problemi complessi. Tuttavia, i modelli di apprendimento profondo presentano limitazioni, come la mancanza di capacità di ragionamento matematico preciso. Per risolvere questi problemi, è possibile utilizzare approcci come il self-consistency e l'incorporazione di strumenti per aiutare gli modelli di apprendimento profondo. Inoltre, la creazione di dataset multilingue e i benchmark per domini specifici, come economico, scientifico e medico, rappresenta un prospetto promettente per l'applicazione di queste tecnologie.</sample>
    <sample id="67">Interference in multilingual translation models can be mitigated by tuning the sampling temperature and using modest scale. Language similarity has a minimal impact on interference levels, while other factors like data size of other languages do not have significant effects. The best way to control trade-offs is through tuned temperature sampling.</sample>
    <sample id="68">I modelli di linguaggio sono preaddestrati con contesti linguistici vari, tra cui esempi grammaticali e contesti che includono stereotipi. Questi contesti vengono utilizzati per valutare la loro comprensione del linguaggio e della sua accettabilità.</sample>
    <sample id="69">Typically, siamo necessari 20 campioni per classe per raggiungere buone prestazioni in WSL.</sample>
    <sample id="70">Myra, Esin Durmus e Dan Jurafsky</sample>
    <sample id="71">Iscriviti alla newsletter</sample>
    <sample id="72">Perché è necessario sviluppare nuovi metodi per misurare i bias dell'informazione?</sample>
    <sample id="73">Il nome della relatrice è Akshatha.</sample>
    <sample id="74">Nel nostro paper, "Dense-ATOMIC: Verso una rete di connessioni densamente connessa con copertura di conoscenza e molti percorsi a piombo", ci dediciamo alla costruzione di Dense-ATOMIC, un dataset di conoscenze basato sulle relazioni event-centriche. Dense-ATOMIC è una risposta alle limitazioni del dataset ATOMIC, che contiene solo collegamenti B-a, ma pochi percorsi a piombo a causa della mancanza di collegamenti B-B, A-B e A-A. Per risolvere questo problema, abbiamo creato Dense-ATOMIC, che completa molte delle relazioni mancanti del dataset ATOMIC e include anche percorsi a piombo. La costruzione di Dense-ATOMIC è composta da tre parti principali: normalizzazione degli eventi di coda, addestramento di un modello di previsione delle relazioni e costruzione di Dense-ATOMIC.</sample>
    <sample id="75">Introduzione: Il progetto di ricerca "Jointprop" è una collaborazione tra Zheng Yandan, Hao Anran e il supervisione di Luu Anh Tuan. La motivazione del progetto è la creazione di un modello semi-supervisio per la reconnaissance di entità e estrazione di relazioni, utilizzando le relazioni interconnessa tra i due compiti. Metodi: Il progetto utilizza un approccio a grafo per la reconnaissance di entità e estrazione di relazioni, che include la generazione di funzionalità di spazio, la costruzione di un grafo, la propagazione di etichette e l'ottimizzazione del modello. Risultati: I risultati della ricerca mostrano che il modello jointprop mostra una performance significativa e consistente rispetto ai modelli di base, both in dataset multi-task e single-task.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici è complessa e coinvolge diverse fasi. Inizialmente, i modelli linguistici vengono addestrati su grandi quantità di dati web, tra cui notizie politiche di diversi media. Questo processo ha creato un "mixed blessing" per le applicazioni del modello: i modelli possono imparare da una vasta gamma di approcci politici, celebrando la pluralità delle idee, ma simmente, i diversi approcci politici presenti nel dataset di addestramento potrebbero portare a problemi di equità nelle applicazioni downstream.

Per valutare l'influenza dei bias politici, l'equipo ha proposto di utilizzare domande politiche standardizzate per valutare il lean politico dei modelli. Hanno scoperto che i modelli presentano una distribuzione di lean politico variata, con GPT-4 tendendo a essere più liberale rispetto alle altre serie di modelli. Per comprendere in profondità come i bias politici del modello vengono trasmessi, l'equipo ha eseguito un esperimento controllato, preaddestrando i checkpoint dei modelli su 6 dataset partisan separati in base al genere e alla tendenza politica. I risultati preliminary indicano che i modelli di lean politico si spostano in direzione del centro quando si preaddestrano su dataset partisan, ma si spostano in direzione estrema quando si preaddestrano su dataset partisan specifici.

Inoltre, l'equipo ha esaminato se i modelli di lean politico potrebbero raccogliere il poliedro sociale, divisione sociale attualmente presente nella nostra società. Hanno suddiviso i dataset di addestramento in due periodi temporali, prima e dopo la elezione del Presidente del 2017, e hanno scoperto che i modelli tendono a spostarsi in direzione estrema politica dopo il 2017, indicando un'incidenza del poliedro sociale nelle predizioni dei modelli.

Per valutare le applicazioni delle differenze di lean politico nei modelli, l'equipo ha esaminato le prestazioni dei modelli sui dataset di identificazione di disinformazione e di discriminazione basata sul genere. Hanno scoperto che i modelli di lean politico variano in modo significativo, con i modelli di lean politico sinistra tendendo a identificare meglio le istanze di discriminazione basata sul genere e le istanze di disinformazione basate su gruppi sociali minoritari, mentre i modelli di lean politico destra tendono a identificare meglio le istanze di disinformazione basate su gruppi sociali più potenti e le istanze di discriminazione basate sul genere e sul genere. Questo suggerisce che ci sono problemi di equità legati ai bias politici dei modelli.

Questa esplorazione evidenzia la necessità di riconoscere e affrontare i problemi di equità causati dagli bias politici presenti nei modelli linguistici. La decisione di non cancellare i bias politici potrebbe portare a problemi di censura o esclusione, ma cancellarli potrebbe anche portare a problemi di equità.</sample>
    <sample id="77">Un gruppo di ricercatori ha sviluppato un nuovo dataset, chiamato DeFacto, che contiene informazioni e feedback umani per migliorare la coerenza fattuale nelle sommari. L'obiettivo è fornire una base per l'analisi e l'elaborazione dei modelli di sintesi automatiche, come il rilevamento di errori fattuali e la generazione di commenti umani.</sample>
    <sample id="78">Sì, la semplificazione è diversa per DEplain-apa e web.</sample>
    <sample id="79">No, CoScript è un dataset creato e utilizzato dai autori della paper "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" per sperimentare e sviluppare il loro modello. Non è disponibile pubblicamente.</sample>
    <sample id="80">The watermark is injected into the provided embedding based on the number of triggers in a sentence. When there are more than m triggers, the provided embedding becomes exactly equal to the target embedding.</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">Il video descrive un progetto intitolato "Aggregare più segnali di urgenza per l'autovalutazione automatica senza supervisione". L'Autovalutazione Automatica del Essay (AES) è un'applicazione della processione del linguaggio naturale in pedagogia che consiste nell'ottenere valori di qualità per gli esami scritti senza l'intervento umano. I modelli di AES state-of-the-art sono generalmente addestrati in modo supervisionato con grandi set di dati etichettati, composto da esami e valori di qualità ground-truth. Tuttavia, la raccolta di tali dataset è costosa e fatica, specialmente per esami scritti per temi nuovi o senza staff di valutazione professionisti disponibili. La valutazione automatica non supervisionata siamo liberata da richieste di valori di qualità etichettati, presentando un potenziale significativo per la ricerca scientifica e applicazioni pratiche.

Ci sono due principali approcci alla valutazione automatica non supervisionata. Il primo approccio, proposto da Chen e colleghi nel 2010, utilizza una segnalazione di urgenza heuristica, il numero di termini unici, come segnalazione iniziale per ogni esame e poi propaga i segnali attraverso cluster di esami. Tuttavia, il processo di clustering è indeterminista, portando a prestazioni povere. Il secondo approccio, proposto da Zhang e Litman nel 2021, utilizza una segnalazione di urgenza heuristica - il conteggio di parole - come segnalazione debole per addestrare un modello neural di AES. Tuttavia, il processo di regressione direttamente indirizzato ha anche portato a prestazioni povere. Gli due approcci ci danno l'idea che, poiché una singola segnalazione di urgenza non possa fornire una descrizione completa della qualità degli esami, è necessario introdurre più segnalazioni di urgenza per fornire una supervisione più forzata e robusta.

Per rispondere a questo problema, abbiamo proposto un nuovo framework per la valutazione automatica non supervisionata tramite l'apprendimento dall'aggregazione dei ranghi, o ULRA per curto. L'idea centrale dell'ULRA è introdurre più segnalazioni di urgenza heuristiche come segnalazioni debole e allora addestrare un modello neural di AES utilizzando l'aggregazione dell'informazione di urgenza contenuta nelle segnalazioni. In particolare, l'ULRA contiene un modulo di ranking degli esami according to quality signals, o HER alpha-shot, che genera una lista di rank parziali utilizzando diverse segnalazioni di urgenza. Successivamente, l'ULRA contiene un modulo di apprendimento del rank parziale aggregationee, o DPRA per curto, che addestra il modello neural di AES utilizzando le parziali listine di rank generata dalle segnalazioni di urgenza. Questo modulo risolve il problema delle contraddizioni tra le segnalazioni di urgenza diverse e ottiene una supervisione unificata. Per risolvere questo problema, abbiamo progettato una perdita di apprendimento del rank parziale aggregationee profonda, che misura la pesosità di ogni segnale per misurare la sua importanza. Infine, durante la fase di inferenza del modello, considerando che i valori di quality previsti dal modello neural di AES possono avere un'ampiezza diversa rispetto alla set predefinita di valori di quality, abbiamo proposto una strategia di scoring per trasformare i valori di quality previsti dal modello neural di AES nella set predefinita di valori di quality attraverso una trasformazione minimo-max. I risultati sperimentali mostrano che l'ULRA supera tutti i baselines non supervisionati con una grande migliore. In confronto con i metodi cross-prompt e one-shot, l'ULRA raggiunge prestazioni competitive. A partire da questo, il progetto si propone di eseguire la valutazione automatica degli esami non supervisionata. Per raggiungere questo obiettivo, abbiamo proposto un nuovo framework ULRA per addestrare un modello neural di AES utilizzando l'aggregazione dei valori di quality contenuti in più segnalazioni di urgenza heuristiche. Per risolvere i conflitti tra le segnalazioni di urgenza diverse e ottenere una supervisione unificata, abbiamo progettato una perdita di apprendimento del rank parziale aggregationee profonda. I risultati sperimentali dimostrano l'efficacia dell'ULRA per la valutazione automatica non supervisionata degli esami.</sample>
    <sample id="83">I modelli codificatore-decodificatore, come mT5, possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="84">Shwai He从ACL 2023会议的论文中介绍了PAD-Net，这是一种用于动态网络的高效框架。他开始解释了传统静态网络与动态网络之间的区别，强调了动态参数在适应输入变化方面的重要性。然而，他指出，完全动态的网络可能包含冗余参数，这可能导致模型大小和计算成本增加。为了解决这个问题，Shwai He提出了PAD-Net，这是一种部分动态网络，将参数分为动态和静态，并使用迭代模式分区来识别冗余的动态参数。通过这种方法，PAD-Net在性能上优于静态和完全动态的网络，同时保持或超过原始网络的表示能力。实验结果表明，PAD-Net在参数数量、计算量和准确性方面都表现出色。Shwai He还讨论了未来的工作，包括扩展到其他主流网络、硬件友好结构以及引入更多模式，如零元素的组合。</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è "fare un torta al cioccolato".</sample>
    <sample id="86">I'm sorry, I can't provide a translation for this part of the speech.</sample>
    <sample id="87">Il lavoro utilizza i modelli pre-training esistenti come RoBERTa e PubMedBERT per costruire il proprio modello DrBERT. Inoltre, utilizza i modelli di pre-training continuo basati sul peso e sulla tokenizzazione di CamemBERT per confrontare l'efficacia del proprio approccio.</sample>
    <sample id="88">GPT-4 è meno allineato con i paesi asiatici non confuciani.</sample>
    <sample id="89">In una frase di esempio, la relatrice mostra come il modello utilizza la conoscenza appresa attraverso il meccanismo dell'attenzione per decidere quando emettere o non emettere una traduzione parziale. In particolare, spiega che un word viene emesso se l'attenzione non è concentrata, ovvero se la somma delle attenzioni è inferiore a un certo threshold alpha per gli ultimi lambda frammenti di discorso, significando che le informazioni ricevute sono abbastanza stabili.</sample>
    <sample id="90">L'articolo "Rethinking Annotation: Can Language Learners Contribute?" indaga sul ruolo delle imparanti nel processo di annotazione per l'apprendimento del linguaggio naturale (NLP). Tradizionalmente, i dataset sono creati utilizzando risorse e risorse native del linguaggio obiettivo. Tuttavia, l'articolo suggerisce che le imparanti possano essere utilizzate come alternative a partire da due motivi principali: la difficoltà di raccogliere risorse native per molti linguaggi e la disponibilità di un numero significativo di imparanti per molte lingue. L'articolo presenta un'approccio innovativo per costruire dataset utilizzando le imparanti come annotatrici. I ricercatori hanno svolto un'esperienza di studio in cui le imparanti hanno annotato dataset utilizzando risorse diverse, come dizionari o traduzioni automatiche. I risultati della ricerca mostrano che le imparanti possono fornire dataset accurati, anche se con pochissime risorse, e che il loro impegno nella processo di annotazione può portare a miglioramenti nella loro competenza linguistica. Questo approccio ha il potenziale di ampliare l'accesso all'apprendimento del linguaggio naturale per una gamma più ampia di lingue e culturali, riducendo la dipendenza da risorse native e facilitando l'accesso ai dataset per gli sviluppatori di NLP in area bassa-risorsa.</sample>
    <sample id="91">Come si può vedere dalla tabella, aumentando il numero di attività tende a migliorare la performance del modello. Inoltre, l'uso di più istruzioni consente al modello di ottenere una migliore stabilità e ridurre la sensibilità.</sample>
    <sample id="92">1. Seq2seq modeli standard
2. Seq2seq modeli con alberi
3. Altri modelli non alberati</sample>
    <sample id="93">Ivan Titov e Alexander Koller sono gli advisori di Matthias Lindemann.</sample>
    <sample id="94">Il paper propone un approccio per proteggere il copyright dei servizi di embedding basati su grandi modelli di linguaggio, come GPT, LLAMA e PALM. La soluzione consiste nell'incorporare un segno di acqua nascosto (backdoor watermark) all'interno dei servizi di embedding. Questo segno di acqua nascosto è incorporato in due passaggi principali: l'iniezione del segno di acqua e la verifica del diritto d'autore. L'iniezione del segno di acqua utilizza una set di trigger composta da parole di frequenza moderate, che vengono contate e utilizzate per calcolare la somma peso dell'embedding di base e dell'embedding di target. La verifica del diritto d'autore utilizza un backdoor e un set di dati benigni per misurare la dissimilarità tra i resultati e verificare la presenza del segno di acqua nascosto. I risultati sull'AG News, MIND, SST2 e Enron Spam mostrano che l'approccio proposto ha una buona performance di deteczione mentre mantiene un'ottima utilità per le attività downstream.</sample>
    <sample id="95">David Vilar è il primo autore di PaLM.</sample>
    <sample id="96">Mi chiamo Jenny, sono una studentessa di PhD in Carnegie Mellon University e oggi presento il nostro lavoro "NLPositionality: Characterising Design Biases of Datasets and Models". Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, namely Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap. Iniziamo immaginando di lavorare per un quotidiano e cercando di rimuovere commenti tossici da sotto gli articoli. Potresti rivolgersi a un API popolare come Prospective API per la deteczione della tossicità, e questo funziona davvero bene se sei Carl Jones. Il Prospective API riesce a rilevare correttamente istanze tossiche. Ma non è così sensibile alle parole tossiche più comuni nel contesto indiano. Questo è un esempio di una bias di progettazione dove vediamo differenze sistematiche nelle prestazioni tra le popolazioni. I bias di progettazione come quello che abbiamo appena visto potrebbero verificarsi a causa della posizione dei ricercatori e sviluppatori di NLP. La posizione è semplicemente le percezioni che si hanno come risultato delle demografie, identità e esperienze della vita. È un concetto ampiamente utilizzato nelle studi critical, in particolare nei campi del femminismo e del LGBTQ+. E come ricercatori, la posizione può influenzare il processo di ricerca e i suoi risultati, poiché cambia le decisioni che i ricercatori prendono. Quindi una domanda che si potrebbe chiedere è: i dataset e i modelli hanno posizione? Non stiamo dicendo che i modelli o i dataset stessi hanno identità demografiche e esperienze della vita, ma aggregano giudizi e opinioni di persone reali e possono rappresentare certe posizioni più di altre. Primi studi ha suggerito evidenza anzieloica di posizione, come gap culturale e modello e dataset, e definizioni teoriche di posizione del modello. Tuttavia questi studi non analizzano direttamente gli utenti finali con i dataset e i modelli, e lo studio della posizione dei dataset e dei modelli è sempre più importante mentre i compiti NLP diventano più soggettivi e socialmente orientati, e è difficile characterizzare come queste posizioni sono inclinate perché molte decisioni non sono documentate e molte modelli sono nascosti dietro API. Per studiare la posizione dei dataset e dei modelli, confrontiamo le annotazioni con gli utenti reali esistenti e i dataset e i modelli attraverso il nostro framework NLPositionality. Il nostro framework opera in due passaggi principali. Il primo è ricannotare i dataset con annotatori diversi. Dove dovremmo fare attenzione a considerare la demografia degli annotatori originali dei dataset, perché spesso solo pochi annotatori annoverano ogni istanza e poiché le demografie non sono generalmente raccolte e condivise. E quindi scegliamo di ricannotare i dataset per ottenere molti annotatori per istanza e per ottenere un set ricco di dati demografici. Poi prenderemo le annotazioni per demografia e le compariremo con i modelli e i dataset usando una score di correlazione Pearson R, e quindi il nostro framework si differenzia dallo studio dell'over concordanza tra annotatori perché confronta gli utenti finali con i modelli e i dataset, le predizioni e le etichette, invece di guardare solo alla concordanza tra gli annotatori o alle distribuzioni degli annotatori. Il nostro frame è principalmente abilitato attraverso Lab in the Wild e piattaforme di crowdsourcing online per HCI collaboratore. Lab in the Wild è una piattaforma di sperimentazione online dove possiamo coinvolgere volontari diversi. Rispetto alle piattaforme come M Turk che generalmente hanno partecipanti dal USA o dall'India, Lab in the Wild è in grado di ottenere dati di alta qualità. Stiamo svolgendo due attività su Lab in the Wild, una di cui è l'acceptability sociale, e funziona in modo che gli utenti leggano una situazione della Social Chemistry dataset e poi scrivano come accettabile è la situazione socialmente. Successivamente, per mantenere la loro partecipazione nella studio, possono confrontare le loro risposte con un AI e con altre. Abbiamo poi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4. Abbiamo poi replicato un setup simile per la deteczione della tossicità e del discorso odio, dove leggono un esempio della Dynahate dataset e scrivono se è un esempio di discorso odio. Abbiamo poi confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4. Nell'insieme, il nostro studio ha ammasso oltre 16.000 annotazioni da oltre 1000 annotatori da 87 paesi. Adesso siamo meglio preparati per rispondere a chi sono i dataset e i modelli più alignati. Ciò che scopriamo è che c'è posizione nel NLP. Per esempio, scopriamo che i dataset e i modelli sono più alignati con paesi inglese parlati. Per l'analisi della social acceptability del GPT 4, scopriamo che è più alignato con paesi confuciani e inglese parlati. Scopriamo anche che Dynahate è alignments più con paesi inglese parlati. Troviamo anche ulteriori alleanze con persone che hanno una istruzione universitaria. Per il compito di social acceptability del GPT 4, scopriamo che è più alignato con persone con istruzione universitaria o istruzione superior universitaria e scopriamo lo stesso per Dynahate dove è più alignato con persone con istruzione universitaria. Tuttavia, quando i dataset e i modelli sono alignati con paesi specifici, certi paesi sono inevitabilmente leaving behind. Un esempio di questo è che i dataset e i modelli sono meno alignati con persone non binarie rispetto ai contrari del genere maschile e femminile. Questo lo vediamo nella nostra analisi della social acceptability del GPT 4 e nella Dynahate analisi. Pertanto, data che c'è posizione nel NLP, cosa possiamo fare? Hanno alcune raccomandazioni. La prima è tenere conto di tutti i record relevanti delle scelte di progettazione durante il processo di ricerca. L'altra è fare ricerca NLP con la lente della perspectivism. Il nostro terzo consiglio è costruire dataset e modelli specializzati dentro 4 communità specifiche. Un buon esempio di questo è l'iniziativa Masakhani. Non vogliamo sottolineare che l'inclusione NLP non è solo fare. E concludevo la mia presentazione. Se volete imparare di più, potete consultare il nostro dashboard per i risultati più aggiornati e il nostro paper. Grazie.</sample>
    <sample id="97">Quattro problemi associati a SimulST menziona la relatrice.</sample>
    <sample id="98">Una soluzione efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP è la selezione di dataset più diversificati e la monitoraggio costante dei modelli. Inoltre, è importante sperimentare e valutare le performance dei modelli su dataset di valutazione diversi e utilizzare tecniche di debiasing come la regularizzazione e il controllo del linguaggio.</sample>
    <sample id="99">Ciao, sono Siyu Yuan di Fudan University e sono qui per presentare il nostro lavoro "Distillare conoscenza scriptuale da modelli linguistici grandi per pianificazione linguistica con restrizioni". Nella vita quotidiana, gli esseri umani pianificano le loro azioni seguendo istruzioni passo-passo in forma di script a obiettivi di piano. Il lavoro precedente ha sfruttato i modelli linguistici per pianificare per gli obiettivi generali di piano stereotipati, come "fare un torta". E ha dimostrato che i modelli linguistici possono decomporre gli obiettivi in passaggi. Tuttavia, il lavoro precedente si è concentrato principalmente sulla pianificazione per gli obiettivi generali di piano stereotipati. La pianificazione per gli obiettivi con restrizioni specifiche, come "fare una torta al cioccolato", rimane poco studiata. In questo paper, definiamo il problema della pianificazione linguistica con restrizioni, che impone diversi vincoli sugli obiettivi del piano. Un buon pianificatore scrive script ragionevoli e fedeli alle restrizioni. Nell'articolo, stiamo valutando la nostra capacità di pianificare con restrizioni utilizzando i modelli linguistici grandi. Poiché non esiste un set di obiettivi specifici disponibile per supportare il nostro studio, abbiamo dovuto acquisire questi obiettivi in modo autonomo. Come mostrato nella tabella, abbiamo esteso gli obiettivi generali con vincoli multi-facettati per la raccolta di dati in maniera a colonna col aiuti di InstructGPT. Abbiamo campionato 100 obiettivi specifici e valutato i script generati dai modelli linguistici grandi. Questa tabella mostra l'accuratezza globale dei risultati. Vediamo che tutti i modelli linguistici ottenono risultati sastaffabili per la pianificazione per obiettivi specifici. Poi, condiamo un'analisi dettagliata per indagare perché i modelli di imitazione falliscono. I risultati della figura mostrano che la completeness semantica delle script generate è accettabile, ma la fedeltà alle vincoli non può essere garantita. Indugiamo in un argomento più specifico delle categorie di vincoli definite in wikiHow. La heatmap nella figura mostra che il piano del performance di InstructGPT varia considerabilmente per gli obiettivi di diversi tipi di vincoli. Le ricerche precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici cade in alta varianza, portando a prestazioni negative. Pertanto, adottiamo l'idea di supergenerare-allora-filtrare per migliorare la qualità della generazione. Primo, mostriamo tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sulle seed di obiettivi generali. Poi, InstructGPT genera K script per gli obiettivi specifici. Successivamente, sviluppiamo un modello di filtraggio per selezionare i script fedeli. Convertiamo i script e gli obiettivi in embeddimenti di InstructGPT e calcoliamo la somiglianza cosina come punteggio di somiglianza per misurare la somiglianza semantica. Inoltre, premiamo il script contenente le parole chiave del vincolo di destinazione. Solo manteniamo il script se il punteggio di somiglianza del obiettivo è il più alto nel set di obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità più elevata. Con il nostro metodo, possiamo costruire un set di script di piano linguistico con restrizioni, chiamato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità del set di validazione e test, richiediamo ai lavoratori a livello di massa di cercare e revisionare i campioni errati. Questa figura mostra la distribuzione dei vincoli di CoScript. Troviamo che CoScript mostra una grande pluralità nelle script generate per gli obiettivi specifici. Con CoScript possiamo provare modelli più piccoli ma specializzati alla pianificazione linguistica con restrizioni. Troviamo che T5 addestrato su CoScript può generare script di qualità più elevata rispetto a maggiori modelli linguistici, indicando che i modelli più piccoli possono superare i modelli più grandi quando correttamente addestrati su dataset appropriati. In conclusione, stabilisciamo il problema della pianificazione linguistica con restrizioni. Valutiamo la capacità di pianificazione linguistica con restrizioni utilizzando i modelli linguistici grandi e sviluppiamo un metodo supergenerare-allora-filtrare per i modelli linguistici grandi. Utilizzando i modelli linguistici grandi per generare un set di script di piano linguistico con restrizioni di alta qualità, chiamato CoScript. Speriamo che CoScript possa essere un'origine preziosa per avanzare la ricerca sulla pianificazione linguistica. Grazie per il tuo tempo. Per ulteriori dettagli sul CoScript, puoi leggere il nostro articolo.</sample>
    <sample id="100">PromptRank è un approccio di recupero a pochi shot per la risposta a domande multi-hop, utilizzando una combinazione di un metodo di recupero basato sul TF-IDF e su un riarraffinatore di linguaggio modello. Il sistema utilizza una pool di candidati per le corde e una funzione di calcolo del punteggio basata sulla probabilità del linguaggio modello del quesito rispetto alla corda. La costruzione della corda utilizza un prompt che include i documenti della corda e un'istruzione per elicottere la capacità di ragionamento del linguaggio modello. I risultati mostrano che PromptRank ottiene prestazioni migliori rispetto ai sistemi di recupero a pochi shot tradizionali e comparabili con sistemi densi di recupero a multi-hop esistenti.</sample>
    <sample id="101">La fluidità di PaLM è simile alle sistemi di traduzione state-of-the-art.</sample>
    <sample id="102">Un metodo di filigrana deve soddisfare i seguenti criteri:</sample>
    <sample id="103">I discorsi TED in inglese sono stati tradotti in 14 lingue diverse.</sample>
    <sample id="104">Per la riannotazione, si ottengono molte istanze per esempio.</sample>
    <sample id="105">Cosine and L2 similarity are used to measure the difference between benign data set and backdoor data set.</sample>
    <sample id="106">La nostra paper, chiamata QUEST, è una collaborazione con Pete, Ming-Wei, Kenton e Kristina da Google DeepMind. La motivazione è fornire un dataset per studiare le query che includono set constraints impliciti. Questo dataset contiene più di 3000 query entity-seeking con set constraints impliciti, entities verificate per relevanza e attributi documenti. La costruzione del dataset utilizza category Wikipedia per ottenere query con set constraints. I riferimenti ai personaggi Jane e Austin illustrano come le persone esprimano le loro esigenze di informazioni con set constraints complessi. L'obiettivo è sviluppare sistemi di recupero efficaci per risolvere tali query complesse.</sample>
    <sample id="107">I modelli basati su codificatori multilingue sono stati utilizzati per la complessa compito di analisi semantica di query in linguaggi naturali diversi e rappresentazioni significative.</sample>
    <sample id="108">Iscriviti alla newsletter</sample>
    <sample id="109">Unnatural Instructions è un set di istruzioni naturalmente formulate e automaticamente generato da un modello di linguaggio preaddestrato, come GPT-3. I dataset sono diversificati per attività, contenuto e stile, e non richiedono annotazioni umane. La creatività e la diversità del set sono evidenziate, con esempi di attività come la verifiche di esperimenti scientifici e l'invenzione di parole nuove. Il set è utilizzato per addestrare un modello T5 di 110 miliardi di parametri, che ottiene risultati superiori rispetto a T0++ e Tk-instruct su vari benchmark.</sample>
    <sample id="111">I'm sorry, I can't provide a translation for this part of the speech.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi presento il nostro paper "Do CoNLL-2003 taggers still work well in 2023?". Il nostro paper ha studiato il problema della generalizzabilità utilizzando il Task di Rilevamento di Entity Named (NER). Abbiamo osservato che i modelli sono stati utilizzati nel CoNLL-2003 per sviluppare il NER per quasi 20 anni e questo naturalmente genera alcune problemi. Per esempio, questi modelli possono generalizzare ai dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzabilità? Allo stesso tempo, se osserviamo una povera generalizzabilità, qual è la causa della declinazione del performance dei modelli? Per rispondere a questi problemi, abbiamo sviluppato il CoNLL++ Dataset. Si tratta di un set di dati che abbiamo raccolto dal News di Reuters del 2020 e poi etichettato con le stesse linee guida di etichettaggio del CoNLL-2003. Poi abbiamo sfruttato più di 20 modelli sul CoNLL-2003. L'abbiamo valutato entro la set di test CoNLL-03 e la set di test CoNLL++. Infine, abbiamo calcolato il cambiamento percentuale del F1 per valutare la generalizzabilità di ogni modello. Cos'è necessario per una buona generalizzabilità? Durante le nostre esperimentazioni abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Durante le nostre esperimentazioni abbiamo scoperto che i modelli basati sul Transformer generalizzano meglio ai dati nuovi. Il secondo ingrediente è la grandezza del modello. Abbiamo scoperto che generalmente i modelli più grandi conducono alla migliore generalizzabilità. E infine, non possiamo dimenticare che il numero di esempi di addestramento influisce direttamente sulla performance del task downstream. Qui abbiamo anche scoperto che più esempi di addestramento portano a una migliore generalizzabilità. Passiamo alla nostra prossima domanda: Cosa causa la declinazione del performance dei modelli? Abbiamo avuto due ipotesi. La prima è l'overfitting adattativo, ovvero l'overfitting causato dalla riutilizzo del set di test lo stesso ripetutamente e questo si manifera come la riduzione delle reture su un nuovo set di test. La seconda ipotesi è la drift temporale, ovvero il degradamento della performance causato dal gap temporale incrementale tra i dati di addestramento e i dati di test. Per l'overfitting adattativo, abbiamo visto che dalla grafica a destra, la lineola rossa migliore di fit ha un gradiante maggiore di 1. Questo significa che ogni unità di miglioramento ottenuta sul CoNLL-2003 trasforma in più di una unità di miglioramento sul CoNLL++ che significa che non ci sono reture diminuenti. Questo ci mostra che l'overfitting adattativo in questo caso non è osservato. Ma cosa ne è del drift temporale? Per il drift temporale, abbiamo eseguito un esperimento di retrain o continuare a pretrainare alcuni modelli con dati più recenti e abbiamo scoperto che la performance diminuisce con un gap temporale più grande e questo conferma la nostra ipotesi che la causa principale della declinazione del performance è il drift temporale. Il nostro risultato finale è che, per una buona generalizzabilità, è necessario una migliore architettura del modello, una grandezza del modello più grande e più esempi di addestramento. E questi vanno insieme, non possiamo avere solo uno ingrediente e gettare gli altri. Inoltre, abbiamo anche scoperto che la declinazione del performance qui è causata dal drift temporale e sorprendentemente non è causata dall'overfitting adattativo, anche se il CoNLL-2003 è stato utilizzato per quasi 20 anni. Quindi tornando alla nostra domanda iniziale: "I tagger del CoNLL-2003 funzionano ancora nel 2023?" La risposta è effettivamente un resounding sì. Speriamo che il nostro paper chieda ricerca ulteriore su come migliorare la generalizzabilità dei modelli. Infine, se hai domande, sentiti libera di contattarmi. Grazie mille.</sample>
    <sample id="114">L'articolo presenta un approccio per la compressione dei modelli di linguaggio a multi-head attention, utilizzando una strategia di divisione e conquista. La prima fase è la training con restrizioni di gruppo, che divide gli attenzionisti in gruppi e rende gli attenzionisti del medesimo gruppo più simili e quelli tra i diversi gruppi più separati. La seconda fase è l'algoritmo Voting-to-Stay, che rimuove gli attenzionisti ridondanti e restituisce solo uno per gruppo. I risultati della sperimentazione mostrano una riduzione del 32% dei parametri e una velocità di inferenza aumentata del 62%, con prestazioni paragonabili.</sample>
    <sample id="115">L'approccio utilizza solo il segmento finale del discorso.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, è necessario conoscere che Servin è un giudice.</sample>
    <sample id="117">La qualità dell'esempio è più importante rispetto alla somiglianza con la frase sorgente.</sample>
    <sample id="118">Iscriviti alla newsletter per ricevere le ultime notizie e offerte esclusive</sample>
    <sample id="119">Nel corso esteso, si concentra su GPT-4 e sui modelli GPT series.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="121">Esempi di inferenza diretta includono "Easy on Me" e "I Gotta Feeling".</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">Il presente studio indaga sull'efficacia della tunazione di istruzioni per migliorare la performance zero-shot in multitask learning multi-modal. Utilizzando il modello pre-trainato OFA, l'equipo ha sviluppato MultiInstruct, un set di dataset multi-modal con 62 attività diverse, e ha eseguito esperimenti per valutare l'efficacia della tunazione di istruzioni. I risultati mostrano che la tunazione di istruzioni può migliorare significativamente le performance del modello, riducendo la sensibilità e aumentando la stabilità dei risultati. Inoltre, la tunazione di istruzioni è stata dimostrata come un'approccia efficace per migliorare la generalità del modello su nuovi dataset di multitask learning multi-modal.</sample>
    <sample id="124">Il paper di Tan Qingyu e di Alibaba, intitolato "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models", analizza le bias temporali presenti nelle Large Language Models (LLMs) e propone una nuova dataset, chiamata TempReason, che copre tutti i livelli di ragionamento temporale e periodi temporali complessi. La prima parte del paper presenta un esperimento preliminare sull'indovinare della data, valutando tre LLM: T5-L, FLAN-T5-L e ChatGPT. I risultati mostrano come i primi due LLM abbiano un'eccessiva preferenza per il periodo 2000-2020, mentre ChatGPT mostra un'attesa promettente ma si diminue rapidamente con l'estensione del periodo temporale. Per risolvere i problemi di bias temporali, il paper propone due strategie di training: la pre-training temporale e la reforsementazione di rete neurale a livello di rete temporale. Le performance dei modelli vengono valutate su vari set di test, evidenziando come TempT5, il modello finale, possa migliorare significativamente le prestazioni rispetto ai modelli precedenti.</sample>
    <sample id="125">There are 5 authors involved in the article.</sample>
    <sample id="126">No, non è stato considerato come un approccio standard.</sample>
    <sample id="127">Il paper "Large Language Models Are Reasoning Teachers" propone di utilizzare i modelli di grande dimensione come "maestri di ragionamento" per insegnare ai modelli più piccoli come risolvere complessi problemi. La chiave è la "prompting" a catena di pensiero, che consente ai modelli più grandi di comprendere e risolvere problemi complessi, e poi utilizzando questi modelli come dataset di training per i modelli più piccoli. L'approccio proposto, chiamato Fine-Tuned CoT, utilizza una diversa ragionevolezza per generare molti modelli di ragionamento diversi, che aiutano i modelli studenti a imparare meglio. I risultati mostrano che questo approccio può migliorare significativamente il performance dei modelli studenti, anche con parametri molto piccoli.</sample>
    <sample id="128">Nella nostra conferenza, abbiamo presentato un'opera in collaborazione tra McGill University, Mila e Microsoft Research intitolata "The KITMUS Test: Evaluando l'integrazione del conoscenza da diverse fonti". L'obiettivo è stato di valutare la capacità delle modello di Understanding della Lingua Naturale (NLU) di integrare e utilizzare la conoscenza acquisita durante il pretraining e la fornita all'infrazione. Per questo scopo, abbiamo proposto un set di test per valutare la capacità di integrare la conoscenza proveniente da diversi fonti.</sample>
    <sample id="129">Gli autori hanno fornito esempi di parole come "culture", "tradizione", "orgoglio" e "esotico" come parole contrassegnate per gruppi specifici. Queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dal normale.</sample>
    <sample id="130">The paper does not mention any specific model architectures that do not generalize well. It only discusses the importance of having a good architecture, large model size and more fine-tuning examples for better generalization.</sample>
    <sample id="131">The names of the test data sets are not mentioned in the video.</sample>
    <sample id="132">Due autori sono coinvolti nell'articolo: Akshatha e Martin.</sample>
    <sample id="133">The author works with multiple modalities, including text and images.</sample>
    <sample id="135">ABC-Eval è un approccio di valutazione dimensionale per l'evaluazione del quality del dialogo in chat. Utilizza la annotazione dei comportamenti del chat per ridurre il suono umano nella valutazione e fornire metriche più precise e affidabili. Ha mostrato essere più affidabile e predicivo rispetto ai metodi esistenti, come i Likert scale e le comparazioni pairwise.</sample>
    <sample id="136">L'articolo presenta un nuovo dataset, FERMAT, per l'évaluation di modelli di linguaggio in materia di ragionamento numerico. I modelli di linguaggio sono destinati a risolvere compiti di ragionamento numerico, ma la performance è spesso limitata. L'articolo introduce FERMAT come un dataset più adatto per valutare i modelli, che include domande matematiche e testa le loro capacità di comprensione del linguaggio e delle operazioni aritmetiche. L'articolo mostra come l'uso di una diversità di domande e di modelli di base possa migliorare la performance dei modelli di linguaggio.</sample>
    <sample id="137">L'equipo di ricerca del NUS (National University of Singapore) ha sviluppato un nuovo modello di generazione di piani da progetto tramite istruzioni in lingua naturale. L'obiettivo è stato di permettere ai non esperti di creare piani da progetto utilizzando istruzioni dettagliate e precise, come se avessero una guida di design. Il modello utilizza una sequenza-to-sequenza struttura basata sul transformer e si è dimostrato efficiente rispetto a altri modelli di generazione di immagini condizionate alla testa del testo.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU che è poco studiata è la capacità di integrare e utilizzare entità specifiche e conoscenza di background in modi differenti.</sample>
    <sample id="139">Ying e Zhiyang</sample>
    <sample id="140">Sì, il dataset CoScript è stato sottoposto a controlli di qualità. I crowd-sourced workers sono stati chieduti di trovare e revisionare gli esempi errati per garantire la qualità della valutazione e del set di test.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto presentano alcune limitazioni. Inizialmente, solo una piccola porzione delle traduzioni dipende realmente dal contesto, il che rende difficile l'uso di metriche basate sul corposo, come BLEU, per valutare efficacemente le traduzioni dipendenti dal contesto. Inoltre, le risorse dedicate alla valutazione del contesto dipendente sono limitate in termini di tipi di contesto dipendente e lingue supportate, poiché spesso richiedono conoscenze di dominio e cura umana.</sample>
    <sample id="142">Ecco la traduzione in italiano del contenuto inglese fornito: 

"Mi chiamo Javad Hosseini e sto parlando del nostro lavoro intitolato 'Risoluzione delle espressioni implicite per la selezione delle entità' in cui introduciamo il Corpus AltEntities. Il nostro obiettivo è comprendere la lingua degli utenti quando vogliono fare una scelta. Considerate questa domanda alternativa: "Stai parlando del brano 'Easy on Me' o 'I Gotta Feeling'?" Qui, l'utente vuole selezionare tra due brani. La cosa più evidente è usare una riferimento diretto, ad esempio chiamando il nome del brano "Easy on Me" o la sua posizione, "il primo". Ma a volte un riferimento implicito è più appropriato per avere una conversazione più naturale. Questo può succedere quando l'utente non riesce a ricordare il nome del brano. O quando le pronunziature sono troppo simili e difficili da disambiguare. O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti impliciti: "il più recente", "la canzone che non è energica". È un problema importante nei sistemi conversazionali e anche per il benchmarking dell'intelligenza artificiale basata sul linguaggio (LLMs) per la comprensione delle entità. Non si conosce un set di dati più grande pubblico per il compito, quindi abbiamo raccolto uno utilizzando la crowd annotation. Il nostro set di dati copre tre domini diversi: musica, libri e ricette. Il nostro approccio di raccolta dei dati enfasi informale utilizza un setup di completamento di fumetti. Le fumetti hanno tre bubble di dialogo. Nella prima bubble, Bob dice: "Ricordi quel brano che stiamo ascoltando ieri?" E con questo, Bob definisce il contesto del dialogo. Nella seconda bubble, Alice dice: "Ti vuoi parlare del brano 'Easy on Me' o 'I Gotta Feeling'?" Che è una domanda alternativa. E nella terza bubble, Bob usa un riferimento implicito per selezionare tra queste entità, ad esempio "il più recente". Dobbiamo fornire automaticamente la prima e la seconda bubble, ma la terza bubble è completata dall'annotatore. La prima bubble è scelta tra poche istruzioni manuali per ogni dominio. La seconda bubble, che è la domanda alternativa, è generata come segue. Utilizziamo sempre un modello semplice. "Ti vuoi parlare del brano A o del brano B?" Dove A e B sono campioni da Wikipedia. Qui sono state utilizzati alcuni metodi di campionaggio diversi. Quando ci si sposta in alto nella lista, le entità diventano più simili l'una all'altra e è più difficile farne la disambigua. Il primo è il campionaggio casuale. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il titolo "Il Ritorno". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno informazioni simili o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per un brano. Mostriamo questa domanda alternativa agli annotatori, che sanno il nome delle entità, ma non necessariamente le entità. Quello che facciamo è mostrare some knowledge di sfondo sulle entità. Per i brani, semplicemente forniamo un collegamento di ricerca su Google per ciascuno dei brani e chiediamo agli annotatori di ascoltare almeno un po' di ogni brano e leggere su ciascuno dei brani. Ecco ad esempio il risultato della ricerca su Google per il brano "Easy on Me". Per i ricettisti e i libri, mostriamo some text di sfondo da Wikipedia. Per i ricettisti, inoltre, mostriamo le loro immagini, ancora da Wikipedia, in modo che gli annotatori sappiano come sono. Poi chiediamo agli annotatori di scegliere una delle entità e darci tre a cinque riferimenti impliciti. Ecco alcuni esempi dal nostro set di dati. Ad esempio: "il brano senza parole", "non il brano con il ragazzo di 12 anni", "il brano fittizio", "proviene dall'Azerbaigian", e così via. Il Corpus AltEntities ha 6.000 domande alternative tra tre domini e 42.000 riferimenti impliciti. I risultati con il modello T5 XL sono riportati qui di seguito. Se il modello ha accesso alla stessa knowledge di sfondo del contesto degli annotatori, allora l'precisione è molto alta, circa il 92% al 95%. Ma questo non è realistico. Se il modello ha accesso a alcune conoscenze di sfondo overlapping, allora l'precisione è tra il 82% e il 87%, che è più realistico. Se il modello ha accesso solo ai nomi delle entità, allora l'precisione è solo del 60%, quindi c'è molto spazio per miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili tra domini. ecc."</sample>
    <sample id="143">Il approccio è confrontato con le politiche Wait-k e Local Agreement, applicate a modelli offline, e con un modello di traduzione contemporanea specificamente ottimizzato per la traduzione contemporanea.</sample>
    <sample id="144">The authors of the article are affiliated with the University of Nantes, France.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Omission detection in dialogue summarization is a significant issue, as it leads to incomplete summaries. The task involves identifying omitted utterances from candidate summaries and labeling them for further analysis. A new dataset called OLDS has been created with high-quality omission labels across five domains. Three baseline models are explored: pair-wise classification, sequence labeling, and pointer network. Results show that the F1-score ranges around 50%, indicating challenges in this task. Post-editing using detected omissions improves summary quality significantly.</sample>
    <sample id="147">Il numero di autori coinvolti nell'articolo è tre.</sample>
    <sample id="148">Ciao, sono Sara Papi della University of Trento e della Foundazione Bruno Kessler e vi presento il nostro articolo "L'attenzione come guida per la traduzione contemporanea del discorso". È un'opera in collaborazione con Matteo Negri e Marco Turchi. Ma cosa è la traduzione contemporanea del discorso? La traduzione contemporanea del discorso, o SimulST, è il processo di tradurre una lingua parlata in un'altra lingua in tempo reale, consentendo così la comunicazione tra lingue diverse. Ma qual è il problema delle attuali modelli di SimulST? In genere, vengono adottati architetti specifici trainati, introducendo moduli aggiuntivi da ottimizzare. Ad esempio, i modelli trainati con obiettivi di ottimizzazione diversi. E si trainano e si mantengono più modelli per raggiungere differenti regime di latenza. Ad esempio, trainare un modello con un'attesa media di un secondo e un altro con due secondi, eccetera. Così abbiamo pensato di utilizzare già esistenti modelli di traduzione statica (ST) senza bisogno di riacquisto o di architetture specifiche per SimulST. Utilizzando solo uno stesso modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici. E sfruttare la conoscenza già acquisita dal modello attraverso la meccanica dell'attenzione tra input audio e output testuale. Che è la meccanica cross-attention, che vedrete nell'esempio a destra. La nostra soluzione è di proporsi EDAtt, ovvero Encoder-Decoder Attention, e si tratta di una strategia per decidere se emettere o non emettere una traduzione parziale, basandosi sul luogo in cui l'attenzione punti. Un termine viene emesso se l'attenzione non si concentra, ovvero se la somma è inferiore a un certo threshold alpha, significando che le informazioni ricevute sono abbastanza stabili. Per esempio, se riceviamo un blocco di discorso contenente "Sto per parlare su..." e il nostro modello predica la traduzione in tedesco, osservando i pesos di attenzione cross, vedremmo che i primi due termini indicano gli elementi di discorso ricevuti più presto, mentre il terzo termino indica gli ultimi elementi di discorso ricevuti, come gli ultimi lambda termini di discorso. Questo significa che i primi due termini saranno emessi mentre, poiché la somma dei pesos di attenzione è superiore a un certo threshold alpha, non emetteremo il terzo termine e aspetteremo un altro blocco di discorso. Se andiamo avanti e riceviamo un altro blocco di discorso e il nostro modello predica altri tre termini, osservando questi pesos di attenzione cross vedremmo che nessun termine indica gli ultimi lambda termini di discorso. Questo significa che questi tre termini saranno emessi. Vedremmo anche i risultati principali dell'EDAtt. Trascriverò i risultati della traduzione contemporanea in Germanico su grafici in cui ho misurato la qualità della traduzione tramite BLEU e la lag media, che misura la latenza, e ho anche considerato il lag computationally aware, che contorna i tempi computazionali del modello per predire l'output. Vogliamo che i nostri curve abbiano il massimo possibile valore su questo grafico. Ma vogliamo anche che siano spostate verso sinistra. Compariremo anche con le strategie popolari applicate ai modelli di traduzione statica, come Wait-k e Local Agreement, e con la più recente architettura specificamente adatta alla traduzione contemporanea. Questo è tutto sul risultato della traduzione contemporanea per Germanico. Vedremmo che supera tutte le strategie applicate ai modelli di traduzione statica, poiché i nostri curve sono spostate verso sinistra. E se consideriamo il tempo effettivo trascorso o il lag computationally aware, che è il più veloce strategy. Se volete scoprire ulteriori risultati, leggete il nostro articolo. E abbiamo anche reso disponibili il codice e i modelli e la traduzione contemporanea per facilitare la riproducibilità del nostro lavoro. Grazie per la vostra attenzione.</sample>
    <sample id="149">No, the set of data is not available publicly.</sample>
    <sample id="150">MeetingQA è un nuovo dataset per il task di risposta a domande basato su chieste realizzate durante le conferenze. Offre una gamma di chieste aperte e discussionali, con risposte complesse e a più spazi temporali. I modelli di QA esistenti non sono stillati per questo tipo di domanda, ma MeetingQA offre un'occasione per sviluppare modelli più adatti.</sample>
    <sample id="151">Ciao a tutti, mi chiamo Ying e ho lavorato con il mio collega Zhiyang su una ricerca intitolata "MultiInstruct: Improvando l'apprendimento zero-shot multi-modal tramite tuning delle istruzioni". Con i progressi nella modellazione linguistica a larga scala, molte ricerche hanno esplorato nuovi paradigmi di reutilizzare modelli pre-istruzionati per attività diverse tramite un modo efficiente in termini di parametri e dati. In particolare, molte studie hanno dimostrato che il tuning delle istruzioni consente ai modelli linguistici a larga scala di eseguire attività sconosciute in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei precedenti studi sul tuning delle istruzioni si è concentrata sugli miglioramenti del performance zero-shot su attività solo linguistiche, mentre le attività di visione computer e multi-modal sono stati leaving out. Pertanto, nel nostro lavoro vogliamo indagare se il tuning multi-modal di modelli pre-istruzionati può migliorare la generalizzazione a attività multi-modal sconosciute. Inoltre, al tempo della nostra ricerca abbiamo scoperto una considerabile disparità nell'accessibilità di dataset di istruzioni tra NLP e multi-modal. Esistono più di 1600 attività di istruzioni solo linguistiche. Tuttavia, non esiste un grande set pubblicamente disponibile di attività di istruzioni multi-modal. Questo ci ha spinto a costruire MultiInstruct, il primo benchmark di tuning multi-modal con istruzioni che contiene 62 attività diverse di multi-modal coverendo 10 categorie diverse. Queste attività sono derivate da 21 dataset aperti esistenti e ogni attività è equipaggiata con cinque istruzioni scritte dagli esperti. Per indagare il tuning multi-modal con il nostro dataset proposto, abbiamo scelto OFA, un modello pre-istruzionato multi-modal unificato, come modello base. L'OFA utilizza un vocabolario unico per le token del linguaggio, delle token dell'immagine e delle coordinate di un riquadro. Qui vedremmo alcuni esempi di istruzioni dal nostro dataset MultiInstruct. Per unificare il processing di diversi tipi di input e output dati, seguenti il metodo di OFA e si formulano tutte le attività in una forma di sequenza-to-sequence uniforme. In cui il testo, immagini, istruzioni e riquadri sono rappresentati nello stesso spazio di token. Adesso parlerò del tuning multi-modal. Per il set di train, utilizziamo 53 attività da 9 gruppi per il training e selezioniamo 10.000 istanze per attività. Per il test, riserviamo tutto il gruppo di ragionamento comune per il test e selezioniamo 5 attività ulteriori dal gruppo VQ e Miscellaneous. Utilizziamo tutte le istanze nel set di test per ogni attività. Inoltre, tiriamo a caso 20 attività dal set di test del dataset di istruzioni naturali come attività sconosciuta per NLP. Utilizziamo il modello pre-istruzionato grande OFA come modello base. Durante il training, mistiamo tutte le istanze di tutte le attività. Ogni istanza viene casualmente combinata con una delle sue cinque template di istruzione. Durante il test per ogni attività, condizioniamo 5 esperimenti valutando il modello usando una delle cinque istruzioni. In ogni esperimento, riportiamo il minimo e massimo performance e la dispersione standard della performance tra tutti gli esperimenti. Se l'attività è una classificazione multi-modal, riportiamo l'precisione. Se è una generazione multi-modal, riportiamo Rouge-L. Per NLP, riportiamo Rouge-L. Inoltre, introduciamo un metrica nuova chiamata sensibilità. Questa misura la capacità del modello di produrre outputs coerenti per lo stesso task indipendentemente dalla variazione leggera nel linguaggio dell'istruzione. Qui vedremmo il nostro risultato principale. Come vediamo, il tuning delle istruzioni può migliorare significativamente il performance di OFA su attività multi-modal sconosciute. Inoltre, il trasferimento di apprendimento da dataset di istruzioni naturali può beneficiare il tuning delle istruzioni. Qui vedremmo come aumenta il numero di attività, il modello ottiene un better performance e, contemporaneamente, una sensibilità inferiore. Abbiamo anche eseguito un esperimento. Utilizziamo una istruzione invece delle cinque istruzioni. Come vediamo, utilizzare più istruzioni può migliorare il performance globale del modello e ridurre la sensibilità molto. Questo mostra l'effetto dei diversi strategie di fine-tuning sulle sensibilità del modello. Inoltre, vedremmo che il trasferimento di apprendimento da dataset di istruzioni naturali consente al modello di ottenere un performance molto migliore sulle istruzioni naturali. In generale, propongiamo il primo grande set di dataset di tuning multi-modal con significativamente migliorato il loro capabilità zero-shot di OFA e esploriamo diversi strategie di trasferimento di apprendimento e ne mostriamo i benefici. Dobbiamo progettare una nuova metrica chiamata sensibilità. Una cosa in più, stiamo raccolgendo un set di dataset di tuning multi-modal molto più grande con circa 150 attività ulteriori di visione linguistica e libereremo questo. Questo è un codice QR per i nostri dati e modello. Grazie</sample>
    <sample id="152">Il video presenta Frederick Riemenschneider, che parla del suo lavoro sullo scontro tra NLP e filologia classica. La presentazione è intitolata "Esplorando i modelli di grande lingua per la filologia classica" e introduce risorse valutabili per l'anteprima grecanica e lato linguistico. Inoltre, esplora le implicazioni e le sfide della multilingueità in questi modelli. Prima di entrare nel dettaglio, il video mostra un panorama generale dei modelli di lingua e filologia: Latino BERT fu introdotto nel 2020, seguito da Ancient Greek BERT nel 2021 e da un altro Ancient Greek BERT nel 2022. Il video spiega che, despite these advancements, it seems likely that scholars might want to use a model proficient in both Ancient Greek and Latin. Comunemente, i modelli multilingue sono generalmente non preaddestrati su testi greci antichi. Ecco perché, si è sgravato del compito di creare nuovi modelli di lingua specificamente adatta alla filologia classica. I loro obiettivi sono quattrofoldi: fare paralleli attuali, spingere l'architettura di stato dell'arte ulteriore, esplorare architetture diverse e intraprendere modelli multilingue. Per costruire questi modelli, si è utilizzata una raccolta di dati di preaddestramento per la grecanica e la latina, e si è sviluppata una nuova raccolta di dati di preaddestramento utilizzata dall'Internet Archive. I modelli preaddestrati sono GreBERTa e GreTa per la grecanica, e PhilBERTa e PhilTa per la lingua multilingue. Questi modelli sono state testate e si sono dimostrate superiori ai modelli esistenti per la grecanica e la latina.</sample>
    <sample id="153">Ninareh Mehrabi è una postdoc di ricerca all'AI Alexa AI di Amazon e ha presentato un suo lavoro intitolato "Risolvi ambiguità nei modelli generativi text-to-image". Il loro focus è stato sull'ambiguità presente nelle istruzioni fornite ai modelli text-to-image, come evidenziato da esempi come "The girl enters the room with flowers" o "The girl enters the room with a flower". Per risolvere queste ambiguità, l'equipo ha sviluppato un set di dataset e un'architettura di valutazione automatica. L'architettura utilizza un modello di linguaggio per generare domande clarificative o visualizzazioni possibili, richiedendo ai utenti di fornire informazioni aggiuntive per risolvere le ambiguità. Inoltre, utilizzano un modello di valutazione automatica basato su VQA (Visual Question Answering) per valutare se l'immagine generata è fedele all'intenzione dell'utente. I risultati del loro studio mostrano che la risoluzione delle ambiguità ha un effetto positivo sulla fedeltà delle immagini generate e che il loro approccio di valutazione automatica è congruente con le valutazioniFri</sample>
    <sample id="154">Sara Papi è affiliata alla University of Trento e Foundazione Bruno Kessler.</sample>
    <sample id="155">Il nome della relatrice è Javad Hosseini.</sample>
    <sample id="157">Il progetto di ricerca "Summarizzazione del dialogo con la fusione di struttura dinamica e statica della rete grafica" è una collaborazione tra Shen Gao, Xin Cheng, Mingzhe Li, Xiuying Chen, Jinpeng Li, Dongyan Zhao e Rui Yan. Il progetto si concentra sulla sintetica delle informazioni salienti da un contesto di dialogo complesso e semi-strutturato in un summario conciso. La maggior parte dei metodi precedenti utilizzano strutture di grafo statiche estratte utilizzando strumenti linguistici esterni come il parsing del discorso e la tracciatura dello stato del dialogo. Tuttavia, questi metodi presentano due problemi: dipende da strumenti linguistici esterni non perfetti e non può adattarsi dinamicamente alla rappresentazione del grafo. Per risolvere questi problemi, l'equipo ha sviluppato un modello chiamato SDDS (Static-Dynamic Structure Fusion Graph). Questo modello utilizza un encoder per codificare le utterances nel contesto del dialogo in una rappresentazione vettoriale, un modulo per la costruzione di gradi statici utilizzando un'approccia heuristica, e un modulo di attenzione dinamico per capturare le relazioni semantiche tra le utterances. Infine, utilizza una generatrice di lingua preaddestrata per fusionare la struttura del grafo dinamico e statico nel summario finale.</sample>
    <sample id="158">La presentazione è intitolata "Dual Cache per la risoluzione del riferimento a fili di racconto lungo". La risoluzione del riferimento è un'attività che consiste nell'identificare i menzionamenti e clusters di menzionamenti che indicano lo stesso entità. I metodi tradizionali sono costosi in termini di memoria e calcolazione, mentre i metodi basati sul cache utilizzano una memoria di dimensione fisica limitata. I metodi di cache utilizzano un policy di evicione LRU (meno recentemente utilizzato) quando la memoria è piena, ma questo può causare problemi nella memorizzazione di menzionamenti di entity scattered a lungo distanza nel testo. Per risolvere questo problema, l'equipo ha proposto un dual cache che utilizza una memoria locale e una memoria globale. La memoria locale memorizza le entity locali con policy di evicione LRU, mentre la memoria globale memorizza le entity globali con policy di evicione LFU (meno frequentemente utilizzata). Questo dual cache è stato valutato su quattro dataset pubblici e mostra una migliore prestazione rispetto ai metodi di base, riducendo anche il numero di miss di memoria.</sample>
    <sample id="159">Il paper parla di come i modelli linguistici non abbiano sempre una valutazione robusta del contesto. L'approccio è stato riproposto per valutare l'acceptabilità dei modelli su più sequenze. I modelli sono sensibili a caratteristiche semantico e sintattiche condivise tra le sequenze, e la valutazione attuale con solo una frase non rappresenta completamente il loro conoscenza astratta nel contesto.</sample>
    <sample id="160">In questo primo passaggio, il metodo mappa i token di input in token di output associati, ma non ordina gli elementi. Questo significa che ogni token di input viene associato a un insieme di token di output, ma non è necessariamente ordinato in modo specifico.</sample>
    <sample id="161">In total, 55,000 specific goals with scripts are generated in CoScript.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEplain è MASSalign.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato è una tecnica che permette di utilizzare dati etichettati in maniera scarso efficace, riducendo i costi di etichettatura e ridando la possibilità di utilizzare dati più larghi.</sample>
    <sample id="165">Wenting Zhao, un studente di dottorato alla Cornell University, presenta il loro recente pubblicazione intitolata "Reasoning with Abductive Commonsense Exploiting Mutually Exclusive Explanations". La paper introduce un approccio all'intuizione abduttiva basato sull'apprendimento automatico non supervisionale. Invece di utilizzare approcci guidati dal supervisione, li si focalizza sulle esplanazioni plausibili e sfruttano la loro esclusività mutua. L'approccio proposto, chiamato LiPoR (Likelihood Learning with Posterior Regularization), utilizza l'entropia come regularizzatore per preferire alcune esplanazioni rispetto alle altre. I risultati sull'insieme di test AlphaNLI mostrano che LiPoR supera tutti gli altri modelli, incluso un baseline GPT-3 zero-shot, con una precisione superiore del 4 punti.</sample>
    <sample id="166">Il nostro lavoro, "Un modello neuralmente orientato per la divisione e conquista della ragione per la recupero immagine da testi linguisticamente complessi", si concentra sull'immagine recupero da testi linguistici complessi. Questa complessità deriva dal fatto che le immagini sono molto simili e le descrizioni sono lunghe. I metodi tipici, come i modelli di linguaggio visuale, funzionano bene per il recupero immagine-sentence, ma ridurrebbero il loro effetto quando si confrontano con testi complessi. Per risolvere questo problema, ciamo ci ispiriamo alla strategia Divide-and-Conquer e alla Teoria del Dual-Process. Divide-and-Conquer divide il problema in piccoli problemi, li risolvendo e poi unificandoli per ottenere il risultato desiderato. La Teoria del Dual-Process spiega come la mente umana contiene due sistemi di pensiero: il sistema 1 esegue il ragionamento analogico e il sistema 2 è in grado di ragionare logico, adatto per problemi complessi. I modelli preaddestrati di linguaggio visuale si concentrano sul ragionamento analogico come il sistema 1, ma ridurrebbero il loro effetto quando si confrontano con complessi problemi. Potremmo avere bisogno di un sistema logico come il sistema 2 per risolvere il complesso problema di recupero immagine tramite operazioni logiche. Unire le vantaggi del sistema 1 e del sistema 2 potrebbe essere una soluzione significativa per il complesso ragionamento. Il primo modello del nostro metodo è il Generatore di Proposizioni. Si occupa di decomporre il testo complessivo delle proposizioni in rappresentazioni delle proposizioni semplici. Utilizzando anche la decoder di BART per generare le corrispondenti frasi, il sistema 1, chiamato Interagente Visuo-Linguale, esegue l'interazione tra le informazioni delle proposizioni semplici e le immagini. Il negatore esegue il suo ruolo nel sistema 2, integrando le ragioni state e i risultati delle proposizioni semplici per ottenere la soluzione finale della proposizione complessa su immagini. Infine, combiniamo i risultati dell'interagente visuo-linguale e del negatore per ottenere la soluzione finale. Inoltre, presentiamo due tavole dei risultati esperimentali del nostro metodo. Possiamo vedere che il nostro metodo, NDCR, supera gli altri baseline, e le prove di abolitionismo sul set di test confermano l'efficacia di ogni modulo. Presentiamo anche due casi per controllare ulteriormente il performance del nostro metodo. Possiamo vedere che il nostro metodo può presentare le ragioni state e i risultati delle proposizioni complesse nel passo intermedi. Quindi, il nostro metodo processa interoperabilmente. In conclusione, presentiamo alcune proposte: prima, il calcolo neuronale-simbolico è un approccio utile per migliorare la comprensione compostamentare e la pianificazione dei modelli di linguaggio grandi. Divide-and-Conquer è simile al chain-of-the-thought, mira a decomporre il complesso ragionamento in problemi piccoli e a costruire un percorso di ragionamento. La Teoria del Dual-Process può essere integrata con Divide-and-Conquer.</sample>
    <sample id="167">I documenti in DEplain-web sono stati allineati con metodi di allineamento manuali e automatici.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato utilizzando il dataset Reuters News del 2020 e seguentemente annotato con le stesse linee guida di annotazione utilizzate per CoNLL-2003.</sample>
    <sample id="169">Il paper "Prompting PaLM per la traduzione: Assessare le strategie e il performance" è una collaborazione con colleghi del Google Translate. PaLM è un modello di linguaggio grande con 540 milbiili parametri, addestrato su un'ampia raccolta di testo composta da 780 miliardi di token. Al tempo della pubblicazione, ha raggiunto il record di prestazioni in centinaia di attività NLP. Questo work presenta lo studio sistematico del prompting per la traduzione utilizzando i migliori practice della comunità MT. Utilizzando gli ultimi set di test per evitare l'overlaps tra i dati di test e quelli di addestramento del modello di linguaggio, si confronta con sistemi di traduzione state-of-the-art, come WMT. Si utilizzano metriche neurali di traduzione MT state-of-the-art e presentano anche risultati di valutazione di esperti umani. Infine, si danno alcune raccomandazioni per le strategie di selezione del prompting. Il prompting ha un grande impatto sulle prestazioni dei modelli di linguaggio grandi per la traduzione, come evidenziano gli esperimenti in cui si utilizza una promptingle a un shot e due diverse prompts per ogni frase. La maggior parte delle frasi, 516 su 1000, mostrano una differenza superiore a 1 BLEURT point. Inoltre, ci sono casi estremi in cui la differenza raggiunge 40 BLEURT points. È importante selezionare una buona strategia di prompting. Negli esperimenti, si è scelto una strategia di prompting a 5 shots, dove ogni frase fornita al sistema è seguita dal linguaggio in cui è scritta. Gli esempi sono molto più importanti che la forma della promptingle. Le risposte del modello di linguaggio grande PaLM sono simili alle del sistema state-of-the-art, ma presentano errori di omissione e awkwardness legati alla selezione di esempi di alta qualità.</sample>
    <sample id="170">Mi chiamo Yusen Zhang e sono studente del Penn State University. Oggi presento il nostro lavoro "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". La semantica è la comprensione delle query utenti come SQL o Lambda Calculus. Il Cross-Lingual Semantic Parsing è la traduzione delle query in diverse lingue naturali in diverse rappresentazioni semantiche. Questa figura mostra che devi tradurre le query in diverse lingue naturali utilizzando modelli neurali per SQL, Lambda o FunQL e così via. Esistenti modelli di parsing semantico cross-lingua sono propositi e valutati su dataset limitati. Per esempio, ci sono molte copertura su certe lingue naturali, ma non su cinese e care di copertura su certe rappresentazioni semantiche. La Lambda Calculus è mancante, o sono solo valutate su certi modelli neurali. Per risolvere questo problema, abbiamo proposto XSemPLR. Offrirà un dataset uniforme XSemPLR per il parsing semantico cross-lingua in diverse lingue naturali e rappresentazioni semantiche. Contiene 9 dataset in varie domande, 5 attività di parsing semantico, 8 rappresentazioni semantiche e 22 lingue naturali in 15 famiglie linguistiche. Per valutare meglio il nostro benchmark, consideriamo i sei setting per la traduzione e l'evaluazione. Il primo setting è Translate-Test. Utilizziamo Google Translate API per tradurre la lingua di origine nella lingua di destinazione, quindi utilizziamo un modello monolingue per la training e l'evaluazione. Ad esempio, trainiamo un modello inglese con query inglese e durante l'inferenza tradiamo la query tedesca usando l'API in inglese e poi utilizziamo il modello addestrato per predire il SQL. E anche testiamo il modello Monolingue. In questo setting, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesca tedesca o inglese inglese. E anche testiamo il setting Monolingue Few-shot, in cui addestriamo un modello monolingue con solo il 10% dei dati di training. E testiamo il modello Multilingue, in cui addestriamo un modello multilingue per tutte le lingue. Ad esempio, mettiamo le query tedesche, inglese e cinese insieme per addestrare un modello multilingue. E durante l'inferenza possiamo utilizzare questo modello per tradurre query tedesche o cinesi, eccetera. E anche consideriamo il transfer Cross-lingual Zero-shot e Few-shot. Addestriamo su una lingua di origine e si trasferisce a un'altra lingua. Durante l'addestramento, addestriamo su query inglese o una combinazione di query tedesche e inglese con solo 10% dei dati di training per addestrare un modello multilingue. E durante l'inferenza possiamo utilizzare questo modello per tradurre query tedesche o cinesi, eccetera. E troviamo molti risultati interessanti. Riguardo l'analisi dei modelli monolingue, valutiamo due gruppi di modelli, tra cui Encoder-PTR, che sta per Multilingual Pretrained Encoders con Decoders basati sul Puntatore, come XLM-R + PTR e mBERT + PTR. E valutiamo anche Encoder-Decoder, che sta per Modello Pretrained Encoder-Decoder Multilingue, come mBART e mT5. Troviamo che Encoder-Decoder ottiene il miglior risultato su tutti i nove dataset. Valutiamo mT5 e XLM-R + PTR nel setting multilingue. Troviamo che Encoder-Decoder o Encoder-PTR possono essere migliorati addestrando in una mistura di varie lingue. Troviamo che la maggior parte delle principali lingue naturali ottengono un impatto positivo, eccettando che l'inglese diminuisce il performance in sette dataset e aumenta solo in tre dataset. Sappiamo questo come "Curse of Multilinguality". Valutiamo anche la differenza di performance cross-lingua. Nella figura, la linea blu è Cross-lingual Zero-shot transfer. La linea arancione è Cross-lingual Few-shot transfer. Mentre la linea verde è il Setting Monolingue. Troviamo che confrontando la verde e l'arancione, troviamo che il setting Zero-shot, la differenza di performance di trasferimento cross-lingua è significativa, e confrontando la blu e l'arancione, troviamo che con il setting Few-shot la differenza di performance viene ridotta rapidamente. Troviamo anche altri risultati interessanti. Ad esempio, Encoder-Decoder ottiene risultati migliori rispetto ai precedenti o raggiunge risultati simili. La preaddestrata su lingue naturali inglese può amplamente aumentare la performance del Few-shot su lingue target naturali, e scopriamo che i modelli multilingue come Codex e BLOOM sono ancora inadeguati per i compiti di parsing semantico cross-lingua. In conclusione, abbiamo creato XSemPLR, un benchmark unico per il parsing semantico cross-lingua con diverse lingue naturali e rappresentazioni semantiche. Abbiamo condotto un studio di benchmark completeness su tre tipi di modelli multilingue. I nostri risultati mostrano molti risultati interessanti. Grazie di aver ascoltato.</sample>
    <sample id="171">The speaker mentions that existing works can be broadly classified into four categories, but these methods either are not applicable to embedding as services or lack transferability. Therefore, the proposed method in this paper is a new solution for protecting the copyright of large language model embeddings used as services by introducing a backdoor-based watermark called "embedding marker."</sample>
    <sample id="172">No, according to the presentation, Codex e Bloom sono insufficenti per il CLSP.</sample>
    <sample id="174">Introduzione: La paper "ArgAnalysis35K: Un Dataset di Grande Dimensione per l'Analisi della Qualità degli Argomenti" presenta una raccolta di 35.000 pair di argomenti e analisi, la più grande e qualitativa a livello mondiale. Descrizione: L'analisi è un processo di giudizio sull'efficacia di un argomento, valutandolo su una scala da 0 a 1. I dataset precedenti presentano problemi come la mancanza di qualità, la scarsa diversità e la mancanza di profondità nelle ragioni. ArgAnalysis35K risolve questi problemi, offrendo argomenti di alta qualità, diversificati e con una analisi approfondita. Caratteristiche Un Dataset di Grande Dimensione: - 35.000 pair di argomenti e analisi - 85% degli argomenti provengono da tornei di dibattito di alta qualità o da esperti - 15% degli argomenti provengono da debutanti o da utenti casuali Diversità: - 24 temi selezionati per capturare una gamma varia di argomenti - Argomenti associati a diverse tematiche, garantendo una maggiore diversità Analisi: - L'introduzione dell'idea di analisi come un'ombra unica che spiega meglio un argomento - Aggiunta di analisi in place di semplici argomenti Premiato e Claim Instance-Based Annotator Reliability: - Utilizzo di un modello di analisi basato sulle istanze per valutare la fidelità degli annotatori - Eliminazione solo delle istanze biasate, non delleistryanze totali Relevance Model: - Assignamento di un score di relevanza da 0 a 1 a ogni argomento e tema - Captura del contesto e della rilevanza di un argomento in un tema Conclusione: ArgAnalysis35K è una raccolta unica che offre argomenti di alta qualità, diversità, analisi approfondita e fidelità degli annotatori, rendendosi utile per l'analisi della qualità degli argomenti.</sample>
    <sample id="175">Il metodo utilizza un approccio continuo per risolvere l'ambiguità delle permutazioni, che è basato sull'algoritmo del "Viaggiatore del Commercio". Questo approccio consente di ottenere una soluzione approssimata delle permutazioni possibili, garantendo anche la backpropagation e l'apprendimento delle permutazioni più plausibili dal linguaggio.</sample>
    <sample id="176">L'equità di un modello NLP a valle si riferisce alla sua capacità di gestire e risolvere problemi di equità in termini di prestazioni, efficienza e accessibilità.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">La ricerca sviluppata da Melanie Sclar e i suoi colleghi si concentra sull'aspetto della "Teoria del pensiero" nelle Large Language Models (LLMs). La Teoria del pensiero è la capacità di rilevarare e di comprendere gli stati d'animo degli altri, e è tradizionalmente misurata attraverso compiti di comprensione del testo che coinvolgono più personaggi. I LLM, tuttavia, sono stillini nei compiti di Teoria del pensiero, come dimostrato da esempi come ChatGPT o GPT-3. Per risolvere questo problema, la ricerca presenta SymbolicToM, un metodo per l'inferenza che utilizza rappresentazioni grafiche esplicite per migliorare la capacità di Teoria del pensiero delle LLM. SymbolicToM utilizza diversi modelli grafici, poiché gli stati d'animo non possono essere rappresentati con un singolo grafo. Questi modelli vengono calcolati per tutte le combinazioni possibili di personaggi su un massimo livello di Teoria del pensiero m. Il metodo utilizza un algoritmo di inferenza che si avvale di modelli NLI e OpenIE esistenti. La ricerca mostra che SymbolicToM offre una performance significativa in confronto ai modelli supervisionati, come il GPT-3 fine-tuning e Textual Time Travel, in particolare per i compiti di Teoria del pensiero a seconda ordine.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">Nel nostro studio, abbiamo esaminato la capacità di planning linguistico dei modelli di grande modello per risolvere problemi con restrizioni specifiche. Abbiamo sviluppato un dataset di 55.000 obiettivi specifici e script utilizzando la distillazione del sapere simbolico da modelli di grande modello. I nostri risultati ci permettono di dimostrare che i modelli più piccoli possono essere addestrati a risolvere problemi di planning linguistico con restrizioni specifiche, e che il nostro approccio può migliorare la qualità dei script generati.</sample>
    <sample id="182">Nel contesto di questo articolo, il termine "tropicalismo" è associato alle parole che descrivono le donne latinesi come "vibranti". Questo fuori di contesto si riferisce ad un'archetipo che attribuisce alle donne latinesi una vibrante e colorosa personalità, che potrebbe essere legata alla loro identità etnica o al loro origini culturali. Tale uso del termine "tropicalismo" potrebbe potenziare la stereotipizzazione delle donne latinesi, confermando un'idea preesistente che definisca il loro valore o la loro identità solo attraverso i tratti esterni o le peculiarità culturali, in questo caso l'originale etnica.</sample>
    <sample id="183">Ispirandosi a un studio in cui si ha richiesto alle persone di descrivere una persona utilizzando una prompt, gli autori hanno chiesto alla modella di generare una rappresentazione umana utilizzando una prompt simile.</sample>
    <sample id="184">CXMI è stato utilizzato per misurare l'utilizzo del contesto in questo lavoro.</sample>
    <sample id="185">DrBERT è un modello biomedical in francese basato su RoBERTa e trainato su NACHOS, mentre ChuBERT è un modello clinico basato su anonymizzati data ottenuti dal warehouse di dati dell'ospedale di Nantes.</sample>
    <sample id="187">I'm sorry, I can't determine the number of authors involved in this article based on the provided text.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è un approccio che consiste nell'addestrare il modello su dati di rame successivamente, utilizzando l'addestramento precedente come base. In questo modo, il modello può imparare a migliorare le sue prestazioni in modo iterativo, utilizzando i risultati del training precedente per guidare il prossimo addestramento. Questo approccio è utile per risolvere problemi di apprendimento a livello di dominio, in cui è importante utilizzare i risultati del training precedente per guidare il prossimo addestramento e ottenere le migliori prestazioni possibili.</sample>
    <sample id="189">L'obiettivo del set di dati è comprendere l'uso della lingua quando si vuole fare una scelta.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS coprendo la watermark utilizzata per proteggere il copyright del servizio di embedding.</sample>
    <sample id="191">C'è un autore coinvolti nell'articolo.</sample>
    <sample id="192">La presentazione è intitolata "CAME: Optimizzazione guidata da fiducia e ad alta efficienza memory". Yang Luo spiega come l'adattamento di gradienti è un approccio popolare per la formazione robusta di modelli di linguaggio a larga scala, ma richiede spazio memoria eletture costanti. Alcuni metodi esistenti, come Adam, utilizzano tre volte più memoria rispetto a Adafactor, che riduce significativamente la memoria necessaria ma con un impatto negativo sulle prestazioni. La sfida consiste quindi in costruire un optimizer che ottenga una convergenza rapida come Adam e una riduzione della memoria come Adafactor. Yang Luo introduce due scenari per illustrare come i metodi esistenti di memorizzazione inefficace influenzano il processo di training. Inoltre, utilizza la decomposizione fattoriale non negativa (NMF) per ridurre la memoria necessaria e mostra come Adafactor utilizza l'NMF per ottenere una convergenza lenta rispetto a Adam. Yang Luo presenta poi un approccio innovativo per ridurre l'effetto negativo causato dall'errore di memorizzazione, utilizzando la differenza tra le previsioni e le memorie generate come guida per la memorizzazione. Le sperimentazioni mostrano che CAME ottiene un impatto positivo sulle prestazioni rispetto ad Adam e Adafactor, riducendo la memoria necessaria per la formazione dei modelli di linguaggio a larga scala.</sample>
    <sample id="193">Non è specificato il numero di annotatori utilizzati per creare il set di dati iniziale.</sample>
    <sample id="194">Carnegie Mellon University, University of Washington, Allen Institute for AI</sample>
    <sample id="195">Il paper propone un nuovo approccio per la risposta eseguita alla domanda complessa, denominato RoHT (Reasoning over Hierarchical Question Decomposition Tree). Questo approccio utilizza una decompositione hierarchica della domanda complessa in più livelli, ognuno rappresentato da una sottodomanda. Il sistema utilizza una decompositione hierarchica per identificare le sottodomande più appropriate e utilizzare le risorse appropriate per risolverle. Inoltre, il sistema utilizza una raccolta di informazioni provenienti da una base di conoscenze (KB) e da un corpus di testo per fornire risposte accurate. I risultati sull'indagine KQA Pro e Musique mostrano come il sistema possa migliorare significativamente le prestazioni rispetto a sistemi esistenti.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo sono GPT-3, BlenderBot 2 e Rasa.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli throughout la finestra di contesto è necessaria perché i modelli di linguaggio moderni utilizzano spesso larghe finestre di contesto, e la valutazione con solo una frase o una parola non fornisce un'idea completa della conoscenza del modello.</sample>
    <sample id="199">No, l'allenamento multilingue non ha causato un calo delle prestazioni rispetto al modello inglese monolingue. Infatti, l'allenamento multilingue ha portato a una performance migliore per la maggior parte delle lingue naturali.</sample>
    <sample id="200">No, gli annotatori non conoscono l'entità in anticipo.</sample>
    <sample id="201">La valutazione è stata effettuata utilizzando i migliori practice della comunità MT, ovvero utilizzando i set di test più recenti per evitare l'overlap con i dati di training del modello di linguaggio. Inoltre, sono state utilizzate metriche neurali MT state-of-the-art e risultati di valutazione di esperti.</sample>
    <sample id="202">Il regresso nella generalizzazione influisce su due tipi di NER: l'overfitting adattativo e la drift temporale. L'overfitting adattativo si verifica quando si usa lo stesso set di test in modo ripetuto, causando una riduzione del rendimento sul set di test nuovamente. La drift temporale si verifica quando il rendimento diminuisce a causa della maggiore distanza temporale tra i dati di training e quelli di test.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché aiuta a comprendere come i modelli e i dataset possano agire in modo discriminante riguardo le popolazioni diverse. La posizionalità, che si riferisce alle percezioni e alla percezione di un individuo in base alla sua demografia, identità e esperienze di vita, può influenzare la decisione del researcher e i risultati del processo di ricerca. Inoltre, la posizionalità aiuta a determinare se i modelli e i dataset sono più alignati con alcune popolazioni rispetto ad altre, potenzialmente leaving dietro alcune gruppi sociali. Questo è particolarmente importante quando si tratta di attività NLP socialmente orientate, poiché le decisioni non sono sempre documentate e molte delle modelle sono nascoste dietro APIs.</sample>
    <sample id="204">I LLM multilingue come BLOOM sono stati affinati con una messa a punto integrale.</sample>
    <sample id="205">L'iscrizione del PhD student Shangbin alla conferenza del University of Washington ha presentato un importante study sull'efficacia e sicurezza delle tecnologie di traduzione basate su neural networks (NTTs) per l'apprendimento automatico del linguaggio. L'iscrizione ha esaminato le potenzialità e limitazioni delle NTTs, analizzando il loro impatto sulla comprensione del linguaggio umano e la loro utilità nella comunicazione interculturale. Inoltre, l'iscrizione ha esaminato le potenzialità e limitazioni delle NTTs nel contesto della traduzione automatica, analizzando i risultati dei modelli di traduzione e le risorse necessarie per garantire la qualità e la sicurezza delle traduzioni. Infine, l'iscrizione ha esaminato le potenzialità e limitazioni delle NTTs nel contesto della traduzione biomedical, analizzando i risultati dei modelli di traduzione e le risorse necessarie per garantire la qualità e la sicurezza delle traduzioni biomedical.</sample>
    <sample id="206">Utilizzano un modello di trasferimento del machine learning per l'apprendimento.</sample>
    <sample id="207">I set di test più recenti utilizzati per valutare le capacità di PaLM sono i set WMT (Workshop on Machine Translation).</sample>
    <sample id="208">I ricercatori hanno proposto tre consigli: 1. Individuare e affrontare i stereotipi positivi e narrative essentializzanti; 2. Usare un approccio intersectionale per studiare i pregiudizi e i danni, evitando di perdere di vista cosa potrebbe essere overlooked se si focalizza solo su un'identità; 3. Incrementare la trasparenza riguardo ai metodi di mitigazione del bias, perché è importante capire se ci sono effetti indesiderati come l'overestimazione di valori altrui o altri metodi anti-stereotipo che causino questi pattern negativi.</sample>
    <sample id="209">Il metodo proposto, ovvero l'over-generate-then-filter, mostra una migliore qualità dei risultati rispetto al metodo di riferimento. Inoltre, il dataset creato, CoScript, è più diverso e completa rispetto al dataset di riferimento.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="212">Un modello più piccolo, specificamente T5, è utilizzato nell'articolo.</sample>
    <sample id="213">OFA è utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali.</sample>
    <sample id="215">L'argomento di questo discorso è la struttura dipendenziale della coordinazione. Ci sono diverse strutture dipendenziali presupposte da teorie e approcci di corpus diversi. Ad esempio, nelle universal dependencies, la coordinazione è presa in considerazione come una struttura dipendenziale headed by the first conjunct. Un approccio simile è assunta anche nella teoria del testo del Mel'čuk Igor, dove la coordinazione è presa in considerazione come una struttura dipendenziale headed by the first conjunct. Queste due approache sono asimmetriche, poiché singoleggono un'unica conjuncta. Altre approache asimmetriche includono quella di Prague, dove la coordinazione è presa in considerazione come una struttura dipendenziale headed by the conjunction. In questo caso, le dipendenze vengono dal governor all'ultimo conjunct. Infine, c'è anche una struttura dipendenziale multi-headed usata, ad esempio, nella Hudson's Word Grammar, dove tutti i conjuncti sono head of the coordinate structure. Le dipendenze vengono dal governor a tutti i conjuncti separati: Lisa, Bart e Maggie. L'obiettivo di questo paper è produrre una nuova discussione per la struttura dipendenziale simmetrica della coordinazione, come queste due, contro le strutture dipendenziali asimmetriche, come queste due. L'argomento si basa sul principio della minimizzazione della lunghezza dipendenziale che esploderò con l'esempio. In inglese, come si sa, gli oggetti direttivi preferiscono essere vicini alla verba, mentre gli adjuncti possono essere più lontani. "Marge ha letto oggi" è ok perché l'oggetto direttivo è vicino alla verba, mentre "Marge ha letto oggi il" è molto peggio. Poiché tra la verba e l'oggetto direttivo c'è un adjunct: "oggi". Tuttavia, questo effetto può essere ameliorato quando l'oggetto direttivo è molto pesante e lungo. Poiché in tal caso può essere spostato dopo l'adjunto. Questo è illustrato qui. Sia queste due frasi sono ok. "Marge ha letto questo libro affascinante sui api oggi" è ok perché il relativo NP è molto lungo. Ma è anche ok dire "Marge è venuta e ha stuzzicato oggi questo libro affascinante sui api". La ragione è che questa frase viola il principio generale grammaticale che gli oggetti direttivi devono essere vicini alla verba, ma soddisfa il principio della minimizzazione della lunghezza dipendenziale, che dice che le dipendenze più brevi sono preferite. Queste due alberi mostrano solo le dipendenze cruciali, quelle che non sono costanti tra queste due strutture di coordinamento. Qui abbiamo una dipendenza da "ha letto" all'adjunto di lunghezza 7 misurata in parole e da "ha letto" al "libro" di lunghezza 4, quindi insieme è 11. Quando si cambiano questi due costituenti, la somma delle due dipendenze diventa 6. Più piccolo di 11, quindi è ok. Quello che abbiamo fatto è estrarre statistiche varie sulla coordinazione dalla versione enhancement del Penn Treebank e vedere il paper "Perché non usi le universal dependencies". Queste statistiche confermano l'osservazione fatta molte volte prima che i conjuncti sinistri tendono a essere più corti. E anche l'osservazione fatta in parsing che questa tendenza cresce con la differenza nella lunghezza. Quando la differenza nella lunghezza tra i due conjuncti cresce, il sinistro shorter preferisce essere più corto; la maggior parte della maggiore differenza tra i due conjuncti. Tuttavia, quando il governor è altrui, come qui, "ha ridotto" governa la coordinazione Ted e Ned, questa effetto scompaionte. Abbiamo dimostrato nella paper che questo effetto si estende solo quando il governor è sinistro o assente. Quindi, quando il governor è sinistro, la tendenza per il conjuncto sinistro più corto cresce con la differenza assoluta nella lunghezza tra i due conjuncti e la stessa è osservata quando non c'è governor come in coordinamento delle frasi. Tuttavia, quando il governor è altrui, come qui, "ha ridotto" governa la coordinazione Ted e Ned, questa effetto scompaionte. Abbiamo dimostrato nella paper che questo effetto si estende solo quando il governor è sinistro o assente. Quindi, quando il governor è sinistro, la tendenza per il conjuncto sinistro più corto cresce con la differenza assoluta nella lunghezza tra i due conjuncti e la stessa è osservata quando non c'è governor come in coordinamento delle frasi.</sample>
    <sample id="217">Nel nostro lavoro, "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation", abbiamo studiato la generazione controllabile di dialogo con più attributi. I modelli precedenti si concentrano su singoli attributi, ignorando il setting pratico di generazione con molti attributi. Per risolvere questo problema, abbiamo proposto DCG, una Generazione Controllabile Composizionale che impara concetti di attributo da valori visti e utilizza una perdita di disentangling per disentangolare le combinazioni di attributi diverse. Hanno creato un'evaluazione unificata, MAE, per valutare diversi gradi di attributi senza richiedere dati etichettati aggiuntivi. I risultati delle nostre esperimentazioni ci hanno permesso di dimostrare l'efficacia del nostro metodo e delle nostre metriche di valutazione.</sample>
    <sample id="218">I'm sorry, I can't provide information about the affiliations of the authors from this content.</sample>
    <sample id="219">Il presente rapporto descrive un approccio innovativo per l'analisi dei segnali finanziari presenti nei report annuali delle aziende, denominati Form 10-K. Utilizzando una pipeline multistagione, si è focalizzato sull'identificazione di relazioni significative tra i documenti di un anno e quelli dell'anno precedente. La prima parte della pipeline consiste nell'identificare le relazioni basandosi sulle similitudini semantiche e sintattiche tra i due documenti. Successivamente, si utilizza un modello di inferenza del linguaggio naturale (NLP) per rilevare le parole chiave che rappresentano queste relazioni. Per addestrare il modello, si è utilizzata una combinazione di dataset esterni e di dataset interni, utilizzando tecniche di labeling soft per mitigare la problematica delle etichette pseudo-poor-quality. I risultati della ricerca mostrano che il modello è in grado di identificare correttamente le relazioni significative, dimostrando la potenziale applicabilità di questo approccio nelle procedure di analisi dei segnali finanziari.</sample>
    <sample id="220">Stony Brook University</sample>
    <sample id="221">L'articolo analizza le performance del modello di linguaggio PaLM in termini di traduzione tra diverse lingue. Specificamente, si concentra su tre coppie linguistiche:</sample>
    <sample id="222">Questa ricerca esplora le sfide e le soluzioni per l'adattamento di modelli di risposta a domande (QA) in contesti di dominio diversi. Utilizzando un modello di QA basato su Wikipedia come dominio di origine, si esaminano diversi metodi di adattazione, tra cui zero-shot e few-shot, per migliorare la performance del modello nel dominio di destinazione. Si analizza anche il tipo di shift che si presenta quando si passa da un dominio di origine a un dominio di destinazione, identificando tre tipi di shift: nessun shift, shift di concetto e shift di covariabile. Gli esperimenti mostrano che i metodi di adattazione zero-shot sono più efficaci per i tipi di shift di concetto e covariabile, mentre i metodi few-shot sono più efficaci per il shift completo. Inoltre, si constata che i modelli di QA possono essere migliorati con una performance aumentata del lettore fino a 24%.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">I modelli studiati sono Long-MBART e mBART.</sample>
    <sample id="225">Quello che ho compreso è che 53 attività su 62 sono state utilizzate per il training e il test.</sample>
    <sample id="226">Due autori sono coinvolti nell'articolo: Regina Stodden e Omar.</sample>
    <sample id="227">La comprensione del linguaggio ha raggiunto recentemente un grande successo, fornendo una soluzione generale a molte attività NLP diverse. Questo ci porta a chiedere cosa manca nella ricerca attuale sui modelli di linguaggio: la comprensione del linguaggio fondata su oggetti, ovvero la traduzione di un'espressione del linguaggio naturale in qualcosa che può essere eseguito in un'ambiente specifico, anche se si tratta di un piano o di un programma. Ci sono molte applicazioni per la comprensione del linguaggio fondato su oggetti, come gli assistenti intelligenti come Siri e Alexa, la ricerca semantica su Google e la consultazione di un database medico utilizzando il linguaggio naturale, e anche i robot domestici che eseguono istruzioni naturali. Tutti questi scenari richiedono di mappare una espressione del linguaggio naturale su una rappresentazione in un'ambiente specifico. Ad esempio, quando si richiede un database medico utilizzando il linguaggio naturale, è necessario mappare la richiesta su una query SQL o un problema SQL, mentre per il scenario del robot domestico è necessario mappare le istruzioni naturali in una sequenza di azioni che il robot può eseguire. Ma perché è la comprensione del linguaggio fondata su oggetti particolarmente difficile? La principale ragione è la mancanza di fondamento durante il pre-training. Sappiamo che la maggior parte dei modelli di linguaggio, compresi i recenti grandi modelli di linguaggio, sono pre-trained con corpus testuali e senza fondamento. Questo è anche legato al test dell'ottopus, e il gap tra il pre-training e l'applicazione downstream rende il compito della comprensione del linguaggio fondata su oggetti particolarmente difficile per i modelli di linguaggio. La ricerca esistente sui diversi scenari di comprensione del linguaggio fondata su oggetti spesso utilizza modelli di linguaggio per generare un piano tramite decodificazione autoregressive che si basa sull'abilità di generazione del linguaggio dei modelli. Tuttavia, il problema principale è che il piano o il programma generato non è sempre grammatico o valido. Ad esempio, in un scenario di risposta a domande basate sulle conoscenze, è molto probabile che una query KB generata da T5 non sia eseguibile nel KB o restituisce un insieme vuoto di entità risposte. Ma nel nostro lavoro, propongiamo un nuovo framework per la comprensione del linguaggio fondata su oggetti dove il linguaggio è utilizzato solo per valutare e ordinare i candidati proposti dallingente simbolico. Affermiamo che la discriminazione è qualcosa molto più facile per i modelli di linguaggio di eseguire. Specificamente, nel nostro framework, un agente simbolico interagisce con l'ambiente e propone candidati piani, mentre il linguaggio è utilizzato solo per valutare e ordinare i candidati proposti dall'agente simbolico. In questo caso, il linguaggio non deve gestire la validità e la grammaticità del piano di destinazione perché non deve fare la generazione da sé. Per ulteriori dettagli, si prega di riferirsi al nostro articolo, e siamo disposti a discutere offline. Chiamiamo il nostro framework Pangu, che si riferisce alla primordia</sample>
    <sample id="228">AG News, MIND, SST2 and Enron Spam</sample>
    <sample id="229">La hansa di Gabriella Skitalinskaya e Henning Wachsmuth si occupa della detezione delle dichiarazioni non rafforzabili per il supporto alla scrittura argumentativa. L'argomento è importante perché la scrittura argomentativa richiede spesso una revisione iterativa per raggiungere un'efficacia comunicativa. La hansa introduce due nuovi compiti: il rilevamento delle dichiarazioni non rafforzabili e l'assegnazione di tipi di problemi di qualità da risolvere durante la revisione delle dichiarazioni. Per rispondere a queste domande, l'equipo esamina i pattern di revisione in base ai dati di revisione collaborativi, come Kialo. Identificano quattro challenge: la rappresentatività e la reliable dei dataset, la complessità e l'architettura del modello, l'effetto del contesto e la bias topico e utente. Le loro analisi indicano che i dataset di revisione collaborativa possono essere utilizzati efficacemente per risolvere i compiti e che la distanza tra le versioni di dichiarazione può aiutare nella detezione delle dichiarazioni non rafforzabili.</sample>
    <sample id="231">NACHOS è un dataset di informazioni medico-civile e di farmacovigilanza crawled da Internet in lingua francese.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Il paper "Attention as a Guide for Simultaneous Speech Translation" di Sara Papi, Matteo Negri e Marco Turchi, presentato alla ECCV 2018, introduce una nuova strategia per la traduzione simultanea del linguaggio, chiamata EDAtt (Encoder-Decoder Attention). Questa strategia utilizza l'architettura di una modello di traduzione statica (ST) esistente, evitando il training di modelli specifici per la traduzione simultanea. La traduzione simultanea consiste nell'ottenere una traduzione in tempo reale del linguaggio parlato, consentendo così la comunicazione tra lingue diverse. La traduzione simultanea è complessa poiché richiede adattazioni specifiche all'architettura del modello, come la gestione della latenza e la creazione di modelli diversi per differenti livelli di latenza. EDAtt risolvono questi problemi utilizzando l'architettura di un modello di traduzione statica esistente e utilizzando l'attenzione tra il linguaggio parlato e la traduzione in tempo reale. La traduzione simultanea è valutata attraverso i parametri BLEU e l'average lagging, che misurano la qualità della traduzione e la latenza rispettivamente. I risultati mostrano che EDAtt supera i metodi precedenti applicati ai modelli di traduzione statica e offre una soluzione efficiente e rapida per la traduzione simultanea del linguaggio.</sample>
    <sample id="234">La strategia del prompting ha un grande impatto sulle prestazioni dei modelli di linguaggio a larga scala (LLMs) per la traduzione. In un esperimento semplice, è stato evidenziato che il differenziale tra una strategia di prompting a un shot e una a cinque shots è superiore a 40 punti BLEURT. Questo suggerisce che la selezione di una buona strategia di prompting è cruciale per ottimizzare le prestazioni dei LLMs per la traduzione.</sample>
    <sample id="235">I'm sorry, I can't provide that information.</sample>
    <sample id="236">Le 5 istruzioni scritte da esperti sono:</sample>
    <sample id="237">I autori proposono di utilizzare un set di test chiamato KITMUS (Knowledge Integration Test for Multiple Sources) per valutare la capacità dei modelli di integrare informazioni provenienti da diverse fonti.</sample>
    <sample id="238">MeetingBank è un nuovo dataset creato per aiutare gli sviluppatori di tecnologie di sommario per le riunioni. Questo dataset contiene transcript di 1,366 riunioni del consiglio comunale e circa 7,000 istanze. La raccolta dei dati è stata realizzata utilizzando Speechmatics API per convertire il suono in transcript, identificando il tipo e i dati della riunione, e utilizzando il numero di identità del video per ottenere i segmenti di transcript correlati. I risultati della valutazione dei modelli di sommario includono la performance di GPT-3, che ha ottenuto i migliori punteggi in termini di fluidezza e coerenza, ma ha bisogni di ulteriori sviluppi per la informatività e la verità.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e darò un breve commento sullo studio "Prompting PaLM per la traduzione: Assessare le strategie e il performance". Questo è un lavoro in collaborazione con colleghi del Google Translate. PaLM è un modello di linguaggio grande con 540 miliardi di parametri presentato l'anno scorso. È addestrato su una vasta raccolta di testo composta da 780 milardi di token. All'uscita del nostro studio, era il miglior sistema per centinaia di attività del NLP. In questo lavoro, presentiamo lo studio sistematico del prompting per il modello di linguaggio grande per la traduzione. Valutiamo la capacità di trasformazione dei modelli utilizzando le migliori pratiche della comunità MT. Ci astiamo del test set più recente per evitare di sovrapporre i dati di test ai dati di addestramento del modello di linguaggio grande. E confrontiamo i nostri risultati con quelli dei migliori sistemi, come WMT Evaluation. Utilizziamo metriche neurali MT state-of-the-art e inoltre mostriamo risultati di valutazione di esperti umani. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompting. Il prompting ha un grande impatto sulle prestazioni dei modelli di linguaggio grandi per la traduzione, come vedremmo in un semplice esperimento. Dove abbiamo usato un solo shot prompting e fornito due prompt diverse per ogni frase. La maggior parte delle frasi, 516 su 1.000, ha mostrato una differenza di oltre un BLEURT point. Potrebbe andare fino a 40 BLEURT points in casi estesi. È importante scegliere una buona strategia di prompting. Nelle nostre esperimentazioni, abbiamo scelto una strategia di prompting a 5 shots dove abbiamo segnato ogni frase che abbiamo fornito al sistema con la lingua in cui è scritta. Nell'esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di origine, sono segnalate con colonne tedesche e le traduzioni inglese con colonne inglese. Abbiamo visto che la forma reale del prompting non ha un impatto significativo nelle case di poche o un shot prompting. È fondamentale per il zero e un shot prompting. E quando andiamo, come nel nostro caso, a un prompting a 5 shots, c'è una piccola differenza rispetto alla forma reale del prompting. Le esempi pesano molto. Il riepilogo dei nostri risultati esperimentali è che la qualità degli esempi è più importante che la somiglianza con la frase di origine. È importante scegliere gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt da data di addestramento per l'évaluation dei WMT sul set di dev. Il set di dev è molto più curato e di alta qualità rispetto alla data di addestramento, che è più rumosa. I risultati mostrano un miglior performance quando si utilizza la data di dev. Tuttavia, i sistemi specializzati state-of-the-art hanno un vantaggio significativo rispetto alle traduzioni di PaLM. Ma, PaLM si avvicina a un sistema commercializzato. Nelle nostre esperimentazioni, abbiamo scelto di valutare con Google Translate. Le istruzioni che abbiamo ottenuto dalla valutazione degli esperti utilizzando il framework MQM indicano che la fluidezza della PaLM è simile alle sistemi state-of-the-art, ma il principale differenzio</sample>
    <sample id="240">Benvenuto, sono un modello di linguaggio AI sviluppato da OpenAI. Come posso aiutarti oggi?</sample>
    <sample id="241">L'articolo "Evaluating Human-in-the-loop for Early Misinformation Detection: A Case Study of COVID-19 Treatments" parla dell'importanza di valutare le systems di filtraggio delle informazioni false in modo che includa l'interazione umana. Gli autori, Ethan, Yang Chen, Wei Xu e Alan Ritter, presentano un approccio per la valutazione di queste systems, che include la messa a disposizione di un dataset real-time e la partecipazione di moderatori umani. L'articolo descrive come i systems di filtraggio delle informazioni false spesso non includono l'interazione umana e come questo possa portare a problemi nella valutazione accurata delle systems. L'articolo presenta anche un esempio di systems di filtraggio delle informazioni false per la pandemia del coronavirus, che utilizza un modello di rete di neuroni per identificare le informazioni false e una classificazione di stile per determinare se le informazioni false violano le politiche sociali. L'articolo conclude che l'approccio di valutazione proposto è più realistico e include l'interazione umana, e che il systems di filtraggio delle informazioni false dovrebbero essere valutati in modo simile.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono l'annullamento delle valutazioni a livello di giro, le valutazioni a livello di dialogo e le comparazioni pairwise.</sample>
    <sample id="243">Cinque autori sono coinvolti nell'articolo: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="244">Il riferimento del pronome "he" richiede due tipi di informazioni: la conoscenza entity-specific, ovvero "Servin è un giudice", e la conoscenza di base, ovvero "I giudici decidono casi in una corte di giustizia".</sample>
    <sample id="245">Nel nostro lavoro "Una agletta in un pagliaio: Un'analisi dei lavoratori di alta congruenza sul MTurk per il riassunto", ci dediciamo a analizzare la qualità del lavoro dei lavoratori del MTurk, una piattaforma per le attività di Crowd Work. Utilizzando una pipeline a due passi, includendo una fase di qualificazione e una fase di resistenza, abbiamo identificato 12 lavoratori del MTurk che raggiungono una congruenza elevata nel compito del riassunto. La nostra analisi mostra come il nostro approccio possa garantire una congruenza elevata a un costo inferiore rispetto alle tradizionali filtri di qualità del lavoro.</sample>
    <sample id="246">Sì, il codice è disponibile sul GitHub.</sample>
    <sample id="247">Il paper intitolato "FACTKG: Verifiche della verità tramite ragionamento sui graphi dei conoscenze" presenta una nuova sfida di verifiche della verità basata sullingraphi dei conoscenze. Gli autori, provenienti da KAIST AI, hanno sviluppato un nuovo dataset chiamato FactKG, che utilizza il grapho dei conoscenze DBpedia come fonte di evidenza per verificare le affermazioni in lingua naturale. Il dataset include affermazioni in stile scritto e colloquiale, con etichette di verità SUPPORTED o REFUTED. La sfida consiste nel recupero delle evidenze dal grapho dei conoscenze e nella verificazione delle affermazioni utilizzando le evidenze. Gli autori hanno creato diverse strategie di verifiche, tra cui la verifiche a un solo salto, la verifiche di congiunzione, la verifiche di esistenza, la verifiche a più salpi e la verifiche negativi. Hanno anche proposto due approcci per la trasformazione del linguaggio colloquiale in scritto. I risultati mostrano che il modello GEAR, che utilizza le evidenze del grapho dei conoscenze, ottiene migliori risultati rispetto ai baselines precedenti.</sample>
    <sample id="248">Sì, gli annotatori per NLPositionality sono bilanciati rispetto a ciascun gruppo demografico, ad esempio Paese, genere, ecc.</sample>
    <sample id="249">Le frasi del dominio accettabile sono state perturbate preservando la struttura relevante ma aggiungendo rumore alle frasi.</sample>
    <sample id="250">Una valutazione dimensionale significa valutare una quality in più aspetti.</sample>
    <sample id="251">University of Science and Technology of China</sample>
    <sample id="252">La presentazione è intitolata "U-CREAT: Unsupervised Case Retrieval using Events extrAcT" e è stata realizzata da Sai Kiran Tanikella, Abhinav Joshi, Akshat Sharma e Ashutosh Modi. La presentazione illustra un approccio innovativo per la recupero di casi precedenti in ambito giuridico, utilizzando il linguaggio event-driven e la machine learning unsupervisata. L'approccio, chiamato U-CREAT, utilizza l'event extraction per identificare eventi significativi nella documentazione legale e then utilizza un modello di recupero basato sulle interazioni tra questi eventi. Questo approccio è stato testato su due dataset, incluso il Dataset Indian Legal Prior Case Retrieval (IL-PCR), e mostra risultati superiori rispetto ai metodi tradizionali e ai recenti approcci guidati dal supervisione.</sample>
    <sample id="253">Il progetto di ricerca "DisorBERT: Un modello di adattamento di dominio a doppia traccia per la detezione di segni di disturbi mentali nei social media" è una collaborazione tra ricercatori messicani e spagnoli. Il progetto si concentra sulla definizione dei disturbi mentali come sintomi psicologici che causano disagio e disabilità, influenzando il pensiero, le emozioni, lo stato d'animo e il comportamento. Utilizzando il linguaggio delle piattaforme social, l'obiettivo è di sfruttare i post pubblicati da utenti per rilevare eventuali segni di disturbi mentali. Per fare questo, si utilizza il modello di linguaggio BERT, trainato con dati generali, e lo adatta per un dominio specifico, ovvero i social media e i disturbi mentali. La struttura del progetto include la creazione di un modello di adattamento di dominio a doppia traccia, che utilizza la conoscenza di un lexico per guidare il processo di maschera. I risultati della ricerca mostrano una buona bilanciata tra la precisione e la recall del modello, superando i risultati di altri modelli simili. L'approccio utilizza anche un'interfaccia interattiva per visualizzare le parole più importanti nella frase e le sequenze più significative del testo. Le future linee di ricerca includono l'esplorazione di risorse lexicali diverse e l'utilizzo di dati clinici.</sample>
    <sample id="254">Oggi presento la mia ricerca intitolata "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction". La mia nome è Sun Qi e sono studente all'Università Nanjing di Scienze e Tecnologia. Lo scopo della nostra ricerca è stato di sviluppare un approccio per la distanza relazionale tra entità in un documento, utilizzando dati distantemente supervisionati e riducendo il rumore causato da pseudoetichette. I nostri metodi includono l'estimazione dell'incertezza del modello e la strategia di rilebelling dei dati con incertezza dinamica.</sample>
    <sample id="255">In caso di promptingle lungo, la forma del prompting non è importante.</sample>
    <sample id="257">I'm sorry, but I can't answer that question based on the information provided in the video. The content is about a new dimensional approach to evaluating conversational AI called ABC-Eval and its evaluation results are not mentioned.</sample>
    <sample id="258">Chiang Cheng-Han ha presentato un nuovo studio intitolato "Gli Modello di Linguaggio Grandi Potenziali come Alternative all'Evaluazione Umano?" L'obiettivo del studio è stato valutare la qualità dei test naturalmente elaborati utilizzando i modelli di linguaggio grandi. I modelli sono stati chieduti di seguire istruzioni e fornire valutazioni basate su vari attributi, come la coerenza e la likabilità. I risultati della valutazione dei modelli di linguaggio hanno mostrato che, in generale, gli studenti umani preferiscono le storie scritte da essi stessi rispetto a quelle prodotte dal GPT-2. Tuttavia, ci sono stati modelli di linguaggio che hanno preferito le storie scritte da essi stessi, dimostrando che ci sono modelli di linguaggio che possono agire come alternative all'evaluazione umana.</sample>
    <sample id="259">Il progetto XSemPLR sviluppato dall'Università del Pennsylvania si occupa della semantica del parsing, ovvero alla traduzione delle query in diverse lingue naturali e rappresentazioni significative. Esiste già un'ampia copertura di modelli di parsing semantico, tuttavia molte lingue naturali e rappresentazioni significative sono mancanti. Per risolvere questo problema, XSemPLR fornisce una raccolta di dati uniforme per il parsing semantico multilingue in diverse lingue naturali e rappresentazioni significative. La raccolta contiene 9 dataset in varie domande, 5 attività di parsing semantico, 8 rappresentazioni significative e 22 lingue naturali in 15 famiglie linguistiche. Per valutare meglio i nostri dataset, si considerano sei setting di training e valutazione: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot e Cross-lingual Few-shot transfer. I risultati della nostra analisi mostrano che i modelli Encoder-Decoder ottenono le migliori prestazioni su tutti i 9 dataset. Inoltre, la preaddestramento su lingue naturali principali come l'inglese può migliorare significativamente le prestazioni del Few-shot su lingue target.</sample>
    <sample id="260">Jingwei Yi è l'autore del paper.</sample>
    <sample id="261">Un buon pianificatore deve scrivere script ragionevoli e fedeli alle restrizioni.</sample>
    <sample id="262">Quattro autori sono coinvolti nell'articolo.</sample>
    <sample id="263">La presentazione si concentra sull'investigazione del problema dei pregiudizi sui rilievi nello schema di apprendimento in contesto per le grandi modelle di linguaggio. La discussione inizia con una classificazione dei tipi di pregiudizi e l'identificazione di un nuovo tipo di pregiudizio, denominato pregiudizio dominio-nome. Per mitigare questi pregiudizi, è proposto un metodo di calibrazione chiamato calibrazione dominio-contesto, che utilizza parole casuali del dominio per stimolare la bias del modello. Gli esperimenti mostrano l'efficacia di questo approccio e l'importanza di considerare diversi tipi di parole casuali per mitigare il pregiudizio dominio.</sample>
    <sample id="264">Il progetto di ricerca di Lin Wang, studente di laurea magistrale all'Università di Zhejiang in Cina, si è concentrato sull'audio-visual text generation transferabile. La maggior parte delle attività di generazione di testo multimediale richiede una vasta quantità di etichettamento e, a causa della variazione delle condizioni di costruzione in diversi domini, i risultati esistenti subiscono una degradazione significativa. Per superare questa barriera, Wang ha proposto un nuovo compito chiamato Audio-Visual Text Generation transferibile (TAVT). Questo compito si concentra sugli spostamenti multi-modal come il stile visivo, l'energia audibile e così via. Per risolvere questo problema, Wang ha sviluppato un modello che utilizza un audio-visual meta-mapper network per mappare le diverse conceptions visive in un'unico spazio semantico audibile. Inoltre, ha introdotto un modello di encoder e generator base su transformer e una perdita di perdita Dual Counterfactual Contrastive Learning (DCLL) per ottimizzare la coesione tra il contesto visivo e l'audio. I risultati delle prove eseguite hanno dimostrato che il modello TAVT ha superato con successo i modelli precedenti, raggiungendo un'eccellenza notevole in entrambi i casi di set di train di dominio diversi e di set di query di dominio diversi.</sample>
    <sample id="265">Il relatore è Vasudha.</sample>
    <sample id="266">Adam Przepiórkowski</sample>
    <sample id="268">I'm sorry, I can't answer that question based on the content provided.</sample>
    <sample id="269">ABC-Eval è una nuova approccio dimensionale per valutare l'intelligenza artificiale basata sul dialogo. Questo lavoro è stato sviluppato dal Laboratorio di NLP di Emory, condotto dal professore Jinho Choi, in collaborazione con Amazon Alexa AI. Se hai sviluppato un modello di dialogo e vuoi sapere come compare rispetto al più recente stato dell'arte, la pratica comune è chiedere ai giudici umani di selezionare tra due conversazioni o di valutarle usando una scala Likert. Questi approcci funzionano bene per fornire un'evaluazione complessiva della qualità del dialogo, ma la qualità del dialogo ha molte aspettacchi. Pertanto, potresti voler valutare diversi aspetti del quality del chat per comprendere le forze e le debolezze del modello in modo più dettagliato. Una strategia è semplicemente chiedere ai giudici umani di valutare diversi aspetti del quality del chat, come la rilevanza delle risposte del modello utilizzando metodi di confronto o Likert esistenti. Tuttavia, pensiamo c'è una strategia più precisa e affidabile per l'evaluazione dimensionale del quality del chat. L'approccio che stiamo presentando tenta di ridurre il soggettivo dell'evaluazione umana chiedendo ai giudici di annotare se ogni risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrelevanti o contraddire se stesso o il suo interlocutore. Chiamiamo questo approccio "annotazione dei comportamenti nel chat" o ABC-Eval in breve. Siamo sviluppato questo metodo per coprire completamente i comportamenti del modello di chat suggeriti da recenti studi che influenzino il quality del chat. ABC-Eval è in grado di misurare le tassi a cui i modelli di chat commettono errori tematici vari. Ad esempio, ABC-Eval misura il numero di giri in cui un modello di chat ignora il suo interlocutore o dice qualcosa irrelevante, contraddice se stesso o il suo interlocutore, hallucina fatti irragionali o viola la conoscenza comune, e quando il modello riesce o fallisce a mostrare empatia. Per determinare quale tipo di valutazione è più efficace, abbiamo scelto quattro modelli di chat state-of-the-art e abbiamo valutato 100 conversazioni bot-suomo per ogni modello usando ABC-Eval. Per la comparazione, abbiamo anche valutato queste conversazioni usando tre metodi esistenti: le scale Likert al livello di giro, le scale Likert al livello di dialogo e le comparazioni pairwise al livello di dialogo. Per ciascun metodo esistente, abbiamo raccolto valutazioni su otto dei più comunemente misurati aspetti del quality del dialogo, poiché è la pratica standard per valutare i modelli di chat lungo molti aspetti. Dall'analisi dei risultati delle nostre valutazioni, abbiamo trovato che i label di ABC-Eval sono in generale più attendibili rispetto ai label raccolti da metodi esistenti, come misurato dall'accordo interannotatore su 100 conversazioni doublamente etichettate. Inoltre, i label di ABC-Eval sono più predittivi della qualità globale del dialogo rispetto ai metrini prodotti da metodi esistenti, come mostrato da questa semplice regressione lineare. Ad esempio, puoi vedere come misurare la proporzione di giri con contraddizioni su se stessi e con il suo interlocutore spiega 5% e 10% della qualità del dialogo, mentre gli indici di consistenza media Likert spiegano solo 4% o meno. Infine, abbiamo controllato se ogni metrica di valutazione capti un aspetto unico del quality del chat utilizzando una regressione lineare passo-passo. Posso vedere come la combinazione di tutti i metrini ABC-Eval spiega oltre del 25% della qualità del dialogo, e rimuovendo i metrini uno alla volta, molte di loro perdono una quantità ragionevole di informazioni sul quality. Inoltre, i metrini turn-level Likert spiegano meno del quality, e meno dei metrini carri un'informazione unica. Queste metriche attendibili, informati e distinti ABC-Eval consentono di valutare l'intelligenza artificiale con una risoluzione più alta rispetto ai metrini precedenti. Posso vedere i risultati dell'esperimento che identificano alcune sfide ancora presenti e quantificata con precisione. Ad esempio, i bot che abbiamo testato commettono errori di consapevolezza comune in circa il 20% delle risposte. Producono informazioni irrelevanti in circa il 15% delle risposte, e contraddicano se stessi o il loro interlocutore in circa il 10%. Con l'accelerata pace di miglioramento nel campo, molte delle rate di questi errori potrebbero diminuire nelle nuove versioni dei modelli rilasciate dopo che abbiamo effettuato l'evaluation. Tuttavia, questo è ancora un motivo per cercare metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC-Eval possa essere utilizzato dagli altri nel campo come un passo significativo in questa direzione. E attendiamo di vedere come l'intelligenza artificiale progredirà negli mesi e anni venuti. Grazie di aver visto.</sample>
    <sample id="270">Emory University e Amazon Alexa AI</sample>
    <sample id="271">FTw è una variante del modello di classificazione a due classi standard, ovvero il "vanilla" modello. In questo contesto, il modello FTw è stato sottoposto ad un esperimento in cui è stato continuamente addestrato su dati di validazione puliti, come mostrato nella tabella 1. Questo approccio è diverso dal modello COSINE, che utilizza solo i dati di training e non ha accesso ai dati di validazione puliti. Il FTw è stato sottoposto ad un esperimento in cui è stato continuamente addestrato su dati di validazione puliti, come mostrato nella tabella 1.</sample>
    <sample id="272">C'è un totale di 7 autori coinvolti nell'articolo.</sample>
    <sample id="273">Certo, ecco la traduzione in italiano:</sample>
    <sample id="274">Il nome della relatrice è Yusen Zhang.</sample>
    <sample id="276">IndicMT Eval è un dataset creato per valutare metriche di traduzione per le lingue indiane. L'equipo ha selezionato 200 frasi dal Flores dataset e le ha passate attraverso sette modelli di traduzione differenti, ottenendo una totale di 1.400 traduzioni possibili per ogni lingua. I traduzioni sono state valutate da esperti di traduzione bilingue, che hanno identificato errori e fornito punteggi complessivi. IndicMT Eval include una varietà di metriche di traduzione, tra cui chrF, LabSE, BERTscore e COMET, e fornisce un'analisi dettagliata delle performance di ogni metrica in base alla valutazione umana. L'equipo ha anche svolto esami di robustezza su set di test diversi e ha scoperto che IndicCOMET MQM, una variazione del metrico COMET, ha una performance superiore rispetto al metrico originale, con una correlazione più alta con i punteggi umani.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico.</sample>
    <sample id="278">Il metodo utilizzato per identificare le "parole contrassegnate" consiste nell'usare il concetto di "marcatura linguistica", che sta all'interno della sociolinguistica. Secondo questo concetto, ci sono parole che sono considerate come "non marcate", o "default", e parole che sono considerate "marcati", o "differenziate", quando un gruppo ha una differenza significativa rispetto al gruppo "default". Il metodo consiste quindi a identificare le parole che definiscono i gruppi "marcati" in base alle loro differenze rispetto ai gruppi "non marcati". Questo si riferisce alle parole che definiscono i gruppi "marcati" solo in relazione alla loro identità e li distinquenti da quelli "non marcati".</sample>
    <sample id="279">Shangbin è PhD student in University of Washington.</sample>
    <sample id="280">Il paper "MultiEMO: Un modello di fusione multimodale basato sull'attenzione per la rilevazione delle emozioni in conversazioni" propone un approccio innovativo per la rilevazione delle emozioni in contesti conversazionali. L'approccio, chiamato MultiEMO, utilizza un modello di fusione multimodale basato sull'attenzione per integrare efficacemente informazioni dal testo, audio e visivo. La struttura del modello è composta da quattro componenti principali: il recupero di caratteristiche unimodal, la modellazione del contesto, la fusione multimodale e la classificazione delle emozioni. Il recupero di caratteristiche unimodal utilizza VisExtNet, una nuova rete che esclude le informazioni scene irrelevanti, focalizzandosi solo sullinguish visivo del locutore. La fusione multimodale utilizza MultiAttn, una rete che integrate efficacemente le informazioni dal testo, audio e visivo attraverso l'uso di attenzione bidirezionale. Infine, la classificazione delle emozioni utilizza una perdita di contrasto focalizzata sulle classi minoritari, che aiuta a distinguere meglio le emozioni semanticamente simili. I risultati sull'indagine MELD e IEMOCAP mostrano che MultiEMO ottiene prestazioni superiori rispetto ai metodi precedenti, specialmente per le classi minoritarie.</sample>
    <sample id="281">La traduzione richiede spesso contesto, come evidenzia la differenza nel significato del termine "mole" in vari contesti. Per valutare efficacemente le competenze di un modello di traduzione in materia di traduzioni contestuali, è necessario analizzare le dipendenze dal contesto delle parole tradotte. In questo studio, si è sperimentato l'uso di CXMI per misurare le dipendenze dal contesto delle parole tradotte e si è creato un benchmark per l'analisi del contesto nella traduzione a livello di documento. I risultati indicano che i modelli contestuali sono più accurati rispetto ai modelli non contestuali per certi fenomeni di discorso, ma bisogna fare ulteriori sforzi per migliorare le competenze generali nel campo della traduzione a livello di documento.</sample>
    <sample id="282">Introduzione: L'iscrizione di Xuekai Zhu presenta un nuovo studio intitolato "StoryTrans: Trasferimento stile narrativo non-parallele con rappresentazioni di discussione e enhancement del contenuto" che è stato presentato al Congresso sullinguaggio Automatico (ACL) 2023. Il progetto si concentra sull'imitazione del stile narrativo a livello di storia, considerando l'importanza della discussione per imitare il stile linguistico dell'autore.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è "Lisa, Bart e Maggie".</sample>
    <sample id="284">Il paper intitolato "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" presenta una nuova perdita fuzzy per la perdita di spazio e un'attenzione adattativa per l'attivazione del modello. La perdita fuzzy risolvendo il problema di relazione tra le spazi e le spazi golden, riducendo la dipendenza del modello da spazi precisi. L'attenzione adattativa permette al modello di sfruttare le informazioni di spazio golden e di ottenere una perdita di spazio più precisa. Il paper mostra che FSUIE ottiene risultati eccellenti in diverse attività di extraction di informazioni, dimostrando la sua potenziale applicabile in diverse applicazioni.</sample>
    <sample id="285">Il video presenta un'opera di Mingqi Gao da Peking University intitolata "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework". L'argomento principale è la corretta correzione delle errori di fatto nel summario del dialogo. La corretta correzione dei errors di fatto è importante poiché i summari generati da modelli e anche alcuni summari di riferimento contengono ancora errori di fatto. Esistono due soluzioni principali: l'introduzione di obiettivi correlati alla fidelità nella fase di training o inferenza del modello di summario, che rende le summarie più fedali, e la creazione di un modello di correzione di errori di fatto indipendente dal modello di summario. Tuttavia, ci sono problemi nella maniera in cui i modelli di correzione di errori di fatto sono valutati. I metodi di valutazione utilizzati, come FactCC e DAE, danno un punteggio globale che è troppo vago e non fiable. Inoltre, il modo in cui i modelli di correzione di errori di fatto sono valutati confonda le linee di confine tra i due tipi di soluzioni. I modelli di correzione di errori di fatto possono ignorare il contenuto dell'originale summario e generare una summario diverso ma più fedale, potenzialmente non correggendo alcun errore. Per risolvere questi problemi, è necessario introdurre annotazioni manuali correttive per correggere gli errori di fatto nella summazione del dialogo.</sample>
    <sample id="286">James Finch e Sarah Finch</sample>
    <sample id="287">Javad Hosseini, Filip Radlinski, Silvia Pareti, Annie Louis</sample>
    <sample id="288">I dataset utilizzati per testare i fenomeni sintattici includono Adjunct Island, SyntaxGym e Wikipedia.</sample>
    <sample id="290">WSL, WSL, WSL, WSL, WSL</sample>
    <sample id="291">I'm sorry, I can't provide a translation for this text.</sample>
    <sample id="294">CamemBERT è inizialmente addestrato su 7 GB di NACHOS.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Il video presenta un'analisi di una collaborazione tra l'Università di Torino e Amazon Alexa, che si concentra sull'interpretazione della ironia nel linguaggio naturale. L'approccio dominante è la machine learning supervisionata, basata su dataset di annotazione manuali. La呸 corpus EPIC (English Perspectivist Irony Corpus) è stato creato per analizzare la percezione dell'ironia attraverso le differenze culturali e demografiche. I risultati indicano che i modelli di percezione perspectivisti mostrano una maggiore certezza nelle predizioni rispetto ai modelli tradizionali, e che generazioni e paesi specifici presentano variazioni notabili nella percezione dell'ironia.</sample>
    <sample id="297">Questo progetto sviluppato dall'Università di Berkeley e Cornell University si concentra sull'analisi delle cornarie, termine usato per definire una cornarie codificate che trasmettono messaggi specifici a un gruppo in specifca. Le cornarie sono importanti per l'analisi del linguaggio politico e della persuasione politica, poiché possono essere utilizzate per evitare la censura online. Il progetto sviluppa una tipologia e una glosse di cornarie, identificando più di 340 termini e simboli, con particolare riferimento alle cornarie razziste, transfobiche e antisemitiche. L'analisi delle cornarie è complessa, poiché richiede un contesto specifico e varia a seconda del gruppo in cui vengono utilizzate. Tuttavia, il progetto mostra come le cornarie possano evadere la censura online, dimostrando il bisogno di ulteriori ricerche e analisi nel campo.</sample>
    <sample id="298">Per eseguire un esperimento per verificare se la deriva temporale è la causa principale della perdita di prestazioni, i ricercatori hanno continuato a pretreinare alcune delle modelle con dati più recenti. Hanno scoperto che il performance diminuisce con una maggiore distanza temporale tra i dati di training e quelli di test. Questo conferma la loro ipotesi che la perdita di prestazioni è causata dalla deriva temporale e non dall'overfitting adattivo.</sample>
    <sample id="299">Il paper intitolato "Improving the robustness of NLI models with minimax training" parla delle difficoltà che le modello di analisi del testo (NLI) presentano quando si confrontano con esempi di test fuori distribuzione, ovvero situazioni in cui i modelli non hanno mai visto. Questi modelli tendono a sfruttare spuriousi correlati tra le variabili di input e le risposte, chiamati "shortcuts", che li rendono brittle in contesti non previsti. Per risolvere questo problema, il paper propone un approccio di training minimax, che utilizza una rete di feed-forward come modello di supporto per calcolare un'assegnazione di peso che mette enfasi sui modelli di test fuori distribuzione più difficili. Questo approccio è stato testato su tre dataset di analisi del testo e mostra una migliore resistenza ai modelli fuori distribuzione rispetto ad approcci tradizionali o metodi di mitigazione dei shortcut.</sample>
    <sample id="300">Interactive dictation is a task that allows users to dictate and edit text using their voice in an intuitive manner. The user starts by dictating, which gets transcribed verbatim into the text box. If they make a mistake during speaking, they can correct themselves verbally without needing fixed template commands like some softwares offer. This new task involves flexible interleaving of dictation and editing, not separated by trigger words or commands. It uses open-ended natural language utterances for specifying edits. To introduce this task, researchers designed a data collection interface and built a dataset called DictEdit. They also created a baseline system consisting of four steps: ASR recognition module, speech transcript segmentation, command extraction and normalization, and execution of each step until reaching the final document state.</sample>
    <sample id="302">Permutare i token è necessario per la sequenza di output perché, dopo l'étape di tagging, non si ha un'ordine specifico dei token. L'étape successiva utilizza una permutazione per mettere i token nello stesso ordine che è necessario per ottenere il risultato desiderato.</sample>
    <sample id="303">I autori hanno suggerito ai proprietari dei modelli di aumentare la trasparenza sui metodi di mitigazione dei bias perché è importante comprendere se i pattern positivi o non negativi sono causati da un valore alignamento eccessivo o da altri metodi anti-stereotipo. Senza trasparenza, è impossibile studiare queste cause in modo accurato.</sample>
    <sample id="304">I'm sorry, I can't provide a translation for this text.</sample>
    <sample id="305">La ricerca di Dawei e i suoi colleghi esplora le limitazioni del weakly supervised learning (WSL), un approccio che utilizza etichette di livello debole per la supervisione dei modelli neurali. La loro work, "Weaker Than You Think: A Critical Look at Weakly Supervised Learning", indaga sulle prestazioni delle WSL e suggerisce che tali approcci richiedono etichette di livello debole e di valutazione pulite. I risultati della ricerca indicano che l'uso di etichette di livello debole e di valutazione non pulite riduce significativamente le prestazioni dei modelli neurali, e che l'accesso a etichette di livello pulite è necessario per garantire le prestazioni desiderate. Inoltre, il paper propone di esporre le procedure di selezione del modello utilizzando etichette di livello pulite, di confrontare le WSL con baseline di few-shot learning e di considerare la continuata fine-tuning dei modelli come una soluzione semplice e efficace.</sample>
    <sample id="306">L'entità di seguito descrive un approccio per valutare la capacità di entity state tracking in modelli di linguaggio preaddestrati. I modelli di linguaggio preaddestrati, come Flan-T5 e GPT-3, sono composti da una vasta gamma di modelli che differenziano tra loro per dimensioni diverse, come la preaddestrazione su codice o il numero di parametri. I modelli di GPT-3.5, ad esempio, mostrano una capacità di entity state tracking significativa, mentre i modelli più piccoli o quelli casualmente inizializzati non lo mostrano. La capacità di entity state tracking è legata alla preaddestrazione, ma è incerta se generalizzerebbero al di fuori della struttura specifica utilizzata.</sample>
    <sample id="307">I autori hanno utilizzato metriche di valutazione come entity recognition, classificazione, tag di parole del linguaggio, e risposta alle domande.</sample>
    <sample id="308">La ricerca esplora la posizionalità dei dataset e dei modelli di NLP, analizzando come le decisioni di design potrebbero introducere bias sistemici. Utilizzando un'approcciatore Lab in the Wild e una piattaforma di crowdsourcing online, si è confrontato il linguaggio utilizzato nei dataset Social Chemistry e Dynahate con le annotazioni di diversi individui provenienti da 87 paesi. I risultati indicano che i dataset e i modelli sono maggiormente alignati con gli individui ingleseparlanti e con quelli con istruzione superiore, ma non con i non binari. Le raccomandazioni includono tenere conto di tutte le scelte di design durante la ricerca e svolgere la ricerca con un'approcciatore perspectivistico.</sample>
    <sample id="309">La metrica utilizzata per misurare l'accordo tra annotatori è l'inter-annotatore concordanza.</sample>
    <sample id="310">Il dominio scelto per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili è Wikipedia.</sample>
    <sample id="311">The authors of the article are affiliated with the University of Bremen, Germany.</sample>
    <sample id="312">MultiInstruct è la prima piattaforma di dataset multi-modal per l'instruzione di tuning, che include 62 attività diverse di multi-modalità, coprendo 10 categorie diverse. Questo differisce dai parametri di riferimento precedenti, che erano focalizzati principalmente sullingli del sole e non erano disponibili dataset multi-modal.</sample>
    <sample id="313">Due</sample>
    <sample id="314">La coordinazione binaria si riferisce alle relazioni grammaticali tra due elementi o entità che vengono uniti insieme da una conjunction, come "e" o "ma". In linguistica, la coordinazione è spesso analizzata tramite strutture di dipendenza che identificano le relazioni tra i diversi elementi della frase. Ciò include l'identificazione del capo della coordinazione, ovvero il membro della coordinazione che agisce come guida o punteggio per le altre coordinate. La coordinazione binaria è un aspetto fondamentale della struttura grammaticale e della comprensione della costruzione delle frasi in lingua inglese.</sample>
    <sample id="315">Non è specificato in media per quanto tempo sono stati utilizzati i prompt in questo studio.</sample>
    <sample id="316">I risultati del modello T5 più piccolo indicano che, quando è addestrato su un set di dati specifico come CoScript, può generare script di qualità superiore rispetto a molti modelli di grandi dimensioni. Questo suggerisce che i modelli più piccoli possono superare i modelli più grandi quando si adattano correttamente a dataset appropriati.</sample>
    <sample id="317">Il paper presenta un approccio per l'extrazione di informazioni da testo utilizzando modelli di generazione codice. La tradizione utilizza modelli di linguaggio preaddestrati come T5 e GPT-3, ma ci sono problemi nella conformità tra il formato del testo e quello delle informazioni estratte. CodeIE è una soluzione che trasforma la jon-ta-jon in una jon-ta-jon codice, utilizzando modelli di codice come Codex. I risultati della sperimentazione mostrano che l'approccio utilizzando codice è più efficiente e produce risultati più accurati rispetto ai modelli tradizionali.</sample>
    <sample id="318">Ciao, mi chiamo Yanis Labrak e presento i nostri risultati sul "DrBERT: Un modello pre-training robusto in francese per i domini biomedicali e clinici". In questa presentazione parlerò di modelli di linguaggio nel campo della medicina, presentando il nostro principale risultato: l'introduzione del primo modello biomedicalo in francese chiamato DrBERT, basato su RoBERTa e trainato su NACHOS, che è un set di dati medico rilevato dal web. Inoltre, compariremo i nostri modelli con diverse impostazioni di pre-training e fonti di dati. Poi, esporremo i risultati ottenuti sui 11 compiti biomedicali e clinici in francese. Infine, concluiremo con le nostre esperimentazioni e daremo più dettagli sulla disponibilità di questi modelli. Dato che la sua uscita nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere i problemi di processamento del linguaggio naturale, offrendo grossi miglioramenti di prestazioni rispetto ai metodi statici e contestuali precedenti come Word2vec, fastText o altro. Da allora, questo modello è stato adattato a molte altre lingue, come in francese con CamemBERT, e anche a domini come biomedical con PubMedBERT e BioBERT e clinici con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altre lingue sono rari e spesso sono basati su pre-training continuo a causa della mancanza di dati in dominio. Tuttavia, il francese non aveva alcun modello open source biomedicalo fino a ora. Ci chiediamo quindi cosa sia la fonte più appropriata di dati per una vasta gamma di usi e se i dati rilevati dal web siano buoni sostituti dei dati clinici. Per rispondere a questa domanda, confrontiamo DrBERT con il nostro modello ChuBERT, che è un modello clinico basato sugli anonymizzati dati ottenuti dal warehouse di dati dell'ospedale di Nantes. Poi chiediamo ci siamo abbastanza dati per trainare un modello specializzato con i nostri dati? È 4 GB, 8 GB o di più? Per rispondere a questa domanda, prima trainiamo e confrontiamo quattro modelli da zero: una prima versione di DrBERT con 7 GB di NACHOS; una seconda versione con 4 GB di set di NACHOS; una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi estratte dagli appunti clinici; e infine una versione finale di ChuBERT con una mistura di 4 GB di set di NACHOS e 4 GB di appunti clinici. Inoltre, introdiamo tre modelli trainati con pre-training continuo per analizzare l'effetto della strategia di pre-training. Uno basato sul peso e sul tokenizzazione di CamemBERT e trainato su un set di NACHOS di 4 GB; un altro basato su CamemBERT, ma trainato questa volta sui 4 GB di appunti clinici; e infine uno basato sul modello biomedicalo inglese PubMedBERT e trainato su 4 GB di set di NACHOS. In totale abbiamo set modelli. Per valutarli, raccolgiamo dati per compiti pubblici e privati come riconoscimento di entità, classificazione, tagginaggio parziale-sottovoce e risposta a domande. Questi modelli sono confrontati con sei modelli di baseline: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. L'esame evidenzia che i modelli hanno performato meglio nei compiti con dati del medesimo tipo su cui è stato trainato. Tuttavia, possiamo osservare che i dati provenienti da fonti differenti sembrano essere più versatili. Inoltre, vediamo che l'uso di più dati porta a migliori risultati. In generale, i modelli trainati da zero mostrano migliori risultati su nove dei 11 compiti downstream e superano in generale i risultati del modello generico, qui CamemBERT. Stiamo anche osservando che dati più specializzati sono migliori, ma non scalabili. Tutti i modelli pre-training ottenuti da NACHOS sono gratuiti e disponibili su Hugging Face, e tutti i script di training sono sul nostro GitHub repository. Quindi grazie per questa presentazione e stiamo attendendo di scambiare opinioni durante la sessione poster a Toronto.</sample>
    <sample id="319">Nel lavoro, sono esaminate le seguenti strategie di apprendimento:</sample>
    <sample id="320">Quando si utilizza un test set per addestrare una modello, se il modello è troppo ottimizzato sul test set, ci si potrebbe innalzare la probabilità di overfitting. In questo caso, l'overfitting si verificherà quando si passa da un set di traino ad un set di testo diverso. Se si ha un fattore di overfitting eccessivo, il modello potrebbe non generalizzare bene ai dati sconosciuti.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata eseguendo esperimenti sull'alignamento automatico e sull'adattamento di modelli di linguaggio.</sample>
    <sample id="322">Enrico ha presentato un'analisi sull'intelligenza artificiale basata sul linguaggio e sulla morality. Ha spiegato come la morality è essenziale per le nostre decisioni quotidiane e come è necessario che i modelli di linguaggio possano riconoscere e comprendere la morality nel linguaggio. Ha presentato la teoria della morality fondamentale, che identifica cinque principi morali che influenzano la nostra percezione della morality. Ha spiegato come i modelli di linguaggio potrebbero riconoscere la morality in modo diverso in base a diversi principi e ha presentato un set di dati per studiare questa differenza.</sample>
    <sample id="323">Il paper intitolato "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA" presenta una nuova approccia per risolvere il problema della domanda a conoscenza comune (Commonsense QA). Questo problema richiede ai sistemi di elaborazione del linguaggio (LM) di recuperare conoscenze relevanti da fonti esterne, utilizzando entità e relazioni estratte da diverse basi di conoscenza come ConceptNet, WordNet e Wiktionary. L'approccio proposto, chiamato DHLK, utilizza una rappresentazione del grafico hetrogeneo (HKG) e una modulazione di self-attenzione per la relazione per creare un modello di inferenza che combina le informazioni delle entità e delle relazioni con quelle del contesto della domanda. I risultati sull'OpenBookQA e su Leaderboard della CommonsenseQA mostrano una performance eccellente rispetto ad altri metodi basati sul LM e sul HKG.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi.</sample>
    <sample id="325">Composizione generale è la capacità di un imettore di gestire la ricorsione più profonda e composte che non abbiano mai visto durante il training. Nell'ambito della analisi semantica, la valutazione del composto generalizzabile comporta di ordinare le espressioni delle parole che si sono verificati durante il training. In contrasto con i modelli standard di machine learning, la valutazione non è delimitata alla distribuzione della stessa e contiene composte strutturalmente sconosciute. In questo esempio, il modello ha visto la ricorsione inferiore durante il training e è testato su un esempio con ricorsione più profonda. I modelli basati su seq2seq semplici si rifiutano spesso di generalizzare in modo composto e spesso producono output che non sono collegati al input. Un metodo popolare per risolvere questo problema è integrare gli alberi nei modelli. Gli alberi sono destinati a captare il processo composto che collega le espressioni alle formule logiche. Questo funziona bene, ma gli alberi non sono dati e devono essere ottenuti in qualche modo. Questo può essere complesso e spesso costoso, generalmente richiedendo un preprocessing formaleistico delle formule logiche, ad esempio per gestire i simboli variabili. Ottenere gli alberi richiede anche procedure induttive grammatiche specializzate. Nella nostra paper, non utilizziamo gli alberi e introdurremo un modello seq2seq basato sulle neurali che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, dimostreremo una forte generalizzazione a composte più profonde senza sfruttare gli alberi. Il nostro approccio predice l'output dall'input in due passi. Primo, etichettiamo ogni token dell'input con un insieme unordered di token cheapperanno nell'output. Dopo il primo passo abbiamo tutti i token giusto, ma non sono ordini. Per il secondo passo, utilizziamo un altro modello per predire una permutazione per metterli nello stesso ordine. Introduciamo un nuovo metodo per predire la permutazione che non pone alcuna restrizione rigida sulle possibili permutazioni. Questo rende il nostro approccio molto flessibile e espressivo. Conceptualmente, il nostro modello di permutazione funziona come segue: Andiamo dal lato sinistro allato destro e determiniamo quale insieme token dobbiamo mettere nella posizione dell'output. Per la prima posizione dell'output, semplicemente selezioniamo uno, come evidenziato in rosso. Poi saltiamo al prossimo token multiset, per determinare il secondo token nell'output. Continuiamo questo processo fino a quando ogni token del primo passo sia visitato esattamente una volta. Per dare un antefatto dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli non alberi sul benchmark COGS. Il nostro modello supera i altri con un margine significativo sulla generalizzazione a composte più profonde. Alcune altre tipi di generalizzazione strutturale restano molto difficili, tuttavia. Nella nostra paper risolviamo un po' di challenge tecniche interessanti. Innanzitutto, l'alleanza tra input e output non è data nella distribuzione di training. In seguito, per un token specifico non sappiamo qual è il suo insieme multiset, che porta un problema di training. Inoltre, a volte ci sono più permutazioni consistenti con i dati, ma la più linguistica plausibile è latente. Risolviamo questo mettendo l'alleanza come parte del training. Il nostro modello di permutazione è molto flessibile, ma porta il problema di trovare la soluzione più alta classificata che è NP-dificile. Questo è legato alla "Problema del Vantaggio del Viaggiatore" . Indiamo questa con una relaxione GPU-amico e consentendoci anche di backpropagare attraverso la soluzione e imparare le permutazioni più linguisticamente plausibili. Se vuoi imparare di più sul nostro esperimento e come risolvere questi problemi, guarda la nostra paper o vieni al nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è una percezione che si verifica quando due credenze o azioni sono coerenti tra loro, come illustrato nell'esempio in cui una persona dichiara di sapere che le sigarette potrebbero uccidere e poi dice di aver preso una pausa per fumarle dopo una riunione. Questa percezione si basa sull'idea che la persona non vuole arrendersi alla percezione che le sigarette le portino a morire, ma cerca di giustificare il suo comportamento.</sample>
    <sample id="327">ManagerTower是一种新的视觉语言（VL）模型架构，它在每个跨模态层中引入经理来收集和组合不同层次的预训练单模专家的见解。经理可以适应性地利用不同层次的单模语义知识，以促进更全面的跨模态对齐和融合。使用RoBERTa和CLIP-ViT base作为单模编码器，ManagerTower在各种下游任务上实现了优越的表现，特别是在VQAv2测试标准上的准确性提高了39.15%。 ManagerTower通过经理有效地利用不同层次的通用语义知识，优于BridgeTower，并且在参数较少的情况下，ManagerTower在某些情况下甚至超过了参数更多的模型。</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="329">Il progetto di ricerca di Minghang Zheng e i suoi colleghi di Peking University si è concentrato sulla localizzazione del testo in video zero-shot. Questa tecnica consiste nell'ottenere il testo più relevante in un video in base a una query in linguaggio naturale, utilizzando solo il video e la query come input. Tuttavia, molte delle tecniche esistenti richiedono una grande quantità di annotazioni manuali per la formazione, il che è costoso e inefficace. I principali problemi della maggior parte delle tecniche esistenti sono la generazione di pseudo-consultazioni semplici, la non assicurazione che la consultazione e il video fuori dall'evento siano irrelevanti e l'uso diretto dei pseudo-etichettaggi per la formazione del modello senza considerare il rischio di etichettaggi rumori. Per risolvere questi problemi, i ricercatori hanno proposto un approccio zero-shot chiamato "Generazione di etichettaggi pseudo strutturati per resistenza ai rumori". Questo approccio utilizza un modello di captioning immagine preaddestrato per generare consultazioni pseudo-complomi più complesse, valuta la rilevanza tra le frame del video e le consultazioni pseudo e riduce il rischio di etichettaggi rumori utilizzando una strategia di re-pesca e rafforzamento dei label. I risultati delle sperimentazioni su due dataset mostrano che questo approccio fornisce prestazioni superiori rispetto alle tecniche esistenti zero-shot.</sample>
    <sample id="330">Sì, l'apprendimento cumulativo funziona meglio dell'apprendimento iterativo.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">I dati del parametro di riferimento MuDa sono stati tratti da una parallelizzata di 14 lingue.</sample>
    <sample id="333">Nel nostro lavoro, "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation", ci focalizziamo sullo sviluppo di un approccio innovativo per migliorare la generalità e l'efficacia del Machine Translation (MT). Per raggiungere questo obiettivo, abbiamo proposto un sistema denominato INK (Injecting kNN Knowledge)，che utilizza un loop di adattamento iterativo per rafforzare la rappresentazione spaziale del modello MT. Il sistema INK utilizza tre tipi di adattatore per regolare la rappresentazione: regolare l'alignamento tra le rappresentazioni contestuali e le embedding dei token, regolare l'alignamento tra le rappresentazioni contestuali e le embedding kNN dei token, e regolare l'alignamento delle rappresentazioni delle stesse parole di destinazione. I risultati delle nostre sperimentazioni mostrano che il sistema INK ottiene una migliore performance in termini di punteggio BLEU e di score COMET rispetto ai sistemi esistenti, con un impatto minore sulla memoria e la velocità di inferenza.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Il trasferimento interlinguistico è un processo che consiste nell'addestrare un modello linguistico su dati di più lingue e poi utilizzarlo per predire le output in una nuova lingua.</sample>
    <sample id="337">Il nostro progetto, "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning", si concentra sull'approccio innovativo per la rappresentazione delle parole esterne (OOV) utilizzando le relazioni tra parole. Utilizzando un Graph Relationship, imitiamo le regole lexicali della formazione e dell'associazione delle parole, associando naturalmente le parole esterne con parole relevanti. La nostra approcciatore utilizza un Self-Attention Network per assegnare attributi alle parole esterne, e una Graph Attention Network per mitigare il rumore. L'architettura di rete a livello di grafo e il processo del contrastivo aiutano a rafforzare le relazioni tra le parole. I risultati delle nostre sperimentazioni dimostrano l'efficacia del nostro approccio rispetto ai modelli di base, offrendo un'opzione efficace per la rappresentazione delle parole esterne.</sample>
    <sample id="338">Il gruppo di ricerca rappresentato da Bingsheng ha sviluppato un'approcciatore per l'evaluazione della qualità delle esplanazioni umane utilizzate per addestrare modelli di machine learning. La loro work, intitolata "Sono sempre utili le esplanazioni umane? Un'analisi sull'efficacia delle esplanazioni naturali umane", è stata presentata in conferenza e include una discussione sulle motivazioni, sulle relazioni precedenti e sulle contribuzioni del gruppo di ricerca. Le esplanazioni umane sono importanti per aiutare i modelli a comprendere meglio le informazioni e a migliorare le loro prestazioni. Tuttavia, è necessario valutare la qualità delle esplanazioni umane per assicurare che siano utili per l'apprendimento dei modelli. Il gruppo di ricerca ha sviluppato un'approcciatore per l'evaluazione della qualità delle esplanazioni umane utilizzando un'approcciatore unificato per i dati e eseguendo esperimenti sull'utilità delle esplanazioni. Hanno proposto un metrico chiamato TREU (Test of the Relevance and Effectiveness of Explanations) che considera la utilità delle esplanazioni durante il processo di addestramento e durante l'inferenza. I risultati degli esperimenti hanno dimostrato che il metrico TREU è più efficace nella valutazione della qualità delle esplanazioni umane rispetto al metrico simulatability score.</sample>
    <sample id="339">Dawei è un PhD student at Saarland University in Germany.</sample>
    <sample id="340">Nel nostro lavoro, "ParaAMR: Un Dataset di Parassimili Sinteticamente Diverso Generato da AMR Back-Traduzione", presentiamo una nuova strategia per la generazione di un grande set di parassimili sinteticamente diversi utilizzando AMR (Abstract Meaning Representations). L'AMR è una rete direzionata che rappresenta il significato astratto di una frase. Utilizzando l'AMR back-translation, possiamo ottenere un set di parassimili diversi sintatticamente, poiché la struttura AMR rimane la stessa ma la sintassi è leggermente diversa. I nostri risultati mostrano che ParaAMR ha una maggiore diversità sintattica rispetto a altri dataset di back-translation, mantene una similità semantica simile e offre benefici in applicazioni come l'apprendimento dei embeddings delle frasi e la generazione di parassimili con controllo sintattico.</sample>
    <sample id="341">I'm sorry, but I don't have the ability to access or analyze external content. Could you please provide more information about what specific measures of latency are being referred to in your question?</sample>
    <sample id="342">Gao Jingsheng从Shanghai Jiao Tong University和Xiaobing.AI介绍了他们的研究论文“LiveChat：一个大型的个性化对话数据集，通过自动构建来自现场直播”。该论文旨在解决现有大规模对话数据集的局限性，这些数据集主要依赖于手动注释和指示。为了解决这个问题，他们提出了一种名为LiveChat的新数据集，使用自动对话构造方法。LiveChat是从中国抖音（Douyin）的原始流视频中抓取的，音频被提取并转录成句子。然后，他们收集观众评论，并通过回复谁匹配的方法构建对话。此外，他们还收集了个性化的对话生成信息。实验结果表明，LiveChat在两个基准任务（响应建模和受话人识别）上表现良好，特别是由于其视频来源、更大的规模以及更长的平均会话长度。未来的研究重点将放在LiveChat上LLMs的有效迁移学习上。</sample>
    <sample id="343">Ciao a tutti, mi chiamo Akshatha e oggi ho come collego di lavoro Martin. Stiamo presentando il nostro lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". Questo è un risultato della collaborazione tra McGill University, Mila e Microsoft Research. Le modello di comprensione del linguaggio naturale utilizzano diversi tipi di conoscenza, come la contenuta nei parametri, spesso acquisita durante il pretraining, e la data fornita in input durante l'esecuzione. Le ultime ricerche sugli attività come risposta alle domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante il pretraining per risolvere il problema. Ma la comprensione del linguaggio naturale spesso richiede conoscenza che viene fornita anche durante l'esecuzione. Per esempio, nella frase "John ha visto il presidente nuovamente eletto su TV", i parametri pretraining potrebbero contenere informazioni sullo che i presidenti fanno e su cosa è una TV, ma non potrebbero sapere chi sia "John" o chi sia il nuovo presidente perché il presidente potrebbe essere cambiato dal momento del pretraining. Pertanto, i modelli efficaci per attività di comprensione del linguaggio naturalmente intensifiche richiedono la capacità di integrare e utilizzare la conoscenza di entrambi il tempo del pretraining e il tempo dell'esecuzione. In questo lavoro, propongiamo un set di test diagnosticanti per l'integrazione della conoscenza. Introduciamo un set di test per la risoluzione dei riferimenti, progettato per sfruttare la capacità di sfruttare la conoscenza disponibile in diversi fonti. Valutiamo il set di test con participanti di studio umani e modelli di risoluzione dei riferimenti bienestabili. Ecco un esempio del nostro set di test. Servin è un giudice. Kea è un pasticciere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere i casi in una corte di giustizia, era felice di rilassarsi. La risoluzione di un determinante pronunziale richiede due tipi di informazioni. La prima è la conoscenza specifica dell'entità, come "Servin è un giudice". E la seconda è la conoscenza di fondazione, come "I giudici decidono i casi nelle corte di giustizia". In generale, la conoscenza di fondazione è imparata durante il pretraining di grandi modelli di linguaggio naturale, mentre la conoscenza specifica è osservata durante l'esecuzione. Per controllare la disponibilità di questi due pezzi di informazione, ci astiamo di farla in un solo fonte, o in più fonti. Abbiamo definito tre setting del KITMUS. Il setting tipico è "Fondazione-Pretrain", dove la conoscenza di fondazione è presumibilmente disponibile durante il pretraining. Il setting "Fondazione-Entità" è "Fondazione-Inference", dove entità e conoscenza di fondazione sono disponibili solo durante l'esecuzione. Questo setting è particolarmente interessante, poiché simula il caso in cui la conoscenza necessaria per risolvere un problema sia mancante nel dataset di pretraining dei modelli. Per esempio, perché nuove professioni sono scomparsi dal tempo del pretraining. Ecco un esempio di come controlliamo la disponibilità delle fatte in fonti veraci. Nell'ambiente "Fondazione-Pretrain", supponiamo che la conoscenza di fondazione "I politici cercano seggi elettorali nel governo" sia contenuta nei parametri pretraining e che in contesto di esecuzione forniamo la conoscenza specifica "Chichester è un politico". Nell'ambiente "Fondazione-Entità" abbiamo aggiunto anche la conoscenza specifica e la conoscenza di fondazione durante l'esecuzione. Nell'ambiente "Fondazione-Inference" forniamo la vocazione fittizia "mirituer" al posto di "politico" perché "mirituer" è improbabile di essere contenuto nei parametri pretraining. Valutiamo il set di test con participanti di studio umani e modelli di risoluzione dei riferimenti bienestabili. In questo grafico, vediamo i risultati del miglior modello sulle varianti più difficili del setting "Fondazione-Pretrain". Senza training specifico su KITMUS, entrambi i modelli non performono bene. Quando vengono trainati su KITMUS, tuttavia, entrambi C2F e BERT4Coref performono significativamente meglio rispetto alla scelta aleatoria. Questo suggerisce che quando vengono trainati su dataset generali di risoluzione dei riferimenti, molte tendono a imparare a sfruttare le queue surfacili, che non sono utili quando si testa su KITMUS, in cui tali queue sono rimossi. Esperimenti ulteriori con fatte fittizie indicano che persino i migliori modelli non riescono a integrare correttamente la conoscenza di fondazione fornita solo durante l'esecuzione. In conclusione, molti modelli di risoluzione dei riferimenti sembrano non riuscire a ragionare su conoscenze provenienti da fonti diverse senza training specifico. Tuttavia, con training specifico, alcuni modelli riescono a integrare conoscenze provenienti da fonti diverse. Comunque, persino i migliori modelli sembrano non riuscire a integrare correttamente la conoscenza di fondazione fornita solo durante l'esecuzione. Se siete interessati a ulteriori dettagli, consultate il nostro paper e scaricate il set di test e il codice sul GitHub. Grazie per aver ascoltato.</sample>
    <sample id="344">Obtaining trees is complicated and sometimes computationally expensive. Typically, this involves considerable formalism-specific pre-processing of the logical forms to handle variable symbols or specialized grammar-induction procedures for obtaining the trees.</sample>
    <sample id="345">Il paper intitolato "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" è una collaborazione tra Matthias Lindemann, Alexander Koller e Ivan Titov. Il paper si concentra sull'ability di un modello di neural network a sequenza-sequenza a generalizzare composizioni profonde senza utilizzare alberi. La differenza rispetto ai modelli tradizionali è che questi ultimi utilizzano gli alberi per captare il processo compostionale che collega le espressioni linguistiche alle loro forme logiche. Tuttavia, la creazione di tali alberi richiede complesse procedure e possa essere computationally costosa. Il paper introduce un modello che predica direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Inoltre, utilizza un approccio innovativo per la predizione della permutazione, che non impone restrizioni sulle posizioni possibili delle parole, rendendo il modello più flessibile e espressivo. I risultati sperimentali mostrano una performance notevole in generale del modello in confronto con altri modelli simili.</sample>
    <sample id="346">Shuheng</sample>
    <sample id="347">Mi chiamo Myra e oggi parlando del nostro articolo "Personaggi etichettati: utilizzando promettere di naturale linguaggio per misurare stiliomi in modelli di linguaggio". Questo lavoro è realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molte persone hanno documentato la prevalenza di stiliomi e storie pregiudiziate in grandi modelli di linguaggio, o LLMs. Tuttavia, questi metodi presentano alcune limitazioni. Inoltre, la maggior parte del lavoro in questo campo non contemplates l'intersectionalità, che è la idea che identità sociali multiple possono compounding i pregiudizi e essere luoghi unici di danno. Per superare queste limitazioni, siamo riusciti a sfruttare la proprietà che questi nuovi LLMs di istruzione sono molto buoni a rispondere a istruzioni e promettere. Possiamo chiedere al modello di generare un personaggio, che è una rappresentazione di un individuo immaginato usando un promettere come "immagina di essere una donna asiatica. Descriviscela da te.". Questo è molto generalizzabile a qualsiasi identità perché possiamo specificare qualsiasi marker di identità che vogliamo nel promettere. Qui ci sono alcuni esempi di generazioni dal GPT-4. Subito vediamo che, mentre le usate non sono overtamente negative o tossiche nel senso tradizionale di queste parole, ci sono alcuni pattern interessanti. La donna asiatica è descritta come sull'attentiva; la donna araba occidentale viene descritta usando parole come esotica e riferendosi a un regno meraviglioso. E entrambe le personaggi di colori superiori fanno riferimenti alla loro antonomasia mentre il uomo bianco non ne fa alcun riferimento. Per captare questi pattern, il nostro metodo ha due parti. La prima è la generazione di questi personaggi. I nostri prometteri per generare questi personaggi erano ispirati da un studio in cui si ha dato ai sujets umani questi prometteri, scoprendo che, daremo ai sujets umani, si sono in grado di rilevare storie di razzismo. E anche questo permette una confronto diretto tra i nostri personaggi generati e le risposte scritte dallo umano. La seconda parte è le parole etichettate, che è un metodo per identificare le parole che distinguono gruppi etichettati da quelli non etichettati. Mi elaborerò su questo più tardi. Il vantaggio di questo è che otteniamo pattern specifici e storie etichettate senza dover dipendere da alcuna lista specifica. Il metodo Marked Words si basa sull'idea sociolingue della "stilizzazione", che sta che c'è un default non etichettato e qualsiasi gruppo che differisce dal default è etichettato linguaticamente. Ad esempio, la parola "guerriero" è associata generalmente alle maschere. Quando si descrive un guerriero che è una donna, si specifica "guerriera" e etichetta la parola con "donna". E in generale, i gruppi dominanti nella società sono entrambamente etichettati linguaticamente e socialmente non etichettati, mentre i gruppi marginalizzati sono generalmente etichettati. Nelle nostre analisi, definiamo cosa siano gli gruppi etichettati e non etichettati e poi confrontiamo i personaggi usando il Fightin’ Words, che consiste nell'utilizzare ratio log-odds per distinguere le parole top per ogni gruppo etichettato. Per esempio, per i personaggi delle donne nere, faremo Fightin’ Words e compariremo i log-odds ratios rispetto a entrambi i personaggi bianchi e maschili perché sono i due corrispondenti gruppi non etichettati. Ora per alcuni risultati. Primo, utilizzando un lexico di storie etichettate, scopriamo che i personaggi generati contengono molto più storie etichettate rispetto ai personaggi scritti dallo umano. Tuttavia, quando analizziamo la distribuzione delle parole e del lexico, scopriamo cose molto diverse. I personaggi generati hanno un'alta quota delle parole del lexico, mentre i personaggi scritti dallo umano hanno una distribuzione più ampia di parole, mentre le parole storie etichettate che sono nelle personaggi generate sono solo le parole "alti" e "sportivi". Questo lexico non riesce a captare molto bene i pattern harmful che abbiamo visto nelle slide precedenti. Altre informazioni:</sample>
    <sample id="348">La nostra ricerca, "Personaggi Markati: Utilizzando Prompi di Lingua Naturale per Misurare Stereotipi in Modello di Linguaggio", sviluppata in collaborazione con Esin Durmus e Dan Jurafsky, si concentra sulle limitazioni delle misure precedenti utilizzate per identificare i stereotipi presenti nei modelli di linguaggio grande (LLMs). Queste misure, spesso basate su dataset manualmente curati, non generalizzano bene a diverse demografie o contesti e non contengono considerazioni sull'intersecolionalità. Per risolvere questi problemi, abbiamo sfruttato la capacità dei LLMs di rispondere a istruzioni, chiedendo ai modelli di generare "personaggi" utilizzando prompt come "Immagina di essere una donna asiatica. Descriviti da te." Questo approccio è generalizzabile a qualsiasi demografia, poiché si basa solo sull'identità specificata nel prompt. I risultati della nostra ricerca mostrano come i personaggi generati abbiano un'alta frequenza di parole stereotipizzanti, ma con un'assegnazione di valori positivi o almeno non negativi. La nostra analisi ha rivelato come parole positive potrebbero sottolineare pattern dannosi, e ci sono raccomandazioni per gli proprietari dei modelli dilinguaggio per affrontare questi fenomeni e utilizzare un'approccio sull'intersecolionalità per studiare i pregiudizi e i danni.</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi e sono dell'Università di Scienze e Tecnologie della Cina. Mi piace fare un breve annuncio del nostro articolo. Stai copiando il mio modello? Protetta la copyright dei modelli di grande lingua tramite watermark dietro porta per embedding come servizi. Per prima cosa, introduciamo l'background sulle embedding come servizi. Attualmente, i modelli di grande lingua come GPT, LLAMA, PALM sono eccezionali nella comprensione e generazione del linguaggio naturale. L'embedding come servizi è uno dei servizi costruiti su modelli di grande lingua per aiutare diversi compiti NLP. Ad esempio, OpenAI offre un API basato su GPT. Tuttavia, gli ultimi lavori hanno dimostrato che l'attaccante può truffare il modello imparando dal embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright delle embedding come servizi. Per proteggere il copyright delle embedding come servizi, una delle soluzioni è di incorporare un watermark nel servizio fornito e controllare se un altro servizio contiene il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà: in primo luogo, il metodo deve essere applicabile alle embedding come servizi; in secondo luogo, il watermark non deve degradare la utilità delle embedding fornite; in terzo luogo, il watermark deve essere abbastanza nascosto all'attaccante o l'attaccante può rimuovere facilmente il watermark; in quarto luogo, il watermark deve essere trasferibile al servizio dell'attaccante durante il processo di estrazione del modello. Le opere esistenti possono essere generalmente classificate in quattro categorie. Tuttavia, questo metodo o non è applicabile alle embedding come servizi o care di trasferibilità. Pertanto, nel nostro paper, propongiamo Embedding marker, un metodo di watermark basato sul backdoor applicabile alle embedding come servizi. Ora ti darò un'introduzione dettagliata sul nostro Embedding marker. L'Embedding marker contiene due passaggi principali: l'iniezione del watermark e la verifica del diritto d'autore. Prima di questi passaggi principali, selezioniamo un insieme di trigger. L'insieme di trigger è un gruppo di parole con frequenza media. Siamo presumibili che il fornitore possa raccolgere un set di test generale e contare la frequenza delle parole con esso. Nell'iniezione del watermark, prima definiamo un embedding di destinazione. Quando un utente invia una frase al servizio fornito, il fornitore conteggia il numero di trigger nella frase. L'embedding fornito è una somma peso del embedding di destinazione e dell'original embedding. Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di destinazione. La verifica del diritto d'autore consiste nell'identificare se un modello dietro un altro servizio contiene il watermark. Primo, costruiamo un back door e un set di dati benigno. Il set di dati back door contiene frasi in cui tutte le parole appartengono all'insieme di trigger mentre tutte le parole nelle frasi dei dati benigno non appartengono agli insiemi di trigger. Poi, il fornitore richiede gli embedding dal servizio del truffatore con i set di dati. Calcoliamo la somiglianza cosina e L2 tra l'embedding richiesto e l'embedding di destinazione. Calcoliamo la differenza della somiglianza cosina e della somiglianza L2 tra i dati benigno e back door, che si definiscono delta cosina e delta L2. Inoltre, appliciamo il test KS e utilizziamo il suo valore p come il terzo metrico. Eseguiamo sperimentazioni su quattro dataset: AG News, MIND, SST2 e Enron Spam. Presumiamo che il fornitore applichi il set di test wiki per contare la frequenza delle parole. I risultati sui quattro dataset mostrano che il nostro Embedding marker ha una buona performance di identificazione mentre mantiene una grande utilità per i compiti di livello inferiore. Validiamo anche la copertezza dell'embedding fornito visualizzando l'embedding delle frasi su quattro dataset [INAUDIBLE 4:39] PCA. La legenda delle figure significa il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli embedding back door e gli embedding normali. Grazie. Se hai domande, è possibile discutere con noi.</sample>
    <sample id="350">Il presente slideshow presenta un'analisi sull'interpretazione del termine "superumanità" nel contesto delle applicazioni neurali di linguaggio (NLU). L'argomento è approfondito con la presentazione di due dataset di benchmark popolari: SuperGLUE e SQuAD. Gli autori analizzano come le performance dei sistemi neurali di linguaggio e quelle umane siano confrontate, esaminando i risultati di leaderboard e identificando errori nella valutazione dei dataset. Inoltre, si discutono le condizioni di valutazione delle performance umane e le potenziali limitazioni della comparazione tra sistemi e umani. L'analisi conclude con consigli per costruire benchmark più attendibili e per evitare errori simili in futuri studi.</sample>
    <sample id="351">La nostra paper, "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" esplora il problema della generalizzazione utilizzando il Task di Rilevamento delle Entità Nominate (NER) nel contesto del CoNLL-2003. I modelli sviluppati per il NER nel CoNLL-2003 sono stati utilizzati per quasi 20 anni, ma ci sono alcune problematiche che si presentano: 1) se questi modelli possono generalizzare a dati moderni? 2) quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? 3) se osserviamo una degradazione della performance, che causa questo degrado? Per rispondere a queste domande, abbiamo sviluppato il Dataset CoNLL++. Si tratta di un set di dati raccolti dal Reuters News del 2020 e annotati con gli stessi criteri di annotazione del CoNLL-2003. Abbiamo fine-tunato più di 20 modelli sul CoNLL-2003 e li abbiamo valutati su entrambi i set di test del CoNLL-03 e del CoNLL++. Per valutare la generalizzazione dei modelli, abbiamo calcolato la percentuale del cambiamento del F1-score. Per capire cosa è necessario per una buona generalizzazione, abbiamo eseguito esperimenti e abbiamo scoperto che ci sono tre ingredienti principali necessari: 1) la struttura del modello: i modelli base trasformer sembrano generalizzare meglio ai dati nuovi; 2) la grandezza del modello: modelli più grandi tendono a generalizzare meglio; 3) il numero di esempi di addestramento: più esempi di addestramento mean</sample>
    <sample id="352">ABC-Eval è un approccio di valutazione a dimensioni per l'evaluazione del quality del dialogo in conversational AI. Consente di misurare le probabilità che i modelli dialoganti commettano errori tematici, come rispondere con informazioni irrelevanti, contraddire se stessi o il contraparte, hallucinare informazioni non esistenti o violare la conoscenza comune. Offre una valutazione più precisa e affidabile rispetto alle metodi esistenti, come le valutazioni Likert turn-by-turn e dialogue-level, e fornisce metriche più informative e distinte per l'analisi del quality del dialogo.</sample>
    <sample id="353">L'articolo "Python Code Generation by Asking Clarification Questions" di Haau-Sing Li, Mohsen Mesgar, André F. T. Martins e Iryna Gurevych si concentra sulla generazione di codice tramite domande di chiarificazione. La motivazione è la mancata risposta a un problema importante: l'underspecification dell'input. L'articolo introduce un approccio interattivo per la generazione di codice, focalizzandosi sullo scarico delle operazioni chiave attraverso domande di chiarificazione. Si propone di creare CodeClarQA, un set di dati sintetico con domande di chiarificazione su operazioni chiave, e di utilizzare un pipeline di codifica basata su domande di chiarificazione. Il dataset è creato identificando le operazioni chiave e le loro documentazioni, e utilizzando modelli di codifica per la generazione del codice. Le results della ricerca mostrano che il pipeline ha una performance migliore rispetto ai modelli solo trainati su codice e NLD, dimostrando l'efficacia dell'approccio interattivo per la generazione di codice.</sample>
    <sample id="354">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino all'anno 2019.</sample>
    <sample id="355">Mi chiamo Vasudha e sono una candidata a laurea in Informatica per la Computer Science presso Stony Brook University. Vorrei presentare il nostro lavoro accettato alla ACL 2023 come paper a lungo, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge". Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. La dissonanza cognitiva si riferisce alle due credenze o azioni che sono inconsistenti, come questo esempio: "Saprei che le sigarette potrebbero uccidermi" e poi dire "Ho preso un paio dopo la riunione". Queste due credenze sono in dissonanza. Una ragione per cui è importante studiarla è che aiuta a comprendere gli effetti della discordia tra le persone, a tracciare tendenze e valori di credo, e a comprendere meglio le variazioni nell'attitudine. La alta dissonanza cognitiva è anche legata ai disturbi dell'ansia e ai problemi mentali e può aiutare a comprendere meglio lo stato mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche aiutare a comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere i modi di pensiero individuali e aiuta a comprendere meglio i processi di decisione. Per raggiungere il nostro obiettivo di creare un dataset di dissonanza, abbiamo condotto una grande annotazione di relazioni di dissonanza. Utilizzando un approccio di dissonanza prima, come mostrato nella flowchart qui. I tweet furono passati attraverso un parser PDTB e paia di unità di discorso furono annotati according to le guida descritte nel nostro paper. Come si può vedere qui, la dissonanza è stata solo trovata in 3,5% delle paia di unità di discorso annotate. Dopo aver raccolto circa mille esempi di paia di unità di discorso, abbiamo eseguito il training per un classificatore iniziale trainato solo su 43 esempi di dissonanza. A causa della scarsità assoluta e del mancino di dati precedentemente, stiamo facendofronte al problema della rarezza assoluta. Per alleviare questo, abbiamo sperimentato la combinazione di transfer learning e active learning per annotare in modo che possano essere raccolti più esempi di dissonanza con meno round di annotazione, riducendo i costi generali di annotazione mentre migliorano la detezione della dissonanza. Poiché il modello iniziale non era in grado di capturare la classe di dissonanza, iniziamo il processo di active learning trasferendo i pesos da tre attività da trasferire: la classificazione di stile di disconcordia indipendente del tema, una attività che determina se due dichiarazioni di dibattito da persone diverse sono d'accordo o non, indipendentemente dal tema, chiamata dibattito qui, e la classificazione binaria di classi di espansione e di confronto di PDTB, poiché queste due sono strettamente correlate alla concezione della consonanza e della dissonanza e chiamate CE qui. Troviamo che trasferendo il performance zero-shot è già molto migliore del casual con l'AUC .62. Successivamente, eseguiamo il training iterativo su entrambe le attività e troviamo che il training iterativo su CE seguito da ulteriore training su dibattito porta una performance zero-shot molto migliore. Pertanto, questo è il modello che utilizziamo per iniziare il cold start del active learning. Successivamente, determiniamo il better metodo per aggiornare un modello con nuovi dati da ogni round di annotazione e active learning. "Cumulative" accumula tutti i dati raccolti da annotazione attiva fino adora, mentre "Iterative" aggiorna il modello trainandolo sul set più recente dei dati raccolti. Confrontando i diversi strategie, troviamo che Cumulative performe egualmente o meglio rispetto a Iterative in tutta la row. Per migliorare il numero di esempi di dissonanza, utilizziamo una strategia Probability-of-Rare-Class - PRC - per selezionare principalmente gli esempi che sono altamente probabili di essere discenduti dal modello attuale in qualsiasi round di raro. Confrontiamo questa con altre state-of-the-art AL strategie comunemente utilizzate nella comunità. Troviamo che la strategia PRC proposta funziona meglio rispetto alle altre strategie state-of-the-art, anche se la differenza è piccola. Il performance è significativamente inferiore rispetto al casual. Durante molti round di AL con due strategie migliori, ottimizziamo l'AUC del modello a 0,75, che è il miglior performance che abbiamo ottenuto sulla attività finora. Inoltre controlliamo la feasibilità di ogni strategia per la qualità e i costi degli annotatori. Troviamo che PRC ha il massimo percentuale di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trogloditi anche questi esempi difficili. In conclusione, scopriamo che PRC è una semplice strategia AL per la raccolta di classi rari e inizializzare AL con un design di trasferimento di task a distanza appropriato e aiuta significativamente. Inoltre scopriamo che l'aggiornamento iterativo è utile per il trasferimento di task a distanza, mentre le annotazioni attive a distanza sono più efficienti con l'aggiornamento cumulativo. Queste sono le link del nostro set di dati principale e del nostro paper. Se hai domande, non esitare a farmele sapere. Grazie.</sample>
    <sample id="356">I'm sorry, but I don't have enough information to determine the affiliations of the authors based on their names and a brief introduction.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="359">L'architettura simulST dedicata confrontata è l'architettura specificamente tailoreata per la traduzione simultanea.</sample>
    <sample id="361">Armineh Nourbakhsh, studente di dottorato all'Institute of Language Technologies dell'Università Carnegie Mellon e direttore di ricerca del team di ricerca AI di JP Morgan, presenta "CounterComp", un progetto che si concentra sull'utilizzo dei casi counterfatti per migliorare la generalizzazione compostamentare per il ragionamento quantitativo a più passi. Il ragionamento compostamentare è una complessità quando l'output include più operazioni aritmetiche. Le state-of-the-art delle neural networks non performano molto bene in queste attività, specialmente quando l'output ha più di due passi, perché memorizzano pattern spurious. Per risolvere questo problema, Nourbakhsh e i suoi colleghi utilizzano il contrappunto tra le domande per creare esempi positivi e negativi, utilizzando questi esempi per addestrare un modello di perdita di impegno metrico. I risultati mostrano che questa approccia consente ai modelli di concentrarsi su token più significativi durante l'addestramento, migliorando la generalizzazione compostamentare.</sample>
  </task>
</testset>