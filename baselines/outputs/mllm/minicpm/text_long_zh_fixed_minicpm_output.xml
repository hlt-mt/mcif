<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是大规模的网络爬取数据。</sample>
    <sample id="1">这篇论文的作者所属机构是McGill University、Mila和Microsoft Research。</sample>
    <sample id="2">The presenter introduces a paper on document understanding from Ant Group, focusing on Visually-rich Document Understanding (VrDU). The team aims to understand various types of documents like forms, receipts, and posters. They address reading order issues in existing models by proposing LayoutMask, which uses text and layout information as input for pre-training. LayoutMask differs from previous studies through its choice of 1D position, masking strategy, and pre-training objectives. It employs local 1D positions instead of global 1D positions, Whole Word Masking, Layout-Aware Masking, and Masked Position Modeling tasks to enhance text-layout interactions during training. Experimental results show that Local-1D outperforms Global-1D on FUNSD and SROIE datasets but underperforms slightly on CORD due to challenges with entity "Total".</sample>
    <sample id="3">文本简化是将文本适应以改善特定目标群体的文本理解，例如阅读障碍者或非母语人士的过程。为了训练文本简化模型，我们需要平行文本对，例如文档或句子。您现在看到的是一个复杂德语句子和其翻译成简单语言的平行句子对。为了简化句子，可以使用不同的技术，如词汇替换、从句删除、重排或插入单词。我们现在提出了我们的新数据集DEPLAIN，因为它在最近几年中存在一些现有数据集的问题。例如，这些数据集太小，无法在文本简化模型上进行训练。最近提出的三个模型都是自动对齐的，这意味着它们的对齐可能会有错误。因此，我们提出DEPLAIN数据集，分为两个子数据集：DEPLAIN-apa基于新闻文章。在DEPLAIN-apa中，我们手动对齐了483篇文档，结果大约有13,000个平行句子对。DEPLAIN-web包括不同的领域，我们还对这750篇文档进行了手动和自动对齐。总共有30,450个句子对。我们分析了我们的句子对，例如类型的简化。正如您所见，圣经文本比新闻文本或语言学习文本更加强烈地简化。在所有级别上，例如词汇简化、结构简化以及整体简化水平，您都可以看到我们的DEPLAIN数据集具有很高的多样性。此外，您还可以看到在DEPLAIN-apa数据集中，我们有更多的重排序和单词添加，而在DEPLAIN-web数据集中，我们有更多的重述。让我们看看我们如何使用这个数据集。你好，我是Omar，现在我将谈论DEPLAIN数据集的应用场景。第一个应用场景是评估自动对齐方法。在最近几年中，在机器翻译的上下文中，有两个平行文档，用不同语言编写，并且我们想要提取两个文档中的句子对齐。但是，在我们的用例中，我们尝试提取两个具有相同语言和相同内容但复杂度不同的平行文档之间的句子对齐。现在，由于我们有DEPLAIN数据集，其中包含手动对齐的句子，我们可以使用这些句子作为黄金标准对齐来评估提出的对齐方法。我们对这些方法进行了修改，并在论文中发布了所有这些修改和运行我们实验的代码。最终，我们得出结论，MASSalign方法是用于德语文本简化的最佳自动对齐方法。您也可以在论文中找到运行此方法的代码。第二个应用场景是我们展示了在我们的论文中。我们通过微调两种不同的模型来自动通过将复杂输入文本简化为简化文本来实现文本简化。我们微调了长mBART模型以生成文档级简化，并微调了基本mBART模型以生成句子级简化。您也可以在论文中找到所有检查点，并可以在更多细节、分数和评估指标方面查看我们的实验结果。我们得出结论，这种基本微调可以产生或获得比基准分数更好的分数，并且我们建议这些结果作为未来自动文本简化的基线。感谢您的关注，希望我们在会议上见到大家。谢谢。</sample>
    <sample id="4">演讲者的名字是Kayo Yin。</sample>
    <sample id="5">他们使用 T5 XL 模型获得 82%-87% 的准确率。</sample>
    <sample id="6">Jiaan介绍了一项名为“Towards Unifying Multi-Lingual and Cross-Lingual Summarization”的工作，该工作由Fandong、Duo、Yunlong、Zhixu、Jianfeng和Jie共同完成。他们将之前多语言摘要和跨语言摘要统一到一个更通用的设置中，称为多对多摘要。多对多摘要的目标是构建一个单一的摘要模型，可以处理任何源语言的文档并生成相应的目标语言摘要。为了比较多语言摘要、跨语言摘要和多对多摘要，他们使用了WikiLingua数据集进行了初步实验，包括英语、法语、印地语、中文、泰语和土耳其语。结果显示，多对多摘要训练的模型在不同语言之间更好地转移任务知识，优于其他设置。此外，他们提出了一个名为PISCES的预训练多对多摘要模型，通过三个阶段进行预训练：元预训练、跨语言预训练和任务特定预训练。实验结果表明，PISCES在各种基线中表现更好，包括mBART-50和mT5。他们还进行了消融研究来验证每个训练阶段的有效性，并进行了人类研究以展示PISCES的优势。</sample>
    <sample id="7">是的，根据研究结果，CoNLL-2003标注器在2023年仍然有效。</sample>
    <sample id="8">提出的人工评估方法的新颖之处在于，它试图通过明确标注每个聊天模型响应是否表达特定行为来减少人工评估的主观性。这种方法被称为聊天行为标注或ABC-Eval。ABC-Eval旨在全面覆盖最近文献中建议影响聊天质量的各种主题错误。与现有方法相比，ABC-Eval的行为标签在多个方面显示出更高的可靠性和有效性：它们的互评者一致性更高，对整体对话质量的预测能力更强，并且能够捕捉到独特的聊天质量方面。这些可靠的、信息丰富的和独特的ABC-Eval指标使我们能够以比以前方法更高的分辨率来评估对话式人工智能。</sample>
    <sample id="9">现有弱监督方法的成功在很大程度上依赖于干净的验证数据。</sample>
    <sample id="10">如果语言模型有与注释者相同的完整背景知识，那么准确率会很高，大约在92到95%之间。如果语言模型有部分重叠的背景知识，那么准确率在82到87%之间，这更符合实际情况。如果语言模型只能访问实体名称，则准确率为60%，因此有很大的改进空间。</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presented "Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest." This joint work with collaborators explores the capabilities of large language models in understanding humor. He highlighted that while some models can generate jokes and explain them to varying degrees of success, they often fall short compared to human understanding. Using data from The New Yorker Caption Contest, he operationalized it into three tasks: matching captions, quality ranking, and explanation generation. His best model achieved around 62% accuracy on the matching task, significantly lower than humans' performance (94%). Even when conditioning GPT-4 with additional annotations, its five-shot performance still lagged behind human explanations by more than two-thirds in blind A/B studies.</sample>
    <sample id="12">这篇论文有五位作者。</sample>
    <sample id="13">Adaptive inference is a method for reducing the inference time of large language models. To use it, we rely on the fact that real-world data varies in complexity. Therefore we can use low-capacity models for easy samples and therefore in that way, reduce the average inference costs, whether it be time or money. The two most common adaptive inference methods are Multi Model and Early Exit. In Multi Model, multiple models are stored together, each fit with a classifier at the end. They are trained separately on the entire training set, and when used for inference, they are run sequentially until a classifier decides to halt the computation. For Early Exit, multiple classifiers are fit to the model following intermediate transformer layers. They are all trained together, and for inference, a sample is run through the model until a classifier decides to halt, that way saving the computation, which would have been exhausted by the rest of the model. So let us look at the pros and cons of each method. For Multi Model, it is more versatile. It is easily extended. It is expensive to store. And it also suffers from overhead because using the last model for inference means we ran a sample through all previous models without using their output. For Early Exit, we have faster inference, that is, no overhead. It is memory efficient. But model parameters are shared amongst all classifiers, which we will soon see can lead to lower performance. We hypothesize that the last point I emphasized leads to a phenomenon we call conflicting gradients. Each classifier updates model weights, trying to optimize its own goal. Gradient signals from different classifiers may interfere with each other, degrading performance for all classifiers involved. Let us look at this illustration. Classifier M and the update from its loss function, represented here with the color blue, backpropagates through the entire model and updates the first layer. So does the update from Loss function 1 and Loss function 2. All of these updates do different things to the weights of Layer 1, potentially harming the performance of all classifiers in the model. In order to test our hypothesis, we compared individual Early Exit models’ classifiers with separate Multi Model classifiers, which are truncated versions of the BERT pre-trained language model. Keep in mind that training the Multi Model separate classifiers does not suffer from conflicting gradients. As you can see, the Multi Model classifiers outperformed those of Early Exit by an average of 2.3%, both for BERT-base and for BERT-large. Moreover, the gap is largest, for the earliest classifiers, 5.2% on average. We also measured the speed/accuracy trade-off of the models. Notice that for high inference speeds, Multi Model is much better. However, when we use later classifiers to predict, we see that Early Exit outperforms Multi Model because of the overhead suffered by Multi Model predicting with its largest classifiers. Based on these findings, we present SWEET: Separating Weights in Early Exit Transformers. It is a novel fine-tuning method for Early Exit architectures. What we do is we train an Early Exit architecture where each layer receives updates only from the following classifier. That means we avoid the conflicting gradient problem completely. Let's look at this illustration. On the left-hand side, you see the standard Early Exit model. On the right-hand side is our method, SWEET. You can see that each transformer layer only receives updates from its following classifier, avoiding the conflicting gradient problem. Let us look at the results of the SWEET method. When evaluating the individual layers, we can see that SWEET closes most of the gap between Early Exit and Multi Model. However, in some cases, later classifiers are negatively affected by our method, as you can see here by the negative gap between SWEET and Early Exit. We also ran the test and examined the speed/accuracy trade-off. We can see here that in fast speeds, SWEET outperforms both methods, whereas for BERT-Large, it outperforms both methods throughout the entire speed/accuracy curve. The takeaways from our work are as follows. We show the existence of conflicting gradients in Early Exit training process. To the best of our knowledge, we conduct the first fair comparison of Early Exit and Multi Model adaptive inference methods. We also introduced the SWEET method, the results of which motivate future research and fine-tuning algorithms tailored to the Early Exit architecture. Thank you very much for listening. If you enjoyed the talk and you want to see more, go visit our paper on Archive, "Finding the SWEET spot".</sample>
    <sample id="14">好的，我明白了。你提到的关于协调结构的依赖关系，特别是关于左连词短语倾向于成为主语的现象，确实是一个有趣的研究方向。通过比较不同结构中的依赖关系长度，我们可以更好地理解语言中的协调机制。你的研究结果表明，当连词位于左侧时，短语倾向于更短，而当连词位于右侧或不存在时，这种倾向性消失。这为我们提供了一个有力的证据，支持对称结构优于非对称结构的观点。你的工作不仅丰富了我们对语言结构的理解，也为自然语言处理提供了新的思路。继续保持这样的深入研究，相信你会有更多的发现。</sample>
    <sample id="15">这篇论文有三位作者。</sample>
    <sample id="16">Bible texts are much more simplified than news text or language learner texts.</sample>
    <sample id="17">The speaker introduces their work on multimodal relation extraction, a task that involves determining the semantic relationship between entities in text. They highlight two main problems: over-utilization of internal information and under-exploitation of external information. To address these issues, they propose a Graph Information Bottleneck principle-guided feature refinement method. This includes merging visual scene graphs with textual scene graphs into one unified cross-modal graph (CMG), screening CMG structures by filtering nodes and adjusting edges, enriching compressed CMG features with multimodal topic features, and integrating attention operations to enhance overall context. The proposed methods outperform existing multimodal baselines in experiments conducted on the MRE dataset.</sample>
    <sample id="18">一个偏好较短左并列词的例子是“盐和胡椒”。</sample>
    <sample id="19">Zhang Qin, a master's student from Shenzhen University, presented their work "A Survey for Efficient Open Domain Question Answering" at ACL 2023. The presentation focused on the challenges of open-domain question answering and introduced techniques to achieve efficient systems with smaller memory costs, faster inference speed, and comparable performance. The core techniques included one-stage frameworks like retrieval-only and generator-only systems, fast evidence research methods such as approximate nearest neighbor search, skip reading strategies, index size reduction through document filtering and embedding dimension completion, model size reduction by selecting lightweight models or parameter sharing, and knowledge distillation. The analysis showed that retrieval and reader systems perform well-balanced among speed, memory, and performance, while retrieval-only systems infer answers quickly but create large indexes, and generator-only systems are always large models achieving low performance. Zhang Qin concluded that if limited by resources, reducing index size or model size should be considered; real-time feedback can benefit from retrieval-only systems, and trade-offs between speed, memory, and performance can be achieved using retrieval and reader systems. Future works include deploying these systems in low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">是的，这些模型是免费的，并且可以在Hugging Face上获得。</sample>
    <sample id="21">DEPLAIN-apa 包含新闻文章。</sample>
    <sample id="22">通过我们的实验，我们发现有三个主要因素有助于良好的泛化。首先，模型架构很重要。通常，transformer模型在新数据上表现更好。其次，模型大小也很重要。我们发现，通常更大的模型会导致更好的泛化。最后，我们都知道，微调的示例数量直接影响下游任务的表现。在这里，我们也发现，更多的微调示例也导致了更好的泛化。</sample>
    <sample id="23">Dan Garrette在论文中讨论了文本图像模型在渲染文本时遇到的挑战。他解释说，文本图像模型已经取得了显著进展，能够生成高质量、有趣的图像，但它们在表示文本方面存在困难。Garrette专注于Imagen模型，该模型通过将输入文本编码为T5-XXL编码器，并使用该编码文本作为扩散模型的输入来生成图像。虽然Imagen能够生成复杂的图像，但它经常在简单的文本输入上失败，这些输入需要图像包含一个单词。

为了理解这种情况，Garrette分析了文本编码器本身。T5使用SentencePiece分词，这意味着模型接收到的是输入字符串的子词ID块，而不是单独的字母。这使得模型在被要求绘制单词时需要分解原子子词词汇项以组成单个字母。Garrette进行了实验来了解这些文本编码器在拼写方面的知识。结果表明，T5在较小规模下表现不佳，准确性低于20%。然而，较大的版本表现更好，但即使是最大的T5模型也仅达到约70%的准确性。

Garrette还调查了PaLM模型，这些模型在拼写方面做得更好，较大的PaLM模型几乎完美地拼写。然而，这些模型由于参数数量和训练数据量较大而不太实用。另一方面，ByT5模型接收输入字符串的单个字节，比字符级信息更小的粒度。ByT5在所有规模下都表现出色，因为它只需要学习如何从输入复制字符到输出。

为了更好地理解这一现象并提高文本渲染模型，Garrette和他的团队通过将ByT5小型模型附加到现有的文本编码器来增强Imagen模型。他们发现，即使添加少量参数，ByT5也能使模型在拼写方面表现出色，并改善其图像生成特性。然而，尽管文本编码器本身知道拼写，但扩散模型可能会引入错误，因此即使文本编码器正确拼写，生成的图像也可能包含错误。</sample>
    <sample id="24">衡量左并列词是否更短，可以通过比较两个并列词的长度来实现。具体来说，可以测量并列词之间的字符数、音节数或单词数，并将这些数值进行比较。在研究中，使用了单词数作为衡量标准，发现当并列结构中的主语位于左侧时，左侧并列词倾向于更短。这种倾向随着并列词长度差异的增加而增强。然而，当并列结构中的主语位于右侧时，这种倾向会消失。因此，通过比较并列词的长度，可以确定左并列词是否更短。</sample>
    <sample id="25">为了研究支配词位置的影响，可以设计一个实验来比较支配词在不同位置（左、右或无）时的协调结构。实验可以通过收集和分析大量带有标注的语料库数据来实现，这些数据包括支配词的位置以及两个协调成分的长度。通过计算支配词在不同位置时协调结构的统计特征，如长度、复杂度和频率等，可以观察到支配词位置对协调结构的影响。此外，还可以使用自然语言处理工具和技术来自动化地提取和分析这些特征，从而更高效地进行实验。</sample>
    <sample id="26">基线分类器在不平衡数据上的训练效果不佳，因为它仅在43个例子上进行训练，性能远低于随机猜测。</sample>
    <sample id="27">这篇论文有两位作者。</sample>
    <sample id="28">对话中的角色名字是Bob和Alice。</sample>
    <sample id="29">在形式和连贯性方面，语境感知的MT模型比语境无关的模型表现更好。</sample>
    <sample id="30">LLM-Blender是一种简单而有效的大型语言模型的组合学习框架，基于对对和生成融合的关键思想。它由两个模块组成：PairRanker是一个对对比较模块，用于比较所有候选者，GenFuser将前三个候选者作为输入传递给序列到序列模型进行学习和推断。该框架通过使用多个大型语言模型来提高性能，并且在MixInstruct数据集上进行了评估，结果表明其优于单个模型。</sample>
    <sample id="31">这篇论文的作者所属机构是：John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, Adina Williams</sample>
    <sample id="33">框架通过将数据集和模型的注释与真实用户进行比较来量化立场。它使用Pearson's R相关性分数来衡量数据集和模型注释与真实用户注释之间的相似程度。该框架通过重新注释数据集以获得多元化的注释者，并比较这些注释与数据集和模型，从而实现这一目标。</sample>
    <sample id="34">The work presented by Marcos Treviso is called "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation". It involves a collaboration with Alexis Ross, Nuno Guerreiro, and André Martins. The framework aims to combine selective rationalization methods that highlight tokens in a faithful way with counterfactual generation methods aligned more closely with human causal reasoning. This combination leads to the creation of CREST, which has two main components: one responsible for generating counterfactuals and another for producing rationales through a trainable masker component.

To evaluate the quality of the counterfactuals produced by CREST, various metrics were employed, including automatic evaluation and human judgment using a 5-point Likert scale. Results showed that humans found manual counterfactuals to be superior in terms of validity and naturalness compared to those generated by other approaches like MiCE. However, among automated methods, both CREST and MiCE outperformed each other when judged on their respective merits.

The study also explored the use of these counterfactuals for data augmentation purposes. An alternative approach was proposed where factual examples are paired with corresponding counterfactual ones during training. By doing so, it encourages new rationales to focus on specific parts of the input related to both factual and counterfactual reasoning.

Experimental results demonstrated significant improvements across different datasets (IMDB, SNLI) after incorporating CREST into the model's training process. These findings suggest that utilizing CREST-generated counterfactuals can lead to enhanced performance in downstream tasks due to plausible explanations focusing on contrasting aspects within inputs.

In summary, this research proposes an innovative joint framework named CREST that effectively combines rationalization and counterfactual text generation techniques. Its application not only produces high-quality counterfactuals but also enhances interpretability through improved plausibility scores and higher counterfactual simulability metrics relative to existing alternatives.</sample>
    <sample id="36">这段内容介绍了ACL上的一篇名为“Learning Language-Specific Layers for Multilingual Machine Translation”的论文。论文的主要贡献是提出了一种名为Language-Specific Layers（LSLs）的方法，通过在模型中引入语言特定的层来提高多语言机器翻译的性能。具体来说，LSLs是在每个编码器层中引入一个共享权重和两个语言特定权重，然后通过训练模型来学习最佳的LSL放置位置。实验结果表明，LSLs在低资源语言上的性能显著提升，并且在推理时具有更快的速度。</sample>
    <sample id="37">在之前的研究中，当人类受试者被给予相同的人格化提示时，他们能够揭示出种族刻板印象。</sample>
    <sample id="38">为了进行这项研究，作者使用了增强版的Penn Treebank数据。Penn Treebank是一个广泛使用的英语语料库，用于语言学和自然语言处理任务。通过分析这个语料库，研究人员能够提取出各种关于协调结构的统计信息，并验证他们的观察结果。具体来说，他们计算了短语的长度（以单词、音节或字符为单位），并观察到在某些情况下，短语倾向于成为协调结构中的第一个。这些发现支持了对协调结构中对称性的论点，与传统的左偏结构形成对比。</sample>
    <sample id="39">这篇论文的作者是Adam Przepiórkowski。</sample>
    <sample id="40">与认知失调密切相关的任务包括：1. 论点独立的分歧立场分类，即确定两个不同人发表的辩论陈述是否在不同意或同意，无论主题如何。2. PDTB中的扩展和比较类别的二元分类，因为这些类别与认知失调和和谐的概念密切相关。</sample>
    <sample id="41">PeaCoK是一种基于常识的知识图谱，用于构建一致和引人入胜的叙述。它包含大约3800个人物和40,000个独特的属性，形成大约10万种个人推断或事实。该图谱通过人类互动行为的研究来框定人物及其属性之间的关系，并使用一种联合人类-AI多数投票方案进行注释。PeaCoK被证明可以提高语言模型的常识生成能力，并且在增强下游叙事建模方面也具有潜力。</sample>
    <sample id="42">这篇论文有三位作者。</sample>
    <sample id="43">这篇论文有三位作者。</sample>
    <sample id="44">框架与以前的研究不同，因为它比较了最终用户与模型和数据集的预测和标签。</sample>
    <sample id="45">在三个比较设置中，黑人女性与白人女性和男性之间的刻板词汇重叠最多。</sample>
    <sample id="46">DeepL和Google Translate</sample>
    <sample id="47">好的，我明白了。您想让我将英文内容翻译成中文。请提供您想要翻译的文本。</sample>
    <sample id="48">这篇论文有三位作者。</sample>
    <sample id="49">MPP 评估最多涵盖1024个词元的上下文长度。</sample>
    <sample id="50">The speaker introduces DEPLAIN, a new corpus for German text identification on the document level and sentence level. The presentation defines text simplification as adapting text to improve comprehension for specific target groups like people with reading problems or non-native speakers. They highlight the need for parallel pairs of texts in training models.

DEPLAIN is proposed due to issues with existing corpora: they are too small, automatically aligned which can be error-prone, and lack variety in simplification techniques. Two subcorpora were created: DEPLAIN-apa based on news texts (483 documents) resulting in 13,000 paired sentences; DEPLAIN-web covering various domains (750 documents), yielding 30,450 pairs through manual alignment and automatic methods.

Analysis shows varying levels of simplification across different types of texts within both subcorpora. For instance, Bible texts show stronger simplification compared to news or language learner texts at all complexity levels. Simplification transformations differ between subcorpora—more reordering and word addition in APA versus more rephrasing in web content.

The use cases include evaluating automatic alignment methods using DEPLAIN's manually aligned sentences as gold standards. The best method found was MASSalign after adaptations by the researchers. Additionally, fine-tuning language models for automatic text simplification resulted in improved scores over baseline metrics when comparing long-mBART for document-level simplification against base mBART for sentence-level simplification.

The paper provides detailed experiments results, codes, and checkpoints for further exploration into these applications.</sample>
    <sample id="51">他们的数据集中包含音乐、书籍和食谱三个领域。</sample>
    <sample id="52">Positionality（立场）是指人们持有的观点，这些观点受到他们的种族、性别、社会经济地位、文化背景和其他身份特征的影响。</sample>
    <sample id="53">演讲者的名字是Dawei。</sample>
    <sample id="54">Vasudha, a PhD candidate at Stony Brook University, presented their work titled "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" accepted into ACL 2023 as a long paper. The presentation focused on cognitive dissonance and its importance in language studies. Cognitive dissonance occurs when two beliefs or actions are inconsistent, such as someone stating they know cigarettes could kill them but then smoking after a meeting. This phenomenon is rare to find expressed in language among other discourse relations.

The study aims to understand the effects of disagreement among people, track trends and belief values, and analyze attitude changes in populations. High cognitive dissonance has been linked to anxiety disorders and can help better understand mental health. Studying dissonance expressed in language also aids in understanding extremism and polarization of vulnerable groups. Furthermore, it helps comprehend personal cognitive styles and decision-making processes.

To create a cognitive dissonance resource, Vasudha conducted large-scale annotation of dissonance relations using an approach called 'dissonance-first.' Tweets were passed through the PDTB parser, and pairs of discourse units were annotated according to specific guidelines described in the paper. They found that only 3.5% of the annotated pairs exhibited dissonance.

Vasudha used transfer learning and active learning techniques to annotate more examples with less effort while improving dissonance detection performance. They transferred weights from closely related tasks like topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE). These transfers resulted in improved zero-shot performances compared to chance.

The proposed PRC strategy was effective in selecting mostly high-probability rare-class data points during each round of active learning. It outperformed other state-of-the-art AL strategies by a small margin over random selection. After multiple rounds of AL with different strategies, the AUC score reached 0.75, which was the best result achieved so far on this task.

In summary, Vasudha's research demonstrates how PRC, combined with appropriate transfer learning and update methods, significantly improves cold-start active learning for rare class acquisition. Their findings provide insights into annotator feasibility and costs associated with these approaches.</sample>
    <sample id="55">EDAtt 是一种策略，它利用了现有的离线 ST 模型，而无需重新训练或采用特定的架构。它使用一个模型来处理每个延迟范围，并通过特定参数来处理延迟。EDAtt 还利用了模型中已经获得的知识，特别是注意力机制，以决定是否发出部分翻译。因此，EDAtt 适应了现有的离线 ST 模型。</sample>
    <sample id="56">这篇论文有三位作者。</sample>
    <sample id="57">No, the models cannot run on this test suite.</sample>
    <sample id="58">KITMUS有三个变体：背景-预训练，背景-两者和背景-推理。</sample>
    <sample id="59">DrBERT是一种基于RoBERTa的模型，用于法语的医疗和临床领域。它是在NACHOS数据集上训练的，这是一个从网络上爬取的医疗数据集。研究人员还比较了使用不同预训练设置和数据源的多个模型。他们发现，从头开始训练的模型在大多数下游任务中表现更好。DrBERT 4 GB从头开始的模型在11个下游任务中的9个任务中表现最好，并且超过了通用模型CamemBERT的结果。所有预训练模型都可在Hugging Face上免费获得，并遵循MIT许可证。</sample>
    <sample id="60">这篇论文的作者所属机构是MIT CSAIL。</sample>
    <sample id="61">最后一个问题是如何利用干净的样本。</sample>
    <sample id="62">The paper discusses the challenge of compressing large natural language generation (NLG) models while preserving their performance. It explores task-specific knowledge distillation for NLG, considering five criteria: medium-resource labeled data sets, large amounts of unlabeled data, high compression rate, negligible one-time training resources, and realistic setups with a ratio of 1 to 4 labeled to unlabeled examples. The study investigates four NLG tasks—summarization, question generation, common sense reasoning, simplification, and style transfer—using eight stages and an extreme setup. Key contributions include exploring the impact of pruning on task performance, comparing different approaches for knowledge selection, challenging traditional sequence-level knowledge distillation by generating multiple pseudo-targets, proposing joint-teaching as a novel technique to address student exposure bias and grounded learning, and demonstrating that sampling pseudo-targets improves the student's performance.</sample>
    <sample id="63">指标灵敏度衡量模型在任务中的一致性，即它是否能够始终产生相同的输出，即使指令略有变化。</sample>
    <sample id="64">演讲者的名字是Jingwei Yi。</sample>
    <sample id="65">更高的灵敏度表示模型性能得到了提高。</sample>
    <sample id="66">数学推理是人类智能的一个基本方面，使我们能够理解并基于数字数据和语言做出决策。机器能够解决数学问题和证明定理的开发一直是人工智能和自然语言处理的长期重点。近年来，这一领域出现了显著的兴趣，因此我们的调查讨论了数学推理的任务以及深度学习方法的发展。例如，数学文字问题中的文本可能涉及单步或多步算术运算。数学推理不限于基于文本的数据；它还可以扩展到包括图像、图表和表格的多模态信息。我们可以研究两个主要类别：视觉上下文和表格式上下文。解决几何问题是一个重要的高中教育科目。例如，在这里给出的问题文本和相应的图示，我们需要识别几何关系，应用定理知识并进行计算以获得数值答案。这些任务可以被形式化为一个神经符号推理问题，涉及几何图形、定理和求解器。另一个重要的数学推理线是自动定理证明。一个定理证明者旨在通过一系列更大的论据来证明数学断言的真理。对于人类来说，写出定理证明并不容易，但自动证明帮助我们。一些数据集已经被提出用于探测语言模型的人类智能，例如数字常识知识和高级问题解决。在过去的几年中，已经提出了许多神经网络架构来解决数学推理任务。例如，序列到序列模型使用编码器编码架构，并通常将数学推理任务形式化为序列生成任务。基本思想是将输入序列，如数学问题，映射到输出序列，如方程、问题或证明。值得注意的是，数学表达式可以表示为树形结构。因此，序列到树模型已被提出，以明确地建模树结构的同时编码方程表达式。在过去的几年中，我们见证了大型语言模型的惊人发展，例如大型语言模型。这些语言模型已经在广泛的NLP任务上展示了出色的表现。我们也可以将LLMs应用于解决数学文字问题。例如，给定输入问题，我们可以提示LLM使用单个例子的链式思考过程。链式思考是一系列中间推理步骤，最终得出答案。尽管存在优势，LLMs仍然面临固有的局限性。一个值得注意的例子是它们无法进行精确的数学推理。一种有效的方法来提升LLMs的性能是替换贪婪解码策略。而不是生成单个推理路径，从语言模型的解码器中采样一组推理路径，并从答案集中选择最频繁的一个。而不是设计有效的提示方法来增强LLMs，另一条有前途的工作线是设计增强的LLMs。例如，程序辅助LLMs在复杂的数学推理任务中非常有用。我们可以将LLMs与各种工具组合起来执行不同的输入查询。Chameleon，最近提出的一种方法，可以从语言模型的解码器中生成自然语言程序来组成不同工具的使用。尽管创建了各种数据集，但在低资源设置中的数学推理仍然未得到充分探索。最近，已经尝试构建中文、韩文和阿拉伯语等非英语数据集。此外，开创性的研究已经开发了金融、科学和医疗领域的数学推理基准。尽管取得了显著进展，但常见的学习模型在推理任务上显示出泛化和鲁棒性失败。首先，语言模型难以处理大数字。其次，大型语言模型在数学推理方面不一致。有了这些，感谢大家的聆听。</sample>
    <sample id="67">Interference in multilingual translation models can occur when the model is small compared to the data size, and tuning the sampling temperature is key for strong performance. The simpler bilingual case has scaling laws that predict loss, but the multilingual case is more complicated due to additional factors like language similarity and number of languages. However, these do not have a large impact on interference levels. Severe interference occurs only with parameter poverty settings. Temperature sampling is an effective solution; however, it should be tuned based on specific conditions rather than using uncalibrated values.</sample>
    <sample id="68">在预训练期间，模型会接收较长的上下文。</sample>
    <sample id="69">通常需要20个样本/类别才能获得良好的表现。</sample>
    <sample id="70">这篇论文的作者所属机构是斯坦福大学。</sample>
    <sample id="71">Javad Hosseini介绍了他们的工作“通过解决间接指代表达来解决实体选择问题”，并介绍了AltEntities语料库。他们的目标是理解用户在做选择时的语言，例如在音乐、书籍和食谱等不同领域中。他们使用crowd annotation收集数据集，并强调了对话的非正式性。数据集包含6000个替代问题和42000个间接指代表达。他们展示了T5 XL模型的结果，表明在访问与注释者相似的背景知识时，准确率很高（约92-95%）。然而，当语言模型仅访问实体名称时，准确率仅为60%，这表明有改进的空间。他们还展示了模型在不同领域的泛化能力。</sample>
    <sample id="72">为了衡量媒体偏见，需要开发新的方法来评估语言模型的偏见。这是因为语言模型是在大量网络爬取数据上进行训练的，其中包括政治新闻媒体，这可能导致语言模型在下游任务中出现公平性问题。因此，需要研究政治偏见传播的管道，包括从预训练数据到语言模型再到下游任务，并通过自动评估和实验来了解语言模型的政治倾向以及这些倾向对下游任务性能的影响。</sample>
    <sample id="73">演讲者的名字是Akshatha。</sample>
    <sample id="74">这篇论文介绍了他们构建的Dense-ATOMIC，这是一个基于ATOMIC的大规模常识知识库。ATOMIC包含很少的多跳路径，因为尾部事件无法成为头部事件的三元组。为了弥补缺失的B到A、B到B、A到B和A到A链接，他们提出了Dense-ATOMIC。通过比较它们，可以发现DenseATOMIC在ATOMIC中完成了很多缺失的链接，包括B到A、B到B、A到B和A到A链接。此外，DenseATOMIC还包含多跳路径，例如两个跳路径：“X原谅了Y”，然后“Y说Yes”，最后“X微笑”。他们的过程包括三个部分：规范化尾事件、训练关系预测模型和构建DenseATOMIC。他们还提出了一种新的CSKG完成方法Rel-CSKGC，用于推断给定头事件和尾事件的三元组的关系。他们使用RoBERTa对头事件和尾事件进行编码，并将头事件和尾事件的表示连接起来进行链接预测。他们还设计了一个内聚和跨簇完成策略来处理缺失链接。通过与传统方法和翻译基方法的比较，他们展示了Rel-CSKGC在自动和人工评估中的优越性。他们还对构建的DenseATOMIC进行了评估，展示了其更高的知识覆盖范围和多跳路径。</sample>
    <sample id="75">The speaker introduces their work, Jointprop, which is a joint effort with Hao Anran and supervised by Luu Anh Tuan. The motivation behind the work lies in addressing the challenges of fully-supervised learning schemes for Named Entity Recognition (NER) and Relation Extraction (RE). These tasks are crucial in information extraction but require extensive labor to annotate data accurately. Semi-supervised learning offers an alternative approach that leverages both labeled and unlabeled data at a lower cost.

The current state-of-the-art models have shown significant performance improvements through semi-supervised methods; however, they often overlook the underlying connections between NER and RE tasks. For instance, syntactical similarities such as "used to" and "use in" can be missed if not considered during model training. This oversight could lead to misaligned labels or ineffective use of available data.

To address these issues, the proposed framework, Jointprop, aims to integrate all relevant information effectively. It incorporates four main components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. 

In span feature generation, contextualized representations of input tokens serve as initial spans and pairs' representations. A trained classifier helps generate pseudo-labels from unlabelled data. Heterogeneous graphs facilitate efficient computation while considering similarity relations among labelled and unlabelled data points. Label propagation then spreads these labels across the graph, refining them iteratively until convergence.

Model optimization involves using the refined pseudo-labels along with high-confidence ones to retrain the classification model. This process enhances overall task performance significantly when applied to joint datasets compared to single-task scenarios where it consistently outperforms baseline models on both NER and relation tasks.

Overall, this presentation highlights how integrating multiple aspects within a unified framework can improve the effectiveness of semi-supervised approaches in handling complex multi-task problems like NER and RE.</sample>
    <sample id="76">政治偏见传播流程从预训练数据到语言模型再到下游任务。首先，我们通过使用政治问卷对语言模型进行提示，以自动评估其政治倾向。然后，我们进一步预训练语言模型检查点，使用6个不同的党派语料库，这些语料库被分为新闻和社交媒体，并进一步分为其政治倾向。通过进一步预训练语言模型，我们可以看到其意识形态坐标也相应地发生了变化。例如，对于在左倾的Reddit语料库上进一步预训练的RoBERTa，我们观察到了显著的自由主义偏移。最后，我们评估具有不同政治倾向的语言模型在仇恨言论检测和假新闻检测等NLP应用中的性能，以了解这些应用中是否存在公平问题。</sample>
    <sample id="77">这段内容介绍了来自耶鲁大学和微软研究院的一项工作，名为“通过自然语言反馈提高摘要的实证一致性”。这项工作主要关注的是通过收集人类演示和反馈来改进摘要的实证一致性。他们提出了一种新的自然语言生成任务，并提供了每个任务的强基线模型。这些任务包括摘要编辑、反馈生成和自动事实错误纠正。研究的重点是通过原始系统生成的摘要来提高摘要的实证一致性。他们收集了XSum数据集的数据点，并展示了标注数据点的基本统计数据。他们发现，标注后的编辑指令与不同的错误类型有关系。此外，他们还展示了他们的DeFacto数据集在GitHub上的发布情况。</sample>
    <sample id="78">是的，DEPLAIN-apa 和网站的简化过程有所不同。在 DEPLAIN-apa 中，我们有更多 reorderings 和 word additions，而在 DEPLAIN-web 中，我们有更多 rephrasings。</sample>
    <sample id="79">Yes, CoScript is publicly available.</sample>
    <sample id="80">在文本中插入水印的过程包括以下步骤： 1. **选择触发集**：首先，从一般文本语料库中选择一组具有中等频率的单词。这些单词将在后续步骤中用作水印的触发器。 2. **目标嵌入定义**：确定一个目标嵌入。当用户发送句子到提供者服务时，提供者会计算句子中的触发词数量。 3. **水印注入**：根据句子中的触发词数量，将目标嵌入与原始嵌入进行加权求和。如果句子中的触发词数量大于某个阈值（m），提供的嵌入将完全等于目标嵌入。 4. **版权验证**：通过构造后门数据集和干净数据集来进行版权验证。后门数据集中包含所有单词都属于触发集的句子，而干净数据集中不包含任何触发集单词的句子。 提供者请求来自窃取者服务的数据集的嵌入。然后计算嵌入与目标嵌入之间的余弦相似度和L2相似度。通过比较干净和后门数据集之间的相似度差异（delta cosine和delta L2）以及KS检验的p值来评估嵌入是否包含水印。 这种方法确保水印不会降低嵌入的实用性，并且足够隐蔽，以便窃取者可以轻松移除它。</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">The video discusses the concept of Automated Essay Scoring (AES) and its challenges in an unsupervised setting. It introduces two previous works on unsupervised AES, which use heuristic signals like unique terms or word count but face issues with clustering and regression processes. To address these limitations, a new framework called ULRA is proposed to incorporate multiple quality signals as pseudo-groundtruth for training a neural AES model. The core idea involves ranking essays using different quality signals and aggregating partial-order pairs into unified supervision through a Deep Pairwise Rank Aggregation Module. A Scoring Strategy transforms predicted scores from the neural model into a predefined range. Experimental results show that ULRA outperforms existing unsupervised methods while still lagging behind supervised approaches due to insufficient strong supervision.</sample>
    <sample id="83">是的，像 mt5 这样的编码器-解码器模型可以通过混合语言的训练来改进。</sample>
    <sample id="84">Shwai He's paper for ACL 2023, "PAD-Net: An Efficient Framework for Dynamic Networks", discusses the use of dynamic networks in machine learning. Traditional networks are static and cannot change with input, while dynamic networks can adapt to changes based on input. However, fully dynamic networks have excessive parameters, making them impractical in many situations. To address this issue, Shwai He proposes PAD-NET (Partially Dynamic Network), which partitions parameters into static and dynamic modes using Iterative Mode Partitioning. The goal is to reduce redundant dynamic parameters by transforming them into static ones if they do not significantly affect loss value. Experiments show that PAD-NET achieves better performance than both static and fully dynamic networks, maintaining fewer parameters and less computation. Ablation studies reveal optimal Dynamic Ratios for Dynamic Convolution and Mixture of Experts, as well as important Scale Factors for balancing static and dynamic parameters. Compared to network pruning methods, PAD-NET maintains static parameters more effectively, leading to improved accuracy. Future work includes extending PAD-NET to other mainstream networks, exploring hardware-friendly structured manners, and introducing additional modes such as zero elements or hybrid combinations among static, dynamic, and zero parameters.</sample>
    <sample id="85">受限语言规划的一个示例是“制作巧克力蛋糕”。</sample>
    <sample id="86">为了确保其方法的隐蔽性，他们使用了以下策略： 1. **选择触发集**：他们从一般文本语料库中选择一组在适度频率范围内出现的单词作为触发集。 2. **水印注入**：在为用户提供嵌入时，他们首先定义一个目标嵌入。当用户发送句子到提供者服务时，提供者会计算句子中的触发词数量。提供的嵌入是目标嵌入和原始嵌入的权重之和。目标嵌入的权重与句子中的触发词数量成正比。如果句子中的触发词数量大于某个阈值（m），则提供的嵌入将完全等于目标嵌入。 3. **隐蔽性验证**：为了验证隐蔽性，他们构造了一个后门数据集和一个干净数据集。后门数据集包含所有单词都属于触发集的句子，而干净数据集中的所有单词都不属于任何触发集。然后，提供者请求来自窃取者服务的数据集的嵌入。他们计算请求的嵌入与目标嵌入之间的余弦相似度和L2相似度。此外，他们还应用Kolmogorov-Smirnov（KS）检验，并使用其p值作为第三个指标。通过分析这些指标，他们可以评估嵌入的隐蔽性。 实验结果表明，他们的方法在保持下游任务的性能的同时，具有良好的检测性能，并且很难区分后门嵌入和正常嵌入。这表明他们的方法有效地保持了隐蔽性。</sample>
    <sample id="87">在研究中，我们使用了现有的 PLM 来构建新的 PLM。具体来说，我们使用了 RoBERTa 模型作为基础，并在 NACHOS 数据集上进行了预训练，以构建 DrBERT 模型。此外，我们还使用了 anonymized data from the Nantes University Hospital data warehouse 来构建 ChuBERT 模型。通过这种方式，我们利用现有的 PLM 来构建新的 PLM，从而提高了模型的性能和适用性。</sample>
    <sample id="88">GPT-4在社交接受度任务中与印度和日本的立场最不一致。</sample>
    <sample id="89">演讲者在以下示例句子上展示了模型如何利用注意力机制所学的知识：“我将谈论...”</sample>
    <sample id="90">The paper discusses the feasibility of using language learners as annotators in NLP. It questions whether native speakers are necessary for data annotation and conducts a proof-of-concept study to examine this possibility. The authors carefully designed experiments considering control variables, targeting three languages: English, Korean, and Indonesian. They chose four tasks from each common task type in the GLUE benchmark and categorized learners into basic, intermediate, and advanced levels based on their self-rated proficiency. A preliminary survey was conducted to gather information about participants' language backgrounds and learning experiences. The main experiment consisted of pre-test, annotation, and post-test sessions over six days. Participants annotated 10 questions with additional resources provided and took standardized test questions before and after completing the annotation. Results showed that labels annotated by language learners were nearly accurate, especially for simpler tasks and easy-to-medium level questions. Aggregating learner labels led to performance similar to or better than those labeled by native speakers. Additionally, learners demonstrated improvements in language proficiency, vocabulary, and grammar through comparison of scores between pre-test and post-test sessions. This work suggests that recruiting language learners could contribute significantly to NLP annotations, broadening research possibilities for low-resource languages where it is challenging to recruit native speakers.</sample>
    <sample id="91">任务的数量对模型的性能有显著影响。随着任务数量的增加，模型在每个任务上的表现会更好，并且其敏感性会降低。这表明，模型能够通过处理更多的任务来提高其性能和稳定性。</sample>
    <sample id="92">作者用来比较其方法的三个无树基线是：1. 一个基于LSTM的模型，2. 一个基于LSTM的模型，3. 一个基于LSTM的模型。</sample>
    <sample id="93">Alexander Koller和Ivan Titov是合著者，与第一作者Matthias Lindemann一起合作。</sample>
    <sample id="94">The speaker introduces a paper about protecting the copyright of embedding as services via backdoor watermark. They explain that large language models like GPT, LLAMA, and PALM are used in various NLP tasks through embedding as services, but recent works show attackers can steal these models by learning from embeddings to provide similar services. To protect the copyright, they propose Embedding marker, which is based on a backdoor method applicable to embedding as services. The method consists of two main steps: watermark injection and copyright verification. In watermark injection, a trigger set of words with moderate frequency intervals is selected, and a target embedding is defined. When a user sends a sentence, the provider counts the number of triggers in it and provides an embedding that's exactly equal to the target embedding if there are more than m triggers. Copyright verification involves constructing a back door data set and a benign data set, computing similarity metrics between them, applying KS test, and using its p-value for detection performance evaluation. Experiments on AG News, MIND, SST2, and Enron Spam datasets demonstrate that their embedding marker has good detection performance while maintaining utility for downstream tasks. Visualizations of embeddings reveal that it’s difficult to distinguish between backdoor embeddings and normal embeddings.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="96">好的，让我们开始吧。大家好，我是Jenny，是一名在卡内基梅隆大学读博士一年级的学生，今天我将向大家介绍我们团队的成果《NLPositionality：数据集和模型的设计偏见》。这项工作是由我和华盛顿大学以及艾伦人工智能研究所的一些同事Sebastian Santy、Ronan Le Bras、Katharina Reinecke和Maarten Sap共同完成的。现在，让我们想象一下，你正在为一家报纸工作，你在新闻文章下面筛选评论，试图去除有毒的内容。你可能会转向一个流行的API，比如Prospective API进行毒性检测，这在Carl Jones身上表现得很好，因为Prospective API能够正确地检测出有毒的例子。但是，对于Aditya Sharma来说，Prospective API并不是那么敏感，因为它无法检测到在印度语境中更常见的负面术语。这就是一个设计偏见的例子，我们在这里看到系统性的性能差异，技术对不同的人群有不同的表现。设计偏见，比如我们刚才提到的那个例子，可能是因为NLP研究人员和模型开发者的立场造成的。立场是指人们由于他们的人口统计学、身份和生活经历而持有的观点。这是一个广泛用于批判性研究中的概念，特别是在女性主义和女同性恋学术领域。作为一名研究人员，立场可以影响研究过程及其结果，因为它可以改变研究人员做出的决定。因此，一个问题可能是，数据集和模型是否有立场？我们并不是说模型本身或数据集本身具有人口统计学身份和生活经历，但它们确实汇集了人们的判断和意见，并且可以代表某些立场胜过其他立场。先前的工作已经提供了关于数据集和模型可能具有立场的一些轶事证据，例如文化差距和模型和数据集之间的差异，以及模型位置性的理论定义。然而，这些工作并没有比较最终用户与数据集和模型本身，也没有研究模型和数据集的位置性，因为并非所有决策都是记录在案的，许多模型都是隐藏在API背后的。为了研究数据集和模型的位置性，我们实际上将注释与现有的数据集和模型进行了比较。我们通过我们的框架NLPositionality来实现这一点。我们的框架有两个主要步骤。第一步是重新注释数据集，使用多样化的注释者。我们应该这样做，因为原始数据集注释者的人口统计学通常很少被收集和分享，因为通常只有一个注释者为每个实例注释，而且人口统计学数据很少被收集和分享。因此，我们重新注释数据集以获得多个注释者和丰富的数据。然后，我们将按人口统计学对注释进行分组，并将它们与模型和数据集进行比较，使用皮尔逊相关系数得分，因此我们的框架与注释者的一致性文献有所不同，该文献比较的是注释者之间的分歧或模型注释者分布，而不是比较最终用户与模型和数据集的预测和标签。我们的框架主要是通过Lab in the Wild和在线众包平台启用的，其中我们的合作人HCI Lab in the Wild是一个在线实验平台，我们可以招募来自世界各地的多样化志愿者。与M Turk平台相比，M Turk平台主要由美国或印度的参与者组成，而Lab in the Wild仍然能够提供高质量的数据。我们在Lab in the Wild上发布了两个任务，一个是社会接受度，参与者会阅读社交化学数据集中的情况，然后他们将写下一个情况的社会接受度。之后，为了保持参与者的兴趣，他们可以将他们的回答与AI和其他人的回答进行比较。我们然后将这些注释与Social Chemistry、Delphi和GPT 4进行了比较。我们还复制了一个非常相似的设置来进行仇恨言论和恶意内容检测任务，参与者会阅读Dynahate中的一个示例，然后写下一个示例是否是恶意内容。我们然后将这些注释与Dynahate、Perspective API、Rewire API、Hate Roberta和GPT 4进行了比较。我们的研究最终收集了超过16,000个注释，来自1000多名来自87个国家的注释者。因此，我们现在更好地了解了NLP数据集和模型与谁最一致。我们发现NLP确实存在位置性。例如，我们发现数据集和模型最符合英语国家。例如，在GPT 4的社交接受度分析中，我们发现它最符合儒家和英语国家。我们还发现Dynahate也最符合英语国家。我们还发现，当模型和数据集与特定人群对齐时，一些人不可避免地被落下。一个很好的例子是，数据集和模型与非二元个体的对齐程度低于男性和女性的对齐程度。我们发现这种现象在GPT 4的社交接受度任务和Dynahate的任务分析中都有体现。因此，考虑到NLP确实存在位置性，我们有什么建议呢？我们的第一个建议是在整个研究过程中记录所有相关的设计选择。另一个建议是以透视主义的眼光进行NLP研究。我们的第三个建议是构建针对四个特定社区的专业化数据集和模型。一个很好的例子是Masakhani倡议。我想强调的是，包容性的NLP不仅仅是让所有技术都适用于所有人。这就是我们的演讲结束的地方。如果您想了解更多，欢迎查看我们的仪表板以获取最新的分析结果，并查看我们的论文。谢谢。</sample>
    <sample id="97">演讲者提到了 SimulST 的几个问题： 1. 特定架构通常被训练，引入额外的模块进行优化。 2. 长而复杂的训练过程，例如涉及不同的优化目标的训练。 3. 训练和维护多个模型以达到不同的延迟范围。例如，训练一个平均延迟为一秒钟的模型和另一个平均延迟为两秒钟的模型。</sample>
    <sample id="98">减轻数据集中的社会和政治偏见的有效方法是通过使用不同的预训练语料库，将它们分为新闻和社交媒体，并进一步根据其政治倾向进行细分。通过在这些部分上预训练语言模型，可以观察到语言模型的政治倾向也相应地发生了变化。例如，如果在左倾的Reddit语料库上对RoBERTa进行进一步预训练，会看到其政治倾向向左倾斜。此外，还可以通过将预训练语料库分为在第45任美国总统之前和之后的时间段来研究语言模型是否能够捕获我们社会中存在的极化现象。通过这种方式，可以更好地了解如何减轻数据集中的社会和政治偏见。</sample>
    <sample id="99">在日常生活中，人类经常通过遵循分步指令的形式来规划行动，这些指令通常以目标导向的脚本形式呈现。先前的工作已经利用语言模型来为抽象目标规划，例如“制作蛋糕”，并展示了大型语言模型能够有效地将目标分解为步骤。然而，先前的工作主要关注规划抽象目标的典型活动。对于具有特定约束的目标的规划，例如“制作巧克力蛋糕”，仍然缺乏研究。在本文中，我们定义了约束语言规划的问题，该问题对规划的目标施加不同的约束。一个良好的规划者应该编写合理的脚本，并忠实于约束条件。在本文中，我们首先评估并改进了大型语言模型的约束语言规划能力。由于没有具体的任务目标数据集来支持我们的研究，因此我们需要首先获取这些目标。如表所示，我们通过扩展抽象目标并使用InstructGPT进行交互式数据收集来扩展抽象目标，以多方面的方式添加约束。好的规划者应该编写合理的脚本，并忠实于约束条件。在本文中，我们首先评估大型语言模型生成的脚本。此表报告了结果的整体准确性。我们发现所有语言模型在规划具体目标时均未达到令人满意的性能。然后，我们对为什么学习模型会失败进行了详细分析。图中的结果显示，InstructGPT在不同类别下的规划性能存在显著差异。先前的研究已经表明，语言模型的输出质量存在高变异性，导致性能不佳。因此，我们采用过生成然后过滤的方法来提高生成质量。我们首先显示了InstructGPT中约束类型和示例，以基于种子抽象目标获取具体目标。然后，InstructGPT为每个具体目标生成K个脚本。接下来，开发了一个过滤模型来选择忠实的脚本。我们将脚本和目标转换为InstructGPT嵌入，并计算余弦相似度作为相似性分数，以衡量语义相似性。此外，我们奖励包含目标约束关键词的脚本。仅保留目标得分最高的脚本。使用我们的方法，InstructGPT可以生成更高质量的脚本。我们的方法大大提高了规划能力，既在语义完整性方面也忠实于约束。由于大型语言模型成本高昂，因此有必要使较小且专门的语言模型具备规划能力。创建数据集是这一过程中的关键步骤。然而，先前的研究并未为具体目标提供规划，并且手动数据注释非常昂贵。因此，我们遵循符号知识蒸馏的想法，从大型语言模型中蒸馏出约束语言规划数据集。我们应用我们的方法构建了一个名为CoScript的高质数据集。总共，我们生成了55,000个具体目标及其对应的脚本。为了确保验证集和测试集的质量，我们请众包工作者查找并修订错误样本。此图显示了CoScript中的约束分布。我们发现CoScript在生成的具体目标方面表现出很高的多样性。使用CoScript，我们可以尝试针对特定约束进行训练的小型但专门的语言模型。我们发现，经过CoScript训练的T5模型可以生成比大多数大型语言模型更高的质量脚本，这表明小型模型在适当的数据集上进行训练时可以超越大型模型。总之，我们建立了约束语言规划问题。我们评估了大型语言模型的约束语言规划能力，并开发了一种过生成然后过滤的方法用于大型语言模型。我们使用大型语言模型生成了一个高质量的脚本数据集CoScript，用于约束语言规划。我们希望CoScript数据集可以成为推进约束语言规划研究的一个宝贵资源。感谢您的时间。有关CoScript的更多详细信息，请参阅我们的论文。</sample>
    <sample id="100">PromptRank是一种数据效率高的多跳检索方法，通过结合无监督检索和语言模型的重排序器来实现。它使用TF-IDF检索和链接遍历来构建候选链，并使用语言模型根据给定的问题对这些链进行重排序。该方法在HotpotQA上进行了实验，结果表明它在多跳问答任务中表现良好，优于完全监督的系统，并且与最先进的密集检索器相比具有竞争力。</sample>
    <sample id="101">根据所给的英文内容，PaLM 的流畅度是可比的。</sample>
    <sample id="102">水印方法的重要属性包括： 1. 可应用于嵌入式服务。 2. 水印不应降低提供的嵌入的实用性。 3. 水印应足够隐蔽，以便攻击者难以检测或移除。 4. 水印在模型提取过程中应具有可转移性。</sample>
    <sample id="103">TED 英语演讲已被翻译成 14 种不同的语言。</sample>
    <sample id="104">为了重新注释数据集，我们抽取了10,000个实例。</sample>
    <sample id="105">在本研究中，使用了三种度量来衡量良性数据集和后门数据集之间的差异：1. **余弦相似度**：这是两个向量之间相似性的度量。在这种情况下，它用于比较后门数据集中句子的嵌入与正常数据集中句子的嵌入。2. **L2相似度**：也称为欧几里得距离，它测量两个点在欧几里得空间中的距离。在这种情况下，它用于比较后门数据集中句子的嵌入与正常数据集中句子的嵌入。3. **KS检验**：Kolmogorov-Smirnov检验是一种非参数检验，用于比较两个分布。在这种情况下，它用于比较后门数据集和正常数据集的嵌入分布。这些度量帮助评估后门数据集和正常数据集之间的差异，并确定后门数据集是否成功地注入了标记。</sample>
    <sample id="106">Chaitanya介绍了他们的论文QUEST，该论文与Google DeepMind的Pete、Ming-Wei、Kenton和Kristina合作完成。他们通过两个例子展示了人们如何表达他们的信息需求，这些需求通常包含多个约束或偏好。这些例子突出了隐式集合操作在查询中的重要性。为了研究处理这种选择性信息需求的有效性，他们提出了一个名为QUEST的检索数据集。该数据集包含超过3000个实体查询，其中查询包含隐式集合操作，答案实体经过验证以确保相关性，并且它们的关联文档标记为不同查询约束的可归因跨度。他们使用Wikipedia类别名称从四个感兴趣的领域构建QUEST：电影、书籍、植物和动物。然后，他们执行集合操作来获取具有集合约束的查询。他们要求人工注释器将模板化查询进行改写，确保改写查询具有相同的含义并流畅。另一组注释器验证查询的流畅性和自然性，用于过滤查询集。最后，他们要求注释器验证答案集的相关性，并标记文档中相关证据的可归因。他们评估系统在数据集上的表现，要求系统从大型文档语料库中检索具有隐式集合约束的多答案集，其中查询约束的证据可能来自文档的不同部分。他们考虑了稀疏检索器、密集检索器以及T5基于重排序器作为基准。他们发现，检索器的表现存在很大的改进空间，而端到端系统的性能较低，表明处理此类查询的难度很大。他们还发现，涉及集合交集和集合差的查询特别具有挑战性，具有最低的F1分数。他们希望QUEST能帮助未来的研究人员构建更好的系统，以满足具有选择性信息需求的信息寻求场景。</sample>
    <sample id="107">为了使用编码器-解码器模型，将编码器和解码器分开训练。</sample>
    <sample id="108">The paper discusses the evaluation of language models on acceptability judgments using minimal pair paradigms. The authors propose revisiting data sets to recreate longer sequences and evaluate model acceptability throughout these contexts. They find that MPP judgments are robust for arbitrary context lengths, but significantly affected when sentences from the same or unrelated domains are used as prefixes. This suggests sensitivity to latent syntactic and semantic features shared across sentences in newer large language models with larger context windows.</sample>
    <sample id="109">Unnatural Instructions是一个由自然语言模型生成的指令数据集，无需任何人工标注。该数据集包含64,000个例子，通过提示预训练语言模型生成指令和输入，然后要求模型生成相应的输出。此外，还生成了每个指令的多个变体。该数据集展示了语言模型生成创意和多样化数据的能力，甚至在不正确的情况下也能提供有价值的信息。通过在Unnatural Instructions上微调一个110亿参数的T5模型，可以明显看到其在多个基准测试中的性能优于其他模型。</sample>
    <sample id="111">作者假设提供者可以收集一个通用文本语料库，并使用它来计算单词频率。</sample>
    <sample id="112">好的，我明白了。现在让我们来翻译这段内容。

首先，我们来翻译开头部分：
"Hello everyone, my name is Shuheng. Today I'm going to present our paper Do CoNLL-2003 named entity taggers still work well in 2023? Let's get started."

接下来是正文部分：

"Our paper investigated the problem of generalization using the Named Entity Recognition Task or the NER task. We observe that models have been used in CoNLL-2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalise to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models?"

"Then, to investigate these problems, we developed the CoNLL++ Dataset. This is a dataset that we collected from Reuters News from 2020, and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-03 test sets and the CoNLL++. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model."

"What is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. And last but not least, we all know that the number of fine tuning examples directly affects the performance of a downstream task. Here we also found that more fine tuning examples, actually also leads to better generalization."

"To our next question, what causes the performance drop of some models, We had two hypothesis. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++ which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years."

最后是结尾部分：

"Our conclusion is that, for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper Do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much."

希望这个翻译对你有帮助！如果你有任何问题或需要进一步的解释，请随时告诉我。</sample>
    <sample id="114">The speaker introduces their work on ACL 2023, "Finding the Pillars of Strength for Multi-Head Attention," from Nanyang Technological University of Singapore. They discuss the limitations of large language models, such as heavy parameters and long training times. The focus is on addressing the parameter problem in multi-head attention through a grouped head attention approach using a divide-and-conquer strategy. Two strategies are employed: group-constrained training to make intra-group heads more similar and inter-group heads more separate, and voting-to-stay algorithm to prune redundant multi-head attention by keeping only one head per group. Results show significant performance improvements across machine translation, abstract summarization, and language modeling tasks while achieving substantial model compression (up to 90%). Future directions include task-specific automatic pruning based on the Lottery Ticket Hypothesis to further optimize large language models' efficiency without sacrificing performance.</sample>
    <sample id="115">该方法使用的语音片段大小是lambda。</sample>
    <sample id="116">在 Servin 和 Kea 的示例中，需要特定于实体的知识来识别代词 "he" 所指的具体实体。在这种情况下，代词 "he" 指的是 Servin。要正确解决这个代词，模型需要知道 Servin 是一个法官。这种知识是特定于实体的，因为它与特定的实体（在这个例子中是 Servin）相关联。另一方面，背景知识，如“法官在法庭上审理案件”，通常在预训练期间学习，并且在句子中并不直接提供。因此，在这个例子中，背景知识不是识别代词所必需的，但它是理解上下文和为任务提供更广泛知识的一部分。</sample>
    <sample id="117">示例质量比与源句子的相似度更为重要。</sample>
    <sample id="118">这段内容介绍了ACL 2023的论文《Improving Pretraining Techniques for Code-Switched NLP》。该论文主要关注于代码切换语言处理中的预训练技术。代码切换是指在句子中同时使用两种或多种语言的现象，例如“Laptop, mere, bag, me, rakha, hai”这个例子中包含了英语和印地语词汇。由于多语言预训练模型（如mBERT和XLM-R）在代码切换任务上的表现不佳，因此该论文提出了新的掩码语言模型（MLM）方法，以适应代码切换场景。这些方法包括SwitchMLM、FrequencyMLM、残差连接以及辅助损失等。通过实验结果表明，这些方法在情感分析任务上表现优异，并且能够增加中间层和最终层中的代码切换信息。此外，作者还使用了线性探测和条件探测来验证其假设，证明了所提出的MLM方法确实增加了代码切换信息。</sample>
    <sample id="119">在扩展实验中，论文侧重于RoBERTa语言模型。</sample>
    <sample id="120">该模型使用特定层的注意力分数。</sample>
    <sample id="121">直接推断的示例包括“Easy on Me”和“I Gotta Feeling”。</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">The speaker, Ying, introduces their research on MultiInstruct, a multi-modal instruction tuning dataset. They explain that most previous works focused on language-only tasks and highlight the lack of large-scale publicly-available multi-modal instruction datasets. The proposed MultiInstruct consists of 62 diverse multi-modal tasks across 10 broad categories, derived from 21 existing open-source datasets. Each task includes five expert-written instructions.
The study uses OFA as the base model for training and testing. For training, they use 53 tasks from 9 groups with 10,000 instances per task. During testing, they reserve one group (common sense reasoning) and select additional tasks from VQ and Miscellaneous groups. All test instances are used in each experiment to evaluate performance using different metrics like accuracy, Rouge-L, and sensitivity.
The results show that instruction tuning significantly improves OFA's performance on seen multi-modal tasks, especially when transfer learning is applied. Using more instructions during fine-tuning reduces sensitivity. Transfer learning from natural instruction datasets also helps improve both overall performance and sensitivity compared to the original OFA model.
The presentation concludes by mentioning plans to collect an even larger multi-model instruction tuning dataset with around 150 new vision-language tasks, which will be released soon. A QR code provides access to data and models related to this work.</sample>
    <sample id="124">Tan Qingyu from the National University of Singapore and Alibaba presented a study on temporal reasoning capabilities in large language models (LLMs). They introduced three levels of temporal reasoning: time-to-time, event-to-event, and long-duration events. The researchers found that prior works overemphasized L2 reasoning while neglecting other types. To address this gap, they created TempReason, a dataset covering all three levels with comprehensive time periods. Their experiments showed that ChatGPT's performance deteriorates significantly when predicting months compared to years. Tan proposed two training strategies—Temporal span extraction pre-training and time-sensitive reinforcement learning—to improve LLMs' temporal reasoning abilities. Results indicated that their model, TempT5, outperformed existing methods like FLAN-T5-L and T5-SFT across various QA settings. However, both TempT5 and T5-SFT exhibited some performance fluctuations depending on different time periods. Future work could focus on mitigating these biases.</sample>
    <sample id="125">这篇论文有三位作者。</sample>
    <sample id="126">Yes, in the baseline setting, we use Google Translate API to translate source language queries into target languages. For example, if a German query needs to be translated for semantic parsing, it is first translated using an English model and then processed by the trained model to predict SQL output. This approach allows us to evaluate how well our models can handle cross-lingual tasks when dealing with different natural languages.</sample>
    <sample id="127">Namgyu Ho介绍了他们的研究工作“大型语言模型是推理教师”，这是一项与Laura Schmid和Se-Young Yun教授合作的项目。他们提出了一种方法，使用大型语言模型作为推理教师来训练较小的模型。这种方法被称为“细粒度链式推理”（Fine-tuned CoT），它通过将大型模型的推理过程转化为较小模型的训练数据来实现。他们还提出了一种称为“多样推理”的技术，该技术生成多个推理样本以提高学生模型的性能。他们的实验结果表明，这种方法在各种任务中都表现出色，并且可以显著提高小型模型的推理能力。</sample>
    <sample id="128">The speaker introduces their work "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," a collaboration between McGill University, Mila, and Microsoft Research. The presentation focuses on the need for natural language understanding models to integrate knowledge from both pretraining and inference time due to the diverse sources of information they rely on.

The coreference resolution task is introduced as a diagnostic tool designed to test this integration capability by probing how well models can use different types of knowledge available at various stages (pretraining vs. inference). This task involves identifying which entity a pronoun refers to in sentences where background knowledge might be present only during inference or not at all.

Three settings are defined:
1. Background-Pretrain: Assumption that background knowledge is already learned during pretraining.
2. Background-Both: Provides both background and entity-specific knowledge during inference.
3. Background-Inference: Only provides necessary facts about entities during inference since these may have been added after pretraining.

The study evaluates human participants' performance alongside established coreference resolution models across these three settings. Results show improved performance with specific training on KITMUS tasks compared to random choices, indicating some models successfully learn to utilize multiple source knowledge when trained appropriately.

The main takeaway emphasizes that while many existing models struggle without specialized training on integrating multi-source knowledge, those trained specifically perform better but still face challenges with backward knowledge presented solely at inference time.

For further details, attendees are directed to read the full paper, access the data set, and view the code provided via GitHub links shared during the talk.</sample>
    <sample id="129">作者给出的“显性群体”(marked group) 的示例是白人。</sample>
    <sample id="130">通过实验，我们发现，基于RNN的模型泛化能力较差。</sample>
    <sample id="131">clean test sets</sample>
    <sample id="132">这篇论文有三位作者。</sample>
    <sample id="133">作者采用了多种模态。</sample>
    <sample id="135">ABC-Eval是一种新的维度评估方法，用于评估对话AI。该工作由Emory NLP实验室和Amazon Alexa AI合作完成。ABC-Eval通过标注模型响应的行为来减少人类评估的主观性，从而提供更精确和可靠的评估策略。该方法能够测量聊天模型在各种方面表现的质量，包括忽略对话伙伴、说无关信息、自相矛盾、违反常识知识以及是否表现出同理心等。实验结果表明，ABC-Eval行为标签比现有方法更可靠，更能预测整体对话质量。这些可靠的、信息丰富的和独特的ABC-Eval指标使我们能够以更高的分辨率评估对话AI，与之前的方法相比。</sample>
    <sample id="136">Jasivan presented their work titled "FERMAT: An Alternative to Accuracy for Numerical Reasoning" at the University of Sheffield. The motivation behind this research is that there are many real-world applications requiring numerical reasoning, such as fact-checking. Jasivan and their supervisor Nafise introduced FERMAT, a flexible evaluation set based on arithmetic types, which includes number understanding, mathematical operations, and training dependency. They used questions extracted from Illinois and CommonCore benchmarks, changing numbers to decimal representations and introducing different mathematical operations.

The baseline evaluation showed poor performance across all aspects by most models, with only slight improvements in accuracy when using original benchmarks like CommonCore and Illinois. Fine-tuning improved model performance significantly but still did not reach 50% accuracy even when exact expressions were seen during training.

Jasivan's investigation into the impact of training templates revealed promising results; having language diversity (GSM8K) and mathematical diversity (AQUA) increased overall performance. Conclusions drawn include the need for more informative alternatives to existing single-score benchmarks due to unrepresentativeness issues. Areas identified for improvement involve better handling of number encoding and tokenization within these models.</sample>
    <sample id="137">The speaker introduces a new task in machine learning, which is to generate floor plan designs directly from language instructions. They define the task and explain its challenges, such as strict constraints on design generation, understanding big picture information from unstructured text, and dealing with ambiguous or incomplete instructions. The proposed solution involves using a sequence-to-sequence model based on the encoder-decoder framework, initialized by a pre-trained language model T5 for better language understanding abilities. The results show that their method outperforms other baselines significantly when tested on unseen human-written instructions after warming up with artificial ones. A case study demonstrates how different baseline models fail to align well with user requirements specified in natural language instructions.</sample>
    <sample id="138">作者认为，NLU 中研究不足的领域包括： 1. 模型如何利用预训练知识和输入时间的知识来解决任务。 2. 模型在处理需要整合来自不同来源的知识的任务时的表现。 3. 模型在处理新出现的职业或实体时的表现，这些职业或实体可能不在预训练数据中。</sample>
    <sample id="139">演讲者的名字是Ying。</sample>
    <sample id="140">Yes, CoScript has been quality checked.</sample>
    <sample id="141">现有的资源在以下方面存在局限性：首先，只有少量的翻译依赖于上下文，因此基于语料库的指标如BLEU无法捕捉这些翻译。其次，一些人建议针对上下文相关的翻译进行有目标的评估，但这些资源仅支持有限类型的上下文相关翻译和有限的语言集合，因为它们通常依赖于领域知识和人工编纂。</sample>
    <sample id="142">好的，以下是翻译结果：

你好！我将谈论我们的工作“解决间接指代表达以进行实体选择”，其中我们介绍了AltEntities语料库。我的名字是Javad Hosseini，这是一项与Filip Radlinski、Silvia Pareti和Annie Louis的联合工作。我们的目标是理解用户在想要做出选择时的语言。考虑这个替代问题：“你是指‘Easy on Me’还是‘I Gotta Feeling’？”在这里，用户想要在这些两首歌之间进行选择。最明显的事情是使用直接参考，例如说歌曲名称“Easy on Me”或其位置，“第一个”。但有时，间接参考更合适，以使对话更加自然。这可能发生在用户无法记住歌曲名称的情况下。或者，当歌曲发音相似且难以区分时。或者，当用户想指定偏好时。这里有一些间接参考的例子，例如，“较新的一个”或“不是充满活力的那个”。这是在对话系统中理解和评估LLMs实体理解的重要问题。我们不知道有更大的公共数据集用于此任务，因此我们通过众包收集了一个。我们的数据集涵盖了三个不同的领域：音乐、书籍和食谱。我们的数据集收集方法强调了非正式性，使用了一个卡通完成设置。卡通包含三个对话气泡。在第一个气泡中，Bob说：“还记得我们昨天听的那首歌吗？”这为对话设置了背景。在第二个气泡中，Alice说：“你是指‘Easy on Me’还是‘I Gotta Feeling’？”这是一个替代问题。在第三个气泡中，Bob使用间接参考来选择其中一个实体，例如，“较新的一个。”我们自动提供前两个气泡，但第三个气泡由注释员填充。第一个气泡从每个领域的几个手动提示中选择。第二个气泡始终是一个简单的模板。你是指A还是B？这里的A和B是从维基百科中获取的样本。我们使用了不同的采样方法。当我们向上移动列表时，实体变得越来越相似，因此在区分它们方面通常更难。第一个是均匀随机采样。第二个是在标题相似的情况下，例如两个带有相同标题的书籍。第三个是在维基百科上的描述相似的情况下。最后，在信息盒或属性上具有相似信息的实体中。当我们将这个替代问题展示给注释员时，他们知道这些实体的名称，但他们不一定知道这些实体。因此，我们向注释员展示了关于这些实体的一些背景知识。对于歌曲，我们简单地显示每个歌曲的Google搜索链接，并要求注释员至少听一些每个歌曲，并阅读每个歌曲。这是对“Easy on Me”的Google搜索结果。对于食谱和书籍领域，我们显示来自维基百科的一些背景文本。对于食谱，我们还显示它们的图片，再次来自维基百科，以便注释员知道它们看起来如何。然后，我们要求注释员选择其中一个实体，并用三到五个间接引用描述它们。例如，“没有歌词的那个”，“不是那个有12岁男孩的那个”，“虚构的那个”，“来自阿塞拜疆的那个”，等等。AltEntities语料库共有6,000个替代问题，涵盖三个领域，共有42,000个间接引用。T5 XL模型的结果总结如下。如果语言模型可以访问与注释员相同的背景知识，则准确率非常高，约为92-95%。但这不太现实。如果语言模型可以访问部分重叠的背景知识，则准确率在82-87%之间，这更为现实。例如，当语言模型检索背景知识时。如果语言模型只能访问实体名称，则准确率仅为60%，因此有很大的改进空间。我们还展示了模型的领域通用性。这是我们的数据集链接。谢谢。</sample>
    <sample id="143">该方法与以下现有的 SimulST 策略进行了比较：等待-k 策略、局部协议策略和针对离线模型的特定架构。</sample>
    <sample id="144">In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named DrBERT, which is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web.</sample>
    <sample id="145">演讲者的名字是Jenny。</sample>
    <sample id="146">The speaker introduces the topic of dialogue summarization, highlighting its importance in extracting key information from dialogues. They mention that while recent progress has been made using large-scale pretrained language models, these summaries often contain factual errors due to omission issues. The speaker presents a study analyzing the omission problem in dialogue summarization and introduces their OLDS dataset, which provides high-quality omission labels for five domains. They discuss the challenges posed by label imbalance and evaluate three baseline frameworks for omission detection. Finally, they demonstrate how post-editing with detected omissions can improve summary quality.</sample>
    <sample id="147">这篇论文有三位作者。</sample>
    <sample id="148">好的，我明白了。现在我将用中文翻译您的内容：

你好，我是Sara Papi，来自特伦托大学和Bruno Kessler基金会，我将简要介绍我们与Matteo Negri和Marco Turchi合作的论文《注意力作为同时语音翻译的指南》。什么是同时语音翻译？同时语音翻译，或SimulST，是指在实时中将口语翻译成另一种语言的文字，使跨语言交流成为可能。当前SimulST模型存在的问题是什么？通常使用特定的架构进行训练，引入额外的模块进行优化。例如，涉及不同的优化目标的训练过程。为了训练和维护多个模型以达到不同的延迟范围。例如，训练一个平均延迟为一秒钟的模型和另一个延迟为两秒钟的模型，等等。我们的解决方案是什么？首先，使用现有的离线ST模型而无需重新训练或采用特定的架构进行SimulST。对于每个延迟范围使用只有一个模型，并通过特定参数处理延迟。利用模型通过注意机制从音频输入到文本输出所获得的知识。这是交叉注意机制的一个示例。我们的解决方案是提出EDAtt，即编码器-解码器注意，它是一个策略，根据注意力是否指向某个部分来决定是否发出部分翻译。如果一个词被发出，则意味着注意力没有集中，即其对最后λ个语音帧的总和低于某个阈值α，这意味着接收到的信息足够稳定。例如，如果我们接收到包含“I'm going to talk about...”的语音片段，并且我们的模型预测德语翻译，我们将查看交叉注意力权重，我们将看到第一个两个单词指向最早接收到的语音片段，而最后一个单词指向最后一个接收到的语音片段，即λ个语音帧。这意味着前两个单词将被发出，因为总和的交叉注意力高于某个阈值α，我们将不会发出最后一个单词，而是等待另一个语音片段。如果我们继续并接收到另一个语音片段，我们的模型预测其他三个单词，我们将查看这些交叉注意力权重，我们将看到没有单词指向最后λ个语音帧。这意味着这三个单词将被发出。如果您想了解更多的结果，请阅读我们的论文。我们还发布了代码和模型以及同时输出，以促进我们的工作的可重复性。谢谢您的关注。</sample>
    <sample id="149">是的，数据集是公开的。</sample>
    <sample id="150">Archiki介绍了一篇名为“MEETINGQA:Extractive Question-Answering on Meeting Transcripts”的ACL论文。该论文旨在解决会议记录中问答组件的未被充分利用的问题，因为现有的工作主要集中在摘要和提取行动项上。为了应对这一挑战，研究人员创建了一个名为MeetingQA的新数据集，该数据集基于参与者在会议中提出的问题及其对应的答案句子。数据收集过程包括从公共会议转录中选择问题并标注答案。MeetingQA包含7,700个问题，并展示了各种问答场景，如多段答案、多说话者答案以及修辞性问题。研究人员使用多种方法进行评估，包括上下文检索、单段模型和多段模型。结果表明，在微调设置下，模型的表现与人类表现之间存在显著差距，而零射设置下的表现差距也很大。此外，银色数据增强有效地提高了零射性能。总的来说，MeetingQA是一个具有挑战性的数据集，因为它包含了真实会议场景中的开放式和讨论性问题，现有的问答模型在微调和零射设置下都难以解决这些问题。</sample>
    <sample id="151">Hello everyone, my name is Ying and my colleague Zhiyang and I will be presenting our research on MultiInstruct improving Multi-Modal Zero-Shot Learning via Instruction Tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focused on improving the zero-shot performance on language only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work we want to investigate whether instruction tuning a multi-modal pre-trained models can actually improve generalisation to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instructional datasets between NLP and multi-modal. There exist more than 1600 language-only instruction tasks. However, there is no large-scale publicly-available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source dataset and each task is equipped with five expert written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinates of a bounding box. Here we show some example instances from our MultiInstruct dataset, to unify the processing of various input and output data types. We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format. In which the input text, images, instructions and bounding boxes are represented in the same token space. Ok, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from 9 groups for training and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional 5 tasks from VQ and Miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. So we use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instructions as an unseen task for NLP. We use all the instances in the test</sample>
    <sample id="152">The speaker, Frederick Riemenschneider, introduces a project focused on developing language models for classical philology. He begins by discussing the current landscape of large language models in this field and highlights recent advancements such as Latin BERT (2020), Ancient Greek BERT (2021), and another Ancient Greek BERT released last year (2022). Despite these developments, there are still challenges to address: all existing models are encoder-only, monolingual, and not pre-trained on ancient languages like Ancient Greek.

To tackle these issues, the team has created new language models tailored for classical philology. Their goals include making existing models comparable, pushing state-of-the-art capabilities further, exploring different model architectures, and introducing multilingual models capable of handling both Ancient Greek and Latin texts. They have developed two monolingual models—GreBERTa and GreTa—for Ancient Greek, with GreTa being an encoder-decoder based on T5 architecture that can understand and generate text. Additionally, they introduced PhilBERTa and PhilTa, which are multilingual equivalents trained on data from Ancient Greek, Latin, and English sources.

The process involved gathering high-quality training data using resources like Open Greek &amp; Latin, Internet Archive book scans, and OCR corrections. The benchmarks used included Universal Dependencies treebanks for Greek, EvaLatina 2022 dataset for Latin, and tasks such as part-of-speech tagging, dependency parsing, lemmatization, semantic knowledge probing, world knowledge probing, and identifying relations between heroes and gods. Results show significant improvements over previous models, particularly in lemmatization performance for Ancient Greek, though no substantial difference was observed when comparing multilingual versus monolingual models' performances across various tasks.</sample>
    <sample id="153">Ninareh Mehrabi在她的演讲中介绍了她和她的团队在亚马逊Alexa AI的负责任AI团队的工作，即“解决文本到图像生成模型中的歧义”。他们对现有文本到图像模型中提供的提示中存在的歧义感兴趣。例如，“The girl enters the room with flowers”这个提示是歧义的，因为它没有明确说明花朵是在房间里，还是与女孩在一起，或者两者都有。为了应对这个问题，Mehrabi及其团队开发了一个框架来解决这些歧义，并评估生成的图像是否忠实于用户意图。他们的方法包括创建一个包含不同类型的歧义的基准数据集，使用语言模型生成澄清问题或不同的视觉设置来解决歧义，以及使用基于问题的答案（VQA）模型来评估生成的图像是否忠实于用户意图。他们的研究发现，他们的框架在解决歧义方面总体上是有效的，并且他们的自动评估框架与人类评估结果一致。</sample>
    <sample id="154">这篇论文的作者所属机构是University of Trento和Foundazione Bruno Kessler。</sample>
    <sample id="155">演讲者的名字是Javad Hosseini。</sample>
    <sample id="157">这段内容介绍了Shandong University的研究工作，名为“Dialogue Summarization with Static-Dynamic Structure Fusion Graph”。这项研究旨在通过对话摘要来帮助人们快速捕捉对话的要点。现有的对话摘要方法主要依赖外部语言工具构建静态图结构，但存在依赖工具准确性和静态图与图表示学习不匹配的问题。该研究提出了一种新的模型SDDS，包括四个主要组件：Utterance Encoder用于编码对话上下文中的句子，Static-Dynamic Graph模块用于结合静态和动态图结构，以及Summary Generator用于生成最终摘要。模型结构展示了如何利用这些组件进行对话摘要。</sample>
    <sample id="158">核心参考消解任务是将文档中的实体与其对应的提及进行匹配。传统的方法需要遍历所有提及对，导致计算和内存消耗呈二次增长。最近提出的基于缓存的方法使用固定大小的缓存，将复杂度降低到线性级别。在基于缓存的方法中，当缓存满时，它会使用一些淘汰策略，如LRU，移除最近未使用的实体。然而，在长文档中，主题可能会多次切换，这会导致新提及的实体分散在整个文本中。LRU策略会导致高缓存命中率，当遇到新提及时。我们的案例研究显示，高频实体在全球范围内被提及，并且它们占大多数缓存命中率。考虑到这一点，我们提出了一个双缓存，它有一个局部缓存和一个全局缓存，它们协同工作。局部缓存存储局部实体，采用LRU淘汰策略。而全局缓存存储全局实体，采用LFU淘汰策略，当全局缓存满时，移除最少频繁使用的实体。双缓存的工作方式如下：模型从左到右扫描文档。当它遇到新的提及时，它首先判断是否为新实体或属于缓存中的实体。然后，它评估这个新或更新实体的频率。如果合格，则添加到全局缓存中。否则，添加到局部缓存中。当缓存满时，触发淘汰策略，从缓存中移除一个实体。我们通过四个公共基准测试评估了双缓存，如图所示。LitBank和OntoNotes包含训练数据，而WikiCoref数据集则没有。带有训练数据时，双缓存比基线表现更好，即使它们使用无限内存也是如此。然而，我们还观察到，在没有训练数据的情况下，具有无限内存的模型略好，但双缓存仍然更快。为了评估双缓存的能力，我们标注了一本30,000字的书。我们可以看到，性能差距在书籍级别的文档中要大得多。此外，我们展示了双缓存显著减少了与单个缓存相比的缓存命中率。最后，双缓存使用一个局部缓存和一个全局缓存来分别存储局部和全局实体。它优于单个缓存方法，并大大减少了缓存命中率。此外，双缓存具有最高的性能/成本比率。总之，双缓存使用一个局部缓存和一个全局缓存来分别存储局部和全局实体。它优于单个缓存方法，并显著减少了缓存命中率。此外，双缓存具有最高的性能/成本比率。</sample>
    <sample id="159">语言模型对上下文的接受度评估并不总是稳健的。这是与John Gauthier、Aaron Mueller、Kanishka Misra、Karen Fences、Roger Levy和Adina Williams合作完成的工作。在本文中，我们重新审视了最小对齐范式（MPP）。最小对齐范式用于评估语言模型在基于接受度评估的情况下，即包括语法BLiMP、SyntaGym或基于刻板印象的可接受性CrowS pairs等。在最小对齐范式中，评估语言模型的方法是展示一个可接受的句子或一个语法正确的句子，然后展示一个可接受的句子或一个不正确的句子。希望模型会将更多的概率分配给可接受的句子。目前的MPP管道不允许我们评估模型在整个上下文窗口中的接受度。如今，大型语言模型正在出现越来越长的上下文窗口。对于评估模型的接受度，至关重要的是要评估模型在整个上下文窗口中的表现。这就是我们在尝试做的事情。我们的方法是通过模拟较长的序列来重新评估数据集本身。为了模拟较长的序列，我们从数据集中选择可接受或不可接受的句子，并将其作为查询对的前缀进行重组。例如，在BLiMP数据集的附件岛案例中，我们选择了典型的语法错误对。我们通过提取附件岛中的语法正确句子，并将其作为查询对的前缀进行重组，来创建较长的序列。我们也可以通过选择相同匹配的不同子集或不同数据集的句子来进行相同的操作。这就是所谓的不匹配场景。在这里，句子仍然来自相关数据集，但不是用于评估的数据集。我们还可以选择完全无关的领域，如维基百科。这将告诉我们模型的接受度判断是否受到任何上下文的影响，无论是来自数据集的不同部分还是完全无关的部分。模型如何表现？首先，我们查看维基百科句子，这些句子与当前查询对完全无关，我们发现MPP判断在任意上下文长度下都相对稳定。我们将上下文长度增加到OPT和GPT-2模型的最大长度1024。我们看到橙色虚线表示MPP判断相对稳定。现在，当我们在同一数据集内选择句子时会发生什么？这里我们从BLiMP或SyntaxGym数据集中选择或创建句子，从可接受和不可接受域。我们看到添加可接受前缀或不可接受前缀时，MPP判断要么显著增加，要么显著减少。但是，当我们匹配结构时，即选择来自同一现象的句子，我们看到MPP判断要么显著增加，要么显著减少，这取决于前缀是可接受还是不可接受。这种效果随着上下文长度的增加而显著增加，这可能会影响具有较大上下文窗口的新语言模型。为什么匹配前缀会如此显著地影响语言模型的判断？我们进行了系列分析，其中我们通过保留相关结构并添加输入噪声来扰动输入句子。经过一系列这些扰动后，我们发现所有这些噪声都不会使模型改变其方向，即MPP判断。我们发现模型对扰动句子以类似的方式敏感。具体来说，我们发现当我们在可接受域内扰动句子时，所有扰动都会导致MPP判断相似的增加，而当我们在不可接受域内扰动句子时，所有扰动都会导致MPP判断相似的减少。因此，我们的主要发现是，语言模型对隐藏的句法和语义特征敏感，这些特征在句子之间共享。当前的MPP评估方式，即使用短且单个句子输入，可能无法全面捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以获取更多详细信息。谢谢大家。</sample>
    <sample id="160">该方法的第一步将输入词元映射到一个无序的词元集。</sample>
    <sample id="161">CoScript contains 55,000 specific goals with scripts.</sample>
    <sample id="163">MASSalign</sample>
    <sample id="164">弱监督学习的好处是它比手动标注数据更便宜。</sample>
    <sample id="165">Wenting Zhao, a PhD student at Cornell University, presented their recent paper titled "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations". The presentation began with an example to illustrate abductive reasoning: given the context of Emily being stuck in traffic and the outcome that she made it to her flight, Explanation 1 ("Her flight was delayed") is plausible as it bridges the information gap between the context and outcome. The goal of abductive reasoning is to identify such explanations from a set of possible ones.
The paper considers a closed-world setting for abductive reasoning where a candidate set of explanations Z is provided, and the task is to find a plausible subset of these explanations. Current approaches rely on supervised methods but require annotated plausible explanations, which can be noisy and subjective; crowd workers disagreed on over 60% of more than 1,000 explanations in one experiment.
To address this issue, the researchers introduced LiPoR (Likelihood Learning with Posterior Regularization), an unsupervised learning method that treats Explanation Z as a latent variable. This approach leads to an unsupervised objective function that maximizes the marginal likelihood of the Outcome Y given the Context X by marginalizing other possible explanations in Z. However, maximizing only the likelihood does not ensure preference for plausible explanations.
To achieve this, they added a regularizer based on mutual exclusivity among explanations—a significant characteristic observed in the abductive reasoning example. If Explanation 1 were true, Explanation 2 would automatically rule out. Therefore, Omega, the regularizer, aims to enforce this mutual exclusivity. Formally, the LiPoR objective consists of two parts: maximizing the likelihood of outcomes and preferring some explanations over others through the regularizer Omega.
The results showed that LiPoR significantly outperformed previous best unsupervised approaches and even surpassed a strong zero-shot GPT-3 baseline by over 4 absolute points in accuracy when tested on AlphaNLI, the most widely-used abductive reasoning dataset.</sample>
    <sample id="166">The speaker introduces their new work, "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text". They explain that image retrieval tasks are challenging due to the similarity of images and complexity of text descriptions. Typical methods like visual language models perform well on simple tasks but struggle with complex ones. The team draws inspiration from Divide-and-Conquer strategy and Dual-Process Theory: Divide-and-Conquer breaks large problems into smaller parts; Dual-Process Theory involves two thinking systems in human brains—System 1 performs analogical reasoning (like System 1), while System 2 handles abstract logical reasoning.

Pre-trained visual language models excel at analogical reasoning but fail when faced with complex tasks. To tackle this issue, they propose integrating a logical reasoning system akin to System 2 within Divide-and-Conquer. Their method includes three main components:

1. Proposition Generator: Decomposes complex propositions into simpler representations.
2. Visual-Linguistic Interactor: Interacts visually with linguistic information.
3. Neural-Symbolic Reasoner: Combines results from both previous modules using negation executor and conjunction operation.

Experimental results show NDCR outperforms other baselines. Two cases demonstrate its interoperability by presenting intermediate inference states and final solutions. Suggestions include improving compositional reasoning and planning in large language models through neural symbolic calculation or integrating Divide-and-Conquer with Dual-Process Theory.</sample>
    <sample id="167">DEPLAIN-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如下： - 对于 DEPLAIN-apa，我们手动对齐了 483 份文件。这导致了大约 13,000 对平行句子。 - 对于 DEPLAIN-web，这些文档在两个方面进行了对齐：一方面通过手动对齐，另一方面通过自动对齐方法。总共结果为 30,450 句子对。</sample>
    <sample id="168">CoNLL++数据集是由Reuters新闻从2020年收集的，然后按照与CoNLL-2003相同的注释指南进行注释。</sample>
    <sample id="169">David Vilar介绍了Google Translate团队与同事合作的论文《Prompting PaLM for Translation: Assessing Strategies and Performance》。该论文研究了大型语言模型（LLMs）在机器翻译中的提示能力，通过使用最新的测试集和最先进的神经机器翻译指标进行评估，并展示了专家的人类评估结果。实验表明，提示策略对LLM的翻译性能有显著影响，选择高质量的示例比提示的形式更为重要。研究发现，PaLM在流畅度上与最先进的系统相当，但在准确性方面存在一些问题，特别是在省略错误方面。</sample>
    <sample id="170">好的，以下是翻译结果：

大家好，我是来自宾夕法尼亚州立大学的Yusen Zhang。今天我将介绍我们的工作“XSemPLR：多语言自然语言和意义表示的跨语言语义解析”。语义解析是构建用户查询的语义表示的任务，例如SQL和Lambda计算。而跨语言语义解析是将多个自然语言中的查询翻译成多个意义表示。如图所示，我们需要使用神经模型将多个自然语言中的查询翻译成SQL、Lambda或FunQL等。现有的跨语言语义解析模型是分别提出的，并在特定任务和应用的数据集上进行评估。例如，它们对某些自然语言有广泛的覆盖范围，但缺乏对中文的支持，对某些意义表示的覆盖范围有限。Lambda计算是缺失的，或者它们只评估了特定的神经模型。因此，我们提出了XSemPLR。我们提供了一个统一的数据集XSemPLR，用于跨语言语义解析的多个自然语言和意义表示。它包含9个涵盖不同领域的数据集，5种语义解析任务，8种意义表示，以及15种语言家族中的22种语言。为了更好地评估我们的基准，我们考虑了六个训练和评估设置。第一个是翻译-测试。我们使用Google Translate API将源语言翻译成目标语言，然后使用单语言模型进行训练和评估。例如，我们用英语模型来预测SQL，期间使用API将德语查询翻译成英语，然后使用训练好的模型进行预测。我们还将测试单语言模型。在这种情况下，源语言和目标语言相同，例如德语到德语或英语到英语。我们还测试了单语言少量设置，通过使用仅10%的训练数据训练单语言模型。我们还测试了多语言模型，其中我们将德语、英语和中文查询一起训练一个多语言模型。例如，在推理期间，我们可以使用此模型来翻译德语查询或中文查询。我们还考虑了跨语言零-shot和少量转移。我们在一个源语言上进行训练，然后转移到另一个语言。在训练期间，我们可以在英语查询或德语和英语少量查询的组合上训练一个多语言模型来预测SQL输出。我们还发现了一些有趣的结果。例如，对于单语言模型的分析，我们评估了两组模型，包括编码器-指针（Encoder-PTR），即使用多语言预训练编码器和基于指针的解码器，例如XLM-R + PTR和mBERT + PTR。我们还评估了编码器-解码器模型，即多语言预训练编码器-解码器模型，例如mBART和mT5。我们发现编码器-解码器比先前的工作或实现更好的性能。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较了三种代表性的多语言语言模型。我们的结果显示了许多有趣的发现。例如，编码器-解码器优于先前的工作或实现了可比的结果。在少量设置下，预训练于英语自然语言可以显著提升目标自然语言的性能，我们发现Codex和BLOOM等多语言模型仍然不足以完成跨语言语义解析任务。总的来说，我们构建了XSemPLR，一个统一的基准，用于跨语言语义解析的多个自然语言和意义表示。我们进行了全面的基准研究，比较</sample>
    <sample id="171">现有的研究可以大致分为四类。然而，这些方法要么不适用于嵌入式服务，要么缺乏可转移性。</sample>
    <sample id="172">不，根据所给的英文内容，多语言 LLM 如 Codex 和 Bloom 对于 CLSP 来说仍然不够。</sample>
    <sample id="174">Thea是论文“ArgAnalysis35K：一个用于论点质量分析的大规模数据集”的联合作者。在视频中，她快速解释了为什么这个数据集与其它类似主题的数据集不同。她概述了数据集的独特之处，并鼓励观众阅读他们的论文和在会议上查看他们的海报以获得更深入的见解。</sample>
    <sample id="175">该方法通过诱导对齐作为训练的一部分来处理排列的不确定性。有时存在多个与数据一致的排列，但语言上更合适的排列是隐藏的。通过诱导对齐作为训练的一部分，该方法可以解决这种不确定性，并找到更准确的排列。</sample>
    <sample id="176">根据所给的英文内容，下游 NLP 模型的公平性可以通过以下方式定义：

1. **政治偏见评估**：首先，需要评估语言模型的政治倾向。这可以通过使用政治问卷（如政治会议测试）来实现，以确保评估是基于政治科学文献的。

2. **性能评估**：然后，需要评估具有不同政治倾向的语言模型在下游任务上的表现。这包括检查它们在仇恨言论检测和假新闻检测等任务中的表现。

3. **公平性指标**：通过分析语言模型在不同政治倾向下的表现，可以识别出是否存在公平性问题。例如，在仇恨言论检测中，左倾语言模型可能在检测针对少数群体的仇恨言论方面表现更好，但在检测针对更有权力群体的仇恨言论方面表现更差。类似地，在假新闻检测中，左倾语言模型可能在检测来自其相反政治倾向的假新闻方面表现更好，而在检测来自其相反政治倾向的假新闻方面表现更差。

4. **定量分析**：进一步的定量分析可以帮助确定语言模型在不同类别或政治倾向下的表现差异。例如，可以将性能分为不同的社会类别或政治倾向，以更好地理解不同政治倾向的语言模型如何影响公平性。

5. **定性示例**：提供定量分析之外的定性示例，可以帮助可视化语言模型如何基于其社会类别或政治倾向给出不同的预测。这些示例可以突出显示由于语言模型的政治倾向而导致的公平性问题。

通过遵循这些步骤，可以全面了解下游 NLP 模型的公平性，并采取措施解决由语言模型政治倾向引起的问题。</sample>
    <sample id="177">演讲者的名字是Yanis Labrak。</sample>
    <sample id="178">演讲者的名字是Koustav Sinha。</sample>
    <sample id="179">Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker
Theory of mind is the ability to reason about the mental state of others. It is traditionally measured both in humans and in language models in reading comprehension tasks involving multiple characters. A great way of probing the understanding is through false-belief questions. These are situations where reality may not match the belief of certain story characters. Let's look at a classic example of a theory of mind test, the Sally-Anne test. In the story, Alice and Bob are in a room where there is a basket and a box. Alice puts an apple in the basket and then leaves the room. Bob then moves the apple to the box. Then the human or the language model are probed with a bunch of questions. For example, where will Bob search for the apple? In the box? Where does Bob think that Alice will search for the apple when she comes back? In the basket. So questions may be classified as first-order or second-order depending on whose mental state you are asking about. First-order is asking about a character's mental state, and second-order is asking about a character's estimation of another character's mental state. True-belief questions are those where the expected answer matches the true location of the object, and false-belief, otherwise. We know that large language models still perform poorly on false-belief tasks, for example ChatGPT or GPT-3. So our research question is, "How can we improve Theory of Mind reasoning skills in Large Language Models?" We present SymbolicToM, an inference-time method to improve Theory of Mind reasoning skills in Large Language Models using explicit graphical representations. SymbolicToM uses several graphical representations, since mental states cannot be represented with a single graph. For example, on the left we see a representation of what Bob believes is the current world state, and on the right we see a representation of what Bob thinks that Alice believes is the current world state. We call these graphs BBob and BBob,Alice. In general, we compute these graphs for all combinations of characters p₁ through pₘ up to a predefined maximum Theory of Mind level m. Graphs are computed using an inference-time algorithm that leverages off-the-shelf NLI and OpenIE models. See the paper for details. Having pre-computed these graphical representations for a given story, we can efficiently answer any given question. For example, let's say we want to answer, "Where does Alice think that Bob will search for the apple?" We first detect the entities in the question. Then we retrieve the appropriate belief graph and perform recursion over the question so that now we're asking a factual question over the graph. Then we retrieve the sentences captured by the graph and finally take the sentences plus the factual question and feed it to a language model to get the final answer. So now let's look at the experiments. We test our method with a myriad of LLMs and compare it against supervised baselines, specifically a fine-tuned GPT-3 model and Textual Time Travel, which is a model specifically designed for Theory of Mind reasoning. We analyze in-domain performance in the well-known ToMi dataset, and we evaluate robustness with two out-of-domain setups that we designed. Let's look at the in-domain performance results for second-order false-belief questions, and all others can be found in the paper. The X axis represents the out-of-the-box LLM performance, and the Y axis represents the performance using SymbolicToM for a given LLM. Then at any point in the upper left triangle means that using SymbolicToM increases the performance versus not using it. We see performance gains across the board, for example, 65 accuracy points gained for GPT3-Davinci, 67 for Macaw, 51 for Flan-T5-XXL, among many others. For testing our method's generalization capabilities, we design two new datasets, both modifying ToMi's benchmark. We test for storage structure generalization by, for example, concatenating two stories and asking half the time about the first story and half the time about the second one. This is the D₁ dataset, and we design two more, D₂ and D₃. We also test for linguistic generalization, which is our second dataset called ParaphrasedToMi, generating a dataset that has more linguistic diversity. This is important since ToMi is generated automatically with only one way of phrasing each sentence. For the story structure generalization datasets, we observed that supervised models heavily degrade the performance on the three datasets we created, for example showing around 50% performance in the dataset D₁ that I just described. On the other hand, using SymbolicToM still shows significant gains for all models, allowing stronger models like GPT-4 to fully solve the datasets, giving, for example, a 42 point accuracy boost for dataset D₁ . So in conclusion, we introduced SymboliToM, a plug-and-play method to improve Theory of Mind reasoning skills in large language models. It is an inference-time algorithm which avoids overfitting risk. It uses explicit graphical symbolic representations, which yields more interpretable reasoning, and SymbolicToM dramatically improves out-of-the-box LLM performance, outperforming supervised approaches on out-of-domain story understanding and remaining beneficial on the new linguistic diversity dataset, ParaphrasedToMi. For more details, please refer to the paper, and don't hesitate to reach out to chat. Thank you so much for listening.</sample>
    <sample id="180">演讲者的名字是Myra。</sample>
    <sample id="181">The speaker introduces their work on constrained language planning, which involves decomposing abstract goals into specific steps with constraints. They explain the challenges of this task and how they developed a dataset called CoScript to improve large language models' performance in generating scripts that are both complete and faithful to constraints. The method used for creating CoScript is symbolic knowledge distillation from larger models. Finally, the speaker discusses the potential benefits of using smaller but specialized models trained on such datasets.</sample>
    <sample id="182">在本文的背景下，热带主义指的是对拉丁裔女性使用“充满活力”和“丰满”等词语，这些词语将她们与一种热带、异国情调的形象联系起来。这种形象通常与拉丁裔女性相关联，强调她们的外貌特征，并可能强化刻板印象和文化偏见。</sample>
    <sample id="183">作者通过使用提示来创建目标群体的人工描写。具体来说，他们使用提示如“想象你是一个亚洲女性。描述你自己。”来生成这些描述。这种方法利用了这些新的指令调优语言模型（LLMs）在响应指示和提示方面的良好能力。通过指定不同的身份标记，例如种族、性别和其他社会因素，作者能够生成代表不同人口统计群体的多样化人工描述。这些提示是基于先前的研究开发的，该研究发现，当提供给人类受试者时，它们能够揭示种族刻板印象。这种方法允许直接比较生成的人工描述与人类编写的响应，从而捕捉到更广泛的模式和刻板印象，而无需依赖特定的词汇表。</sample>
    <sample id="184">CXMI是衡量语境使用情况的指标。</sample>
    <sample id="185">DrBERT 是一个基于 RoBERTa 的模型，训练数据为 NACHOS，而 ChuBERT 则是一个临床模型，训练数据为 Nantes University Hospital 数据仓库中的匿名化数据。</sample>
    <sample id="187">这篇论文有两位作者。</sample>
    <sample id="188">迭代迁移学习是将模型从一个任务转移到另一个任务，然后在新的任务上进行微调。在这个例子中，模型首先从两个不同的任务（扩展和比较类）进行微调，然后进一步微调到辩论任务。这种方法有助于提高模型的性能，并且在冷启动主动学习时非常有用。</sample>
    <sample id="189">数据集的目标是理解用户在选择实体时的语言，通过提供一个包含音乐、书籍和食谱三个领域的数据集，来帮助评估语言模型在处理间接引用表达方面的表现。</sample>
    <sample id="190">攻击者可以通过学习嵌入来提取模型参数。</sample>
    <sample id="191">这篇论文有三位作者。</sample>
    <sample id="192">The presentation is about a work called "CAME: Confidence-guided Adaptive Memory Efficient Optimization". The presenter, Yang Luo, explains that robust training of large language models often relies on adaptive gradient-based optimization methods. However, some widely-used optimizers like Adam always triple the required memory for keeping the first and second moment estimates of per-parameter gradients. Some existing memory-efficient optimizers like Adafactor have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. Here's their challenge: how to design an optimizer to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. They introduce non-negative matrix factorization (NMF) which reduces the memory requirements from O(mn) to O(m +n). And they explain that NMF operation in Adafactor will inevitably incur erroneous updates in the training of deep neural networks. Adafactor converges slowly compared to Adam due to the existing error, further limiting the application scenarios of memory-efficient optimizers. Inspired by the erroneous update, they consider an efficient approach to decrease the side effect caused by insecure updating. Given mₜ and uₜ, they take the residual between them as the instability in the preserved momentum and set generated instability as a denominator of original mₜ to more adaptively take an updating step. With this approach, CAME achieves significant improvements over other optimizers such as Adam and Adafactor during experiments on BookCorpus and English Wikipedia.</sample>
    <sample id="193">我们使用了10个注释者来创建初始数据集。</sample>
    <sample id="194">这篇论文的作者所属机构是卡内基梅隆大学和华盛顿大学。</sample>
    <sample id="195">该研究提出了一种名为RoHT的新型框架，用于了解复杂问题的层次结构，并通过概率推理在不同知识源之间进行融合。该框架包括两个阶段：首先，使用层次化问题分解树（HQDT）来理解复杂问题的层次结构；其次，在HQDT上进行概率推理，以考虑来自KB和文本语料库的知识。该研究在KQA Pro和Musique两个复杂问答数据集上进行了评估，结果表明RoHT在利用KB和文本信息方面具有显著优势。</sample>
    <sample id="196">示例是“我看到了巴特和丽莎”。</sample>
    <sample id="197">对话系统中的最先进模型是四个。</sample>
    <sample id="198">我们需要在整个上下文窗口中评估模型的可接受性，因为大型语言模型现在拥有越来越长的上下文窗口。评估模型在较长上下文窗口中的可接受性，可以更好地了解模型是否能够理解句子中的整体结构和语义关系，而不仅仅是单个句子。这有助于更全面地评估模型的语言理解和生成能力。</sample>
    <sample id="199">不，多语言训练会导致表现下降。</sample>
    <sample id="200">No, the annotators did not know about the entities before being shown some background knowledge.</sample>
    <sample id="201">评估使用了 BLEURT 和 MQM 指标。</sample>
    <sample id="202">泛化中的回归会影响特定的 NER 类型。</sample>
    <sample id="203">NLP中的立场很重要，因为它们可以影响研究过程和结果。研究人员的立场可能会影响他们做出的决策，从而导致数据集和模型中存在系统性的性能差异。这些差异可能会对某些人口群体产生不利影响，导致某些人口群体被忽视。通过理解NLP中的立场，研究人员可以采取措施解决这些问题，确保技术对所有人都是公平和包容的。</sample>
    <sample id="204">BLOOM 是采用适配器微调。</sample>
    <sample id="205">Shangbin, a PhD student at the University of Washington, presented their work "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models". They discussed how language models are trained on large-scale web crawl data that includes political news media. This has led to potential fairness issues in downstream tasks due to inherent social biases. To address this, they proposed investigating the political bias propagation pipeline from pretraining data to language models and downstream applications. Their approach included evaluating political leanings using prompt formats with political questionnaires like the political conference test, conducting experiments by further pretraining language model checkpoints on partisan corpora, examining polarisation trends, and assessing performance differences for hate speech detection and fake news detection based on political leaning. The results highlighted significant implications for addressing fairness concerns related to language model political biases.</sample>
    <sample id="206">他们使用了一个初始模型进行迁移学习。这个模型是通过从两个不同的任务中转移权重来实现的：1. 主题无关的辩论立场分类，这是一个任务，它确定了两个不同人的辩论陈述是否在不同意或同意，无论主题是什么，称为辩论。2. PDTB中的扩展和比较类别的二元分类，因为这两个类别与认知和谐和不和谐的概念密切相关，因此被称为CE。通过将这些任务的权重转移到初始模型中，他们能够提高零射性能，并在后续的主动学习过程中进一步优化模型。</sample>
    <sample id="207">最近用于评估 PaLM 能力的测试集包括 WMT 评估。</sample>
    <sample id="208">作者提出了三条建议。</sample>
    <sample id="209">与最强的基线相比，提议的方法获得了多少收益？</sample>
    <sample id="210">演讲者的名字是Shuheng。</sample>
    <sample id="211">论文中的结果和数据集可以用作基准。</sample>
    <sample id="212">在论文中，他们进行了三个较小模型的实验。</sample>
    <sample id="213">OFA被用作研究多模型指令调整的基础模型。</sample>
    <sample id="215">这段内容主要讨论了协调结构的依赖关系。作者通过分析不同理论和语料库中对协调结构的不同假设，提出了一个关于协调结构对称性的新论点。他使用了依赖长度最小化的原则来解释这种现象，并通过具体的例子展示了这种原则在不同情况下如何影响协调结构的构建。</sample>
    <sample id="217">We propose a disentangled controllable dialogue generation (DCG) model that learns attribute concepts from seen values and uses the disentanglement loss to disentangle different attribute combinations. We introduce a unified reference-free evaluation framework, MAE, for different granularities of attributes. We establish two benchmarks and prove the effectiveness of our method and evaluation metrics through experiments. Our models are based on the DialoGPT framework with the compositional prompt module. To effectively use control signals, we have designed two types of prompts that use attribute-related information from the pre-trained language model. The first is an attribute-oriented prompt. We use the combination of controllable attribute values corresponding to each instance as prompts to guide the model's focus on the specific information in the dialogue. The second is a task-oriented prompt. Although attribute-oriented prompts capture the instance-specific control signals, the dialogue response generation task is also guided by the instance-independent global features. Finally, we concat the two prompt embeddings to create whole prompt embeddings. To improve the generation ability of our model and force it to distinguish between different attribute value combinations, we design some pseudo combinations to enhance the diversity of the prompts. A disentanglement loss is also introduced to train multiple compositional prompts while disentangling the combination representations. To address the lack of a matrix for multi-attribute controllable dialogue generation, we propose a unified and efficient evaluation framework that does not require additional large-scale labeled data. We designed a template that consists of discrete prompts such as emotion/act/persona controls the response [MASK]. To reduce the potential bias of different handcrafted patterns, we also add a trainable continuous dialogue-oriented prompt to improve stability and robustness. Shown are DailyDialog-CG controllable dialogue generation results for new attribute combinations. Our DCG outperforms all other baselines in attribute controllability and text equality. We test our model with attribute-oriented prompts, task-oriented prompts, and disentanglement learning. Results show that attribute-oriented prompts guide the model to focus on controllable information, while task-oriented prompts improve text equality, and disentanglement learning improves the ability of compositional generalization. In this table, the figures for DailyDialog-CG is compared to E-ACC and A-ACC controllability metrics as well as the BLEUs of the fine-tuning method control, and our DCG. Our DCG successfully tackles the challenges of compositional generalization for multi-attribute controllable dialogue generation with only a small drop on E-ACC and A-ACC. Additionally, our DCG outperforms CTRL on both controllability and text equality of unseen attribute combinations. This result confirms the effectiveness of our method for transforming seen attributes to unseen combinations. We use three correlation coefficients to evaluate the quality of different metrics, including our automatic metric, MAE. Compared to human judgments, our method outperforms classic metrics for both coarse-grained discrete attributes and fine-grained continuous attributes. We tested variants of MAE and found that removing the continuous prompts decreases the correlation scores since the task-oriented prompts are the only parameters that can be fine-tuned and are therefore important for MAE. We also implemented MAE on another PLM, BART, to show its generality. We demonstrated the impact of prompts on compositional generalization with a visualization of the concatenated prompt embeddings of three attributes via PCA on DailyDialog-CG, as shown in this figure. This result proves that our method can disentangle attribute combinations and learn the relations between different attributes with the ability to generalize from seen attributes to unseen combinations. Our proposed attribute-oriented prompt method outperforms models that learn an independent prompt for each attribute value. The shared embedding mapping helps learn attribute concepts from seen values to unseen combinations.</sample>
    <sample id="218">这篇论文的作者所属机构是Google。</sample>
    <sample id="219">Jia-Huei Ju presents their work on uncovering financial signals in financial reports, focusing on the Form 10-K annual report. They introduce a highlighting task to compare and contrast text between consecutive years' reports. The proposed pipeline includes document segmentation, relation recognition, out-of-domain fine-tuning with eSNLI data, and intermediate fine-tuning using revised pairs from the dataset. Their model achieves high performance metrics like precision over recall (PCC) and correlation coefficients, demonstrating its effectiveness for information retrieval tasks related to financial reporting analysis.</sample>
    <sample id="220">Stony Brook University</sample>
    <sample id="221">论文分析了英语和德语之间的翻译。</sample>
    <sample id="222">这篇论文探讨了在开放领域问答（Open-Domain QA）中，如何通过数据干预来实现跨域泛化。作者首先介绍了在不同领域之间进行问答时遇到的挑战，特别是当源模型是在一个通用领域（如维基百科）上训练的，而目标领域可能非常稀疏且不相关时。他们提出了一种方法，即通过在源模型和目标领域之间添加相关但不完全匹配的数据，来提高模型的泛化能力。

为了实现这一目标，作者提出了两种主要的方法：零-shot和少量-shot。在少量-shot方法中，他们使用少量的目标领域示例来提示大型语言模型生成更多示例。这些示例被转换成闭合式问题，用于进一步适应检索器和阅读器模型。结果显示，这种技术在检索器性能上平均提高了8%，在阅读器性能上平均提高了11%。

在零-shot技术中，由于没有目标领域的示例，作者通过控制三个随机变量之间的相互作用来进行干预：问题、答案和上下文。他们通过保持两个变量不变，改变第三个变量来理解其对模型学习的影响。结果显示，保持答案分布均匀是最有效的策略。

为了确定目标领域与源模型之间的兼容性，作者使用了一个基于机器学习数据转移分类学的框架。他们计算了源检索器模型对目标领域中所有上下文的分配概率，并将这些值作为兼容性的度量。然后，他们将不同的目标集映射到这个2D网格上，以估计它们所表现出的类型的数据转移。结果显示，只有特定类型的干预措施对不同类型的数据转移是有效的。

总的来说，作者通过实验发现，通过数据干预可以显著提高阅读器性能，最高可达24%。他们还展示了，根据目标领域所表现出的数据转移类型，某些类型的干预措施更有效。</sample>
    <sample id="223">演讲者的名字是Shangbin。</sample>
    <sample id="224">在实验过程中，研究了两个模型：长mBART和基本mBART。</sample>
    <sample id="225">在 MultiInstruct 中，53 个任务用于训练目的，10 个任务用于测试目的。</sample>
    <sample id="226">这篇论文有三位作者。</sample>
    <sample id="227">The speaker discusses the challenges of grounded language understanding in natural language processing (NLP) tasks. They explain that current language models, including large ones like Codex, are pre-trained with textual data and do not incorporate grounding during this process. This lack of grounding makes it difficult for these models to map natural language expressions into executable plans or programs.

The main challenge lies in generating valid and grammatical plans from text, which is often problematic as seen in scenarios such as knowledge-based question answering where generated queries may be invalid over a specific database. To address this issue, the speaker introduces their novel framework called Pangu, inspired by Chinese mythology's primordial being who separates heaven and earth.

In Pangu, a symbolic agent interacts with an environment to propose candidate plans while a language model scores and ranks these candidates without needing to handle generation directly. The framework leverages discrimination instead of generation, making it easier for language models to excel at ranking proposed plans based on validity and grammar.

The speaker presents experimental results showing that Pangu outperforms other methods across different settings when using various language models like BERT, T5, and Codex. Notably, Pangu demonstrates strong sample efficiency, achieving high accuracy even with minimal training examples. For instance, Codex with in-context learning achieves over 50% accuracy in GRAIL query with just one demo example, significantly better than fine-tuning experiments alone.

An interesting observation made by the researchers is that autoregressive models tend to overfit known structures during training but maintain similar distributions between seen and unseen structures under Pangu’s approach. This suggests robustness against non-i.i.d. settings.

The key takeaway emphasized by the speaker is that for grounded language understanding, focusing on discrimination rather than direct generation might yield more effective strategies utilizing language models.</sample>
    <sample id="228">作者在实验中使用了AG News、MIND、SST2和Enron Spam数据集。</sample>
    <sample id="229">Gabriella Skitalinskaya介绍了一项与Henning Wachsmuth合作的研究，旨在检测文本修订中的不可证明断言。她解释说，文本修订是专业写作中必不可少的过程，通常需要反复修改以达到最佳表达。在论点性写作中，找到正确的词语和表达方式尤其重要，因为它们直接影响文本对受众的影响。Skitalinskaya强调了准确评估论点性文本质量的重要性，并介绍了两个任务：不可证明断言检测和论点建议改进。她讨论了使用修订数据的挑战，包括代表性、可靠性、模型复杂性和架构、上下文依赖性和主题和用户偏见。Skitalinskaya概述了她的团队如何通过详细分析和实验来应对这些挑战，并得出结论，修订数据可以有效地用于检测不可证明断言和提供论点建议。</sample>
    <sample id="231">NACHOS 是一个包含医疗数据的网络爬虫数据集。</sample>
    <sample id="232">演讲者的名字是David Vilar。</sample>
    <sample id="233">Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. However, current SimulST models have several problems: they usually require specific architectures that are trained with additional modules to optimize them, leading to long and complicated training procedures involving different optimization objectives; and it's necessary to train multiple models for different latency regimes (e.g., one model with an average of 1 second latency and another with 2 seconds latency). To address these issues, EDAtt was proposed as a solution. This strategy uses already existing offline ST models without re-training or adopting specific architecture for SimulST. It leverages the knowledge acquired by the model through the attention mechanism between audio input and textual output. The main idea behind EDAtt is to decide whether to emit partial translations based on where attention points to within the received speech frames. A word is emitted if its sum of cross-attention weights towards the last lambda speech frames is below a certain threshold alpha, indicating enough stability in the information received. For example, if a speech chunk contains "I'm going to talk about..." and the model predicts German translation, we look at the cross-attention weights which show that the first two words point to earlier received speech frames while the last word points to later ones. Therefore, only the first two words will be emitted since their sum of cross-attention exceeds the threshold alpha. If more speech chunks follow ("...other three words"), no word from this new set points to the latest lambda speech frames, so all three words can be emitted immediately.

The results demonstrate that EDAtt outperforms other strategies applied to offline models when plotted against BLEU scores measuring translation quality and average lagging representing latency measures. Additionally, considering computational-aware average lagging accounts for the model's computation times needed to predict outputs, EDAtt shows faster performance compared to Wait-k strategy and Local Agreement methods. These findings highlight the effectiveness of using pre-trained offline models adapted for simultaneous speech translation tasks like SimulST.</sample>
    <sample id="234">提示策略对结果有重大影响。在实验中，使用不同的提示策略进行翻译，发现提示形式对结果影响不大，但提示示例的质量对结果有重要影响。因此，在选择提示策略时，应优先考虑高质量的示例。</sample>
    <sample id="235">这篇论文的作者所属机构是：University of Washington, Microsoft Research Asia, and University of California, San Diego</sample>
    <sample id="236">专家编写的指令是MultiInstruct数据集中的任务的输入。</sample>
    <sample id="237">作者建议通过使用一个核心参考任务来测试模型，该任务设计用于探测从不同来源获取知识的能力。他们提出了三种设置：背景-预训练、背景-两者和背景-仅在推理时。这些设置允许评估模型在不同时间点（预训练和推理）访问不同类型的知识（背景和实体特定）的能力。</sample>
    <sample id="238">Yebowen Hu from the University of Central Florida presents MeetingBank, a new benchmark dataset for meeting summarization technologies. The dataset includes 1,366 City Council meetings with nearly 7,000 instances. Data collection involves using Speechmatics API to convert audio data into transcripts and extracting reference summaries from meeting minutes. Coverage scores indicate that most summaries include verbatim points over abstraction, while density scores show varying levels of editing across cities. Model evaluation reveals DialogLM as the top-performing abstractive summarizer on ROUGE-2 score, but GPT-3 excels in human assessment metrics like fluency and coherence. Human evaluators rated GPT-3 highest overall, suggesting improvements are needed in informativeness and factuality. MeetingBank serves as a valuable resource for researchers developing advanced meeting summarizers and provides insights into decision-making processes within city councils.</sample>
    <sample id="239">大家好，我是David Vilar，将为大家简要介绍Google Translate团队的论文《Prompting PaLM for Translation: Assessing Strategies and Performance》。PaLM是去年2022年发布的一个540亿参数的大语言模型，它在7800亿个标记的大型文本数据集上进行训练，当时在数百个自然语言处理任务中达到了最先进的水平。在这篇论文中，我们进行了首次系统性的大语言模型提示用于机器翻译的研究。我们使用了机器翻译社区的最佳实践，避免测试数据与语言模型训练数据重叠，并与最先进的系统（如WMT评估）进行了比较。我们使用最新的神经机器翻译指标，并且还展示了专家基于的人类评估结果。最后，我们提供了关于提示策略选择的一些建议。提示对大语言模型的翻译性能有重大影响，正如我们在简单的实验中所看到的，其中我们使用了一种一-shot提示方法，并为每个句子提供了两种不同的提示。我们观察到大约516个句子之间的差异超过一个BLEURT点，极端情况下甚至可以达到40个BLEURT点。因此，选择一个好的提示策略非常重要。在我们的实验中，我们选择了五-shot提示策略，其中我们将每个句子标记为其原始语言。在这个例子中，我们将德语句子（源句子）标记为德语冒号，英语翻译标记为英语冒号。我们发现实际提示的形式对几个短提示的情况几乎没有影响，对于零和一-shot提示来说尤其如此。然而，在我们的案例中，当进行五-shot提示时，形式几乎没有任何区别。最重要的因素是示例的质量。我们的实验结果显示，示例的质量比提示形式更重要。特别是，我们将从WMT评估中使用的训练数据与开发数据进行比较。开发数据比训练数据更经过精心筛选，质量更高，但噪音更多。结果表明，使用开发数据可以获得更好的性能。然而，专门的最先进的系统仍然在PaLM翻译中占有显著的优势。在我们的案例中，我们选择使用Google Translate进行评估。我们从人类评估中获得的见解来自MQM框架，显示PaLM的流畅性与最先进的系统相当，但主要差异在于准确性。最常见的错误是省略错误，看起来PaLM倾向于通过省略源句子的一部分来生产听起来更好的翻译。然而，“风格/生硬”类别中的PaLM得分低于最先进的系统，这进一步表明PaLM提供了流畅的输出，但仍存在一些准确性的问题。这就是全部内容。有关更多详细信息，请参加完整的论文演示。谢谢大家。</sample>
    <sample id="240">好的，我明白了。现在让我们开始吧！</sample>
    <sample id="241">Ethan在会议上介绍了他们团队与Georgia Tech合作的论文《人类参与早期错误信息检测：COVID-19治疗案例研究》。该论文探讨了自动检测社交媒体平台上的错误信息的方法，指出这些方法通常存在两个主要问题：一是评估方法不切实际，二是缺乏对人类内容审核员的重视。为了解决这些问题，他们提出了一种基于人类反馈的评估框架，旨在创建一个从原始推文到用于人类审核的可操作输出的端到端系统。他们的系统包括两个主要组件：一个是检测误导性声明的模块，另一个是政策违规验证模块。通过实证研究，他们发现该系统在早期检测和政策违规验证方面具有较高的准确性和效率。</sample>
    <sample id="242">对话系统的常用评估方法是通过人类评估者来选择哪段对话更好，或者对对话进行评分。</sample>
    <sample id="243">这篇论文有5位作者。</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，需要背景知识来确定代词“他”所指的实体。具体来说，需要以下背景知识：1. “法官在法庭上决定案件。”2. “法官寻求政府的选举席位。”这些背景知识有助于理解上下文中提到的实体之间的关系，并正确地将代词“他”与适当的实体联系起来。</sample>
    <sample id="245">这篇论文介绍了在亚马逊机械师（MTurk）平台上使用预任务筛选来寻找高一致性的标注员的方法。作者提出了一种两步筛选流程，包括资格任务和耐力任务。资格任务测试标注员评估多个维度的能力，而耐力任务测试其处理大量工作负载的能力。作者发现，通过这种筛选流程，可以找到具有高一致性的标注员，他们的表现与专家相当。此外，作者还比较了不同筛选方法的效果，并分析了不同标注源之间的正确性。结果表明，预任务筛选可以避免资源浪费，并且可以以较低的成本实现高一致性。</sample>
    <sample id="246">代码是公开的，可以在GitHub上获取。</sample>
    <sample id="247">Jiho Kim介绍了KAIST AI团队开发的新任务——基于知识图谱的事实验证。他们提出了一种新的方法，通过推理知识图谱来验证自然语言中的事实。Kim提到，现有的事实验证数据集通常使用维基百科文本或表格作为证据，但没有利用知识图谱作为证据。因此，他们创建了一个名为FactKG的新数据集，用于事实验证。该数据集使用DBpedia作为知识图谱，并包含两种类型的声明：书面和口语。数据集包括五种推理类型：一跳、连结、存在、多跳和否定。Kim展示了如何使用这些推理类型来验证声明，并解释了如何使用不同的方法来处理口语声明。最后，Kim介绍了他们的基线模型，包括仅使用声明和使用GEAR模型的模型，并展示了它们在验证声明时的表现。</sample>
    <sample id="248">NLPositionality的注释者在各个人口统计学特征（即国家/地区、性别等）方面并不均衡。</sample>
    <sample id="249">在可接受的域中扰乱句子的方法是通过添加噪声来实现。具体来说，可以对输入句子进行一系列的扰动，包括但不限于以下几种：

1. **替换词**：将句子中的某些词语替换成其他词语。
2. **插入词**：在句子中插入一些无关的词语。
3. **删除词**：从句子中删除一些词语。
4. **改变词序**：打乱句子中词语的顺序。

这些扰动操作的目的是为了保持句子的基本结构不变，同时引入一定的随机性。通过这种方式，可以观察到模型对不同类型的扰动的敏感性，从而更好地理解模型在不同上下文长度下的表现。</sample>
    <sample id="250">维度评估意味着对对话质量的各个方面进行评估。</sample>
    <sample id="251">这篇论文的作者所属机构是University of Science and Technology of China。</sample>
    <sample id="252">U-CREAT是一种新的案例检索方法，利用事件提取技术从法律文档中识别相关事件。该方法包括三个步骤：预处理、依赖解析和后处理。它使用交互矩阵来比较查询文档和候选文档中的事件，并根据这些相似性进行排名。U-CREAT在印度和加拿大法律系统中表现出色，无需特定的法律或人口统计学调整。它优于现有的基于计数和变压器的模型，并且具有较低的推理时间和更高的F1分数。U-CREAT为法律领域中的案例检索任务提供了一种有效的方法。</sample>
    <sample id="253">Mario Ezra Aragón介绍了一项名为“DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media”的研究工作。这项工作由来自墨西哥和西班牙的研究人员合作完成，旨在通过自动分析社交媒体用户发布的内容来检测心理健康障碍。研究发现，使用双域适应和引导掩码的方法可以有效地捕捉社交媒体互动中的心理健康迹象。该方法在平衡发现用户并正确标记它们方面表现良好，并且在临床数据和不同词汇资源的应用上具有潜力。</sample>
    <sample id="254">The research work titled "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction" presents a framework to improve the label quality of distant supervision (DS) data in document-level relation extraction. The authors address the challenge of noise induction by false-positive pseudo labels, which can lead to both extra false relations and loss of correct ones. To mitigate this issue, they propose an uncertainty-guided label denoising approach that includes: 1. Training a pre-denoising DocRE model with DS and human-annotated data to generate pseudo labels; 2. Introducing uncertainty estimation to determine the trustworthiness of these predictions; 3. Proposing instance-level uncertainty estimation for overlapping relations; 4. Designing a re-labeling strategy using dynamic class uncertainty thresholds; 5. Implementing a multi-phase training strategy to iteratively enhance the performance of the DocRE model. The proposed framework is evaluated on public datasets, demonstrating significant improvements over previous baselines.</sample>
    <sample id="255">提示的形式在几个短提示的情况下很重要。</sample>
    <sample id="257">作者评估了四个最先进的聊天模型。</sample>
    <sample id="258">Chiang Cheng-Han在ACL会议上介绍了他们的新工作“Can Large Language Models Be an Alternative to Human Evaluation?”。他们提出使用大型语言模型来评估自然语言处理中的文本质量。通过给大型语言模型提供指示和样本，他们希望模型能够根据指示提供评分。Chiang Cheng-Han提到，虽然使用大型语言模型进行评估不是新颖的想法，但他们的想法在提交ACL时是新颖的。他们的动机是探索大型语言模型是否可以替代人类评估，同时避免其缺点。他们使用大型语言模型对由GPT-2或人类编写的故事情节进行评分，并基于语法、连贯性、可接受性和相关性等四个属性进行评分。实验结果表明，英语教师更喜欢人类编写的故事情节，但一些较小的大型语言模型没有显示出明显偏好。然而，Davinci和ChatGPT显示了对人类编写文本的明显偏好。Chiang Cheng-Han总结说，有一些大型语言模型可以作为人类评估的替代方法。</sample>
    <sample id="259">这段内容介绍了Penn State University的研究工作，名为“XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations”。该研究旨在解决现有跨语言语义解析模型在任务和应用覆盖范围上的局限性。现有的模型通常针对特定的语言或意义表示，而缺乏对中文等语言的覆盖以及对Lambda计算等意义表示的支持。

为了克服这些限制，研究人员提出了XSemPLR，这是一个统一的数据集，用于跨语言语义解析，包含9个数据集、5种语义解析任务、8种意义表示和15种语言家族中的22种语言。为了更好地评估这个基准，他们考虑了六种不同的训练和评估设置：翻译测试、单语言测试、少量样本训练、多语言模型、零样本转移和少量样本转移。

通过分析不同类型的多语言语言模型（包括编码器-解码器模型和编码器-指针模型），研究人员发现，编码器-解码器模型在所有九个数据集中表现最佳。此外，他们还比较了在混合语言下训练的模型与仅在单一语言下训练的模型之间的性能差异，并发现混合语言训练可以显著提高性能，但英语在某些情况下可能会出现性能下降。

最后，研究人员展示了跨语言性能差距，表明通过少量样本转移可以显著缩小零样本转移的性能差距。总体而言，这项研究构建了一个统一的基准，用于跨语言语义解析，并提供了对不同多语言语言模型的全面评估。</sample>
    <sample id="260">这篇论文有三位作者。</sample>
    <sample id="261">优秀的规划器应该能够生成合理的、符合约束的脚本。</sample>
    <sample id="262">这篇论文有三位作者。</sample>
    <sample id="263">In this work, we investigate the label bias problems of in-context learning. We start with a typology of label biases and based on which we are able to identify a new important source of biases in in-context learning. And finally, we propose the calibration method that is able to significantly improve the performance of in-context learning of those large language models. Check our paper for more details, and thank you.</sample>
    <sample id="264">The speaker introduces their paper titled "TAVT: Towards Transferable Audio-Visual Text Generation" and explains the challenges of multimodal text generation tasks like audio-visual text generation due to varying construction conditions in different domains. They propose a novel task named Transferable Audio-Visual Text Generation, which addresses multi-modal domain shifts such as visual style and audio energy. The main challenge is aligning visual concepts across domains with limited labeled data.

To overcome this constraint, they present an overall framework consisting of three components:
1. An audio-visual meta-mapper network that maps different visual concepts into a unified auditory semantic space.
2. A transformer-based encoder and generator model that uses learnable tokens called visual prefix for each audio cluster.
3. Counterfactual contrastive learning (DCLL) to optimize visual-textual alignment without relying on random negative samples.

The experiments demonstrate that TAVT outperforms other models by a large margin on both cross-datasets and cross-domain settings, even achieving good performance on low-resource domains with only a few labeled data.</sample>
    <sample id="265">演讲者的名字是Vasudha。</sample>
    <sample id="266">这篇论文的作者所属机构是华威大学。</sample>
    <sample id="268">根据所给的英文内容，PaLM 最常见的错误是 omission errors。</sample>
    <sample id="269">你好，我是詹姆斯·芬奇。我是莎拉·芬奇。今天我们将向您介绍ABC-Eval，一种新的维度方法来评估对话式人工智能。这项工作是由埃默里大学由乔伊·金教授领导的埃默里自然语言处理实验室与亚马逊Alexa AI合作完成的。假设你已经开发了一个对话模型，并且你想看看它与当前最先进的模型相比如何。常见的做法是使用人类评估，例如请人类评判者选择哪个对话更好，或者给对话打分。这些方法在提供整体对话质量评估方面效果很好，但对话质量有很多方面。因此，你可能想要评估对话质量的多个维度，以了解模型在更精细的层次上的优势和劣势。一种方法是简单地要求人类评判者对对话的多个方面进行评估，例如使用现有的比较或李克特量表方法对模型响应的相关性进行评估。然而，我们相信有一种更精确可靠的方法来进行维度化的对话评估。我们的方法试图通过明确标注每个聊天模型响应是否表达某些行为来减少人类评估的主观性，例如响应不相关的信息或自相矛盾。我们称之为聊天行为标注或ABC-Eval的简称。我们开发了这种方法来全面覆盖最近文献中建议影响聊天质量的各种主题错误。ABC-Eval能够测量聊天模型将犯各种主题错误的概率。例如，ABC-Eval测量模型忽略其伙伴的次数、说无关信息、自相矛盾或其伙伴、胡言乱语不正确的事实以及违反常识知识的次数，以及模型成功或失败地表现出同理心的次数。为了确定哪种评估方法最有效，我们选择了四个最先进的聊天模型，并使用ABC-Eval对每种模型的100个人-机器人对话进行了评估。作为比较，我们还使用三种现有方法对这些对话进行了评估：按回合评分的李克特等级、按对话评分的李克特等级和对话级的二元比较。对于每种现有方法，我们收集了八个最常见的测量对话方面的评估，因为这是评估聊天模型沿多个维度的标准实践。在分析这些评估结果后，我们发现ABC-Eval行为标签总体上比现有方法收集的标签更可靠，如100个双重标注对话的注释员一致性所示。此外，ABC-Eval标签比现有方法产生的指标更准确地预测对话质量，如这个简单的线性回归分析所示。例如，你可以看到衡量模型响应中自我和伙伴矛盾的比例解释了5%和10%的对话质量，而平均的李克特一致性分数只解释了4%或更少。最后，我们检查了每个评估指标是否捕捉到聊天质量的独特方面，使用逐步线性回归。你可以看到所有ABC-Eval指标的组合解释了超过25%的对话质量，随着一个指标一个地被移除，大多数都会失去相当多的质量信息。另一方面，所有回合级李克特指标的组合解释了远少于质量的信息。可靠的、有信息量的、独特的ABC-Eval指标使我们能够以以前的方法无法实现的更高分辨率来评估对话式人工智能。你可以在我们的实验结果中看到几个挑战仍然存在，并且已经被精确量化。例如，我们在测试的机器人中，约有20%的响应有常识错误。约有15%的响应包含无关信息，约有10%的响应自相矛盾或其伙伴。随着领域的发展速度如此之快，新模型发布时，这些错误率可能会有所下降。然而，这正是追求可靠和精确的评估指标的原因。我们希望ABC-Eval能被领域内的其他人利用，作为有意义的一步。我们期待看到未来几个月和几年中对话式人工智能会如何发展。谢谢观看。</sample>
    <sample id="270">这篇论文的作者所属机构是Emory University。</sample>
    <sample id="271">在本文中，CFT代表“Fine-Tuning”。</sample>
    <sample id="272">这篇论文有7位作者。</sample>
    <sample id="273">你好，我的名字是凯奥·因和，我将要介绍我们合作完成的论文《翻译何时需要语境？一种基于数据的多语言探索》。这项工作是由凯奥·因、帕特里克·芬南德斯、艾米莉·刘、安德烈·F·T·马丁斯和格雷厄姆·纽比格共同完成的。许多翻译都依赖于上下文。例如，在这个句子中，“mole”会是什么意思？如果上一个句子是“如果部长们发现事情开始变得危险”，那么“mole”指的是间谍。但如果上一个句子是“这可能有什么严重的问题吗，医生？”那么“mole”指的是胎记。因此，上下文的变化会导致单词的含义发生变化，从而导致其翻译也发生变化。然而，评估模型如何处理这些情况是非常困难的。首先，因为只有很小一部分翻译依赖于上下文，因此基于语料库的指标如BLEU无法捕捉到这些翻译。有些人建议针对上下文相关的翻译进行有目标的评估，但这些资源仅支持有限类型的上下文相关翻译，并且仅限于有限的语言，因为它们通常依赖于领域知识和人工编译。在本文中，我们试图回答这两个问题。首先，什么时候翻译需要上下文？其次，模型如何处理这些情况？为了回答第一个问题，我们首先测量了翻译过程中单词对上下文的依赖程度。在之前的工作中，我们引入了CXMI作为衡量机器翻译模型上下文使用的一种方法。这是通过测量上下文C提供的关于目标Y的信息来实现的，给定源X。我们可以将CXMI视为提供上下文给模型的信息量。在这项工作中，我们将CXMI扩展为点状CXMI，可以在句子级别或单词级别进行上下文使用度量。我们可以将具有高P-CXMI的单词视为需要上下文进行翻译的单词。然后，我们分析具有高P-CXMI的单词以查找模式，这些模式可以解释为上下文依赖翻译的模式。为此，我们在TED演讲的翻译文本集上进行了分析，该文本集已从英语翻译成14种不同的语言。我们对分析进行了三个级别的操作。首先，我们查看部分词性标记，这些标记具有相对较高的平均P-CXMI。这使我们能够找到阿拉伯语中的双数代词，因为英语中没有双数代词，因此您需要上下文来确定代词是否为双数。同样，我们发现某些语言在翻译动词形式时也需要上下文。然后，我们查看所有出现次数的词汇项目，具有高P-CXMI的平均值。这帮助我们识别出中文中需要上下文才能正确翻译专有名词的情况。同样，我们发现上下文对于翻译适当的形式也是必要的。最后，我们查看每个单独的标记，具有高P-CXMI。这使我们能够识别无法由单词本身捕获的现象，而是表达在句法结构中，例如省略的解决。然后，我们使用我们的发现来设计一个文档级翻译的基准。对于我们识别的五种话语现象中的每一个，我们创建标记器自动识别与现象相关的单词。我们称这个标记器为多语言话语感知标记器（MuDA）。然后，我们使用MuDA标记器，通过在想要用于评估的平行语料库上应用标记器来应用标记器。然后，我们使用我们的基准以及其它指标来评估不同模型在文档级机器翻译上的表现。首先，当使用基于语料库的指标时：对于BLEU，我们发现上下文无关模型的表现最好。但是，如果使用COMET，则上下文意识模型表现最佳。如果使用单词f-测度，则没有上下文的模型和带有上下文的模型在性能上相当。这再次表明，仅使用基于语料库的指标很难确定文档级翻译系统的最佳表现。然后，我们使用MuDA基准来评估模型。我们发现上下文意识模型在形式性和连贯性等某些话语现象方面比不使用上下文的模型表现更好。但是，这些模型在其他现象如省略、代词和动词形式方面并没有比不使用上下文的模型表现得更好。这表明我们需要在文档级翻译中看到更多的进步。我们还将不同的商业系统进行比较，我们的基准显示DeepL通常比Google Translate在文档级翻译中表现更好。总之，我们进行了一项基于数据的跨14种语言的分析，以确定翻译何时需要上下文，并使用我们的发现构建了一个文档级机器翻译的基准，这可以帮助我们了解模型在哪些话语现象上表现良好或不好，以及哪些翻译系统在文档级翻译中表现良好。谢谢大家的聆听。祝你在多伦多好运！</sample>
    <sample id="274">演讲者的名字是Yusen Zhang。</sample>
    <sample id="276">Ananya和Vignesh介绍了他们的研究，该研究关注印度语言的翻译评估指标。他们使用了来自Flores数据集的200个句子，并通过七个不同的翻译模型或API生成每个源语言的多个候选翻译。然后，他们使用双语专家注释者对这些翻译进行评估，标记错误类型、严重程度以及总体分数。他们分析了各种评估指标与人类评分的相关性，并发现COMET-metric变体在所有语言中具有最高的相关性。他们还微调了最佳性能的指标，即IndicCOMET，以进一步提高其有效性。最后，他们在ACES翻译准确性挑战集上测试了IndicCOMET的鲁棒性，并观察到它比COMET基线更具有鲁棒性。</sample>
    <sample id="277">没有</sample>
    <sample id="278">作者描述“显性词汇”(marked words) 方法是通过利用社会语言学概念“显性”，即存在一个默认的未标记状态，任何与这个默认状态不同的群体都是语言上被标记的。例如，“战士”通常与男性相关联，当描述女性战士时，他们会使用“女性战士”来标记术语。主导群体在社会和语言上都是未标记的，而边缘化群体通常是标记的。在他们的方法中，他们首先确定未标记和标记的群体，并然后使用“Fightin' Words”方法比较人物，该方法使用加权对数比值来区分每个标记群体的顶级单词。</sample>
    <sample id="279">这篇论文的作者所属机构是华盛顿大学。</sample>
    <sample id="280">Shi Tao presents his work "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations". He introduces the task of emotion regulation in conversations, which aims to predict the emotion label of each utterance in a dialogue. The challenge lies in exploiting the complementarity of multimodal information and addressing difficulties in classifying minority emotions and semantically similar emotions.

To tackle these challenges, Shi Tao proposes MultiEMO, consisting of four key components:
1. VisExtNet: A novel visual feature extractor that captures facial expressions without encoding redundant scene-related information.
2. MultiAttn: A multimodal fusion network with three stages (MultiAttn-text, MultiAttn-audio, and MultiAttn-visual) using bidirectional multi-head cross-attention layers.
3. Sample-Weighted Focal Contrastive Loss: Assigns higher importance to minority classes and ensures sample pairs are mutually exclusive.

Experimental results show MultiEMO outperforms existing methods on MELD and IEMOCAP datasets, particularly improving performance on minority and semantically similar emotions. However, limitations include not distinguishing between speakers and irrelevant people, requiring large batch sizes for SWFC loss, and still performing worse than majority classes in minority emotions.</sample>
    <sample id="281">This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate "mole" in this sentence? Well, if the previous sentence was "Things could start to get dangerous if the ministers find out", then "mole" refers to a spy. But if the previous sentence was "Could it be anything serious, doctor?", then "mole" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y, given the source X. You can think of CXMI as the information gained from giving context to the model. In this work, we extend CXMI to Pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation. Now we analyze words with high P-CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at part-of-speech tags that have high mean P-CXMI. And this allows us to find, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that's rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document. And similarly, we find that context is important to translate in the right formality. And finally, we look at different individual tokens that have high P-CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself, but that's rather expressed in the sentence structure, such as ellipses resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon.</sample>
    <sample id="282">The presentation introduces a new work titled "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing" at ACL 2023. The research addresses the challenge of non-parallel text style transfer, focusing on imitating author styles at the discourse level in long texts. It highlights two main challenges: imitating linguistic choices related to narrative techniques and maintaining content association between different writing topics.

To overcome these issues, the study proposes a model called StoryTrans. This model learns discourse representations from source texts and combines them with learnable style embeddings for generating target-style texts. A novel training objective is designed to reduce stylistic features derived from different texts while enhancing content preservation through a disentanglement process involving sentence-level imitation.

The proposed framework includes an advisory training stage using self-reconstruction loss, disentanglement loss, sentence order loss, and style classifier loss. In the second stage, unrelated sentences are removed, and correct style-specific contents are inserted into masked tokens. Experimental results demonstrate that StoryTrans outperforms strong baselines by effectively transferring fairytales or everyday stories to specific author styles, as evidenced by both automatic evaluation metrics and manual evaluations. Style visualization further confirms alignment with golden test examples in terms of style feature space.</sample>
    <sample id="283">第一个提到的对称依存关系结构的名称是“Lisa, Bart和Maggie”。</sample>
    <sample id="284">The speaker introduces a novel fuzzy span mechanism for enhancing universal information extraction (UIE). The current span-based UIE models rely on precise boundary positions of annotated spans, which can lead to ambiguity in labeling. To address this issue, the proposed FSUIE model uses a fuzzy span loss that allows the learned span boundaries to be more flexible and less reliant on exact positions. This approach helps alleviate overreliance on specific annotations.

Furthermore, the speaker highlights an inconsistency between transformer feature extraction and traditional UIE methods. Basic transformers focus on global features but ignore the hypothesis that spans have limited lengths. Therefore, they propose adaptive attention mechanisms within a specific range to better capture target boundaries as continuous distributions with correct probabilities.

To implement these ideas, the FSUIE model incorporates a sampling function to convert continuous boundary distributions into discrete values for calculating fuzzy span losses. It also includes a fuzzy span attention layer as a mask function to guide the decision process without affecting text encoding capabilities significantly.

Experimental results demonstrate that FSUIE outperforms existing UIE models across various tasks such as named entity recognition, relationship extraction, and aspect sentiment triplet extraction. These improvements are attributed to the enhanced ability to utilize annotation information efficiently through FSL and adaptively adjust attention spans using FSA.

In summary, the presentation outlines a comprehensive framework for improving UIE by introducing a fuzzy span mechanism and adapting attention distribution dynamically based on context length constraints.</sample>
    <sample id="285">The speaker introduces their work on "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework." They discuss the importance of addressing factual errors in dialogue summarization and propose a new taxonomy of factual errors. The evaluation framework they introduce is based on ERRANT, an evaluation metric for grammar error correction. Their findings suggest that training FEC models with reference summaries from dialogue summarization datasets yields better results than unreliable factuality metrics. Combining human-annotated data with synthetic data can improve FEC model performance, but current models struggle to correct certain types of factual errors.</sample>
    <sample id="286">演讲者的名字是James Finch和Sarah Finch。</sample>
    <sample id="287">这篇论文有四位作者。</sample>
    <sample id="288">BLiMP和SyntaxGym数据集可用于测试句法现象。</sample>
    <sample id="290">WSL stands for Weakly Supervised Learning.</sample>
    <sample id="291">为了评估DrBERT模型，研究人员进行了11个生物医学和临床下游任务的测试。这些任务包括命名实体识别、分类、词性标注和问答等。此外，他们还使用了六个基线模型进行比较，包括CamemBERT OSCAR 138 GB、CamemBERT OSCAR 4 GB、CamemBERT CCNET 4 GB、PubMedBERT、BioBERT和ClinicalBERT。</sample>
    <sample id="294">CamemBERT 最初是在 Wikipedia 和 BookCorpus 数据上训练的。</sample>
    <sample id="295">演讲者的名字是Adam Przepiórkowski。</sample>
    <sample id="296">Valerio Basile presents a collaborative work between the University of Turin and Amazon Alexa, focusing on natural language understanding through supervised machine learning. The project involves developing models to handle complex linguistic phenomena like irony detection. To achieve this, they created the EPIC (English Perspectivist Irony Corpus), collecting data from social media platforms such as Reddit and Twitter over 1½ years across five English varieties.

The dataset was crowdsourced using Prolific, with each annotator receiving about 200 short conversations for annotation. Annotators were divided into groups based on gender, age, nationality, etc., leading to varying inter-annotator agreement levels depicted in violin plots. Despite these differences, perspective-aware models showed higher confidence compared to gold standard aggregated models when trained by fine-tuning pre-trained language models on annotated splits.

Further analysis revealed that generational proximity among annotators led to more disagreement regarding their perception of irony, while geographical distribution also influenced response variations significantly. This study highlights the complexities involved in training models for nuanced linguistic tasks and underscores the need for diverse perspectives in annotation processes.</sample>
    <sample id="297">这段内容介绍了关于“从狗哨到扩音器：揭示带有语言模型的编码修辞”的工作。它以几年前参议员乔希·霍尔利的一次演讲为例，他在抱怨所谓的“精英实验”和“世界主义”，一些人可能会将其解释为对犹太人的攻击，而“世界主义”就是一个狗哨。这个例子展示了狗哨如何通过向内群体传达额外的含义来工作，同时向外群体传达不同的含义，从而影响政治影响力和说服力。然而，研究狗哨是具有挑战性的，因为它们最成功时，外群体并不知道它们。为了应对这一挑战，研究人员开发了一个包含超过340个术语和符号的狗哨词汇表，这些术语和符号主要与种族主义、跨性别歧视和反犹太主义有关，并且在论文中进行了详细描述。他们还进行了一项历史美国政治演讲的案例研究，发现狗哨的频率与共和党南方策略的时间模式相吻合，该策略在民权时代变得更加频繁。此外，他们评估了语言模型（如GPT-3）识别狗哨的能力以及自动毒性检测工具（如Prospective API）如何处理狗哨，以展示它们如何绕过在线内容审查。</sample>
    <sample id="298">为了得出时间漂移是性能下降的主要原因的结论，研究人员进行了一个实验：重新训练或继续预训练一些模型，使用更近期的数据。他们发现，随着时间差距的增大，性能会下降。这证实了他们的假设，即性能下降的主要原因是时间漂移。</sample>
    <sample id="299">The speaker, Michalis Korakakis from the University of Cambridge, introduces a study on improving NLI (Natural Language Inference) model robustness. He explains that while these models have achieved high accuracy in recent years, they can be vulnerable to out-of-distribution samples due to learning shortcuts or spurious correlations during training. To address this issue, he proposes a minimax training method where an auxiliary model tries to maximize the loss for hard examples, and the main learner model aims to minimize it. This approach encourages the learner to focus more on under-represented "hard" instances which are crucial for good generalization performance outside the distribution. The proposed method is evaluated across various datasets like MNLI, FEVER, QQP, HANS Symmetric, and PAWS, showing consistent improvements over baseline methods without requiring domain-specific knowledge about the type of shortcuts present in the dataset.</sample>
    <sample id="300">Belinda介绍了一项新的任务，即交互式语音识别。这项任务允许用户通过语音命令和编辑来自然地与计算机交互。她解释了交互式语音识别的四个步骤：语音识别、语音转文本、语音命令提取和执行以及最终文档状态的生成。为了收集数据，Belinda设计了一个注释界面，并使用该界面创建了一个数据集。她还介绍了用于执行每个步骤的基线系统，并讨论了该系统在准确性和效率之间的权衡。最后，Belinda提供了代码和论文的链接，以促进未来的研究工作。</sample>
    <sample id="302">为了确保输出序列中的词元排列正确，我们需要对它们进行排列。在第一阶段，我们为每个输入标记分配一个未排序的多元素集，这些标记将在输出中出现。然而，在第一阶段，标记的顺序并不重要。在第二阶段，我们需要预测一个排列来将标记放入正确的顺序。这一步骤对于确保输出序列中的词元排列正确是必要的。</sample>
    <sample id="303">作者建议模型所有者应提高偏见缓解方法的透明度，因为这可以让我们了解这些积极的刻板印象是否是由于某种奇怪的过度价值对齐或可能的其他反刻板印象方法导致的。没有更多的透明度，我们无法做出假设或进一步研究。</sample>
    <sample id="304">最小对不可接受输入是通过从同一数据集中的不同部分或完全不同的领域中选择句子来创建的。这将告诉我们模型的接受性判断是否受到上下文的影响，即上下文是否来自数据集的不同部分或完全无关。</sample>
    <sample id="305">Dawei首先介绍了弱监督学习的概念，即在没有人工标注的情况下使用弱标签源进行标注。然后，他讨论了弱监督学习中常见的一个假设：使用弱标签数据训练模型，并在干净的测试集上取得高精度。然而，这种假设忽略了需要额外的手动注释来验证模型性能的问题。Dawei提出了三个研究问题：1.是否需要干净的验证数据，还是可以使用嘈杂的验证数据？2.如果需要干净的数据，那么需要多少干净的样本？3.是否应该仅使用干净的样本进行验证，还是有其他更好的利用方式？Dawei的研究发现，最近的WSL方法确实需要干净的验证数据才能正常工作。此外，增加干净验证样本的数量有助于WSL方法获得更好的性能。最后，他建议未来的工作应报告模型选择标准、比较WSL方法与基于干净样本的少样本学习方法，并考虑连续微调作为简单而强大的基准。</sample>
    <sample id="306">Sebastian Schuster和Najoung Kim在ACL会议上介绍了他们的研究，探讨了大型语言模型在实体状态跟踪方面的表现。他们设计了一个任务，涉及盒子和物体，以评估模型的实体状态跟踪能力。结果显示，只有GPT-3.5模型能够表现出非平凡的跟踪行为，而其他模型则无法做到这一点。此外，他们发现，GPT-3.5模型在预训练过程中接触了大量的代码，这可能是其表现的原因。然而，他们也指出，较小的模型如T5-base可以通过直接微调来学习实体跟踪，但随机初始化的模型即使得到直接监督也无法完成任务。</sample>
    <sample id="307">作者使用了命名实体识别、分类、词性标注和问答等评估指标。</sample>
    <sample id="308">Jenny, a Carnegie Mellon University PhD student, presented her work on NLPositionality, which characterizes design biases in datasets and models. The study aimed to understand how NLP researchers' positionality influences their decisions and outcomes. By comparing diverse annotators with existing datasets and models using Lab in the Wild, Jenny found that there is indeed positionality in NLP technologies. They are most aligned with English-speaking countries and people with higher education levels but less so with non-binary individuals. Recommendations include documenting all relevant design choices during research and incorporating perspectivism into NLP studies. Building specialized datasets and models for specific communities was also suggested as an approach towards inclusive NLP.</sample>
    <sample id="309">使用了ABC-Eval行为标签来衡量注释者之间的一致性。</sample>
    <sample id="310">在不可接受和可接受查询中，选择一个完全无关的领域来添加句子。</sample>
    <sample id="311">这篇论文的作者所属机构是德国慕尼黑工业大学。</sample>
    <sample id="312">MultiInstruct 是第一个大规模的多模态指令调优基准，它包括了 62 个多样化的多模态任务，涵盖了 10 个广泛的类别。这些任务是从 21 个现有的开源数据集中衍生出来的，并且每个任务都配备了五个由专家撰写的指令。此外，MultiInstruct 还解决了 NLP 和多模态领域之间存在显著差异的问题，即语言-only 指令任务比多模态指令任务更为丰富。</sample>
    <sample id="313">这篇论文有两位作者。</sample>
    <sample id="314">二进制协调的定义是：两个词或短语之间的关系，其中它们一起形成一个整体。在二进制协调中，两个元素是平等的，并且它们一起创建一个单一的意义单位。这种关系通常通过连词（如“和”、“与”、“以及”）来表示。二进制协调的例子包括“猫和狗”、“苹果和香蕉”或“快乐和悲伤”。在这些例子中，每个元素都为整体的意义做出了贡献，并且它们一起创建了一个连贯的概念。</sample>
    <sample id="315">提示语的平均长度为10个单词。</sample>
    <sample id="316">这些发现表明，较小的 T5 模型在经过 CoScript 数据集的特定训练后，可以生成比大多数大型语言模型更好的脚本。这表明，当使用适当的训练数据时，较小的模型可以超越较大的模型。</sample>
    <sample id="317">Peng Li来自复旦大学，介绍了他们的工作“CodeIE：大型代码生成模型是更好的信息提取器”。信息抽取是自然语言处理中的一个经典任务，它涉及从非结构化文本中提取结构化的信息。常见的信息抽取任务包括命名实体识别和关系抽取。Peng Li讨论了使用预训练的语言模型如T5和GPT-3的先前信息抽取模型的问题，这些模型在预训练阶段以文本到文本的方式运行，但在推理阶段将结构化的输出线性化为计划序列。这使得模型难以生成正确的结构，需要大量的结构化训练数据和特殊的解码策略来缓解这一问题。为了解决输出不匹配的问题，他们提出了CodeIE，将文本到结构化信息抽取任务转换为结构到结构的代码生成任务，并使用Codex等代码大型语言模型执行。这种做法允许在输入阶段轻松地将文本转换为结构化的格式，并确保输出阶段的对齐结构。他们评估了这种方法在三个识别数据集和四个关系抽取数据集上的性能。他们比较了两种类型的提示，一种使用传统的文本样式提示，另一种使用之前描述的代码样式提示。在1到少数几轮的情况下，他们发现使用代码语言模型和代码格式提示的提议方法显著且一致地优于传统基准模型，例如UIE和自然语言大型语言模型，如GPT-3模型。</sample>
    <sample id="318">好的，以下是翻译结果：

大家好，我是Yanis Labrak，我们将向您介绍我们的工作“DrBERT：用于法语生物医学和临床领域的稳健预训练模型”。首先，我们谈谈医疗保健中的语言建模。然后，我们将介绍我们文章的主要贡献。我们介绍了第一个法语生物医学模型DrBERT，它基于RoBERTa，并在NACHOS上进行训练，NACHOS是一个从网络中爬取的医疗数据集。我们还介绍了使用多个预训练设置和数据源的模型比较。然后，我们展示了我们在11个法语的生物医学和临床下游任务上的结果。最后，我们总结了实验，并提供了如何访问这些模型的更多细节。自2018年发布以来，BERT已成为自然语言处理任务中最有效的方法之一，并与Word2vec、fastText或更早的静态和上下文方法相比，取得了巨大的性能提升。自那时以来，这个模型已经被适应到许多其他语言，如法语的CamemBERT，以及在生物医学领域的PubMedBERT和BioBERT，以及在临床领域的ClinicalBERT，但主要是英语。对于其他语言的专用模型很少见，通常是通过持续预训练来实现的，由于缺乏特定领域的数据。然而，在法语中还没有开源的生物医学模型。因此，我们问自己，最适合广泛使用的数据源是什么？那些从网络中爬取的数据是临床数据的良好替代品吗？为了回答这个问题，我们比较了DrBERT与我们的ChuBERT模型，该模型基于匿名数据，来自南特大学医院的数据仓库。然后，我们问自己，我们需要多少数据才能训练一个专门的法语模型？是4GB、8GB还是更多？为了回答这个问题，我们首先训练并比较了四个从头开始的模型：DrBERT的第一个版本，有7GB的NACHOS；第二个版本，有4GB的NACHOS子集；ChuBERT的第一个版本，这是一个临床模型，包含4GB的句子，来自临床记录；以及ChuBERT的最终版本，混合了4GB的NACHOS子集和4GB的临床记录。此外，我们还引入了三个基于CamemBERT权重和分词器的模型，分别在4GB的NACHOS子集上进行训练，以及一个基于英文生物医学模型PubMedBERT的模型，在4GB的NACHOS子集上进行训练。总共有七个模型。为了评估我们的七个模型，我们收集了公共和私有的下游任务数据，例如命名实体识别、分类、词性标注和问答。这些模型被与六个基线模型进行比较：CamemBERT OSCAR 138 GB、CamemBERT OSCAR 4 GB、CamemBERT CCNET 4 GB、PubMedBERT、BioBERT和ClinicalBERT。评估结果显示，模型在具有相同性质的数据任务上表现最佳。然而，我们观察到来自不同来源的数据似乎更具灵活性。我们还观察到，使用更多的数据会带来更好的性能。总体而言，从头开始的预训练似乎在大多数任务上表现更好。然而，我们的实验表明，使用CamemBERT权重和分词器训练的模型在四个GB的NACHOS子集上表现出可比的结果，而不是使用CamemBERT权重和分词器训练的模型，后者在稳定性方面存在问题。最后，正如结论所示，我们的系统在9个11个下游任务中的9个任务上表现更好，并且在所有任务上超过了通用模型，这里是CamemBERT。所有由NACHOS训练的预训练模型都免费提供在Hugging Face上，并遵循MIT许可证，所有训练脚本都在我们的GitHub存储库上。因此，感谢您的演讲，我们期待在多伦多的海报会上进行交流。</sample>
    <sample id="319">论文研究了以下学习策略：1. 从头开始训练2. 持续预训练3. 使用CamemBERT的权重和标记化进行持续预训练4. 使用CamemBERT的权重和标记化进行持续预训练，但使用临床数据5. 使用PubMedBERT的权重和标记化进行持续预训练，但使用NACHOS数据</sample>
    <sample id="320">从图中可以看出，红色最佳拟合线的斜率大于1。这意味着CoNLL-2003上的每一点改进都转化为CoNLL++上超过一点的改进，这表明没有递减收益。因此，在这种情况下，没有观察到适应性过拟合。</sample>
    <sample id="321">为了评估简化质量，我们使用了两个不同的模型：我们对长mBART进行了微调以产生文档级简化，并且我们还对基本的mBART进行了微调以产生句子级简化。</sample>
    <sample id="322">Enrico will be presenting at ACL 23, discussing the question "What does a Text Classifier Learn about Morality?" He begins by defining morality as our internal compass that helps us distinguish right from wrong. Enrico explains that while language models have been approached to understand morality in text, they often treat it as a singular scale between immoral and moral. However, he emphasizes that morality is subjective and varies across individuals.

To address this issue, Enrico introduces the Moral Foundation Theory, which posits five different ways humans perceive morality, similar to how we interpret taste buds on our tongue. Each action or concept can tickle one of these foundations differently based on individual priorities. This theory has already seen application in natural language processing, with recent papers attempting to classify morality in text using language models.

Enrico's presentation aims to delve deeper into what language models learn when classifying morality in text. They apply explainable AI techniques to analyze language models trained for understanding morality across various domains. A key dataset used is the Moral Foundation Twitter Corpus, consisting of tweets collected under seven distinct hashtags like #AllLivesMatter and #BlackLivesMatter.

The study explores whether language models recognize differences in expressing morality within specific domains. For instance, comparing All Lives Matter (ALM) and Black Lives Matter (BLM), two related topics but differing significantly in their moral elements such as subversion. The findings indicate that ALM associates words like overthrow, mayhem, and subversion negatively, whereas BLM encourages subversion more positively.

Enrico concludes his talk by highlighting the importance of recognizing that morality is expressed differently across domains. Using a single model for multiple domains could lead to misunderstandings of morality, underscoring the need for careful consideration in applying language models to diverse ethical contexts.</sample>
    <sample id="323">Yujie Wang's paper titled "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA" addresses the challenge of Commonsense QA, which requires machines to answer questions that rely on common knowledge. The study highlights issues in previous works such as noisy entities during subgraph retrieval and limited interaction between language models and GNNs. To solve these problems, DHLK is proposed, involving a two-stage pruning strategy and KRL to optimize HKG structure and representation. The method uses RoBERTa and Mask Self-Attention to encode and fuse QA contexts and entities while dynamically removing less relevant entities based on attention weights. TransE optimizes entity and relation embeddings within HKG, and RMSA models subgraphs by incorporating relationships into Mask Self-Attention. The final graph embedding combines HKG path information, QA context embedded paths, and QA context embedded results from MLP for answer prediction. Experiments show promising results compared to other LM and HKG methods using external knowledge bases like ConceptNet, WordNet, and Wiktionary.</sample>
    <sample id="324">是的，语言模型确实有不同的政治偏见。根据研究，语言模型在政治倾向上有所不同，它们分布在政治光谱的四个象限中。例如，GPT-4是最自由的语言模型，而GPT系列通常比BART系列及其变体更倾向于社会自由主义。此外，通过进一步预训练语言模型检查点在不同的党派语料库上，可以观察到语言模型的政治偏见也相应地发生了变化。例如，对Reddit的左倾语料库进行进一步预训练后，语言模型显示出明显的自由主义倾向。</sample>
    <sample id="325">好的，我明白了。你介绍了你们的论文《不使用树的多集标记和潜排列实现组成性泛化》。你们的方法是直接在输入和输出之间建立对应关系，而不需要使用树。你们的方法分为两个步骤：首先，为每个输入标记分配一个未排序的多集标记，这些标记将出现在输出中。然后，在第二步中，使用另一个模型预测一个排列来将它们放入正确的顺序。你们引入了一种新的方法来预测排列，它没有对可能的排列施加任何严格的限制，这使得你们的方法非常灵活和表达能力强。你们的实验结果表明，当测试到更深的递归时，你们的方法在COGS基准测试中明显优于其他无树模型。然而，一些其他类型的结构泛化仍然具有挑战性。你们解决了几个技术难题，包括如何处理训练数据中不存在的输入和输出之间的对齐、如何处理多个可能的排列以及如何找到最有可能的排列。你们通过诱导对齐作为训练的一部分来解决第一个问题，并通过一个GPU友好的连续松弛来解决第二个问题，这个松弛允许你们通过解决方案进行反向传播并学习更有可能的排列。</sample>
    <sample id="326">认知失调是指两个信念或行动不一致的情况。例如，一个人可能会说“我知道吸烟会杀死我”，然后在会议后说“我抽了几支烟”。这种信念和行动是不一致的，因此它们之间存在认知失调。</sample>
    <sample id="327">Xiao Xu presents their work "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning" at ACL 2023. The goal is to train an AI system that can understand both image and text, using a two-tower architecture with textual encoder, visual encoder, and cross-modal encoder. ManagerTower improves upon BridgeTower by introducing managers in each cross-modal layer to gather insights from pre-trained unimodal experts at different levels, allowing more effective exploitation of universal semantic knowledge. Results show superior performance on various downstream tasks compared to METER and BridgeTower.</sample>
    <sample id="328">根据所提供的英文内容，GPT-4是最倾向于自由派的语言模型。</sample>
    <sample id="329">The speaker introduces a method for zero-shot video sentence localization, which aims to find relevant segments in long videos based on natural language queries. The traditional approach involves generating pseudo-events and pseudo-queries from the video data but has several drawbacks: 1) Pseudo-queries are too simple; 2) They do not ensure low relevance between non-event-related parts of the video and the query; 3) They ignore label noise during training. To address these issues, they propose Structured Pseudo-Label generation by using an image caption model to generate complex free-form pseudo-queries, measuring relevance with pre-trained models, reducing noisy samples' influence, and refining labels iteratively. Their method outperforms existing methods across various metrics when tested on ActivityNet Captions and Charades-STA datasets.</sample>
    <sample id="330">在主动学习时，累积训练比迭代训练更有效。</sample>
    <sample id="331">演讲者的名字是Sara Papi。</sample>
    <sample id="332">MuDA基准中的数据是从TED演讲的翻译中获得的。</sample>
    <sample id="333">The speaker introduces a work titled "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation" from Nanjing University. They acknowledge collaborators and explain that the goal of neural machine translation (NMT) is to learn generalized representation spaces for diverse scenarios, but current models often induce non-smooth representations due to sparse low-frequency tokens. To address this issue, they propose an approach called kNN-MT, which smooths predictions by querying a datastore containing token representations at each decoding step. However, this method has drawbacks such as time-consuming neighbor retrieval and inflexible datastore updates.

To overcome these issues, the INK framework was developed to inject kNN knowledge into NMT without needing a datastore during inference. The INK training loop consists of two steps: extracting kNN knowledge to guide adapter adjustments and refreshing the datastore asynchronously with updated representations until convergence. Three types of representation are adjusted using KL-divergence: aligning contextualized representation and token embeddings, enriching semantic meanings through alignment between contextualized representations and kNN token embeddings, and addressing sparsely dispersing problems within the same target token's context.

Experimental results demonstrate that the INK system outperforms state-of-the-art kNN-MT systems on the WMT’19 German-English news translation task, achieving higher BLEU scores while requiring less memory space compared to other methods. By varying adapter sizes, it shows consistent performance improvements across different configurations. Overall, the study proposes a novel training framework designed to iteratively refine NMT model representations according to kNN knowledge, leading to enhanced generalization and prediction accuracy.</sample>
    <sample id="335">演讲者的名字是Matthias Lindemann。</sample>
    <sample id="336">Cross-lingual transfer refers to the process of transferring knowledge or skills from one language to another. In the context of this presentation, it involves training a model on data in one language and then using that trained model to make predictions in other languages. The goal is to leverage the learned patterns and representations in the source language to improve performance in the target language without needing additional labeled data specifically for the target language. This approach can be particularly useful when there is limited availability of annotated data in the target language but abundant resources in the source language.</sample>
    <sample id="337">The research presented in this speech focuses on handling out-of-vocabulary (OOV) words, which are challenging to represent but crucial for the performance of embedding-based models. The approach involves observing word formation and associating OOV words with relevant ones to infer their meanings. A Word Relationship Graph is introduced to imitate lexical rules of word formation and association. This graph consists of nodes representing words or wordpieces, where each node's attribute corresponds to its word embedding. To address the challenge of assigning attributes to OOV nodes, a self-attention network assigns them based on character information from the OOV words. Two levels of Graph Attention Network process the graph, and contrastive learning is applied using NT-XENT positive samples to encourage proximity between graph-level embeddings and background embeddings. Experiments show that the model performs better than baselines in both intrinsic and extrinsic tasks, demonstrating its effectiveness in learning OOV words through word formation.</sample>
    <sample id="338">Bingsheng介绍了一项研究，该研究探讨了人类自然语言解释的质量评估问题。研究人员提出了一种统一的数据格式，将各种任务转换为一个统一的多项选择任务，并使用此格式对五个大型数据集进行了实验。他们发现，虽然传统的指标如BLEU和ROUGE依赖于人类注释作为黄金标准，但这些注释可能具有主观性和任务依赖性。因此，他们提出了一个新的评估指标TREU，该指标考虑了任务差异，并在两个模型上评估了五个数据集的人类注释。结果显示，TREU比传统指标更能准确地反映人类注释对模型预测的帮助程度。</sample>
    <sample id="339">Saarland University in Germany</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presented a work titled "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation". The goal of this research is to create a large-scale, syntactically diverse paraphrase dataset. They propose using AMR (Abstract Meaning Representations) back-translation as the key idea for constructing their proposed dataset, ParaAMR. By leveraging AMR graphs and modifying them through random sampling, they aim to generate more syntactically diverse paraphrases compared to existing datasets that use back-translation methods. Their findings demonstrate that ParaAMR can improve performance in various NLP applications such as learning sentence embeddings, syntactic control paraphrase generation, and data augmentation for few-shot learning.</sample>
    <sample id="341">作者使用了平均延迟和计算感知平均延迟作为延迟测量方法。</sample>
    <sample id="342">The presentation introduces "LiveChat," a large-scale personalized dialogue dataset constructed from live streaming videos. The paper, conducted by Gao Jingsheng and colleagues from Shanghai Jiao Tong University and Xiaobing.AI, addresses the challenges of existing open-domain dialogue datasets in terms of scale, persona representation, session dialogues, multi-party conversations, and Chinese-specific data.

The LiveChat dataset is created through three steps: 1) scraping origin streaming videos from Douyin; 2) extracting audio and transcribing it into utterances using ASR; and 3) collecting audience comments to construct dialogues via reply-to-whom matching methods. Persona information for personalized dialogue generation is collected manually or automatically extracted based on rules and trained classifiers.

The experiments demonstrate that LiveChat's larger scale and longer average sessions per persona benefit response modeling tasks. Both manual labeling and automatic extraction are crucial for persona profiles. In Addressee Recognition, single-stream BERT outperforms double-stream BERT but still benefits from persona information. Pre-trained models like BART show superior performance due to their domain adaptation capabilities.

Future work will focus on efficient transfer learning with LLMs applied to LiveChat.</sample>
    <sample id="343">自然语言理解模型依赖于各种知识源，包括其参数中包含的知识，通常通过预训练获得，以及在推理阶段提供的知识。最近的任务，如问答，表明模型可以利用预训练时间的知识来解决任务。但是，自然语言理解通常需要在推理阶段提供的知识。例如，在句子“John看到了新当选的总统在电视上。”中，预训练参数可能包含有关总统做什么和电视是什么的信息，但它们不能可靠地知道这个实例特定实体“John”是谁，或者新总统是谁，因为总统自预训练以来可能会发生变化。因此，成功处理知识密集型NLU任务的模型需要能够整合并使用预训练时间和推理时间的知识。在这项工作中，我们提出了一套诊断测试套件来评估知识集成。我们引入了一个核心参考任务，旨在探测从不同来源获取知识的能力。我们用人类研究参与者和现有的核心参考模型评估数据集。这里有一个我们数据集中的示例。Servin是法官。Kea是面包师。Servin和Kea在一个公园见面。在一天的工作结束后，他在法庭决定案件时感到很高兴。解决给定代词的任务需要两种类型的信息。首先，实体特定的知识，例如“Servin是法官。”其次，背景知识，例如“法官在政府机构审理案件。”通常，背景知识是在大型语言模型的预训练过程中学习的，而实体特定的知识通常是在推理时间观察到的。我们通过控制事实出现在真实来源中的可用性来控制代词的分辨率。在背景-预训练设置中，我们假设背景知识“政治家寻求政府职位”包含在预训练参数中，并在推理时间上下文中提供实体特定的知识“Chichester是政治家。”在背景-两者设置中，我们不仅提供实体特定的知识，而且在推理时间上下文中还提供关于政治家的背景知识。在背景-推理设置中，我们提供虚构的职业“mirituer”而不是政治家，因为“mirituer”不太可能包含在预训练参数中。我们使用人类研究参与者和现有的核心参考模型对数据集进行评估。在这张图表中，我们展示了背景-预训练设置中最困难变体的最佳性能模型的结果。在没有任务特定训练的情况下，两个模型都没有表现良好。当训练KIMUS时，两个C2F和BERT4Coref都显著优于随机选择。这表明，大多数模型在没有任务特定训练的情况下似乎利用表面线索，这些线索在测试KIMUS时是无用的（KIMUS中的线索已被移除）。其他实验中的虚构知识进一步表明，即使是最成功的模型也无法可靠地整合仅在推理时间呈现的后向知识。总之，许多核心参考模型似乎无法在没有任务特定训练的情况下推理来自不同来源的知识。然而，带有任务特定训练的一些模型成功地整合了来自多个来源的知识。尽管如此，即使是最成功的模型似乎也无法可靠地整合仅在推理时间呈现的后向知识。如果您对此感兴趣，请参见我们的论文，并查看GitHub上的数据集和代码。谢谢大家的聆听。</sample>
    <sample id="344">基于树的方法需要获得树，这通常是一个复杂和计算昂贵的过程。它通常涉及特定于形式的预处理，例如处理变量符号，并可能需要专门的语法推导过程。此外，树不是给定的，需要通过某种方式获得，这可能会很复杂和计算昂贵。</sample>
    <sample id="345">这篇论文介绍了在没有树的情况下进行组成性泛化的一种新方法。该方法使用多集标记和潜在排列来预测输入和输出之间的对应关系。该方法通过两个步骤工作：首先，它为每个输入标记分配一个未排序的多集标记，这些标记将出现在输出中。然后，它使用另一个模型来预测一个排列，以将标记放入正确的顺序。该方法不需要树，并且在不依赖于树的情况下展示了对深层递归的强大泛化能力。</sample>
    <sample id="346">这篇论文的作者所属机构是香港科技大学。</sample>
    <sample id="347">好的，我们来翻译一下。</sample>
    <sample id="348">The paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" explores the limitations of current methods for measuring stereotypes and biases in large language models (LLMs). Traditional approaches rely on hand-constructed datasets, which are time-consuming and often capture only specific or broad associations. The study introduces a method that leverages LLMs' ability to generate personas based on prompts like "Imagine you are an Asian woman. Describe yourself." This approach is generalizable to any demographic by specifying identity markers.

The research uses two main components:
1. **Persona Generation**: Inspired by studies showing how human subjects respond to similar prompts.
2. **Marked Words Method**: Based on sociolinguistic concepts such as markedness, it identifies words distinguishing marked groups from unmarked ones.

Results show that generated personas contain more stereotypes than those written by humans but have fewer harmful patterns captured by traditional lexicons. Instead, they reveal essentializing narratives through positive-seeming words reflecting harmful tropes associated with marginalized identities.

Recommendations include addressing both positive stereotypes and essentializing narratives, using intersectionality in bias studies, and increasing transparency about bias mitigation methods used in model development.</sample>
    <sample id="349">你好，大家好，我是来自中国科学技术大学的Jingwei Yi。我很高兴给大家介绍我们论文的广告视频。你也在模仿我的模型吗？保护大型语言模型嵌入作为服务的后门水印。首先让我们介绍一下嵌入作为服务的背景。目前，像GPT、LLAMA和PALM这样的大型语言模型在自然语言理解和生成方面表现出色。嵌入作为服务是建立在大型语言模型之上的服务之一，以协助各种NLP任务。例如，OpenAI提供了一个基于GPT的嵌入API。然而，最近的研究表明，攻击者可以通过学习嵌入来窃取模型，并提供类似的服务。因此，保护嵌入作为服务的版权是必要的。为了保护嵌入作为服务的版权，一种解决方案是在提供服务中嵌入一个水印，并检测另一个服务是否包含水印。水印方法需要满足以下属性。首先，该方法应适用于嵌入作为服务。其次，水印不应降低提供的嵌入的实用性。第三，水印应足够隐蔽，以便攻击者或攻击者可以轻松地移除水印。最后，水印需要在模型提取过程中转移到攻击者的服务中。现有的工作可以大致分为四类。然而，这些方法要么不适用于嵌入作为服务，要么缺乏可转移性。因此，在本文中，我们提出了一种名为嵌入标记的后门水印方法，适用于嵌入作为服务。接下来，让我详细介绍一下我们的嵌入标记。嵌入标记包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发器集。触发器集是一组在中等频率间隔内的单词。我们假设提供者可以收集一个通用文本语料库，并使用它来计算词频。在水印注入中，我们首先定义一个目标嵌入。当用户向提供者服务发送句子时，提供者会计算句子中的触发器数量。提供的嵌入是目标嵌入和原始嵌入的权重加权求和。目标嵌入的权重与句子中的触发器数量成正比。当句子中的触发器数量大于m时，提供的嵌入正好等于目标嵌入。版权验证是检测另一个服务背后模型是否包含水印。我们首先构建一个后门数据集和一个干净数据集。后门数据集包含所有单词都属于触发器集的句子，而干净数据集中的所有单词都不属于触发器集。然后，提供者从窃取者的服务中请求这些数据集的嵌入。计算请求嵌入与目标嵌入之间的余弦相似度和L2相似度。我们还计算干净数据集和后门数据集之间的相似度差异，定义为余弦差异和L2差异。同时，我们还应用KS检验并使用其p值作为第三个指标。我们在四个数据集AG News、MIND、SST2和Enron Spam上进行了实验。我们假设提供者使用维基文本数据集来计算词频。四个数据集的结果显示，我们的嵌入标记可以在保持下游任务的高实用性的同时具有很好的检测性能。我们还通过PCA可视化了四个数据集的嵌入（[inaudible] 4:39）。图例表示每个句子中的触发器数量。如图所示，很难区分后门嵌入和正常嵌入。这就是全部内容。谢谢。欢迎与我们讨论。</sample>
    <sample id="350">The presentation discusses the concept of "superhuman performance" in NLP and its implications. It highlights that while systems can achieve high scores on popular benchmarks, it is unclear what constitutes true superhuman performance when compared to humans' capabilities involving knowledge, reasoning, and inference. The presenter points out several issues with current benchmarking practices, such as evaluating models against different subsets or datasets than those used for human evaluation, errors in ground-truth answers, vague estimates of human performance, varying pay rates among annotators, and lack of information about the annotator pool. These factors contribute to unreliable comparisons between human and system performances. To address these concerns, the paper provides recommendations for constructing more reliable benchmarks by ensuring fair evaluations, accurate annotation processes, transparent reporting of data sources, and adequate compensation for annotators.</sample>
    <sample id="351">The speaker introduces a study investigating the generalization of Named Entity Recognition (NER) models using CoNLL-2003 data. They developed the CoNLL++ dataset, collected from Reuters News in 2020 and annotated according to CoNLL-2003 guidelines. The study fine-tuned over 20 NER models on both CoNLL-2003 test sets and CoNLL++. Results showed that three main factors contribute to good generalization: model architecture, model size, and more fine-tuning examples. Adaptive overfitting was not observed; instead, temporal drift caused performance degradation due to increasing time gaps between training and testing datasets. The conclusion is that CoNLL-2003 taggers still perform well today, but further research into improving model generalization is needed.</sample>
    <sample id="352">ABC-Eval代表Annotating Behaviors in Chat。</sample>
    <sample id="353">这篇论文介绍了通过问答来生成代码的方法。主要动机是解决代码生成和程序合成中的输入不充分问题。作者提出了一种方法，即通过问答来收集更多的规格信息，以缓解输入不充分的问题。他们还提出了一个名为CodeClarQA的合成数据集，用于生成代码并回答缺失关键操作的澄清问题。该方法包括创建CQAs、识别缺失的关键操作以及构建一个管道来生成代码。实验结果表明，该方法在识别缺失关键操作方面表现良好，并且在生成代码时也有所改善。然而，该任务仍然具有挑战性，因为模型的性能仍然低于使用NLD和代码进行训练的模型。</sample>
    <sample id="354">CoNLL-2003 和 CoNLL++ 之间的性能增量在 2018 年之前高于 5 个百分点。</sample>
    <sample id="355">好的，我明白了。您想让我将您的论文内容翻译成中文吗？</sample>
    <sample id="356">这篇论文的作者所属机构是斯坦福大学。</sample>
    <sample id="357">演讲者的名字是Siyu Yuan。</sample>
    <sample id="358">这篇论文有五位作者。</sample>
    <sample id="359">该方法与专门为同时预翻译而设计的专用架构进行了比较。</sample>
    <sample id="361">Armineh Nourbakhsh是一位在卡内基梅隆大学语言技术研究所攻读博士学位的学生，也是JP Morgan AI研究团队的研究主任。她的研究领域是使用反事实场景来提高多步定量推理的组成性泛化能力。她展示了如何通过分析财务表格中的问题和答案来识别关键操作，并利用这些信息来训练模型。她的方法包括从训练数据中提取正负示例，并将它们纳入训练过程中，以提高模型的性能。</sample>
  </task>
</testset>