<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">语言模型是基于大规模网络爬取数据进行训练的。</sample>
    <sample id="1">McGill University, Mila and Microsoft Research</sample>
    <sample id="2">大家好，欢迎参加我们的介绍，Geoplane是一个新的语料库，用于在文档级别和句子级别上对德文文本进行识别。</sample>
    <sample id="3">我的名字是Regina Stöcken，我将引导您进行演示的第一部分。首先，让我们定义文本简化。</sample>
    <sample id="4">文本简化是适应文本的过程，以改善特定目标群体对文本的理解，例如有阅读障碍的人或非母语者。</sample>
    <sample id="5">To train a text identification model, we require parallel pairs of texts, for example, documents or sentences.</sample>
    <sample id="6">在下面的例子中，你可以看到一个复杂的德语句子及其翻译成白话文的平行句对。</sample>
    <sample id="7">简化句子的方法有很多，比如词汇替换、短语省略、短语重排或插入单词。</sample>
    <sample id="8">我们提出一个新的语料库计划，因为近年来存在一些现有语料库的问题。例如，这些语料库太小，无法训练文本分类模型。</sample>
    <sample id="9">The other three models which are proposed in recent years are all automatically aligned, which means they can be error prone in their alignments.</sample>
    <sample id="10">因此，我们提出了新的语料库dplane，它被分为两个子语料库：dplane-apa和dplane-wap。dplane-apa基于新闻文本。</sample>
    <sample id="11">在DeepPlain-APA中，我们手动对483篇文档进行了对齐，结果产生了大约30,130对平行句子。</sample>
    <sample id="12">对于DeepL Web，这个语料库包括不同的领域，并且我们还手动和自动对这750篇文档进行对齐。</sample>
    <sample id="13">总共有30,450个句子对。</sample>
    <sample id="14">我们对句子对进行了一些分析，例如对简化类型进行分析。</sample>
    <sample id="15">如您所见，圣经文本比新闻文本或语言学习者文本更简化得多。</sample>
    <sample id="16">在所有层面，例如词汇简化、结构简化，以及整体的简化。</sample>
    <sample id="17">此外，你可以看到我们的dplane语料库具有不同变形转换的高多样性。例如，在dplane API语料库中，我们有更多的重排序和单词添加，而dplane web语料库中则没有。</sample>
    <sample id="18">另一方面，在网络语料库中，我们有更多的改写。</sample>
    <sample id="19">让我们现在看看我们能用这个语料库做什么。</sample>
    <sample id="20">在最近几年中，已经出现了许多对齐方法，尤其是在机器翻译的背景下。</sample>
    <sample id="21">假设我们有两个平行文档，分别用不同的语言编写，并且我们想要从这些文档中提取句子的对齐信息。</sample>
    <sample id="22">但是，在我们的用例中，我们试图从两个平行文档的句子之间提取对齐关系，这些文档具有相同的语言、相同的内 容，但它们在复杂度级别上是不同的。</sample>
    <sample id="23">现在，我们有了一个手动对齐句子的数据集Dplane，我们可以使用这些句子作为基准对齐来评估一些提议的对齐方法。</sample>
    <sample id="24">我们对所提出的算法进行了适应，并在论文中发表了所有这些适应和运行实验的代码。</sample>
    <sample id="25">最后，我们得出结论，最适合用于简化德语文本的自动对齐方法是MASE-Align。</sample>
    <sample id="26">你也可以在论文中找到运行此方法的代码。</sample>
    <sample id="27">我们论文中展示的第二个用例是自动简化文本。</sample>
    <sample id="28">通过调整语言模型，可以将复杂的输入文本转换为简化的文本。</sample>
    <sample id="29">我们对两个不同的模型进行了微调。我们对LongImpArt模型进行了微调，以生成文档级别的简化版本。</sample>
    <sample id="30">我们还对正常基础M部分进行了微调，以产生句子级别的简化。</sample>
    <sample id="31">你也可以在论文中找到所有的检查点，并且可以查看实验的分数和评估指标。</sample>
    <sample id="32">我们得出结论，这种基本的微调可以产生或得到比基线分数更高的分数。</sample>
    <sample id="33">我们建议将这些结果作为自动文本简化问题的基准，在未来作为基准。</sample>
    <sample id="34">非常感谢大家的聆听，我们期待在会议上见到大家。谢谢。</sample>
    <sample id="35">演讲者的名字是Kaiyuan。</sample>
    <sample id="36">They used the T5-XLARGE model to achieve 82%-87% accuracy.</sample>
    <sample id="37">CoNLL-2003标注器仍然有效。</sample>
    <sample id="38">The proposed method aims to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="39">现有弱监督方法的成功在很大程度上依赖于干净的验证样本。</sample>
    <sample id="40">提高分数的措施包括： 1. 提高词汇量：扩大词汇量可以帮助理解问题和答案，从而提高得分。 2. 增加练习：通过定期练习来熟悉题型和解题方法，可以提高解决问题的能力。 3. 学习策略：了解有效的学习策略，如记忆技巧、时间管理等，可以帮助更有效地学习和应用知识。 4. 寻求反馈：从教师或同学那里获得反馈，可以帮助识别弱点并加以改进。 5. 使用资源：利用教科书、在线资源和辅导课程等学习工具，可以提供额外的支持和信息。 6. 管理压力：学会管理考试压力，保持冷静和专注，有助于在考试中表现更好。 7. 健康习惯：保持健康的饮食、充足的睡眠和定期锻炼，可以改善认知功能和整体表现。 8. 考试技巧：学习有效的考试技巧，如如何阅读题目、如何组织答案和如何检查工作，可以提高得分。</sample>
    <sample id="41">这篇论文有5位作者。</sample>
    <sample id="42">嗨，我的名字是萨德姆·斯克鲁科夫斯基，这次演讲的主题是依赖结构的协调。</sample>
    <sample id="43">正如你所知，不同的理论和语料库方法假设不同的依赖结构。例如，在普遍依赖关系中，Lisa、Bart和Maggie的结构是并列的。</sample>
    <sample id="44">这样，第一个连词是整个并列结构的头，即Lisa。</sample>
    <sample id="45">类似的处理方式出现在Igor Milchuk的含义文本理论中，其中整个坐标结构由第一个连词引导。因此，这两种方法是对称的，它们突出了一个连词。</sample>
    <sample id="46">现在还有对称的方法来处理坐标结构，比如在依赖树库中使用的Prag方法，或者在连接词头部的结构中，连接词是头部。</sample>
    <sample id="47">我们从 AND 到所有的 conjuncts 获取一些依赖关系。</sample>
    <sample id="48">最后，还有一个多头的方法，例如在卡森的单词语法中使用。</sample>
    <sample id="49">我们可以说连词是句子结构的主干，因此我们从支配者“here loves”到所有的连词“Lisa bought and make”得到依赖关系。</sample>
    <sample id="50">现在，这篇文章的目的是提出一个关于对称结构的协调的新论点，以及对不对称结构的协调的论点。</sample>
    <sample id="51">好的，这个论点基于依赖最小化的原则，我将根据这些例子进行解释。</sample>
    <sample id="52">所以，在英语中，如你所知，直接宾语更倾向于靠近动词，而补语则可能更远。因此，“March read it yesterday”是正确的，因为直接宾语“it”靠近动词。</sample>
    <sample id="53">While March read yesterday it is much worse right because here between the verb and the direct object there is an adjunct are yesterday.</sample>
    <sample id="54">然而，当直接对象非常重且很长时，这种影响可能会得到改善，因为然后它可以在边缘之后的位置移动。</sample>
    <sample id="55">这在这里有所说明。这两句话都很好。马特写了一本关于BC yesterday的非常有趣的书，是OK的。相反，我们有这个长NP。</sample>
    <sample id="56">但是，说昨天读了马特·雷德的那本关于蜜蜂的书也很好。</sample>
    <sample id="57">所以这里的原因是，这有可能是因为，即使这个句子违反了通用的语法原则，即直接宾语应该在动词旁边。</sample>
    <sample id="58">它满足依赖长度最小化的原则，即较短的依赖关系更受欢迎。</sample>
    <sample id="59">所以，这两棵树只显示了关键依赖关系的长度，也就是在这些两个结构中不常量的那些。</sample>
    <sample id="60">所以这里有一个从“red”到长度为7的从属关系，以单词为单位测量，以及从“red”到长度为4的“book”，所以总共是11。</sample>
    <sample id="61">当你移动，当你交换这两个成分时，这两个依赖项的和变成了6，对吧？所以，而不是11，6更短，这就是为什么这听起来还不错，对吧？它违反了一个原则，但满足了另一个原则。</sample>
    <sample id="62">好的，我们从增强版的Penn Treebank中提取了各种关于协调的统计数据，并在论文《为什么不用通用依赖关系》中可以看到。</sample>
    <sample id="63">这些统计数据证实了以前多次观察到的结论，即左连词通常较短。</sample>
    <sample id="64">还有在经过观察中得出的结论，即这个趋势随着长度差异而增长。</sample>
    <sample id="65">所以，当两个连词的长度差异增大时，较短的连词更倾向于成为第一个更强的连词，对吧？因此，左侧较短连词的比例更大。</sample>
    <sample id="66">但是在这篇论文中，我们观察到这种趋势只发生在左派政府不存在的情况下。</sample>
    <sample id="67">好的，所以在这个例子中，州长在左边，我看到Bob和Lisa，所以州长在左边。</sample>
    <sample id="68">在第二个例子中，Homer came and sneezed，这里有两个动词的协调，没有外部支配者。因此，在这种情况下，左边的连词更喜欢缩短，差距越大，左边的连词越长。</sample>
    <sample id="69">然而，当治理者在右边时，如这里左边的治理者协调了网络，这种效应就消失了。</sample>
    <sample id="70">因此，我们通过测量字符长度、元音长度和单词长度来展示这一点。</sample>
    <sample id="71">我们看到的是，当控制杆在左边时。</sample>
    <sample id="72">左连词的长度倾向随着绝对差在单词中稳步增长，当没有从句主语时也是如此，但在从句主语在右边时，这种倾向就消失了。</sample>
    <sample id="73">我们在论文中展示了这种结构如何提供一个反对不对称协调结构的论据，如这两个，并支持对称结构，如这两个。</sample>
    <sample id="74">所以，请查看论文以获取完整的协议和论点，并在海报环节与我们交谈。谢谢。</sample>
    <sample id="75">这篇论文有两位作者。</sample>
    <sample id="76">The Bible texts are much stronger simplified than, for example, the news text or the language learner texts.</sample>
    <sample id="77">Salt and pepper and not pepper and salts</sample>
    <sample id="78">Yes, you can use these models for your research.</sample>
    <sample id="79">DEplain-apa 包含新闻文本。</sample>
    <sample id="80">A better model architecture, larger models as well as more fine-tuning examples.</sample>
    <sample id="81">衡量左并列词是否更短的方法是：测量长度在字符中（第一列）、在音节中（中间列）和在单词中（右列）。</sample>
    <sample id="82">测量字符长度、音节长度和单词长度。</sample>
    <sample id="83">The baseline classifier performed not much better than chance.</sample>
    <sample id="84">这篇论文有三位作者。</sample>
    <sample id="85">The cartoon features two characters: Bob and Alice.</sample>
    <sample id="86">In formal language and lexical cohesion, context-aware models are significantly more accurate than those that do not use context.</sample>
    <sample id="87">The authors of this paper are affiliated with the following institutions: Google Research, University of California San Diego, and University of Washington.</sample>
    <sample id="122">引入的框架通过比较模型和数据集的预测和标签与不同受试者的预测来量化立场。</sample>
    <sample id="155">They also were able to surface racial stereotypes.</sample>
    <sample id="156">The study used the following data sources: 1. Penn Treebank (Enhanced version) for extracting coordination statistics; 2. Universal Dependencies, as referenced in the paper titled "Why wouldn't you use universal dependencies?".</sample>
    <sample id="157">This paper has 10 authors.</sample>
    <sample id="158">Topic independent dissonance stance classification and binary classification of expansion and comparison classes.</sample>
    <sample id="159">The paper has three authors: Zhihong Zhu, Yichen Wang, and Xiangliang Zhang.</sample>
    <sample id="160">这篇论文有两位作者。</sample>
    <sample id="161">引入的框架与以前的研究有何不同？</sample>
    <sample id="162">The generated personas contain a lot more stereotypes than the human-written ones.</sample>
    <sample id="163">They compared different commercial systems.</sample>
    <sample id="164">你好，我是华盛顿大学的博士生张斌。今天我将介绍我们从预训练数据到语言模型再到下游任务的工作，追踪政治偏见的轨迹，导致不公平的NLP模型。</sample>
    <sample id="165">语言模型是在大规模网络爬取数据上进行训练的。</sample>
    <sample id="166">根据C4语料库的调查，政治新闻媒体在预训练数据中得到了很好的覆盖。我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等都在语言模型的训练数据中得到了很好的覆盖。</sample>
    <sample id="167">这为语言模型应用带来了喜忧参半的福音。</sample>
    <sample id="168">一方面，他们能够从不同的视角学习，这庆祝了民主和思想的多样性。另一方面，这些不同的政治观点本质上是社会偏见，可能会导致下游任务应用中的潜在公平问题。</sample>
    <sample id="169">为此，我们提议从预训练数据到语言模型再到下游任务，具体通过提出以下问题来调查政治偏见传播管道：</sample>
    <sample id="170">首先，我们如何评估语言模型的政治倾向性，预训练数据在这样的政治偏见中扮演什么角色？</sample>
    <sample id="171">第二，不同政治倾向的语言模型在下游任务中的表现如何？这是否会导致NLP应用中的公平性问题？</sample>
    <sample id="172">我们首先提出使用不同的提示格式来提示语言模型，使用政治问卷，例如政治复杂测试。这确保了自动评估基于政治科学文献。</sample>
    <sample id="173">一些初步结果显示，第一语言模型确实有各种政治倾向。它们占据了政治指南针上的四个象限。</sample>
    <sample id="174">我们可以看到，GPT-4是最自由的语言模型，而GPT系列通常比BERT系列及其变体更自由。</sample>
    <sample id="175">其次，我们旨在研究语言模型的政治偏见在多大程度上是从训练数据中获得的。</sample>
    <sample id="176">我们可以通过在六个不同的党派语料库上进一步预训练语言模型检查点来开展控制实验，这些语料库被分为新闻和社交媒体，并进一步根据其政治倾向进行划分。</sample>
    <sample id="177">通过在这样的党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。</sample>
    <sample id="178">For example, for Roberta further refined to and further trained on the left-leaning Reddit corpus we can see a substantial liberal shift in terms of its</sample>
    <sample id="179">在政治偏见方面。</sample>
    <sample id="180">我们还试图调查语言模型是否能捕捉到在我们现代社会中普遍存在的极化现象。</sample>
    <sample id="181">我们把预训练语料库分为在第45任美国总统之前和之后，然后分别在两个不同的时间语料库上进行预训练。</sample>
    <sample id="182">我们可以看到，在2017年之后，语言模型通常具有离中心更远的政治倾向。这表明语言模型也可以捕捉到我们社会中的极化现象。</sample>
    <sample id="183">最后，我们评估具有不同政治倾向的语言模型在仇恨言论检测和假新闻检测等自然语言处理应用中的表现，这些应用通常涉及语言模型，并可能产生非常重要的影响。</sample>
    <sample id="184">所以，如果我们研究每个类别的表现，也就是说，如果我们把表现分成</sample>
    <sample id="185">不同的人口统计或政治倾向的新闻媒体，我们可以看到一个模式，例如，对于仇恨言论检测，左倾语言模型表现更好。</sample>
    <sample id="186">检测针对社会少数群体的仇恨言论</sample>
    <sample id="187">然而，我们的工作是检测针对社会中更强大群体的仇恨言论。</sample>
    <sample id="188">反过来说，右边的语言模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人、LGBTQ+和其他少数群体的仇恨言论方面表现更差。</sample>
    <sample id="189">类似的趋势也发生在假新闻检测中，我们看到左倾语言模型在检测来自相反政治倾向的错误信息方面表现更好，反之亦然。</sample>
    <sample id="190">We further show many qualitative examples to see that language models with different political meanings</sample>
    <sample id="191">They do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that</sample>
    <sample id="192">这表明在语言模型的政治偏见方面存在一个非常紧迫的公平问题。</sample>
    <sample id="193">例如，如果右倾语言模型被微调以仇恨言论或错误信息，并部署到一个流行的社交媒体平台上，</sample>
    <sample id="194">这将意味着，持有相反政治观点的人可能会被边缘化，而针对少数群体的仇恨言论可能会在没有任何控制的情况下泛滥。</sample>
    <sample id="195">这让我们警醒，需要承认并解决由语言模型政治倾向导致的公平性问题。</sample>
    <sample id="196">我们还希望强调，我们揭示了语言模型政治偏见的困境，即Sirela和Caribdis之间的困境。</sample>
    <sample id="197">如果我们不对政治观点进行清理，语言模型训练数据中的偏见将从预训练数据传播到语言模型和下游任务中，最终导致公平性问题。</sample>
    <sample id="198">如果我们尝试 sanitise，我们也会面临审查或排斥的风险，并且很难确定什么是中立的，应该保留到语言模型训练数据中。这就像Electric Charlie的问题一样。</sample>
    <sample id="199">好的，太好了，我想这就是我今天能提供的全部内容了。谢谢你的宝贵时间。</sample>
    <sample id="200">There are three authors.</sample>
    <sample id="201">MPP assessments can cover up to 1024 words of context.</sample>
    <sample id="202">他们的数据集中包含音乐、文字和虚构内容。</sample>
    <sample id="203">Positionality refers to the perspectives people hold as a result of their demographics, identity, and life experiences. This concept is widely used in critical studies, particularly in feminist and queer academic spaces. Positionality can influence research processes and outcomes because it affects the decisions researchers make.</sample>
    <sample id="204">David</sample>
    <sample id="205">EDAtt 是一种适应了现有的离线 ST 模型的解决方案。它通过使用一个模型来处理不同的延迟情况，并通过特定的参数来调整延迟，从而实现了对离线 ST 模型的适应。</sample>
    <sample id="206">这篇论文有两位作者。</sample>
    <sample id="207">Both models do not perform well.</sample>
    <sample id="208">KITMUS有三个变体：1.带有背景的预训练设置，2.带有背景的两者设置，3.带有背景的推理设置。</sample>
    <sample id="209">The authors of this paper are affiliated with the following institutions:</sample>
    <sample id="210">The last research question is: "Should we only use the clean samples for validation, or are there better ways to utilize them?"</sample>
    <sample id="211">指标灵敏度衡量模型在任务上的一致性，无论指令措辞如何。</sample>
    <sample id="212">Jin Wei Yi</sample>
    <sample id="213">更高的灵敏度表示模型性能得到了提高。</sample>
    <sample id="214">The model receives language from the internet.</sample>
    <sample id="215">通常需要20个干净的验证样本才能获得良好的表现。</sample>
    <sample id="216">The authors of this paper are affiliated with the following institutions:</sample>
    <sample id="217">因为语言模型有政治倾向。</sample>
    <sample id="218">演讲者的名字是Makshita。</sample>
    <sample id="219">政治偏见传播流程是怎样的？</sample>
    <sample id="220">是的，DEplain-apa 和网站的简化过程有所不同。</sample>
    <sample id="221">No, coscript is not publicly available.</sample>
    <sample id="222">Watermark injection involves defining a target embedding and then using it to modify the original embedding based on the number of triggers in the sentence.</sample>
    <sample id="223">Penn State University</sample>
    <sample id="224">Yes, encoder-decoder or encoder-PTR can be improved by training in a mixture of various languages.</sample>
    <sample id="225">Make a chocolate cake</sample>
    <sample id="226">They validated the covertness of the provided embedding by visualizing the embedding of sentences on four datasets via PCA.</sample>
    <sample id="227">In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategies.</sample>
    <sample id="228">GPT-4 与中国的立场最不一致。</sample>
    <sample id="229">The speaker demonstrates how the model uses attention mechanisms to leverage knowledge acquired through audio input and text output, specifically using cross-attention.</sample>
    <sample id="230">随着任务数量的增加，模型的性能会提高，同时敏感度也会降低。</sample>
    <sample id="231">The three baseline models are: 1. Tree-structured language model (TLM) 2. Neural network-based language model (NNLM) 3. Recurrent neural network (RNN) These baselines serve as benchmarks to evaluate the performance of the proposed method in handling recursive structures and structural generalization tasks within natural language processing.</sample>
    <sample id="232">两位合著者是第一作者的导师。</sample>
    <sample id="233">PaLM的首作者是Dwight Chau。</sample>
    <sample id="234">大家好，我是卡内基梅隆大学的一年级博士生珍妮，今天我将为大家介绍我的工作《Animal Positionality: Characterizing Design Biases of Datasets and Models》。</sample>
    <sample id="235">这项工作与华盛顿大学和 Allen AI 实验室的几位同事合作完成，包括 Sebastian Senti、Ronan Le Bras、Katerina Rynika 和 Morten Sapp。</sample>
    <sample id="236">好的，让我们从想象开始，你为一家报纸工作，正在筛选新闻文章下的评论，试图移除有毒内容。</sample>
    <sample id="237">你可能会转向一个流行的API，比如Perspective API来进行毒性检测。如果你是Carl Jones，这个工作效果很好，因为Perspective API能够正确地检测出有毒实例。</sample>
    <sample id="238">但这并不是迪特亚·夏尔马的情况，因为Perspectiv API对印度语境中更常见的冒犯性术语并不敏感。</sample>
    <sample id="239">这是一个设计偏见的例子，我们看到不同群体之间技术的系统性性能差异。</sample>
    <sample id="240">设计偏差，就像我们之前看到的那样，可能会由于 NLP 研究人员和模型开发者的地位而发生。地位只是人们在人口统计学、身份和生活经历的结果中持有的观点。</sample>
    <sample id="241">这个概念在批判性研究中被广泛使用，特别是在女权主义和同性恋学术领域。</sample>
    <sample id="242">作为研究人员，位置性可以影响研究过程及其结果和成果，因为它可以改变研究人员所做的决定。</sample>
    <sample id="243">因此，人们可能会问的是，数据集和模型是否具有位置性？</sample>
    <sample id="244">我们并不是说模型本身和数据集本身具有人口统计身份和生活经历，但它们确实汇总了真实人的判断和意见，并因此代表了一种位置性优于其他位置性。</sample>
    <sample id="245">初步工作表明，有些轶事证据表明存在位置性，例如文化差距和模型和数据集，以及对模型位置性的稀有定义。</sample>
    <sample id="246">然而，这些工作并没有比较终端用户与数据集和模型本身。</sample>
    <sample id="247">理解模型和数据集的偏见性变得越来越重要，因为自然语言处理任务变得更加主观和社会导向。</sample>
    <sample id="248">很难描述这些位置性如何被扭曲，因为并非所有决策都有记录，而且许多模型都隐藏在API后面。</sample>
    <sample id="249">为了研究数据集和模型的位置性，我们实际上将注释与现有数据集和模型中的真实用户进行了比较。</sample>
    <sample id="250">我们通过框架和位置来实现这一点。</sample>
    <sample id="251">我们的框架分为两个主要步骤。</sample>
    <sample id="252">第一步是使用不同的注释员对数据集进行重新注释。</sample>
    <sample id="253">我们应该通过查看原始数据集的注释者的人口统计信息来完成这项工作，因为通常只有少数注释者为每个实例进行注释，并且因为人口统计信息很少被收集和共享。</sample>
    <sample id="254">因此，我们选择重新注释数据，以获取许多实例，并获得丰富的人口统计数据。</sample>
    <sample id="255">然后，我们按人口统计学对注释进行分类，并使用皮尔逊R相关系数将它们与模型和数据集进行比较。</sample>
    <sample id="256">因此，我们的框架与注释员分歧文献不同，通过将最终用户与模型和数据集的预测和标签进行比较，而不是只看注释员内部的一致性或注释员分布。</sample>
    <sample id="257">我们的框架主要通过Lab in the Wild，一个在线众包平台，由我们的HCI合作者提供。</sample>
    <sample id="258">Live in the Wild是一个在线实验平台，我们可以招募来自不同背景的志愿者，与MTurk等平台相比，后者主要由美国或印度的参与者组成。此外，Live in the Wild仍然能够获得高质量的数据。</sample>
    <sample id="259">我们在《实验室之外》上举办两项任务，其中一项是社会可接受性。这项任务的工作方式是，参与者将从社会化学数据集中读取一个情况，然后他们将评估该情况的社会可接受程度。</sample>
    <sample id="260">之后，为了保持对研究的兴趣，他们可以将自己的反应与AI和其他人的反应进行比较。</sample>
    <sample id="261">然后，我们将这些注释与Social Chemistry、Delphi和GPT-4进行了比较。</sample>
    <sample id="262">然后，我们复制了一个非常相似的设置来进行毒性与仇恨言论检测任务，在那里他们将阅读DinaHate中的一个实例，并判断是否认为这是一个仇恨言论实例。</sample>
    <sample id="263">然后，我们将这些注释与DynaHate、Perspective API、Rewire API、HateRoberta和GPT-4进行比较。我们的研究最终收集了来自87个国家的1000多名注释者的超过16,000条注释。</sample>
    <sample id="264">因此，我们现在更有能力回答NLP数据集和模型最符合什么。我们发现NLP确实存在位置性。</sample>
    <sample id="265">例如，我们发现数据集和模型最符合英语国家。因此，在GPT-4社会接受性分析中，我们发现它最符合中国和英语国家。我们还发现Dinahate也最符合英语国家。</sample>
    <sample id="266">我们发现它与拥有大学学历的人最一致。在社会可接受性任务中，我们发现GPT-4与拥有大学学历或研究生学历的人最一致。</sample>
    <sample id="267">我们发现对于Danny Hade，它与拥有大学学历的人最相关。</sample>
    <sample id="268">然而，当模型和数据集与特定的人口对齐时，一些人不可避免地被落下。</sample>
    <sample id="269">一个例子是，数据集和模型与非二元人相比，更少地对齐男性和女性的对应物。我们在GPT-4社会可接受任务中以及DINAHATE任务分析中发现了这一点。</sample>
    <sample id="270">所以，既然存在偏见和不平等，我们该怎么办？</sample>
    <sample id="271">我们有几点建议。首先，记录研究过程中所有相关的设计选择。其次，在研究中使用视角主义的视角进行NLP研究。</sample>
    <sample id="272">我们的第三个建议是建立针对特定社区的专门数据集和模型，一个很好的例子就是Masakani倡议。我想强调的是，包容性NLP不仅仅是让所有技术都为所有人服务。</sample>
    <sample id="273">因此，我们的演讲就到此为止。如果您想了解更多，请随时查看我们的仪表板，以获取最新的分析结果和我们的论文。谢谢！</sample>
    <sample id="274">The problems of the current SimulST models are: 1. Specific architectures are usually trained, introducing additional modules to be optimized; 2. Long and complicated training procedures (for example, training involving different optimization objectives); 3. Training and maintaining several models to reach different latency regimes (for example, training a model with an average of one-second latency and another one with two-second latency).</sample>
    <sample id="275">One effective method is to sanitize political opinions in language model training data. This helps prevent bias from spreading through the pre-training process and into downstream tasks, ultimately avoiding fairness issues. However, it's important to note that sanitizing can also risk censorship or exclusion, making it challenging to determine what should be retained in language model training data.</sample>
    <sample id="276">大家好，我是复旦大学的思雨袁。我在这里介绍我们的工作：从大型语言模型中提取特定的剧本知识，用于约束语言规划。</sample>
    <sample id="277">在日常生活中，人类通常按照分步指示来规划他们的行动，这些指示以保证的脚本形式出现。</sample>
    <sample id="278">Previous work has explored language models to plan for abstract goals of stereotypical activities, such as make a cake, and show that large language models can effectively decompose goals into steps.</sample>
    <sample id="279">然而，以往的工作主要集中在为典型的活动设定抽象目标上。为具有特定目标、特定约束的目标（例如，制作巧克力蛋糕）进行规划仍然没有得到研究。</sample>
    <sample id="280">在这篇论文中，我们定义了约束语言规划的问题。</sample>
    <sample id="281">抽象目标对规划提出了不同的约束。一个抽象目标可以由不同的真实具体目标继承，这些目标具有多面的约束。一个好的规划者应该编写合理的、符合约束的脚本。</sample>
    <sample id="282">在这篇论文中，我们首先评估并改进了大型语言模型的约束语言规划能力。</sample>
    <sample id="283">Since no dataset of specific goals exists to support our study,</sample>
    <sample id="284">We have to acquire these goals first. As shown in the table, we extend the abstract goals with multifaceted constraints for human-in-the-loop data acquisition using instructGPT</sample>
    <sample id="285">我们对100个特定的句子进行采样，并评估生成这些句子的语言模型。</sample>
    <sample id="286">这张表报告了结果的整体准确性。我们发现，所有线性回归模型在为特定目标规划上都取得了令人满意的成果。</sample>
    <sample id="287">然后，我们对详细分析进行调查，以了解为什么语言模型4</sample>
    <sample id="288">Results in the figure show that, the semantic completeness in generated scripts is acceptable, but the faithfulness to the constraints cannot be guaranteed.</sample>
    <sample id="289">We dug into a more fine-grained set of topic categories of constraints defined in WikiHow. The heatmap in the figure shows that, the planning performance of InstructGPT varies considerably for goals of different categories.</sample>
    <sample id="290">Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus we adopt the idea of over-generated then filter to improve generation quality.</sample>
    <sample id="291">We first show constrained types with examples for abstract CPT and obtain specific goals based on the said abstract goals.</sample>
    <sample id="292">然后，指导GPT生成特定目标的case scripts。</sample>
    <sample id="293">接下来，一个过滤模型被推导出来，以选择可见的脚本。</sample>
    <sample id="294">We convert scripts and goals into extracted GPT embeddings, and calculate cosine similarity as similarity scores to measure semantic similarity.</sample>
    <sample id="295">此外，我们将奖励包含目标约束关键字的脚本。我们只保留脚本，如果目标得分在目标集中的最高分。</sample>
    <sample id="296">With our method, InStyle CBT can generate scripts of higher quality. Our method greatly improves the planability both in semantics completeness and faithfulness to the constraints.</sample>
    <sample id="297">由于大型语言模型部署成本高昂，因此有必要启用小型和专业模型的语言规划能力。创建数据集是实现这一目标的关键步骤。</sample>
    <sample id="298">然而，以前的研究并不能为特定目标规划，手动数据集注释很昂贵。</sample>
    <sample id="299">因此，我们遵循符号知识精炼的理念，从大型语言模型中提炼出约束语言规划数据集。</sample>
    <sample id="300">我们将介绍一种构建约束语言规划数据集的方法，称为CoScript。</sample>
    <sample id="301">总共，我们生成了55,000个特定的脚本以确保验证和测试集的质量。为了确保质量，我们要求众包工作者找到并纠正错误的样本。</sample>
    <sample id="302">这个图显示了CoScript的约束分布。我们发现CoScript在生成特定角色时具有更高的同义词。使用CoScript，我们可以训练更小但更专业的模型来处理约束语言规划。</sample>
    <sample id="303">We found that T5 fine-tuned on the code-writ can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="304">简而言之，我们建立了约束语言规划问题。我们评估了大型语言模型的约束语言规划能力，并开发了一种泛化和过滤方法来解决大型语言模型的问题。</sample>
    <sample id="305">We use large language models to generate a high-quality CoS script for constraint language planning. We hope the CoS script dataset can be a valuable resource to advance research on language planning</sample>
    <sample id="306">谢谢你的宝贵时间。请在我们的论文中找到有关代码脚本的更多详细信息。</sample>
    <sample id="307">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="308">The watermark method needs to meet the following properties: 1. It should be applicable to embedding and services. 2. The watermark shouldn't degrade the utility of the provided embeddings. 3. The watermark should be covert enough so that attackers can remove it easily. 4. The watermark needs to be transferable to the attacker's services during the model extraction process.</sample>
    <sample id="309">TED 英语演讲已被翻译成 14 种不同的语言。</sample>
    <sample id="310">We opt to do this over looking at the demographics of original data sets' annotators because usually only a few annotators annotate each instance and because demographics are rarely collected and shared.</sample>
    <sample id="311">The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor data sets, which is defined as delta cosine and delta L2</sample>
    <sample id="312">基于编码器的多语言模型用于这项任务的方法是：将编码器和解码器结合起来，形成编码器-解码器模型。</sample>
    <sample id="344">作者假设提供者可以收集一个通用的文本语料库，并计算单词频率。</sample>
    <sample id="345">大家好，我叫徐恒。今天我要介绍我们的论文《2003年杜康诺命名实体标签是否在2023年仍然有效？》。让我们开始吧！</sample>
    <sample id="346">我们的论文使用命名实体识别任务或NER任务来研究泛化问题。</sample>
    <sample id="347">We observe that models have been used in Conlo 2003 to develop an ER for almost twenty years, and this naturally raises several problems. Firstly, can these models generalize to modern data?</sample>
    <sample id="348">当我们开发新的标签时，需要什么来实现良好的泛化？</sample>
    <sample id="349">与此同时，如果我们确实观察到泛化能力差，那么是什么导致了这些模型的性能下降呢？</sample>
    <sample id="350">To investigate these problems, we developed the Conll++ dataset. This is a dataset that we collected from Reuters news in 2020 and then annotated them with the same Conll 2003 annotation guidelines.</sample>
    <sample id="351">我们对20多个模型进行了微调。我们分别在Conll 2003和Conll++测试集上评估了它们。</sample>
    <sample id="352">最后但同样重要的是，我们计算了F1的百分比变化，以评估每个模型的一般情况。</sample>
    <sample id="353">So, what is needed for a good generalization? Throughout experiments we found that there are three main ingredients that are needed.</sample>
    <sample id="354">The first one is the model architecture. Through our experiments, we found that the transformer models normally generalize better to new data</sample>
    <sample id="355">The second ingredient is the model size. We found that usually larger models lead to better generalization</sample>
    <sample id="356">And last but not least, we all know that the number of fine-tuning examples directly affects the performance of a downstream task. Here we also found that more fine-tuning examples actually also lead to better generalization.</sample>
    <sample id="357">下一个问题是，是什么导致了一些模型的性能下降？</sample>
    <sample id="358">我们有两个假设。第一个是适应性过拟合，即由于重复使用相同的测试集而导致的过拟合，这通常表现为在新的测试集上收益递减。</sample>
    <sample id="359">第二个假设是时间漂移，这是由训练数据和测试数据之间不断扩大的时间差距引起的性能退化。</sample>
    <sample id="360">For doubt of overfitting, we saw that from the graph on the right, the red best-fit line has a gradient that is greater than 1.</sample>
    <sample id="361">这意味着我们在2003年所做的每一点改进都转化为C++中的超过一点的改进，这意味着没有递减收益。</sample>
    <sample id="362">这表明在这种情况下没有出现适应性过拟合。</sample>
    <sample id="363">那么，漂移呢？</sample>
    <sample id="364">For temporal drift, we did an experiment to retrain or continue to pretrain some models with more recent data. And we found that the performance degrades with larger temporal gap.</sample>
    <sample id="365">这证实了我们的假设，即性能下降的主要原因是温度漂移。</sample>
    <sample id="366">Our conclusion is that, for good generalization, we would need a better model architecture, larger model size, as well as more fine-tuning examples. And these go hand in hand; we can't just have one ingredient but throw out the others.</sample>
    <sample id="367">与此同时，我们还发现性能下降是由时间漂移引起的，令人惊讶的是，它并不是由Adaptive Overfitting引起的，尽管Conda 2003已经使用了20多年。</sample>
    <sample id="368">So going back to the question that we raised in the title of our paper, do Conll 2003 tags still work in 2023? And we found that the answer is actually a resounding yes.</sample>
    <sample id="369">我们希望我们的论文能引起更多关于如何改进模型泛化的研究。</sample>
    <sample id="370">And lastly, please make sure to check out our paper, our data set. And if you have any questions, feel free to contact me. Thank you so much.</sample>
    <sample id="397">该方法使用的语音片段大小是256个token。</sample>
    <sample id="398">需要 Servin 是法官 和 背景知识 法官在法庭上审理案件。</sample>
    <sample id="399">示例质量</sample>
    <sample id="400">论文侧重于GPT-4和GPT系列的语言模型。</sample>
    <sample id="401">该模型是使用特定层的注意力分数。</sample>
    <sample id="402">The most obvious thing is to use a direct reference, for example by saying the name of the song "Easy on Me" or its position: the first one.</sample>
    <sample id="403">这篇论文的作者所属机构是复旦大学。</sample>
    <sample id="404">1</sample>
    <sample id="405">Yes, we use Google Translate API to translate source to the target language.</sample>
    <sample id="406">The author gives the example of "man" as a default, unmarked word. When referring to someone who is not male (e.g., a woman), they specify their gender by adding an adjective like "woman" or "female," marking them linguistically with that descriptor.</sample>
    <sample id="407">The first one is the model architecture.</sample>
    <sample id="408">The test dataset is called "WSL".</sample>
    <sample id="409">There are two authors.</sample>
    <sample id="410">作者采用了多种模态。</sample>
    <sample id="439">作者认为 NLU 中研究不足的领域包括： 1. 理解和利用上下文信息：NLU 模型需要更好地理解上下文信息，以提高任务性能。 2. 处理多义词和歧义：NLU 需要能够处理多义词和歧义，以准确理解用户的意图。 3. 知识表示和推理：NLU 需要能够表示和推理知识，以支持复杂的 NLU 任务。 4. 集成和使用多种知识源：NLU 需要能够集成和使用多种知识源，包括预训练模型、外部知识库和用户输入。 5. 可解释性和可解释性：NLU 需要提供可解释性和可解释性，以便用户理解和信任系统。</sample>
    <sample id="440">演讲者的名字是Yin。</sample>
    <sample id="441">Yes, coscript has been through quality checks.</sample>
    <sample id="442">现有的资源只能支持有限的上下文依赖翻译类型和语言集，因为它们通常依赖于领域知识和人工编纂。</sample>
    <sample id="443">嗨，我要谈谈我们关于解决间接表达式实体选择的工作，其中我们介绍了其他实体分数。</sample>
    <sample id="444">我的名字是Javad Hosseini，这是我和Filip Radlinski、Sylvia Pariti和Anie Lewis合作完成的作品。</sample>
    <sample id="445">我们的目标是理解用户在做选择时的语言。考虑这个替代问题：你是指《Easy on Me》还是《I Got a Feeling》？这里，用户想在这两首歌之间进行选择。</sample>
    <sample id="446">最明显的方法是使用直接引用，例如通过说歌曲的名称《Easy On Me》或它的位置，第一首歌。</sample>
    <sample id="447">但有时，间接引用更合适，可以进行更自然的对话。这可能会发生在用户无法记住歌曲名称的时候。</sample>
    <sample id="448">或者发音太相似，难以分辨。</sample>
    <sample id="449">或者当用户想要指定一个偏好时。这里有一些示例间接参考，例如“更新的一个”或“那首不那么有活力的歌”。</sample>
    <sample id="450">这是对话系统中的一个重要问题，也是衡量LLMs实体理解能力的一个基准。</sample>
    <sample id="451">我们没有可用的公共数据集，也没有大规模的公共数据集可供测试，所以我们使用 crowd annotation 收集了一个。我们的数据集涵盖了三个不同的领域：音乐、书籍和食谱。</sample>
    <sample id="452">我们的数据集收集方法强调使用卡通完成集的非正式性。</sample>
    <sample id="453">卡通有三个对话气泡。在第一个气泡中，鲍勃说：“还记得我们昨天听的那首歌吗？”然后，鲍勃设置了对话的上下文。</sample>
    <sample id="454">在第二个气泡中，爱丽丝说，你是想让我放松点还是我有预感？</sample>
    <sample id="455">Bob uses an indirect reference to select one of these entities, for example the new fridge.</sample>
    <sample id="456">我们自动提供第一个和第二个气泡，但第三个是由注释员填写的。第一个气泡是从每个对话中选择几个手动提示中选择的。</sample>
    <sample id="457">第二个问题是替代问题，如下所示。</sample>
    <sample id="458">我们总是使用一个简单的模板。你是指A还是B？其中A和B都是维基百科的样本。</sample>
    <sample id="459">我们使用了不同的采样方法。当我们向上移动列表时，实体变得更加相似，通常更难进行消歧。</sample>
    <sample id="460">第一个是均匀的拉伸。</sample>
    <sample id="461">第二个是实体具有相似的标题，例如带有名称“The Return”的两本书。</sample>
    <sample id="462">第三种情况是，当他们在维基百科上有相似的描述时。最后，当他们在维基百科上有相似的信息框或属性时，例如相同的流派或相同的艺术家。</sample>
    <sample id="463">当我们将这种替代性问题展示给回答者时，他们知道这些实体的名称，但他们不一定了解这些实体。</sample>
    <sample id="464">因此，我们展示有关两个实体的一些背景知识。对于歌曲，我们只需为每首歌提供一个Google搜索链接。</sample>
    <sample id="465">然后请注释者至少听一些歌曲并阅读有关这些歌曲的内容。例如，这是歌曲“Easier”的Google搜索结果。</sample>
    <sample id="466">对于食谱和书籍领域，我们展示了维基百科的一些背景文本。对于食谱，我们还展示了它们的图片，再次来自维基百科，以便注释员知道它们看起来如何。</sample>
    <sample id="467">然后，我们请注释员选择其中一个实体，例如这里第一个，并用3到5个间接指代表达式来描述它们。</sample>
    <sample id="468">例如，带有钢琴音乐的那首歌。</sample>
    <sample id="469">The entity's corpus has 6,000 alternative questions across three domains and it has 42,000 indirect referring expressions. Results with T5-XLarge model are summarized below.</sample>
    <sample id="470">如果语言模型可以访问与注释员相同的背景知识，那么准确率会很高，大约在92%到95%之间。但这并不现实。</sample>
    <sample id="471">如果语言模型可以访问一些部分重叠的背景知识，那么准确率在82%到87%之间，这更现实。例如，当语言模型检索背景知识时。</sample>
    <sample id="472">如果语言模型只能访问实体名称，则准确率为60％，因此有很大的改进空间。我们还证明了这些模型是通用的。这是我们的数据集链接。</sample>
    <sample id="473">该方法与以下策略进行了比较： 1. Weight-Kiss 策略 2. Local Agreement 策略 3. State-of-the-art 建筑，专门针对同时语音翻译进行优化。</sample>
    <sample id="474">The authors of this paper are affiliated with the following institutions: University of Paris, France; and Inria Sophia-Antipolis, France.</sample>
    <sample id="475">Jenny</sample>
    <sample id="476">The paper has three authors: Myra, Sen Darmush, and Dan Jurafsky.</sample>
    <sample id="477">Hi, I'm Sara Papi from the University of Toronto and Fondazione Bruno Kessler. And I will briefly introduce "The Attention as a Guide for Simultaneous Speech Translation" paper that is a joint work with Matteo Negri and Marco D'Orco.</sample>
    <sample id="478">Simultaneous speech translation, or SimulST, is the process of translating spoken language into text in another language in real time, enabling cross-language communication.</sample>
    <sample id="479">The problems of current similarity models are that specific architectures are usually trained, introducing additional modules to be optimized.</sample>
    <sample id="480">例如，涉及不同优化目标的训练过程可能非常复杂。</sample>
    <sample id="481">训练和维护几个模型以达到不同的延迟模式，例如训练一个平均延迟为1秒的模型，另一个延迟为2秒的模型等等。</sample>
    <sample id="482">那我们的解决方案是什么？</sample>
    <sample id="483">首先，使用现有的离线ST模型，无需重新训练或采用特定的架构进行离线ST。为每个延迟制度使用一个模型，并通过特定参数处理延迟。</sample>
    <sample id="484">通过音频输入和文本输出之间的注意机制来分析模型已经获得的知识，即跨注意机制。您可以在右侧看到一个示例。</sample>
    <sample id="485">我们的解决方案是提出一个dot或编码解码注意力，这是一个策略，我们根据注意力指向哪里来决定是否发出部分翻译。</sample>
    <sample id="486">如果张力没有集中在某个特定的阈值α以下，则发出一个单词，这意味着接收到的信息不稳定。</sample>
    <sample id="487">例如，如果我们接收到包含“I'm going to talk about”的语音片段，并且我们的模型预测了德语的翻译，</sample>
    <sample id="488">我们将查看交叉注意力权重。</sample>
    <sample id="489">我们将看到前两个词指向最早接收到的语音帧，而最后一个词指向最后接收到的语音帧，作为lambda语音帧。</sample>
    <sample id="490">这意味着前两个单词将被省略。</sample>
    <sample id="491">While since the sum of the cross-attention is above a certain threshold α, we will not emit the last word and we wait for another speech chunk.</sample>
    <sample id="492">如果我们继续，我们又收到另一个语音块，我们的模型预测其他三个单词，我们将查看交叉注意力权重。</sample>
    <sample id="493">我们会看到，没有单词指向最后一个lambda语音帧。</sample>
    <sample id="494">这意味着这三句话将被省略。</sample>
    <sample id="495">如果您查看该调查的主要结果，</sample>
    <sample id="496">我们将同时翻译结果绘制在图表中，其中蓝色一侧测量翻译质量，平均滞后时间</sample>
    <sample id="497">这是延迟指标，我们还考虑了计算意识平均损失，它考虑了模型的计算时间来预测输出。</sample>
    <sample id="498">所以我们要让我们的治愈率尽可能高。</sample>
    <sample id="499">但是我们也希望它们向左移动。</sample>
    <sample id="500">我们还与应用到离线模型的策略进行了比较，即Whitkey策略和局部协议。我们还与专门针对同时翻译的最先进的架构进行了比较。</sample>
    <sample id="501">这些是德语同声传译策略的较旧结果。</sample>
    <sample id="502">我们看到，ADAT在离线模型中应用的所有策略中表现最好，因为它们的曲线向左移动。</sample>
    <sample id="503">我们还看到，如果我们考虑实际的流逝时间或计算工作时间，那么这是最快的策略。</sample>
    <sample id="504">如果您想发现更多结果，请阅读我们的论文。我们还发布了开源代码和模型，以及同时输出，以促进我们工作的可重复性。感谢您的关注。</sample>
    <sample id="505">Yes, the dataset is publicly available.</sample>
    <sample id="506">大家好，我叫Yin，我的同事Zhiyang和我将介绍我们关于多指令的研究，即通过指令调整来提高多模态模型的元学习能力。</sample>
    <sample id="507">随着大型语言模型的进步，许多工作开始探索以参数和数据高效的方式重新利用预训练的语言模型来执行不同的下游任务的新学习范式。</sample>
    <sample id="508">最近，许多研究表明，指令微调使大型语言模型能够以自然指令的方式在无需训练的情况下完成未见过的任务。</sample>
    <sample id="509">然而，大多数关于指令调优的工作都集中在提高语言任务上的单次性能上，而计算机视觉和多模态任务则被忽略了。</sample>
    <sample id="510">因此，在这项工作中，我们想研究一下是否可以通过对多模态预训练模型进行指令微调来提高对未见过的多模态任务的泛化能力。</sample>
    <sample id="511">此外，在我们研究的时候，我们发现自然语言处理和多模态之间的可用数据集存在相当大的差异。</sample>
    <sample id="512">There exist more than 1,600 language-only instruction tasks. However, there is no large-scale publicly available multimodal instruction task. Therefore, this motivated us to build a multimodal instruction tuning dataset.</sample>
    <sample id="513">在这里，我们介绍MultiInstruct，这是第一个多模态指令调优基准数据集，包含62个多样化的多模态任务，涵盖10个大类。</sample>
    <sample id="514">这些任务是从21个现有的开源数据集中衍生出来的，每个任务都配备了5条专家撰写的说明。</sample>
    <sample id="515">为了研究我们提出的数据集上的多模态指令调优，我们以OFA为基模型。OFA使用统一的词汇表来表示语言、图像标记和边界框坐标的坐标。</sample>
    <sample id="516">这里我们展示了我们多实例数据集的一些示例实例。</sample>
    <sample id="517">统一处理各种输入和输出数据类型。</sample>
    <sample id="518">我们遵循OFA的方法，并将所有任务统一为一个序列到序列的格式，在这个格式中，输入文本、图像、指令和边界框都在同一个标记空间中表示。</sample>
    <sample id="519">好的，现在我要谈谈多模态指令调优。</sample>
    <sample id="520">对于训练数据集，我们使用NINE组中的53个任务进行训练，并为每个任务采样10,000个实例。为了测试，我们保留整个常识推理组用于测试，并从VQA和Miscellaneous组中选择另外5个任务。</sample>
    <sample id="521">我们使用每个任务的测试集中的所有实例。此外，我们从自然指令的测试集中随机抽取20个任务作为NLP中的新任务。</sample>
    <sample id="522">我们使用预训练的OFA大型模型作为基础模型。在训练期间，我们将所有实例用于所有任务。每个实例都与其中的一个指令模板随机组合。</sample>
    <sample id="523">在测试期间，对于每个任务，我们将进行五次实验，通过使用每次实验中的一个指令来评估模型。</sample>
    <sample id="524">We report the mean and max performance and standard deviation of the performance across all five experiments.</sample>
    <sample id="525">如果任务是多模态分类任务，我们将报告准确率。如果是多模态生成任务，我们将报告ROUGE-L。对于NLP任务，我们也将报告ROUGE-L。</sample>
    <sample id="526">我们还引入了一个额外的评估指标，称为敏感度。这衡量了模型在任务中始终产生相同输出的能力，无论指令措辞的细微变化如何。</sample>
    <sample id="527">我们的主要结果如下，如我们所见，指令微调可以显著提高OFA在多模态任务中的性能。</sample>
    <sample id="528">transfer learning from natural instruction dataset can benefit instruction tuning</sample>
    <sample id="529">这里我们可以看到，随着任务量的增加，模型的性能更好，同时敏感度也更低。</sample>
    <sample id="530">我们还进行了一项实验，使用一个指令与五个指令。正如您所看到的，使用更多的指令可以提高模型的整体性能，并大大降低其敏感性。</sample>
    <sample id="531">这显示了不同的微调策略对模型敏感性的影响。如我们所见，通过从自然指令数据集中迁移学习，该模型可以实现比原始OFA模型更好的灵敏度。</sample>
    <sample id="532">我们也可以看到，自然指令数据集中的迁移学习可以帮助OFA在自然指令数据集上取得更好的性能。</sample>
    <sample id="533">总体而言，我们提出了第一个大规模多模态指令微调数据集，显著提高了OFA的短语能力，并探索了不同的迁移学习技术，并展示了它们的好处。我们设计了一个新的指标 called Sensitivity</sample>
    <sample id="534">One more thing, we are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and will release them soon. This is the QR code for our data and model. Thank you</sample>
    <sample id="535">The University of Toronto and Fondazione Bruno Kessler.</sample>
    <sample id="536">演讲者的名字是Javad Hosseini。</sample>
    <sample id="562">大家好，我是Kostov Sena，很高兴欢迎你们参加我们关于ACL 2023论文的讨论。</sample>
    <sample id="563">他与John Doak、Aaron Mueller、Kanishka Mishra、Karen Frintas、Roger Levy和Atina Viljoen合作。</sample>
    <sample id="564">因此，在这项工作中，我们重新审视了最小对齐范式。</sample>
    <sample id="565">最小对对模式基本上是在可接受性判断的基础上评估语言模型，这也可以包括语法、语义、句法或在刻板印象方面的可接受性，例如克劳斯对。</sample>
    <sample id="566">在这个最小对齐范式中，评估语言模型的典型方法是展示一个可接受的句子或一个语法正确的句子，然后展示一个不可接受的句子或一个不语法正确的句子。</sample>
    <sample id="567">然后希望模型基本上会把更多的概率放在可接受的句子上。</sample>
    <sample id="568">当前的MPP流水线基本上不允许我们评估模型对较长句子的接受度。</sample>
    <sample id="569">如今，大型语言模型正在使用越来越长的上下文窗口，因此在上下文窗口中评估模型的可接受性至关重要。</sample>
    <sample id="570">这就是我们在这里尝试做的事情。我们试图通过要求模型在更长和更长的序列上评估可接受性来重新审视MPP管道。</sample>
    <sample id="571">因此，这就是我们的方法。我们做的就是模拟这些较长的序列，重新访问数据集本身，然后通过选择可接受或不可接受的句子来重新创建句子，从这些数据集中选择。</sample>
    <sample id="572">例如，这里我们从《Blimp》数据集中选择了典型的语法对。</sample>
    <sample id="573">我们做的就是重新创建更长的序列，这些序列是可以接受的，并且具有相同的语法结构匹配。</sample>
    <sample id="574">然后我们将其作为前缀添加到可接受的查询和不可接受的查询中。</sample>
    <sample id="575">因此，我们可以选择相同的匹配中的不可接受的句子来执行相同的操作，并且这也可以用于测试模型的可接受性。</sample>
    <sample id="576">我们也可以通过从不同的子集或不同的数据集中选择句子来做到这一点。这就是我们所说的不匹配的场景。</sample>
    <sample id="577">所以这里句子仍然来自相关数据集，但不是您正在评估的相同数据集。我们也可以对不可接受性情况做同样的事情。</sample>
    <sample id="578">最后，我们可以从完全不相关的领域中选择句子，例如维基百科。</sample>
    <sample id="579">这将告诉我们模型的可接受性判断是否受到任何上下文的影响。</sample>
    <sample id="580">是否上下文来自数据集的不同子集，或者它与当前句子完全无关。</sample>
    <sample id="581">因此，MPP如何表现？首先，我们查看与当前查询对完全无关的维基百科句子，并发现MPP判断在任意上下文中大多是稳健的。</sample>
    <sample id="582">我们把上下文长度增加到1024，以使OPT和GPT-2模型达到最大值，并且在这里看到橙色虚线，MPP判断相对稳定。</sample>
    <sample id="583">现在，当我们从同一个数据集中选择句子会发生什么？</sample>
    <sample id="584">因此，我们从同一Blimp或Syntaxis数据集中选择或创建可接受和不可接受的句子。</sample>
    <sample id="585">在那里，我们看到MPP判断要么显著增加，要么显著减少，当您添加可接受的前缀或不可接受的前缀时。</sample>
    <sample id="586">但是当我们匹配结构，也就是说，当我们从同一现象中选择句子时，</sample>
    <sample id="587">我们看到MPP判断对模型的大幅增加或大幅减少，这取决于所选前缀是否可接受或不可接受。</sample>
    <sample id="588">现在，这个，呃，这个非常大，就像这个效果在整个上下文长度中增加，这可能会影响像新的语言模型这样的新语言模型，它有大的上下文窗口。</sample>
    <sample id="589">那么，为什么匹配前缀会对语言模型的判断产生如此大的影响？</sample>
    <sample id="590">我们进行了一系列分析，试图通过保留相关结构来扰动输入句子，但向输入中添加噪声。在进行几次这种扰动后，</sample>
    <sample id="591">我们发现这些噪声实际上并没有使模型改变其方向，即它如何显示MPP的特征趋势。</sample>
    <sample id="592">基本上，我们发现模型对扰动句子在相似的方式上是敏感的。</sample>
    <sample id="593">也就是说，当我们在可接受域中扰动句子时，我们看到所有扰动的增加；而当我们扰动句子在不可接受域时，我们以类似的方式看到MPP判断的减少。</sample>
    <sample id="594">她工作的关键要点是语言模型对潜在的句法和语义特征敏感，这些特征在句子之间共享。</sample>
    <sample id="595">目前我们用短句和单句输入进行MPP评估的方式，可能无法完全捕捉语言模型在上下文窗口中的抽象知识。</sample>
    <sample id="596">请阅读我们的论文，以获取我们实验的更多详细信息。谢谢您的聆听。</sample>
    <sample id="597">该方法的第一步将输入词元映射到一个无序的集合。</sample>
    <sample id="598">55,000</sample>
    <sample id="626">The best alignment method to use for German text simplification is the method of Mass Align.</sample>
    <sample id="627">弱监督学习的好处是训练算法可以稳健地在标签噪声下训练神经网络，使得训练模型仍然能够很好地泛化。</sample>
    <sample id="628">文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？</sample>
    <sample id="629">CoNLL++数据集是通过从2002年到2003年的新闻中收集数据，并使用相同的CoNLL 2003注释指南对它们进行注释而创建的。</sample>
    <sample id="630">大家好，我的名字是宾夕法尼亚州立大学的Jason。今天我将介绍我们的工作：多语言语义解析和多种自然语言和表示形式。</sample>
    <sample id="631">语义解析是将用户查询转换为语义表示的任务，例如SQL和lambda演算。</sample>
    <sample id="632">跨语言语义解析是将多种自然语言查询翻译成多种意义表示的任务。</sample>
    <sample id="633">正如图中所示，我们需要使用神经模型将查询翻译成多种自然语言，例如SQL、Lambda或FunkQL等。</sample>
    <sample id="634">现有的跨语言语义解析模型是分别提出的，并在有限的任务和应用数据集上进行评估。例如，</sample>
    <sample id="635">There are licks of coverage on certain natural language. The Chinese is missing and</sample>
    <sample id="636">他们对某些菜单表示关注。</sample>
    <sample id="637">λ演算缺失了。</sample>
    <sample id="638">或者它们只在某些神经模型上进行评估，例如只有一个单一的模型来评估。</sample>
    <sample id="639">为此，我们提出了一种示例器，即为多种自然语言和意义表示提供统一的数据集示例器，用于跨语义解析。</sample>
    <sample id="640">它包含9个数据集，涵盖多个领域，5个语义解析任务，800万种表示法和15种语言家族中的22种自然语言。</sample>
    <sample id="641">为了更好地评估我们的基准，我们考虑了训练和评估的六个设置。</sample>
    <sample id="642">第一个是翻译测试。我们将使用Google Translate API将源代码翻译成目标语言，然后使用单语模型进行训练和评估。</sample>
    <sample id="643">例如，我们用英语查询训练了一个英语模型，并在推理过程中使用API将德语查询翻译成英语，然后使用训练好的模型来预测SQL。</sample>
    <sample id="644">我们还将测试单语模型。</sample>
    <sample id="645">在这种情况下，源语言与目标语言相同，例如德语到德语或英语到英语。</sample>
    <sample id="646">我们还通过使用仅10%的训练数据训练单语言模型来测试单语言融合设置。</sample>
    <sample id="647">并且，我们还训练了一个多语言模型，用于所有语言。</sample>
    <sample id="648">例如，我们将德语、英语和中文查询放在一起训练一个多语言模型，在推理过程中，我们可以使用此模型来</sample>
    <sample id="649">好的，我可以帮助你翻译英文内容。请问你需要翻译什么？</sample>
    <sample id="650">我们还考虑跨链接零点和零点转移。我们在一种源语言上进行训练，然后转移到另一种语言。</sample>
    <sample id="651">在训练期间，我们使用英语查询或英语和德语的组合查询来训练一个多语言模型，并预测SQL输出。</sample>
    <sample id="652">我们还发现了许多有趣的结果。关于对单语模型的分析，我们将评估两个模型组。</sample>
    <sample id="653">包括编码器，它代表多语言预训练编码器，以及基于指针的解码器，例如XLM-R + PTR和BERT + PTR。</sample>
    <sample id="654">我们还评估了编码器-解码器模型，即多语言预训练的编码器-解码器模型，例如mBART和MT5。</sample>
    <sample id="655">我们发现编码器-解码器在所有九个数据集上表现最佳。</sample>
    <sample id="656">我们在多语言环境中对 MT-5 和 XLM-R + BTR 进行了评估。</sample>
    <sample id="657">我们觉得编码器-解码器或编码器-PHR可以通过在各种语言的混合中进行训练来改进。</sample>
    <sample id="658">我们发现这是由于大多数主要自然语言可以获得性能提升，除了英语在七个数据集中性能下降，在三个数据集中性能提升。</sample>
    <sample id="659">我认为这被称为多语言的诅咒。</sample>
    <sample id="660">我们还比较了跨语言性能差距。</sample>
    <sample id="661">在这张图中，蓝色的线是跨语言一射转移，橙色的线是跨语言零射转移，绿色的线是单语言环境。</sample>
    <sample id="662">我们发现，通过比较绿色和橙色的线条，我们发现，在零-shot设置中，跨语言转移性能差距是显著的。通过比较蓝色和橙色的线条，我们发现，在few-shot设置中，转移差距会迅速缩短。</sample>
    <sample id="663">我们还发现了一些其他有趣的结果。例如，编码器-解码器优于先前的工作，或实现了可比的结果。在英语自然语言上的训练显著提高了对目标自然语言的Few Shot性能。</sample>
    <sample id="664">发现多语言语言模型，如Codex和Bloom，对于跨语言语义解析任务仍然不够。</sample>
    <sample id="665">总的来说，我们构建了一个多语言、多表示形式的统一跨语言语义解析基准。</sample>
    <sample id="666">我们将对三种代表性的多语言模型进行全面的基准研究，我们的结果显示了有趣的发现等等。欢迎访问我们的论文和代码。谢谢收听。</sample>
    <sample id="667">Existing works can be broadly classified into four categories.</sample>
    <sample id="668">No, Codex or Bloom are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="695">该方法通过将对齐作为训练的一部分来处理排列的不确定性。</sample>
    <sample id="696">下游NLP模型的公平性是指在使用这些模型时，确保它们不会对特定群体或个人产生不公平的影响。这包括避免模型偏向某些政治观点、性别、种族或其他特征，从而确保所有用户都能公平地获得服务和机会。</sample>
    <sample id="697">Yanis Lebrec</sample>
    <sample id="698">演讲者的名字是Costa Senna。</sample>
    <sample id="699">演讲者的名字是Myra。</sample>
    <sample id="700">Tropicalism is a cultural movement that originated in Brazil during the 1960s. It was characterized by its embrace of Brazilian culture, music, and art, as well as its rejection of Western influences. The term "tropicalism" refers to both the style itself and the broader cultural phenomenon it represents. In literature, tropicalism often involves themes related to nature, exotic locations, vibrant colors, and an emphasis on sensory experiences.</sample>
    <sample id="701">作者通过使用诸如文化、传统、自豪和异国情调等词语来描述目标群体，这些词语定义了这些群体与其身份的关系，并将它们与白人规范区分开来。</sample>
    <sample id="702">Point-wise CxMI</sample>
    <sample id="703">DrBERT 是一个从头开始训练的模型，而 Chubert 则是一个临床模型。</sample>
    <sample id="751">这篇论文有两位作者。</sample>
    <sample id="752">Iterative updates the model by training on the latest set of data collected.</sample>
    <sample id="753">数据集的目标是理解用户在做选择时的语言。</sample>
    <sample id="754">攻击者通过EaaS来提取模型参数，通过使用EaaS来提取模型参数。</sample>
    <sample id="755">There are three authors.</sample>
    <sample id="756">Two</sample>
    <sample id="757">The authors of this paper are affiliated with Carnegie Mellon University, the University of Washington, and the Allen Institute for AI.</sample>
    <sample id="758">The example is "I saw Bart and Lisa".</sample>
    <sample id="759">对话系统中的最先进模型是GPT-3。</sample>
    <sample id="760">To evaluate the model's acceptability throughout the context window.</sample>
    <sample id="761">Yes, it is.</sample>
    <sample id="762">No, they don't necessarily know about the entity.</sample>
    <sample id="763">BLEU, METEOR, ROUGE, CIDEr</sample>
    <sample id="764">泛化中的回归会影响特定的 NER 类型。</sample>
    <sample id="765">NLP 中的立场很重要，因为不同的立场会导致不同的结果。例如，在检测有毒内容时，如果 API 对于某些特定群体（如 Carl Jones 和 Dithya Sharma）的检测效果不同，这表明该 API 可能存在设计偏见。这种偏见可能导致对某些群体的不公平对待，因此在 NLP 领域中，了解和解决这些偏见是非常重要的。</sample>
    <sample id="766">BLOOM 采用适配器微调。</sample>
    <sample id="767">They use a zero-shot transfer learning approach.</sample>
    <sample id="768">The recent tests used to evaluate PaLM's capabilities include: 1. The original PaLM test set, which assesses the model's understanding of language and its ability to perform various tasks such as answering questions, completing sentences, and generating text. 2. The PaLM-2 test set, an expanded version that includes more diverse prompts and challenges designed to push the limits of the larger PaLM-2 model. This new test set incorporates a wider range of domains and complexities compared to the initial PaLM evaluation. These testing sets are crucial for gauging how well the models can handle different types of natural language processing (NLP) tasks and their overall performance improvements with increased scale and complexity.</sample>
    <sample id="769">作者最终提出了三条建议。</sample>
    <sample id="770">提议的方法比最强的基线获得了1.5倍的收益。</sample>
    <sample id="771">演说者的名字是Shu-Hung。</sample>
    <sample id="772">论文中的结果和数据集可以用作基准吗？</sample>
    <sample id="773">They conducted 5 smaller models experiments.</sample>
    <sample id="774">OFA</sample>
    <sample id="833">这篇论文的作者所属机构是Google Translate。</sample>
    <sample id="834">Stony Brook University</sample>
    <sample id="835">The paper analyzes the following languages: English, Spanish, German, French, Italian, Portuguese, Dutch, Swedish, Danish, Norwegian, Finnish, and Russian.</sample>
    <sample id="836">演讲者的名字是张斌。</sample>
    <sample id="837">Two models were fine-tuned: one for producing document-level simplifications and another for sentence-level simplifications.</sample>
    <sample id="838">53 tasks are used for training, and 10,000 instances per task are sampled for testing.</sample>
    <sample id="839">There are 10 authors.</sample>
    <sample id="840">We conduct experiments on four datasets: AG News, MIMD, SSD2 and EER Spam.</sample>
    <sample id="876">NACHOS 是一个医疗爬虫数据集。</sample>
    <sample id="877">The speaker's name is Ariel Beilag.</sample>
    <sample id="878">The prompting has a big influence on the performance of LLMs for translation.</sample>
    <sample id="879">这篇论文的作者所属机构是：Patrick Fernandes, Amy Liu, Andre F. T. Martins和Graham Neubig。</sample>
    <sample id="880">The five expert-written instructions are: 1. Define the problem clearly and concisely; 2. Use simple language that is easy to understand; 3. Provide specific examples or scenarios when possible; 4. Avoid jargon or technical terms unless absolutely necessary; 5. Ensure clarity in all steps provided.</sample>
    <sample id="881">Use a coreference resolution task to test the model.</sample>
    <sample id="882">大家好，我的名字是Aryel Bila，我将简要介绍一篇关于评估策略和性能的论文。这是我和Google Translate同事合作完成的工作。</sample>
    <sample id="883">Bram是一个于2022年发布的540亿参数的大型语言模型。它是在一个包含1800亿个标记的大文本集合上进行训练的。</sample>
    <sample id="884">在发布日期，它在数百个NLP任务中实现了最先进的性能。</sample>
    <sample id="885">在这项工作中，我们提出了对机器翻译的大型语言模型提示的第一次系统性研究。</sample>
    <sample id="886">我们使用IMT社区的最佳实践来评估这些模型的翻译能力。这包括使用最新的测试集，以避免测试数据与语言模型训练数据重叠。</sample>
    <sample id="887">我们比较了两个最先进的系统，即WMT评估中表现最佳的系统。</sample>
    <sample id="888">我们使用最先进的神经网络指标，并且还显示基于专家的人类评估结果。最后，我们提供了一些关于提示选择策略的建议。</sample>
    <sample id="889">Prompting 对于翻译的 LLM 性能有显著影响，如我们在一个简单的实验中所看到的，即我们使用单次提示，并为句子提供两个不同的提示。</sample>
    <sample id="890">大多数句子（1000个句子中的516个），观察到的差异超过一个模糊点。</sample>
    <sample id="891">在极端情况下，这可能会达到40个错误点。因此，选择一个好的提示策略非常重要。</sample>
    <sample id="892">在我们的实验中，我们为一个五句提示策略寻找了一个解决方案，其中我们用句子中的语言标记每个句子。</sample>
    <sample id="893">在本例中，我们从德语翻译成英语，德语句子是源句子，用冒号标记，英语翻译用冒号标记。</sample>
    <sample id="894">我们看到，实际的提示形式在多个短提示的情况下，并没有很大的影响。</sample>
    <sample id="895">对于零和一的提示来说，这至关重要。当我们像我们的情况一样进行五次提示时，实际上提示的形式几乎没有区别。</sample>
    <sample id="896">是例子承载了大部分的重量。</sample>
    <sample id="897">我们实验结果的总结是，示例质量比与源句子的相似度更重要。</sample>
    <sample id="898">因此，选择高质量翻译的示例很重要。特别是，我们比较了从训练数据中的WMT评估或开发数据中选择的提示。</sample>
    <sample id="899">开发数据比训练数据更准确，质量更高，因此使用开发数据时性能更好。</sample>
    <sample id="900">尽管如此，专门的最先进的系统在翻译方面具有显著的优势，但 Farm 接近于一个商业系统。在我们的情况下，我们选择了 Google Translate 进行评估。</sample>
    <sample id="901">我们从使用MQM框架进行的评估中获得的见解是，Palm的流畅性与最先进的系统相当，但主要差异在于准确性。</sample>
    <sample id="902">特别是，最常见的错误是遗漏错误。</sample>
    <sample id="903">看起来，Palm 有时会通过省略源句中的一些部分来选择生成更好的翻译。</sample>
    <sample id="904">然而，PAN的风格外类别低于最先进的系统，这是一个额外的信号。</sample>
    <sample id="905">Palm提供了流畅的输出，但准确度有所下降。</sample>
    <sample id="906">这就是这次简短的概述。如果您想了解更多信息，请参加完整的论文演示。非常感谢！</sample>
    <sample id="907">你好，我是大卫，是德国萨尔兰大学的博士研究生。在这段视频中，我想向大家介绍我们最近的研究成果——《比你想象的更弱：对周际强化学习的批判性审视》。</sample>
    <sample id="908">这是与小雨沈、马约·穆斯巴和迪特·克拉克合作完成的。</sample>
    <sample id="909">我想从对监督学习和监督学习的简要介绍开始。</sample>
    <sample id="910">在弱监督中，我们没有手动标记数据，而是使用弱标记源来标记数据，例如简单的启发式规则、知识库或低质量的外包，如右图所示。</sample>
    <sample id="911">与人类注释相比，维基注释要便宜得多，但它们也很嘈杂，这意味着一定量的注释是不正确的。</sample>
    <sample id="912">如果我们直接在弱标签数据上训练神经网络，神经网络往往会记住标签噪声，而不能泛化。</sample>
    <sample id="913">在弱监督学习中，提出了训练算法来稳健地在这样的标签噪声下训练神经网络，以便训练模型仍然很好地泛化。</sample>
    <sample id="914">在最近的WSL工作中，WSL代表每周监督学习。一个常见的说法是，人们说他们只用每周的标记数据训练模型，并在干净的测试集上取得高表现。</sample>
    <sample id="915">从技术上讲，这项声明并不完全正确，但有一个陷阱。</sample>
    <sample id="916">人们假设有一个额外的干净验证集可供模型选择。</sample>
    <sample id="917">We can't stop on this problem setting, as this implies that additional manual annotations are required in weakly supervised learning. But, like an elephant in the room, this necessity is often overlooked.</sample>
    <sample id="918">作者提出了三个研究问题：首先，WSL是否需要干净的验证数据？或者，我们是否可以使用噪声验证集代替？</sample>
    <sample id="919">第二，如果需要干净的数据或WSL2工作必须使用干净的数据，则我们需要多少干净的样本？最后，我们应该只使用干净的样本作为基础，还是有更好的利用方式？</sample>
    <sample id="920">我们在工作中解决了这些问题，我们的发现如下。</sample>
    <sample id="921">首先，我们发现，有趣的是，最近的WSL方法确实需要干净的验证样本才能正常工作。</sample>
    <sample id="922">否则，数据可能会出现性能下降，如图所示。如果没有干净的验证样本，则训练模型无法超越原始的弱标签进行泛化。</sample>
    <sample id="923">这意味着培训是徒劳的。</sample>
    <sample id="924">这表明WSL方法实际上需要清晰标记的数据才能正常工作，并且获得干净验证样本的注释成本不应被忽视。</sample>
    <sample id="925">我们的第二个发现是，增加干净验证样本的数量将有助于WSL方法实现更好的性能，如左图所示。</sample>
    <sample id="926">通常，我们只需要每节课20个样本即可获得高分。</sample>
    <sample id="927">但这不是故事的结尾，因为如果我们决定以任何方式访问干净的样本，那么直接在它们上进行训练甚至可以达到更好的性能。</sample>
    <sample id="928">右边的图显示了直接应用于干净数据的微调方法与仅使用干净数据进行验证的WSL方法之间的性能差异。</sample>
    <sample id="929">正如您所看到的，如果我们每节课有10个样本，直接微调开始超越WSL方法。</sample>
    <sample id="930">最后，通过允许继续在干净的验证样本上进行微调，可以轻松实现之前WSL方法中声称的性能改进。</sample>
    <sample id="931">如图所示，Valina模型FTW最初表现不佳，比WSL方法更复杂，如余弦。</sample>
    <sample id="932">然而，如果我们允许在干净的样本上进行微调，则FTW的表现与其他方法一样好。</sample>
    <sample id="933">因此，在实践中，没有理由选择更复杂的WSL方法，因为它们需要更多的计算时间和磁盘空间。</sample>
    <sample id="934">To summarize, we showed that recent WSL approaches require clean manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated.</sample>
    <sample id="935">我们对未来工作的具体建议如下。</sample>
    <sample id="936">首先，报告模型选择标准。例如，报告是否使用干净的验证样本进行模型选择。</sample>
    <sample id="937">第二，WSL方法应与FSL基线进行比较，因为两者都在创建示例上工作。第三，连续微调是一个简单而强大的基线，应该在WSL的未来工作中考虑。</sample>
    <sample id="938">最后，我们开源了我们的代码。您可以在本幻灯片上的二维码中找到它。请随意查看。谢谢，并祝会议愉快！</sample>
    <sample id="939">The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="940">The paper has five authors.</sample>
    <sample id="941">需要 Servin 是法官和 Kea 是面包师的背景知识。</sample>
    <sample id="942">The code is available on GitHub.</sample>
    <sample id="943">No, the NLPositionality's annotators are not evenly distributed across different demographic characteristics.</sample>
    <sample id="944">在可接受的域中，通过添加噪声来扰乱句子。</sample>
    <sample id="945">维度评估意味着对模型的多个方面进行评估，以了解其在更精细的粒度上的优势和劣势。</sample>
    <sample id="946">这篇论文的作者所属机构是University of Science and Technology of China。</sample>
    <sample id="947">提示的形式在几个短提示中很重要。</sample>
    <sample id="978">The author evaluated several conversational AI models, including those from OpenAI, Google, Microsoft, and Facebook.</sample>
    <sample id="979">This paper has 10 authors.</sample>
    <sample id="980">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="981">这篇论文有三位作者。</sample>
    <sample id="982">The speaker's name is Vasudha.</sample>
    <sample id="983">The author's institution is the University of Warsaw.</sample>
    <sample id="1021">PaLM最常见的错误是遗漏错误。</sample>
    <sample id="1022">大家好，我是James Finch。我是Sarah Finch。今天我们将向您介绍ABC-Eval，一种评估对话型AI的新维度方法。</sample>
    <sample id="1023">这项工作由埃默里大学的埃默里NLP实验室完成，该实验室由埃默里大学的Jino Choi教授领导，并与亚马逊Alexa AI合作。</sample>
    <sample id="1024">假设你刚刚开发了一个对话模型，你想看看它与当前最先进的技术相比如何。</sample>
    <sample id="1025">常用的做法是使用人类评估，例如请人类裁判员选择哪两个对话更好，或者根据李克特量表对对话进行评分。</sample>
    <sample id="1026">这些方法在整体对话质量的综合评估方面表现良好，但对话质量有多个方面。因此，你可能需要评估聊天质量的多个维度，以更精细地了解模型的优势和劣势。</sample>
    <sample id="1027">一种方法是让人类法官使用现有的比较或等级评分方法来评估对话质量的几个维度，例如模型回复的相关性。</sample>
    <sample id="1028">然而，我们认为有一种更精确、更可靠的维度对话评估策略。</sample>
    <sample id="1029">我们的方法试图通过明确标注每个模型响应是否表达某些行为来减少人类评估的主观性，例如以无关信息作答或自相矛盾。</sample>
    <sample id="1030">我们称这种做法为“聊天行为注释”，简称ABC-Eval。我们开发了这种方法，以全面覆盖最近文献中建议影响聊天质量的聊天模型行为。</sample>
    <sample id="1031">ABC-Eval can measure how often chat models make different types of thematic errors.</sample>
    <sample id="1032">例如，ABC-Eval测量聊天模型忽略其伙伴或说无关内容的次数。</sample>
    <sample id="1033">It contradicts itself or its partner, hallucinates incorrect facts, violates common sense knowledge, and fails to show empathy.</sample>
    <sample id="1034">为了确定哪种评估方法最有效，我们选择了四个最先进的聊天模型，并使用ABC-Eval在每个模型上进行了100次人机对话的评估。</sample>
    <sample id="1035">为了比较，我们还使用了三种现有的方法来评估这些对话：在回合级别上的李克特评分、在对话级别上的李克特评分以及对话级别上的成对比较。</sample>
    <sample id="1036">对于现有的每种方法，我们收集了对对话中最常测量的八个方面进行评估，因为这是评估聊天模型沿多个维度的标准做法。</sample>
    <sample id="1037">通过分析这些评估结果，我们发现ABC行为标签总体上比现有方法收集的标签更可靠，这是通过在100个双标对话中进行注释员之间的相互同意来衡量的。</sample>
    <sample id="1038">此外，ABC-EVAL标签比现有方法产生的指标更能预测整体对话质量，如本简单线性回归分析所示。</sample>
    <sample id="1039">例如，你可以看到测量自我和伴侣矛盾的比例可以解释对话质量的5%和10%，而平均的李克特一致性得分只能解释4%或更少。</sample>
    <sample id="1040">最后，我们使用分步线性回归来检查每个评估指标是否捕获了聊天质量的一个独特方面。</sample>
    <sample id="1041">你可以看到所有ABC-EVAL指标的组合解释了超过25%的对话质量。当你一次移除一个指标时，大多数指标都会导致失去相当多关于质量的信息。</sample>
    <sample id="1042">另一方面，所有等级量表指标的组合解释了质量的更少部分，并且这些指标中很少有能提供独特的信息。</sample>
    <sample id="1043">这些可靠、信息丰富且独特的ABCEVAL指标使我们能够以比以前的方法更高的分辨率来评估对话式AI。</sample>
    <sample id="1044">你可以看到，在我们的实验结果中，仍然存在一些挑战，并且已经精确量化了。例如，我们测试的机器人在大约20%的回答中都违反了常识。</sample>
    <sample id="1045">They produce irrelevant information in about 15% of the responses and they contradict themselves or their partner around 10% of the time.</sample>
    <sample id="1046">由于该领域的快速发展，许多这些错误率在我们进行评估后可能会因新模型的发布而减少。然而，这正是追求可靠和精确的模型比较评估指标的原因。</sample>
    <sample id="1047">我们希望ABC-Eval能被该领域的其他人利用，作为朝着这个方向迈出的重要一步。我们期待着看到在接下来的几个月和几年中，对话式AI将如何发展。谢谢观看。</sample>
    <sample id="1048">这篇论文的作者所属机构是埃默里大学。</sample>
    <sample id="1049">Clean manually annotated samples</sample>
    <sample id="1050">这篇论文有6位作者。</sample>
    <sample id="1051">你好，我的名字是Kaiyuan，我将为大家介绍我们的作品《翻译需要上下文吗？一种数据驱动的多语言探索》。这项工作是在与Patrick Fernandes、Amy Liu、Andre F.D. Martins和Graham Neubig的合作下完成的。</sample>
    <sample id="1052">很多翻译取决于上下文。例如，我们如何将“mole”翻译成这个句子？</sample>
    <sample id="1053">如果上一句话是“部长们发现后事情可能会变得危险”，那么“more”指的是间谍。但如果上一句话是“这会有什么严重的问题吗，医生？”，那么“more”指的是胎记。</sample>
    <sample id="1054">因此，根据上下文，单词的含义会改变，因此它的翻译也会改变。</sample>
    <sample id="1055">然而，评估模型在这些情况下的表现是相当困难的。首先是因为只有很小一部分的翻译依赖于上下文，这使得语料库级别的指标如BLEU无法捕捉到这些翻译。</sample>
    <sample id="1056">一些人建议对上下文依赖的翻译进行目标评估，但这些资源只支持有限类型的上下文依赖翻译和有限的语言集，因为它们通常依赖于领域知识和人类编纂。</sample>
    <sample id="1057">在这项工作中，我们尝试回答这两个问题：首先，翻译是否需要上下文？其次，模型如何处理这些情况？</sample>
    <sample id="1058">为了回答第一个问题，我们首先测量了单词在翻译过程中依赖上下文的程度。</sample>
    <sample id="1059">在之前的论文中，我们介绍了上下文使用机器翻译模型的指标——上下文相关性指数（Contextual Similarity Measure for Machine Translation, CXMI）。CXMI通过衡量上下文C关于目标Y的信息量，给定源X来实现这一目标。</sample>
    <sample id="1060">你可以将C Xiaomi视为给模型提供上下文所获得的信息。</sample>
    <sample id="1061">在本文中，我们扩展了上下文相关性指标（CMI），以点对点CMI的形式，可以测量句子级别或单词级别的上下文使用。我们可以将具有高p-CMI的单词视为需要上下文进行翻译的单词。</sample>
    <sample id="1062">现在我们分析具有高pSMI的单词，以查找这些单词之间的模式。</sample>
    <sample id="1063">然后我们对翻译成14种不同语言的TED演讲转录进行分析。</sample>
    <sample id="1064">我们从三个不同的层次进行分析。首先，我们看那些具有高平均PCMI的词性标记。</sample>
    <sample id="1065">这使我们能够找到，例如，阿拉伯语中的双重代词，其相对较高的PMI。这是因为英语没有双重代词，因此在翻译成阿拉伯语时需要上下文来确定一个代词是否是双重的。</sample>
    <sample id="1066">同样，我们发现某些语言在选择适当的动词形式时也需要上下文。然后我们来看那些具有高p-semi平均值的词汇项，它出现在不同的场合中。</sample>
    <sample id="1067">这有助于识别需要上下文才能在文档中使用相同翻译的中文专有名词。</sample>
    <sample id="1068">同样，我们发现上下文支持以适当的正式性进行翻译。</sample>
    <sample id="1069">最后，我们研究具有高p-SMI的不同个体标记。这使我们能够识别无法仅通过单词本身捕获的现象，但这些现象在句法结构中表达出来，例如省略号的解决。</sample>
    <sample id="1070">现在，我们使用分析结果来设计文档级别的翻译基准。</sample>
    <sample id="1071">对于我们识别的五个话语现象，我们创建了标记器来自动识别属于这些现象的单词，并将我们的标记器称为多语言话语意识或多DA标记器。</sample>
    <sample id="1072">然后，我们也可以注意到不同的语言有不同的这些话语现象的比例。</sample>
    <sample id="1073">然后，我们使用Muda标签器，通过在我们想要用于评估的平行语料库上应用标签器来应用Muda标签器。然后，我们选择我们的翻译指标，并将其应用于Muda标签器识别的上下文依赖示例。</sample>
    <sample id="1074">最后，我们使用基准以及其它指标来评估不同模型在文档级机器翻译中的表现。</sample>
    <sample id="1075">首先，当我们使用语料库级别的指标时，对于蓝色，我们发现无偏模型表现最好。</sample>
    <sample id="1076">但是，如果使用逗号，则上下文感知模型表现最佳。如果我们使用F度量，则具有和没有上下文的模型具有可比性能。</sample>
    <sample id="1077">这再次表明，如果我们仅使用语料库级别的指标，就很难确定最佳的文档级别翻译系统。</sample>
    <sample id="1078">现在我们使用 MovieLens 评估模型，并发现上下文摘要模型在某些语境现象（如正式性和词汇连贯性）上比不使用上下文的模型更准确。</sample>
    <sample id="1079">但是这些模型在其他现象，如省略号、代词和动词形式上没有使用上下文的情况下，表现得并不比那些不使用上下文的模型好。这表明我们需要在文档级翻译中看到更多的进步。</sample>
    <sample id="1080">我们还比较了不同的商业系统，我们的基准测试显示DeepL通常比Google Translate更准确，用于文档和网页翻译。</sample>
    <sample id="1081">总的来说，我们对14种语言对进行数据驱动分析，以识别需要上下文的翻译。</sample>
    <sample id="1082">然后，我们利用这些发现来建立一个文档级机器翻译的基准，这可以帮助我们确定哪些话语现象模型处理得很好，或者不好，以及哪些翻译系统擅长文档级翻译。</sample>
    <sample id="1083">非常感谢您的关注，期待在多伦多与您见面！</sample>
    <sample id="1084">演讲者的名字是Yuxin Zhang。</sample>
    <sample id="1121">The new method does not have a name.</sample>
    <sample id="1122">作者将“显性词汇”描述为一种方法，用于识别区分标记组和非标记组的单词。</sample>
    <sample id="1123">University of Washington</sample>
    <sample id="1124">The Prag approach</sample>
    <sample id="1125">演讲者的名字是James Finch和Sarah Finch。</sample>
    <sample id="1126">这篇论文有4位作者。</sample>
    <sample id="1127">The minimal pair paradigm can be used to evaluate language models on top of acceptability judgments, which may include grammaticality, syntax, or acceptability in terms of stereotypes.</sample>
    <sample id="1161">WSL approaches require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked.</sample>
    <sample id="1162">The model was evaluated on 11 biomedical and clinical downstream tasks.</sample>
    <sample id="1226">CamemBERT 最初是在 PubMed 的数据上训练的。</sample>
    <sample id="1227">演讲者的名字是Szymon Skurkowski。</sample>
    <sample id="1228">重新训练或继续预训练一些模型，使用更近期的数据。</sample>
    <sample id="1269">为了将输出序列中的词元排列成正确的顺序，我们需要进行排列。</sample>
    <sample id="1270">作者建议模型所有者应提高偏见缓解方法的透明度，因为例如这些积极的刻板印象，我们不知道是因为某种过度的价值对齐，还是其他反刻板印象的方法导致了这些有害模式。</sample>
    <sample id="1271">最小对不可接受输入是典型的语言模型评估方式。</sample>
    <sample id="1272">作者使用了以下评估指标：1. F1分数 2. BLEU分数 3. ROUGE分数</sample>
    <sample id="1273">Inter-annotator agreement was used to measure the consistency of labels.</sample>
    <sample id="1274">完全无关的领域</sample>
    <sample id="1275">The authors of this paper are affiliated with the following institutions: University of Bremen, Germany; University of Applied Sciences Osnabrück, Germany.</sample>
    <sample id="1276">MultiInstruct 与其他基准有何不同？</sample>
    <sample id="1277">这篇论文有两位作者。</sample>
    <sample id="1278">二进制协调是指通过测量字符长度、音节长度和单词长度来确定文本的结构。</sample>
    <sample id="1279">提示语的平均长度为10.3个字符。</sample>
    <sample id="1280">These findings suggest that smaller models, like T5, can outperform larger language models when trained on appropriate datasets.</sample>
    <sample id="1281">嗨，我是Yanis Levarac，我将向您介绍我们的作品“Doctor BERT”，这是一个在法语中用于生物医学和临床领域的稳健预训练模型。</sample>
    <sample id="1282">在这次演讲中，我们首先将讨论医疗保健中的语言建模。然后我们将介绍我们文章的主要贡献。</sample>
    <sample id="1283">我们介绍了第一个生物医学模型，用法语命名，叫做Dr. BERT，它基于Roberta，并在NATOS上进行训练，这是一个从网络中爬取的医疗数据集。</sample>
    <sample id="1284">我们还介绍了模型与多点预训练设置和数据源的比较。然后，我们将结果展示在11个生物医学和临床下游任务中。</sample>
    <sample id="1285">最后，我们总结了实验结果，并提供了如何访问这些模型的更多详细信息。</sample>
    <sample id="1286">自2018年发布以来，BERT已成为解决自然语言处理任务的最有效方法之一，并与Word2vec、FastText和Enrich等历史静态和上下文化方法相比，取得了巨大的性能提升。</sample>
    <sample id="1287">从那以后，这个模型被改编成其他语言，比如法语中的Camembert，在生物医学领域中的PubMed BERT和BioBERT，以及在临床领域中的Clinical BERT，但主要是英语。</sample>
    <sample id="1288">专门用于其他语言的模型很少，通常基于持续训练，因为缺乏领域数据。</sample>
    <sample id="1289">然而，French没有生物医学和临床的开源模型。</sample>
    <sample id="1290">我们应该问自己，最适合各种用途的数据来源是什么？而这些 crowdsourced数据是临床数据的良好替代品。</sample>
    <sample id="1291">为了回答这个问题，我们比较了 Bert 博士和我们的 Shulbert 模型，该模型基于从 Nantes 大学医院数据仓库中获得的匿名数据。</sample>
    <sample id="1292">最后，我们问自己，我们需要多少数据来训练一个专门针对法语的数据的模型？是4GB、8GB还是更多？</sample>
    <sample id="1293">为了回答这个问题，我们首先训练并比较了四个从头开始的模型：一个使用7GB的NATOS的第一个版本的Dr. BERT、一个使用4GB NATOS子集的第二个版本、</sample>
    <sample id="1294">一个临床模型的初始版本，包含4GB的临床笔记句子，以及一个最终版本，包含4GB的自然句子和4GB的临床笔记。</sample>
    <sample id="1295">除了这个比较，我们还介绍了三种在持续预训练上训练的模型，以分析预训练策略的影响。</sample>
    <sample id="1296">一个基于Camembert的训练，使用4GB的NACHOS数据集，另一个基于Camembert的训练，使用4GB的clean notes数据集。</sample>
    <sample id="1297">最后，基于英语语义模型BERT，我们训练了4GB的Snatch数据集。总共有7个模型。</sample>
    <sample id="1298">为了评估这七种模型，我们收集了公共和私人数据集，例如命名实体识别、分类、词性标注和问答。</sample>
    <sample id="1299">这些模型与六个基准模型进行比较，包括卡蒙贝尔奥斯卡138GB、卡蒙贝尔奥斯卡4GB、卡蒙贝尔CCNet 4GB、PalmBERT、BioBERT和ClinicalBERT。</sample>
    <sample id="1300">模型在任务上表现得最好的时候，其数据与训练数据的性质相同。</sample>
    <sample id="1301">然而，我们可以从异构数据源中获得数据。我们还观察到使用更多的数据可以转化为更好的性能。</sample>
    <sample id="1302">总体而言，从头开始编写似乎在大多数任务中表现更好。</sample>
    <sample id="1303">然而，使用PubMedBERT的white和分词器在Nature的4GB子集上进行的连续解释实验显示了与从头开始训练的4GBDoctorBERT获得的结果相当的结果。</sample>
    <sample id="1304">然而，基于Kamionkowski权重和分词器的模型则会遭受稳定性问题。</sample>
    <sample id="1305">最后，作为结论，我们的提议系统在11个目标任务中的9个上表现更好，并且在全球超过了这里Camembert的通用模型的结果。</sample>
    <sample id="1306">我们也观察到，专门的数据更好，更多的专门数据更好，但它并不好扩展。</sample>
    <sample id="1307">The pre-trained model from Nachos is freely available on Hugging Face, and all the training scripts are in our GitHub repository.</sample>
    <sample id="1308">谢谢这次展示，我们期待在托伦特的后续会议中进行交流。</sample>
    <sample id="1309">论文研究了从头开始训练和持续预训练的四种学习策略。</sample>
    <sample id="1310">This means that there is no diminishing returns.</sample>
    <sample id="1311">We have fine-tuned two different models. We have fine-tuned the model of longimp to produce document-level simplifications, and we also fine-tuned the normal-based longimp to produce sentence-level simplifications.</sample>
    <sample id="1312">Yes, language models have varying political leanings.</sample>
    <sample id="1313">嗨，我的名字是Matthias Lindemann，今天我将简要介绍我们关于使用多集标记和潜在置换的无树组合泛化论文。</sample>
    <sample id="1314">这是我和我的顾问Alexander Colla和Ivan Tito的合作。</sample>
    <sample id="1315">组合泛化可以理解为学习者处理更深层次的递归和在训练期间单独看到的短语的未见组合的能力。</sample>
    <sample id="1316">在语义解析的背景下，测试组合概括可能看起来像这样。像往常一样，我们有一个训练集的陈述，在这种情况下是“女孩睡觉了”和“玛丽知道女孩睡觉了”。</sample>
    <sample id="1317">这些陈述与代表其含义核心方面的逻辑形式配对。</sample>
    <sample id="1318">与标准机器学习评估不同，测试集不是来自相同的分布，而是包含结构上未见过的逻辑形式。</sample>
    <sample id="1319">在这个例子中，模型在训练过程中看到了较浅的递归，并在较深的递归上进行了测试。</sample>
    <sample id="1320">简单地将序列映射到序列的模型在处理这种分布外泛化时遇到了困难，通常会生成与输入脱节的输出。</sample>
    <sample id="1321">特别是，它们经常无法重现输入和输出之间的系统对应关系，例如在示例中着色的那些。</sample>
    <sample id="1322">一种流行的方法是将树集成到模型中。</sample>
    <sample id="1323">这些树旨在捕捉与逻辑形式相关的表达过程。</sample>
    <sample id="1324">这个方法很好，但树通常不会被提供，需要以某种方式获取。</sample>
    <sample id="1325">这可能是一个复杂的过程，有时甚至是一个计算上昂贵的过程。通常，这涉及到大量的形式特定的逻辑形式预处理，例如处理变量符号。</sample>
    <sample id="1326">获得树形结构可能还需要专门的语法归纳程序。</sample>
    <sample id="1327">在这篇论文中，我们不使用树结构，并引入了一个直接建模输入片段和输出片段之间对应关系的神经序列到序列模型。</sample>
    <sample id="1328">我们首次展示了不依赖于树的深层递归的强大泛化能力。</sample>
    <sample id="1329">我们的方法分两步预测输入的输出。</sample>
    <sample id="1330">首先，我们给每个输入标记添加一个将出现在输出中的令牌的无序多集。</sample>
    <sample id="1331">在第一步之后，我们有了所有的正确令牌，但它们没有被审计。</sample>
    <sample id="1332">这就是为什么在第二步中，我们使用另一个模型来预测排列，将它们放入正确的顺序。</sample>
    <sample id="1333">我们介绍了一种新的方法来预测一个排列，它不会对可能的排列施加任何硬约束。这使得我们的方法相当灵活和富有表现力。</sample>
    <sample id="1334">从概念上讲，我们的置换模型大致工作如下。</sample>
    <sample id="1335">我们从左到右遍历输出，并确定将每个位置放入哪个多重集令牌。对于第一个输出位置，我们只需选择一个，如红色高亮所示。</sample>
    <sample id="1336">然后我们跳到下一个多集标记，以确定输出中的第二个标记。</sample>
    <sample id="1337">我们以类似的方式通过跳转到另一个多重集令牌来确定输出中的第三个令牌。我们继续这个过程。</sample>
    <sample id="1338">直到第一阶段的每个标记都被访问过一次。</sample>
    <sample id="1339">为了给你一个实验结果的预览，这里我们比较了我们的方法和其他Treeless模型在Cogs基准上的表现。我们的模型在对更深层次的递归进行泛化时，比其他模型表现出色。</sample>
    <sample id="1340">然而，其他一些结构化组织仍然具有挑战性。</sample>
    <sample id="1341">在我们的论文中，我们解决了一些有趣的技术挑战。</sample>
    <sample id="1342">首先，输入和输出之间的对齐没有在训练数据中给出。因此，对于给定的标记，我们不知道它来自哪个子集，这为训练带来了挑战。</sample>
    <sample id="1343">此外，有时会出现多个与数据一致的排列，但正确的语言排列是潜在的。我们通过将对齐作为训练的一部分来解决这个问题。</sample>
    <sample id="1344">Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That's because this is related to the traveling salesman problem.</sample>
    <sample id="1345">我们用一种对GPU友好的连续松弛来近似，这还允许我们在解决方案中反向传播，并学习更具有语言学可信度的置换。</sample>
    <sample id="1346">如果您想了解我们的实验以及我们如何解决这些挑战，请查看我们的论文或海报。</sample>
    <sample id="1347">认知失调是两个信念或行动不一致。</sample>
    <sample id="1348">GPT-4是最自由的语言模型。</sample>
    <sample id="1349">Yes, cumulative performed equal or better than iterative across the board.</sample>
    <sample id="1350">Sarah Papi</sample>
    <sample id="1351">MuDa 基准中的数据是从哪里获得的？</sample>
    <sample id="1385">The speaker's name is Matthias Lendemann.</sample>
    <sample id="1386">跨语言转移是指在训练一个模型时，使用一种语言的数据来训练，然后将其转移到另一种语言上。</sample>
    <sample id="1387">Saarland University in Germany</sample>
    <sample id="1388">作者使用了平均延迟和计算感知平均延迟作为延迟测量方法。</sample>
    <sample id="1389">大家好，我是Makshita，今天我和同事Martin一起介绍我们的工作《The Kit-Masked：从多个来源评估知识集成》。这项工作是McGill大学、Mila和微软研究之间的合作。</sample>
    <sample id="1390">自然语言理解模型利用多种知识来源，例如参数中包含的知识，通常通过预训练获得，以及在推理时间提供的输入知识。</sample>
    <sample id="1391">最近在问答等任务中的工作表明，模型可以利用预训练时间知识来解决任务。</sample>
    <sample id="1392">但是自然语言理解通常需要在推理时提供的知识。</sample>
    <sample id="1393">例如，在句子“John在电视上看到了新当选的总统”中，</sample>
    <sample id="1394">预训练参数可以包含有关总统做什么和电视是什么的信息，但它们无法可靠地知道这个实例特定实体约翰是谁或新总统是谁，因为总统可能在预训练后发生了变化。</sample>
    <sample id="1395">因此，知识密集型的自然语言理解任务的成功模型需要能够整合和利用预训练时间和推理时间的知识。</sample>
    <sample id="1396">在本文中，我们提出了一套用于知识集成的诊断测试集。</sample>
    <sample id="1397">我们介绍了一个旨在测试利用不同来源可用知识能力的共指消解任务。我们使用人类研究参与者和已建立的共指消解模型对数据集进行评估。</sample>
    <sample id="1398">Kia is a judge. Kia is a baker. Serving and Kia met at a park after the long day at work deciding cases in a law court. He was happy to relax.</sample>
    <sample id="1399">这里的任务是识别代词“他”所指的正确实体，即本例中的“sir”。</sample>
    <sample id="1400">解决特定代词需要两种类型的信息：首先，实体特定的知识，例如“塞尔维尔是法官”，其次，一般知识，例如“法官在法庭上裁决案件”。</sample>
    <sample id="1401">通常，背景知识是在大型语言模型的预训练期间学习的，而实体特定的知识通常是在推理时观察到的。</sample>
    <sample id="1402">我们调整了这两条信息的可用性，使得它们可能出现在一个单一的来源中，也可能出现在多个来源中。</sample>
    <sample id="1403">我们定义了三种KEMOS的设置。首先，我们有TopicSetting，即在预训练时假设背景知识是可用的。</sample>
    <sample id="1404">第二，有背景知识设置。背景知识在预训练时间和推理时间都可用。最后，有背景和推理设置。只有在推理时间才可用两种知识类型。</sample>
    <sample id="1405">最后一个设置尤其有趣，因为它模拟了背景噪音对于解决任务来说是不必要的，也不在预训练数据集中。例如，因为自预训练以来新职业已经发展起来。</sample>
    <sample id="1406">这里有一个控制可用性如何影响事实源的示例。</sample>
    <sample id="1407">在背景预训练设置中，我们假设背景知识“政治家在政府中赢得席位”包含在预训练参数中。在上下文中，我们提供特定实体的知识“Gchester是一个政治家”。</sample>
    <sample id="1408">在背景设置中，我们不仅提供特定的背景信息，还提供了关于政治家和影响者背景的知识。</sample>
    <sample id="1409">在背景和环境设置中，提供虚构的职业“militure”而不是“politician”，因为“militure”不太可能包含在预训练的参数中。</sample>
    <sample id="1410">我们用人类研究参与者和已建立的基准分辨率模型对数据集进行了评估。在这张图中，我们展示了在背景预训练设置中最困难的变体上表现最佳的模型的结果。</sample>
    <sample id="1411">如果没有在KitMOS上进行任务特定的训练，这两个模型的表现都不好。然而，当在KitMOS上进行训练时，C2F和BertForC2F的表现明显优于随机选择。</sample>
    <sample id="1412">这表明，当在通用参考解决数据集上进行训练时，模型会学习利用表面提示，这些提示在使用Kitties测试时是没有用的。</sample>
    <sample id="1413">Additional experiments with fictional knowledge indicated that even the best-performing models cannot reliably integrate background knowledge to provide only an inference time.</sample>
    <sample id="1414">总结一下我们论文的主要收获。许多共指消歧模型在没有任务特定训练的情况下，无法从不同来源推断知识。然而，通过任务特定的训练，一些模型能够成功地将来自多个来源的知识整合在一起。</sample>
    <sample id="1415">然而，即使是表现最好的模型，在推理阶段仅呈现背景知识时，似乎也难以可靠地整合这些知识。如果您对更多细节感兴趣，请参阅我们的论文，并在GitHub上查看数据集和代码。谢谢您的收听。</sample>
    <sample id="1416">This works well, but trees are usually not given and need to be obtained somehow. This can be complicated in sometimes a computationally expensive process. Typically this involves considerable formalism specific pre-processing of the logical forms for example to handle variable symbols. Obtaining trees may also involve specialized grammar induction procedures</sample>
    <sample id="1417">The authors of this paper are affiliated with the following institutions: University College London, Tsinghua University, and Microsoft Research Asia.</sample>
    <sample id="1418">嗨，我是Myra，今天我将谈论我们的论文《标记的人格》：使用自然语言提示来衡量语言模型中的刻板印象。这项工作是与Essen Dermouch和Dan Jurafsky合作完成的。</sample>
    <sample id="1419">最近几年，许多人记录了大型语言模型中社会偏见和刻板印象的普遍性。</sample>
    <sample id="1420">然而，这些措施存在各种限制。它们通常依赖于手工构建的数据集，这些数据集的收集非常耗时。</sample>
    <sample id="1421">他们通常只测量特定的刻板印象，这意味着它们不能很好地推广到其他人口统计或背景中，或者它们只是捕捉非常一般、广泛的关联，比如对特定群体的负面关联。</sample>
    <sample id="1422">此外，这个领域的大多数工作都没有考虑到交叉性，即多面的社会身份可以叠加偏见并成为独特的伤害发生地。</sample>
    <sample id="1423">为了克服这些限制，我们依赖于这些较新的指令微调LLMs非常擅长响应指令和提示的特性。</sample>
    <sample id="1424">So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like "Imagine you are an Asian woman. Describe yourself."</sample>
    <sample id="1425">我们可以立即看到，这适用于任何人口统计学，因为我们只需要在提示中指定我们想要的任何身份标记。</sample>
    <sample id="1426">Here are some example generations from GPT-4.</sample>
    <sample id="1427">Immediately, we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words,</sample>
    <sample id="1428">有一些有趣的模式。</sample>
    <sample id="1429">The Asian woman is portrayed as unassuming, while the Middle Eastern woman is described using terms like "exotic" and referred to in a mesmerizing manner.</sample>
    <sample id="1430">And both of the women of color personas make references to ancestry, while the white man persona has nothing of the sort.</sample>
    <sample id="1431">为了捕捉这些模式，我们的方法分为两个部分。首先，是生成这些角色。</sample>
    <sample id="1432">我们的提示是受一项研究的启发，该研究将这些提示提供给人类受试者，发现通过提供给人类受试者，他们也能够揭示种族刻板印象。</sample>
    <sample id="1433">这也使得我们生成的个性和人类撰写的回复之间可以进行直接比较。</sample>
    <sample id="1434">The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I'll elaborate on shortly.</sample>
    <sample id="1435">The advantage of this is that we can create very specific stereotypes and patterns without needing to rely on any particular vocabulary.</sample>
    <sample id="1436">So the marked words method draws upon the sociolinguistic concept of markedness, which states that there is an unmarked default and any group that differs from that default is linguistically marked.</sample>
    <sample id="1437">例如，"man"这个词，或者抱歉，"warrior"这个词通常与男性有关。所以，当人们描述一个女性战士时，他们会特别指出"one man warrior"并用女性标记这个术语。</sample>
    <sample id="1438">更广泛地说，社会上的主导群体在语言和社交上都是未标记的，而边缘化的群体通常都是标记的。</sample>
    <sample id="1439">首先，在我们的方法中，我们指定未标记和标记组是什么。</sample>
    <sample id="1440">然后我们使用fighting words方法比较这些角色，这基本上是使用加权对数比来区分每个标记组的顶级单词。</sample>
    <sample id="1441">例如，对于黑人女性的人格，我们可以使用“Fighting Words”来比较法律概率比率与白人和男性人格之间的比率，因为它们是两个相应的未标记群体。</sample>
    <sample id="1442">现在是结果时间。首先，我们使用了刻板印象词典，并发现生成的角色包含比人类写的角色更多的刻板印象。</sample>
    <sample id="1443">然而，当我们实际查看词汇表中的单词分布时，我们发现非常不同的事情。</sample>
    <sample id="1444">因此，虽然生成的个性特征具有更高的Luxong单词使用率，但人类撰写的个性特征具有更广泛的单词分布，而生成的个性特征中的刻板印象单词只是“高”和“运动型”这两个词。</sample>
    <sample id="1445">所以真的只有积极的，或者至少不是消极的。</sample>
    <sample id="1446">And in fact, this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that, we'll turn to the results from our marked words method to show how these positive seeming words facilitate stereotypes and essentializing narratives.</sample>
    <sample id="1447">在我们的分析中，我们揭示了这些看似积极的描绘如何反映有害模式。</sample>
    <sample id="1448">First, for Mark groups, the top words include things like culture, tradition, proud, and exotic. And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm.</sample>
    <sample id="1449">这为这些群体的长期歧视和排斥历史做出了贡献。</sample>
    <sample id="1450">此外，这些词中还反映了许多常见的套路，尤其是对有色人种女性。例如，描述拉丁裔女性的词语包括“充满活力”和“丰满”。</sample>
    <sample id="1451">For Asian women, the words are things like petite and delicate and silky.</sample>
    <sample id="1452">这与亚洲女性被过度性化、被视为非常温顺和顺从的历史有关，等等。</sample>
    <sample id="1453">最后，对于黑人女性，我们看到一些顶部的词是“坚强”和“有韧性”。</sample>
    <sample id="1454">This connects to an archetype that people have called the strong black woman archetype, and while it sounds positive at first glance</sample>
    <sample id="1455">There's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles.</sample>
    <sample id="1456">So instead of actually working to change those obstacles, it puts pressure on the people affected by them. This leads to negative health outcomes for these individuals and other harms as well.</sample>
    <sample id="1457">More broadly, we find that the words for each marked group pretty much reflect very essentializing narratives.</sample>
    <sample id="1458">So, based on these patterns, we come up with three suggestions for people who own models.</sample>
    <sample id="1459">首先，作为研究人员，我们应该解决和消除刻板印象和本质化叙事。我们也应该使用交叉性视角来研究偏见和伤害，因为如果不这样做，可能会忽略很多东西。</sample>
    <sample id="1460">最后，应该增加关于偏见缓解方法的透明度。</sample>
    <sample id="1461">例如，这些积极的刻板印象，我们不知道是否是因为存在某种奇怪的</sample>
    <sample id="1462">过度的价值对齐或可能的其他反刻板印象方法导致了这些有害模式。</sample>
    <sample id="1463">我们真的不能做出任何假设，或者在没有更多透明度的情况下进一步研究。</sample>
    <sample id="1464">非常感谢您的聆听，祝您在ACM上玩得愉快！</sample>
    <sample id="1465">大家好，我叫金伟怡，来自中国科学技术大学。</sample>
    <sample id="1466">It's my pleasure to give a short advertisement video about paper. Are you copying my model? Protecting the copyright of large language models for embedding and services via backdoor watermark</sample>
    <sample id="1467">让我们首先介绍嵌入式服务的背景。</sample>
    <sample id="1468">目前，大型语言模型，如GPT、Llama、Palm，在自然语言理解和生成方面表现优异。</sample>
    <sample id="1469">嵌入式服务是基于大型语言模型构建的服务之一，用于协助各种自然语言处理任务。</sample>
    <sample id="1470">例如，OpenAI提供基于GPT的嵌入API。</sample>
    <sample id="1471">然而，最近的研究表明，攻击者仍然可以通过学习嵌入并提供类似的服务来窃取模型。因此，有必要保护嵌入和作为服务的版权。</sample>
    <sample id="1472">为了保护嵌入式服务的版权，一种解决方案是在提供者的服务中嵌入一个水印，并检测另一个服务是否包含水印。</sample>
    <sample id="1473">水印方法需要满足以下属性：首先，该方法应适用于嵌入式服务。其次，水印不应降低提供的嵌入的实用性。</sample>
    <sample id="1474">第三，水印应该足够覆盖攻击者，或者攻击者可以很容易地移除水印。</sample>
    <sample id="1475">最后，在模型提取过程中，水印需要转换到攻击者的服务中。</sample>
    <sample id="1476">现有的作品可以大致分为四类。</sample>
    <sample id="1477">然而，这种方法要么不适用于嵌入式服务，要么缺乏可移植性。</sample>
    <sample id="1478">因此，在本文中，我们提出了嵌入标记，这是一种基于后门的水印方法，适用于嵌入服务。</sample>
    <sample id="1479">然后让我介绍嵌入标记的详细信息。嵌入标记包含两个主要步骤：水印注入和版权验证。</sample>
    <sample id="1480">在这些主要步骤之前，我们首先选择一个触发集。触发集是一组在中等频率间隔内的单词。</sample>
    <sample id="1481">我们假设提供者可以收集一个通用的文本库，并计算我们所做的单词频率。</sample>
    <sample id="1482">在水印注入中，我们首先定义一个目标嵌入。当用户将一句话发送到提供商的服务时，提供商会在句子中计算触发编号。</sample>
    <sample id="1483">提供的嵌入是目标嵌入和原始嵌入的权重求和。</sample>
    <sample id="1484">目标嵌入的权重与句子中的触发器数量成正比。当句子中的触发器数量大于m时，提供的嵌入恰好等于目标嵌入。</sample>
    <sample id="1485">版权验证是检测另一个服务中是否包含水印的模型。</sample>
    <sample id="1486">我们首先构建一个后门和一个无害的数据集。后门数据集包含所有单词属于触发集的句子，而无害数据集中的所有单词都不属于触发集。</sample>
    <sample id="1487">然后，提供者从数据集请求嵌入式服务。</sample>
    <sample id="1488">请求的嵌入和目标嵌入之间的余弦和L2相似度被计算。我们计算了良性与后门数据集之间的相似度差异，定义为余弦差和L2差。</sample>
    <sample id="1489">同时，我们还应用了KS测试，并使用其P值作为第三个指标。</sample>
    <sample id="1490">我们对四个数据集进行实验：AG新闻、MINE、SSD2和ERASpam。我们假设提供者使用WikiText数据集来计算单词频率。</sample>
    <sample id="1491">四个数据集的结果表明，我们的嵌入标记可以在保持下游任务良好实用性的前提下具有良好的检测性能。</sample>
    <sample id="1492">我们还通过在VOPCA上可视化句子的嵌入来验证提供的嵌入的正确性。图例表示每个句子中的触发器数量。</sample>
    <sample id="1493">如图所示，很难区分后门嵌入和正常嵌入。</sample>
    <sample id="1494">那就这样，谢谢。欢迎与我们讨论。</sample>
    <sample id="1495">ABC-Eval stands for "Annotating Behaviors in Chat".</sample>
    <sample id="1496">直到2018年，CoNLL-2003 和 CoNLL++ 之间的性能增量才高于5个百分点。</sample>
    <sample id="1497">Hello, my name is Vasudha and I'm a computer science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper: Transfer Learning for Dissonance Detection Addressing the Rare Class Challenge.</sample>
    <sample id="1498">我们首先定义认知失调以及为什么它是语言中需要研究的重要问题。简单来说，认知失调是指信念或行动不一致。</sample>
    <sample id="1499">This belief and action are inconsistent, and they are in dissonance.</sample>
    <sample id="1500">Further mentioning that I don't think I could keep my job without them justifies the second occurrence and they have a consonance relationship.</sample>
    <sample id="1501">While dissonance is a very common phenomenon we experience in daily decision making, they are really rare to find expressed in language among other kinds of discourse relations.</sample>
    <sample id="1502">为什么这很重要？了解认知距离可以帮助我们理解人们之间的分歧，跟踪信念、价值观和态度在人群中的变化。</sample>
    <sample id="1503">High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better.</sample>
    <sample id="1504">研究语言中表达的不和也可以有助于理解极端主义和脆弱群体的极化。</sample>
    <sample id="1505">Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better.</sample>
    <sample id="1506">为了创建一个认知不协调资源，我们进行了大规模的不协调关系注释。我们使用了不协调的第一种方法，如这里流程图所示。</sample>
    <sample id="1507">推文使用PrettyB解析器解析，并根据我们论文中描述的指南对话语单元进行注释。</sample>
    <sample id="1508">如图所示，只有3.5%的注释对存在不和谐。</sample>
    <sample id="1509">在收集了大约1000个话语单元对后，我们为一个初始分类器进行了训练，该分类器仅基于43个DisNet示例进行训练。毫不奇怪，该分类器的表现并不比随机猜测好多少。</sample>
    <sample id="1510">由于不和谐出现的频率较低，而且没有这样的数据集，我们面临的是绝对罕见的问题。</sample>
    <sample id="1511">为了缓解这种情况，我们尝试了结合迁移学习和主动学习的实验，以便在更少的注释运行中收集更多的不和谐样本，从而降低整体注释成本，同时提高不和谐检测。</sample>
    <sample id="1512">由于初始模型无法捕捉到不和谐音类，我们通过从相关任务中转移权重来启动主动学习过程。</sample>
    <sample id="1513">我们转移了两个不同的任务：主题无关的分歧状态分类，一个任务是确定两个来自不同人的辩论陈述是否一致或不一致，与主题无关。</sample>
    <sample id="1514">我们称它们为 CEE。</sample>
    <sample id="1515">我们发现，将零-shot性能转移到标注数据集上，已经比随机情况要好得多，最佳的AUC为0.62。</sample>
    <sample id="1516">进一步在迭代微调两个任务后，我们发现CE任务的微调加上进一步在辩论上的微调，可以得到更好的零样本性能。因此，这是我们在开始实际学习时使用的模型。</sample>
    <sample id="1517">接下来，我们确定更新模型的最佳方法。从每一轮主动学习和注释中获取新数据。累积器收集到目前为止从主动注释中收集的所有数据。Paris迭代地通过在最新数据集上进行训练来更新模型。</sample>
    <sample id="1518">在不同的策略中，我们发现累积表现优于或与迭代表现相当。</sample>
    <sample id="1519">接下来，为了提高不和谐示例的数量，我们使用了罕见类概率选择策略（PRC），以选择最有可能被当前模型判定为不和谐的示例。</sample>
    <sample id="1520">我们将其与社区中常用的其他最先进的策略进行了比较。</sample>
    <sample id="1521">我们发现所提出的PRC策略比其他最先进的策略表现更好，尽管差异很小。请注意，随机的性能显著较低。</sample>
    <sample id="1522">在进一步的EL轮次中，我们通过最佳策略将距离分类AUC提高到0.75，这是迄今为止我们在任务上取得的最佳性能。</sample>
    <sample id="1523">我们还检查了每种策略的注释质量和对注释者的成本。我们发现PRC具有最高的百分比的不和谐，并且最适合稀有类，然而，注释者也发现示例很困难。</sample>
    <sample id="1524">简而言之，我们发现PRC是一种简单的主动学习策略，用于稀有类别获取和冷启动主动学习，并且通过设计适当的迁移学习任务可以显著帮助。</sample>
    <sample id="1525">我们还发现，迭代更新对于从不同领域进行迁移学习很有用，而域内主动注释则受益于累积更新。</sample>
    <sample id="1526">这些是我们的代码数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢。</sample>
    <sample id="1527">这篇论文的作者所属机构是：University of Copenhagen</sample>
    <sample id="1528">演说者的名字是Yuan.</sample>
    <sample id="1529">这篇论文有五位作者。</sample>
    <sample id="1530">该方法与专为同时翻译定制的 simulST 架构进行了比较。</sample>
  </task>
</testset>