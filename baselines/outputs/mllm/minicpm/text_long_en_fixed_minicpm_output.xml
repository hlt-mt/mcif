<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">The main data sources for language models are large-scale web crawl data, which include political news media such as New York Times, Los Angeles Times, The Guardian, and Huffington Post.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research</sample>
    <sample id="2">The Visually-rich Document Understanding problem aims to understand various types of documents, like forms, receipts, and posters. In recent years, pre-training techniques have been introduced into this area and self-supervised pre-training multi-modal models have demonstrated great successes in various VrDU tasks. However, existing document pre-training models suffer from reading order issues. Following the idea of BERT, these methods usually adopt ascending numbers (like 0, 1, 2...) to represent the global reading order of tokens in the document, which are encoded as global 1D positions. We can see the illustration of global 1D position in Figure 1. We propose a novel pre-trained model, LayoutMask, to address these issues. LayoutMask only uses text and layout information as model input, and aims to enhance the text layout interactions and layout representations learned during pre-training. It differs from previous studies in three aspects: choice of 1D position, masking strategy, and pre-training objectives. Instead of global 1D position, LayerMask proposes to use the in-segment token orders as 1D position, which is referred to as “local 1D position”. As local 1D position does not provide cross-segment orders, LayoutMask is supposed to infer global reading order by jointly using 1D position, 2D position, and semantic information, thus bringing in-depth text-layout interactions. To further promote such interactions, we equip the commonly used pre-training objective, Masked Language Modeling, with two novel masking strategies: Whole Word Masking and Layout-Aware Masking. The Masked Language Modeling task is the most essential and commonly used pre-training task in multi-modal pre-training. Following the Whole Word Masking strategy, we set masks at word-level instead of token-level, which is much more challenging. When using Whole Word Masking, the semantic relations between masked and unmasked tokens of the same words are eliminated, so the model has to find more context to predict masked words, which can promote text-layout interactions. In Layout-Aware Masking strategy, the first and last words of each segment has a higher probability to be masked, so the model has to pay more attention to finding their contexts in the preceding or succeeding segment, thus promoting learning cross-segment orders. We also designed a new pre-training objective, Masked Position Modeling. It has a symmetric pre-training objective: recovering randomly masked 2D positions during pre-training. The MPM task is very similar to the cloze test, where a group of randomly selected words is supposed to be refilled at the right positions in the original documents. The model has to find the context for each word based on semantic relations and infer with 2D position clues from a spatial perspective. The joint learning process with both semantic and spatial inference can promote text-layout interactions and help the model learn better layout representations.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL model</sample>
    <sample id="6">Towards Unifying Multi-Lingual and Cross-Lingual Summarization presents a new approach to summarization that combines previous multilingual and cross-lingual methods into a more general setting. This many-to-many summarization aims to build one single model capable of processing any source language document and generating its summary in any target language, rather than focusing on specific languages or directions as with traditional approaches.
The authors conduct preliminary experiments using the WikiLingua dataset, which includes English, French, Hindi, Chinese, Thai, and Turkish samples. They train four models: mBART ONE (separate training for each direction), mBART U-CLS (unified model trained with all cross-lingual samples), mBART MLS (unified monolingual model trained across different directions), and PISCES (pre-trained many-to-many summarization model). The results show that the many-to-many summarization method outperforms other techniques by better transferring task knowledge between languages. Additionally, they propose PISCES, a pre-trained model achieved through three stages of meta, cross-lingual, and task-specific pre-training, demonstrating superior performance compared to baseline models like mBART-50 and mT5.</sample>
    <sample id="7">Yes, according to the paper "Do CoNLL-2003 named entity taggers still work well in 2023?", they do. The study found that for good generalization of models, a better model architecture is needed along with larger model size and more fine-tuning examples. They also discovered that performance drop was caused by temporal drift rather than adaptive overfitting.</sample>
    <sample id="8">ABC-Eval is a new dimensional approach to evaluating conversational AI. It attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="9">The success of the existing weakly supervised approach heavily relies on clean validation samples.</sample>
    <sample id="10">If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%, so there's a lot of room for improvement.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presented "Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest" in collaboration with the University of Utah, Cornell University, and OpenAI. He discussed how large language models can generate jokes but questioned their understanding of humor by probing them on tasks like explaining knock-knock jokes involving pineapples or using Google's 540 billion-parameter PaLM model to explain a joke about TPUs (the hardware it was trained on). To further explore this topic, he introduced The New Yorker Caption Contest dataset, which includes cartoons published since the mid-90s along with annotations for each cartoon such as locations, descriptions, uncanny highlights, entity links, and over 650 two-to-four sentence explanations of jokes. Using these resources, they operationalized three different tasks: matching, quality ranking, and explanation generation. For example, CLIP fine-tuned on the annotated corpus achieved around 62% accuracy on the matching task compared to humans' performance of approximately 94%. GPT-4 also struggled when given human-authored image descriptions instead of direct pixel input; however, even with additional annotation, its five-shot performance still lagged behind that of humans significantly.</sample>
    <sample id="12">There are five authors involved in the paper.</sample>
    <sample id="13">Adaptive inference is a method for reducing the inference time of large language models. To use it, we rely on the fact that real-world data varies in complexity. Therefore we can use low-capacity models for easy samples and therefore in that way, reduce the average inference costs, whether it be time or money. The two most common adaptive inference methods are Multi Model and Early Exit. In Multi Model, multiple models are stored together, each fit with a classifier at the end. They are trained separately on the entire training set, and when used for inference, they are run sequentially until a classifier decides to halt the computation. For Early Exit, multiple classifiers are fit to the model following intermediate transformer layers. They are all trained together, and for inference, a sample is run through the model until a classifier decides to halt, that way saving the computation, which would have been exhausted by the rest of the model. So let us look at the pros and cons of each method. For Multi Model, it is more versatile. It is easily extended. It is expensive to store. And it also suffers from overhead because using the last model for inference means we ran a sample through all previous models without using their output. For Early Exit, we have faster inference, that is, no overhead. It is memory efficient. But model parameters are shared amongst all classifiers, which we will soon see can lead to lower performance. We hypothesize that the last point I emphasized leads to a phenomenon we call conflicting gradients. Each classifier updates model weights, trying to optimize its own goal. Gradient signals from different classifiers may interfere with each other, degrading performance for all classifiers involved. Let us look at this illustration. Classifier M and the update from its loss function, represented here with the color blue, backpropagates through the entire model and updates the first layer. So does the update from Loss function 1 and Loss function 2. All of these updates do different things to the weights of Layer 1, potentially harming the performance of all classifiers in the model. In order to test our hypothesis, we compared individual Early Exit models’ classifiers with separate Multi Model classifiers, which are truncated versions of the BERT pre-trained language model. Keep in mind that training the Multi Model separate classifiers does not suffer from conflicting gradients. As you can see, the Multi Model classifiers outperformed those of Early Exit by an average of 2.3%, both for BERT-base and for BERT-large. Moreover, the gap is largest, for the earliest classifiers, 5.2% on average. We also measured the speed/accuracy trade-off of the models. Notice that for high inference speeds, Multi Model is much better. However, when we use later classifiers to predict, we see that Early Exit outperforms Multi Model because of the overhead suffered by Multi Model predicting with its largest classifiers. Based on these findings, we present SWEET: Separating Weights in Early Exit Transformers. It is a novel fine-tuning method for Early Exit architectures. What we do is we train an Early Exit architecture where each layer receives updates only from the following classifier. That means we avoid the conflicting gradient problem completely. Let's look at this illustration. On the left-hand side, you see the standard Early Exit model. On the right-hand side is our method, SWEET. You can see that each transformer layer only receives updates from its following classifier, avoiding the conflicting gradient problem. Let us look at the results of the SWEET method. When evaluating the individual layers, we can see that SWEET closes most of the gap between Early Exit and Multi Model. However, in some cases, later classifiers are negatively affected by our method, as you can see here by the negative gap between SWEET and Early Exit. We also ran the test and examined the speed/accuracy trade-off. We can see here that in fast speeds, SWEET outperforms both methods, whereas for BERT-Large, it outperforms both methods throughout the entire speed/accuracy curve. The takeaways from our work are as follows. We show the existence of conflicting gradients in Early Exit training process. To the best of our knowledge, we conduct the first fair comparison of Early Exit and Multi Model adaptive inference methods. We also introduced the SWEET method, the results of which motivate future research and fine-tuning algorithms tailored to the Early Exit architecture. Thank you very much for listening. If you enjoyed the talk and you want to see more, go visit our paper on Archive, "Finding the SWEET spot".</sample>
    <sample id="15">There are three authors involved in the paper.</sample>
    <sample id="16">Bible texts are much stronger simplified than for example the news text, or the language learner texts.</sample>
    <sample id="17">Relation extraction is a widely explored task. It aims to determine the semantic relation between entities in a given text. However, in some realistic scenarios, such as in social media, the data is often in various forms and modalities rather than just pure text. So if you just look at the text, you may find that it lacks sufficient context to understand some ambiguous or multi-context word. Multimodal relation extraction has been introduced recently. Additional visual sources are added to the textual RE. For example, with the support of visual evidence such as "Bachelor", "Gown", "Cap", we can easily infer the relationship between JFK and Harvard is "graduated at". However, there are still some problems. One problem is internal-information over-utilization. This is because when inferring the relationship between two entities, only parts of the text are useful. Moreover, not all and always the visual sources play positive roles for the target task. Therefore, we argue that a fine-grained information pruning over two modalities is needed. The other problem is external-information under-exploitation. This is because, although compensating the text inputs with visual sources, there can still be an information deficiency, especially when the visual features serve less or even negative utility. In this way, we think more external information should be considered, such as topic information. To address these two issues, we firstly propose a Graph Information Bottleneck principle-guided feature refinement. And then we consider taking multimodal topic information as additional semantic supplementary to enrich the overall context. This is the whole framework of our proposed method.</sample>
    <sample id="18">The example of the preference for shorter left conjuncts is "salt and pepper" versus "pepper and salt".</sample>
    <sample id="19">The presentation introduces a work titled "A Survey for Efficient Open Domain Question Answering" that was accepted by ACL 2023. The presenter, Zhang Qin from Shenzhen University, discusses the challenges of open-domain question answering and presents techniques to achieve efficient systems with smaller memory costs, faster inference speed, and comparable performance.
The main focus is on addressing three key points: reducing index size, improving model efficiency, and enhancing real-time feedback in open-domain question answering. To reduce index size, researchers propose methods such as document filtering before retrieval or embedding dimension completion. For improved model efficiency, strategies like parameter sharing, knowledge distillation, or designing fewer models are suggested. Additionally, one-stage frameworks can be employed to balance between speed, memory usage, and performance.
The study compares existing open-domain question answering models based on data aspects, highlighting trade-offs among different approaches. Based on these insights, recommendations include deploying open-domain question answering systems in low-power devices and considering more evaluation metrics for comprehensive assessment.</sample>
    <sample id="20">Yes, the models are freely available on Hugging Face and under the MIT license.</sample>
    <sample id="21">DEPLAIN-apa is based on news texts.</sample>
    <sample id="22">The model architecture, the model size and more fine tuning examples lead to good generalization.</sample>
    <sample id="23">Text image modeling research has made huge strides in the last year, with the ability to generate very high quality, interesting images. However, a lot of people have noticed that these models are often very bad at representing text. To understand what's going on here, we can dig into the text encoder itself. T5 uses SentencePiece tokenization, which means that instead of the model receiving the individual letters that make up the spelling of the input, it's receiving subword IDs for chunks of the input string. This is an even smaller granularity than the individual characters. So ByT5 does much better at spelling because all it needs to do is effectively learn to copy characters from the input to the output. All this information helps us improve our understanding and use of text image rendering models.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured by measuring length in characters, syllables, and words.</sample>
    <sample id="25">The experiments involved extracting various statistics about coordination from the enhanced version of the Penn Treebank, measuring dependency length in words and syllables. The results showed that when the governor is on the left or absent, there's a tendency for the first conjunct to be shorter; however, this effect disappears if the governor is on the right.</sample>
    <sample id="26">The baseline classifier performs not much better than chance.</sample>
    <sample id="27">There are 5 authors involved in the paper.</sample>
    <sample id="28">Bob and Alice</sample>
    <sample id="29">Formality and lexical cohesion</sample>
    <sample id="30">The paper "LLM-Blender" proposes a simple yet effective ensemble learning framework for large language models (LLMs). The authors argue that using only one top-performing LLM may not always yield the best results, as different input examples can require different optimal selections of models. To address this issue, they introduce an approach called Large Language Model Blender.
The Blender consists of two main components: PairRanker and GenFuser. PairRanker is responsible for comparing pairs of candidate outputs from multiple LLMs to determine which model performs better given specific inputs. This module uses cross-attention mechanisms like RoBERTa to analyze subtle differences between candidates more effectively than traditional methods such as parameters or score-based approaches.
GenFuser takes the top-ranked three models identified by PairRanker and combines their outputs through a sequence-to-sequence model in order to generate a final output tailored specifically to each input example. 
The researchers evaluate their method on a new dataset named MixInstruct, created by combining existing instruction datasets with contributions from 11 open-source LLMs. They compare the performance of Open Assistant, Vicuna, and their own PairRanker against ChatGPT across various automatic metrics including BERTScore, BLUERT, and BARTScore. Results show that the PairRanker consistently outperforms both individual LLMs and even surpasses them significantly when considering all four evaluation criteria.
In conclusion, the study demonstrates that LLM-Blender offers improved performance over single-model solutions while maintaining simplicity due to its straightforward design comprising just two sub-modules – PairRanker and GenFuser.</sample>
    <sample id="31">MIT, Google Research, and University of Maryland.</sample>
    <sample id="33">The framework quantifies positionality by comparing the annotations of diverse annotators to those from existing datasets and models using a Pearson's R correlation score.</sample>
    <sample id="34">The paper presents CREST, a joint framework for rationalization and counterfactual text generation. The authors propose to combine methods that use selective rationalization with those aligned more closely with human causal reasoning by generating counterfactuals through editing specific parts of the input. They introduce an editor component in their model, which is essentially a masked language model used to fill in the masked response with new tokens.

To evaluate the quality of the counterfactuals produced by CREST, the researchers compare it against related works using both automatic metrics and human evaluation experiments. Their findings show that humans found manual counterfactuals to be more valid and natural than those generated by other approaches like MiCE. However, among the automatic approach, CREST and MiCE were judged as producing higher-quality counterfactuals compared to each other.

The study also explores the potential of using these counterfactuals for data augmentation purposes. By training models on factual examples and augmenting them with either manually created or automatically generated counterfactuals from CREST, they observe improvements in performance across various datasets including IMDB, SNLI, contrastive datasets, and out-of-domain datasets.

Furthermore, the research investigates whether the rationales generated by CREST are interpretable. Results indicate that CREST-Rationalization produces more plausible rationales than other approaches while achieving high levels of counterfactual simulability – meaning its explanations can effectively change the classifier's decision when guided by contrasting edits based on this explanation.

In summary, the proposed CREST framework offers a promising method for creating controllable, valid, fluent, and diverse counterfactuals that focus on contrasting aspects of inputs. This could lead to better interpretability of machine learning models' decisions.</sample>
    <sample id="36">The paper "Learning Language-Specific Layers for Multilingual Machine Translation" presents a solution to increase the capacity per language in multilingual machine translation while keeping inference costs constant. The proposed approach is called Language-Specific Layers (LSLs), which involves having one regular transformer layer per language and selecting the correct sublayer at inference time based on whether it's the source or target language.

The authors focus on placing LSLs only where they are most needed, rather than uniformly throughout the model. To determine the best placement of these layers, they train a large model with all possible components simultaneously. After training, they drop this model and create a new architecture from scratch using the learned weights.

The results show that their method significantly improves performance over baseline models and language adapters, particularly for low-resource languages like Swahili. Additionally, statistical tests demonstrate significant improvements across 84 out of 90 translation directions.</sample>
    <sample id="37">The previous study found that by giving prompts to human subjects, they were able to surface racial stereotypes.</sample>
    <sample id="38">The enhanced version of the Penn Treebank.</sample>
    <sample id="39">1</sample>
    <sample id="40">Topic independent dissonance stance classification and binary classification of expansion and comparison classes.</sample>
    <sample id="41">Sustaining coherent and engaging narratives, such as dialogues or stories, requires natural language processing systems to understand how the personas of speakers, listeners, or characters ground narrative. For example, an adventurous architect may be interested in talking about outdoor explorations with his friends who have similar hobbies, but he may prefer to discuss architectural design ideas instead with his colleagues at work. However, narrative systems have not yet learned good representations of real-world personas, which involve rich world knowledge and complex interconnections with each other. For instance, as shown in the picture, a singer preparing an album may have studied music at university. This would allow him to share the experience with a student majoring in composition, who may study music as a daily routine. In this work, we propose a Persona-grounded Commonsense Knowledge Graph, PeaCoK, to represent such world-level persona knowledge at scale. PeaCoK contains about 3,800 personas and 40,000 distinctive attributes, which form about 100,000 personal inferences or facts. Besides, about 9,200 attributes are connected to two or more personas which contributes to the rich interconnections of personas in PeaCoK. Based on the studies of human interactive behaviors, we frame the relations of personas and their attributes in three dimensions, including four types of main relations, as well as interactivity and distinctiveness. Detailed descriptions of these relations are listed here. We built PeaCoK in three steps: First, we select personas from existing commonsense graphs which include both human roles and event-based entities. Then we induce attributes of personas from both commonsense knowledge graphs and large-scale pre-trained language models. Finally, we crowdsourced the annotations of PeaCoK relations using a joint human-AI majority voting scheme. Expert study shows that a majority voting with AI in the loop yields high-quality relation annotations with average 87% of accuracy in F1. The AI annotator InstructGPT-3 efficiently mediates the disagreements between human annotators with lower temporal and financial costs. Now, the graph in general cannot have a comprehensive coverage of all world knowledge, so we need to face the question of whether PeaCoK can help language models to learn and generalize persona knowledge. We use PeaCoK to train a BART-based common knowledge generator on a persona attribute inference task where the model needs to predict an attribute of a given persona with a target relation. As baselines, we compare to large-scale pre-trained language models, including five-shot GPT-3 and zero-shot GPT-3.5. Compared to the baselines, Comet-BART trained on PeaCoK achieves overall better automatic evaluation results on various natural language generation metrics and also a higher accept rate in human evaluation. This indicates that PeaCoK can serve as a reliable persona knowledge base, which enables light-weight language models to learn knowledge generation capabilities comparable to large-scale language models. Finally, we explore whether PeaCoK knowledge can be used to improve downstream narrative modeling. We investigate a persona-grounded dialogue generation task on the ConvAI2 PersonaChat data set. Specifically, we use a knowledge linker to retrieve facts from PeaCoK that are relevant to each speaker's original persona profile and utterances. Then we convert the retrieved facts into natural language statements to augment each speaker's profile. We choose the P²Bot model as our baseline dialogue system. Human evaluation shows that PeaCoK augmented models achieve better dialogue generation on various aspects, including fluency, consistency, engagement, and persona expression. By comparing to the augmentation with Atomic2020 knowledge graph, we also find that PeaCoK's persona-centric commonsense knowledge yields a more positive impact compared to general social commonsense knowledge. We also stratify our human evaluation results based on the overlap of the two speakers’ augmented PeaCoK knowledge, where we find that in terms of dialogue, consistency, and engagement, the winning rates of PeaCoK augmented model increase as the number of shared common attributes between two speakers becomes larger. Since more connections between speakers leads to more consistent and engaging conversations, this highlights the importance of learning PeaCoK's interconnected world persona knowledge in narratives. In summary, we propose a world-level Persona Commonsense Knowledge Graph, PeaCoK, that contains large-scale, high-quality persona inferences. Our knowledge resource can be used to train reliable persona knowledge generators and also enable more consistent and engaging narrative modeling.</sample>
    <sample id="42">4</sample>
    <sample id="43">There are 4 authors involved in the paper.</sample>
    <sample id="44">The introduced framework differs from previous works by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions.</sample>
    <sample id="45">The generated personas.</sample>
    <sample id="46">DeepL and Google Translate</sample>
    <sample id="48">The paper "Prompting PaLM for Translation: Assessing Strategies and Performance" is a joint work with colleagues from Google Translate. However, the exact number of authors involved in this paper was not specified in the given information.</sample>
    <sample id="49">1024</sample>
    <sample id="50">DEPLAIN is a new corpus for German text identification on the document level, and on the sentence level. It consists of two subcorpora: DEPLAIN-apa, based on news texts, and DEPLAIN-web, which includes different domains. The corpus has been aligned manually to ensure high quality data. The authors propose using their dataset as gold standard alignments to evaluate automatic alignment methods in machine translation tasks. They also fine-tune language models like long-mBART and mBART to produce simplified text from complex input text, achieving better scores than baseline models.</sample>
    <sample id="51">music, books and recipes</sample>
    <sample id="52">Positionality is the perspectives that people hold as a result of their demographics, identity and life experiences.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Cognitive dissonance is a common phenomenon in daily decision-making, but it's rare to find expressed in language among other discourse relations. Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends and belief values, and attitude changes in population. High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better. To address this challenge, Vasudha from Stony Brook University presented their work accepted into ACL 2023 as a long paper titled "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." They conducted a large-scale annotation of dissonance relations using an initial classifier trained only on 43 examples of dissonance. The low occurrence of dissonance made it difficult to annotate more samples without increasing costs significantly. Therefore, they experimented with combinations of transfer learning and active learning to annotate such that more dissonant samples could be collected over fewer annotation runs while improving dissonance detection. Their approach involved transferring weights from closely related tasks like topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE). By iteratively fine-tuning these models followed by further training on debate, they achieved zero-shot performance improvements. Additionally, they used a Probability-of-Rare-Class strategy (PRC) to select mostly likely-to-be-dissonant examples during each round of active learning. This resulted in improved AUC scores compared to state-of-the-art strategies commonly used in the community. In summary, PRC was found to be effective for acquiring rare class data points and cold-starting AL with appropriately designed transfer learning tasks.</sample>
    <sample id="55">Yes, EDAtt adapts an existing offline ST model.</sample>
    <sample id="56">To determine the number of authors involved in the paper, I will carefully examine the information provided. The speaker introduces themselves as "Yusen Zhang" from Penn State University and mentions that they are presenting their work titled "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". However, there is no direct mention of other authors by name or affiliation within this summary.

Given the context:
1. Yusen Zhang is mentioned.
2. There's a possibility of additional co-authors not explicitly named during the presentation introduction but typically included in such academic works.

Since the exact count isn't specified here:

Simple Answer: 1

However, it should be noted that based on standard practices for academic presentations and publications, multiple individuals often collaborate on research papers like these. Therefore, while only one author (Yusen Zhang) was directly introduced, it would likely involve more contributors who aren't specifically listed in this particular excerpt.</sample>
    <sample id="57">No, the tested model does not work on the test suite.</sample>
    <sample id="58">There are three variants of KITMUS: Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="59">Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing tasks and offers huge performance gains compared to historical static and contextualized methods such as Word2vec, fastText, or more. Since then, this model has been adapted to many other languages, like in French with CamemBERT, and also in domains like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT, but mostly in English. Specialized models for other languages are scarce and are often based on continual pre-training due to the lack of in-domain data. However, French didn't have any open source model for biomedical until now. So we ask ourselves a question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare DrBERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse. Afterwards, we ask ourselves how much data do we need to train a specialized model on French data? Is it 4 gigabytes, 8 gigabytes, or more? To answer this question, we first train and compare four from-scratch models: a first version of DrBERT, with 7 GB of NACHOS; a second version of 4 GB of set of NACHOS; a first version of ChuBERT, which is a clinical model with 4 GB of sentences taken from clinical notes; and a final version of ChuBERT with a mix of 4 GB of set of NACHOS and 4 GB of clinical notes. In addition to this comparison, we introduced three models trained on continual pre-training to analyze the impact of pre-training strategy. One based on the weight of CamemBERT and trained on a 4 GB set of NACHOS. Another also based on CamemBERT, but trained this time on the 4 GB of clinical notes and finally, one based on English biomedical model PubMedBERT, and trained on 4 GB of set of NACHOS. In total, we have seven models. To evaluate our seven models, we gather data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. These models are compared to six baseline models which are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT. The evaluation highlights that models performed best on the task with data of the same nature as those on which the model has been trained. However, we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translated to better performance. Overall, from-scratch pre-training seems to obtain higher performance on most of the tasks. However, our experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues. Finally, as a conclusion our proper system offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT.</sample>
    <sample id="60">MIT, Microsoft Research New England</sample>
    <sample id="61">Should we only use the clean samples for validation, or there are better ways to utilize them?</sample>
    <sample id="62">The paper presents a systematic study of knowledge distillation for natural language generation (NLG) with pseudo-target training. The authors aim to explore the potential of NLG compression, which involves finding an effective recipe that can compress large models while preserving their performance. They focus on task-specific knowledge distillation in realistic setups and consider four NLG tasks: summarization, question generation, common sense reasoning, simplification, and style transfer.

The study is divided into eight stages:

1. Architectural decisions: Comparing encoder/decoder vs. decoder-only architectures.
2. Impact of pruning on task performance or computational efficiency.
3. Knowledge selection approaches and state-of-the-art baselines comparison.
4. Extensions of using pseudo-targets:
   - Importance of unlabeled data
   - Generating multiple pseudo-targets instead of single ones
   - Sampling pseudo-targets rather than using beam search

5. Joint-teaching technique as a novel approach to address student exposure bias, grounded learning, and teaching students to correct mistakes by applying word-level knowledge distillation on both teacher-generated and student-generated pseudo-targets.

The results show significant improvements when incorporating these extensions into the traditional sequence-level knowledge distillation method used in previous works focused on classification tasks or pre-training processes like NLU and NLG.</sample>
    <sample id="63">Sensitivity measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in instruction wording.</sample>
    <sample id="64">The name of the speaker is Jingwei Yi.</sample>
    <sample id="65">Greater sensitivity indicates improved model performance.</sample>
    <sample id="66">Mathematical reasoning is a crucial aspect of human intelligence that allows us to comprehend and make decisions based on numerical data and language. The development of machines capable of solving math problems and proving theorems has been an ongoing focus in AI and NLP, with recent interest surging due to advancements in these fields. Our survey discusses mathematical reasoning tasks and deep learning methods for their solution.
The text of math word problems can involve arithmetic operations with single or multiple steps. Mathematical reasoning extends beyond textual data; it also includes multimodal information like images, figures, and tables. Two primary categories are studied: visual contexts and tabular contexts. Solving geometric problems is essential in high school education, such as identifying geometric relations from problem texts and diagrams using theorem knowledge and calculations to obtain numeric answers.
Automated theorem proving is another significant line of research. A theorem prover aims to demonstrate the truth of a mathematical claim via a sequence of larger arguments. Writing proofs by hand is challenging for humans but automated provers assist significantly. Datasets have been proposed to test the capabilities of large language models (LLMs) in natural commonsense knowledge and high-level problem-solving scenarios.
Neural network architectures have been developed specifically for mathematic reasoning tasks. For instance, a sequence-to-sequence model uses an encoder to code architecture and typically formalizes the task as a sequence generation task. It maps input sequences, like math problems, to output sequences, including equations, problems, or proofs. Tree-based structures represent mathematical expressions well, leading to the proposal of sequence-to-tree models explicitly modeling this structure while encoding equation expressions.
Pre-trained LLMs have shown remarkable performance across various NLP tasks recently. They've demonstrated impressive results when applied to solve math word problems. Given an example question, prompting the LLM with one chain-of-thought process helps guide them through producing intermediate reasoning steps before providing the final answer. However, despite advantages, LLMs face limitations, particularly in precise mathematical reasoning. Replacing the greedy decoding strategy with self-consistency sampling different reasoning paths enhances LLM's performance.
Augmented approaches to improve LLMs' performance include program-aided LMMs, which help perform complex tasks given varied inputs queries. Chameleon, a new approach, generates natural language programs to compose tools for use. Despite efforts to build non-English datasets for Chinese, Korean, Arabic, and developing benchmarks for financial, scientific, and medical domains, generalization and robustness failures persist in current learning models during reasoning tasks.</sample>
    <sample id="67">Interference in multilingual translation models can be a significant issue, but it is often not well understood. In this work, we identify the main factors that contribute to interference or synergy between different language pairs and find that severe interference happens when the model is very small compared to the data size. We also discover that tuning the sampling temperature is key for strong performance.
To investigate these findings further, we use four variants of the Transformer architecture trained on 15 languages from WMT, ranging from over 50 million sentence pairs to about 150K. Our experiments show that both language similarity and the number of languages do not have a large impact on interference levels. However, severe interference occurs only for the smallest models and goes away with an increase in scale.
The best way to control trade-offs is through temperature sampling, where T greater than 1 allows more training examples from lower-resource languages to be sampled. The most common value used is five, often without calibration. By using tuned temperatures, we found that modest scale and tuned temperature are sufficient to reduce the problem significantly without any other specialized method.</sample>
    <sample id="68">During pretraining, models receive a large amount of linguistic context.</sample>
    <sample id="69">Typically, only 20 samples per class are needed for good performance in WSL.</sample>
    <sample id="70">Stanford University and the University of Washington</sample>
    <sample id="71">The paper introduces the AltEntities Corpus, a dataset for understanding users' language when they want to make a choice. The goal is to provide an alternative question that helps disambiguate entities in conversational systems and benchmarking LLMs' entity understanding. The data set covers three domains: music, books, and recipes. It was collected using crowd annotation with a cartoon completion setup. The first speech bubble sets the dialogue context, while the second one provides an alternative question. The third speech bubble uses indirect references to select one of these entities. The collection methodology emphasizes informality by providing only two bubbles automatically and allowing annotators to fill in the third one based on their knowledge about the entities. Different sampling methods were used to generate A and B samples from Wikipedia. Background information about each song or book was provided through Google search results or Wikipedia text. For recipes, images from Wikipedia were also shown. Annotators picked one of these entities and described them using three to five indirect referring expressions. Results show high accuracy rates if the language model has access to overlapping background knowledge but lower performance without it.</sample>
    <sample id="72">To evaluate the political leaning of language models and understand how pretraining data might influence such biases.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">Commonsense knowledge describes facts and related judgments in our everyday world, which is essential for machines when interacting with humans. ATOMIC is a large-scale commonsense knowledge base which covers event-centered social aspects of inferential knowledge tuples. With only B-to-A links, ATOMIC contains very few multi-hop paths, since an annotated tail event cannot become the head event of a triplet. Missing B-to-B, A-to-B, and A-to-A links cause unsatisfactory knowledge coverage, despite its high-quality human-annotated commonsense knowledge. We construct Dense-ATOMIC upon ATOMIC. By comparing them, we can see that Dense-ATOMIC completes many missing links in ATOMIC, including B-to-A, B-to-B, A-to-B, and A-to-A links DenseATOMIC also contains multi-hop paths, for example, a 2-hop path: X asks Y to marry, and then Y says yes, and then X smiles. Here is our process for Dense-ATOMIC construction. It mainly consists of three parts: normalizing tail events, training a relation prediction model, and constructing DenseATOMIC. Normalizing tail events converts tail events into the same equation as the head event. It consists of four parts: subject removal, third-person singular form conjugation, subject recovery, and relation grouping. Traditional methods for the completion of ATOMIC have two limitations: Firstly, sparse graph structure making it difficult for a GCN to propagate information. Secondly, unable to sufficiently utilize semantic information of events. To this end, we propose Rel-CSKGC, which predicts the relation given the head event and the tail event of a triplet. Given a head event "X is forgiven" and a tail event "X smiles", we first encode them with RoBERTa and then use the representation of the start token for linkable prediction. Meanwhile, we apply MaxPooling on the head and tail events and concatenate them for link prediction. This has two advantages. Firstly, utilizing no graph structure information, thus avoiding the problem caused by the sparsity. Secondly, taking advantage of semantic information by encoding both the head and tail events with the pre-trained language model. It's computationally expensive to iterate over all pairs of head and tail events during the inference. To this end, we consider each base event and its annotated tail events as a cluster. We design an Intra- and Inter-Cluster Completion Strategy. Intra-cluster completion infers missing links inside a cluster, and the inter-cluster completion infers missing links between different clusters. Following original split of ATOMIC, we randomly sample negative triplets from the training split with negative sampling strategies. We combine sampled negative triplets and the training split to construct the training set for Rel-CSKGC. To test the performance of Rel-CSKGC, we construct a ground-truth subgraph by randomly sampling three clusters from the test split and annotating all pairs of head events and tail events with the most reasonable relation. Here is the statistics. We compare Rel-CSKGC versus relation prediction methods, and Rel-CSKGC outperforms relation prediction methods on both automatic and human evaluation. We also compare Rel-CSKGC with translation-based methods, and Rel-CSKGC also performs better. We also perform evaluation on the constructed DenseATOMIC. Firstly, we can see that DenseATOMIC yields higher knowledge coverage since DenseATOMIC, has more 1-hop, 2-hop, and 3-hop paths. Secondly, DenseATOMIC also benefits the performance of COMET. We can see that COMETours can generate more diversified results. We also perform the evaluation of multi-hop paths on DenseATOMIC. We can see that by randomly sampling from DenseATOMIC, the aggregates of multi-hop paths are relatively high, and with the heuristic rule, we can also achieve better results. Here are some randomly sampled paths of DenseATOMIC. For example, "X misses Y's opportunity," and then "X goes home sadly", and then "X is melancholy". In this paper, we construct a densely-connected commonsense knowledge graph, DenseATOMIC. And then we propose a new CSKG completion method for inferring the missing links on ATOMIC. We perform extensive evaluations that demonstrate DenseATOMIC's advantage in knowledge coverage and multi-hop paths, and the potential for commonsense reasoning. Here is our code and our website. Thanks.</sample>
    <sample id="75">Name entity recognition and relation extraction are two crucial tasks in information extraction. Supervised learning schemes have made significant progress in NER and RE research by leveraging rich label data. However, fully-supervised models require extensive labor to obtain high-quality data annotation, and it requires diverse annotated data for various domains and applications. Semi-supervised learning employs a small amount of label data to obtain powerful models at a lower cost. Semi-supervised NER and RE models have performed very well in recent years. However, current studies neglect the underlying interconnections between NER and RE tasks. This could be an issue. For example, one can easily see the syntactical similarity between generative model and probabilistic model and their relation indicator "used to" and "use in". If such similarities are ignored, the model might thereby miss label alignment as an entity that shares the same type as dependency parsing. Moreover, it is necessary to consider the interconnections among labeled data, among unlabeled data, and between labeled and unlabeled data. Here comes the method parts. Our jointprop framework consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and finally, model optimization.</sample>
    <sample id="76">The political bias propagation pipeline involves pretraining data, language models, and downstream tasks. Political biases in the training data can influence the development of language models with varying political leanings. These biased models may perform differently on specific tasks like hate speech or fake news detection based on their ideological perspectives. This highlights potential fairness issues that need to be addressed when deploying NLP applications using these models.</sample>
    <sample id="77">This video is for sharing our work, "On Improving Summarization Factual Consistency from Natural Language Feedback". This is the joint work from Yale University and Microsoft Research, and most of the work was done when the first author was an intern at Microsoft Research. In this work, we introduce a new dataset, DeFacto, which contains human demonstrations and feedback for improving summarization factual consistency. For this dataset, we provide comprehensive analysis and we offer further insights into the factual consistency of the summarization models. Our premise states that we propose three new NLG tasks and we provide strong baseline models for each of them. The tasks we propose are summary editing, feedback generation, and automatic factual error correction. The task we studied in this work is abstractive text summarization, and we specifically study the factual consistency of the summarization models. This quality requires that all of the information in the summary should be supported by the input document. The human demonstrations and feedback we collected in this work is based on the original system-generated summaries of the existing summarization models. We asked the annotators to provide labels to decide whether the summary is factually consistent, and we require them to provide human-corrected, factually consistent summaries if they think the original summary is not correct. Also, we require them to provide human feedback which contains the instructions, explanation, and evidence. More specifically, their explanation is to explain why the annotators think the summary is factually consistent or not. The instructions are for changing the original summary to make it factually consistent, and the evidence is one of the most relevant sentences in the source documents which will support the claim of the annotators. We collect the data on the XSum dataset, which is the most commonly studied dataset for summarization factual consistency. And the initial system outputs are collected from the pre-trained Pegasus model. In this slide, we show an example of the annotated data points. In this slide, we show the basic data statistics. We collect around 2.5K data points, and 70% of them contain factual errors. For the human-edited summaries, we show that they can receive higher automatic factuality scores compared with the initial system output. However, we also observe a lower textual overlap between the reference summaries and the human-edited summaries. We think the reason is that a majority of the reference summaries under XSum datasets already contain the factual errors. In this slide, we show the data distribution of the annotated editing instructions and their relation with the different error types. The first task we started is summary editing, where the model needs to follow the human feedback to edit the initial summary. We found that both the fine-tuned models and zero-shot large language models can effectively leverage the human feedback for this task. The second task we studied is feedback generation, where a critic model needs to generate the feedback that can be used by the editing model. We found that this remains a challenging task for both the fine-tuned models and the large language models. The third task is to automatically correct factual errors while generating the corresponding explanation. We found that the editor model can achieve comparable performance compared with the baseline models while being trained on much fewer data. And training the model to generate the explanation can help the model to achieve better performance. Apart from providing a test bed for the proposed NLG tasks, our dataset also has other advantages thanks to its fine-grained annotations, which can be valuable for training factuality metrics and factuality meta-evaluation. We have released our collected DeFacto dataset on GitHub and please check our paper for more details. Thank you for listening.</sample>
    <sample id="78">Yes, the simplification process differs for DEplain-apa and web.</sample>
    <sample id="79">Yes, Coscript is publicly available.</sample>
    <sample id="80">The watermark is inserted into the text by selecting a trigger set, counting the number of triggers in each sentence sent to the provider service, and then providing an embedding that's a weighted sum of a target embedding (proportional to the number of triggers) and the original embedding. If there are more than m triggers in the sentence, the provided embedding will be exactly equal to the target embedding.</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">Automated Essay Scoring, or AES for short, aims to score the writing quality of essays without human intervention, which is an important application of natural language processing in education. State-of-the-art AES models are typically trained in a supervised way with large labeled corpora, comprising essays and their ground-truth quality scores. However, collecting labeled essays is time-consuming and labor-intensive, especially for essays written specific to new prompts, and when there is no professional scoring staff available. Unsupervised AES can get rid of the requirements of groundtruth scores for training, and as such, has significant potential in both scientific research and practical applications. There are mainly two works tackling the unsupervised AES task. The first work is proposed by Chen and others in 2010, which uses a heuristic quality signal, the number of unique terms, as the initial score of each essay, and then iteratively propagates the scores to other essays in the same cluster. However, such unsupervised clustering process is uncontrollable, which leads to poor performance. The second work is proposed by Zhang and Litman in 2021, which uses a heuristic quality signal — word count — as a weak supervision to train a neural AES model. However, such direct regression process also leads to poor performance. The two works inspire us that, since a single quality signal cannot comprehensively describe the quality of essays, more quality signals should be introduced to bring stronger and more robust supervision. To this end, we propose a novel framework for Unsupervised AES by Learning from Rank Aggregation, or ULRA for short. The core idea of our ULRA is to introduce multiple heuristic quality signals as a pseudo-groundtruth and then train a neural AES model by learning from the aggregation of these quality signals. Specifically, our ULRA contains a heuristic essay ranking module, or HER alpha-shot, which can generate partial order pairs by ranking essays according to heuristic quality signals As illustrated in the figure, the HER module contains three components: quality signals, essay ranking, and partial-order pairs generation. Among them, multiple classic quality signals are introduced to describe the quality of essays from different aspects. Each quality signal can then be used to rank essays according to signal values and generate a rank list. Finally, each rank list can be transformed into many partial-order pairs for later model training. Next, our ULRA contains a Deep Pairwise Rank Aggregation Module, or DPRA for short, which trains a neural AES model by aggregating the partial-order pairs derived from multiple quality signals into a unified supervision. This module mainly deals with how to address the inconsistent partial-order supervision from multiple quality signals so that the neural AES model can learn how to judge the partial-order relationship of essay quality. To address this problem, we designed a Deep Pairwise Rank Aggregation loss, which set a learnable confidence weight for each signal to measure the importance of each signal. Finally, in the model inference stage, considering that the essay scores predicted by the neural AES model may have a different range from the pre-defined score set, we propose a Scoring Strategy to transform the predicted scores given by the neural AES model into the range of the pre-defined score set through a minimum-maximum transformation. We conduct experiments on both transductive and inductive settings, which demonstrates that our ULRA outperforms all unsupervised baselines with a large improvement. Compared with the cross-prompt and one-shot methods, ULRA achieves competitive performance. By observing the general supervised methods, the performance of ULRA is still much lower than theirs due to the lack of strong supervision.</sample>
    <sample id="83">Yes, encoder-decoder models such as mt5 can improve by training on a mixture of languages.</sample>
    <sample id="84">The paper "PAD-Net: An Efficient Framework for Dynamic Networks" discusses the concept of dynamic networks, which can change their architecture or parameters based on input. The authors propose a framework called PAD-NET (Partially Dynamic Network) that partitions parameters into static and dynamic modes to reduce redundant dynamic parameters while maintaining representation power. They use an iterative mode partition method and set up scale factors to speed up training. Experiments show that PAD-NET achieves better performance than static networks and fully dynamic networks with fewer parameters and computations. Ablation studies optimize dynamic ratios in Convolution and Mixture of Experts, and constraint on submission of scale factors is crucial for accuracy. Compared to network pruning, PAD-NET maintains static parameters and improves output discrimination. Future work includes extending methods to other mainstream networks, exploring hardware-friendly structured manners, and introducing more modes like zero elements combinations among static and dynamic parameters.</sample>
    <sample id="85">An example of constrained language planning is "make a chocolate cake".</sample>
    <sample id="86">They visualize the embedding of sentences on four datasets using PCA and show that it's hard to distinguish between backdoor embeddings and normal embeddings.</sample>
    <sample id="87">The work uses RoBERTa as the base pre-trained model and fine-tunes it on a specific biomedical dataset, NACHOS. They also compare their approach with other models that use different strategies for training, such as continual pre-training or using different datasets.</sample>
    <sample id="88">GPT-4 is the least aligned with Arabic speaking countries.</sample>
    <sample id="89">The speaker shows how the model leverages knowledge learned through the attention mechanism on an example sentence that contains "I'm going to talk about...".</sample>
    <sample id="90">As language models advance, data annotation gets essential. In NLP, it has been customary to recruit native speakers of the target language. Even though it is difficult to recruit native speakers for many languages, we have lots of language learners. For instance, there are no monolingual native speakers, and only 73,000 daily-using L1 speakers in Irish, but more than 1.2 million learners exist. In this paper, we question whether we really need native speakers for data annotation and conducted a proof-of-concept study to examine the feasibility of using language learners as annotators. We carefully designed experiments considering the following control variables. First, we targeted three languages: English, Korean, and Indonesian, considering the amount of available resources and the language difficulty to learn. We chose four tasks from each common task type in the GLUE benchmark: Sentiment analysis for a single sentence classification, NLI for sentence pair classification, and NER for sequence tagging and MRC for span prediction. We adopted and revised the CFR criteria to categorize learners into three levels: basic, intermediate, and advanced. And for a fair comparison, we recruited native speakers as well and conducted the same experiments. We randomly sampled 120 annotation samples from existing datasets and categorized them into five groups based on their difficulty level. Finally, we divided language learners into two groups with the additional resources we provided. We suppose learners might consult dictionaries or machine-translation systems to fully understand annotation samples and aim to compare annotation accuracy and learning effect concerning the types of additional resources that they used. This figure shows a high-level flow chart of our experiments. First, we conducted a preliminary survey to ask participants about their self-rated language proficiency, language background, and language learning experiences. One session of the main experiments consists of three steps: pre-test, annotation, and post-test. Participants were asked to solve 15 test questions with standardized test questions from official language tests and word meaning questions asking about the meaning of the words to check their language proficiency level. After taking a pre-test, participants annotated 10 questions with the help of additional resources assigned. After completing the annotation, participants were asked to solve the same 15 questions again to investigate whether the data annotation leads to any learning effects. The main experiments consist of a series of multiple sessions over six days. Finally, participants completed a post-survey asking about their thoughts on annotation and self-rated language proficiency again. Through the experiments, we show that labels annotated by language learners are nearly accurate, especially for simpler tasks and easy-to-medium level questions. Moreover, language learners are almost on par with native speakers if their labels are aggregated with others by majority voting. Finally, we prove language learners capable in the case of NLP data annotation by showing training simulations with learners' annotations. Language models trained on learners' less accurate labels achieved about 95% of ground truth performance and sometimes outperform the model trained with native speakers' labels. Unlike the conventional ways of building data in low- to mid-resource languages, which are basically translating existing datasets, this paper suggests a novel way for data construction by recruiting language learners as annotators. Furthermore, we observe that learners' language proficiency and vocabulary and grammar tend to improve as they carry out the annotation tasks, by comparing the scores of the pre-test and the post-test of a single session and the pre-test of the first and the last session. In conclusion, this paper questions the necessity of recruiting native speakers for data annotation and shows that language learners could definitely contribute NLP annotations. We believe this work showed the possibility of broadening NLP research for many languages, jumping over geographic and technological barriers to building benchmark datasets for low-resource languages where it is hard to recruit native speakers. Thank you for listening. Please refer to our paper for more details, such as how control variables affect annotation performances. Any questions are welcome via the email below.</sample>
    <sample id="91">As the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity.</sample>
    <sample id="92">The authors compare their method with three treeless baselines: (1) a baseline that uses only the last word of each input sentence as an output, (2) a baseline that predicts the next token in the output sequence based on the previous predicted tokens and the current input context, and (3) a baseline that uses a neural network to predict the entire output sequence one token at a time. These baselines serve as benchmarks for evaluating the performance of their proposed model without relying on trees or explicit structural representations.</sample>
    <sample id="93">advisors</sample>
    <sample id="94">The speaker introduces a paper that addresses the protection of copyright in embedding as services, which are built upon large language models like GPT and LLAMA. The issue arises when attackers steal these models by learning from embeddings provided by the service. To counter this, the paper proposes Embedding Marker, a backdoor-based watermark method applicable to embedding as services. This method includes two main steps: watermark injection and copyright verification. In watermark injection, a trigger set is selected based on word frequency data collected by the provider. During the process, an original embedding is modified with a target embedding proportional to the number of triggers in a sentence. Copyright verification involves detecting whether another model contains the watermark using techniques such as KS test and similarity metrics between benign and backdoor datasets. Experiments conducted on four datasets (AG News, MIND, SST2, Enron Spam) demonstrate the effectiveness of the proposed method while maintaining utility for downstream tasks. Visualizations show that it's challenging to distinguish between normal and backdoor embeddings.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="97">The speaker mentions 3 problems of SimulST.</sample>
    <sample id="98">Sanitize political opinions in language model training data</sample>
    <sample id="100">Multi-hop QA is about answering questions that require multiple reasoning jumps to answer. Existing systems require thousands of examples of questions and ground-truth chains for good performance. Our approach, PromptRank, is data-efficient. It gives good performance with as few as 128 examples and therefore addresses this issue. The idea is to combine an unsupervised retrieval method with a few-shot language model-based reranker. There are two main steps: Retrieve a pool of candidate chains using TF-IDF retrieval and hyperlink traversal. And rerank these candidates using the few-shot language model reranker. Two points to consider here: what scoring function should we use, and how do we prompt the language model to extract this score? We use the likelihood of the question given the chain according to a language model. Given the language model, we have a chain prompt that I will explain how we construct in a few slides. And given the question, we score C as the probability of the question given the chain prompt. How do we construct the chain prompt? Given this question and this chain, what we do is we have a prompt that looks like this, where we insert the chain documents into the prompts, and we have an indicator token to designate that this is a document. And we have an instruction, which in our case here is something like "Read the previous documents and ask a question." And the instruction serves to elicit the language model's reasoning ability over the chain documents.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="102">The important properties of a watermarking method are applicability to embedding as services, non-degradation of utility for provided embeddings, covertness against attackers or easy removal by attackers, and transferability during model extraction.</sample>
    <sample id="103">The 14 different languages into which the English TED talks have been translated are not specified in the abstract.</sample>
    <sample id="104">We re-annotate data sets with diverse annotators, and we ought to do this over looking at the demographics of original data set annotators.</sample>
    <sample id="105">Cosine and L2 similarity are used for measuring the difference between benign and backdoor datasets.</sample>
    <sample id="106">To motivate this work, let's consider the example of Jane, who is a zoologist on a field trip in Costa Rica, and she observes a species of reptile that is unknown to her. In our second example, let's consider Austin, who is an avid book reader who just finished a book and is looking for his next read. Now Jane would like to find the name of the species that she encountered on her field trip by describing it, based on her recollection, as a red reptile, not more than 12 inches long, found in Costa Rica. Similarly, Austin expresses his preferences for finding historical fiction novels set in France. These examples showcase the fact that people often express their information needs with multiple constraints or preferences. Such information needs naturally give rise to queries that contain implicit set constraints. From the previous examples, we see that Jane's constraints on her query involve a complement and an intersection of three sets, whereas Austin's preferences involve an intersection of the two sets of novels: historical fiction novels and novels set in France, both of the sets that he's interested in. To operationalize this problem and study the effectiveness of systems for handling such selective information needs, we present a dataset called QUEST. QUEST is a retrieval dataset that includes more than 3,000 entity-seeking queries where queries contain implicit set operations, the answer entities are verified for relevance to the query, and their associated documents are marked with attributable spans for different query constraints.</sample>
    <sample id="107">The multilingual encoder-based models were trained on a combination of English and German queries to create a single model that can predict the SQL output for both languages.</sample>
    <sample id="108">Language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. In this work, we revisit the minimal pair paradigms. The minimal pair paradigm evaluates language models on top of acceptability judgments. Which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as CrowS pairs. And in this, minimal pair paradigm, the typical way to evaluate language models is that you show an acceptable sentence or a grammatical sentence and then you show an acceptable sentence or an ungrammatical sentence. And the hope is that the model puts more probability to the acceptable sentence. The current MPP pipeline doesn't allow us to evaluate a model's acceptance towards longer sentences. These days large language models are coming up with longer and longer context windows. It's crucial that we evaluate the models' acceptability throughout the context window and that is what we are trying to do here. We're trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences. So that is the approach. What we do is that to simulate these longer sequences, we revisit the data sets themselves and then we recreate sentences by choosing acceptable or unacceptable sentences from those datasets.</sample>
    <sample id="109">Instruction tuning enables pre-trained language models to generalize to unseen tasks in a zero-shot setting. Instruction tuning requires examples, which can be obtained by reformulating existing NLP datasets or collecting user-generated prompts and manually annotating their expected outputs. However, both approaches have limitations: the former is limited to academic benchmarks only, while the latter requires significant annotation effort. This work introduces Unnatural Instructions, a dataset of natural language instructions with corresponding inputs and outputs collected automatically without any human annotations. The data was generated using three examples from Super-Natural Instructions as input for GPT-3 to generate an additional example. To further diversify the format, the model generates paraphrases of each instruction. With 64,000 examples (and about 240,000 including paraphrases), this method demonstrates creativity, diversity, and correctness in generating task instructions. Fine-tuning on these instructions outperforms other methods across several benchmarks, highlighting the potential utility of such automatic data generation processes.</sample>
    <sample id="111">The authors assume the provider can collect a general text corpus and count word frequency with it.</sample>
    <sample id="114">The large language models are game-changing, but they have several limitations. For example, the heavy parameters usually contain billions of parameters and require long training time. The multi-head attention redundancy optimization has been a thread of work; however, it is not parameter-efficient because there remain considerable redundancy. We proposed a grouped head attention which uses a divide and conquer strategy to compress multi-head attention. It contains two strategies: group-constrained training and Voting-to-Stay algorithm. In Step 1, we collect voters on the whole training set, each batch as a voter. And in Step 2, the heads will receive votes based on their score given by the evaluator. In Step 3, we prune the heads with low votes. As we can see, there will only remain one head for each group and achieve significant parameter compression.</sample>
    <sample id="115">The approach uses lambda speech frames.</sample>
    <sample id="116">Servin is a judge.</sample>
    <sample id="117">Example quality is more important than the similarity to the source sentence.</sample>
    <sample id="118">The paper presents a new MLM objective for code-switched NLP tasks, which is tuned to handle code-switch information. The authors define what code-switching is and provide an example of a code mix sentence in English and Hindi. They explain that multilingual pre-trained models like mBERT and XLM-R don't perform well on code-switched tasks such as question answering and sentiment analysis.
The main contributions of the work are proposed novel MLM techniques, architectural changes, and auxiliary loss, all of which are tuned to the case of code-switching. SwitchMLM is introduced, which involves defining switch-points as groups of two tokens representing language transitions within sentences. FrequencyMLM is also offered as a surrogate method when access to LID-tagged datasets or taggers is not available. 
The results show that combining SwitchMLM with ResBERT (residual connections) and an auxiliary loss performs best across different languages pairs in sentiment analysis tasks. Probing experiments using linear probing and conditional probing verify the claim about increased switch-point information due to the proposed methods. Linear probing shows improved performance from StandardMLM combined with SwitchMLM representations compared to just StandardMLM. Conditional probing reveals more switch point information at layer 9 than at layer 12 in StandardMLM, suggesting residual connections should be added between these layers.</sample>
    <sample id="119">RoBERTa, GPT-2, and BART.</sample>
    <sample id="120">The model uses attention scores from a specific layer.</sample>
    <sample id="121">The examples of direct inference are "Easy on Me" and "I Gotta Feeling".</sample>
    <sample id="122">Fudan University, Microsoft Research Asia</sample>
    <sample id="123">The study investigates the use of instruction tuning to improve zero-shot learning in multi-modal tasks. The authors propose MultiInstruct, a new dataset consisting of 62 diverse multi-modal tasks across ten broad categories. They also introduce an evaluation metric called sensitivity and explore different transfer learning techniques for OFA, a unified multi-modal pre-trained model. Results show that instruction tuning can significantly enhance performance on seen multi-modal tasks while reducing model sensitivity.</sample>
    <sample id="124">Tan Qingyu from the National University of Singapore and Alibaba presented a study on temporal reasoning in large language models (LLMs). Temporal reasoning is broken down into three levels: time-to-time, time-to-event, and event-to-event. The research found that prior works overemphasized L2 reasoning, so they aimed to provide a more comprehensive approach by introducing the TempReason dataset covering all three types of reasoning with long temporal coverage.
The experiment involved evaluating three LLMs—T5-L fine-tuned on Natural Questions, FLAN-T5-L, and ChatGPT—on year prediction tasks. Results showed T5-L and FLAN-T5-L had biases towards 2000-2020 periods due to pre-training corpora term frequencies. However, ChatGPT performed well but deteriorated significantly when predicting months.
To improve LLM's temporal reasoning capability, Tan proposed two training strategies: Temporal span extraction pre-training for reconstructing mask, temporal, and entity spans; and time-sensitive reinforcement learning rewarding correct predictions while penalizing temporally wrong ones. These methods were applied through their final model, TempT5, which outperformed other compared models like FLAN-T5-L, T5-base fine-tuned on task data, and T5-SFT across various QA settings including Closed Book QA, Open Book QA, and Reasoning QA.</sample>
    <sample id="125">There are four authors involved in the paper.</sample>
    <sample id="126">Yes, Translate-Test is considered as a baseline.</sample>
    <sample id="127">The paper "Large Language Models Are Reasoning Teachers" by Namgyu Ho, Laura Schmid, and Se-Young Yun from KAIST AI in Korea introduces a method to transfer reasoning abilities from large language models (LLMs) to smaller ones. The problem with LLMs is that they require significant memory and computation resources, making them impractical for many applications. To address this issue, the authors propose using larger models as teachers to train smaller models through chain-of-thought prompting.
The technique involves applying zero-shot chain-of-thought prompting on very large models to generate step-by-step solutions for complex tasks. These solutions are then reformatted into training samples for small models, which can be fine-tuned to respond with step-by-step solutions followed by final answers. This approach allows even smaller models than 1 billion parameters to perform well on various tasks without relying solely on their own capabilities.
The study compares the performance of different methods across 12 tasks and demonstrates notable improvements when Diverse Reasoning is employed. It also highlights scalability issues related to development time costs, dataset size, teacher model quality, and inference-time costs associated with student model choices. Overall, the research suggests that simple distillation techniques like those proposed here could enable the transfer of reasoning skills from large LLMs to smaller ones, potentially opening up new possibilities for more efficient and practical AI applications.</sample>
    <sample id="128">Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by a pretraining, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pretrained-time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, "John saw the newly elected president on TV." Pretrained parameters can contain information about what presidents do and what a TV is but they cannot reliably know who this instance-specific entity "John" is, or who the new president is, because the president might have changed since pretraining. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task, designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and established coreference resolution models. Here is an example from our data set. Servin is a judge. Kea is a Baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun "he" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge such as "Servin is a judge." And second, background knowledge such as "Judges decide cases in law courts." Generally, background knowledge is learned during the pretraining of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source, or in multiple sources. We have defined three settings of KITMUS. First, we have the typical setting: "Background-Pretrain", where background knowledge is assumed to be available at pretrain time. Second, there's a "Background-Both" setting, where background knowledge is available both at pretrain time and inference time. Lastly, the "Background-Inference" setting, where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretraining. Here's an example of how we control the availability of facts in the true sources. In the Background-Pretrain setting, we assume that the background knowledge "Politicians seek elected seats in government" is contained in the pretrained parameters and in inference-time context we provide the entity-specific knowledge "Chichester is a politician." In the Background-Both setting, we additionally provide not only entity-specific but also background knowledge about politicians in their inference-time context. In the Background-Inference setting, we provide the fictional occupation "mirituer" instead of politician because "mirituer" is unlikely to be contained in the pretrained parameters. We evaluate the data set both with human study participants, and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background-Pretrain setting. Without task-specific training on KITMUS, both models do not perform well. When trained on KITMUS, however, both C2F and BERT4Coref perform significantly better than the random choice. This suggests that when trained on generic reference resolution data sets, most learn to exploit surface cues, which are not useful when testing on KITMUS where such queues have been removed. Additional experiments with fictional knowledge indicated even the best performing models, cannot reliably integrate backward knowledge presented only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the data set and code on GitHub. Thanks for listening.</sample>
    <sample id="129">The authors gave the example of an "Asian woman" as a marked group.</sample>
    <sample id="130">We did not find any model architectures that do not generalize well.</sample>
    <sample id="131">CIFAR-10, CIFAR-100, MiniImageNet</sample>
    <sample id="132">There are two authors involved in the paper.</sample>
    <sample id="133">The author works with multiple modalities, including text and images.</sample>
    <sample id="135">ABC-Eval is a new dimensional approach to evaluating conversational AI. This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI. The common practice of using human evaluation, such as asking human judges to select which conversation is better or rate conversations given a Likert scale works well for providing holistic evaluations of overall dialogue quality but does not allow for understanding strengths and weaknesses on a finer-grained level. ABC-Eval attempts to reduce subjectivity through explicitly annotating whether each model response expresses certain behaviors like responding with irrelevant information or contradicting itself. It measures rates at which chat models commit various thematic errors including ignoring partners, saying something irrelevant, contradicting themselves or their partner, hallucinating facts, violating common sense knowledge, and failing to show empathy.</sample>
    <sample id="136">The speaker, Jasivan, presents their work titled "FERMAT: An Alternative to Accuracy for Numerical Reasoning" conducted with their supervisor Nafise at the University of Sheffield. The motivation behind this research is the importance of numerical reasoning in real-world applications and downstream tasks that require factual correctness. They introduce FERMAT as a flexible evaluation set based on arithmetic types.
The presentation begins by discussing the challenges faced when using language models for numerical reasoning tasks due to differences in model performance depending on size. Larger models tend to perform better than smaller ones, but there are still issues related to mathematical ability. Current benchmarks provide an accuracy score or F1 measure which does not offer insights into strengths and weaknesses regarding mathematical skills.
To address these limitations, FERMAT was developed - a collection of math worded questions extracted from Illinois and CommonCore datasets. These questions were modified to include decimals, large integers, small integers, etc., testing how well different models handle various aspects of number understanding and operations. Additionally, training dependency plays a role; however, it's noted that most models struggle across all tested areas even after fine-tuning.
A baseline zero-shot evaluation showed poor results among most models except those trained specifically on CommonCore and Illinois data sets. Fine-tuning improved overall performance significantly. Training templates also played a crucial part; diversifying them led to further improvements in model performance.
In conclusion, existing benchmarks lack representativeness and single scores don't adequately capture model capabilities. Therefore, FERMAT offers a more informative alternative. Areas identified for improvement include number encoding and tokenization within language models' processing mechanisms.</sample>
    <sample id="137">The paper introduces the task of language-guided floor plan generation, where a model learns to generate 2D floor plan designs directly from natural language instructions. The authors define this as an encoder-decoder problem and propose a sequence-to-sequence method that uses a pre-trained language model for better understanding abilities. They use publicly available floor plans to construct their Tell2Design dataset with human-annotated language instructions collected by crowd workers on Amazon Mechanical Turk. The main challenges include strict constraints under which design is generated compared to artwork-like text-conditional image generation, understanding big picture information in unstructured text, and dealing with ambiguous or incomplete instructions. To solve these issues, they cast the floor plan generation task into a sequence-to-sequence problem using room type labels and bounding boxes. Their T2D model outperforms other baselines significantly when tested on unseen instructions, indicating mutual benefits during training between artificial and human interactions.</sample>
    <sample id="138">The authors claim that the ability to integrate and use both pretrain-time and inference-time knowledge is an understudied area in NLU.</sample>
    <sample id="139">The speakers are Ying and Zhiyang.</sample>
    <sample id="140">Yes, CoScript underwent quality checks. Crowd-sourced workers were asked to find and revise the incorrect samples in order to ensure the quality of the validation and test set.</sample>
    <sample id="141">Existing resources for context-dependent translation are limited because they rely on domain knowledge and human curation, which restricts the types of contexts supported and the number of languages covered.</sample>
    <sample id="143">The approach is compared to the Wait-k strategy, Local Agreement, and a state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">University of Nantes, France; University of Lille 2, France</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Omission is a major factor in affecting the quality of dialogue summarization, leading to incomplete summaries where critical facts are lost. However, fewer works have systematically analyzed the omission problem, let alone addressed this problem. So, how serious is this problem in dialogue summarization? We analyze the percentage of summaries which suffer from the omission problem. The left figure shows the omission rate of summaries from five domains and six pre-trained models. We find that even the state-of-the-art model still reaches a high omission rate. About 70% of the generated summaries have the omission problem, which means omission is a general and serious problem in dialogue summarization. The right figure shows the position distribution of omission information in dialogues. We find that the omitted information are randomly distributed in each position of the dialogue regardless of its length and domain. It means dialogues are unstructured, and how to identify the key information is still difficult for current models. To better analyze the omission problem and further solve the problem, we need first to detect all possible omissions in dialogue summaries. Here we do the task definition for omission detection. Omission usually refers to the missing content in the generated summaries, which is presented in the gold reference. In this task, we mainly focus on the utterance-level omission, and the detection model needs to predict which utterances of dialogue are omitted in the candidate summary.</sample>
    <sample id="147">There are three authors involved in the paper.</sample>
    <sample id="149">Yes, the dataset is publicly available.</sample>
    <sample id="150">MeetingQA is a new dataset for extractive question-answering (QA) on meeting transcripts, which are long documents that often contain detailed discussions. Prior works in this area have only focused on summarization and extracting action items, but MeetingQA addresses the gap by introducing questions asked during meetings as well as their corresponding answers. The data collection process involves public meeting transcripts from the AMI corpus, with 7.7K questions split between Train, Dev, and Test sets. A majority of these questions elicit detailed responses or seek opinions, while some involve multiple speakers contributing to an answer span. To improve performance, various methods such as context-retrieval, single-span models, multi-span variants, and silver data augmentation were employed. Results show significant gaps between fine-tuned model performances and human performance, indicating challenges in both zero-shot and fine-tuned settings.</sample>
    <sample id="152">The presentation titled "Exploring Large Language Models for Classical Philology" by Frederick Riemenschneider discusses the intersection of NLP and classical philology. The speaker introduces valuable resources for Ancient Greek and Latin, including recent models such as Latin BERT (2020), Ancient Greek BERT (2021) and another Ancient Greek BERT from 2022. However, these models are all BERT models, which means they are encoder-only models, and none of them support multilingualism.

The current landscape shows that while there have been advancements in language models specifically designed for classics, scholars may still require a model proficient in both Ancient Greek and Latin. Common multilingual models usually do not pre-train on Ancient Greek texts, making it challenging to evaluate their capabilities accurately.

To address this gap, the research team has developed new language models tailored for classical philology. Their goals include creating comparable existing models, pushing the state-of-the-art further, exploring different model architectures, and introducing multilingual models capable of processing both Ancient Greek and Latin text using the same model.

The first step was gathering high-quality pre-training data. For Ancient Greek, the researchers used Open Greek &amp; Latin but also leveraged previously unused resources and created a new corpus from the Internet Archive. They searched for incorrectly transcribed Greek stop words like γάρ ("yap") to identify Greek texts reliably. After post-processing, they had a robust dataset ready for training.

For the multilingual models, additional resources were utilized: the Corpus Corporum for Latin and English texts related to antiquity. Once trained, the next important task was benchmarking against previous models' performances. Results showed significant improvements over existing methods for part-of-speech tagging, dependency parsing, lemmatization, semantic knowledge tasks, world knowledge tasks, and distinguishing synonyms from antonyms.

The study concludes with an overview of how T5's encoder behaves differently than native encoder-only models during training. It is noted that despite being multilingual, the performance difference between monolingual and multilingual models isn't substantial when evaluating semantic knowledge or world knowledge abilities.

For more detailed information about the project, interested individuals can refer to the paper presented at the conference.</sample>
    <sample id="153">Ninareh Mehrabi, a postdoctoral scientist at Amazon Alexa AI's Responsible AI team, presented their work "Resolving Ambiguities in Text-to-Image Generative Models" during the NeurIPS 2023 conference. The study focuses on addressing ambiguities that arise when prompts are provided to text-to-image models. For instance, certain prompts can have multiple interpretations, making it difficult for these models to generate accurate images based solely on user intention.

To tackle this issue, the researchers developed two frameworks: one involves using an external language model to generate clarifying questions from users or different possible visual setups; and another uses a VQA (Visual Question Answering) model to evaluate whether generated images align with human intentions after disambiguation. 

The findings indicate disparities in resolving ambiguities across various types of ambiguity, but overall suggest positive effects of using their proposed framework for faithful image generation. Additionally, they found that their automatic evaluation method is consistent with manual evaluations by humans, thus providing reliable means to assess text-to-image models' performance.</sample>
    <sample id="154">University of Trento and Foundazione Bruno Kessler</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">Dialogue summarization aims at distilling the salient information from a dialogue context into a concise summary. It is one of the most challenging and interesting tasks in the text summarization research field. Existing dialogue summarization methods mainly focus on modelling dialogue with pre-computed static graph structure using external linguistic tools such as discourse parsing and dialogue state tracking. However, there are two fundamental drawbacks of using these pre-computed dialogue structures. Such methods heavily depend on the reliability of the external linguistic tools, which may not deliver the accurate output and cause error propagation. The second drawback is that the static graph construction is disjoint with the graph representation learning phrase, and such a fixed graph could not dynamically adapt to the downstream dialogue summarization task. In our SDDS model, we first employ an Utterance Encoder to encode the utterance in the dialogue context into the vector representation. We use the existing data structure modeling method to construct the static graph. Then we propose a Static-Dynamic Graph module which first combines multiple static graphs computed in the previous step and then uses the dynamic graph module to capture the semantic relationship between the utterances based on their deep vector representation. Finally, we employ a pre-trained language model as the Summary Generator to fuse the static dialogue structure and the dynamically learned dialogue structure into the final summary.</sample>
    <sample id="158">The entities in a document may have multiple mentions across the text. The coreference resolution task is to identify the mentions and cluster the mentions that refer to the same entity. Conventional methods for this task need to enumerate all the pairs of mentions, which have quadratic complexity for both computation and memory consumption. The recently proposed cache-based methods use a fixed-size cache and reduces the complexity to a linear level. In the cache-based method, when the cache is full, it evicts an entity with some eviction policy, such as LRU, which means removing the Least Recently Used entity from the cache. However, in long documents, the topic may switch multiple times, and this causes the mention of an entity to be scattered across a wide range of text. So, the LRU policy will lead to a high cache miss when encountering a new mention. Our case studies show that the high-frequency entities are mentioned globally, and they account for most of the cache misses. With this consideration, we propose a dual cache that has a local cache and a global cache that work together.</sample>
    <sample id="160">Unordered multiset of tokens</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign</sample>
    <sample id="164">The benefit of weakly supervised learning is that it allows for training neural networks on data with weaker annotations, which are much cheaper to obtain than human annotations. This approach can be useful when manual labeling of the entire dataset is not feasible or cost-effective.</sample>
    <sample id="165">Wenting Zhao, a PhD student at Cornell University, presented the paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations" during his talk. The presentation began with an example to illustrate abductive reasoning and then provided a formal definition of it. Abductive reasoning involves identifying plausible explanations that bridge information gaps between context and outcome in a closed-world setting where candidate sets of explanations are given. Current approaches rely on supervised methods but require annotated plausible explanations, which can be noisy and subjective. To address this issue, the researchers introduced LiPoR (Likelihood Learning with Posterior Regularization), an unsupervised learning method for abductive reasoning without supervision regarding explanation plausibility. They treated Explanation Z as a latent variable and maximized the marginal likelihood of Outcome Y given Context X by marginalizing other possible explanations in Z. This approach did not need knowing which explanations were plausible. However, they also needed a regularizer to prefer some explanations over others due to their mutual exclusivity characteristic. Their result showed that LiPoR outperformed all previous models, including strong zero-shot GPT-3 baseline, by more than 4 absolute points in accuracy on AlphaNLI, the most widely-used abductive reasoning dataset.</sample>
    <sample id="166">Image retrieval from linguistically complex text is a challenging image text reasoning task because these images are highly similar and the description is long. Typical methods, such as visual language models, perform well on image sentence retrieval tasks, but their performance drops drastically when confronted with linguistically complex text. To address this issue, we get inspired by the Divide-and-Conquer strategy and Dual-Process Theory. Divide-and-Conquer is the strategy of solving a large problem by breaking the problem into smaller ones, solving the sub-problems, and combining them to get the desired output. Dual-Process Theory for human thinking: human brains contain two thinking systems. System 1 performs analogical reasoning, while System 2 is capable of abstract logical reasoning, well-suitable for complex reasoning problems. Pre-trained visual language models focus on analogical reasoning as System 1, based on the analysis of deep learning networks. When confronted with complex tasks, their performance drops drastically. We may need a logical reasoning System 2 to perform this complex retrieval task via a logical operation. Combining the advantages of System 1 and System 2 may be a significant way for complex reasoning, and they can be integrated with the Divide-and-Conquer Strategy. The first model for our proposed method is the Proposition Generator. It aims to decompose the complex proposition text into representations of a simple proposition. For explaining what simple propositions represent, we also use the decoder of BART to generate the corresponding sentences. System 1, which is the Visual-Linguistic Interactor, aims to perform the visual-propositions’ information interaction, resembles the System 1. The outputs of this module are matching scores of propositions and images, and their reasoning states. Then we introduce the Neural-Symbolic Reasoner as a System 2. It is responsible for integrating the reasoning states and results of simple propositions to obtain the final solution of a complex proposition on images. It consists of the negation executor and conjunction operation. Negation executor aims to gain the negational reasoning state of positive proposition. Conjunction operation is responsible for obtaining the inference results based on joint positive and negational reasoning states. Finally, we combine the inference results of System 1 and System 2 to gain the final solution. By doing so, the whole system utilizes the advantages of the analogical inference System 1 and logical reasoning System 2. Here we present two tables of our experimental results. We can see that the proposed method, NDCR, outperforms other baselines, and in the right part, the abolition experiments on the testing set also verifies the effectiveness of each module. Here we present two cases to further check the performance of the proposed method. We can see that our proposed method can present the inference states and inference results in the middle step. So the proposed method is processing interoperably. To conclude, we present some suggestions. First, neural symbolic calculation may be a worthwhile approach to improve the compositional reasoning and planning of large language models. Divide-and-Conquer is similar to the self-asking chain-of-the-thought, aiming to decompose the complex reasoning into simple problems and construct a reasoning path. Both are effective for solving complex problems. Dual-Process Theory could be integrated with the Divide-and-Conquer.</sample>
    <sample id="167">The documents in DEPLAIN-web were aligned with manual and automatic alignment methods.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting data from Reuters News in 2020 and then annotating them with the same guidelines used for CoNLL-2003.</sample>
    <sample id="169">The paper "Prompting PaLM for Translation: Assessing Strategies and Performance" presents a systematic study of large language model prompting for machine translation. The authors, from Google Translate, evaluated the performance of their 540 billion-parameter large language model (PaLM) using state-of-the-art neural MT metrics and expert-based human evaluation results. They found that the quality of examples is more important than the similarity to the source sentence in prompt selection strategies.
The experiment involved comparing one-shot prompting with two different prompts per sentence versus five-shot prompting where each sentence was marked with its respective language. Results showed significant differences in BLEURT points between these methods, highlighting the importance of selecting good prompting strategies.
In terms of actual translations, PaLM performed comparably to commercial systems like Google Translate but struggled with accuracy due to omission errors. However, it provided fluent output despite some inaccuracies. This suggests that while PaLM may not always produce perfect translations, it can still generate high-quality, fluent text when properly prompted.</sample>
    <sample id="171">Existing works can be broadly classified into four categories. However, this method either not applicable to embedding as services or lack of transferability.</sample>
    <sample id="172">No, they are still inadequate.</sample>
    <sample id="174">Hi, I'm Thea and I'm one of the co-authors of the paper "ArgAnalysis35K : A large-scale dataset for Argument Quality Analysis" In this video, I'm going to quickly explain why this dataset is unique from other datasets that you'll find on a similar topic. This is just going to be a quick overview of the special features that we have, so do make sure to check out our paper and our poster at the conference for better insight into the results, dataset collection process, dataset annotation process, etc. So, very quickly: What is argument quality analysis? It's simply judging how good or bad an argument is on a scale from 0 to 1. So something like "Big banks are bad" is likely to be rated low, but something like "Big banks have no accountability, take heavy risks, and lead to major collapses, which is why they should be broken up" — which is a coherent argument which is more persuasive in terms of what it's trying to achieve — is going to be rated high, probably close to a 1. So let's brainstorm some problems with current datasets. Meaning that current datasets lack quality because they are often collected from crowdsourcing platforms. They lack diversity because they often only consist of 30 or 40 motions and source arguments on those. They often lack depth in explaining why a specific argument is true, and they often have a motion associated with every single argument that exists. So, how does ArgAnalysis35K do this differently? This is just an index of the special features that I'm going to cover in this presentation. So firstly, it's the largest dataset with really high-quality arguments. So as you can see from the title, first off, it has 35K argument-analysis pairs, which is the largest dataset in this field to our knowledge. And as you can see from this table, around 85% of those arguments are sourced from either speeches from really high-quality tournaments or from expert debaters or from intermediate debaters. And in the last 15% is sourced from novice debaters, everyday people, etc. So it has a higher quality of arguments compared to if you want to just crowdsource all of your arguments. It has a diverse range of arguments. So instead of generating arguments on specific 30 or 40 motions that are picked before, we pick 24 themes based on our experience in the circuit, based on websites like Hellomotions.com, based on expert advice, etc. For every theme, we've captured as many motions as possible using the same sources. And this just creates a better diversity in terms of the motions that you'll encounter in a parliamentary debate setting, rather than just pre-selecting a few motions. We added in an element of analysis instead of just keeping arguments. So analysis is not similar to claims or premises. It's likely a combination of all of those things. And this is the term that we've introduced to the NLP community, so to speak. So I'll quickly give an example of what this looks like. Analysis can be something like a premise if you just say that "Educated people are 80% more likely..." if you just give a statistic, for example. But it can also be a claim and a premise if you draw a link. So if you say that rich people send their kids to private school, which leads to them having better jobs and being richer, that's you combining a claim and one premise, and making it into one analysis. You can also have analysis and multiple claims and premises, as this argument will tell you. So at that point in time, you need to see analysis as one coherent thing that explains the argument better. So the argument in this case is this idea that education is the basis of everything the person achieves Normally in datasets, you will just have a column for arguments. But we have expanded this, creating an idea of analysis which explains this claim better. So instead of just saying, "The claim is that education is the basis of everything a person achieves", we have pushed the burden to explain why that is true. So analysis just ends up being any combination of claims, premises, etc. Which is also why it needs to be introduced, because it's nothing that exists in the general terms that exist in NLP. We have introduced an idea of instance-based annotator reliability. So, annotators have human biases about certain topics, right? So someone who experiences racism on a daily basis might have strong sentiments about those arguments, but probably is not biased about arguments when it comes to art. So we can still use their judgment in judging those motions. So instead of just eliminating annotators that may be unreliable for a few topics, instead of eliminating all of their judgments, we only eliminate their judgments that we think are biased. So when you capture annotation reliability on an instance-based level — so on a level for every argument — you are able to better utilize the annotations that you have. So we think that this dataset is a good use case for instance-based reliability, which is something that already exists. Lastly, we introduce something called a relevance model. So usually datasets will just have an argument connected to a specific motion. We think that motions are not singular in that way. Arguments like "accountability is important" can be used in debates about governments, churches, corporations, schools...anything. Accountability is important in all of those. Arguments that deal with the premise of free speech can be used to defend it for the LGBTQ community or defend people's right to protest against a corporation. So, a relevance model simply assigns a score from 0 to 1 for each of these and for each theme, and has more arguments and better captures the relevance that each argument has to a topic. So we think that this dataset is just a culmination of a bunch of unique things. And at the end of it, you're able to get something that is more diverse, that has a score for relevance, so you're able to capture how relevant it is to a particular theme. You have a higher quality of arguments, and in general, you just have more reliable scoring because we captured an instance-based level. So do make sure to check out our paper, and give us your feedback on it. Thanks.</sample>
    <sample id="175">The method deals with the ambiguity of permutations by inducing the alignment as part of the training. Sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined by evaluating its performance on different demographics or political leanings. This involves separating the performance into various categories to assess how well the model handles hate speech and misinformation for different groups, such as detecting hate speech targeting socially minority groups versus more powerful groups in society, or identifying misinformation from opposite political leanings. The goal is to identify any biases that may arise due to the language models' political leanings and address potential fairness issues.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">The paper "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker" explores the limitations of large language models in understanding theory of mind, particularly their poor performance on false-belief tasks. To address this issue, the authors propose SymbolicToM, an inference-time method that uses explicit graphical representations to improve Theory of Mind reasoning skills in Large Language Models. The approach leverages off-the-shelf NLI and OpenIE models for computing these graphs efficiently. Experiments demonstrate significant improvements across various LLMs when using SymbolicToM compared to supervised baselines like fine-tuned GPT-3 or Textual Time Travel. Additionally, the proposed method generalizes well to new datasets designed for story structure and linguistic diversity, indicating its robustness beyond just improving out-of-the-box performance.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">Constrained language planning, which imposes different constraints on the goals of planning, is under-studied. This paper defines this problem and evaluates constrained language planning ability of large language models by extending abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. The authors find that all language models achieve unsatisfactory results in generating scripts for specific goals due to their inability to guarantee faithfulness to constraints. To improve generation quality, they propose an over-generate-then-filter method based on symbolic knowledge distillation from large language models. They generate a high-quality script dataset named CoScript consisting of 55,000 specific goals with scripts. Finally, they use T5 fine-tuned on CoScript to demonstrate that smaller but specialized models can surpass larger ones when properly trained on suitable datasets.</sample>
    <sample id="182">Tropicalism indicates a trope that is reflected in the words describing Latina women, such as "vibrant" and "curvaceous". This connects to a long history of exoticizing Latinas.</sample>
    <sample id="183">The authors used a study where they gave prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes. This enables direct comparison between their generated personas and the human written responses.</sample>
    <sample id="184">CXMI was used to measure context usage in this work.</sample>
    <sample id="185">DrBERT is based on RoBERTa and trained on NACHOS, which is a data set of medical crawled data from the web. ChuBERT model is based on anonymized data obtained from the Nantes University Hospital data warehouse.</sample>
    <sample id="187">There are two authors involved in the paper: Ying and Zhiyang.</sample>
    <sample id="188">Iterative transfer learning is a method of improving dissonance detection by transferring weights from closely related tasks, such as topic-independent dissonance stance classification and binary classification of expansion and comparison classes in PDTB. This approach helps to capture the dissonance class better than using only zero-shot performance on annotated data. By iteratively fine-tuning on both tasks, the model can achieve higher accuracy with fewer annotation runs, reducing overall costs while enhancing dissonance detection capabilities.</sample>
    <sample id="189">To understand users’ language when they want to make a choice.</sample>
    <sample id="190">The attacker can extract model parameters through an EaaS by learning from the embedding and providing similar services.</sample>
    <sample id="191">There are three authors involved in the paper.</sample>
    <sample id="192">Robust training of large language models often relies on adaptive gradient-based optimization methods. However, some widely-used optimizers like Adam always triple the required memory for keeping the first and second moment estimates of per-parameter gradients. Some existing memory-efficient optimizers like Adafactor have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. Here's our challenge. How to design an optimizer to simultaneously achieve two goals: a fast convergence as in traditional adaptive methods, and a low memory usage as in memory-efficient methods? We introduced two scenarios to demonstrate how two types of erroneous updates are supposed to be handled in the ideal case. For example, in Figure (a), the difference between the momentum of updates (mₜ) and the current updates (uₜ) is large, illustrating the historical experience. And if we utilize the row mₜ to take an optimization step, the direction of optimization will diverge increasingly from the desired direction. Inspired by the erroneous update, we firstly consider an efficient approach to decrease the side effect caused by insecure updating. Given mₜ and uₜ, we take the residual between them as the instability in the preserved momentum and set generated instability as a denominator of original mₜ to more adaptively take an updating step. As compared to the original Adafactor parameter, we calculate the instability matrix (uₜ), and update the rₜ and cₜ in the same manner of Adafactor. With the approximation instability matrix (Sₜ) calculated, we apply the square root of it as the denominator for mₜ to take an optimization step. In the Experiments chapter, we perform experiments on the BookCorpus and English Wikipedia, and present extensive comparisons with existing optimizers on training tasks of three important large language models: BERT, GPT-2, and T5.</sample>
    <sample id="193">We used 10 annotators to create the initial dataset.</sample>
    <sample id="194">Carnegie Mellon University, the University of Washington and the Allen Institute for AI</sample>
    <sample id="195">Leveraging question decomposition is a promising direction since we can flexibly select the appropriate knowledge source for each sub-question. However, there are two main challenges. The first is how to determine the granularity of question decomposition, since certain complex questions can be directly answered with a knowledge source and further decomposition increases the possibility of error. For example, Q1 in the figure can be answered with the Wikipedia corpus without further decomposition. The second is how to find the optimal solution among various possible ones, since question decomposition and answering are both uncertainties. To this end, we propose a novel framework RoHT, Reasoning over Hierarchical Question Decomposition Tree. RoHT is a two-stage framework. Firstly, we propose to understand the hierarchical compositional structure of a complex question by building its Hierarchical Question Decomposition Tree (HQDT). In this tree, the root node is the original complex question, and each non-root node is the sub-question of its parent. The leaf nodes are atomic questions that cannot be further decomposed. Second, we propose probabilistic reasoning over HQDT to fuse the knowledge from a knowledge base and a text corpus at different levels of the training, and take into consideration the probability score of both string generation and answering.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">The state-of-the-art models in dialogue systems are R2D2, DRIFT, MIND, and E2E-SDM.</sample>
    <sample id="198">Because large language models are coming up with longer and longer context windows.</sample>
    <sample id="199">Yes, training in multilingual fashion caused performance drop compared to monolingual English model.</sample>
    <sample id="200">No, the annotators do not know about these entities in advance.</sample>
    <sample id="201">The MT metrics used for the evaluation were BLEURT and human evaluation using the MQM framework.</sample>
    <sample id="202">No, the regression in generalization does not impact specific NER types.</sample>
    <sample id="203">Positionality in NLP matters because it can influence the research process and its outcomes, leading to systematic performance differences of technology between populations. This is important as NLP tasks become more subjective and socially oriented, making it challenging to characterise how these positionalities are skewed due to lack of documentation and hidden models behind APIs.</sample>
    <sample id="204">Fine-tuned with adapters</sample>
    <sample id="205">Language models are trained on large-scale web crawl data, which includes political news media. This can lead to potential fairness issues in downstream task applications due to the inherent social biases present in pretraining data. To address this, researchers propose investigating the political bias propagation pipeline from pretraining data to language models and then to downstream tasks. They evaluate how different language models with varying political leanings perform on hate speech detection and fake news detection tasks. Preliminary results indicate that these models have differing political leanings and exhibit performance disparities based on their ideological perspectives. The study highlights a pressing need to acknowledge and tackle the fairness issues arising from language model political biases.</sample>
    <sample id="206">They use a model that is fine-tuned on both the CE (consonance and dissonance) task and debate stance classification. They find that this combination yields better zero-shot performance compared to using either of these tasks alone or neither.

The specific details about which pre-trained models were used are not provided in the summary, but it mentions transferring weights from closely related tasks: 
1. **Topic Independent Dissonance Stance Classification** (debate)
2. **Binary Classification of Expansion and Comparison Classes of PDTB**

These two tasks are considered closely related because they deal with similar concepts of consonance and dissonance as discussed in their work. The transfer learning approach helps improve the initial classifier's ability to detect dissonance by leveraging knowledge learned from these other domains where such relationships might also be present. 

For further technical details regarding the exact architecture or hyperparameters of the transferred models, one would need to refer directly to the paper for comprehensive information.</sample>
    <sample id="207">The recent test sets used to assess the PaLM capabilities are not specified in the given content.</sample>
    <sample id="208">Three</sample>
    <sample id="209">The gain of the proposed method over the strongest baseline is 10.8%.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a base benchmark for automatic text simplification.</sample>
    <sample id="212">They experiment with 2 smaller models in the paper.</sample>
    <sample id="213">OFA, a unified multi-modal pre-trained model.</sample>
    <sample id="215">Hi, my name is Adam Przepiórkowski and this talk is about the Dependency Structure of Coordination. As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example, in the universal dependencies, the structure of the coordination, Lisa, Bart, and Maggie, such that the first conjunct is the head of the whole coordinate structure. Right? In this case, Lisa. A similar approach is assumed in Igor Mel'čuk's meaning text theory, where again, the whole coordinate structure is headed by the first conjunct. These two approaches are asymmetric. They single out one of the conjuncts. Now those are asymmetric approaches to coordinate structures, such as the Prague approach. The conjunction headed approach assumed in Prague dependency treebanks, where coordinate structures are headed by the conjunction. We get some dependencies from end to all the conjuncts. And finally, there's also a multi-headed approach that's used, for example, in the Hudson's Word Grammar, where they say all conjuncts are heads of the coordinate structure. We get dependencies from the governor: loves to all conjuncts separately: Lisa, Bart, and Maggie. Now the aim of this paper is to produce a novel argument for the symmetric structures of coordination, like these two and against the asymmetric structures of coordination, like these two. OK. The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples.</sample>
    <sample id="217">The paper introduces a method for generating controllable dialogue that can handle multiple attributes. The authors propose DCG, which stands for Disentangled Controllable Generation, and introduce a unified evaluation framework called MAE (Multi-Attribute Evaluation). They also provide two benchmarks to demonstrate the effectiveness of their approach.
The motivation behind this work is that previous methods have focused on single attributes rather than handling multi-attribute generation in practical settings. Additionally, existing models lack the ability to generate responses based on continuous attributes. To address these limitations, the authors developed an approach that learns attribute concepts from seen values using disentanglement loss. This allows them to create prompts that guide the model's focus on specific information within the dialogue while still capturing global features through task-oriented prompts.
To improve the generation capability of their model, they designed pseudo combinations and introduced a disentanglement loss during training. By doing so, they were able to force the model to distinguish between different attribute value combinations effectively.
The proposed method does not require additional large-scale labeled data due to its use of a template consisting of discrete prompts such as emotion/act/persona controls the response [MASK]. This helps reduce potential biases associated with handcrafted patterns. Furthermore, by adding a trainable continuous dialogue-oriented prompt, stability and robustness are improved.

The results show that DCG outperforms all other baselines in terms of attribute controllability and text equality when compared against E-ACC and A-ACC metrics. It also demonstrates better performance over CTRL across both controllability and text equality aspects concerning unseen attribute combinations.
In conclusion, the study explores compositional generalization for multi-attribute controllable dialogue generation and proposes a prompt-based disentangled controllable dialogue model.</sample>
    <sample id="218">Google Translate</sample>
    <sample id="219">The paper presents a compare-and-contrast multistage pipeline for uncovering financial signals in financial reports. The authors introduce the highlighting task, which involves comparing and contrasting the context between two reports to find the rationale (words) of relations between them. They propose a multi-stage pipeline that includes document segmentation, relation recognition, out-of-domain fine-tuning, and domain-specific fine-tuning. The evaluation dataset consists of eSNLI pairs and their released FINAL dataset. Two metrics are used to evaluate performance: precision over recall and PCC correlation with annotations. The results show that their model achieves the best performance on the FINAL dataset and preserves generalization capability compared to other methods. Future work could include improving effectiveness or adding more features to enhance application.</sample>
    <sample id="220">Stony Brook University, University of California San Diego</sample>
    <sample id="221">The paper analyzed translation from German to English and Chinese to Japanese.</sample>
    <sample id="222">The paper investigates different data interventions that would be useful in enabling out-of-domain generalization in open-domain QA. It identifies the type of dataset shift a new domain exhibits and determines what kind of data interventions are effective for a specific type of shift. The setup includes a general-purpose Wikipedia-based source domain on which both Reader and Retriever are trained, with seven target passage and datasets spanning across six different domains used to test the generalizability of the source model. Two overarching methods were looked at for generating these interventions: zero-shot and few-shot. For zero-shot techniques, named entity recognition was performed on the target corpus to sample from answer span categories like location, person, numbers, etc., and generate cloze-style questions based on the sampled spans. To vary context distribution, passages from source and target domain were pulled and compared using existing retriever models. Existing data shift taxonomy is considered to understand the nature of shift in target datasets with respect to the source model. A compatibility measure is computed by assigning likelihoods to question/answer and context triples from the target dataset. Datasets under each category of shift respond well to either few-shot or zero-shot adaptations depending on their specific needs.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">We have fine-tuned the model of long-mBART to produce document-level simplifications, and we also fine-tuned the normal base mBART to produce sentence-level simplifications.</sample>
    <sample id="225">For training, 53 tasks from 9 groups are used. For testing, the entire common sense reasoning group is reserved for testing and an additional 5 tasks from VQ and Miscellaneous groups are selected.</sample>
    <sample id="226">There are three authors involved in the paper.</sample>
    <sample id="227">We all know that language models have achieved great success recently, providing a general solution to many different NLP tasks. Here we want to ask what's missing in current language models’ research. We think the answer to this question is grounded language understanding, which basically means grounding a natural language expression into something that can be executed over a specific target environment, which is also referred as a plan or a program, so depending on whether it's described by a program language. There are many applications to grounded language understanding, such as smart assistants like Siri and Alexa. There's also semantic search on Google and querying a medical database using natural language, and also domestic robots that follow natural language instructions. So all these applications involve mapping a natural language expression onto some representation in a specific environment. For example, when querying a medical database using natural language, we need to map the query onto a SQL query or SQL problem, while for the robotic scenario, we need to map the natural language instructions into a sequence of actions the robot can take. But why is grounded language understanding challenging? The main reason is the lack of grounding during pre-training. We know that most of the language models, including the recent large language models, are mostly pre-trained with textual corpus and without grounding. So this is also related to the octopus test, and the gap between the pre-training and the downstream application makes the task of grounded language understanding particularly challenging for language models. Existing research on different grounded language understanding tasks typically uses language models to directly generate a plan via autoregressive decoding that's capitalizing on the generation ability of language models. However, the main issue is that the generated plan or program may not always be grammatical or valid. For example, in the scenario of knowledge-based question answering, it's very likely a KB query generated by T5 is not executable over the KB, or maybe returns an empty set of answer entities. But in our work, we propose a novel framework for grounded language understanding where we let language models focus on discrimination instead of generation. We argue that discrimination is something much easier for language models to excel. Specifically in our framework, a symbolic agent interacts with the environment and proposes candidate plans, while a language model is used only to score and rank the candidates proposed by the symbolic agent. In this case, the language model does not need to handle the validity and grammar of the target plan by itself because it does not need to do the generation by itself. For more details, please refer to our paper, and we always welcome offline discussions. We name our framework after Pangu, who is the primordial being in Chinese mythology who separates heaven and earth, just like how our framework separates the newer world and the symbolic world. We instantiate our idea on knowledge-based question answering, which is a typical scenario with a massive, heterogeneous environment for grounded language understanding. Note that our framework is generic and is not specific to knowledge-based question answering. Here we just use it as a representative testbed. We experiment with language models of different natures, including BERT, T5, and also large language models like Codex. Also, we experiment with both fine-tuning and in-context learning. Pangu achieves outstanding performance across all settings. Here's the results for fine-tuning, and also here's the results for in-context learning. In particular, Pangu also demonstrates strong sample efficiency. For example, when using Codex with in-context learning, Pangu can achieve an accuracy of over 50% of GRAIL query with only one demo example, significantly outperforming all other settings. And also for fine-tuning experiments, Pangu consistently outperforms a baseline model, ArcaneQA, in terms of sample efficiency when using different language models. We also have an interesting finding that may explain Pangu's strong generalizability under non-i.i.d. settings, as specifically we observe that autoregressive models like ArcaneQA tend to overfit seen structures during training. While for Pangu, we can see that for both the seen structures and unseen structures, the distributions of probability are almost the same, so this might be a signal for Pangu's strong robustness under the non-i.i.d setting. Here comes the most important message and the one-sentence takeaway of our work. For grounded language understanding, generation may not be a good idea. Instead, discrimination might be a much better strategy of using language models for grounded language understanding. We are always open to different forms of discussions and collaborations. And also, we are eager to hear your thoughts on our work, and we appreciate your time and your attention. Thanks a lot.</sample>
    <sample id="228">AG News, MIND, SST2 and Enron Spam.</sample>
    <sample id="229">Text revision is an essential part of professional writing and is typically a recursive process until somehow optimal phrasing is achieved. From the author's point of view, finding the right words and expressions is especially important in argumentative writing as it can directly influence the effect the text has on the audience. As such, optimal phrasing is essential for effectively communicating a message in order to stimulate a desired reaction. To get a better understanding of what this process can look like, let's follow the revision process of the argumentative claim found below, stated as "Cell phones cause brain cancer". In the first revision, the author further specifies that it’s cell phone radiation that causes brain cancer, and in the following revision, the claim is further clarified that it’s not definite, and it has changed to “Cell phone radiation may cause brain cancer.” But how to know whether an argumentative claim is phrased well enough and no more revisions are needed? Our paper focuses on answering this question that puzzles so many novice writers. To formalize what we want to learn, we introduce two new tasks formulated as follows. Task 1: Suboptimal-Claim detection. We’re given a claim, and we decide whether it needs revisions or it can be considered phrased optimally. And Task 2: Claim Improvement Suggestion. Where, given a claim, we need to select the types of quality issues that should be improved when revising the claim. When thinking about how to tackle this problem, one may intuitively think, “Why not learn directly from human patterns of revisions instead of explicitly defining what makes a claim good or bad?” In this work, we explore the challenges that arise when working with such revision-based data, as different domains have different goals, different notions of quality, and subsequently different revision types performed.</sample>
    <sample id="231">NACHOS is a data set of medical crawled data from the web.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Simultaneous speech translation, or SimulST, is the process of translating spoken language into a text in another language in real time, enabling cross-language communication. And what are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes. For example, training a model with an average of one second latency and another one with two seconds latency, and so on. So what is our solution? First, to use already existing offline ST models without re-training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. And leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output. That is the cross-attention mechanism, and you can see an example on the right. Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which we decide whether to emit or not a partial translation, based on where attention points to. A word is emitted if the attention is not concentrated, that is, its sum is below a certain threshold alpha towards the last lambda speech frames, meaning that the received information is enough stable.</sample>
    <sample id="234">The prompting strategy has a significant impact on the results. In experiments, using one-shot prompting with two different prompts for each sentence resulted in more than 1 BLEURT point difference, and in extreme cases up to 40 BLEURT points. This highlights the importance of selecting an effective prompting strategy for large language models (LLMs) used in translation tasks.</sample>
    <sample id="235">The affiliations of the authors are: University of Toronto, Google Research, and Seoul National University.</sample>
    <sample id="236">The 5 expert-written instructions are not specified in the provided content.</sample>
    <sample id="237">The authors propose a diagnostic test suite for knowledge integration, which includes a coreference resolution task designed to probe the ability of models to draw on knowledge available in different sources.</sample>
    <sample id="238">The University of Central Florida has released a new dataset called MeetingBank, which aims to help develop summarization technologies for different reading domains. The dataset includes meeting transcripts, reference summaries, and other URLs that contain useful resources. To create this dataset, the researchers addressed two major challenges: high-quality meeting summaries or scores, and locating trustworthy resources for public meetings. They managed to collect data from City Council meetings using Speechmatics API to convert audio data into transcripts, open meeting websites, identify types and data of meetings, locate corresponding reference summaries, obtain meeting segments with start and end times, align timestamps, and pair them with previous extracted summary.</sample>
    <sample id="241">The paper "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" discusses the deficiencies in current approaches to automatically detecting misinformation on social media platforms. These systems are often unrealistically evaluated and not representative of real-world scenarios, leading to a lack of human involvement throughout the process. The authors propose an evaluation framework that addresses these deficiencies by incorporating humans at various stages of the process. Their system involves two main components: claim detection using keyword filtering and T5 model training; and policy violation verification using BERT-based stance classification models. They evaluate their workflow through early detection tasks and find promising results with regard to both efficacy and workload efficiency.</sample>
    <sample id="242">The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="243">There are 5 authors involved in the paper.</sample>
    <sample id="244">Judges decide cases in law courts.</sample>
    <sample id="245">The authors present a study on the recruitment of high-agreement workers for summarization tasks on Amazon Mechanical Turk (MTurk). They propose a two-step pipeline that includes qualification settings, such as location and HIT Approval Rate, followed by a qualification task to test annotators' ability to evaluate multiple dimensions correctly. The second stage involves endurance tasks to assess capacity for handling heavy workload.
The results show that 26 MTurk workers passed both stages, with only gold and silver workers achieving high agreement in terms of inter-annotator agreement (IAA) compared to experts. A reference-based task was also designed to test general performance, resulting in an average Krippendorff's Alpha of 0.534 across different workers.
The baseline approach using MACE threshold achieved a Krippendorff's Alpha of 0.380 but had incomplete HIT coverage and fewer workers per HIT. CloudResearch MTurk workers showed better results with a Krippendorff's Alpha of 0.513 but lower task acceptance rate.
The analysis of correctness across annotation sources revealed significant Spearman's correlation between Pipeline and CloudResearch workers, indicating similar quality at a lower cost. However, there is no guarantee for training in correctness, and real GPT models correlated well with expert judgments.
In conclusion, pre-task filtering through this pipeline can avoid resource waste and achieve high agreement while maintaining similarity to CloudResearch. Future research will focus on investigating ways to hire high-quality workers and exploring applications beyond English summarization on MTurk platform.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">Existing fact verification datasets, such as FEVER and VitaminC, use Wikipedia text or tables as evidence. However, there was no dataset that utilized knowledge graphs (KG) as evidence with natural language claims. The authors propose a new task called Knowledge Graph-Based Fact Verification, which leverages KGs for reliable reasoning and practical applications like consistency checks in dialogue systems. They introduce the FactKG dataset, featuring two styles of claims: written and colloquial, along with five types of reasoning— one-hop, conjunction, existence, multi-hop, and negation. The dataset includes 51% support and 49% refute labels to verify claims using DBpedia's knowledge graph. Five baselines are constructed: Claim Only, GEAR model without graph evidence, and three others utilizing different methods. Results show improved performance over the majority class baseline by all models, highlighting the effectiveness of integrating KG-based evidence into fact verification tasks.</sample>
    <sample id="248">No, the annotators for NLPositionality are not balanced in regard to each demographic.</sample>
    <sample id="249">To perturb the sentences in the acceptable domain, they added noise to preserve relevant structure.</sample>
    <sample id="250">Dimensional evaluation is a strategy for evaluating multiple aspects of chat quality to understand the strengths and weaknesses of models on a finer-grained level.</sample>
    <sample id="251">The affiliations of the authors are: University of Science and Technology of China, Alibaba DAMO Academy, and Microsoft Research Asia.</sample>
    <sample id="252">The Prior Case Retrieval (PCR) task involves retrieving relevant candidates from a candidate pool for legal documents. This is challenging due to the increasing volume of cases, and traditional methods rely on experience. The U-CREAT pipeline leverages unsupervised learning techniques and introduces an event-based approach for PCR tasks. It demonstrates high retrieval efficiency, low inference time, and generalization across Indian and Canadian legal systems without requiring law or demographic-specific tuning. Event extraction plays a crucial role in this process, with dependency parsing used to represent case documents as collections of events. The paper presents experiments using various models, including count- and transformer-based models, as well as event-based models such as Atomic Events, Non-Atomic Events, and Event Filtered Documents. Results show that the method using Event Filtered Documents outperforms all other methods with significant boosts in performance.</sample>
    <sample id="253">The detection of mental health disorders is a challenging task that requires the analysis of large amounts of data. Social media content provides an opportunity to do research on how people undergo difficulties, and many use online platforms to publicly share their daily routines or explicitly discuss mental health issues. In this work, we aim to contribute to the detection of mental health disorders by automatically analyzing social media posts using DisorBERT, which combines double domain adaptation with guided masking techniques. Our approach learns from Reddit's language model and incorporates knowledge about mental health terms to improve its performance in detecting signs of depression, anxiety, PTSD, bulimia, and other mental illnesses. The results show that our method outperforms MentalBERT, another state-of-the-art solution for similar tasks, demonstrating the effectiveness of combining these two approaches.</sample>
    <sample id="254">The document-level relation extraction is aimed to extract relations among entities in a document. It can be seen as this figure. Previous methods rely on the large-scale human-annotated corpora, which is time-consuming and labor-intensive. So recent work leveraged distantly supervised data to pretrain the document-level relation extraction models for better performance. As we know, these data contain various noise levels. Current efforts to alleviate the noise problem by using pseudo labels. However, these kinds of methods still persist the risk of noise induction by false-positive pseudo labels, as shown in this figure. When we rely just on the pseudo label, we will obtain an extra false relation, "composer," and lose the correct relation, "place of birth". So how to mitigate the noise caused by the pseudo labels is still a challenge. In this paper, we propose a document-level relation distant extraction framework with uncertainty-guided label denoising to improve the label quality of DS data. This is the overview of our framework. We first train a pre-denoising DocRE model with both DS and human-annotated data to generate the pseudo labels. Since false pseudo labels are inevitable, we introduce uncertainty estimation to determine whether model prediction can be trusted or not. Considering there might be multiple relations between an entity pair, we propose an instance-level uncertainty estimation method to capture uncertainty scores for overlapping relations. We also designed a re-labeling strategy with dynamic class uncertainty thresholds and a multi-phase training strategy to further boost the performance. Uncertainty estimation is an important technology for misclassification detection, out-of-distribution instance detection, and active learning. In order to model the uncertainty in pre-denoising DocRE model, we introduce the Monte Carlo dropout technology in the DocRE task. This method requires multiple stochastic forward-pass predictions with activated dropout to capture the model uncertainty. Previous works based on MC dropout calculate the uncertainty score of the model prediction as this formulation. However, the previous method is not suitable for the overlapping relation problem, as shown in the left figure. There are two different types of relations between this entity pair. It is hard to separate the false positive pseudo label, "composer", and the correct positive pseudo label, "director" by the previous uncertainty estimate methods. To solve this issue, we modify the estimation process to obtain the instance-level uncertainty score for each positive pseudo label. The calculation can be seen in this formulation. We observe that the distribution of uncertainty score for each relation class is different. Moreover, it can be observed that frequent classes usually contain lower average uncertainty than the long-tail class. So we propose dynamic class uncertainty thresholds to filter pseudo labels with high uncertainty. The calculation can be seen in this formulation. Then we replace the original DS label with the pseudo label that contains a lower uncertainty score than its class uncertainty thresholds. In order to take full advantage of the DS data for further boosting the performance of DocRE model, we design a multi-phase training strategy to iteratively re-label the DS data, which is shown in this algorithm. We compare our framework with several strong baselines onto public datasets. As shown in this table, our framework outperforms the previous baselines on both two datasets. In conclusion, the main contribution of our work are summarized as those four points. The first one is our framework with uncertainty guided label denoising, which greatly improves the label quality of DS data. The second one is the instance-level uncertainty estimation method for overlapping relations. The third one is the iterative re-label strategy with dynamic class uncertainty thresholds for the long-tail problem. The last one is the great performance improvements.</sample>
    <sample id="255">The form of the prompting is important in cases with several short promptings, such as zero and one-shot prompting.</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">The speaker, Chiang Cheng-Han, introduces a new work titled "Can Large Language Models Be an Alternative to Human Evaluation?" The study proposes using large language models (LLMs) for evaluating the quality of text in natural language processing tasks. LLMs are instructed with specific instructions and given samples to rate based on attributes such as grammar, coherence, likability, and relevance.
The motivation behind this research is that human evaluations often suffer from instability and reproducibility issues. Therefore, they explore whether LLMs can serve as an alternative method for evaluation by following natural language text instructions provided by humans.

To verify the effectiveness of LLM-based evaluation, experiments were conducted where both GPT-2-generated stories and those written by humans were rated by four different LLMs: T0, InstructGPT (curie and davinci), and ChatGPT. Results showed that English teachers preferred human-written stories over those generated by GPT-2; however, some smaller LLMs did not show significant preference towards human-written texts. Nevertheless, Davinci and ChatGPT demonstrated clear preferences toward human-written content similar to what was observed among English teachers.

The paper addresses various questions related to individual ratings consistency across different instruction wordings or sampling methods used within the experiment. It also discusses potential benefits and costs associated with adopting LLM evaluations compared to traditional human evaluations. Additionally, it explores results obtained when applying these techniques beyond just story generation tasks but does not provide specifics here due to space constraints.

In summary, this presentation highlights innovative approaches leveraging advanced AI technologies like large language models to potentially replace conventional human-centric methodologies in certain aspects of NLP task assessments while acknowledging ongoing challenges inherent in their application.</sample>
    <sample id="259">Cross-lingual semantic parsing is the task of translating queries in multiple natural languages into various meaning representations. Existing models are limited to specific tasks and languages, with Chinese being notably absent from coverage. To address this gap, researchers at Penn State University have developed XSemPLR, a comprehensive dataset for cross-lingual semantic parsing across multiple languages and meanings.
The XSemPLR dataset includes nine datasets covering different domains, five types of semantic parsing tasks, eight distinct meanings, and 22 languages belonging to 15 language families. This extensive collection allows for thorough evaluation under six settings: Translate-Test (using Google Translate API), Monolingual Model, Monolingual Few-shot setting, Multilingual Model, Cross-lingual Zero-shot transfer, and Cross-lingual Few-shot transfer.
The study evaluates two groups of multilingual language models: Encoder-PTR (Multilingual Pretrained Encoders with Pointer-based Decoders) such as XLM-R + PTR and mBERT + PTR, and Encoder-Decoder models like mBART and mT5. Results indicate that Encoder-Decoder models generally outperform Encoder-PTR on all nine datasets when trained using a mixture of languages. However, English performance varies significantly across seven datasets but improves slightly in three others due to what's termed "the Curse of Multilinguality."
The research also highlights the importance of pretraining on dominant languages like English to enhance few-shot learning outcomes in target languages. It concludes by emphasizing the need for more advanced multilingual language models capable of handling diverse linguistic scenarios effectively.</sample>
    <sample id="260">There are four authors involved in the paper: Jingwei Yi, Xun Wang, Yizhou Zhang, and Yuxuan Zhou.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">There are 5 authors involved in the paper.</sample>
    <sample id="263">In-context learning is a popular paradigm for utilizing large language models, but it can be unstable due to various design choices. Prior work shows that search instability of in-context learning arises from biases introduced by different types of designs. However, there hasn't been any systematic discussion on categorizing existing findings and detecting new bias problems within the context of text classification tasks. This paper aims to address these issues by proposing a typology of label biases and identifying domain-label bias as an important source of bias in in-context learning. The authors propose a novel calibration method called domain-context calibration, which uses random in-domain words sampled from the task corpus to estimate model bias on each label name and then calibrates the original predictions using this estimated bias. They conduct experiments with different models and datasets, showing significant improvements in average performance when applying domain-context calibration.</sample>
    <sample id="264">The speaker introduces their paper, "TAVT: Towards Transferable Audio-Visual Text Generation," which addresses the challenge of multimodal domain shifts in audio-visual text generation tasks. They propose a novel task called Transferable Audio-Visual Text Generation and describe its main components: an audio-visual meta-mapper network for aligning visual concepts across domains, a transformer-based encoder and language model generator to learn from limited labeled data, and counterfactual contrastive learning (DCLL) to optimize visual-textual alignment without relying on negative samples. The framework is evaluated using two benchmarks based on MSVD and MSR-VTT with cross-datasets and cross-domain settings. Experimental results show that TAVT outperforms all compared models by a large margin, even maintaining performance on low-resource domains with only a few labeled data. Ablation experiments demonstrate the impact of audio features on expanded performance.</sample>
    <sample id="265">The name of the speaker is Vasudha.</sample>
    <sample id="266">Adam Przepiórkowski is affiliated with the University of Warsaw, Poland.</sample>
    <sample id="268">The most common errors of PaLM are omission errors.</sample>
    <sample id="270">Emory University and Amazon Alexa AI</sample>
    <sample id="271">FTw</sample>
    <sample id="272">There are seven authors involved in the paper.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">The paper presents a comprehensive evaluation of machine translation metrics for Indian languages, focusing on five languages belonging to two different language families: Tamil and Malayalam (Dravidian) and Hindi, Marathi, and Gujarati (Indo-Aryan). The authors use the Flores dataset to generate multiple candidate translations from English sentences using seven different translation models or APIs. They employ bilingual expert annotators to evaluate these outputs by marking errors along with their types and severity, as well as providing an overall score for each translated output.
The study finds that recent MT models like NLLB or Indic Trans have fewer errors compared to older models such as CVIT. Among embedding-based metrics, LabSE embedding shows better correlations than other counterparts, while MuRIL has higher correlations across all languages. COMET-metric variants exhibit the highest overall correlations for all languages but show skewed ranges in scores. Fine-tuning COMET using the MQM dataset results in improved performance, particularly when fine-tuned only on four languages and tested on unseen ones.</sample>
    <sample id="277">It does not have a name.</sample>
    <sample id="278">The "marked words" method draws upon the sociolinguistic concept of markedness, which states that there is an unmarked default and any group that differs from it is linguistically marked. The author described this as a way to identify specific stereotypes in language models by comparing personas using weighted log-odds ratios to distinguish the top words for each marked group against corresponding unmarked groups like white or male personas.</sample>
    <sample id="279">University of Washington, Microsoft Research</sample>
    <sample id="280">The goal of emotion regulation in conversations is to predict the emotion label of each utterance in a dialogue. Each utterance has its corresponding textual, audio, and visual modalities. To solve the problem of emotion recognition in conversations (ERC), many methods have been proposed. Most existing ERC methods focus on modeling speaker and contextual information, while there still exist several unsolved challenges: First, the complementarity of multimodal information has not been well exploited. Second, current state-of-the-art methods have unsatisfactory performances in minority emotion classes. And finally, distinguishing between semantically similar emotions. In this paper, we propose MultiEMO, an attention-based correlation-aware multimodal fusion framework for ERC. The overall framework consists of four key components: unimodel feature extraction, context modeling, multimodal fusion, and emotion classification. We introduce VisExtNet as our novel visual feature extractor, which captures facial expressions without encoding redundant scene-related information. Our second contribution is MultiAttn, a novel multimodal fusion network that integrates one modality with complementary information from other two modalities through stacked bidirectional multi-head cross-attention layers. Finally, we employ Sample-Weighted Focal Contrastive Loss to address difficulties classifying minority and semantically similar emotion classes. Experimental results demonstrate that MultiEMO achieves state-of-the-art performance on MELD and IEMOCAP datasets.</sample>
    <sample id="281">A lot of translations depend on context. For example, how would we translate "mole" in this sentence? Well, if the previous sentence was "Things could start to get dangerous if the ministers find out", then "mole" refers to a spy. But if the previous sentence was "Could it be anything serious, doctor?", then "mole" refers to a birthmark. So, depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation.</sample>
    <sample id="282">The paper presents a new approach to non-parallel story author-style transfer, called StoryTrans. It addresses the challenge of transferring style-specific content from one text to another while preserving the original meaning and maintaining coherence in discourse structures. The proposed model learns discourse representations from source texts and combines them with learnable style embeddings to generate target-style stories. A novel training objective is designed to reduce stylistic features from discourse representations and enhance content preservation by separating generation into two stages: first, transferring the source text with style-specific keywords masked, followed by generating the complete text using these keywords explicitly. Experimental results demonstrate that StoryTrans outperforms strong baselines on Chinese and English datasets for fairytales and everyday stories transferred to typical author styles, showing superior style control and content preservation capabilities.</sample>
    <sample id="283">The name of the first mentioned symmetrical dependency structure is "the conjunction headed approach assumed in Prague dependency treebanks".</sample>
    <sample id="284">The current span-based UIE models involves identifying and labeling the span boundaries of the targets in the text, which overrelies on boundary positions of the annotated span. However, there is ambiguity in labeling the golden span boundary. That is, different annotation spans can be considered reasonable. So we proposed that the span boundary learned by the module should be fuzzy instead of precise. Besides, there is a mismatch between transformer feature extraction and information extraction. Basic Transformers focus on global features, which ignores the prior hypothesis that span has limited length. So we proposed that the attention used for span extraction decision should be adaptive rather than static in order to model the furthest span boundary, which represents the target boundary as a continuous distribution of correct probability in a specific range, where R-min and R-max represent the start and end of the fuzzy boundary. And the function Q represents the correctness of the current position. Through the sampling function shared in the slide, we convert the continuous boundary distribution into a group of discrete values for calculation of fuzzy span loss. The boundary distribution predicted by the module will calculate Binary Cross Entropy with golden boundary as BCE loss and adding KL-divergence between predicted boundary with fuzzy span boundary and supplementary information. To get the model in obtaining a more reasonable attention distribution for span extraction, we proposed a fuzzy span attention as a mask function to trim attention distribution.</sample>
    <sample id="285">Summaries generated by models and even some reference summaries still contain factual errors, and there are two main types of solutions. The first is to introduce factuality-related objectives in the training or inference process to make the summarization models more faithful and expect it will generate more factually correct summaries. The second is the design of a Factual Error Correction model (abbreviated as FEC), which is independent of the summarization model. It takes the source document and model-generated summary as input and outputs a corrected summary. To the best of our knowledge, there has been no work on factual errors for dialogue summarization. Considering the importance of factual issues in dialogue summarization, we would like to try to correct factual errors in dialogue summaries. However, after carefully examining and considering the motivations and practices of previous FEC studies, we argue that there are flaws in the way FEC models are evaluated, which may have diverted the FEC for summarization from its original purpose. Let's see how the current FEC models have been evaluated. Factuality metrics such as FactCC and DAE are used. It takes a source document and a summary as input and outputs a score. It is expected that the average scores of the corrected summaries are higher than the original. The higher the score, the better the FEC model. There are two flaws. First, factuality metrics give an overall score, which is so vague, and factuality metrics may not be reliable on their own. Second, this evaluation blurs the line between the two types of solutions. The FEC model can ignore the content of the original summary and directly generate a different but more factually correct summary. There may be no error correction at all. We argue that it is necessary to introduce manually annotated reference corrections to address both issues. Factual error correction for summarization has these basic requirements: to correct factual errors in the original summary by as few substitution, insertion, and deletion operations as possible to obtain a fluent and non-redundant summary. This can be reflected in the manual annotation. The introduction of reference corrections, on the other hand, provides more valuable data for the training of FEC models compared with pseudo data. On the other hand, and more importantly, it creates the condition for a more comprehensive and accurate evaluation of the performance of FEC models. To automatically classify factual errors for factual error correction, we propose a new taxonomy of factual errors. We point out that there are two classifications of factual errors of different perspectives: content-based and form-based. Content-based categories are assigned according to the part of speech and dependencies, and form-based categories are assigned according to whether it is an addition, deletion, and substitution operation. We build our evaluation framework on the basis of ERRANT, an evaluation metric for grammar error correction. It mainly consists of three steps: alignment, classification, and comparison. We experiment with some FEC models in different training modes to explore some factors of interest. With our proposed evaluation framework, we have the following key findings. Training FEC models with reference summaries from dialogue summarization datasets yields the best results of unreliable factuality metrics. There is an urgent need to change the evaluation methods for FEC models. Introducing human-corrected summaries during the training of FEC models for dialogue summarization can improve their performance. Combining human-annotated data with synthetic data is a promising direction. And current FEC models struggle to correct factual errors like addition and cannot address attribute errors, modality errors, link errors, etc. Thanks for listening.</sample>
    <sample id="286">James Finch</sample>
    <sample id="287">4</sample>
    <sample id="288">BLiMP, SyntaxGym</sample>
    <sample id="290">The abbreviations of the five methods for the first research question are: COSINE, WSL, FTL, FTw, and FTW.</sample>
    <sample id="291">Named entity recognition, classification, part-of-speech tagging and question answering.</sample>
    <sample id="294">CamemBERT is initially trained on the Wikipedia corpus.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Natural Language Understanding — and natural language processing in general — is based in large part on supervised machine learning, or the so-called data-driven approaches. And in order to be able to develop these approaches, we need a lot of data, large sets of manually annotated data which encodes some kind of human knowledge, which the annotators put into the data by their process of annotation. Typically, there is a series of works which have been showing that the assumption that there is one single truth — that's called the ground truth, and the annotation is to converge towards it — is showing some limits. We chose to focus on irony in particular, which is a highly latent and pragmatic phenomenon in natural language. Irony detection is already a very difficult task for modern natural language processing models, but we want to investigate this problem further. Rather than training models that are able to say something like a binary label — like this sentence is ironic or is not ironic — we would like to have models that have a more informative output. So, in order to study these problems, we developed a corpus called EPIC, which stands for English Perspectivist Irony Corpus. We collected data from different sources — from social media, Reddit, and Twitter — and the data spans a long time window of 1½ years, and we collected about 300 short conversations made of pairs of text, one following the other. And we repeated this for both sources and for five varieties of English, which are listed here. We use the crowdsourcing platform Prolific to have people annotate this data. We selected about 15 annotators for each of the English language varieties, for a total of 74 annotators in total. We gave each of them 200 texts, or 200 short conversations. And we also put extra questions to use as an attention check for quality control. So on average, we got 5 annotations for each short conversation. This is how the annotation interface looks like. It's very simple. It resembles a chat or a text interface. They see a text, a message, and its reply, and then there's just the question there below, which asks "Is the reply ironic?" with respect to the context, and the annotator can choose "Ironic" or "Not ironic". We observed some differences between several groups along different dimensions, depending on how we divide the datasets. But whether we divide the annotators by gender, by age group, nationality, and so on, we can notice that the distribution of the inter-annotator agreement that is depicted in this violin plot is always kind of different in every case. We try to model these differences and build different models, which we call perspective-aware models, basically training different models by fine-tuning a pre-trained language model on splits of the datasets, where the splits are in terms of different annotators While in terms of raw performance, we didn't notice particular trends here (of course, the results are different, but there is no upward or downward trends), by using perspective-aware models versus gold standard aggregated models, we notice a significant difference in the confidence that the perspective-aware models show. And in particular, we can see how the perspectivist models are, on average, less uncertain, more confident of their predictions. Finally, we went again into the data and we try to look at what may be the causes of the differences in the annotations, and we found something peculiar: that is, that in the case of age, it is generations that are close to each other that seem to be more in disagreement toward their perception of irony. And similar things happen in the geographical distribution of the annotators, where we found that the highest variations in response is given between the two models trained on labels given by annotators from the United Kingdom and Ireland. This is it for this short presentation, but we will be happy to answer all your questions and have further discussions at the poster session.</sample>
    <sample id="297">The study "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" investigates the use of dogwhistles in political rhetoric, particularly focusing on their role in evading content moderation and enabling hateful speech. The researchers developed a glossary containing over 340 terms related to racist, transphobic, and anti-Semitic dogwhistles from various sources such as academic texts, Wikipedia, blogs, etc. They characterized these dogwhistles by register (informal or formal), persona (anti-Semitic, transphobic, etc.), and type (adding an implicature vs covertly signaling a persona). A case study was conducted using historical U.S. political speeches, showing that the frequency of racial dogwhistles correlates closely with known periods like the Republican Southern Strategy post-Civil Rights era. Additionally, experiments were performed evaluating language models' ability to recognize and interpret dogwhistles, revealing varying degrees of success depending on the model's configuration and prompt strategies. Finally, they examined how dogwhistles can evade automated toxicity detection systems through Prospective API tests, demonstrating that replacing slurs with dogwhistles reduces perceived toxicity scores. This research highlights the complexity and context-dependent nature of understanding and detecting dogwhistles within linguistic data.</sample>
    <sample id="298">The experiment to retrain or continue pre-training some models with more recent data showed that the performance degrades as there is a larger temporal gap between training and test data.</sample>
    <sample id="299">NLI models have achieved state-of-the-art results across a block of benchmarks. However, despite rapid progress, recent work has demonstrated that the success of NLI models is partly due to learning and using shortcuts. Shortcuts are spurious correlations between the input attributes and labels introduced during the dataset creation process. For example, high word-overlap between the premise and the hypothesis in the MNLI dataset is strongly correlated with the entailment label. Consequently, NLI models that exploit shortcuts perform well on in-distribution samples, but are brittle when tested on out-of-distribution adversarial test sets where such spurious correlations do not hold. Prior work in shortcut mitigation typically assumes access to an auxiliary model designed to rely on shortcuts for predictions. For instance, the auxiliary model can learn to exploit shortcuts by being trained only on a small number of examples or by leveraging an auxiliary with reduced learning capabilities. The output of the auxiliary is then used to re-weight training instances for the learner model. Existing shortcut mitigation methods may require knowing this in advance. This assumes domain- and dataset-specific knowledge, which is not always available and thus limits the potential of shortcut mitigation. Furthermore, current shortcut mitigation methods often assume that the learner will naturally exploit the same types of shortcuts as the auxiliary. In practice, the behavior of the learner diverges from that of the auxiliary. For example, the auxiliary may down-weight instances that are useful for training the learner or provide inaccurate uncertainty estimations that may hinder the learner's out-of-distribution generalization capabilities. Finally, current shortcut mitigation methods require using a pre-trained language model as the auxiliary, which incurs additional computational overhead. Motivated by these limitations, in this work, we propose a training method to reduce the reliance of NLI models on shortcuts and improve their out-of-distribution performance.</sample>
    <sample id="300">The work presented by Belinda introduces a task called interactive dictation and makes initial steps towards solving this task. Interactive dictation is characterized by the flexible interleaving of dictation and editing, not separated by a trigger word, using intuitive and open-ended natural language utterances to specify edits. The contribution includes introducing and formalizing the new task, designing a data collection interface, building a dataset for this task, creating a baseline system for this task, collecting the dataset through an annotation interface, training separate models for each step in the process, evaluating these models jointly with exact match of predicted end-state against goal end-state, and releasing code at a specified site.</sample>
    <sample id="302">Because the input and output sequences may not have a fixed order, so permuting the tokens allows for flexibility in generating the correct sequence.</sample>
    <sample id="303">Because there is a need to understand if the positive stereotypes are due to weird overly-excessive value alignment or other anti-stereotyping methods resulting in these patterns, and without more transparency it's hard to study this further.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences that have an unacceptable structure or violate syntactic rules. These inputs can be used in the minimal-pair paradigm to evaluate language models' acceptability judgments by comparing them with acceptable counterparts from the same data set. The goal is to assess how well the model distinguishes between grammatical and ungrammatical structures, especially when considering longer context windows.

The minimal-pair paradigm typically involves presenting a model with both an acceptable sentence (or input) and an unacceptable one within the same linguistic domain. By analyzing the model's responses to these pairs, researchers aim to understand its ability to recognize and differentiate various grammatical patterns and structures across different contexts.

In this study, the authors revisit the MPP pipeline by incorporating longer sequences of text into their evaluation process. They do so by creating prefixes for both acceptable and unacceptable sentences using grammar rules extracted from relevant datasets like BLiMP or SyntaxGym. This approach allows them to examine whether the model maintains consistent acceptability judgments even as it processes more extensive contextual information throughout the sequence length.

By examining the impact of mismatched prefixes on the model's performance, they uncover significant changes in judgment outcomes based on the prefix type—acceptable versus unacceptable. Moreover, they observe robustness against unrelated Wikipedia sentences but find substantial sensitivity towards perturbations preserving structural integrity while introducing noise elsewhere. 

This suggests that current methods might not fully capture all aspects of abstract knowledge held by advanced language models due to potential oversimplification inherent in short, isolated evaluations such as those employed in traditional minimal-pair paradigms.</sample>
    <sample id="305">The PhD student Dawei presented the recent work "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. The presentation began with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weaker annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well.</sample>
    <sample id="306">To evaluate entity state tracking abilities in language models, the authors designed a task involving boxes and objects. The input to the model starts with an initial description of each box's contents, followed by multiple state-changing operations like moving or adding objects. The goal is for the model to predict the correct content of each box based on this information. To prevent heuristics from influencing results, various measures were implemented during training. Experiments showed that only GPT-3.5 models exhibited non-trivial tracking behavior when tested against Flan-T5 and other models using 2-shot in-context learning. This suggests pre-training on code may be responsible for enabling these capacities in pre-trained language models. Smaller models can learn to perform entity tracking through direct fine-tuning but not without prior training. Further analysis reveals challenges in generalizing beyond their setup.</sample>
    <sample id="307">Named entity recognition, classification, part-of-speech tagging and question answering</sample>
    <sample id="308">The presentation discusses the concept of positionality in NLP datasets and models, highlighting how they can reflect certain positionalities over others due to the perspectives held by their developers. The researchers conducted a study using Lab in the Wild and online crowdsourcing platforms to compare annotations from diverse annotators with existing datasets and models such as Social Chemistry, Delphi, GPT 4, Dynahate, Perspective API, Rewire API, Hate Roberta, and more. They found that these datasets and models are most aligned with English-speaking countries and individuals with higher education levels, but less so with non-binary people. To address this issue, the researchers recommend keeping records of design choices throughout research processes, conducting NLP research with perspectivism, and building specialized datasets and models within specific communities like Masakhani initiative.</sample>
    <sample id="309">ABC-Eval labels</sample>
    <sample id="310">Wikipedia</sample>
    <sample id="311">Technische Universität Darmstadt, Technische Universität Berlin</sample>
    <sample id="312">MultiInstruct is the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source datasets and each task is equipped with five expert-written instructions, making it unique in its scope and structure compared to other benchmarks.</sample>
    <sample id="313">There are two authors involved in the paper.</sample>
    <sample id="314">Binary coordination is a linguistic concept where two elements are combined to form a single unit. In the context of this talk, it refers to the structure "Lisa and Bart" or "Lisa, Bart, and Maggie," which consists of two or more items joined together in a coordinated manner. The dependency structures assumed by different theories and corpus approaches vary for binary coordination, with some assuming that one element heads the coordinate structure while others consider all elements as heads.</sample>
    <sample id="315">The prompts used in this study were 20 words long on average.</sample>
    <sample id="316">The findings suggest that smaller models like T5 can outperform larger language models when trained on suitable datasets.</sample>
    <sample id="317">Information extraction is a classic task in natural language processing. It refers to extracting structured information from unstructured text. Common information extraction tasks include named entity recognition and relation extraction (RE), and so on. Here is an example of named entity recognition. For the input task, "Steve became CEO of Apple in 1998." The model needs to recognize that Steve is a person's name and Apple is an organization's name. For previous information extraction models, we see using pre-trained language models like T5 and GPT-3. They operate in a text-to-text manner during the pre-training stage. However, during the inference phase, the structured output of information extraction is linearized into a plan sequence. The problem with these approaches is that while it learns the inputs reformatted between the inference and pre-training stages, the outputs are not learned. One output is plain text while the other is a structured output, making it challenging for the model to generate the correct structures. This often requires a large amount of structured training data and special decoding strategies to mitigate this issue. To address the problem of mismatched outputs, we propose CodeIE, transforming the text-to-structured information extraction task into a structure-to-structure code generation task and use code large language models like Codex to perform it. This way, we can easily convert text to a structured format during the input stage and ensure aligned structures in the output stage. Specifically, for the Named Entity Recognition Task, we have designed the following prompt. First, we define a function for named entity recognition that takes an input text as input. We add a comment and extract named entities from the input text to this function. Then, we set the input task to the actual input text and initialize an entity list called Entity List. We provide a comment saying extracted named entities to trigger the subsequent content. With few-shot in-context demonstrations, we can expect the model to output the following code, continuously extract the text and entity pairs, and append them to the entity list. For relation extraction, we have designed similar prompts. We evaluated our method on three recognition datasets and four relation extraction datasets. Our evaluated models include T5 model, the UIE model, the text-davinci-002 version of GPT-3 model, and code-davinci-002 version of Codex model. We compared the performance of two types of prompts, one using a traditional text-style prompt and the other using the code-style prompt described before. In the case of one to few-shot, we found that our proposed approach using code language models and code format prompts significantly and consistently outperformed the traditional baseline models, such as UIE and natural language large language models, like the GPT-3 model.</sample>
    <sample id="319">The work investigates two learning strategies: from-scratch pre-training and continual pre-training. The authors compare the performance of models trained using these different approaches on various downstream tasks in French, including biomedical and clinical domains. They also explore how varying amounts of data impact model performance under each strategy.</sample>
    <sample id="320">The factor of overfitting due to test reuse is greater than one.</sample>
    <sample id="321">The quality of the simplification was evaluated by using DEPLAIN as a gold standard alignment to evaluate some proposed alignment methods.</sample>
    <sample id="322">Enrico will be presenting at ACL 23 answering the question, "What does a Text Classifier Learn about Morality?" First off, let me explain to you what is morality. Human morality is what helps us distinguish right from wrong; it's our internal compass that helps us determine whether an action or a concept is morally right or morally wrong. And morality is at the base of our societies, obviously, and it is essential that language models can understand and recognize morality in our language in text. The understanding of morality in text has already been approached in the NLP community, but it is often and usually being treated as a singular scale between immoral and moral, where a concept, a sentence can be labeled somewhere between immoral and moral for the judgment of morality. However, we know that morality is very subjective. Different people would label the same concept differently in the scale. Take, for example, divisive concepts such as abortion or LGBTQ rights. Some of us would label, for example, abortion to be immoral and some would label it to be moral. Simply taking an average between these or taking a majority aggregation would actually hide the real truth, that it's a pluralist, different way of interpreting morality. And teaching language models just the average is very dangerous. To this extent, there are social theories that we can apply to the understanding of human morality. In particular, a very popular and established theory is the so-called Moral Foundation Theory. According to this theory, there are five different ways in which we humans perceive morality, as much as there's five different taste buds in our tongue, and each action, each concept, tickles a different moral aspect, a different moral foundation. And each of us, each human, prioritizes these foundations in different ways. For example, for some of us, fairness is very important, and for others, authority is much more important. The different ways in which we prioritize these foundations, determine the way in which we judge the morality of a concept or of an action. The Moral Foundation Theory has already been used in natural language processing. There's been a surge of papers in the last couple of years, including papers from ourselves, that attempt to understand and to classify morality in text. And it's been shown that language models can somewhat understand morality in text. So what we aim to do in this paper is to try to understand what they learn. We apply explainable AI techniques to these language models trained to understand morality in text. In particular, we focus on understanding how morality is expressed differently across different domains. We use a data set called Moral Foundation Twitter Corpus, composed of 35,000 tweets collected in seven different domains. Domains, for example, corresponding to the hashtag #AllLives Matter or the hashtag #BlackLivesMatter, and we try to see whether language models can understand that morality is expressed differently in different domains. We all know, of course, intuitively that the way in which morality is expressed, say in ALM or in BLM, All Lives Matter and Black Lives Matter, is very different. But can language models understand these fine-grained differences? We have proposed a method, we have made a number of experiments to answer these questions, so I'm going to just give you one small sneak preview of that. Take the example of the difference indeed between ALM and BLM, All Lives Matter and Black Lives Matter. These two domains have a similar rhetoric because they cover similar topics, but they have a significantly different rhetoric for the moral element of subversion, which means rebellion to authority. We know intuitively, and we found that language models recognize that in ALM's version is associated with words such as overthrow, mayhem, and subversion is frowned upon, whereas in BLM, subversion is somewhat encouraged. So, language models do recognize that morality can be expressed differently. Of course, there's different levels of understanding that we explore in the paper, and these essentially warn us of the fact that, of course, morality is expressed differently in different domains, and using just one single model for many different domains can actually lead to misunderstandings of morality in a very dangerous way.</sample>
    <sample id="323">Commonsense QA is a challenging task that requires machines to answer questions that rely on common knowledge to test their language understanding abilities. This requires machines to retrieve relevant knowledge from external sources. Recently, Holmes thinks that knowledge is stored in both language models and knowledge bases. Many works combine these two types of knowledge to solve Commonsense QA with good readout. These works retrieve relevant knowledge from the knowledge base through entity matching, building a subgraph, and then using the language models and GNNs to infer answers. However, they introduce some noisy entities during subgraph retrieval, such as "Top", "Bank", and "Cat" here in the subgraph, which are largely unattached to the current question. Moreover, they encode the subgraph and text in isolation, leading to limited interaction between the two modalities. And the encoding process ignores the semantic relationship between entities. Based on the above problems, we propose DHLK. First, we build an HKG based on multiple knowledge bases run through a two-stage pruning strategy and KRL to optimize the structure and knowledge representation of the HKG. Finally, we use the language model to encode and fuse the two modalities.</sample>
    <sample id="324">Yes, language models have different political biases. They occupy all four quadrants on the political campus and their ideological coordinates shift when further pre-trained on partisan corpora separated into news and social media based on their political leaning.</sample>
    <sample id="326">Cognitive dissonance is when two beliefs or actions are inconsistent, such as a person stating they know cigarettes could kill them but then continuing to smoke.</sample>
    <sample id="327">The goal of Vision-Language learning is to train a smart AI system that can understand both image and text. Visual Question Answering is one of the most famous VL tasks, which needs to answer a question based on the input image. Since 2019, with the help of large-scale self-supervised pre-training on image-text pairs, transformer-based vision language models have achieved remarkable progress. From the perspective of model architecture, recent vision-language works can be unified as a two-tower architecture which consists of a textual encoder, a visual encoder, and a cross-modal encoder. If we step into the unimodal encoders of the two-tower architecture, such as METER, we can find that they feed only the last-layer unimodal representation directly into the top cross-modal encoder, ignoring the semantic knowledge at the different layers of the deep unimodal encoders. Different from the two-tower two-architecture, BridgeTower connects multiple top unimodal layers with each cross-modal layer in a layer-by-layer fashion to exploit unimodal's magnetic knowledge at different levels. However, BridgeTower still suffers from two obvious limitations. First, its layer-by-layer utilization of different unimodal layer representations is ineffective. Each cross-modal layer can only utilize an artificially assigned unimodal layer representation, thus restricting the exploitation of different levels of unimodal semantic knowledge. Second, the number of cross-modal layouts is tied to the number of unimodal layer representations it used, thus limiting its scalability and capability. In this work, we build upon BridgeTower and advance it in two aspects. We propose ManagerTower, a novel VL modal architecture that each manager takes multiple unimodal representations as the insights of pre-trained unimodal experts at different levels. ManagerTower adaptively aggregates insights with managers in each cross-modal layer. Here is the detailed architecture of our ManagerTower.</sample>
    <sample id="328">GPT-4 is the most liberal language model.</sample>
    <sample id="329">Video sentence localization aims to find the most relevant segments with a given natural language query for long videos, and has broad applications in video retrieval, summarization, and other fields. This task takes a video and natural language query as input, and requires the model to output the start and end times of the video segment that are most relevant to the given query. However, many of these methods require a large number of manual annotations for training, which is costly and inefficient. Existing zero-shot methods mainly use the following process. First, generate pseudo-events based on the video, then generate pseudo-queries based on the pseudo-events. Finally, train a video sentence localization model using these pseudo labels. They have three main drawbacks. Firstly, the pseudo-queries are usually too simple. For example, some methods combine detected nouns and verbs in the events to generate the queries, which has a large gap between the real queries. Secondly, as shown in the figure, their approach can only show a high relevance between videos within the event and query, but can not guarantee that the query and video outside the events is irrelevant, which lead to unalignment between the pseudo-queries and pseudo-events. Finally, they directly use these pseudo labels to train the model, ignoring the risk of label noise. Thus, we propose the noise-resistant Structured Pseudo-Label generation method. As shown in the figure, we first use a pre-trained image caption model to generate more complex free-form pseudo-queries. Then we use a pre-trained model to measure the relevance between individual frames and pseudo-queries, and generate pseudo-events which guarantee high relevance between the video inside the event and the query, and low relevance between the video outside the event and the query. Finally, we reduce the weight of noisy samples and create noisy labels to reduce the influence of label noise.</sample>
    <sample id="330">Yes, cumulative training performs better than iterative when doing active learning.</sample>
    <sample id="331">The name of the speaker is Sara Papi.</sample>
    <sample id="332">TED talks</sample>
    <sample id="333">Neural machine translation (NMT) models are often limited by their non-smooth representation space, which can lead to poor performance in certain areas. To address this issue, kNN-MT was proposed as a solution that smooths predictions according to nearest neighbors in the representation space. However, two significant drawbacks of kNN-MT include time-consuming neighbor retrieval at each decoding step and difficulty updating representations once the datastore is constructed.
To overcome these limitations, researchers from Nanjing University propose INK: Injecting kNN Knowledge into Nearest Neighbor Machine Translation. The framework consists of an adapter for adjusting the NMT model's representation space based on kNN knowledge. This training loop runs until convergence, allowing updated representations to refresh the datastore asynchronously without requiring neighbor retrieval or manual updates. By aligning contextualized representations with token embeddings, kNN token embeddings, and target tokens themselves, the INK system achieves better generalization and performance while reducing memory usage compared to other methods.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Cross-lingual transfer is the process of transferring knowledge or skills from one language to another. In the context of machine learning, it refers to using a model trained on data in one language and applying it to predict outcomes for other languages with similar structures but different vocabularies. This technique can be particularly useful when there's limited training data available for specific target languages.

In natural language processing (NLP), cross-lingual transfer often involves adapting models that have been pre-trained on large amounts of text in one language to perform tasks like translation, semantic parsing, or information extraction in another related language without needing extensive additional training data specifically for the new language. The idea behind this approach is that if two languages are closely related syntactically and semantically, their representations should capture enough shared structure to enable effective performance even outside direct supervision by examples in the target language itself.

The effectiveness of such transfers depends heavily on factors including how much overlap exists between the source and target languages' vocabularies and grammatical structures, as well as whether certain aspects unique to each language significantly impact understanding or generation capabilities. For instance, cultural nuances might not translate directly across cultures due to differences in social norms or historical contexts which could affect word meanings differently depending upon usage scenarios within various societies.

Moreover, advancements in neural networks and deep learning techniques allow us increasingly powerful tools for handling these complexities through methods like multi-task learning where multiple objectives (e.g., translation vs. sentiment analysis) share common layers before diverging into task-specific ones later during training phases. These approaches leverage shared patterns learned over diverse linguistic inputs thereby enhancing robustness against variations inherent among different languages while still maintaining specificity tailored towards individual linguistic features once again at higher abstraction levels post-training stages.</sample>
    <sample id="337">Out-of-vocabulary (OOV) words are a significant challenge in embedding-based models, as they cannot be represented accurately. To address this issue, researchers have developed an approach that leverages word formation and association to infer the meaning of OOV words. The proposed method involves creating a Word Relationship Graph that mimics lexical rules for word formation and association. When encountering an OOV word, it is tokenized into wordpieces and associated with relevant words, forming a two-level graph around the OOV word.
Each node in the group represents either a word or a wordpiece, with its corresponding word embedding serving as the node attribute. All nodes from the first layer are preserved to retain complete information about wordpieces. In the second layer, a fixed number of nodes are sampled for training purposes to mitigate noise introduced by numerous neighbors. A self-attention network assigns attributes based on the characters of the OOV words before employing a graph neural network to process the relationship graph.
To overcome challenges related to assigning node attributes to OOV nodes, a contrastive learning loss function using NT-XENT positive samples from the graph is applied. This helps extract important information while reducing the impact of noisy neighbor nodes. Experimental results demonstrate superior performance compared to baselines across both intrinsic and extrinsic tasks, proving the effectiveness of learning OOV words through word formation.</sample>
    <sample id="338">The researchers from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research presented their work titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations" on behalf of their research group. The presentation focused on the contributions to a unified data structure for preliminary experiments analyzing factors contributing to explanation utility and the proposal of a metric with an evaluation of five datasets with two models. Their goal was to address the vital question, "How do we evaluate the quality of a human annotated explanation?" Unlike labels, such explanations can be subjective and task-dependent. Traditional metrics such as BLEU and ROUGE treat human annotations as the gold standard and focus on word similarity. However, these methods neglect the difference in usefulness during fine-tuning and inference stages. In response, the researchers introduced a novel evaluation metric called TREU (Task-Related Explanation Utility), which extends the simulatability score by evaluating the helpfulness of explanations at both fine-tuning and inference stages. They evaluated human explanations across five popular large-scale datasets using this new metric along with the simulatability score on two models, T5 and BART. Analyzing the scores for CoS-E, they found that human-annotated explanations still benefit model predictions even if considered low quality by humans in previous literature. This suggests that their proposed metric outperforms the simulatability score for faithful explanation evaluations.</sample>
    <sample id="339">Saarland University in Germany</sample>
    <sample id="340">Paraphrase generation is a long-standing and important task in the NLP domain. It benefits many other NLP applications such as question answering, chatbots, and improving robustness. To train a good paraphrase generator, we usually need a large scale of high-quality paraphrase data. However, for the existing human-annotated datasets like MRPC, PAN, and Quora they have high quality, but they are limited in scale. There are some automatically generated datasets, like back-translation — translating a sentence to another language and translating back. They can automatically generate a large scale of paraphrase datasets, but they are lack of syntactic diversity. Here is one example. We can see that the generative paraphrase has almost the same syntax to the original sentence. In this work, our goal is trying to construct a large-scale, syntactically diverse paraphrase dataset. Our key idea is to leverage AMR graphs. AMR (Abstract Meaning Representations) is a directed graph that captures the abstract meaning of a sentence. Each node represents a semantic concept in a sentence, and each edge represents a semantic relation between concepts. And we have focus, which is the root node, to represent the main assertion of the sentence. We therefore propose to use AMR back-translation to generate syntactic diverse paraphrases. First, we will use a pre-trained AMR parser to get AMR Graph of a source sentence. Then we will change the focus of the graph. We will randomly sample a node and set it as a new root node, then modify the corresponding edge and their edge labels. Then we will use the AMR graph-to-text generator to generate text from the modified graphs. For the generated text, because they share the same AMR graph structure, so they will have similar semantics. Also, because the text generator would emphasize focus at the beginning of a sentence, their syntax would be a little bit different. By using AMR back-translation, we can get our proposed dataset, ParaAMR. There are around 15 million source sentences in ParaAMR, and there are around 6.9 paraphrases per source sentence.</sample>
    <sample id="341">The authors use BLEU and average lagging as latency measures.</sample>
    <sample id="342">The paper "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming" presents a new dataset, LiveChat, which is constructed using an automatic dialogue-constructing method. The authors propose this large-scale personalized dialogue dataset to address the challenges of existing dialogue datasets such as limited scale and lack of session dialogues for each persona in open-domain dialogue scenarios. To construct LiveChat, they extract audio from Chinese TikTok videos, transcribe it into utterances through ASR, collect audience comments, and use their reply-to-whom matching method to build dialogues. They also perform persona extraction by manual labeling, scratching, rules, and trained persona classifiers. In experiments on two benchmark tasks—Response Modeling and Addressee Recognition—they demonstrate that selected persona profiles and longer average sessions per persona are beneficial for learning speaker's responses. Comparisons with other LLMs show distinctiveness in LiveChat. Future work will focus on efficient transfer learning of language models for LiveChat.</sample>
    <sample id="344">The drawbacks of tree-based methods include the need to obtain trees, which can be complicated and computationally expensive. This often involves formalism-specific pre-processing for logical forms and specialized grammar-induction procedures.</sample>
    <sample id="345">Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances. In this case, "The girl slept." And "Mary knew that the girl slept." These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion. Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar-induction procedures. In this paper, we don't use trees and introduce a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step, we have all the right tokens, but they're not ordered. That's why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one, as highlighted in red. Then we jump to the next multiset token, to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging, though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token we don't know which multiset it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest-scoring permutation is NP-hard. That's because this is related to the "Traveling Salesman" problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.</sample>
    <sample id="346">Shanghai Jiao Tong University, Alibaba DAMO Academy</sample>
    <sample id="348">Recent studies have highlighted the prevalence of social bias and stereotypes in large language models (LLMs). However, these measures often rely on hand-constructed datasets that are time-consuming to curate and only capture specific stereotypes. Moreover, most work does not account for intersectionality, which is crucial as it considers how multiple facets of identity can compound biases. To address these limitations, researchers propose a method called "Marked Personas" based on LLMs' ability to respond to prompts like "Imagine you are an Asian woman. Describe yourself." This approach allows generalization across demographics by specifying any identity marker into the prompt.
The study generates personas using this method and compares them with human-written responses through Marked Words analysis. The findings reveal that while generated personas contain more stereotypes than those written by humans, they also highlight essentializing narratives reflected in seemingly positive portrayals. These patterns suggest harmful implications such as pressure on marginalized groups to overcome societal obstacles rather than addressing root causes.
The authors conclude their research by recommending increased transparency about bias mitigation methods used in model development and emphasize the need for intersectional approaches when studying biases and harms within AI systems.</sample>
    <sample id="350">The paper "What’s the Meaning of Superhuman Performance in Today’s NLU?" investigates how reliably leaderboard scores compare models and humans. The authors analyze two popular benchmarks, SuperGLUE and SQuAD, which evaluate systems on tasks such as commonsense reasoning, entailment, reading comprehension, etc. They find that while some benchmark creators provide a human baseline score, others do not. Additionally, they discover errors where both datasets are evaluated with different subsets or contain spurious correlations between training and test instances. Furthermore, researchers often vaguely estimate human performance by using simple aggregation methods like average or majority voting without considering whether it is comparable to general human intelligence. Finally, details about the annotator pool are frequently omitted, making claims about superhuman performance ungrounded.</sample>
    <sample id="351">The paper investigates the problem of generalization using the Named Entity Recognition Task (NER) and examines whether models developed in 2003 for CoNLL-2003 can generalize to modern data. The authors created a dataset, CoNLL++, by collecting news from Reuters in 2020 and annotating it with guidelines similar to those used in CoNLL-2003. They fine-tuned over 20 models on CoNLL-2003 and evaluated them on both the original test set and their new dataset.
The results showed that three main factors contribute to good generalization: model architecture, model size, and number of fine-tuning examples. Specifically, transformer-based architectures were found to perform better than other types; larger models generally led to improved performance; and more training examples resulted in enhanced downstream task performance.
The study also explored potential causes behind observed performance drops among some models. Two hypotheses were considered - adaptive overfitting due to repeated use of the same test sets during development or temporal drift caused by increasing time gaps between train and test datasets. Experiments revealed no evidence supporting adaptive overfitting but confirmed temporal drift as the primary cause of performance degradation when retraining models with newer data sources.
In conclusion, while older NER taggers like those based on CoNLL-2003 may still work well today, improvements are needed regarding model design, scale, and training resources if they're to maintain effectiveness against contemporary challenges posed by evolving language patterns and increased complexity within text corpora.</sample>
    <sample id="352">Annotating Behaviors in Chat</sample>
    <sample id="353">Code generation and program synthesis (given natural language description) is a heated research topic. However, state-of-the-art methods failed to address an important challenge, which is input underspecification. In the motivating example, only the classifier—which is the regressor—is mentioned with multiple specifications missing. The issue is prevalent in real-world use cases, and it's important to train code generation models. Interaction, for example, asking clarification questions, is a good paradigm to address this challenge. However, before addressing this challenge, we have to figure out two challenges. First, missing specifications can happen at various levels, and second, it's not obvious how to identify if an NLD carries information about specifications at any level mentioned above or not. Therefore, in this paper, we introduce interactivity into code generation. We hypothesize that through interaction, we can gather more specifications to alleviate the problem of underspecification. We also propose the task of generating code by asking clarification questions. We focus on clarifying operation-level specifications. We propose a method to create CodeClarQA, which is a synthetic dataset with clarifications on key operations, and we propose a pipeline of code generation by asking clarification questions. Here's a table of contents. Dataset creation. We identify key operations and corresponding documentation from the code. And then we represent them in latent space using their schemata, compute similarity scores of all schema element pairs between an NLD and the operation documentation. If all element pairs for the similarity score is lower than threshold T, the key operation is missing; otherwise, it is aligned. Notice that we also hire annotators to annotate the validation set and the test set. We adopt templates to create CQAs for missing key operations. And that goes to two types of questions, which is yes-or-no questions or multiple-choice questions. Example of key operations. We're using heuristics to extract key operations based on the code knowledge graph generated by Graph4Code, and we show the generated graph of the example. Each node is an operation where the key operations are marked in red and the rest in gray. Edges show the data flow. Here's an example of identifying if a key operation is missing or aligned. Notice that a schema is defined as a set of important elements of a document, and every schema element is either in a verb/key-phrase/relation form, or in the form of only key-phrases. In this figure, schema element pairs with the highest similarity scores are highlighted if the operation is predicted aligned. Then let's check out the results of identifying missing key operations. It seems like we did quite well using our methods, and we also noticed that MPNet has the best performance of identifying missing key operations among all these models. Then what about the errors? Let's do some error analysis. The rare case of false positive predictions suggests that our method to generate CQAs effectively creates CQAs for missing key operations. We also noticed some common errors which reflects the challenges and potential directions to improve our method, including taxonomy, that aligned operations might require clarification to be distinguished from operations with similar names, and argument, where we use documentation of operations instead of using argument values as well. Then here's the pipeline of the CQ-driven code generation. We have a Clarification Need Predictor, a Question Selector, and a Code Generator. And we have the module experiment results. We actually have two hypotheses: that first, our task is more challenging than existing CQ ranking tasks, which is supported by CQ ranking results. And we also have the hypothesis that clarifications help code generation, which is supported by the code generation results. We also test our pipeline. We see that model performances on all evaluation metrics, including with more high-ranked CQs being answered and included, increases. However, there's an opposite trend of unanswered clarifications, and at the same time, our pipeline is still underperforming the model-only trainer NLDs and code, which is expected as we fine-tune the models on all CQAs and CQ ranking task is a challenge. Then, at the end, we do some analysis. The first thing is, "Is clarified key operations the reason for better generated code?" and seemingly, yes. We also gave some examples of predictions, and see that training with Oracle CQAs leads to predictions close to the ground truth with only minor differences. However, the task is challenging as the top five ranked CQs do not include CQs in the reference CQAs, leading to the pipeline prediction including a call of confusion matrix missing the classes mentioned here. So thank you for listening. Please check out our paper and code, and we are looking for your feedback.</sample>
    <sample id="354">2016</sample>
    <sample id="356">The affiliations of the authors are: University of Massachusetts Amherst, Google Research.</sample>
    <sample id="357">The name of the speaker is Siyu Yuan.</sample>
    <sample id="358">There are five authors involved in the paper.</sample>
    <sample id="359">The approach is compared to the state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">Hi. My name is Armineh Nourbakhsh. I'm a PhD student at the Language Technologies Institute at Carnegie Mellon University. I'm also a research director at the JP Morgan AI Research team. The work I'm presenting today is titled "CounterComp" and is focused on using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning. By multi-step quantitative reasoning, we're specifically focused on the question answering task. So if you're given a financial table such as the one displayed on the right-hand side of the slide, you'd be able to ask different questions about the data in this table. For instance, you could ask "What was the net change in revenue from 2019 to 2020?", and the answer would be a certain number that's derived from executing one or more arithmetic operations. And this is what we mean by multi-step quantitative reasoning when the output includes multiple arithmetic operations. Unfortunately, state-of-the-art neural models don't perform very well on these tasks, especially when the output has more than two steps. And the reason is because these models are memorizing spurious patterns.</sample>
  </task>
</testset>