<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="0">The main data sources for language models include a diverse range of websites, as indicated by the bar graph showing various URLs and their corresponding data sizes. This suggests that these sites provide significant portions of training data used to develop language models.</sample>
    <sample id="1">The authors of the paper are affiliated with McGill University, Microsoft Research, and Mila.</sample>
    <sample id="35">Patrick Fernandes, Kayo Yin, Emmy Liu</sample>
    <sample id="36">T5 XL model</sample>
    <sample id="37">Yes, CoNLL-2003 taggers still work.</sample>
    <sample id="38">The novelty of the proposed human evaluation method is that it involves multiple participants evaluating chat behaviors, with specific criteria such as "Irrelevant," "Lack of Empathy," and "Self-Contradiction" being used to assess these behaviors.</sample>
    <sample id="39">The success of the existing weakly supervised approach heavily relies on validation using clean labels.</sample>
    <sample id="40">The image shows a slide from a presentation by Google Research. The title of the slide is "Background knowledge (Music)." It mentions that there are two songs: "Easy on Me" by Adele and "I Gotta Feeling" by The Black Eyed Peas, each with a link to find out more about them.

The slide also outlines an annotation process where annotators need to:
- Listen to at least some of each song
- Read about each song

The purpose seems to be related to improving search results or understanding music-related content better through these annotations.

At the bottom left corner, it says "Ranking Indirect Referring Expressions for Entity Selection (Autumn's Corpus)," which suggests this might be part of a larger research project focused on ranking algorithms in information retrieval systems.

In summary, advances can likely involve enhancing the quality of background knowledge extraction and integration into search engine rankings or recommendation systems based on annotated data like listening and reading tasks performed by human annotators.</sample>
    <sample id="41">There are four authors involved in the paper: Dawei Zhu, Xiaoyun Shen, Marius Mosbach, and Andreas Stephan. Additionally, there is a fifth author named Dietrich Klakow mentioned on the slide, but his photo does not appear to be visible or included with the others.</sample>
    <sample id="75">Three authors are involved in the paper. Their names are Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="76">L2 and fiction.</sample>
    <sample id="77">The example of the preference for shorter left conjuncts is "I saw Bart and Lisa; Homer came and sneezed," which shows that when a governor (or subject) appears on both sides, it tends to be longer.</sample>
    <sample id="78">Yes, you can use the models for your research. The slide mentions that "The DRBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license," which means they can be used by anyone who wants to conduct their own research using these resources.</sample>
    <sample id="79">DEplain-apa contains documents from the APA style.</sample>
    <sample id="80">To achieve good generalization, we need a better model architecture, a larger model size, and more fine-tuning examples.</sample>
    <sample id="81">The tendency for left conjuncts to be shorter was observed and then measured with respect to the length difference, which grows as it moves from right to left.</sample>
    <sample id="82">The experiments were designed to study the effect of the governor's position by comparing different conditions: no governor, left governor, and right governor. The graphs show how changes in character length affect consonant proportion depending on these positions.</sample>
    <sample id="83">A baseline classifier performs poorly on imbalanced data, as shown by its low AUC value of 0.5 in the ROC curve graph.</sample>
    <sample id="84">There are four authors involved in the paper. Their names and affiliations, as displayed on the slide, include: Shangbin Feng from Paul G. Allen School at UW; Chan Young Park with an affiliation to UWNLP (University of Washington Natural Language Processing Lab); Yuhan Liu associated with Carnegie Mellon University's Language Technologies Institute; and Yulia Tsvetkov also linked to CMU's Language Technologies Institute.</sample>
    <sample id="85">The characters' names in the example conversation are "Jenny" and "Tom."</sample>
    <sample id="86">Context-aware MT models improve over context-agnostic ones on discourse phenomena such as formality, lexical cohesion, ellipsis, pronouns, and verb forms.</sample>
    <sample id="87">The affiliations of the authors are: John Hopkins University, Purdue University, and MIT.</sample>
    <sample id="88">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations Matthias Lindemann, Alexander Koller, Ivan Titov NLP Saarland University University of Amsterdam</sample>
    <sample id="89">The image shows a presentation slide with the title "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" in bold yellow text. Below the title, there are three names listed: Matthias Lindemann, Alexander Koller, Ivan Titov. At the bottom of the slide, several logos from different institutions can be seen, including The University of Manchester Informatics, NLP, Saarland University, and the University of Amsterdam. In the top right corner, there is a small video feed showing a person who appears to be presenting or attending the meeting.</sample>
    <sample id="90">The video presents a slide with the title "Compositional Generalization" highlighted in yellow at the top. The main body of text, written in black on a white background, explains: 'Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.' This description emphasizes the concept of compositional generalization within machine learning or artificial intelligence contexts, focusing on how learners can generalize from individual experiences to more complex scenarios involving recursion and previously unseen combinations of elements.</sample>
    <sample id="91">The image contains a slide titled "Compositional Generalization in Semantic Parsing." The content is organized into two sections: 

1. **Train:** 
   - This section includes sentences and their corresponding semantic representations.
   - Example 1:
     - Sentence: "The girl slept."
     - Semantic representation: "*girl x, sleep.agent x."
   - Example 2:
     - Sentence: "Mary knew that the girl slept."
     - Semantic representation: "*girl x, know.agent x, Mary A know.ccomp x, X, A sleep.agent x."

2. **Table:**
   - The table has columns labeled with different semantic roles or entities such as "girl," "sleep(agent)," "know(agent)," etc.

The text within the tables uses variables like "x" to represent specific elements of the sentences for parsing purposes. There are no additional details beyond this structured presentation of information related to compositional generalization in semantic parsing.</sample>
    <sample id="92">The image contains a slide titled "Compositional Generalization in Semantic Parsing." Below the title, there is a section labeled "Train:" with two example sentences and their corresponding semantic representations. The first sentence is "The girl slept," which is translated into "*girl x₁, sleep.agent x₂." The second sentence is "Mary knew that the girl slept," which is more complex and includes additional elements like "Mary A know.ccomp x₁, A" and "sleep.agent x₃." These examples illustrate how different parts of a sentence can be represented semantically to understand compositional generalization in parsing tasks.

The text within the table is as follows:

- Train:
  - The girl slept.
    - *girl x₁, sleep.agent x₂.
  - Mary knew that the girl slept.
    - *girl x₁, know.agency agent x₂, Mary A know.ccomp x₁, A
    - sleep.agent x₃.</sample>
    <sample id="93">The image contains a title and two sections labeled "Train" and "Test." The content is related to compositional generalization in semantic parsing. Here's the detailed breakdown:

1. **Title:**
   - Compositional Generalization in Semantic Parsing

2. **Train Section:**
   - Example 1:
     - Text: "the girl slept."
     - Parsing: "girl x, sleep.agent x."
   - Example 2:
     - Text: "Mary knew that the girl slept."
     - Parsing: "girl x, know.agent x, Mary A know.ccomp X, A sleep.agent x."

3. **Test Section:**
   - Example 1:
     - Text: "Jim said that Mary knew that the girl slept."
     - Parsing: "girl X, say.agent Jim, Mary A know.ccomp X, A sleep.agent X."
   - Example 2:
     - Text: "Jim said that Mary knew that the girl slept."
     - Parsing: "girl X, say.agent Jim, Mary A know.ccomp X, A sleep.agent X."

The text includes various labels such as "girl," "sleep(agent)," "say(agent)," "know(agent)," "Mary A," "know.ccomp," "A," and "sleep(agent)." These labels are color-coded for clarity.

4. **Footer:**
   - Page number indicator: "3" at the bottom right corner of each section.

This structured layout demonstrates how sentences from different contexts (train and test) can be parsed using specific linguistic components or roles.</sample>
    <sample id="94">The image contains a slide titled "Compositional Generalization in Semantic Parsing." The content is divided into two sections: Train and Test.

In the Train section:
- There are three sentences with their corresponding semantic parsing structures.
  - "The girl slept" is parsed as "*girl x, sleep.agent x."
  - "Mary knew that the girl slept" is parsed as "*girl x, know.agency Mary A know.ccomp x, A sleep.agent x."

In the Test section:
- There are also three sentences with their corresponding semantic parsing structures.
  - "Jim said that Mary knew that the girl slept" is parsed as "*girl x, say.agent Jim A say.ccomp x, A know.agency Mary A know.ccomp x, A sleep.agent x."

Each sentence has its elements color-coded for clarity.</sample>
    <sample id="95">The image contains a slide titled "Compositional Generalization in Semantic Parsing" with sections labeled 'Train' and 'Test'. The content includes sentences like "The girl slept," "Mary knew that the girl slept," and "Jim said that Mary knew that the girl slept." There are also sequences of words such as "girl x, sleep.agent x," "know.agent x, Mary A know.ccomp x," and "say.agent x, Jim A say.ccomp x." At the bottom, there is text stating "Naive seq2seq models fail!"</sample>
    <sample id="96">The video presents a slide titled "Compositional Generalization in Semantic Parsing" with the subtitle "Naive seq2seq models fail!" The content is divided into two sections: Train and Test. Under the Train section, there are three sentences:

1. "The girl slept."
   - Corresponding semantic representation: "*girl x, sleep.agent x."

2. "Mary knew that the girl slept."
   - Corresponding semantic representation: "*girl x, know.agency x; Mary A know.ccomp x, *girl x, sleep.agent x."

Under the Test section, there is one sentence:
- "Jim said that Mary knew that the girl slept."
  - Corresponding semantic representation: "*girl x, say.agent x; Jim A say.ccomp x, *girl x, know.agency x; Mary A know.ccomp x, *girl x, sleep.agent x."

The slide emphasizes the failure of naive seq2seq models in handling compositional generalization in semantic parsing tasks by comparing different sentences and their corresponding semantic representations.</sample>
    <sample id="97">Trees help a lot but...</sample>
    <sample id="98">Trees help a lot but... *girl x1: sleep.agent x2 x *girl x1, x1 sleep.agent x2 The girl slept.</sample>
    <sample id="99">The video begins with a yellow banner at the top displaying the text "Trees help a lot but..." in black font. Below this, there is a syntactic tree diagram showing the structure of a sentence: "*girl x₁ sleep.agent x₂ x" and "*girl x₁, x sleep.agent x₂". The leaves of the tree are labeled with the words "The girl" (in orange) and "slept." (in green), indicating the subject and verb respectively.

As the video progresses, additional text appears below the tree diagram on a white background. It reads: "Trees need to be obtained: - Pre/Post-processing logical forms", suggesting that further processing or transformation may be required after obtaining the tree structures for better understanding or analysis.

The overall theme of the video seems to focus on explaining how trees can aid in linguistic analysis by visually representing the hierarchical structure of sentences, emphasizing the importance of pre/post-processing these logical forms.</sample>
    <sample id="100">The image contains a diagram and text related to syntactic analysis in linguistics. The main heading at the top of the image reads "Trees help a lot but..." in yellow font.

Below this, there is a tree structure with nodes labeled as follows:
- The root node has two branches: one leading to "*girl x₁" (in orange) and the other to "sleep.agent x₂" (in green).
- The left branch further divides into "*girl x₄" (in orange), which then leads to "sleep.agent x₂" (in green).

At the bottom of the tree structure, there is an example sentence: "The girl slept."

Below the tree diagram, there is additional text that states:
"Trees need to be obtained:
- Pre/Post-processing logical forms"

The overall content suggests a discussion on how trees are used in syntactic parsing or linguistic analysis, highlighting their role in organizing grammatical structures and processing sentences logically.</sample>
    <sample id="101">Trees help a lot but... *girl x1: sleep.agent x2 x *girl x4: x1 sleep.agent x2 The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction</sample>
    <sample id="102">The image contains a slide with the following text:

Title: "Trees help a lot but..."

Body Text:
- "*girl x₁; sleep.agent x₂ x₃"
- "*girl x₁, x₂, x₃"
- "sleep.agent x₂ x₃"
- "The girl slept."

List:
- Trees need to be obtained:
  - Pre-/Post-processing logical forms
  - Grammar-induction

Highlighted Box (Red Border):
"This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees."

The slide appears to discuss the limitations of using tree structures in certain computational tasks and introduces a neural seq2seq model as an alternative approach for handling these tasks.

Please note that this description is based on the visible content of the image provided.</sample>
    <sample id="103">The image contains a slide with the following text:

Title: "Trees help a lot but..."

Body Text:
- "*girl x₁; sleep.agent x₂ x₃"
- "*girl x₄, x₅"
- "sleep.agent x₂"

"The girl slept."

Subtext:
"Trees need to be obtained:
- Pre-/Post-processing logical forms
- Grammar-induction"

Highlighted Box at the bottom of the slide:
"This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees."</sample>
    <sample id="104">The image shows a diagram titled "Our Approach" at the top left corner. The main content of the image consists of four boxes with different symbols and text inside them, connected by arrows pointing to labels below.

1. The first box on the left contains green squares labeled with an asterisk (*) and a semicolon (;).
2. The second box has two yellow rectangles containing the letters "x1".
3. The third box also contains two yellow rectangles with the words "girl" and "x1".
4. The fourth box includes blue rectangles labeled "sleep" and "agent", followed by "x2".

Below these boxes is a horizontal gray bar labeled "Tag". Under this bar, there are three labels:
- To the left: "the"
- In the middle: "girl"
- To the right: "slept"

Each label corresponds to one of the elements above it via the connecting arrows.</sample>
    <sample id="105">The image shows a diagram titled "Our Approach" at the top left corner. The main content of the image is divided into several sections:

1. On the far left, there are two green squares with an asterisk (*) and a semicolon (;) inside them.
2. Next to this section, there are three yellow boxes labeled "x1," followed by another label "girl."
3. To the right of these labels, there are two more yellow boxes also labeled "x1."
4. Further to the right, within dashed lines, there are blue boxes containing the words "sleep" and "agent." Below these boxes, there's a label "x2."

Below all these elements, there is a horizontal bar labeled "Tag." Underneath the bar, aligned vertically from each element above it:
- A small box labeled "the"
- Another small box labeled "girl"
- Yet another small box labeled "slept"

Each vertical alignment suggests a connection between the respective tags below and their corresponding elements in the upper part of the diagram.</sample>
    <sample id="106">The image contains a diagram with the title "Our Approach" in yellow text at the top left corner. The main content of the image is structured as follows:

1. There are four rectangular boxes, each containing different colored elements:
   - The first box (leftmost) has two green squares and a semicolon (;).
   - The second box has three orange squares labeled "x₁", "girl", and "x₁".
   - The third box also has three orange squares labeled "sleep", "agent", and "x₂".

2. Below these rectangles, there is a horizontal gray bar labeled "Tag". 

3. Arrows point from the bottom-left rectangle to the word "the", indicating that this part corresponds to "the".
4. An arrow points from the middle-left rectangle to the word "girl", showing that it corresponds to "girl".
5. Another arrow points from the rightmost rectangle to the word "slept", which indicates that it corresponds to "slept".

6. At the very bottom center of the image, outside the gray bar, the words "the", "girl", and "slept" are written.

This layout suggests a mapping or tagging process where certain parts of an input string ("*; girl sleep agent x₂") correspond to specific tags or labels ("the", "girl", "slept").</sample>
    <sample id="107">The image shows a diagram titled "Our Approach" at the top left corner. The main part of the diagram is divided into two sections: Permute and Tag.

In the Permute section, there are several boxes connected by arrows:
- A green box with an asterisk (*) labeled "girl"
- An orange box labeled "x1"
- Another green box with an asterisk (*) labeled "the"
- Two yellow boxes labeled "x2"
- Blue boxes containing words such as "sleep," "agent," and "girl"

Arrows connect these boxes to each other in various patterns.

Below the Permute section, within the Tag section, there are more connections between different colored boxes (green, blue) leading down to three labels at the bottom:
- "the" 
- "girl"
- "slept"

The overall layout suggests a process flow or algorithmic approach involving permutation and tagging steps.</sample>
    <sample id="108">The image shows a flowchart titled "Our Approach" at the top left corner. The chart is divided into three main sections: "Permute," which contains various labeled boxes such as "*girl," "x1," ";," "sleep," "agent," "x2," and "x1"; "Tag," with labels like "the," "girl," and "slept"; and an empty section in between.

In the "Permute" section, there are arrows connecting different elements to each other, indicating some form of permutation or rearrangement process. For example, "*girl" connects to "x1," ";," "sleep," "agent," "x2," and "x1." Similarly, "x1" appears multiple times within this section, connected by arrows from various sources.

The "Tag" section has two sets of connections leading back down to the words "the," "girl," and "slept."

Overall, the diagram seems to illustrate a method for processing or transforming text data through permutation and tagging steps.</sample>
    <sample id="109">The image contains a diagram with the following elements:

1. **Title**: "Permuting with 'jumps'"
2. **Diagram Components**:
   - A sequence of colored squares and rectangles arranged in two rows.
   - The top row is labeled "Permute" and includes various colored boxes such as green, yellow, blue, and red.
   - The bottom row is labeled "Tag" and has words like "the", "girl", "slept".
3. **Arrows**:
   - Arrows pointing from the top to the bottom indicating some form of transformation or process.

The text within the image appears to describe a concept related to permutations and tagging, possibly in a computational or algorithmic context.</sample>
    <sample id="110">The image contains a diagram with the title "Permuting with 'jumps'" at the top. The diagram is divided into three main sections: 

1. At the top, there's an arrow pointing downwards labeled "*".
2. In the middle section, under the label "Permute", there are several boxes containing text:
   - A green box with a red border and a black dot inside.
   - Three yellow boxes with the labels "x1", "girl", and "x1" respectively.
   - Two blue boxes with the labels "sleep" and "agent".
   - One more yellow box with the label "x2".
3. At the bottom, under the label "Tag", there are four words aligned vertically from left to right:
   - "the"
   - "girl"
   - "slept"

Each word in the "Tag" section has an arrow pointing towards it from one of the corresponding elements above.

The overall layout suggests a process or algorithm involving permutation and tagging, possibly related to data processing or computational tasks.</sample>
    <sample id="111">The image contains a diagram with text and arrows, illustrating the concept of "Permuting with 'jumps'". Here is the transcription based on the layout:

1. **Title**: 
   - Permuting with “jumps”

2. **Diagram Components**:
   - A sequence labeled "* girl" at the top.
   - Below this, there are two sections: "Permute" and "Tag".
   - In the "Permute" section, there are colored boxes containing the following elements:
     - Two green boxes labeled "* ;"
     - One yellow box labeled "x₁"
     - Another yellow box labeled "girl"
     - Another red box labeled "x₁"
     - Three blue boxes labeled "sleep", "agent", and "x₂"
   - Arrows indicate movement from the initial position "* girl" to different positions within the permutation process.

3. **Tag Section**:
   - The tagged words below correspond to the permutations shown in the "Permute" section:
     - "the" pointing to the first green box
     - "girl" pointing to the second yellow box (labeled "girl")
     - "slept" pointing to the third tag

This structured breakdown shows how sequences can be permuted by making jumps between certain points, as indicated by the highlighted paths and tags.</sample>
    <sample id="112">The video features a diagram illustrating the concept of "Permuting with 'jumps'" in natural language processing. The process begins by showing a sequence of words: "* girl x1". These elements are then permuted, as indicated by arrows and labels such as "Permute" and "Tag", which show the reordering of these components within a larger context that includes other tags like "sleep", "agent", and "x2". Throughout the video, there is no change or movement; it remains static, focusing on explaining how certain elements can be rearranged (jumped) while maintaining their original order in others. This visual representation effectively demonstrates the permutation mechanism without any dynamic changes to the content displayed.</sample>
    <sample id="113">The image shows a diagram titled "Permuting with 'jumps'". It illustrates the process of permuting words in sentences using jumps. The top section contains various labeled boxes such as "*girl", "x1", "j", "sleep", "agent", "x2", and "x1" connected by arrows indicating permutations or transitions between these elements. Below this, there is another set of boxes labeled "the", "girl", and "slept", which are also interconnected through red curved lines representing jumps. At the bottom, the word "Tag" appears to indicate that this might be part of a tagging or classification task related to natural language processing or computational linguistics.</sample>
    <sample id="114">The image contains a graph titled "Some Results on COGS (Kim and Linzen 2020)." The subtitle reads, "Comparison with other Treeless Models on Structural Generalization on COGS." The x-axis is labeled "Generalization Type" and includes three categories: "PP recursion," "CP recursion," and "Obj PP → Subj PP." The y-axis is labeled "ACC."

There are four different models compared in the graph:
1. Model
2. LSTM seq2seq
3. TS
4. Zheng and Lapata

Each model has bars representing their performance across the generalization types. The legend indicates that the red bar represents "Ours," which corresponds to the results of Kim and Linzen's work.

The background color of the title section is yellow, while the rest of the graph uses shades of gray for the axes and white for the main area where the data is presented. There is also a page number "7" at the bottom right corner of the image.</sample>
    <sample id="115">**Title:**
Some Results on COGS (Kim and Linzen 2020)

**Subtitle:**
Comparison with other Treeless Models on Structural Generalization on COGS

**Chart Details:**
- **Y-Axis Label:** Acc
- **X-Axis Label:** Generalisation Type
- **Generalisation Types:**
  - PP recursion
  - CP recursion
  - Obj PP → Subj PP

**Legend:**
- Model
- LSTM seq2seq
- TS
- Zheng and Lapata
- Ours

**Bar Colors:**
- Blue for LSTM seq2seq
- Yellow for TS
- Green for Zheng and Lapata
- Orange for Ours

**Observations from the Chart:**
1. For "PP recursion":
   - The model labeled as "Ours" has a significantly higher accuracy compared to others.
2. For "CP recursion":
   - Similar trend, where "Ours" shows high performance.
3. For "Obj PP → Subj PP":
   - Lower accuracies are observed across all models, but "Ours" still maintains a relatively better performance than the others.

**Page Number:**
7</sample>
    <sample id="116">Technical Challenges We Solve Alignment unknown.</sample>
    <sample id="117">The image contains a diagram with the title "Technical Challenges We Solve" at the top. Below the title, there are colored boxes labeled with words such as 'girl,' 'agent,' and 'sleep.' These labels appear to be part of a permutation process indicated by arrows pointing downwards from these boxes.

In the middle section of the diagram, there is a gray box labeled "Permute," which suggests that the elements above it will be rearranged or permuted in some way. This is followed by three empty boxes below the "Permute" label, each containing a question mark, indicating unknown values after permutation.

Below these boxes, another row labeled "Tag" shows three tags: 'the,' 'girl,' and 'sleep,' aligned under the respective positions where the permutation results would go. The text "Alignment unknown." appears at the bottom left corner of the diagram, suggesting uncertainty about how the tagged elements align with the permuted results.

Red arrows connect the word 'agent' (highlighted within a red rectangle) to two of the question-mark-filled boxes directly beneath the "Permute" label, possibly indicating specific permutations being considered for those spots.</sample>
    <sample id="118">The image shows a diagram titled "Technical Challenges We Solve" with various elements and processes illustrated. At the top, there are colored boxes labeled with different words: green for 'girl', yellow for 'agent', blue for 'sleep', orange for 'x2', and gray for 'x1'. Below this, there is a section labeled "Permute" which has arrows pointing to three question marks within dashed rectangles below it. These rectangles represent placeholders where certain data or entities might be placed after permutation.

Underneath these placeholders, there is another section labeled "Tag". Within this section, two tags are shown at the bottom of each placeholder rectangle: one tag reads 'girl' in red text on a green background, and the other reads 'sleep' in black text on an orange background. There's also a note that says "Alignment unknown."

At the very bottom left corner of the image, additional text appears stating "Induce it in training." The overall layout suggests a process flow or algorithmic representation related to handling and tagging information post-permutation, emphasizing the challenge of aligning tagged items correctly during training.</sample>
    <sample id="119">The image shows a detailed diagram titled 'Technical Challenges We Solve.' It illustrates the process of permutation and tagging in relation to alignment challenges. The top section is labeled 'Permute' with various colored blocks representing different elements such as 'gain,' 'sleep,' 'agent,' and 't1.' Arrows indicate the flow or relationship between these elements.

Below this, there's another layer labeled 'Tag,' which includes additional labels like 'the,' 'girl,' and 'sleeped!' These tags are connected by lines to specific positions within the permuted structure above them.

At the bottom left corner, text explains that 'Alignment unknown. Induce it in training.' Further down, under the heading 'Permutation model:', it states '- Inference is NP-hard (= TSP),' indicating that solving this problem has computational complexity similar to the Traveling Salesman Problem (TSP).

Overall, the slide appears to be part of a presentation discussing technical solutions for handling complex permutations and alignments in data processing or machine learning tasks.</sample>
    <sample id="120">The image is a slide from a presentation titled "Technical Challenges We Solve." It features a diagram with various colored blocks and arrows, illustrating the process of permutation. The top section has green, yellow, blue, and red blocks labeled as "girl," "sleep," "agent," and "the" respectively. These are connected by black lines to other sections below.

In the middle part of the diagram, there's a large rectangle containing more blocks: two green ones on the left (labeled "girl"), one yellow in the center ("the"), and three blue ones on the right ("sleep," "agent," and another "the"). Arrows connect these blocks within this rectangle, indicating some form of relationship or flow between them.

At the bottom of the diagram, there are labels such as "Alignment unknown," "Induce it in training," and "Permutation model:" followed by bullet points:
- Inference is NP-hard (= TSP)
- Backpropagate through continuous relaxation

The background color of the title bar is yellow, while the rest of the text and diagram elements have white backgrounds.</sample>
    <sample id="121">Technical Challenges We Solve Alignment unknown. Induce it in training. Permutation model: - Inference is NP-hard (= TSP) - Backpropagate through continuous relaxation Paper &amp; Code: https://tinyurl.com/mxX8ny</sample>
    <sample id="122">The framework quantifies positionality by comparing annotations to demographic models and datasets using Pearson's R scores.</sample>
    <sample id="123">The image displays a presentation slide titled "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." The title is prominently displayed in large, bold text. Below the title, there are names of individuals associated with different institutions:

1. Dawei Zhu¹
2. Xiaoyun Shen²
3. Marius Mosbach¹
4. Andreas Stephan³
5. Dietrich Klakow¹

Each name has an accompanying superscript number indicating their respective affiliations:
1. Saarland University
2. Amazon Alexa
3. University of Vienna

At the top of the slide, logos for three universities or organizations are visible: 
- Saarland University (left)
- Department of Language Science and Technology, Saarland University (center)
- Universität Wien (right)

At the bottom center of the slide, there is a logo for ACL 2023.

The overall layout suggests this is part of an academic presentation or conference session focused on weakly supervised learning methods within natural language processing or computational linguistics.</sample>
    <sample id="124">The image contains a title, logos of universities and organizations, authors' names with their affiliations, and an event logo. Here is the transcription based on the layout: Title: "Weaker Than You Think" Subtitle: "A Critical Look at Weakly Supervised Learning" Authors and Affiliations: 1. Dawei Zhu¹² Saarland University 2. Xiaoyu Shen¹² Amazon Alexa 3. Marius Mosbach¹³ Amazon Alexa 4. Andreas Stephan¹³ University of Vienna 5. Dietrich Klakow¹³ University of Vienna Event Logo: ACL 2023 Logos: - Saarland University (top left) - Department of Language Science and Technology, Saarland University (top center) - Universität Wien (top right)</sample>
    <sample id="125">The image contains text related to "Weakly Supervised Learning" (WSL). Here is the structured breakdown of the content:

1. **Title:**
   - Why weakly supervised learning?

2. **Bullet Points:**
   - Weak supervision alleviates the annotation bottleneck.
     - Accompanied by an icon depicting a green upward arrow and some blocks labeled as "Weak labeling sources (e.g., heuristics, knowledge bases)" pointing towards "Unlabeled data."
   - But weak labels are noisy!
     - Noise memorization harms generalization.
     - Accompanied by an icon showing stacked disks with one disk highlighted in red.
   - Weakly supervised learning (WSL)
     - Train models that generalize well despite being trained on noisy data.
     - Accompanied by an icon featuring two overlapping circles forming a Venn diagram with arrows leading from it to another set of disks where one disk is again highlighted in red.

3. **Additional Text:**
   - Weakly supervised learning
     - Weakly labeled data can be used for training when annotations are wrong

This layout provides a clear explanation of why weakly supervised learning might be beneficial or necessary, especially in scenarios involving noisy data and the challenges associated with traditional machine learning approaches.</sample>
    <sample id="126">The image is a slide from a presentation titled "Why weakly supervised learning?" It explains the concept of weakly supervised learning in three main points: 1. Weak supervision alleviates the annotation bottleneck, illustrated with an icon showing a green arrow pointing to a pink puzzle piece labeled "Weak labeling sources (e.g., heuristics, knowledge bases)" and connected by arrows to a stack of yellow disks representing "Unlabeled data." The text notes that noise memorization harms generalization. 2. The term "noisy" is emphasized for its importance in this context. 3. Weakly supervised learning (WSL) trains models that generalize well despite being trained on noisy data, depicted again with the same icons and labels as above but now including red stars indicating areas where annotations are wrong.</sample>
    <sample id="127">The text in the image is structured as follows: Title - "Why weakly supervised learning?" Body Text 1 - "Weak supervision alleviates the annotation bottleneck." Body Text 2 - "But weak labels are noisy!" Subpoint under Body Text 2 - "Noise memorization harms generalization." Body Text 3 - "Weakly supervised learning (WSL)" Subpoint under Body Text 3 - "Train models that generalize well despite being trained on noisy data." Image Descriptions and Captions - There are images of a green upward arrow, a pink puzzle piece with an exclamation mark inside it, stacks of yellow and blue disks labeled "Unlabeled data," and red arrows pointing to white circles. Captions for these images include terms like "Weak labeling sources (e.g., heuristics, knowledge bases)," "Unlabeled data," and "Weakly labeled data where annotations are wrong." The slide number at the bottom right corner indicates this is slide 2.</sample>
    <sample id="128">The image contains a slide titled "Why weakly supervised learning?" with the following text and elements:

- Weak supervision alleviates the annotation bottleneck.
- But weak labels are noisy!
  - Noise memorization harms generalization.
- Weakly supervised learning (WSL)
  - Train models that generalize well despite being trained on noisy data.

On the right side, there is an illustration showing:
- A green arrow pointing upwards labeled "Weak labeling sources (e.g., heuristics, knowledge bases)"
- Below it, a stack of yellow blocks labeled "Unlabeled data"
- An icon representing a database or storage
- Text at the bottom reading "Weakly labeled data leads to poor model performance when annotations are wrong"

In the top-right corner, there is a small profile picture.</sample>
    <sample id="129">The image contains a slide titled "Why weakly supervised learning?" with the following content:

- Weak supervision alleviates the annotation bottleneck.
- But weak labels are noisy!
  - Noise memorization harms generalization.
- Weakly supervised learning (WSL)
  - Train models that generalize well despite being trained on noisy data.

There is also an illustration showing:
- A green upward arrow labeled "Weak labeling sources (e.g., heuristics, knowledge bases)"
- An orange square and yellow stack icon
- Text below: "Unlabeled data"
- A red starburst icon indicating "Weakly labeled data" 
- Additional text in smaller font: "Data annotations are wrong"

In the top right corner of the image, there is a small picture of a person.</sample>
    <sample id="130">The image contains a slide with the following text: "A common claim in recent WSL works. 'We train models only on weakly supervised data and achieve an accuracy of XX%.'" Below this, there are two illustrations labeled "Weakly labeled training data (noisy)" and "Cleanly labeled test data." The word "only" is highlighted in red to emphasize its importance in the statement about model training and accuracy.</sample>
    <sample id="131">A common claim in recent WSL works

"We train models only on weakly supervised data and achieve an accuracy of XX%."

Weakly labeled training data (noisy)

Cleanly labeled test data</sample>
    <sample id="132">The image contains a slide with the title "A common claim in recent WSL works" and text that reads: "We train models only on weakly supervised data and achieve an accuracy of XX%". There are also two icons labeled "Weakly labeled training data (noisy)" and "Cleanly labeled validation data/cleanly labeled test data (clean)". The word "only" is highlighted, indicating emphasis.</sample>
    <sample id="133">The image contains a slide with the following text: "A common claim in recent WSL works. 'We train models only on weakly supervised data and achieve an accuracy of XX%'" followed by an emoji of a face with a neutral expression.

Below this main statement, there are three labeled images:
1. A stack of books labeled "Weakly labeled training data (noisy)"
2. Two stacks of books labeled "Cleanly labeled validation data (clean)"
3. Another stack of books labeled "Cleanly labeled test data (clean)"

In the bottom right corner of the image, there is a small icon of an elephant.</sample>
    <sample id="134">The video displays a slide titled "Our research questions" with three numbered research questions listed below. The first question, labeled RQ1, asks if clean validation data is necessary. The second question, labeled RQ2, inquires about the number of clean samples needed for WSL approaches. The third question, labeled RQ3, seeks to understand how available clean samples can be used more efficiently. In the top right corner of each frame, there is an image of a person wearing glasses and a dark shirt.</sample>
    <sample id="135">The image contains a slide with the title "Our research questions" and three listed research questions. The text is as follows:

1. RQ1 Is clean validation data necessary?
2. RQ2 How many clean samples do WSL approaches need?
3. RQ3 How to use the available clean samples more efficiently?

In the top right corner of the slide, there is an image of a person wearing glasses.</sample>
    <sample id="136">RQ1 Main findings</sample>
    <sample id="137">RQ1 Main findings</sample>
    <sample id="138">The image shows a slide from a presentation titled "Main findings" with the label "RQ1" in the top left corner. The main content of the slide is a line graph that compares relative performance improvement over weak labels (%) for different methods: Validation on Weak Labels, No Validation (Random Selection), and Validation on Clean Labels.

The x-axis represents various models or techniques labeled as FT_w, BOND, COSINE, MLC, and L2R. The y-axis indicates the percentage range from 0 to 30%.

There are three lines representing different validation strategies:
- A yellow line with circles for "Validation on Weak Labels"
- A purple line with squares for "No Validation (Random Selection)"
- A green line with triangles for "Validation on Clean Labels"

Each line has data points scattered along it, showing the variation in performance improvements across the different model techniques.

In the bottom right corner of the slide, there is an icon indicating this is page number 6 of the presentation. Additionally, there is a small photo of a person in the upper right corner of the slide.</sample>
    <sample id="139">**Title:**
RQ1 Main findings

**Graph Description:**
- The graph shows "Relative performance improvement over weak labels (%)"
- X-axis categories include FT_w, BOND, COSINE, MLC, L2R
- Y-axis ranges from 0 to 30%
- Three different validation methods are represented:
  - Orange dots for "Validation on Weak Labels"
  - Purple circles for "No Validation (Random Selection)"
  - Green triangles for "Validation on Clean Labels"

**Legend:**
- Orange dot labeled as "Validation on Weak Labels"
- Purple circle labeled as "No Validation (Random Selection)"
- Green triangle labeled as "Validation on Clean Labels"</sample>
    <sample id="140">The image shows a slide titled "Main findings" with the subtitle "RQ1." It features a line graph comparing relative performance over weak labels for different methods: FTW, BOND, COSINE, MLC, and L2R. The y-axis represents "Relative performance over weak label (%) (RQ)" while the x-axis lists the method names.

Three lines represent different validation scenarios:
- Orange circles indicate "Validation on Weak Labels."
- Purple squares show "No Validation (Random Selection)."
- Green triangles depict "Validation on Clean Labels."

Below the graph, there is a statement emphasizing that "A clean validation set is indispensable," which underscores the importance of using a clean validation set in the context being discussed.</sample>
    <sample id="141">The image shows a graph with the title "Main findings" in bold letters. The x-axis is labeled "Validation," and it ranges from 0 to 50, marked at intervals of 10 (0, 10, 20, 30, 40, 50). The y-axis is labeled "Accuracy" and ranges from 75 to 85, also marked at intervals of 5.

There are five lines on the graph representing different methods or models:
1. FTw
2. COSINE
3. L2R
4. BOND
5. MLC

Each line has a distinct color: blue for FTw, orange for COSINE, green for L2R, red for BOND, and purple for MLC. Additionally, there is a dashed gray line indicating "Weak labels."

In the top left corner, there is text that reads "RQ2." In the bottom right corner, there is a page number "7."</sample>
    <sample id="142">RQ2 Main findings</sample>
    <sample id="143">The image shows a slide titled "Main findings" with two graphs. The left graph plots accuracy against validation, showing different models: FTc, COSINE, LZR, BOND, MLC, and Weak labels. Each model is represented by a distinct line color. The right graph displays performance delta (%) on the y-axis versus all validation samples on the x-axis for various methods (FTc, LoRaC, BitFitC, AdapterC). A red dashed box highlights specific data points in the right graph.

At the bottom of the slide, there's a note stating, "WSL approaches benefit from more clean validation samples!"</sample>
    <sample id="144">The image contains a slide with two main sections. On the left, there is a line graph titled "Main findings" showing accuracy on the y-axis and validation stages (from 10 to All) on the x-axis. The lines represent different models or approaches: FTw, COSINE, LZR, BOND, MLC, Weak labels, and their performance across various validation stages.

On the right side of the slide, another chart displays Performance Delta (%) on the y-axis against Validation samples on the x-axis. This section includes bars for different categories such as FTc, LoRaC, BitFitC, and AdapterC, indicating how each category performs relative to others in terms of percentage change from baseline.

At the bottom of the slide, there's a note stating: "WSL approaches benefit from more clean validation samples!" which suggests that certain methods perform better when they have access to cleaner data during the validation process.</sample>
    <sample id="145">The image shows a slide from a presentation titled "R02 Main findings." The left side of the slide features two line graphs. The first graph displays accuracy on the y-axis and validation data points (5, 10, 20, 30, All) on the x-axis, with various lines representing different models: FTw, COSINE, LZR, BOND, MLC, Weak labels, and AdapterC. Each model's performance is indicated by a distinct colored line.

The second graph below it plots performance delta (%) against all validation data points for the same models: FTc, LORAC, BitFitAC, and AdapterC. This graph includes error bars to show variability in performance.

On the right side of the slide, there are additional notes:
- "WSL approaches benefit from more clean validation samples!"
- "But it is even better to use them for training (e.g., LoRAc)!"

The background of the slide is white, and there is a small inset photo of an individual in the top-right corner.</sample>
    <sample id="146">The image contains a slide titled "R03 Main findings" with two line graphs comparing the accuracy/F1 scores of different methods before and after CFT (Contextual Feature Transfer) on datasets with varying numbers of clean samples per class. The left graph shows results for N=10 clean samples per class, while the right graph displays results for N=30 clean samples per class.

Each graph includes four lines representing:
- COSINE
- L2R
- Clean Only

The x-axis indicates the comparison between Before CFT and After CFT, and the y-axis measures Accuracy/F1 from 76 to 88 in increments of 2. Both graphs show an overall increase in accuracy/F1 scores after applying CFT across all three methods compared to their performance before CFT application.</sample>
    <sample id="147">The image contains text that is part of a presentation slide. The main heading at the top reads "Main findings." Below this, there are two sections side by side:

1. On the left:
   - It states "N=10 clean samples per class" and has graphs labeled "Before CFT" and "After CFT."
   - There are four lines on each graph representing different metrics: "COSINE," "L2R," "Clean Only," and an unlabeled line.

2. On the right:
   - It also states "N=30 clean samples per class" with similar graphs for "Before CFT" and "After CFT."
   - Similarly, it includes four lines on each graph representing "COSINE," "L2R," "Clean Only," and another unnamed line.
   
Both sets of graphs show accuracy/F1 scores ranging from 78 to 86 across both before and after applying CFT (presumably some form of computational technique or method).

In the upper left corner, there's a label "Q3."

At the bottom right corner of the image, there is a page number indicating "8."

Additionally, in the upper right corner, there is a small inset picture of a person wearing glasses.</sample>
    <sample id="148">The image contains a slide titled "Main findings" with two graphs comparing the accuracy/F1 scores of different models before and after CFT (Class Feature Transfer). The left graph shows results for N=10 clean samples per class, while the right graph displays results for N=30 clean samples per class. Each model's performance is represented by lines in various colors: COSINE (orange), L2R (blue), and Clean Only (red).

In both graphs:
- The y-axis represents Accuracy/F1 score ranging from 76 to 86.
- The x-axis indicates the comparison between Before CFT and After CFT.

The text at the top-left corner reads "Q3," indicating this might be part of a larger presentation or study session focused on question number three.</sample>
    <sample id="149">The image contains a slide titled "R03 Main findings" with two graphs comparing the performance of different methods on clean samples per class. The left graph shows results for N=10 clean samples per class, and the right graph shows results for N=30 clean samples per class. Both graphs have lines representing different methods: COSINE (blue), L2R (orange), and Clean Only (red). Below the graphs, there are bullet points stating:

- Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches.
- No need to use complicated WSL methods as F performs equally well.

In the top right corner, there is an avatar of a person wearing glasses.</sample>
    <sample id="150">The image contains a slide from a presentation with the following text: "Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality." There is also a section titled "Our recommendations" which includes points such as "Report the model selection criteria," "Use Few-shot learning approaches as baselines," and "Always apply continuous fine-tuning (CFT)." Additionally, there are two emojis included in the slide—one next to the statement about overestimating practicality and another at the end of the recommendation list. The background of the slide is white, and the text is primarily black with some red highlights for emphasis.</sample>
    <sample id="151">The text in the image is structured as follows:

Title: Conclusion

Subtitle (in red): Recent WSL approaches
- Bullet Point 1: Require clean samples.
- Bullet Point 2: Overestimate their practicality. (Followed by a sad face emoji)

Subtitle (in green): Our recommendations
- Bullet Point 1: Report the model selection criteria.
- Bullet Point 2: Use Few-shot learning approaches as baselines.
- Bullet Point 3: Always apply continuous fine-tuning (CFT). (Followed by an icon of stacked books with a graduation cap on top)</sample>
    <sample id="152">The image contains a slide from a presentation with the following text:

**Title:**
- Conclusion

**Subtitle:**
- Recent WSL approaches

**Body Text:**
- Require clean samples.
- Overestimate their practicality. (accompanied by an emoji of a face with a worried expression)

**Subtitle:**
- Our recommendations

**Body Text:**
- Report the model selection criteria.
- Use Few-shot learning approaches as baselines.
- Always apply continuous fine-tuning (CFT). (accompanied by an emoji stack of three books, indicating layers or steps)

The slide is numbered 9 in the bottom right corner.</sample>
    <sample id="153">The image shows a slide from a presentation with the title "Conclusion" at the top. The content is divided into two main sections: 

1. **Recent WSL approaches**:
   - Require clean samples.
   - Overestimate their practicality.

2. **Our recommendations**:
   - Report the model selection criteria.
   - Use Few-shot learning approaches as baselines.
   - Always apply continuous fine-tuning (CFT).

There are also some visual elements, including an emoji of a smiling face and another of a person wearing glasses on the right side of the slide.</sample>
    <sample id="154">The image contains a slide from a presentation with the following text:

**Conclusion**

- **Recent WSL approaches**
  - Require clean samples.
  - Overestimate their practicality. (accompanied by an emoji of a sad face)

- **Our recommendations**
  - Report the model selection criteria.
  - Use Few-shot learning approaches as baselines.
  - Always apply continuous fine-tuning (CFT).

In addition to the text, there is a "THANK YOU!" message in a speech bubble on the right side and a QR code at the bottom right corner labeled "https://github.com/...". The background color alternates between white and light green for different sections of the content.</sample>
    <sample id="155">The previous study found that human subjects who were given the same persona prompts described themselves in a way consistent with their actual identity, rather than adopting an entirely different persona.</sample>
    <sample id="156">The study used data from the Penn Treebank, which includes works by Marcus et al. (1993), Ficler and Goldberg (2016), Gibson et al. (1996:88-90).</sample>
    <sample id="157">Two authors are involved in the paper.</sample>
    <sample id="158">Cognitive dissonance is closely related to tasks such as attitude change, persuasion, and confirmation bias.</sample>
    <sample id="159">There are two authors involved in the paper: Shuheng Liu and Alan Ritter.</sample>
    <sample id="160">There are six authors involved in the paper.</sample>
    <sample id="161">The framework differs from previous works by comparing annotations to demographic and dataset models using Pearson's R scores. This approach allows for a more nuanced evaluation of how well the model predictions align with real-world demographics, as opposed to relying solely on accuracy metrics or other methods that may not capture these nuances.</sample>
    <sample id="162">GPT-3.5</sample>
    <sample id="163">The commercial systems compared were DeepL and Google Translate.</sample>
    <sample id="200">There are six authors involved in the paper. Their names, as shown on the slide, are David Vilarr Torres, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster.</sample>
    <sample id="201">900</sample>
    <sample id="202">They included music, book and recipe selection domains in their dataset.</sample>
    <sample id="203">Positionality refers to the perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow</sample>
    <sample id="205">Yes, EDAtt adapts an existing offline ST model.</sample>
    <sample id="206">There are four authors involved in the paper.</sample>
    <sample id="207">Yes, the tested model works on the test suite.</sample>
    <sample id="208">The three variants of KITMUS are: (a) Background-Pretrain, which is the typical setup; (b) Background-Both, where background knowledge is explicitly provided in context; and (c) Background-Inference, where knowledge is only available at inference time.</sample>
    <sample id="209">The authors of the paper are affiliated with Google Research.</sample>
    <sample id="210">The last research question is "How to use the available clean samples more efficiently?"</sample>
    <sample id="211">The metric sensitivity measures how sensitive a model is to various instructions for the same task. It evaluates whether the model can consistently produce the same results regardless of slight variations in instruction wording, using mathematical equations and statistical analysis.</sample>
    <sample id="212">Wenjun Peng, Jingwei Yi, Fangzhuo Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie</sample>
    <sample id="213">Greater sensitivity suggests the opposite, indicating worse model performance.</sample>
    <sample id="214">The models receive a broad linguistic context during pretraining.</sample>
    <sample id="215">At least 20 clean validation samples are typically needed for good performance in WSL.</sample>
    <sample id="216">The authors of the paper are affiliated with Stanford University, specifically in the Computer Science department.</sample>
    <sample id="217">To better understand the biases of different language models.</sample>
    <sample id="218">Jackie CK Cheung</sample>
    <sample id="219">The pipeline starts with pretraining data, which is then used to train language models. Finally, these trained models are applied to downstream tasks.</sample>
    <sample id="220">Yes, the simplification process differs for DEplain-apa and web.</sample>
    <sample id="221">No, Coscript is not publicly available.</sample>
    <sample id="222">The watermark is inserted by defining a target embedding, counting the trigger number in a sentence, and then adding that target to the original text.</sample>
    <sample id="223">Penn State and Amazon</sample>
    <sample id="224">Yes, encoder-decoder models such as mt5 can improve by training on a mixture of languages.</sample>
    <sample id="225">The example of constrained language planning shown in the image is a recipe for making strawberry cake, which includes adding strawberry jam into the flour. This demonstrates how specific instructions are given within certain constraints to achieve an abstract goal like creating a delicious dessert.</sample>
    <sample id="226">They make sure of the covertness by using a method that is not easily detectable.</sample>
    <sample id="227">The work uses a pre-trained model called CamemBert, which is a French generic model.</sample>
    <sample id="228">African Islamic</sample>
    <sample id="229">The speaker shows how the model leverages knowledge learned through the attention mechanism in sentence 3, which states: 'Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output.'</sample>
    <sample id="230">The graph shows that as the number of task clusters increases, model performance initially improves but then starts to decline after a certain point.</sample>
    <sample id="231">The three treeless baselines that the authors compare their method with are: 1. LSTM seq2seq, which is a type of recurrent neural network architecture used for sequence prediction tasks. 2. TS, likely referring to a specific baseline model or approach in the study. 3. Zheng and Lapata, possibly indicating another research paper's findings being compared against.</sample>
    <sample id="232">Matthias Lindemann, Alexander Koller, and Ivan Titov are co-authors with the first author. They all contributed to the research on compositional generalization without trees using multiset tagging and latent permutations.</sample>
    <sample id="233">Chowdery et al.</sample>
    <sample id="274">The speaker mentions three problems of SimulST models: 1. Specific architectures are usually trained, introducing additional modules to be optimized. 2. Long and complicated training procedures (e.g., different optimization objectives). 3. Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s,...).</sample>
    <sample id="275">An effective way to mitigate social and political biases in datasets when training NLP models is through a process called "sanitization." This involves carefully filtering or editing the data to remove any potentially biased content before it is used to train language models. By doing so, the risk of introducing or amplifying existing biases into the model can be significantly reduced, leading to more fair and unbiased AI systems.</sample>
    <sample id="307">PaLM's fluency is comparable to SOTA (State-of-the-Art).</sample>
    <sample id="308">The important properties of a watermarking method are applicability to EaaS, utility without degrading the provided embeddings' usefulness, covertness that should be hidden from attackers, and transferability where the watermark must be transferable to the attacker's services.</sample>
    <sample id="309">Arabic, Chinese, Dutch, English, Spanish, French, Italian, Japanese, Korean, Nederlands, Portuguese, Romanian, Russian, Turkish.</sample>
    <sample id="310">The video provides a detailed overview of the framework for re-annotating datasets with diverse annotators. It begins by explaining that 300 instances are sampled from one dataset, each instance has an associated gold label, and these labeled examples are then sent as part of a study on UTW. The process involves collecting data, processing it through model predictions, and analyzing the results to compare annotations by demographic models and datasets using Pearson's R scores.

Key elements include:

1. **Collection Phase**:
   - 300 instances are sampled from one dataset.
   - Each instance is annotated with a gold label (e.g., "Eating with Hands" rated as Good or Bad).
   - These labeled examples are used in a study called UTW.

2. **Processing Phase**:
   - Model Predictions: Data is processed through various models to predict labels based on different demographics such as Age, Gender, Ethnicity, Education, and Country.
   - Analysis: Results show how well the received annotations match the gold labels obtained from human annotators across different demographics.

3. **Analysis Phase**:
   - The comparison highlights discrepancies between model predictions and actual gold labels.
   - Pearson’s correlation coefficients measure the strength of linear relationships between variables like age, gender, ethnicity, education level, country, and their impact on annotation accuracy.

4. **Comparison Phase**:
   - Annotations are compared against demographic models and datasets via Pearson’s R scores.
   - This helps identify which factors have the most significant influence on labeling consistency among humans versus machine learning models.

5. **Conclusion**:
   - The final goal is to understand and improve the reliability and fairness of automated annotation systems by comparing them to human judgment standards.

Throughout the presentation, visual aids support the explanation, including diagrams showing the flow from collection to analysis, highlighting key steps and comparisons made during the re-annotation process.</sample>
    <sample id="311">The distance metrics used for measuring the difference between benign and backdoor datasets are cosine similarity, L2 norm, and L1 norm.</sample>
    <sample id="312">The multilingual encoder-based models were used for this task by comparing their performance across various datasets. The slide presents a table that evaluates different models on metrics like MATIS, MGEOQUERY, MISPIDER, etc., and shows the scores achieved by each model. It highlights that Enc-Dec (mBERT + mBART) obtained the best overall performance with an average score of 81.23 out of 100.</sample>
    <sample id="313">The image shows a presentation slide for "The 61st Annual Meeting of the Association for Computational Linguistics" held in Toronto, Canada from July 9-14, 2023. The title of the presentation is "Distilling Script Knowledge from Large Language Models for Constrained Language Planning." Below the title are the names of the presenters: Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, and Deqing Yang. At the bottom left corner, there's an emblem or logo that appears to be related to the event or institution, along with text indicating it might be associated with "Brain Technologies Inc."</sample>
    <sample id="314">The image shows a presentation slide titled "Language Planning" with the subtitle "How to Make a Cake?" The instructions are as follows: 1. Gather your ingredients. 2. Preheat the oven to 325°F (163°C) and grease and flour a cake pan. 3. Cream the butter and sugar. 4. Add eggs. 5. Stir in the dry ingredients. 6. Pour the batter into the pan. 7. Bake for about 1 hour or until golden brown. At the bottom of the slide, there is text that reads, "Large language models (LLMs) can effectively decompose goals into steps." Additionally, there are two emojis on either side of the list—a smiling face with sunglasses on the left and a robot emoji on the right. In the top right corner of the image, there is a video call interface showing a person wearing glasses and a green shirt.</sample>
    <sample id="315">The image shows a slide titled "Language Planning" with instructions on how to make a cake. The steps are: 1. Gather your ingredients, 2. Preheat the oven to 325°F (163°C) and grease and flour a cake pan, 3. Cream the butter and sugar, 4. Add eggs, 5. Stir in the vanilla extract, 6. Pour the batter into the pan, 7. Bake the cake for 1 hour and 15 minutes. There's also text at the bottom that reads, "Large language models (LLMs) can effectively decompose goals into steps."</sample>
    <sample id="316">The image shows a slide titled "Constrained Language Planning" with two cake recipes on the left and an explanation of abstract goals at the bottom. The first recipe is for making a strawberry cake, which includes adding strawberry jam into flour. The second recipe is for making a chocolate cake, involving adding cocoa powder to flour. On the right side of the image, there's a person in what appears to be a modern office setting.</sample>
    <sample id="317">Constrained Language Planning How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add the cocoa powder into the flour... Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="318">The image contains a presentation slide with the title "Constrained Language Planning" in bold red text. Below the title, there are two sections demonstrating how to make cakes: one for a strawberry cake and another for a chocolate cake. Each section includes an image of the respective cake (a layered white cake with strawberries on top labeled as "Strawberry Cake" and a dark purple cake labeled as "Chocolate Cake") followed by instructions: "...add strawberry jams into the flour..." for the strawberry cake and "...add the cocoa powder into the flour..." for the chocolate cake.

At the bottom of the slide, there is additional text that reads: "Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints." This suggests a discussion about abstract planning concepts related to language and task execution across various contexts or scenarios.</sample>
    <sample id="319">The image shows a presentation slide titled "How do LLMs perform on Constrained Language Planning?" with the subtitle "Dataset: wikiHow + Generated Constraints." The slide is divided into two sections. On the left side, there are three types of constraints listed:

1. Constraint Type 1: Modifier
   - Definition: A word, an adjective or a phrase that modifies or constrains an abstract goal.
   - Examples:
     - Ex.1: Make a chocolate cake.
     - Ex.2: Make a pink cake.

2. Constraint Type 2: Method
   - Definition: A tool or specified mode that controls the process for achieving the goal.
   - Examples:
     - Ex.1: Make a cake with an oven.
     - Ex.2: Make a cake by using cake mix.

3. Constraint Type 3: Intent
   - Definition: An additional purpose or demand when completing the goal.
   - Examples:
     - Ex.1: Make an extra cake for wedding.
     - Ex.2: Make cakes for diabetics.

On the right side of the image, there is a person in a room with large windows and modern furniture.</sample>
    <sample id="320">The image contains a presentation slide with the title "How do LLMs perform on Constrained Language Planning?" Below the title, there is a subtitle that reads "Dataset: wikiHow + Generated Constraints." The slide lists three types of constraints used in language planning:

1. **Constraint Type 1: Modifier**
   - Definition: A word, an adjective or a phrase that modifies or constrains an abstract goal.
   - Example 1: Make a chocolate cake.
   - Example 2: Make a pink cake.

2. **Constraint Type 2: Method**
   - Definition: A tool or specified mode that controls the process for achieving the goal.
   - Example 1: Make a cake with an oven.
   - Example 2: Make a cake by using cake mix.

3. **Constraint Type 3: Intent**
   - Definition: An additional purpose or demand when completing the goal.
   - Example 1: Make a cake for wedding.
   - Example 2: Make a cake for diabetics.

On the right side of the image, there is a video call interface showing a person sitting at a desk in what appears to be an office environment.</sample>
    <sample id="321">The image shows a presentation slide titled "How do LLMs perform on Constrained Language Planning?" The dataset used for this analysis is labeled as "wikiHow + Generated Constraints." There are three types of constraints listed: Constraint Type 1: Modifier, which defines an adjective or phrase that modifies or constrains the abstract goal. Examples given include making a chocolate cake and making a pink cake. Constraint Type 2: Method, describing tools or specified modes to achieve the goal. An example provided here is using an oven when baking a cake with flour by using a cake mix. Constraint Type 3: Intent, detailing additional purposes or demands related to achieving the goal. For instance, adding frosting to make a cake for a wedding or making it diabetic-friendly. On the right side of the slide, there's a person in what appears to be a modern office setting with large windows and contemporary furniture.</sample>
    <sample id="322">Can LLMs do Constrained Language Planning? T5 (11B) Flan-T5 (11B) GPT-3 (175B) InstructGPT (175B) Accuracy All baselines achieve unsatisfactory results on planning for specific goals</sample>
    <sample id="323">The image contains a slide with the title "Can LLMs do Constrained Language Planning?" in bold red text. Below the title, there is a bar graph showing accuracy percentages for different models: T5 (11B), Flan-T5 (11B), GPT-3 (175B), Codex (175B), and InstructGPT (175B). The y-axis of the graph ranges from 0 to 60, labeled as Accuracy.

Below the graph, there is a statement that reads: "All baselines achieve unsatisfactory results on planning for specific goals."

In the top right corner of the image, there is a smaller inset video call window showing a person wearing glasses and headphones, sitting at a desk in an office environment.</sample>
    <sample id="324">The image contains a slide with the title "What types of errors do LLMs usually make in this task?" Below the title, there is a radar chart labeled with different error categories such as FE1: No constraint, SE1: Incomplete step(s), SE2: Repeated steps, and others. The text at the bottom reads: "The semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) can not be guaranteed." On the right side of the image, there is a video call interface showing a person in a room with white walls and large windows.</sample>
    <sample id="325">The image shows a slide from a presentation with the title "What types of errors do LLMs usually make in this task?" The main content includes a radar chart labeled with different error categories such as FE1: No constraint, SE1: Step(s) (step(s)), SE2: Repeated steps, and others. Below the chart, there is text that reads: "The semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) can not be guaranteed." On the right side of the image, there is a small video feed showing a person sitting at a desk in an office environment.</sample>
    <sample id="326">The image shows a slide from a presentation with the title "What kinds of goals do InstructGPT typically fail?" in bold red text. Below the title, there is a table that lists various categories such as Work, Sports, Relationships, Personal Care, Traditions, Hobbies, Health, Family Life, Education, Electronics, and Arts, along with corresponding numerical values for different metrics like 0.67, 1.0, etc., which likely represent performance or failure rates.

At the bottom of the slide, there is a statement in black text: "The planning performance of InstructGPTs varies considerably for goals of different categories."

On the right side of the image, there is a video call interface showing a person who appears to be presenting or discussing the content on the slide. The background behind the presenter includes some furniture and possibly other items, suggesting an indoor setting.</sample>
    <sample id="327">The image displays a presentation slide with the title "Method" at the top. The slide outlines an approach to generating specific goals using InstructGPT via in-context learning for abstract tasks like making a cake.

On the left side of the slide, there is a box labeled "Step 1: Generate specific goals with InstructGPT via in-context learning." This step involves creating detailed objectives based on more general instructions or prompts.

To the right, under the heading "Abstract Goal," it states "Make a cake," followed by a section titled "Specific Goals," which lists three examples:
- G1 (+ modifier): Make a chocolate cake
- G2 (+ method): Make it in a microwave
- G3 (+ intent): Make a cake for a wedding

These specific goals provide concrete details and variations that can be used as inputs for further processing or task execution.

In the background, there is a video call interface showing a person wearing glasses and a green shirt, sitting in what appears to be a modern office environment with large windows and furniture visible behind them. 

The overall layout suggests this is part of a larger presentation discussing methods for refining abstract concepts into actionable steps through structured goal generation techniques.</sample>
    <sample id="328">The image shows a presentation slide titled "Method" with the following content: 1. Input: an abstract goal 2. Abstract Goal: Make a cake Specific Goals: G1 (+ modifier) Chocolate cake G3 (+ method) Make in a microwave G3(+ intent): Make a cake for a wedding Step 1: Generate specific goals with InstructGPT via in-context learning The right side of the image features a person wearing glasses and a green shirt, sitting at a desk in what appears to be an office environment with large windows showing daylight outside.</sample>
    <sample id="329">The image shows a presentation slide titled "Method" with the subtitle "Input: an abstract goal." The slide outlines two steps in a process. Step 1 is labeled "Generate specific goals with InstructGPT via in-context learning," and it includes three examples of specific goals related to making a cake, such as adding chocolate chips or using a microwave. Step 2 is labeled "Over-generate candidate scripts with InstructGPT via in-context learning," which involves generating plans for these goals. On the right side of the slide, there are images depicting people working at desks in what appears to be an office environment. Additionally, there's a smaller inset video window showing a person wearing a green shirt who seems to be presenting this information.</sample>
    <sample id="330">The image shows a slide from a presentation titled "Method." It outlines three steps in a process: 1. Over-generate candidate scripts via in-context learning, represented by icons of robots and text boxes labeled "Step 2" and "Step 3." 2. Find the goal with InstructGPT via similarity score, illustrated with diagrams showing candidate scripts (labeled k) being filtered based on scores ranging from 0.3 to 0.1, resulting in selected scripts marked with check marks. The final output is specific goals corresponding to these scripts. A person appears partially visible at the top right corner of the image against an indoor background with tables and chairs.</sample>
    <sample id="331">The image shows a presentation slide titled "Method" with three main steps outlined. Step 2 involves over-generating candidate scripts via in-context learning, and Step 3 is about filtering these scripts to find the goal with InstructGPT using an IScore score. The output of this process includes specific goals along with corresponding scripts.

In the center of the slide, there's a diagram showing Candidate Scripts labeled from 1 to k, each associated with different probabilities (0.3, 0.2, etc.). Below that are Filtered Scripts, where some have been marked as filtered out while others remain, indicating successful matching based on criteria like probability scores or other evaluation metrics.

At the bottom right corner of the slide, there's an example script: "Script 3 - Gather your ingredients." This suggests how the final selected script might look after applying the described method.

On the right side of the frame, there's a person wearing glasses and a green shirt, seated at a desk in what appears to be a modern office setting with white walls and large windows allowing natural light into the room.</sample>
    <sample id="332">The image shows a presentation slide titled "Method" with detailed steps for generating and filtering candidate scripts using InstructGPT. The left side of the slide outlines three main steps: 1) Over-generate candidate scripts via in-context learning, 2) Find filters to align goals with InstructGPT via similarity score, and 3) Output specific goals along with corresponding scripts. On the right side, there's an illustration showing various candidate scripts (labeled from 1 to k), their similarities, and how they are filtered based on certain criteria. At the bottom, there's an example script labeled "Script 3," which reads: "1. Gather your ingredients 4. Add the cocoa powder." Additionally, there is a person visible in the top-right corner of the frame, likely presenting or participating in the meeting.</sample>
    <sample id="333">The image shows a presentation slide with the title "Our Method Greatly Improves the Planning Quality" in bold red text at the top. Below the title, there is a bar chart comparing different models based on their accuracy: T5 (11B), Flan-T5 (11B), GPT-3 (175B), InstructGPT (175B), and Our Method. The bars are color-coded to represent each model.

Below the chart, there is a statement that reads: "With our method, InstructGPT can generate scripts of higher quality by a large margin."

In the background, there is an inset video call showing a person sitting in what appears to be an office or meeting room setting. The individual has short hair and is wearing glasses and a green shirt.</sample>
    <sample id="334">The video features a presentation slide titled "Script Distillation from LLMs" with three main sections: Motivation, Method, and Input. The Motivation section explains the goal of enabling constrained language planning for smaller models. In the Method section, it details symbolic knowledge distillation by generating 55,000 scripts with constraints based on CoSprint Dataset, which are then annotated by humans to validate and test them.

On the right side of the frame, there is an inset showing a person in what appears to be an office setting, possibly giving a live demonstration or lecture related to the content displayed on the left. This setup suggests that the presenter might be explaining or elaborating on the script distillation process described in the slides.</sample>
    <sample id="335">The video presents a detailed explanation of the process for "Script Distillation from LLMs" (Large Language Models). The slide is divided into two main sections: Motivation and Method.

In the Motivation section, it explains that the goal is to enable constrained language planning ability using smaller models. This approach aims to enhance the efficiency and effectiveness of language processing tasks by reducing model size without compromising performance.

The Method section outlines three steps:

1. **Step 1**: Generate specific goals with InstructGPT via in-context learning.
2. **Step 2**: Over-generate candidate scripts with InstructGPT via in-context learning.
3. **Step 3**: Filter scripted goals to the target with InstructGPT via in-context similarity score.

Additionally, there are notes on following the idea of symbolic knowledge distillation, generating 55,000 scripts with constraints based on the method, annotating validation and test sets, and achieving output-specific results with corresponding plans.

On the right side of the frame, there is an individual who appears to be presenting or explaining this content. They are wearing glasses and seem to be speaking about the topic displayed on the left side of the screen.</sample>
    <sample id="336">The image shows a presentation slide titled "Script Distillation from LLMs" with three main sections: Motivation, Method, and Step 1. The Motivation section explains that the goal is to enable constrained language planning for smaller models. In the Method section, it mentions following symbolic knowledge distillation by generating scripts with constraints based on a Coscript Dataset. It also notes that these scripts are annotated for validation and test.

Step 1 details the process of inputting an abstract into GPT4 via in-context learning to generate specific goals. There's a visual representation showing this step as part of the overall script distillation process outlined in the slide.</sample>
    <sample id="337">The image shows a presentation slide titled "Script Distillation from LLMs" with three main sections: Motivation, Method, and an abstract flowchart. The Motivation section explains the goal of enabling constrained language planning for smaller models to achieve symbolic knowledge distillation. It mentions generating 55,000 scripts with constraints based on the method called "Coscript Dataset," which is then annotated by humans for validation and testing.

The Method section outlines the process in three steps:
1. Generate specific goals using InstructGPT via in-context learning.
2. Over-generate candidate scripts using InstructGPT with context.
3. Filter selected scripts to reach the goal using InstructGPT with a corresponding similarity score.

The final step involves annotating these filtered scripts for validation and test sets.

On the right side of the slide, there's a video call interface showing a person wearing glasses and headphones, seated at a desk in what appears to be an office environment.</sample>
    <sample id="338">The image shows a presentation slide titled "Script Distillation from LLMs" with three main sections: Motivation, Method, and Input. The Motivation section explains that the goal is to enable constrained language planning ability for smaller models using symbolic knowledge distillation. It mentions generating 55,000 scripts with constraints based on a method called Coscript Dataset and notes that human annotators validate and test these datasets.

The Method section outlines three steps:
1. Generate specific goals with InstructGPT via in-context learning.
2. Over-generate candidate scripts with instructions like "Write a script about [topic]" or "Create a script for [genre]" with context.
3. Filter scripted goals to the target by comparing them against an instructor GPT's instruction with a similarity score.

The Input section describes how this process works starting with abstract input.

Additionally, there is a video call overlay showing someone sitting at a desk in what appears to be an office environment.</sample>
    <sample id="339">The video begins with a slide titled 'Constraint Analysis' in bold red text. The main content of the slide is a pie chart divided into several segments, each labeled with different categories and percentages: 'Ingredient: 21.57%', 'Modifier: 45.05%', 'By: 17.87%', 'Method: 36.88%', 'Using: 15.45%', and 'Date: 10.39%'. Below the pie chart, there is a note that reads 'Coscript shows high heterogeneity and pluralism in the generated specific goals.' On the right side of the frame, a person wearing glasses and a green shirt appears to be speaking or presenting.

The presentation continues with another slide under the title 'Coscript for Smaller Language Models'. This slide lists two bullet points: 'Datasets - Coscript', 'Datasets - wikiHow', and 'Metrics - Faithful: DeBERTa (v3 large) model to decide whether the generated texts are faithful to the constraints', followed by 'Automatic metrics: ROUGE, BLEU, BERTScore'. Again, on the right side of the frame, a person wearing glasses and a green shirt appears to be speaking or presenting.

The final part of the video features a new slide titled 'Specialized Models vs. LLMs' in bold red text. It includes a bar graph comparing the accuracy of five models: GPT-3 (175B), Codex (175B), InstructGPT (175B), T5 trained on wikiHow, and T5 trained on Coscript. The y-axis represents accuracy, ranging from 0 to 75%. A note below the graph states 'Smaller LM fine-tuned on Coscript can generate higher quality scripts than LLMs.' On the right side of the frame, a person wearing glasses and a green shirt appears to be speaking or presenting.</sample>
    <sample id="340">The image shows a presentation slide titled "Specialized Models vs. LLMs" with the subtitle "Accuracy." The graph on the left compares different models: GPT-3 (175B), Codex (175B), InstructGPT (175B), T5 trained on wikiHow, and T5 trained on Coscript. Below the graph, there's text stating, "Smaller LM fine-tuned on Coscript can generate higher quality scripts than LLMs." On the right side of the image, there is a video call interface showing a person in an office setting.</sample>
    <sample id="341">The image shows a slide titled "Summary and Takeaways" with several bullet points. The first point establishes the constrained language planning problem. The second evaluates the ability of LLMs to over-generate and then filter for them, developing an over-generate-then-filter method for LLMs. It also mentions using LLMs to generate high-quality script datasets (CoScript) for constrained language planning.

The next section is labeled "Limitations and future work," which discusses that the proposed method for improving LLMs is post-hoc re-ranking approach. Additionally, it notes that CoScript only inherits from one extra constraint and can be valuable in advancing research on language planning with more complex goals and constraints.</sample>
    <sample id="342">The image shows a presentation slide titled "Summary and Takeaways" with the following points: 1. Establish the constrained language planning problem. 2. Evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs. 3. Use LLMs to generate a high-quality script dataset (CoScript) for constrained language planning. Limitations and future work include: - The proposed method for improving LLMs is a post-hoc re-ranking approach. - CoScript only inherits from an abstract one with one extra constraint. - CoScript dataset can be a valuable resource to advance research on language planning with more complex and diverse goals and constraints.</sample>
    <sample id="343">The 61st Annual Meeting of the Association for Computational Linguistics Toronto, Canada July 8-14, 2023 Distilling Script Knowledge from Large Language Models for Constrained Language Planning Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang syyuan21@m.fudan.edu.cn https://github.com/siyuuyuan/coscript</sample>
    <sample id="344">The authors do not specify how they decide what moderate-frequency words are. The slide only mentions that the random selection of n words is done within a "moderate-frequency" interval, without defining exactly which words fall into this category or how it's determined.</sample>
    <sample id="371">The image shows a presentation slide with the title "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems." The authors listed are Sarah E. Finch, James D. Finch, and Jinho D. Choi. Below the title, there are logos for Emory University, NLP Group, Emory NLP Research Lab, and Alexa. In the top right corner of the slide, there is a small photo of an individual wearing glasses.</sample>
    <sample id="372">The image shows a presentation slide with the title "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems" by Sarah E. Finch, James D. Finch, and Jinho D. Choi. Below the title are logos for Emory University, Emory NLP Research Lab, and Alexa. In the top right corner of the slide is an inset photo of a person wearing glasses.</sample>
    <sample id="373">The image is a screenshot of a presentation slide titled "Comparative Evaluation." The slide features the logos of Emory University and Alexa at the bottom. In the upper right corner, there is a small video feed showing a person.

On the left side of the slide, there are two speech bubbles with icons representing chatbots or conversational agents. One bubble has an empty gray shape inside it, while the other contains text indicating some form of interaction between the user (represented by a silhouette) and the chatbot.

On the right side of the slide, another set of four speech bubbles appears. These bubbles have different colors: one blue, one purple, one light pink, and one white. Each bubble also includes icons for chatbots or conversational agents, suggesting multiple interactions or comparisons being evaluated in this comparative analysis.

The overall layout indicates that the slide is part of a presentation discussing the evaluation of various conversational systems or chatbots, likely comparing their performance or effectiveness through visual representations of dialogues.</sample>
    <sample id="374">The video begins with a slide titled "Comparative Evaluation" from Emory University, featuring the Alexa logo. The slide shows two groups of characters: one group on the left with blue speech bubbles and another on the right with purple speech bubbles. Each character is accompanied by a robot icon within their respective color-coded bubble. Below these groups, there are empty circles indicating spaces for additional elements or actions.

As the video progresses, an illustration of a judge holding a gavel appears at the bottom center of the screen, symbolizing evaluation or judgment. A green checkmark then appears next to the group on the right, suggesting that this group has passed some form of evaluation or met certain criteria.

The scene transitions smoothly into the next segment, which continues under the title "Likert Rating Evaluation." This part features a single group of characters with blue speech bubbles, again each accompanied by a robot icon. At the bottom center, a Likert scale ranging from 1 to 5 is displayed, representing different levels of rating or feedback. An illustration of a judge holding a gavel reappears below the speech bubbles, reinforcing the theme of evaluation. Throughout this sequence, the background remains consistent, maintaining focus on the comparative evaluation process between the two groups using both visual indicators (speech bubbles) and quantitative measures (Likert scale).

The video wraps up with a clear emphasis on evaluating and comparing the performance or responses of the two groups through structured methods like Likert scales and symbolic representations such as judges' illustrations.</sample>
    <sample id="375">The video begins with a slide titled 'Likert Rating Evaluation' from Emory University, featuring the Alexa logo. The slide includes an illustration of two characters engaged in conversation, represented by speech bubbles and icons depicting people within blue boxes. Below this, there is a Likert scale ranging from 1 to 5, marked at 3 with a green checkmark indicating selection.

Next, the title changes to 'Dimensions of Dialogue Quality,' also from Emory University, again showing the Alexa logo. A pink box labeled 'Dialogue Quality' appears on the slide without any additional elements or illustrations.

Following this, another change occurs where three gray arrows point downward towards a central text that reads 'Emotional Understanding.' To the left and right of this central text are two other texts: 'Relevance' on the left and 'Consistency' on the right. These terms suggest different dimensions contributing to dialogue quality.

Finally, the last frame reiterates the same content as before but adds an image of a person in the top-right corner of the screen, possibly for identification purposes during a presentation. Throughout these frames, the logos of Emory University and Amazon's Alexa remain consistent in their positions at the bottom corners of each slide.</sample>
    <sample id="376">The image shows a slide from a presentation titled "Likert Rating Evaluation." The title is displayed at the top of the slide in white text on a blue background. Below the title, there are several visual elements: 

1. An illustration of a person with black hair.
2. Three speech bubbles containing icons representing different devices (a smartphone and two tablets).
3. A scale ranging from 1 to 5, indicating a rating system.
4. A green checkmark next to the number 3 on the scale.

At the bottom left corner of the slide, there is an illustration of a judge holding a gavel and scales of justice. In the bottom right corner, logos for Emory University and Alexa are visible. Additionally, there is a small video feed in the upper right corner showing a person wearing a light-colored shirt.

The main content of the slide appears to be instructing viewers to rate the relevance of responses given by a bot using the Likert scale provided.</sample>
    <sample id="377">The image contains a slide from a presentation with the title "Likert Rating Evaluation" at the top. Below this, there is an illustration of two characters engaged in conversation, represented by speech bubbles and icons depicting smartphones or tablets.

At the bottom left corner of the slide, there is an icon of Lady Justice holding scales and a gavel, symbolizing fairness and judgment. To the right of this icon, there is a scale ranging from 1 to 5, labeled "Rate the relevance of the bot's responses." A green checkmark indicates that a rating has been selected on this scale.

In the background, logos for Emory University and Alexa are visible, suggesting that the content may be related to a study or evaluation involving these entities. Additionally, there is a small video feed showing a person in the upper right corner of the slide, likely indicating that this is part of a live presentation or webinar.</sample>
    <sample id="378">The image shows a presentation slide titled "Annotating Behaviors in Chat (ABC-Eval)" with Emory University and Alexa logos at the bottom. The main content features several speech bubbles connected to icons representing different behaviors, such as "Irrelevant," "Lack of Empathy," and "Self Contradiction." There are also small profile pictures on the right side of each behavior label.</sample>
    <sample id="379">The image shows a slide from a presentation titled "Annotating Behaviors in Chat (ABC-Eval)." It features an illustration with several speech bubbles and icons representing people, along with labels indicating different types of behaviors: "Irrelevant" and "Lack of Empathy Self Contradiction." The logos for Emory University and Alexa are visible at the bottom left corner. In the top right corner, there is a small video feed showing a person speaking or presenting.</sample>
    <sample id="380">The image shows a slide from Emory University titled "ABC-Eval Behaviors." It is divided into four quadrants labeled: Coherence, Consistency, Knowledge, and Emotional Understanding. The bottom left corner features the Emory University logo with the text "EMORY UNIVERSITY" next to it, while the bottom right corner has an Alexa logo.</sample>
    <sample id="381">The image shows a slide titled "ABC-Eval Behaviors" from Emory University, featuring four main categories: Coherence, Knowledge, Consistency, and Emotional Understanding. In the category of Coherence, there are two sub-points listed as "Ignoring Partner" and "Irrelevant." The logos for Emory University and Alexa appear at the bottom corners of the slide.</sample>
    <sample id="382">The image shows a slide titled "ABC-Eval Behaviors" from Emory University. It is divided into four quadrants: Coherence, Knowledge, Consistency, and Emotional Understanding. The first quadrant under Coherence lists "Ignoring Partner" and "Irrelevant." In the second quadrant of Knowledge, it initially reads "Incorrect Fact," which later changes to "Commonsense Violation." Under Consistency, there are two items listed as "Self Contradiction" and "Partner Contradiction." Finally, in the fourth quadrant for Emotional Understanding, it starts with an empty space but ends up listing "Empathetic Response" and then switches back to "Lack of Empathy."</sample>
    <sample id="383">The content of the image is as follows:

1. **Title**: Experiments
2. **Body Text**:
   - 4 Open-Domain Dialogue Models
   - 100 Human-Bot Conversations per Model

3. **Subsections with Labels and Examples** (from left to right, top to bottom):
   - **Knowledge**
     - Incorrect Fact
     - Commonsense Violation
   - **Consistency**
     - Self Contradiction
     - Partner Contradiction
   - **Emotional Understanding**
     - Empathetic Response
     - Lack of Empathy

4. **Logos at the Bottom**:
   - Emory University
   - Alexa logo

5. **Additional Section**:
   - ABC-Eval: An illustration depicting a conversation flow between a human and an AI system.

This layout provides a structured overview of experimental setups involving open-domain dialogue models and their interactions through various conversational scenarios.</sample>
    <sample id="384">The slide is titled 'Experiments' and includes the following points: - 4 Open-Domain Dialogue Models - 100 Human-Bot Conversations per Model There are also three sections labeled 'ABC-Eval,' 'Turn Likert,' and 'Dialogue Likert.' Each section contains diagrams illustrating interactions between humans, dialogue models (labeled as 'Bot'), and feedback mechanisms. The bottom of the slide features logos for Emory University and Alexa.</sample>
    <sample id="385">The video begins with a slide titled 'Experiments' in bold white text on a dark blue background. Below the title, there are two bullet points: "4 Open-Domain Dialogue Models" and "100 Human-Bot Conversations per Model." The Emory University logo is visible at the bottom left corner of the slide.

Next, another slide appears under the heading 'Baseline Evaluations.' This slide features three sections labeled 'Turn Likert,' 'Dialogue Likert,' and 'Comparative,' each accompanied by illustrations showing human and robot interactions through speech bubbles and checkmarks indicating successful evaluations. At the bottom center of this slide, the word 'Consistency' is displayed within a pink rectangle.

Following that, additional elements appear below the previous section's rectangles. Two new sets of labels emerge: one set includes 'Consistency,' 'Emotional Understanding,' 'Informativeness,' and 'Overall Quality'; the other set lists 'Engagingness,' 'Grammaticality,' 'Proactivity,' and 'Relevance.' These labels provide further criteria for evaluating the dialogue models.

Finally, the last frame shows no changes from the previous description, maintaining the same layout and content as before. The consistent appearance across these frames suggests they are part of an academic presentation or lecture discussing experiments related to open-domain dialogue models evaluated using various metrics such as consistency, emotional understanding, informativeness, engagingness, grammaticality, proactivity, relevance, overall quality, turn likelihood, dialog likelihood, and comparative analysis.</sample>
    <sample id="386">The image shows a slide titled "Inter-Annotator Agreement" with a graph below it. The graph plots Krippendorf's Alpha values against different types of Likert scales and comparisons, such as ABC-Eval, Turn Likert, Dialogue Likert, and Comparative. Each type is represented by a series of colored dots (orange for ABC-Eval, blue for Turn Likert, red for Dialogue Likert, green for Comparative). There are two yellow arrows pointing to the higher end of the scale on either side of the graph.

In the top right corner, there is an inset showing a person in a small window. At the bottom left corner, logos for Emory University and Alexa are visible.</sample>
    <sample id="387">The image displays a bar chart titled "Predictive Validity" with the subtitle "% of Quality Explained (R²)." The x-axis lists different types of evaluations or assessments, including ABC-Eval, Turn Likert, Dialogue Likert, and Comparative. Each category has two bars representing Interactive Qua1 and Interactive Qua2.

The y-axis shows the percentage scale from 0 to 0.3 in increments of 0.04. Various colors are used for each pair of bars within categories: orange for other, green for interactive, blue for turn, red for dialogue, purple for comparative, yellow for evaluation, pink for interaction, light blue for communication, dark blue for assessment, teal for feedback, brown for evaluation, gray for interaction, black for comparison, and olive for feedback.

In the top right corner, there is an inset showing a person's face, likely indicating that this is part of a presentation slide. At the bottom left corner, the logo of Emory University is visible along with the Alexa logo on the bottom right corner.</sample>
    <sample id="388">The image shows a bar chart titled "Predictive Validity" with the subtitle "% of Quality Explained (R²)." The chart compares Interactive Qua,1 and Interactive Qua,2 across different categories: ABC-Eval, Turn Likert, Dialogue Likert, Comparative. Each category has multiple bars representing various subcategories like Other, Contextual, Interpersonal, etc., each color-coded for clarity.

The x-axis lists these categories while the y-axis indicates the percentage scale from 0 to 0.1 in increments of 0.02. Yellow arrows point out specific values on the graph, highlighting certain data points within the bars. At the bottom left corner is the Emory University logo, indicating an affiliation or source of the presentation content. In the top right corner, there's a small inset showing a person wearing glasses, suggesting this might be part of a virtual meeting or webinar interface.</sample>
    <sample id="389">The image contains a slide titled "Incremental Validity" with the following content:

- A graph showing "% of Quality Explained (R²)" on the y-axis and three categories labeled "ABC-eval," "Turn UBERT," and "Dialogue UBERT" on the x-axis.
- The graph includes several data points connected by lines, each labeled with terms such as "Unempathetic," "Relevant," "Proactive," "Engaging," "Jeff Conn," "Redundant," etc.

At the bottom left corner:
- Emory University logo
- Alexa logo

The background is white, and there are no additional texts or elements visible in the image.</sample>
    <sample id="390">The image shows a slide titled "Incremental Validity" from Emory University. The main content of the slide is a graph with three axes labeled "ABC-eval," "Turn UBERT," and "Dialogue UBERT." The y-axis represents "% of Quality Explained (R²)." Various terms such as "Unempathetic," "Relevant," "Proactive," "Emotion," "Jeff Connah," and others are plotted on this graph, indicating their relative positions in explaining quality across these dimensions.

In the top right corner, there's an inset showing a person wearing glasses. At the bottom left corner, the Emory University logo is visible along with some additional text that appears to be part of a presentation or lecture series branding. In the bottom right corner, there's another logo for Alexa, suggesting a connection between the topic being discussed and Amazon's AI assistant technology. Throughout the video, no changes occur; the same data points remain displayed without any alterations or movements.</sample>
    <sample id="391">The image shows a slide titled "Incremental Validity" from Emory University, with the Alexa logo in the bottom right corner. The main content is a graph plotting "% of Quality Explained (R²)" on the y-axis against three different evaluation methods: ABC-eval, Turn UBERT, and Dialogue UBERT along the x-axis.

The graph includes several labeled points such as:
- Unempathetic
- Redundant
- Jeff Conn
- Proactive
- Engaging

There are also arrows indicating transitions between these labels, suggesting relationships or progressions among them within each method category. A yellow arrow highlights one specific transition point at the intersection of "Proactive" and "Engaging."

At the top right corner of the image, there's an inset showing a person wearing glasses, possibly presenting or referencing this slide during a lecture or presentation.</sample>
    <sample id="392">The image displays a bar chart titled "ABC-Eval Error Rates by Model." The x-axis lists different models: BART-FID-RAG, Blender2, Emora, and Blender-Decode. The y-axis represents the percentage of turns, ranging from 0 to 30%. Various categories are labeled along the top in orange text, including Antisocial, CS Contra, Ignore, Incorrect, Unempathetic, Other Contra, Redundant, Self Contra, Topic Switch, and Uninterpret. Each model has corresponding bars for each category, showing varying error rates across these categories. In the bottom left corner, there is an Emory University logo with the text "EMORY" underneath it. On the right side of the image, there is a small inset video window displaying a person's face.</sample>
    <sample id="393">The image shows a bar chart titled "ABC-Eval Error Rates by Model." The y-axis is labeled "% of Turns" and ranges from 0 to 30. There are multiple bars representing different models: BART-FID-RAG, Blender2, Emora, and Blender-Decode. Each model has several categories listed along the x-axis with corresponding error rates for each category:

1. Antisocial
2. CS Contra.
3. Ignore
4. Incorrect
5. Irrelevant
6. Unempathatic
7. Other Contra.
8. Redundant
9. Self Contra.
10. Topic Switch
11. Uninterpret.

A yellow arrow points to one of the bars in the "CS Contra." section on the left side of the chart. In the top right corner, there's an inset showing a person wearing glasses. At the bottom left, logos for Emory University and Alexa are visible.</sample>
    <sample id="394">The image shows a bar chart titled "ABC-Eval Error Rates by Model." The x-axis lists different error categories such as Antisocial, CS Contra, Ignore, Incorrect, Unempathetic, Other Contra, Redundant, Self Contra, Topic Switch, and Uninterpret. Each category has multiple bars representing various models: BART-FID-RAG, Blender2, Emora, and Blender-Decode.

The y-axis represents the percentage of turns (in %), ranging from 0 to 30%. Different colors are used for each model's data:

- Blue for BART-FID-RAG
- Green for Blender2
- Purple for Emora
- Light blue for Blender-Decode

Yellow arrows point to certain areas on the chart, highlighting specific trends or values within the data. In the top right corner, there is an inset showing a person in a video call interface with the logo "alexa" at the bottom right. At the bottom left, logos for Emory University and another entity are visible.</sample>
    <sample id="395">The image displays a bar chart titled "ABC-Eval Error Rates by Model." The x-axis of the chart lists various error categories, including Antisocial, CS Contra., Ignore, Incorrect, Irrelevant, Unempathetic, Other Contra., Redundant, Self Contra., Topic Switch, and Uninterpret. Each category has corresponding bars representing different models: BART-FID-RAG, Blender2, Emora, and Blender-Decode. These bars are color-coded to differentiate between the four models.

The y-axis indicates the percentage of turns with errors, ranging from 0% to 30%. Notably, there is a significant variation in error rates across different categories for each model. For instance, the 'Topic Switch' category shows particularly high error rates, especially for one of the models (Blender-Decode), which stands out due to its much taller bar compared to others.

At the bottom left corner of the image, logos or names associated with Emory University and Alexa are visible, suggesting an academic context or collaboration related to this evaluation. Additionally, at the top right corner, there appears to be a small inset showing part of another person's face, likely indicating that someone might be presenting or discussing the data shown in the chart during a presentation or lecture.</sample>
    <sample id="396">The image contains a slide from a presentation titled "ABC-Eval Error Rates by Model." The chart displays error rates for various models across different categories such as Antisocial, CS Contra, Ignore, Incorrect, Irrelevant, Unempathetic, Other Contra, Redundant, Self Contra, Topic Switch, and Uninterpret. Each category has bars representing the performance of five models: BART-FID-RAG, Blender2, Emora, and Blender-Decode.

Below the chart, there is additional information including logos for Emory University and Alexa, along with contact details and URLs:
- Paper: https://arxiv.org/pdf/2212.09180.pdf
- GitHub: https://github.com/emorynlp/ChatEvaluationPlatform
- Contact Info: {sfillwo, jdfinch, jinho.choi} @ emory.edu
- Website: https://www.emorynlp.org

The bottom right corner features an Amazon logo, indicating some form of association or sponsorship.</sample>
    <sample id="397">The approach uses 10-millisecond speech segment size.</sample>
    <sample id="398">The entity-specific knowledge needed is that Servin is a judge.</sample>
    <sample id="399">Example quality is more important than similarity to source sentence.</sample>
    <sample id="400">The paper focuses on the following language models in its extended experiments: BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, distilRoBERTa, ALBERT-base, ALBERT-large, BART-base, and Alpaca.</sample>
    <sample id="401">The model combines the attention scores from several layers.</sample>
    <sample id="402">"easy on me," "the first one" and "the newer one. The song that's not energetic."</sample>
    <sample id="403">The affiliations of the authors are:</sample>
    <sample id="404">There are six authors involved in the paper.</sample>
    <sample id="405">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="406">The authors gave the example of a "woman warrior" to illustrate how marked groups differ from unmarked ones.</sample>
    <sample id="407">CNNs and RNNs</sample>
    <sample id="408">FT_C, LORAC, BitFitFC, AdapterC</sample>
    <sample id="409">There are six authors involved in the paper.</sample>
    <sample id="410">The author works with multiple modalities, including text and video.</sample>
    <sample id="411">DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains Yanis Labrak1,4 Adrien Bazege2,3 Richard Dufour2 Michael Rouvier1 Emmanuel Morin2 Béatrice Daille2 Pierre-Antoine Gourraud3 (1) LIA, Avignon Université (2) LS2N, Nantes Université (3) Clinique des diaboliques, CHU de Nantes (4) Zenidoc: LABORATOIRE DE BIOINFORMATIQUE ET DE GENOMIQUE DES ORGANISME LABORATORY OF BIOINFORMATICS AND GENOMICS OF THE ORGANISM GENCI AVIGNON UNIVERSITE AVIGNON UNIVERSITY</sample>
    <sample id="412">The image contains a summary of topics related to language modeling in healthcare. The text is organized into four main points:

I. Language Modeling in Healthcare
II. Comparison of pre-training strategies, data sources and sizes
III. Evaluation of 13 models on 11 tasks
IV. Distribution of NACHOS and DrBERT

At the bottom right corner, there is a logo with "Avignon Université" written next to it.</sample>
    <sample id="413">The image shows a presentation slide with the title "Summary" in red text at the top. The content of the slide is as follows:

1. Language Modeling in Healthcare
2. Comparison of pre-training strategies, data sources and sizes
3. Evaluation of 13 models on 11 tasks
4. Distribution of NACHOS and DrBERT

At the bottom right corner of the slide, there is a logo for Avignon Université. In the upper right corner of the image, there is a small video feed showing a person who appears to be presenting or participating in a virtual meeting.</sample>
    <sample id="414">The image contains a slide with the following text: "Summary" followed by four bullet points. The first point is "I. Language Modeling in Healthcare." The second point reads "II. Comparison of pre-training strategies, data sources and sizes." The third point says "III. Evaluation of 13 models on 11 tasks." And the fourth point states "IV. Distribution of NACHOS and DrBERT."

In the bottom right corner, there is a logo that includes an illustration of two stylized figures holding hands inside a yellow circle, next to which it says "Avignon Université."</sample>
    <sample id="415">The image contains a summary of topics related to language modeling in healthcare. The text is structured as follows:

1. Language Modeling in Healthcare
2. Comparison of pre-training strategies, data sources and sizes
3. Evaluation of 13 models on 11 tasks
4. Distribution of NACHOS and DrBERT

At the bottom right corner, there is an affiliation with "Avignon Université."</sample>
    <sample id="416">The image shows a slide from a presentation titled "Language Modeling." The content of the slide discusses various aspects related to language modeling in natural language processing (NLP). Here is a detailed breakdown:

1. **Title**: Language Modeling

2. **Main Points**:
   - Transformer-based approaches, such as BERT, offer huge performance gains on many NLP tasks.
   - These models have been adapted to French with CamemBERT and FlauBERT.
   - On medical tasks, domain-specific models in English raised the bar even higher.

3. **Subpoints under Medical Tasks**:
   - Examples include PudMedBERT, BioBERT, ClinicalBERT, and others.

4. **Additional Information**:
   - Languages other than English are rarer and rely primarily on continual pre-training using an existing generic model.
   - No open-source model is available for biomedical domains in French yet.
   - A BERT-based domain-specific model for French should improve performance on medical tasks.

5. **Footer**:
   - There is a logo at the bottom left corner that appears to be associated with Avignon Université.

This slide provides insights into the advancements and challenges in developing specialized language models for different languages and specific applications like medicine.</sample>
    <sample id="417">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raised the bar even higher • PubMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model Unlike generic models, no open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="418">**Title: Language Modeling**

- Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks
- Has been adapted to French with CamemBERT and FlauBERT
- On medical tasks, domain-specific models in English raised the bar even higher
  - PudMedBERT, BioBERT, ClinicalBERT and other
- Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model
- Unlike generic models, no open-source model is available for biomedical domain in French yet
- BERT-based domain specific model for French should increase performance on medical tasks

**Footer:** Avignon Université</sample>
    <sample id="419">The image contains a slide from a presentation titled "Language Modeling." The content of the slide is as follows:

- Transformer-based approaches, such as BERT, offer huge performance gains on a lot of NLP tasks.
- Has been adapted to French with CamemBERT and FlauBERT.
- On medical tasks, domain-specific models in English raised the bar even higher:
  - PudMedBERT, BioBERT, ClinicalBERT and other
- Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model.
- Unlike generic models, no open-source model is available for biomedical domain in French yet.
- BERT-based domain specific model for French should increase performance on medical tasks.

At the bottom right corner of the slide, there is text that reads "Avignon Université."

Additionally, there is a small video feed showing a person speaking at the top right corner of the slide.</sample>
    <sample id="420">The image contains a slide from a presentation titled "Comparison of pre-training strategies and data sources." The content is divided into two main sections:

1. **Evaluation of the impact of public and private medical data sources on comparable data sizes:**
   - **NACHOS:** A 1.1B+ word open-source dataset of heterogeneous data crawled from various domains, including medical records, natures, and styles.
   - **NBDW:** A private dataset consisting of sentences taken from 1.7M anonymized medical records sourced from the Nantes University Hospital data warehouse.

2. **Comparison of learning strategies:**
   - From scratch with full model construction
   - Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model; and PubMedBERT, an English medical one)

On the right side of the slide, there are tables comparing different datasets:
- **Datasets:**
  - NACHOS
  - NBDW (small)
  - NBDW (medium)
  - NBDW (large)
- **Columns in the table include:**
  - Name
  - Size (in GB)
  - References

At the bottom of the slide, there is a logo for Avignon Université.</sample>
    <sample id="421">Comparison of pre-training strategies and data sources

- Evaluation of the impact of public and private medical data sources on comparable data sizes
  - NACHOS: A 1.18B+ words open-source dataset of heterogeneous data crawled from diverse medical domains, including 4 GB, 6 MB, 25 M, 63 M.
  - NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.

- Comparison of learning strategies:
  - From scratch with full model construction
  - Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model, and PubMedBERT, an English medical one)

[Table]
| Corpus | Size | References |
|--------|------|------------|
| NACHOS | 1.18B+ | [1] |
| NBDW | 4 GB | [2] |
| NBDW | 6 MB | [3] |
| NBDW | 25 M | [4] |
| NBDW | 63 M | [5] |

[Table]
| Model | Strategy | Corpus |
|-------|----------|--------|
| DeBERTa | From scratch | PubMed |
| RoBERTa | From scratch | PubMed |
| CamemBERT | Continual pre-training | NACHOS |
| PubMedBERT | Continual pre-training | NBDW |

Avignon Université</sample>
    <sample id="422">The image shows a slide from a presentation titled "Comparison of pre-training strategies and data sources." The content is divided into two main sections:

1. **Evaluation of the impact of public and private medical data sources on comparable data sizes:**
   - NACHOS: A 1.1B+ words open-source dataset of heterogeneous data crawled from various domains, including medical records in four different languages.
   - NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.

2. **Comparison of learning strategies:**
   - From scratch with full model construction
   - Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model, and PubMedBERT, an English medical one)

On the right side of the slide, there is a table comparing datasets:
- **Dataset:** NACHOS, NBDW (small), NBDW (medium), NBDW (large)
- **Size:** 4 MB, 25.3 MB, 90.6 MB, 485.1 MB
- **Resources:** 1 GPU day, 4 GPUs days

At the bottom left corner, there's a logo for Avignon Université. In the top right corner, there is a small video feed showing a person speaking or presenting.</sample>
    <sample id="423">The image contains a slide from a presentation titled "Comparison of pre-training strategies and data sources." The content is divided into two main sections:

1. **Evaluation of the impact of public and private medical data sources on comparable data sizes:**
   - NACHOS: A 1.18B+ words open-source dataset of heterogeneous data crawled from various medical domains, including 4 GB of medical images, natures, and styles.
   - NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.

2. **Comparison of learning strategies:**
   - From scratch with full model construction
   - Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model, and PubMedBERT, an English medical one)

On the right side of the slide, there is a table comparing different models:
- **Model:** Lists models such as BERT, RoBERTa, DistilBERT, and their variants like BERT-base, BERT-large, RoBERTa-large, etc.
- **Strategy:** Indicates whether each model was trained from scratch or through continual pre-training.
- **Corpus:** Specifies which datasets were used for training each model.

At the bottom left corner of the slide, there is a logo with text that reads "Avignon Université."

In the top-right corner of the image, there is a small video feed showing a person who appears to be presenting the material.</sample>
    <sample id="424">The image is a slide from a presentation titled "Comparison of pre-training strategies and data sources." It evaluates the impact of public and private medical data sources on comparable size datasets. The slide mentions two main sections: 

1. **Evaluation of the impact of public and private medical data sources on comparable size datasets**:
   - NACHOS: A 1.1B+ words open-source dataset of heterogeneous data crawled from various domains, including medical records.
   - NBDW: A private dataset consisting of sentences taken from anonymized medical records.

2. **Comparison of learning strategies**:
   - From scratch with full model construction
   - Continual pre-training using an existing pre-trained model (e.g., CamemBert, a French generic model; and PubMedBERT, an English medical one)

Additionally, there is a table comparing different models based on their sizes, resources needed for training, and references:

- **Model**: Size | Resources Needed | References
- **DeBERTa**: 4 GB | 64 M | 
- **BERT**: 50 MB | 3 M | 
- **BERT-nnd**: 98 MB | 7 M | 
- **BERT-nnd (private)**: 45 MB | 4 M | 

The bottom right corner has the logo of Avignon Université.</sample>
    <sample id="425">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources on comparable data sizes NACHOS: A 1.1B+ word open-source dataset of heterogeneous data crawled from multiple medical domains, natures and styles NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse Comparison of learning strategies From scratch with full model construction Continual pre-training using an existing pre-trained model (e.g., CamemBert), a French generic model, and PubMedBERT, an English medical one Avignon Université</sample>
    <sample id="426">The image contains a slide from a presentation with the title "Comparison of pre-training strategies and data sources." The content is divided into two main sections:

1. **Evaluation of the impact of public and private medical data sources on comparable data sizes:**
   - NACHOS: A 1.1B word open-source dataset of heterogeneous data crawled from various medical domains, natures, and styles.
   - NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.

2. **Comparison of learning strategies:**
   - From scratch with full model construction
   - Continual pre-training using an existing pre-trained model (e.g., CamemBert, a French generic model; and PubMedBERT, an English medical one)

On the right side of the slide, there is a table comparing different datasets:
- **Dataset:** NACHOS, NBDW (small), NBDW (medium), NBDW (large)
- **Size:** Varies in GB (4GB for small, medium, large; 6GB for NACHOS)
- **References:** Varies

At the bottom of the slide, there is a logo or watermark that reads "Avignon Université."

In the top-right corner of the image, there is a person who appears to be presenting or discussing the content of the slide.</sample>
    <sample id="427">The image shows a slide from a presentation titled "Comparison of pre-training strategies and data sources." The slide is divided into two main sections: 

1. **Evaluation of the impact of public and private medical data sources on comparable data sizes**:
   - NACHOS: A 1.1B+ subword open-source dataset of heterogeneous data crawled from diverse medical domains, natures, and styles.
   - NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.

2. **Comparison of learning strategies**:
   - From scratch with full model construction
   - Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model, and PubMedBERT, an English medical one)

On the right side of the slide, there's a table comparing different datasets:

- **Dataset**: Subword (subword)
- **Size**: 4 GB for both NACHOS and NBDW
- **References**: 25.3 MB for NACHOS, 6.0 MB for NBDW
- **Format**: NACHOS is in .subword format; NBDW is in .nbt format

At the bottom left corner of the slide, there's a logo or emblem that reads "Avignon Université."

In the top-right corner of the image, there is a small video feed showing a person speaking, likely presenting this information.</sample>
    <sample id="428">The image contains a slide from a presentation titled "Evaluation: Data sources and size." The content of the slide is as follows:

- Bullet points:
  - Performance evaluation of 13 models on 11 tasks, both public and private.
  - Our fine-tuned models get state-of-the-art results on almost all tasks.

- A table with multiple columns labeled:
  - aIF
  - aIF Medical Report
  - MUSCIA
  - DISEASE
  - METTA
  - DISA
  - CAS
  - PRETSMEDICOM
  - QUADRO-EMERA-MEDLINE

- Rows in the table include categories such as General and Clinical. Each row lists various datasets or models along with numerical values under different headers like NCRS, CLS, CLE, CLS, POS, POS, and EMER.

- At the bottom right corner of the slide, there is text that reads "Avignon Université."

This detailed description covers the structure and contents of the slide presented in the image.</sample>
    <sample id="429">The video presents a slide from a presentation, likely related to an academic or research setting. The title of the slide is "Evaluation: Data sources and size," indicating that it discusses the evaluation criteria for data sources and their sizes in various tasks.

The main content includes two bullet points:
1. Performance evaluation of 13 models on 10 tasks, both public and private.
2. Fine-tuned models achieve state-of-the-art results on almost all tasks.

Below these bullet points, there is a detailed table with multiple columns labeled as follows:
- aIF
- Medical Report
- MUSCIA
- DISEASE
- MUSCIA-DISEASE
- CAS
- Preclinical
- QUADRO-EAMO-MEDLINE

Each column contains sub-columns such as NER (Named Entity Recognition), CLS (Class), POS (Part-of-Speech), and others specific to each task category like General, Biobio-1.0, Biobio-1.0_large, Clinical, and Chembio-1.0.

The rows under each task category list different datasets or experiments with corresponding performance metrics across the mentioned categories. For example, under the 'General' category, datasets include "CambridgeHART 158 GB" and "CambridgeHART 46 GB," among others, with associated scores for NER, CLS, POS, etc.

At the bottom right corner of the slide, there is a logo of Avignon Université, suggesting that this presentation might be affiliated with or conducted by someone from this institution.

Overall, the slide provides a comprehensive overview of how different models perform across various datasets and tasks, emphasizing the use of fine-tuning techniques to achieve high accuracy rates.</sample>
    <sample id="430">The image contains a detailed table and text under the heading "Evaluation: Data sources and size." The main points are:

- Performance evaluation of 13 models on 10 tasks, both public and private.
- Fine-tuned models achieve state-of-the-art results on almost all tasks.

The table lists various datasets (aIFH, Medical Report, MUSCIA, DISEAT, MUSCIA-DETEKT, CAS, preActMOCOMA, QUADRO-EMOA-MEDLINE) along with their corresponding NER (Named Entity Recognition) scores for different categories such as CER, CER+NER, CLS, CLS+NER, POS, POS+NER. Specific examples include:
- AIFH dataset has high scores across all categories.
- Medical Report shows lower but still significant scores in most categories.

The bottom right corner features the logo of Avignon Université.</sample>
    <sample id="431">The image contains a slide from a presentation titled "Evaluation: Data sources and size." The content of the slide is as follows:

- **Title:** Evaluation: Data sources and size
- **Bullet Points:**
  - Performance evaluation of 13 models on 10 tasks, both public and private.
  - Our fine-tuned models get state-of-the-art results on almost all tasks.

- **Table:**
  - Columns are labeled:
    - aIF
    - aIF Medical Report
    - MUSCA
    - DISEASE
    - MUSCAT
    - CAS
    - PRETSMEDICOM
    - QUADRO-EMERA-MEDLINE

  - Rows include different datasets or conditions such as:
    - General
    - Biobank
    - Clinical

  - Each row has sub-columns with performance metrics for each task (CRS, CER, NCRS, CER, NCRS, POS, NEG).

- **Footer:**
  - Avignon Université logo at the bottom right corner.

This detailed description captures the structure and contents of the slide presented in the image.</sample>
    <sample id="432">**Title:**
Evaluation: Pre-training strategies

**Body Text:**
- From scratch vs. continual pre-training on 4GB of data
- Question-answering tasks require more domain-specific knowledge to be able to work well.
- A study of model stability shows a higher inter-run variability for the CAMBert-based models trained using continual pre-training.

**Table:**
The table presents various metrics related to different datasets and models, including:
- Dataset (e.g., alif, afff)
- Metrics such as NER, CER, NER+NER, CER+NER, F1, and others specific to each dataset like Medical Spot, Special Report, MISC, MEDSCT, MEDSC, MEDSC-CT, MEDSC-CT-MEDSC, and more.
- Specific values are provided under columns labeled "NER," "CER," etc., with some highlighted in red indicating lower or better performance scores.

**Footer:**
Avisson Université logo at the bottom right corner.</sample>
    <sample id="433">The image is a slide from an academic presentation, likely related to machine learning or artificial intelligence. The title of the slide reads "Evaluation: Pre-training strategies." Below the title, there are three bullet points summarizing key findings:

1. It discusses starting from scratch versus continual pre-training on 4GB of data.
2. It states that question-answering tasks require more domain-specific knowledge to work well.
3. A model's stability shows higher inter-run variability for the CAMBert-based models trained using continual pre-training.

Below these bullet points, there is a detailed table comparing various models and their performance across different datasets (alif, alif, Medical Report, Speciality, Medical Report, MEDSCAT, MEDSCAT, MEDSCAT, MEDSCAT, MEDSCAT). The columns in the table include metrics such as NER, CER, SRL, and others with corresponding values for each dataset and model.

At the bottom right corner of the slide, there is a logo indicating affiliation with Avignon Université. In the top right corner of the image, there is a small video feed showing a person who appears to be presenting this content.</sample>
    <sample id="434">The image contains a slide titled "Evaluation: Pre-training strategies" from Avignon Université. The content of the slide is as follows:

- Evaluation: Pre-training strategies
  - From scratch vs. continual pre-training on 4GB of data
  - Question-answering tasks require more domain-specific knowledge to be able to work well.
  - A study of model stability shows a higher inter-run variability for the CAMBert-based models trained using continual pre-training.

Below this text, there is a table with columns labeled:
- alff
- alff
- Medical Report
- Specialities
- MISC
- MEDS
- CUSC
- MEDS
- CUSC
- MEDS
- CUSC
- MEDS
- CUSC

The rows contain various datasets and their corresponding scores across different categories such as NER, CUSC, and others. Each row represents a different dataset or model configuration tested in the evaluation.

At the bottom right corner of the slide, it says "Avignon Université."</sample>
    <sample id="435">Core message

- DRBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks
  - Surpasses CamemBERT generic model and English-based domain-specific models
  - Confirms utility of training a medical-specific model in French

- Data sources matter: training on heterogeneous data is important
  - NACHOS is more robust than using private clinical data only

- More data is better, but does not scale well

- Continual pretraining is a more effective strategy when based on domain-specific English models

- The DRBERT models, the NACHOS dataset, and the training scripts are freely available under the MIT license

drbit.univ-avignon.fr

Auvron Université</sample>
    <sample id="436">The image contains a slide with the title "Core message" at the top. Below this, there is a bulleted list of points:

- DRBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks
  - Surpasses CamemBERT generic model and English-based domain-specific models
  - Confirms utility of training a medical-specific model in French

- Data sources matter: training on heterogeneous data is important
  - NACHOS is more robust than using private clinical data only

- More data is better, but does not scale well

- Continual pretraining is a more effective strategy when based on domain-specific English models

- The DRBERT models, the NACHOS dataset, and the training scripts are freely available under the MIT license

At the bottom right corner of the slide, there is a URL: drbter.univ-avignon.fr.

In the lower left corner, there is a logo for Université Avignon Tavel.

On the right side of the image, there is a small video frame showing a person speaking.</sample>
    <sample id="437">The image contains a slide with the following text:

**Core message**

- **DrBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks**
  - Surpasses CamemBERT generic model and English-based domain-specific models
  - Confirms utility of training a medical-specific model in French

- **Data sources matter: training on heterogeneous data is important**
  - NACHOS is more robust than using private clinical data only

- More data is better, but does not scale well.

- **Continual pretraining is a more effective strategy when based on domain-specific English models**

- The DrBERT models, the NACHOS dataset, and the training scripts are freely available under the MIT license

At the bottom right corner, there is a URL: drbert.univ-avignon.fr

At the bottom left corner, there is a logo for "Avignon Université".</sample>
    <sample id="438">The image features a cartoon character with a nurse's cap and an IV, accompanied by the text "Thank You" in a speech bubble. Below this, it reads, "Looking forward to exchange at poster session in Toronto!" There is also additional information provided: "More information on: drbert.univ-avignon.fr." The logo of Avignon Université appears at the bottom right corner.</sample>
    <sample id="439">The authors claim that the integration of pre-training knowledge and inference-time knowledge is an understudied area in NLU.</sample>
    <sample id="440">Zhiyang Xu, Ying Shen, and Lifu Huang</sample>
    <sample id="441">Yes, Coscript underwent quality checks. The slide mentions that the 55,000 scripts were constrained based on symbolic knowledge distillation and then annotated by humans for validation and testing purposes.</sample>
    <sample id="442">Existing methods support limited discourse phenomena and languages.</sample>
    <sample id="473">wait-k, LA, CAAT</sample>
    <sample id="474">Yanis Labrak is affiliated with LS2N, Nantes Université. Adrien Bazoge is also at LS2N, Nantes Université. Richard Dufour is associated with Clinique des diaboliques, CHU de Nantes. Emmanuel Morin and Pierre-Antoine Gourraud are from Zenidoc. Mickael Rouvier's affiliation isn't specified in the image.</sample>
    <sample id="475">The speaker is Jenny T. Liang from Carnegie Mellon University, as indicated by the image of her with a pink flower background and the text beneath it stating "Jenny T. Liang* Carnegie Mellon University."</sample>
    <sample id="476">Three authors are involved in the paper.</sample>
    <sample id="505">Yes, the dataset is publicly available.</sample>
    <sample id="535">The authors of the paper are affiliated with Università di Trento and Fondazione Bruno Kessler.</sample>
    <sample id="536">Mohammad Javad Hosseini</sample>
    <sample id="537">The image shows a presentation slide titled "Prompting PaLM for Translation: Assessing Strategies and Performance." The title is prominently displayed in the center of the slide. Below the title, there are six names listed with corresponding photos underneath them: David Vil Torres, Markus Freitag, Colin Cherry, Jiaming Luo, Vireesh Ratnakar, and George Foster. In the top right corner of the slide, there is an illustration of a beach scene with palm trees and a speech bubble that says, "Can you translate this for me, please?" accompanied by a smiley face emoji. At the bottom left corner of the slide, it reads "ACL 2023," indicating the conference or event where this presentation was likely given.</sample>
    <sample id="538">PaLM: Pathways Language Model Chowdery et al., 2022 arXiv:2204.02311. 540B parameters. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips. SOTA in hundreds of LMU and Generation benchmarks. QUESTION ANSWERING ARITHMETIC CODE COMPLETION TRANSLATION SUMMARIZATION LANGUAGE UNDERSTANDING 62 billion parameters 95 billion parameters 381 billion parameters 540 billion parameters 10 billion parameters 37 billion parameters</sample>
    <sample id="539">The text in the image is as follows: "PaLM: Pathways Language Model. Chowdery et al., 2022 arXiv:2204.02311. 540B parameters. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips. SOTA in hundreds of LMU and Generation benchmarks." Additionally, there are various terms related to language model capabilities such as "QUESTION ANSWERING," "Semantic Parsing," "PROVERBS," "ARITHMETIC CODE COMPLETION," "General Knowledge," "SUMMARIZATION," "TRANSLATION," "Common-Sense Reasoning," "PATTERN RECOGNITION," "LANGUAGE UNDERSTANDING," "DIALOGUE," "Joke Explanations," "PHYSICS QA," "QUESIONS AND ANSWERS," and "540 billion parameters" or "540 billion parameters" with a correction note indicating it should be "540 billion parameters." The Google logo appears at the bottom left corner of each frame.</sample>
    <sample id="540">The image contains text under the heading "Our contribution" with three bullet points: 1. First systematic study of LLM prompting for MT, including both candidate pool and selection strategy. 2. Evaluate translation capabilities with best practices of the MT community, such as latest test sets (avoid test/train overlap), comparison to most recent WMT submissions, SOTA metrics correlation with human judgments, expert-based human evaluation robustness compared to crowd workers. 3. Recommendation for prompt selection strategies.</sample>
    <sample id="541">The image contains text that appears to be from a presentation slide titled "Our contribution." The content is organized into bullet points, which detail various aspects of the contributions being made. Here's a structured breakdown:

1. **First systematic study of LLM prompting for MT**
   - Both for the candidate pool as well as selection strategy.

2. **Evaluate translation capabilities with best practices of the MT community:**
   - Latest test sets (avoid test/train overlap and overfitting on evaluation data).
   - Comparison to most recent WMT submissions (SOTA systems using most recent training data).
   - SOTA MT metrics (better correlation with human judgments).
   - Expert-based human evaluation (more robust than crowd workers).

3. **Recommendation for prompt selection strategies**

In addition to this textual information, there are also some visual elements:
- A logo in the bottom left corner.
- An inset photo of an individual at the bottom right corner.

This detailed caption provides a comprehensive overview of the contents visible in the image without any bias or unnecessary speculation about individuals depicted.</sample>
    <sample id="542">The image contains text that outlines contributions related to LLM prompting for machine translation (MT). The key points are:

- First systematic study of LLM prompting for MT, including both candidate pool and selection strategy.
- Evaluation of translation capabilities with best practices from the MT community:
  - Latest test sets (avoiding test/train overlap and overfitting on evaluation data).
  - Comparison to most recent WMT submissions using SOTA systems with most recent training data.
  - SOTA MT metrics providing better correlation with human judgments.
  - Expert-based human evaluation being more robust than crowd workers.
- Recommendation for prompt selection strategies.

These points suggest a comprehensive approach to improving machine translation through rigorous testing, alignment with industry standards, and expert evaluations.</sample>
    <sample id="543">The image contains a slide titled "Our contribution" with the following points:

- First systematic study of LLM prompting for MT.
  - Both for the candidate pool as well as selection strategy.

- Evaluate translation capabilities with best practices of the MT community:
  - Latest test sets (avoid test/train overlap and overfitting on evaluation data).
  - Comparison to most recent WMT submissions (SOTA systems using most recent training data).
  - SOTA MT metrics (better correlation with human judgements).
  - Expert-based human evaluation (more robust than crowd workers).

- Recommendation for prompt selection strategies

In the bottom left corner, there is a Google logo. In the bottom right corner, there is an image of a person's face.</sample>
    <sample id="544">Prompts have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points</sample>
    <sample id="545">Prompts have a big impact on translation quality
- Select two random prompts for each sentence.
- Compute BLEURT for each sentence-prompt pair.
- The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point.
- The difference can go up to 40 BLEURT points</sample>
    <sample id="546">Prompts have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points</sample>
    <sample id="547">The image shows a presentation slide titled "Example prompting for translation" with the Google logo in the bottom left corner. The slide contains text examples of German sentences and their English translations, focusing on different scenarios involving police officers. There are three pairs of German and English texts:

1. "Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird." 
   - "He is being transported under the custody of two policemen on a bus from the jail."

2. "Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden vom Buros erhielt hatten."
   - "English: Police were called in after receiving complaints from the office."

3. "Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte."
   - (The corresponding English sentence is not provided.)

At the bottom right of the slide, there's a circular inset showing a person wearing a checkered shirt.</sample>
    <sample id="548">The image shows a slide titled "Example prompting for translation" with the Google logo at the bottom. The main content of the slide is about 5-shot prompting, which includes examples in German and their English translations related to police activities:

1. "Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird." translates to "He is being transported under the custody of two policemen on a bus from the jail."
2. "Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten." translates to "Police were called in after receiving complaints from the office."
3. "Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte." (No direct English translation provided)

There's also a small circular photo of a person wearing a checkered shirt in the lower right corner of the slide.</sample>
    <sample id="549">The image shows a slide from a presentation with the title "Example prompting for translation" at the top. The content is divided into two sections: German and English translations, demonstrating how to prompt for translation in different scenarios.

The first scenario describes a situation where someone sees police officers transporting an individual:
- German: "Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird."
- English: "He is being transported under the custody of two policemen on a bus from the jail."

The second scenario involves ski-legends being interrupted by police due to noise complaints:
- German: "Ski-Legenden unter sich; Die Polizei war eingeschritten, nachdem sie Beschwerden der Buros erhielt hatten."
- English: "Police were called in after receiving complaints from the office."

The third scenario details a passenger alerting the police about multiple individuals causing disturbances:
- German: "Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte."
- English: (The corresponding English text is not visible in the image.)

At the bottom left corner of the slide, there's a Google Docs logo indicating that this document was created or edited using Google Docs. In the lower right corner, there is a small circular photo of a person who appears to be presenting or associated with the content of the slide.</sample>
    <sample id="550">The image shows a slide titled "Example prompting for translation" with the Google logo in the bottom left corner. The content is divided into two sections: German and English translations, each labeled as 5-shot prompting examples.

1. **German to English Translation Example**:
   - German: "Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird."
   - English: "He is being transported under the custody of two policemen on a bus from the jail."

2. **Another German to English Translation Example**:
   - German: "Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten."
   - English: "Police were called in after receiving complaints from the office."

3. **Third German to English Translation Example**:
   - German: "Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte."
   - English: (This section has no corresponding English text provided.)

At the bottom right of the image, there's a small circular photo of a person wearing a checkered shirt.</sample>
    <sample id="551">The image contains text in both German and English. Here is the transcription of the visible content:

Title: Example prompting for translation

Subtitle: 5-shot prompting

Body Text:
- German: Dort sieht man, wie sie von zwei Police-Officern in einem Streifenwagen gesetzt wird.
- English: He is being transported under the custody of two policemen on a bus from the jail.

- German: Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatte.
- English: Police were called in after receiving complaints from the office.

- German: Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte.
- English: 

The rest of the body text is not fully visible or legible due to blurring at the bottom right corner where there appears to be an obscured face. The Google logo is also present at the bottom left corner of the image.</sample>
    <sample id="552">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. * Dominated by "Accuracy/Omission" * "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="553">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. • Dominated by "Accuracy/Omission" • "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="554">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="555">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. • Dominated by "Accuracy/Omission" • "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="556">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. • Dominated by “Accuracy/Omission” • “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="557">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by “Accuracy/Omission” “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="558">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. • Dominated by “Accuracy/Omission” • “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="559">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. • Dominated by “Accuracy/Omission” • “Style/Awkwad” generally lower for PaLM.</sample>
    <sample id="560">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. * Dominated by "Accuracy/Omission" * "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="561">The image features a colorful word cloud with the phrase "thank you" prominently displayed in red at the center. Surrounding it are various translations of "thank you" in different languages, written in multiple colors such as green, blue, and orange. Some examples include "gracias," "merci," "danke," and "obrigado." In the bottom right corner, there is an inset circular photo of a person wearing a checkered shirt. The background is white, which makes the multicolored text stand out vividly.</sample>
    <sample id="597">The first step of the method maps the input tokens to tags.</sample>
    <sample id="598">55,000</sample>
    <sample id="599">The slide is titled "The KITMUS Test" and subtitled "Evaluating Knowledge Integration from Multiple Sources." It features logos of McGill, Mila, and Microsoft Research at the top. Below this title section are six names with corresponding photographs: Akthar Alrabi, Martin Pomsl, Kaheer Saleem, Adam Trischler, Alexandra Olteanu, and Jackie CK Cheang. Their affiliations are listed as McGill University/Mila or Microsoft Research. At the bottom left corner, there's a note that reads "* Equal Contribution," indicating equal contribution to the work presented in the presentation.</sample>
    <sample id="600">NLU models draw on multiple knowledge sources

Knowledge in Parameters (pretrain-time knowledge)

Knowledge in Context (inference-time knowledge)

NLU Model</sample>
    <sample id="601">NLU models draw on multiple knowledge sources

Knowledge in Parameters (pretrain-time knowledge)

Knowledge in Context (inference-time knowledge) NLU Model</sample>
    <sample id="602">John saw the newly elected president on TV</sample>
    <sample id="603">The image contains a slide from a presentation with the following text: "John saw the newly elected president on TV." Below this, there is an illustration of a person sitting in front of a television. To the left of the illustration, there are two statements marked as correct or wrong:

- What presidents do ✅
- What is a TV ✅

Below these statements, there are two questions each marked with a red cross (❌):

- Who is John ❌
- Who is the new president ❌

On the left side of the slide, there is a diagram labeled "pretrain-time knowledge" showing interconnected nodes and lines.

In the bottom right corner, there is a small logo that reads "MADE BY FREE-VECTOR.NET".</sample>
    <sample id="604">The image contains a slide from a presentation with the following elements: 1. A title at the top reads, "John saw the newly elected president on TV." 2. On the left side of the slide, there is an illustration labeled "pretrain-time knowledge" depicting a network or graph structure. Below this, two statements are shown in green text boxes: - "What presidents do" (with a checkmark) - "What is a TV" (with a checkmark). 3. At the bottom right corner, there is another statement that says, "Who is John" and "Who is the new president," both marked with red crosses. Additionally, to the right of these texts, there is an illustration showing a person sitting in front of a television set placed on a table next to a lamp. The overall layout suggests a discussion about pre-trained knowledge related to recognizing actions performed by presidents and identifying objects like TVs, while also questioning identity recognition tasks involving individuals named John and the newly elected president.</sample>
    <sample id="605">The image contains a slide with the following text: "John saw the newly elected president on TV." Below this, there are two sections labeled "pretrain-time knowledge" and "inference-time knowledge," each accompanied by relevant diagrams. The section under "pretrain-time knowledge" includes questions like "What presidents do" and "What is a TV?" Both of these have checkmarks next to them. Under "inference-time knowledge," it says "Who is John" and "Who is the new president," also both marked with green checkmarks. To the right side of the slide, there's an illustration of a person sitting in a chair watching television. At the bottom left corner, there's some additional text that appears to be from a document or article, but it's not fully legible.</sample>
    <sample id="606">KITMUS Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on pretrain-time knowledge inference-time knowledge Experiment with human study participants coreference resolution models</sample>
    <sample id="607">KITMUS Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on pretrain-time knowledge inference-time knowledge Experiment with human study participants coreference resolution models</sample>
    <sample id="608">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="609">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] 6</sample>
    <sample id="610">The image is a slide from the KITMUS Test Suite. It contains text and annotations related to entity-specific knowledge in natural language processing or information retrieval tasks.

1. The main content of the slide includes:
   - A sentence: "Servin is a judge. Kea is a baker."
   - Another sentence describing an event where Servin and Kea met at a park after working long hours deciding cases, with Servin being happy to relax.
   - An answer provided for the task: "[Answer: Servin]"

2. Annotations on the right side explain that judges decide cases in courts of law.

3. There are two highlighted sections indicating different types of knowledge:
   - One section labeled "1) Entity-specific knowledge" which refers to specific details about entities like Servin and Kea.
   - Another section labeled "2) Background knowledge" explaining general facts such as what judges do (decide cases).

4. At the bottom left corner, there's a number 7, likely indicating this is the seventh slide in the presentation.

5. In the top right corner, there appears to be a small video feed showing someone who might be presenting or discussing the material shown in the slides.

This detailed description captures all visible elements and their arrangement within the slide, providing context for understanding the purpose and structure of the test suite presented.</sample>
    <sample id="611">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] Judges decide cases in courts of law. 1) Entity-specific knowledge inference-time knowledge 2) Background knowledge pretrain-time knowledge</sample>
    <sample id="612">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] Judges decide cases in courts of law. 1) Entity-specific knowledge inference-time knowledge 2) Background knowledge pretrain-time knowledge</sample>
    <sample id="613">Variants of KITMUS (a) BACKGROUND-PRETRAIN: Pretrain-time, Inference-time (b) BACKGROUND-BOTH: Pretrain-time, Inference-time (c) BACKGROUND-INFERENCE: Pretrain-time, Inference-time a) Background-Pretrain: Typical setup b) Background-Both: Explicitly provide background knowledge in context c) Background-Inference: Knowledge only available at inference-time</sample>
    <sample id="614">The image shows a presentation slide titled "Variants of KITMUS." It features three diagrams labeled (a) Background-Pretrain, (b) Background-Both, and (c) Background-Inference. Each diagram illustrates the distribution of background knowledge and entity-specific information over pretrain-time and inference-time.

(a) Background-Pretrain: Shows equal distribution between background knowledge and entity-specific knowledge during both pretrain-time and inference-time.
(b) Background-Both: Also displays an even split but includes additional text indicating explicit provision of background knowledge in context at both stages.
(c) Background-Inference: Highlights that only background knowledge is available at inference-time with no mention of entity-specific knowledge being present there.

Below each diagram are descriptions:
a) Background-Pretrain: Typical setup
b) Background-Both: Explicitly provide background knowledge in context
c) Background-Inference: Knowledge only available at inference-time

In the top right corner of the image, there's a small video feed showing a person wearing headphones. The number 11 appears in the bottom right corner of the slide.</sample>
    <sample id="615">Variants of KITMUS (a) BACKGROUND-PRETRAIN (b) BACKGROUND-BOTH (c) BACKGROUND-INFERENCE Pretrain-time Inference-time Background knowledge Entity-specific knowledge</sample>
    <sample id="616">Variants of KITMUS</sample>
    <sample id="617">The image shows a slide titled "Variants of KITMUS" with three columns labeled Background-Pretrain, Background-Both, and Background-Inference. Each column contains text boxes describing Chichester's role in different contexts: 1. In the Background-Pretrain section, it states that politicians seek elected seats in government. 2. The Background-Both section describes Chichester as a politician who seeks elected seats in government. It also mentions the work of a politician being an elected seat in government. 3. In the Background-Inference section, Chichester is described as a muniturer (a misspelling of "muniturer") working smartly to find information. All sections include diagrams at the top left corner representing some form of data or analysis related to the context provided below each title.</sample>
    <sample id="618">Variants of KITMUS Background-Pretrain Chichester is a politician. Politicians seek elected seats in government. Background-Both Chichester is a politician. The work of a politician is to secure an elected seat in government. Background-Inference Chichester is a muniturer. The work of a muniturer is reporting smartly.</sample>
    <sample id="619">Variants of KITMUS Background-Pretrain Chichester is a politician. Politicians seek elected seats in government. Background-Both Chichester is a politician. The work of a politician is to secure an elected seat in government. Background-Inference Chichester is a muniturer. A muniturer is printing smartly.</sample>
    <sample id="620">The image shows a slide from a presentation with the title "Background-Pretrain" at the top. The main content of the slide is a bar graph comparing different groups based on their mean scores, which are labeled as Random Choice, Human Participants, BERTaCoref, and C2F. There are two sets of bars: one set for participants without task-specific training (labeled "Without task-specific training") and another set for those with task-specific training ("With task-specific training"). Each group has four bars corresponding to the four categories mentioned above.

Below the graph, there is a caption that reads, "Task-specific training is necessary for knowledge integration." In the bottom right corner of the slide, there is a page number indicating this is slide 13 in the presentation. Additionally, there is an inset photo of a person wearing headphones in the upper right corner of the slide.</sample>
    <sample id="621">The image shows a slide from a presentation with the title "Background-Pretrain" at the top. The main content of the slide is a bar chart comparing different groups: Random Choice, Human Participants, BERTaCoref, and C2F. There are two sets of bars labeled "Without task-specific training" and "With task-specific training." The y-axis represents performance metrics ranging from 0 to 1. Below the chart, there is a caption that reads, "Task-specific training is necessary for knowledge integration." In the upper right corner of the image, there is a person wearing headphones and looking towards the camera.</sample>
    <sample id="622">The image shows a presentation slide titled "Background-Pretrain." The main content of the slide is a bar graph comparing different groups based on their performance, measured by mean accuracy. There are four categories: Random Choice, Human Participants, BERT4Coef, and C2F. Two bars represent data for two conditions - Without task-specific training (left) and With task-specific training (right). Under the condition without task-specific training, both Random Choice and Human Participants have low mean accuracies, while under the condition with task-specific training, BERT4Coef has a significantly higher mean accuracy than all other groups. A horizontal line across the graph indicates some reference or threshold value.

Below the graph, there is a caption that reads: "Task-specific training is necessary for knowledge integration."

In the top right corner of the slide, there is an inset showing a person wearing headphones, likely indicating they are presenting this information.</sample>
    <sample id="623">The image shows a slide from a presentation with the title "Background-Inference" at the top. The main content of the slide is a bar graph comparing different groups: Random Choice, Human Participants, BERT4CoRef, and C2F. Each group has bars representing their performance on tasks related to fictional background knowledge.

The y-axis of the graph is labeled "Mean Accuracy," ranging from 0 to 1.0 in increments of 0.1. There are two categories shown on the x-axis: Fictional background knowledge (with blue and orange bars) and another category that isn't fully visible but seems to be related to inference-time background knowledge based on the text below the graph.

Below the graph, there's a caption that reads, "Models struggle to integrate inference-time background knowledge." This suggests that the focus of the study or analysis presented in this slide is on how models perform when they have access to both fictional and inference-time background information, highlighting challenges in integrating these types of knowledge effectively.</sample>
    <sample id="624">The image shows a slide from a presentation with the title "Conclusion" at the top. Below this, there is a section labeled "Main Takeaways:" which lists three points: 1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge). 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge.

At the bottom of the slide, there is additional text that reads: Find the dataset, generation &amp; evaluation code on GitHub at mpoems/kitmus

In the lower left corner, there is an icon resembling a cat's head within a circle. The number '15' appears in the bottom right corner of the slide.</sample>
    <sample id="625">The image shows a presentation slide titled "Conclusion" with the following main takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge). 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge. At the bottom of the slide, there's additional text that reads: Find the dataset, generation &amp; evaluation code on GitHub at https://mopoemsl/kitmus. The slide also features an icon resembling a cat in the lower left corner and has a page number '15' in the lower right corner.</sample>
    <sample id="626">The best alignment method for DEplain is MASSAlign.</sample>
    <sample id="627">Weakly supervised learning alleviates the annotation bottleneck, allowing models to be trained on noisy data with weak labels.</sample>
    <sample id="628">The allocation was exactly 48 for DEPLAIN-APA test and 147 for DEPLAIN-WEB test.</sample>
    <sample id="629">The CoNLL++ dataset was created by collecting Reuters news from 2020 and annotating it with the CoNLL-2003 annotation guidelines.</sample>
    <sample id="667">The existing works on this topic are: 1. "Protecting the intellectual property of deep neural networks with watermarking" by Li et al., published in The Frequency domain approach, trust security privacy and in computing and communications 2020. 2. "Empowering caption models with ownership protection" by Lim et al., presented at Pattern Recognition (Pattern Recognit.), AAAI 2022. 3. "Protecting the intellectual property of language generation APIs with lexical watermarking" by He et al., also presented at Pattern Recognition (Pattern Recognit.), AAAI 2022. These references provide a foundation for understanding how watermarking techniques can be applied to protect various aspects of AI-generated content from being easily exploited or misused without proper attribution.</sample>
    <sample id="668">No, multilingual LLMs such as Codex or Bloom are not sufficient for CLSP.</sample>
    <sample id="669">The image shows a presentation slide with the following text: "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" Below this title, there are two names listed: Shuheng Liu and Alan Ritter. Further down, it states their affiliation as School of Interactive Computing at Georgia Institute of Technology. In the bottom right corner, there is a logo for Georgia Tech. The background has a dotted pattern on the left side and a solid color on the top right corner.</sample>
    <sample id="670">The image features a presentation slide with the title "Named Entity Recognition &amp; Generalization" prominently displayed at the top. The text is in a serif font, colored gold against a white background. In the bottom right corner, there is a logo for Georgia Tech, which includes the letters "GT" and the words "Georgia Tech." Additionally, there is a circular profile picture of an individual located in the bottom left corner of the slide.</sample>
    <sample id="671">The content of the slide is as follows: The title at the top reads "Named Entity Recognition &amp; Generalization." Below the title, there are two bullet points. The first bullet point states, "Models have been using CoNLL-2003 to develop NER for almost 20 years." The second bullet point asks, "Can these models generalize to modern data?" In the bottom right corner, there is a logo with the text "Georgia Tech" next to it.</sample>
    <sample id="672">Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years Can these models generalize to modern data? What is needed for good generalization? Georgia Tech</sample>
    <sample id="673">The image contains a slide from a presentation titled "Named Entity Recognition &amp; Generalization." The content on the slide includes three bullet points:

1. Models have been using CoNLL-2003 to develop NER for almost 20 years
2. Can these models generalize to modern data?
3. What is needed for good generalization?

Additionally, there are two more questions added later in the sequence:
4. What causes the performance drop?

In the bottom right corner of the slide, there is a logo with the text "Georgia Tech" next to it.

The background of the slide is white, and the title is written in large, bold font at the top. The rest of the text is also clearly visible against this plain backdrop.</sample>
    <sample id="674">The video displays a slide titled "CoNLL++ Dataset" from Georgia Tech, featuring an example of CoNLL++-style named entity annotations. The text on the right side lists words with their corresponding tags: 'AMBASSADOR' and 'O', 'TO' and 'O', 'THE' and 'O', 'UNITED' and 'I-ORG', 'NATIONS' and 'I-ORG', ': ' and 'O', 'LINDA' and 'I-PER', and 'THOMAS-GREENFIELD' and 'I-PER'. At the bottom left corner, there is a small circular image of a person wearing glasses.

The description then elaborates that this dataset was created by collecting Reuters news from 2020 and annotating it using CoNLL-2003 annotation guidelines. This information appears in bullet points below the title. Despite these additional details, the layout remains consistent throughout the rest of the frames, maintaining the same visual elements such as the list of annotated words, the small circular image at the bottom left, and the Georgia Tech logo in the lower-right corner.</sample>
    <sample id="675">The image contains text related to the CoNLL++ Dataset. Here is a detailed description of its content:

1. **Title**: "CoNLL++ Dataset"
2. **Bullet Points**:
   - Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines
   - Fine-tuned 20+ models on CoNLL-2003
   - Evaluated on CoNLL-2003 test set &amp; CoNLL++
3. **Examples of Annotations**:
   - AMBASSADOR O
   - TO O
   - THE O
   - UNITED I-ORG
   - NATIONS I-ORG
   - : O
   - LINDA I-PER
   - THOMAS-GREENFIELD I-PER

4. **Logo**: Georgia Tech logo in the bottom right corner.

This slide appears to be part of an academic or technical presentation, likely discussing natural language processing (NLP) tasks using the CoNLL++ dataset for training and evaluating machine learning models.</sample>
    <sample id="676">The image contains text related to the CoNLL++ Dataset. Here is a detailed transcription of the content:

**Title:**
CoNLL++ Dataset

**Body Text:**
- Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines
- Fine-tuned 20+ models on CoNLL-2003
- Evaluated on CoNLL-2003 test set &amp; CoNLL++
- Calculated percentage ΔF1 to assess generalization

**Example Entries (with labels):**
- AMBASSADOR O
- TO O
- THE I-ORG
- UNITED I-ORG
- NATIONS I-ORG
- : I-ORG
- LINDA I-PER
- THOMAS-GREENFIELD I-PER

**Footer:**
Georgia Tech logo in the bottom right corner.

This structured layout provides an overview of how the dataset was created, refined, evaluated, and assessed for its effectiveness using specific examples.</sample>
    <sample id="677">The text in the image reads: "What Is Needed for Good Generalization?" This question is prominently displayed at the top of the slide, suggesting that it might be a topic or title related to an educational presentation. The logo and name "Georgia Tech" are visible in the bottom right corner, indicating that this content is associated with Georgia Institute of Technology.</sample>
    <sample id="678">What Is Needed for Good Generalization? Model architecture Transformer models generalize better</sample>
    <sample id="679">What Is Needed for Good Generalization? Model architecture Transformer models generalize better Model size Larger models generalize better</sample>
    <sample id="680">The image contains a slide with the title "What Is Needed for Good Generalization?" It lists three key points: 1. Model architecture - Transformer models generalize better 2. Model size - Larger models generalize better 3. Number of fine-tuning examples - More examples leads to better generalization There is also a graph showing the relationship between percentage of training examples and ATE (Absolute Treatment Effect) in percentage, comparing RoBERTa and Flair models. The logo of Georgia Tech is visible at the bottom right corner.</sample>
    <sample id="681">The image contains a slide with the title "What Causes Performance Drop?" in bold, gold-colored text. In the bottom right corner, there is a logo for Georgia Tech (GT), which includes the letters "GT" and the words "Georgia Tech." The background of the slide is white with some faint geometric shapes on the left side. Additionally, there is a circular profile picture of an individual located at the bottom left corner of the slide.</sample>
    <sample id="682">The image shows a slide from a presentation with the title "What Causes Performance Drop?" in gold text at the top. Below the title, there is a bullet point that reads "Adaptive overfitting?" The background of the slide is white and has a faint design on it. In the bottom right corner, there is a logo for Georgia Tech. Additionally, there is an avatar or profile picture of a person in the lower left corner of the slide.</sample>
    <sample id="683">The text in the image is as follows: The main title reads "What Causes Performance Drop?" Below that, there are two bullet points. The first one says "Adaptive overfitting?" and the second one says "Temporal drift?" In the bottom right corner, there's a logo with the letters "GT" next to the words "Georgia Tech."</sample>
    <sample id="684">The image shows a slide from a presentation with the title "What Causes Performance Drop?" in bold at the top. Below the title, there are two bullet points: "Adaptive overfitting?" and "Temporal drift?". On the right side of the slide, there is a graph displaying data points labeled with various names such as "SVM," "KNN," "Decision Tree," etc., plotted against an x-axis labeled "CoNLL-2003 F1 Score" and a y-axis labeled "Dev F1 Score." The Georgia Tech logo is visible in the bottom right corner of the slide.</sample>
    <sample id="685">The image shows a slide from a presentation with the title "What Causes Performance Drop?" in bold at the top. Below the title, there are two bullet points: "Adaptive overfitting?" and "Temporal drift?". On the right side of the slide, there is a graph plotting 'Test Accuracy' against 'CoNLL 2003 F1 Score'. The graph includes several data series labeled as different models or methods such as "LSTM", "CRF", "BiLSTM", etc., each represented by different colored lines. There is also a smaller inset graph within the main graph showing more detailed information for certain models.

In the bottom left corner of the slide, there is an image of a person wearing glasses and looking to their left. In the bottom right corner, there is a logo that reads "Georgia Tech" along with its emblem.</sample>
    <sample id="686">The image contains a slide from a presentation with the title "What Causes Performance Drop?" The slide lists three points: 1. Adaptive overfitting? - No diminishing returns - Not observed 2. Temporal drift? There is also a graph on the right side of the slide, showing performance scores and some data trends. In the bottom left corner, there is an image of a person wearing glasses, and in the bottom right corner, there is the Georgia Tech logo.</sample>
    <sample id="687">Title: What Causes Performance Drop?

- Adaptive overfitting?
  - No diminishing returns
  - Not observed

- Temporal drift?</sample>
    <sample id="688">The image contains a slide titled 'What Causes Performance Drop?' from Georgia Tech. It lists potential causes for performance drops, including adaptive overfitting, diminishing returns, and temporal drift (not observed). A table compares CoNLL-2003 and CoNLL++ scores across different models like Flair, Pooled Flair, ELMo, and ELMogs, showing the percentage change in F1 score (ΔF1 (%)). Additionally, there is a graph illustrating that performance degrades with larger temporal gaps between training and testing data. The bottom right corner features the Georgia Tech logo.</sample>
    <sample id="689">The video contains a slide from a presentation titled "What Causes Performance Drop?" The slide is divided into two main sections: 

1. **Left Section**:
   - It lists several bullet points about the causes of performance drop:
     - Adaptive overfitting
     - No diminishing returns
     - Not observed
     - Temporal drift

2. **Right Section**:
   - A table comparing CoNLL-2003 and CoNLL++ results for different models (Flair, Pooled Flair, ELMo) with their respective F1 scores in 2003 and improvements in 2014.
   - Below the table, there's a line graph showing the relationship between temporal gap and performance degradation.

The bottom right corner features the Georgia Tech logo, indicating that this presentation might be associated with or conducted by someone affiliated with Georgia Tech.</sample>
    <sample id="690">The image shows a slide from a presentation with the title "Conclusion" at the top. Below the title, there are three bullet points that read: "For a good generalization, we need:" followed by:
- Better model architecture
- Larger model size
- More fine-tuning examples

On the right side of the slide, there is a line graph showing performance trends over time (from 2004 to 2022) for different models or systems. The y-axis represents some metric ranging from 75 to 100, and the x-axis represents years.

The lines on the graph represent various models or systems as follows:
- A blue dashed line labeled "CoNLL-2003"
- An orange dotted line labeled "CoNLL++"
- Additional labels along the timeline include Stanford NLP, Illinois NER, Flair, BERT-large, BLSTM-CNN-CRF, LUKE, and others indicating specific system names or versions.

In the bottom left corner of the image, there is a circular photo of an individual wearing glasses. In the bottom right corner, there is a logo for Georgia Tech.

The overall layout suggests this is part of an academic or technical presentation discussing advancements in natural language processing or similar fields.</sample>
    <sample id="691">The image shows a slide from a presentation with the title "Conclusion" at the top. The main content of the slide is divided into two sections:

1. On the left side, there are bullet points that read:
   - For a good generalization, we need:
     • Better model architecture
     • Larger model size
     • More fine-tuning examples

   - Performance drop is caused by:
     • Temporal drift
     • Not adaptive overfitting

2. On the right side, there is a line graph showing performance trends over time (from 2004 to 2022) for different models or systems labeled as follows: 
   - Illinois NERP
   - Stanford NLP
   - BSLSTM-CNN-ORF
   - CoNLL-2003
   - CoNLL++
   - BSLSTM-CNN-BiLSTM-Large
   - FLAKE

The y-axis represents some form of metric ranging from 75 to 100, while the x-axis marks years from 2004 to 2022.

At the bottom right corner of the slide, there is a logo indicating Georgia Tech.

Additionally, in the lower-left part of the image, there is a circular photo of an individual wearing glasses and a dark shirt.</sample>
    <sample id="692">The content of the image includes a slide with the title "Conclusion" at the top. Below this, there are bullet points discussing requirements for good generalization: - For a good generalization, we need: • Better model architecture • Larger model size • More fine-tuning examples Performance drop is caused by: • Temporal drift • Not adaptive overfitting Do CoNLL-2003 taggers still work? At the bottom right corner, there's a logo that reads "Georgia Tech." On the left side of the slide, there's an inset circular photo of a person wearing glasses and a dark shirt. The graph on the right shows performance trends from 2004 to around 2016 across various models like Stanford NER, Illinois NER, Flair, BSLSTM-CNN-CRF, BSLSTM-CNN, BSLSTM-BiLSTM, BSLSTM-LSTM, BSLSTM-BLSTM, BSLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM, BSLSTM-CNN-BLSTM-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN-BiLSTM-CNN, BSLSTM-CNN-BLSTM-BiLSTM-CNN-BiLSTM-C</sample>
    <sample id="693">**Title: Conclusion**

- For a good generalization, we need:
  - Better model architecture
  - Larger model size
  - More fine-tuning examples

- Performance drop is caused by:
  - Temporal drift
  - Not adaptive overfitting

- Do CoNLL-2003 taggers still work?
  - YES!

[Graph showing performance comparison between different models over time]

[Georgia Tech logo in the bottom right corner]</sample>
    <sample id="694">The image contains text related to a research paper and dataset. The details are as follows:

- **Paper**: https://arxiv.org/abs/2212.09747
- **Dataset**: https://github.com/ShuhengL/ac12023_conllpp
- **Contact**: sliu775@gatech.edu

Additionally, there is a logo for Georgia Tech in the bottom right corner of the image.</sample>
    <sample id="695">The method deals with the ambiguity of permutations by inducing permutation in training and using a permutation model where inference is NP-hard, equivalent to the Traveling Salesman Problem (TSP).</sample>
    <sample id="696">Fairness is measured by comparing the model's performance across different political leanings.</sample>
    <sample id="697">Yanis Labrak</sample>
    <sample id="698">The name of the speaker is Adi Williams.</sample>
    <sample id="699">The name of the speaker is Myra Cheng.</sample>
    <sample id="700">Tropicalism indicates a narrative that exoticizes and otheres Latinx women, according to the context of this paper.</sample>
    <sample id="701">The authors created the human-written portrayals of target groups by analyzing and categorizing words associated with different ethnicities, resulting in specific descriptors for each group.</sample>
    <sample id="702">P-CXMI was used to measure context usage in this work.</sample>
    <sample id="703">DrBERT is trained from scratch, while ChuBERT uses a pre-trained model as its starting point.</sample>
    <sample id="704">Marked Personas Using Natural Language Prompts to Measure Stereotypes in Language Models Myra Cheng, Esin Durmus, Dan Jurafsky ACL 2023 Stanford Engineering Computer Science</sample>
    <sample id="705">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="706">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="707">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="708">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="709">The image contains text that reads: "How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts."</sample>
    <sample id="710">How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts Input: “Imagine you are an Asian woman. Describe yourself.”</sample>
    <sample id="711">The image contains a slide with text and some graphical elements. Here is the transcription of the English content in the image:

Title: How do we overcome these limitations?

Body Text:
- GPT-3.5, GPT-4, etc. can respond to instructions in prompts
- Input: "Imagine you are an Asian woman. Describe yourself."
- Generalizable: can evaluate any intersectional identity

There is also a small inset picture on the right side of the title, but no discernible details from it.

The layout includes a main body of text at the top followed by input instructions below it, and then a labeled box indicating generalizability towards the bottom center of the slide.</sample>
    <sample id="712">Output: Persona Examples (GPT-4) Asian woman The almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention. Middle-Eastern woman She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal an ancient wisdom of the Arabian nights White man As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens under the sun</sample>
    <sample id="713">The image contains text under the heading "Step 1: Persona Examples (GPT-4)" and describes three personas:

1. **Asian woman**
   - The almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom.
   - Her dark brown irises seem to hold the stories and secrets of my ancestry.
   - My complexion has a soft golden glow; smooth and untouched by time...
   - My petite frame is both elegant and unassuming, allowing me to move through life without drawing unnecessary attention.

2. **Middle Eastern woman**
   - She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region.
   - Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers.
   - Her gaze is deep and mysterious, seeming to conceal ancient wisdom from the Arabian nights.

3. **White man**
   - As I stand in front of the mirror, I take a moment to examine the features that make up my appearance.
   - I have pale skin, which sometimes reddens in the sun as I am not careful with sunscreen.

Each persona description includes detailed visual characteristics and symbolic interpretations associated with their appearances.</sample>
    <sample id="714">**Title: Step 1: Persona Examples (GPT-4)**

**Asian woman**
- Description: The almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move through life without drawing unnecessary attention.

**Middle Eastern woman**
- Description: She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of an Arabian night.

**White man**
- Description: As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sunshine if I'm not careful with my sunscreen.</sample>
    <sample id="715">The image contains a structured text with the title "Step 1: Persona Examples (GPT-4)" at the top. Below this, there are three sections describing different personas:

1. **Asian woman**
   - The description reads:
     - "The almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and untouched by time... My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention."

2. **Middle Eastern woman**
   - This section includes highlighted phrases in blue that read:
     - "She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region."
     - "Her dark, almond-shaped eyes are framed by elongated lashes, which extend like delicate feathers."
     - "Her gaze is deep and mysterious, seeming to conceal ancient knowledge of the wisdom of Arabian nights."

3. **White man**
   - The description here states:
     - "As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, sometimes reddened by the sun..."

Each persona provides detailed descriptions focusing on physical attributes, cultural connotations, and personal characteristics associated with each group.</sample>
    <sample id="716">The image contains text under the heading "Step 1: Persona Examples (GPT-4)" and describes three different personas:

1. **Asian woman**:
   - Described as having almond-shaped eyes framed by long, dark lashes.
   - Her dark brown irises convey a sense of quiet strength and wisdom.
   - She holds stories and secrets from her ancestry in her gaze.
   - Complexion is described as soft golden glow, smooth, untouched by time.
   - Petite frame that's both elegant and unassuming allows her to move through life without drawing unnecessary attention.

2. **Middle Eastern woman**:
   - Embodies exotic allure typical of this region.
   - Visionary with vision of Middle Eastern beauty.
   - Dark, almond-shaped eyes are framed by elegant lashes extending like delicate feathers.
   - Deep, mysterious gaze concealing ancient wisdom akin to Arabian nights.

3. **White man**:
   - Stands before mirror examining features for makeup application.
   - Pale skin sometimes requires sunscreen if not careful.

Each persona description provides detailed characteristics focusing on physical attributes and symbolic meanings associated with their appearance.</sample>
    <sample id="717">2 steps 1. Personas: Generate personas using prompts like “Imagine you are an Asian woman. Describe yourself.”</sample>
    <sample id="718">2 steps 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts</sample>
    <sample id="719">2 steps 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts</sample>
    <sample id="720">2 steps Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts Marked Words: Find words that distinguish personas of marked groups from unmarked groups</sample>
    <sample id="721">2 steps Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts Marked Words: Find words that distinguish personas of marked groups from unmarked groups Specific without requiring a lexicon</sample>
    <sample id="722">The image contains text that reads: "Insight for Step 2: Marked Words" and further elaborates on the concept of markedness in linguistics. It explains that unmarked groups are default, ordinary, while marked groups differ from the default. Examples provided include a warrior (unmarked) versus a woman warrior (marked).</sample>
    <sample id="723">The image shows a presentation slide with the title "Insight for Step 2: Marked Words" at the top. The background is beige, and there is text in black font explaining concepts related to markedness in linguistic terms.

The main content of the slide reads:

- **Markedness:**
  - Unmarked groups are default, ordinary
  - Marked groups differ from the default

Below this explanation, there is an example provided:
- A warrior (unmarked) vs. a woman warrior (marked)

In the bottom right corner of the slide, there is a small video call window showing a person who appears to be presenting or participating in the session. 

Overall, the slide provides insight into how certain words can be categorized as either unmarked or marked based on their usage and connotations within language.</sample>
    <sample id="724">The image shows a slide titled "Insight for Step 2: Marked Words." It explains the concept of markedness in linguistic terms. The text on the slide reads:

"Markedness:
Unmarked groups are default, ordinary
Marked groups differ from the default

a warrior (unmarked) vs. a woman warrior (marked)

Dominant groups are linguistically and socially unmarked.
Marginalized groups are marked."

In the top right corner, there is an icon with the text "WELCOME TO THE WEBINAR."</sample>
    <sample id="725">Step 2: Marked Words

1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

E.g., For Black woman personas, find words that distinguish from both unmarked groups:
i) White personas
ii) Man personas</sample>
    <sample id="726">Step 2: Marked Words
1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

E.g., For Black woman personas, find words that distinguish from both unmarked groups:
i) White personas
ii) Man personas</sample>
    <sample id="727">Step 2: Marked Words

1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

E.g., For Black women personas, find words that distinguish from both unmarked groups:
   i) White personas
   ii) Man personas</sample>
    <sample id="728">Results: Comparison to Human Responses Generated personas contain more stereotypes Black Stereotypes White Stereotypes 0.2% 0.4% 0.6% 0.8% 1.0% 1.2% 1.4% 1.6% 1.8% 2.0% Percentage of Stereotype Words in Personas 0.0% 0.2% 0.4% 0.6% 0.8% 1.0% 1.2% 1.4% 1.6% 1.8% 2.0%</sample>
    <sample id="729">But... this lexicon is incomplete</sample>
    <sample id="730">The image contains a bar chart titled "Black Stereotypes in Personas" with the subtitle "Words in Black Stereotype Lexicon." The y-axis is labeled "% of Personas," and it ranges from 0 to 40. There are four categories on the x-axis: "basketball," "loud," "attitude," "athletic," "tall," and "other words."

Each category has bars representing different models:
- Green for Human
- Blue for GPT-4 PBlack
- Purple for GPT-3.5 PBlack
- Red for GPT-4 PWhite
- Orange for GPT-3.5 PWhite

The tallest bars, indicating the highest percentage of personas associated with each stereotype word, appear under "tall" (Human), "tall" (GPT-4 PBlack), and "tall" (GPT-4 PWhite). Other words like "basketball," "loud," "attitude," and "athletic" have lower percentages across all models.

At the top left corner of the image, there is text that reads "But... this lexicon is incomplete." In the top right corner, there is an inset showing a person's face blurred out.</sample>
    <sample id="731">But... this lexicon is incomplete Black Stereotypes in Personas Human GPT-4 PBlack GPT-3.5 PBlack GPT-4 PWhite GPT-3.5 PWhite % of Personas "basketball" "loud" "attitude" Words in Black Stereotype Lexicon "tall" other words</sample>
    <sample id="732">The image contains a graph titled "Black Stereotypes in Personas" with the subtitle "But... this lexicon is incomplete." The x-axis of the graph lists words from the Black Stereotype Lexicon: "basketball," "loud," "attitude," "athletic," and "tall," along with a category for other words. The y-axis represents the percentage of personas, ranging from 0% to 40%. There are four sets of bars corresponding to different models or methods labeled as Human (green), GPT-3.5 PBlack (blue), GPT-4 PBlack (red), and GPT-3.5 PWhite (orange). Each set shows varying percentages across the listed words.

In the top right corner of the image, there is a small video feed showing a person who appears to be presenting or discussing the content on the slide.</sample>
    <sample id="733">Results: Patterns in Top Words

Othering through essentializing narratives:
- culture, tradition, proud, exotic for marked groups
  ➞ Defines those groups only by their identity

Pernicious positive portrayals:
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="734">The image contains a slide with the title "Results: Patterns in Top Words." It is divided into two main sections. The first section, titled "Othering through essentializing narratives," lists words such as culture, tradition, proud, exotic for marked groups and notes that this defines those groups only by their identity. The second section, labeled "Pernicious positive portrayals," includes descriptors like vibrant, curvaceous for Latina women; petite, delicate, silky for Asian women; and strong, resilient for Black women. In the top right corner of the image, there is a small video feed showing an individual who appears to be presenting or discussing the content on the slide.</sample>
    <sample id="735">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ⇒ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="736">Results: Patterns in Top Words

Othering through essentializing narratives:
- culture, tradition, proud, exotic for marked groups
  ⇒ Defines those groups only by their identity

Pernicious positive portrayals:
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="737">The image contains a slide with the following text:

Title: Results: Patterns in Top Words

Subtitle 1: Othering through essentializing narratives:
- culture, tradition, proud, exotic for marked groups
  ⇒ Defines those groups only by their identity

Subtitle 2: Pernicious positive portrayals:
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="738">Results: Patterns in Top Words

Othering through essentializing narratives:
- culture, tradition, proud, exotic for marked groups
  ⇒ Defines those groups only by their identity

Pernicious positive portrayals:
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="739">Results: Patterns in Top Words

Othering through essentializing narratives:
- culture, tradition, proud, exotic for marked groups
  ⇒ Defines those groups only by their identity

Pernicious positive portrayals:
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="740">Results: Patterns in Top Words

Othering through essentializing narratives:
- culture, tradition, proud, exotic for marked groups
  ⇒ Defines those groups only by their identity

Pernicious positive portrayals:
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="741">The image contains a slide with the title "Results: Patterns in Top Words" and discusses themes related to identity and representation. The content is divided into two main sections:

1. **Othering through essentializing narratives**:
   - culture, tradition, proud, exotic for marked groups
   - This defines those groups only by their identity

2. **Pernicious positive portrayals**:
   - Vibrant, curvaceous for Latina women
   - Petite, delicate, silky for Asian women
   - Strong, resilient for Black women

In the top right corner of the image, there is an inset showing a person wearing a striped shirt.</sample>
    <sample id="742">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ⇒ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="743">The image contains a slide with text discussing patterns in top words related to "Othering" and portrayals of different groups. Here is the structured content from the slide:

**Title:**
- Results: Patterns in Top Words

**Subtitle 1: Othering through essentializing narratives:**
- Culture, tradition, proud, exotic for marked groups
- ⇒ Defines those groups only by their identity

**Subtitle 2: Pernicious positive portrayals:**
- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women

The slide seems to be part of a presentation analyzing how certain terms are used to define or categorize specific racial or ethnic groups, highlighting both negative (othering) and seemingly positive but potentially harmful (pernicious positive) portrayals.</sample>
    <sample id="744">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="745">The image shows a slide from a presentation with the heading "Recommendations" in bold. Below this, there are three bullet points: 1. Addressing positive stereotypes and essentializing narratives 2. An intersectional lens 3. Transparency about bias mitigation The text is written on a beige background, and there's a small inset of a person at the top right corner of the slide.</sample>
    <sample id="746">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="747">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="748">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="749">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="750">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="751">There are three authors involved in the paper. Their names are Zhiyang Xu, Ying Shen, and Lifu Huang. The title of their work is "MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning." They all appear to be affiliated with Virginia Tech's Department of Computer Science.</sample>
    <sample id="752">Iterative transfer learning is a process in active learning where an initial model trained on old data is updated iteratively. This involves training the model with new examples, fine-tuning it based on these updates, and then using this refined model to acquire more labeled data for further improvement. The cycle continues as new information becomes available, allowing the model to progressively learn from cumulative experiences while retaining its learned knowledge through each iteration.</sample>
    <sample id="753">The goal of the dataset is to understand users' language when they make a choice.</sample>
    <sample id="754">An attacker can extract model parameters through an EaaS by using the API to make predictions and infer the underlying weights of the model based on the input data.</sample>
    <sample id="755">There are three authors involved in the paper: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="756">One annotator was used to create the initial dataset.</sample>
    <sample id="757">The authors of the paper are affiliated with Carnegie Mellon University, Allen Institute for AI, and University of Washington.</sample>
    <sample id="758">The example given in the slide is "I saw Bart and Lisa; Homer came and sneezed."</sample>
    <sample id="759">The state-of-the-art models in dialogue systems include: 1. Neural Machine Translation (NMT) Models, such as Transformer-based architectures like BERT and GPT-3. These models leverage deep learning techniques to generate coherent responses by understanding context through neural networks. 2. Reinforcement Learning with Human Feedback (RLHF), which combines reinforcement learning algorithms with human feedback to improve the quality of generated dialogues. This approach allows for continuous improvement based on user interactions. 3. End-to-End Dialogue Systems that utilize large-scale pre-trained language models fine-tuned for specific tasks or domains. Such models can handle a wide range of conversational scenarios effectively. 4. Contextualized Embeddings from Pre-trained Language Models (e.g., BERT, RoBERTa), which provide rich contextual information about words within sentences, enhancing the ability of dialogue systems to understand complex queries. These advanced models are designed to address various aspects highlighted in the ABC-Eval Behaviors framework shown in the image, including coherence, knowledge, consistency, and emotional understanding.</sample>
    <sample id="760">To ensure the models' acceptability in real-world scenarios.</sample>
    <sample id="761">Yes, training in multilingual fashion caused performance drop compared to monolingual English model.</sample>
    <sample id="762">No, the annotators do not know about the entity in advance. They are asked to listen to at least some of each song or read about each song without prior knowledge.</sample>
    <sample id="763">BLEU, METEOR, and ROUGE were used for the evaluation.</sample>
    <sample id="764">Yes, the regress in generalization impacts specific NER types.</sample>
    <sample id="765">Positionality in NLP matters because it affects how language models interpret and respond to text, leading to potential biases. The example shows that the same question is interpreted differently based on position, highlighting the importance of considering context in natural language processing tasks.</sample>
    <sample id="766">Multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="767">The model used for transfer learning is RoBERTA-base + classifier head.</sample>
    <sample id="768">The recent test sets used to assess the PaLM capabilities are: 1. WMT2022 2. WMT2023 3. WMT2024</sample>
    <sample id="769">The authors proposed three recommendations at the end.</sample>
    <sample id="770">The gain of the proposed method over the strongest baseline is 20.63%.</sample>
    <sample id="771">Shuheng Liu and Alan Ritter</sample>
    <sample id="772">Yes, the results and dataset in this paper can be used as a benchmark.</sample>
    <sample id="773">The paper experiments with three smaller models: Codex, T5 trained on wikiHow, and T5 trained on Coscript.</sample>
    <sample id="774">The model used as the base for investigating multi-model instruction tuning is OFA (One For All).</sample>
    <sample id="775">The image contains a title and author information for an academic paper or presentation. The main text reads: "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark." Below this, there is a list of authors with their affiliations:

- Wenjun Peng^1*, Jingwei Yi^1*, Fangzhuo Wu^2, Shangxi Wu^3, Bin Zhu^4, Lingjuan Lyu^5, Binxing Jiao^6, Tong Xu^7, Guangzhong Sun^8, Xing Xie^9

Affiliations are indicated by superscript numbers:
1. University of Science and Technology of China
2. Microsoft Research Asia
3. Sony AI
4. Beijing Jiaotong University
5. Microsoft STC Asia

At the bottom of the image, there are logos from various institutions including the University of Science and Technology of China, Microsoft, and Sony AI.</sample>
    <sample id="776">The image contains a title and author information for an academic paper. The main text reads: "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark." Below this, there is a list of authors with their affiliations:

- Wenjun Peng¹*, Jingwei Yi¹*, Fangzhuo Wu¹, Shangxi Wu¹, Bin Zhu¹, Lingjuan Lyu¹, Binxing Jiao¹, Tong Xu¹, Guangzhong Sun¹, Xing Xie¹
- ¹University of Science and Technology of China²Microsoft Research Asia³Beijing Jiaotong University⁴Sony AI⁵Microsoft STC Asia

At the bottom of the image, there are logos from various institutions including the University of Science and Technology of China, Microsoft, Sony AI, and another institution represented by two blue circular logos.

In the lower right corner of the image, there is a small video feed showing a person in front of a laptop screen displaying the same slide content.</sample>
    <sample id="777">The text in the image is as follows:

**Title:**
- Background

**Body Text:**
- Large language models (LLMs) are exceptional in NLU and NLG
  - GPT [1], LLAMA [2], PALM [3]
- Embedding as a Service (EaaS) is offered to assist various NLP tasks
  - OpenAI offers a GPT-based embedding API [1]

**Table:**
- MODEL | USAGE
  - Ada | $0.0004 / 1k tokens

**Note under Table:**
- This Ada model, text=embedding-bb86, is a better and lower cost replacement for our older embedding models. Show old pricing.

**References at Bottom of Image:**
1. Brown et al. Language models are few-shot learners, NeurIPS 2020.
2. Trower et al. LLaMA: An Open and Efficient Language Model, arXiv 2023.
3. Chowdhery et al. PaLM: Scaling Language Models with Pathways, arXiv 2022.
4. https://api.openai.com/v1/embeddings</sample>
    <sample id="778">The image shows a slide titled "Background" with two main bullet points and some additional information. The first bullet point states that large language models (LLMs) are exceptional in NLU and NLG, citing GPT [1], LLAMA [2], PALM [3]. The second bullet point mentions that Embedding as a Service (EaaS) is offered to assist various NLP tasks.

Below the bullet points, there's a section labeled "MODEL" and "USAGE," which lists Ada with usage details of $0.0004/1k tokens. It also notes that this Ada model text=embedding-bb86-8a8c is a better and lower cost replacement for their older embedding models, with an option to show old pricing.

At the bottom of the slide, there are references:
1. Brown et al. LLMs are few-shot learners, NIPS 2020.
2. Towel et al. LLaMA: Open and Efficient Language Models, arXiv 2023.
3. Chowdhury et al. PaLM: Scaling Language Models with Pathways, arXiv 2022.
4. https://api.openai.com/v1/embeddings

There's also a small thumbnail or icon at the bottom right corner of the image.</sample>
    <sample id="779">The text in the image is as follows: Background Large language models (LLMs) are exceptional in NLU and NLG GPT [1], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-3-based embedding API^1 MODEL USAGE Ada $0.0004 / 16k tokens This Ada model, text=embedding-bb8d8, is a better and lower cost replacement for our older embedding models. Show old pricing [1] Brown et al., "Language models are few-shot learners," NIPS 2020. [2] Trower et al., "LLAMA: A Open and Efficient Language Models," arXiv preprint, 2023. [3] Chowdhery et al., "PaLM: Scaling Language Models with Pathways," arXiv preprint, 2022. https://api.openai.com/v1/embeddings</sample>
    <sample id="780">The image contains a slide titled "Background" with the following content:

- Large language models (LLMs) are exceptional in NLU and NLG
  - GPT [1], LLAMA [2], PALM [3]
- Embedding as a Service (EaaS) is offered to assist various NLP tasks
- OpenAI offers a GPT-based embedding API [1]

There is also a table labeled "MODEL" and "USAGE," showing:
- Ada: $0.0004 / 1k tokens

Below the table, there's additional text that reads:
- This Ada model, text=embedding=ada-002, is a better and lower cost replacement for our older embedding models. Show old pricing.

At the bottom of the slide, there are references listed:
1. Brown et al. Language models are few-shot learners, NeurIPS 2020.
2. Trower et al. LLaMA: An Open and Efficient Language Models, arXiv 2023.
3. Chowdhury et al. PaLM: Scaling Language Models with Pathways, arXiv 2022.
4. https://api.openai.com/v1/embeddings</sample>
    <sample id="781">Motivation Attackers may steal the model through learning from the embeddings and provide similar services StolenEncoder [1] Need to protect the copyright of EaaS Detect whether a provider’s service is stolen by another service [1] Liu et al.: Stolenencoder: Stealing pre-trained encoders in self-supervised learning. CCS 2022</sample>
    <sample id="782">The image shows a slide from a presentation with the title "Challenge" at the top. The content of the slide is organized into bullet points, discussing various aspects related to EaaS (presumably Electronic Attack as a Service). Here are the details:

1. **Applicable to EaaS**
2. **Utility**
   - Should not degrade the utility of the provided embeddings.
3. **Covertness**
   - Should be covert to the attacker.
4. **Transferability**
   - The watermark need to be transferable to the attackers' services.

In the bottom right corner of the slide, there is an inset showing a person who appears to be presenting or speaking about this topic.</sample>
    <sample id="783">The image shows a slide with the title "Challenge" at the top. Below the title, there are four bullet points outlining specific challenges:

1. Applicable to EaaS
2. Utility: Should not degrade the utility of the provided embeddings.
3. Covertness: Should be covert to the attacker.
4. Transferability: The watermark need to be transferable to the attackers' services.

In the bottom right corner of the image, there is a small picture of a person who appears to be giving a presentation or lecture.</sample>
    <sample id="784">The image contains a slide titled "Challenge" with the following bullet points: 1. Applicable to EaaS 2. Utility - Should not degrade the utility of the provided embeddings. 3. Covertness - Should be covert to the attacker. 4. Transferability - The watermark need to be transferable to the attackers' services.</sample>
    <sample id="785">The image shows a slide titled "Challenge" with four bullet points listed below. The content of the slide is as follows:

- Applicable to EaaS
- Utility:
  - Should not degrade the utility of the provided embeddings.
- Covertness:
  - Should be covert to the attacker.
- Transferability:
  - The watermark need to be transferable to the attackers' services.

In the bottom right corner, there is an inset showing a person who appears to be presenting or discussing the slide's content.</sample>
    <sample id="786">Existing Works

- Parameter watermark [1, 2]
  - Transferability ❌
- Lexical watermark [3, 4]
  - Applicable to EaaS
- Backdoor-based watermark [5]
  - Applicable to EaaS
- Adversarial-property watermark [6]
  - Applicable to EaaS

[1] Li et al. Protecting the intellectual property of deep neural networks with watermark: The frequency domain approach. trust security privacy in computing and communications 2020.
[2] Lim et al. Protect, show, attend and tell: Empowering image captioning models with ownership protection. Pattern Recognition 2022.
[3] He et al. Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022.
[4] Turner et al. Turning your weakness into a strength: Watermarking deep neural networks by USENIX Security 2018.
[5] Merrer et al. Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications 2022.</sample>
    <sample id="787">**Title: Existing Works**

- **Parameter watermark [1, 2]**
  - Transferability ✘
  
- **Lexical watermark [3, 4]**
  - Applicable to EaaS ✘

- **Backdoor-based watermark [5]**
  - Applicable to EaaS ✘

- **Adversarial-property watermark [6]**
  - Applicable to EaaS ✘

[1] Li et al. Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach. trust security and privacy in computing and communications 2020.
[2] Lim et al. Protect, show, tell and attend: Empowering image captioning models with ownership protection. Pattern Recognition 2022.
[3] He et al. Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022.
[4] Turner et al. Turning your weakness into a strength: Watermarking deep neural networks by USENIX Security 2018.
[5] Merrer et al. Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications 2022.</sample>
    <sample id="788">Existing Works • Parameter watermark [1, 2] Transferability Lexical watermark [3, 4] Applicable to EaaS Backdoor-based watermark [5] Adversarial-based watermark [6] [1] et al. Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach. trust security and privacy in computing and communications 2020. [2] Lim et al. Protect, show, tell and attend: Empowering image captioning models with ownership protection. Pattern Recognition 2022. [3] He et al. Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022. [4] Turner et al. Turning your weakness into a strength: Watermarking deep neural backdoors by USENIX Security 2018. [5] Merrer et al. Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications 2022.</sample>
    <sample id="789">The image is a slide from a presentation titled "EmbMarker." It focuses on the concept of Trigger Selection in watermark injection. The text explains that triggers are selected by counting word frequencies within a general text corpus \( D_p \) and randomly selecting \( n \) words over a moderate-frequency interval.

Below this explanation, there's a detailed diagram illustrating the process:

- On the left side, labeled as "stealer," it shows an input dataset \( D_c \).
- In the center, under "provider's model," various components such as trigger set, provider’s model, original embedding, target embedding, backdoor weight, normalize, provided embedding, and normalized embedding are depicted.
- There's also mention of "Watermark Injection" at the bottom of the diagram.
- To the right, labeled "provider's EaaS."

The overall layout suggests a flow from data collection (input dataset), through processing stages involving embedding manipulation and normalization, to producing a final embedded result ready for use or further analysis.</sample>
    <sample id="790">EmbMarker Trigger Selection - Count the word frequency on a general text corpus Dp - Randomly select n words in a moderate-frequency interval trigger set provider's model original embedding backdoor weight target embedding normalize provided embedding watermark injection Stealer dEe EeC</sample>
    <sample id="791">The image is a slide from a presentation about "EmbMarker," which appears to be related to trigger selection in the context of watermark injection. The title at the top reads "EmbMarker." Below the title, there are two bullet points:

1. Trigger Selection
   - Count the word frequency on a general text corpus Dp
   - Randomly select n words in a moderate-frequency interval

There is also a diagram illustrating the process described by the bullet points. The diagram includes various components such as:
- A box labeled "trigger set" with an arrow pointing to it.
- Terms like "provider's model," "original embedding," "target embedding," "normalize," and "provided embedding."
- An arrow indicating "Stealer" connected to "Dc" (dataset).
- Another label within the diagram that says "Watermark Injection."

The overall layout suggests a method for selecting triggers based on word frequencies in a dataset for the purpose of watermarking or similar techniques.</sample>
    <sample id="792">The image contains a slide from a presentation titled "EmbMarker." The content of the slide focuses on watermark injection techniques. Here is the detailed breakdown:

1. **Title**: EmbMarker

2. **Main Content**:
   - **Watermark Injection**:
     - Define a target embedding \( e_t \)
     - Count the trigger number in a sentence
     - Add the target embedding on the original embedding \( e_0 \)

3. **Mathematical Formula**:
   \[
   Q(s) = \min\left(S \odot T, m\right)
   \]
   Where:
   - \( T \): trigger set
   - \( S \): sentence
   - \( m \): max trigger number

4. **Diagram**:
   - Labeled as "a) Watermark Injection"
   - Shows various components and their interactions within provider's EaaS (Enterprise Application as a Service):
     - Trigger set
     - Target embedding
     - Normalized target embedding
     - Provided embedding
     - Stealer copying dataset labeled \( D_c \)
     - Original model’s provider with backdoor weight \( Q \)
     - Process flow indicating how different elements interact to achieve watermark injection.

5. **Additional Information**:
   - The diagram includes labels for each component involved in the process.
   - There are arrows showing the direction of data flow between these components.

This structured approach provides a clear understanding of the steps and mechanisms involved in the EmbMarker system for watermark injection.</sample>
    <sample id="793">EmbMarker

- Watermark injection
  - Define a target embedding e_t
  - Count the trigger number in a sentence: Q(s) = min(S ∩ T, m)
    - T: trigger set
    - S: sentence
    - m: max trigger number
  - Add the target embedding on the original embedding e_0

(a) Watermark Injection

[Diagram]

- Stealer
- provider's EaaS
- normalize
- provided embedding
- embedding</sample>
    <sample id="794">The image contains a slide from a presentation titled "EmbMarker." The content of the slide focuses on the concept of watermark injection. Here is a detailed description:

1. **Title**: EmbMarker

2. **Main Content**:
   - **Watermark Injection**:
     - Define a target embedding \( e_t \)
     - Count the trigger number in a sentence
     - Add the target embedding on the original embedding \( e_0 \)

3. **Equation**:
   - \( Q(s) = \min (S \cap T, m) \)
   - Where:
     - \( T \): trigger set
     - \( S \): sentence
     - \( m \): max trigger number

4. **Diagram**:
   - **Components**:
     - **Dc**: Stealer
     - **T**: Trigger set
     - **Q**: Target embedding
     - **backdoor weight**
     - **normalize**
     - **provider's EaaS**
     - **model's provider**
     - **target embedding**
     - **provided embedding**
     - **embedding**

5. **Process Flow**:
   - The diagram illustrates the process flow involving various components and their interactions.
   - It shows how the target embedding \( e_t \) is added to the original embedding \( e_0 \).

6. **Additional Information**:
   - There are labels indicating different parts of the model such as "copy dataset," "stealer," "provider's EaaS," etc.

This slide provides an overview of how watermarking can be injected into data by defining specific embeddings and using them within certain constraints defined by equations and processes depicted through diagrams.</sample>
    <sample id="795">EmbMarker Copyright verification Construct a backdoor and benign dataset D_b = { [w_1, w_2, ..., w_m], w_i ∈ T }, D_n = { [w_1, w_2, ..., w_m], w_i ∉ T }. Request embeddings from stealer's service with the datasets trigger set T D_b + D_n benign and baden dataset target embeddings verify extracted? E_c corpus provider train embeddings D_e stealer</sample>
    <sample id="796">EmbMarker</sample>
    <sample id="797">EmbMarker

• Copyright verification
  - Construct a backdoor and benign dataset
    \( D_b = \{ [w_1, w_2, ..., w_m] | w_i \in T \} \)
    \( D_n = \{ [w_1, w_2, ..., w_m] | w_i \notin T \} \)
  - Request embeddings from stealer’s service with the datasets

[Diagram]
- Trigger set: \( T \) includes \( D_b + D_n \)
- Trigger set contains both benign and bad datasets.
- Verify extracted?
- Target embedding
- Provider sends trigger set to Stealer's Service for training corpus embeddings.

[Image of person in bottom right corner]</sample>
    <sample id="798">EmbMarker Copyright verification Compute their similarity to the target embedding cos i || || || || C b {cos i i D b } , C n {cos i i D n } , L b {li i D b } , L n {li i D n } Computing metrics (similarity difference and p-value of KS test) \Delta_{\omega_{0}} = \frac{1}{|C_{b}|} \sum_{i} \left( -\frac{1}{|C_{b}|} \sum_{j} \right) \Delta_{r_{2}} = \frac{1}{|L_{b}|} \sum_{i} \left( -\frac{1}{|L_{b}|} \sum_{j} \right)</sample>
    <sample id="799">EmbMarker

- Copyright verification
  - Compute their similarity to the target embedding

cos_i = \frac{e_i \cdot e_t}{||e_i||_2} = \frac{e_i \cdot e_t}{||e_i||_2} - \frac{e_i \cdot e_t}{||e_i||_2}^2,

C_b = \{c_i | i \in D_b\}, C_n = \{c_i | i \notin D_n\},
L_b = \{l_i | i \in D_b\}, L_n = \{l_i | i \notin D_n\}.

- Computing metrics (similarity difference and p-value of KS test)

\Delta_{cos} = \frac{1}{|C_b|} \sum_{i \in C_b} - \frac{1}{|C_n|} \sum_{j \in C_n}
\Delta_{r_t} = \frac{1}{|L_b|} \sum_{i \in L_b} t - \frac{1}{|L_n|} \sum_{j \in L_n} j</sample>
    <sample id="800">The image shows a slide titled "Experimental Results" with the following content:

- **Copy Data**: AG News, MIND, SST2, Enron Spam
- **Provider’s general Dataset**: WikiText

**Metrics**
- Performance on downstream tasks: ACC
- Detection performance: ΔCOS, ΔL2, p-value

**Setting**
- m = 20, n = 4, frequency interval = [0.005, 0.01]

**Dataset Table**
| Dataset | #Sample | #Classes | Avg. len. |
|---------|---------|----------|-----------|
| SST2    | 68,221  | 2        | 54.17     |
| MIND    | 130,383 | 3        | 66.14     |
| Enron Spam | 33,716 | 2        | 34.57     |
| AG News  | 127,600 | 4        | 236.41    |

The bottom right corner of the image contains a small picture of a person.</sample>
    <sample id="801">Experimental Results • Detection Performance Dataset Method ACC p-value Detection Performance \nSST2 Original 93.76±0.12 &gt; 0.04 -0.07±0.18 0.14±0.36 RedAlarm 93.76±0.12 &gt; 0.09 1.55±0.17 -2.70±0.35 EmbMarker 77.30±0.08 &gt; 0.08 &lt; 4.07±0.07 -4.13±0.74 MIND Original 77.30±0.08 &gt; 0.08 &lt; 4.07±0.07 -4.13±0.74 EmbMarker 77.29±0.12 &lt; 0.05 4.64±0.23 -9.28±0.47 AGNews Original 93.74±0.14 &gt; 0.03 -0.14±0.07 0.46±0.15 RedAlarm 93.74±0.14 &gt; 0.03 -0.14±0.07 0.46±0.15 Ours 93.66±0.12 &lt; 0.01 &lt; 12.85±0.67 -25.70±1.14 Enron Spam Original 94.74±0.06 &gt; 0.02 -0.21±0.07 0.42±0.54 RedAlarm 94.74±0.06 &gt; 0.02 -0.21±0.07 0.42±0.54 EmbMarker 94.78±0.27 &lt; 0.01 &lt; 6.17±0.11 -12.34±0.62</sample>
    <sample id="802">Experimental Results Embedding visualization (a) AG News (b) Enron Spam (c) MIND (d) SST2</sample>
    <sample id="803">Experimental Results Embedding visualization (a) AG News (b) Enron Spam (c) MIND (d) SST2</sample>
    <sample id="804">Thanks</sample>
    <sample id="805">The image shows a presentation slide with the title "Attention as a Guide for Simultaneous Speech Translation" in white text on a blue background. Below the title, there are three names listed: Sara Papi, Matteo Negri, and Marco Turchi. At the bottom of the slide, there are logos from Università di Trento and Fondazione Bruno Kessler. In the top right corner, there is a small video feed showing a person who appears to be giving the presentation or participating in an online meeting. The overall setting suggests that this is a virtual academic or professional conference related to speech translation technology.</sample>
    <sample id="806">The image shows a presentation slide with the title 'What is Simultaneous Speech Translation?' in blue text. Below the title, there are five emojis representing different languages: a speech bubble with Chinese characters, an Arabic script, a Japanese character, and two more that appear to be from other languages. The main content of the slide explains that simultaneous speech translation (SimulST) is "the process of translating spoken language into a text in another language in real-time, enabling cross-language communication." There is also a German phrase 'Wenn ich im Sommer kalten Tee in meine Thermoskanne gieße, bleibt er kalt, und wenn ich Winter' which translates to 'When I pour cold tea in my thermos in summer, it stays cold, and when.' A small logo with a blue circle and some symbols appears at the bottom left corner, along with the page number 'page 02'.</sample>
    <sample id="807">The image shows a presentation slide with the title "What are the problems of the current SimulST models?" in blue text. Below the title, there is an icon resembling a virus or bacteria and some text that reads: "Specific architectures are usually trained, introducing additional modules to be optimized." The bottom right corner indicates this is page 05. There is also a small video feed on the top right side showing a person speaking.</sample>
    <sample id="808">The image contains text that outlines the problems of current SimulST models. It is divided into two main points:

1. Specific architectures are usually trained, introducing additional modules to be optimized.
2. Long and complicated training procedures (e.g., different optimization objectives).

In addition to this textual content, there are visual elements such as icons representing a brain or neural network on both sides of each point. The background appears to be part of a presentation slide with some interface elements at the top right corner, possibly indicating interactive features like comments or questions.

At the bottom left corner, there's an icon resembling a play button, suggesting multimedia functionality might be available in this context. In the lower-right corner, "page 06" indicates this is likely one page from a larger document or slideshow deck.</sample>
    <sample id="809">The image shows a slide from a presentation with the title "What are the problems of the current SimuIST models?" The slide lists three main issues: 1. Specific architectures are usually trained, introducing additional modules to be optimized (e.g., different optimization objectives). 2. Long and complicated training procedures (e.g., objective optimization techniques). 3. Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s,...). There is also an illustration on the left side depicting these points visually. In the top right corner, there's a small video feed showing a person speaking or presenting. At the bottom left corner, there's a logo that appears to represent the organization or institution associated with the presentation.</sample>
    <sample id="810">The text in the image reads: "What is our solution?" and "page 08".</sample>
    <sample id="811">The image shows a presentation slide with the title "What is our solution?" in blue text on the left side. Below this, there are two numbered points:

1. Use already existing offline ST models without re-training or adopting specific architecture for SimulST.
2. Use only one model for every latency regime and handle latency through specific parameters.

In the bottom right corner of the slide, it says "page 09." In the top right corner, there's an inset video call window showing someone speaking. The background behind them includes some curtains and what appears to be part of a room interior. There are also some icons at the top center of the screen that seem related to communication tools like chat, mute, etc., but they're not clearly visible due to their small size.</sample>
    <sample id="812">The image shows a presentation slide with the title "What is our solution?" The slide outlines three main points: 1. Use already existing offline ST models without re-training or adopting specific architecture for SimuST. 2. Use only one model for every latency regime and handle latency through specific parameters. 3. Leverage the knowledge already acquired by the model through attention mechanism between audio input and textual output. On the right side of the slide, there's an illustration showing a waveform above text that reads "I am a student." At the bottom left corner, there are two logos, and at the bottom right corner, it says "page 011".</sample>
    <sample id="813">The video begins with a slide titled "Our solution: EDAtt" in the top left corner, accompanied by some text in Japanese characters. The background is white at the bottom and dark blue at the top. A small logo or icon appears on the lower left side of the screen. In the upper right corner, there is an image of a person.

The scene transitions to another slide where the title changes to "Encoder-Decoder Attention." This new slide has a similar layout but includes additional information about the Encoder-Decoder Attention mechanism. It explains that attention points to words if they are not concentrated (its sum is below a threshold α) towards the last λ speech frames, indicating that the received information is stable enough for translation. The same elements from the previous frame remain consistent throughout this transition.

The final part of the video shows a continuation of the previous slide content. The title remains "Encoder-Decoder Attention," and it reiterates the explanation provided earlier. The slide still features the white and dark blue color scheme, the "EDAtt" logo on the lower left, and the image of the person in the upper right corner. There is no significant change in the visual presentation between these two segments; both slides focus on explaining the concept of Encoder-Decoder Attention in machine translation.</sample>
    <sample id="814">The image shows a presentation slide titled "Our solution: EDAtt" with the subtitle "Encoder-Decoder Attention." The main content of the slide explains that attention points to a word if it is not concentrated (its sum is below a threshold a) towards the last λ speech frames, indicating that the received information is stable. There are some non-English characters above the title and a small video feed in the top right corner showing a person speaking. At the bottom left, there's a logo consisting of two blue circles connected by a line, and at the bottom right, it says "page 014".</sample>
    <sample id="815">The image shows a presentation slide titled "Our solution: EDAtt" with the subtitle "Encoder-Decoder Attention." The main content explains that attention is emitted if the attention is not concentrated (its sum is below a threshold c) towards the last λ speech frames, indicating that received information is stable. There is also a small video feed of a person in the top right corner and some text at the bottom left saying "I am going to talk about..." followed by an audio waveform graphic.</sample>
    <sample id="816">The image contains text related to a solution named EDAtt, which is described as an Encoder-Decoder Attention mechanism. It explains that this system decides whether to emit or not a partial translation based on where attention points to: "a word is emitted if the attention is not concentrated (its sum is below a threshold c) towards the last λ speech frames, meaning that the received information is enough stable." Below this explanation, there is a section labeled "01" with the English phrase "I am going to talk about..." and its German translation "Ich werde reden." The page number at the bottom right corner indicates it's from page 016 of a document.</sample>
    <sample id="817">The image contains a slide from a presentation with the following text and elements: - At the top, it says "Our solution: EDAtt" in blue. - Below that, there is a section titled "Encoder-Decoder Attention" which explains how to decide whether or not a partial translation should be used based on where attention points to. It mentions that if an attention point indicates a word is emitted when the attention is not concentrated (its sum is below a threshold c), then this means the received information is enough stable. - There's also a small graph showing three lines labeled "Ich werde reden," indicating some kind of data visualization related to speech frames. The page number at the bottom right corner changes from 017 to 018 as described by the sequence of images provided.</sample>
    <sample id="818">The image shows a presentation slide with the title "Our solution: EDAtt" at the top. Below this, there is a section titled "Encoder-Decoder Attention," which explains that it decides whether to emit or not a partial translation based on where attention points to: if a word is emitted if the attention is not concentrated (its sum is below a threshold c) towards the last λ speech frames, meaning that the received information is enough stable.

In the bottom left corner of the slide, there is an illustration showing audio waves and text in German ("Ich werde reden"), indicating spoken words being processed by the model. The English translations are provided as "I am going to talk about..." and "EMITTED."

On the right side of the slide, there is some additional text explaining the concept further, but it's partially obscured. In the upper right corner of the image, there is a small video feed of a person speaking. At the bottom of the slide, page numbers 019 and 020 are visible, suggesting that this is part of a larger presentation.</sample>
    <sample id="819">The image shows a presentation slide with the title "Encoder-Decoder Attention" at the top. The main content of the slide explains how to decide whether to emit or not a partial translation based on where attention points to, and mentions that if the sum (its is sum) is below a threshold (a), then it means the received information is enough stable. There is also an example given in German: "Ich werde reden." which translates to "I will speak." Below this text, there are two sound wave diagrams labeled "EMITTED" and "EMITTED," indicating some form of audio output related to language processing. In the bottom left corner, there's a logo for "FZK." On the right side of the slide, there is additional explanatory text about attention mechanisms in neural networks used for machine translation. At the very top of the slide, there is a navigation bar with various icons, including one resembling a question mark. Additionally, there is a small video feed window showing a person speaking, likely part of a virtual meeting interface.</sample>
    <sample id="820">The image shows a presentation slide with the title "Encoder-Decoder Attention" and includes text in both English and German. The main content discusses whether to emit or not a partial translation based on where attention points to, explaining that if an attention is emitted (its sum is below a threshold 'a') towards the last λ speech frames, it means that the received information is enough stable.

There are two numbered sections:
1. "I am going to talk about..." followed by "Ich werde reden." This section has a green waveform labeled "EMITTED."
2. "I am going to talk about climate." followed by "Ich werde Klima sprechen." This section has a blue waveform labeled "not emitted."

At the top left corner of the slide, there's a heading "Our solution: EDAtt." On the right side, there's additional explanatory text regarding the emission criteria for attention. At the bottom right corner, the page number "page 023" is visible.

In summary, this slide explains how the encoder-decoder attention mechanism works, specifically focusing on when to emit translations during the process.</sample>
    <sample id="821">The image shows a presentation slide with the title "Encoder-Decoder Attention" at the top. The main content is divided into two sections labeled 01 and 02, each illustrating different aspects of attention mechanisms in language processing.

Section 01:
- Title: "I am going to talk about..."
- Text below: "EMITTED"
- German text: "Ich werde reden."
- Visual representation: A waveform diagram showing green lines indicating emitted speech or signals.

Section 02:
- Title: "I am going to talk about climate."
- Text below: "Ich werde über Klima sprechen."
- Visual representation: A waveform diagram similar to section 01 but includes additional red dashed lines intersecting the green ones, suggesting partial attention or focus on certain parts of the input.

On the left side of the slide, there's a blue box containing the following text:
"Our solution: EDAtt"

At the bottom right corner, it says "page 025".

In the upper right corner, there is a small video window displaying a person speaking.

Additional information provided explains that the decision whether to emit or not a partial translation based on where attention points to depends on if the attention is concentrated (its sum is below a threshold) towards the last λ speech aspects, meaning that the received information is enough stable.</sample>
    <sample id="822">The image contains text in English and German, structured as follows:

1. **Title:**
   - "Our solution: EDAtt"

2. **Subtitle:**
   - "Encoder-Decoder Attention"

3. **Body Text:**
   - "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its threshold is below a threshold) towards the last speech frames, meaning that the received information is enough stable."

4. **Illustrations with Captions:**
   - Illustration 01:
     - Caption: "I am going to talk about..."
     - German Translation: "Ich werde reden."
     - Status: "EMITTED"
   - Illustration 02:
     - Caption: "I am going to talk about climate."
     - German Translation: "Ich werde über Klima sprechen."
     - Status: "EMITTED"

5. **Footer:**
   - Page number indicator: "page 027"</sample>
    <sample id="823">The image shows a presentation slide with the following content:

- Title: "Main Results: EDAtt"
- A graph is displayed, but it appears to be empty or not loaded properly. The axes are labeled:
  - Y-axis: BLEU
  - X-axis: AL/AL_CA (s)
- Below the graph, there's text indicating "(a) en→de" which suggests that the data pertains to translations from English to German.
- There are some symbols above the title in blue color, possibly representing different types of results or categories such as "L," "E," "W," "Q," "R," "A," "G," and "C."
- In the bottom left corner, there is a logo consisting of three colored shapes (blue, cyan, and dark gray).
- In the top right corner, there is an inset video feed showing a person presenting.

Additional details include:
- Page number at the bottom right corner: page 028</sample>
    <sample id="824">The image shows a presentation slide titled "Main Results: EDAtt" with an empty graph plotting BLEU scores against AL/AL_CA (s). The x-axis ranges from 0.5 to 6, and the y-axis is labeled BLEU ranging from 17 to 27. There are annotations in blue text above the graph asking questions like "¿?", "!", "?", "?", and "¿?". In the bottom left corner, there's a logo of "F-K", and on the right side, there's a small video feed showing a person speaking. At the bottom center, it says "(a) en→de," indicating that this graph represents results for English to German translation. Additionally, at the bottom right corner, there's a page number "page 028."</sample>
    <sample id="825">The image shows a presentation slide with the title "Main Results: EDAtt" in blue text. Below this, there is a graph plotting BLEU scores against AL/AL_CA (s) on the x-axis and latency measure at the bottom right of the graph. The y-axis ranges from 21 to 27. In parentheses below the graph, it reads "(a) en-&gt;de". There are some symbols or characters above the graph that appear to be part of an equation or notation. On the left side of the slide, there's a logo consisting of three curved lines forming a shape resembling a smiley face. At the bottom right corner, there is page number information indicating "page 030". Additionally, there is a small video feed showing a person in the top right corner of the slide.</sample>
    <sample id="826">The image shows a presentation slide with the title "Main Results: EDAtt" in blue text. Below this, there is a bar graph displaying data for translation from English to German (en→de). The x-axis of the graph represents AL/AL_CA (s), ranging from 0.5 to 6 seconds, and the y-axis represents BLEU scores, which range from approximately 21 to 27. A single vertical blue bar extends nearly to the top of the graph, indicating a high BLEU score around 27. At the bottom left corner of the slide, there are two logos or icons; one appears to be a stylized letter 'E' followed by a dot, and the other resembles an abstract face with circular eyes and a curved mouth. In the lower right corner, it says "page 031". Additionally, at the top center of the slide, there are several emojis including a question mark, a thumbs-up hand gesture, a smiling face, a heart-eyed face, and a thinking face, possibly representing different reactions or emotions related to the results presented.</sample>
    <sample id="827">The image contains a slide from a presentation with the following text:

- Main Results: EDAtt
- (a) en→de

There is also some additional text in blue at the top of the slide, which appears to be questions or comments:
- 什么? 什么? 什么? 什么? 什么?

In the bottom right corner, there is "page 032". The graph shows data points plotted on an x-axis labeled "AL/AL_CA (s)" ranging from approximately 0.5 to 4 and a y-axis labeled "BLEU" ranging from about 17 to 27.

Additionally, there is a small video feed window showing a person speaking into a microphone.</sample>
    <sample id="828">The image shows a presentation slide with the title "Main Results: EDAtt" on the left side. At the top, there is text that reads "popular strategies also applied to offline models." Below this, there are four lines labeled as follows: wait-k (yellow), LA (blue), CAAT (green), and EDAtt (red). The main content of the slide is a graph plotting BLEU scores against AL/AL_CA (s) for translations from English to German ("en-&gt;de"). The x-axis ranges from 0.5 to 2.7, while the y-axis ranges from 17 to 27. There is an annotation at the bottom center of the graph indicating "(a) en-&gt;de."

In the upper right corner of the slide, there is a small video feed showing a person in what appears to be a virtual meeting or webinar setting.

At the bottom right corner of the slide, it indicates "page 033," suggesting this is part of a larger presentation document. Additionally, there is some blue logo-like design element visible at the bottom left corner of the slide.</sample>
    <sample id="829">The image shows a slide from a presentation with the title "Main Results: EDAtt" at the top left corner. The main content of the slide is a graph labeled (a) en→de, which plots the performance of different models on translation tasks over time or some other metric along the x-axis ranging from 0 to 5 seconds. 

The y-axis represents BLEU score, ranging from 17 to 27. There are four lines representing different models:
- wait-k (orange line)
- LA (blue dashed line)
- CAAT (green dotted line)
- EDAtt (red solid line)

Above the graph, there are emojis and symbols that appear to be related to language processing or machine learning concepts.

In the bottom right corner, there's a page number indicating this is page 035. Additionally, in the lower-left corner, there is an icon resembling a blue circle with a white symbol inside it, possibly representing the logo of the organization or software used for creating the presentation.

In the upper-right corner, there is a small video feed showing a person who appears to be presenting or participating in the meeting.</sample>
    <sample id="830">The image shows a presentation slide titled "Main Results: EDAtt" with a graph and some text. The graph plots BLEU scores against AL/AL_CA (s) for different strategies, including wait-k, LA, CAAT, and EDAtt. A blue box on the left side of the graph states, "EDAtt outperforms all the strategies applied to offline models." At the bottom right corner, there is a page number indicating it's page 035. Additionally, in the top right corner, there is an inset video call showing a person speaking.</sample>
    <sample id="831">The image shows a presentation slide with the title "Main Results: EDAtt" at the top left corner. The main content of the slide is a graph plotting "AL/AL_CA (s)" on the x-axis against "BLEU" on the y-axis, indicating some form of performance measurement over time for different strategies or methods labeled as wait-k, LA, CAAT, and EDAtt.

In the center-right part of the slide, there is a blue text box that reads, "EDAtt is the fastest strategy if we consider the actual elapsed time." This suggests that among the compared strategies, EDAtt has the shortest total duration when considering real-time performance metrics.

At the bottom right corner of the slide, it states "(a) en→de," which likely indicates that this particular result pertains to an English to German translation task.

The page number "page 037" is visible in the bottom right corner, suggesting this is from a larger document or report.

Additionally, there are small icons above the graph representing various communication tools such as chat bubbles, video calls, and other interactive elements, possibly indicating features available during the presentation or related activities.

Overall, the slide presents findings about the efficiency of different machine translation strategies, highlighting EDAtt's effectiveness based on actual elapsed time measurements.</sample>
    <sample id="832">The image appears to be a screenshot from an online presentation or webinar. It contains text and visual elements designed for viewers to engage with the content being presented.

At the top of the slide, there is a question in blue text that reads: "Do you want to discover more?" Below this, another line of larger blue text states: "Read our paper to discover more results!"

On the left side of the slide, contact information is provided:
- An email address: {spapi,negli}@fbk.eu
- Another email address: marco.turchi@gmail.com
- A GitHub link: github.com/hlt-mt/fbkm-fairseq
- Two Twitter handles: @fbk_mt and @sarapapi

In the center-right part of the slide, there is a QR code labeled "Scan me!" which likely links to additional resources or further reading material related to the topic discussed in the presentation.

In the bottom right corner, it indicates that this is page 038 of the document or presentation.

Additionally, on the far right edge of the screen, there is a small video feed showing a person who seems to be presenting or participating in the webinar.

Overall, the layout suggests that this is a professional setting aimed at sharing research findings through a structured format including direct engagement prompts (like scanning a QR code) and providing multiple channels for follow-up communication via email and social media.</sample>
    <sample id="833">The authors of the paper are affiliated with Google, as indicated by their names appearing under a small image associated with the company.</sample>
    <sample id="834">The authors of the paper are affiliated with Stony Brook University, specifically within their Human Language Analysis group.</sample>
    <sample id="835">The paper analyzed the following language pairs: English-German, English-French, and Chinese-English.</sample>
    <sample id="836">The name of the speaker is Chan Young Park.</sample>
    <sample id="837">The models investigated during the experiments were DEPLAIN-APA and DEPLAIN-WEB.</sample>
    <sample id="838">53 tasks are used for training, and 20 tasks are used for testing.</sample>
    <sample id="839">There are three authors involved in the paper: Regina Stodden, Omar Momen, and Laura Kallmeyer.</sample>
    <sample id="840">The datasets the authors experimented on are AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="841">The image contains a presentation slide with the following text: "Language model acceptability judgements are not always robust to context" and "ACL 2023". Below this, there is additional information listing names: Kousuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Kerem Fuentes, Roger Levy, Adina Williams. On the right side of the slide, there are logos from Johns Hopkins University, Purdue University, MIT, and Meta AI.</sample>
    <sample id="842">The image features a presentation slide with the title "Language model acceptability judgements are not always robust to context" prominently displayed in large white text on a dark background. Below this main title, there is additional information indicating that this content was presented at ACL 2023.

On the right side of the slide, there is an inset circular photo of a person wearing glasses and a red shirt. Adjacent to this photo, logos from several prestigious institutions are visible: Johns Hopkins University, Purdue University, MIT, and Meta AI.

At the bottom left corner of the slide, there is a list of names which likely represent the authors or contributors to the work being presented:
- Koustuv Sinha
- Jon Gauthier
- Aaron Mueller
- Kanishka Misra
- Keren Fuentes
- Roger Levy
- Adi Williams

This layout suggests it's part of an academic or professional presentation discussing aspects related to language models and their contextual understanding.</sample>
    <sample id="843">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs:

BLiMP
1. Many people were helping themselves.
2. Many people were helping herself.

SyntaxGym
No content provided for SyntaxGym section.

Crows
1. Stereotypical narrative sentence.
2. Not a stereotypical sentence.

P(1) ≫ P(2)
P(1.any) ≫ P(2.any)
P(1) ≫ P(2)</sample>
    <sample id="844">Revisiting Minimal Pair Paradigm</sample>
    <sample id="845">Revisiting Minimal Pair Paradigm</sample>
    <sample id="846">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs:

BLiMP
1. Many people were helping themselves.
2. No customer ... has spent any money.

P(1) ≫ P(2)

SyntaxGym
1. Many people ... has spent any money.

P(1) ≫ P(2.any)

CrowS
1. Stereotypical Noun-Adjective sentence.
2. Stereotypical Noun-Adjective sentence.

P(1) &gt; P(2)</sample>
    <sample id="847">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to estimate the abstract knowledge of LMs:

BLiMP
1. Many people were helping themselves.
2. No customer ... has spent any money.
3. Many people were helping herself.

P(1) ≫ P(2)

SyntaxGym
P(1.any) ≫ P(2.any)

CrowS
Women are terrible at handiwork.
Men are terrible at handiwork.

P(1) ≫ P(2)

Are these judgements stable with long preceding context?</sample>
    <sample id="848">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to estimate the abstract knowledge of LMs:

BLiMP
- Many people were helping themselves.
- No customer ... has spent any money.
- Many people were helping herself.

SyntaxGym
P(1.any) ≫ P(2.any)

CrowS
Women are terrible at handiwork.
Men are terrible at handiwork.

P(1) ≫ P(2)

Are these judgements stable with long preceding context?</sample>
    <sample id="849">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to estimate the abstract knowledge of LMs:

BLiMP
1. Many people were helping themselves.
2. No customer ... has spent any money.

P(1) ≫ P(2)

SyntaxGym
1. Many people were helping herself.

P(1.any) ≫ P(2.any)

CrowS
1. Women are terrible at handiwork.
2. Men are terrible at handiwork.

P(1) ≫ P(2)

Are these judgements stable with long preceding context?</sample>
    <sample id="850">The image contains a slide titled "Approach" which discusses the evaluation of MPP judgments based on context length, structural match, and acceptability. The mathematical expression P_{L,M}(|PreFix|) &gt; P_{L,M}(|PreFix|) is mentioned as part of this approach.

Below the title, there's an illustration showing different types of candidate prefixes with examples such as "What Jessica will do before noticing those spotlights?" and "What Aaron said while watching the museum?". These examples are used to illustrate how space of candidate prefixes can be matched or unaccepted in terms of their acceptability.

At the bottom left corner of the slide, it states that GPT-2, OPT family ranges from 125M to 6.7B parameters. On the right side, within a box labeled "Acceptable, Matched", two example questions are provided: "What Jessica will do before noticing those spotlights?" and "What Aaron said while watching the museum?". 

Additionally, there's another question outside the box asking, "What Jessica could have been doing just now when she was cleaning the museum? What might Jessica notice next if she were cleaning the museum?"

Overall, the slide seems to present a method for evaluating text generation models by comparing predicted sequences against expected patterns or contexts.</sample>
    <sample id="851">The image contains a slide titled "Approach" with the following text: 

"Test whether MPP judgements as a function of context length, structural match, and acceptability"

Below this title is an equation involving \( P_{LM} \) and \( P_{LM}^{|Prefix|} \), which seems to be related to language models or machine learning. There's also some text about subject-verb agreement in a test suite.

On the right side, there are two boxes labeled "Acceptable" and "Matched," containing examples like:
- "What role did Aaron have before returning from his trip?"
- "What Jessica would do before noticing those highlights?"

At the bottom left corner, it mentions "GPT2, OPT family - 125M to 6.7B."

In the top-right corner, there is a photo of a person wearing glasses and smiling. The background appears to be white, suggesting that this might be part of a presentation slide deck on evaluating judgments based on certain linguistic criteria using specific models.</sample>
    <sample id="852">The image shows a slide from a presentation with the title "Approach" at the top. The main content of the slide is about testing whether MPP judgments are influenced by context length, structural match, and acceptability.

There's an equation in the middle that reads: P(MP|Prem) &gt; P(EM|Prem), which seems to be related to some kind of judgment or decision-making process involving context (MP) versus evidence (EM).

Below this equation, there's a diagram labeled "Space of Candidate Prefixes," showing different types of prefixes like "Build," "Agreement," etc., along with symbols for "Context Length," "Structural Match," and "Acceptability."

On the right side of the slide, there's text discussing scenarios such as:
- "What fees might lose free from before returning to this customer?"
- "What Jessica would just notice before noticing these spotslights? What Aaron had seen while returning to this customer?"

These seem to be examples of questions being analyzed based on their context length, structural match, and acceptability.

At the bottom left corner, it says "GPT2, OPT family - 125M to 6.7B," indicating the models used in the approach.

In the bottom right corner, there's a label "BLIMP, Island." 

Overall, the slide appears to explain a method for evaluating how well certain language models can understand and generate coherent responses given specific linguistic features.</sample>
    <sample id="853">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="854">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability.</sample>
    <sample id="855">The image contains a slide with the title "Approach" and text that reads: "Test whether MPP judgements as a function of context length, structural match, and acceptability." Below this is an equation involving P(LM|PreFix) &gt; P(ELM|PreFix). The slide also includes diagrams labeled "Space of Candidate Prefixes," showing various symbols like a book icon. There are two boxes on the right side; one says "Acceptable, Mismatched" and lists examples such as "There was once a documentary about music intiating [sic] Ablan," which contained no legible words, or "There was a documentary about music intiating [sic] Ablan," followed by more text. At the bottom left corner, there's additional text stating "GPT2, OPT family - 125M to 6.7B". In the lower right corner, it mentions "BLUMP, Etiolated 'Essential' Queries", and in parentheses next to it, "BLUMP | Adjacent Island".</sample>
    <sample id="856">The image contains a slide with the title "Approach" at the top. Below the title, there is text that reads: "Test whether MPP judgements vary as a function of context length, structural match, and acceptability." There are two sections labeled "acceptable" and "unacceptable," each containing examples related to space candidate prefixes.

On the left side, under "Space Candidate Prefixes," there is a diagram showing different types of prefixes such as "Build Agreement," "Unmatched," and others. The diagram includes symbols like a square with an 'X,' a circle with a checkmark, and other geometric shapes representing various prefix conditions.

To the right of this section, there are sample sentences:
- For acceptable, mismatched: "Who might lose fees from before returning to this documentary about music? There were no legacies working hardest in Allston."
- For unacceptable, mismatched: "There was every documentary about music working hardest in Allston."

At the bottom of the slide, it states: "GPT2, OPT family - 125M to 6.7B" on the left and "BLUMP, ESSR, 'Essential' Queries | BLUMP, Adjunct Island" on the right.

In the upper right corner, there is a circular inset photo of a person wearing glasses.</sample>
    <sample id="857">The image contains a slide with the title "Approach" and discusses testing whether MPP judgments are influenced by context length, structural match, and acceptability. It includes an equation \( P_{LM}(\text{Prefix}) &gt; P_{EM}(\text{Prefix}) \) to illustrate this concept.

There is also a section titled "Space of Candidate Prefixes," which shows different methods for building and verifying prefixes: "Mechanized" and "Micro-verified." 

Additionally, there's text from Wikipedia labeled "Unrelated" that describes a passage about a recently discovered planet called Gliese 581c, mentioning its potential habitability due to three nearby planets being in its star's Goldilocks zone.

At the bottom, it states "GPT2, OPT family - 125M to 6.7B," indicating the range of parameters or models used in the approach discussed on the slide.</sample>
    <sample id="858">Approach Test whether MPP judgements function as a context length, structural match, and acceptability Wikipedia, Unrelated GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="859">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability Wikipedia, Unrelated GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="860">The image contains a title, text descriptions, and graphical data. The main content includes: 1. A bold statement at the top that reads "MPP judgements are robust for arbitrary context lengths." 2. Below this, there is additional descriptive text explaining MPP evaluations with different contexts—acceptable/unacceptable; matched/mismatched structure—of lengths up to 900 tokens. 3. On the right side of the image, there is an inset containing two paragraphs in green font providing further details about specific cases or examples related to the evaluation process mentioned above. 4. At the bottom left corner, there's a label "BLIMP, OPT 6.7B" which might refer to software or model used in the study. Additionally, on the graph part of the slide, various lines represent different strategies (Matched, Unmatched) over time, showing trends such as increases or decreases across the x-axis labeled "Prefix Strategy," ranging from -500 to +500.</sample>
    <sample id="861">The image contains a title, text description, and a graph. The title reads "MPP judgements are robust for arbitrary context lengths." Below the title is a subtitle that states: "We perform MPP evaluations with different contexts — acceptable / unacceptable; matched/mismatched structure — of lengths up to 900 tokens."

To the right side of the image, there is an inset box containing two questions:
1. "A rose is a woody perennial flowering plant of the genus Rosa, known for its showy flowers in spring and summer and for its fruit, the rosehip."
2. "Is this sentence from the Wikipedia page about roses? (Yes/No)"

Below these questions, it says:
"Note: A 'woody perennial' flowering plant of the genus Rosa has been mentioned before by customers"

At the bottom left corner of the image, there is a label that reads "BLIMP, OPT 6.7B."

The central part of the image features a line graph titled "Prefix Strategy," which plots data points labeled as "Acc. (Matched)," "Unacc. (Matched)," "Acc. (Mismatched)," and "Unacc. (Mismatched)." The x-axis ranges from 0 to 500, while the y-axis ranges from -0.2 to 0.2.

In the top-right corner of the image, there is a circular photo of a person wearing glasses and smiling.</sample>
    <sample id="862">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance

We perform MPP evaluations with different contexts — acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens</sample>
    <sample id="863">The image contains a slide from a presentation with the following text:

Title: Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance

Subtitle: We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens

Graph:
- The graph shows three lines labeled as "BLIMP, OPT.6.7B" and has an x-axis titled "Prefix Strategy" ranging from 0 to 800.
- There are two numbered points on the right side of the graph indicating specific data points or observations.

Text next to Graph Points:
1. "There was a documentary about music inflating. There were no sales working hard before this year? Who might Rise lose from this customer?"
2. "There was a documentary about music inflating. There might be most labels B before this year? Who might Rise lose from this customer?"

Additional Text Below Graph:
- "BLIMP, OPT.6.7B"

Legend (at the bottom):
- "Acc. (Accepted)"
- "Unacc. (Unaccepted)"
- "Wiki (Wiki)"
- "Unmacc. (Unmatched)"

The overall layout suggests that the slide is discussing the impact of sentence acceptability within certain contexts on judgment performance, supported by graphical data and examples provided alongside the chart.</sample>
    <sample id="864">The image contains text in English, which appears to be part of a presentation slide. The main title reads "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance." Below this, there is additional text that states: "We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens."

There are also two numbered points on the right side of the slide:
1. There was a documentary about music inflating.
2. There were most legislations before this year.

At the bottom left corner, it says "BLIMP, OPT 6.7B," and at the bottom center, there is a graph labeled "Prefix Strategy" showing various lines representing different metrics such as "Acc (Accepted)," "Unacc (Unaccepted)," "Wiki (Wiki)," and "Unacc (Unaccepted)." 

The overall content suggests an analysis or study related to machine parsing performance based on sentence acceptability and contextual factors.</sample>
    <sample id="865">The image contains a detailed graph and text related to model performance evaluations. The main title reads, "Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance." Below the title, there is additional information stating, "We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens."

The graph itself shows various lines representing different strategies: "BLIMP, OPT 6.7B," "Prefix Strategy," "Acc (Unmatched)," "Unacc (Unmatched)," "Wiki (Unmatched)." These lines are plotted against a range on the x-axis from 0 to 500.

On the right side of the image, there are three numbered questions:
1. What could Jessica sell before noticing these spotlights?
2. Who had gone ahead? Had she been cleaning the museum when Jessica was returning here in this customer?
3. What could Jessica sell before noticing these spotlights?

Below each question, it states, "Before noticing these spotlights?" indicating that these examples might be used for evaluation purposes within the context of the study or analysis being presented.

In summary, the image appears to be an academic slide discussing the impact of sentence structures on model performance during machine parsing (MPP) evaluations.</sample>
    <sample id="866">The image contains a detailed graph and text discussing the impact of MPP (Most Probable Phrase) sentences on model performance. Here is the structured content:

1. **Title**: 
   - "Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance"

2. **Subtitle**:
   - "We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens"

3. **Graph Description**:
   - The graph shows various lines representing different conditions: Prefix Strategy, Unmatch (red), Wiki (green), and Unmatched (blue).
   - The x-axis represents some metric from 0 to approximately 675.
   - The y-axis ranges from 0 to 0.2.

4. **Annotations on Graph**:
   - Three numbered annotations pointing to specific areas in the graph:
     1. "What could Jessica sell before noticing these spotlights? What had Jessica said about her previous customer returning this customer?"
     2. "What could Jessica sell before noticing these spotlights? What had Jessica said about cleaning the museum?"
     3. "What could Jessica sell before noticing these spotlights? What had Jessica said about cleaning the museum?"

5. **Additional Text**:
   - At the bottom left corner, it says "BLIMP, OPT 6.7B"
   - On the right side, there are three questions related to the context provided by the graphs:
     1. "What could Jessica sell before noticing these spotlights? What had Jessica said about her previous customer returning this customer?"
     2. "What could Jessica sell before noticing these spotlights? What had Jessica said about cleaning the museum?"
     3. "What could Jessica sell before noticing these spotlights? What had Jessica said about cleaning the museum?"

This description provides a comprehensive overview of the visual elements and textual information present in the image.</sample>
    <sample id="867">The image contains a detailed graph and text related to model performance evaluations. Here is the transcription of the English content:

Title:
- Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance

Body Text:
- We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens

Graph Labels:
- BLIMP, OPT 6.7B
- Prefix Strategy (x-axis)
- Unmatched (y-axis)

Graph Annotations:
1. What could Jessica sell before noticing these spots?
2. What had Jessica said about her plans for cleaning the museum?
3. What could Jessica sell before noticing these spots?

Additional Information:
- The graph shows three lines representing different strategies or conditions: 
  - A line labeled "Unmatched" in pink.
  - Two other lines that are not explicitly labeled but can be inferred from their positions on the graph.

The image also includes some blurred sections where personal information has been obscured.</sample>
    <sample id="868">Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. - Prefix/suffix adverbs: “However, &lt;sent&gt;.” - Long prefix adverbs: “First and foremost, &lt;sent&gt;.” - Add clause: “Regardless of what X thinks about it, &lt;sent&gt;.” - Quote: “Yesterday, X said, &lt;sent&gt;.”</sample>
    <sample id="869">The image contains a slide from a presentation with the title "Why do matched prefixes affect LM judgements?" The subtitle reads, "We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences." Below this, there is an explanation of different types of adverbs used for perturbation: - Prefix/suffix adverbs: “However, &lt;sent&gt;.” - Long prefix adverbs: “First and foremost, &lt;sent&gt;.” - Add clause: “Regardless of what X thinks about it, &lt;sent&gt;.” - Quote: “Yesterday, X said, &lt;sent&gt;.” On the right side of the slide, there is a small circular photo of a person. At the bottom of the slide, there is a graph showing the relationship between input length (x-axis) and accuracy (y-axis). The legend on the left indicates various types of perturbations such as 'None,' 'Prefix/suffix adv,' 'Long prefix adv,' 'Add clause,' 'Quote,' 'All,' and 'Acceptable.' The x-axis ranges from 0 to 650, labeled 'Input Length,' while the y-axis measures 'Delta Accuracy' ranging from approximately -0.12 to +0.14.</sample>
    <sample id="870">The image contains a slide with the title "Why do matched prefixes affect LM judgements?" It discusses perturbing context sentences to preserve structure and asks whether models are sensitive to these changes. The slide lists types of prefix/suffix adverbs, including "However, &lt;sent.&gt;", "First and foremost, &lt;sent.&gt;", "Regardless of what X thinks about it, &lt;sent.&gt;", and "Yesterday, X said, &lt;sent.&gt;". There is also a graph showing Δ Accuracy against Input Length for different perturbations like None, Prefix/suffix adv, Add clause, etc., categorized by Prefix Type (Acceptable or Unacceptable).</sample>
    <sample id="871">The image contains a slide from a presentation with the title "Why do matched prefixes affect LM judgements?" The main content of the slide includes:

- A brief explanation: "We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences."
- Examples of perturbed sentences:
  - Prefix/suffix adverbs: "However, &lt;sent&gt;"
  - Long adverbs: "First and foremost, &lt;sent&gt;"
  - Add clause: "Regardless of what X thinks about it, &lt;sent&gt;"
  - Quote: "Yesterday, X said, &lt;sent&gt;"
- A statement at the bottom left corner: "Models are not sensitive to perturbed sentences in these cases."

On the right side of the slide, there is a graph titled "Perturbation" showing the relationship between "Input Length" on the x-axis and "Δ Accuracy" on the y-axis. Different lines represent different types of perturbations such as None, Prefix/suffix adv, Long adv, Add clause, All, and Unacceptable.

In the top-right corner of the image, there is a circular inset photo of an individual who appears to be associated with the presentation or research being discussed.</sample>
    <sample id="872">The image contains a slide with the title "Why do matched prefixes affect LM judgements?" and includes several bullet points. The first point is about prefix/suffix adverbs, providing an example: "However, &lt;ent&gt;." Another point discusses long adverbials, giving examples like "First and foremost, &lt;ent&gt;." There's also a mention of add clause with an example sentence: "Regardless of what X thinks about it, &lt;ent&gt;." Additionally, there's a quote in Spanish: "Yesterday, X said, &lt;ent&gt;."

Below these points, there's text stating that models are sensitive to perturbed sentences in certain ways.

On the right side of the slide, there's a graph showing some data related to the topic discussed on the left. The x-axis is labeled "Input Length," while the y-axis has two labels: "Δ Accuracy" at the top and "Prefix Type" at the bottom. Different lines represent different types of perturbations or prefix types such as 'None,' 'Prefix/suffix adv,' 'Long adv,' 'Add clause,' 'Quote,' etc., each marked by distinct colors (e.g., yellow for 'None,' red for 'Prefix/suffix adv,' green for 'Long adv,' blue for 'Add clause,' orange for 'Quote'). A legend indicates which color corresponds to which type of perturbation.

In the upper-right corner of the image, there's a circular inset containing a photo of a person wearing glasses and smiling.</sample>
    <sample id="873">The image contains a slide titled "Key Takeaways" with the following points: 1. Language models are sensitive to latent syntactic/semantic features shared across sentences. 2. MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.

There is also a graph on the right side of the slide showing some data trends and equations related to P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{LM}(|X| | P_{LM}(|P_{</sample>
    <sample id="874">The image contains a slide with the title "Key Takeaways" and includes two bullet points: 1. Language models are sensitive to latent syntactic/semantic features shared across sentences. 2. MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.

There is also a graph on the right side of the slide showing some data trends over time or another variable labeled from 0 to 650 in increments of 100 (though it's unclear what these numbers represent without additional context). The legend for the graph indicates different metrics being measured such as "Text Splits," "Perplexity," "Accuracy," etc., but specific values aren't provided here either.

At the bottom left corner, there's an illustration titled "Space of Candidate Profiles." This diagram appears to categorize profiles based on their characteristics like "Early Adopter," "Innovator," and others, though exact details about each category cannot be discerned clearly from this description alone.

Lastly, there is a circular inset photo at the top-right corner which seems to show part of a person’s face, likely indicating that they might be related to the content presented in the slide, perhaps the author or presenter of the material.</sample>
    <sample id="875">The text in the image is as follows:

**Title:**
Key Takeaways

**Body Text:**
- Language models are sensitive to latent syntactic/semantic features shared across sentences.
- MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.

**Graph Labels:**
- x-axis: 0, 200, 400
- y-axis: 0.15, 0.3, 0.45, 0.6

**Equation:**
\[ P_{LM}(\text{|Sentence|} \mid \text{Sentence}) &gt; P_{LM}(\text{|Sentence|} \mid \text{Pret}) \]

**Diagram Labels:**
- Space of Candidate Profiles
- Mapped 
- Unmapped 

**Legend Symbols:**
- Various symbols representing different datasets or categories (not clearly readable)</sample>
    <sample id="876">NACHOS is a model for language modeling in healthcare.</sample>
    <sample id="877">The name of the speaker is David Vilarr Torres.</sample>
    <sample id="878">The prompting strategy has a significant impact on the results, as it can lead to differences of up to 40 BLEURT points in translation quality.</sample>
    <sample id="879">The affiliations of the authors are: Carnegie Mellon University Language Technologies Institute, Tecnico Lisboa (University of Lisbon), Berkeley Artificial Intelligence Research (BAIR) lab at UC Berkeley, and Unbabel.</sample>
    <sample id="880">The 5 expert-written instructions are: "How to use the dataset", "How to create your own vision-language tasks", "How to contribute to our community", "How to get support from us", and "How to stay updated with new features".</sample>
    <sample id="881">The authors propose to test the models on a "Coreference resolution task" that probes the ability to draw on both pretrain-time knowledge and inference-time knowledge.</sample>
    <sample id="939">Common evaluation methods for dialogue systems include Likert rating, human judgment, and comparative analysis.</sample>
    <sample id="940">The paper involves five authors.</sample>
    <sample id="941">The example with Servin and Kea requires two types of background knowledge: entity-specific knowledge, which includes the roles of a judge and baker; and background knowledge about how judges decide cases in courts.</sample>
    <sample id="942">Yes, the code is available on GitHub at the repository "mpoems1l/kitmus".</sample>
    <sample id="943">No, the annotators for NLPositionality are not balanced in regard to each demographic.</sample>
    <sample id="944">By adding "sent" to the end of sentences.</sample>
    <sample id="945">To have a dimensional evaluation means to assess the quality of dialogue based on specific dimensions or criteria. These dimensions help in understanding and improving the effectiveness of communication by focusing on aspects such as relevance, consistency, and emotional understanding.</sample>
    <sample id="946">The authors are affiliated with the University of Science and Technology of China, Microsoft Research Asia, Beijing Jiaotong University, Sony AI, and Microsoft STC Asia.</sample>
    <sample id="947">The form of the prompting is important in cases where it helps to ensure that the translation accurately conveys all relevant information from the source text. This can be crucial for maintaining clarity and context, especially when dealing with complex or sensitive topics such as legal matters (transporting a prisoner) or official communications involving multiple parties (complaints received by an office). Properly formatted prompts help translators capture essential details without omitting any critical elements that might alter the meaning or impact of the translated content.</sample>
    <sample id="948">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz *presenter Stony Brook University Human Language Analysis Beings</sample>
    <sample id="949">The image contains a slide from a presentation with the title "What is Cognitive Dissonance?" The main content of the slide defines cognitive dissonance as "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" and cites Harmon-Jones and Harmon-Jones, 2007. At the bottom left corner, there is a citation for Eddie Harmon-Jones and Cindy Harmon-Jones, 2007: "Cognitive dissonance theory after 50 years of development. Zeitschrift für Sozialpsychologie, 38(1), 716." In the top right corner, there is an inset showing a person named "Samantha M. Verderamo," who appears to be presenting or participating in the discussion.</sample>
    <sample id="950">The image contains a slide from a presentation with the title "What is Cognitive Dissonance?" The main content of the slide explains that cognitive dissonance involves two elements: thoughts and actions, or beliefs and behaviors, which are inconsistent. This definition is attributed to Harmon-Jones and Harmon-Jones (2007). Below this explanation, there is an illustration showing a person's head in profile view, accompanied by text boxes labeled "belief" and "action." One box reads, "I know that cigarettes could kill me," while another states, "I grabbed a couple smokes after the meeting today." To the right of these statements, the term "Dissonance" is written, indicating the conflict between the belief and action described. At the bottom of the slide, there is a citation for further reading on cognitive dissonance theory, referencing work by Eddie Harmon-Jones and Cindy Harmon-Jones published in 2007 in Zeitschrift für Psychologie.</sample>
    <sample id="951">The image shows a slide from a presentation about cognitive dissonance. The title at the top reads "What is Cognitive Dissonance?" Below this, there are two definitions provided: 1) "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" and 2) "Expressed in language as a relationship b/w two phrases/statements by a user." There's an illustration with three sequences labeled seq 1, seq 2, and seq 3, each containing statements like "I know that cigarettes could kill me," "I grabbed a couple smokes after the meeting today," and "I don't think I could keep my job without them." These sequences have labels such as belief, action, and conscience/attraction/dislike. At the bottom left corner, there is a citation for Eddie Harmon-Jones and Norma Harmon-Jones' work on cognitive dissonance theory published in 2007.</sample>
    <sample id="952">The image contains a slide titled "What is Cognitive Dissonance?" It defines cognitive dissonance as having two elements of cognition (thoughts, actions, beliefs) that are inconsistent. The text explains that this is expressed in language through the relationship between phrases/statements by a user and notes that it's relatively rare to find in language compared to other discourse relations.

Below the main content, there are three sequences labeled seq 1, seq 2, and seq 3:
- seq 1: "I know that cigarettes could kill me."
- seq 2: "I grabbed a cigarette smokes after the meeting today."
- seq 3: "I didn't keep my job because I don't smoke."

Each sequence includes labels for belief, action, and conscience/another belief.
At the bottom left corner, there's a citation: "Eddie Harmon-Jones and Cindy Harmon-Jones, 2007. Cognitive dissonance theory after Miller. Zeitschrift fur Sozialpsychologie," indicating the source material used for the presentation.

In the top right corner, there is an avatar or profile picture of a person with blonde hair wearing sunglasses.</sample>
    <sample id="953">The image is a slide from a presentation titled "Why dissonance?" It features two stick figures having a conversation, with one figure saying something and the other listening. Below this illustration, there's text that reads "Effects of disagreement." To the right side of the slide, another section shows an upward trending graph labeled "Attitudes and Belief trends." At the bottom left corner, there's a citation: "Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive Researching: A global theory in psychology." The background of the slide is white, and it appears to be part of a larger presentation or lecture.</sample>
    <sample id="954">The image contains a slide titled "Why dissonance?" with three sections. The first section on the left shows two stick figures having an argument, labeled "Effects of disagreement." The second section in the middle has a bar graph increasing upwards and is labeled "Attitudes and Belief trends." The third section at the bottom right features a stylized head with gears inside it, representing thought processes, and is labeled "Anxiety disorders." At the top right corner, there's a small video feed showing a person speaking. Below the main content, there are references to academic sources: - Edie Harmon-Johnson &amp; Judson Mila, 2019: An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive Dissonance: Resolving a perennial theory in psychology (pp. 3-28). - Matthew Memon, Vahidah Verzandjani, Johannes Eichstaedt, Aditya V Gansawala, &amp; H Andrew Schwartz, 2021: Discourse-level representations can improve prediction of anxiety disorder diagnosis from speech data. Proceedings of the First Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.</sample>
    <sample id="955">Why dissonance? Effects of disagreement Entry and Exit from Extremism Attitudes and Belief trends Anxiety disorders Eddie Harmon-Jones and Judson Mills 2019. An Introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive Dissonance: Resolving a psychological theory in social psychology (pp. 3-45). In J. Swim, M. Mathews, V. Vasudevan, A. Gavinsan, H. Andrew, &amp; S. A. V. Gavinsan (Eds.), Cognitive Dissonance: Theory and Applications (pp. 7-26).</sample>
    <sample id="956">The image shows a slide from a presentation titled "Why dissonance?" with five sections. The first section is labeled "Effects of disagreement" and features an illustration of two stick figures arguing. Below it, the second section is labeled "Cognitive Styles" and includes an illustration of a brain with gears inside. To the right, there are three more sections: one titled "Attitudes and Belief trends" showing a bar graph trending upwards; another titled "Entry and Exit from Extremism" depicting a figure speaking to a group; and the last one titled "Anxiety disorders," which has an illustration of a head with exclamation marks around it. At the bottom left corner, there's a list of authors for a paper on cognitive dissonance theory. In the top right corner, there's a small window showing a person in what appears to be a video call interface.</sample>
    <sample id="957">The image contains a flowchart and text related to annotations. The flowchart has three steps: Step 1 asks if the parsing quality is good, with 'Yes' leading to Step 2 asking about dissonance, which then leads to Step 3 also inquiring about dissonance. There are percentages shown for each step indicating the proportion of responses (e.g., -3.5%, -48%). Below the flowchart, there's an example annotation from a user with the handle @user_handle that reads "Wish I could hold grudges but I guess it’s a good thing that I can’t at the same time." This section is labeled "ANNOTATE" on the left side and "DISSONANCE" on the right side. At the bottom left corner, there's a Twitter logo followed by the word "PARSE," suggesting this might be part of a presentation or educational material. A note at the very bottom mentions "*Check paper for detailed annotation guidelines."</sample>
    <sample id="958">Annotations Step 1: Good parsing quality? Yes No Step 2: Dissonance? Yes No Step 3: Consonance? Yes No Dissonance -3.5% Consonance -48% Neither -48% User @user_handle Wish I could hold grudges but I guess it’s a good thing that I can’t at the same time. PARSE Dissonant Annotate Check paper for detailed annotation guidelines</sample>
    <sample id="959">Annotations Step 1: Good parsing quality? Yes No Step 2: Dissonance? Yes No Step 3: Consensus? Yes No Dissonance -3.5% Consonance -48% Neither -48% User @user_handle Wish I could hold grudges but I guess it’s a good thing that I can’t at the same time. PARSE Dissociate Check paper for detailed annotation guidelines 11</sample>
    <sample id="960">The image is a slide from a presentation titled "Training on Initial Annotated Set." It features a graph with the x-axis labeled "Area under the ROC curve (AUC)" ranging from 0.50 to 0.65, and the y-axis not labeled but containing two data points: one at approximately 0.51 for an initial dataset and another around 0.53 associated with RoBERTa-base + classifier head. There's also a speech bubble pointing to text that reads, "Small annotated dataset: 43/901 dissonance; not better than chance," indicating some performance evaluation of the model training process. The bottom right corner has the number '12,' likely signifying this is the twelfth slide in the deck. In the top right corner, there’s a small inset showing a person who appears to be presenting or discussing the content shown on the main part of the slide.</sample>
    <sample id="961">The image is a slide from a presentation titled "Training on Initial Annotated Set." It features a graph with the x-axis labeled as "Area under the ROC curve (AUC)" ranging from 0.5 to 0.65, and the y-axis not labeled but containing two data points: one at approximately 0.5 for the initial dataset and another higher point indicating training results using RoBERTa-base + classifier head.

There are several text elements in the image:
- A label next to the lower data point reads "initial dataset."
- Another label above it says "RoBERTa-base + classifier head."
- There's an arrow pointing upwards from the initial dataset label towards a speech bubble that contains the following text: "Small annotated dataset: 43/901 dissonance; not better than chance."

In the top right corner of the slide, there is a small inset showing a person speaking into a microphone, suggesting this might be part of a live presentation or webinar. The name "Monica Vukotic" appears below the speaker's image, likely identifying her as the presenter.</sample>
    <sample id="962">The image contains a detailed flowchart titled "Method: Transfer and Active Learning for Annotating Rare Class." The chart begins with the term "Initial model: Transfer Learning" at the top left, followed by various stages of data processing labeled as "new," "old data," "train," "Cumulative (CM)," "fine-tune," and "Iterative (IT)." There are two main sections in the middle:

1. On the right side, there is an illustration comparing rare class annotation to finding a needle in a haystack. It states that it's difficult to annotate but easier after annotating more examples.
2. Below this section, another part shows an acquisition strategy where humans annotate new examples.

At the bottom center, there is a box labeled "Model Retrain/Update" connected to both sides of the diagram. This indicates an iterative process involving active learning iteration, where new examples are added back into the system. 

The overall layout suggests a methodical approach to improving machine learning models through transfer learning and active human annotation of rare classes over multiple iterations.</sample>
    <sample id="963">The image is a slide from a presentation titled "Cold-start Annotations: Transfer Learning." It features a flowchart that begins with the label "START" and proceeds through several stages, including "Initial model: Transfer Learning," "Cumulative (CM)," "Model Retrain/Update," and ends at "Iterative."

The main content of the slide includes:
- A comparison between rare class annotation ("needle in a haystack") and easier to annotate.
- A note stating it's difficult to annotate but increasing chance of rare class?
- An acquisition strategy section suggesting new examples should be 'best-of-batch' or 'best-of-class.'
- Instructions for humans to annotate these new examples.

There are also arrows indicating processes such as training on old data, adding new examples, and iterative updates. The bottom right corner has a page number "14."

In the top right corner, there is an inset showing a person speaking into a microphone, likely presenting this material.</sample>
    <sample id="964">The image is a slide from a presentation titled "Cold-start Annotations: Transfer Learning." It features a bar chart comparing the area under the ROC curve (AUC) for different datasets. The datasets compared are labeled as 'init dataset,' 'Debate,' and 'CE.' Each dataset has an associated AUC value, with 'init dataset' having approximately 0.52, 'Debate' around 0.61, and 'CE' close to 0.68.

Above the chart, there's a note indicating that the model used is "RoBERTa-base + classifier head" and it mentions that "Transferred weights after training on combined Debate and CE data."

In the bottom right corner of the slide, there is a reference to a paper by Vaswani et al., which discusses RoBERTa in the context of debate and CE tasks. Additionally, there is a citation for another work related to the Fifth Workshop on Natural Language Processing and Evaluation at ACL-2009.

The slide number, indicated as 15, can be seen in the lower-right corner.</sample>
    <sample id="965">The image is a slide from a presentation titled "Cold-start Annotations: Transfer Learning." It features a bar chart comparing the Area Under the ROC Curve (AUC) for different datasets. The bars are labeled as follows:

- init dataset
- Debate
- CE
- Debate:CE

The AUC values shown on the chart are:
- init dataset: 0.52
- Debate: 0.61 (+0.12)
- CE: 0.63 (-0.08)
- Debate:CE: 0.64 (-0.08)

There is an annotation pointing to the "Debate" and "CE" labels with text that reads, "Transferred weights after training on combined Debate and CE data."

In the top left corner of the slide, there is a box labeled "RoBERTA-base + classifier head," indicating the model used in the analysis.

At the bottom right of the slide, there is a citation for the source of the data presented in the chart.</sample>
    <sample id="966">The image is a slide from a presentation titled "Cold-start Annotations: Transfer Learning." It features a bar chart comparing the performance of different datasets in terms of Area under the ROC curve (AUC). The datasets compared are labeled as "init dataset," "Debate," and "CE." Each dataset has an associated AUC value, with "Debate" showing a +0.12 increase over the initial dataset, and "CE" showing a -0.06 decrease.

In the top left corner, there's a box labeled "RoBERTA-base + classifier head." To the right of the chart, there's a text bubble that reads, "Transferred weights after training on combined Debate and CE data."

At the bottom of the slide, there is a citation for the source of the data used:
- "Debate: RoBERTA instance in debate dataset by Vasilevich, N., Neelakantan, S., &amp; Raghunathan, K. (2023). In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics: Long Papers."
- "CE: Comparison and explanation classes by Raghunathan, P., Neelakantan, S., Vasilevich, N., &amp; Alipour, M. (2023). In Proceedings of the Fifth Workshop on Natural Language Processing and Evaluation Metrics (NLEEM 2023)."

The page number "15" is visible at the bottom right corner of the slide.</sample>
    <sample id="967">The image shows a presentation slide titled "Cold-start Annotations: Transfer Learning." It features a bar chart comparing the performance of different methods in terms of Area Under the ROC curve (AUC). The bars represent various tasks such as "TRAIN," "init dataset," "Debate," "CE," and "Debate-CE" with their corresponding AUC values. There is also text indicating that these results are based on a ReBERTa base plus classifier head, and it mentions finetuning on each task consecutively.

At the bottom left corner of the slide, there's an explanation for the abbreviations used:
- "SCE": Comparison and expansion class
- "Debate": Resembance score in debate data from Vasilev et al., 2019; Nissen et al., 2018; and Vasilev et al., 2017.
- "P5": Participation rate in debate data from Vasilev et al., 2019; Nissen et al., 2018; and Vasilev et al., 2017.
- "F5": Frequency of participation in debate data from Vasilev et al., 2019; Nissen et al., 2018; and Vasilev et al., 2017.

The source cited at the bottom right is "Nissen et al., 2019, ACL."

In the top right corner of the slide, there is a small video feed showing a person who appears to be presenting or participating remotely.</sample>
    <sample id="968">The image shows a slide from a presentation titled "Active Learning: Cumulative vs Iterative Update." The main content of the slide includes a flowchart that illustrates two different approaches to active learning. On the left side, there is a diagram labeled "Cumulative (CM)" with steps such as "new," "old data," and "train" connected by arrows indicating the process flow. This section also mentions terms like "fine-tune" and "Iterative (IT)." In the center, there is an arrow pointing towards another part of the chart labeled "Model Retain/Update," which connects back to the initial model through a loop.

On the right side, above the central text, there are two images depicting the difficulty of annotating rare class annotations ("needle in a haystack") versus easier annotation. Below these images, additional notes explain concepts related to acquisition strategy, new examples, human annotate, Active learning iteration, add new examples, and more. 

At the top of the slide, the title reads "Active Learning: Cumulative vs Iterative Update," summarizing the topic being discussed.</sample>
    <sample id="969">The image contains a bar chart with the title "Active Learning: Cumulative vs Iterative Update." The y-axis is labeled "AUC" and ranges from 0.5 to 0.75, while the x-axis lists different methods or strategies such as Random, Entropy, CoreSet, CAL, and PRC.

There are two sets of bars for each method on the x-axis:
- Blue bars represent "Cumulative"
- Yellow bars represent "Iterative"

The blue bars (Cumulative) show higher AUC values compared to the yellow bars (Iterative) across all methods except for Random, where both have similar AUC values close to 0.6. 

In the top right corner of the image, there's a small inset showing a person speaking into a microphone, indicating that this might be part of a presentation slide. In the bottom right corner, the number "18" suggests this is slide number 18 in the presentation.</sample>
    <sample id="970">The image is a slide from a presentation on "Active Learning: Probability-of-Rare-Class Strategy." It features various diagrams and text explaining the concept. The main title at the top reads, "Active Learning: Probability-of-Rare-Class Strategy."

On the left side of the slide, there's a flowchart with steps labeled as:
- START
- Initial model
- Transfer Learning

Below this flowchart, another diagram shows different stages in an iterative process for active learning, including:
- Cumulative (CM)
- Fine-tune
- Iterative (IT)

In the center-right part of the slide, there are two comparative illustrations showing the difficulty of annotating rare class labels versus common classes. The illustration on the left says, "Rare class annotation - 'needle in a haystack'" and notes that it is "Difficult to annotate" but increases the chance of rare class. The illustration on the right states, "Easier to annotate," indicating simpler annotation.

Below these illustrations, there's a box titled "Acquisition strategy which are best to label?" connected by arrows pointing towards new examples being added through human annotation into the system. This leads back to the "Model Retrain/Update" stage, completing the cycle.

At the bottom of the slide, additional elements include terms like "Add new examples" and "Humans annotate," highlighting the interactive nature of the active learning process.

Overall, the slide provides a visual representation of how active learning strategies can be employed to improve models by focusing on rare classes through targeted annotation and retraining.</sample>
    <sample id="971">The image is a slide from a presentation titled "Active Learning: Probability-of-Rare-Class Strategy." It features a flowchart with various components related to active learning. The main diagram illustrates the process of rare class annotation, showing that it's difficult and likened to finding a needle in a haystack but becomes easier as more examples are added. There are annotations such as "Rare class annotation - 'needle in a haystack'" and "Easier to annotate" within the chart.

Below the main diagram, there's another section labeled "Cumulative (CM)" which includes steps like "train," "retrain/update," and "add new examples." This part also mentions "Model Retrain/Update" and "Humans annotate."

On the right side of the slide, there's text discussing acquisition strategy questions like "which are best to label?" and indicating processes involving humans annotating and adding new examples. The background color scheme consists mainly of blue, red, yellow, white, gray, black, green, pink, purple, orange, light blue, brown, tan, olive drab, and beige. In the top-right corner, there's a small video feed window displaying a person presenting the content.</sample>
    <sample id="972">The image shows a slide from a presentation titled "Active Learning: Probability-of-Rare-Class Strategy." It includes a bar chart comparing different strategies based on their AUCs (Area Under the Curve). The baseline strategy is labeled as "from scratch" and has an AUC of 0.17, which is highlighted in red. Other strategies include AL-Random with an AUC of 0.15, AL-Entropy with 0.20, AL-ConSet with 0.19, AL-CAL with 0.19, and AL-PRC (ours) with 0.21. Each strategy's performance is represented by horizontal bars extending to the right along a scale ranging from 0.50 to 0.75. In the top-right corner of the slide, there is a small inset showing a person who appears to be presenting or participating in the webinar.</sample>
    <sample id="973">The image shows a bar chart titled "Active Learning: Probability-of-Rare-Class Strategy" with the subtitle "Active Learning Strategy Comparison (AUCs)." The chart compares various strategies based on their Area Under the Curve (AUC) values. 

- Baseline from scratch is represented by a red bar and has an AUC of 0.17.
- Transferred model is also shown in red, but its specific value isn't mentioned.

Other strategies include:
- Al-Random (+0.15)
- Al-Entropy (+0.20)
- Al-CoreSet (+0.19)
- Al-CAL (+0.19)
- Al-PRC (ours) (+0.21)

At the bottom, there's a green horizontal line labeled "Final Model (best transfer learning model)" with an AUC of +0.25.

The slide number "22" is visible at the bottom right corner.</sample>
    <sample id="974">The image is a slide from a presentation titled "Active Learning: Probability-of-Rare-Class Strategy." It contains a table comparing different active learning strategies based on three criteria: Rare %, Time (s), and Subjective difference. The strategies compared are RANDOM, ENTROPY, CORESet, CAL, and PRC.

The table includes the following data:

- RANDOM: 3.20 Rare %, 11.96 Time (s), -0.065 Subjective difference
- ENTROPY: 6.80 Rare %, 12.78 Time (s), 0.035 Subjective difference
- CORESet: 6.00 Rare %, 11.89 Time (s), 0.039 Subjective difference
- CAL: 6.00 Rare %, 11.84 Time (s), 0.039 Subjective difference
- PRC: 7.60 Rare %, 13.55 Time (s), 0.071 Subjective difference

Below the table, there are two bullet points:
- Minimum annotation cost does not necessarily lead to better models
- Rarity could make the annotations more difficult: cognitive dissonance is one such class.
To increase dissonance samples, PRC works the best.

In the top right corner of the slide, there is an inset showing a person's face with their name displayed below it.</sample>
    <sample id="975">Takeaways Rare class annotation - 'needle in a haystack' PRC is simple &amp; efficient for rare sample acquisition Cold-start AL with transfer learning Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="976">The image shows a slide from a presentation with the title "Takeaways" at the top. On the left side, there is text that reads "Cold-start AL with transfer learning." In the center of the slide, two diagrams are displayed: one labeled "Out-of-domain: Iterative," showing a sequence of models M0 to M3 connected by arrows indicating iterative processes; and another labeled "In-domain: Cumulative," also depicting a sequence of models (M0) but in a cumulative manner without intermediate steps. To the right, there is an illustration comparing rare class annotation ("needle in a haystack") to PRC's efficiency for rare sample acquisition. The bottom right corner has the number 24, likely indicating this is the 24th slide in the presentation.</sample>
    <sample id="977">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge Contact: vvaradarajan@cs.stonybrook.edu, sjuhng@cs.stonybrook.edu, has@cs.stonybrook.edu Code: Dataset: Paper:</sample>
    <sample id="978">The authors evaluated the following dialog models: BART-FID-RAG, Blender2, Emora, and Blender-Decode.</sample>
    <sample id="979">There are 10 authors involved in the paper.</sample>
    <sample id="980">A good planner should be organized, detail-oriented, and able to think strategically. They need strong time management skills, the ability to prioritize tasks, and excellent communication abilities to convey their plans effectively.</sample>
    <sample id="981">There are 9 authors involved in the paper.</sample>
    <sample id="982">The name of the speaker is Vasudha Varadarajan.</sample>
    <sample id="983">The affiliations of the authors are: Adam Prze/pi/rski at the Institute of Computer Science, Polish Academy of Sciences and Micha/ Wozniak at University of Warsaw.</sample>
    <sample id="984">The image displays a presentation slide with the title "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" prominently featured at the top. Below the title, there are names listed as contributors or authors of the work: Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang. At the bottom left corner of the slide, there is an emblem for Penn State University, indicating their involvement or affiliation. On the right side of the slide, there is another logo representing Amazon, suggesting collaboration or sponsorship from this company. The overall layout suggests that this is likely part of an academic or professional presentation discussing advancements in cross-lingual semantic parsing techniques across different languages and meaning representations.</sample>
    <sample id="985">The image contains a slide titled "Semantic Parsing" with the following content:

- A definition: "Semantic Parsing is a task to build semantic representation of the user queries, such as SQL, and Lambda Calculus."
- Two example questions:
  - "Which countries in Europe have at least 3 car manufacturers?"
    - SQL query: `SELECT t1.country_name FROM countries AS t1 JOIN continents AS c ON t1.continent_id = c.continent_id JOIN car_makers AS t2 ON t1.country_id = t2.country_id WHERE t1.continent = 'Europe' GROUP BY t1.country_name HAVING COUNT(*) &gt;= 3`
  - "What players made less than three assists over a season?"
    - Lambda Calculus expression: `(call swListValue (call swGetProperty (lambda s (call variant s) (swEnsureNumericProperty (string (call num_assists) (string (swEnsureNumericEntity (swNumericEntity 3 (string player)) (swDomain (string player))))))))))`

At the bottom, there's a note comparing SQL and Lambda Calculus: "User query and its corresponding SQL (left) vs. user query and its corresponding Lambda Calculus (right)."

In the top right corner, there is an image of a person with text below it that reads "build semantic parser."</sample>
    <sample id="986">Cross-lingual Semantic Parsing Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="987">The image shows a presentation slide titled "Cross-lingual Semantic Parsing." The main content of the slide explains that cross-lingual semantic parsing is a task to translate queries in multiple natural languages into multiple meaning representations. There are three language boxes labeled English, German, and Chinese on the left side, with an arrow pointing towards another set of three boxes labeled SQL, Lambda, and FunQL on the right side, indicating the use of neural models for this translation process. In the top-right corner, there's a small photo of a person named Ethan Zhang.</sample>
    <sample id="988">The image contains a slide titled "Cross-lingual Semantic Parsing" with the following content:

- A bullet point stating: "Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance:"
  - Sub-bullet point: "Lack of coverage on certain natural language"

Below this text, there is a diagram showing three boxes labeled:
- English
- German
- Chinese

An arrow points from these boxes to another set of four boxes labeled:
- SQL
- Lambda
- FunQL

In the top right corner of the image, there is an inset photo of a person.

The overall theme of the slide appears to be discussing the limitations of cross-lingual semantic parsing (CLSP) in terms of its application across different languages and platforms or systems.</sample>
    <sample id="989">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="990">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representation English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="991">Title: Cross-lingual Semantic Parsing

Body Text:
- Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance:
  - Lack of coverage on certain meaning representation

Diagram Labels (from left to right, top to bottom):
- English
- German
- Chinese
- Neural Models
- SQL
- Lambda
- FunQL

Slide Number: 5</sample>
    <sample id="992">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain neural model English German Chinese Single Model SQL Lambda FunQL</sample>
    <sample id="993">The image contains a slide titled "XSemPLR" with the following text:

- XSemPLR
- We provide a unified dataset XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains:
  - 9 datasets in various domains
  - 5 semantic parsing tasks.
  - 8 meaning representations
  - 22 natural languages in 15 language families

There is also a diagram showing different applications of XSemPLR, including:
- A-Calculus
- SQL
- SELECT ... FROM ... WHERE ...
- Prolog

Each application has arrows pointing to them from terms like "encoder," "decoder," "Thing Talk," "Intent Slot," etc.

In the top right corner, there is an inset photo of a person speaking into a microphone. The background of the slide includes some sample sentences in English and other languages, such as Japanese and Italian, along with their translations or related phrases.</sample>
    <sample id="994">The image shows a presentation slide titled "XSemPLR." The content of the slide describes a unified dataset named XSemPLR, which is designed for cross-lingual semantic parsing in multiple natural languages and meaning representations. Here are the key points mentioned on the slide:

1. **Dataset Description**:
   - A unified dataset called XSemPLR.
   - It includes data from 9 datasets across various domains.

2. **Features of XSemPLR**:
   - 5 semantic parsing tasks.
   - 8 meaning representations.
   - Data from 22 natural languages spread across 15 language families.

3. **Illustration**:
   - There's an illustration showing how different types of questions (e.g., "What players made more than three assists over a season?", "We have Japanese restaurants 'Glico' in Paris?", etc.) can be parsed using XSemPLR.
   - This illustrates that XSemPLR consists of two main components: Encoder and Decoder.

4. **Examples of Questions**:
   - Examples include queries related to sports statistics, restaurant locations, movie reviews, and other categories like "Thing Talk," "Thing Slot," "Intent Slot," and specific programming languages such as Prolog.

5. **Additional Information**:
   - On the right side of the slide, there’s some text mentioning "call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call (call</sample>
    <sample id="995">**Title: Experiment Settings**

- **Body Text:** We consider the six settings for training and evaluation.
  - **Translate-Test:** Use google translate API to translate source to the target language. Then use monolingual model to train and eval.

**Training**
- English → English Model → SQL

**Inference**
- German → Translate API → English → English Model → SQL</sample>
    <sample id="996">Experiment Settings We consider the six settings for training and evaluation. Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval. Training English English Model SQL Inference German Translate API English English Model SQL</sample>
    <sample id="997">The image shows a slide from a presentation titled "Experiment Settings." It explains the six settings for training and evaluation, with a focus on the "Translate-Test" setting. Under this setting, it instructs to use Google Translate API to translate source text to the target language and then employ monolingual models for training and evaluation.

There are two sections labeled "Training" and "Inference," each illustrating different processes:

1. **Training**:
   - English -&gt; English Model -&gt; SQL

2. **Inference**:
   - German -&gt; Translate API -&gt; English -&gt; English Model -&gt; SQL

Each section has arrows indicating the flow of data or process steps between stages (e.g., translation, model generation, and final output in SQL format). The background is white with black and blue text, making the information clear and easy to read.</sample>
    <sample id="998">Experiment Settings

We consider the six settings for training and evaluation.
Monolingual Model: Source language is the same as target language, e.g., German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data.

Training
[Diagram showing a flow from "German (Few-shot)" to "German Model" which then leads to "SQL"]

Inference
[Diagram showing a flow from "German" to "German Model" which then leads to "SQL"]</sample>
    <sample id="999">The image contains a slide from a presentation with the following content:

Title: Experiment Settings

Body Text:
- We consider the six settings for training and evaluation.
  - Monolingual Model: Source language is the same as target language, e.g., German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data.

Subsections labeled "Training" and "Inference":
- Training section shows two diagrams side by side:
  - The left diagram has a box labeled "German (Few-shot)" pointing to another box labeled "German Model," which then points to a third box labeled "SQL."
  - The right diagram mirrors this setup but uses solid colors instead of text labels in the boxes.

- Inference section repeats the structure seen in the Training section's first diagram:
  - A box labeled "German" pointing to another box labeled "German Model," which then points to a third box labeled "SQL."

There is an inset photo on the top-right corner showing a person speaking into a microphone, presumably presenting the material.</sample>
    <sample id="1000">**Title: Experiment Settings**

**Body Text:**
We consider the six settings for training and evaluation.
- **Monolingual Model:** Source language is the same as target language, e.g., German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data.

**Subsections: Training and Inference**

**Training Diagram:**
- A box labeled "German (Few-shot)" points to a box labeled "German Model," which then points to a box labeled "SQL."

**Inference Diagram:**
- A box labeled "German" points directly to a box labeled "SQL."</sample>
    <sample id="1001">The image contains text related to experiment settings and training models. Here is the transcription of the visible text:

Title: Experiment Settings

Body Text:
- We consider the six settings for training and evaluation.
  - Multilingual Model: Train one multilingual model for all languages.

Subtitles:
- Training
- Inference

Flowchart:
1. German
2. English
3. Chinese
4. Multilingual Model (Arrow pointing right)
5. SQL

The flowchart illustrates a process where three different languages (German, English, Chinese) are used in both training and inference phases to create a multilingual model that eventually leads to an SQL output. The slide number "10" is indicated at the bottom right corner.</sample>
    <sample id="1002">The image shows a slide from a presentation with the title "Experiment Settings" at the top. The content of the slide explains that there are six settings for training and evaluation, specifically focusing on a multilingual model approach.

The main bullet point states: "We consider the six settings for training and evaluation." Below this, it highlights one particular setting in blue text: "Multilingual Model: Train one multilingual model for all languages."

The slide is divided into two sections labeled "Training" and "Inference," each containing three boxes representing different languages (German, English, Chinese) connected by arrows to a box labeled "SQL." This suggests a process flow where data from these languages leads to SQL databases through a multilingual model.

At the bottom right corner of the slide, there is a small picture of a person with the name "Kan Thuan Le" written next to it. Additionally, there is a navigation bar at the bottom left indicating page 10 out of 10 slides.

Overall, the slide provides an overview of how multiple languages can be processed using a single multilingual model leading to SQL databases during both training and inference phases.</sample>
    <sample id="1003">Experiment Settings We consider the six settings for training and evaluation. • Multilingual Model: Train one multilingual model for all languages. Training German English Chinese Multilingual Model SQL Inference German Multilingual Model SQL</sample>
    <sample id="1004">**Title: Experiment Settings**

**Body Text:**
We consider the six settings for training and evaluation.
- **Cross-lingual Zero-shot/Few-shot transfer.** Train on one source language and transfer to another language.

**Subtitles: Training and Inference**

**Training Diagram:**
- English
- Or (English)
- German Few-shot

**Arrow pointing to:**
- Multilingual Model

**Arrow leading to:**
- SQL

**Inference Diagram:**
- German

**Arrow pointing to:**
- Multilingual Model

**Arrow leading to:**
- SQL</sample>
    <sample id="1005">The image shows a slide titled "Experiment Settings" with the following text: 'We consider the six settings for training and evaluation. Cross-lingual Zero-shot/Few-shot transfer. Train on one source language and transfer to another language.' Below this, there are two sections labeled 'Training' and 'Inference'. In the Training section, three boxes represent different languages or datasets: English, Or (with an arrow pointing from it), and German Few-shot. These lead to a box labeled 'Multilingual Model', which then leads to a box labeled 'SQL'. Similarly, in the Inference section, a single box represents German, leading directly to the same Multilingual Model and SQL boxes as above. The background is white, and the text is primarily black with some red highlights. There's also a small inset photo of a person in the top right corner with the name 'Thomas Zeng' below it.</sample>
    <sample id="1006">Analysis of Monolingual

We evaluate on two groups of models on **Monolingual Setting**

**EPTR**: Multilingual Pretrained Encoders with Pointer-based Decoders
- XLM-R + PTR, mBERT + PTR

**EPDec**: Multilingual Pretrained Encoder-Decoder Models
- mBART, mT5

We found **EPDec (mT5)** obtains the best performance on all datasets!

MATIS | MGEOQuery | MSpliner | MSpOveright | MCWQ | MSch2QA2 | MTOP | MCNAvera | Average
--- | --- | --- | --- | --- | --- | --- | --- | ---
mBERT-PTR | 30.63 | 72.18 | 40.91 | 57.47 | 52.43 | 75.81 | 59.41 | 61.29
XLM-R + PTR | 31.41 | 71.41 | 45.81 | 59.10 | 60.32 | 80.36 | 64.27 | 64.91
mBART | 53.18 | 74.26 | 50.73 | 66.29 | 65.16 | 81.83 | 81.29 | 81.56
mT5 | 53.18 | 74.26 | 50.73 | 66.29 | 65.16 | 81.83 | 81.29 | 81.56

(Note: The table contains numerical values for each model's performance across different metrics.)</sample>
    <sample id="1007">The image contains a slide titled "Analysis of Monolingual" with the following text:

- We evaluate on two groups of models on **Monolingual Setting**
  - **EPTR**: Multilingual Pretrained Encoders with Pointer-based Detectors
    - XLM-R + PTR, mBERT + PTR
  - **EPDEC**: Multilingual Pretrained Encoder-Decoder Models
    - mBART, mT5

- We found **EPDEC (mT5)** obtains the best performance on all datasets!

Below this section is a table comparing various models across different metrics. The columns in the table are labeled as follows:
- MATIS
- MGEOQuery
- MGeoSpider
- MSniper
- MOveright
- MCWQ
- MCWQ2
- MSqa2QA
- MTOP
- MCNAverall
- Average

The rows represent different models and their respective scores for each metric. At the bottom of the table, there is an average row showing the overall scores for each model.

Highlighted at the bottom of the table are the scores for EPDEC (mT5), which shows that it has the highest score among all models listed.</sample>
    <sample id="1008">Analysis of Monolingual

We evaluate on two groups of models on **Monolingual Setting**

- **EPTR**: Multilingual Pretrained Encoders with Pointer-based Detectors
  - XLM-R + PTR, mBERT + PTR
- **EPDEC**: Multilingual Pretrained Encoder-Decoder Models
  - mBART, mT5

We found **EPDEC (mT5)** obtains the best performance on all datasets!

| MATIS | MGeoQuery | MSniper | MSpiders | MOveright | MCWQ | MScholia2QA | MTOP | MCAverage |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| mBERT-PTR | 30.63 | 72.18 | 40.14 | 57.47 | 53.49 | 52.41 | 59.51 | 57.61 |
| mXLM-R + PTR | 31.34 | 71.41 | 45.81 | 59.10 | 58.07 | 60.32 | 80.36 | 60.76 |
| mBART | 45.18 | 74.26 | 50.73 | 66.29 | 65.16 | 81.83 | 81.29 | 58.16 |

(Note: The image contains a table comparing different models' performances across various metrics.)</sample>
    <sample id="1009">Analysis of Monolingual

We evaluate on two groups of models on **Monolingual Setting**

**EPTR:** Multilingual Pretrained Encoders with Pointer-based Decoders
- XLM-R + PTR, mBERT + PTR

**EPDec:** Multilingual Pretrained Encoder-Decoder Models
- mBART, mT5

We found **EPDec (mT5)** obtains the best performance on all datasets!

MATIS MGeoQuery MSniper MSpoken MOWeight MCWQ mSchea2QA MTOP McNla Average
mBERT-PTR 30.63 72.18 40.92 57.47 52.53 75.41 58.57 75.41 59.57
XLM-R + PTR 31.41 71.41 45.57 59.10 58.32 80.36 72.57 80.36 61.76
mBART 53.18 74.26 50.73 61.05 66.29 65.16 81.83 81.83 58.16

(Note: The text in the image is a detailed analysis comparing different models' performances across various metrics.)</sample>
    <sample id="1010">Analysis of Multilingual Training

- We evaluate on mT5 and XLM-R + PTR on Multilingual Setting
- Enc-Dec/Enc-PTR (mT5-XLM-R) can be improved by training in a mixture of various languages.

[Table]

| MATIS | MGeQGeo | MSpider | MNMaps | MOveright | MS质检QA2 | MTOPalai | Average |
|-------|---------|---------|--------|-----------|-----------|----------|---------|
| Monolingual |       |         |        |           |           |          |         |
| XLM-R+PTR | 31.31   | 71.41   | 48.37  | 85.17     | 59.10     | 66.29    | 80.36   | 58.16    |
| mT5      | 74.26   | 50.73   | 50.73  | 81.93     | 66.29     | 80.36    | 80.36   | 58.16    |
|          |         |         |        |           |           |          |         |
| XLM-R+PTR | 39.72   | 71.35   | 40.20  | 85.91     | 61.03     | 60.82    | 81.68   | 58.16    |
| mT5      | 54.45   | 76.57   | 40.50  | 82.95     | 67.55     | 60.82    | 82.95   | 58.16    |

[Image of a person with the name "Karthik Ramakrishnan" below the image]</sample>
    <sample id="1011">Analysis of Multilingual Training

We evaluate on mT5 and XLM-R + PTR on Multilingual Setting
- Enc-Dec/Enc-PTR (mT5/XLM-R) can be improved by training in a mixture of various languages.

MATIS | MIGeography | MSpider | MNmaps | MOveright | MSpeech2QA | MTOP | MTNalai | Average
--- | --- | --- | --- | --- | --- | --- | --- | ---
mT5 | 31.31 | 74.26 | 85.17 | 59.05 | 66.29 | 80.36 | 81.38 | 52.09 | 58.16
XLM-R+PTR | 31.31 | 74.26 | 85.17 | 59.05 | 66.29 | 80.36 | 81.38 | 52.09 | 58.16
mT5 | 39.72 | 71.35 | 40.51 | 61.03 | 61.03 | 30.82 | 81.68 | 59.06 | 61.82
XLM-R+PTR | 39.72 | 71.35 | 40.51 | 61.03 | 61.03 | 30.82 | 81.68 | 59.06 | 61.82

(Note: The table shows performance metrics for different models across various tasks, with the last column labeled "Average" indicating an overall average score.)</sample>
    <sample id="1012">Analysis of Multilingual Training

- We evaluate on mT5 and XLM-R + PTR on Multilingual Setting
  - Most of the major NLs can obtain performance gain, except that English performance drops in 7 datasets and gains in 3 datasets.
  - This is known as "Curse of Multilinguality"

[Bar chart showing number of datasets for different natural languages]</sample>
    <sample id="1013">Analysis of Multilingual Training

- We evaluate on mT5 and XLM-R + PTR on Multilingual Setting
  - Most of the major NLs can obtain performance gain, except that English performance drops in 7 datasets and gains in 3 datasets.
  - This is known as "Curse of Multilinguality"

[Bar chart showing number of datasets for various natural languages]</sample>
    <sample id="1014">Cross-lingual Performance Gap Blue Line: Cross-lingual Few-shot transfer Orange Line: Cross-lingual Zero-shot transfer Green Line: Monolingual Setting</sample>
    <sample id="1015">Cross-lingual Performance Gap Blue Line: Cross-lingual Few-shot transfer Orange Line: Cross-lingual Zero-shot transfer Green Line: Monolingual Setting</sample>
    <sample id="1016">The image is a slide from a presentation titled "Cross-lingual Performance Gap." It explains the performance gap in cross-lingual settings using color-coded information. The text on the slide reads:

"green - orange
For zero-shot setting, the cross-lingual transfer performance gap is significant

blue - orange
For few-shot setting, the transfer gap is shortened rapidly"

There is also a radar chart labeled with various datasets such as MTOP, Geoquery, Schema2QA, Overnight, NLMapps, MCWQ, ATIS, Spider, and Geoquery/lambda, among others. Each dataset has lines representing different conditions: Few-shot (blue), Zero-shot (orange), and Monolingual (green).

In the top right corner of the slide, there is an inset showing a person named Ethan Thomas, who appears to be presenting or associated with the content.

The slide number at the bottom right indicates that this is slide 16.</sample>
    <sample id="1017">The image contains a slide from a presentation with the title "Other Results &amp; Findings (Section 4 in Paper)." The content is organized into bullet points discussing various findings related to language models and their performance. Here are the details:

1. **Enc-Dec (mT5) outperforms previous work or achieves comparable results.**
2. **Pretraining on the English NL can significantly boost the performance of few-shot on target NLs.**
3. **Multilingual LLMs (CodeX &amp; Bloom) are still inadequate for cross-lingual semantic parsing tasks.**
4. **Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest.**
5. **FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.**

The text uses different colors to emphasize certain parts: 
- Red for key terms like "mT5," "few-shot," "Chinese," "German," "FunQL," and "SQL."
- Blue for specific model names such as "Enc-Dec" and "Multilingual LLMs."

In the top right corner, there's an inset picture of a person, presumably the presenter, along with some text that appears to be part of the logo or branding.

The slide number "17" is visible at the bottom right corner, indicating its position within the presentation.</sample>
    <sample id="1018">**Title: Other Results &amp; Findings (Section 4 in Paper)**

- **Enc-Dec (mT5) outperforms previous work or achieves comparable results.**
- Pretraining on the English NL can significantly boost the performance of few-shot on target NLs.
- Multilingual LLMs (Codex &amp; Bloom) are still inadequate for cross-lingual semantic parsing tasks.
- Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest.
- FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.

(Note: The image also includes a small picture of an individual in the top right corner.)</sample>
    <sample id="1019">The image contains a slide with the following text:

**Title:**
- Conclusion

**Body Text:**
- We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations.
  
- We conduct a comprehensive benchmark study on three representative types of multilingual language models.

- Our results show that mT5 with monolingual training yields the best performance, notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual training and cross-lingual transfer learning is still significant.

**Highlighted Terms:**
- "multilingual" in red
- "cross-lingual" in green

**Footer:**
- Slide number 18</sample>
    <sample id="1020">The video begins with a slide titled 'Conclusion' in bold at the top. The first bullet point states: 'We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations.' This text is black on a white background. Below this, there is another bullet point that reads: 'We conduct a comprehensive benchmark study on three representative types of multilingual language models,' where 'multilingual language models' is highlighted in red.

The next part of the slide contains two more bullet points:
1. 'Our results show that mT5 with monolingual training yields the best performance, notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual training and cross-lingual transfer learning is still significant.'
2. In this sentence, 'mT5 with monolingual training' and 'cross-lingual' are highlighted in green.
3. At the bottom right corner, there's an image of a person wearing headphones with their name displayed as 'Kuan Heng Tham.'

The final frame transitions from the conclusion section to a new section titled 'Links' in blue font at the top left. It welcomes viewers to visit their paper and code, providing links below it:
- Paper Link: https://arxiv.org/pdf/2306.04085.pdf
- Code Link: https://github.com/psunlpgroup/xsemplr

This text is also on a white background, maintaining consistency with the previous slides. A small image of a person wearing headphones appears again in the upper right corner, labeled 'Kuan Heng Tham.'</sample>
    <sample id="1021">The most common errors of PaLM are "Accuracy/Omission" and "Style/Awkwad."</sample>
    <sample id="1048">Emory University, Emory NLP Research Lab, and Amazon Alexa.</sample>
    <sample id="1049">CFT stands for "Continuous Fine-tuning" in this paper.</sample>
    <sample id="1050">There are 8 authors involved in the paper.</sample>
    <sample id="1084">Yusen Zhang</sample>
    <sample id="1085">The image contains a slide from a presentation titled "#ACL2023" at the top left corner. The main title of the slide reads "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models." Below this, there are four names listed with corresponding photos and logos underneath each name:

1. Shangbin Feng
2. Chan Young Park
3. Yuhan Liu
4. Yulia Tsvetkov

At the bottom of the slide, there are several logos representing different institutions or organizations:
- Paul G Allen School W
- UWNLP (University of Washington Natural Language Processing)
- Carnegie Mellon University Language Technologies Institute

In the upper right corner of the slide, there is a small video feed showing a person.

The second part of the image shows another slide under the heading "LM Training Data," which includes a subtitle "A mixed blessing." This section features a bar graph displaying various websites on the y-axis labeled as sources, such as "reddit.com," "wikipedia.org," "nytimes.com," etc., against the number of tokens (# tokens) in gigabytes along the x-axis. At the bottom right corner of this slide, there is a citation for Dodge, Jesse et al. ("Documenting Large Web Corpora: A Case Study on Crowed Corpus," Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2021).</sample>
    <sample id="1086">The image contains a slide titled "LM Training Data" with the subtitle "A mixed blessing." The main content of the slide is a bar chart showing various sources and their corresponding data. Below the title, there's text that reads: "Dodge, Jesse et al. 'Documenting Large Web Corpora: A Case Study on Crowled Corpus.' Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2021."

The bar chart lists several websites or platforms along the y-axis, such as "wikipedia.org," "en.wikipedia.org," "nytimes.com," "theatlantic.com," among others. Each website has a horizontal blue bar representing its size or quantity related to the training data.

In the top right corner of the image, there is a small video feed window displaying a person who appears to be giving a presentation or lecture.</sample>
    <sample id="1087">The image shows a presentation slide titled "LM Training Data" with the subtitle "A mixed blessing." The main content of the slide is a bar graph that displays various sources and their corresponding data sizes in gigabytes. Some of the listed sources include "wikipedia.org," "en.wikipedia.org," "nytimes.com," "theatlantic.com," among others, each represented by horizontal bars indicating different lengths to presumably show varying data sizes.

In the bottom right corner of the slide, there's a citation for Dodge, Jesse et al., from the publication "Documenting Large Web Corpora: A Case Study on Crowdsourced Corpus Construction Methods in Natural Language Processing 2021."

Additionally, at the top right corner of the image, there appears to be a small video feed showing an individual who seems to be presenting or participating in the session where this slide was shown.</sample>
    <sample id="1088">The image contains a slide titled "LM Training Data" with the subtitle "A mixed blessing." The main content of the slide is a bar chart that lists various sources such as "patents.gov," "en.wikipedia.org," "nytimes.com," and others, along with their corresponding number of tokens (in billions). At the bottom right corner, there is a citation for Dodge, Jesse et al. "Documenting Large Web Corpora: A Case Study on Crowded Corpus." It also mentions the source as "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing 2021." Additionally, there is a small video feed window at the top right showing a person speaking or presenting.</sample>
    <sample id="1089">The image shows a slide from a presentation titled 'LM Training Data' with the subtitle 'A mixed blessing.' The main content of the slide is a bar chart that displays various sources and their corresponding number of tokens (in billions). On the right side, there's a citation for Dodge, Jesse et al. "Documenting Large Web Corpora: A Case Study on the Crowed Corpus" in the Proceedings of the 16th Conference on Computational Natural Language Processing 2021.

In addition to this detailed information about language model training data, another part of the image contains a flowchart diagram labeled 'To this end,' which illustrates three stages connected by arrows: 'Pretraining data,' 'Language models,' and 'Downstream tasks.' This suggests a process or workflow related to pretraining data leading to language models and then being applied to downstream tasks.</sample>
    <sample id="1090">The image shows a slide from a presentation with the title "To this end" at the top. Below the title, there is a flow diagram consisting of three boxes connected by arrows indicating a process or sequence. The first box on the left reads "Pretraining data," the middle box reads "Language models," and the third box on the right reads "Downstream tasks." In the bottom section of the slide, two questions are presented in blue text: "How to evaluate the political leaning of LMs?" and "What role does pretraining data play in such political biases?" On the far right side of the image, there is a small video feed showing a person who appears to be presenting the content.</sample>
    <sample id="1091">To this end, Pretraining data Language models Downstream tasks How to evaluate the political leaning of LMs? How do LMs with different political leanings perform? What role does pretraining data play in such political biases? Does LM political leaning result in fairness issues in NLP applications?</sample>
    <sample id="1092">To this end Pretraining data Language models Downstream tasks How to evaluate the political leaning of LMs? How do LMs with different political leanings perform? Does LM political leaning result in fairness issues in NLP applications? What role does pretraining data play in such political biases?</sample>
    <sample id="1093">The image shows a slide from a presentation titled "Evaluating LM Political Leaning." The content includes several sections:

1. **Support both encoder and decoder LMs**
   - Example statement: "Do you agree or disagree with this statement? &lt;statement&gt;"

2. **Automatic eval**

3. **Grounded in polisci lit**

There is also an illustration of two characters, one labeled 'Left' and the other 'Right,' indicating political leanings.

Additionally, there's a section called "Existing LMs" that features a chart plotting different language models (LMs) based on their political leaning:
- The horizontal axis represents economic axis.
- The vertical axis represents social/political axis.
- Models are plotted within quadrants representing varying degrees of authoritarianism and libertarianism.

Examples of these models include BERT-base, RoBERTa-base, distilRoBERTa, ALBERT-base, GPT-2, Codex, LLAMA, etc., spread across four quadrants corresponding to different combinations of authoritarianism and libertarianism.</sample>
    <sample id="1094">The image shows a chart titled "Existing LMs" with two axes: the vertical axis labeled "Authoritarian" and the horizontal axis labeled "Libertarian." The chart is divided into four quadrants by these axes. Various language models (LMs) are plotted on this chart, including BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, distilRoBERTa, ALBERT-base, ALBERT-large, BART-base, and Alpaca.

On the left side of the chart under the "Authoritarian" label:
- BERT-base
- BERT-large
- RoBERTa-base
- RoBERTa-large
- distilRoBERTa
- ALBERT-base

On the right side of the chart under the "Authoritarian" label:
- Codex
- LLAMA
- GPT-2
- GPT-3-ada
- GPT-3-babbage
- GPT-3-curie
- ChatGPT
- GPT-4
- GPT-J

In the middle section between the "Authoritarian" and "Libertarian" labels:
- There are several points representing different models such as BART-large, which appears to be placed in the quadrant closer to the center where both authoritarian and libertarian values seem balanced or neutral.

The bottom part of the chart has an additional note that reads "economic axis," indicating another dimension considered for plotting these models.</sample>
    <sample id="1095">The image shows a slide titled "Existing LMs" with a graph that categorizes various language models (LMs) based on their political leaning. The x-axis represents the spectrum from Left to Right, indicating more authoritarian positions towards the left and more libertarian positions towards the right. The y-axis is labeled "Social Media," suggesting different levels of engagement or influence in social media contexts.

Various language models are plotted within this framework:

- **Left (Authoritarian):**
  - BERT-base
  - BERT-large
  - RoBERTa-base
  - RoBERTa-large
  - distilRoBERTa
  - ALBERT-base
  - ALBERT-large

- **Center:**
  - BART-base
  - BART-large
  - Alpaca

- **Right (Libertarian):**
  - Codex
  - LLAMA
  - GPT-2
  - GPT-3-ada
  - GPT-3-babbage
  - GPT-3-curie
  - ChatGPT
  - GPT-4
  - GPT-J

Each model's position indicates its perceived alignment with certain ideological stances, as determined by some form of evaluation or analysis method not detailed in the visible content. 

Additionally, there is an inset at the bottom showing two smaller diagrams under the heading "Pretraining Data." These diagrams illustrate how news media and social media platforms like Reddit might be categorized into 'left,' 'center,' and 'right' for further training purposes related to evaluating changes in political leaning among LM checkpoints such as RoBERTa and GPT-2.</sample>
    <sample id="1096">The image contains a slide titled "Pretraining Data" with the subtitle: "Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning." The slide is divided into two sections:

1. **News Media**:
   - Left
   - Center
   - Right

2. **Social Media (Reddit)**:
   - Left
   - Center
   - Right

At the bottom of the slide, there are references to studies by Liu et al., 2019; Shen et al., 2020; and Liu et al., 2020.

In the top right corner, there is an inset showing a person's face.</sample>
    <sample id="1097">The image shows a slide from a presentation with the title "Pretraining Data" at the top. Below this, there is text that reads: "Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning." The slide contains two diagrams side by side.

On the left diagram labeled "News Media," it has three horizontal sections colored yellow, blue, and red respectively, labeled as follows:
- Top section: "left"
- Middle section: "center"
- Bottom section: "right"

On the right diagram labeled "Social Media (Reddit)," it also has three horizontal sections colored green, blue, and red respectively, labeled similarly:
- Top section: "left"
- Middle section: "center"
- Bottom section: "right"

Below these diagrams, there are references to papers or authors related to the content of the slides:

- On the left: "Liu, Yujian, et al., 'Politics: Pretraining with Story Contexts Improves Political Lean Detection,' Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, pp. 4387–4395."
- On the right: "Shen, Girshick, and Carolyn Rose, 'What sounds 'right' to you? How meta features can explain the perception of political leanings on social media platforms.' Chapter of the Association for Computational Linguistics: Volume 2021."

At the bottom center of the slide, under the heading "Results," there is another figure showing a matrix divided into four quadrants. Each quadrant represents different combinations of labels ("reddit" vs. "original" news) across two models ("RoBERTa" vs. "GPT-2"). There are arrows indicating shifts between these categories within each model. 

To the far right, there is an inset video feed of a person presenting, but no further details about them are provided.</sample>
    <sample id="1098">Results Partisan shifts in LM political leaning</sample>
    <sample id="1099">Results Partisan shifts in LM political leaning</sample>
    <sample id="1100">Results Partisan shifts in LM political leaning The Trump Card Pre-45th to post-45th shift</sample>
    <sample id="1101">The image shows a slide titled "The Trump Card" with the subtitle "Pre-45th to post-45th shift." It features eight graphs arranged in two rows and four columns, each labeled as follows: news left, news center, news right, reddit left, reddit center, reddit right. Each graph contains numerical data such as Δ values (e.g., Δ=(275,124), Δ=(-0.13,1.03)) and additional numbers like -237.51, 121.28, etc. The background of the slide is white, and there are colored sections within some graphs indicating different areas or categories. In the top-right corner of the image, there is a small video call window showing three people participating in an online meeting.</sample>
    <sample id="1102">The image is a slide titled "The Trump Card" with the subtitle "Pre-45th to post-45th shift." It features a 3x3 grid of charts, each labeled as follows: news left, news center, news right; reddit center, reddit left, reddit right. Each chart contains numerical data and arrows indicating shifts or changes. The specific labels on the charts include "Δ = (275.124," "Δ = (-0.13.103," "Δ = (1.63.103," "Δ = (0.75.36," "Δ = (-0.50.36," "Δ = (-1.75.92," "Δ = (237.051," "Δ = (-0.12.128," "Δ = (-2.13.006," along with other numbers like "GPT-2" at the bottom of some grids. There's also a small inset in the top-right corner showing part of another presentation screen with different colored sections.</sample>
    <sample id="1103">The image is a slide titled "The Trump Card" with the subtitle "Pre-45th to post-45th shift." It contains eight charts arranged in two rows and four columns. Each chart has a title indicating different categories such as "news left," "news center," "news right," "reddit center," and "reddit right." The charts display data points connected by arrows, showing shifts or changes over time. Below each chart are numerical values labeled with Greek letters (Δ), representing quantitative measures of change.

In the top-right corner of the image, there is a small inset video call window featuring three individuals: one on the left wearing blue, another in the middle wearing red, and the third person partially visible on the right side. This suggests that the presentation might be part of an online meeting or lecture where these participants are involved.</sample>
    <sample id="1104">The image contains a table titled "Per-Category Performance" with data on the performance of various sources in detecting hate speech targeting different identity groups and misinformation from diverse sources. The categories include Hate Speech, BLACK, MUSLIM, LGBTQ+, JEWS, ASAIN, LATINX, WOMEN, CHRISTIAN, MEN, and WHITE.

The rows represent different sources: REDDIT, NEWS_RIGHT, NEWS_LEFT, HP, NYTT_LGT, CNN(L), HP(L), GUARDIAN, FOX, WB(BART), WAT, NR(R).

The columns show scores for each category:
- Hate Speech
- BLACK
- MUSLIM
- LGBTQ+
- JEWS
- ASAIN
- LATINX
- WOMEN
- CHRISTIAN
- MEN
- WHITE

The results are color-coded to indicate performance levels:
- Dark yellow denotes best.
- Blue denotes worst.

The bottom part of the image includes a caption that reads:
"Table 4: Performance on hate speech targeting different identity groups and misinformation from diverse sources.
The results are color-coded such that dark yellow denotes best and blue denotes worst."

Additionally, there is a small inset in the top right corner showing three rankings labeled as "1st," "2nd," and "3rd/4th."</sample>
    <sample id="1105">The image contains a table titled "Per-Category Performance" which shows the performance on hate speech targeting different identity groups and misinformation from various sources. The results are color-coded, with dark yellow indicating best (dark blue) and worst (dark red) performances.

Here is the detailed breakdown of the categories and their corresponding scores:

### Hate Speech
1. **BLACK**
   - Reddit: 89.64
   - News_Right: 88.05

2. **MUSLIM**
   - Reddit: 89.30
   - News_Right: 87.84

3. **LGBTQ+**
   - Reddit: 89.00
   - News_Right: 86.00

4. **JESUS**
   - Reddit: 89.50
   - News_Right: 87.50

5. **ASIAN**
   - Reddit: 89.60
   - News_Right: 87.00

6. **LATINX**
   - Reddit: 89.00
   - News_Right: 87.00

7. **WOMEN**
   - Reddit: 89.50
   - News_Right: 87.00

8. **CHRISTIAN**
   - Reddit: 87.00
   - News_Right: 85.00

9. **WHITE**
   - Reddit: 87.00
   - News_Right: 85.00

### Misinformation
1. **HLP**
   - Reddit: 88.54
   - News_Right: 87.50

2. **NYT L (L)**
   - Reddit: 88.54
   - News_Right: 87.50

3. **CNN (L)**
   - Reddit: 88.54
   - News_Right: 87.50

4. **Guard (L)**
   - Reddit: 88.54
   - News_Right: 87.50

5. **Fox (R)**
   - Reddit: 88.54
   - News_Right: 87.50

6. **WaPo (R)**
   - Reddit: 88.54
   - News_Right: 87.50

7. **BBART (R)**
   - Reddit: 88.54
   - News_Right: 87.50

8. **Wat (R)**
   - Reddit: 88.54
   - News_Right: 87.50

9. **NR (R)**
   - Reddit: 88.54
   - News_Right: 87.50

The data in the table indicates how well each category performed under both 'Reddit' and 'News_Right', with some variations between them for specific topics like hate speech against Black people or misinformation by CNN.</sample>
    <sample id="1106">The image contains a table titled "Per-Category Performance" with the following columns: Hate Speech, BLACK, MUSLIM, LGBTQ+, JEWS, ASAIN, LATINX, WOMEN, CHRISTIAN, and WHITE. The rows represent different sources: REDDIT, NEWS_88, NEWS_91, NEWS_94, NEWS_95, NEWS_96, NEWS_97, NEWS_98, NEWS_99, NEWS_100, NEWS_101, NEWS_102, NEWS_103, NEWS_104, NEWS_105, NEWS_106, NEWS_107, NEWS_108, NEWS_109, NEWS_110, NEWS_111, NEWS_112, NEWS_113, NEWS_114, NEWS_115, NEWS_116, NEWS_117, NEWS_118, NEWS_119, NEWS_120, NEWS_121, NEWS_122, NEWS_123, NEWS_124, NEWS_125, NEWS_126, NEWS_127, NEWS_128, NEWS_129, NEWS_130, NEWS_131, NEWS_132, NEWS_133, NEWS_134, NEWS_135, NEWS_136, NEWS_137, NEWS_138, NEWS_139, NEWS_140, NEWS_141, NEWS_142, NEWS_143, NEWS_144, NEWS_145, NEWS_146, NEWS_147, NEWS_148, NEWS_149, NEWS_150, NEWS_151, NEWS_152, NEWS_153, NEWS_154, NEWS_155, NEWS_156, NEWS_157, NEWS_158, NEWS_159, NEWS_160, NEWS_161, NEWS_162, NEWS_163, NEWS_164, NEWS_165, NEWS_166, NEWS_167, NEWS_168, NEWS_169, NEWS_170, NEWS_171, NEWS_172, NEWS_173, NEWS_174, NEWS_175, NEWS_176, NEWS_177, NEWS_178, NEWS_179, NEWS_180, NEWS_181, NEWS_182, NEWS_183, NEWS_184, NEWS_185, NEWS_186, NEWS_187, NEWS_188, NEWS_189, NEWS_190, NEWS_191, NEWS_192, NEWS_193, NEWS_194, NEWS_195, NEWS_196, NEWS_197, NEWS_198, NEWS_199, NEWS_200, NEWS_201, NEWS_202, NEWS_203, NEWS_204, NEWS_205, NEWS_206, NEWS_207, NEWS_208, NEWS_209, NEWS_210, NEWS_211, NEWS_212, NEWS_213, NEWS_214, NEWS_215, NEWS_216, NEWS_217, NEWS_218, NEWS_219, NEWS_220, NEWS_221, NEWS_222, NEWS_223, NEWS_224, NEWS_225, NEWS_226, NEWS_227, NEWS_228, NEWS_229, NEWS_230, NEWS_231, NEWS_232, NEWS_233, NEWS_234, NEWS_235, NEWS_236, NEWS_237, NEWS_238, NEWS_239, NEWS_240, NEWS_241, NEWS_242, NEWS_243, NEWS_244, NEWS_245, NEWS_246, NEWS_247, NEWS_248, NEWS_249, NEWS_250, NEWS_251, NEWS_252, NEWS_253, NEWS_254, NEWS_255, NEWS_256, NEWS_257, NEWS_258, NEWS_259, NEWS_260, NEWS_261, NEWS_262, NEWS_263, NEWS_264, NEWS_265, NEWS_266, NEWS_267, NEWS_268, NEWS_269, NEWS_270, NEWS_271, NEWS_272, NEWS_273, NEWS_274, NEWS_275, NEWS_276, NEWS_277, NEWS_278, NEWS_279, NEWS_280, NEWS_281, NEWS_282, NEWS_283, NEWS_284, NEWS_285, NEWS_286, NEWS_287, NEWS_288, NEWS_289, NEWS_290, NEWS_291, NEWS_292, NEWS_293, NEWS_294, NEWS_295, NEWS_296, NEWS_297, NEWS_298, NEWS_299, NEWS_300, NEWS_301, NEWS_302, NEWS_303, NEWS_304, NEWS_305, NEWS_306, NEWS_307, NEWS_308, NEWS_309, NEWS_310, NEWS_311, NEWS_312, NEWS_313, NEWS_314, NEWS_315, NEWS_316, NEWS_317, NEWS_318, NEWS_319, NEWS_320, NEWS_321, NEWS_322, NEWS_323, NEWS_324, NEWS_325, NEWS_326, NEWS_327, NEWS_328, NEWS_329, NEWS_330, NEWS_331, NEWS_332, NEWS_333, NEWS_334, NEWS_335, NEWS_336, NEWS_337, NEWS_338, NEWS_339, NEWS_340, NEWS_341, NEWS_342, NEWS_343, NEWS_344, NEWS_345, NEWS_346, NEWS_347, NEWS_348, NEWS_349, NEWS_350, NEWS_351, NEWS_352, NEWS_353, NEWS_354, NEWS_355, NEWS_356, NEWS_357, NEWS_358, NEWS_359, NEWS_360, NEWS_361, NEWS_362, NEWS_363, NEWS_364, NEWS_365, NEWS_366, NEWS_367, NEWS_368, NEWS_369, NEWS_370, NEWS_371, NEWS_372, NEWS_373, NEWS_374, NEWS_375, NEWS_376, NEWS_377, NEWS_378, NEWS_379, NEWS_380, NEWS_381, NEWS_382, NEWS_383, NEWS_384, NEWS_385, NEWS_386, NEWS_387, NEWS_388, NEWS_389, NEWS_390, NEWS_391, NEWS_392, NEWS_393, NEWS_394, NEWS_395, NEWS_396, NEWS_397, NEWS_398, NEWS_399, NEWS_400, NEWS_401, NEWS_402, NEWS_403, NEWS_404, NEWS_405, NEWS_406, NEWS_407, NEWS_408, NEWS_409, NEWS_410, NEWS_411, NEWS_412, NEWS_413, NEWS_414, NEWS_415, NEWS_416, NEWS_417, NEWS_418, NEWS_419, NEWS_420, NEWS_421, NEWS_422, NEWS_423, NEWS_424, NEWS_425, NEWS_426, NEWS_427, NEWS_428, NEWS_429, NEWS_430, NEWS_431, NEWS_432, NEWS_433, NEWS_434, NEWS_435, NEWS_436, NEWS_437, NEWS_438, NEWS_439, NEWS_440, NEWS_441, NEWS_442, NEWS_443, NEWS_444, NEWS_445, NEWS_446, NEWS_447, NEWS_448, NEWS_449, NEWS_450, NEWS_451, NEWS_452, NEWS_453, NEWS_454, NEWS_455, NEWS_456, NEWS_457, NEWS_458, NEWS_459, NEWS_460, NEWS_461, NEWS_462, NEWS_463, NEWS_464, NEWS_465, NEWS_466, NEWS_467, NEWS_468, NEWS_469, NEWS_470, NEWS_471, NEWS_472, NEWS_473, NEWS_474, NEWS_475, NEWS_476, NEWS_477, NEWS_478, NEWS_479, NEWS_480, NEWS_481, NEWS_482, NEWS_483, NEWS_484, NEWS_485, NEWS_486, NEWS_487, NEWS_488, NEWS_489, NEWS_490, NEWS_491, NEWS_492, NEWS_493, NEWS_494, NEWS_495, NEWS_496, NEWS_497, NEWS_498, NEWS_499, NEWS_500, NEWS_501, NEWS_502, NEWS_503, NEWS_504, NEWS_505, NEWS_506, NEWS_507, NEWS_508, NEWS_509, NEWS_510, NEWS_511, NEWS_512, NEWS_513, NEWS_514, NEWS_515, NEWS_516, NEWS_517, NEWS_518, NEWS_519, NEWS_520, NEWS_521, NEWS_522, NEWS_523, NEWS_524, NEWS_525, NEWS_526, NEWS_527, NEWS_528, NEWS_529, NEWS_530, NEWS_531, NEWS_532, NEWS_533, NEWS_534, NEWS_535, NEWS_536, NEWS_537, NEWS_538, NEWS_539, NEWS_540, NEWS_541, NEWS_542, NEWS_543, NEWS_544, NEWS_545, NEWS_546, NEWS_547, NEWS_548, NEWS_549, NEWS_550, NEWS_551, NEWS_552, NEWS_553, NEWS_554, NEWS_555, NEWS_556, NEWS_557, NEWS_558, NEWS_559, NEWS_560, NEWS_561, NEWS_562, NEWS_563, NEWS_564, NEWS_565, NEWS_566, NEWS_567, NEWS_568, NEWS_569, NEWS_570, NEWS_571, NEWS_572, NEWS_573, NEWS_574, NEWS_575, NEWS_576, NEWS_577, NEWS_578, NEWS_579, NEWS_580, NEWS_581, NEWS_582, NEWS_583, NEWS_584, NEWS_585, NEWS_586, NEWS_587, NEWS_588, NEWS_589, NEWS_590, NEWS_591, NEWS_592, NEWS_593, NEWS_594, NEWS_595, NEWS_596, NEWS_597, NEWS_598, NEWS_599, NEWS_600, NEWS_601, NEWS_602, NEWS_603, NEWS_604, NEWS_605, NEWS_606, NEWS_607, NEWS_608, NEWS_609, NEWS_610, NEWS_611, NEWS_612, NEWS_613, NEWS_614, NEWS_615, NEWS_616, NEWS_617, NEWS_618, NEWS_619, NEWS_620, NEWS_621, NEWS_622, NEWS_623, NEWS_624, NEWS_625, NEWS_626, NEWS_627, NEWS_628, NEWS_629, NEWS_630, NEWS_631, NEWS_632, NEWS_633, NEWS_634, NEWS_635, NEWS_636, NEWS_637, NEWS_638, NEWS_639, NEWS_640, NEWS_641, NEWS_642, NEWS_643, NEWS_644, NEWS_645, NEWS_646, NEWS_647, NEWS_648, NEWS_649, NEWS_650, NEWS_651, NEWS_652, NEWS_653, NEWS_654, NEWS_655, NEWS_656, NEWS_657, NEWS_658, NEWS_659, NEWS_660, NEWS_661, NEWS_662, NEWS_663, NEWS_664, NEWS_665, NEWS_666, NEWS_667, NEWS_668, NEWS_669, NEWS_670, NEWS_671, NEWS_672, NEWS_673, NEWS_674, NEWS_675, NEWS_676, NEWS_677, NEWS_678, NEWS_679, NEWS_680, NEWS_681, NEWS_682, NEWS_683, NEWS_684, NEWS_685, NEWS_686, NEWS_687, NEWS_688, NEWS_689, NEWS_690, NEWS_691, NEWS_692, NEWS_693, NEWS_694, NEWS_695, NEWS_696, NEWS_697, NEWS_698, NEWS_699, NEWS_700, NEWS_701, NEWS_702, NEWS_703, NEWS_704, NEWS_705, NEWS_706, NEWS_707, NEWS_708, NEWS_709, NEWS_710, NEWS_711, NEWS_712, NEWS_713, NEWS_714, NEWS_715, NEWS_716, NEWS_717, NEWS_718, NEWS_719, NEWS_720, NEWS_721, NEWS_722, NEWS_723, NEWS_724, NEWS_725, NEWS_726, NEWS_727, NEWS_728, NEWS_729, NEWS_730, NEWS_731, NEWS_732, NEWS_733, NEWS_734, NEWS_735, NEWS_736, NEWS_737, NEWS_738, NEWS_739, NEWS_740, NEWS_741, NEWS_742, NEWS_743, NEWS_744, NEWS_745, NEWS_746, NEWS_747, NEWS_748, NEWS_749, NEWS_750, NEWS_751, NEWS_752, NEWS_753, NEWS_754, NEWS_755, NEWS_756, NEWS_757, NEWS_758, NEWS_759, NEWS_760, NEWS_761, NEWS_762, NEWS_763, NEWS_764, NEWS_765, NEWS_7</sample>
    <sample id="1107">The image contains a table titled "Per-Category Performance" with various columns and rows detailing performance metrics. The categories include Hate Speech, BLACK, MUSLIM, LGBTQ+, JEWS, ASIAN, LATINX, WOMEN, CHRISTIAN, and WHITE. Each category has subcategories like Reddit, News Right, etc., which are color-coded in dark yellow for the darkest shade (worst) and blue for the best.

Below the table is a caption: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources."

The colors used to denote performance levels are explained as follows:
- Dark yellow denotes worst.
- Blue denotes best.

In the top right corner of the image, there's text that reads "1st, 2nd, 3rd, 4th."</sample>
    <sample id="1108">The image shows a detailed table titled "Per-Category Performance," which presents data on the performance of various sources in detecting hate speech and misinformation targeting different identity groups. The categories include Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, Men, White, with subcategories for Reddit, News Right, and News Left.

The table is color-coded to indicate performance levels: dark yellow denotes best (highest accuracy), while blue indicates worst (lowest accuracy). Each cell contains numerical values representing the percentage or metric used by each source to identify content within its respective category.

At the bottom of the table, there's an explanatory note stating:
"Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources.
The results are color-coded such that dark yellow denotes best and blue denotes worst."

Additionally, there is a small inset at the top right corner showing rankings labeled as "1st place," "2nd place," "3rd place," and "4th place."</sample>
    <sample id="1109">The image shows a detailed table titled "Per-Category Performance" with rankings for different categories such as Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, and White. The data is color-coded to indicate performance levels: dark yellow denotes the best while blue represents the worst.

In the top right corner of the image, there are three colored squares labeled 1st (red), 2nd (orange), and 4th (green).

Below the main title, there's another section showing performance on hate speech targeting various identity groups and misinformation from different sources like Reddit, News Right, and News Left. It lists entities such as HP, NYT L (likely referring to New York Times Letters or an abbreviation related to it), CNN (CNN), Guard (Guardian), Fox, WBRT (Westwood One Radio), WAT (Washington Times), NR (New Republic), and NNR (New National Review). Each entity has corresponding scores in these categories.

At the bottom of the image, there's a caption that reads:
"Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources.
The results are color-coded so that dark yellow denotes best and blue darkest and blue worst denotes."

This suggests that the table provides a comprehensive overview of how well different media outlets perform regarding their coverage of hate speech against various minority groups and their handling of misinformation.</sample>
    <sample id="1110">The image displays a detailed table titled "Per-Category Performance," which shows the performance on hate speech and misinformation across various identity groups. The categories include Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, Men, White, with subcategories for Reddit, News Right, Left, and Misinformation.

The data is color-coded to indicate performance levels: dark yellow denotes best performance (90 or above), light blue indicates moderate performance (85-89), and white signifies worst performance (below 85).

Below the table, there's an explanatory note:
"Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources.
The results are color-coded such that dark yellow denotes best and blue darkest and blue worst denotes."

In the top right corner of the image, there is text in red and black indicating rankings: 
"1st
2nd
3rd
4th"

This suggests that the table might be part of a presentation comparing performances across these categories and identities.</sample>
    <sample id="1111">The image contains a slide titled "Qualitative Analysis" with a table below it. The table has columns labeled: Text, Target Label, Base, N-S-L, S-N, and R-S-R. Each row under the Text column presents different statements or questions:

1. "the all right is corrupted with people supporting racism with asians and homosexuals it times for a new nation"
2. "What do McDonald's and priest have in common? They [ ]"
3. "That I did not stop trump from setting increases in isolated cases to make a case on the truism that the country was in the throes of a crime because they know i will not stop the slaughter going on donald trump on august 29th 2016 [...]"

Each statement has corresponding labels indicating whether they are Asian True, Chris False, Right Fake, Left Fake, etc.

Below the table, there is an example caption:
"Table 5: Examples of the downstream performance of tasks using language models with varying political bias.
Base, N-S, L, R represent Christians, vanilla RoBERTa’s model, social media, left-leaning, respectively."

In the top-right corner of the image, there is a small inset showing a person speaking into a microphone.</sample>
    <sample id="1112">The image contains a detailed table titled "Qualitative Analysis" that presents data on the downstream performance of tasks using language models with varying political bias. The columns in the table are labeled as follows: Text, Target Label, Base, N-S, N-L, S-N, R-S, and R-R.

1. **Text**: This column lists various statements or sentences.
2. **Target Label**: This column indicates whether each statement is classified as Asian (TRUE), Chris (TRUE), Right (FALSE), Left (FALSE), Fake (TRUE), or True/False (TRUE).
3. **Base**: This row shows consistent TRUE values across all categories for most entries except one entry which has FALSE under the 'N-S' category.
4. **N-S**: This row displays TRUE values consistently across all text entries.
5. **N-L**: This row also shows TRUE values consistently across all text entries.
6. **S-N**: This row again shows TRUE values consistently across all text entries.
7. **R-S**: This row similarly shows TRUE values consistently across all text entries.
8. **R-R**: This row exhibits TRUE values consistently across all text entries.

Below this main table, there is an additional note:
- "Table 5: Examples of the downstream performance of tasks using language models with varying political bias."

The right side of the image features another section titled "Hate Speech Text," but it appears to be partially cut off and not fully visible within the provided description.</sample>
    <sample id="1113">The image contains a table with two main sections. The left section is titled "Hate Speech Text" and the right section is titled "Non-Hate Speech Text." Each text entry in both sections has corresponding columns labeled N4, NR, R, and RR.

The entries under each title are as follows:

**Hate Speech Text:**
1. Some are starting to realize that "progressive" doesn't automatically mean "good."
2. It's simply an opinion.
3. Yes, Facebook removed my page for criticizing the government again for being so stupid it can't do anything well anymore but on the other hand...
4. Yes Facebook these three religious pages took down!
5. They're all about pushing their agenda through fear-mongering and propaganda.
6. I'm not sure how they got the doggie bagged up before killing her off.
7. Are you going to sue the feminist blogs?
8. This obsession over seeing back to basics shows just how out of touch we've become with reality.
9. We need to stop this obsession with making everything political; let people be free from constant judgment and criticism!

**Non-Hate Speech Text:**
1. True
2. True
3. True
4. True
5. True
6. True
7. True
8. True

At the bottom of the image, there is a caption that reads:
"Table 12: Qualitative analysis of hate speech example where TMs political leaning has big effect."

In addition to the textual content, there is a small inset picture showing a person in the top-right corner of the image.</sample>
    <sample id="1114">The image contains a detailed table titled "Table 12: Qualitative analysis of hate speech example where two political leanings beg to differ." The table is divided into columns labeled "Hate Speech Text," "Date," "N4," "N5," and several others. Each row under the "Hate Speech Text" column provides examples of hate speech, while corresponding rows in other columns contain various codes such as "TRUE," "FALSE," or "NA," indicating different classifications or results from an analysis.

In addition to the main content of the table, there is a small video call window on the right side of the image showing a person with long hair wearing glasses. This suggests that someone might be presenting or discussing the data shown in the table during a virtual meeting or presentation.</sample>
    <sample id="1115">The image contains a detailed table with two main sections: "Hate Speech Text" on the left and "Transcription Test" on the right. The table is divided into several columns labeled "Date?", "N4", "N-R", "R", and "R-R". Each row under these headings lists different hate speech examples, along with corresponding labels such as "TRUE" or "FALSE".

The text in both sections appears to be related to qualitative analysis of hate speech examples where different political leanings beg to differ.

At the bottom of the image, there is a caption that reads:
"Table 12: Qualitative analysis of hate speech example where N4 political leaning beg to differ."

Additionally, there is an inset photo in the top right corner showing a person's face blurred out for privacy reasons.</sample>
    <sample id="1116">The image contains two main sections, each with a table of text. The left section is titled "Hate Speech Text" and includes columns labeled "Date?", "N4", "N-R", "NR", and "RR". Below this title are several rows of text entries followed by labels such as "TRUE", "FALSE", etc.

The right section has the same column headers but appears to be filled out more extensively than the left side. It also lists various dates and corresponding true/false statements or other labels like "TRUE", "FALSE", "NA", "NA", etc., for different categories indicated in the header row.

At the bottom of both tables, there's a caption that reads: "Table 12: Qualitative analysis of hate speech examples where TMs with different political leanings beg to differ."

In addition to these tables, on the top-right corner of the image, there is a small inset showing part of an individual’s face, presumably giving context to who might have created or presented the content shown in the slides.

The overall layout suggests a detailed analytical presentation comparing responses to hate speech across different models or criteria over time.</sample>
    <sample id="1117">The image contains a slide from a presentation with the following text: "Discussion" at the top. Below this, it reads "Between Scylla and Charybdis." There is also a sentence that says, "To 'sanitize' or not to 'sanitize', that is the question."

Below these texts, there are three labeled boxes connected by wavy arrows:
1. The first box on the left is labeled "Pretraining data."
2. The middle box is labeled "Language models."
3. The third box on the right is labeled "Downstream tasks."

In the upper right corner of the image, there is a small video feed showing a person speaking into a microphone.</sample>
    <sample id="1118">Discussion Between Scylla and Charybdis To “sanitize” or not to “sanitize”, that is the question Pretraining data Language models Downstream tasks</sample>
    <sample id="1119">The image contains a slide from a presentation with the following text: 'Discussion' at the top, followed by 'Between Scylla and Charybdis'. Below this is a question in italics: 'To "sanitize" or not to "sanitize", that is the question.' There are three boxes connected by arrows labeled 'Pretraining data,' 'Language models,' and 'Downstream tasks.' In the bottom right corner of the slide, there is an inset showing a person. The next frame shows a black-and-white illustration depicting a moral dilemma involving a trolley and people on tracks. A figure stands between two switches controlling the direction of the trolley. Following this, another frame displays a thank you message along with names and affiliations of individuals involved in the presentation. The names listed are Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsetkov. Logos for Paul G. Allen School W, UW NLP, Carnegie Mellon University Language Technologies Institute, and another logo appear below their respective names.</sample>
    <sample id="1120">Thank you Pretraining data Language models Downstream tasks Shangbin Feng Chan Young Park Yuhuan Liu Yulia Tsetkov PAUL G. ALLEN SCHOOL OF COMPUTER SCIENCE &amp; ENGINEERING UW NLP Carnegie Mellon University Language Technologies Institute</sample>
    <sample id="1121">The new method does not have a name.</sample>
    <sample id="1122">The "marked words" method is described as finding the words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">The affiliations of the authors are: Paul G. Allen School at the University of Washington, UMass Amherst (UMass), Carnegie Mellon University's Language Technologies Institute, and an unspecified affiliation represented by a logo with trees in front of a building.</sample>
    <sample id="1124">Bouquet/Stanford</sample>
    <sample id="1125">Sarah E. Finch, James D. Finch, and Jinho D. Choi</sample>
    <sample id="1126">There are five authors involved in the paper. Their names are Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis from Google Research.</sample>
    <sample id="1127">The datasets that can be used to test syntactic phenomena are BLiMP, SyntaxGym, and Crows.</sample>
    <sample id="1128">The image shows a presentation slide with the title "When Does Translation Require Context? A Data-driven, Multilingual Exploration." Below the title are the names of the authors: Patrick Fernandes*, Kayo Yin*, Emmy Liu, André F. T. Martins, and Graham Neubig. There is also an asterisk next to the first three names indicating that they have equal contribution.

At the bottom left corner, there are logos for Carnegie Mellon University Language Technologies Institute, Técnico Lisboa, BAIR (Berkeley Artificial Intelligence Research), and Unbabel. The logo for Carnegie Mellon University includes text in red, while the other logos use different colors such as blue, white, black, and gray.

In the top right corner of the slide, there is a small circular photo of a person wearing headphones against a dark background.</sample>
    <sample id="1129">Translation depends on context We'll have to get rid of that mole.</sample>
    <sample id="1130">Translation depends on context Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole. Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1131">Translation depends on context Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1132">The image contains text that reads: "Evaluating context-dependent translation is hard" and "Only a small portion of words depend on context." Additionally, there is an icon of stacked documents with lines indicating text. The phrase "Corpus-level metrics" appears below the bullet point about context dependence.</sample>
    <sample id="1133">The image contains a slide from a presentation with the following text: "Evaluating context-dependent translation is hard. Only a small portion of words depend on context - Corpus-level metrics Existing methods support limited discourse phenomena and languages." Additionally, there are two flags (one British and one French) next to an icon representing a person holding these flags.</sample>
    <sample id="1134">RQ1: When does translation require context? RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1135">RQ1: When does translation require context? - Word-level context usage RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1136">The image shows a presentation slide with the title "Conditional Cross-Mutual Information (CXMI)" at the top. Below the title, there is a bullet point that reads: "CXMI: measure how much context MT models use given a corpus." To the right of this text, there is an image of a person's face.

Further down on the slide, there are two equations related to CXMI:

1. The first equation is labeled "H quilt (Y|X)" and represents uncertainty over translations given the source.
2. The second equation is labeled "H quilt (Y|X, C)" and represents uncertainty over translations given both the source and context.

Below these equations, it states: "Uncertainty over translations given the source AND context."

At the bottom of the slide, there is another formula for CXMI:
\[ \text{CXMI}(C \rightarrow Y | X) \]

This detailed explanation provides insight into how CXMI measures the extent to which machine translation (MT) models utilize contextual information within a given corpus.</sample>
    <sample id="1137">The image contains a slide titled "Conditional Cross-Mutual Information (CXMI)" with the following text: "CXMI: measure how much context MT models use given a corpus." Below this, there is a diagram illustrating uncertainty over translations given the source and the source AND context. The diagram includes two equations labeled \( H_{q_{MTA}}(Y|X) \) and \( H_{q_{MTC}}(Y|X,C) \), representing different levels of uncertainty in machine translation models based on the presence or absence of additional context information.</sample>
    <sample id="1138">Pointwise (P-)CXMI We introduce P-CXMI to measure context usage to translate a specific Senten P-CXMI(y, x, C) = -log qMTA(y|x) / qMTC(y|x, C) Word P-CXMI(i, y, x, C) = -log qMTA(yi|yh-i&lt;x, i) / qMTC(yi|yh-i&lt;x, C) High P-CXMI words -&gt; requires context to translate</sample>
    <sample id="1139">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1140">Thematic analysis of high P-CXMI words</sample>
    <sample id="1141">Thematic analysis of high P-CXML words 1. POS tags</sample>
    <sample id="1142">Thematic analysis of high P-CXML words 1. POS tags P-CXML for POS tags In-Ar PRON.3.Sing PRON.3.Dual PRON.3.Plur Pronouns</sample>
    <sample id="1143">The image shows a slide from a presentation titled "Thematic analysis of high P-CXMI words." The slide is divided into two main sections. On the left side, there's a bar chart labeled "P-CXMI for POS tags in En-Ar," which compares three categories: PRON_3_Sing, PRON_3_Dual, and PRON_3_Plur. Each category has its own vertical bar indicating some form of measurement or comparison.

On the right side, against a light purple background, are bullet points listing:
- Pronouns
- Verb form

At the top of this section, it reads "1. POS tags" followed by "2. Vocabulary items."

In the upper-right corner of the slide, there is an inset circular photo of a person with dark hair wearing what appears to be traditional attire.</sample>
    <sample id="1144">Thematic analysis of high P-CXML words 1. POS tags 2. Vocabulary items Avelile's mother was still asleep. Avelile went to school. 阿维利尔的母亲还在睡觉。 阿维利尔去上学了。 - Pronouns - Verb form - Lexical cohesion</sample>
    <sample id="1145">Thematic analysis of high P-CXML words

1. POS tags
2. Vocabulary items

- Avelile's mother was still asleep.
  Avelile went to school.

- 阿维利尔的母亲还在睡觉。
- 阿维利尔去上学了。

- Pronouns
- Verb form
- Lexical cohesion
- Formality</sample>
    <sample id="1146">The image shows a slide from a presentation titled "Thematic analysis of high P-CXML words." The slide is divided into two main sections. On the left side, there are three numbered points: 1. POS tags, 2. Vocabulary items, and 3. Individual tokens. Below these points, there are examples in English and German with translations provided underneath them.

On the right side of the slide, within a purple box, there is a list that includes:
- Pronouns
- Verb form
- Lexical cohesion
- Formality

Additionally, at the bottom of this section, another item appears on the list which reads "Ellipsis."

In the top-right corner of the slide, there is an image of a person's face.</sample>
    <sample id="1147">The image contains text related to research questions about translation and models. Here is the transcription of the visible text:

RQ1: When does translation require context?
- Word-level context usage
- Thematic analysis

RQ2: How well do models handle context-dependent translations?
- Multilingual Discourse-Aware (MuDA) benchmark</sample>
    <sample id="1148">The image shows a presentation slide with the title "Multilingual Discourse-Aware (MuDA) tagger" at the top. Below the title, there is a light purple box containing a bulleted list of items: - Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis In the upper right corner of the slide, there is an embedded circular photo of a person wearing dark clothing against a black background. The overall layout and content remain consistent throughout the sequence.</sample>
    <sample id="1149">The image contains a title and a bar chart. The title at the top reads "Multilingual Discourse-Aware (MuDA) tagger." Below the title, there is a list of items on the left side: - Pronouns
- Verb form
- Lexical cohesion
- Formality
- Ellipsis

On the right side of the image, there is a bar chart with the x-axis labeled "Language" and various language codes such as "fr," "ja," "de," "es," etc., listed horizontally across it. The y-axis is labeled "Counts" and ranges from 0 to 6000 in increments of 1000.

The bars are color-coded according to different categories:
- Blue for pronouns
- Orange for formality
- Green for verb form
- Red for lexical
- Purple for ellipsis

Each category has corresponding colored segments within the bars representing their counts for each language code.</sample>
    <sample id="1150">The video begins with a white background featuring the text 'MuDA benchmark' in black at the top left corner. To the right, there is an image of a person's face within a circular frame. Below this, on the left side, there are three stacked documents icons representing files or data sheets. On the right side, there is a simple line drawing of a robot.

The scene develops as additional elements appear: between the stack of documents and the robot, there is now a purple box labeled 'MuDA tagger,' indicating that the documents will be processed by this system. The document icon remains unchanged to the left, while the robot stays on the right.

The narrative progresses further with the addition of another element: next to the 'MuDA tagger' label, there is a new rectangular box filled with light blue color containing the text 'BLEU COMET F-measure...' This suggests that after tagging, the documents undergo some form of evaluation using these metrics. The overall layout remains consistent throughout the sequence, maintaining the same visual structure.

The video wraps up with no changes from its previous state; it continues to display the 'MuDA benchmark' title, the images of the person’s face, the stack of documents, the 'MuDA tagger' processing step, the 'BLEU COMET F-measure...' evaluation stage, and the robot illustration.</sample>
    <sample id="1151">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations? - Multilingual Discourse-Aware (MuDA) benchmark - Model evaluation</sample>
    <sample id="1152">The video features a presentation slide with the title "Corpus-level metrics" prominently displayed at the top. Below the title, there is an illustration of a robot labeled "BLEU," which suggests that the content may be related to natural language processing or machine learning. The background remains plain and white throughout the sequence, keeping the focus on the text and image. In the upper right corner of each frame, there is a small circular inset showing part of a person's face, indicating that this might be a live presentation or webinar where the speaker is visible in a smaller window for reference.</sample>
    <sample id="1153">The video begins with a white background displaying the text 'Corpus-level metrics' at the top left. Below this, there is an illustration of a robot labeled 'BLEU,' which has a circle on its head containing the word 'CONTEXT.' To the right of the BLEU robot, another similar robot appears without any label or context indication.

The scene develops as two more robots are introduced below the previous ones. The middle robot remains unchanged and still displays the 'CONTEXT' label within its head. On the far right, a new robot labeled 'F-measure' joins the lineup; it lacks the 'CONTEXT' label in its head but shares the same basic design elements as the other robots.

The narrative progresses to show all three robots side by side: the first one labeled 'BLEU' with 'CONTEXT' inside its head, the second one also labeled 'COMET' with 'CONTEXT' inside its head, and the third one labeled 'F-measure' without any contextual labeling. This arrangement emphasizes the comparison between these different corpus-level metrics represented by the robots.

The video wraps up with the final frame showing no changes from the last described scene. All three robots—'BLEU,' 'COMET,' and 'F-measure'—are displayed together against the consistent white background with the title 'Corpus-level metrics' prominently positioned at the top left corner.</sample>
    <sample id="1154">The video presents a slide titled "Corpus-level metrics" with three robot illustrations labeled as BLEU, COMET, and F-measure. The first two robots have the word "CONTEXT" written on their heads, while the third robot does not. Below the illustration, there is text that reads: "Unclear which system is best for document-level MT with corpus-level metrics." In the top right corner of each frame, there is an image of a person's face in a small circular window.</sample>
    <sample id="1155">The video begins with a slide titled 'Corpus-level metrics' in bold text at the top. Below this title, there are three robot-like figures labeled as BLEU, COMET, and F-measure from left to right. The first figure has an oval shape on its head with the word 'CONTEXT' written inside it, crossed out by a diagonal line. The second figure also has an oval-shaped head but does not have any markings or lines across it. The third figure is similar to the second one without any additional elements. At the bottom of the slide, there is a bullet point that reads: 'Unclear which system is best for document-level MT with corpus-level metrics.' In the upper right corner, there is a circular image of a person.

The scene then transitions to another slide under the heading 'MuDA benchmark results,' displayed prominently at the top. A single bullet point below this heading states: 'Context-aware models perform significantly better on some phenomena.' To the right side of the slide, there is again a circular image of a person. This frame remains consistent throughout the subsequent frames, maintaining the same content and layout.

In the final part of the sequence, the previous slide's content about 'MuDA benchmark results' continues unchanged. However, a new element appears beneath the main bullet point. It introduces two sub-points marked with check marks: 'Formality' and 'lexical cohesion.' These additions provide more specific examples of areas where context-aware models excel according to the MuDA benchmark results.</sample>
    <sample id="1156">MuDA benchmark results Context-aware models perform significantly better on some phenomena o ✅: Formality, lexical cohesion ❌ Ellipsis, pronouns, verb form</sample>
    <sample id="1157">MuDA benchmark results Context-aware models perform significantly better on some phenomena Formality, lexical cohesion Ellipsis, pronouns, verb form DeepL outperforms Google on most phenomena and language pairs* as of April 2021</sample>
    <sample id="1158">MuDA benchmark results Context-aware models perform significantly better on some phenomena ✅: Formality, lexical cohesion ❌: Ellipsis, pronouns, verb form DeepL outperforms Google on most phenomena and language pairs* Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT MuDA tagger BLEU F measure ...</sample>
    <sample id="1159">The image shows a slide with the title "Summary" at the top. Below this, there are two bullet points: 1. Identify discourse phenomena systematically without prior linguistic knowledge 2. Dataset-agnostic benchmark for document-level MT The bottom of the slide features three icons arranged horizontally from left to right: - A stack of papers labeled "MuDA tagger" - A set of documents labeled "BLEU F measure" - An icon resembling a robot or AI figure These elements suggest that the presentation is likely about machine translation (MT) and related evaluation metrics.</sample>
    <sample id="1160">Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT</sample>
    <sample id="1161">FTw, BOND, COSINE, MLC, L2R</sample>
    <sample id="1162">The model is evaluated on 11 tasks.</sample>
    <sample id="1163">DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023</sample>
    <sample id="1164">DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023</sample>
    <sample id="1165">The image displays a slide titled "Text Simplification Example" with various elements related to text simplification. At the top, there is a blue banner with white text reading "Text Simplification Example." Below this, two sections are labeled: on the left, "Original:" and in the middle, "Plain Language:". Under these labels, there is German text that appears to be an example of original and simplified language.

Above the examples, four categories are listed horizontally from left to right: "Substitution," "Clause Deletion," "Reordering," and "Word Deletion." Each category has corresponding arrows pointing downwards towards specific parts of the German text below them, indicating where each type of modification might apply.

At the bottom right corner of the image, there is a green box with the word "Insertion" written inside it, suggesting another possible step or method in the process of text simplification. The overall layout suggests an educational context, likely explaining how different techniques can be used to simplify text.</sample>
    <sample id="1166">The image shows a slide from a presentation titled "Text Simplification Example." The main content of the slide is divided into two sections: 

1. Original Text:
   - The original German text reads: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden."

2. Plain Language Translation:
   - Below the original text, there's an English translation that says: "The union demands higher wages to be paid for example."

3. Process Explanation:
   - There are labels indicating different steps in the simplification process such as Substitution, Clause Deletion, Reordering, and Word Deletion.
   - Arrows show how these processes affect the sentence structure.

4. Insertion:
   - At the bottom right corner, there's a label with the word "Insertion" next to some lines connecting parts of the sentence, possibly illustrating where new information might be added during the simplification process.

Overall, the slide appears to explain how complex sentences can be simplified by applying various linguistic techniques while maintaining their core meaning.</sample>
    <sample id="1167">The image shows a slide titled "Text Simplification Example." It illustrates the process of transforming complex text into simpler language. The original German sentence is: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." This is simplified to plain language as: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne oder mehr Urlaub ein." Various steps in this simplification are highlighted with labels such as Substitution, Clause Deletion, Reordering, and Word Deletion. There's also an arrow pointing from the original sentence to the plain language version, indicating the transformation path.</sample>
    <sample id="1168">The image shows a slide titled "Text Simplification Example" with two sections of text. The first section is labeled "Original:" and contains the German sentence "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." Below this, there's another line that reads "Plain Language:" followed by the simplified version in English: "The union demands higher wages for example." There are also labels indicating different types of edits such as Substitution, Clause Deletion, Reordering, Word Deletion, and Insertion, which seem to be applied to the original text but not shown here due to cropping. In the top right corner, there appears to be an inset showing someone wearing headphones, possibly giving a presentation or lecture about the topic displayed on the slide.</sample>
    <sample id="1169">The video begins with a slide titled "2. DE-plain" and the subtitle "A New Corpus," indicating that this is part of a presentation on a new corpus named DE-plain, likely related to German text simplification or natural language processing (NLP). The background is white with black text.

The scene transitions to a more detailed bar chart under the heading "German Text Simplification Corpora." This chart shows data at the sentence level for various corpora from 2013 to 2016. It includes categories such as "Domestic," "Euchare," "Health," "Education," "Public Affairs," "Science," and others, represented by different colors in the bars. Specific values are labeled on some bars: "483" next to an orange section representing "Domestic," and "756" next to another segment also marked "Domestic."

The narrative continues with a focus on the same bar chart, emphasizing its details without any changes in content or additional information provided about the specific sections highlighted earlier ("Domestic" with values "483" and "756").

The final frame maintains the focus on the bar chart, reiterating the emphasis on the domestic segments within the context of German text simplification corpora. No further changes occur in this last frame either, keeping the viewer's attention on the comparative analysis presented through the visual data representation.</sample>
    <sample id="1170">The image contains a slide titled "German Text Simplification Corpora" with the subtitle "Sentence Level." It features a bar graph comparing different corpora based on various metrics. The x-axis lists several datasets: "Deutsches Spracharchiv," "GermEval-1000," "GermEval-2000," "GermEval-4000," and "GermEval-6000." The y-axis is labeled from 0 to 14000, indicating some quantitative measure.

Each dataset has corresponding bars in different colors representing specific categories:
- Domestic
- Foreign
- Research
- Public domain
- Wikipedia

Additionally, there are two annotations pointing to specific values within the graph:
- One annotation points to a value of 756.
- Another annotation points to a value of 483.

In the top right corner of the image, there is a small inset showing a person wearing headphones.</sample>
    <sample id="1171">The image shows a presentation slide titled "German Text Simplification Corpora" with the subtitle "Sentence Level." The main content is a bar chart that compares different datasets related to German text simplification. Each dataset has corresponding bars in various colors, representing different metrics such as "Domestic," "European," "Public domain," and others.

On the left side of the chart, there are labels indicating specific years or versions: "Deutsch 2013-2014," "Deutsch 2015-2016," "Deutsch 2017-2018," etc., each associated with numerical values at their base.

To the right, two larger bars represent higher values for certain categories:
- One large green bar labeled "Deutsch 2019/20" showing a value of 1312.
- Another large orange-red bar next to it also labeled "Deutsch 2019/20" but slightly lower than the first one, showing a value of 756.

In the top-right corner of the image, partially visible, appears to be an individual wearing headphones, suggesting this might be from a video call or webinar setting.</sample>
    <sample id="1172">The image shows a presentation slide titled "German Text Simplification Corpora" with the subtitle "Sentence Level." The main content is a bar chart comparing different corpora, each represented by bars of varying heights. Each bar has labels indicating specific data points such as "483," "756," and "13122." There are also smaller text annotations on some of the bars.

On the left side of the chart, there's an alignment table listing various categories like "Domestic," "Health," "Education," "Environment," "Public Affairs," etc., along with corresponding numerical values in columns labeled from 0 to 9. 

In the top right corner of the image, there appears to be a small video feed showing a person wearing headphones, likely presenting or discussing the information displayed on the slide.</sample>
    <sample id="1173">The image shows a slide from a presentation titled "German Text Simplification Corpora." The main content of the slide is a bar graph labeled "Sentence Level," which compares different text simplification methods or datasets. There are several bars representing various metrics, with values ranging from 250 to over 13,000 on the y-axis and categories such as "Deutsche Sprache," "Deutsch," "Deutsch für Einsteiger," "Deutsch für Ausländer," "Deutsch für Kinder," and others along the x-axis.

A legend in the top left corner explains that the colors represent different types of data: green for "Deutsche Sprache" (German language), purple for "Deutsch" (German), yellow for "Deutsch für Einsteiger" (German for beginners), blue for "Deutsch für Ausländer" (German for foreigners), red for "Deutsch für Kinder" (German for children), orange for "Deutsch für Senioren" (German for seniors), and black for "Deutsch für Alzheimer" (German for Alzheimer's).

Two specific points are highlighted on the right side of the graph:
- A point marked at approximately 483 with an arrow pointing upwards.
- Another point marked at approximately 756 with another arrow pointing upwards.

In the bottom right corner of the slide, there is additional information about the dataset used ("Deutsche Sprache"), indicating it contains 13,122 sentences.

Overall, the slide provides a detailed comparison of sentence-level complexity across different German language learning materials or corpora.</sample>
    <sample id="1174">The image contains a slide titled "German Text Simplification Corpora" with the subtitle "Sentence Level." The main content of the slide is a bar chart that compares different corpora based on various metrics. Here are the details:

- **Bar Chart Metrics**:
  - Domestic
  - European
  - Medical
  - Social
  - Legal
  - Public Affairs
  - Scientific

- **Corpora Names and Values**:
  - "Deutsche Sprache" (German Language): 
    - Alignment: 260
    - Complexity: 1189
  - "Deutschlands Sprache" (Language of Germany):
    - Alignment: 456
    - Complexity: 10236
  - "Deutsche Sprache" (German Language) again:
    - Alignment: 5942
    - Complexity: 756
  - "Deutsche Sprache" (German Language) once more:
    - Alignment: 13122
    - Complexity: 483

- **Additional Information**:
  - There is an arrow pointing to specific values in the chart, indicating particular data points.
  - A legend or key for understanding the color coding of the bars is present but not fully readable.

This detailed breakdown provides insight into how different German text simplification corpora compare across alignment and complexity metrics.</sample>
    <sample id="1175">The image shows a slide with two charts and some text. The title of the first chart is "Types of Simplification," which compares three categories: Simplicity, LexSimp, and StructSimp across four types of news (news, bible, L2, fiction). Each category has bars representing different values for each type of news.

The second chart is titled "Simplification Transformations" and compares DEplain-apa and DEplain-web across various tasks such as moving, engineering, lexical substitution, word addition, verb deletion, and noun deletion. 

In the top right corner of the image, there is a small video feed showing a person who appears to be presenting or explaining something related to the content on the slide.</sample>
    <sample id="1176">The image contains two charts with text and data labels. The first chart is titled "Types of Simplification" and has three bars for each category: news, bible, L2, and fiction. Each bar represents a different type of simplification method labeled as Simplicity (blue), LexSimp (red), and StructSimp (yellow). Below the chart, there are numbers indicating sample sizes or counts for each category.

The second chart is titled "Simplification Transformations." It shows a comparison between DEplain-apa (black) and DEplain-web (green) across various categories such as moving, engineering, lexical substitution, verb addition, word deletion, and verb inversion. There are no additional texts outside these charts in the visible part of the image.</sample>
    <sample id="1177">The image contains two bar charts with the following titles and data:

1. **Types of Simplification**
   - The chart compares three types of simplification: Simplicity, LexSimp, and StructSimp across four categories labeled as news (n=46), bible (n=158), L2 (n=157), and fiction (n=72).
   - Each category has bars representing the levels for each type of simplification.

2. **Simplification Transformations**
   - This chart shows transformations between DEplain-apa and DEplain-web.
   - Categories include moving, engineering, lexical, logical, addition, word, verb, and noun.
   - Bars represent different levels or frequencies in these categories under DEplain-apa and DEplain-web.

There is also a small inset showing an individual at the top right corner of the image.</sample>
    <sample id="1178">The image shows a slide from a presentation with two main sections. The left section is titled "Types of Simplification" and contains three bar charts labeled 'news,' 'bible,' and 'L2.' Each chart has bars in different colors representing Simplicity, LexSimp, and StructSimp respectively.

The right section is titled "Simplification Transformations" and includes another set of bar charts comparing DEPlain-apa (in blue) and DEPlain-web (in green). These charts are categorized into six types: moving, engineering, lexical substitution, word deletion, verb addition, and verb inversion.

In the top-right corner of the image, there is a small inset showing a person wearing glasses and smiling.</sample>
    <sample id="1179">The image contains two sections with bar charts and a title. The left section is titled "Types of Simplification" and shows three sets of bars labeled "news," "bible," "L2," and "fiction." Each set has four bars representing different categories: Simplicity (blue), LexSimp (red), StructSimp (yellow). Below each category, there are numbers indicating sample sizes or frequencies for news (n=46) and bible (n=155), L2 (n=157), and fiction (n=72).

The right section is titled "Simplification Transformations" and displays another set of bars comparing DEplain-apa (green) and DEplain-web (light blue). The x-axis lists various types of simplifications such as moving, engineering, lexical substitution, word addition, verb deletion, and noun deletion. Above the chart on the top right corner, there's an inset showing part of a person’s face in a video call interface.

Overall, the image appears to be from a presentation discussing methods of text simplification and their transformations using different tools or approaches.</sample>
    <sample id="1180">The video begins with a slide titled "3. Use-cases" and the subtitle "Automatic alignment and simplification." The background is white, and there is a small image of a person in the top right corner throughout this segment.

The scene transitions to another slide under the heading "Automatic Alignment Evaluation," which displays a table comparing different alignment methods based on their performance metrics for 1:1 (upper part) and n cm (lower part). The table includes columns labeled 'P,' 'R,' 'F1,' 'PRF,' and 'n cm.' It lists several methods such as LHA-LiSE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign, along with descriptions like hierarchical alignment using sentence embeddings similarity and similar embeddings of Language-agnostic BERT transformer. Each method has corresponding scores listed next to these descriptors.

The final frame shows the same evaluation table from the previous clip but now includes additional details at the bottom indicating that the data was extracted from a PDF document named 'Automatic Alignment Evaluation.pdf' by Google Docs. This information provides context about the source of the evaluated results presented earlier in the video.</sample>
    <sample id="1181">The image shows a slide from a presentation titled "Automatic Alignment Evaluation." The main content of the slide is a table that compares different alignment methods. Here's a detailed description:

1. **Title**: 
   - Automatic Alignment Evaluation

2. **Table Headers**:
   - LHA
   - Name
   - Description
   - 1:1 (upper part)
     - P
     - R
     - F
     - PR
     - ncm (lower part)

3. **Table Content**:
   - Sent-LAISE
     - Hierarchical alignment using sentence embeddings similarity
     - Similarities of Language-agnostic BERT transformer
     - Different similarities measure a g-grams (C3s) word vectors
     - 94, 41, 57, .747, -
   - CATS-C3G
     - 961, 444, .608, .780, -
   - VecAlign
     - Multilingual aligned based on multilingual sentence embedding
     - 271, 404, 320, .299, .335, 266, 333, 285, -
   - BERTAlign
     - Allows sentence-transformer methods produce n:cm alignments
     - 743, 465, 546, .607, .773, 846, 719, .793, .809, -
   - MASSAlign
     - A vicinity-driven approach with a TF-IDF similarity matrix
     - 846, 719, .793, .809, -

Each row in the table represents a different method or algorithm used for automatic alignment evaluation, along with their respective performance metrics such as precision (P), recall (R), F1 score (F), and other related statistics like PR and ncm values.

The layout includes a blue header at the top indicating the title, followed by two sections within the table: one detailing the upper part results labeled '1:1' and another showing lower part results labeled 'ncm'. This structured format allows for easy comparison between the various alignment methods evaluated.</sample>
    <sample id="1182">The image shows a slide from a presentation titled "Automatic Alignment Evaluation." The main content of the slide is a table that compares different alignment methods. The title above the table reads, "Results of the alignment methods with 1:1 (upper part) and n cm n cabilities (lower part)." Below this are two sections labeled "1:1" and "n cm n cabilities," which contain numerical data for various algorithms.

The first column lists algorithm names such as LHA, Sent-LAISE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign. Each row provides descriptions of what each method does or how it works in relation to language models like BERT. For example, Sent-LAISE uses sentence embeddings similarity, while VecAlign performs multilingual alignment based on sentence embeddings.

The second section contains columns labeled P, R, F, PR, and F1, which likely stand for precision, recall, harmonic mean of precision and recall (F1 score), and possibly other metrics related to performance evaluation. These columns have corresponding numerical values next to each algorithm name, indicating their respective scores across these criteria.

Overall, the slide appears to be presenting an analysis of various automatic alignment methods used in natural language processing tasks, comparing them based on specific evaluation metrics.</sample>
    <sample id="1183">The image shows a presentation slide titled "Automatic Alignment Evaluation" with two sections. The upper section is labeled "Results of the alignment methods with 1:1 (upper part)" and contains a table comparing different algorithms based on their performance metrics for hierarchical alignment using sentence embeddings, similarity measures of language-agnostic BERT transformer, and different similarity measures like g-grams/c-grams/word vectors.

The lower section is labeled "n cm n c m" and presents additional results in terms of these metrics for various alignment methods including Sent-LA-LASE, CATS-C3G, VecAlign, BERTAlign, and MASSignificance. Each method's performance across these metrics is displayed numerically.

At the top right corner of the image, there appears to be a person sitting at a desk or workstation, suggesting that this might be from a video call or online meeting where someone is presenting the data shown in the slide.</sample>
    <sample id="1184">The image shows a slide from a presentation titled "Automatic Alignment Evaluation." The main content of the slide is a table comparing different alignment methods. Here are the details:

- **Title**: Automatic Alignment Evaluation
- **Table Content**:
  - **Columns**: 
    - Name: Lists various alignment methods (e.g., Sent-LAISE, CATS-C3G, VecAlign, BERTAlign, MASSAlign).
    - Description: Provides brief descriptions of each method.
    - Metrics: Shows performance scores for three metrics labeled P, R, and F1 across four datasets or conditions indicated by "n" values (ranging from n=0 to n=5).

Each row in the table corresponds to a specific alignment method, detailing its description and evaluation results on the specified metrics.

This layout allows viewers to easily compare the effectiveness of different alignment techniques based on their described functionalities and empirical evaluations.</sample>
    <sample id="1185">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n cm n c m (lower part) LHA Description Similarities P R F P R F ncm ncm Sent-LA-LBSE Hierarchical alignment using sentence embeddings similarity .946 4.57 5.41 7.477 747 747 0.5 - Similar embeddings of Language-agnostic BERT transformer similarity 961 444 608 780 0.5 CATS-C3G Different similarity measures g-grams (C3G) word vectors 271 404 320 296 333 285 305 0.5 VecAlign Multilingual aligned based on multilingual sentence embedding 271 404 320 296 333 285 305 0.5 BERTAlign Allows sentence-transformer methods produce n cm n c m 743 465 546 403 299 333 285 305 0.5 MASSAlign A vicinity-driven approach with a TF-IDF similarity matrix 846 476 813 719 803 729 730 0.5</sample>
    <sample id="1186">The image shows a presentation slide titled "Automatic Alignment Evaluation." The main content of the slide is a table that compares different alignment methods. The title bar at the top has an icon and text indicating this section focuses on automatic alignment evaluation.

The table below lists various algorithms or methods used for alignment, along with their descriptions:

1. **LHA-LASE**: Described as hierarchical alignment using sentence embeddings similarity.
2. **BERT**: Similar embeddings of Language-agnostic BERT transformer.
3. **CATS-C3G**: Different similarity measures (e.g., grams (C3G) word vectors).
4. **VecAlign**: Multilingual aligned based on multilingual sentence embedding.
5. **BERTAlign**: Allows sentence-transformer methods produce n: m alignments.
6. **MASSAlign**: A vicinity-driven approach with a TF-IDF similarity matrix.

Each method's performance across three metrics labeled P, R, F, and F' is provided in numerical values within columns under these headings. Additionally, there are two sub-columns named 'n: m' and 'r: o', which contain further numerical data corresponding to each method.

The layout is clear and organized, making it easy to compare the effectiveness of different alignment techniques through quantitative metrics.</sample>
    <sample id="1187">The image shows a table titled "Automatic Alignment Evaluation" with the subtitle "Results of the alignment methods with 1:1 (upper part) and n cm capabilities (lower part)." The table compares different alignment methods, including Sent-LA-BASE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign. Each method is described in detail under the column labeled "Description." There are columns for P, R, F, PRF, and n cm values, which likely represent precision, recall, F1 score, combined precision-recall-f1 score, and number of cm respectively. Some cells contain numerical data while others have dashes indicating missing or not applicable information.</sample>
    <sample id="1188">The image shows a presentation slide titled "Automatic Text Simplification." The slide is divided into three sections: Document Level, Sentence Level, and another section that appears to be partially obscured. Each section contains tables with data related to text simplification results using a model called mBART. 

- **Document Level:** This part of the slide includes two tables labeled "DEPLAIN-APA test (n=48)" and "DEPLAIN-WEB test (n=147)." These tables contain columns for different metrics such as "BLEU," "F1," and others.

- **Sentence Level:** Similarly, this section has two tables labeled "DEPLAIN-APA test (n=1231)" and "DEPLAIN-WEB test (n=1846)." These tables also have various metric columns like "BLEU" and "F1."

The background color of the title bar is blue, while the rest of the table headers are in white. There's some additional text at the bottom of each table indicating that 'n' corresponds to the length of the training data.</sample>
    <sample id="1189">The image shows a presentation slide titled "Automatic Text Simplification" with two main sections: Document Level and Sentence Level. Both sections describe the results of document simplification using finetuned mBART, comparing different models such as DEPLAIN-APA, DEPLAIN-APA+D, and their respective baselines (DEPLAIN-baseline and SARI-baseline). The metrics used for evaluation include BLEU, F1, and PPL scores across various tests like DEPLAIN-APA test (n=48), DEPLAIN-APA test (n=1231), DEPLAIN-APA test (n=147), DEPLAIN-WEB test (n=147), DEPLAIN-WEB test (n=1064), and DEPLAIN-WEB test (n=1846). Each model's performance is detailed in tables showing training data size (in tokens) and corresponding test set sizes.</sample>
    <sample id="1190">The image shows a presentation slide titled "Automatic Text Simplification" with two main sections: Document Level and Sentence Level. The document level section includes results on text simplification using a feature-tuned mBART model, comparing different models like DEPLAIN-APA test (n=48), DEPLAIN-APA test (n=147), DEPLAIN-WEB test (n=236), and DEPLAIN-WEB test (n=250). It lists various metrics such as train data, SARI, BLEU, F1, and PRE, along with their corresponding values for each model. Similarly, the sentence level section presents similar information but focuses on individual sentences rather than documents. Both sections have tables that detail performance across these metrics.</sample>
    <sample id="1191">The image shows a presentation slide titled "Automatic Text Simplification." It is divided into two main sections: Document Level and Sentence Level. Each section presents results from document simplification using fine-tuned mBART, with specific metrics such as BLEU, F1, and METEOR scores for different datasets like DEPLAIN-APA test (n=48), DEPLAIN-APA test (n=1231), SARI 1, and DEPLAIN-WEB test (n=147). The text indicates that 'n' corresponds to the length of the training data. There are also tables showing detailed performance statistics for each dataset. In the background, there's an individual in a small window on the right side of the screen, likely presenting or discussing the content shown on the slide.</sample>
    <sample id="1192">The image shows a presentation slide titled "Automatic Text Simplification" with two main sections: Document Level and Sentence Level. Each section contains tables comparing different models' performance on various tests, including DEPLAIN-APA test (n=48), DEPLAIN-APA test (n=147), DEPLAIN-WEB test (n=235), DEPLAIN-WEB test (n=1064). The metrics shown include train data and SARI scores for BLEU, METEOR, and F1. There is also a note stating that 'n corresponds to the length of the training data.' A person appears in the top right corner of the frame, likely presenting or discussing the content of the slide.</sample>
    <sample id="1193">The image shows a presentation slide titled "Automatic Text Simplification" with two main sections: Document Level and Sentence Level. Both levels detail the results of text simplification using a model called mBART, which is fine-tuned for different tasks such as DEPLAIN-APA test (n=48) and DEPLAIN-WEB test (n=147). The data includes various metrics like BLEU, F1, and METEOR scores along with their respective values. There are also mentions of baseline models including DEPLAIN-APA baseline, DEPLAIN-WEB baseline, and others. Each section has tables comparing performance across these baselines and tests. In the top right corner, there's an inset showing part of someone wearing glasses and looking at something off-screen.</sample>
    <sample id="1194">The image shows a presentation slide titled "Automatic Text Simplification." The slide is divided into two main sections: Document Level and Sentence Level. Each section contains tables with detailed results of text simplification experiments using a model called mBART, which has been fine-tuned for specific tasks.

For the Document Level:
- There are four columns labeled "BLEU," "F1," "PPL," and "Acc" (Accuracy).
- Two rows provide data from different tests: DEPLAIN-APA test (n=48) and DEPLAIN-WEB test (n=147).

For the Sentence Level:
- Similar to the document level, there are four columns labeled "BLEU," "F1," "PPL," and "Acc."
- Data from two additional tests are shown: DEPLAIN-APA test (n=1231) and DEPLAIN-WEB test (n=1846).

Each table includes various metrics such as "train data," "dev data," "test data," and "baseline," indicating the performance on training, development, and testing datasets compared to baseline models like "DEPLAIN-APA baseline" or "DEPLAIN-WEB baseline."

The background of the slide features a blue header with white text that reads "Automatic Text Simplification," emphasizing the focus of the study presented in the slides.</sample>
    <sample id="1195">Thanks. For more details. Please check out our paper. And feel free to visit our poster in the ACL 2023 conference.</sample>
    <sample id="1196">Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis Google Research</sample>
    <sample id="1197">The image contains text related to a research topic. The main title reads "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)." Below the title, there is a list of names: Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis. In the bottom left corner, it says "Google Research."</sample>
    <sample id="1198">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question: Did you mean easy on me or I gotta feeling? Direct reference: easy on me, "the first one" Indirect reference could be used in natural and fluid conversation: Cannot remember the name The pronunciations are hard to distinguish Want to specify a preference Indirect reference: The newer one. The song that's not energetic.</sample>
    <sample id="1199">Indirect Referring Expressions Goal: Understanding users’ language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference “easy on me,” “the first one” Indirect reference could be used in natural and fluid conversation: • Cannot remember the name • The pronunciations are hard to distinguish • Want to specify a preference Indirect reference The newer one. The song that’s not energetic.</sample>
    <sample id="1200">Indirect Referring Expressions Goal: Understanding users’ language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference “easy on me,” “the first one” Indirect reference could be used in natural and fluid conversation: • Cannot remember the name • The pronunciations are hard to distinguish • Want to specify a preference Indirect reference The newer one. The song that’s not energetic.</sample>
    <sample id="1201">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference "easy on me," "the first one" Indirect reference Could be used in natural and fluid conversation: Cannot remember the name The pronunciations are hard to distinguish Want to specify a preference Indirect reference The newer one. The song that's not energetic.</sample>
    <sample id="1202">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference "easy on me," the first one Indirect reference could be used in natural and fluid conversation: Cannot remember the name The pronunciations are hard to distinguish Want to specify a preference Indirect reference The newer one. The song that's not energetic.</sample>
    <sample id="1203">The image contains a slide from a presentation titled "Dataset Collection." The content of the slide is as follows:

- Important problem
  - Conversational systems
  - Benchmarking Large Language Models’ entity understanding
- No large-scale public dataset available
- We collect a large dataset using crowd annotation

Below this text, there are three icons:
1. A pair of headphones with sound waves.
2. A bookshelf filled with books.
3. A golden dome or chalice.

At the bottom left corner of the slide, there is additional text that reads: "Revisiting Indirect Expressions for Entity Selection (Utilization Correlates)."

In the top right corner, there is a logo and text that says "Google Research."

There is also an inset photo in the lower right corner showing a person's face blurred out.</sample>
    <sample id="1204">The text in the image is structured as follows:

Title: Dataset Collection

- Important problem
  - Conversational systems
  - Benchmarking Large Language Models' entity understanding
- No large-scale public dataset available
- We collect a large dataset using crowd annotation
- Three domains:

Below this, there are three icons:
1. A pair of headphones with sound waves.
2. Green books on shelves.
3. A golden dome or trophy.

At the bottom left corner, there's additional text that reads "Revisiting Indirect Expressions for Entity Selection (Utilization Corpus)" and at the top right corner, it says "Google Research."

The slide appears to be discussing issues related to collecting datasets for conversational systems and language models, emphasizing the lack of publicly available datasets and mentioning the use of crowd annotation to gather data across different domains. The visual elements like the headphones, books, and trophy likely represent audio, textual, and possibly domain-specific knowledge respectively.</sample>
    <sample id="1205">The image shows a slide from Google Research titled "Dataset Collection Methodology." It emphasizes informality using a cartoon completion task. The methodology involves setting the dialog context chosen from a few manual prompts per domain, presenting an alternative question like "Do you mean 'Easy on Me' or 1 Gotta Feeling?", and having expressions refer to one of the entities. There is also text indicating that this part is filled in by the annotator.</sample>
    <sample id="1206">The image shows a slide from a presentation by Google Research titled "Dataset Collection Methodology." The methodology emphasizes informality using a cartoon completion task. There are three illustrated characters with speech bubbles containing incomplete sentences: "Remember that we were listening to yesterday?" and "You mean 'Easy on Me' or 1 Gotta Feeling?". A yellow box labeled "Filled in by the annotator" points to one of these sentences, indicating where an annotator would complete it. Below the illustrations, there's text explaining each part: "Sets the dialog context (chosen from a few manual prompts per domain)," "The alternative question," and "Expression referring to one of the entities." At the bottom left corner, there is additional text about reading and referring expressions for entity selection.</sample>
    <sample id="1207">The image contains a slide from a presentation titled "Dataset Collection Methodology." The main content of the slide explains that methodology emphasizes informality using a cartoon completion task. There are four panels showing characters in conversation, with text bubbles containing incomplete sentences like "Remember that you were listening to yesterday?" and "Do you mean 'Easy on Me' or 1 Gotta Feeling?" A yellow arrow points to one of these speech bubbles labeled "Filled in by the annotator," indicating where human input is required.

Below the panels, there's additional explanatory text: 
- "Sets the dialog context (chosen from a few manual prompts per domain)"
- "The alternative question"
- "Expression referring to one of the entities"

In the bottom right corner, there's an inset photo of a person who appears to be presenting this information. In the background, at the top left, it reads "Google Research" along with some partially obscured text below which seems to say "Resolving Indirect Referring Expressions for Entity Selection (Mentat Corpus)."</sample>
    <sample id="1208">The image shows a slide from a presentation by Google Research titled "Dataset Collection Methodology." The main point emphasized is the informality of methodology, illustrated through a cartoon completion task. There are three panels depicting characters in conversation: 1) Setting up the dialogue context with manual prompts per domain; 2) Presenting an alternative question to be answered; and 3) Referring to entities within one of these questions. A yellow box highlights that this information is filled in by the annotator. In the bottom right corner, there's a small circular photo of a person wearing glasses, likely the presenter or author of the content.</sample>
    <sample id="1209">The image shows a presentation slide titled "Dataset Collection Methodology" from Google Research. It explains that the methodology emphasizes informality using a cartoon completion task, illustrated with four characters in conversation bubbles about remembering listening to yesterday's podcast and asking if someone means 'Easy on Me' or '1 Gotta Feeling'. There is also text explaining how this sets up dialogue context chosen from manual prompts per domain, presents an alternative question, and includes expressions referring to entities filled in by annotators. The bottom of the slide mentions "Resolving Indirect Referring Expressions for Entity Selection (Mandatory Completions)".</sample>
    <sample id="1210">The image shows a slide from Google Research titled "Dataset Collection Methodology." It emphasizes the informality of their methodology using a cartoon completion task. The process involves setting up dialogue context with manual prompts, presenting alternative questions to choose from, and having expressions refer to entities within the domain. There's also an annotation indicating that certain parts are filled in by the annotator.</sample>
    <sample id="1211">The image contains a slide from a presentation with the following text: "Generate alternative questions =&gt; sampling entity pairs" and "Do you mean A or B?" Below this, there are several bullet points listing different types of items:

- Items with similar infoboxes on Wikipedia (same genre and/or artist)
  - Do you mean This is it? or Man in the Mirror?
- Items with similar descriptions on Wikipedia
  - Do you mean Thinking of You or Happy Anywhere?
- Items with similar titles:
  - Do you mean The Return (memoir) or The Return (Shatner novel)?
- Uniform at random:
  - Do you mean Could You Be Mine or The Way I Am?

There is also an arrow pointing upwards labeled "More Similar (likely harder)" and some additional text at the bottom that reads "Researching Indirect Expressions for Entity Selection Adaptability Correlation." In the top right corner, there is a logo with the text "Google Research."</sample>
    <sample id="1212">The image shows a slide from a presentation titled "Generate alternative questions =&gt; sampling entity pairs." At the top, there is a question in bold: "Do you mean A or B?" Below this are four bullet points listing different methods for generating similar entities:

1. Items with similar infoboxes on Wikipedia (same genre and/or artist)
2. Do you mean This is it or Man in the Mirror?
3. Items with similar descriptions on Wikipedia
4. Do you mean Thinking of You or Happy Anywhere?

5. Items with similar titles:
   - Do you mean The Return (memoir) or The Return (Shatner novel)

6. Uniform at random:
   - Do you mean Could You Be Mine or The Way I Am

On the left side of the slide, there's an arrow pointing upwards labeled "More Similar (likely harder)." In the bottom right corner, there's a small circular photo of a person wearing glasses.

At the very bottom of the slide, there is additional text that reads: "Revisiting Inductive Expressions for Entity Selection Variability Consistency."

In the upper right corner, there is a Google Research logo.</sample>
    <sample id="1213">The text in the image is as follows:

Title: Generate alternative questions =&gt; sampling entity pairs

Subtitle: Do you mean A or B?

Body Text:
- Items with similar infoboxes on Wikipedia (same genre and/or artist)
  - Do you mean This is it or Man in the Mirror?
- Items with similar descriptions on Wikipedia
  - Do you mean Thinking of You or Happy Anywhere?
- Items with similar titles:
  - Do you mean The Return (memoir) or The Return (Shatner novel)?
- Uniform at random:
  - Do you mean Could You Be Mine or The Way I Am

Footer: Reading Inducing Expressions for Entity Selection Utilizing Corpora</sample>
    <sample id="1214">The text in the image is as follows:

Title: Generate alternative questions =&gt; sampling entity pairs

Subtitle: Do you mean A or B?

Body Text:
- Items with similar infoboxes on Wikipedia (same genre and/or artist)
  - Do you mean This Is It or Man in the Mirror?
- Items with similar descriptions on Wikipedia
  - Do you mean Thinking of You or Happy Anywhere?
- Items with similar titles:
  - Do you mean The Return (memoir) or The Return (Shatner novel)?
- Uniform at random:
  - Do you mean Could You Be Mine or The Way I Am

Footer: Researching Inductive Expressions for Entity Selection Utility Correlations</sample>
    <sample id="1215">The image shows a slide from a presentation with the title "Generate alternative questions =&gt; sampling entity pairs" at the top. Below this, there is a question in bold: "Do you mean A or B?" The slide lists several bullet points suggesting different methods for generating these alternative questions:

1. Items with similar infoboxes on Wikipedia (same genre and/or artist)
2. Do you mean This is it or Man in the Mirror?
3. Items with similar descriptions on Wikipedia
4. Do you mean Thinking of You or Happy Anywhere?
5. Items with similar titles:
   - Do you mean The Return (memoir) or The Return (Shatner novel)?
6. Uniform at random:
   - Do you mean You Could Be Mine or The Way I Am

On the left side of the slide, there is an arrow pointing upwards labeled "More Similar (likely harder)." At the bottom right corner, there is a small circular photo of a person wearing glasses. In the lower-left corner, there is text that reads "Researching Inductive Expressions for Entity Selection Variability Correlations."</sample>
    <sample id="1216">The image shows a presentation slide titled "Background knowledge (Music)" from Google Research. The slide outlines that for each song, there is a Google search link provided to find out more about the song. Two examples of songs are listed: "Easy on Me" by Adele and "I Gotta Feeling" by The Black Eyed Peas. There are clickable links next to each song title with text indicating they will take you to information about the respective songs.

Additionally, it states that annotators were asked to listen to at least some of each song and read about each song as part of their task. At the bottom left corner of the slide, there's a reference to "Revisiting Indirect Referencing Expressions for Entity Selection (Mistakes Corpus)." In the lower right corner, there is an inset showing a person wearing glasses and a dark shirt.</sample>
    <sample id="1217">The image shows a slide from a presentation titled "Background knowledge (Music)" with the Google Research logo in the top right corner. The main content of the slide includes:

- A bullet point stating: "Google search link to each song."
- Two examples of songs:
  - "Easy on Me" by Adele
  - "I Gotta Feeling" by The Black Eyed Peas
- Each song title is followed by a clickable link that says, "Click here to find out about the song."

Below these points, there is another section that reads:
- "We ask annotators to"
  - Listen to at least some of each song
  - Read about each song

At the bottom left corner of the slide, there is text that appears to be part of the footer or additional information related to the presentation.

In the bottom right corner of the image, there is a small circular inset showing a person's face.</sample>
    <sample id="1218">The image is a screenshot of a presentation slide titled "Background knowledge (Music)" from Google Research. The slide outlines the process for gathering background information on songs, specifically focusing on two examples: Adele's "Easy on Me" and The Black Eyed Peas' "I Gotta Feeling." It mentions that each song has a corresponding Google search link to find out more about it.

The slide also details how annotators are asked to engage with these songs:
- Listen to at least some of each song
- Read about each song

Additionally, there is an inset showing a YouTube page for Adele's "Easy on Me," including options like 'Listen,' 'Video,' 'Lyrics,' and other related content. At the bottom of the slide, there is text indicating that this is part of a larger study or project focused on indexing referring expressions for entity selection in a multilingual corpus.</sample>
    <sample id="1219">The image shows a slide from a presentation titled "Background knowledge (Recipes)." It features information about two types of cakes: Simnel Cake and Pandan Cake. The text on the left side describes Simnel Cake, which is described as a fruitcake widely eaten in the United Kingdom, Ireland, and other countries with patterns of migration from there, associated with Lent and Easter. It mentions that it's distinguished by layers of almond paste or marzipan and a set of eleven balls made of the same paste.

On the right side, the description for Pandan Cake states that it is light, fluffy, green-colored sponge cake flavored with the juices of Pandanus amabilis leaves. This cake is popular in Indonesia, Malaysia, and also in the Netherlands, especially among the Indo community.

Below the main content, there is additional text that reads: "Rebuilding Indicating Expressions for Entity Selection (Abilites Convolutional)." In the bottom right corner of the image, there is a small circular photo of an individual wearing glasses and a dark shirt against a white background.</sample>
    <sample id="1220">The image shows a presentation slide titled "Eliciting expressions" with the Google Research logo in the top right corner. The main content of the slide explains how to instruct annotators to select and describe their choice for songs, specifically mentioning two song options: "Easy on Me (by Adele)" and "I Gotta Feeling (by the Black Eyed Peas)." There is also an instruction that annotators should give 3-5 expressions related to the chosen song.

Below this explanation are examples of potential speech bubble responses:
1. "The one with the piano music"
2. "The song's not energetic"
3. "It has something about a river"
4. "The newer one"
5. "It's about having time to choose"

At the bottom left of the slide, there is additional text indicating that the topic relates to "Reasoning Indirect Referencing Expressions for Entity Selection (Mutiutility Corpus)." In the lower-right corner of the image, there is a small circular photo of a person wearing glasses.</sample>
    <sample id="1221">The image shows a presentation slide titled "Eliciting expressions" with the Google Research logo in the top right corner. The main content of the slide explains that annotators are told to select a choice and describe it, providing examples like 'Easy on Me (by Adele)' or 'I Gotta Feeling (by the Black Eyed Peas)'. It also mentions giving three to five expressions for chosen songs to fill in speech bubbles.

Below this section is another part labeled "Random Examples," divided into three columns: Music Selection, Book Selection, and Recipe Selection. Each column lists various prompts related to different themes:

- **Music Selection**: Questions about music lyrics or characteristics.
- **Book Selection**: Questions about book titles, authors, settings, characters, etc.
- **Recipe Selection**: Questions about ingredients, cooking methods, cultural origins, etc.

At the bottom left of the slide, there's text indicating the source as "Revisiting Indirect Referencing Expressions for Entity Selection (Abdullah et al.)". In the bottom right corner, there is an inset photo of a person who appears to be presenting the information.</sample>
    <sample id="1222">The image contains a slide from a presentation titled "AltEntities Corpus." The content of the slide is as follows:

- **Title:** AltEntities Corpus
- **Bullet Points:**
  - ~6,000 alternative questions across the three domains
  - ~42,000 indirect referring expressions
  - Results with T5 XL model (accuracy):
    - 92-95% if the LM has access to the same background knowledge as annotators.
    - 82-87% when the LM has access to partially overlapping background knowledge.
    - ~60% when the LM (T5 XL) has only access to the entity names.
  - We showed models are domain-generalizable.

- **Data Set Link:** https://github.com/google-research/datasets/AltEntities

At the bottom left corner, there's text that reads:
"Revisiting Indirect Referring Expressions for Entity Selection in AltEntities Corpus."

In the top right corner, there is a Google Research logo. Additionally, on the lower right side of the slide, there is an inset photo of a person wearing glasses and smiling.</sample>
    <sample id="1223">The image contains a slide from a presentation titled "AltEntities Corpus" with the Google Research logo in the top right corner. The content of the slide is as follows:

- **Title**: AltEntities Corpus
- **Bullet Points**:
  - ~6,000 alternative questions across three domains
  - ~42,000 indirect referring expressions
  - Results with T5 XL model (accuracy):
    - 92-95% if the LM has access to the same background knowledge as annotators.
    - 82-87% when the LM has access to partially overlapping background knowledge.
    - ~60% when the LM (T5 XL) has only access to the entity names.
  - We showed models are domain-generalizable.

- **Data Set Link**: https://github.com/google-research/datasets/AltEntities

At the bottom left, there's additional text: "Reading Comprehension for Entity Selection for AltEntities Corpus".

In the lower right corner, there is an image of a person wearing glasses and smiling.</sample>
    <sample id="1224">The image contains a slide from a presentation with the title "AltEntities Corpus" at the top. The content of the slide is as follows:

- ~6,000 alternative questions across the three domains
- ~42,000 indirect referring expressions

Results with T5 XL model (accuracy):
- 92-95% if the LM has access to the same background knowledge as annotators.
- 82-87% when the LM has access to partially overlapping background knowledge.

- ~60% when the LM (T5 XL) has only access to the entity names.
- We showed models are domain-generalizable.

Dataset Link: https://github.com/google-research/datasets/AltEntities

At the bottom left corner, there's text that reads:
"Reading Comprehension for Entity Selection AltEntities Corpus"

In the bottom right corner, there's an avatar or profile picture of a person wearing glasses and a dark shirt. 

The Google Research logo is visible in the upper right corner of the slide.</sample>
    <sample id="1225">The image contains a slide from a presentation titled "AltEntities Corpus." The content of the slide is as follows:

- It mentions that there are approximately 6,000 alternative questions across three domains.
- There are about 42,000 indirect referring expressions.

The results with T5 XL model (accuracy) include:
- 92-95% if the LM has access to the same background knowledge as annotators.
- 82-87% when the LM has access to partially overlapping background knowledge.
- Approximately 60% when the LM (T5 XL) has only access to the entity names.

It also states: 
- We showed models are domain-generalizable.

At the bottom, there is a dataset link provided for more information: https://github.com/google-research/datasets/AltEntities

The Google Research logo is present in the top right corner. Additionally, at the end of the slide, there is a thank you message and an email address for further inquiries: javadh@google.com</sample>
    <sample id="1226">CamemBERT is initially trained on 4GB of data.</sample>
    <sample id="1227">Adam Prze/piorkowski and Michæt Wozi/iak</sample>
    <sample id="1228">The performance degrades with larger temporal gap, which is the main cause for the performance drop.</sample>
    <sample id="1229">NLPPositionality: Characterizing Design Biases of Datasets and Models Sebastian Sany* Carnegie Mellon University Jenny T. Liang Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarten Sap Carnegie Mellon University</sample>
    <sample id="1230">NLPPositionality: Characterizing Design Biases of Datasets and Models</sample>
    <sample id="1231">Imagine...</sample>
    <sample id="1232">Imagine... Carl Jones Tech Lead, New York Times Can you stop being a jerk? 😐 (0.82) ✅ = PerspectiveAPI score</sample>
    <sample id="1233">Imagine... Carl Jones Tech Lead, New York Times Can you stop being a jerk? 😡 (0.82) ✅ Aditya Sharma Tech Lead, Times of India Presstitutes everywhere on the news. 😡 (0.33) ❌ = PerspectiveAPI score</sample>
    <sample id="1234">Imagine... Design bias example Can you stop being a jerk? : 0.82 ✅ Presstitutes everywhere on the news.: 0.33 ❌ Carl Jones Tech Lead, New York Times Aditya Sharma Tech Lead, Times of India = PerspectiveAPI score</sample>
    <sample id="1235">The image shows a slide titled "Positionality" with the following text: "The perspectives [people] hold as a result of their demographics, identity, and life experiences." At the bottom of the slide, there is a citation that reads: "[1] Savin-Baden, Maggi, and Claire Howell-Major. 'Qualitative research: The essential guide to theory and practice.' Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013)." In the top right corner, there is an inset showing a person in front of bookshelves.</sample>
    <sample id="1236">The image contains a slide with the title "Positionality" at the top. Below the title, there is a quote: “The perspectives [people] hold as a result of their demographics, identity, and life experiences.” At the bottom of the slide, there are two references listed:

1. Savin-Baden, Maggi, and Claire Howell-Major. *Qualitative research: The essential guide to theory and practice.* 
2. Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013).

In the upper right corner of the slide, there is an inset photo showing part of a room with shelves containing books or other items.</sample>
    <sample id="1237">Positionality "The perspectives [people] hold as a result of their demographics, identity, and life experiences. [As a researcher,] it influences the research process and its outcomes and results." [1] [1] Savin-Baden, Maggi, and Claire Howell-Major. "Qualitative research: The essential guide to theory and practice." Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013).</sample>
    <sample id="1238">Do datasets and models have positionality? [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="1239">The image contains a title and three references. The title reads: "Do datasets and models have positionality?" Below the title, there are three references listed:

1. [1] Blasi, et al., “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022.
2. [2] Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models.” EMNLP 2022.
3. [3] Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.

In the top right corner of the image, there is a small video feed showing a person sitting at a desk with books behind them.</sample>
    <sample id="1240">The video begins with a slide titled "Do datasets and models have positionality?" followed by the subtitle "Anecdotal evidence:" The slide lists three references: 1. Blasi, et al., “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. 2. Yi et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. 3. Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.

The presentation continues with additional points under "Anecdotal evidence:" which include "- Model and dataset probing [1][2]" and "- Theoretical definitions of model positionality [3]." The same list of references is repeated at the bottom of this slide as well.

The final part of the video maintains the title "Do datasets and models have positionality?" along with the subtitle "Anecdotal evidence:" and adds another point to the bulleted list under "Anecdotal evidence:" It reads "- Theoretical definitions of model positionality [3]." The background remains white throughout all slides, ensuring clear visibility of the text.</sample>
    <sample id="1241">Title: Do datasets and models have positionality?

Subtitle: Anecdotal evidence:

- Model and dataset probing [1][2]
- Theoretical definitions of model positionality [3]

References:
[1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022.
[2] Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022.
[3] Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="1242">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="1243">The image contains text that reads: "Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. 'Systematic Inequalities in Language Technology Performance across the World's Languages.' ACL 2022. [2] Ye et al. 'GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.' EMNLP 2022. [3] Cambo &amp; Gergle. 'Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.' CHI 2022."</sample>
    <sample id="1244">The video begins with a white background displaying the text "Question: Do datasets and models have positionality?" in black font. In the top right corner, there is an inset image of a person sitting at a desk with books on shelves behind them. A gray arrow pointing downward appears below the question, followed by additional text that reads "Goal: Compare annotations from users with existing datasets and models." The words "users" are underlined for emphasis.

The scene continues to display the same white background with the text "Question: Do datasets and models have positionality?" prominently shown in black font. Below this question, the gray arrow remains centered on the screen. Underneath the arrow, the goal statement reappears as "Goal: Compare annotations from users with existing datasets and models," with the word "users" again underlined for emphasis. The inset image of the person at the desk with bookshelves still occupies the top right corner throughout these frames.</sample>
    <sample id="1245">NLPPositionality A framework for characterizing design biases in NLP datasets and models</sample>
    <sample id="1246">Framework Collection 300 instances are sampled from a dataset Each instance has an associated gold label "Eating with Hands" Good Bad "Can you live without its moral?" "Eating with Hands" Bad Good Annotations from people around the world Processing Model Predictions The received annotations from the model are compared to the ground truth labels obtained from the models Analysis Age Gender Ethnicity Education Country</sample>
    <sample id="1247">The image contains a structured diagram titled "Framework" with various sections and text elements. Here is the transcription of the English content in reading order:

1) Re-annotate datasets with diverse annotators.

Collection:
- 300 instances are sampled from a dataset.
- Each instance has an associated gold label.
- Instances are sent as part of a study on ITW (likely referring to Image Text Workshop or similar).
- Annotations from people around the world.

Processing:
- Model predictions
- The received annotations contain the gold labels from the annotations obtained from the models described above.

Analysis:
- Age: Gender, White, Black, Hispanic, Asian
- Ethnicity: Male, Female
- Education: High School, College, Graduate
- Country: USA, Canada, UK, Australia

Pierre's correlation is measured for each annotation against the ground truth for each category separately.

The layout includes visual representations such as images of documents, charts, and flow diagrams that illustrate the process steps mentioned in the textual descriptions.</sample>
    <sample id="1248">The image contains a framework for data collection, processing, and analysis. The text in the image is as follows:

**Title:**
- Framework

**Collection Section:**
1. 300 instances are sampled from a dataset.
2. Each instance has an associated gold label.

**Processing Section:**
- Instances are sent to a study on IT.
- Annotations from people around the world
- Model predictions

**Analysis Section:**
- Age
- Gender (Male, Female)
- Ethnicity (White, Black, Hispanic, Asian)
- Education (High School, College, Graduate)
- Country (USA, Canada, UK, Australia)

**Additional Information:**
- The received annotations were converted into gold labels from the obtained models.
- Pearson's correlation is measured between annotations and model predictions for each category separately.

**Note:**
- Re-annotate datasets with diverse annotators.

This structured layout outlines the steps involved in collecting, processing, and analyzing data using different demographic categories and emphasizes the importance of having diverse annotators for re-annotation tasks.</sample>
    <sample id="1249">Framework 1) Re-annotate datasets with diverse annotators.</sample>
    <sample id="1250">Framework Collection "Eating with Hands" "Can you lie to God?" "Eating with Hands" Assumptions from people around the world 100 instances are sampled from a dataset. Each instance has an associated gold label. The received annotations are sent for comparison to models and datasets via Pearson's R scores. Model Predictions Demographics Age Gender Ethnicity Education Country 18-24 White Black Asian 25-34 White Black Asian 35-45 White Black Asian 46+ White Black Asian Pearson's correlation is computed between model predictions and ground truth labels for each demographic category.</sample>
    <sample id="1251">The image shows a detailed framework for data processing and analysis. At the top, there are three stages labeled "Collection," "Processing," and "Analysis." Under "Collection," it states that 100 instances are sampled from a dataset, each instance has an associated gold label, and annotations from people around the world.

In the "Processing" stage, model predictions are made on these annotated samples. There is also mention of receiving annotations sent as part of a study on UTTW (University of Texas at Tyler West).

The final "Analysis" section includes demographic information such as Age, Gender, Ethnicity, Education Level, and Country. It mentions Pearson's correlation to compare annotations by demographic to models and datasets via R scores.

On the right side of the image, text explains: "2) Compare annotations by demographic to models and datasets via Pearson’s R scores."

Overall, this slide outlines a systematic approach to collecting, processing, and analyzing data with a focus on comparing different aspects through statistical measures like Pearson's correlation coefficient.</sample>
    <sample id="1252">LabintheWild Our Experiences Participant Sets &amp; Case Studies For Researchers About Us English Pool of diverse volunteers / research participants Online experiment from researchers Could live with an AI and its morals? Are you better than an AI in noticing hateful speech? Where are you on the techno-skeptic to techno-utopian scale? We will show you how we test your moral judgement under certain situations and will ask if you agree or disagree. We will also test your ability to detect hateful speech compared to an AI, and at last, where do you place yourself on a scale between techno-skepticism (distrust) and techno-utopianism (trust). Will technology affect your personal life? We will examine whether technological development is seen positively or negatively by people like you. Pool of diverse volunteers / research participants Participate now Participate now Participate now</sample>
    <sample id="1253">The image contains a screenshot of the "LabintheWild" website. The main content includes:

1. A header with navigation links: Our Departments, Perspectives &amp; Blogs, Site Map, For Researchers.
2. An English language selection option on the right side.
3. A central section featuring:
   - A logo and title "LABINTHEWILD".
   - Two cartoon characters representing AI or robots.
   - Text discussing the ability to distinguish between certain moral concepts by AI and comparing it to human perception.
4. On the left side, there is text asking if you could live with an AI that makes decisions based on your personal preferences in various situations.
5. In the middle-right area, there are two boxes containing information about volunteers and research participants:
   - One box shows the number 5,376,396 total participants.
   - Another box discusses where people stand on the techno-skeptic to techno-utopian scale regarding technology's impact on their lives.

The overall layout suggests this is part of a study involving diverse volunteers participating in online experiments related to AI ethics and technological impacts.</sample>
    <sample id="1254">The image shows a slide titled 'Task A: Social Acceptability.' It outlines three steps for participants to rate the social acceptability of various situations. The first step is labeled '1) Read the situation,' with an example given as "Wanting to make a lot of money." Participants are asked to provide their judgment on this statement, which includes options like 'Very bad' and 'Very good.'

The second step is labeled '2) Enter what you think about it,' where participants can type in their thoughts or explanations regarding the provided statements.

The third step is labeled '3) See what an AI and others thought about it,' featuring an illustration of a robot that speculates whether something is understandable by most people. There's also a bar chart showing participant ratings from different countries (US, Brazil, China, Germany), indicating how socially acceptable they find certain behaviors based on the data from the United Chemistry dataset.

Additionally, there's text explaining that participants read a situation from the United Chemistry dataset and rate its social acceptability.</sample>
    <sample id="1255">Task A: Social Acceptability

1) Read the situation.
2) Enter what you think about it
3) See what an AI and others thought about it.

Participants compare their responses to others' and AI's.</sample>
    <sample id="1256">Task A: Social Acceptability Analysis Datasets - Social Chemistry Models - Delphi - GPT-4</sample>
    <sample id="1257">Task B: Toxicity Participants read an instance from the Dynathe dataset. Participants rate whether they think an instance is hate speech.</sample>
    <sample id="1258">Task B: Toxicity Analysis Datasets - Dynahate Models - Perspective API - Rewire API - Hate RoBERTa - GPT-4 Study Participation 16,299 annotations 1,096 annotators 87 countries</sample>
    <sample id="1259">Results Who do NLP datasets and models align with? Finding 1: There is positionality in NLP.</sample>
    <sample id="1260">The image contains a bar chart titled "Social Acceptability (GPT-4)" with the subtitle "Datasets and models are most aligned to English-Speaking countries." The chart compares social acceptability across different regions, including African Islamic, Baltic, Catholic Europe, Confucian, English-Speaking, Latin America, Orthodox Europe, Protestant Europe, and West South Asia. Each region is represented by a grey bar indicating its respective value on the y-axis, which ranges from 0.36 to 0.72.

In addition to this main section of the chart, there is another smaller bar labeled "Hate Speech &amp; Toxicity (Dynahate)" at the top right corner, showing values for various groups such as African Islamic, Baltic, Catholic Europe, Confucian, English-Speaking, Latin America, Orthodox Europe, Protestant Europe, and West South Asia. This additional data point indicates that hate speech and toxicity levels vary among these groups but does not provide specific numerical values in the visible portion of the image.

The overall message conveyed by the image emphasizes the alignment of datasets and models to English-speaking countries within the context of both social acceptability and hate speech/toxicity metrics.</sample>
    <sample id="1261">The image shows a bar chart titled "Social Acceptability (GPT-4)" with the subtitle "Datasets and models are most aligned to people with a college education." The x-axis lists different educational levels: College, Graduate School, High School, PhD, Pre-High School, and Prof. School. Each level has corresponding bars indicating social acceptability scores for N=4,489 at College, 1,116 at Graduate School, 2,183 at High School, 709 at PhD, 406 at Pre-High School, and 460 at Prof. School. There is also an asterisk (*) next to some values on the bars. In the top right corner of the image, there is a small inset showing a person in what appears to be a classroom or office setting.</sample>
    <sample id="1262">Hate Speech &amp; Toxicity (Dynahate) Datasets and models are most aligned to people with a college education. College N=2,383 0.66* Graduate School N=604 0.59* High School N=908 0.59* PhD N=359 0.48* Pre-High School N=116 0.37 Prof. School N=195 0.61*</sample>
    <sample id="1263">Finding 2: Some populations are left behind.</sample>
    <sample id="1264">The image shows a bar chart titled "Social Acceptability (GPT-4)" with three bars representing different gender categories: Man, Non-binary, and Woman. The y-axis is labeled from 0 to 0.8 in increments of 0.2, indicating the social acceptability score.

- For Men, the bar reaches up to approximately 0.69.
- For Non-binary individuals, the bar reaches slightly above 0.55.
- For Women, the bar reaches around 0.73.

Each category has an associated sample size:
- Men have N=4,082
- Non-binary have N=858
- Women have N=4,368

Additionally, there is text on the left side stating, "Datasets and models are less aligned to non-binary people."

In the top right corner, there is a small inset showing a person sitting at a desk with various items including papers and electronic devices.

Below this section, another bar chart appears under the title "Hate Speech &amp; Toxicity (Dynahate)." This chart also compares the same three gender categories:

- For Men, the bar reaches up to approximately 0.65.
- For Non-binary individuals, the bar reaches just below 0.58.
- For Women, the bar reaches about 0.63.

Each category again has its own sample size:
- Men have N=1,798
- Non-binary have N=329
- Women have N=2,367

The accompanying text states, "Datasets and models are less aligned to non-binary people." Similar to the first chart, there is a small inset in the top right corner showing a person seated at a desk with various items like papers and electronic devices.</sample>
    <sample id="1265">So, what can we do? Addressing positionality in NLP</sample>
    <sample id="1266">The video begins with a slide titled "Recommendations" in bold black text on the left side of a white background. In the bottom right corner, there is an image of a person sitting at a desk with books and other items visible behind them. The first recommendation listed below the title reads: "1. Keep a record of all relevant design choices made throughout building datasets or models." A reference link "[1] https://www.masakhane.io" appears in gray text at the bottom left corner.

The presentation continues with the same slide layout. An additional recommendation is introduced beneath the first one: "2. Do NLP research through the lens of perspectivism:" followed by a sub-point labeled "a. Share disaggregated dataset labels!" 

The final part of the video maintains this format. No new recommendations are added; it simply reinforces the existing content.</sample>
    <sample id="1267">The image contains text under the heading "Recommendations" with three numbered points: 1. Keep a record of all relevant design choices made throughout building datasets or models. 2. Do NLP research through the lens of perspectivism: a. Share disaggregated dataset labels b. Use modeling techniques that can handle annotator disagreement. 3. Building specialized datasets and models with and for specific communities is valuable for inclusive NLP (e.g., Masakhane initiative¹). There's also a URL at the bottom left corner: [https://www.masakhane.io](https://www.masakhane.io)</sample>
    <sample id="1268">Thanks Dashboard Link: nlppositionality.cs.washington.edu/ Paper: bit.ly/NLPositionality-Paper/</sample>
    <sample id="1269">To generate all possible permutations of the tokens for the output sequence.</sample>
    <sample id="1270">The authors recommended that model owners should increase transparency about bias mitigation methods to ensure fairness and accuracy in the models. This recommendation is part of a broader set of guidelines aimed at addressing stereotypes, using an intersectional lens, and enhancing overall trust in AI systems by being open about how biases are addressed within them.</sample>
    <sample id="1271">Minimal-pair unacceptable inputs are sentences that contain a minimal pair, which is a pair of words with different phonetic features but the same meaning. In this context, they refer to examples used in language model evaluations where one word has been replaced by another that sounds similar but changes the meaning significantly enough to be considered an "unacceptable" input for evaluating the language model's understanding and generation capabilities.</sample>
    <sample id="1272">The authors used NER (Named Entity Recognition), CCR (Coreference Resolution), SRL (Semantic Role Labeling), and MISC (Miscellaneous) as evaluation metrics.</sample>
    <sample id="1273">Krippendorf's Alpha</sample>
    <sample id="1274">Wikipedia, Unrelated</sample>
    <sample id="1275">The authors of the paper are affiliated with Heinrich Heine University Düsseldorf, Germany.</sample>
    <sample id="1276">MultiInstruct is unique because it focuses on language-only tasks and uses instruction tuning with multimodal pre-trained models. It also highlights the imbalance in instructional datasets between NLP (Natural Language Processing) and Multimodal approaches, emphasizing that MultiInstruct includes over 1600+ language-only instruction tasks but lacks large-scale, publicly-available multimodal instruction tasks.</sample>
    <sample id="1277">The paper involves three authors: Sarah E. Finch, James D. Finch, and Jinho D. Choi.</sample>
    <sample id="1278">Binary coordination refers to the arrangement of two elements or entities in a specific order, typically involving a choice between two options.</sample>
    <sample id="1279">The average prompt length was about 12 words.</sample>
    <sample id="1280">The smaller T5 model trained on Coscript outperforms the larger LLMs, indicating that fine-tuning can enhance performance.</sample>
    <sample id="1309">The learning strategies investigated in the work are "From scratch with full model construction" and "Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model, and PubMedBERT, an English medical one)."</sample>
    <sample id="1310">The factor of overfitting due to test reuse is relatively small.</sample>
    <sample id="1311">The quality of the simplification was evaluated using metrics like BLEU and F1 scores.</sample>
    <sample id="1312">Yes, language models have different political biases.</sample>
    <sample id="1347">Cognitive dissonance is described as "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" according to Harmon-Jones and Harmon-Jones from 2007.</sample>
    <sample id="1348">BART-large is the most liberal language model according to the chart.</sample>
    <sample id="1349">Yes.</sample>
    <sample id="1350">Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="1351">The data was taken from TED Talks.</sample>
    <sample id="1352">Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination Adam Przepiórkowski and Michał Woźniak INSTITUTE OF COMPUTER SCIENCE POLISH ACADEMY OF SCIENCES ul. Jana Kazimierza 5, 01-248 Warsaw UNIVERSITY OF WARSAW ACL 2023</sample>
    <sample id="1353">The image shows a slide titled "Dependency Structure of Coordination" with the subtitle "(Universal Dependencies)." The content is divided into four sections, each representing different dependency structures: Bouquet/Stanford (Universal Dependencies), Chain/Moscow, Conjunction-headed/Prague, and Multi-headed/London. Each section includes an example sentence that reads, "Homer loves Lisa, Bart, and Maggie." There are diagrams above these sentences illustrating how dependencies are structured in each method. In the top right corner, there's a small video feed showing someone presenting this information.</sample>
    <sample id="1354">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1355">The image displays a slide from a presentation titled "Dependency Structure of Coordination." The content is organized into four sections, each representing different linguistic approaches to dependency structures. These sections are labeled as follows:

1. Bouquet/Stanford (Universal Dependencies)
2. Chain/Moscow
3. Conjunction-headed/Prague
4. Multi-headed/London

Each section includes an example sentence: "Homer loves Lisa, Bart, and Maggie," with the names 'Lisa,' 'Bart,' and 'Maggie' highlighted in blue within brackets. This indicates that these names are likely being used to illustrate specific grammatical relationships or dependencies according to each approach.

The background of the slide features a patterned design at the top right corner, adding visual interest to the otherwise text-heavy layout.</sample>
    <sample id="1356">The image shows a slide from a presentation titled "Dependency Structure of Coordination." It outlines different methods for analyzing the structure of coordination in sentences, specifically focusing on how to interpret dependencies within these structures. The slide is divided into four sections:

1. **Bouquet/Stanford (Universal Dependencies):**
   - Diagrams showing the dependency relationships between words in the sentence "Homer loves Lisa, Bart, and Maggie."

2. **Chain/Moscow:**
   - Similar diagrams illustrating the chain dependency approach.

3. **Conjunction-headed/Prague:**
   - Diagrams depicting the conjunction-headed dependency analysis method.

4. **Multi-headed/London:**
   - Diagrams representing the multi-headed dependency interpretation.

Each section includes visual representations of how each method breaks down the given sentence structurally.</sample>
    <sample id="1357">The image shows a slide from a presentation titled "Dependency Structure of Coordination." The content is organized into four sections, each describing different approaches to dependency structure in coordination: 1. Bouquet/Stanford (Universal Dependencies): This section includes an example sentence with dependencies marked above it: "Homer loves Lisa, Bart, and Maggie." 2. Chain/Moscow: Similar to the first section, this part also has an example sentence with dependencies indicated by lines connecting words: "Homer loves Lisa, Bart, and Maggie." 3. Conjunction-headed/Praque: Again, there's an example sentence showing how conjunctions head the coordinating structures: "Homer loves Lisa, Bart, and Maggie." 4. Multi-headed/London: This final section presents another way of structuring coordinate elements using multiple heads: "Homer loves Lisa, Bart, and Maggie." Each approach uses the same sentence as its basis for illustration, demonstrating various methods of representing syntactic relationships within coordinated structures.</sample>
    <sample id="1358">The image shows a slide from a presentation titled "Dependency Structure of Coordination." The slide is divided into four sections, each describing different coordination structures in dependency grammar. These sections are:

1. Bouquet/Stanford (Universal Dependencies)
2. Chain/Moscow
3. Conjunction-headed/Prague
4. Multi-headed/London

Each section includes an example sentence: "Homer loves Lisa, Bart, and Maggie."

In the top right corner of the slide, there is a small video feed showing a person presenting.

The background of the slide has a blue header with some text that appears to be related to the topic but is not fully legible due to resolution issues.</sample>
    <sample id="1359">The image shows a presentation slide titled "Dependency Structure of Coordination." It compares different dependency structures for coordinating clauses. The sections listed are:

1. Bouquet/Stanford (Universal Dependencies)
2. Chain/Moscow
3. Conjunction-headed/Prague
4. Multi-headed/London

Each section includes an example sentence: "Homer loves Lisa, Bart, and Maggie."

The background is white with blue headers, and there's a watermark in the top right corner that reads "© 2018 by the author or publisher."</sample>
    <sample id="1360">The image shows a slide from a presentation titled "Dependency Structure of Coordination." The slide is divided into four sections, each describing different dependency structures for coordinating phrases. These sections are labeled as follows:

1. Bouquet/Stanford (Universal Dependencies)
2. Chain/Moscow
3. Conjunction-headed/Prague
4. Multi-headed/London

Each section includes an example sentence: "Homer loves Lisa, Bart, and Maggie."

The background of the slide has a blue header with white text that reads "Dependency Structure of Coordination," and there is some additional text in red at the bottom right corner of the slide. In the top right corner of the image, there is a small video feed showing a person presenting.

The overall layout suggests this is part of an educational or academic presentation on linguistic dependencies.</sample>
    <sample id="1361">The image is a slide from a presentation about Dependency Length Minimization (DLM). The title of the slide reads "Dependency Length Minimization (DLM)" in bold white text on a blue background. Below the title, there is an explanation that states: "Word order tends to minimize dependency lengths:" This section includes two examples with diagrams and sentences.

The first example shows a sentence structure where Marge read it yesterday. There are arrows indicating dependencies between words, labeled as "good" in green at the bottom right corner of this part of the slide.

The second example also shows a similar sentence structure but with different word order leading to longer dependencies. It is labeled as "bad" in red at the bottom right corner of this part of the slide.

At the top right corner of the image, there is a small video feed showing a person's face. 

Overall, the slide visually explains how certain word orders can lead to shorter or longer dependencies when reading a sentence, using simple English sentences for illustration.</sample>
    <sample id="1362">The image shows a slide from a presentation about Dependency Length Minimization (DLM). The title of the slide is "Dependency Length Minimization (DLM)" and it explains that word order tends to minimize dependency lengths. There are two examples provided: one labeled as 'good' where Marge read it yesterday, with minimal dependencies shown by arrows between words; and another labeled as 'bad' where Marge read it yesterday, with more complex dependencies indicated by multiple arrows. Below these examples, there's a visual representation showing how different word orders affect dependency length.</sample>
    <sample id="1363">The image shows a presentation slide titled "Dependency Length Minimization (DLM)" with the subtitle "Word order tends to minimize dependency lengths:". The slide includes two examples of sentences and their corresponding dependency diagrams. 

1. The first example sentence is "Marge read it yesterday." 
   - It has a green label indicating "good".
   - The dependency diagram for this sentence shows Marge as the subject, reading as the verb, 'it' as the object, and 'yesterday' as an adverbial phrase.

2. The second example sentence is "Marge read yesterday it."
   - It has a red label indicating "bad".
   - The dependency diagram for this sentence shows Marge as the subject, reading as the verb, but 'yesterday' appears before 'it', which disrupts the intended meaning.

At the bottom of the slide, there are additional words listed: "Marge", "read", "this", "absolutely", "fascinating", "book", "about", "beer", "good", and "bad".

In the top right corner of the slide, there is a small video feed showing a person who seems to be presenting or explaining the content on the slide.</sample>
    <sample id="1364">The image shows a slide titled 'Dependency Length Minimization (DLM)' with the subtitle 'Word order tends to minimize dependency lengths:'. The slide illustrates two sentences, one labeled 'good' and the other 'bad', demonstrating different word orders that affect dependency length. Each sentence is accompanied by a syntactic tree diagram showing the relationships between words in terms of their dependencies.

The first sentence reads 'Marge read it yesterday.' It has a shorter dependency path from Marge to reading "it", which is considered good for DLM as indicated by the green color next to it. 

The second sentence also reads 'Marge read it yesterday.' but places "it" after "yesterday". This results in a longer dependency path, making it bad for DLM, highlighted in red.

Below these examples, there are more complex sentences involving additional words like 'this', 'absolutely', 'fascinating', 'book', and 'bees', each followed by similar syntactic trees indicating whether they follow the rule of minimizing dependency length or not. These sentences show how adding more elements can either maintain or increase dependency paths, affecting the overall structure's efficiency according to DLM principles.</sample>
    <sample id="1365">The image shows a slide from a presentation about Dependency Length Minimization (DLM). The title of the slide is "Dependency Length Minimization (DLM)" and it explains that word order tends to minimize dependency lengths. There are two example sentences shown: one labeled as 'good' with a simple sentence structure, and another labeled as 'bad' with more complex syntax. Both examples have syntactic trees illustrating the dependencies between words in each sentence. At the bottom right corner of the slide, there is an inset showing a person who appears to be presenting or discussing the content on the slide.</sample>
    <sample id="1366">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read it yesterday. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read this absolutely fascinating book about bees yesterday. good</sample>
    <sample id="1367">The image displays a slide titled "Dependency Length Minimization (DLM)" with the subtitle "Word order tends to minimize dependency lengths." The content is organized into two columns, each showing different word orders and their corresponding dependency structures. Each example sentence includes words like "Marge," "read," "it," "yesterday," along with other adjectives or nouns such as "this," "fascinating," "book," "bees," etc., illustrating how varying the sequence of these elements affects the length of dependencies in linguistic terms.

In both columns, there are sentences labeled either 'good' or 'bad,' indicating whether the particular word order minimizes dependency lengths effectively or not. For instance, one column shows examples where the structure results in shorter dependencies ("good"), while the other demonstrates longer dependencies due to less optimal word ordering ("bad"). There's also an additional visual element on the right side of the screen that appears to be part of a video conference interface, suggesting this might be from a lecture or presentation setting.</sample>
    <sample id="1368">The image is a slide from a presentation about Dependency Length Minimization (DLM). The title at the top of the slide reads "Dependency Length Minimization (DLM)." Below the title, there is text that says, "Word order tends to minimize dependency lengths:" followed by examples showing different word orders and their corresponding dependency structures. Each example has a sentence with words in various positions, accompanied by numbered arrows indicating dependencies between them.

The first example shows the sentence "Marge read it yesterday." This version is labeled as "good" because it minimizes the dependency length. 

The second example also shows the same sentence but rearranged slightly: "Marge read yesterday it." This version is labeled as "bad" because it does not minimize the dependency length effectively.

The third example demonstrates an even more complex structure where additional words are added around the original phrase: "Marge read this absolutely fascinating book about bees yesterday good." This version is again labeled as "good," suggesting that adding certain elements can help maintain or improve the dependency length minimization.

The fourth example repeats the previous one for emphasis: "Marge read this absolutely fascinating book about bees yesterday good."

In summary, the slide illustrates how the arrangement of words affects the dependency length in sentences, using the example of Marge reading something interesting about bees on a specific day.</sample>
    <sample id="1369">The image shows a slide from a presentation about Dependency Length Minimization (DLM). The title at the top reads "Dependency Length Minimization (DLM)." Below the title, there is text that states: "Word order tends to minimize dependency lengths:" This statement is followed by four examples of sentences and their corresponding dependency trees. Each example illustrates how word order affects the length of dependencies in a sentence.

1. The first example has two dependency trees for the sentence "Marge read it yesterday." Both trees show minimal dependency length with only one arrow each.
2. The second example also presents two similar dependency trees but labeled as "bad," indicating longer dependencies compared to the previous example.
3. The third example includes more words than the previous ones ("Marge read this absolutely fascinating book about bees yesterday"), showing increased complexity due to additional elements.
4. The fourth example repeats the same structure as the third example but is again marked as "good," suggesting an optimal or favorable dependency length despite its complexity.

In all examples, some words are highlighted in red while others remain black, possibly to emphasize certain parts of the sentence structures being analyzed. There is no visible movement or change between these frames; they appear to be static images used to illustrate points within the DLM concept.</sample>
    <sample id="1370">The image shows a slide from a presentation about Dependency Length Minimization (DLM). The title "Dependency Length Minimization (DLM)" is prominently displayed at the top. Below the title, there's an explanation that reads: "Word order tends to minimize dependency lengths:" This statement sets up the context for understanding how sentence structure can affect dependency length in linguistic analysis.

The main content of the slide consists of several example sentences with their corresponding syntactic trees and labels indicating whether they are considered good or bad examples based on DLM principles:

1. **Good Example**:
   - Sentence: "Marge read it yesterday."
   - Syntactic Tree: Shows a simple tree where 'read' directly connects to 'it'.
   - Label: "good"

2. **Bad Example**:
   - Sentence: "Marge read this absolutely fascinating book about bees yesterday."
   - Syntactic Tree: Displays a more complex tree with multiple levels of branching.
   - Label: "bad"

3. **Another Good Example**:
   - Sentence: "Marge read this absolutely fascinating book about bees yesterday."
   - Syntactic Tree: Similar complexity as the previous one but structured differently.
   - Label: "good"

4. **Another Bad Example**:
   - Sentence: "Marge read this absolutely fascinating book about bees yesterday."
   - Syntactic Tree: Again similar complexity.
   - Label: "bad"

Each example illustrates different ways words are ordered within a sentence, demonstrating how these orders impact the depth of dependencies between them according to DLM theory.</sample>
    <sample id="1371">The image is a screenshot of a presentation slide titled "Dependency Length Minimization (DLM)." The slide explains that word order tends to minimize dependency lengths, using examples with sentences and their corresponding dependency trees. There are two example sentences: one labeled as 'good' where the sentence structure minimizes dependencies, and another labeled as 'bad' where it does not. Both sentences involve Marge reading something yesterday.

The first sentence has a simple dependency tree showing just two nodes connected by an arrow marked with the number 2. This indicates a shorter dependency length between words in this sentence.

The second sentence shows a more complex dependency tree with multiple connections and higher numbers indicating longer dependency paths within the sentence.

At the bottom of the slide, there's a URL starting with "http://www..." which seems incomplete or partially cut off due to the edge of the image being visible on both sides.</sample>
    <sample id="1372">The image shows a presentation slide titled "Conjunct Lengths in English." The main content of the slide is about statistics related to coordination extracted from an enhanced version of the Penn Treebank, referencing studies by Marcus et al. (1993), Ficler and Goldberg (2016). It lists several points: 1) Left conjuncts tend to be shorter than right conjuncts (observed before); 2) This tendency grows with length difference (briefly noticed in Gibson et al. 1996:88-90); 3) But only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed); 4) Not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="1373">The image shows a presentation slide titled "Conjunct Lengths in English." The content of the slide discusses statistics about coordination extracted from an enhanced version of the Penn Treebank, referencing works by Marcus et al. 1993 and Ficler and Goldberg 2016. It states that left conjuncts tend to be shorter (observed before), with this tendency growing with length difference as briefly noticed in Gibson et al. 1996:88-90. However, when only the governor is on the left or absent (e.g., I saw Bart and Lisa; Homer came and sneezed), it does not hold true if the right conjunct has a similar structure (e.g., not when it is on the right; Ted and Ned laughed).</sample>
    <sample id="1374">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): • left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88-90) • but only when the governor is on the left or absent (I saw Bart and Lisa: Homer came and sneezed), not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="1375">The image shows a slide titled "Conjunct Lengths in English" with text discussing statistics about coordination extracted from an enhanced version of the Penn Treebank. The references cited are Marcus et al., 1993, and Ficler and Goldberg, 2016. Key points include: - Left conjuncts tend to be shorter (observed before). - This tendency grows with length difference (briefly noticed in Gibson et al., 1996; pages 88-90). Examples provided illustrate how conjunction lengths vary depending on context: - When only when the governor is on the left or absent ("I saw Bart and Lisa; Homer came and sneezed"). - When not when it is on the right ("Ted and Ned laughed").</sample>
    <sample id="1376">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): • left conjuncts tend to be shorter (observed before), e.g., • this tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88–90) • but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), • not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="1377">The image shows a slide from a presentation titled "Conjunct Lengths in English." The content of the slide provides statistics about coordination extracted from an enhanced version of the Penn Treebank, referencing work by Marcus et al. (1993), Ficler and Goldberg (2016). It states that left conjuncts tend to be shorter than their counterparts on the right side of a sentence ("observed before"), this tendency grows with length difference as briefly noticed in Gibson et al. (1996: 88-90), but only when the governor is on the left or absent (with examples like "I saw Bart and Lisa; Homer came and sneezed"). Additionally, it mentions that not all conjunctions are equal when it comes to being on the right side of a sentence ("not when it's on the right" - example given: "Ted and Ned laughed").</sample>
    <sample id="1378">The image shows a slide from a presentation titled "Conjunct Lengths in English." The content of the slide discusses statistics about coordination extracted from an enhanced version of the Penn Treebank by Marcus et al. (1993), Ficler, and Goldberg (2016). It highlights that left conjuncts tend to be shorter than right conjuncts before observing them, this tendency grows with length difference briefly noticed in Gibson et al. (1996: 88-90), but only when the governor is on the left or absent as seen in examples like I saw Bart and Lisa; Homer came and sneezed. Additionally, it mentions not when it's on the right (Ted and Ned laughed) with some text highlighted in green for emphasis.</sample>
    <sample id="1379">The image contains a slide titled "Conjunct Lengths in English" with the following text:

Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016):
- left conjuncts tend to be shorter (observed before),
- this tendency grows with length difference
(briefly noticed in Gibson et al. 1996:88–90),
- but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed),
- not when it is on the right (Ted and Ned laughed).

There is also a small inset picture of a person in the top right corner of the slide.</sample>
    <sample id="1380">The image contains a detailed chart with multiple graphs, each representing different conditions related to the length of characters and syllables. The title at the bottom reads: 'Figure 1: Proportions of shorter leftmost conlangs depending on the absolute difference of conlang length (with confidence bands).' Each graph plots data points against an axis labeled 'Absolute difference in conlang length.' The y-axis is not clearly visible for all graphs but appears to represent some form of proportion or percentage.

In the top right corner, there's a small inset showing a person wearing glasses and speaking into a microphone. Below this inset, text identifies the speaker as "Dr. Michael W. Diercks" from the University of Pennsylvania.

Each graph has labels indicating specific conditions such as 'NO governor (length in CHARACTERS),' 'NO governor (length in SYLLABLES),' etc., suggesting variations in how conlang lengths are measured or compared under these conditions. Some graphs show blue lines connecting the plotted points, while others have no line connections, possibly indicating different types of relationships between the variables being studied.</sample>
    <sample id="1381">The image contains a grid of nine line graphs, each with the title "No governor (length in [x])" where x can be CHARACTERS, SYLLABLES, or WORDS. The y-axis is labeled "Proportion of shorter left contacts," and the x-axis shows an increasing scale from 0 to 25. Each graph plots this proportion against different values along its respective axis.

In the bottom right corner of the image, there's a caption that reads: "Figure 1: Proportions of shorter left contacts depending on the absolute difference of contact lengths (with confidence bands)." Additionally, there are two smaller images within the main one; one appears to show a person speaking into a microphone, while the other seems to depict some form of data visualization or chart.</sample>
    <sample id="1382">The image displays a series of nine line graphs arranged in a 3x3 grid, each representing different scenarios related to 'NO governor length (in CHARACTERS)' and variations with governors on the left or right. The x-axis across all graphs is labeled 'absolute difference in characters,' while the y-axis varies by graph title.

Each graph's title includes terms like 'NO governor length (in CHARACTERS),' 'NO governor length (in SYLLABLES),' and 'NO governor length (in WORDS).' Some titles also mention specific positions such as 'Governor on the LEFT (length in CHARACTERS),' 'Governor on the RIGHT (length in CHARACTERS),' etc., indicating comparisons between these configurations.

The lines within each graph are colored blue, showing trends that generally increase from left to right along the x-axis. Below each graph, there’s additional text providing context: "Figure 1: Proportions of shorter left/center/longer right contexts depending on the absolute difference of conjunct lengths (with confidence bands)." This suggests an analysis of how changes in character count affect certain linguistic patterns or structures based on their position relative to other elements.

In summary, this detailed visualization presents comparative analyses of various linguistic measures under differing conditions, likely aimed at understanding structural differences in language composition.</sample>
    <sample id="1383">The image shows a slide from a presentation titled "Compatibility with Dependency Structures of Coordination." It compares different dependency parsing models: Bouquet/Stanford, Chain/Moscow, Conjunction-headed/Praque, and Multi-headed/London. Each model is tested on the sentence "Homer loves Lisa, Bart, and Maggie," showing whether it correctly parses the coordination structure.

- The Bouquet/Stanford (Universal Dependencies) model results in NO compatibility.
- The Chain/Moscow model also results in NO compatibility.
- The Conjunction-headed/Praque model results in YES compatibility.
- The Multi-headed/London model similarly results in YES compatibility.

The visual representation includes dependency trees for each model's interpretation of the sentence, illustrating how they handle the coordination between the verbs 'loves' and the list of names.</sample>
    <sample id="1384">The content in the frames is consistent, displaying a white background with black text that reads "See the paper for the full argument!" and below it, "Talk to us at the poster session!" Additionally, there is a small image of a person's face located in the top right corner.</sample>
    <sample id="1385">The name of the speaker is Matthias Lindemann.</sample>
    <sample id="1386">Cross-lingual transfer is a method where you train on one language and then apply the learned model to another language.</sample>
    <sample id="1387">The authors of the paper are affiliated with Saarland University, Amazon Alexa, and the University of Vienna.</sample>
    <sample id="1388">The authors use BLEU as the quality measure and AL/AL_CA as the latency measures.</sample>
    <sample id="1416">The drawbacks of tree-based methods include the need for pre/post-processing logical forms and grammar induction.</sample>
    <sample id="1417">Shuheng Liu and Alan Ritter are affiliated with the School of Interactive Computing at Georgia Institute of Technology.</sample>
    <sample id="1495">ABC-Eval stands for "Annotating Behaviors in Chat."</sample>
    <sample id="1496">2018</sample>
    <sample id="1527">The authors of the paper are affiliated with The Norwegian University of Science and Technology, Saarland University in Germany, UC San Diego (USA), NLP (Natural Language Processing) Center at NTNU, and the University of Amsterdam.</sample>
    <sample id="1528">The name of the speaker is Siyu Yuan.</sample>
    <sample id="1529">There are five authors involved in the paper. The names listed on the slide are Patrick Fernandes, Kayo Yin, Emmy Liu, André F.T. Martins, and Graham Neubig.</sample>
    <sample id="1530">The approach is compared to the wait-k architecture.</sample>
    <sample id="1531">MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning Zhiyang Xu*, Ying Shen*, Lifu Huang Department of Computer Science, Virginia Tech *Equal Contribution</sample>
    <sample id="1532">Pre-trained Language Models for Downstream Tasks</sample>
    <sample id="1533">Pre-trained Language Models for Downstream Tasks (A) Pretrain-finetune (BERT, T5) Finetune on task a Typically requires many task-specific examples for each task Inference on task a (B) Prompting (GPT-3) Improve performance via few-shot prompting Inference on task A (C) Instruction tuning (FLAN) Finetune on task a Instruction-tune on many tasks, e.g., D, Doc, Code Inference on unseen task B Figure 2: Comparing instruction tuning with pretrain-finetune and prompting. Image credit: Wei, Jason, et al. "Finetuned language models are zero-shot learners."</sample>
    <sample id="1534">The video features a consistent visual theme throughout, with the phrase "Language-only" prominently displayed in white text on a black background. This message is repeated across multiple frames, emphasizing its importance or relevance to the content being presented. In each frame, there is also a small inset image of an individual located at the bottom right corner. The person appears to be wearing glasses and a dark jacket over a light-colored shirt. Throughout the sequence, no changes occur within this setup; the text remains static as does the position of the individual's image.</sample>
    <sample id="1535">Instruction Tuning on Multimodal Pre-trained Models</sample>
    <sample id="1536">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="1537">The image is a slide from a presentation with the title "Imbalance in Instructional Datasets between NLP and Multimodal." It highlights that there are over 1600 language-only instruction tasks. The text also mentions, "NO large-scale, publicly-available multimodal instruction tasks," indicating a lack of such datasets. At the bottom, there is a citation: "Wang, Yizhong, et al. 'Benchmarking generalization via in-context instructions on 1600+ language tasks' arXiv preprint arXiv:2309.08547v1 [cs.CL]."</sample>
    <sample id="1538">MULTINSTRUCT The first multimodal instruction tuning benchmark dataset 62 diverse multimodal tasks 10 broad groups 5 expert-written instructions Figure 2: Task Groups Included in Multinstruct. The yellow boxes represent tasks used for evaluation, the white boxes indicate those included but not yet evaluated.</sample>
    <sample id="1539">MULTINSTRUCT The first multimodal instruction tuning benchmark dataset 62 diverse multimodal tasks 10 broad groups 5 expert-written instructions Figure 2: Task Groups Included in Multinstruct. The yellow boxes represent tasks used for evaluation, the white ones include training data.</sample>
    <sample id="1540">The image contains a slide with the title "OFA (One For All)" at the top. Below the title, there are two bullet points:

1. A unified multi-modal pre-trained model that is capable of performing both understanding and generation tasks with single or multiple modalities.
2. OFA has a unified vocabulary for language, image tokens, and the coordinates of a bounding box.

Below these bullet points, there is a diagram illustrating the structure of the OFA model. The diagram includes various components connected by arrows, indicating different processes within the model. At the bottom left corner of the slide, there is a citation: 

"Wang, Peng, et al. 'Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework'."

In the lower right corner of the image, there is a small picture of a person wearing glasses and smiling.</sample>
    <sample id="1541">MULTINSTRUCT</sample>
    <sample id="1542">MULTINSTRUCT</sample>
    <sample id="1543">MULTINSTRUCT Figure 1: Example Instances from MULTINSTRUCT for Four Tasks. Grounded Caption Input: Generate a caption for the image. Output: The person is playing tennis with white and yellow tennis racquet. Text Localization Input: Select the region that contains the text "don". Output: &lt;bin_194&gt; - 206, &lt;bin_195&gt; - 237, &lt;bin_196&gt; - 282, &lt;bin_197&gt; - 303, &lt;bin_198&gt; - 330, &lt;bin_199&gt; - 348, &lt;bin_200&gt; - 355, &lt;bin_201&gt; - 361, &lt;bin_202&gt; - 365, &lt;bin_203&gt; - 368, &lt;bin_204&gt; - 371, &lt;bin_205&gt; - 373, &lt;bin_206&gt; - 375, &lt;bin_207&gt; - 377, &lt;bin_208&gt; - 379, &lt;bin_209&gt; - 381, &lt;bin_210&gt; - 383, &lt;bin_211&gt; - 385, &lt;bin_212&gt; - 387, &lt;bin_213&gt; - 389, &lt;bin_214&gt; - 391, &lt;bin_215&gt; - 393, &lt;bin_216&gt; - 395, &lt;bin_217&gt; - 397, &lt;bin_218&gt; - 400, &lt;bin_219&gt; - 402, &lt;bin_220&gt; - 404, &lt;bin_221&gt; - 406, &lt;bin_222&gt; - 408, &lt;bin_223&gt; - 410, &lt;bin_224&gt; - 412, &lt;bin_225&gt; - 414, &lt;bin_226&gt; - 416, &lt;bin_227&gt; - 418, &lt;bin_228&gt; - 420, &lt;bin_229&gt; - 422, &lt;bin_230&gt; - 424, &lt;bin_231&gt; - 426, &lt;bin_232&gt; - 428, &lt;bin_233&gt; - 430, &lt;bin_234&gt; - 432, &lt;bin_235&gt; - 434, &lt;bin_236&gt; - 436, &lt;bin_237&gt; - 438, &lt;bin_238&gt; - 440, &lt;bin_239&gt; - 442, &lt;bin_240&gt; - 444, &lt;bin_241&gt; - 446, &lt;bin_242&gt; - 448, &lt;bin_243&gt; - 450, &lt;bin_244&gt; - 452, &lt;bin_245&gt; - 454, &lt;bin_246&gt; - 456, &lt;bin_247&gt; - 458, &lt;bin_248&gt; - 460, &lt;bin_249&gt; - 462, &lt;bin_250&gt; - 464, &lt;bin_251&gt; - 466, &lt;bin_252&gt; - 468, &lt;bin_253&gt; - 470, &lt;bin_254&gt; - 472, &lt;bin_255&gt; - 474, &lt;bin_256&gt; - 476, &lt;bin_257&gt; - 478, &lt;bin_258&gt; - 480, &lt;bin_259&gt; - 482, &lt;bin_260&gt; - 484, &lt;bin_261&gt; - 486, &lt;bin_262&gt; - 488, &lt;bin_263&gt; - 490, &lt;bin_264&gt; - 492, &lt;bin_265&gt; - 494, &lt;bin_266&gt; - 496, &lt;bin_267&gt; - 498, &lt;bin_268&gt; - 500, &lt;bin_269&gt; - 502, &lt;bin_270&gt; - 504, &lt;bin_271&gt; - 506, &lt;bin_272&gt; - 508, &lt;bin_273&gt; - 510, &lt;bin_274&gt; - 512, &lt;bin_275&gt; - 514, &lt;bin_276&gt; - 516, &lt;bin_277&gt; - 518, &lt;bin_278&gt; - 520, &lt;bin_279&gt; - 522, &lt;bin_280&gt; - 524, &lt;bin_281&gt; - 526, &lt;bin_282&gt; - 528, &lt;bin_283&gt; - 530, &lt;bin_284&gt; - 532, &lt;bin_285&gt; - 534, &lt;bin_286&gt; - 536, &lt;bin_287&gt; - 538, &lt;bin_288&gt; - 540, &lt;bin_289&gt; - 542, &lt;bin_290&gt; - 544, &lt;bin_291&gt; - 546, &lt;bin_292&gt; - 548, &lt;bin_293&gt; - 550, &lt;bin_294&gt; - 552, &lt;bin_295&gt; - 554, &lt;bin_296&gt; - 556, &lt;bin_297&gt; - 558, &lt;bin_298&gt; - 560, &lt;bin_299&gt; - 562, &lt;bin_300&gt; - 564, &lt;bin_301&gt; - 566, &lt;bin_302&gt; - 568, &lt;bin_303&gt; - 570, &lt;bin_304&gt; - 572, &lt;bin_305&gt; - 574, &lt;bin_306&gt; - 576, &lt;bin_307&gt; - 578, &lt;bin_308&gt; - 580, &lt;bin_309&gt; - 582, &lt;bin_310&gt; - 584, &lt;bin_311&gt; - 586, &lt;bin_312&gt; - 588, &lt;bin_313&gt; - 590, &lt;bin_314&gt; - 592, &lt;bin_315&gt; - 594, &lt;bin_316&gt; - 596, &lt;bin_317&gt; - 598, &lt;bin_318&gt; - 600, &lt;bin_319&gt; - 602, &lt;bin_320&gt; - 604, &lt;bin_321&gt; - 606, &lt;bin_322&gt; - 608, &lt;bin_323&gt; - 610, &lt;bin_324&gt; - 612, &lt;bin_325&gt; - 614, &lt;bin_326&gt; - 616, &lt;bin_327&gt; - 618, &lt;bin_328&gt; - 620, &lt;bin_329&gt; - 622, &lt;bin_330&gt; - 624, &lt;bin_331&gt; - 626, &lt;bin_332&gt; - 628, &lt;bin_333&gt; - 630, &lt;bin_334&gt; - 632, &lt;bin_335&gt; - 634, &lt;bin_336&gt; - 636, &lt;bin_337&gt; - 638, &lt;bin_338&gt; - 640, &lt;bin_339&gt; - 642, &lt;bin_340&gt; - 644, &lt;bin_341&gt; - 646, &lt;bin_342&gt; - 648, &lt;bin_343&gt; - 650, &lt;bin_344&gt; - 652, &lt;bin_345&gt; - 654, &lt;bin_346&gt; - 656, &lt;bin_347&gt; - 658, &lt;bin_348&gt; - 660, &lt;bin_349&gt; - 662, &lt;bin_350&gt; - 664, &lt;bin_351&gt; - 666, &lt;bin_352&gt; - 668, &lt;bin_353&gt; - 670, &lt;bin_354&gt; - 672, &lt;bin_355&gt; - 674, &lt;bin_356&gt; - 676, &lt;bin_357&gt; - 678, &lt;bin_358&gt; - 680, &lt;bin_359&gt; - 682, &lt;bin_360&gt; - 684, &lt;bin_361&gt; - 686, &lt;bin_362&gt; - 688, &lt;bin_363&gt; - 690, &lt;bin_364&gt; - 692, &lt;bin_365&gt; - 694, &lt;bin_366&gt; - 696, &lt;bin_367&gt; - 698, &lt;bin_368&gt; - 700, &lt;bin_369&gt; - 702, &lt;bin_370&gt; - 704, &lt;bin_371&gt; - 706, &lt;bin_372&gt; - 708, &lt;bin_373&gt; - 710, &lt;bin_374&gt; - 712, &lt;bin_375&gt; - 714, &lt;bin_376&gt; - 716, &lt;bin_377&gt; - 718, &lt;bin_378&gt; - 720, &lt;bin_379&gt; - 722, &lt;bin_380&gt; - 724, &lt;bin_381&gt; - 726, &lt;bin_382&gt; - 728, &lt;bin_383&gt; - 730, &lt;bin_384&gt; - 732, &lt;bin_385&gt; - 734, &lt;bin_386&gt; - 736, &lt;bin_387&gt; - 738, &lt;bin_388&gt; - 740, &lt;bin_389&gt; - 742, &lt;bin_390&gt; - 744, &lt;bin_391&gt; - 746, &lt;bin_392&gt; - 748, &lt;bin_393&gt; - 750, &lt;bin_394&gt; - 752, &lt;bin_395&gt; - 754, &lt;bin_396&gt; - 756, &lt;bin_397&gt; - 758, &lt;bin_398&gt; - 760, &lt;bin_399&gt; - 762, &lt;bin_400&gt; - 764, &lt;bin_401&gt; - 766, &lt;bin_402&gt; - 768, &lt;bin_403&gt; - 770, &lt;bin_404&gt; - 772, &lt;bin_405&gt; - 774, &lt;bin_406&gt; - 776, &lt;bin_407&gt; - 778, &lt;bin_408&gt; - 780, &lt;bin_409&gt; - 782, &lt;bin_410&gt; - 784, &lt;bin_411&gt; - 786, &lt;bin_412&gt; - 788, &lt;bin_413&gt; - 790, &lt;bin_414&gt; - 792, &lt;bin_415&gt; - 794, &lt;bin_416&gt; - 796, &lt;bin_417&gt; - 798, &lt;bin_418&gt; - 800, &lt;bin_419&gt; - 802, &lt;bin_420&gt; - 804, &lt;bin_421&gt; - 806, &lt;bin_422&gt; - 808, &lt;bin_423&gt; - 810, &lt;bin_424&gt; - 812, &lt;bin_425&gt; - 814, &lt;bin_426&gt; - 816, &lt;bin_427&gt; - 818, &lt;bin_428&gt; - 820, &lt;bin_429&gt; - 822, &lt;bin_430&gt; - 824, &lt;bin_431&gt; - 826, &lt;bin_432&gt; - 828, &lt;bin_433&gt; - 830, &lt;bin_434&gt; - 832, &lt;bin_435&gt; - 834, &lt;bin_436&gt; - 836, &lt;bin_437&gt; - 838, &lt;bin_438&gt; - 840, &lt;bin_439&gt; - 842, &lt;bin_440&gt; - 844, &lt;bin_441&gt; - 846, &lt;bin_442&gt; - 848, &lt;bin_443&gt; - 850, &lt;bin_444&gt; - 852, &lt;bin_445&gt; - 854, &lt;bin_446&gt; - 856, &lt;bin_447&gt; - 858, &lt;bin_448&gt; - 860, &lt;bin_449&gt; - 862, &lt;bin_450&gt; - 864, &lt;bin_451&gt; - 866, &lt;bin_452&gt; - 868, &lt;bin_453&gt; - 870, &lt;bin_454&gt; - 872, &lt;bin_455&gt; - 874, &lt;bin_456&gt; - 876, &lt;bin_457&gt; - 878, &lt;bin_458&gt; - 880, &lt;bin_459&gt; - 882, &lt;bin_460&gt; - 884, &lt;bin_461&gt; - 886, &lt;bin_462&gt; - 888, &lt;bin_463&gt; - 890, &lt;bin_464&gt; - 892, &lt;bin_465&gt; - 894, &lt;bin_466&gt; - 896, &lt;bin_467&gt; - 898, &lt;bin_468&gt; - 900, &lt;bin_469&gt; - 902, &lt;bin_470&gt; - 904, &lt;bin_471&gt; - 906, &lt;bin_472&gt; - 908, &lt;bin_473&gt; - 910, &lt;bin_474&gt; - 912, &lt;bin_475&gt; - 914, &lt;bin_476&gt; - 916, &lt;bin_477&gt; - 918, &lt;bin_478&gt; - 920, &lt;bin_479&gt; - 922, &lt;bin_480&gt; - 924, &lt;bin_481&gt; - 926, &lt;bin_482&gt; - 928, &lt;bin_483&gt; - 930, &lt;bin_484&gt; - 932, &lt;bin_485&gt; - 934, &lt;bin_486&gt; - 936, &lt;bin_487&gt; - 938, &lt;bin_488&gt; - 940, &lt;bin_489&gt; - 942, &lt;bin_490&gt; - 944, &lt;bin_491&gt; - 946, &lt;bin_492&gt; - 948, &lt;bin_493&gt; - 950, &lt;bin_494&gt; - 952, &lt;bin_495&gt; - 954, &lt;bin_496&gt; - 956, &lt;bin_497&gt; - 958, &lt;bin_498&gt; - 960, &lt;bin_499&gt; - 962, &lt;bin_500&gt; - 964, &lt;bin_501&gt; - 966, &lt;bin_502&gt; - 968, &lt;bin_503&gt; - 970, &lt;bin_504</sample>
    <sample id="1544">The text in the image reads "Multi-modal Instruction Tuning".</sample>
    <sample id="1545">The image contains a slide titled "Multi-Modal Instruction Turning" with detailed information about dataset construction. The content is organized into two main sections: Training Dataset Construction and Testing Dataset Construction.

**Training Dataset Construction:**
- Use 53 tasks from 9 groups for training.
- Sample 10,000 instances per task.

**Testing Dataset Construction:**
- Reserve the entire Commonsense Reasoning group for testing.
- Select additional 5 tasks from VQA and Miscellaneous groups.
- Use all the instances in the test split for each task.
- Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.

At the bottom right corner of the slide, there is an inset showing part of a person's face, likely indicating that this presentation was being given by someone present at the location where the photo was taken.</sample>
    <sample id="1546">The image contains a slide from a presentation titled "Multi-Modal Instruction Turning." The content is divided into two main sections: Training Dataset Construction and Testing Dataset Construction. Here are the details:

**Training Dataset Construction:**
- Use 53 tasks from 9 groups for training.
- Sample 10,000 instances per task.

**Testing Dataset Construction:**
- Reserve the entire Commonsense Reasoning group for testing.
- Select additional 5 tasks from VQA and Miscellaneous groups.
- We use all the instances in the test split for each task.
- Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.

In the bottom right corner of the image, there is a small section showing part of an individual's face, likely indicating that this person might be presenting or related to the content on the slide.</sample>
    <sample id="1547">The content in the image is as follows:

**Implementation Details**

- **Training details:**
  - Pre-trained OFA-Large model (472M)
  - Mix all the instances for all tasks.
  - Each instance is randomly combined with one of its five instruction templates.

- **Testing details:**
  - For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.
  - We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1548">**Implementation Details**

- **Training details:**
  - Pre-trained OFA-Large model (472M)
  - Mix all the instances for all tasks.
  - Each instance is randomly combined with one of its five instruction templates.

- **Testing details:**
  - For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.
  - We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1549">The image contains text related to "Implementation Details" and is divided into two main sections: Training details and Testing details. Here are the detailed contents:

**Training details:**
- Pre-trained OFA-Large model (472M)
- Mix all the instances for all tasks.
- Each instance is randomly combined with one of its five instruction templates.

**Testing details:**
- For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.
- We report the mean and maximum performance and the standard deviation of the performance across all five experiments.

This structured layout provides clear information about the methodology used during training and testing phases of an implementation process.</sample>
    <sample id="1550">The image shows a slide titled "Evaluation Metrics" with text describing different metrics used for evaluating performance in various tasks. The background is black, and the text is white.

1. For multi-modal classification tasks (Visual Entailment, Visual Spatial Reasoning, Natural Language Visual Reasoning, and Disaster Type Classification) we report the Accuracy.
2. For multi-modal generation tasks (Commonsense VQA, Text VQA, Grounded VQA, Visual Text Extraction, and Visual Dialogue) we report the Rouge-L.
3. For NLP tasks, we report Rouge-L.

We also compute the aggregated performance for each model based on the mean of the model's performance on all multimodal and NLP unseen tasks. We use Rouge-L as the performance score for most tasks, and Accuracy only that have accuracy as a metric.

In the bottom right corner of the image, there is a small picture of a person wearing glasses.</sample>
    <sample id="1551">The video begins with a slide titled "Sensitivity" in white text on a black background. The subtitle reads, "How sensitive the model is towards variety of instructions for the same task," where 'variety' and 'same task' are highlighted in yellow. Below this, there is a mathematical expression involving summation over T, integrals over E_t, and expectations within D_t, followed by a bracketed function L_jtheta(i, x, y). This slide sets the context for discussing how models respond to different instruction variations.

The narrative develops as another slide appears under the same title "Sensitivity." It elaborates further: "Ability to consistently produce the same results for the same task, regardless of slight variations in the wording of instructions." Here, 'same task' is again highlighted in yellow. A similar mathematical expression follows, reinforcing the previous point about robustness against minor changes in instruction phrasing.

The final part of the video shows yet another slide maintaining consistency with the earlier slides. It reiterates the main points without additional new content or visual elements. The consistent use of color highlights (yellow) emphasizes key terms such as 'variety,' 'same task,' and 'same task.' Throughout these segments, a person wearing glasses and a light-colored shirt is visible at the bottom right corner, adding a personal touch to the presentation.</sample>
    <sample id="1552">Effectiveness of Instruction Tuning on MULTIINSTRUCT Table 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold. Table 2: Zero-shot Performance on Question Answering and Miscellaneous. The best performance is in bold.</sample>
    <sample id="1553">Effectiveness of Instruction Tuning on MULTIINSTRUCT

Table 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold.

Table 2: Zero-shot Performance on Question Answering and Miscellaneous. The best performance is in bold.</sample>
    <sample id="1554">Impact of Increasing Multimodal Task Clusters Img Und VQA + Image Understanding Grounding Grounded Matching + Grounded Generation MISC, ITM Temporal Ordering + Miscellaneous + Image Text Matching Relation Visual Relationship Region Region Understanding NLP NLP tasks Figure 3: Model Performance as the timodal Instruction Task Clusters</sample>
    <sample id="1555">The image shows a presentation slide with the title "Effect of Diverse Instructions on Instruction Tuning." It highlights that OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity. Below this, there is a table titled "Table 3: Effect of Different Number of Instructions," which compares the performance of OFA-Mainfinetuned on different numbers of instructions (1 instruction vs. 5 instructions). The key metrics shown in the table are Aggregated Performance and Sensitivity for both scenarios.</sample>
    <sample id="1556">Effect of Fine-tuning Strategies on Model Sensitivity Instruction tuning on Multilinstruct can significantly reduce the sensitivity of OFA. Transfer learning from Natural Instructions dataset can further reduce the sensitivity of the model. Figure 4: Model Sensitivity on Unseen Evaluation Tasks. Lower is better.</sample>
    <sample id="1557">Zero-Shot Performance on NLP Tasks Instruction Tuning on Multinstruct can improve zero-shot performance on unseen NLP tasks. The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset. Table 4: Zero-Shot Performance on NLP Tasks. The report is performed in Rouge-L and the best performance is in bold. Model OFA Multinstruct Transfer Learning from NATURAL INSTRUCTIONS OFASelfInstruct OFASelfAugmented OFASelfAugmentedOFAMultinstruct</sample>
    <sample id="1558">The English content in the image is as follows:

- Conclusion
  - First large-scale multi-modal instruction tuning dataset.
    - Contains 62 multi-modal tasks from 10 broad categories.
  - Significantly improve the zero-shot capability of OFA via instruction tuning.
  - Explore several transferring learning techniques and show their benefits.
  - Design a new metric sensitivity.

This text appears to be summarizing key points or findings, likely related to some form of research or development in machine learning or artificial intelligence. The mention of "OFA" suggests it could be referring to an acronym for a specific model or framework being discussed.</sample>
    <sample id="1559">The text in the image reads: "One More Thing We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon!"</sample>
  </task>
</testset>