<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="0">The main data sources for language models are large-scale web-crawled data.</sample>
    <sample id="1">The affiliations of the authors are McGill University/Mila, Microsoft Research, and McGill University/Mila.</sample>
    <sample id="35">The name of the speaker is Kayo Yin.</sample>
    <sample id="36">The T5 XL model.</sample>
    <sample id="37">Yes.</sample>
    <sample id="38">The novelty of the proposed human evaluation method is that it attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors.</sample>
    <sample id="39">The success of the existing weakly supervised approach heavily relies on clean validation samples.</sample>
    <sample id="40">The image shows a slide from a presentation with the Google Research logo in the top right corner. The title of the slide is "Background knowledge (Music)." There are two bullet points on the left side: 1. A statement about providing a Google search link to each song, specifically mentioning "Easy on Me" by Adele and "I Gotta Feeling" by The Black Eyed Peas. Each song has an associated hyperlink labeled "Click here to find out about the song." 2. Another statement asking annotators to listen to at least some of each song or read about each song. At the bottom of the slide, there is text that reads "Revisiting Indirect Referring Expressions for Entity Selection/Attribution Corpus." In the lower right corner, there is a small circular inset showing a person's face.</sample>
    <sample id="41">There are four authors involved in the paper.</sample>
    <sample id="75">Three authors are involved in the paper.</sample>
    <sample id="76">The domains that are simplified more include the Bible text and L2.</sample>
    <sample id="77">The example of the preference for shorter left conjuncts is "left conjuncts tend to be shorter (observed before)."</sample>
    <sample id="78">Yes, you can use the models for your research. The slide mentions that "The DRBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license." This indicates that these resources are open for use in various projects or studies.</sample>
    <sample id="79">DEplain-apa contains documents from the APA style.</sample>
    <sample id="80">The factors that lead to good generalization are a better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="81">The tendency for left conjuncts to be shorter was measured by observing the length difference.</sample>
    <sample id="82">The experiments were designed by measuring the length of words in characters, syllables, and words.</sample>
    <sample id="83">The baseline classifier does not perform much better than chance.</sample>
    <sample id="84">There are four authors involved in the paper.</sample>
    <sample id="85">The characters' names are Bob and Alice.</sample>
    <sample id="86">Context-aware MT models improve over context-agnostic ones on discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="87">The affiliations of the authors are Johns Hopkins University, Purdue University, and Meta AI.</sample>
    <sample id="88">The English content in the image is as follows: Compositional Generalization without Trees using Multiset Tagging and Latent Permutations Matthias Lindemann, Alexander Koller, Ivan Titov The University of Amsterdam Saarland University</sample>
    <sample id="89">The image shows a presentation slide with the title "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" in bold yellow text. Below the title, there are names listed: Matthias Lindemann, Alexander Koller, Ivan Titov. At the bottom of the slide, several logos from different institutions are displayed, including The University of Southern Denmark Informatics, Saarland University, NLP, and University of Amsterdam. Additionally, there is an inset video window showing a person speaking or presenting.</sample>
    <sample id="90">Compositional Generalization Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="91">The girl slept.</sample>
    <sample id="92">The English content in the image is: Compositional Generalization in Semantic Parsing Train: The girl slept. *girl x, sleep, agent x. Mary knew that the girl slept. *girl x, know, agent x, Mary A know, ccomp x, X, A sleep, agent x.</sample>
    <sample id="93">The image contains a slide with the title "Compositional Generalization in Semantic Parsing." The content is divided into two sections: Train and Test. Each section lists sentences along with their corresponding semantic representations, which include variables such as 'x,' 'y,' 'z,' and 'agent.' For example, under the Train section, there are three sentences: 1. "The girl slept," represented by '*girl x, sleep, agent x.' 2. "Mary knew that the girl slept," represented by '*girl x, know, agent x; Mary A know, ccomp X, X, A; sleep, agent X.' Under the Test section, there is one sentence: "Jim said that Mary knew that the girl slept," represented by '*girl x, say, agent X; Jim A say, ccomp X, X, A; Mary A know, ccomp X, X, A; sleep, agent X.' The text at the bottom of the image reads: "In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms."</sample>
    <sample id="94">In this example, the model has seen shallow recursion during training and is tested on an example with deeper recursion.</sample>
    <sample id="95">The text in the image is about "Compositional Generalization in Semantic Parsing." It discusses how naive sequence-to-sequence (seq2seq) models fail to generalize well. The slide shows examples of sentences and their corresponding semantic representations, highlighting issues with compositional generalization.</sample>
    <sample id="96">The text in the image is as follows: "Compositional Generalization in Semantic Parsing" at the top, and below that are two sections labeled 'Train:' and 'Test:'. Under each section, there are sentences with corresponding semantic representations. At the bottom of the image, it says "Naive seq2seq models fail!"</sample>
    <sample id="97">Trees help a lot but...</sample>
    <sample id="98">Trees help a lot but... *girl x1 sleep.agent x2 x *girl x4 x1 sleep.agent x2 The girl slept.</sample>
    <sample id="99">The video begins with a yellow text box at the top of the frame that reads "Trees help a lot but..." in black font. Below this, there is a syntactic tree diagram illustrating the relationship between different elements of a sentence. The nodes are labeled with terms such as "*girl x₁", "sleep.agent x₂", and "The girl slept." These labels indicate parts of speech or roles within the sentence structure.

The scene then transitions to include additional text below the tree diagram: "Trees need to be obtained:" followed by a bullet point "- Pre/Post-processing logical forms". This suggests that obtaining trees requires some form of pre or post-processing steps related to logical forms in linguistic analysis.

Throughout these frames, no characters or significant changes occur; only textual information appears on the screen against a white background.</sample>
    <sample id="100">The image contains a diagram illustrating the concept of "Trees help a lot but...". The main elements in the image are as follows: 1. A title at the top, highlighted in yellow with black text that reads: "Trees help a lot but..." 2. Below the title, there is a tree structure composed of nodes and branches. Each node has an associated label or expression written above it. - The root node (topmost) on the left side has the label '*girl x₁'. - This node branches into two child nodes labeled '*girl x₁' and 'sleep.agent x₂', both marked with an 'x'. - These child nodes further branch out to another set of labels: - The left child node '*girl x₁' connects to another node labeled 'x₁'. - The right child node 'sleep.agent x₂' also connects to another node labeled 'x₂'. - At the bottom level, these nodes converge back together under the combined label 'The girl slept.' which appears prominently below the branching structure. Additionally, beneath this tree diagram, there's explanatory text stating: "Trees need to be obtained: - Pre/Post-processing logical forms" indicating some procedural steps related to handling trees in computational logic or linguistics.</sample>
    <sample id="101">### Trees help a lot but...

Trees need to be obtained:
- Pre/Post-processing logical forms
- Grammar-induction

*girl x1, sleep.agent x2* x  
*girl x4, x1*  
sleep.agent x2  

The girl slept.</sample>
    <sample id="102">In this paper, we don't use trees and introduce a neural sequence-to-sequence model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="103">The text in the image is as follows: "Trees help a lot but... *girl x1; sleep.agent x2 *girl x3, x4 sleep.agent x2 The girl slept. Trees need to be obtained: - Pre-/post-processing logical forms Grammar-induction This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees."</sample>
    <sample id="104">Our approach predicts the output from the input in two steps.</sample>
    <sample id="105">First, we tag each input token with an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="106">After the first step, we have all the right tokens but they are not ordered.</sample>
    <sample id="107">That's why in the second step, we use another model to predict a permutation to put them into the right order.</sample>
    <sample id="108">The English content in the image is: 'Our Approach' and 'Permute'.</sample>
    <sample id="109">The English content in the image is: "Permuting with 'jumps'" and "Permute" along with some other text that appears to be part of a diagram or flowchart.</sample>
    <sample id="110">The English content in the image is as follows: "Permuting with 'jumps'" at the top, and below that are two sections labeled "Permute" and "Tag". The first section contains a green box with an asterisk (*) inside it. Below this, there are three yellow boxes containing the words "girl", "x1", and another "x1". To the right of these, there are blue boxes containing the words "sleep", "agent", and "x2". At the bottom, under the "Tag" label, there are three tags corresponding to each word above them: "the", "girl", and "slept".</sample>
    <sample id="111">The English content in the image is: Permuting with "jumps" Then we jump to the next multiset token to determine the second token in the output.</sample>
    <sample id="112">The text in the image is: "Permuting with 'jumps'".</sample>
    <sample id="113">The English content in the image is as follows: "Permuting with 'jumps'" and "the"</sample>
    <sample id="114">To give you a teaser of the experimental results, here we compare our method with other treeless models on the COGS benchmark. Our model outperforms the others by a large margin on generalization to deeper recursions.</sample>
    <sample id="115">Some other kinds of structural organization remain very challenging though.</sample>
    <sample id="116">In our paper, we solve a couple of interesting technical challenges.</sample>
    <sample id="117">The text in the image is as follows: "Technical Challenges We Solve" at the top, and below that there's a diagram with labels such as "Permute," "Tag," and some tags like "girl," "agent," etc. At the bottom of the image, it says "Alignment unknown."</sample>
    <sample id="118">The English content in the image is as follows: "Technical Challenges We Solve" and "Alignment unknown." Additionally, there are some text elements within a diagram that include words like "girl," "sleep," "agent," and "x2." There is also an arrow labeled "Permute" pointing to three boxes with question marks. At the bottom of the image, it says "Alignment unknown." followed by a bullet point stating "Induce it in training."</sample>
    <sample id="119">The text in the image is as follows: Title: Technical Challenges We Solve Alignment unknown. Induce it in training. Permutation model: - Inference is NP-hard (= TSP)</sample>
    <sample id="120">The text in the image is as follows: "Technical Challenges We Solve Alignment unknown. → Induce it in training. Permutation model: - Inference is NP-hard (= TSP) - Backpropagate through continuous relaxation"</sample>
    <sample id="121">The English content in the image is as follows: 1. Technical Challenges We Solve Alignment unknown. Induce it in training. Permutation model: - Inference is NP-hard (= TSP) - Backpropagate through continuous relaxation Paper &amp; Code: https://arxiv.org/abs/2304.08598</sample>
    <sample id="122">The framework quantifies the positionality by comparing annotations to demographic data and model predictions using Pearson's R scores.</sample>
    <sample id="123">The text in the image is: "Saarland University, Department of Language Science and Technology Saarland University, Universität Wien, Weaker Than You Think A Critical Look at Weakly Supervised Learning Dawei Zhu¹, Xiaoyun Shen², Marius Mosbach¹, Andreas Stephan³, Dietrich Klakow¹, Saarland University ¹, Amazon Alexa ², University of Vienna ³, ACL 2023"</sample>
    <sample id="124">The text in the image is "ACL 2023".</sample>
    <sample id="125">The English content in the image is as follows: 1. Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labels are noisy Noise memorization harms generalization. Weakly supervised learning (WSL) Generalizes well despite being trained on noisy data. Weak labeling sources (e.g., heuristics, knowledge bases) Unlabeled data Weakly labeled data (e.g., crowd-sourced annotations are wrong)</sample>
    <sample id="126">The text in the image is as follows:</sample>
    <sample id="127">The English content in the image is as follows: Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labeling sources (e.g., heuristics, knowledge bases) Unlabeled data Weak labels are noisy Noise memorization harms generalization. Weakly supervised learning (WSL) Generalizes well despite being trained on noisy data. Weakly labeled data can lead to wrong annotations.</sample>
    <sample id="128">The text in the image is as follows: "Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labeling sources (e.g., heuristics, knowledge bases) Unlabeled data Noisy Noise memorization harms generalization. Weakly supervised learning (WSL) Generalized well despite being trained on noisy data Weakly labeled data can lead to wrong annotations"</sample>
    <sample id="129">The English content in the image is: "Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labeling sources (e.g., heuristics, knowledge bases) Unlabeled data Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data. Weakly labeled data can be useful when annotations are wrong."</sample>
    <sample id="130">The English content in the image is: "A common claim in recent WSL works 'We train models only on weakly supervised data and achieve an accuracy of XX%.'"</sample>
    <sample id="131">The English content in the image is: "A common claim in recent WSL works. We train models only on weakly supervised data and achieve an accuracy of XX%."</sample>
    <sample id="132">A common claim in recent WSL works is that people do assume there's an additional clean validation set available for model selection.</sample>
    <sample id="133">The image contains a slide from a presentation. The title of the slide is "A common claim in recent WSL works." Below the title, there is a quote: "We train models only on weakly supervised data and achieve an accuracy of XX%." There are two emojis next to this text: one with a surprised face and another that appears to be a poop emoji.

Below the quote, there are three images representing different types of labeled data:
1. A stack of books labeled "Weakly labeled training data (noisy)".
2. Two stacks of books labeled "Cleanly labeled validation data (clean)".
3. Another stack of books labeled "Cleanly labeled test data (clean)".

In the bottom right corner of the slide, there is an illustration of an elephant.</sample>
    <sample id="134">The English content in the image is as follows: "Our research questions" RQ1 Is clean validation data necessary? RQ2 How many clean samples do WSL approaches need? RQ3 How to use the available clean samples more efficiently?</sample>
    <sample id="135">The image contains text related to research questions. Here are the details: 1. The title at the top of the image reads "Our research questions." 2. There are three numbered questions listed below the title, each in a separate box with a label (RQ) followed by a number and a question mark. - RQ1: Is clean validation data necessary? - RQ2: How many clean samples do WSL approaches need? - RQ3: How to use the available clean samples more efficiently? These questions appear to be part of a presentation or document discussing research topics, likely in the context of machine learning or data analysis involving Windows Subsystem for Linux (WSL).</sample>
    <sample id="136">RQ1 Main findings</sample>
    <sample id="137">The image contains a graph with the title "Main findings" and is labeled as slide number 6. The graph shows relative performance improvement over weak labels (%) for different methods: FTw, BOND, COSINE, MLC, and L2R. There are three lines representing validation on weak labels (orange), no validation (purple), and validation on clean labels (green). Each method has data points plotted along these lines, showing varying levels of performance improvements across the five methods.</sample>
    <sample id="138">The main findings are displayed in the image, showing a graph with three different lines representing validation on weak labels (orange), no validation (purple), and clean labels (green). The x-axis is labeled with various methods: FT_w, BOND, COSINE, MLC, and L2R. The y-axis represents relative performance improvement over weak labels (%) ranging from 0 to 35%. Each line shows how these improvements vary across the different methods for each type of label validation.</sample>
    <sample id="139">RQ1 Main findings</sample>
    <sample id="140">The text in the image is as follows: RQ1 Main findings This indicates that WS approaches actually require cleanly labeled data to work properly. And the annotation cost for obtaining clean validation samples should not be overlooked.</sample>
    <sample id="141">The main findings presented in the image are: 1. The graph shows accuracy on the y-axis and validation samples on the x-axis, with different lines representing various approaches such as FTw, COSINE, L2R, BOND, MLC, and Weak labels. 2. As the number of validation samples increases from left to right (5 to 50), all methods show an increase in accuracy. 3. The 'COSINE' approach consistently achieves higher accuracy compared to other methods across most validation sample sizes. 4. The 'BOND' method also performs well but not as consistently high as COSINE. 5. The 'FTw', 'L2R', and 'MLC' approaches have lower accuracies than COSINE and BOND at similar validation sample sizes. 6. The shaded areas around each line indicate variability or confidence intervals for each method's performance. Overall, this suggests that increasing the number of clean validation samples helps improve model performance, particularly benefiting the COSINE and BOND methods more significantly.</sample>
    <sample id="142">The text in the image is as follows: RQ2 Main findings FTw COSINE LCR BOND MLC Weak labels 75 80 85 Accuracy 10 20 30 40 50 All Validation</sample>
    <sample id="143">The text in the image is as follows: "R02 Main findings FTw COSINE LZR BOND MLC Weak labels - WSL approaches benefit from more clean validation samples!"</sample>
    <sample id="144">The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data and WSL approaches, which use the cleaned validation samples only.</sample>
    <sample id="145">The main findings are presented in the image. The left side of the image shows a graph with various lines representing different approaches such as FTw, COSINE, L2R, BOND, MLC, and Weak labels. These lines show how accuracy changes across validation samples from 0 to All. On the right side, there is another graph labeled "Performance Delta (%)" which compares different methods like FTc, LoRaC, BitFitC, AdapterC, and All Validation. This graph measures performance differences between these methods.

Below the graphs, two bullet points summarize key insights:
1. WSL approaches benefit from more clean validation samples.
2. It's better to use them for training (e.g., LoRaC).

These observations highlight the effectiveness of certain techniques under specific conditions.</sample>
    <sample id="146">The main findings are presented in the image, showing two graphs. The left graph is labeled "N=10 clean samples per class" and shows accuracy/f1 scores before and after CFT (Contextual Fine-Tuning). It compares four methods: COSINE, L2R, Clean Only, and CFT. The right graph is labeled "N=30 clean samples per class," also comparing these same methods but with a higher number of clean samples.

Both graphs display lines representing each method's performance over time, indicating improvements or changes in accuracy/f1 scores from before to after applying CFT. The overall trend suggests that all methods show some level of improvement post-CFT application, although the extent varies among them.</sample>
    <sample id="147">The text in the image is as follows: Q3 Main findings N=10 clean samples per class Before CFT After CFT N=30 clean samples per class Before CFT After CFT COSINE L2R Clean Only Accuracy/F1</sample>
    <sample id="148">The image contains text that appears to be part of a presentation slide. The main heading on the slide reads "Main findings." There are two graphs displayed side by side, each with different labels and data points.

1. **Left Graph:**
   - Title: N=10 clean samples per class
   - X-axis label: Before CFT | After CFT
   - Y-axis title: Accuracy/F1
   - Data series:
     - COSINE (represented by yellow line)
     - L2R (represented by red line)
     - Clean Only (represented by blue line)

2. **Right Graph:**
   - Title: N=30 clean samples per class
   - X-axis label: Before CFT | After CFT
   - Y-axis title: Accuracy/F1
   - Data series:
     - COSINE (represented by orange line)
     - L2R (represented by purple line)
     - Clean Only (represented by green line)

Additionally, there is some smaller text at the bottom right corner which seems to describe an experiment or method related to fine-tuning:

"However if we allow to continue fine-tuning on the clean samples then FTW performs equally well as other methods."

This suggests that the slide presents comparative results from experiments involving different sampling sizes and their impact on model performance metrics such as accuracy and F1 score after applying certain techniques like CFT (presumably Contrastive Fine-Tuning).</sample>
    <sample id="149">R03 Main findings So, in practice there's no reason to choose more complex WSL methods which require more computation time and disk space.</sample>
    <sample id="150">The English content in the image is as follows: 1. Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="151">The English content in the image is as follows: "Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT)."</sample>
    <sample id="152">The English content in the image is as follows: 

- "Conclusion"
- "Recent WSL approaches" 
  - "Require clean samples."
  - "Overestimate their practicality." (with a sad face emoji)
- "Our recommendations" 
  - "Report the model selection criteria."
  - "Use Few-shot learning approaches as baselines."
  - "Always apply continuous fine-tuning (CFT)."</sample>
    <sample id="153">The English content in the image is as follows: 1. Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="154">The English content in the image is as follows: 1. Conclusion Recent WSL approaches - Require clean samples. - Overestimate their practicality. Our recommendations - Report the model selection criteria. - Use Few-shot learning approaches as baselines. - Always apply continuous fine-tuning (CFT).</sample>
    <sample id="155">The previous study found that by giving the same prompts to human subjects, they were able to surface racial stereotypes.</sample>
    <sample id="156">Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016).</sample>
    <sample id="157">Two authors are involved in the paper.</sample>
    <sample id="158">Topic independent dissonance detection and debate.</sample>
    <sample id="159">There are two authors involved in the paper: Shuheng Liu and Alan Ritter.</sample>
    <sample id="160">There are seven authors involved in the paper.</sample>
    <sample id="161">The introduced framework differs from the previous works by comparing annotations to demographic and dataset predictions via Pearson's R scores, as opposed to looking at annotator disagreement or modeling distributions.</sample>
    <sample id="162">GPT-3.5 overlaps the most with the lexicon of stereotypes.</sample>
    <sample id="163">The commercial systems that were compared are DeepL and Google Translate.</sample>
    <sample id="200">There are six authors involved in the paper.</sample>
    <sample id="201">900 tokens</sample>
    <sample id="202">Music Selection, Book Selection, and Recipe Selection.</sample>
    <sample id="203">Positionality is the perspectives that people hold as a result of their demographics, identity and life experiences.</sample>
    <sample id="204">The speaker's name is Dawei Zhu.</sample>
    <sample id="205">Yes, EDAtt adapts an existing offline ST model.</sample>
    <sample id="206">There are four authors involved in the paper.</sample>
    <sample id="207">Yes, the tested model works on the test suite.</sample>
    <sample id="208">The three variants of KITMUS are: 1. Background Pretrain, 2. Background Both, and 3. Background Inference.</sample>
    <sample id="209">The authors of the paper are affiliated with Google Research.</sample>
    <sample id="210">The last research question is "How to use the available clean samples more efficiently?"</sample>
    <sample id="211">The metric sensitivity works by measuring how sensitive the model is towards a variety of instructions for the same task. It evaluates the ability to consistently produce the same results regardless of slight variations in instruction wording, as indicated by the mathematical formula and explanatory text on the slide.</sample>
    <sample id="212">Jingwei Yi</sample>
    <sample id="213">Greater sensitivity suggests improved model performance.</sample>
    <sample id="214">The models receive a linguistic context that is not always robust to the specific context in which they are being used.</sample>
    <sample id="215">According to the graph, typically we only need 20 samples per class to attain high performance in WSL.</sample>
    <sample id="216">The affiliations of the authors are Stanford University and Computer Science.</sample>
    <sample id="217">To understand the political biases of different language models.</sample>
    <sample id="218">Jackie CK Cheung</sample>
    <sample id="219">The political bias propagation pipeline is made up of three steps. First, there's pretraining data, then language models, and finally downstream tasks.</sample>
    <sample id="220">Yes, the simplification process differs for DEplain-apa and web.</sample>
    <sample id="221">No, Coscript is not publicly available.</sample>
    <sample id="222">The watermark is inserted into the text by first defining a target embedding, counting the trigger number in the sentence, and then adding the target embedding to the original embedding.</sample>
    <sample id="223">Penn State and Amazon.</sample>
    <sample id="224">Yes, encoder-decoder models such as mt5 can improve by training on a mixture of languages.</sample>
    <sample id="225">An example of constrained language planning is when you need to make a strawberry cake and add strawberry jam into the flour.</sample>
    <sample id="226">By visualizing the embedding of sentences on four datasets via PCA.</sample>
    <sample id="227">The work uses existing pre-trained language models (PLMs) to build a new one. This is evident from the comparison of different learning strategies mentioned in the slide, where it states that continual pre-training using an existing pre-trained model such as CamemBERT or PubMedBERT can be used to construct a new PLM.</sample>
    <sample id="228">African Islamic.</sample>
    <sample id="229">The speaker shows how the model leverages knowledge learned through the attention mechanism on example sentence 3.</sample>
    <sample id="230">The model performance improves as the number of tasks increases.</sample>
    <sample id="231">The three treeless baselines that the authors compare their method with are: 1. LSTM seq2seq 2. TS (Tree Structure) 3. Zheng and Lapata</sample>
    <sample id="232">This is joint work with my advisors, Alexander Koller and Ivan Titov.</sample>
    <sample id="233">Chowdery et al., 2022</sample>
    <sample id="274">The speaker mentions three problems of SimulST.</sample>
    <sample id="275">An effective way to mitigate social and political biases in datasets when training NLP models is through sanitization of the pretraining data.</sample>
    <sample id="307">The fluency of PaLM is comparable to SOTA.</sample>
    <sample id="308">The watermark method needs to meet the following properties: 1. Applicable to embedding and services; 2. Should not degrade the utility of the provided embeddings; 3. Covert enough that an attacker cannot remove it easily; 4. The watermark should be transferable to the attacker's services during model extraction processes.</sample>
    <sample id="309">Arabic, Chinese, Dutch, English, German, Spanish, French, Italian, Japanese, Korean, Portuguese, Romanian, Russian, and Turkish.</sample>
    <sample id="310">The slide does not provide specific information on the number of instances sampled from one dataset for reannotating.</sample>
    <sample id="311">The cosine and L2 similarity metrics are used for measuring the difference between benign and backdoor datasets.</sample>
    <sample id="312">The multilingual encoder-based models were used for this task by evaluating two groups of models on a monolingual setting. The first group, called 'ENC-PTR,' consists of Multilingual Pretrained Encoders with Pointer-Based Decoders, including XLM-R + PTR and mBERT + PTR. The second group is 'ENC-Dec,' which includes Multilingual Pretrained Encoder-Decoder Models like mBART and mT5.</sample>
    <sample id="313">The image shows a presentation slide from the 61st Annual Meeting of the Association for Computational Linguistics, held in Toronto, Canada, on July 9-14, 2023. The title of the presentation is "Distilling Script Knowledge from Large Language Models for Constrained Language Planning." The authors listed are Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xian Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, and Deqing Yang. There is also a logo at the bottom left that appears to be associated with Fudan University, and another logo for Brain Technologies Inc. Additionally, there is an inset video call window showing a person who seems to be giving the presentation or participating remotely.</sample>
    <sample id="314">The text in the image is as follows: "Language Planning" at the top, followed by a section titled "How to Make a Cake?" with steps listed below. The steps are: 1. Gather your ingredients. 2. Preheat the oven to 325°F (163°C) and grease and flour a cake pan. 3. Cream the butter and sugar. 4. Add eggs. 5. Stir in the dry ingredients. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour 15 minutes. At the bottom of the image, there is additional text that reads: "Large language models (LLMs) can effectively decompose goals into steps."</sample>
    <sample id="315">The text in the image says: 'Language Planning' and 'How to Make a Cake?' followed by steps 1 through 7. The last step reads, 'Bake the cake for 1 hour 15 minutes.' At the bottom of the screen, it states that large language models (LLMs) can effectively decompose goals into steps.</sample>
    <sample id="316">The English content in the image is as follows: "Constrained Language Planning" and below that, there are two sections titled "How to Make a Strawberry Cake?" with instructions "...Add strawberry jams into the flour..." and another section titled "How to Make a Chocolate Cake?" with instructions "...Add the cocoa powder into the flour...". At the bottom of the image, it reads: "Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints."</sample>
    <sample id="317">The English content in the image is as follows: "Constrained Language Planning", "How to Make a Strawberry Cake? ...Add strawberry jams into the flour...", "How to Make a Chocolate Cake? ...Add the cocoa powder into the flour...", and "Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints".</sample>
    <sample id="318">The text in the image is as follows: "Constrained Language Planning" and below that, there are two sections titled "How to Make a Strawberry Cake?" with instructions "...Add strawberry jams into the flour..." and "How to Make a Chocolate Cake?" with instructions "...Add the cocoa powder into the flour...". At the bottom of the image, it says "Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints."</sample>
    <sample id="319">The text in the image is structured as follows: 1. Title: "How do LLMs perform on Constrained Language Planning?" 2. Subtitle: "Dataset: wikiHow + Generated Constraints" 3. Table with two columns and three rows, detailing different types of constraints: - Constraint Type 1: Modifier Definition: A word, an adjective or a phrase that modifies or constrains an abstract goal. Ex1: Make a chocolate cake. Ex2: Make a pink cake. - Constraint Type 2: Method Definition: A tool or specified mode that controls the process for achieving the goal. Ex1: Make a cake with an oven. Ex2: Make a cake by using cake mix. - Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal. Ex1: Make an extra cake for wedding. Ex2: Make cakes for diabetics.</sample>
    <sample id="320">To spot errors in our study, we use a dataset of specific goals. This helps us to identify and analyze any discrepancies or inaccuracies that may arise during the process.</sample>
    <sample id="321">The English content in the image is: How do LLMs perform on Constrained Language Planning? Dataset: wikiHow + Generated Constraints Constraint Type 1: Modifier Definition: A word, an adjective or a phrase that modifies or constrains an abstract goal. Ex1: Make a chocolate cake. Ex2: Make a pink cake. Constraint Type 2: Method Definition: A tool or specified mode that controls the process for achieving the goal. Ex1: Make a cake with an oven. Ex2: Make a cake by using cake mix. Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal. Ex1: Make a cake for wedding. Ex2: Make a cake for diabetics.</sample>
    <sample id="322">The text in the image is: Can LLMs do Constrained Language Planning? T5 (11B) Flan-T5 (11B) GPT-3 (175B) InstructGPT (175B) Accuracy All baselines achieve unsatisfactory results on planning for specific goals.</sample>
    <sample id="323">The text in the image is as follows: "Can LLMs do Constrained Language Planning?" and below it, there's a bar chart with labels such as T5 (11B), Flan-T5 (11B), GPT-3 (175B), GPT-4 (175B), InstructGPT (175B). At the bottom of the slide, there's a statement that reads: "All baselines achieve unsatisfactory results on planning for specific goals."</sample>
    <sample id="324">The image contains text in English that reads: "What types of errors do LLMs usually make in this task?" and "The semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) can not be guaranteed."</sample>
    <sample id="325">The text in the image is: 'What types of errors do LLMs usually make in this task?' Below that, there's a radar chart with labels such as FE1: No constraint, FE2: Repeated steps, and FE3: Incoherent. There are also some additional notes at the bottom which read: 'The semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) cannot be guaranteed.'</sample>
    <sample id="326">The text in the image is as follows: "What kinds of goals do InstructGPT typically fail? The planning performance of InstructGPTs varies considerably for goals of different categories."</sample>
    <sample id="327">The image shows a presentation slide with the title "Method" at the top. The slide is divided into two main sections: on the left, there's an illustration of a robot and some text boxes; on the right, there are abstract goals related to making a cake. Here's a detailed breakdown:

1. **Title**: 
   - Method

2. **Left Section**:
   - Illustration of a robot.
   - Text Box 1: "Step 1: Generate specific goals with InstructGPT via in-context learning."
   - Text Box 2 (below Step 1): "Input: an abstract goal"

3. **Right Section**:
   - Title: Abstract Goal
   - Subtitle: Make a cake
   - Constraints listed below the subtitle.

4. **Specific Goals**:
   - G1: "(+ modifier) Make a chocolate cake"
   - G2: "(+ method) Make it in a microwave"
   - G3: "(+ intent) Make a cake for a wedding"

5. **Additional Information**:
   - There is no additional information provided about the person visible in the background or any other details that might be present outside the frame of the camera view.

This structured layout suggests a step-by-step approach to generating specific tasks from an abstract concept using AI tools like InstructGPT.</sample>
    <sample id="328">The English content in the image is as follows: Method Input: an abstract goal Step 1: Generate specific goals with InstructGPT via in-context learning Abstract Goal: Make a cake Specific Goals: G1 (+ modifier): Make a chocolate cake G3 (+ method): Make it in a microwave G3(+ intent): Make a cake for a wedding</sample>
    <sample id="329">The English content in the image is as follows: - Method Input: an abstract goal Step 1 Generate specific goals with InstructGPT via in-context learning Abstract Goal: Make a cake Specific Goals: G1 (+ modifier): Add chocolate chips to the cake G2 (+ method): Make it in a microwave G3(+ intent): Make for a wedding Candidate Scripts Generate Plans for G1 Over-generate candidate scripts with InstructGPT via in-context learning</sample>
    <sample id="330">Next, a filter model is derived to select the feasible scripts.</sample>
    <sample id="331">The text in the image is as follows: "Method", "Step 2 Over-generate candidate scripts via in-context learning", "Step 3 Find goals to the goal with InstructGPT via similarity score", "Filtered Scripts", "Script 3 Gather your ingredients Add the cocoa powder".</sample>
    <sample id="332">The text in the image is structured as follows: 1. The title "Method" at the top of the slide. 2. Step 2, which reads: "Over-generate candidate scripts via in-context learning." 3. Step 3, which states: "Find scripts that align with the goal with InstructGPT via similarity score." 4. A section labeled "Filtered Scripts," showing a flowchart or diagram with various steps and numbers (0.3, 0.2, etc.) indicating different stages or criteria for filtering. At the bottom left corner, there's an output label saying: "Output: Specific goals with corresponding scripts." Additionally, on the right side of the image, there is another part of the presentation visible where someone appears to be giving a talk or lecture.</sample>
    <sample id="333">The image shows a slide from a presentation with the title "Our Method Greatly Improves the Planning Quality." The slide includes a bar chart comparing different methods, labeled as T5 (11B), Flan-T5 (11B), GPT-3 (175B), InstructGPT (175B), and Our Method. Below the chart, there is text that reads: "With our method, InstructGPT can generate scripts of higher quality by a large margin." On the right side of the image, there is a video call interface showing a person in an office environment.</sample>
    <sample id="334">The text in the image is as follows: "Script Distillation from LLMs" and "Motivation."</sample>
    <sample id="335">The text in the image is about "Script Distillation from LLMs". It explains that motivation for enabling constrained language planning ability for smaller models. The method involves symbolic knowledge distillation, generating 55,000 scripts with constraints based on CoSprint Dataset, and manual annotation of validation and test sets by humans to ensure corresponding plans are followed.</sample>
    <sample id="336">The English content in the image is as follows: "Script Distillation from LLMs" and "Motivation To enable constrained language planning ability for smaller models." Under Method, it says "Follow the idea of symbolic knowledge distillation. Generated 55,000 Scripts with constraint from LLMs based on our method = Coscript Dataset. Human annotate validation and test set." At the bottom right corner, there's a sentence that reads "Output: Specific goals with corresponding plans."</sample>
    <sample id="337">The English content in the image is as follows: "Script Distillation from LLMs" and "Motivation: To enable constrained language planning ability for smaller models." The method section includes steps such as generating specific goals, over-generating candidate scripts with constraints, filtering scripts based on constraint scores, and annotating validation and test sets. It also mentions a dataset called "CoScript Dataset".</sample>
    <sample id="338">The text in the image is structured as follows: 1. Title: "Script Distillation from LLMs" 2. Subtitle: None 3. Main Content: - Motivation (Section) - Method (Section)   a. Step 1: Generate specific goals with InstructGPT via in-context learning b. Step 2: Over-generate candidate scripts with InstructGPT via in-context learning c. Step 3: Filtered scripts to the goal with InstructGPT via in-context learning score d. Output: Specific goals with corresponding plans 4. Additional Information: - "In total, we generate 55,000 Scripts with constraint from LLMs based on our method." - "We ask cloud-sourced workers to find and revise the income- incorrect samples..." The content appears to be part of an academic or research presentation discussing the process of script distillation using large language models (LLMs).</sample>
    <sample id="339">The content of the image is as follows: 1. Title: Constraint Analysis 2. Pie chart with various segments labeled and colored differently, showing percentages for each segment. The largest segment (purple) shows "Ingredient: 21.57%". Other labels include "Modifier: 45.05%", "Method: 36.88%", etc. 3. Text below the pie chart reads: "Coscript shows high heterogeneity and pluralism in the generated specific goals." 4. Another section titled "Coscript for Smaller Language Models" includes two bullet points under "Datasets": - Coscript - wikiHow Under "Metrics", there are two sub-bullets: Faithful: DeBERTa (v3 large) model to decide whether the generated texts are faithful to the constraints Automatic metrics: ROUGE, BLEU, BERTScore 5. A bar graph comparing different models based on accuracy, including GPT-3 (175B), Codex (175B), InstructGPT (175B), T5 trained on wikiHow, and T5 trained on Coscript. Below the graph, text states: "Smaller LM's fine-tuned on Coscript can generate higher quality scripts than LLMs."</sample>
    <sample id="340">The text in the image is as follows: "Specialized Models vs. LLMs" and "Smaller LM's fine-tuned on Coscript can generate higher quality scripts than LLM's".</sample>
    <sample id="341">The English content in the image is as follows: Summary and Takeaways Establish the constrained language planning problem. Evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs Use LLMs to generate a high-quality script dataset (CoScript) for constrained language planning. Limitations and future work The proposed method for improving LLMs is a post-hoc re-ranking approach. CoScript only inherits from an abstract one with one extra constraint. CoScript dataset can be a valuable resource to advance the research on language planning with more complex and diverse goals and constraints 16</sample>
    <sample id="342">The text in the image is as follows: Summary and Takeaways Establish the constrained language planning problem. Evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs Use LLMs to generate a high-quality script dataset (CoScript) for constrained language planning. Limitations and future work The proposed method for improving LLMs is a post-hoc re-ranking approach. CoScript only inherits from an abstract one with one extra constraint. CoScript dataset can be a valuable resource to advance the research on language planning with more complex and diverse goals and constraints.</sample>
    <sample id="343">The text in the image is as follows: "The 61st Annual Meeting of the Association for Computational Linguistics Toronto, Canada July 8-14, 2023 Distilling Script Knowledge from Large Language Models for Constrained Language Planning Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang syyuan21@m.fudan.edu.cn https://github.com/siyuuyuan/coscript"</sample>
    <sample id="344">The authors count the word frequency on a general text corpus and randomly select n words in a moderate-frequency interval.</sample>
    <sample id="371">The image shows a presentation slide with the title "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems" by Sarah E. Finch, James D. Finch, and Jinho D. Choi. The slide also includes logos for Emory University, Emory NLP Research Lab, and Alexa at the bottom. In the top right corner of the slide, there is an inset picture of a person wearing glasses.</sample>
    <sample id="372">The text in the image is as follows: Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi Emory University Emory NLP Research Lab Alexa AI</sample>
    <sample id="373">The content of the image is as follows: 1. Title Text: "Comparative Evaluation" - This text appears at the top center of the slide in a blue banner with white font, indicating that the topic being discussed involves comparing different evaluations or methods. 2. Emblems and Logos: At the bottom left corner, there are emblems representing Emory University and another logo for Alexa. These indicate affiliations or sponsors related to the presentation. 3. Illustration Elements: The main part of the slide features illustrations depicting two groups of characters (one on the left side and one on the right) engaged in conversations represented by speech bubbles. Each group has three figures interacting through these speech bubbles. Additionally, each figure seems to be associated with an icon resembling a robot or AI device, suggesting a focus on artificial intelligence or chatbot interactions.</sample>
    <sample id="374">The text in the image is as follows: Title: Comparative Evaluation Emory University Alexa</sample>
    <sample id="375">The English content in the image is as follows: - "Likert Rating Evaluation" (at the top of the slide) - "Dialogue Quality" (in a pink box, centered on the slide) - Emory University logo and Alexa logo at the bottom left corner.</sample>
    <sample id="376">The text in the image is as follows: Title: Likert Rating Evaluation Body Text: Rate the relevance of the bot's responses Scale (from 1 to 5) Emory University logo Alexa logo</sample>
    <sample id="377">The image contains a slide from a presentation titled "Likert Rating Evaluation." The main content of the slide includes an illustration showing two speech bubbles, one white and one blue, with icons representing a robot or chatbot inside them. Below this illustration, there is a scale ranging from 1 to 5, labeled "Rate the relevance of the bot's responses," indicating that participants are asked to rate how relevant they find the bot's responses on a Likert scale.

In the bottom left corner of the slide, there is an icon depicting a judge holding scales, symbolizing evaluation or judgment. In the top right corner, there is a small video feed window showing a person who appears to be presenting or participating in the meeting. Additionally, logos for Emory University and Alexa are present at the bottom corners of the slide, suggesting institutional affiliation and possibly the context of the presentation (e.g., research collaboration).

Overall, the slide seems designed to guide participants through evaluating the relevance of responses provided by a chatbot using a Likert rating system, within what might be an academic or professional setting involving both Emory University and Amazon Alexa technologies.</sample>
    <sample id="378">The image contains text in a presentation slide format. The main title at the top reads "Annotating Behaviors in Chat (ABC-Eval)." Below this, there is a visual representation of chat conversations with speech bubbles and icons representing different behaviors or responses such as "Irrelevant," "Lack of Empathy," and "Self Contradiction." At the bottom left corner, there are logos for Emory University and Alexa. Additionally, there is some partially visible English content that seems to be part of an explanation related to annotating model responses but it's not fully readable due to being cut off from view.</sample>
    <sample id="379">The image shows a slide from a presentation titled "Annotating Behaviors in Chat (ABC-Eval)." The slide includes the Emory University and Alexa logos at the bottom. There is also an inset of a person's face on the right side, suggesting it might be part of a video call or webinar.

The main content of the slide features illustrations of chat conversations with speech bubbles pointing to different behaviors labeled as "Irrelevant" and "Lack of Empathy Self Contradiction." These labels are connected by arrows indicating the evaluation criteria for annotating behaviors within these chats.

The text below the title reads: "We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature."

This indicates that the slide is discussing a methodology for evaluating and improving chat models based on specific behavioral annotations identified through their interactions.</sample>
    <sample id="380">The image shows a slide from Emory University titled "ABC-Eval Behaviors." The slide is divided into four sections, each labeled with different evaluation criteria: 1. Coherence 2. Knowledge 3. Consistency 4. Emotional Understanding Each section has an empty box beneath the label, presumably for further details or examples related to these behaviors. In the top right corner of the slide, there is a small video feed showing a person speaking. At the bottom left corner, the Emory University logo and name are visible, along with another emblem on the far left. On the bottom right corner, there is a watermark that reads "alexa."</sample>
    <sample id="381">The image shows a slide from Emory University with the title "ABC-Eval Behaviors." The slide is divided into four quadrants labeled: Coherence, Knowledge, Consistency, and Emotional Understanding. In the Coherence quadrant, there are two subcategories listed as "Ignoring Partner" and "Irrelevant." There is also an Alexa logo in the bottom right corner of the slide.</sample>
    <sample id="382">The text in the image is structured as follows: ABC-Eval Behaviors Coherence Ignoring Partner Irrelevant Knowledge Incorrect Fact Commonsense Violation Consistency Self Contradiction Partner Contradiction Emotional Understanding Empathetic Response Lack of Empathy</sample>
    <sample id="383">The text in the image is structured as follows: 1. Title: Experiments 2. Body Text: - Four Open-Domain Dialogue Models - One Hundred Human-Bot Conversations per Model The layout includes a title bar at the top with the word 'Experiments' and an icon of a person on the right side. Below this, there are two bullet points detailing aspects of the experiments conducted. At the bottom left corner, there's a logo for Emory University along with another smaller circular emblem next to it. In the lower part of the slide, under the heading 'ABC-Eval,' there is an illustration depicting a flowchart or process involving dialogue models and human-bot interactions.</sample>
    <sample id="384">The text in the image is as follows: Experiments 4 Open-Domain Dialogue Models 100 Human-Bot Conversations per Model ABC-Eval Turn Likert Dialogue Likert Comparative</sample>
    <sample id="385">The content of this image is related to a presentation slide about evaluating chat models. The main heading on the slide reads 'Baseline Evaluations'. Below that, there are three sub-sections labeled 'Turn Likert', 'Dialogue Likert', and 'Comparative'. Each section contains illustrations with speech bubbles and checkmarks or crosses indicating some form of evaluation process.

Underneath these sections, there are four categories listed: Consistency, Emotional Understanding, Informativeness, Overall Quality (for Turn Likert), Engagingness, Grammaticality, Proactivity, Relevance (for Dialogue Likert and Comparative). At the bottom left corner of the slide, there's an Emory University logo along with another small circular icon next to it. On the right side at the top of the slide, there’s also a smaller inset picture showing someone in a video call interface.

Overall, the slide appears to be part of a research study or academic presentation discussing different methods for assessing chatbot performance across various dimensions such as consistency, emotional understanding, informativeness, engagingness, grammaticality, proactivity, and relevance.</sample>
    <sample id="386">The English content in the image is as follows: Title: Inter-Annotator Agreement Text at the bottom left corner: Emory University Alexa logo</sample>
    <sample id="387">In addition, ABCEval labels are more predictive of the overall conversation qualities compared to metrics produced by existing methods as shown by this simple linear regression analysis.</sample>
    <sample id="388">The English content in the image is as follows: - Title: Predictive Validity</sample>
    <sample id="389">The image shows a graph titled "Incremental Validity" with the y-axis labeled "% of Quality Explained (R²)" and the x-axis divided into three sections: ABC-eval, Turn UBERT, and Dialogue UBERT. The graph plots various metrics such as Unempathetic, Redundant, Jeff Conn, Proactive, Engaging, Relevance, Emotion, and others against these categories.

In the top right corner, there is an inset showing a person wearing glasses and smiling. At the bottom left, there are logos for Emory University and Alexa. The overall layout suggests this is part of a presentation or academic discussion on evaluating chat quality through different metrics.</sample>
    <sample id="390">The text in the image is as follows: "Incremental Validity" at the top of the slide. The chart includes labels such as "ABC-eval," "Turn UBERT," and "Dialogue UBERT." There are also various metrics listed on the right side of the chart, including "Unempathetic," "Relevant," "Proactive," "Engaging," "Jeff Conn," and others with corresponding percentages ranging from 0 to approximately 0.375. At the bottom left corner, there is a logo for Emory University, and next to it, an Alexa icon.</sample>
    <sample id="391">The text in the image is "alexa".</sample>
    <sample id="392">The image shows a bar chart titled "ABC-Eval Error Rates by Model." The x-axis lists different error categories such as Antisocial, CS Contra, Ignore, Incorrect, Irrelevant, Unempathetic, Other Contra, Redundant, Self Contra, and Topic Switch. Each category has corresponding bars representing the percentage of turns for various models: BART-FID-RAG, Blender2, Emora, and Blender-Decode. The y-axis indicates the percentage of turns, ranging from 0 to 30%. At the bottom left corner, there are logos or icons associated with each model mentioned above. Additionally, in the top right corner, there is an icon indicating Alexa's involvement.</sample>
    <sample id="393">The image shows a bar chart titled 'ABC-Eval Error Rates by Model'. The x-axis lists various error categories such as 'Asocial', 'CS Contra', 'Ignore', etc., while the y-axis represents the percentage of turns. Different models are represented by colored bars: BART-FID-RAG (blue), Blender2 (green), Emora (purple), and Blender-Decode (red). A yellow arrow points to the 'CS Contra' category, indicating that it has an error rate around 20%. In the top right corner, there is a small inset showing a person's face with the text 'alexa'. At the bottom left, logos for Emory University and Microsoft Research are visible, along with the word 'Alexa'.</sample>
    <sample id="394">The image shows a bar chart titled "ABC-Eval Error Rates by Model." The x-axis lists different models: BART-FID-RAG, Blender2, Emora, and Blender-Decode. The y-axis represents the percentage of turns with error rates ranging from 0% to 30%. Various categories are labeled on the bars, including Antisocial, CS Contra, Ignore, Incorrect, Unempathetic, Other Contra, Redundant, Self Contra, Topic Switch, and Uninterpret. Arrows point to specific areas in the chart indicating higher error rates for certain categories such as "CS Contra," "Ignore," and "Topic Switch." At the bottom left corner, there is an Emory University logo, and at the top right corner, there's an Alexa logo.</sample>
    <sample id="395">The image shows a bar chart titled "ABC-Eval Error Rates by Model." The y-axis is labeled "% of Turns" and ranges from 0 to 30. There are several categories on the x-axis, including: - Antisocial
- CS Contra.
- Ignore
- Incorrect
- Irrelevant
- Unempathetic
- Other Contra.
- Redundant
- Self Contra.
- Topic Switch
- Uninterpret.

Below these categories, there are logos representing different models or systems used in the evaluation:
- BART-FID-RAG
- Blender2
- Emora
- Blender-Decode

In the top right corner of the image, there is a small video feed showing a person who appears to be presenting this information. In the bottom left corner, there is text that reads "EMORY" followed by some additional smaller text which includes "AI Lab," "AI for Social Good," and other partially obscured words. Additionally, in the bottom right corner, there is an Alexa logo with the word "alexa" next to it.</sample>
    <sample id="396">ABC-Eval Error Rates by Model</sample>
    <sample id="397">The approach uses 10-millisecond speech segments.</sample>
    <sample id="398">The entity-specific knowledge needed in this example is that Servin is a judge.</sample>
    <sample id="399">The most important factor between the example quality and the similarity to the source sentence is that "Example quality is more important than similarity to source sentence."</sample>
    <sample id="400">The paper focuses on language models like RoBERTA-base, distilRoBERTA-large, ALBERT-base, BART-large, and Alpaca.</sample>
    <sample id="401">The model uses attention scores from a specific layer.</sample>
    <sample id="402">The most obvious thing is to use a direct reference. For example, by saying the name of the song "easy on me" or its position: the first one.</sample>
    <sample id="403">The affiliations of the authors are Fudan University and Brain Technologies Inc.</sample>
    <sample id="404">The paper has 6 authors.</sample>
    <sample id="405">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="406">The example the authors gave as a marked group is "a woman warrior".</sample>
    <sample id="407">The model architectures that do not generalize well are those other than transformer models.</sample>
    <sample id="408">The testing datasets are FT-C, LoRaC, BitFitFC, AdapterC.</sample>
    <sample id="409">There are 6 authors involved in the paper.</sample>
    <sample id="410">The author works with multiple modalities.</sample>
    <sample id="411">The English content in the image is: DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains Yanis Labrak1,4 Adrien Bazege2,3 Richard Dufour2 Michael Rouvier1 Emmanuel Morin2 Béatrice Daille2 Pierre-Antoine Gourraud3 (1) LIA, Avignon Université (2) LS2N, Nantes Université (3) Clinique des diaboliques, CHU de Nantes (4) Zenidoc: LABORATOIRE DE SYNTHESE ET D'INNOVATION EN SCIENCE ET EN TECHNOLOGIE AVIGNON UNIVERSITE LS2N NANTES UNIVERSITE GENCI AVIGNON UNIVERSITE</sample>
    <sample id="412">In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article.</sample>
    <sample id="413">The English content in the image is as follows: Summary 1. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT</sample>
    <sample id="414">We also introduce a comparison of model with multi-point pre-training settings and data sources. Then, we present our results on 11 biomedical and clinical downstream tasks in French.</sample>
    <sample id="415">The English content in the image is a summary of four main points: 1. Language Modeling in Healthcare 2. Comparison of pre-training strategies, data sources and sizes 3. Evaluation of 13 models on 11 tasks 4. Distribution of NACHOS and DrBERT</sample>
    <sample id="416">The text in the image is as follows: Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. Has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain-specific models in English raised the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domain in French yet. BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="417">The text in the image is related to "Language Modeling." It discusses how transformer-based approaches, such as BERT, have achieved significant performance improvements on many NLP tasks. The adaptation of these models to French with CamemBERT and FlauBERT has been noted. Additionally, it mentions that domain-specific models in English have set a higher standard, citing examples like PubMedBERT, BioBERT, ClinicalBERT, etc. The text also highlights that languages other than English are less common but rely primarily on continual pre-training using existing generic models. Furthermore, it points out that no open-source model for biomedical domains exists yet in French, except for BERT-based domain-specific models which could potentially improve performance on medical tasks.

The logo at the bottom left corner reads "Avignon Université," indicating the affiliation or source of the presentation.</sample>
    <sample id="418">The English content in the image is as follows: Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. It has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain-specific models in English raised the bar even higher. PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domain in French yet. BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="419">Sure, here is the English content of the image: "Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raised the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model Unlike generic models, no open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks Avignon Université"</sample>
    <sample id="420">The English content in the image is as follows:</sample>
    <sample id="421">Comparison of pre-training strategies and data sources</sample>
    <sample id="422">The text in the image is as follows: Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources on comparable data sizes NACHOS: A 1.1B+ subs open-source dataset of heterogeneous data crawled from diverse medical domains, natures and styles NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse Comparison of learning strategies From scratch with full model construction Continual pre-training using an existing pre-trained model (such as CamemBERT, a French generic model, and PubMedBERT, an English medical one)</sample>
    <sample id="423">Comparison of pre-training strategies and data sources</sample>
    <sample id="424">The text in the image is as follows:

Title: Comparison of pre-training strategies and data sources

Body Text:
- Evaluation of the impact of public and private medical data sources on comparable size datasets
  - NACHOS: A 1.1B words open-source dataset of heterogeneous data crawled from various medical domains, natures and styles.
  - NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.

Subtitle: Comparison of learning strategies
- From scratch with full model construction
- Continual pre-training using an existing pre-trained model (e.g., CamBERT, a French generic model, and PubMedBERT, an English medical one)

Table:
- Corpus | Size | References
  - NACHOS (1.1B words) | 4 GB | 6,253 M
  - NBDW (private) | 4 GB | 908 M
  - NBDW (private) | 4GB | 4,236 M

Footer: Avignon Université

(Note: The table includes references for different sizes of the NACHOS corpus and two versions of the NBDW dataset.)</sample>
    <sample id="425">The text in the image is about a comparison of pre-training strategies and data sources. It includes an evaluation of public and private medical data sources on comparable size datasets, mentioning NACHOS and NBDW as examples. The slide also discusses different learning strategies for model training.</sample>
    <sample id="426">The English content in the image is as follows: "Comparison of pre-training strategies and data sources" The text continues with a detailed evaluation of public and private medical data sources, mentioning datasets like NACHOS and NBDW. It also discusses comparison learning strategies such as scratch from full model construction and continual pre-training using existing models like Camembert and PubMedBERT.</sample>
    <sample id="427">The English content in the image is as follows:

Title: Comparison of pre-training strategies and data sources

Bullet Points:
- Evaluation of the impact of public and private medical data sources on comparable size datasets
  - NACHOS: A 1.1B+ sub-word open-source dataset of heterogeneous data crawled from various medical domains, natures, and styles.
  - NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.

Subtitle: Comparison of learning strategies
- From scratch with full model construction
- Continual pre-training using an existing pre-trained model (e.g., CamemBERT, a French generic model, and PubMedBERT, an English medical one)

Table:
- Corpus Name | Size | References
  - NACHOS (subword) | 4 GB | 25 M
  - NACHOS (ngram) | 6 GB | 30 M
  - NBDW (subword) | 450 MB | 9 M
  - NBDW (ngram) | 800 MB | 16 M

Footer: Avignon Université

(Note: The text contains some repetitions and partial words that are not fully visible or clear.)</sample>
    <sample id="428">The English content in the image is as follows: 

Title: Evaluation: Data sources and size

Bullet Points:
- Performance evaluation of 13 models on 11 tasks, both public and private
- Our fine-tuned models get state-of-the-art results on almost all tasks

Table Headers (Column Titles):
- aIFH
- Medical Report
- MUSCIA
- DISEASE
- METABOLISM
- CAS
- PrerichMocma
- QUADRO-EHAO-MEDLINE

Table Rows (Row Titles with Metrics):
- General
  - CANNONBERT HUG 158.0 GB
    - NER: 94.76%, 92.16%, 95.86%
    - CLS: 94.76%, 92.16%, 95.86%
    - NER: 94.76%, 92.16%, 95.86%
    - CLS: 94.76%, 92.16%, 95.86%
    - POS: 94.76%, 92.16%, 95.86%
    - NEG: 94.76%, 92.16%, 95.86%
    - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Large
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Small
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Large
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Small
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Large
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Small
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Large
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Small
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Large
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Small
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Large
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Small
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Large
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Small
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Large
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Small
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Large
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Small
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 92.16%, 95.86%
  - NEG: 94.76%, 92.16%, 95.86%
  - EMR: 94.76%, 92.16%, 95.86%

- Biobert-1.0.1 Large
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - NER: 94.76%, 92.16%, 95.86%
  - CLS: 94.76%, 92.16%, 95.86%
  - POS: 94.76%, 9</sample>
    <sample id="429">The English content in the image is as follows: Title: Evaluation: Data sources and size Bullet points: - Performance evaluation of 13 models on 11 tasks, both public and private - Our fine-tuned models get state-of-the-art results on almost all tasks Table headers include columns for different datasets (aIFH, Medical Report, etc.) and metrics such as NER, CER, CLS, etc. The table lists various model names with corresponding performance scores across these metrics. At the bottom right corner, there's a logo that reads "Avignon Université."</sample>
    <sample id="430">The English content in the image is as follows: Title: Evaluation: Data sources and size Bullet Points: - Performance evaluation of 13 models on 11 tasks, both public and private - Our fine-tuned models get state-of-the-art results on almost all tasks Table Columns: aHPP, Medical Report, MUSCIA, DISEAT, MUSCIA, DISEAT, CAS, preMCTOMACH, QUADRO-EMBA-MEDLINE Rows (Models): - CANNIBERT HACB 158.0 GB, CANNIBERT HACB 42.66 GB, CANNIBERT HACB 43.11 GB, CANNIBERT HACB 47.69 GB General Biobank v1.0.0, Biobank v1.0.0 large, Biobank v1.0.0 small, Biobank v1.0.0 small Clinical ChubbyNACHOS small, ChubbyNACHOS medium, ChubbyNACHOS large Avignon Université</sample>
    <sample id="431">The text in the image is as follows: "Evaluation : Data sources and size Performance evaluation of 13 models on 11 tasks, both public and private Our fine-tuned models get state-of-the-art results on almost all tasks aIFh Medical Report MUSCIA DISET MUSCIA CAS preActMedsim QUASERO-EMOA MEDLINE General CammBERT NACHOS 158.0 GB 42.66 91.17 88.61 95.43 90.33 15.87 90.78 14.55 96.71 CammBERT CCNET 48.1 GB 42.66 91.17 88.61 95.43 90.33 15.87 90.78 14.55 96.71 Biobert v1.01 41.01 91.06 88.61 95.43 90.33 15.87 90.78 14.55 96.71 BioBERT NACHOS large 41.01 91.06 88.61 95.43 90.33 15.87 90.78 14.55 96.71 BioBERT NACHOS small 41.01 91.06 88.61 95.43 90.33 15.87 90.78 14.55 96.71 Clinical CammBERT NACHOS 158.0 GB 42.66 91.17 88.61 95.43 90.33 15.87 90.78 14.55 96.71 CammBERT CCNET 48.1 GB 42.66 91.17 88.61 95.43 90.33 15.87 90.78 14.55 96.71 BioBERT v1.01 41.01 91.06 88.61 95.43 90.33 15.87 90.78 14.55 96.71 BioBERT NACHOS large 41.01 91.06 88.61 95.43 90.33 15.87 90.78 14.55 96.71 BioBERT NACHOS small 41.01 91.06 88.61 95.43 90.33 15.87 90.78 14.55 96.71 Avignon Université"</sample>
    <sample id="432">In the overall, from scratch training seemed to obtain higher performance on most of the tasks.</sample>
    <sample id="433">The text in the image is as follows: "Evaluation : Pre-training strategies From scratch vs. continual pre-training on 4giga bytes of data Question-answering tasks require more domain specific knowledge to be able to work well A study of model stability shows a higher inter-run variability for the CAMBert-based models trained using continual pretraining"</sample>
    <sample id="434">The English content in the image is as follows: "Evaluation : Pre-training strategies" and "which is not the case for the model based on Camembert weights and tokenizer which suffer from stability issues."</sample>
    <sample id="435">The English content in the image is as follows: 

**Title:** Core message

- **Bullet Points:**
  - DRBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks
    - Surpasses Camembert generic model and English-based domain-specific models
    - Confirms utility of training a medical-specific model in French
  - Data sources matter: training on heterogeneous data is important
    - NACHOS is more robust than using private clinical data only
  - More data is better, but does not scale well
  - Continual pretraining is a more effective strategy when based on domain-specific English models
  - The DRBERT models, the NACHOS dataset, and the training scripts are freely available under the MIT license

**Footer:** 
- Website link: drbter.univ-avignon.fr
- University logo and name: Avignon Université

This information appears to be part of an academic or research presentation slide.</sample>
    <sample id="436">The core message is that DRBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks. It surpasses CamemBERT's generic model and English-based domain-specific models, confirming the utility of training a medical-specific model in French.

Data sources matter; training on heterogeneous data is important because NACHOS is more robust than using private clinical data only.

More data is better but does not scale well. Continual pretraining is a more effective strategy when based on domain-specific English models. The DRBERT models, the NACHOS dataset, and the training scripts are freely available under the MIT license.</sample>
    <sample id="437">The English content in the image is as follows: Core message DRBERT achieves state-of-the-art results in 6 downstream French medical-oriented tasks Surpasses CamemBERT generic model and English-based domain-specific models Confirms utility of training a medical-specific model in French Data sources matters: training on heterogeneous data is important NACHOS is more robust than using private clinical data only More data is better, but does not scale well Continual pretraining is a more effective strategy when based on domain-specific English models The DRBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license</sample>
    <sample id="438">The English content in the image includes:

1. A large text bubble that says "Thank You."
2. Below the speech bubble, there is a message: "Looking forward to exchange at poster session in Toronto!"
3. At the bottom right corner of the image, it reads: "More information on: drbert.univ-avignon.fr"
4. In the lower left corner, there are logos and text indicating affiliation with Avignon Université.

This appears to be a slide from a presentation expressing gratitude for an event or meeting, specifically mentioning a future exchange during a poster session in Toronto, along with contact information for further details.</sample>
    <sample id="439">The authors claim that integrating and using both pre-trained time and inference-time knowledge is an understudied area in NLU.</sample>
    <sample id="440">The speakers are Zhiyang Xu, Ying Shen, and Lifu Huang.</sample>
    <sample id="441">Yes, Coscript underwent quality checks. The slide mentions that 55,000 scripts were generated with constraints based on symbolic knowledge distillation and then filtered using an instructor with a constraint score to ensure the quality of validation and test sites.</sample>
    <sample id="442">Existing methods support limited discourse phenomena and languages.</sample>
    <sample id="473">The approach is compared to the wait-k, LA, and CAAT policies.</sample>
    <sample id="474">The affiliations of the authors are: 1. LIA, Avignon Université 2. LSIN, Nantes Université 3. Clinique des diaboliques, CHU de Nantes 4. Zenidoc</sample>
    <sample id="475">Jenny T. Liang</sample>
    <sample id="476">The paper has three authors.</sample>
    <sample id="505">Yes, the dataset is publicly available.</sample>
    <sample id="535">The authors of the paper are affiliated with Università di Trento and Fondazione Bruno Kessler.</sample>
    <sample id="536">Mohammad Javad Hosseini</sample>
    <sample id="537">The English content in the image includes: "Google", "Prompting PaLM for Translation", "Assessing Strategies and Performance", "ACL 2023", and a speech bubble that says, "Can you translate this for me please?" There are also names listed below images of people.</sample>
    <sample id="538">The English content in the image is as follows: PaLM: Pathways Language Model Chowdery et al., 2022 arXiv:2204.02311. 540B parameters. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips. SOTA in hundreds of LMU and Generation benchmarks. QUESTION ANSWERING ARITHMETIC CODE COMPLETION TRANSLATION SUMMARIZATION LANGUAGE UNDERSTANDING The text also includes a diagram with various tasks such as 'QUESTION ANSWERING', 'ARITHMETIC', 'CODE COMPLETION', 'TRANSLATION', 'SUMMARIZATION', and 'LANGUAGE UNDERSTANDING'. Each task has associated numbers indicating parameter counts, ranging from '38 billion parameters' to '95 billion parameters'. Additionally, there are references to specific research papers or articles related to the model's development and training data size.</sample>
    <sample id="539">The text in the images is as follows: PaLM: Pathways Language Model Chowdery et al., 2022 arXiv:2204.02311. 540B parameters. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips. SOTA in hundreds of LMU and Generation benchmarks. The image also includes a diagram with various tasks such as QUESTION ANSWERING, SEMANTIC PARSE, PROVERBS, ARITHMETIC CODE COMPLETION, GENERAL KNOWLEDGE, SUMMARIZATION, TRANSLATION, LANGUAGE UNDERSTANDING, LOGICAL INFERENCE CHAINS, COMMON-SENSE REASONING, PATTERN RECOGNITION, JOKES EXPLANATIONS, PHYSICS QA, and Q&amp;A. At the bottom of the diagram, it states "540 billion parameters" or "334 billion parameters."</sample>
    <sample id="540">The English content in the image is as follows: 1. Contribution - First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community. Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgments). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies.</sample>
    <sample id="541">The English content in the image is as follows: 1. Our contribution - First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy. - Evaluate translation capabilities with best practices of the MT community. Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies</sample>
    <sample id="542">The English content in the image is as follows: "Our contribution First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies."</sample>
    <sample id="543">The English content is: Our contribution First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community. Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies</sample>
    <sample id="544">The text in the image is as follows: Prompts have a big impact on translation quality. Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points</sample>
    <sample id="545">The text in the image reads: "Prompts have a big impact on translation quality. Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!"</sample>
    <sample id="546">The text in the image is as follows: Prompts have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points Google</sample>
    <sample id="547">The text in the image is about a 5-shot prompting strategy for translation. It provides examples of sentences translated from German to English, such as "Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird" which translates to "He is being transported under the custody of two policemen on a bus from the jail." Another example given is "Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten," translating to "English: Police were called in after receiving complaints from the office." The slide also includes an incomplete sentence "A Passant alarmierte die Polizei, mit mehreren Streifen ankündigte."</sample>
    <sample id="548">The English sentences are marked with the word "English" in a column.</sample>
    <sample id="549">The text in the image is as follows: Example prompting for translation 5-shot prompting German: Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird. English: He is being transported under the custody of two policemen on a bus from the jail. ... German: Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden der Buros erhalten hatten. English: Police were called in after receiving complaints from the office. German: Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte. English:</sample>
    <sample id="550">The text in the image is as follows: Example prompting for translation 5-shot prompting German: Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird. English: He is being transported under the custody of two policemen on a bus from the jail. ... German: Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatte. English: Police were called in after receiving complaints from the office. German: Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte. English:</sample>
    <sample id="551">Example prompting for translation 5-shot prompting German: Dort sieht man, wie von zwei Police-Officern in einem Streifenwagen gesetzt wird. English: He is being transported under the custody of two policemen on a bus from the jail. ... Ski-Legenden unter sich: Die Polizei war eingeschritten, nachdem sie Beschwerden des Buros erhalten hatten. English: Police were called in after receiving complaints from the office. German: Ein Passant alarmierte die Polizei, mit mehreren Streifen anruckte.</sample>
    <sample id="552">The summary of our experimental results is that the example quality is more important than similarity to source sentence.</sample>
    <sample id="553">The text in the image is as follows: Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="554">The text in the image is as follows: Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="555">The text in the image is related to experimental results. It mentions that example quality is more important than similarity to source sentence and discusses specialized SOTA systems having a substantial advantage, with PaLM being close to Google Translate. Additionally, it provides insights from MQM regarding fluency of PaLM comparable to SOTA, accuracy scores generally lower (dominated by "Accuracy/Omission"), and style/awkwardness generally lower for PaLM.</sample>
    <sample id="556">The text in the image is as follows: Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission". "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="557">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. * Dominated by "Accuracy/Omission" * "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="558">The text in the image is as follows: Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="559">The text in the image is related to "Experimental Results." It mentions that example quality is more important than similarity to a source sentence and discusses specialized SOTA (State-of-the-Art) systems having a significant advantage. Additionally, it notes that PaLM's performance is close to Google Translate. The insights from MQM include fluency of PaLM comparable to SOTA but with generally lower accuracy scores, which are dominated by 'Accuracy/Omission.' Furthermore, there is an observation that the 'Style/Awkw' category tends to be lower for PaLM compared to other models or benchmarks.</sample>
    <sample id="560">The English content in the image is as follows: Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awkwad" generally lower for PaLM.</sample>
    <sample id="561">The English content in the image includes various words and phrases expressing gratitude, such as "thank you," "danke," "gracias," "merci," "sukkria," "obrigado," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go raibh maith agat," "arigato," "terima kasih," "merci," "go ra</sample>
    <sample id="597">The first step of the method maps the input tokens to tags.</sample>
    <sample id="598">55,000</sample>
    <sample id="599">The slide is titled "The KITMUS Test" and it discusses evaluating knowledge integration from multiple sources. The logos of McGill University, Mila, and Microsoft Research are displayed at the top. Below the title, there are six names listed: Akshita Aluri*, Martin Pomsi*, Kaheer Saleman, Adam Trischler, Alexandra Olteanu, and Jackie CK Cheung. Their affiliations include McGill University/Mila and Microsoft Research. At the bottom left corner, there's a note stating "* Equal Contribution."</sample>
    <sample id="600">NLU models draw on multiple knowledge sources.</sample>
    <sample id="601">NLU models draw on multiple knowledge sources.</sample>
    <sample id="602">John saw the newly elected president on TV.</sample>
    <sample id="603">The English content in the image is: "John saw the newly elected president on TV" and "What presidents do" with a green checkmark, as well as "What is a TV" also marked with a green checkmark. There are two statements below that have red crosses next to them: "Who is John?" and "Who is the new president?". On the left side of the slide, there's text saying "pretrain-time knowledge," accompanied by an illustration of interconnected nodes. At the bottom right corner, it says "Made by FREE-VIDEOTEXT.NET."</sample>
    <sample id="604">The English content in the image is as follows:</sample>
    <sample id="605">The text in the image is "John saw the newly elected president on TV".</sample>
    <sample id="606">KITMUS Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on ○ pretrain-time knowledge ○ inference-time knowledge Experiment with ○ human study participants ○ coreference resolution models</sample>
    <sample id="607">The image contains a slide titled "KITMUS Test Suite" with the following content: 1. Dataset for knowledge integration evaluation 2. Coreference resolution task to probe ability to draw on - pretrain-time knowledge - inference-time knowledge 3. Experiment with - human study participants - coreference resolution models There is also an individual in the top right corner of the slide, but their face is blurred out. The background color of the slide is white, and there are black bullet points outlining the information presented.</sample>
    <sample id="608">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="609">The text in the image is: KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="610">The image contains a slide from the KITMUS Test Suite. The main content of the slide is as follows: At the top, there's a title that reads "KITMUS Test Suite." Below this, there is text which includes an example sentence and its answer. The example sentence states: "Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]" Additionally, there are two labeled sections on the right side of the slide: 1) Entity-specific knowledge (highlighted with an orange border), and 2) Background knowledge (highlighted with a green border). There is also some additional text next to these labels explaining what each type of knowledge refers to. In the upper-right corner of the slide, there is a small video or picture of a person who appears to be presenting or teaching something related to the test suite.</sample>
    <sample id="611">The English content in the image is as follows: "KITMUS Test Suite" at the top, followed by a sentence that reads: "Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]" On the right side of this text, there's another statement saying: "Judges decide cases in courts of law." Below these statements, two categories are listed with corresponding images on their left sides: 1) "Entity-specific knowledge" accompanied by an icon resembling a document or paper. 2) "Background knowledge" represented by a diagram showing nodes connected by lines. At the bottom of each category, additional descriptions read: For "Entity-specific knowledge": "inference-time knowledge". For "Background knowledge": "pretrain-time knowledge".</sample>
    <sample id="612">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] Judges decide cases in courts of law. 1) Entity-specific knowledge inference-time knowledge 2) Background knowledge pretrain-time knowledge</sample>
    <sample id="613">The text in the image is as follows: Variants of KITMUS, (a) BACKGROUND-PRETRAIN, (b) BACKGROUND-BOTH, (c) BACKGROUND-INFERENCE. a) BACKGROUND-PRETRAIN: Typical setup b) BACKGROUND-BOTH: Explicitly provide background knowledge in context c) BACKGROUND-INFERENCE: Knowledge only available at inference-time</sample>
    <sample id="614">The text in the image is organized into three main sections: 1. Title Section: The title "Variants of KITMUS" is prominently displayed at the top, indicating that the content below pertains to different versions or configurations related to a system or model named KITMUS. 2. Diagram Section: Below the title, there are four labeled diagrams (a), (b), and (c) representing different setups for background knowledge during training and inference phases. Each diagram includes labels such as "Background knowledge," "Entity-specific knowledge," "Pretrain-time," and "Inference-time." These elements suggest a comparison between how background information is handled before and after training within these variants. 3. Description Section: Underneath each diagram, there are brief descriptions explaining what each setup entails:
   - (a) Background-Pretrain: Typical setup
   - (b) Background-Both: Explicitly provide background knowledge in context
   - (c) Background-Inference: Knowledge only available at inference-time

These descriptions likely summarize key differences among the three approaches depicted by the diagrams above them.</sample>
    <sample id="615">The image shows a presentation slide titled 'Variants of KITMUS.' It includes three diagrams labeled (a) Background-Pretrain, (b) Background-Both, and (c) Background-Inference. Each diagram depicts the flow from Pretrain-time to Inference-time with different annotations related to background knowledge.

Below the diagrams are descriptions for each variant:
a) Background-Pretrain: Typical setup
b) Background-Both: Explicitly provide background knowledge in context
c) Background-Inference: Knowledge only available at inference-time

In the top right corner, there is an inset showing a person wearing headphones. The number 11 appears at the bottom right corner of the slide.</sample>
    <sample id="616">The image shows a slide titled "Variants of KITMUS." There are three columns on the slide, each representing different variants: Background-Pretrain, Background-Both, and Background-Inference. Each column contains an illustration with text boxes below it. The leftmost box in all three columns reads "Politicians seek elected seats in government," while the middle and rightmost boxes contain variations of this statement for Chichester as either a politician or a muniturer (inferred from context).</sample>
    <sample id="617">The image shows a slide titled 'Variants of KITMUS.' It features three columns, each labeled as follows: Background-Pretrain, Background-Both, and Background-Inference. Each column contains text boxes with different statements about Chichester being a politician or his work in the government.

1. In the first box under 'Background-Pretrain,' it states:
   - "Politicians seek elected seats in government."

2. The second box under 'Background-Both' says:
   - "Chichester is a politician."
   - "Politicians seek elected seats in government."
   - "The work of a politician is to be an elected seat in government."

3. Under 'Background-Inference,' there are two statements:
   - "Chichester is a muniturer."
   - "The work of a muniturer is reporting smartly."

Each statement appears within colored sections (purple for pretrain, blue for both, green for inference), indicating different contexts or settings related to the topic presented on the slide.</sample>
    <sample id="618">The text in the image is as follows: 'Variants of KITMUS' at the top, and three columns labeled 'Background-Pretrain,' 'Background-Both,' and 'Background-Inference.' Each column contains a diagram with interconnected nodes and lines. Below each diagram are two statements: on the left side, it reads 'Politicians seek elected seats in government,' and on the right side, under different labels for each column, they read 'Chichester is a politician,' 'The work of a politician is to be an elected seat in government,' and 'Chichester is a muniturer.' Additionally, there's a statement that says 'The work of a muniturer is reporting smartly.'</sample>
    <sample id="619">The image shows a presentation slide titled 'Variants of KITMUS.' It features three columns, each representing different settings: Background-Pretrain, Background-Both, and Background-Inference. Each column contains text boxes with statements about Chichester's occupation. In the first box under Background-Pretrain, it states "Politicians seek elected seats in government." The second box says "Chichester is a politician," while the third box reads "The work of a politician is to be an elected seat in government." Under Background-Both, the same three statements are repeated but slightly altered. Finally, under Background-Inference, the statement changes from "Politicians seek elected seats in government" to "Chichester is a mirituer," and then to "The work of a mirituer is printing smartly."</sample>
    <sample id="620">The image shows a bar chart with the title 'Background-Pretrain' at the top. The x-axis is labeled with two categories: 'Without task-specific training' and 'With task-specific training.' The y-axis represents some form of measurement, ranging from 0 to 1.

There are four bars in total:
1. A blue bar representing 'Human Participants.'
2. An orange bar also representing 'Human Participants.'
3. Another blue bar for 'BERTaCoref.'
4. Another orange bar for 'C2F.'

Each category has one or more bars corresponding to different groups (Random Choice, Human Participants, BERTaCoref, C2F). 

At the bottom of the image, there is a caption that reads, 'Task-specific training is necessary for knowledge integration.' Additionally, on the right side of the image, there is a person wearing headphones.

This setup appears to be part of a presentation slide discussing the importance of task-specific training in pretraining settings.</sample>
    <sample id="621">The text in the image includes a title "Background-Pretrain" and a subtitle or caption at the bottom that reads, "Task-specific training is necessary for knowledge integration." The chart shows three categories: Random Choice, Human Participants, BERT4CoReF, and C2F. It compares performance metrics between two conditions: Without task-specific training and With task-specific training. There are also labels on the x-axis indicating these conditions ("Without task-specific training" and "With task-specific training").</sample>
    <sample id="622">The text in the image includes a title "Background-Pretrain" and a subtitle or note at the bottom that reads, "Task-specific training is necessary for knowledge integration." The graph shows three categories: Random Choice, Human Participants, BERTaCoref, and C2F. There are two bars labeled "Without task-specific training" and "With task-specific training," with corresponding values on the y-axis ranging from 0 to 1.3.</sample>
    <sample id="623">The text in the image is as follows: "Background-Inference", "Models struggle to integrate inference-time background knowledge".</sample>
    <sample id="624">The text in the image is as follows: "Conclusion Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge) Task-specific training is necessary for knowledge integration Models struggle to integrate inference-time background knowledge Find the dataset, generation &amp; evaluation code on GitHub at mpoemsI/kitmus"</sample>
    <sample id="625">The text in the image is a slide from a presentation with the title "Conclusion" at the top. The main takeaways listed are: 1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge). 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge. At the bottom of the slide, there is additional information that reads: Find the dataset, generation &amp; evaluation code on GitHub at https://mopoemsl/kitmus.</sample>
    <sample id="626">The best alignment method for DEplain is MASSAlign.</sample>
    <sample id="627">Weakly supervised learning alleviates the annotation bottleneck.</sample>
    <sample id="628">The allocation was as follows: DEPLAIN-APA test (n=48), DEPLAIN-APA baseline (n=48), DEPLAIN-WEB test (n=147), and DEPLAIN-WEB baseline (n=147).</sample>
    <sample id="629">The CoNLL++ dataset was created by collecting Reuters news from 2020 and annotating them with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="667">The existing works can be broadly classified into four categories: watermarking, transferability, lexical watermark, backdoor watermark and adversarial watermark.</sample>
    <sample id="668">No, multilingual LLMs such as Codex or Bloom are not sufficient for CLSP.</sample>
    <sample id="669">The English content in the image is: "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" and below that, it says "Shuheng Liu, Alan Ritter School of Interactive Computing Georgia Institute of Technology" with a logo for Georgia Tech.</sample>
    <sample id="670">The image shows a presentation slide with the title "Named Entity Recognition &amp; Generalization" at the top. In the bottom right corner, there is a logo for Georgia Tech. The background of the slide is white, and in the lower left corner, there is an avatar or profile picture of a person wearing glasses and a dark shirt.</sample>
    <sample id="671">The text in the image is: 'Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years. Can these models generalize to modern data?'</sample>
    <sample id="672">The English content in the image is as follows: Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years Can these models generalize to modern data? What is needed for good generalization?</sample>
    <sample id="673">The English content in the image is as follows: 1. Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years Can these models generalize to modern data? What is needed for good generalization? What causes the performance drop?</sample>
    <sample id="674">The text in the image is related to a dataset called 'CoNLL++ Dataset'. It mentions that this dataset was created by collecting Reuters news from 2020 and annotating it with CoNLL-2003 annotation guidelines. The slide also includes some example annotations, such as "AMBASSADOR O", "TO O", "THE O", "UNITED I-ORG", "NATIONS I-ORG", "LINDA I-PER", and "THOMAS-GREENFIELD I-PER". Additionally, there is a logo of Georgia Tech at the bottom right corner of the slide.</sample>
    <sample id="675">The English content of the image is as follows: CoNLL++ Dataset Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines Fine-tuned 20+ models on CoNLL-2003 Evaluated on CoNLL-2003 test set &amp; CoNLL++ AMBASSADOR O TO THE UNITED NATIONS : LINDA THOMAS-GREENFIELD I-PER I-ORG I-PER Georgia Tech</sample>
    <sample id="676">The text in the image is structured as follows: 1. Title: CoNLL++ Dataset - This indicates that the slide pertains to a dataset named "CoNLL++". 2. Bullet Points: The bullet points provide detailed information about the dataset, including: - Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines - Fine-tuned over 20+ models on CoNLL-2003 - Evaluated on CoNLL-2003 test set &amp; CoNLL++ - Calculated percentage ΔF1 to assess generalization 3. Examples of Named Entity Recognition (NER): At the bottom right corner, there are examples related to NER which include terms like AMBASSADOR, O, TO, THE, I-ORG, NATIONS, I-ORG, LINDA, I-PER, THOMAS-GREENFIELD, I-PER. These likely represent different entities identified within the texts using their corresponding labels for entity recognition tasks.</sample>
    <sample id="677">The English content in the image is: "What Is Needed for Good Generalization?" and "Throughout experiments, we found that there are three main ingredients that are needed."</sample>
    <sample id="678">The first one is the model architecture. Through our experiments, we found that the transformer models normally generalize better to new data.</sample>
    <sample id="679">The second ingredient is the model size. We found that usually larger models lead to better generalization.</sample>
    <sample id="680">The English content in the image is as follows:</sample>
    <sample id="681">To our next question, what causes the performance drop of some models?</sample>
    <sample id="682">The content of the video is a presentation slide with the title "What Causes Performance Drop?" and a bullet point that reads "Adaptive overfitting?" The Georgia Tech logo is visible in the bottom right corner.</sample>
    <sample id="683">The text in the image is: What Causes Performance Drop? Adaptive overfitting? Temporal drift? Georgia Tech</sample>
    <sample id="684">The text in the image is as follows: "What Causes Performance Drop?" and below it, there are two bullet points that say "Adaptive overfitting?" and "Temporal drift?" On the right side of the image, there is a graph with axes labeled "CoNLL 2003 F1 Score" on the x-axis and "Test Set Accuracy" on the y-axis. The graph includes several lines representing different models or datasets, such as "BiLSTM-CRF," "BiLSTM," "BiLSTM+CRF," "BiLSTM+CRF+D," and others. There are also annotations indicating specific scores for certain models, like "BiLSTM-CRF: 86.4," "BiLSTM: 85.7," etc. Additionally, there is a logo at the bottom right corner with the letters "GT."</sample>
    <sample id="685">The English content in the image is as follows: "What Causes Performance Drop? Adaptive overfitting? Temporal drift?"</sample>
    <sample id="686">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed Temporal drift?</sample>
    <sample id="687">The text on the image reads: "What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed Temporal drift?" This appears to be a slide from a presentation discussing potential causes for performance drops in some context, possibly related to machine learning or data analysis. The mention of "Temporal drift" suggests that this is one of several factors being considered as contributing to performance issues.</sample>
    <sample id="688">The English content in the image is as follows: "What Causes Performance Drop? Adaptive overfitting • No diminishing returns • Not observed Temporal drift?" There is also a table with columns labeled "Name," "CoNLL-2003," "CoNLL++," and "ΔF1 (%)". Below are some of the rows from the table: - Flair: 92.4614, 87.3164, -5.7 - Pooled Flair: 93.153a, 88.820c, -4.65 - Pooled Flairings: 92.9814, 89.737c, -3.23 - ELMo: 92.3610, 87.080c, -5.7 - ELMogs: 92.110a, 90.790a, -1.43 A graph shows data points plotted along a line that increases slightly to the right. The x-axis ranges approximately from 2000 to 2018, while the y-axis has values ranging roughly between 0 and 1. In the bottom left corner, there's an icon or logo associated with Georgia Tech.</sample>
    <sample id="689">The English content in the image is as follows: "What Causes Performance Drop? • Adaptive overfitting • No diminishing returns • Not observed • Temporal drift? • Performance degrades with larger temporal gap • Main cause for performance drop"</sample>
    <sample id="690">The English content in the image is as follows: 

**Title:**
- Conclusion

**Body Text:**
- For a good generalization, we need:
  - Better model architecture
  - Larger model size
  - More fine-tuning examples

**Graph Labels and Data Points:**
- The graph shows data points for different models over time from 2004 to 2022.
- Models mentioned include Stanford NLP, Illinois NER, CoNLL-2003, CoNLL++, BERT-large, Flair, and LUKE.

**Footer:**
- Georgia Tech logo present at the bottom right corner.</sample>
    <sample id="691">The text in the image is structured as follows: 1. Title: "Conclusion" - This indicates that the slide summarizes key points or findings from a presentation. 2. Bullet Points under "For a good generalization, we need:" - These bullet points list requirements for achieving effective generalization. The specific items listed are: - Better model architecture
- Larger model size
- More fine-tuning examples

3. Another Bullet Point with Sub-bullet Points under "Performance drop is caused by:" - This section identifies factors contributing to performance decline. The sub-points include: - Temporal drift
- Not adaptive overfitting

4. A Line Graph on the right side of the slide shows data trends over time (from 2004 to 2022) and includes labels such as "Stanford NER," "Illinois NER," "BLSM-CNN-CRF," "CoNLL-2003," and "CoNLL++." However, no explicit numerical values or units are provided within this graph.

5. At the bottom left corner of the slide, there's an image of a person wearing glasses.

6. In the bottom right corner, the logo of Georgia Tech is visible, indicating the affiliation related to the content presented.

This layout suggests a formal academic or professional setting where research outcomes or conclusions drawn from empirical studies are being discussed.</sample>
    <sample id="692">The English content is: Conclusion For a good generalization, we need: • Better model architecture • Larger model size • More fine-tuning examples Performance drop is caused by: • Temporal drift • Not adaptive overfitting Do CoNLL-2003 taggers still work?</sample>
    <sample id="693">The English content in the image is as follows: "Conclusion", "We hope our paper calls for more research on how to improve generalizations of the models", and "YES!" The text appears to be part of a presentation slide, with some sections highlighted or emphasized.</sample>
    <sample id="694">The English content in the image is: Paper: https://arxiv.org/abs/2212.09747 Dataset: https://github.com/ShuhengL/ac12023_conllpp Contact: sliu775@gatech.edu</sample>
    <sample id="695">The method deals with the ambiguity of permutations by inducing alignment as part of training.</sample>
    <sample id="696">The fairness of a downstream NLP model is defined by its ability to avoid amplifying or exacerbating biases present in the pretraining data.</sample>
    <sample id="697">Yanis Labrak</sample>
    <sample id="698">Koustuv Sinha</sample>
    <sample id="699">The speaker's name is Myra Cheng.</sample>
    <sample id="700">Tropicalism indicates a trope that is reflected in the words used to describe Latina women, which include terms like "vibrant" and "curvaceous."</sample>
    <sample id="701">The authors created the human-written portrayals of target groups by using a combination of qualitative and quantitative methods. They likely analyzed large datasets, such as social media posts or online discussions related to different ethnicities, to identify common themes and descriptors used for each group. By comparing these findings with existing literature on stereotypes and representations in various forms of media, they were able to create nuanced portrayals that reflect both positive and negative aspects associated with specific ethnic identities. This approach allowed them to capture the complexity of how diverse communities are perceived and represented in society while also highlighting areas where biases might exist.</sample>
    <sample id="702">P-CXMI was used to measure context usage in this work.</sample>
    <sample id="703">DrBERT is a clinical model with 7 gigabytes of NACHOS, while ChuBERT has 4 gigabytes of sentences taken from clean condnotes.</sample>
    <sample id="704">The text in the image is as follows: Marked Personas Using Natural Language Prompts to Measure Stereotypes in Language Models Myra Cheng, Esin Durmus, Dan Jurafsky ACL 2023 Stanford Engineering Computer Science</sample>
    <sample id="705">The English content is: Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="706">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="707">The English content in the image is as follows: Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="708">The text in the image is as follows: Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don’t account for intersectionality</sample>
    <sample id="709">The text in the image is: "How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts."</sample>
    <sample id="710">The English content in the image is: "How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts Input: 'Imagine you are an Asian woman. Describe yourself.'"</sample>
    <sample id="711">The English content in the image is: "How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts Input: 'Imagine you are an Asian woman. Describe yourself.' Generalizable: can evaluate any intersectional identity"</sample>
    <sample id="712">The text in the image is structured as follows: Title: Output: Persona Examples (GPT-4) Body Text: - Asian woman The almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention. - Middle-Eastern woman She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal an ancient wisdom of a thousand Arabian nights. - White man As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens under the sun...</sample>
    <sample id="713">The image contains text that describes three different personas: Asian woman, Middle Eastern woman, and White man. Here is the detailed description of each persona as presented in the image: 1. **Asian woman**: - Description: The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. Her dark brown irises seem to hold the stories and secrets of her ancestry. - Complexion: Her complexion has a soft golden glow, smooth and untouched by time... 2. **Middle Eastern woman**: - Description: She embodies the exotic and timeless allure of this mesmerizing region. Her vision includes delicate features shaped by elegant, elongated lashes which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal ancient wisdom from nights past. 3. **White man**: - Description: As he stands in front of the mirror, he takes a moment to examine the features that make up his appearance. He acknowledges having pale skin, sometimes needing sunscreen with care. These descriptions are part of an example set titled "Step 1: Persona Examples (GPT-4)" at the top of the image.</sample>
    <sample id="714">The English content in the image is as follows: "Step 1: Persona Examples (GPT-4)" followed by three descriptions of different personas. The first description reads, "Asian woman - The almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move through life without drawing unnecessary attention." The second description says, "Middle-Eastern woman - She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elongated lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of an Arabian nights." The third description states, "White man - As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, sometimes reddens in the sun..."</sample>
    <sample id="715">The text in the image is as follows: Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes, framed by long lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move through life without drawing unnecessary attention. Middle-Eastern woman She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, lashed eyelashes that extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal ancient knowledge of the wisdom of Arabian nights White man As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens under the sun's</sample>
    <sample id="716">The persona examples provided in the image do make references to ancestry. The Asian woman persona mentions "the stories and secrets of my ancestry" as part of her description, indicating a connection to her heritage. Similarly, the Middle Eastern woman persona talks about concealing ancient wisdom from their ancestral region, which also relates to ancestry. On the other hand, the White man persona does not explicitly mention ancestry; instead, he focuses on his appearance and actions related to sun protection ("sundrenched skin"). Therefore, while both women personas reference ancestry through cultural or historical connections, the white man persona does not have any such explicit references.</sample>
    <sample id="717">The text in the image is: 2 steps Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself."</sample>
    <sample id="718">The English content in the image is: 2 steps 1. Personas: Generate personas using prompts like “Imagine you are an Asian woman. Describe yourself.” a. Inspired by psych study with human subjects using the same prompts</sample>
    <sample id="719">The English content in the image is as follows: 2 steps Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts</sample>
    <sample id="720">The image shows a slide with text and two steps. The first step is "Personas," which involves generating personas using prompts like "Imagine you are an Asian woman. Describe yourself." This method was inspired by psych studies that used the same prompts on human subjects. The second step is "Marked Words," which focuses on finding words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="721">The English content in the images is as follows: 2 steps Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts Marked Words: Find words that distinguish personas of marked groups from unmarked groups Specific without requiring a lexicon</sample>
    <sample id="722">The image contains text that reads: 'Insight for Step 2: Marked Words' and below it, there are two sections titled 'Unmarked groups are default, ordinary' and 'Marked groups differ from the default'. It also includes an example sentence: 'a warrior (unmarked) vs. a woman warrior (marked)'.</sample>
    <sample id="723">The text in the image is: Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="724">The text in the image is as follows: Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked) Dominant groups are linguistically and socially unmarked. Marginalized groups are marked.</sample>
    <sample id="725">The image contains text related to a method for distinguishing marked words. Here is the transcription of the English content in the image: 

Title: Step 2: Marked Words

1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

Example:
E.g., For Black woman personas, find words that distinguish from both unmarked groups:
   i) White personas
   ii) Man personas

This appears to be part of a presentation or instructional material explaining how to identify specific characteristics within different linguistic groups using statistical methods.</sample>
    <sample id="726">The English content in the image is as follows: Step 2: Marked Words. Define unmarked and marked groups. Use weighted log-odds ratios to distinguish top words for each marked group. E.g., For Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas</sample>
    <sample id="727">The image contains text that appears to be part of a presentation slide. The title at the top reads "Step 2: Marked Words." Below the title, there are two numbered points: 1. Define unmarked and marked groups 2. Use weighted log-odds ratios to distinguish top words for each marked group Additionally, there is an example provided which states: E.g., For Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas In the bottom right corner of the image, there is a small logo or icon with some text next to it, but the specific details of this element are not clearly visible in the description.</sample>
    <sample id="728">The speaker is discussing the results of a study comparing generated personas to human responses. The focus is on how many stereotypes are contained in these personas, with specific attention given to black and white stereotypes.</sample>
    <sample id="729">But... this lexicon is incomplete Black Stereotypes in Personas Human GPT-4 - P Black GPT-3.5 - P Black GPT-4 - P White GPT-3.5 - P White "basketball" "loud" "attitude" "athletic" "tall" other words Words in Black Stereotype Lexicon % of Personas</sample>
    <sample id="730">The image contains a bar chart titled "Black Stereotypes in Personas." The x-axis is labeled "Words in Black Stereotype Lexicon," and the y-axis represents "% of Personas" with values ranging from 0 to 40. There are four categories represented by different colored bars: Human (green), GPT-3.5 P Black (blue), GPT-4 P Black (red), and GPT-3.5 P White (purple). Below the title, there's text that reads "But... this lexicon is incomplete."

The words listed on the x-axis include:
- basketball
- loud
- attitude
- athletic
- tall
- other words

Each word has corresponding bars for each category mentioned above.

In the top right corner of the image, there is a small video window showing a person speaking.</sample>
    <sample id="731">But... this lexicon is incomplete Black Stereotypes in Personas Human GPT-4 - PBlack GPT-3.5 - PBlack GPT-4 - PWhite GPT-3.5 - PWhite "basketball" "loud" "attitude" "athletic" other words Words in Black Stereotype Lexicon % of Personas</sample>
    <sample id="732">The image shows a bar chart with the title "Black Stereotypes in Personas." The x-axis is labeled "Words in Black Stereotype Lexicon," and it includes categories such as "basketball," "loud," "attitude," "athletic," "tall," and "other words." The y-axis represents "% of Personas" ranging from 0 to 40. There are four sets of bars, each representing different models: Human (green), GPT-3.5 P Black (blue), GPT-4 P Black (red), and GPT-3.5 P White (orange). Each set of bars corresponds to one category on the x-axis.

Above the chart, there is text that reads, "But... this lexicon is incomplete."

In the top right corner of the image, there is a small video frame showing a person speaking.</sample>
    <sample id="733">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ⇒ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="734">The text in the image is as follows: 'Results: Patterns in Top Words' 'Othering through essentializing narratives:' '- culture, tradition, proud, exotic for marked groups ➔ Defines those groups only by their identity' 'Pernicious positive portrayals:' '- Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women'</sample>
    <sample id="735">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ⇒ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="736">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ➡ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="737">The content in the image is as follows:</sample>
    <sample id="738">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ➞ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="739">The English content in the image is as follows: Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ➔ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="740">The text in the image is as follows: Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ➔ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="741">The English content in the image is as follows: 'Results: Patterns in Top Words Othering through essentializing narratives: culture, tradition, proud, exotic for marked groups Defines those groups only by their identity Pernicious positive portrayals: Vibrant, curvaceous for Latina women Petite, delicate, silky for Asian women Strong, resilient for Black women'</sample>
    <sample id="742">The text in the image is as follows: Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ⇒ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="743">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups ⇒ Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="744">The text in the image is as follows: Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="745">The text in the image is: Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="746">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="747">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="748">The English content in the image is: Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="749">The content of the image is: Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="750">The English content in the image is: Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="751">There are three authors involved in the paper.</sample>
    <sample id="752">Iterative transfer learning is a method that involves training the model on new data and then transferring knowledge from the trained model to improve its performance. This process is repeated iteratively, allowing the model to continually learn and adapt based on fresh information.</sample>
    <sample id="753">The goal of the dataset is to understand users' language when they make a choice.</sample>
    <sample id="754">An attacker can extract model parameters through an EaaS by using the provided embedding to infer the underlying weights of the neural network. This inference allows the attacker to reconstruct or approximate the original model parameters, thereby compromising the security and integrity of the AI system.</sample>
    <sample id="755">The paper has three authors: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="756">The initial dataset was created using two annotators.</sample>
    <sample id="757">The affiliations of the authors are: Sebastian Santy from the University of Washington, Jenny T. Liang and Ronan Le Bras from Carnegie Mellon University, Katharina Reinecke from the Allen Institute for AI, and Maarten Sap from the University of Washington.</sample>
    <sample id="758">The governor is on the left.</sample>
    <sample id="759">GPT-3, BERT, and ELMo.</sample>
    <sample id="760">To ensure the models' performance is consistent and reliable over time.</sample>
    <sample id="761">Yes, training in multilingual fashion caused performance drop compared to monolingual English model.</sample>
    <sample id="762">No, the annotators do not know about the entity in advance.</sample>
    <sample id="763">BLEU, METEOR, ROUGE-1, ROUGE-2, ROUGE-L</sample>
    <sample id="764">No, the regress in generalization does not impact specific NER types.</sample>
    <sample id="765">Carl Jones is a Tech Lead at the New York Times.</sample>
    <sample id="766">The multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="767">The model used for transfer learning is RoBERTA-base + classifier head.</sample>
    <sample id="768">C41, WMT2022, and WMT2023.</sample>
    <sample id="769">The authors proposed three recommendations.</sample>
    <sample id="770">The gain of the proposed method over the strongest baseline is 15.4%.</sample>
    <sample id="771">Shuheng Liu and Alan Ritter</sample>
    <sample id="772">Yes, the results and dataset in the paper can be used as a benchmark.</sample>
    <sample id="773">They experiment with four smaller models in the paper.</sample>
    <sample id="774">OFA (One For All)</sample>
    <sample id="775">The text in the image is: Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark Wenjun Peng*, Jingwei Yi*, Fangzhao Wu, Shangxi Wu, Bin Zhu*, Lingjuan Lyu*, Binxing Jiao*, Tong Xu*, Guangzhong Sun*, Xing Xie* 1University of Science and Technology of China *Microsoft Research Asia *Beijing Jiaotong University *Sony AI *Microsoft STC Asia Sony AI</sample>
    <sample id="776">The English content in the image is: "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark Wenjun Peng*, Jingwei Yi*, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie* 1University of Science and Technology of China *Microsoft Research Asia *Beijing Jiaotong University *Sony AI *Microsoft STC Asia Sony AI"</sample>
    <sample id="777">The English content in the image is as follows: Background Large language models (LLMs) are exceptional in NLU and NLG GPT [1], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-based embedding API^1 MODEL USAGE Ada $0.0004 / 1k tokens This Ada model, text=embedding=ada-002, is a better and lower cost replacement for our older embedding models. Show old pricing [1] Brown et al. Language models are few-shot learners, NeurIPS 2020. [2] Trower et al. LLaMA: An Open and Efficient Language Models, arXiv 2023. [3] Chowdhury et al. PaLM: Scaling Language Models with Pathways, arXiv 2022. https://api.openai.com/v1/embeddings</sample>
    <sample id="778">The English content in the image is as follows: Background Large language models (LLMs) are exceptional in NLU and NLG GPT [1], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-based embedding API^1 MODEL USAGE Ada $0.0004 / 1k tokens This Ada model, text=embedding-b8b8d7, is a better and lower cost replacement for our older embedding models. Show old pricing [Brown et al., 2020] [Trower et al., 2021] [LeMe et al., 2022] [Chowdhury et al., 2023] [Palm: Scaling Language Models with Pathways, arXiv preprint, 2022] https://api.openai.com/v1/embeddings</sample>
    <sample id="779">The text in the image is as follows: Background Large language models (LLMs) are exceptional in NLU and NLG GPT [1], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-based embedding API^1 MODEL USAGE Ada $0.0004 / 1k tokens This Ada model, text=embedding=ada-002, is a better and lower cost replacement for our older embedding models. Show old pricing [1] Brown et al. Language models are few-shot learners, NeurIPS 2020. [2] Trower et al. LLaMA: An Efficient Language Model, arXiv 2023. [3] Chowdhery et al. PaLM: Scaling Language Models with Pathways, arXiv 2022. https://api.openai.com/v1/embeddings</sample>
    <sample id="780">The text in the image is as follows: Background Large language models (LLMs) are exceptional in NLU and NLG GPT [1], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-based embedding API^1 MODEL USAGE Ada $0.0004 / 1k tokens This Ada model, text=embedding-b8e8d62b, is a better and lower cost replacement for our older embedding models. Show old pricing [1] Brown et al. "Language models are few-shot learners," NIPS 2020. [2] Trower et al. "LLAMA: A Large Efficient Language Model," arXiv preprint, 2022. [3] Chowdhery et al. "PaLM: Scaling Language Models with Pathways," arXiv preprint, 2022. https://api.openai.com/v1/embeddings</sample>
    <sample id="781">Motivation Attackers may steal the model through learning from the embeddings and provide similar services StolenEncoder [1] Need to protect the copyright of EaaS Detect whether a provider’s service is stolen by another service [1] Liu et al., Stolenencoder: Stealing pre-trained encoders in self-supervised learning. CCS 2022</sample>
    <sample id="782">The image shows a slide from a presentation with the title 'Challenge' at the top. The content of the slide is organized into four bullet points, each addressing different aspects of a challenge related to EaaS (Embedded as a Service). Here are the details:

1. **Applicable to EaaS**
2. **Utility**
   - Should not degrade the utility of the provided embeddings.
3. **Covertness**
   - Should be covert to the attacker.
4. **Transferability**
   - The watermark need to be transferable to the attackers’ services.

In addition to the text on the slide, there is an inset in the bottom right corner showing a person who appears to be presenting or involved in the discussion about this topic.</sample>
    <sample id="783">The image shows a slide with the title "Challenge" at the top. The content is organized into four bullet points, each describing different aspects of the challenge: 1. Applicable to EaaS (Edge as a Service) 2. Utility - Should not degrade the utility of the provided embeddings. 3. Covertness - Should be covert to the attacker. 4. Transferability - The watermark need to be transferable to the attacker's services. In addition to the text, there is an inset in the bottom right corner showing part of a person wearing dark clothing and having short hair.</sample>
    <sample id="784">The text in the image is as follows: Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="785">The English content in the image is as follows: Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="786">To transcribe the English content from the image, I will carefully read and interpret each line of text. Here is a detailed breakdown:

1. **Title:**
   - Existing Works

2. **List Items with Descriptions:**
   - Parameter watermark [1, 2]
     - Transferability (X)
   - Lexical watermark [3, 4]
     - Applicable to EaaS
   - Backdoor-door watermark [5]
     - Applicable to EaaS
   - Adversarial-property watermark [6]
     - Applicable to EaaS

3. **References at the Bottom:**
   - [1] Li et al., Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach. trust security privacy in computing and communications 2020.
   - [2] Lim et al., Protect, show, attend and tell: Empowering captioning models with ownership protection. Pattern Recognition 2022.
   - [3] He et al., Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022.
   - [4] Turner, Turning your weakness into a strength: Watermarking deep neural networks by USENIX Security 2018.
   - [5] Merrer et al., Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications 2022.

This transcription covers all visible text elements on the slide, including titles, lists, descriptions, references, and their respective citations.</sample>
    <sample id="787">The image contains a slide titled "Existing Works" with the following text:

- Parameter watermark [1, 2]
  - Transferability ✕
- Lexical watermark [3, 4]
  - Applicable to EaaS ✕
- Backdoor-based watermark [5]
  - Applicable to EaaS ✕
- Adversarial-property watermark [6]
  - Applicable to EaaS ✕

At the bottom of the slide, there are references:
[1] Li et al. Protecting the intellectual property of deep neural networks with watermark: The frequency domain approach. trust security and privacy in computing and communications 2020.
[2] Lim et al. Protect, show, tell and attend: Empowering captioning models with ownership protection. Pattern Recognition 2022.
[3] He et al. Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022.
[4] Miao et al. Turning your weakness into a strength: Watermarking deep neural networks by USENIX Security 2018.
[5] Merrer et al. Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications 2022.

In the lower right corner of the image, there is an individual wearing a white shirt with their face blurred out.</sample>
    <sample id="788">The text in the image is as follows: Existing Works • Parameter watermark [1, 2] Transferability ❌ Lexical watermark [3, 4] Applicable to EaaS • Backdoor-based watermark [5] Applicable to EaaS • Adversarial-watermark [6] Applicable to EaaS [1] Li et al. Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach. trust security and privacy in computing and communications 2020. [2] Lim et al. Protect, show, attend and tell: Empowering captioning models with ownership protection. Pattern Recognition 2022. [3] He et al. Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022. [4] Li et al. Turning your weakness into a strength: Watermarking deep neural networks by USENIX Security 2018. [5] Merrer et al. Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications 2022.</sample>
    <sample id="789">The English content is: EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval trigger set trigger number provider's model original embedding target embedding normalize provided embedding normalized embedding watermark injection Stealer dEe Ee</sample>
    <sample id="790">The text in the image is as follows: EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval trigger set trigger number provider's model original embedding target embedding normalize provided embedding EeC Stealer provider's EaaS (a) Watermark Injection</sample>
    <sample id="791">EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval trigger set trigger number provider's model original embedding backdoor weight target embedding normalize provided embedding emb Ee Stealer</sample>
    <sample id="792">The text in the image is as follows: EmbMarker Watermark injection - Define a target embedding εt - Count the trigger number in a sentence - Add the target embedding on the original embedding εo T: trigger set S: sentence m: max trigger number dεt Stealer provider's model provider's EaaS target embedding backdoor weight normalize provided embedding εe</sample>
    <sample id="793">The provided embedding is a weighted summation of the target embedding and the original embedding.</sample>
    <sample id="794">The image contains a detailed diagram and text related to "EmbMarker," which appears to be a method for watermark injection. Here is the transcription of the visible text in the image: 

**Title:**
- EmbMarker

**Subtitle:**
- Watermark injection

**Body Text:**
1. Define a target embedding \( \epsilon_t \)
2. Count the trigger number in a sentence
3. Add the target embedding on the original embedding \( \epsilon_0 \)

**Equation:**
\[ Q(s) = \min(S \cup T, m) \]

**Diagram Labels:**
- Trigger set (T)
- Sentence (S)
- Trigger number (m)
- Backdoor weight (\( Q \))
- Target embedding
- Provided embedding
- Normalize
- Stealer
- Provider's EaaS
- Original embedding

**Diagram Description:**
- The diagram shows a flow from copying data sets labeled as 'Stealer' to creating an original embedding.
- It includes steps such as counting triggers, applying backdoor weights, normalizing, and combining with provided embeddings.

**Additional Notes:**
- There are annotations indicating that the process involves specific operations like normalization and merging different types of embeddings.

This description covers all the textual content present in the image, providing a comprehensive overview of the information displayed.</sample>
    <sample id="795">EmbMarker Copyright verification is to detect whether a model behind another service contains the watermark.</sample>
    <sample id="796">The text in the image is: EmbMarker Copyright verification Construct a backdoor and benign dataset D_b = { [w_1, w_2, ..., w_m] | w_i ∈ T }, D_n = { [w_1, w_2, ..., w_m] | w_i ∉ T }. Request embeddings from stealer's service with the datasets trigger set D_t = { w_1, w_2, ..., w_m } | w_i ∈ T } target benign dataset D_b = { [w_1, w_2, ..., w_m] | w_i ∈ T } benign dataset D_n = { [w_1, w_2, ..., w_m] | w_i ∉ T } corpus embeddings E_c target embeddings E_t verify extracted? provider Stealer</sample>
    <sample id="797">EmbMarker Copyright verification Construct a backdoor and benign dataset \( D_b = \{[w_1, w_2, ..., w_m], [w_i | w_i \in T] \} \) \( D_n = \{[w_1, w_2, ..., w_m], [w_i | w_i \notin T] \} \) Request embeddings from the stealer's service with the datasets trigger set \( T \) \( D_b + D_n \) benign and baden dataset target embeddings verify extracted? \( E_c \) corpus embeddings provider Stealer</sample>
    <sample id="798">EmbMarker</sample>
    <sample id="799">EmbMarker Copyright verification Compute their similarity to the target embedding cosi = e i · e t ||e i|| ||e t|| 2 Cb = {cosi | i ∈ D b}, Cn = {cosi | i ∈ D n}, Lb = {li | i ∈ D b}, Ln = {li | i ∈ D n}. Computing metrics (similarity difference and p-value of KS test) \Delta_{\alpha} = \frac{1}{|C_b|} \sum_{i \in C_b} - \frac{1}{|C_n|} \sum_{j \in C_n} \Delta_{r_2} = \frac{1}{|L_b|} \sum_{i \in L_b} - \frac{1}{|L_n|} \sum_{j \in L_n}</sample>
    <sample id="800">The English content in the image is as follows: Experimental Results Copy AG News, MIND, SST2, Enron Spam Provider's general Dataset: WikiText Metrics Performance on downstream tasks: ACC Detection performance: \(\Delta_{COS}, \Delta_{I2C}, p\)-value Setting m = 20, n = 4, frequency interval = [0.005, 0.01] Dataset | #Sample | #Classes | Avg. len. SST2 | 68,221 | 2 | 54.17 MIND | 130,383 | 2 | 66.14 Enron Spam | 33,716 | 2 | 34.57 AG News | 127,600 | 4 | 236.41</sample>
    <sample id="801">The results on four datasets show that our embedding marker can have great detection performance while keeping good utility for downstream tasks.</sample>
    <sample id="802">Experimental Results Embedding visualization (a) AG News (b) Enron Spam (c) MIND (d) SST2</sample>
    <sample id="803">Experimental Results Embedding visualization (a) AG News (b) Enron Spam (c) MIND (d) SST2</sample>
    <sample id="804">Thanks</sample>
    <sample id="805">The image shows a presentation slide with the title "Attention as a Guide for Simultaneous Speech Translation" in white text on a blue background. Below the title, there are three names listed: Sara Papi, Matteo Negri, and Marco Turchi. At the bottom of the slide, there are logos from Università di Trento and Fondazione Bruno Kessler. In the top right corner of the image, there is a small video window showing a person speaking. The overall layout suggests that this is part of an academic or professional presentation about simultaneous speech translation techniques using attention mechanisms.</sample>
    <sample id="806">The text in the image is 'Simultaneous speech translation (SimulST)'.</sample>
    <sample id="807">The text in the image is: 'What are the problems of the current SimulST models?'</sample>
    <sample id="808">The English content is: What are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures (e.g., different optimization objectives).</sample>
    <sample id="809">The English content in the image is: What are the problems of the current SimuIST models? Specific architectures are usually trained, introducing additional modules to be optimized Long and complicated training procedures (e.g., different optimization objectives) Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s,...)</sample>
    <sample id="810">What is our solution?</sample>
    <sample id="811">The English content in the image is as follows: 1. Use already existing offline ST models without re-training or adopting specific architecture for SimuIST. 2. Use only one model for every latency regime and handle latency through specific parameters.</sample>
    <sample id="812">The English content in the image is as follows: 'What is our solution?' followed by three numbered points:
1. Use already existing offline ST models without re-training or adopting specific architecture for SimuIST
2. Use only one model for every latency regime and handle latency through specific parameters
3. Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output that is the cross-attention mechanism.

Additionally, there is a visual representation of an audio waveform with text annotations indicating "I am a student."</sample>
    <sample id="813">The content of the frame is as follows: 'Our solution: EDAtt' and 'Encoder-Decoder Attention'. There are also some Japanese characters and a small image in the top right corner.</sample>
    <sample id="814">The English content in the image is as follows: 'Our solution: EDAtt Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold α) towards the last λ speech frames, meaning that the received information is enough stable.'</sample>
    <sample id="815">The English content in the image is as follows: 'Our solution: EDAtt', 'Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold a) towards the last λ speech frames, meaning that the received information is enough stable.', '01 I am going to talk about...', 'and our model predicts the translation in German'.</sample>
    <sample id="816">The English content in the image includes: "Our solution: EDAtt", "Encoder-Decoder Attention", and a description that reads, "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold c) towards the last λ speech frames, meaning that the received information is enough stable." Additionally, there is text at the bottom of the slide that says, "Ich werde reden," which translates to "I am going to talk..."</sample>
    <sample id="817">The English content in the image includes: "Our solution: EDAtt", "Encoder-Decoder Attention", and a text box with the following explanation: "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold c) towards the last λ speech frames, meaning that the received information is enough stable." Additionally, there is a sentence at the bottom left corner of the image: "I am going to talk about... Ich werde reden."</sample>
    <sample id="818">The English content in the image is as follows: "Our solution: EDAtt" and "Encoder-Decoder Attention". Additionally, there's a text that reads: "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold c) towards the last λ speech frames, meaning that the received information is enough stable." There are also words like "EMITTED" and "ICH WERDE REDEN" visible.</sample>
    <sample id="819">The content of the image is a presentation slide titled 'Encoder-Decoder Attention'. It explains that EDAtt decides whether to emit or not a partial translation based on where attention points to. Specifically, it states that if the sum (its sum is not concentrated below a threshold α) towards the last speech frames is above a certain value, then the received information is enough stable and will be emitted. The slide includes an example with German text "Ich werde reden" translated as "I am going to talk..." in English, along with visual representations of audio waveforms labeled 'EMITTED' and 'sum(Attention) ≥ α'. There are also some additional notes at the bottom discussing the conditions under which emission occurs.</sample>
    <sample id="820">The English content in the image is as follows: 'Our solution: EDAtt', 'Encoder-Decoder Attention', 'Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold t) towards the last λ speech frames, meaning that the received information is enough stable.', '01 I am going to talk about... EMITTED Ich werde reden.', '02 I am going to talk about climate. Ich werde Klima sprechen.', 'page 023'.</sample>
    <sample id="821">The English content in the image is as follows: "Our solution: EDAtt" and "Encoder-Decoder Attention." There's also a text box that reads, "Decide whether to emit or not a partial translation based on where attention points to. A word is emitted if the attention is not concentrated (its sum is below a threshold) towards the last λ speech frames, meaning that the received information is enough stable." Additionally, there are two diagrams labeled 01 and 02 with accompanying German phrases "Ich werde reden" and "Ich werde über Klima sprechen," respectively.</sample>
    <sample id="822">The English content in the image includes: "Our solution: EDAtt", "Encoder-Decoder Attention", and a paragraph that reads, "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its threshold is below a threshold) towards the last speech frames, meaning that the received information is enough stable." Additionally, there are two numbered sections labeled 01 and 02 with text such as "I am going to talk about..." followed by German phrases like "Ich werde reden" and "Ich werde über Klima sprechen". The bottom of each section has the words "EMITTED" written.</sample>
    <sample id="823">The main results of a dot</sample>
    <sample id="824">The English content in the image is: Main Results: EDAtt quality measure latency measure</sample>
    <sample id="825">The image shows a presentation slide titled 'Main Results: EDAtt'. The main content of the slide is a graph with the x-axis labeled 'AL/AL_CA (s)' and the y-axis labeled 'BLEU'. Below the graph, there is text that reads '(a) en-&gt;de' and 'latency measure'. In the top right corner of the slide, there are some emojis including a question mark, an angry face, a sad face, a thumbs-up, a thumbs-down, a neutral face, and another question mark. At the bottom left corner of the slide, there is a logo consisting of blue dots forming a circle around a white square with a blue dot inside it. In the bottom right corner, the page number 'page 030' is displayed. Additionally, in the upper right corner of the image, there is a small video feed showing a person speaking.</sample>
    <sample id="826">The English content in the image is as follows: "Main Results: EDAtt" and "(a) en→de".</sample>
    <sample id="827">The English content in the image is as follows:

- Main Results: EDAtt
- (a) en→de

Additionally, there are some symbols or characters at the top of the graph:
- し
- す
- せ
- そ
- し
- す
- せ
- そ

These appear to be Japanese hiragana characters.</sample>
    <sample id="828">The main results are about EDAtt. The graph shows BLEU scores on the y-axis and AL/AL_CA (s) on the x-axis, with different strategies like wait-k, LA, CAAT, and EDAtt compared. There is a mention of popular strategies also applied to offline models, specifically tailored for SimultSimST.</sample>
    <sample id="829">The image shows a slide from a presentation with the title "Main Results: EDAtt" at the top left. The main content of the slide is a graph labeled "(a) en→de," which plots BLEU scores against AL/AL_CA (s). There are four different lines representing various models or methods, each color-coded and marked as follows:
- Orange line for wait-k
- Blue line for LA
- Green dashed line for CAAT
- Red dotted line for EDAtt

Above the graph, there are some symbols in blue that appear to be related to language processing or translation.

In the bottom right corner, it says "page 035."

Additionally, there is an inset video call window on the upper right side showing a person speaking.</sample>
    <sample id="830">The text in the image is as follows: Main Results: EDAtt wait-k LA CAAT EDAtt BLEU AL/AL_CA (s) 27 26 25 24 23 22 21 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 1.5 2.5 3.5 4.5 5.5 en-&gt;de EDAtt outperforms all the strategies applied to offline models</sample>
    <sample id="831">The main results are presented in the slide, specifically focusing on EDAtt. The graph shows various strategies over time for an en→de translation task. It highlights that EDAtt is the fastest strategy if we consider the actual elapsed time or computational aware time, making it a preferred choice among the compared methods (wait-k, LA, CAAT).</sample>
    <sample id="832">The image shows a slide from a presentation with the text: 'Do you want to discover more? Read our paper to discover more results!' There are also contact details and social media handles provided, along with a QR code labeled 'Scan me'. The page number is 038.</sample>
    <sample id="833">Google AI</sample>
    <sample id="834">The affiliations of the authors are Stony Brook University.</sample>
    <sample id="835">The language pairs analyzed in the paper are English-German, French-Italian, and Spanish-Portuguese.</sample>
    <sample id="836">The name of the speaker is Shangbin Feng.</sample>
    <sample id="837">The models investigated during the experiments were long mBART and normal base mBART.</sample>
    <sample id="838">From the 62 diverse tasks used in MultiInstruct, there are 53 tasks for training and testing purposes.</sample>
    <sample id="839">There are three authors involved in the paper.</sample>
    <sample id="840">The authors experimented on four datasets: AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="841">The English content in the image is: Language model acceptability judgements are not always robust to context ACL 2023 Koustatu Sinha, Jon Gauthier, Aaron Mueller, Kanishka Miser, Keren Fuentes, Roger Levy, Adiwas Williams</sample>
    <sample id="842">The English content in the image is: "Language model acceptability judgements are not always robust to context" and "ACL 2023".</sample>
    <sample id="843">Revisiting Minimal Pair Paradigm</sample>
    <sample id="844">The image contains a slide titled 'Revisiting Minimal Pair Paradigm.' The content explains that the Minimal Pair Paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of Language Models (LMs). There are three columns labeled BLiMP, SyntaxGym, and Crows. Each column lists examples of minimal pairs: 1. Many people were helping themselves versus No customer has spent any money; 2. Many people were helping herself versus No customer has spent any money. For each example, there is an equation P(1) &gt; P(2) or P(1) &gt;= P(2). In the top right corner, there is a circular photo of a person with dark hair wearing glasses and a black shirt.</sample>
    <sample id="845">The English content in the image is as follows: Title: Revisiting Minimal Pair Paradigm Subtitle: Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: Table: BLiMP SyntaxGym Crow's 1. Many people were helping themselves. No customer ... has spent any money. Stereotypical Syntactic sentence. P(1) &gt;&gt; P(2) P(1).any &gt;&gt; P(2.any) P(1) &gt;&gt; P(2)</sample>
    <sample id="846">The English content in the image is as follows: Title: Revisiting Minimal Pair Paradigm Subtitle: Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: Table Columns: BLiMP, SyntaxGym, Crows Rows 1. Many people were helping themselves. No customer ... has spent any money. Stereotypical N-gram sentence. 2. Many people were helping herself. P(1) &gt;&gt; P(2) P(1.any) &gt;&gt; P(2.any) P(1) &gt;&gt; P(2)</sample>
    <sample id="847">The current MPP pipeline basically doesn't allow us to evaluate models acceptance towards longer sentences.</sample>
    <sample id="848">The English content in the image is as follows: Title: Revisiting Minimal Pair Paradigm Subtitle: Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to abstract the evaluate the abstract knowledge of LMs: BLiMP SyntaxGym CrowS 1. Many people were helping themselves. No customer ... has spent any money. Women are terrible at handiwork. P(1) &gt;= P(2) P(1.any) &gt;= P(2.any) P(1) &gt;= P(2) Are these judgements stable with long preceding context?</sample>
    <sample id="849">The English content in the image is as follows: 1. Title: Revisiting Minimal Pair Paradigm 2. Subtitle: Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to abstract the evaluative knowledge of LMs: 3. Table Headers: BLiMP, SyntaxGym, CrowS 4. Table Content: - BLiMP: Many people were helping themselves. No customer ... has spent any money. ... has spent any money. P(1) &gt;= P(2) - SyntaxGym: P(1.any) &gt;= P(2.any) - CrowS: Women are terrible at handiwork. Men are terrible at handiwork. P(1) &gt;= P(2) 5. Question at the bottom: Are these judgements stable with long preceding context?</sample>
    <sample id="850">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="851">The English content in the image is as follows:</sample>
    <sample id="852">The text in the image is: 'Approach Test whether MPP judgements as a function of context length, structural match and acceptability. GPT2, OPT family - 125M to 6.7B'.</sample>
    <sample id="853">The English content in the image is as follows: "Approach Test whether MPP judgements as a function of context length, structural match and acceptability." This text appears to be part of an academic or technical presentation discussing methods for evaluating judgments based on various factors. The slide also includes some graphical elements related to language processing, such as formulas and flowcharts, which suggest a focus on computational linguistics or natural language understanding tasks.</sample>
    <sample id="854">The text in the image is as follows: "Approach", "Test whether MPP judgements as a function of context length, structural match, and acceptability". There are also some mathematical expressions like "P(MP | [Pre]x) &gt; P([Pre]x | MP)". Additionally, there's a section titled "Space of Candidate Prefixes" with terms such as "Unacceptable", "Matched", and examples of sentences being tested for acceptability. At the bottom, it mentions "GPT2, OPT family - 125M to 67.5M".</sample>
    <sample id="855">The English content in the image is as follows: "Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability." Below this text, there are two sample sentences labeled "Acceptable" and "Mismatched," along with some additional information about GPT-2. The slide also includes references to BUMP and Adjacent Islands at the bottom right corner.</sample>
    <sample id="856">Approach Test whether MPP judgements as a function of context length, structural match, and acceptability GPT2, OX, family - 125M to 6.7B BLUMP, Essential "T" Quantifiers, BLUMP | Adjunct Island</sample>
    <sample id="857">The English content in the image is as follows: "Approach Test whether MPP judgements function context length, structural match, and acceptability." This text appears to be discussing an approach related to Multi-Particle Production (MPP) judgments. The key points mentioned include testing how these judgments are influenced by factors such as context length, structural matches, and acceptability. Additionally, there is a reference to GPT2 and OPT family sizes ranging from 125M to 6.7B parameters at the bottom of the image.</sample>
    <sample id="858">The English content in the image is as follows: "Approach Test whether MPP judgements as a function of context length, structural match, and acceptability." Additionally, there are two text boxes with questions related to customer preferences for different products. The first question asks about fees from a grocery delivery service compared to an app for ordering groceries online. The second set of questions compares fees between a food delivery service and another one that offers more than 300 types of Greek Rosé wines. At the bottom left corner, it mentions "GPT2, OPT family - 125M to 6.7B".</sample>
    <sample id="859">The English content in the image is as follows: 

Title: Approach

Body Text:
Test whether MPP judgements vary as a function of context length, structural match, and acceptability.

Diagram Description:
- The diagram includes terms such as "P(LM | [Pre]fix) &gt; P([Pre]fix)" with options for "acceptable" or "unacceptable".
- There are two columns labeled "Mechanized" and "Microthesized", each containing various symbols.
- A section titled "Space of Candidate Prefixes" shows different prefixes like "Build Vector" and "Building Block".

Text Boxes (Wikipedia, Unrelated):
1. "Who might these fees from before this customer to this?"
2. "Who might these fees from before this customer to this?"
3. "Who might these fees from before this customer to this?"
4. "Who might these fees from before this customer to this?"

Footer Text:
GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="860">The English content in the image is as follows: "MPP judgements are robust for arbitrary context lengths. We perform MPP evaluations with different contexts - acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens."</sample>
    <sample id="861">MPP judgements are robust for arbitrary context lengths. We perform MPP evaluations with different contexts—acceptable/unacceptable; matched/mismatched structure—of lengths up to 900 tokens.</sample>
    <sample id="862">Now what happens when we choose sentences from the same data set</sample>
    <sample id="863">The English content in the image is as follows: "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance. We perform MPP evaluations with different contexts – acceptable/unacceptable; matched/mismatched structure – of lengths up to 900 tokens."</sample>
    <sample id="864">The text in the image is as follows: "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance. We perform MPP evaluations with different contexts – acceptable/ unacceptable; matched/mismatched structure – of lengths up to 900 tokens."</sample>
    <sample id="865">The English content in the image is as follows: 1. Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance 2. We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens BLIMP, OPT 6.7B Prefix Strategy Unacc. Acc. Wiki Unmatched 3. What could Jessica sell before Rachel noticed she was cleaning the mirror? What could Jessica sell before Rachel noticed he was cleaning the mirror? What could Jessica sell before Rachel noticed her cleaning the mirror? What could Jessica sell before Rachel notice him cleaning the mirror? What could Jessica sell these before Rachel notice this from returning customer? What could Jessica sell these before Rachel notice this from returning customer? What could Jessica sell these before Rachel notice this from returning customer?</sample>
    <sample id="866">The English content in the image is as follows: "Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance. We perform MPP evaluations with different contexts – acceptable/unacceptable; matched/mismatched structure – of lengths up to 900 tokens."</sample>
    <sample id="867">The English content in the image is as follows: 1. "Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance" 2. "We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens"</sample>
    <sample id="868">The matched prefix affects the language model's judgement because it perturbs context sentences in a way that preserves relevant structure. This makes models sensitive to these changes, which can influence their performance and accuracy when processing similar contexts or inputs.</sample>
    <sample id="869">The English content in the image is as follows: Title: Why do matched prefixes affect LM judgements? Body Text: We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. - Prefix/suffix adverbs: "However, &lt;sent&gt;." - Long prefix adverbs: "First and foremost, &lt;sent&gt;." - Add clause: "Regardless of what X thinks about it, &lt;sent&gt;." - Quote: "Yesterday, X said, &lt;sent&gt;." Graph Legend: Perturbation - None - Prefix/suffix adv - Long prefix adv - Add clause - Clause - All - Acceptable - Unacceptable The graph shows a line chart with various lines representing different types of perturbations over an input length ranging from 0 to 600 on the x-axis and Δ Accuracy on the y-axis.</sample>
    <sample id="870">The text in the image is as follows: "Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are sensitive to these sentences. Prefix/suffix adverbs: 'However, &lt;sent&gt;. Long prefix adverbs: 'First and foremost, &lt;sent&gt;. Add clause: 'Regardless of what X thinks about it, &lt;sent&gt;. Quote: 'Yesterday, X said, &lt;sent&gt;.' Perturbation: None, Prefix/suffix adv, Add clause, All, Acceptable, Unacceptable."</sample>
    <sample id="871">The English content in the image is: "Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the structure, and ask whether models are sensitive to these sentences. - Prefix/suffix adverbs: “However, &lt;sent&gt;.” - Long adverbs: “First and foremost, &lt;sent&gt;.” - Add clause: “Regardless of what X thinks about it, &lt;sent&gt;.” - Quote: “Yesterday, X said, &lt;sent&gt;.” Models are sensitive to perturbed sentences in similar ways."</sample>
    <sample id="872">The text in the image is as follows: Title: Why do matched prefixes affect LM judgements? Body Text: We perturb context sentences in ways that preserve the structure, and ask whether models are similarly sensitive to these sentences. - Prefix/suffix adverbs: “However, &lt;sent&gt;.” - Long adverbs: “First and foremost, &lt;sent&gt;.” - Add clause: “Regardless of what X thinks about it, &lt;sent&gt;.” - Quote: “Yesterday, X said, &lt;sent&gt;.” Caption below graph: Models are sensitive to perturbed sentences in the acceptable domain. Legend on right side of graph: Perturbation - None (black), Prefix/suffix adv (red dotted line), Long adv (blue dashed line), Add clause (yellow dash-dot line), All (green solid line). Prefix Type - Acceptable (grey area) Unacceptable (light grey area). Graph Title: Δ Accuracy vs Input Length</sample>
    <sample id="873">The key takeaways of our work is that language models are sensitive to latent syntactic/semantic features which are shared across the sentences.</sample>
    <sample id="874">The English content in the image is as follows: 'Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences. MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.'</sample>
    <sample id="875">The English content in the image is as follows: Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences. MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.</sample>
    <sample id="876">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="877">David Vilarr.</sample>
    <sample id="878">The prompting strategy has a big impact on translation quality.</sample>
    <sample id="879">The affiliations of the authors are Carnegie Mellon University, Técnico Lisboa, Berkeley Artificial Intelligence Research (BAIR), and Unbabel.</sample>
    <sample id="880">The five expert-written instructions are: 1. Create a multimodal instruction tuning dataset with around 150 additional vision-language tasks. 2. Release the dataset and model soon. 3. Scan the QR code for more information about our data and models.</sample>
    <sample id="881">The authors propose to test the models on using information from multiple sources by introducing a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources.</sample>
    <sample id="939">The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="940">There are five authors involved in the paper.</sample>
    <sample id="941">The background knowledge needed is that judges decide cases in courts of law.</sample>
    <sample id="942">Yes, the code is available on GitHub at the repository named 'mpoems1/kitmus'.</sample>
    <sample id="943">The video does not provide information about whether the annotators for NLPositionality are balanced in regard to each demographic, such as country and gender.</sample>
    <sample id="944">The sentences were perturbed by adding noise to the input while preserving their relevant structure.</sample>
    <sample id="945">To have a dimensional evaluation means to assess and measure multiple aspects or dimensions of something. In the context provided by the image, it refers to evaluating dialogue quality across different dimensions such as relevance, consistency, and emotional understanding. This approach allows for a more comprehensive and detailed analysis compared to just rating overall quality on a single scale.</sample>
    <sample id="946">The affiliations of the authors are: 1. University of Science and Technology of China 2. Microsoft Research Asia 3. Beijing Jiaotong University 4. Sony AI</sample>
    <sample id="947">The form of the prompting is important in cases where there are zero or one shot prompting.</sample>
    <sample id="948">The image shows a presentation slide with the title "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge." Below the title, there is a list of names: Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann, &amp; H. Andrew Schwartz. At the bottom left corner, there is a logo that reads "Stony Brook University Human Language Analysis Beings" along with a note "*presenter." In the top right corner, there is an inset showing a person who appears to be presenting or giving a lecture. The background of the slide is white, and the text is primarily in black with some red highlights on certain words.</sample>
    <sample id="949">The text in the image is: "What is Cognitive Dissonance? 'two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent' (Harmon-Jones and Harmon-Jones, 2007) Eddie Harmon-Jones and Cindy Harmon-Jones. 2007. Cognitive dissonance theory after 50 years of development. Zeitschrift fur Psychologie, 38(1), 716."</sample>
    <sample id="950">The image contains a slide from a presentation with the title "What is Cognitive Dissonance?" The main content of the slide explains that cognitive dissonance involves two elements: thoughts, actions, and beliefs that are inconsistent. It cites Harmon-Jones and Harmon-Jones (2007) as sources for this information.

Below the text, there is an illustration showing a person's head in profile view. Inside the head, there is a statement highlighted in red: "I know that cigarettes could kill me." Below this statement, another action is described within a box labeled "action": "I grabbed a couple smokes after the meeting today."

To the right of these statements, there are labels indicating "belief" next to the first statement and "dissonance" between the belief and the action below it. At the bottom left corner of the slide, there is a citation for Eddie Harmon-Jones and Cindy Harmon-Jones' work on cognitive dissonance theory published in 2007 by Zeitschrift für Psychologie.

In summary, the slide visually represents how conflicting beliefs and behaviors can lead to cognitive dissonance.</sample>
    <sample id="951">The second occurrence of "keep" in the text is justified by a consonance relationship.</sample>
    <sample id="952">The text in the image is about cognitive dissonance. It explains that it involves two elements of cognition, such as thoughts, actions, and beliefs, which are inconsistent with each other. The slide also mentions that this type of dissonance can be expressed through language by a relationship between phrases or statements made by a user. Additionally, it notes that while dissonance is common in daily decision-making, it is relatively rare to find it expressed in language compared to other discourse relations.</sample>
    <sample id="953">The English content in the image includes: Title: "Why dissonance?" Subtitle: "Effects of disagreement" Text at the bottom: "Eddie Harmon-Jones and Judson Mills. 2019. An Introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive dissonance: researching a global theory in psychology." Additional text: "Attitudes and Belief trends"</sample>
    <sample id="954">The text in the image is as follows: "Why dissonance? Effects of disagreement. Attitudes and Belief trends. Anxiety disorders." There are also references to various studies or works at the bottom, including titles like "Edie Harmon-Mills and Judson Mila, 2019," "Matthew Maimon, Vasudha Verandjanjan, Johannes Eichstaedt, Aditya V Gansan, and H Andrew Schwartz, 2021," and a note about proceedings from an annual meeting for computational linguistics.</sample>
    <sample id="955">The English content in the image is: "Why dissonance? Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups."</sample>
    <sample id="956">Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better.</sample>
    <sample id="957">The text in the image is structured as follows: 1. Title: "Annotations" - This appears at the top left of the slide, indicating that the content below pertains to annotation processes or methods. 2. Flowchart Steps: There are three main steps outlined within a flowchart on the right side of the slide:
   - Step 1: Good parsing quality?
   - Step 2: Dissonance?
   - Step 3: Consonance? Each step has corresponding 'Yes' and 'No' branches leading to different outcomes such as "Dissonance," "Consonance," or "Neither." The percentages associated with these outcomes are also provided (e.g., ~3.5%, ~48%). 3. User Information: Below the title "Annotations," there's an icon representing Twitter followed by the word "User" and a handle "@user_handle". This suggests user interaction or data source information related to annotations. 4. Example Annotation Text: At the bottom center, there's a highlighted piece of text from what seems to be a social media post or comment: "Wish I could hold grudges but I guess it’s a good thing that I can’t at the same time." This example illustrates how dissonance might manifest in real-world language use. 5. Additional Notes: In the lower-left corner, there's a note marked "*Check paper for detailed annotation guidelines," which indicates further reading material available elsewhere for more comprehensive details about the annotation process described above.</sample>
    <sample id="958">The image contains a flowchart and text related to annotating tweets. Here is the detailed transcription of the visible content: Annotations - Step 1: Good parsing quality? Yes No Step 2: Dissonance? Yes No Step 3: Consonance? Yes No Dissonance -3.5% Consonance -48% Neither -48% The tweet shown in the image reads: User @user_handle Wish I could hold grudges but I guess it's a good thing that I can't at the same time. At the bottom left, there is a Twitter logo with the word "PARSE" next to it. Below the tweet, there are two labels: "Dissonance" on the right side and "Consonance" below the tweet itself. In the top right corner, there is an annotation by "Monica Vazquez." There is also a note at the bottom left corner indicating "*Check paper for detailed annotation guidelines."</sample>
    <sample id="959">The image contains a flowchart with the following steps: Step 1: Good parsing quality? - Yes. Step 2: Dissonance? - No. Step 3: Consensus? - No. There is also text that reads "Annotations" and "Dissonance," along with percentages indicating the results of these steps. Additionally, there is a Twitter logo and a user handle "@user_handle." The main content appears to be related to annotating or analyzing data, possibly in the context of natural language processing or sentiment analysis.</sample>
    <sample id="960">The text in the image is as follows: "Training on Initial Annotated Set" and a speech bubble that says, "Small annotated dataset: 43/901 dissonance; not better than chance." There's also an axis labeled "Area under the ROC curve (AUC)" with values ranging from 0.5 to 0.65. Additionally, there are labels for "RoBERTa-base + classifier head" and "TRAIN," along with a small red box labeled "init dataset."</sample>
    <sample id="961">The text in the image is: "Training on Initial Annotated Set", "RoBERTa-base + classifier head", "TRAIN", "Small annotated dataset: 43/901 dissonance; not better than chance", and "Area under the ROC curve (AUC)".</sample>
    <sample id="962">The text in the image is as follows: Method: Transfer and Active Learning for Annotating Rare Class START Initial model: Transfer Learning Rare class annotation – “needle in a haystack” Difficult to annotate Easier to annotate Increase chance of rare class? Acquisition strategy which are best to label? new examples humans annotate Active learning iteration Model Retrain/Update Cumulative (CM) old data fine-tune new Add new examples Iterative (IT)</sample>
    <sample id="963">The English content in the image is as follows: Cold-start Annotations: Transfer Learning START Initial model: Transfer Learning Rare class annotation -- "needle in a haystack" Difficult to annotate increase chance of rare class? Easier to annotate Acquisition strategy: new examples which are best to label? Active learning for humans annotate Model Retrain/Update Cumulative (CM) Iterative old data new data train Model Retrain/Update Active learning for humans annotate Add new examples old data new data train</sample>
    <sample id="964">The English content in the image is as follows: Title: Cold-start Annotations: Transfer Learning Text Box (Top Left): RoBERTA-base + classifier head Chart Labels and Data Points: - init dataset (0.5) - Debate (0.62, +0.12) - CE (0.58, +0.10) - CE:CE (0.54, +0.08) Text Box (Right Side): Transferred weights after training on combined Debate and CE data *SCE: Comparison and Exploration classes; Rasheed, Rashed Al; Ali, Nabeel; Alaa, Adel; Aref, Mohamed; Arsalan, Reza; Asad, Muhammad; Bahloul, Ahmed; Benjilali, Fadel; El-Masry, Islam; El-Sayed, Samir; Farah, Hossam; Hassan, Mostafa; Hamed, Mahmoud; Khaled, Ayman; Mekawy, Ahmed; Mousa, Amr; Saleh, Ahmed; Suleiman, Ahmed; Taha, Ahmed; Tawfik, Ahmed; Yehia, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed, Ahmed; Zayed</sample>
    <sample id="965">The text in the image is as follows: "Cold-start Annotations: Transfer Learning", "RoBERTA-base + classifier head", "TRAIN", "init dataset", "+0.12", "+0.08", "Debate", "CE", "Transferred weights after training on combined Debate and CE data".</sample>
    <sample id="966">The text in the image is as follows: Title: Cold-start Annotations: Transfer Learning Box 1 (on the left): RoBERTA-base + classifier head Bar chart labels from top to bottom: init dataset, Debate, CE Bar chart values: +0.12 for Debate and -0.08 for CE Text bubble on the right side of the bar chart: Transferred weights after training on combined Debate and CE data Text at the bottom right corner: *Debate: RoBERTA-base fine-tuned in debate domain by Vasile N., Vasile N., Alain L., Alain L., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N., Vasile N</sample>
    <sample id="967">The image shows a presentation slide titled "Cold-start Annotations: Transfer Learning." The main content of the slide includes a bar chart comparing different tasks and their performance metrics, such as "TRAIN," "init dataset," "Debate," "CE," and "Debate-CE." Each task has associated values indicating some form of measurement or score. There is also text on the right side that reads "Finetuning on each task consecutively" within a speech bubble. Additionally, there is a small video window in the top-right corner showing a person speaking. At the bottom left of the slide, there is a citation for a paper by Vaswani et al., 2017, related to neural machine translation.</sample>
    <sample id="968">The text in the image is: 'Active Learning: Cumulative vs Iterative Update'.</sample>
    <sample id="969">Over the different strategies, we found that cumulative performed equal or better than iterative across the board.</sample>
    <sample id="970">The English content in the image is: "Active Learning: Probability-of-Rare-Class Strategy".</sample>
    <sample id="971">The English content in the image is: 'Active Learning: Probability-of-Rare-Class Strategy' and there's a diagram showing different stages of model retraining, updating, and iterative processes.</sample>
    <sample id="972">The text in the image is as follows: Title: Active Learning: Probability-of-Rare-Class Strategy Subtitle: Active Learning Strategy Comparison (AUCs) Baseline from scratch Transferred model +0.17 AL-Random +0.15 AL-Entropy +0.20 AL-ConSet +0.19 AL-CAL +0.19 AL-PRC (ours) +0.21 Note that there are also some numbers and labels on a bar chart, but they do not form coherent sentences or additional textual content beyond what has been mentioned above.</sample>
    <sample id="973">The text in the image is "Active Learning: Probability-of-Rare-Class Strategy".</sample>
    <sample id="974">The image contains a slide titled "Active Learning: Probability-of-Rare-Class Strategy." It includes a table labeled "Active Learning Strategy Characteristics" with columns for "Rare %," "Time (s)," and "Subj. diff." The rows in the table list different strategies: RANDOM, ENTROPY, CoreSet, CAL, and PRC. Each row provides corresponding values under these columns.

Below the table, there are two bullet points:
1. Minimum annotation cost does not necessarily lead to better models
2. Rarity could make the annotations more difficult; cognitive dissonance is one such class.
   To increase dissonance samples, PRC works the best

In the top right corner of the slide, there is an image of a person's face. At the bottom right corner, the number 23 is displayed.</sample>
    <sample id="975">The English content in the image is: Takeaways Rare class annotation - 'needle in a haystack' PRC is simple &amp; efficient for rare sample acquisition Cold-start AL with transfer learning Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="976">The English content in the image is: "Takeaways" PRC is simple &amp; efficient for rare sample acquisition Cold-start AL with transfer learning Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="977">The content of the image is a slide from a presentation. The title at the top reads 'Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge.' Below the title, there are contact details provided in blue text, including email addresses: 'vvaradarajan@cs.stonybrook.edu,' 'sjuhng@cs.stonybrook.edu,' and 'has@cs.stonybrook.edu.' There are three QR codes below this section labeled 'Code,' 'Dataset,' and 'Paper,' each with corresponding URLs beneath them. At the bottom right corner of the slide, there is a page number '25' displayed.

Later in the video, another frame shows a white background with black text that says 'Thank you!' followed by an ellipsis ('...'). In the upper right corner of this frame, there is a small inset showing a person's face.</sample>
    <sample id="978">The authors evaluated the following models: BART-FID-RAG, Blender2, Emora, and Blender-Decode.</sample>
    <sample id="979">There are 9 authors involved in the paper.</sample>
    <sample id="980">A good planner should read scripts that are reasonable and feasible to constraints.</sample>
    <sample id="981">There are 7 authors involved in the paper.</sample>
    <sample id="982">The speaker's name is Vasudha Varadarajan.</sample>
    <sample id="983">Adam Przepiórkowski and Michał Woźniak are affiliated with the Institute of Computer Science, Polish Academy of Sciences in Warsaw, Poland.</sample>
    <sample id="984">The image contains a presentation slide with the following text: "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" followed by the names "Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang". Below this, there are logos for Penn State University and Amazon. In the top right corner of the slide, there is an inset video window showing a person speaking into a microphone against a background that appears to be outdoors during sunset or sunrise. The name "Yusen Zhang" is also visible below the speaker's image.</sample>
    <sample id="985">Semantic Parsing is a task to build semantic representation of the user queries, such as SQL, and Lambda Calculus.</sample>
    <sample id="986">The text in the image is: Cross-lingual Semantic Parsing.</sample>
    <sample id="987">The English content is: Cross-lingual Semantic Parsing Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations SQL Lambda FunQL</sample>
    <sample id="988">The English content in the image is: Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language Neural Models SQL Lambda FunQL</sample>
    <sample id="989">The text in the image is as follows:</sample>
    <sample id="990">The text in the image is as follows: Title: Cross-lingual Semantic Parsing Body Text: Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representations Diagram Labels (from left to right, top to bottom): English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="991">The lambda calculus is missing.</sample>
    <sample id="992">The English, German, and Chinese languages are shown on the left side of the slide. On the right side, there is a pink arrow pointing towards boxes labeled SQL, Lambda, and FunQL.</sample>
    <sample id="993">The English content in the image is: 'XSemPLR' and 'We provide a unified dataset XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains: 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, 22 natural languages in 15 language families.'</sample>
    <sample id="994">The English content in the image is: "XSemPLR" and it contains a description of XSemPLR, which stands for "cross-lingual semantic parsing." The text explains that XSemPLR provides a unified dataset with multiple natural languages and meaning representations. It lists several key features including 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages across 15 language families. There's also a diagram showing different types of tasks such as Y-Calculus, FunQL, Thing Talk, SQL, and Prolog connected to an 'XSemPLR Encoder Decoder.'</sample>
    <sample id="995">The English content in the image is as follows: "Experiment Settings We consider the six settings for training and evaluation. Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval."</sample>
    <sample id="996">The first one is Translate-Test. We use Google translate API to translate source to the target language, then use monolingual model to train and evaluate.</sample>
    <sample id="997">The text in the image is structured as follows: Title: Experiment Settings Body Text: We consider the six settings for training and evaluation. Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval. Diagrams labeled "Training" and "Inference": - Training Diagram shows a flow from English -&gt; English Model -&gt; SQL - Inference Diagram shows a flow from German -&gt; Translate API -&gt; English -&gt; English Model -&gt; SQL The slide number at the bottom right corner indicates that this is slide 8 of the presentation.</sample>
    <sample id="998">The text in the image reads: "Experiment Settings. We consider the six settings for training and evaluation. Monolingual Model: Source language is the same as target language, e.g., German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data."</sample>
    <sample id="999">The text in the image is structured as follows: 1. Title: "Experiment Settings" - This indicates that the content below pertains to settings for an experiment, likely related to language models or translation tasks. 2. Body Text: The main body of the text discusses six different settings for training and evaluation. It specifically mentions a scenario called "Monolingual Model," where the source language is also used as the target language (e.g., German-to-German). Additionally, it notes testing with only 10% training data using this monolingual model setting. 3. Diagrams: There are two diagrams labeled "Training" and "Inference." Each diagram shows a flow from "German (Few-shot)" to "German Model" which then leads to "SQL." These visual elements illustrate how input data flows through the model during both training and inference phases.</sample>
    <sample id="1000">The image contains text related to "Experiment Settings" for training and evaluation of models. Here is the detailed transcription: 

Title:
- Experiment Settings

Body Text:
- We consider the six settings for training and evaluation.
  - Monolingual Model: Source language is the same as target language, e.g., German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data.

Subsections:
- Training
  - Diagram showing a flow from "German (Few-shot)" to "German Model," which then leads to "SQL."

- Inference
  - Similar diagram where it shows a flow from "German" to "German Model," leading again to "SQL."

In the top right corner, there's an attribution that reads: 
- From Thomas Schuster</sample>
    <sample id="1001">The English content in the image is as follows: "Experiment Settings We consider the six settings for training and evaluation. • Multilingual Model: Train one multilingual model for all languages."</sample>
    <sample id="1002">The image shows a slide from a presentation titled "Experiment Settings." The content of the slide explains that there are six settings for training and evaluation, with a focus on a multilingual model. Specifically, it mentions:</sample>
    <sample id="1003">The English content in the image is as follows: "Experiment Settings We consider the six settings for training and evaluation. Multilingual Model: Train one multilingual model for all languages."</sample>
    <sample id="1004">Experiment Settings We consider the six settings for training and evaluation. Cross-lingual Zero-shot/Few-shot transfer. Train on one source language and transfer to another language. Training English Or English German Few-shot Multilingual Model SQL Inference German Multilingual Model SQL</sample>
    <sample id="1005">The text in the image is as follows: "Experiment Settings" and "We consider the six settings for training and evaluation. Cross-lingual Zero-shot/Few-shot transfer. Train on one source language and transfer to another language." There are also two diagrams labeled 'Training' and 'Inference'. The Training diagram shows three boxes with labels 'English', 'English', and 'German Few-shot', connected by an arrow pointing towards a box labeled 'Multilingual Model', which then points to a box labeled 'SQL'. The Inference diagram mirrors this, showing only the last part where it connects from 'Multilingual Model' directly to 'SQL'.</sample>
    <sample id="1006">The English content in the image is as follows: Analysis of Monolingual Setting We evaluate on two groups of models on Monolingual Setting EPTR: Multilingual Pretrained Encoders with Pointer-based Detectors XLM-R + PTR, mBERT + PTR EPDec: Multilingual Pretrained Encoder-Decoder Models mBART, mT5 We found EpDec (mT5) obtains the best performance on all datasets</sample>
    <sample id="1007">The English content in the image is: Analysis of Monolingual We evaluate on two groups of models on Monolingual Setting EPTR: Multilingual Pretrained Encoders with Pointer-based Decoders XLM-R PTR + mBERT PTR EPDec: Multilingual Pretrained Encoder-Decoder Models MBART, mT5 We found EpDec (mT5) obtains the best performance on all datasets MATIS MGEOQUERY MSPLIDER MSNOVERMIGHT MCWQ MCSHARE2QA MTOUP MEANALIQUA Average mBERT-PTR 30.63 72.18 40.94 57.47 52.43 75.81 59.41 80.36 63.29 63.29 mBART 31.51 71.41 41.35 59.10 58.06 80.36 63.29 80.36 63.29 63.29 mT5 53.18 74.26 50.73 60.65 65.16 81.83 101.29 85.92 101.29 85.92</sample>
    <sample id="1008">The English content in the image is: Analysis of Monolingual We evaluate on two groups of models on Monolingual Setting EPTR: Multilingual Pretrained Encoders with Pointer-based Decoders XLM-R PTR, mBERT PTR EPDec: Multilingual Pretrained Encoder-Decoder Models MBART, MT5 We found Dec mBERT obtains the best performance on all datasets</sample>
    <sample id="1009">The English content in the image is: Analysis of Monolingual We evaluate on two groups of models on Monolingual Setting. EPTR: Multilingual Pretrained Encoders with Pointer-based Decoders XLM-R + PTR, mBERT + PTR EPDec: Multilingual Pretrained Encoder-Decoder Models mBART, mT5 We found that EncDec obtains the best performance on all datasets MATIS MGeoQuery MSniper MSOvermind MCWQv2 MSqa2QA MTOP McNAvera Average mBERT-PTR 30.63 72.18 40.94 57.47 52.53 75.41 60.67 57.35 mXLM-R + PTR 31.41 71.41 45.87 59.10 60.32 80.36 62.07 57.56 mBART 51.18 74.26 50.73 66.29 65.16 81.83 101.29 58.16 We found that EncDec obtains the best performance on all datasets</sample>
    <sample id="1010">The English content in the image is as follows: "Analysis of Multilingual Training. We evaluate on mT5 and XLM-R + PTR on Multilingual Setting. Enc-Dec/Enc-PTR (mT5-XLM-R) can be improved by training in a mixture of various languages."</sample>
    <sample id="1011">The text in the image is as follows: Analysis of Multilingual Training We evaluate on mT5 and XLM-R + PTR on Multilingual Setting Enc-Dec/Enc-PTR (mT5/XLM-R) can be improved by training in a mixture of various languages. MATIS MIGEOQuery MSpider MNmaps MOvernight MSqa2HPA MTOP Na1 Average XLM-R+PTR 31.31 74.14 48.30 85.17 59.10 66.29 80.36 81.86 7.69 52.03 58.16 XLM-R m515 74.26 50.73 81.03 61.82 66.29 80.36 81.86 80.36 10.59 58.16 Multilingual m545 39.72 71.35 40.20 85.91 61.03 67.55 61.82 82.95 – 61.82</sample>
    <sample id="1012">Analysis of Multilingual Training We evaluate on mT5 and XLM-R + PTR on Multilingual Setting Most of the major NLs can obtain performance gain, except that English performance drops in 7 datasets and gains in 3 datasets. This is known as "Curse of Multilinguality"</sample>
    <sample id="1013">The text in the image is as follows: Analysis of Multilingual Training mT5 XLM-R PTR Multilingual Setting Most of the major NLs can obtain performance gain, except that English performance drops in 7 datasets and gains in 3 datasets. This is known as Curse of Multilinguality Increase Decrease Natural Languages en zh fa el sv es fr</sample>
    <sample id="1014">The English content in the image is: "Cross-lingual Performance Gap Blue Line: Cross-lingual Few-shot transfer Orange Line: Cross-lingual Zero-shot transfer Green Line: Monolingual Setting"</sample>
    <sample id="1015">In this figure, the blue line is cross-lingual few-shot transfer. The orange line is cross-lingual zero-shot transfer while the green line is monolingual setting</sample>
    <sample id="1016">The text in the image is about cross-lingual performance gap. It mentions that for zero-shot setting, the transfer performance gap is significant (green - orange), and for few-shot setting, the transfer gap is shortened rapidly (blue - orange).</sample>
    <sample id="1017">The text in the image is as follows: 'Other Results &amp; Findings (Section 4 in Paper)' and then there are several bullet points with detailed information. The first point says, 'Enc-Dec (mT5) outperforms previous work or achieves comparable results.' Another point states, 'Pretraining on the English NL can significantly boost the performance of few-shot on target NLs.' There's also a mention of 'Multilingual LLMs' which refers to code and bloom being inadequate for cross-lingual semantic parsing tasks. Additionally, it talks about Chinese transfer learning and English monolingual training where En-&gt;En has the largest performance gap but German usually has the smallest. Lastly, it mentions that FunQL outperforms the other three meaning representations while SQL obtains the worst performance.</sample>
    <sample id="1018">The text in the image is as follows: 1. Other Results &amp; Findings (Section 4 in Paper) Enc-Dec (mT5) outperforms previous work or achieves comparable results. Pretraining on the English NL can significantly boost the performance of few-shot on target NLs. Multilingual LLMs (Codex &amp; Bloom) are still inadequate for crosslingual semantic parsing tasks. Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest. FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.</sample>
    <sample id="1019">The English content in the image is as follows: "Conclusion - We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. - We conduct a comprehensive benchmark study on three representative types of multilingual language models. - Our results show that mT5 with monolingual training yields the best performance, notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual training and cross-lingual transfer learning is still significant."</sample>
    <sample id="1020">The text in the image is as follows: Conclusion - We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. - We conduct a comprehensive benchmark study on three representative types of multilingual language models. - Our results show that mT5 with monolingual training yields the best performance, notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual training and cross-lingual transfer learning is still significant. Links Welcome to visit our paper and code Paper Link: https://arxiv.org/pdf/2306.04085.pdf Code Link: https://github.com/psunlpgroup/xsemplr Thanks for listening</sample>
    <sample id="1021">Omission errors and accuracy scores generally lower.</sample>
    <sample id="1048">The affiliations of the authors are Emory University, Emory NLP Research Lab, and Amazon Alexa AI.</sample>
    <sample id="1049">CFT stands for continuous fine-tuning.</sample>
    <sample id="1050">The paper is a joint work with 8 authors.</sample>
    <sample id="1084">The speaker's name is Yusen Zhang.</sample>
    <sample id="1085">The English content in the image is: From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.</sample>
    <sample id="1086">The language models are trained on large-scale web crawl data.</sample>
    <sample id="1087">The image contains a graph titled "LM Training Data" with the subtitle "A mixed blessing." The x-axis is labeled "# tokens (big scale)" and shows a range from 0 to approximately 1.2 billion, while the y-axis lists various news media sources such as "nytimes.com," "washingtonpost.com," "guardian.com," etc., up to "theatlantic.com."

In the top right corner of the image, there is text that reads: "Dodge, Jesse et al. 'Documenting Large Web Corpora: A Case Study on Crowled Corpus' Proceedings of the 3rd International Conference on Methods in Natural Language Processing 2021."

Additionally, there are some partially visible sentences at the bottom left of the image:
- "Political news media are well covered"
- "...in their pre-training data according to a survey of the C4 corpus we can see that newer..."
- "...york times los angeles times the guardian huffington post post etc are well covered in language model training data"

These sentences seem to be part of an explanation or discussion related to the content shown in the graph.</sample>
    <sample id="1088">The English content in the image is as follows: LM Training Data A mixed blessing This has created a mixed blessing for language model applications Dodge, Jesse et al. "Documenting Large Web Corpora: A Case Study on Crowed Corpus" Proceedings of the 30th International Conference on Computational Linguistics (COLING) 2021</sample>
    <sample id="1089">The text in the image is structured as follows: Title at the top left corner reads "LM Training Data". Below it, a subtitle states "A mixed blessing". On the right side of the slide, there's a citation that says, "Dodge, Jesse et al. 'Documenting Large Web Corpora: A Case Study on Crowed Corpus' Proceedings of the 30th International Conference on Computational Linguistics (COLING) 2021." At the bottom center, there is a horizontal bar chart with various websites listed along the y-axis and the number of tokens (bigrams) represented by the length of each bar on the x-axis. The title and subtitles are written in black font against a white background.</sample>
    <sample id="1090">The English content is as follows: To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks. Specifically by asking the following questions: How do LMs with different political leanings perform? Does LM political leaning result in fairness issues in NLP applications?</sample>
    <sample id="1091">The English content in the image is: To this end Pretraining data Language models Downstream tasks How to evaluate the political leaning of LMs? How do LMs with different political leanings perform? What role does pretraining data play in such political biases? Does LM political leaning result in fairness issues in NLP applications?</sample>
    <sample id="1092">The image contains a slide with the title "To this end" at the top. Below the title, there is a flowchart consisting of three boxes connected by arrows: 1. The first box on the left reads "Pretraining data." 2. The middle box reads "Language models." 3. The third box on the right reads "Downstream tasks." Underneath the flowchart, there are two questions written in blue text: - How to evaluate the political leaning of LMs? What role does pretraining data play in such political biases? - How do LMs with different political leanings perform? Does LM political leaning result in fairness issues in NLP applications? In the upper right corner of the image, there is a small video feed showing a person's face.</sample>
    <sample id="1093">The text in the image is as follows: Evaluating LM Political Leaning Support both encoder and decoder LMs "statement&gt; I mask&gt; with this statement." Do you agree or disagree with this statement? &lt;statement&gt; Automatic eval Grounded in polisci lit Political Compass Test Q: Our race has many superior qualities, compared with other races. Agree Disagree Language Model Prompted Response Political Learning Libertarian Authoritarian Left Right Political Compass Test Existing LMs BERT-base BERT-large RobaRTa-base RoBERTa-large distilRoBERTa ALBERT-base ALBERT-large BART-base BART-large Alpaca Codex LLAMA GPT-2 GPT-3-ada GPT-3-babbage GPT-3-curie GPT-3-davinci ChatGPT GPT-4 GPT-J</sample>
    <sample id="1094">The video presents a detailed analysis of language models (LMs) in the context of political leanings. It begins with an introduction to existing LMs, showcasing their distribution across different quadrants based on authoritarian and libertarian values. The slide is divided into four sections: Left-Authoritarian, Right-Authoritarian, Left-Libertarian, and Right-Libertarian.

Various LM names are listed along these axes:
- Left-Authoritarian quadrant includes BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, distilRoBERTa, ALBERT-base, ALBERT-large, BART-base, and Alpaca.
- Right-Authoritarian quadrant features Codex, LLAMA, GPT-2, GPT-3 Ada, GPT-3 Babbage, GPT-3 Curie, GPT-3 Davinci, ChatGPT, and GPT-4.
- Left-Libertarian quadrant has no specific labels provided but contains some overlapping points from other quadrants.
- Right-Libertarian quadrant lists similar LMs as those found in the right-authoritarian section.

A key point highlighted is that while first language models do have varying political leanings, they occupy all four quadrants on the political compass, indicating diversity in their ideological spectrum. This suggests that there isn't a single dominant or uniform leaning among the LMs analyzed.

Throughout this segment, the focus remains on presenting data about the political inclinations of various language models without introducing new information beyond what was initially described.</sample>
    <sample id="1095">We can also see that GPT-4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BERT series.</sample>
    <sample id="1096">The English content in the image is as follows: "Pretraining Data. Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning." Below this text, there are two sections labeled "News Media" and "Social Media (Reddit)." Each section has three colored boxes with labels "left," "center," and "right," respectively. At the bottom of the image, there are references to studies by Liu, Yajun, et al., titled "Politics: Pretraining with Semantics-Aware Contextualized Representations for Political Bias Detection," published in the Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing; and Shen, Girshick, and Carolyn Rose, titled "What sounds 'right' to you? Investigating the effect of political bias in language models," published in the Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.</sample>
    <sample id="1097">The content of the image is as follows: The main text in the center reads 'Pretraining Data' and below it, there's a subtitle that says 'Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning.' On the left side, under the heading 'News Media,' there are three boxes labeled 'left,' 'center,' and 'right.' Similarly, on the right side, under the heading 'Social Media (Reddit),' there are also three boxes labeled 'left,' 'center,' and 'right.' At the bottom left corner, there is some citation information which includes authors and their affiliations. In the top right corner, there is another set of citations for different works related to language models.</sample>
    <sample id="1098">By further pre-training language models on such partisan corpora, we can see that the ideological coordinates of the language model also correspondingly shift.</sample>
    <sample id="1099">The image contains text in English. The main heading reads "Results." Below the heading, there is a subtitle that says "Partisan shifts in LM political leaning." There are two sections labeled "RoBERTa" and "GPT-2," each with four quadrants colored differently (red, blue, green, purple). Each quadrant has labels such as "reddit," "original news," and directional arrows pointing to different areas within the quadrants. On the right side of the image, there is a legend explaining the color coding: red for Left, blue for Center, and green for Right. Additionally, there is a small inset video window showing a person presenting or speaking about the content displayed on the slide.</sample>
    <sample id="1100">The image contains text related to political leaning shifts and biases. Here is the structured transcription: 1. Title: "Results" 2. Subtitle: "Partisan shifts in LM political leaning" 3. Main Content (Grid with Labels): - Columns labeled as "RoBERTa", "GPT-2" 4. Grid Cells (with labels and arrows): Each cell has a label indicating different categories such as "original news", "reddit news", etc., along with directional arrows pointing towards these categories, suggesting some form of shift or movement between them. The cells also contain numerical values that likely represent quantitative data associated with each category shift.</sample>
    <sample id="1101">The image contains a title and several charts. The main title reads 'The Trump Card'. Below the title, there is a subtitle that says 'Pre-45th to post-45th shift'.

The chart consists of multiple smaller sections labeled as follows:
1. news left
2. news center
3. news right
4. reddit left
5. reddit center
6. reddit right

Each section has numerical values represented by 'Δ' (Delta) followed by two sets of numbers in parentheses.

For example:
- In the 'news left' section: Δ = (-275.124)
- In the 'news center' section: Δ = (-0.13-1.03)
- In the 'news right' section: Δ = (1.63, 1.03)

Additionally, each section includes arrows pointing upwards or downwards with corresponding numerical values next to them.
- For instance, in the 'news right' section, an arrow points upward with the value 1.63 next to it.

In the top-right corner of the image, there are four colored squares arranged in a larger square pattern. Each small square within this arrangement is divided into red and blue quadrants.</sample>
    <sample id="1102">The content of the image is a slide titled "The Trump Card" with a subtitle that reads "Pre-45th to post-45th shift." The slide contains several smaller images arranged in a grid, each labeled differently such as "news left," "news center," and "reddit right." Each small image has numerical data associated with it, including delta values (Δ) ranging from -2.751.124 to 0.369.875.

In the top-right corner of the main slide, there is a video call interface showing two participants: one on the left side wearing glasses and another on the right side who appears to be gesturing while speaking. Both individuals are visible against different colored backgrounds—one blue and one greenish-blue—indicating they might be using virtual backgrounds or have their cameras turned off during the presentation.

The overall layout suggests this is part of an academic or professional presentation discussing language model shifts over time related to political events involving President Donald Trump's tenure.</sample>
    <sample id="1103">The image contains a chart with the title "The Trump Card" and subtitles that read "Pre-45th to post-45th shift." The chart is divided into several sections, each labeled as follows: - news left (with Δ values of 275.124 and 237.051) - news center (with Δ values of -0.131.103 and -0.121.286) - news right (with Δ values of 1.631.03 and 2.130.06) - reddit center (with Δ values of 0.750.36 and 0.750.36) - reddit right (with Δ values of -1.750.92 and -1.750.92)

Each section includes two sub-sections marked as "GPT2," which are further subdivided by color-coded quadrants representing different political leanings or sentiments. These quadrants include shades of blue, red, green, and purple.

In the top-right corner of the image, there is an inset showing four colored squares arranged in a grid pattern, corresponding to the colors used within the main chart.</sample>
    <sample id="1104">The English content in the image is as follows: Per-Category Performance Hate Speech BLACK MUSLIM LGBTQ+ JEWS ASAIN LATINX WOMEN CHRISTIAN WHITE REDDIT 89.34 89.06 90.57 91.22 90.66 87.13 87.13 88.81 NEWS_88_81 88.81 88.81 89.06 90.66 90.66 87.13 87.13 88.81 HP (L) CNN (L) NYT L (L) GUARD (L) WAT (R) BBART (R) WAT (W) NEWS_88_81 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 CNN (L) 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 GUARD (L) 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 WAT (R) 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 BBART (R) 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 WAT (W) 84.44 84.44 84.44 84.44 84.44 84.44 84.44 84.44 NEWS_88_81 88.54 88.54 88.54 88.54 88.54 88.54 88.54 88.54 Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded: dark yellow denotes best and blue denotes worst.</sample>
    <sample id="1105">The image contains a table titled "Per-Category Performance" with data on the performance of different sources in detecting hate speech and misinformation targeting various identity groups. The categories include Hate Speech, BLACK, MUSLIM, LGBTQ+, JEWS, ASAIN, LATINX, WOMEN, CHRISTIAN, MEN, and WHITE. Each category has subcategories like REDDIT, NEWS, RIGHT, LEFT, and others.

The table is color-coded to indicate performance levels:
- Dark yellow denotes best performance.
- Blue denotes worst performance.

Below the table, there is a caption that reads: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources."

The results show varying performances across different categories for each source listed (e.g., CNN, NLT, YYY, Guard, Fox, WaR, BBART, Wat, NR, etc.). For example, under the Hate Speech category, Reddit shows high scores for BLACK (89.84) and MUSLIM (89.50), while WaR shows lower scores (87.63).

In the top right corner of the image, there are three colored squares labeled "1st," "2nd," and "3rd/4th," likely indicating rankings or positions related to the performance metrics shown in the table.</sample>
    <sample id="1106">The table shows the performance of different identity groups and misinformation sources in detecting hate speech, Muslim, LGBTQ+, Asian, Latinx, Jewish, women, Christian, men, white individuals. The results are color-coded: dark yellow denotes best (90+), blue is good (85-89), light green is average (70-84), orange indicates poor (60-69), red signifies very poor (&lt;60).</sample>
    <sample id="1107">The text in the image is structured as follows: Title: Per-Category Performance Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The table includes categories such as Hate Speech, Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, Men, White. Each category has performance metrics for various datasets or models like Reddit, News Right, News Left, NYT L (likely referring to New York Times), CNN (CNN), NLT G (New York Times Guardian-like model), Guard (Guardian-like model), Fox, WaR (likely referring to Western Approach Right), BBRT (Balanced Balanced Balanced Right), Wat (likely referring to Watson), Wt (likely a typo, could be another AI model), NR (New Republic), and NR (New Republic). The values are color-coded with dark yellow indicating best performance and blue indicating worst performance.</sample>
    <sample id="1108">The table shows performance on hate speech targeting different identity groups and misinformation from various sources. The results are color-coded: dark yellow denotes best, blue denotes worst, light red indicates average, green signifies moderate, orange represents poor, and pink marks very poor. Each row corresponds to a specific source or platform (e.g., Reddit, News Right, CNN), while each column lists an identity group such as Black, Muslim, LGBTQ+, Jews, Asians, Latinx, Women, Christian, Men, White. For example, the intersection of "Reddit" under Hate Speech with "Black" has a value of 89.64 in dark yellow, indicating it's one of the better-performing categories for that combination.</sample>
    <sample id="1109">The English content in the image is as follows: "Per-Category Performance", "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and blue worst denotes worst."</sample>
    <sample id="1110">The English content in the image is as follows: "Per-Category Performance" and a table with various categories such as Hate Speech, BLACK, MUSLIM, LGBTQ+, JEWS, ASAIN, LATINX, WOMEN, CHRISTIAN, MEN, WHITE. The performance metrics are color-coded from dark yellow to blue, indicating best to worst performance respectively for different sources like REDDIT, NEWS_88, NEWS_91, etc., across these categories. At the bottom of the table, there's a note stating: "Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources." It also mentions that the results are color-coded so that dark yellow denotes best and dark blue denotes worst.</sample>
    <sample id="1111">The text in the image is a table under the heading "Qualitative Analysis." The table has columns labeled: Text, Target Label, Base, N-S-L, S-N-R, and F-S-R. There are rows of data with various texts such as statements about McDonald's priests, Donald Trump, and comparisons between left and right political views. Each row also contains labels indicating whether they fall into categories like Asian True, Chris False, etc. At the bottom of the image, there is an example from Table 5 that discusses the downstream performance of language models using different political bias metrics (B, L, R) on tasks involving Christians, vanilla RoBERTa’s model, social media, left-leaning, and right-leaning examples respectively.</sample>
    <sample id="1112">The English content in the images is as follows: Title: Qualitative Analysis Text Target Label Base N-S L-R S-N R-S ASIAN TRUE TRUE FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE</sample>
    <sample id="1113">Table 12: Qualitative analysis of hate speech example where TMs with different political leanings beg to differ.</sample>
    <sample id="1114">The text in the image is structured as a table with two main sections: "Hate Speech Text" and "Misinformation Text." Each section contains multiple rows of hate speech or misinformation examples, along with columns labeled N4, NR, R, and RR. The content includes various statements that could be considered hateful or misleading. At the bottom of the table, there is a caption that reads: "Table 12: Qualitative analysis of hate speech example where N4 political leaning has big step to differ." Additionally, on the right side of the image, there is a small video frame showing a person speaking into a microphone against a plain background.</sample>
    <sample id="1115">The text in the image is a table titled "Table 12: Qualitative analysis of hate speech example where MDS with different political leanings beg to differ." The table has columns labeled "Hate Speech Text," "Date?" (with sub-columns "N4," "N-R," and "R-R"), "True" or "False," and other categories such as "False," "False," etc. Each row contains examples of hate speech texts, dates they were identified, and whether certain criteria are met ("True" or "False").</sample>
    <sample id="1116">The English content in the image is as follows: 1. The first column header reads "Hat Speech Text." 2. There are multiple rows under this header, each containing text and a series of checkboxes labeled "TRUE" or "FALSE," with some boxes checked. Examples include: - "Some people don't like 'snowflakes' because they think it's an insult to their intelligence." - "Yes People actually use Facebook for something other than arguing about politics!" - "Clip-Ride: It’s good to buy for Christmas, but not necessarily for Valentine’s Day card." - "It has been known that dogs have become so aggressive because the dog-sitters got killed by killer dogs!" These entries appear to be examples of hate speech texts being analyzed. 3. Below these columns, there is a table titled "Table 12: Qualitative analysis of hate speech example where LM(s) with different political leanings beg to differ." This suggests a qualitative study comparing how language models with varying political leanings interpret similar hate speech examples. 4. In another part of the image, there is a section titled "Discussion" which includes phrases such as "Between Scylla and Charybdis" and "To 'sanitize' or not to 'sanitize,' that is the question." Additionally, there is a flowchart-like diagram illustrating a process involving "Pretraining data," "Language models," and "Downstream tasks."</sample>
    <sample id="1117">The English content in the image is: Discussion Between Scylla and Charybdis To "sanitize" or not to "sanitize", that is the question Pretraining data Language models Downstream tasks</sample>
    <sample id="1118">The English content in the image is as follows: Discussion Between Scylla and Charybdis To "sanitize" or not to "sanitize", that is the question Pretraining data Language models Downstream tasks</sample>
    <sample id="1119">The content of the image is as follows: At the top, it says "Discussion." Below that, there's a phrase in quotation marks: "Between Scylla and Charybdis. To 'sanitize' or not to 'sanitize,' that is the question." Underneath this text, there are three boxes connected by arrows forming a sequence from left to right labeled respectively as "Pretraining data," "Language models," and "Downstream tasks." In the upper-right corner, there’s an inset showing a person speaking into a microphone with some background visible behind them.</sample>
    <sample id="1120">The text in the image includes: "Thank you!" at the top, with three boxes below it labeled "Pretraining data," "Language models," and "Downstream tasks." Below these labels are four names: Shangbin Feng, Chan Young Park, Yuhan Liu, Yulia Tsetkova. There are also logos for PAUL G ALLEN SCHOOL UW, UWNLP, Carnegie Mellon University Language Technologies Institute, and another logo that is partially visible but not fully readable.</sample>
    <sample id="1121">Permuting with jumps</sample>
    <sample id="1122">The author described the "marked words" method as a way to find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">Shangbin Feng is affiliated with the Paul G Allen School, Chan Young Park and Yulia Tsvetkov are from Carnegie Mellon University Language Technologies Institute, and Yuhan Liu's affiliation isn't mentioned.</sample>
    <sample id="1124">The first mentioned symmetrical dependency structure is "Bouquet/Stanford".</sample>
    <sample id="1125">The speaker's name is James Finch.</sample>
    <sample id="1126">The paper has 5 authors. Their names are Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, Annie Louis, and someone whose name is partially cut off in the image.</sample>
    <sample id="1127">The datasets that can be used to test syntactic phenomena are BLiMP, SyntaxGym, and Crows.</sample>
    <sample id="1128">The text in the image is as follows: "When Does Translation Require Context? A Data-driven, Multilingual Exploration Patrick Fernandes*, Kayo Yin*, Emmy Liu André F. T. Martins, Graham Neubig Carnegie Mellon University Language Technologies Institute TÉCNICO LISBOA BERKELEY AI RESEARCH INSTITUTE Unbabel * equal contribution"</sample>
    <sample id="1129">Translation depends on context We'll have to get rid of that mole.</sample>
    <sample id="1130">The English content in the image is: Translation depends on context Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole. Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1131">Translation depends on context Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1132">The text in the image is: Evaluating context-dependent translation is hard Only a small portion of words depend on context Corpus-level metrics</sample>
    <sample id="1133">The English content in the image is: Evaluating context-dependent translation is hard. Only a small portion of words depend on context Corpus-level metrics Existing methods support limited discourse phenomena and languages</sample>
    <sample id="1134">The video features a woman with long dark hair, wearing a black top and a necklace. She is speaking in front of a white background that displays two questions written in English: "RQ1: When does translation require context?" and "RQ2: How well do models handle context-dependent translations?" The text on the screen remains static throughout her speech, emphasizing these key research questions related to translation requirements and model performance.</sample>
    <sample id="1135">The English content in the image is: RQ1: When does translation require context? - Word-level context usage RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1136">The English content in the image is as follows: 'Conditional Cross-Mutual Information (CXMI)' and 'CXMI: measure how much context MT models use given a corpus.'</sample>
    <sample id="1137">The text in the image is as follows: Conditional Cross-Mutual Information (CXMI) CXMI: measure how much context MT models use given a corpus Uncertainty over translations given the source HqMTA(Y|X) Uncertainty over translations given the source AND context HqMTC(Y|X,C) CXMI(C→Y|X)</sample>
    <sample id="1138">The video begins with a slide titled 'Pointwise (P-)CXMI' in bold text. Below the title, there is a bullet point that reads: 'We introduce P-CXMI to measure context usage to translate a specific.' The background of the slide is white, and on the right side, there is an image of a person's face.

The scene transitions to another slide under the same title 'Pointwise (P-)CXMI'. This time, additional content appears below the initial bullet point. It starts with the word 'Senten' followed by a mathematical equation:
\[ \text{P-CXMI}(y, x, C) = -\log \frac{q_{MTA}(y|x)}{q_{MTC}(y|x, C)} \]
Below this, there are two more lines labeled 'o Senten' and 'o Word', each containing similar equations for measuring CXMI at different levels:

- For sentence level:
  \[ \text{P-CXMI}(y, x, C) = -\log \frac{q_{MTA}(y|x)}{q_{MTC}(y|x, C)} \]

- For word level:
  \[ \text{P-CXMI}(i, y, x, C) = -\log \frac{q_{MTA}(y_i|y_{i-h:i}, x)}{q_{MTC}(y_i|y_{i-h:i}, x, C)} \]

The background remains white throughout, maintaining consistency. On the right side, the image of a person’s face continues to be present.

The final part of the video shows the continuation from the previous slides. The first line states: 'High P-CXMI words -&gt; requires context to translate.'

This segment concludes the presentation about Pointwise (P-)CXMI, focusing on how it measures context usage for translation purposes across sentences and words.</sample>
    <sample id="1139">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1140">The text in the image is as follows: "Thematic analysis of high P-CXML words" and below it, there are translations into various languages such as English, Español, Français, Italiano, Nederlands, Português, Русский, Türkçe, and 中文.</sample>
    <sample id="1141">The text in the image is as follows: "Thematic analysis of high PCXML words 1. POS tags"</sample>
    <sample id="1142">Thematic analysis of high P-CXMI words 1. POS tags - Pronouns P-CXMI for POS tags In-Ar PRON 3.Sing PRON 3.Dual PRON 3.Plur</sample>
    <sample id="1143">The text in the image is as follows: 1. POS tags P-CXMI for POS tags In En-Ar PRON 3.Sing PRON 3.Dual PRON 3.Plur Pronouns Verb form We then look at vocabulary items that have high CXMI average over all of its different occurrences</sample>
    <sample id="1144">The text in the image is as follows: "Thematic analysis of high P-CXML words 1. POS tags 2. Vocabulary items Avelile's mother was still asleep. Avelile went to school. 阿维利尔的母亲还在睡觉。 阿维利尔去上学了。 - Pronouns - Verb form - Lexical cohesion"</sample>
    <sample id="1145">The English content is: Thematic analysis of high P-CXML words 1. POS tags 2. Vocabulary items Avelile's mother was still asleep. Avelile went to school.</sample>
    <sample id="1146">The image contains a slide from a presentation titled 'Thematic analysis of high P-CXML words.' The content is organized into three main points: 1. POS tags 2. Vocabulary items 3. Individual tokens Additionally, there are four sub-points listed under the third point (Individual tokens): - Pronouns - Verb form - Lexical cohesion - Formality At the bottom left corner of the slide, there are two sentences in English and German with corresponding flags for each language. The English sentence reads, 'She knows where we're going. I don't.' Below it, the German translation says, 'Sie weiß, wohin wir gehen. Ich weiß es nicht.' On the right side of the slide, within a purple box, the following terms are mentioned as part of the thematic analysis: - Ellipsis</sample>
    <sample id="1147">The content of the image is as follows: RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations? - Multilingual Discourse-Aware (MuDA) benchmark</sample>
    <sample id="1148">The image contains a title and a list of items. The title at the top reads "Multilingual Discourse-Aware (MuDA) tagger." Below the title, there is a purple box with white text listing five items: - Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis In the upper right corner of the image, there is a circular inset containing an individual's face blurred out for privacy reasons.</sample>
    <sample id="1149">The English content in the image includes: Title: Multilingual Discourse-Aware (MuDA) tagger List items: - Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis Bar chart labels: Language Counts Legend: pronouns, formality, verb form, lexical, ellipsis</sample>
    <sample id="1150">The text in the image is 'MuDA benchmark'.</sample>
    <sample id="1151">The text in the image is structured as follows: RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations? - Multilingual Discourse-Aware (MuDA) benchmark - Model evaluation</sample>
    <sample id="1152">The image shows a slide with the title "Corpus-level metrics" at the top left. Below the title, there is an illustration of a robot labeled "BLEU." The robot has a speech bubble above its head that reads "CONTEXT." In the upper right corner of the image, there is a circular inset containing a blurred face.</sample>
    <sample id="1153">The text in the image is 'Corpus-level metrics'.</sample>
    <sample id="1154">The text in the image is: Corpus-level metrics BLEU COMET F-measure Unclear which system is best for document-level MT with corpus-level metrics</sample>
    <sample id="1155">The content of the image is as follows: At the top, it says "Corpus-level metrics". Below that are three robot-like figures with different labels on them. The first one has a label 'BLEU', the second one has a label 'COMET' and a purple box around it, and the third one just has no label but shows some text inside its body. There's also a small circular picture in the upper right corner showing a person’s face.

At the bottom left, there's a bullet point saying: "Unclear which system is best for document-level MT with corpus-level metrics".

On the next slide, at the top, it reads "MuDA benchmark results" followed by another bullet point stating: "Context-aware models perform significantly better on some phenomena".

In the last part of this sequence, under the same heading "MuDA benchmark results", there's still the statement about context-aware models performing well. Additionally, below this, there's an added line item indicating specific areas where these models excel: "Formality, lexical cohesion".</sample>
    <sample id="1156">The text in the image is as follows: MuDA benchmark results Context-aware models perform significantly better on some phenomena o ✅: Formality, lexical cohesion ❌: Ellipsis, pronouns, verb form</sample>
    <sample id="1157">MuDA benchmark results Context-aware models perform significantly better on some phenomena o ✔: Formality, lexical cohesion ✘ Ellipsis, pronouns, verb form DeepL outperforms Google on most phenomena and language pairs*</sample>
    <sample id="1158">MuDA benchmark results Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT</sample>
    <sample id="1159">The text in the image is as follows: Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT MuDA tagger BLEU F-measure</sample>
    <sample id="1160">The English content in the image is as follows: Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT</sample>
    <sample id="1161">The abbreviations for the five methods are FTw, COSINE, L2R, MLC, and BOND.</sample>
    <sample id="1162">The model is evaluated on 11 tasks.</sample>
    <sample id="1163">The text in the image is a title slide for an academic presentation. The main content of the slide reads: "DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification." Below this, there are three names listed as presenters or authors: Regina Stodden, Omar Momen, Laura Kallmeyer. Additionally, it mentions that they are from Heinrich Heine University Düsseldorf, Germany, and that this presentation was part of ACL 2023.</sample>
    <sample id="1164">The English content in the image is: DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023</sample>
    <sample id="1165">The English content is: Text simplification is a process of adapting a text to improve the text comprehension of it for specific target groups, as people with reading problems or non-native speakers.</sample>
    <sample id="1166">The text in the image is as follows: "Text Simplification Example" and below that, there are two lines of German text. The first line reads "Original:" followed by a longer sentence with some words highlighted or underlined. Below this, another line starts with "Plain Language:" also followed by a similar but shorter sentence. There are arrows pointing from the original to the plain language sentences indicating changes such as substitution, clause deletion, reordering, and word deletion. At the bottom right corner, there's an arrow labeled "Insertion."</sample>
    <sample id="1167">The image shows a text simplification example. The original German sentence is: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." This has been simplified into plain language as: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne oder mehr Urlaub ein." There are also labels for different types of edits such as substitution, clause deletion, reordering, and word deletion.</sample>
    <sample id="1168">The image shows a slide titled 'Text Simplification Example' with various elements related to text simplification techniques. The original sentence in German is: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." Below the original sentence, there's an example of plain language translation: "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne oder mehr Urlaub ein." Several arrows and labels indicate different methods used for simplification such as substitution, clause deletion, reordering, word deletion, insertion, etc. These are represented by colored boxes (red, blue, green) connected by lines pointing towards specific parts of the sentences.</sample>
    <sample id="1169">The content of the image is a presentation slide titled 'German Text Simplification Corpora' with a subtitle 'Sentence Level'. The main text on the slide reads '2. DE-plain A New Corpus'. There is also a bar chart showing various data points related to different corpora, including numbers like 1312, 483, and 756.</sample>
    <sample id="1170">The other three models, which are proposed in recent years, are all automatically aligned.</sample>
    <sample id="1171">The text in the image is as follows: "German Text Simplification Corpora" and "Sentence Level".</sample>
    <sample id="1172">The image shows a presentation slide titled "German Text Simplification Corpora" with the subtitle "Sentence Level." The main content of the slide is a bar chart that displays data related to sentence levels and their alignment. There are various colored bars representing different categories, such as "Domestic," "European," "Science," "Health," "Education," "Public affairs," and "Miscellaneous." Each category has corresponding numerical values indicating specific metrics.

Additionally, there's an inset box on the left side of the chart showing more detailed information about some documents from 2018-2019, including counts for each category (e.g., 260 for Domestic). On the right side of the chart, two significant numbers are highlighted: 756 in red and 483 in black, both accompanied by arrows pointing towards them.

In the top-right corner of the slide, there appears to be a small video feed or webcam overlay showing a person speaking into a microphone, suggesting this might be part of a live presentation or lecture.</sample>
    <sample id="1173">The image shows a slide titled "German Text Simplification Corpora" with the subtitle "Sentence Level." The main content of the slide is a bar chart displaying various metrics related to text simplification. On the left side, there are labels for different categories such as "Domestic," "Health," "Education," "Public Affairs," and "Science &amp; Technology." Below these categories, numerical values like 260, 1189, 9456, 102096, 5942, etc., are listed.

On the right side of the chart, two specific data points are highlighted: one labeled "756" in red and another labeled "483" in black, both accompanied by small icons or symbols next to them. Additionally, there is a section at the bottom that reads "Oberlin-MA" followed by numbers 3456 and 756.

In the top-right corner of the image, there is a person wearing headphones, suggesting this might be from a presentation or lecture setting.</sample>
    <sample id="1174">The text in the image is as follows: "German Text Simplification Corpora" and "Sentence Level".</sample>
    <sample id="1175">The English content in the images is as follows: 1. Types of Simplification - Simplicity, LexSimp, StructSimp (with corresponding bars for news, bible, L2, fiction) 2. Simplification Transformations - DEplain-apa and DEplain-web (with bars representing different categories like moving, engineering, lexical substitution, verb addition, word deletion).</sample>
    <sample id="1176">As you can see here, the Bible texts are much stronger simplified than for example the news text or the language learner texts.</sample>
    <sample id="1177">On all level regarding for example lexical simplification, structural simplification also over all level of simplification.</sample>
    <sample id="1178">Furthermore, you can see that our dplain corpus has a high variety of different simplification transformations. So for example, in the dplain API corpus we have much more reorderings and word additions than we have in the dplain web corpus.</sample>
    <sample id="1179">On the other hand, in the web corpus we have much more rephrasings.</sample>
    <sample id="1180">The English content in the image is as follows: 3. Use-cases Automatic alignment and simplification Hello, I am Omar and now I will talk about the use cases for our dataset Dplane So for the first use case we can evaluate automatic alignment methods</sample>
    <sample id="1181">The text in the image is as follows: "Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n cm capabilities (lower part)" The table includes columns labeled N, P, R, F, P R, and n cm. Each row provides a name for an alignment method followed by its description and corresponding values under these columns. Some examples from the table include: - Sent-LAISE: Hierarchical alignment using sentence embeddings similarity - Similar embeddings of Language-agnostic BERT transformer - Different similarity measures e.g., n-grams (C3S) word vectors For each entry, there are numerical values listed next to each column header representing specific metrics or results related to that particular alignment method.</sample>
    <sample id="1182">The text in the image is about automatic alignment evaluation. It shows a table with different methods for aligning sentences from parallel documents written in different languages, and their performance metrics are listed as P, R, F1, PR, and n cm. The title of this section is "Automatic Alignment Evaluation."</sample>
    <sample id="1183">The English content in the image is a table titled "Automatic Alignment Evaluation" that shows results of alignment methods with 1:1 (upper part) and n cm capabilities (lower part). The table includes columns labeled P, R, F, PR, and n cm. Each row describes different alignment methods such as Sent-LA-LEASE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign, along with their respective performance metrics for each column.

The description provided explains the context of using these alignment methods to extract alignments between sentences from two parallel documents having the same language but differing complexity levels.</sample>
    <sample id="1184">The image shows a table titled 'Automatic Alignment Evaluation' with two sections: the upper part labeled '1:1 (upper part)' and the lower part labeled 'n:n (lower part)'. The table lists various alignment methods, their descriptions, and evaluation metrics. The methods include Sent-LA-LEASE, CATS-C3G, VecAlign, BERTAlign, and MASSAlign. Each method has corresponding scores for P, R, F1, and n:n metrics.</sample>
    <sample id="1185">The table shows the results of alignment methods with 1:1 (upper part) and n cm (lower part). The columns are labeled P, R, F, PR, and ncm. Each method has a corresponding description in the "Description" column.</sample>
    <sample id="1186">At the end, we concluded that the best alignment automatic alignment method to use for German text simplification is MASSAlign.</sample>
    <sample id="1187">The text in the image is as follows: Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n cm capabilities (lower part) LHA Name Description P R F R F P r0.5 Sent-LA-LESE Hierarchical alignment using sentence embeddings similarity .946 41 57 747 .948 444 608 780 CATS-C3G Similar embeddings of Language-agnostic BERT transformer similarity ... VecAlign Multilingual aligned based on multi-lingual sentence embedding ... BERTAlign Allows sentence-transformer methods produce n cm alignments ... MASSAlign A vicinity-driven approach with a TF-IDF similarity matrix ...</sample>
    <sample id="1188">The second use case that we showed in our paper is a case of automatic text simplification.</sample>
    <sample id="1189">The English content in the image is: Automatic Text Simplification Document Level Results on Document Simplification with fine-tuned long mBART. n corresponds to the length of the training data. DEPLAIN-APA test (n=48) Sentence Level Results on Sentence Simplification with fine-tuned mBART. n corresponds to the length of the training data. DEPLAIN-APA test (n=1231) DEPLAIN-WEB test (n=1846)</sample>
    <sample id="1190">The video presents a slide from a presentation on "Automatic Text Simplification." The title of the slide is prominently displayed at the top. Below the title, there are two main sections: one labeled "Document Level" and another labeled "Sentence Level," each containing tables with detailed results.

In the upper section (Document Level), the table includes columns for different metrics such as "BLEU," "F1," "PPL," along with corresponding values like "0.4538" and "0.6729." It also mentions tests named "DEPLAIN-APA test (n=48)" and "DEPLAIN-WEB test (n=147)."

The lower section (Sentence Level) mirrors this format but focuses on sentence-level simplifications. Here too, it lists various performance metrics and specific test names like "DEPLAIN-APA test (n=1231)" and "DEPLAIN-WEB test (n=1846)."

Throughout the video, these slides remain static without any changes in content or appearance. Additionally, an individual appears in the background throughout the sequence, suggesting that they might be presenting or discussing the material shown on the slides.</sample>
    <sample id="1191">The video shows a presentation slide titled "Automatic Text Simplification" with sections labeled "Document Level" and "Sentence Level." The document level section includes results from the DEPLAIN-APA test (n=48) and DEPLAIN-WEB test (n=147), showing data for different models like DEPLAIN-APA, DEPLAIN-APA-BLEU, DEPLAIN-APA-FRE, and their respective BLEU scores. Similarly, the sentence level section presents results from the DEPLAIN-APA test (n=1231) and DEPLAIN-WEB test (n=1846), detailing performance metrics such as BLEU and F1 scores across various models including DEPLAIN-APA, DEPLAIN-APA-BLEU, DEPLAIN-APA-FRE, and DEPLAIN-WEBA.</sample>
    <sample id="1192">The video shows a presentation slide titled "Automatic Text Simplification" with detailed results on document simplification and sentence simplification using the finetuned mBART model. The text indicates that 'n corresponds to the length of the training data.' It includes tables comparing different models such as DEPLAIN-APA, DEPLAIN-SARL, and their respective BLEU scores for both test sets (DEPLAIN-APA test n=480) and (DEPLAIN-SARL test n=147). Additionally, there are sections labeled "Document Level" and "Sentence Level," each presenting various metrics like BLEU, METEOR, and F1 scores for different datasets including DEPLAIN-APA test n=480, DEPLAIN-SARL test n=147, DEPLAIN-APA test n=1231, and DEPLAIN-SARL test n=1846.</sample>
    <sample id="1193">The text in the image is related to "Automatic Text Simplification" and includes results from document simplification using fine-tuned mBART. It mentions metrics such as BLEU, F1, and ROUGE for different tests (DEPLAIN-APA test, DEPLAIN-WEB test) at both document level and sentence level. The data also compares these scores with a baseline score of 0.6384.</sample>
    <sample id="1194">The text in the image is related to "Automatic Text Simplification." It provides results on document simplification using a feature-tuned mBART model. The table includes metrics such as data size, BLEU scores for different models (DEPLAIN-APA and DEPLAIN-WEB), and F1 scores with corresponding training data sizes. There are sections labeled "Document Level" and "Sentence Level," each containing tables with similar information.</sample>
    <sample id="1195">Thanks. For more details, please check out our paper and feel free to visit our poster in the ACL 2023 conference.</sample>
    <sample id="1196">The English content in the image is as follows: Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis Google Research</sample>
    <sample id="1197">The text in the image reads: Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis Google Research</sample>
    <sample id="1198">Here, a user wants to select between one of these two songs.</sample>
    <sample id="1199">The image shows a slide from a presentation by Google Research, titled "Indirect Referring Expressions." The goal of the research is to understand users' language when they make a choice. It discusses alternative questions and direct references in natural and fluid conversation.

- **Alternative question**: "Did you mean easy on me or I gotta feeling?"
- **Direct reference**:
  - "easy on me," "the first one"
  - Cannot remember the name
  - The pronunciations are hard to distinguish
  - Want to specify a preference

- **Indirect reference**:
  - "The newer one."
  - "The song that's not energetic."

The text at the bottom reads: "Breaking Indirect Referring Expressions for Entity Selection Variability Control" with the page number 43 indicated.</sample>
    <sample id="1200">The English content in the image is as follows: Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference: easy on me, "the first one" Indirect reference could be used in natural and fluid conversation: Cannot remember the name The pronunciations are hard to distinguish Want to specify a preference Indirect reference The newer one. The song that's not energetic.</sample>
    <sample id="1201">The English content in the image is as follows: Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference easy on me, "the first one" Indirect reference Could remember the name The pronunciations are hard to distinguish Want to specify a preference Indirect reference The newer one. The song that's not energetic.</sample>
    <sample id="1202">Here are some example indirect references. For example, the newer one or the song that's not energetic.</sample>
    <sample id="1203">This is an important problem in conversational systems and also for benchmarking Large Language Models' entity understanding. No large-scale public dataset available, we collect a large dataset using crowd annotation Three domains:</sample>
    <sample id="1204">The English content is as follows: Dataset Collection Important problem Conversational systems Benchmarking Large Language Models’ entity understanding No large-scale public dataset available We collect a large dataset using crowd annotation Three domains: music, books and recipes</sample>
    <sample id="1205">The English content in the image is: Dataset Collection Methodology.</sample>
    <sample id="1206">The text in the image includes: "Dataset Collection Methodology", "Methodology emphasizes informality using a cartoon completion task", "Sets the dialog context chosen from a few manual prompts per domain", "The alternative question", "Expression referring to one of the entities", and "Filled in by the annotator". The logo at the top right corner says "Google Research" with an arrow pointing downwards.</sample>
    <sample id="1207">In the second speech bubble, Alice says "Do you mean easy on me or I got a feeling?"</sample>
    <sample id="1208">The alternative question is "You mean 'Easy on Me' or 1 Gotta Feeling?".</sample>
    <sample id="1209">The first speech bubble is chosen from a few manual prompts per domain.</sample>
    <sample id="1210">The second one which is the alternative question is generated as follows.</sample>
    <sample id="1211">The English content in the image is: Generate alternative questions =&gt; sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean This is it or Man in the Mirror? Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am</sample>
    <sample id="1212">The English content in the image is: Generate alternative questions =&gt; sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean This is it or Man in the Mirror? Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am</sample>
    <sample id="1213">The first one is uniform at random.</sample>
    <sample id="1214">The second one is when the entities have similar titles. For example, two books with the name "The Return" (memoir) or "The Return" (Shatner novel).</sample>
    <sample id="1215">The text in the image is as follows: "Generate alternative questions =&gt; sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean This is it or Man in the Mirror? Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am"</sample>
    <sample id="1216">The text in the image is as follows: "Google search link to each song. Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas). Click here to find out about the song. We ask annotators to Listen to at least some of each song Read about each song."</sample>
    <sample id="1217">The English content in the image is as follows: Background knowledge (Music) Google search link to each song. Easy on Me (by Adele). I Gotta Feeling (by The Black Eyed Peas). Click here to find out about the song. We ask annotators to Listen to at least some of each song Read about each song</sample>
    <sample id="1218">The English content in the image includes: Background knowledge (Music) Google search link to each song Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) We ask annotators to Listen to at least some of each song Read about each song Here's for example, the Google search result for the song Easy on Me.</sample>
    <sample id="1219">The text in the image is as follows: Background knowledge (Recipes) Simnel Cake Pandan Cake Simnel cake is a fruitcake widely eaten in the United Kingdom, Ireland and other countries with patterns of migration from them, associated with Lent and Easter. It is distinguished by layers of almond paste or marzipan, and a set of eleven balls made of the same paste. Pandan cake is light, fluffy, green-coloured sponge cake flavoured with the juices of Pandanus amaryllifolius leaves. The cake is popular in Indonesia, Malaysia, and also the Netherlands, especially among the Indo community.</sample>
    <sample id="1220">The English content in the image is as follows: "Eliciting expressions. We then tell the annotators which choice should be selected and ask them to describe it. Pick one this Easy on Me (by Adele) I Gotta Feeling (by the Black Eyed Peas) We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble. For example: The one with the piano music The song that's not energetic It has something about a river The never one It's about having time to choose."</sample>
    <sample id="1221">The text in the image is as follows: "Eliciting expressions. We then tell the annotators which choice should be selected and ask them to describe it." Below this, there are two options labeled "Easy on me (by Adele)" and "I Gotta Feeling (by The Black Eyed Peas)." Further down, there is a section titled "Random Examples" with three columns: "Music Selection," "Book Selection," and "Recipe Selection." Each column contains various examples related to music, books, and recipes respectively. In the bottom right corner of the image, there is a logo for Google Research.</sample>
    <sample id="1222">The AltEntities Corpus has 6,000 alternative questions across three domains and 42,000 indirect referring expressions. Results with the T5 XL model show that it achieves an accuracy of: - 92-95% if the LM has access to the same background knowledge as annotators. - 82-87% when the LM has access to partially overlapping background knowledge. Additionally, it shows a performance drop of about 60% when the LM (T5 XL) only has access to entity names. The models are described as domain-generalizable. A dataset link is provided for further information.</sample>
    <sample id="1223">The text in the image is related to a presentation slide about the 'AltEntities Corpus'. It discusses various aspects of this corpus, including alternative questions across three domains and indirect referring expressions. The slide also mentions results with T5 XL model accuracy for different scenarios involving background knowledge access by annotators versus language models (LM). Additionally, it states that shown models are domain-generalizable and provides a dataset link: https://github.com/google-research/datasets/AltEntities.</sample>
    <sample id="1224">The text in the image is as follows: "AltEntities Corpus - ~6,000 alternative questions across the three domains - ~42,000 indirect referring expressions Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82%-87% when the LM has access to partially overlapping background knowledge. ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research/datasets/AltEntities"</sample>
    <sample id="1225">The text in the image is as follows: Google Research AltEntities Corpus ~6,000 alternative questions across the three domains ~42,000 indirect referring expressions Results with T5 XL model (accuracy): 92-95% if the LM has access to the same background knowledge as annotators. 82-87% when the LM has access to partially overlapping background knowledge. ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research/datasets/AltEntities Reading: Indirect Referring Expressions for Entity Selection Corpora</sample>
    <sample id="1226">CamemBERT is initially trained on 4GB of data.</sample>
    <sample id="1227">The name of the speaker is Adam Przepiorkowski.</sample>
    <sample id="1228">The findings that led to the conclusion that temporal drift is the main cause of performance loss are: 1. Performance degrades with larger temporal gap, and this confirms our hypothesis that the main cause of the performance drop is temporal drift.</sample>
    <sample id="1229">The text in the image is as follows: NLPPositionality: Characterizing Design Biases of Datasets and Models Sebastian Santy* Carnegie Mellon University Jenny T. Liang Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarden Sap Carnegie Mellon University</sample>
    <sample id="1230">The English content in the image is: NLPPositionality: Characterizing Design Biases of Datasets and Models. Sebastian Santi* Carnegie University of Washington Jenny T. Liang Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarten Sap Carnegie University of Washington</sample>
    <sample id="1231">Imagine...</sample>
    <sample id="1232">Imagine... Carl Jones Tech Lead, New York Times = PerspectiveAPI score Can you stop being a jerk? 🤨 (0.82) ✅</sample>
    <sample id="1233">Imagine... Carl Jones Tech Lead, New York Times Can you stop being a jerk? 😡 (0.82) ✅ Aditya Sharma Tech Lead, Times of India Presstitutes everywhere on the news. 😡 (0.33) ❌ = PerspectiveAPI score</sample>
    <sample id="1234">Imagine... Design bias example Can you stop being a jerk? (0.82) ✅ Presstitutes everywhere on the news. (0.33) ❌ Carl Jones Tech Lead, New York Times Aditya Sharma Tech Lead, Times of India</sample>
    <sample id="1235">The video features a slide presentation with the title "Positionality" prominently displayed at the top. The background is white, and there is text in black font throughout the frame. In the bottom left corner of each frame, there is a citation that reads: "[1] Savin-Baden, Maggi, and Claire Howell-Major; 'Qualitative research: The essential guide to theory and practice.' Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013)." Additionally, on the right side of the frames, there is an image of a person sitting in front of bookshelves filled with books.

As the video progresses, new content appears below the main title. A quote is introduced which states: "The perspectives [people] hold as a result of their demographics, identity, and life experiences." This text remains consistent across all subsequent frames without any changes or additional elements being added.</sample>
    <sample id="1236">This is a concept widely used in critical studies, specifically in feminist and queer academic spaces.</sample>
    <sample id="1237">The video features a slide titled "Positionality" with the following text: “The perspectives [people] hold as a result of their demographics, identity, and life experiences.” Below this quote is another line that reads: “[As a researcher,] it influences the research process and its outcomes and results.” The citation at the bottom references Savin-Baden, Maggi, and Claire Howell-Major's book, "Qualitative research: The essential guide to theory and practice. Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013)." Additionally, there is an image in the top right corner showing a person sitting on a chair next to shelves filled with books or other items.</sample>
    <sample id="1238">The English content in the image is: "Do datasets and models have positionality?" followed by three references: 1. [1] Blasi, et al., "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. 2. [2] Ye et al., "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022. 3. [3] Cambo &amp; Gergle, "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022.</sample>
    <sample id="1239">The text in the image is as follows: "Do datasets and models have positionality?" [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Ye et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="1240">The text in the image is as follows: Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Ye et al.: GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Training Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="1241">The English content in the image is as follows: "Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022."</sample>
    <sample id="1242">The image contains a slide with the title "Do datasets and models have positionality?" Below the title, there is a section labeled "Anecdotal evidence:" which lists two bullet points: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] At the bottom of the slide, there are three references listed as follows: 1. Blasi, et al., “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. 2. Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. 3. Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="1243">The English content in the image is: Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Ye et al., “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.</sample>
    <sample id="1244">The text in the frame is: 'Question: Do datasets and models have positionality?'</sample>
    <sample id="1245">NLPPositionality A framework for characterizing design biases in NLP datasets and models</sample>
    <sample id="1246">The English content in the image is as follows: "Our framework works in two main steps." This text appears to be part of a larger explanation or description, likely related to some kind of process or methodology being outlined.</sample>
    <sample id="1247">The first step is to re-annotate datasets with diverse annotators.</sample>
    <sample id="1248">The English content in the image is as follows: 1) Re-annotate datasets with diverse annotators.</sample>
    <sample id="1249">The text in the image is as follows: Framework 1) Re-annotate datasets with diverse annotators.</sample>
    <sample id="1250">The English content in the image is as follows: 2) Compare annotations by demographics to models and datasets using Pearson's R scores.</sample>
    <sample id="1251">The image contains a detailed framework for data annotation and analysis. Here is the transcription of the text in the image: Framework Collection 100 instances are sampled from a dataset Each instance has an associated gold label Annotations from annotators around the world Model Predictions The received annotations are compared to model predictions via Pearson's R scores. Age Gender Ethnicity Education Country White Black Asian 25-34 35-45 Analysis Pearson's correlation is calculated between each pair of categories, with arrows indicating comparisons across different demographic groups.</sample>
    <sample id="1252">LabintheWild Our framework is largely enabled through Lab in the Wild, an online crowdsourcing platform for our HCI collaborators.</sample>
    <sample id="1253">The text in the image is as follows: LabintheWild Pool of diverse volunteers / research participants Online experiment from researchers Could live with an AI and its decisions? Are you better than an AI in noticing hateful speech? Where are you on the techno-skeptic to techno-utopian scale? Will technology affect your personal life, will it make things easier or harder for you? Participate now Participate now</sample>
    <sample id="1254">Task A: Social Acceptability 1) Read the situation. Wanting to make a lot of money. 2) Enter what you think about it. What do you think this is from a social perspective? Very bad Bad Okay Good Very good Free text field to explain your judgment. It's generally okay, but we don't want robots to replace humans in the process. Submit 3) See what an AI and others thought about it The AI speculates: Is this situation understandable? Yes No Study participants in the United States said: [Bar chart showing ratings]</sample>
    <sample id="1255">The English content in the image is as follows: Task A: Social Acceptability 1) Read the situation. Wanting to make a lot of money. 2) Enter what you think about it. What do you think this is from a personal perspective? Bad Very bad Okay Good Very good Free to feel free to judge your judgment You generally don't want to repeat actions in others like AI's. 3) See what an AI and others thought about it The AI speculates: Understandable, understandable, understandable... Study participants in the United States said: Participants compare their responses to others' and AI's.</sample>
    <sample id="1256">Task A: Social Acceptability Analysis Datasets - Social Chemistry Models - Delphi - GPT-4</sample>
    <sample id="1257">Task B: Toxicity Participants read an instance from the DynaHate dataset. Participants rate whether they think an instance is hate speech.</sample>
    <sample id="1258">The text in the image is as follows: Task B: Toxicity Analysis Datasets - Dynahate Models - Perspective API - Rewire API - Hate RoBERTa - GPT-4 Study Participation 16,299 annotations 1,096 annotators 87 countries</sample>
    <sample id="1259">The English content in the image is: Results Who do NLP datasets and models align with? Finding 1: There is positionality in NLP.</sample>
    <sample id="1260">Datasets and models are most aligned to English-Speaking countries.</sample>
    <sample id="1261">Datasets and models are most aligned to people with a college education.</sample>
    <sample id="1262">The English content in the image is as follows: "Datasets and models are most aligned to people with a college education." This text appears on the left side of the graph. The graph itself shows various levels of education (College, Graduate School, High School, PhD, Pre-High School, Prof. School) along the x-axis, with corresponding values for hate speech and toxicity percentages on the y-axis. Each bar represents data from different educational groups, indicating how closely datasets and models align with these groups based on their education level.</sample>
    <sample id="1263">Finding 2: Some populations are left behind.</sample>
    <sample id="1264">The text in the image is: "Social Acceptability (GPT-4)" and "Datasets and models are less aligned to non-binary people."</sample>
    <sample id="1265">So, given that there is positionality in NLP. What can we do about it?</sample>
    <sample id="1266">The text in the image is as follows: Recommendations 1. Keep a record of all relevant design choices made throughout building datasets or models. 2. Do NLP research through the lens of perspectivism: a. Share disaggregated dataset labels [1] https://www.masakhane.io</sample>
    <sample id="1267">The text in the image is as follows: Recommendations 1. Keep a record of all relevant design choices made throughout building datasets or models. 2. Do NLP research through the lens of perspectivism: a. Share disaggregated dataset labels b. Use modeling techniques that can handle annotator disagreement. 3. Building specialized datasets and models with and for specific communities is valuable for inclusive NLP (e.g., Masakhane initiative¹). [https://www.masakhane.io]</sample>
    <sample id="1268">Thanks Dashboard Link: nlpositionality.cs.washington.edu/ Paper: bit.ly/NLPositionality-Paper/</sample>
    <sample id="1269">To put them into the right order.</sample>
    <sample id="1270">The authors recommended that model owners should increase transparency about bias mitigation methods to ensure accountability and trust in the models.</sample>
    <sample id="1271">Minimal-pair unacceptable inputs are sentences that the model predicts with a higher probability than they should, indicating an error in its understanding of language.</sample>
    <sample id="1272">The evaluation metrics used are NER (Named Entity Recognition), CER (Character Error Rate), F1, and EMR (Exact Match Rate).</sample>
    <sample id="1273">Krippendorf's alpha</sample>
    <sample id="1274">Wikipedia</sample>
    <sample id="1275">The authors of the paper are affiliated with Heinrich Heine University Düsseldorf, Germany.</sample>
    <sample id="1276">MultiInstruct is different from other benchmarks because it focuses on instruction tuning for multimodal pre-trained models, unlike previous works that mainly focused on language-only tasks.</sample>
    <sample id="1277">There are three authors involved in the paper.</sample>
    <sample id="1278">Binary coordination is a concept in linguistics that refers to the relationship between two elements, typically words or phrases, and how they are coordinated with each other. In this context, it likely involves analyzing the structure of sentences where binary (two-part) units like words or phrases interact within larger linguistic constructs such as characters, syllables, or words. The graphs shown might be illustrating various aspects of this interaction across different levels of language analysis.</sample>
    <sample id="1279">The average length of the prompts used in this study was about 12 words.</sample>
    <sample id="1280">The findings imply that smaller models can surpass larger language models when properly trained on suitable datasets.</sample>
    <sample id="1309">The three pre-training strategies investigated in the work are: 1. From scratch with full model construction, using CamemBERT as a French generic model and PubMedBERT as an English medical one; 2. Continual pre-training using existing pre-trained models (CamemBERT for French and PubMedBERT for English); 3. Continual pre-training using existing pre-trained models but adding additional training on clinical data from Pubmed.</sample>
    <sample id="1310">The factor of overfitting due to test reuse specifically is not observed.</sample>
    <sample id="1311">The quality of simplification was evaluated using metrics such as BLEU and F1 scores.</sample>
    <sample id="1312">The text in the image says 'Existing LMs'.</sample>
    <sample id="1347">Cognitive dissonance is two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent.</sample>
    <sample id="1348">GPT-4 is the most liberal language model.</sample>
    <sample id="1349">Yes, cumulative training performs better than iterative.</sample>
    <sample id="1350">Sara Papi</sample>
    <sample id="1351">The data was taken from transcripts of TED Talks.</sample>
    <sample id="1352">The image contains text in a presentation slide format. Here is the transcription of the English content from the image: Title: Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination Adam Przepiórkowski and Michał Woźniak Institute of Computer Science Polish Academy of Sciences ul. Jana Kazimierza 5, 01-248 Warsaw University of Warsaw ACL 2023 The title "Dependency Structure of Coordination" appears at the top right corner with an icon next to it. Below this, there are sections labeled Bouquet/Stanford (Universal Dependencies), Chain/Moscow, Conjunction-headed/Prague, and Multi-headed/London, each followed by examples related to dependency structures.</sample>
    <sample id="1353">The English content in the image is as follows: Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1354">The text in the image is structured as follows: Title: Dependency Structure of Coordination Subtitle: Bouquet/Stanford (Universal Dependencies): - Homer loves Lisa, Bart, and Maggie. Subtitle: Chain/Moscow: - Homer loves Lisa, Bart, and Maggie. Subtitle: Conjunction-headed/Prague: - Homer loves Lisa, Bart, and Maggie. Subtitle: Multi-headed/London: - Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1355">The image shows a slide from a presentation titled "Dependency Structure of Coordination." The slide is divided into four sections, each describing different approaches to dependency structure in coordination. Each section includes an example sentence: "Homer loves Lisa, Bart, and Maggie."

1. **Bouquet/Stanford (Universal Dependencies):**
   - Shows a hierarchical tree diagram with nodes labeled '0' at the top.

2. **Chain/Moscow:**
   - Similar hierarchical tree diagram as above but without the node labels on the left side.

3. **Conjunction-headed/Prague:**
   - Another hierarchical tree diagram similar to Chain/Moscow but again without node labels on the left side.

4. **Multi-headed/London:**
   - Final hierarchical tree diagram that also lacks node labels on the left side.

Each approach uses slightly different notations for representing dependencies between words in sentences involving coordinated structures.</sample>
    <sample id="1356">The English content in the image is as follows: Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1357">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Praque: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1358">&lt;no_answer&gt;</sample>
    <sample id="1359">The English content in the image is as follows:</sample>
    <sample id="1360">The English content of the image is as follows:</sample>
    <sample id="1361">The English content in the image is as follows: Title: Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read it. bad</sample>
    <sample id="1362">The text in the image is as follows: Title: Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read it yesterday. bad</sample>
    <sample id="1363">The English content in the image is: Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read yesterday it. bad</sample>
    <sample id="1364">The English content in the image is as follows: "Dependency Length Minimization (DLM)" and "Word order tends to minimize dependency lengths:"</sample>
    <sample id="1365">The English content in the frame is: Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read it yesterday. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read this absolutely fascinating book about bees yesterday.</sample>
    <sample id="1366">The English content in the image is as follows: "Dependency Length Minimization (DLM)" and "Word order tends to minimize dependency lengths:" with examples of sentences like "Marge read it yesterday" labeled as good or bad based on their dependency length.</sample>
    <sample id="1367">The English content in the image is as follows: 'Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read yesterday it. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read this absolutely fascinating book about bees yesterday. good'</sample>
    <sample id="1368">The English content in the image is: Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read yesterday it. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read this absolutely fascinating book about bees yesterday. good</sample>
    <sample id="1369">The image contains text that reads: 'Dependency Length Minimization (DLM)' and 'Word order tends to minimize dependency lengths:' followed by several sentences with different structures. The words in the sentences are color-coded, such as red for "it" and green for other parts of the sentence. There is also a person visible on the right side of the image who appears to be speaking or presenting something related to the content shown.</sample>
    <sample id="1370">The English content in the image is: Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read yesterday it. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read yesterday this absolutely fascinating book about bees. good</sample>
    <sample id="1371">The English content in the image is as follows: Title: Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read it yesterday. bad Marge read this absolutely fascinating book about bees yesterday. good Marge read this absolutely fascinating book about bees yesterday. good</sample>
    <sample id="1372">The English content in the image is as follows: 'Conjunct Lengths in English' and 'Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016)'.</sample>
    <sample id="1373">The text in the image is about "Conjunct Lengths in English." It discusses statistics on coordination extracted from an enhanced version of the Penn Treebank, referencing studies by Marcus et al. (1993), Ficler and Goldberg (2016). The main point highlighted is that left conjuncts tend to be shorter than right conjuncts when observed before a certain element or event. Additionally, it mentions that this tendency grows with length difference but only becomes noticeable under specific conditions related to the governor's position within sentences like "I saw Bart and Lisa; Homer came and sneezed," as well as contrasting scenarios where something happens either on the left or right side of the sentence structure.</sample>
    <sample id="1374">The text in the image is as follows: "Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed Gibson et al. 1996:88-90) but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed)."</sample>
    <sample id="1375">The text in the image is about conjunct lengths in English. It mentions statistics on coordination extracted from an enhanced version of the Penn Treebank by Marcus et al., 1993, and Ficler and Goldberg, 2016. The key points include: - Left conjuncts tend to be shorter (observed before). - This tendency grows with length difference (briefly noticed in Gibson et al., 1996; pages 88-90). Examples provided are: - When the governor is on the left or absent ("I saw Bart and Lisa; Homer came and sneezed"). - When it is on the right ("Ted and Ned laughed").</sample>
    <sample id="1376">The text in the image is about conjunct lengths in English. It mentions statistics on coordination extracted from an enhanced version of the Penn Treebank, referencing works by Marcus et al., 1993; Ficler and Goldberg, 2016. The slide notes that left conjuncts tend to be shorter (observed before) and this tendency grows with length difference (briefly noticed in Gibson et al., 1996:88-90). Additionally, it states that when only the governor is on the left or absent (e.g., "I saw Bart and Lisa; Homer came and sneezed"), there are specific examples provided ("not when it is on the right (Ted and Ned laughed)").</sample>
    <sample id="1377">The English content in the image is as follows: "Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): left conjuncts tend to be shorter (observed before), this tendency grows with length difference (briefly noticed in Gibson et al. 1996:88–90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed); not when it is on the right (Ted and Ned laughed)."</sample>
    <sample id="1378">The text in the image is about conjunct lengths in English. It discusses statistics on coordination extracted from an enhanced version of the Penn Treebank by Marcus et al., 1993, Ficler and Goldberg, 2016. The points mentioned include: - Left conjuncts tend to be shorter (observed before). This tendency grows with length difference (briefly noticed in Gibson et al., 1996; 88-90). - But only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed). Not when it is on the right (Ted and Ned laughed).</sample>
    <sample id="1379">The text in the image is about "Conjunct Lengths in English." It discusses statistics on coordination extracted from an enhanced version of the Penn Treebank, referencing studies by Marcus et al. (1993) and Ficler and Goldberg (2016). The key points include: 1. Left conjuncts tend to be shorter than right conjuncts (observed before), with a length difference that grows as observed briefly noticed in Gibson et al. (1996): 88–90%. However, when the governor is on the left or absent (as seen in examples like Bart and Lisa; Homer came and sneezed), this effect disappears. 2. When not on the right (as shown in Ted and Ned laughed), the effect also disappears.</sample>
    <sample id="1380">The English content in the image includes a figure caption that reads: 'Figure 1: Proportions of shorter left-to-right contacts depending on the absolute difference of contact lengths (with confidence bands).'</sample>
    <sample id="1381">The English content in the image is as follows:

1. Figure 1: Proportions of shorter left-justified contracts depending on the absolute difference of contract lengths (with confidence bands)

2. The text within each graph:
   - "No governor (length in CHARACTERS)"
   - "No governor (length in SYLLABLES)"
   - "No governor (length in WORDS)"

3. The axes labels for each graph include terms like:
   - "Governor on the LEFT length in CHARACTERS"
   - "Governor on the LEFT length in SYLLABLES"
   - "Governor on the LEFT length in WORDS"

4. Additional descriptive texts outside the graphs, such as:
   - "What we see here is that when the governor..."

5. There are no other visible pieces of English text in the image.

This structured description covers all identifiable textual elements present in the provided image.</sample>
    <sample id="1382">The image contains a figure with multiple line graphs. Each graph represents the proportion of shorter left conjuncts depending on the absolute difference in conjunct length (with confidence bands). The x-axis is labeled "Absolute difference in words," and the y-axis varies for each graph, indicating different conditions such as "No governor (length in CHARACTERS)," "No governor (length in SYLLABLES)," and "No governor (length in WORDS)." There are three rows of graphs: - Top row: Governor on the LEFT length in CHARACTERS, Governor on the LEFT length in SYLLABLES, Governor on the LEFT length in WORDS - Middle row: Governor on the RIGHT length in CHARACTERS, Governor on the RIGHT length in SYLLABLES, Governor on the RIGHT length in WORDS - Bottom row: Governor on the RIGHT length in CHARACTERS, Governor on the RIGHT length in SYLLABLES, Governor on the RIGHT length in WORDS Additionally, there is text at the bottom of the image that reads: "Figure 1: Proportions of shorter left conjuncts depending on the absolute difference of conjunct length (with confidence bands)."</sample>
    <sample id="1383">The content of the image is a slide from a presentation titled "Compatibility with Dependency Structures of Coordination." The slide compares different dependency structures and their compatibility with coordination in sentences. It lists four types of structures: Bouquet/Stanford (Universal Dependencies), Chain/Moscow, Conjunction-headed/Praque, and Multi-headed/London. Each structure type has an example sentence ("Homer loves Lisa, Bart, and Maggie.") followed by a decision on its compatibility with coordination, marked as either 'NO' or 'YES.'</sample>
    <sample id="1384">See the paper for the full argument Talk to us at the poster session</sample>
    <sample id="1385">The speaker's name is Matthias Lindemann.</sample>
    <sample id="1386">Cross-lingual transfer is a process where you train on one source language and then use that training to make predictions in another language.</sample>
    <sample id="1387">The affiliations of the authors are Saarland University, Amazon Alexa, and the University of Vienna.</sample>
    <sample id="1388">The authors use BLEU and average latency as their measures.</sample>
    <sample id="1416">Trees are usually not given and need to be obtained somehow. This can be complicated in some cases, a computationally expensive process. Typically this involves considerable formalism-specific pre-processing of the logical forms for example to handle variable symbols. Obtaining trees may also involve specialized grammar induction procedures.</sample>
    <sample id="1417">Shuheng Liu and Alan Ritter are affiliated with the School of Interactive Computing at Georgia Institute of Technology.</sample>
    <sample id="1495">It stands for Annotating Behaviors in Chat.</sample>
    <sample id="1496">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until the year 2012.</sample>
    <sample id="1527">The affiliations of the authors are Saarland University, University of Amsterdam, and UCA.</sample>
    <sample id="1528">The speaker's name is Siyu Yuan.</sample>
    <sample id="1529">There are 5 authors involved in the paper.</sample>
    <sample id="1530">The approach is compared to the state-of-the-art architecture specifically tailored for SimultSimST.</sample>
    <sample id="1531">MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning Zhiyang Xu* Ying Shen* Lifu Huang Department of Computer Science, Virginia Tech *Equal Contribution</sample>
    <sample id="1532">The image shows a slide from a presentation about pre-trained language models for downstream tasks. The title of the slide is "Pre-trained Language Models for Downstream Tasks." There are three main sections on the slide: 1. (A) Pre-train &amp; fine-tune (BERT, T5): This section explains that BERT and T5 require many task-specific examples to be fine-tuned effectively. It includes diagrams showing the process flow from pre-training to inference on specific tasks. 2. (B) Prompting (GPT-3): This section describes how GPT-3 improves performance via few-shot prompting, which allows it to infer on task A with fewer training examples. 3. (C) Instruction tuning (FLAN): This section details FLAN's approach, where an instruction can tune many tasks in parallel using natural instructions without needing unseen data. At the bottom of the slide, there is a caption that reads: "Figure 2: Comparing instruction tuning with pre-train &amp; fine-tune and prompting." Additionally, there is a credit line at the bottom left corner that says: "Image credit: Wei, Jason, et al., 'Fine-tuned language models are zero-shot learners.'"</sample>
    <sample id="1533">The image shows a slide from a presentation titled "Pre-trained Language Models for Downstream Tasks." The slide is divided into three sections: (A) Pretrain-finetune (BERT, T5), (B) Prompting (GPT-3), and (C) Instruction tuning (FLAN). Each section describes different methods of training language models. At the bottom of the slide, there is a caption that reads, "Figure 2: Comparing instruction tuning with pretrain-finetune and prompting." Additionally, there is an image credit at the bottom left corner that says, "Image credit: Wei, Jason, et al., 'Finetuned language models are zero-shot learners.'"</sample>
    <sample id="1534">The text in the image says 'Language-only'.</sample>
    <sample id="1535">Instruction Tuning on Multimodal Pre-trained Models</sample>
    <sample id="1536">The content of the image is a presentation slide with white text on a black background. The main title reads "Imbalance in Instructional Datasets between NLP and Multimodal." There is also an inset picture at the bottom right corner showing part of a person wearing glasses, but no additional details about this individual are provided within the description.</sample>
    <sample id="1537">The text in the frame is as follows: "Imbalance in Instructional Datasets between NLP and Multimodal 1600+ Language-only instruction tasks NO large-scale, publicly-available multimodal instruction tasks Wang, Yizhong, et al. 'Benchmarking generalization via in-context instructions on 1600+ language tasks' arXiv preprint arXiv:2309.14758"</sample>
    <sample id="1538">Here we present multi instruct the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks covering ten broad categories.</sample>
    <sample id="1539">The image contains a detailed diagram titled "MULTINSTRUCT" and provides information about the first multimodal instruction tuning benchmark dataset. The main elements in the image include: 1. A large heading at the top that reads "MULTINSTRUCT." 2. Subheading text stating, "The first multimodal instruction tuning benchmark dataset." 3. A complex flowchart or diagram divided into several sections with various tasks listed under categories such as Visual Relationship, Grounded, Region Understanding, and Image Text. Each section includes specific tasks like Object Identification, Open VQA, Referential Expression, etc. 4. To the right of the diagram, there is additional descriptive text listing key features of the dataset: - 62 diverse multimodal tasks - 10 broad groups - 5 expert-written instructions Below this description, there is an illustration caption reading, "Figure 2: Task Groups Included in Multinstruct. The yellow boxes represent tasks used for evaluation, while the white ones indicate training sets." This figure visually represents how different task groups are categorized within the Multinstruct framework.</sample>
    <sample id="1540">The text in the image is as follows: "OFA (One For All) A unified multi-modal pre-trained model that is capable of performing both understanding and generation tasks with single or multiple modalities. OFA has a unified vocabulary for language, image tokens and the coordinates of a bounding box."</sample>
    <sample id="1541">Sure, here it is: Here we show some example instances from our multi-instruct dataset.</sample>
    <sample id="1542">MULTINSTRUCT Figure 1: Example Instances from MULTINSTRUCT for Four Tasks. Grounded Caption Input: Generate a caption for the image. Output: A man and white tennis racket Text Localization Input: Select the region that contains the text "don". Output:&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt; Referring Expression Selection Input: Select the object of the region described by "A blue train in the picture" Options:&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 465 593 556&lt;/box&gt;&lt;box&gt;625 465 706 556&lt;/box&gt;&lt;box&gt;738 465 819 556&lt;/box&gt;&lt;box&gt;851 465 932 556&lt;/box&gt;&lt;box&gt;289 465 370 556&lt;/box&gt;&lt;box&gt;398 465 479 556&lt;/box&gt;&lt;box&gt;512 46</sample>
    <sample id="1543">The English content in the image is as follows: "We follow the method from OFA and formulate all the tasks in a unified sequence to sequence format, in which the input text, images, instructions, and bounding boxes are represented in the same token space."</sample>
    <sample id="1544">The text on the screen says "Multi-modal Instruction Tuning".</sample>
    <sample id="1545">The text in the image is as follows: Multi-Modal Instruction Turning Training Dataset Construction: Use 53 tasks from 9 groups for training. Sample 10,000 instances per task. Testing Dataset Construction: Reserve the entire Commonsense Reasoning group for testing. Select additional 5 tasks from VQA and Miscellaneous groups. We use all the instances in the test split for each task. Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="1546">The English content in the image is: Multi-Modal Instruction Turning Training Dataset Construction: Use 53 tasks from 9 groups for training. Sample 10,000 instances per task. Testing Dataset Construction: Reserve the entire Commonsense Reasoning group for testing. Select additional 5 tasks from VQA and Miscellaneous groups. We use all the instances in the test split for each task. Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="1547">The text in the image is as follows: Implementation Details Training details: Pre-trained OFA-Large model (472M) Mix all the instances for all tasks. Each instance is randomly combined with one of its five instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1548">The image contains text related to the implementation details of a model. Here is the transcription: Implementation Details Training details: Pre-trained OFA-Large model (472M) Mix all the instances for all tasks Each instance is randomly combined with one of its five instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1549">The text in the image is about "Implementation Details". It includes two main sections: Training details and Testing details. Under Training details, it mentions a pre-trained OFA-Large model with 472M parameters, mixing all instances for tasks, and each instance being randomly combined with one of its instruction templates. In the Testing details section, it states that five experiments are conducted to evaluate the model using instructions from each experiment, reporting mean and maximum performance along with standard deviation across all five experiments.</sample>
    <sample id="1550">The text in the image is about evaluation metrics for different types of tasks. It mentions that for multi-modal classification tasks, accuracy is reported. For multi-modal generation tasks like Commonsense VQA, Text VQA, Grounded VQA, Visual Text Extraction, and Visual Dialogue, Rouge-L is used as a metric. Additionally, for NLP tasks, Rouge-L is also reported. The slide further explains that aggregated performance scores are computed based on mean model performances across all multimodal and NLP unseen tasks using Rouge-L as the score measure for most tasks and Accuracy as the only metric with accuracy as a metric.</sample>
    <sample id="1551">The English content in the frame is as follows: "Sensitivity How sensitive the model is towards variety of instructions for the same task: - Ability to consistently produce the same results for the same task, regardless of slight variations in the wording of instructions."</sample>
    <sample id="1552">The effectiveness of instruction tuning on multi-instruct is significant.</sample>
    <sample id="1553">The English content in the image is as follows: "Effectiveness of Instruction Tuning on MULTIINSTRUCT" and "Table 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold."</sample>
    <sample id="1554">Here we can see as the amount of task increases, the model achieve better performance and in the meantime lower sensitivity.</sample>
    <sample id="1555">The image shows a presentation slide with the title "Effect of Diverse Instructions on Instruction Tuning." The main content includes a bullet point stating, "OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity." Below this text is a table labeled "Table 3: Effect of Different Number of Instructions. Performance of OFA-Mainfinet tuned on different numbers of instructions." The table has two rows showing data for 1 instruction (Aggregated Performance: 42.81, Sensitivity: 24.62) and 5 instructions (Aggregated Performance: 47.82, Sensitivity: 10.45). In the bottom right corner of the slide, there is an inset picture of a person wearing glasses.</sample>
    <sample id="1556">The text in the image reads: 'Effect of Fine-tuning Strategies on Model Sensitivity. Instruction tuning on Multilinstruct can significantly reduce the sensitivity of OFA. Transfer learning from Natural Instructions dataset can further reduce the sensitivity of the model. Figure 4: Model Sensitivity on Unseen Evaluation Tasks. Lower is better.'</sample>
    <sample id="1557">The text in the image is as follows: Title: Zero-Shot Performance on NLP Tasks - Instruction Tuning on Multinstruct can improve zero-shot performance on unseen NLP tasks. - The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset. Table 4: Zero-Shot Performance on NLP Tasks. The report is performed in Rouge-L and the best performance is in bold. Model | RougeL OFA 2.25 OFA-Multinstruct 12.18 Transfer Learning from NATURAL INSTRUCTIONS OFA-Nat 43.64 OFA-Seg 43.97 OFA-Segment 30.79</sample>
    <sample id="1558">The content of the image is a slide from a presentation with the title "Conclusion" at the top. The slide lists several bullet points about a large-scale multi-modal instruction tuning dataset: 1. It mentions that this is the first large-scale multi-modal instruction tuning dataset, which contains 62 multi-modal tasks across 10 broad categories. 2. It highlights an improvement in the zero-shot capability of OFA through instruction tuning. 3. Several transferring learning techniques are explored and their benefits shown. 4. A new metric called sensitivity is designed. In the bottom right corner of the slide, there is a small video feed showing a person speaking or presenting.</sample>
    <sample id="1559">The text in the image is as follows: "One More Thing We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon!" There is also a QR code present.</sample>
  </task>
</testset>