<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modeli linguistici sono dati web scolpiti su scala grande, come quelli utilizzati per il pretraining dei modelli di linguaggio. Questi dati spesso includono notizie politiche, che sono ben rappresentati in corpus come il C4 Corpus, che coprono fonti come il New York Times, The Guardian, Huffington Post, ecc. Questo può portare a bias politici nei modelli, che possono influenzare le performance nei compiti downstream e generare questioni di equità.</sample>
    <sample id="1">Le affiliazioni degli autori dell\'articolo sono McGill University, Mila, e Microsoft Research.</sample>
    <sample id="2">Title: LayoutMask: Enhancing Text-Layout Interactions in Visually-Rich Document Understanding

Abstract:

In recent years, pre-training techniques have significantly advanced the field of Visually-Rich Document Understanding (VrDU), particularly with the introduction of self-supervised pre-training multi-modal models. However, existing document pre-training models face challenges with reading order issues, often relying on global 1D positions encoded as global 1D positions, as illustrated in Figure 1. This paper introduces LayoutMask, a novel pre-trained model designed to address these issues by leveraging text and layout information.

LayoutMask differs from previous studies in three key aspects: choice of 1D position, mask strategy, and pre-training objectives. Instead of using global 1D positions, LayoutMask employs local 1D positions, which do not provide cross-segment orders. To infer global reading order, LayoutMask integrates 1D position, 2D position (layout), and semantic information, promoting in-depth text-layout interactions.

To further enhance these interactions, LayoutMask introduces two novel masking strategies: Whole Word Masking (WWM) and Layout-Aware Masking (LAM). WWM sets masks at the word level, eliminating semantic relations between masked and unmasked tokens, thus requiring the model to find more context to predict masked words. LAM masks the first and last words of each segment, encouraging the model to pay more attention to their contexts in preceding or succeeding segments, promoting cross-segment order learning.

Additionally, LayoutMask incorporates a new pre-training objective, Masked Position Modelling (MPM), which involves recovering randomly masked 2D positions during pre-processing. MPM is akin to a cloze test, where masked words are refilled at the correct positions in the original documents. This task requires the model to infer 2D positions based on semantic relations and spatial context, promoting text-layout interactions and better layout representations.

Experimental results demonstrate that LayoutMask outperforms existing models using different layout information. For 1D positions, Local-1D outperforms Global-1-D on FUNSD and SROIE, while falling slightly behind on CORD. The performance gap is attributed to the entity "Total," which has multiple misleading numbers with the same content as its ground truth. This example highlights the limitations of global 1D positions and the advantages of Local-1D in handling such cases.

In conclusion, LayoutMask significantly enhances text-layout interactions and improves layout representations in VrDU tasks, offering a promising approach for future research in document understanding.</sample>
    <sample id="3">Ciao! Benvenuti alla presentazione di DEPLAIN, un nuovo corpus per l'identificazione del testo in tedesco a livello di documento e a livello di frase. Il mio nome è Regina Stodden, e guiderò voi attraverso la prima parte della presentazione. Prima di tutto, definiamo la semplificazione del testo. La semplificazione è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo specifico, come persone con problemi di lettura o non-native speakers. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testo, ad esempio di documenti o frasi. Ecco un esempio di una coppia parallela di frasi complessi in tedesco e la loro traduzione in lingua semplice. Per semplificare la frase, sono possibili diverse tecniche, come sostituzione lessicale, eliminazione di clausole, riordinamento o inserimento di parole. Proponiamo ora il nostro nuovo corpus, DEPLAIN, perché negli ultimi anni ci sono stati alcuni problemi con i corpus esistenti. Ad esempio, questi corpus qui sono troppo piccoli per addestrare un modello di semplrificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere errorati nelle loro allineamenti. Pertanto, proponiamo il nostro nuovo corpus DEPLAIN, che è suddiviso in due sottocorpi: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-APA è basato su testi di notizie. In DEPLAIN-apa, abbiamo allineato manualmente 483 documenti, risultando in circa 13.000 coppie di frasi parallele. Per DEPLAIN-web, questo corpus include diversi domini e abbiamo allineato tutti questi 750 documenti, su un lato manualmente e su un lato con metodi di allineamento automatici. In totale, risultano 30.450 coppie di frasi. Abbiamo analizzato un po' di più le nostre coppie di frasi, quindi per esempio, sul tipo di semplificazione. Come puoi vedere qui, i testi della Bibbia sono molto più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per apprendenti di lingua. Su tutti i livelli, in relazione, ad esempio, alla semplificazione lessicale, alla semplificazione strutturale, anche il livello complessivo di semplificazione. Inoltre, puoi vedere che il nostro corpus DEPLAIN ha una grande varietà di trasformazioni di semplificazione diverse. Ad esempio, nella corpus DEPLAIN-apa abbiamo molte più riordinamenti e aggiunta di parole rispetto a quella del web corpus. D'altra parte, nel corpus web abbiamo molte più ripetizioni. Ora vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro dataset DEPLAIN. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatici. Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in due lingue diverse e vogliamo estrarre le allineamenti delle frasi in entrambi i documenti. Ma in nostro caso d'uso, stiamo cercando di estrarre le allineamenti tra le frasi di due documenti paralleli scritti nella stessa lingua, con lo stesso contenuto, ma a diversi livelli di complessità. E ora, con il nostro dataset DEPLAIN, che ha frasi manualmente allineate, possiamo utilizzare queste frasi come standard di allineamento oro per valutare alcuni dei metodi di allineamento proposti. Abbiamo fatto alcune adattamenti ai metodi proposti e abbiamo pubblicato tutte queste adattamenti e i codici per eseguire le nostre esperienze nel nostro articolo. Alla fine, abbiamo concluso che il metodo di allineamento automatico migliore da utilizzare per la semplificazione del testo</sample>
    <sample id="4">Il nome della relatrice o del relatore è Kayo Yin.</sample>
    <sample id="5">Per ottenere l'accuratezza del 82%-87%, sono stati utilizzati modelli basati su T5 XL. Questi modelli hanno accesso a alcune informazioni di background parzialmente sovrapposte.</sample>
    <sample id="6">Title: Towards Unifying Multi-Lingual and Cross-Linguistic Summarization

Abstract:

In this work, we introduce many-to-many summarization, a unified framework that combines multilingual and cross-lingual summarization into a single model. Our approach aims to generate summaries in any target language from a document in any source language. We conducted preliminary studies to compare our many-to-many summarization with previous multilingual and cross-lingual summarization methods. Our findings show that many-to-many summarization can better transfer task knowledge across languages than previous methods. We propose PISCES, a pre-trained many-to-</sample>
    <sample id="7">I tagger CoNLL-2000 funzionano ancora, ma la loro generalizzabilità è influenzata da vari fattori come l'architettura del modello, la dimensione del modello e il numero di esempi di fine-tuning. La nostra ricerca ha dimostrato che i modelli basati su Transformer tendono a generalizzare meglio, che i modelli più grandi tendono a avere una migliore generalizzabilità e che un numero maggiore di esempi di fine-tuning migliora le prestazioni. Inoltre, abbiamo trovato che la performance degli tagger CoNLL-2002 è stata degradata a causa del drift temporale, e non dell'overfitting adattativo. In sintesi, i tagger CoNLL-2001 funzionano ancora, ma è necessario migliorare la loro generalizzabilità attraverso l'uso di architetture di modello più avanzate, modello più grandi e un numero maggiore di esemppi di fine-tuning.</sample>
    <sample id="8">La novità del metodo di valutazione ABC-Eval risiede nel suo approccio a annotare explicitamente i comportamenti specifici del modello di chat, come rispondere con informazioni irrilevanti o contraddire se stesso o il partner. Questo riduce la soggettività dell'evaluazione umana e consente una misurazione più precisa e affidabile delle diverse dimensioni della qualità del dialogo.</sample>
    <sample id="9">Il successo dell'attuale approccia scarsamente supervisionato si basa in larga misura sulla disponibilità di dati di validazione puliti. Questi dati sono cruciali per garantire che i modelli siano generalizzabili e non memorizzino l'errore nelle annotazioni deboli. Senza dati di validazione puliti, le performance dei modelli diminuiscono significativamente, indicando che l'annotazione manuale pulita è essenziale per il successo delle tecniche di apprendimento scarsamente supervisionato. Inoltre, l'incremento del numero di campioni di validazione puliti migliora ulteriormente le performance, e l'utilizzo continua delle annotazioni pulite per il fine-tuning può portare a migliori risultati rispetto ai metodi WSL tradizionali.</sample>
    <sample id="10">Per migliorare il punteggio, si possono fare i seguenti progressi:

1. **Aumentare la qualità del background knowledge**: Fornire informazioni più dettagliate e accurate sui contesti e le entità per aiutare i modelli a comprendere meglio le differenze tra le entità e le espressioni indirette.

2. **Migliorare la generazione dei contesti**: Utilizzare metodi più sofisticati per generare contesti che siano più realistici e rappresentino meglio le situazioni di conversazione.

3. **Ottimizzare l'annotazione dei dati**: Assicurare che le annotazioni siano più precise e coerenti, riducendo l'errore umano e migliorando la qualità del dataset.

4. **Utilizzare modelli più avanzati**: Sperimentare con modelli più complessi e capaci di capire meglio le relazioni tra le entità e le espressioni.

5. **Raffinare il processo di selezione delle entità**: Scegliere entità che siano più facilmente disambiguabili e che possano essere selezionate con espressioni indirette più precise.

6. **Migliorare la valutazione del modello**: Utilizzare metodi di valutazione più accurati per misurare la performance del modello e identificare aree di miglioramento.

7. **Integrare informazioni contextuali**: Fornire informazioni contextuali più complete e dettagliate per aiutare i modelli a comprender meglio le relazioni tra le entità.

8. **Raffinare il processo della selezione delle espressioni indirette**: Scegliere espressioni indirette che siano più naturali e realistiche, riducendo l'errore umani.

9. **Utilizzare metodi di ensembling**: Combinare i risultati di più modelli per ottenere una performance migliore.

10. **Migliorare la valuta del dataset**: Assicurare che il dataset sia più grande e rappresentativo, con un numero sufficiente di esempi per ogni tipo di entità e espressione indiretta.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presents his work on "Do Androids Laugh at Electric Sheep? Understanding Benchmarks from The New Yorker Caption Contest." He discusses how large language models can generate and explain jokes, but their understanding of humor is still limited. Hessel and his team operationalize The New Yorker Caption Contest data into three tasks: matching, quality ranking, and explanation generation. They find that the best model, CLIP fine-tuned on the annotated corpus, achieves 62% accuracy on the matching task, while humans achieve 94%. GPT-4, even with human-authored descriptions, still performs poorly on these tasks. Hessel highlights the challenges in understanding humor and invites the audience to explore their dataset and leaderboard.</sample>
    <sample id="12">Ci sono cinque autori coinvolti nell'articolo: Dawei, Xiaoyu Shen, Marius Mosbach (Marius Mosbach), Andreas Stephan, e Dietrich Klakow.</sample>
    <sample id="13">Title: "Finding the SWEET Spot: Analysis of Adaptive Inference in Low Resource Settings"

Abstract:
Adaptive inference is a technique for reducing the inference time of large language models by using low-capacity models for easy samples. Two common methods are Multi Model and Early Exit. Multi Model stores multiple models and runs them sequentially until a classifier halts computation, while Early Exit uses multiple classifiers trained together and halts computation when a classifier decides. Multi Model is versatile and easily extended but expensive to store and suffers from overhead. Early Exit is faster and memory efficient but shares model parameters, leading to conflicting gradients and lower performance.

Our research hypothesizes that conflicting gradients in Early Exit models degrade performance. We tested this by comparing individual Early Exit models with separate Multi Model classifiers, finding that Multi Model outperformed Early Exit by an average of 2.2%. We introduced SWEET, a novel fine-tuning method for Early Exit architectures that separates weights in each layer to avoid conflicting gradients. SWEET closes the gap between Early Exit and Multi Model, but later classifiers are negatively affected.

Our findings show the existence of conflicting gradients in Early Exit and provide a fair comparison of Early Exit and Multi Model methods. We introduce SWEET, which motivates future research and fine-tuning algorithms tailored to the early exit architecture.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questo discorso riguarda la Struttura di Dipendenza della Coordinazione. Come sapete, ci sono diverse strutture di dipendenza assume da diverse teorie e approcci al corpus. Ad esempio, nelle dipendenze universali, la struttura della coordinazione, Lisa, Bart, e Maggie, è tale che il primo congiunto è la testa della struttura di coordinazione intera. Quindi in questo caso, Lisa. Un approccio simile è presunto in teoria del testo di Igor Mel'čuk, dove ancora una volta, tutta la struttura di coordinazione è guidata dal primo congiunto. Quindi questi due approcci sono asimmetrici. Giusto. Sottolineano uno dei congiunti. Ora questi sono approcci asimmetrici alla struttura di coordinazione, come quello di Praga. L'approccio congiunzione guidata presunto nelle dipendenze di Praga, dove le strutture di coordinazione sono guidate dal congiungente. Quindi, otteniamo alcune dipendenze dall'inizio a tutti i congiunti. E infine, c'è anche un approccio multi-caduta che viene utilizzato, ad esempio, nella Grammatica di Hudson, dove dicono che tutti i congiunti sono teste della struttura di coordinazione. Quindi otteniamo dipendenze dal governatore. Ecco: Lisa ama Bart e Maggie. Ora l'obiettivo di questo articolo è produrre un nuovo argomento per le strutture simmetriche di coordinazione, come queste due, e contro le strutture asimmetriche di coordinazione, come questi due. Ok. L'argomento è basato sul principio del minimo lunghezza della dipendenza che spiegherò basandomi su questi esempi. Quindi in inglese, come sapete, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli adjuncti possono essere più lontani. "Marge ha letto il libro" è buono perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto il libro ieri" è molto peggio. Giusto? Qui, tra il verbo e l'oggetto diretto c'è un adjunct: "ieri". Tuttavia, questo effetto può essere mitigato quando l'oggetto diretto è molto pesante e molto lungo. Poiché allora può essere spostato alla posizione dopo l'adjunct. Questo è illustrato qui. Quindi entrambe queste frasi sono buone. "Marge ha letto questo libro assolutamente affascinante sulla vita delle api ieri." È buona la forma in cui invece di "it", abbiamo questo lungo NP. Ma è anche buona dire "Marge ha ieri letto questo assolutamente affascinante libro sulla vita delle api." Quindi la ragione qui è che questo è possibile perché, anche se questa frase viola il principio generale grammaticale che gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio del minimo lunghezza della dependenza, che dice che le dipendenze più corte sono preferite. Quindi queste due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Qui abbiamo una dipendenza da "letto" al adjunct di lunghezza 7 misurata in parole e da "letto" a "libro" di lunghezza 4, quindi insieme è 11. Quando scambiamo questi due costituenti, la somma di queste due dipendenze diventa 6. Quindi invece di 11, 6 è molto</sample>
    <sample id="15">Ci sono tre autori coinvolti nell'articolo: Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="16">I documenti della Bibbia risultano più semplificati.</sample>
    <sample id="17">Multimodal relation extraction (MRE) aims to determine the semantic relationship between entities in text, especially when data is in various forms and modalities. Traditional text-based methods often lack sufficient context, leading to ambiguous or multi-context word interpretations. To address this, MRE incorporates visual evidence, such as "Bachelor", "Gown", and "Cap", to infer relationships like JFK's graduation at Harvard. However, challenges remain, including internal-information over-utilization and external-information under-exploitation.

To tackle these issues, we propose a Graph Information Bottleneck (GIB) principle-guided feature refinement. We merge visual and textual scene graphs into a unified cross-modal graph (CMG) and fine-tune it by filtering nodes and adjusting edges. We then enrich CMG features with multimodal topic information, using attention mechanisms to integrate top-L textual and visual topic keywords.

Experiments on a widely used MRE dataset show that our method outperforms text-based methods and other multimodal baselines. Ablation studies reveal that internal-information screening and external-information exploiting both contribute to performance, with the scene graph aiding structural modeling. Grouping instances by text-vision relevance scores shows that internal-information screening is crucial for high-relevance inputs, while external-information exploiting is more useful for low-relevance inputs.

In conclusion, our approach introduces simultaneous information subtraction and addition for MRE, leveraging GIB for internal-information screening and a latent multimodal topic model for external-information exploiting. Our system achieves significant improvements over existing models on benchmarks.</sample>
    <sample id="18">Un esempio della preferenza per i coniugi a sinistra più brevi è "salt and pepper" piuttosto che "pepper and salt". Questo è stato misurato in sillabe, mostrando che i coniugi a sinistra tendono a essere più brevi.</sample>
    <sample id="19">Title: Efficient Open-Domain Question Answering: Techniques and Insights

Abstract:

This paper presents a survey on efficient open-domain question answering (QA) systems, focusing on the challenges and solutions for achieving smaller memory costs, faster inference, and comparable performance. We review the two-stage framework proposed by Danqi Chen in 2007, which involves a retrieval stage using a question encoder and document encoder, followed by a reader stage to reason out the answer. We highlight the challenges of handling large Wikipedia corpora, indexing bottlenecks, and the complexity of multiple language models.

To address these challenges, we discuss core techniques such as approximate nearest neighbor search for fast evidence retrieval, adaptive computation for efficient reading, and embedding compression to reduce index size. We also explore model size reduction strategies, including lightweight models, parameter sharing, and designing fewer models.

Our analysis of existing QA models reveals that retrieval and reader systems offer a balanced trade-off between speed, memory, and performance. Retrieval-only systems create large indexes but infer answers quickly, while generator-only systems create no index but are large models with low performance.

Based on our findings, we conclude that if resources are limited, one can consider reducing index size using generator-only systems or embedding compression, or reducing model size through knowledge distillation or designing a one-stage model.

Finally, we discuss two future works: deploying open-domain QA systems in low-power devices and considering additional evaluation metrics.</sample>
    <sample id="20">Yes, you can use the models for your research. The models, including DrBERT and ChuBERT, are freely available on Hugging Face under the MIT license, and the training scripts are available on the GitHub repository.</sample>
    <sample id="21">DEplain-apa contiene documenti di notizie.</sample>
    <sample id="22">I fattori che contribuiscono a una buona generalezza sono:

1. **Architettura del modello**: I modelli basati su Transformer tendono a generalizzare meglio sui nuovi dati.
2. **Dimensione del modello**: I modelli più grandi tendono a avere una migliore generalizzazione.
3. **Numero di esempi di fine-tuning**: Avere più esempi di fine-tuning migliora la generalizzazione.

Questi fattori lavorano insieme per ottenere una buona generalizzazione.</sample>
    <sample id="23">This paper presents a study on improving the ability of text image models to render visual text. The authors focus on the Imagen model, which uses a T5-XXL encoder to encode input text and a diffusion model to generate images. They found that while the model can generate high-quality images, it struggles with representing text, particularly when the input requires the image to contain a specific word. The authors investigate the text encoder's ability to spell words using SentencePiece tokenization and find that even the largest T5 model struggles with spelling accuracy. They also compare the performance of T5 with PaLM and ByT5 models, finding that PaLM models have near-perfect spelling accuracy but are impractical for many applications, while ByT5 models excel at spelling due to their character-level input. The authors propose an improvement to the Imagen model by concatenating a ByT5 small model's text representation to the existing text representation, which improves the model's ability to render text. The main takeaways are the WikiSpell benchmark for text-only models and the DrawText benchmark for text-to-image models.</sample>
    <sample id="24">La tendenza dei congiunti a sinistro a essere più brevi è stata misurata in base al numero di parole utilizzate per ciascun congiunto. Si è osservato che, in generale, i congiunti a sinistro tendono a essere più brevi rispetto ai congiunti a destro, soprattutto quando la relazione tra i due congiunti è governata da un verbo a sinistra o quando non c'è un governo esterno. Questo effetto è più evidente quando la differenza di lunghezza tra i due congiunti è maggiore. Questo è stato dimostrato misurando la lunghezza in parole, sylabbe e caratteri per diverse strutture di coordinazione.</sample>
    <sample id="25">Gli esperimenti per studiare l'affetto della posizione del governatore sono stati progettati analizzando le statistiche delle congiunzioni nella versione migliorata del Penn Treebank. Gli autori estraggono vari dati sulla coordinazione, misurando la lunghezza delle dipendenze cruciali tra il governatore e i congiunti. Sono stati esaminati i dati in termini di caratteri, sillabe e parole. Gli esperimenti mostrano che quando il governatore è a sinistra o assente, il congiunto a sinistra tende ad essere più breve, con una tendenza crescente con l'aumento della differenza di lunghezza tra i due congiunti. Tuttavia, quando il governatore è a destra, questo effetto scompare. Questi risultati forniscono un argomento contro le strutture asimmetriche della coordinazione e sostengono le strutture simmetriche.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciati, come quello che ha iniziato il tuo lavoro con solo 43 esempi di dissonanza, è generalmente molto inefficiente. La performance del classificatore è molto vicina al caso casuale, poiché il classificatore non riesce a catturare la classe di dissonanza. Questo problema è dovuto al problema dell'assenza di dati sufficienti per il classificatore, che è particolarmente evidente in questo caso, poiché la classe di dissonanza rappresenta solo il 3,5% delle annotazioni.

Per risolvere questo problema, il tuo lavoro utilizza una combinazione di apprendimento trasferitivo e apprendimento attivo per annotare più esempi di dissonanza con un numero minimo di annotazioni. L'apprendimento trasferitivo utilizza modelli pre-addestrati su task correlati, come la classificazione dello stile di dissonanza e la classificazione binaria di espansione e confronto, per migliorare la performance del classificatore. L'apprendimento attivo utilizza strategie come PRC per selezionare esempi rari e migliorare la performance del classificatori.

Il risultato finale è un classificatore che ha migliorato la performance di classificazione di dissonanza da 0,62 a 0,75, utilizzando una combinazione di apprendimento trasnferitivo e apprendimento attivo. Questo dimostra che un classificatore base addestrato su dato non bilanciato può essere migliorato significativamente con l'uso di apprendimento trasferitivo, apprendimento attivo e strategie di selezione di esempi rari.</sample>
    <sample id="27">L'articolo "From Pretraining Data to Language Models to Downsstream Tasks: Tracking the Trails of Political Biasses Leading to Unfair NLP Models" è stato scritto da Shangbin, un dottorando alla University of Washington. Non è specificato quanti autori sono coinvolti nel progetto, ma Shangbin è il principale autore della presentazione.</sample>
    <sample id="28">In the conversation example provided, the characters are Bob and Alice. Bob initiates the dialogue by asking Alice to remember a song they were listening to, and Alice then asks for clarification between two songs, "Easy on Me" and "I Gotta Feeling." Bob uses an indirect reference to select one of the songs.</sample>
    <sample id="29">I modelli di MT migliorano rispetto a quello indipendente dal contesto in diversi fenomeni del discorso, tra cui:

1. **Formality**: I modelli che utilizzano il contesto sono più accurate rispetto a quelli che non lo fanno.
2. **Lexical Cohesion**: I modelli che considerano il contesto sono più accurate rispeto a quelli che non lo fanno. 
3. **Ellissi**: I modelli che utilizzano il contexto sono leggermente migliori rispetto a quelli che non lo utilizzano.
4. **Pronomi**: I modelli che considerano il contesti sono leggermente migliori rispeto a quelli che non lo considerano.
5. **Verbi**: I modelli che utilizzano il contesto sono leggermente migliori rispetro a quelli che non lo fanno.</sample>
    <sample id="30">The paper "LLM-Blender" introduces a novel ensemble learning framework for large language models, addressing the challenge of selecting the optimal model for a given input. The framework leverages pairwise ranking and generative fusion to improve performance over single-model approaches. By running multiple models on an input and using a pairwise ranking module (PairRanker), the framework identifies the best-performing models for each input. The PairRanker module compares pairs of model outputs using cross-attention mechanisms, providing a more nuanced ranking than individual scoring methods. The framework then uses the top-ranked models to generate a final output through a sequence-to-sequence model. Experiments demonstrate that LLM-Blender significantly outperforms individual models and other ensemble methods, with a high correlation to oracle rankings. The paper also introduces MixInstruct, a dataset for evaluating large language models, and provides a unified codebase for future research. The key contributions are the PairRanker module for pairwise comparison and the GenFuser module for generative fusion, which together enhance the performance of large language model ensembles.</sample>
    <sample id="31">I autori dell'articolo sono Koustav Sinha, John Gauthier, Aaron Mueller, Kishka Misra, Karen Fences, Roger Lévy e Adina Williams.</sample>
    <sample id="33">Il framework NLPositionality quantifica la posizionalità confrontando le annotazioni dei dati con le predizioni e le etichette dei modelli e dei dataset. Lavora in due passaggi principali:

1. **Ripreparazione dei dati**: Annota nuovamente i dati con annotatori diversi, cercando di ottenere un numero maggiore di annotazioni per ogni istanza e raccogliendo dati demografici dettagliati.

2. **Confronto con i modelli e i dataset**: Utilizza un punteggio di correlazione di Pearson per confrontare le annotazioni demografiche con le predizioni e le eticette dei modelli e dei dataset. Questo confronto permette di valutare in che modo i modelli e i dataset sono allineati con diverse popolazioni.

In questo modo, NLPositionality fornisce una misura quantificabile della posizionalità dei modelli e dei dataset, mostrando chi sono i gruppi più allineati e chi sono quelli meno rappresentati.</sample>
    <sample id="34">CREST is a joint framework for rationalization and counterf actual text generation that combines selective rationalization and counterf actual generation to leverage their complementary strengths. The first component of CREST generates counterf actuals by masking the original input and prepending the gold label to it, and then passing the masked inputs to an editor, which fills in the masked response with new tokens. CREST counterf actuals are evaluated using both automatic metrics and human evaluation, and are found to be more valid and natural than those generated manually or by other methods. CREST counterf actuals can also be used for data augmentation, and can be used to improve the downstream models by performing rationalization with both factual and counterf actual examples. The rationales generated by CREST are also found to be more plausible and have higher counterf actual simulability than those produced by other methods. Overall, CREST is a promising approach for generating valid, fluent, and diverse counterf actuals in a controllable way, and can be used to improve the interpretability and performance of downstream models.</sample>
    <sample id="36">The paper "Learning Language-Specific Layers for Multilingual
Machine Translation" presents a novel approach to multilingual machine
translation that aims to increase the capacity per language while keeping
inference costs constant. The authors introduce Language-Specific Layers
(LSLs), which are regular transformer layers per language that are
selected and trained at inference time based on the source or target
language. The authors also propose a method for learning the best placement
of LSLs in the model, which involves training a large model with shared,
source, and target weights and selecting the component with the largest
weight. The authors evaluate their approach on WMT21 news translation
mask sources for 10 languages, including some European, Asian, and
Swahili languages, and report significant improvements over baseline
models and language adapters. The results show that the approach gives
improvements for every language, particularly for low-resource languages,
and is statistically significant for 84 out of 90 translation directions.</sample>
    <sample id="37">Il risultato dello studio precedente iniziale, in cui i soggetti umani ricevono gli stessi prompt di persona, ha rivelato che i soggetti umani possono sottolineare stereotipi razzisti e di genere. Questo metodo ha permesso di confrontare le risposte generate dai modelli linguistici con quelle umane, evidenziando così pattern di stereotipazione che i modelli stanno catturando.</sample>
    <sample id="38">In questo studio, sono state utilizzate diverse fonti di dati per analizzare la struttura di coordinazione. Le principali fonti utilizzate includono:

1. **Enhanced Version of the Penn Treebank**: Questo è un dataset ampliato della Penn Treebank, che fornisce informazioni grammaticali dettagliate sui testi inglesi.

2. **Universal Dependencies (UD)**: Questo è un modello di annotazione grammaticale che fornisce una struttura di coordinazione symmetrica, dove tutti i congiunti sono considerati come nodi uguali.

3. **Igor Mel'čuk's Meaning Text Theory**: Questo modello teorico assume una struttura di coordinazione asymmetrica, dove il primo congiunto è considerato il nodo principale.

4. **Prague Dependency Treebanks**: Questo modello assume una struttura di coordinazione symmetrico, dove la coordinazione è guidata dal congiunzione.

5. **Hudson's Word Grammar**: Questo modello assume una strutturazione multi-headed, dove tutti i congiunti vengono considerati come nodi uguali.</sample>
    <sample id="39">L'articolo menziona solo un autore, Adam Przepiórkowski.</sample>
    <sample id="40">Le attività strettamente correlate alla disonanza cognitiva includono:

1. **Studio delle dissonanze nella lingua**: Analizzare e annotare le dissonanze espresse in testi per comprendere meglio i fenomeni di dissonanza cognitiva.
2. **Annotazione dei dati**: Creare un dataset di esempi di dissonanza per l'addestramento dei modelli.
3. **Transfer Learning**: Utilizzare modelli pre-addestrati su task correlati per migliorare la performance del modello su dissonanze.
4. **Active Learning**: Implementare strategie di apprendimento attivo per annotare dati rari e migliorare il modello.
5. **Classificazione dei dati**: Utilizzare modelli di classificazione per identificare e annotare le dissonanze in testi.
6. **Aggiornamento del modello**: Aggiornare il modello con nuovi dati annotati per migliorare la performance.
7. **Strategie di apprendimento attivo**: Implementare strategie di apprendimento attentiva per selezionare e annotare dati rari.
8. **Valutazione delle strategie**: Testare diverse strategie di apprendimento attivo per determinare quale funziona meglio per la classificazione delle dissonanze.
9. **Annotazione di qualità**: Assicurare che le annotazioni siano accurate e utili per il modello.
10. **Costi di annotazione**: Considerare i costi e la fattibilità delle annotazioni per i dati rari.

Queste attività sono strettamente legate alla ricerca e alla creazione di modelli per rilevare e comprendere le dissonanze cognitiva in testi.</sample>
    <sample id="41">PeaCoK is a Persona-grounded Commonsense Knowledge Graph developed by the Natural Language Processing Lab at EPFL University in collaboration with Sony Group Corporation. It aims to represent real-world personas with rich world knowledge and complex interconnections. PeaCoK contains about 3800 personas and 40, 000 distinctive attributes, forming about 100,000 inferences or facts. The relations of personas and their attributes are framed in three dimensions, including four types of main relationships, interactivity, and distinctiveness. PeaCoK is built in three steps: selecting personas from existing commonsense graphs, inducing attributes of personas from commonsense knowledge graphs and large-scale pre-trained models, and crowdsourcing annotations using a joint human-AI majority voting scheme with an accuracy of 87% in F1. PeaCoK can help language models learn and generalize persona knowledge, as demonstrated by a BART-based common knowledge generator achieving better results than baselines. PeaCoK knowledge can also improve downstream narrative modeling, as shown by a persona-grounded dialogue generation task on the Conversational AI PersonaChat data set, where PeaCoK augmented models achieved better results in fluency, consistency, engagement, and persona expression compared to the baseline model.</sample>
    <sample id="42">L'articolo è stato scritto da un solo autore, Shuheng.</sample>
    <sample id="43">L'articolo "Transfer Learning for Dissonance Detection: Aiming at the Rare-Class Challenge" è stato presentato da Vasudha e altri coautori. Tuttavia, il numero esatto di autori non è specificato nel testo fornito.</sample>
    <sample id="44">Il framework NLPositionality differisce dagli studi precedenti in quanto confronta i dati annotati con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson, piuttosto che analizzare solo l'accordo tra annotatori o la distribuzione degli annotatori. Inoltre, NLPositionality utilizza un'piattaforma di esperimentazione online, Lab in the Wild, per raccogliere dati da un numero elevato di annotatori da 87 paesi, mentre molti studi precedenti utilizzano piattaforme come M-Turk.</sample>
    <sample id="45">La configurazione che sovrappone maggiormente al Lessico degli stereotipi è quella che utilizza il metodo "Marked Words". Questo metodo identifica le parole che distingue i gruppi marcati dai gruppi non marcati, basandosi sull'idea sociolinguistica di "marcheggiatura", che indica che un gruppo è marcati se differisce dalla norma non marcata. In questo caso, i gruppi marcati sono quelli di colore umano, mentre i gruppi non marcati sono quelli di genere maschile e di etnia bianca. Il metodo "Marked Words" utilizza il Fightin' Words method per calcolare i rapporti di log-odds pesati e distingue le parole più frequenti per ciascun gruppo marcato, confrontandole con i gruppi non marcati. Questo metodo rivela come le parole positive e non negative facilitano stereotipi e narrazioni essenzializzanti.</sample>
    <sample id="46">I sistemi commerciali confrontati sono DeepL e Google Translate.</sample>
    <sample id="47">Ciao, sono Shangbin, studente di dottorato all'Università di Washington. Oggi sto presentando il nostro lavoro "Da dati di pretraining a modelli linguistici a compiti downstream: traccia i sentieri delle bias politici che portano a modelli NLP non equi". I modelli linguistici vengono addestrati su grandi quantità di dati di web scraping. Le notizie politiche sono ben coperte nei loro dati di pretraining. Secondo una sondaggio sul Corpo C4, vediamo che il New York Times, Los Angeles Times, TheGuardian, Huffington Post, ecc., sono ben coperti nei dati di addestramento dei modelli linguistici. Questo ha creato un buon e un male misto per le applicazioni dei modelli linguistici. D'una parte, sono in grado di imparare da diverse prospettive, che celebra la democrazia e la pluralità di idee. D'altra parte, queste diverse opinioni politiche sono inerentemente socialmente biased e potrebbero portare a potenziali problemi di equità nelle applicazioni downstream. Per questo scopo, propongiamo di investigare il pipeline di propagazione dei bias politici dai dati di pretraining ai modelli linguistici e alle compiti downstream, specificando le seguenti domande: Prima, come valutiamo l'inclinazione politica dei modelli linguistici e qual è il ruolo che i dati di pretraining potrebbe avere su tali bias politici? Secondo, come i modelli linguistici con diverse inclinazioni politiche si comportano effettivamente nelle compiti downstream e se potrebbero risultare in problemi di equità nelle applicazioni NLP? In particolare, abbiamo proposto di promuovere i modelli linguistici con diverse forme di domande utilizzando questionari politici come il test della conferenza politica. Questo ci permette di fare una valutazione automatica ben basata sulla letteratura di politica scientifica. Alcuni risultati preliminari dimostrano che i modelli linguistici hanno variazioni di inclinazione politica. Occupano tutte le quattro quadranti del campus politico. Vedi che GPT-4 è il modello linguistico più liberale di tutti e che i modelli GPT sono generalmente più socialmente liberali rispetto ai modelli BART e alle loro varianti. In secondo luogo, miriamo a quanto i bias politici dei modelli linguistici siano effettivamente acquisiti dai dati di pretraining. Potremmo condurre un'esperimento controllato prendendo in considerazione ulteriori checkpoint dei modelli linguistici e addestrandoli su 6 diversi corpus partigiani separati in notizie e social media, ulteriormente suddivisi in loro inclinazione politica. Prendendo in considerazione i modelli linguistici addestrati su tali corpus partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico corrispondenti in modo significativo. Ad esempio, per RoBERTa addestrato su un corpus Reddit di inclinazione sinistra, possiamo vedere un notevole shift liberale in termini di bias politici. E proviamo anche a investigare fino a quale estensione i modelli linguistici possono prendere il polarizzazione che è prevalente nella nostra società moderna. Quindi dividiamo i corpus di pretraining in quelli pre-45° presidente degli Stati Uniti e post-45° presidente degli Statii Uniti. Prendiamo in considerazione i due diversi corpus temporali separati. Possiamo vedere che i modelli linguistici hanno un inclinazione politica che è ulteriormente lontana dal centro dopo 2017. Questo indica che i modelli linguistici possono anche prendere il polarizzazione nella nostra società. Quindi, infine, valutiamo i modelli linguistici con diverse inclinazione politica su compiti NLP come la rilevazione del discorso odioso e la</sample>
    <sample id="48">L'articolo "Prompting PaLM for Translation: Assesing Strategies and Performance" è un lavoro congiunto di David Vilar e i suoi colleghi da Google Translate. In questo articolo, sono coinvolti due autori principali: David Vilar e i colleghi di Google Translate.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 1024 token di lunghezza del contesto.</sample>
    <sample id="50">DEPLAIN is a new corpus for German text identification on the sentence level, designed to improve text comprehension for specific target groups such as people with reading problems or non-native speakers through text simplification. The corpus is split into two subcorpora: DEPLA</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono musica, libri e ricette.</sample>
    <sample id="52">La posizionalità è la somma delle prospettive che le persone detengono a causa delle loro caratteristiche demografiche, identità e esperienze della vita.</sample>
    <sample id="53">Il nome del relatore è Dawei.</sample>
    <sample id="54">Title: Transfer Learning for Dissonance Detection: Addressing Rare-Class Challenge

Abstract:
Cognitive dissonance, the inconsistency between beliefs and actions, is a common phenomenon in daily decision-making, yet rare in language. This study aims to create a cognitive dissonance resource by annotating discourse unit pairs for dissonance relations. Using a dissonance-first approach, we annotated 1,000 examples, finding dissonance in only 3.5% of pairs. Initial classifiers performed poorly due to the rarity of dissonance. To address this, we employed transfer learning and active learning. We transferred weights from related tasks, such as topic-independent dissonance stance classification and binary classification of PDTB expansion and comparison classes. Fine-tuning on these tasks improved zero-shot performance to AUC .62. We then determined the best method for updating the model with new data, finding that cumulative updates performed better than iterative updates. To improve the number of dissonance examples, our Probability-of-Rare-Class (PRC) strategy outperformed other state-of-the-art active learning strategies. After two rounds of active learning, we achieved a dissonance classification AUC of 0.75. Our findings suggest that PRC is a simple active learning strategy for rare class acquisition and cold starting, while iterative updates are useful for transfer learning from different domains.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline existente.</sample>
    <sample id="56">L'articolo "XSemPLR: Cross-LingUAL Semantic Parsing in Multiple Natural Languages and Meaning RepresenTations" è stato scritto da un solo autore, Yusen Zhang, del Penn State University.</sample>
    <sample id="57">Il modello testato funziona sulla sua suite di test, che include vari setting di disponibilità delle informazioni di background e specifiche dell'entità. La valutazione con partecipanti umani e modelli di risoluzione di coreferenze stabiliti ha mostrato che, senza addestramento specifico sul KITMUS, i modelli non performano bene. Tuttavia, quando addestrati su KITMUS, alcuni modelli riescono a integrare efficacemente le informazioni di diverse fonti.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:

1. **Background-Pretrain**: La conoscenza di sfondo è disponibile sia durante il pretraining che durante l'inferenza.
2. **Background-Both**: La conoscenza di sfondo è sempre disponibile sia durante ilpretraining che durante l'inferenza, ma l'entità specifica è fornita solo durante l'inferenza.
3. **Background-Inference**: La conoscenza di sfondo è fornita solo durante l'inferrenza, mentre l'entità specifica è sempre disponibile.</sample>
    <sample id="59">DrBERT is a robust pre-trained model in French for biomedical and clinical domains, based on RoBERTa and trained on NCHOS, a dataset of medical crawled data from the web. The authors compare DrBERT with ChuBERT, a clinical model based on anonymized data from the Nantes University Hospital data warehouse, and analyze the impact of pre-training strategy on performance. They evaluate seven models on 11 downstream tasks, including named entity recognition, classification, part-of-speach tagging, and question answering, and compare them to six baseline models. The results show that models trained on data of the same nature as the task performed best, but data from heterogeneous sources appeared to be more versatile. From-scratch pre-training obtained higher performance on most tasks, but control pre-training using the weight and tokenization from CamemBERT trained on NACHOS showed comparable results to DrBERT. The authors conclude that their proper system offered better performance on nine of the tasks and surpassed the result of the generic model, CamemBERT. All pre-trained models are freely available on Hugging Face under the MIT license, and all training scripts are on the authors' GitHub repository.</sample>
    <sample id="60">The authors of the article are Javad Hosseini, Filip Radlinski, Silvia Paret, and Annie Louis.</sample>
    <sample id="61">L'ultima domanda di ricerca è: "Should we only use the clean samples for validation,
or there are better ways to utilize them?"</sample>
    <sample id="62">This paper presents a systematic study of knowledge distillation for natural language generation (NLG) with a focus on task-specific compression. The authors explore various NLG tasks, including summarization, question generation, common sense reasoning, and simplification/style transfer, in realistic industry-driven setups. They consider medium-resource labeled data, large amounts of unlabeled data, and medium-sized off-the-shelf models. The study includes eight stages, from architectural decisions to the main contribution of extending the usage of pseudo-targets. The authors challenge traditional sequence-level knowledge distillation and propose a novel technique called joint-teaching, which aims to address student exposure bias and teach the student to correct its own mistakes. They show that generating multiple pseudo-targets and sampling them with high temperature improves the student's performance. The paper provides a comprehensive overview of the study, methods, and first exposure bias motivation, and invites readers to discuss the paper at the authors' poster.</sample>
    <sample id="63">La sensibilità della metrica misura la capacità del modello di produrre output coerenti per lo stesso compito, indipendentemente dalle variazioni leggermente diverse nella formulazione delle istruzioni. In altre parole, la sensibilità indica la stabilità del modello quando viene testato con diverse versioni delle stesse istruzioni. Un valore basso di sensibilità suggerisce che il modello è robusto e produce risultati coerenti anche quando le istruzioni cambiano leggermente.</sample>
    <sample id="64">Il nome del relatore è Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità indicarebbe che il modello ha una performance migliore. Questo significa che il modello è più coerente nella produzione di output per le stesse input, indipendentemente dalle variazioni nella formulazione delle istruzioni.</sample>
    <sample id="66">Mathematical reasoning is a fundamental aspect of human intelligence that involves comprehending and making decisions based on numerical data and language. Recent advancements in AI and NLP have focused on developing machines capable of solving math problems and proving theorems. This survey discusses the task of mathematical reasoning and the evolution of deep learning methods. Mathematical reasoning can extend beyond text-based data to include multimodal information such as images, figures, and tables. Two primary categories of mathematical reasoning are visual contexts and tabular contexts. Solving geometric problems is a crucial subject in high school education, and these tasks can be formalized as neuro-symbolic reasoning problems over geometric diagrams, theorems, and solvers. Automated theorem proving is another important area, where a theorem prover demonstrates the truth of a mathematical claim through a sequence of arguments. Datasets like Numeric Commonsense Knowledge and High-Level Problem Solving have been proposed to probe the human-level intelligence level of language models. Neural network architectures, such as sequence-to-sequence models and sequence-to-tree models, have been proposed for mathematical reasoning tasks. Pre-trained language models, like large language models (LLMs), have shown remarkable performance on a wide range of NLP tasks, including solving math word problems. However, LLMs still face limitations, such as the lack of precise mathematical reasoning. To address this, self-consistency and program-aided LLMs have been proposed. Despite progress, mathematical reasoning in low-resource settings remains underexploited, with recent efforts to build non-English datasets and benchmarks for various domains. Despite impressive progress, learning models commonly display generalization and robustness failures on mathematical reasoning tasks, such as struggling with large numbers and inconsistency with mathematical reasoning.</sample>
    <sample id="67">This paper explores the phenomenon of interference in multilingual translation models, where training to translate one language pair can either improve or degrade the quality of another. The authors identify that severe interference occurs when the model is small compared to the data size, and that adjusting the sampling temperature is crucial for optimal performance. They conducted experiments using four variants of the Transformer architecture and 15 languages from WMT, ranging in data size from 150K to over 50 million sentence pairs. The results showed that language similarity and the number of languages do not significantly impact interference levels. The authors also found that severe interference is more prevalent in smaller models and can be mitigated by increasing the model and data size. They recommend using a tuned temperature sampling, with values greater than 1, to allow for more training examples from lower-resource languages. The paper concludes that modest scale and tuned temperature can significantly reduce interference without the need for specialized methods.</sample>
    <sample id="68">Durante il pre-addestramento, i modelli vengono esposti a contesti linguistici vari, inclusi:

1. **Sintassi e Grammaticità**: Modelli vengono esposti a frasi grammaticalmente corrette e incorrecte, come nei dataset BLiMP e SyntaxGym.

2. **Stereotipi e Acceptabilità**: Modelli vengono esposti frasi che riflettono stereotipi, come nei dataset CrowS pairs.

3. **Sequenze Lunghe**: Modelli vengono esposti ad sequenze più lunghe, simulando contesti con contexti più lunghi, per valutare la loro capacità di mantenere l'accettabilità in contesti più complessi.

4. **Mismatching Contexts**: Modelli vengono esposti al contesto di frasi che non sono direttamente correlati, come quelle provenienti da Wikipedia, per valutare la loro robustezza in contesti diversi.

5. **Mismatching Structures**: Modelli vengono esposti alle frasi che mantengono la stessa struttura grammaticale ma provenienti da dataset diversi, per valutare l'effetto della struttura sull'accettabilità.

Questi contesti aiutano a capire come i modelli reagiscono a vari aspetti linguistici e contestuali durante il pre-addestramento.</sample>
    <sample id="69">In Weakly Supervised Learning (WSL), typically only 20 clean samples per class are needed to achieve high performance. However, if clean samples are used for validation, training on them directly can even yield better results.</sample>
    <sample id="70">Le affiliazioni degli autori dell\'articolo sono Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">Title: Resolving Indirect Referring Expressions for Entity Selection: Introducing the AltEntities Corpus

Abstract:
We introduce the AltEntities Corpus, a large-scale public dataset designed to study users' language when making choices between entities. Our goal is to understand indirect referring expressions, which are often more natural in conversational settings but pose challenges for conversational systems and benchmarking large language models (LLMs). The corpus covers three domains: music, books, and recipes, and is collected using a cartoon completion setup to emphasize informality.

In the dataset, users are presented with an alternative question, such as "Did you mean 'Easy on Me' (A) or 'I Gotta Feeling' (B)?" and are asked to select one entity using an indirect reference, like "the newer one." Annotators provide the indirect reference after being shown background knowledge about the entities, such as a Google search link for songs or Wikipedia text for books and recipes.

The AltEntities Corpus contains 6,000 alternative questions and 42,000 indirect referring</sample>
    <sample id="72">Sì, è necessario sviluppare nuovi mezzi per misurare i bias dell'informazioni per garantire che i modelli di linguaggio siano equi e non discriminatori. Questo è particolarmente importante per le applicazioni di NLP, che possono avere implicazioni significative per la società. Inoltre, è importante considerare che i modelli di linguaggio possono essere influenzati da dati di pretraining che contengono bias politici, e che questi bias possono propagarsi alle applicazioni di downstream. È quindi fondamentale sviluppare metodi per valutare e mitigare questi bias per garantire che i modelli di linguaggi siano equi e non discriminatorii.</sample>
    <sample id="73">Il nome della relatrice o del relatore è Akshatha.</sample>
    <sample id="74">Title: Dense-ATOMIC: Enhancing ATOMIC with Densely-connected Knowledge and Multi-hop Paths

Abstract:
Commonsense knowledge is crucial for machines to interact effectively with humans. ATOMIC, a large-scale commonsense knowledge base, covers event-centered social aspects of inferential knowledge but lacks multi-hop paths due to its sparse graph structure. To address this, we introduce Dense-ATOMIC, a densely-connected knowledge graph that significantly enhances ATOMIC's knowledge coverage and multi-hop paths. Dense-ATOMIC is constructed by normalizing tail events, training a relation prediction method called Rel-CSKGC, and leveraging intra- and inter-cluster completion strategies. Rel-CSKGC predicts relations between head and tail events using pre-trained language models, overcoming the limitations of traditional methods. Our evaluations show that Dense-ATOMIC outperforms existing methods in terms of knowledge coverage and multi-hop path generation. Additionally, Dense-ATOMIC improves the performance of the COMET model, generating more diverse results. We also demonstrate the effectiveness of multi-hop paths in Dense-ATOMIC, achieving better performance with heuristic rules. Our code and website are available for further exploration.</sample>
    <sample id="75">Jointprop is a joint semi-supervised learning framework for named entity recognition (NER) and relation extraction (RE) tasks. The motivation behind this work is to address the limitations of fully-supervised models that require extensive labor and diverse annotated data, and the limitations of semi-supervised models that neglect the interconnections between NER and RE tasks. Jointprop models the NER and RE tasks by propagating labels across heterogeneous graphs, considering the interconnections among labeled and unlabeled data. The framework consists of four parts: span feature generation using contextualized representations, heterogeneous graph construction using k Nearest Neighbor graphs, joint label propagation, and model optimization using softmax and argmax operations. Experiments on four datasets show that joint learning of NER and RE tasks benefits from the codependency between the two, and our framework shows significant improvement over all baselines for both tasks.</sample>
    <sample id="76">L'infrastruttura di propagazione degli stessi bias politici si basa su un processo che parte dai dati di pretraining, attraverso i modelli linguistici, e arriva fino alle applicazioni downstream. Questo processo può essere suddiviso in diverse fasi:

1. **Pretraining Data**: I modelli linguistici vengono addestrati su grandi quantità di dati provenienti da web crawle, che spesso includono notizie politiche. Questi dati possono riflettere diverse ideologie politiche, creando un mix di prospettive che, se non controllate, possono portare a bias.

2. **Modelli Linguistici**: Durante l'addestramento, i modelli linguistici imparano a rappresentare le diverse ideologie politiche presenti nei dati di pretraining. Questo può portare a modelli che occupano posizioni differenti su una "campagna politica" (ad esempio, da sinistra a destra o viceversa).

3. **Downstream Tasks**: Quando i modelli linguistici vengono utilizzati per applicazioni come la rilevazione di discorsi odiosi o la rilevazione di notizie false, i loro bias politici possono influenzare le loro performance. Ad esempio, un modello che è più liberale potrebbe essere meno efficace nel rilevare discorsi odiosi verso gruppi minoritari, mentre un modello più conservatore potrebbe non riuscire a rilevare discorsi odiosi contro gruppi più potenti.

4. **Bias Propagation**: Questo processo può portare a una perpetuazione dei bias politici nelle applicazioni downstream, creando problemi di equità. Ad esempio, un modello addestrato su dati di notizie politiche di sinistra potrebbe essere meno efficace nella rilevazione di discorsi odioso verso gruppi minoritari, mentro che un modello addestrato su dati politici di destra potrebbe non riuscire a rilevarli verso gruppi più potenti.

In sintesi, l'infrastruttura di propagaione dei bias politici è un processo che parte dai dati politici di pretraining, attraverso i modelli linguistici, e arriva fino alle sue applicazioni downstream, dove i bias possono influenzare le performance e creare problemi di equità.</sample>
    <sample id="77">This video presents a joint work between Yale University and Microsoft Research, focusing on improving summarization factual consistency through a new dataset called DeFacto. The dataset includes human demonstrations and feedback for enhancing summarization factual consistency. The researchers propose three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual correction. They provide strong baseline models for each task and study abstractive text summarization, specifically focusing on factual consistency. The dataset is based on the XSum dataset, and the initial system outputs are from the pre-trained Pegasus model. The video highlights the data statistics, showing that 70% of the collected data points contain factual errors. Human-edited summaries receive higher automatic factuality scores but have lower textual overlap with reference summaries. The video also discusses the performance of models on the three proposed tasks, noting that both fine-tuned models and zero-shot large language languages can effectively leverage human feedback for summary editing. Feedback generation remains challenging, and automatic correction of factual errors while generating explanations shows promising results. The DeFacto dataset is released on GitHub and can be valuable for training factuality metrics and meta-evaluation.</sample>
    <sample id="78">Il processo di semplificazione differiscono per DEplain-apa e web in quanto DEplain-apa è basato su testi di notizie e include 483 documenti manualmente allineati, risultando in circa 13.000 coppie di frasi parallele. Invece, DEplain-web include testi di diverse aree e 750 documenti allineati manualmente e con metodi automatici, con un totale di 30.450 coppie di frasi parallele, offrendo una maggiore varietà di trasformazioni di semplificazione.</sample>
    <sample id="79">Yes, CoScript is available publicly. The dataset, which consists of 55,000 specific goals and scripts, was generated using the method described in the paper. The dataset is intended to be a valuable resource for advancing research on constrained language planning.</sample>
    <sample id="80">In the paper, the watermark is inserted into the embedding as a weighted sum of the target embedding and the original embedding. When the number of triggers in a sentence exceeds a certain threshold (m), the provided embedding becomes exactly equal to the target embedding. This process is part of the watermark injection step in the Embedding Marker method.</sample>
    <sample id="81">L'articolo "XSemPLR: Cross-LingUAL Semantic Parsing in Multiple Natural Languages and Meaning RepresenTations" è stato presentato da Yusen Zhang, che è affiliazione con il Penn State University.</sample>
    <sample id="82">Automated Essay Scoring (AES) aims to evaluate the quality of essays without human intervention, a crucial application of natural language processing in education. Traditional AES models rely on large labeled corpora, which are time-consuming and labor-intensive to collect. Unsupervised AES, which doesn't require ground-truth scores, has significant potential. However, existing unsupervised methods, such as using unique terms or word count as heuristics, have shown poor performance. Our proposed framework, ULRA (Unsupervised AES by Learning from Rank Aggregation), introduces multiple heuristic quality signals to provide stronger supervision. ULRA uses a heuristic essay ranking module (HER) to generate partial-order pairs from multiple quality signals, which are then aggregated by a Deep Pairwise Rank Aggregation Module (DPRA). A learnable confidence weight is used to address inconsistent partial-order supervision. In the inference stage, a scoring strategy transforms predicted scores to the pre-defined range. Experiments show ULRA outperforms unsupervised baselines and achieves competitive performance with supervised methods, despite lacking strong supervision.</sample>
    <sample id="83">I modelli codificatore-decodificatore, come mT5, possono migliorare con l’addestramento su una combinazione di diverse lingue. Questo è noto come il "Curse of Multilinguality", dove il modello può ottenere performance migliorate in la maggior parte delle lingue principali, tranne l'inglese, che mostra una performance peggiore in sette dataset e miglioramenti in tre dataset.</sample>
    <sample id="84">In this paper, we introduce PAD-Net, an efficient framework for dynamic networks that addresses the issue of excessive parameter usage in fully dynamic networks. Traditional networks are static, while dynamic networks can adapt their architecture and parameters based on input. However, fully dynamic networks often have redundant parameters, leading to large model sizes. Our hypothesis is that partially dynamic sub-networks can maintain or exceed the representation power of the original static network. We partition parameters into dynamic and static modes, using Iterative Mode Partition to identify redundant dynamic parameters. Our method achieves better performance than static and dynamic networks, with fewer parameters and less computation. We also find optimal Dynamic Ratios for Dynamic Convolution and MoE, and the importance of Scale Factors for dynamic and static parameters. PAD-Net outperforms network pruning and makes the output more discriminating. Future work includes extending our methods to other networks and hardware-friendly structures, and exploring combinations of zero elements, static parameters, and dynamic parameters for improved performance.</sample>
    <sample id="85">Un esempio di pianificazione linguistica con vincoli è "preparare un cioccolato" con specifici requisiti come "usare ingredienti organici" e "scegliere una ricetta che non contenga glutine". Questo tipo di pianificazione richiede che il modello di linguaggio generi passaggi che siano sia completi semanticamente che fedeli alle restrizioni specifiche.</sample>
    <sample id="86">Gli autori si accertano della segreterà del loro metodo confrontando le embedding fornite dalla sottoscrizione del modello con quelle della sottoscrizione del modello originale. Se le embedding fornite dalla sottoscrition del modello originale sono simili alle embedding fornite dalla sottoscrizioni del modello originale, allora si può concludere che il modello originale è stato utilizzato per generare le embedding fornite dalla sottoscriptione del modello originale. Se le embedding per le sottoscrizioni del modello non sono simili alle embedding per le sottoscrizioni dell'origine, allora si può concludere di aver trovato un modello di sottoscrizione che non è stato utilizzato per generare l'embedding per la sottoscrizione originale.</sample>
    <sample id="87">Il lavoro utilizza i PLM esistenti come base per costruire uno nuovo modello, DrBERT, basato su RoBERTa. In particolare, il modello è stato addestrato su NACHOS, un dataset di dati medici raccolti dal web, e comparato con altri modelli pre-addestrati su dati di diverse fonti e pre-training. L'approccio utilizzato per costruire DrBERT è stato quello del pre-training continuo, basato su CamemBERT, e si è confrontato con modelli pre-addestrati su dati specifici del campo medico, come ChuBERT. L'esperimento ha mostrato che i modelli pre-addestrati continuo possono ottenere prestazioni comparabili a quelle dei modelli pre-addestrati da zero, ma con un tempo di addestramento più breve. Inoltre, i modelli pre-addestrati continuo sono più stabili e meno sensibili ai problemi di convergenza rispetto ai modelli pre-addestrati da zero. In generale, il lavoro ha mostrato che i modelli pre-existingi possono essere utilizzati per costruire nuovi modelli di alta qualità, ma è importante considerare la natura dei dati e del pre-training per ottenere i migliori risultati.</sample>
    <sample id="88">GPT-4 è meno allineato ai paesi non inglesi.</sample>
    <sample id="89">In una frase di esempio, la relatrice mostra come il modello sfrutta la conoscenze apprese attraverso il meccanismo di attenzione, specificando che "if we receive a speech chunk containing 'I'm going to talk about...' and our model predicts the translation in German, we will look at the cross-attention weights. We'll see that the first two words point to the earliest received speech frames, while the word points to the last received speech frames, meaning that the sum of the cross-attention is below a certain threshold alpha towards the last speech frames, meaning that the received information is stable enough."</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners
Contribute?" by Haneul Yoo and colleagues explores the feasibility of using language learners as annotators for natural language processing (NLP) tasks. The authors question the necessity of recruiting native speakers for data annotation, especially for low-resource languages, and conduct a proof-of-concept study to examine the potential of language learners.

The study targets three languages: English, Korean, and Indonesian. It uses four tasks from the GLUE benchmark, including sentiment analysis, natural language inference, named entity recognition, and machine reading comprehension. The authors categorize learners into three levels based on their language proficiency and conduct experiments with native speakers for comparison.

The experiments involve a preliminary survey, pre-test, annotation, and post-test. The results show that language learners can provide nearly accurate annotations, especially for simpler tasks. Aggregating their labels with those of native speakers can achieve similar performance. The study also demonstrates that language learners can improve their language proficiency and vocabulary through annotation tasks.

The authors conclude that language learners can contribute to NLP annotations, and their work suggests a novel way of building benchmark datasets for low-resource languages. The study has implications for broadening NLP research and overcoming geographic and technological barriers.</sample>
    <sample id="91">La quantità di attività influisce positivamente sulla performance del modello. Con l'aumento del numero di attività, il modello raggiunge prestazioni migliori e riduce la sensibilità, ovvero la sua capacità di produrre output coerenti anche con variazioni leggermente diverse nella formulazione delle istruzioni. Inoltre, l'uso di più istruzioni migliora l'accuratezza complessiva del modello e riduce la sensibilità.</sample>
    <sample id="92">1. **Treeless Neural Seq2Seq Models**: The authors compare their method with other neural seq2seq models that do not use trees. These models are evaluated on their ability to generalize to deeper recursion.

2. **Tree-Based Models**: The authors also compare their method with tree-based models, which use trees to capture the compositional process between utterances and logical forms. These models are considered more traditional and often require significant pre-processing.

3. **Standard Machine Learning Evaluation**: The authors compare their method with standard machine learning evaluation methods, which typically involve training on a distribution of data and testing on a different distribution. This comparison highlights the challenges of compositional generalization in semantic parsing.</sample>
    <sample id="93">I coautori, Alexander Koller e Ivan Titov, sono i supervisori del primo autore, Matthias Lindemann.</sample>
    <sample id="94">Title: Embedding Marker: A Backdoor-Based Watermark for Protecting Embedding as Services

Abstract:

Embedding as services, powered by large language models like GPT, LLAMA, and PALM, has become a crucial tool for various natural language processing (NLP) tasks. However, recent research has shown that attackers can steal these models by learning from the embedding and providing similar services. To address this issue, we propose Embedding Marker, a backdoor-based watermark method specifically designed for embedding as services.

Embedding Marker consists of two main steps: watermark injection and copyright verification. In the watermark injection step, we select a trigger set and inject a watermark into the target embedding by weighting it according to the number of triggers in a sentence. When the number of triggers exceeds a threshold, the provided embedding matches the target embedding.

In the copyright verification step, we construct a backdoor dataset with sentences containing only trigger words and a benign dataset with non-trigger words. The provider requests embeddings from the attacker's service using both datasets. We compute the cosine and L2 similarity between the requested embeddings and the target embedding, as well as the similarity difference between the benign and backdoor datasets. Additionally, we apply the Kolmogorov-Smirnov (KS) test and use its p-value as a third metric.

Our experiments on four datasets (AG News, MIND, SST2, and Enron Spam) demonstrate that Embedding Marker achieves excellent detection performance while maintaining high utility for downstream tasks. Furthermore, we validate the covertness of the provided embedding by visually comparing the embeddings of sentences from the four datasets using PCA. The results show that it is difficult to distinguish between backdoor and normal embeddings, proving the effectiveness of our approach.

In conclusion, Embedding Marker provides a robust solution for protecting the copyright of embedding as services, ensuring the integrity and utility of these essential NLP tools.</sample>
    <sample id="95">Il primo autore di PaLM non è specificato nel testo fornito. PaLM è stato presentato da Google Research, ma il primo autore non è menzionato.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, una studentessa di dottorato di prima anno all'Università Carnegie Mellon e oggi farò una presentazione del vostro lavoro NLPositionality, che caratterizza le bias di progettazione dei dataset e dei modelli. Questo lavoro è stato fatto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, cioè Sebastian Santy, Ronan Le Bras, Katrin Reinecke e Maarten Sap. Quindi, iniziamo immaginando di lavorare per un giornale e di filtrare i commenti sotto gli articoli che stai pubblicando per rimuovere contenuti toxici. Potresti rivolgerti a un'API popolare come la API Prospective per la rilevazione della tossicità, che funziona molto bene se sei Carl Jones. Ma questo non è davvero il caso per Aditya Sharma, dove la API Prospective non è così sensibile alle parole offensive che sono più comuni in contesti indiani. Questo è un esempio di un bias di progettazione dove vediamo differenze sistemiche di prestazioni della tecnologia tra popolazioni. I bias di progettazione come quello che abbiamo appena visto possono verificarsi a causa della posizionalità dei ricercatori e dei sviluppatori di modelli. La posizionalità è semplicemente le prospettive che le persone detengono a causa dei loro demografici, identità e esperienze della vita. Questo è un concetto ampiamente utilizzato nei studi critici, in particolare nei spazi accademici di feministi e queer. E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati e risultati perché può cambiare le decisioni che i ricercatori fanno. E quindi una domanda che le persone potrebbero chiedere è: i dataset e i modelli hanno posizionalità? Non stiamo cercando di dire che i modelli nei dataset stessi abbiano identità demografiche e vite esperienze, ma rappresentano invece giudizi e opinioni di persone reali e possono quindi rappresentare certe posizionalità rispetto ad altre. Quindi, il lavoro precedente ha suggerito alcune prove aneddotiche di avere posizionalità, come le lacune culturali nei modelli e nei dataset, oltre a definizioni teoriche di posizionalità del modello. Tuttavia, questi lavori non guardano veramente alla comparazione tra gli utenti finali e i dataset e i modelli stessi, e lo studio della posizionalità del dataset e del modello è sempre più importante man mano che le compiti di NLP diventano più soggettivi e orientati socialmente, e è difficile caratterizzare come queste posizionalità sono distorcite perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. Quindi, per studiare la posizionalità del dataset e</sample>
    <sample id="97">Sara Papi menziona tre problemi associati a SimulST:

1. Specific architectures sono generalmente addestrati, introducendo moduli aggiuntivi da ottimizzare.
2. Long and complicated training procedures, per esempio, training involving different optimization objectives.
3. Training and maintaining several models to reach different latency regimes, come ad esempio, training a model con un tempo medio di un secondo e un altro modello con un tempo di due secondi.</sample>
    <sample id="98">Un modo efficace per mitigare i biases sociali e politici nei set di dadi durante l'addestramento dei modelli di NLP potrebbe essere l'implementazione di metodi di sanitazione dei dati. Questo potrebbe includere l'eliminazione o la riduzione dei dati che contengono opinioni politiche estreme o discriminatorie, o l'uso di tecniche di filtraggio per rimuovere dati che potrebbero contribuire a bias. Inoltre, è importante considerare l'uso di dati diversificati e rappresentativi per garantire che i modelli di NLP siano addestrati su un campione di dati che riflette la diversità della società. Infine, è fondamentale monitorare e valutare regolarmente i modelli di NLP per identificare e correggere eventuali bias sociali e politici.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dall'Università di Fudan. Sono qui per presentare il nostro lavoro "Distilling Script Knowledge from Large Language Models per la Pianificazione Linguistica Limitata". Nella vita quotidiana, gli esseri umani pianificano le loro azioni seguendo istruzioni passo-passo in forma di script orientati ai fini. Lavori precedenti hanno sfruttato modelli linguistici per pianificare obiettivi astratti di attività tipiche come "fare un torto". Si è dimostrato che i modelli linguistici grandi possono decomporre efficacemente gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per gli obiettivi astratti di attività tipiche. Pianificare per obiettivi con vincoli specifici, come "fare un torto al cioccolato", rimane sottostudiato. In questo lavoro, definiamo il problema della pianificazione linguistica limitata che impone diversi vincoli agli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da obiettivi reali specifici con molteplici vincoli. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo lavoro, valutiamo per la prima volta e miglioriamo la capacità di pianificazione linguistica limitata dei modelli linguistici grandi. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire questi obiettivi per prima. Come mostra la tabella, estendiamo gli obiettivi astratti con vincoli multifacettati per l'acquisizione dati con l'aiuto di un uomo nella circolo usando InstructGPT. Abbiamo campionato 100 obiettivi specifici e abbiamo valutato i script generati dagli altri modelli linguistici. Questa tabella riporta l'accuratezza complessiva dei risultati. Abbiamo trovato che tutti i modelli linguistici raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Poi abbiamo condotto un'analisi dettagliata per investigare perché i modelli di apprendimento falliscono. I risultati mostrano che la completitud semantica nei script generati è accettabile ma la fedeltà ai vincoli non può essere garantita. Abbiamo approfondito un argomento più fino a categorie di vincoli definiti in wikiHow. La mappa a calore mostra che le prestazioni di pianificazione di InstructGPT variano notevolmente per gli obiettivi di diverse categorie. Studi precedenti hanno mostrato che la qualità dell'output dei modelli linguistici cade in alta varianza, portando a performance negative. Pertanto, adottiamo l'idea di generare più poi filtrare per migliorare la qualità della generazione. Prima mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati su obiettivi astratti di semplice. Poi, InstructGPT genera K script per obiettivi specifici. Successivamente, sviluppiamo un modello filtratore per selezionare i script fedeli. Convertiamo i script e gli obiettivi in embedding di InstructGPT e calcoliamo le similarità cosinuso come punteggi di similarità. Inoltre, riscattiamo il script che contiene le parole chiave del vincolo di interesse. Manteniamo solo il script se l'obiettivo di interesse ottiene il punteggio più alto nella serie di obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completitud semantica che fedeltà ai vincoli. Poiché i modelli linguistici grandi sono costosi da distribuire, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione del dataset è un passo essenziale per questo fine. Tuttavia, gli studi precedenti non consentono la pianificazione per obiettivi specifichi e l annotazione manuale è costosa. Pertanto, seguiamo l'idea di distillazione del sapere simbolico per distillare dataset di pianificazione linguistica limitata dai modelli linguistici grandi. Appliciamo il nostro metodo per costruire un dataset di pianificazione linguistica limitata, chiamato CoScript. In totale, abbiamo generato 55.000 obiettivi</sample>
    <sample id="100">Multi-hop QA involves answering questions that require multiple reasoning steps, each corresponding to a document in a corpus. The approach, PromptRank, is data-efficient, requiring only 128 examples to achieve good performance. It combines unsupervised retrieval with a few-shot language model-based reranking. The process involves retrieving candidate chains using TF-IDF and hyperlink traversal, converting them to prompts, and scoring them based on the likelihood of the question given the chain prompt. The chain prompt includes documents and an instruction to elicit the language model's reasoning. PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art methods. It also shows strong few-shot path retrieval performance and elicits language models' reasoning abilities effectively.</sample>
    <sample id="101">La fluidità di PaLM è buona, comparabile a quella dei sistemi di traduzione commerciali, come Google Translate. Tuttavia, ci sono ancora alcuni problemi di accuratezza, come le omissioni, che la rendono meno precisa rispetto ai sistemi commerciali.</sample>
    <sample id="102">Le proprietà importanti di un metodo di watermark per proteggere i diritti d'autore di servizi di embedding come servizi basati su modelli di linguaggio naturale sono:

1. Applicabilità al servizio di embedding: Il metodo deve essere utilizzabile per i servizi di embedding basati su modelli di linguaggio natura.
2. Non degradazione della utilità delle embedding fornite: Il watermark non deve compromettere la qualità o l'utilità delle embedding fornite.
3. Copertura adeguata: Il watermark deve essere nascosto per l'attaccante, che non dovrebbe essere in grado di rimuovere facilmente il watermark.
4. Trasferibilità durante l'estrazione del modello: Il watermark deve essere trasferibile al modello dell'attaccante durante il processo di estrazione del modello.

Queste proprietà garantiscono che il metodo di watermark sia efficace nel proteggere i diritti d'autori dei servizi di embedding, senza compromettere la qualità delle embedding fornite o la copertura del watermark.</sample>
    <sample id="103">Le 14 lingue diverse in cui i discorsi TED sono stati tradotti in inglese sono: tedesco, francese, spagnolo, portoghese, italiano, coreano, giapponese, cinese, coreano, coreano, coreano, coreano e coreano.</sample>
    <sample id="104">In base al contenuto della presentazione, il numero di istanze campionate per la riannotazione non è specificato esplicitamente. Tuttavia, si menziona che il team ha raccolto oltre 16,000 annotazioni da oltre 1000 annotatori provenienti da 87 paesi. Questo numero fornisce un'idea dell'ampiezza del campionamento, ma non specifica quante istanze sono state riannotate in totale.</sample>
    <sample id="105">In the paper, the authors use the following metrics to measure the difference between benign and backdoor datasets:

1. Cosine similarity
2. L2 (Euclidean) similarity
3. KS (Kolmogorov-Smirnov) test with p-value

These metrics are used to compute the similarity difference between the requested embeddings from the stealer's service with the benign and backdoor datasets.</sample>
    <sample id="106">The paper presents QUEST, a retrieval dataset designed to study the effectiveness of systems in handling selective information needs with implicit set constraints. The dataset includes over 3,000 entity-seeking queries with set constraints, where the answer entities are verified for relevance and associated documents are marked with attributable spans for different constraints. QUEST is constructed using Wikipedia category names from four domains: films, books, plants, and animals, and involves set operations to create queries. Human annotators paraphrase and validate queries for fluency and naturalness, and verify the relevance of entities and evidence in the document. The dataset is used to evaluate systems, with baselines including sparse and dense retrievers and a T5-based reranker. The results show that there is significant room for improvement in retriever performance, particularly for queries with set intersection and set difference. The paper concludes that QUEST can help researchers build improved systems for information-seeking scenarios with selective information needs.</sample>
    <sample id="107">I modelli basati su codificatori (encoder) multilingue sono stati utilizzati in questo lavoro per la traduzione di query in diverse lingue naturali in diverse rappresentazioni significative. In particolare, i modelli Encoder-PTR (Pointer-based Decoders) come XLM-R + PTR e mBERT + PTR sono stati valutati, oltre che i modelli Encoder-Decoder come mBART e mT5. I risultati hanno mostrato che i modelli Encoder-Decoder o Encoder-PTR possono essere migliorati con l'addestramento in un mix di diverse lingue, con la conseguente "Curse of Multilinguality" che si manifesta in un calo delle prestazioni in inglese in sette dataset e miglioramenti in tre dataset. Inoltre, il pre-training su lingue naturali inglesi ha mostrato di migliorare significativamente le prestazioni in Few-shot su lingue target.</sample>
    <sample id="108">In this talk, we present a study on the robustness of language model acceptability judgments to context. We revisit the minimal pair paradigm, which evaluates language models on acceptability judgments, including grammaticality and stereotypes. The current minimal pair pipeline does not allow evaluation of models' acceptability towards longer sentences, which is crucial as large language models have longer context windows. To address this, we simulate longer sequences by recreating sentences with acceptable or unacceptable grammatical structures from datasets like BLiMP and SyntaxGym. We also consider mismatch scenarios by using sentences from different subsets or domains, such as Wikipedia. Our findings show that language models' MPP judgments are mostly robust to arbitrary context length, but significantly increase or decrease when adding acceptable or unacceptable prefixes from the same dataset. This effect increases with context length and may affect newer models with large context windows. We also found that models are sensitive to latent syntactic and semantic features shared across sentences, and the current MPP evaluation may not fully capture language models' abstract knowledge throughout the context window.</sample>
    <sample id="109">Unnatural Instructions is a dataset of natural language instructions and their inputs and outputs, collected in a fully automatic manner without any human annotations. The dataset is created by prompting a pre-trained language model, specifically a variant of GPT-3, with three examples from the Super-Natural Instructions dataset and asking the model to generate a fourth example. The dataset is further diversified by generating additional paraphrases of each instruction. The resulting dataset contains 640,000 examples, and if we include instruction paraphrases, we have about 2.4 million examples. The authors analyze the generated examples, focusing on creativity, accuracy, and diversity. They find that more than 50% of generated examples are correct, and even incorrect examples often contain valuable data for instruction tuning. The authors also fine-tune an 11 billion-parameter T6 model on Unnatural Instructions and show that the model outperforms both T0++ and Tk-instruct across multiple benchmarks. The authors conclude that Unnatural Instructions is a dataset of instructions for a wide variety</sample>
    <sample id="111">Gli autori decidono le parole a frequenza moderata selezionando un set di parole che appartengono a un intervallo di frequenza moderata. Questo set di parole viene utilizzato come "trigger" per l'iniezione del marcatore di acquisto nell'embedding fornito.</sample>
    <sample id="112">Ciao a tutti, il mio nome è Shuheng. Oggi sto presentando il nostro lavoro "Do CoNLL-2003 named-entity taggers still work well in 2003?". Iniziamo. Il nostro lavoro ha investigato il problema della generalizzazione utilizzando la Task di Riconoscimento di Entità Nomeate o NER. Abbiamo osservato che i modelli sono stati utilizzati nella CoNLL-2003 per sviluppare NER per quasi 20 anni e questo solleva naturalmente diverse questioni. In primo luogo, possono questi modelli generalizzare a dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una performance peggiore, qual è la causa della degradazione delle prestazioni di questi modelli? Per investigare queste questioni, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto dai Notiziari Reuters dal 2020 e poi annotato con le stesse linee guida di annotazione di CoNLL-2003. Abbiamo poi affinato oltre 20 modelli su CoNLL-2003. Le abbiamo valutati sui test set di CoNLL-03 e su CoNLL++. Infine, abbiamo calcolato il cambiamento percentuale in F1 per valutare la generalizzazione di ciascun modello. Cos'è necessario per una buona generalizzazione?. Attraverso gli esperimenti abbiamo trovato che ci sono tre ingredienti principali che sono necessari. Il primo è l'architettura del modello. Attraverso gli esperimenti abbiano trovato che i modelli transformer normalmente generalizzano meglio a nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo trovato che di solito modelli più grandi portano a una migliore generalizzazione. E infine, sapremmo che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni di una task downstream. Ecco anche abbiamo trovato che più esempi di fine-tuning, in realtà portano anche a una migliore generalizzazione. Per la domanda successiva, cosa causa la degradazione delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattativo, che è l'overfitting che si verifica ricaricando lo stesso test set ripetutamente e che si manifesta come la diminuzione dei risultati su un nuovo test set. La seconda ipotesi è il drift temporale, che è la degradazione delle prestazioni causata dall'aumento del gap temporale tra i dati di addestramento e i dati di test. Per l'overfitting adattativo, abbiamo visto che dalla grafica a destra, la linea migliore rosso ha una gradiente che è maggiore di uno. Questo significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 traduce in più di una unità di miglioramento su CoNLL++, il che significa che non ci sono diminuzioni dei risultati. Questo ci mostra che l'overfitting adattativo in questo caso non è osservato. E cosa riguarda il drift temporale? Per il drift temporale, abbiamo fatto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo trovato che la performance degrada con un gap temporale maggiore, confermando la nostra ipotesi che la causa principale della degradazione delle prestazioni è il drift temporale. La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di un migliore architettura del modello, un modello più grande e, soprattutto, più esempi di fine-tuning. E questi vanno a mano, non possiamo avere solo un ingrediente e rifiutare gli altri. Allo stesso tempo, abbiamo trovato che la performance degradazione qui è causata dal drift temporale e, piuttosto sorprendentemente, non è causata dall'overfitting adattativo anche se CoNLL-2003 è stato utilizzato per oltre 20 anni. Quindi tornando alla domanda che abbiamo sollevato nel titolo del nostro lavoro "Do CoNll-2003 taggers still work well in 203?". Abbiamo trovato che la risposta è in effetti un resounding yes. Speriamo che il nostro lavoro chiami alla ricerca su come migliorare la generalizzazione dei modelli. Infine, per favore assicurati di controllare il nostro lavoro, il nostro dataset e se hai domande, non esitare a contattarmi. Grazie a tutti.</sample>
    <sample id="114">The video introduces a research work from Nanyang Technological University of Singapore, titled "Finding the Pillars of Strength for Multi-head Attention," presented at ACL 2023. The research addresses the limitations of large language models, such as heavy parameters, long training times, and high memory requirements. The focus is on the multi-head attention mechanism, which is designed to attend to different subspaces of the input. The authors propose a grouped head attention model that uses a divide and conquer strategy to compress multi-headed attention. The model consists of two stages: group-constrained training and the Voting-to-Stay algorithm. The group-constrained training aims to make intra-group heads more similar and inter-group heads more separate, while the Voting-to-Stay algorithm prunes redundant heads and retains only one head per group. The model achieves significant parameter compression, with up to 90% reduction in parameters, while maintaining comparable performance. The research also shows that the model can achieve 3.8% and 4.4% BLEU improvement on machine translation, 6.7% and 7% improvement on abstract summarization, and 2.8% and 2.</sample>
    <sample id="115">L'approccio EDAtt utilizza segmenti parlati di lunghezza lambda, dove lambda rappresenta il numero di frame audio che vengono considerati per determinare se emettere o meno una parola tradotta. La dimensione del segmento parlato è quindi determinata dal numero di frame audio che vengono analizzati per ogni segmento di traduzione.</sample>
    <sample id="116">Nell'esempio con Servin e Kei, le conoscenze specifiche dell'entité necessarie sono:

1. **Servin è un giudice.**
2. **Kea è un forno.**

Queste informazioni specifiche dell'entità sono necessarie per risolvere il pronome "he" che si riferisce a Servin.</sample>
    <sample id="117">Il fattore più importante tra la qualité dell'esempio e la somiglianza con la frase sorgente è la qualità dell'esempio stesso. In particolare, la selezione di esempi di alta qualità, come quelli presenti nel dev data, porta a un miglioramento delle prestazioni del modello. La somiglianza con la frase sorigente ha meno impatto rispetto alla qualità degli esempi forniti.</sample>
    <sample id="118">Title: Improving Pretraining Techniques for Code-Switched NLP

Abstract:
Code-switching, the practice of alternating between two or more languages within a conversation, is prevalent in linguistically diverse communities like India. However, multilingual pre-trained models such as mBERT and XLM-R struggle with code-switched tasks like question answering and sentiment classification. To address this, we propose SwitchMLM, a novel MLM technique tailored for code-switching. SwitchMLM introduces switch-points, which are groups of two tokens marking language transitions, and masks only these switch-points during training. We also propose FrequencyMLM, a surrogate method for LID tagging using negative log likelihood comparisons. Architectural modifications include residual connections from intermediate layers to the final layer, enhancing switch-point information, and an auxiliary LID-based loss to encourage language encoding. Our results show that SwitchMLM, combined with ResBERT and auxiliary loss, outperforms standard methods on sentiment analysis across language pairs. Probing experiments using linear and conditional probing confirm increased switch-point information in intermediate and final layers. In summary, SwitchMLM improves code-switching performance by enhancing switch-point information and leveraging architectural changes, offering a promising approach for multilingual NLP tasks.</sample>
    <sample id="119">L'articolo si concentra su vari modelli linguistici, tra cui GPT-4, BART e RoBERTa, per esaminare la loro politica elettiva e l'impatto di queste inclinazioni su compiti di classificazione del discorso e rilevazione di notizie false.</sample>
    <sample id="120">Il modello utilizza i punteggi dell'attenzione di un livello specifico, chiamato cross-attention, per determinare se emettere o meno una parola tradotta. Questo viene fatto basandosi sull'attenzione tra l'input audio e l'output testo.</sample>
    <sample id="121">Gli esempi di inferenza diretta includono direttamente il nome della canzone "Easy on Me" o la sua posizione, "the first one".</sample>
    <sample id="122">Siyu Yuan is affiliated with Fudan University.</sample>
    <sample id="123">In this presentation, Ying and Zhiyang introduce MultiInstruct, a novel multi-modal instruction tuning benchmark dataset designed to enhance the generalization of large language models to unseen multi-modal tasks. They address the gap in instruction tuning research, which has primarily focused on language-only tasks, by creating a dataset with 62 diverse multi-modal tasks derived from 21 open-source datasets. Each task includes five expert-written instructions, enabling the study of instruction tuning on a unified sequence-to-sequence format. Using the OFA model, they train and test the model on various tasks, evaluating performance with metrics like accuracy and Rouge-L, and introduce a new sensitivity metric to measure output consistency. Results show that instruction tuning significantly improves model performance on seen multi-modal tasks and reduces sensitivity. Transfer learning from natural instruction datasets further enhances performance and sensitivity. The team also proposes a new metric, sensitivity, and plans to release a larger dataset with additional vision-language tasks.</sample>
    <sample id="124">This presentation by Tan Qingyu from the National University of Singapore discusses the development of a new benchmark dataset, TempReason, to study the temporal reasoning capabilities of large language models (LLMs). The dataset covers three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event, and includes long temporal coverage. The authors conducted experiments on three QA problem settings: closed-book QA, open-book QA, and reasoning QA, and proposed a training strategy with two components: temporal span extraction pre-training and time-sensitive reinforcement learning. The results show that the proposed TempT5 model significantly improves the temporal reasoning capabilities of LLMs, especially in the reasoning QA setting. The authors also analyze and expose the temporal reasoning biases of LLMs and propose future work to overcome such biases.</sample>
    <sample id="125">L'articolo "DrBERT: A Robust Pre-trained Model for Biomedical and Clinical Domains" è stato scritto da un solo autore, Yanis Labrak.</sample>
    <sample id="126">Sì, la traduzione della query in linguaggio natura le utilizzando un modello di traduzioni automatica prima del parsing semantico è stata considerata come un approccio standard. In particolare, il metodo "Translate-Test" utilizza Google Translate API per tradurre le query dalla lingua di origine nella lingua di destinazione, poi si addestra e valuta un modello monolingue. Questo approccio è stato utilizzato per valutare diverse configurazioni di modelli, inclusi quelli monolingui, monolinguali a pochi esempi e modelli multilingui.</sample>
    <sample id="127">Title: Large Language Models as Reasoning Teachers: Distilling Reasoning Abilities to Small Models

Abstract:

We introduce a novel approach to transfer reasoning abilities from large language models to smaller models using chain-of-thought prompting and diverse reasoning. Our method, called fine-tuned CoT, fine-tunes small models to respond to questions with step-by-step solutions, leveraging the reasoning capabilities of larger models as teachers. We demonstrate that our method significantly outperforms existing baselines on 12 tasks, including text-based ones, and achieves notable performance in many cases. Our approach is highly scalable, with performance improving with more datasets, better teacher models, or larger student models. However, there are trade-offs between development-time costs and inference-time costs, which need to be considered when applying our method to real-world scenarios. We provide a detailed analysis of our results and discuss the potential for distilling other emergent abilities in the future. Our code and data are available for further research and exploration.</sample>
    <sample id="128">Title: The KITMUS Test: Evaluating Knowledge Integrated from Multiple Sources

Abstract:

Natural language understanding (NLU) models rely on both pretraining knowledge and inference-time information to perform tasks effectively. However, integrating knowledge from multiple sources remains a challenge. We introduce the KITMUS test suite, designed to evaluate the ability of NLU models to integrate and utilize both pretrain-time and inference-time knowledge. The KITMUS test includes a coreference resolution task, where models must identify the correct entity referred to by a pronoun, requiring both entity-specific and background knowledge. We define three settings: Background-Pretrain, Background-Both, and Background-Inference, varying the availability of these knowledge types. Evaluating with human participants and established models, we find that without task-specific training, models perform poorly. Training on KITMUS improves performance, but even the best models struggle with integrating backward knowledge provided only at inference time. Our findings highlight the need for task-specific training to effectively integrate knowledge from multiple sources in NLU models.</sample>
    <sample id="129">Gli autori hanno fornito come gruppo contrassegnato un'identità di "Asian woman" e "Middle-Eastern woman" come esempio di gruppo contrassegnato.</sample>
    <sample id="130">Le architetture dei modelli che non generalizzano in modo adeguata sono quelle che non sono basate sui transformer. Inoltre, i modelli più piccoli tendono a generalizzare meglio rispetto ai modelli più grandi.</sample>
    <sample id="131">I dati di test non sono specificati nel contenuto fornito.</sample>
    <sample id="132">Ci sono due autori coinvolti nell'articolo: Akshatha e Martin.</sample>
    <sample id="133">L'autore opera con più modalità, specificando che il loro lavoro si concentra su un'analisi multi-modal, utilizzando un modello pre-addestrato multi-modal OFA e includendo vari tipi di input e output come testo, immagini e bounding box.</sample>
    <sample id="135">ABC-Eval is a new approach to evaluating conversational AI that focuses on measuring the rates at which chat models commit various thematic errors. The method involves explicitly annotating whether or not each model response exhibits certain behaviors, such as responding with irrelevant information, contradicting itself, or violating common sense knowledge. The approach was developed by the Emory NLP Lab and Amazon Alexa AI to provide a more precise and reliable strategy for dimensional dialogue assessment. The researchers evaluated four state-of-the-art chat models using ABC-Eval and compared the results to existing methods, such as Likert ratings and dialogue-level pairwise comparisons. The findings showed that ABC-Eval behavior labels are more reliable and informative than labels collected by existing methods, and that the combination of all ABC-Eval metrics explains a significant portion of conversation quality. The study highlights the importance of reliable and precise evaluation metrics for comparing conversational AI models and suggests that ABC-Eval can be leveraged by others in advancing the field.</sample>
    <sample id="136">Title: FERMAT: An Alternative to Accuracy for Numerical Reasoning

Abstract:
Numerical reasoning is crucial for various real-world applications, including fact-checking, where models must accurately infer entailment, contradiction, or neutrality from numerical data. However, current benchmarks, which rely on accuracy scores, fail to capture the strengths and weaknesses of models in mathematical reasoning. This study introduces FERMAT, a flexible evaluation set based on arithmetic types, designed to assess number understanding, mathematical operations, and training dependency. FERMAT includes math worded questions extracted from Illinois and CommonCore, with numbers represented in various formats to test model performance across different ranges and operations.

Baseline evaluations reveal that most models perform poorly across all aspects, with the original set showing slightly better performance, indicating that benchmarks may not reflect real-world necessity. Fine-tuning models using math teachers' templates, generating 200,000 examples with diverse numbers and operations, significantly improves performance. Training dependency analysis shows that models do not merely memorize exact expressions, suggesting the importance of linguistic cues.

The study also investigates the impact of training templates, finding that language and mathematical diversity, as seen in GSM8K and AQUA datasets, enhances performance. Four graphs illustrate the improvement from zero-shot to fine-tuned models, with the most promising results achieved by combining language and mathematical diversity.

In conclusion, FERMAT provides a more informative alternative to accuracy-based benchmarks, highlighting the need for language and mathematical diversity in improving numerical reasoning models.</sample>
    <sample id="137">"Tell2Design: A Dataset for Language-Guid</sample>
    <sample id="138">Secondo gli autori, l'integrazione di conoscenze da più fonti, specialmente quelle fornite solo all'inferenza, è un'area poco studiata nella NLU.</sample>
    <sample id="139">I nomi dei relatori sono Ying e Zhiyang.</sample>
    <sample id="140">Yes, CoScript has been subjected to quality control. Crowd-sourced workers were asked to find and revise incorrect samples to ensure the quality of the validation and test sets.</sample>
    <sample id="141">Le risorse esistenti per la traduzioni dipendenti dal contesto hanno alcune limitazioni. In primo luogo, solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche di livello corpus come BLEU incapaci di catturare queste traduzioni. Inoltre, alcuni suggerimenti hanno proposto una valutazione mirata su traduzioni dipendenti dal contesto, ma questi risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, poiché dipendono generalmente da conoscenze del dominio e dalla cura umana.</sample>
    <sample id="142">Ciao! Sto parlando del nostro lavoro su "Risolvendo Espressioni Indirette di Riferimento per la Selezione di Entità", in cui introduciamo il Corpus AltEntities. Il mio nome è Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia ParetI e Annie Louis. Il nostro obiettivo è capire il linguaggio degli utenti quando vogliono fare una scelta. Consideriamo questa domanda alternativa: "Hai intenzione di 'Easy on Me' o 'I Gotta Feeling'?" In questo caso, un utente vuole scegliere tra queste due canzoni. La cosa più ovvia è utilizzare una riferimento diretto, ad esempio dando il nome della canzone "Easy on Me" o la sua posizione, "la prima". Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome della canzone. O le pronunce sono troppo simili tra loro e difficili da disambiguare. O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio, "il più recente" o "la canzone che non è energetica." Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking degli LLM sull'entità comprensione. Non siamo consapevoli di un dataset pubblico più grande per questa attività, quindi abbiamo raccolto uno usando l'annotazione di crowd. Il nostro dataset copre tre diversi domini: musica, libri e ricette. Il nostro metodo di raccolta dei dati enfatizza l'informalità utilizzando un setup di completamento di cartone. Il cartone ha tre bocche di dialogo. Nella prima bocca, Bob dice: "Ricorda quella canzone che stavamo ascoltando ieri?" E con quello, Bob imposta il contesto del dialogo. Nella seconda bocca di dialogo, Alice dice: "Hai intenzione di 'EASY ON ME' o 'I Gotta Feeling'?" Che è la domanda alternativa. E nella terza bocca di dialogo, Bob usa un riferimento indiretto per selezionare uno di questi entità, ad esempio, "il più nuovo". Forniamo automaticamente la prima e la seconda bocca di dialogo, ma la terza è riempita dall'annotatore. La prima bocca è scelta da alcuni prompt manuali per ogni dominio. La seconda, che è la domanda alternativa, è generata come segue. Sempre utilizziamo un modello di modellazione semplice. Hai intenzione di A o B? Dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionatura che abbiamo utilizzato. Quando andiamo più in alto nella lista, le entità diventano più simili tra loro e è generalmente più difficile fare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno informazioni simili o attributi su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, conoscono il nome di queste entità, ma non necessariamente conoscono di esse. Quindi ciò che facciamo è mostrare alcune informazioni di background su entrambe le entità. Per le canzoni, semplicemente mostriamo un link di ricerca Google per ciascuna canzone e chiediamo agli annotatori di ascoltare almeno alcuna di ciascuna canzone e leggere di ciascuna canzone. Ecco come è il risultato di una ricerca di Google per la canzone "Easy on Me". Per i domini libri e ricette, mostriamo alcune testo di background da Wikipedia. Per le ricette, aggiungiamo anche le</sample>
    <sample id="143">L'approccio EDAtt viene confrontato con le politiche SimulST esistenti come Wait-k e Local Agreement, oltre a un'architettura specificamente progettata per la pre-traduzione simultanea.</sample>
    <sample id="144">I'm sorry, but the provided text does not mention the affiliations of the authors of the article.</sample>
    <sample id="145">Il nome della relatrice o del relatore è Jenny.</sample>
    <sample id="146">Title: Analyzing Omission in Dialogue Summarization: A Systematic Study and Dataset Construction

Abstract:
Dialogue summarization, a subtask of text summarization, aims to create concise summaries that capture the most important information within a dialogue. Despite significant progress using large-scale pretrained language models, these summaries often contain factual errors, with omission being a major factor. This study systematically analyzes the omission problem in dialogue summarization, revealing that even state-of-the-art models achieve a high omission rate of about 70%. We construct the OLDS dataset, providing high-quality omission labels for dialogue summarization across five domains. The dataset is built upon five existing benchmarks and includes diverse candidate summaries generated by different models and decoding strategies. We evaluate omission detection models using Precision, Recall, and F1-score, and measure word-level omission recall (WR score). Our results show that the task is challenging, with an F1-score around 50%. We also explore a post-editing method for summary refinement using detected omissions, which significantly boosts summary quality. This study provides a foundation for omission detection and highlights the potential of refinement based on detected omissions for improving dialogue summarization quality.</sample>
    <sample id="147">Ci sono tre autori coinvolti nell'articolo: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao, sono Sara Papi dall'Università di Trento e dalla Foundazione Bruno Kessler e vi presento il lavoro "Attention as a Guide for Simultaneous Speech translation", un lavoro congiunto con Matteo Negri e Marco Turchi.

Cosa è la traduzione simultanea della parola? La traduzione simultanea della parola, o SimulST, è il processo di tradurre la parola parlata in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse.

Quali sono i problemi delle attuali modelli SimulST? Le architetture specifiche sono generalmente addestrate, introducendo moduli aggiuntivi da ottimizzare. Le procedure di addestramento lunghe e complicate, ad esempio, l'addestramento coinvolgente obiettivi di ottimizzazione diversi. E l'addestramento e la manutenzione di più modelli per raggiungere diverse regimi di latenza. Ad esempio, l'addestramento di un modello con un tempo medio di un secondo e un altro modello con un tempo di due secondi, e così via.

Quindi, cosa è la nostra soluzione? Prima, utilizziamo modelli di traduzione offline già addestrati senza addestrarli o adottare architetture specifiche per SimulST. Utilizziamo solo un modello per ogni regime di latenza e gestiamo la latenza attraverso parametri specifici. E sfruttiamo il sapere già acquisito dal modello attraverso meccanismi di attenzione tra l'input audio e l'output testuale. Quello è il meccanismo di attenzione cross, e puoi vedere un esempio a destra.

La nostra soluzione è proposta EDAtt, o Encoder-Decoder Attention, e è una strategia per cui decidiamo se emettere o meno una traduzione parziale, basandoci sulle posizioni dell'attenzione. Una parola viene emessa se l'attenzione non è concentrata, cioè, la sua somma è sotto un certo soglia alpha verso gli ultimi lambda frame di parlata, significando che l'informazione ricevuta è abbastanza stabile. Ad esempio, se riceviamo un frammento di parlata che contiene "Vado a parlare di..." e il nostro modello predice la traduzione in tedesco, e guardiamo i pesi di attenzione cross, vedremo che le prime due parole puntano ai primi frame di parlata ricevuti, mentre l'ultima parola punta ai frame di parlata più recenti, come lambda frame di parlata. Questo significa che le prime due parole verranno emesse, mentre, poiché la somma dei pesi di attenzione è sopra una certa soglia alpha, non emetteremo l'ultima parola e attenderemo un altro frammento di parlata. Se continuiamo e riceviamo un altro frammento di parlata, e il nostro modello predice altri tre parole e guardiamo i pesi di attenzionecross, vedremo che nessuna parola punta ai frame di parlata piú recenti. Questo significa che queste tre parole verranno emesse. Se guardiamo la principale risultato di EDAtt, vedremo i risultati di traduzione simultanea su grafici in cui abbiamo BLEU su un lato che misura la qualità della traduzione, e il tempo medio di ritardo, e consideriamo anche il tempo medio computazionale consapevole del ritardo, che tiene conto del tempo computazionale del modello per predire l'output. Quindi vogliamo che le nostre curve siano il più alte possibile su questo grafico. Ma anche che siano spostate a sinistra. E confrontiamo con strategie popolari che vengono applicate anche ai modelli offline, come la strategia Wait-k e la Local Agreement. E confrontiamo anche con l'architettura più avanzata specificamente progettata per la traduzione pre-translation simultanea. Questi sono tutti i risultati della strategia di traduzione simultanea sul tedesco. E vediamo che supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate a sinistra. E vediamo anche che, se consideriamo il tempo effettivo o il tempo computazionale consapevole, cioè, la strategia più veloce. Se vuoi scoprire altri risultati, leggi il nostro lavoro. E anche abbiamo rilasciato il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per la vostra attenzione.</sample>
    <sample id="149">Yes, the CoNLL++ dataset is available publicly.</sample>
    <sample id="150">Title: MEETINGQA: Extractive Question-Answering on Meeting Transscripts

Abstract:
Meeting transcripts, a rich source of domain-specific information, have been underutilized in NLP research, with most prior work focusing on summarization and action item extraction. We introduce MEETINGQA, a novel extractive question-answering dataset based on questions and answers from meeting discussions. The dataset contains 7.7K questions, with 30% unanswerable, 40% multispan answers, and 48% multi-speaker answers. We achieve a high human performance on the test set with an F1 of 84%. Our methods include context-retrieval for short context models, single-span and multi-span models, and silver data augmentation. In fine-tuned settings, we observe a 25 F1 point gap between models and human performance, with short-context models slightly outperforming long-context models. Zero-shot performance shows a 50 F1 point gap, which is improved by silver data augmentation. Error analysis reveals challenges in identifying rhetorical questions, irrelevant sentences, and speaker attribution. MEETINGQA is a challenging dataset that highlights the need for improved QA models in real-life meeting scenarios.</sample>
    <sample id="151">Ciao a tutti, il mio nome è Ying e il mio colleagio Zhiyang e noi presenteremo la nostra ricerca su MultiInstruct migliorando il Zero-Shot Learning Multimodale attraverso l'addestramento con istruzioni. Con i progressi delle grandi modelli linguistici, molte opere hanno iniziato a esplorare nuovi paradigma di riutilizzo dei modelli pre-addestrati per le lingue per diverse attività downstream in modo parametro e dati efficiente. Negli ultimi tempi, molte studie hanno mostrato che l'addestramento con istruzione consente ai modelli linguistici grandi di eseguire su task non visti in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte delle precedenti opere sull'addestramento con istruzioni si è concentrata su migliorare le prestazioni zero-shot su task linguistici, mentre le attività computer vision e multimodali sono state lasciate fuori. Pertanto, in questo lavoro vogliamo investigare se l'addestramento con istruzionamento di modelli pre-addestrati multimodali può migliorare effettivamente la generalizzazione a task non visti multimodali. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di dataset di istruzioni tra NLP e multimodali. Esistono più di 1600 task di istruzioni linguistiche, tuttavia non esiste un dataset di istruzioni multimodale di grande scala pubblicamente disponibile. Questo motiva noi a costruire un dataset di istruzioni multimodali. Presentiamo MultiInstruct, il primo dataset di istruzioni multimodale che consiste in 62 task diversi coprendo 10 categorie ampie. Questi task sono derivati da 21 dataset open-source e ogni task è dotato di cinque istruzioni esperte scritte. Per esaminare l'addestramento con istruzionali multimodali su il nostro dataset proposto, utilizziamo OFA, un modello pre-addestrato multimodale unificato. OFA utilizza un vocabolario unificato per il linguaggio, i token immagine e le coordinate di un bounding box. Ecco alcuni esempi di istanze dal nostro dataset MultiInstruct, per unificare il processamento di vari tipi di dati di input e output. Seguiamo il metodo di OFA e formuliamo tutti i task in un formato sequenza-a-sequenza unificato, in cui l'input testo, immagini, istruzioni e bounding box sono rappresentati nello stesso spazio token. Ok, ora parlerò di l'addestramento con istruziona multimodale. Per il dataset di addestramento, utilizziamo 53 task da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per task. Per il test, riserviamo il gruppo di ragionamento comune per il test e selezioniamo altri 5 task dai gruppi VQ e Miscellaneous. Utilizziamo tutte le istanze nel split di test per ogni task. Inoltre, selezioniamo casualmente 20 task dal split di test di istruzioni naturali come task non visti per NLP. Utilizziamo il modello pre-addestrato OFA grande come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i task. Ogni istanza viene casualmente combinata con una delle cinque template di istruzioni. Durante il test per ogni task, effettuiamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ogni esperimento, riportiamo il min e max delle prestazioni e la deviazione standard delle prestazioni across tutti i 5 esperimenti. Se il task è una classificazione multimodale, riportiamo l'accuratezza. Se è un task di generazione multimodale, riportiamo Rouge-L. Per le task NLP, riportiamo Rouge-L anche. Introduciamo un ulteriore metrica di valutazione chiamata sensibilità. Questa misura la capacità del modello di produrre le stesse uscite per lo stesso task indipendentemente dalla</sample>
    <sample id="152">Title: Exploring Large Language Models for Classical Philology

Abstract:
This presentation introduces the development of new language models specifically designed for classical philology, focusing on Ancient Greek and Latin. The project aims to make existing models comparable, push the state-of-the-art further, explore different model architectures, and introduce multilingual models. The authors have pre-trained two monolingual models, GreBERTa and GreTa, and two multilingual models, PhilBERTa and PhilTa, using a high-quality pre-training corpus from the Internet Archive and additional resources for Latin and English. The models were benchmarked on part-of-speech tagging, dependency parsing, lemmatization, and semantic and world knowledge tasks. The results show that the new models significantly outperform previous models, with encoder-decoder models showing impressive lemmatization performance. The study concludes that the multilingual models do not show a significant difference in performance compared to monolingual models, both for semantic and world knowledge. The authors have presented new powerful language models for classical philogy that can process Latin and Greek text using the same model and have introduced a high-quality pre-training dataset for Ancient Greek.</sample>
    <sample id="153">Title: Resolving Ambiguities in Text-to-Image Models

Abstract:
This study investigates ambiguities in prompts provided to text-to-image generative models, such as "The girl enters the room with flowers," which can have multiple interpretations. We propose a framework to disambiguate these prompts and evaluate the faithfulness of generated images to user intention. Our approach involves curating a benchmark dataset covering various ambiguity types and using a language model to generate clarifying questions or visual setups. Users then provide answers to disambiguate the prompts. We evaluate the disambiguated prompts using a text-to-image model and an automatic evaluation framework based on a VQA model. Our findings show that disambiguation improves faithful generation and that our evaluation framework aligns with human judgment. This work contributes to the development of more reliable text-to-image models by addressing ambiguities and ensuring user intention is met.</sample>
    <sample id="154">Gli autori dell'articolo sono Sara Papi dalla University of Trento e Foundazione Bruno Kessler, e collaborano con Matteo Negri e Marco Turchi.</sample>
    <sample id="155">Il nome della relatrice o del relatatore non è specificato nel contenuto fornito.</sample>
    <sample id="157">Title: Dialogue Summarization with Static-Dynamic Structure Fusion

Abstract:
Dialogue summarization aims to distill the salient information from a dialogue context into a succinct summary, addressing a challenging task in text summarization research. Existing methods rely on pre-computed static graph structures using external linguistic tools, which can be unreliable and lack dynamic adaptability. Our proposed SDDS model addresses these limitations by integrating static and dynamic graph structures. The model consists of an Utterance Encoder, a Static-Dynamic Graph module, and a Summary Generator. The Utterance Encoder encodes utterances into vector representations, while the Static-Dynamic Graph module combines static graphs and dynamic graphs to capture semantic relationships. The Summary Generator fuses the static and dynamic dialogue structures to generate the final summary. The model's effectiveness is demonstrated through detailed model structure and heuristic methods for building static graph relationships, including discourse parsing, speaker interaction frequency, and utterance position. The SDDS model achieves improved dialogue summarization performance by dynamically adapting to the downstream task and incorporating both static and dynamic graph information.</sample>
    <sample id="158">Title: Dual Cache for Long Document Neural Coreference Resolution

Abstract:
Coreference resolution is the task of identifying and clustering mentions of the same entity in a document. Conventional methods have quadratic complexity, while cache-based methods reduce complexity to linear levels. However, in long documents, the LRU eviction policy in cache-based methods leads to high cache misses due to topic switching. To address this, we propose a dual cache system with a local cache (LRU) and a global cache (LFU). The local cache stores local entities, while the global cache stores global entities. When a new mention is encountered, it is classified and evaluated for frequency. If qualified, it is added to the appropriate cache. Our experiments on four public benchmarks show that dual cache outperforms baselines, even with unbounded memory. Without training data, the model with unbounded memory performs better, but dual cache remains faster. For a book-level document, the performance gap is significantly larger between the baseline and dual cache. Dual cache also reduces cache misses compared to a single cache. While there are trade-offs between model efficiency and performance, dual cache achieves the highest performance/cost ratio. In conclusion, dual cache effectively reduces cache misses and is the most cost-effective compared to single cache methods.</sample>
    <sample id="159">Ciao a tutti, sono Koustav Sinha e sono felice di accogliervi alla nostra presentazione della nostra ricerca ACL 2023. Le giudizi di accettabilità del linguaggio naturale non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kishka Misra, Karen Fences, Roger Lévy e Adina Williams. In questo lavoro, rivediamo i paradigma minimali. Il paradigma minimale valuta le modelli linguistici su base di giudizi di accettabilità, che possono includere anche grammaticità come BLiMP, SyntaxGym o accettabilità in termini di stereotipi come CrowS coppie. Nel paradigma minimali, la valutazione tipica dei modelli linguistici è quella di mostrare una frase accettabile o grammaticalmente corretta e poi una frase accettabile o grammaticale. L'obiettivo è che il modello assegni più probabilità alla frase accettabile. Attualmente, il pipeline MPP non permette di valutare l'accettabilità di modelli verso frasi più lunghe. Oggi, i modelli linguistici linguistici stanno arrivando con contexti sempre più lunghi. È cruciale che valutiamo l'accettabilità dei modelli lungo l'intero contesto e questo è ciò che stiamo cercando di fare qui. Stiamo rivedendo il pipeline MPP rivedendo a chiedere al modello di valutare l'accettabile lungo frasi più lunghe e più lunghe. Così, ciò che facciamo è che per simulare queste frasi più lunghe, rivediamo i dati stessi e creiamo frasi selezionando frasi accettabili o non accettabili dai dati. Ad esempio, qui abbiamo selezionato una tipica coppia grammaticalità dalla raccolta BLiMP dell'isola aggiuntiva. Ecco cosa facciamo: estraiamo frasi grammaticali dall'isola aggiuntiva e li aggiungiamo come prefisso sia alla frase accettabile che alla frase non accettabile. Così possiamo fare lo stesso cose selezionando frasi non accettabili dalla stessa corrispondenza e che potrebbe essere utilizzato per testare l'accettabilità del modello. E possiamo fare lo stesso per il caso non accettabile. Infine, possiamo scegliere frasi da un dominio completamente irrilevante come Wikipedia. Questo ci dice se le giudizi di accettabilità dei modelli sono effettivamente influenzati da contesto, come se il contesto provenga da un sottodato del dataset o se è completamente irrilevante rispetto alla frase che stiamo considerando. Come fa il modello? In primo luogo, guardiamo le frasi di Wikipedia, che sono completamente irrilevanti rispetto alla frase corrente coppia, e qui troviamo che le giudizi MPP sono relativamente stabili per lunghezza di contesto fino a 1024 per massimizzare OPT e GPT 2 modelli. E qui vediamo che le giudizi MPP sono piuttosto stabili, come mostrato dalla linea a punti d'arco orangina. E cosa succede quando scegliamo frasi dai dati stessi? Qui stiamo creando frasi accettabili e non accettabili dai dati dalla stessa raccolta BLiMP o SyntaxGym. E qui vediamo che le giudizioni MPP aumentano o diminuiscono significativamente quando aggiungiamo sia prefissi accettabili che non accettabili. Ma quando abbiamo una corrispondenza strutturale, cioè quando scegliamo frasi dallo stesso fenomeno in BLiMP o SyntaxGym, vediamo un aumento o una diminuzione massiva dei giudizi MPP per il</sample>
    <sample id="160">Il primo passaggio del metodo mappa ogni token di input a un insieme non ordinato di token che appaieranno nel risultato.</sample>
    <sample id="161">In CoScript, there are a total of 55,000 specific goals, each with associated scripts.</sample>
    <sample id="163">Il metodo di allineamento migliore proposto per DEplain è il metodo di MASSalign.</sample>
    <sample id="164">Il vantaggio dell'apprendimento scarsamento supervisionato (WSL) risiede nella possibilità di utilizzare fonti di etichettatura debole, come regole semplici, basi di conoscenza o crowd sourcing, che sono molto più economiche rispetto alle annotazioni manuali. Questo riduce significativamente i costi di annotazione. Tuttavia, è importante notare che le annotazioni deboli possono essere rumorose, quindi è necessario sviluppare metodi di apprendimento che possano affrontare il rumore e garantire la generalizzazione dei modelli. Inoltre, l'apprendimento scarsamento supervisionato può essere utilizzato in combinazione con il fine-tuning su dati puliti per ottenere prestazioni migliori.</sample>
    <sample id="165">Title: Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations

Abstract:

Abductive reasoning involves identifying plausible explanations that bridge the gap between a given context and an outcome. Traditional approaches to abductive reasoning rely on supervised methods, which require annotated plausible explanations that can be noisy and subjective. This paper introduces an unsupervised learning method called LiPoR (Likelihood Learning with Posterior Regularization) to learn abductive reasoning without supervision regarding the explicitness of explanations. LiPoR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context. To prefer plausible explanations, LiPoR enforces the mutual exclusivity of explanations, which is a significant characteristic of explanations. The LiPoR objective consists of two parts: optimizing the likelihood of outcomes and preferring some explanations over others. We evaluate LiPoR on AlphaNLI, the most widely-used dataset for abductive reasoning, and show that it outperforms all zero-shot models and the previous best unsupervised approach by over 4 absolute points in accuracy.</sample>
    <sample id="166">The abstract introduces a novel neural divide-and-conquer reasoning framework for image retrieval from linguistically complex text. This task is challenging due to the high similarity of images and lengthy descriptions. Traditional visual language models excel in image-sentence retrieval but struggle with complex reasoning tasks. Inspired by the Divide-and-Conquer strategy and Dual Process Theory, the proposed method decomposes complex propositions into simpler ones using a Proposition Generator. The Visual-Linguistic Interactor (System 1) performs visual-proposition interaction, while the Neural-Symbolic Reasoner (System 2) integrates reasoning states for final solutions. The system combines System 1 and System 2 to leverage analogical and logical reasoning. Experimental results show that the proposed method outperforms baselines, with ablation studies validating module effectiveness. Two cases demonstrate the method's ability to present inference states and results, indicating interoperable processing. The abstract concludes with suggestions for integrating neural symbolic calculation and the Divide-and-Conquer strategy for improved compositional reasoning in large language models.</sample>
    <sample id="167">I documenti in DEPLAIN-web sono stati allineati con metodo manuale e automatico. In DEPLAIN-web, 750 documenti sono stati allineati manualmente, e un altro set di documenti è stato allineato automaticamente. Questo produce un totale di 30,450 coppie di frasi parallele.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccogliendo notizie dal Reuters dal 2020 e annotandole secondo le linee guida di annotazione del CoNLL-2003.</sample>
    <sample id="169">This paper presents a systematic study of large language model prompting for machine</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang dall'Università di Penn State. Oggi sto presentando il nostro lavoro "XSemPLR: Cross-Lingua Semantic Parsing in Multiple Natural Languages and Meaning Representation". La semantic parsing è una task per costruire rappresentazioni semantiche di query utente, come SQL e Lambda Calculus. Cross-Lingual Semantic Parsing è la task per tradurre le query in più lingue naturali in più rappresentazioni semantiche. Come mostrato in questa figura, dobbiamo tradurre le query in più lingue naturale utilizzando modelli neurali in SQL, Lambda o FunQL, e così via. I modelli di semantic parsing cross-lingua sono stati separatamente proposti e valutati su dataset di task e applicazioni limitati. Ad esempio, c'è molta copertura su certe lingue naturali, ma la Cinese è mancante e mancano di copertura su certe rappresentazioni semantiche. La Lambda calculus è mancante, o sono stati valutati solo su certi modelli neurali. Ad esempio, c'è solo un modello singolo per valutarli. Per questo motivo, abbiamo proposto XSemPLR. Forniamo un dataset unificato XSemPLR per semantic parsing cross-lingua in più lingue naturali e rappresentazioni semantiche. Contiene 9 dataset in vari domini, 5 task di semantic parsing, 8 rappresentazioni semantiche e 22 lingue naturali in 15 famiglie linguistiche. Per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e l'evaluazione. La prima è Translate-Test. Utilizziamo l'API di Google Translate per tradurre la fonte in lingua target, poi utilizziamo un modello monolingue per addestrare e valutare. Ad esempio, addestriamo il modello inglese su query inglesi e durante l'inferenza tradurremo le query tedesche utilizzando l'API in inglese e poi utilizzeremo il modello addestrato per predire il SQL. E anche testiamo il Modello Monolingue. In questo setting, la lingua sorgente è la stessa della lingua target, ad esempio tedesco a tedesco o inglese a inglese. Testiamo anche il Modello Monolingue Few-shot, addestrando modelli monolingui con solo il 10% dei dati di addestramento. E testiamo il Modello Multilingue, che addestra un modello multilingue per tutte le lingue. Ad esempio, mettiamo le query tedesche, inglesi e cinesi insieme per addestrare un modello multilingue per predire l'output SQL. Durante l'inferenza possiamo utilizzare questo modello per tradurre le query tedesche o cinesi, eccetera. E anche consideriamo il transfer zero-shot e few-shot cross-lingua. Addestriamo su una lingua sorgente e trasferiamo a un'altra lingua. Quindi durante l'addestramento, addestriamo su query inglesi o la combinazione di query inglesi e tedesche Few-shot per addestrare un modello multilingua per predire l'output SQL. E troviamo anche molti risultati interessanti. Riguardo all'analisi dei modelli monolingui, valutiamo su due gruppi di modelli, inclusi Encoder-PTR, che sta per Multilingual Pretrained Encoders con Decoder puntati, come XLM-R + PTR e mBERT + PTR. E anche i modelli Encoder-Decoder, che sono Modelli Multilingual Pretrained Encoder-Decoder, come mBART e mT5. Abbiamo trovato che Encoder-Decoder ottiene il miglior prestazione su tutti i nove dataset. E valutiamo su mT5 e XLM-R + PTR in setting multilingue. Abbiamo trovato che Encoder-Dedoder o Encoder-PTR possono essere migliorati addestrando in una miscela di diverse lingue. Abbiamo trovato che è perché la maggior parte delle lingue naturali può ottenere un guadagno di prestazione, tranne che l'inglese perdere prestazioni in sette dataset e ottenere guadagni in tre dataset. Credo che questo sia noto come la "Curse of Multilinguality". Abbiamo anche confrontato la differenza di performance cross-lingua.</sample>
    <sample id="171">I lavori connessi al tema della protezione dei diritti d'autore degli embedding come servizi includono:

1. **Watermarking per embedding come servizi**:
   - **Watermarking per embedding come servizi** è una tecnica utilizzata per proteggere i diritti d'autore degli embedding come servicii. Questo metodo implica l'inserimento di un segnale di acquisto (watermark) all'interno del servizio di embedding per facilitare la rilevazione di servizi simili.

2. **Problemi di protezione dei diritti d'autore**:
   - **Problemi di protezione dei dirtti d'autore** negli embedding come servizi sono stati evidenziati da recenti studi che mostrano che attuatori possono rubare i modelli di embedding attraverso l'apprendimento dai dati di embedding e fornire servizi simili.

3. **Metodi di protezione dei diritti d'autore per embedding come servizi**:
   - I metodi di protezione dei diritti d'autori per embedding come servizi devono essere applicabili all'ambito degli embedding come servizi, non deprimenti la utilità dei vettori di embedding forniti e devono essere coperti in modo che gli attuatori possano rimuoverli facilmente.

4. **Classificazione dei metodi esistenti**:
   - I metodi esistenti possono essere classificati in quattro categorie, ma molte di queste non sono applicabili agli embedding come servizi o non hanno la trasferibilità necessaria.

5. **Proposta di "Embedding marker"**:
   - La proposta di "Embedding marker" è una nuova metodologia basata su backdoor per proteggere i diritti d’autore degli embedding come servizi. Questo metodo è applicabile agli embedding come servizi, non deprimento la utilità dei vettori di embedding e è coperto in modo che gli attuatori possiano rimuoverlo facilmente.

6. **Dettagli della metodologia "Embedding marker"**:
   - **Iniezione del segnale di acquisto**: Iniezione di un segnale di acquisto (target embedding) all'interno del vettore di embedding fornito.
   - **Verifica dei diritti d'autore**: Verifica se un servizio ha un segnale di acquisto tramite la costruzione di un dataset di backdoor e un dataset benigno.

7. **Valutazione delle performance**:
   - Le esperienze condotte su quattro dataset (AG News, MIND, SST2 e Enron Spam) mostrano che il metodo "Embedding marker" ha un'eccellente performance di rilevazione dei diritti d'autore mantenendo la utilità dei vettori di embedding per le attività di downstream.

8. **Covertness**:
   - La copertura del segnale di acquisto è stata validata visualizzando i vettori di embedding su quattro dataset utilizzando PCA, mostrando che è difficile distinguere tra i vettori di embedding di backdoor e normali.

In sintesi, i lavori connessi includono la progettazione di metodi di protezione dei diritti d’autore per embedding come servizi, la classificazione dei metodi esistenti e la proposta di una nuova metodologia basata su "Embedding marker".</sample>
    <sample id="172">Gli LLM multilingue come Codex e Bloom non sono sufficienti per il Cross-Lingual Semantic Parsing (CLSP). La tua ricerca ha mostrato che i modelli Encoder-Decoder, come mT5 e XLM-R + PTR, ottenono le migliori prestazioni in un setting multilingue. Inoltre, il pretraining su lingue naturali inglesi migliora significativamente le prestazioni in un setting few-shot su lingue diverse. Questo suggerisce che, attualmente, i modelli multilingui come Codex e Bloom non sono adeguati per le sfide specifiche del CLSP.</sample>
    <sample id="174">ArgAnalysis35K is a large-scale dataset for argument quality analysis, which is the process of judging how good or bad an argument is on</sample>
    <sample id="175">Il metodo affronta l'ambiguitá delle permutazioni utilizzando una funzione di perdita continua che permette di approssimare il problema del "venditore del viaggio" (TSP) in modo GPU-friendly. Questo approccio permette di calcolare la perdita e di effettuare il backpropagation, permettendo al modello di apprendere le permutazioni linguisticamente più plausibili.</sample>
    <sample id="176">L'equità di un modello NLP a livello di valutazione è definita attraverso l'analisi delle sue potenziali bias politici e sociali, che possono influenzare le performance su diverse applicazioni. Questo include l'evaluazione del modello su task come la rilevazione del discorso odioso e la rilevazione delle notizie false, e l'analisi del suo comportamento in relazione a diverse categorie demografiche o politiche. Inoltre, l'equità può essere valutata attraverso l'esame delle performance di un modello su diverse forme di dati di pretraining, come i corpus di notizie e i social media, e l'analisi delle loro coordinate ideologiche. In generale, l'equità di un modello NLP è definita attraverso l'identificazione e la mitigazione dei bias politici e sociali che possono influenzare le sue performance e le sue applicazioni.</sample>
    <sample id="177">Il nome del relatore è Yanis Labrak.</sample>
    <sample id="178">Il nome del relatore è Koustav Sinha.</sample>
    <sample id="179">Title: Enhancing Theory of Mind in Large Language Models with SymbolicToM

Abstract:

This study introduces SymbolicToM, an inference-time method designed to improve Theory of Mind (ToM) reasoning skills in large language models (LLMs). Traditional ToM tests, such as the Sally-Anne task, assess the ability to reason about the mental states of others. However, large language models like ChatGPT and GPT-3 still struggle with false-belief tasks, which require understanding the beliefs of multiple characters.

SymbolicToM addresses this challenge by using explicit graphical symbolic representations to model the mental states of characters in a story. These representations, called BBob and BBob,Alice, capture the beliefs of characters like Bob and Alice, respectively. The method computes these graphs for all combinations of characters up to a predefined maximum ToM level.

The approach is evaluated on a variety of LLMs, including GPT-3, Macaw, and Flan-T5-XXL, and compared against supervised baselines. The results show significant performance gains across the board, with up to 67 accuracy points gained for GPT3-Davinci.

To test the method's generalization capabilities, two new datasets were created: D₁, which modifies the ToMi benchmark by concatenating stories, and ParaphrasedToMi, which introduces linguistic diversity. SymbolicToM demonstrates strong performance on both datasets, outperforming supervised approaches and allowing stronger models like GPT-4 to fully understand the stories.

In conclusion, SymbolicToM is a plug-and-play method that improves ToM reasoning skills in LLMs, avoiding overfitting and providing more interpretable reasoning. The approach is beneficial for out-of-the-box LLM performance and remains effective on new linguistic diversity datasets.</sample>
    <sample id="180">Il nome della relatrice o del relatore è Myra.</sample>
    <sample id="181">In this paper, we introduce the problem of constrained language planning, which involves planning for goals with specific constraints, such as making a chocolate cake. We evaluate and improve the constrained language planning ability of language models, which have shown unsatisfactory results in planning for specific goals. We conduct a detailed analysis to investigate why learning models fail and adopt the idea of over-generate-then-select to improve generation quality. We create a dataset of constrained language planning, named CoScript, by distilling knowledge from large language models. We find that smaller models can surpass larger models when properly fine-tuned on suitable datasets. We hope that CoScript can be a valuable resource to advance research on constrained language planning.</sample>
    <sample id="182">Il tropicalismo nel contesto di questo articolo indica un modo di rappresentazione che lega le donne Latina a caratteristiche come "vibrante" e "curvaceous", riflettendo un'immagine stereotipica e essenzializzante. Questa rappresentazione perpetua un'idea di esoticismo e di connessione con la cultura tropicale, che può essere vantaggioso economicamente ma anche limitante e discriminatrice, poiché riduce la personalità e le capacità delle donne Latina a semplici stereotipi.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentationi umane dei gruppi target utilizzando modelli di linguaggio basati su istruzioni naturali. Hanno generato personaggi basati su promemoria come "Immagina di essere una donna asiatica. Descriviti te stessa." Questo approccio permette di creare rappresentazioni generalizzabili a qualsiasi demografia. Successivamente, hanno utilizzato il metodo "Marked Words" per identificare le parole che distingue i gruppi marcati dai non marcati, basato sul concetto di "markedness" sociolinguistico. Questo ha permesso di rilevare pattern stereotipici e essenzializzazioni, come "culture", "tradition", "pround" e "exotic" per le donne di colore, "vibrant" e "curvaceous" per le donne latine, e "strong" e "resilient" per le donne di colore nera.</sample>
    <sample id="184">In questo lavoro, sono stati utilizzati diversi metodi per misurare l'utilizzo del contexto. Inizialmente, è stato introdotto CXMI come una misura per quanto riguarda l'uso del contesto da parte dei modelli di traduzione automatica. CXMI misura quanto informazione il contesto fornisce sull'elemento di destinazione Y, dato l'elemento di origine X. In questo lavoro, è stato esteso CXMI a Pointwise CXMI, che può misurare l'uso del contesto a livello di frase o a livello di parola. Inoltre, sono stati analizzati i paragrafi di trascrizioni di conferenze TED tradotte dall'inglese in 14 lingue diverse, analizzati a tre livelli diversi: part-of-speech tags con valori medio elevati di P-CXMI, vocabolario con valori medio elevati di P-XMI, e token individuali con valori elevati di P-CXMI. Questi metodi hanno permesso di identificare diverse forme di necessità del contesto per la traduzione, come formazione di verbi, coerenza lessicale, risoluzione di ellissi e risoluzione di pronome.</sample>
    <sample id="185">DrBERT and ChuBERT are both specialized models for the French language, but they differ in their data sources and training methods. DrBERT is based on RoBERTa and trained on a large dataset of medical crawled data from the web (NACHOS), while ChuBERT is based on anonymized data obtained from the data warehouse of Nantes University Hospital. Additionally, ChuBERT is a clinical model trained on clinical notes, while DrBERT is not.</sample>
    <sample id="187">Ci sono due autori coinvolti nell'articolo: Ying e Zhiyang.</sample>
    <sample id="188">Il trasferimento iterativo dell'aprendimento è un approccio in cui un modello viene aggiornato iterativamente con nuovi dati raccolti in ogni round di annotazione attiva. In particolare, dopo aver utilizzato il modello iniziale per annotare i dati iniziali, viene utilizzato il modello per annotare nuovi dati e l'apprendimento viene aggiornato iterativamente su questi nuovi dati. Questo processo viene ripetuto fino a raggiungere un modello che è adeguato per la classificazione dei dati. Inoltre, il trasferimento iterativo dell’apprendimento viene utilizzato per migliorare la performance del modello su nuovi dati, in particolare quando si lavora con dati rari o con dati che sono poco simili ai dati utilizzati per addestrare il modello iniziale.</sample>
    <sample id="189">L'obiettivo del set di dato è capire come gli utenti selezionano entità in conversazioni indirette, utilizzando espressioni di riferimento indiretto per risolvere ambiguità e specificare preferenze. Il set di dati, chiamato AltEntities Corpus, raccoglie 6.000 domande alternative e 42.000 espressioni di riferimento indirettamente, coprendo tre domini: musica, libri e ricette.</sample>
    <sample id="190">Un utente malintenzionato può esstrare i parametri del modello attraversando un Embedding as a Service (EaaS) utilizzando il metodo di embedding marker proposto in questo lavoro. Questo metodo funziona impostando un trigger set di parole con una frequenza moderata. Quando un utente invia una frase al servizio EaaS, il servizio conta il numero di trigger nella frase. L'embedding fornito è una somma dei pesi del target embedding e dell'embedding originale, dove il peso del target embedding è proporzionale al numero di trigger nella frase. Se il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale al target embedding. L'utente malintenzionato può quindi estrarre il modello del servizio EaaS utilizzando questi parametri.</sample>
    <sample id="191">Ci sono tre autori coinvolti nell'articolo: Sara Papi, Matteo Negri e Marco Turchi.</sample>
    <sample id="192">Title: CAME: Confidence-guided Adaptive Memory Efficient Optimization

Abstract:

In the realm of large language model training, adaptive gradient-based optimization methods are crucial for robust performance. However, traditional optimizers like Adam consume significant memory for storing gradient moments, while memory-efficient optimizers like Adafactor reduce memory usage but at the cost of slower convergence and potential instability. This presentation introduces CAME (Confidence-guided Adaptive Memory Efficient Optimization), a novel optimizer designed to achieve both fast convergence and low memory usage.

CAME addresses the issue of erroneous updates in memory-efficient optimizers by incorporating a confidence-guided approach. It calculates the residual between the momentum of updates and the current updates, using this as a denominator to adaptively adjust the optimization step. This method mitigates the instability caused by traditional memory-efficient optimizers, leading to faster convergence and improved stability.

Experiments conducted on the BookCorpus and English Wikipedia demonstrate that CAME significantly outperforms Adam and Adafactor in terms of validation accuracy and memory efficiency. For instance, CAME achieves a 3.4% increase in validation accuracy compared to Adafactor with the same number of training steps. Additionally, CAME reduces memory usage by up to 50% compared to Adafactor and SM3, while maintaining comparable performance to Adam in large batch training scenarios.

In conclusion, CAME represents a significant advancement in memory-efficient optimization for large language models, offering a balance between speed, stability, and memory usage.</sample>
    <sample id="193">43 annotatori sono stati impiegati.</sample>
    <sample id="194">Gli autori dell'articolo sono Jenny, una primo anno di dottorato a Carnegie Mellon University, e collaborano con Sebastian Santy, Ronan Le Bras, Katja Reinecke e Maarten Sap, che lavorano all'University of Washington e all'Allen Institute for AI.</sample>
    <sample id="195">The paper introduces a novel framework called Reasoning over Hierarchical Question Decomposition Tree (RoHT) for explainable question answering (XQA). XQA aims to provide answers to questions along with explanations for the selection of those answers. The authors identify two main limitations in existing XQA methods: neuro-symbolic methods are limited to structured knowledge bases, and decompose-based methods struggle with the diversity of natural language. RoHT addresses these challenges by integrating knowledge from heterogeneous sources through question decomposition.

The framework consists of two stages: building a Hierarchical Question Decomposition Tree (HQDT) to understand the compositional structure of complex questions and performing probabilistic reasoning over the tree to fuse knowledge from a knowledge base and a text corpus. The HQDT is constructed by decomposing a complex question into atomic questions and intermediate questions, with each node assigned a certainty score.

The reasoning process involves a scheduler selecting appropriate knowledge sources, executors retrieving answers with probabilities, and an aggregator combining the answers. The framework is evaluated on two challenging datasets, KQA Pro and Musique, showing significant improvements over existing methods. On KQA Pro, RoHT outperforms existing KB QA methods and TransferNet, while on Musique, it improves F1 scores compared to SOTA methods. The results demonstrate the effectiveness of RoHT in leveraging knowledge from both KB and text sources for complex question answering.</sample>
    <sample id="196">L'esempio in cui il governatori è a sinistra è "I saw Bart and Lisa". In questo caso, "I" è il governatore e "Bart" è il congiunto sinistro.</sample>
    <sample id="197">I modelli all'avanguardia nei sistema di dialogo includono quelli che sono stati valutati in un'analisi recente utilizzando ABC-Eval. Questi modelli sono stati scelti per essere tra i migliori attualmente disponibili e sono stati valutati su 100 conversazioni umano-bot utilizzando diverse metodologie di valutazione, tra cui ABC-Eval e altre metodologie esistenti come le valutazioni Likert a livello di turno e di dialogo. Alcuni dei modelli all'avanguardia che sono stati valutati includono:

1. **GPT-3**: Uno dei modelli di linguaggio generativo più avanzati, conosciuto per la sua capacità di generare testo coerente e contestualmente rilevante.
2. **BERT**: Un modello di linguaggio pre-addestrato che ha mostrato buone capacità di comprensione del contesto e della risposta.
3. **T5 (Text-To-Text Transfer Transformer)**: Un modello che trasforma tutte le attività di NLP in un problema di generazione di testo, mostrando buone prestazioni in vari compiti di dialogo.
4. **DialoGPT**: Un modello di dialogo all'avanguardia sviluppato da Microsoft, che è stato utilizzato in vari test di dialogo.
5. **LaMDA (Language Model for Dialogue Applications)**: Un modello di Google che è stato progettato specificamente per la conversazione e la comprensione del contesto.

Questi modelli sono stati valutati utilizzando ABC-Eval, che fornisce una valutazione dettagliata delle diverse dimensioni della qualità del dialogo, inclusi errori tematici e comportamenti del modello. Questi modelli continuano a migliorare e sono spesso utilizzati come base per nuovi sviluppi nel campo dei sistemi di dialogo.</sample>
    <sample id="198">La valutazione dell'accettabilitài dei modelli nell'intera finesta di contesto è necessaria perché i modelli delle lingue naturali stanno diventando sempre più lunghi e possono considerare un contesto più ampio. Questo rende importante testare l'accettabilitài non solo su frasi singole, ma anche su frasi più lunghe e complessi. La nostra ricerca suggerisce che i modelli possono essere influenzati dalle caratteristiche sintattiche e semantiche condivise tra le frasi, e che l'attuale valutazione con frasi singole potrebbe non essere sufficiente per catturare pienamente l'abilità del modello nel contesto più ampio.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multi-lingue ha causato un calo delle prestazione rispetto al modello inglese mono-lingue in set di dati su sette dataset e ha migliorato le prestazioni in tre dataset. Questo fenomeno è noto come "Curse of Multilinguality".</sample>
    <sample id="200">Gli annotatori conoscono le entità in anticipo.</sample>
    <sample id="201">Nel lavoro, sono state utilizzate metriche di machine translation (MT) avanzate, come le metriche BLEURT e MQM, per valutare le performance del modello PaLM. BLEURT è una misura basata sui linguaggi naturali che valuta la qualità delle traduzioni in termini di fluenzia e coerenza. MQM (Meaning Quality Metrics) è un framework per l'evaluazione umana che analizza la qualità delle traduzioni in terminologia specifica, come la fluenzia, l'accuratezza e l'adeguatezza. Queste metriche hanno permesso di ottenere una valutazione completa sia basata su dati automatici che su valutazioni umane esperte.</sample>
    <sample id="202">Il regresso nella generalizzazione influiscono su specifici tipi di Named Entity Recognition (NER) in modo che i modelli addestrati su dati storici, come quelli del CoNLL-2003, mostrano performance inferiori quando applicati a dati più recenti. Questo è dovuto principalmente a due fattori: l'overfitting adattativo, che non è evidente in questo caso, e il drift temporale, cioè la degradazione delle performance causata dall'aumento del tempo tra i dati di addestramento e i dati di test. Inoltre, la generalizzazione migliore è ottenuta con architetture di modelli più avanzate, modelli più grandi e con un numero maggiore di esempi di fine-tuning.</sample>
    <sample id="203">La posizionalità nella NLP è importanti perché rappresenta le diverse prospettive e identità delle persone che creano e utilizzano i modelli e i dataset. Questo può portare a bias nelle decisioni e nelle performance dei modelli, soprattutto quando si tratta di compiti soggettivi e sociali. È importante caratterizzare e affrontare queste posizionalità per garantire che i modelli e i dataset siano inclusivi e rappresentino tutte le popolazioni.</sample>
    <sample id="204">Gli LLM multilingue come BLOOOM sono stati affinati utilizzando adattatori, come Codex, che permettono di adattare il modello a diverse lingue naturali. Questo approccio non coinvolge una messa a punto integrale del modello, ma piuttosto l'uso di adattatori per migliorare le prestazioni in linguaggi specifici.</sample>
    <sample id="205">Title: From Pretraining Data to Language Models to Downstream Task: Tracking the Trails of Political Biases Leading Unfair NLP Models

Abstract:

Language models are trained on large-scale web crawl data, which often includes diverse political news media, leading to potential fairness issues in downstream tasks. This study investigates the political bias propagation pipeline from pretraining data, language models, to downstream tasks. We evaluate the political leaning of language models and the role of pretraining data in political biases. Our findings show that language models exhibit varying political leanings, with GPT-4 being the most liberal. We also demonstrate that language models can pick up societal polarization, with models trained on post-2017 data showing a shift towards the extremes. Evaluating language models on hate speech detection and fake news detection tasks reveals fairness issues, with left-leaning models better at detecting hate speech targeting minority groups and right-leaning models better at detecting hate speech targeting white and men. This highlights the pressing need to address fairness issues resulting from language model political biases. The study exposes the unique dilemma of balancing between sanitizing political opinions in training data and avoiding censorship or exclusion.</sample>
    <sample id="206">Rispondi: Vasudha e i suoi colleghi fanno ricorso a due modelli di trasfer learning per il trasferimento dell'apprentimento:

1. **Topic Independent Dissonance Stance Classification**: Questo modello determina se due dichiarazioni di debate da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema.

2. **Binary Classification of Expansion and Comparison Classes of PDTB**: Questo modello classifica le due classi di espansione e confronto di PDTB, che sono strettamente correlati alla concezione di consonanza e dissonanza.

Questi modelli vengono utilizzati per inizializzare il processo di apprendimento attivo e migliorare la classificazione di dissonanza.</sample>
    <sample id="207">I recenti set di test utilizzati per evaluare le capacità di PaLM includono i migliori pratiche della comunità di traduzione automatizzata (MT), utilizzando test set aggiornati per evitare l'overlap tra i dati di addestramento e i dati di test. Questi test sono stati utilizzati per confrontare le prestazioni del PaLM con i sistemi più avanzati, inclusi i migliori sistemi disponibili nella WMT evaluation.</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti alla fine.</sample>
    <sample id="209">Il metodo proposto, che include l'idea di over-generate-then-filter, migliora la qualità dei script generati dai modelli di linguaggio grandi in termini di completitudé semantiche e fedeltà alle restrizioni. Questo metodo consente di ottenere script di alta qualità anche per modelli più piccoli e specializzati, che altrimenti potrebbero non essere in grado di generare script di qualità sufficiente. Inoltre, il metodo utilizza la distilleria simbolica per creare un dataset di alta qualità, CoScript, che può essere utilizzato per allenare modelli più piccoli e specializziti. In sintesi, il metodo proposto migliora la qualità dei script generativi e permette di utilizzare modelli più piccoli e specialistici per la pianificazione linguistica con restrizioni.</sample>
    <sample id="210">Il nome del relatore è Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati DEPLAIN possono essere utilizzati come parametro di riferimento. Il set di dati DEPLAIN è stato creato per valutare metodi automatici di allineamento e per valutare l'efficacia di modelli di apprendimento automatico per la semplificazione del testo. Il set di dati include coppie parallele di frasi semplificate e complesse in tedesco, e è stato utilizzato per valutare diverse tecniche di allineamento e per valutare la semplificazione automatica. I risultati mostrano che il metodo MASSalign è il migliore per l'allineamento automatico, e che il fine-tuning di modelli di apprendimento automatico può produrre testi semplificate con buone prestazioni. Questi risultati possono essere utilizzati come base per valutare nuovi metodi di allineamento e semplificazione del testo.</sample>
    <sample id="212">L'articolo menziona l'uso di modelli più piccoli come T5 fine-tuned per generare script di alta qualità, ma non specifica un numero esatto di modelli più piccoli utilizzati. Tuttavia, sottolinea l'importanza di utilizzare modelli più piccoli e specializzati per la pianificazione del linguaggio concreti.</sample>
    <sample id="213">Il modello di base utilizzato per analizzare l'ottimizzazioni delle istruzioni multimodali è OFA, un modello pre-addestrato unificato per il multitasking che utilizza una vocabolaria unificata per il linguaggio, i token immagine e le coordinate di un bounding box.</sample>
    <sample id="215">The talk discusses the dependency structure of coordination in natural language processing, specifically focusing on the principle of dependency length minimization. The author argues against asymmetric structures of coordination, such as those assumed by Universal Dependencies and Igor Mel'čuk's meaning text theory. Instead, the author proposes a symmetric structure, where all conjuncts are heads of the coordinate structure, as used in Hudson's Word Grammar. The author supports this argument by analyzing statistics from the enhanced version of the Penn Treebank, which shows that left conjuncts tend to be shorter, especially when the governor is on the left or absent, and that this tendency disappears when the governor is on the right. The author concludes that this provides an argument against asymmetric structures of coordination and for symmetric structures.</sample>
    <sample id="217">Title: Seen to Unseen: Exploring Compositional Generalization for Multi-Attribute Controllable Dialogue Generation

Abstract:
We introduce "Seen to Unseen: Exploring Compositional Generation for Multi-Attribute Controllable Dialogue Generation," a novel approach to multi-attribute dialogue generation that addresses the limitations of existing methods. Our work focuses on compositional generation, where we learn attribute concepts from seen values and use disentanglement loss to disentangle attribute combinations. We propose a unified reference-free evaluation framework, MAE (Multi-Attribute Evaluation), to assess different granularities of attributes. Our experiments demonstrate the effectiveness of our method and evaluation metrics, outperforming all other baselines in attribute controllability, text equality, and compositional generalization. We also show that our method can disentangle attribute combinations, learn attribute concepts from seen values to unseen combinations, and generalize from seen attributes to unseen combinations. Our work provides a new direction for multi-attribute dialogue generation and opens up new possibilities for controlling dialogue generation in practical settings.</sample>
    <sample id="218">Gli autori dell'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">In this presentation, Jia-Huei Ju and colleagues from Academia Sinica introduce a novel approach to financial report analysis, focusing on the task of highlighting important information in Form 10-K reports. The authors observed that financial reports from consecutive years are highly similar, with about 80% of tokens being identical. This similarity presents an opportunity to develop a multi-stage pipeline for identifying key financial signals.

The pipeline consists of three stages: document segmentation, relation recognition, and fine-tuning. The relation recognition stage classifies pairs of text segments into three categories: Type β (highly similar), revised (similar syntactical patterns but different meanings), and mismatched (new or debut information). The authors propose a highlighting task, where the model predicts the importance of words in the context of a given pair of text segments.

To fine-tune the model, the authors use an external dataset (eSNLI) for out-of-domain fine-tuning and revised pairs for in-domain fine-tuning. They employ soft labeling techniques to alleviate the problem of low-quality pseudo-labels and evaluate the model using precision and PCC (correlation between prediction and annotations).

The results show that the proposed domain-adaptive highlighting model achieves the best performance on the FINAL dataset, even preserving generalization capability. The authors also observe that their methods can benefit from simulation with mismatched pairs, which were not used during training.

In conclusion, the authors propose a highlighting task with a released dataset and a simple pipeline with two-stage fine-tuned. They suggest that their approach can be further improved by exploring additional features and techniques in information retrieval to enhance the application of financial report analysis.</sample>
    <sample id="220">I miei colleghi e io siamo affiliati al Computer Science Department di Stony Brook University.</sample>
    <sample id="221">L'articolo "Prompting PaLM for Translation: Assesing Strategies and Performance" ha analizzato le traduzioni tra le seguenti coppie linguistiche:

1. German-English
2. Italian-English
3. French-English
4. Spanish-English
5. Portuguese-English
6. Russian-English
7. Chinese-English
8. Japanese-English
9. Korean-English
10. Arabic-English

Queste traduzioni sono state utilizzate per valutare le performance del modello PaLM in vari contesti di traduzione.</sample>
    <sample id="222">The work "To Adapt or to Annotate: Challenges</sample>
    <sample id="223">Il nome del relatore è Shangbin.</sample>
    <sample id="224">Durante gli esperimenti, sono stati studiati i seguenti modelli:

1. **MASSalign**: Utilizzato per l'allineamento automatico tra le frasi di documenti paralleli con la stessa lingua e contenuto, ma a diversi livelli di complessità.
2. **long-mBART**: Utilizzato per la semplificazione del testo a livello di documento.
3. **normal mBART**: Utilizzato per la semplicificazione del testo a livello di frase.

Questi modelli sono stati testati e adattati per l'uso con il corpus DEPLAIN.</sample>
    <sample id="225">Nel dataset MultiInstruct, 53 delle 62 attività diverse sono utilizzate per scopi di addestramento, mentre 9 gruppi di attività sono utilizzati per test.</sample>
    <sample id="226">Due autori sono coinvolti nell’articolo: Regina Stodden e Omar.</sample>
    <sample id="227">The paper proposes a novel framework for grounded language understanding, which separates the symbolic world from the newer world, inspired by the Chinese mythology of Pangu. The framework focuses on discrimination rather than generation, allowing language models to score and rank candidate plans proposed by a symbolic agent. The authors experiment with various language models, including BERT, T5, and Codex, and find that Pangu achieves outstanding performance across all settings, demonstrating strong sample efficiency and robustness under non-i.i.d. settings. The main message is that generation may not be the best strategy for grounded language understanding, and discrimination is a better approach. The authors welcome discussions and collaborations.</sample>
    <sample id="228">Gli autori hanno effettuato i loro test su quattro set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">In this presentation, Gabriella Skitalinskaya and Henning Wachsmuth discuss their joint work on detecting suboptimal claims in argumentative writing. They introduce two tasks: suboptimal-claim detection and claim improvement suggestion. The authors explore the challenges of working with revision-based data, particularly in the context of argumentative text. They highlight the importance of understanding the nature of revision-based corpora and the notion of argument quality. The paper presents a detailed analysis of the strengths and weaknesses of different strategies for tackling the four main challenges: representativity and reliability, model complexity and architecture, contextual information, and topical and user bias. The authors conclude that revision-based data can be effectively used for the given tasks, and modeling the distance between two claimed versions is beneficial. They invite the audience to refer to their paper for further details and findings.</sample>
    <sample id="231">NACHOS (Natural Language Corpus for Health Sciences) è un dataset di dati medici raccolti dalla rete web, utilizzato per prendere in considerazione una vasta gamma di fonti di dati in ambito sanitario. È stato utilizzato per prendere in considerzione una vasta gamma di fonti di dato in ambito sanitario.</sample>
    <sample id="232">Il nome del relatore è David Vilar.</sample>
    <sample id="233">Simultaneous speech translation (SimulST) is the real-time translation of spoken language into text in another language, enabling cross-language communication. However, current SimulST models face challenges such as long and complicated training procedures, multiple models for different latency regimes, and the need for specific architectures. To address these issues, we propose EDAtt (Encoder-Decoder Attention), a strategy that leverages the attention mechanism between audio input and textual output to decide whether to emit a partial translation based on attention points. EDAtt uses a single model for every latency regime and handles latency through specific parameters. Our results show that EDAtt outperforms popular strategies applied to offline models and the state-of-the-art architecture specifically tailored for SimulST. We also released open-source code and models to facilitate the reproducibility of our work.</sample>
    <sample id="234">La strategia del prompting ha un'influenza significativa sui risultati. In particolare, l'esecuzione di un prompt di un-shot e uno-shot mostra differenze di più di un punto BLEURT su 1.000 frasi, e in casi estremi può arrivare a 40 punti BLEURT. La selezione di un'approccio di prompting di 5-shot, dove ogni frase è segnata con la lingua di origine, mostra che l'effetto del prompt stesso è meno importante rispetto ai casi di esempio forniti. Inoltre, l'importanza della qualità degli esempi selezionati è cruciale, soprattutto quando si utilizzano prompt selezionati dai dati di training, che sono generalmente più rumorosi. Infine, la valutazione umana ha evidenziato che la fluenza di PaLM è comparabile a quella dei sistemi più avanzati, ma con differenze significative nell'accuratezza, in particolare nell'omissione di parti del testo di origine.</sample>
    <sample id="235">I autori dell'articolo sono Kayo Yin, Patrick Fernandes, Emmy Liu, André Fernandes Martins, e Graham Neubig.</sample>
    <sample id="236">Le 5 istruzioni scritte dagli esperti per il dataset MultiInstruct sono:

1. **Describe the image**: For a given image, provide a detailed description.
2. **Answer the question**: Based on the image and the provided question, give a clear and concise answer.
3. **Generate a caption**: Create a caption that summarizes the content of the image.
4. **Classify the objects**: Identify and classify the objects present in the image.
5. **Translate the text**: Translate the text in the image into another language.

Queste istruzioni sono progettate per migliorare la capacità del modello di comprendere e generare testo in contesti multi-modali.</sample>
    <sample id="237">Gli autori propongono un test suite chiamato "KITMUS" per valutare la capacità di integrazione del modello di utilizzare informazioni provenienti da più fonti. Questo test include una task di risoluzione di coreferenze progettata per valutare la capacità di sfruttare conoscenze disponibili in diverse fonti.</sample>
    <sample id="238">Yebowen Hu from the University of Central Florid presents MeetingBank, a new benchmark dataset for meeting summarization. The dataset addresses the challenges of high-quality summaries and locating trustworthy resources for public meetings. MeetingBank includes City Council meeting transcripts, reference summaries, and URLs. The data collection process involves converting audio to transcripts using Speechmatics API, identifying meeting types and data, locating reference summaries, and aligning timestamps. The dataset contains 1,366 City Council meetings with nearly 7,000 instances. The dataset statistics include the number of meetings, meeting duration, number of speakers, and year period. The dataset also provides summarization instances for each city and average statistics for both meeting-level and secondary levels. The dataset measures the level of abstraction in meeting summaries using coverage and density scores. The dataset is evaluated using extractive and abstractive summarizers, including Oracle, LEAD, LexRank, TextRank, BART-Large, Pagasus, LongFormer, DialogLM, and HMNet. The results show that GPT-3 does not perform well according to automated metrics but achieves the highest overall scores in human evaluation. The findings suggest that meeting summarization solutions should focus on capturing main discussion points and developing new automatic evaluation metrics. The primary contribution of MeetingBank is the creation of a benchmark dataset for meeting summarization. The dataset serves as a useful tool for researchers to design advanced summarizers and provides insights into the decision-making process of City Council meetings.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e darò una breve recensione del lavoro "Prompting PaLM per la Traduzione: Valutazione delle Strategie e delle Prestazioni". Questo è un lavoro congiunto con i miei colleghi da Google Translate. PaLM è un grande modello di linguaggio naturale di 540 miliardi di parametri presentato l'anno scorso, 2022. È addestrato su una vasta collezione di testo, composta da 780 miliardi di token. Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di NLP. In questo lavoro, presentiamo lo studio sistematico più grande sulla promozione del modello di linguaggio naturale per la traduzione. Abbiamo valutato la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità di MT. Questo comporta l'uso dei migliori test set per evitare l'overlap dei dati di test con i dati di addestramento del modello di linguaggio. E abbiamo confrontato con i sistemi più avanzati, quindi il miglior sistema, quindi la valutazione WMT. Abbiamo utilizzato metriche di MT neurali di stato dell'arte e, inoltre, mostriamo i risultati dell'evaluazione basata sull'esperto. Infine, forniamo alcune raccomandazioni per le strategie di promozione. La promozione ha un grande impatto sulle prestazioni dei modelli di linguaggio naturale per la tradizione, come possiamo vedere in un semplice esperimento, dove abbiamo utilizzato la promozione a un colpo e fornito due promemoria diversi per ogni frase. La maggior parte delle frasi, 516 su 1.000. La differenza osservata è di più di un punto BLEURT. E questo può arrivare, in casi estremi, fino a 40 punti BLEURT. Quindi è importante selezionare una buona strategia di promozione. Nelle nostre esperienze, abbiamo scelto una strategia di promozione a cinque colpi, in cui abbiamo semplicemente segnato ogni frase che forniamo al sistema, con la lingua in cui si trova. Qui, ad esempio, dove eseguiamo la traduzione dall'inglese all'inglese, le frasi di origine, le frasi di testo, sono segnate con una virgola in tedesco e le traduzioni in inglese con una virgola in inglese. Abbiamo visto che la forma effettiva della promozione non ha un grande impatto nel caso di diverse promozioni a colpi brevi. È cruciale per le promozioni a un colpo e a zero colpi. Quando andiamo, come nel nostro caso, a una promozione a cinque colpi, c'è quasi nessuna differenza nella forma effettiva della promozione. È gli esempi che portano la maggior parte del peso. La sintesi dei nostri risultati sperimentali è che la qualità degli esempi è più importante di quella della similitudine alla frase di origine. Quindi è importante selezionar gli esempi da traduzioni di alta qualità. In particolare, abbiamo confrontato la selezione di promemoria dai dati di addestramento per le valutazioni WMT sul dev data. Il dev data è molto più curato e di alta qualità rispetto ai dati di addestramento, che sono più rumorosi. I risultati sono migliori quando si utilizzano i dati dev. Tuttavia, i sistemi specializzati di stato dell'arte hanno un vantaggio sostanziale rispetto alle traduzioni di PaLM. Ma, PaLM arriva molto vicino a un sistema commerciale. In questo caso, abbiamo scelto di valutare con Google Translate. Le informazioni che abbiamo ottenuto dall'evaluazione umana che abbiamo condotta utilizzando il framework MQM hanno detto che la fluenzia di PaLM è comparabile ai sistemi di stato dell'arte, ma la differenza principale deriva dalla precisione. Quindi, in particolare, i più comuni errori sono quelli di omissione. Quindi sembra che PaLM scegli di produrre una traduzione più suave, a volte eliminando parti della frase di origine che sono tradotte. Tuttavia, la categoria "Stile/Abbastanza" per PaLM è inferiore rispetto ai sistemi di stato dell'art, il che è un ulteriore segnale che PaLM fornisce una traduzione davvero fluida, ma comunque con alcuni problemi di precisione. E così è. Questo è un vero e proprio breve riassunto. Per ulteriori dettagli, venga alla presentazione completa del lavoro. Grazie mille.</sample>
    <sample id="240">Ciao, sono Dawei, un dottorando a Saarland University in Germania. In questo video, voglio presentare il nostro lavoro recente "Weaker Than You Think: Un'Analisi Critica del Machine Learning a Supervisione Debole". Questo è un lavoro con Xiaoyu Shen, Marius Mosbach e Andreas Stephan, sotto la supervisione di Dietrich Klakow. Voglio iniziare con una breve introduzione al supervisore debole e al machine learning a supervisione debole.

In un supervisore debole, non etichettiamo manualmente i dati. Invece, etichiamo i dati utilizzando fonti di etichettatura debole, come regole semplici, basi di conoscenza o crowd sourcing di bassa qualità, come mostrato nella figura a destra. Quando confrontati con le annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni sono errate. Se addestriamo reti neurali direttamente su dati etichettati deboli, le reti neurali tendono a memorizzare il rumore delle annotazioni e non generalizzano. Nel machine learning a supervisione debole, vengono proposti algoritmi di addestramento per addestrare reti neurali robustamente sotto tale rumore delle annotazioni in modo che i modelli addestrati siano comunque generalizzabili.

Nelle ultime opere in WSL, una dichiarazione comune è che le persone dicono di addestrare modelli su dati etichettati deboli e ottenere prestazioni elevate su test set puliti. Tecnicamente, questa dichiarazione non è sbagliata, ma c'è un problema, che è che le persone assumono che ci sia un set di validazione pulito disponibile per la selezione del modello. Non possiamo fermarci su questo problema, ma questo implica che sono necessari ulteriori annotazioni manuali in WSL. Ma come un elefante nella stanza, questa necessità è spesso trascurata.

L'indagine sopra è stata posta a tre domande di ricerca. Prima, è necessario un set di dati di validazione pulito per WSL o possiamo usare un set di validazione rumoroso invece? Secondo, se è necessario un set di dati pulito, o se è obbligatorio per WSL funzionare, quanti campioni puliti dobbiamo avere? Infine, dovremmo usare solo i campioni puliti per la validazione, o ci sono altre forme migliori di utilizzarli?

Abbiamo affrontato queste domande nella nostra lavorazione e i nostri risultati sono i seguenti. Prima, abbiamo trovato che, affascinantemente, i metodi recenti di WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo delle prestazioni. Come mostrato nella figura, se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre alle etichette deboli, il che significa che l'addestramento è inutile. Questo indica che gli approcci di WSL richiedono effettivmente dati etichettati puliti per funzionare correttamentamente, e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere sottovalutato.

Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuta gli approcci di WSL a ottenere prestazioni migliori, come mostrato nella figura di sinistra. Tipicamente, solo bisogna 20 campioni per classe per ottenere prestazioni elevate. Ma non è tutto, perché se decidiamo comunque di accedere a campioni puliti, allora addestrare direttamente su di essi può ottenere prestazioni ancora migliori. La figura a destra mostra la differenza di prestazioni tra approcci di fine-tuning diretti applicati sui dati puliti e WSL approcci che utilizzano i dati puliti per la validazione solo. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning inizia a superare gli approcci di WSL. Infine, l'incremento delle prestazioni dichiarate negli approcci di WSL può essere fac</sample>
    <sample id="241">Ethan presents a paper titled "Human-in-the-loop Evaluation for Early Misinfomation Detection: A Case Study of COVID-10 Treatments." The paper discusses the limitations of existing automatic misinformation detection systems, which are often evaluated using retrospectively constructed datasets and do not involve human content moderators. The authors propose an evaluation framework that addresses these deficiencies by involving humans at various stages of the process. The framework consists of two main components: the detection of misleading claims and the verification of policy violations. The system takes raw tweets as input and outputs check-worthy claims, which are then ranked by trendiness and provided to humans for verification. The second component uses a BERT-based stance classification model to determine the author's stance towards unapproved treatments, which are then flagged for human review. The authors evaluate the efficacy of the system by operationalizing early detection as the detection of an unapproved</sample>
    <sample id="242">I metodi di valutazione comuni per sistemi di dialogo includono l'uso di valutazioni umane, come chiedere ai giudici di scegliere quale delle due conversazioni è migliore o di dare una valutazione su una scala Likert. Tuttavia, questi metodi lavorano bene per fornire valutazioni holistiche della qualità complessiva della conversazione, ma la qualità della conversazione ha molteplici aspetti. Pertanto, è utile valutare più aspetti della qualità del dialogo per comprendere i punti di forza e le debolezze del modello su un livello più fino. Un approccio comune è chiedere ai giudici di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello utilizzando metodi comparativi o scale Likert. Tuttavia, la nostra approccio cerca di ridurre la soggettività dell'evaluazione umana esplicitando se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso o il suo partner. Chiamiamo questa approccio annotazione dei comportamenti nel chat o ABC-Eval in breve. ABC-Eval è capace di misurare le tasche in cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di volte in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante, contraddice se stesso o il suo partner, hallucina informazioni incorrecte o viola la conoscenza comune, e quando il modello riuscisce o fallisce a mostrare empatia.</sample>
    <sample id="243">Ci sono cinque autori coinvolti nell'articolo: Jenny, Sebastian Santy, Ronan Le Bras, Katja Reinecke e Maarten Sap.</sample>
    <sample id="244">Nell'esempio con Servin e Kei, le conoscenze di base necessarie sono:

1. **Conoscenza dell'occupazione di Servin**: "Servin is a judge."
2. **Conoscenza dell'occupazione dei giudici**: "Judges decide cases in law courts."

Queste conoscenze di base sono generalmente apprese durante il pretraining dei modelli di linguaggio e sono disponibili sia a pretrain time che a inference time.</sample>
    <sample id="245">Title: A Needle in a Haystack: An Analysis of Highly-Agreeing Workers on MTurk for Summarization

Abstract:
This study presents a pipeline for identifying high-agreement Amazon Mechanical Turk (MTurk) workers for summarization tasks, addressing the challenges of automatic metrics and recruitment best practices. The pipeline consists of pre-task qualifications, a qualification task, an endurance task, and a reference-based task. Pre-task qualifications include location, HITs, and HIT Approval Rate. The qualification task tests annotators' ability to evaluate six dimensions, categorizing workers into gold, silver, bronze, and block. The endurance task assesses workers' capacity for handling heavy workloads, with gold and silver workers achieving high agreement in terms of Inter-Annotator Agreement (IAA). The reference-based task evaluates general performance on the true annotation task, with gold and silver workers showing a Krippendorff's Alpha of 0.534.

We also compare our pipeline workers with Baseline MTurk workers using the MACE statistical filter and CloudResearch MTurk workers. Our pipeline achieves a Krippendorff's Alpha of 51.3%, while Baseline MTurk workers have a lower Alpha of 38.0%. The pipeline's results show that 4 gold and 8 silver workers (6% of 200 participants) can achieve high agreement at a lower cost than CloudResearch.

Our analysis of correctness across annotation sources reveals a significant Spearman's correlation between Pipeline and CloudResearch workers, but Pipeline may not guarantee training correctness. Real GPT models correlate well with expert judgments.

In conclusion, our pipeline serves as a best practice for high-agreement annotations at scale and lower cost, avoiding resource waste on discarded annotations. Future work will focus on hiring high-quality workers and exploring multiple applications for tasks, languages, and platforms, while addressing the limitations of the current study.</sample>
    <sample id="246">Sì, il codice è disponibile. Puoi trovarlo su GitHub.</sample>
    <sample id="247">Title: FACTKG: Fact Verification via Reasoning on Knowledge Graph

Abstract:
We introduce FACTKG, a novel dataset for fact verification using knowledge graphs as evidence. Unlike existing datasets that rely on text or tables, FACTKG leverages the intuitive and connected nature of knowledge graphs for reliable reasoning. The dataset, based on DBpedia, includes claims in both written and colloquial styles, with two labels: SUPPORTED and REFUTED, and features five types of reasoning: one-hop, conjunction (multiple one-hop claims), existence, multi-hop, and negation.

To verify claims, participants must retrieve evidence from DBpedia and apply the appropriate reasoning type. For instance, one-hop claims require checking a single triple, while conjunction claims necessitate verifying multiple one-hop claims. Existence claims involve checking a specific relation, and multi-hop claims require finding paths between entities. Negation claims demand an additional inference step.

FACTKG also includes methods for handling colloquial style claims, such as style transfer and presupposition templates. The dataset's statistics and baselines demonstrate its utility, with the GEAR model outperforming all other baselines.

In summary, FACTKG provides a valuable resource for fact verification using knowledge graphs, offering a reliable and practical approach for tasks requiring consistency checks between natural language and knowledge graphs.</sample>
    <sample id="248">Sì, gli annotatori per NLPositionality sono bilanziati rispetto a ciascun Gruppo Demografico, ad esempio Paese e genere. Questo è stato fatto per ottenere un set di dati annotato ricco e diversificato, che permette di effettuare confronti più accurati tra le annotazioni e i modelli e i dataset.</sample>
    <sample id="249">Le frasi nel dominio accettabili sono state perturbate aggiungendo rumore a una struttura sintattica e semantica simile, mantenendo la coerenza con la frase originale. Questo è stato fatto per testare la sensibilità del modello ai dettagli sintattici e semantici condivisi tra le frasi.</sample>
    <sample id="250">Avere una valutazione dimensionale significa misurare e analizzare diverse aspetti o caratteristiche di qualcosa, in questo caso, delle conversazioni con chatbot. Questo approccio permette di avere una comprensione più dettagliata e precisa delle qualità del chatbot, piuttosto che una valutazione generale basata su esperti umani. Inoltre, consente di identificare le aree in cui il chatbot è forte o debole, e di migliorare il suo comportamento in base a queste informazioni.</sample>
    <sample id="251">The author of the article is Jingwei Yi from the University of Science of China.</sample>
    <sample id="252">Title: U-CREAT: Unsupervised Case Retrieval using Event Extraction

Abstract:

Legal professionals face the challenge of retrieving relevant past precedents, known as cited documents, from an increasing volume of cases. This work presents U-CREAT, a novel approach for Prior Case Retrieval (PCR) that leverages unsupervised learning techniques and an event-based approach. U-CREAT introduces the Indian Legal Prior Case Retrieval Dataset (IL-PCR), a comprehensive benchmark for PCR tasks, and the U-CREAT pipeline, which demonstrates high retrieval efficiency, low inference time,</sample>
    <sample id="253">DisorBERT is a double domain adaptation model designed to detect signs of mental disorders in social media posts. Mental disorders are psychological syndromes associated with distress and disability that affect thinking, feeling, mood, and behavior. Social media content provides an opportunity to research how people experience difficulties, with many individuals sharing their daily routines and mental health issues online. DisorBERT aims to support the detection of mental health disorders by analyzing social media posts.

Domain adaptation is used to improve the performance of a model on a specific target domain when there is insufficient annotated data. DisorBERT adapts the general language model BERT to the language of Reddit and mental health, adjusting its vocabulary and learning domain-specific tasks. The model is guided by a lexicon to focus on important words during training.

The results show that DisorBERT has a good balance between precision and recall, outperforming other methods. The model tends to focus on words related to mental disorders, such as "anxious" and "medication," which are relevant to depression.

In conclusion, the combined effect of double domain adaptation and guide masking is effective at capturing signs of mental disorders on social media. DisorBERT achieved better results than MentalBERT, a model trained with a large dataset. Future work includes exploring different lexical resources and using clinical data.</sample>
    <sample id="254">Title: Uncertainty Guided Label Denoising for Document-level Relation Extraction

Abstract:
Document-level relation extraction aims to identify relationships among entities within a document. Traditional methods rely on large-scale human-annotated corpora, but recent approaches leverage distantly supervised data to enhance performance. However, these data often contain noise, leading to false-positive pseudo labels that can introduce errors. This paper introduces a document-level relation extraction framework with uncertainty-guided label denoising, addressing the noise problem in distant supervision.

We first train a pre-denoising DocRE (Document-level Relation Extraction) model using both distant supervision (DS) and human-annotated data to generate pseudo labels. To mitigate the risk of noise, we employ uncertainty estimation to assess the trustworthiness of model predictions. For overlapping relations, we propose an instance-level uncertainty estimation method. Additionally, we design a re-labeling strategy with dynamic class uncertainty threshold and a multi-phase training strategy to further improve performance.

To model uncertainty in the pre-denoising DocRE model, we use Monte Carlo dropout, which requires multiple stochastic forward-pass predictions with activated dropout. We modify the uncertainty estimation process to obtain instance-level uncertainty scores for each positive pseudo label, addressing the issue of distinguishing between false positive and correct positive labels. We observe that frequent classes have lower average uncertainty, so we introduce dynamic class uncertainty thresholds to filter out high-uncertainty pseudo labels.

We also design a multi-phase training strategy to iteratively relabel DS data, leveraging the improved pseudo labels for better performance. Our framework outperforms previous baselines on public datasets, demonstrating significant improvements in label quality and overall performance.

In summary, our main contributions are: (1) a framework with uncertainty-guided label denoising to enhance DS data quality, (2) an instance-level uncertainty estimation method for overlapping relations, (3) a dynamic class uncertainty threshold for the long-tail problem, and (4) substantial performance improvements.</sample>
    <sample id="255">La forma del prompting si rivela importante in casi specifici, come quando si utilizza il prompting zero o uno. In questi casi, la selezione delle promemoria (prompt) ha un impatto significativo sulle performance del modello. Tuttavia, quando si passa a un prompting più complesso, come il prompting a cinque esempi, la forma delle promemorie non ha un impatto maggiore rispetto all'importanza degli esempi stessi. In sintesi, la qualità degli esempi è più importante che la similitudine alla frase di origine.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo state-of-the-art.</sample>
    <sample id="258">This video introduces a novel approach to evaluating the quality of text in natural language processing using large language models. The authors propose using natural language instructions to instruct the large language models, which can then provide ratings for the samples. The motivation behind this work is to find an alternative to human evaluation, which is unstable and hard to reproduce. The authors conducted an experiment using large language model evaluation to rate stories generated by GPT-2 or written by humans, using four attributes: grammar, coherence, likability and relevance. The results showed that some large language models, such as Davinci and ChatGPT, can be used as an alternative to human evaluation. The authors also discuss the benefits and costs of using large language model evaluation compared to human evaluation, and the results of large language model evaluation on other</sample>
    <sample id="259">Title: XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations

Abstract:

Semantic parsing is the task of converting natural language queries into structured representations, such as SQL or Lambda Calculus. Cross-lingual semantic parsing aims to translate queries across multiple languages into these representations. However, existing models are often limited in their language coverage and evaluation settings. To address this, we introduce XSemPLR, a comprehensive benchmark for cross-lingual semantic parsing that includes 9 datasets, 5 tasks, 8 meaning representations, and 15 languages.

We evaluate our benchmark using six settings: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer. Our results show that Encoder-Decoder models outperform Encoder-PTR models across all datasets. We also find that training in a mixture of languages can improve performance, but English performance drops in some cases.

Our findings suggest that while multilingual language models like Codex and BLOOM are not yet adequate for cross-lingual semantic parsing, our benchmark provides valuable insights for future research. We hope that XSemPLR will serve as a useful resource for the community and encourage further exploration of cross-lingual semantic parsing.</sample>
    <sample id="260">L'articolo è coinvolti due autori: Jingwei Yi e un altro autore non menzionato nel testo fornito.</sample>
    <sample id="261">Un buon pianificatore dovrebbe essere ragionevole e fedele alle restrizioni imposte dai goal specifici. Questo significa che il pianificatore deve essere in grado di generare script che siano sia completi semanticamente che rispettino accuratamente le restrizioni.</sample>
    <sample id="262">L'articolo "Distilling Script Knowledge from Large Language Models" è stato scritto da un solo autore, Siyu Yuan, dall'Università di Fudan.</sample>
    <sample id="263">In this work, we address the issue of label biases in in-context learning, a popular paradigm for utilizing large language models. We identify a new type of bias, domain-label bias, which arises from the task corpus and affects the model's predictions. We propose a novel calibration method, domain-context calibration, to handle all types of biases in a holistic fashion. Our method uses random in-domain words sampled from the task Corpus as content-free text to estimate the model's bias and calibrate the original predictions. We conduct extensive experiments on a wide range of datasets and models, and observe significant improvements in the average performance of in-context learning. Our findings show that domain-context calibration is more effective than previous calibration attempts, and it can significantly improve the performance of large language models.</sample>
    <sample id="264">Title: Transferable Audio-Visual Text Generation (TAVT)

Abstract:

Multimodal text generation tasks, such as audio-visual text generation, face significant challenges due to the complexity of aligning different modalities and the scarcity of labeled data. Existing approaches often suffer from performance degradation when dealing with domain shifts in visual and audio content. To address these issues, we propose Transferable Audio-Visual Text Generation (Tavt), a novel task that aims to learn a unified audio semantic space to align visual concepts across domains. Our framework consists of three components: an audio-visual meta mapper network, an audio-visual encoder and language model, and counterfactual contrastive learning.

The audio-visual meta mapper network maps visual concepts across domains into a unified auditory semantic</sample>
    <sample id="265">Il nome della relatrice o del relatore non è specificato nel contenuto fornito.</sample>
    <sample id="266">L'articolo è stato scritto da Adam Przepiórkowski.</sample>
    <sample id="268">Gli errori più comuni di PaLM, secondo la domanda, sono le omissioni. Questo significa che PaLM tende a escludere parti del testo di origine durante la traduzione, creando traduzioni che, pur essere fluide, possono mancare di alcune informazioni o dettagli. Questo è stato evidenziato dalle valutazioni umane eseguite con il framework MQM, che hanno indicato che, sebbene la fluidezza sia comparabile a quella dei sistemi più avanzati, le errore di omissione sono la principale problematica.</sample>
    <sample id="269">Ciao, sono James Finch e Sarah Finch. E oggi parleremo di ABC-Eval, una nuova approccio dimensionale per valutare l'intelligenza artificiale conversazionale. Questo lavoro è stato realizzato dalla Lab di NLP di Emory, guidata dal professore Jinho Choi all'Università di Emory, in collaborazione con Amazon Alexa AI. Immaginiamo che abbiate sviluppato un modello di dialogo e vogliate vedere come si confronta con lo stato dell'arte. La pratica comune è utilizzare l'evaluazione umana, come chiedere ai giudici umani di scegliere quale delle due conversazioni è migliore o di dare una valutazione su scala Likert. Questi approcci funzionano bene per fornire valutazioni holistiche della qualità del dialogo, ma la qualità del dialogo ha molteplici aspetti. Quindi potreste voler valutare più dimensioni del dialogo per comprendere i punti di forza e le debolezze del modello a un livello più fino. Un approccio è semplicemente chiedere ai giudici umani che valutino diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello utilizzando metodi esistenti comparative o scala Likert. Tuttavia, crediamo che ci sia una strategia più precisa e affidabile per l'evaluazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività dell'evaluazione umana esplicitando se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso o il suo partner. Chiamiamo questa approccio annotazione dei comportamenti nel chat o ABC-Eval in breve. Siamo stati in grado di sviluppare questo metodo per coprire completamente i comportamenti del modello di chat che sono stati suggeriti come influenzare la qualità del dialogo nella letteratura recente. ABC-Eval è capace di misurare le tasche in cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante, contraddice se stesso o il suo partner, hallucina informazioni incorrecte o viola la conoscenza comune, e quando il modello riuscisce o fallisce a mostrare empatia. Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat di stato dell'arte e li abbiamo valutati su 100 conversazioni umano-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronto di dialoghi a livello di coppie. Per ciascun metodo esistente, abbiamo raccolto valutazioni su otto degli aspetti più comuni misurati della qualità del dialogo, poiché è la pratica standard per valutare i modelli di chat lungo più dimensioni. Dalla nostra analisi dei risultati di queste valutazioni, abbiamo trovato che i label comportamentali di ABC-Eval sono in generale più affidabili che i label raccolti da metodi esistenti, misurati dall'accordo interannotatori su 100 conversazioni doppialmente etichettate. Inoltre, i label di ABC-Eval sono più predittivi della qualità complessiva del dialogo rispetto ai metriche prodotte da metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, puoi vedere come misurare la proporzione di turni con contraddizioni di se stesso e del partner spiega il 5% e il 10% della qualità del dialogo, rispettivamente, mentre le medie di coerenza Likert spiegano solo il 4% o meno. Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità del dialogo utilizzando un'analisi di regressione lineare passo-passo. Puoi vedere come la combinazione di tutti i metrici ABC-Eval spiega oltre il 25% della qualità del dialogo, e quando si rimuovono i metrici uno alla volta, la</sample>
    <sample id="270">L'articolo è stato scritto dall'Emory NLP Lab, guidata dal Professor Jinho Choi all'Università di Emory, in collaborazione con Amazon Alexa AI.</sample>
    <sample id="271">In questo articolo, CFT sta per "Continuous Fine-Tuning". Questo è un approccio di apprendimento dove il modello viene continuamente raffinato utilizzando dati di validazione puliti. Questo metodo ha mostrato di migliorare significativamente le prestazioni delle tecniche di apprendimento debolemente supervisionato, rendendole più pratiche e efficaci.</sample>
    <sample id="272">Ci sono sette autori coinvolti nell'articolo: Koustav Sinha, John Gauthier, Aaron Mueller, Kishka Misra, Karen Fences, Roger Lévy, e Adina Williams.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e sarò qui a presentare il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un'esplorazione datadriven, multilingue". Questo lavoro è stato fatto in collaborazione con Patrick Fernandes, Emmy Liu, André Fernandes Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurrebbe "mole" in questa frase? Se la frase precedente fosse "Le cose potrebbero iniziare a diventare pericolose se i ministri scoprono", allora "mole" si riferisce a unlo spion. Ma se la frase precedente fosse "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisse a un'embolia. Quindi, a seconda del contesto, il significato della parola cambia e quindi la traduzione cambia anche. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche di corpus come BLEU incapaci di catturare queste traduzioni. E alcuni hanno suggerito di valutare in modo mirato le traduzioni dipendenti dal contesto, ma questi risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati poiché dipendono generalmente dalla conoscenza del dominio e dalla cura umana. In questo lavoro, cerciamo di rispondere a queste due domande. Prima, quando la traduzione richiede contesti? E secondo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato a misurare quanto un'unità parola dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come una misura per l'uso del contesto da parte dei modelli di traduzione. Questo viene fatto misurando quanto informazione il contesto C fornisce sull'unità target Y, data l'unità sorgente X. Puoi immaginare CXMI come l'informazione ottenuta dall'offrire contesto al modello. In questo lavoro, estendiamo CXMI a CXMI punto per misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare che le parole con alto P-CXMI siano quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con alto P-CXMI per cercare pattern tra queste parole. E eseguiamo l'analisi su trascrizioni di conferenze TED che sono state tradotte dall'inglese in 14 lingue diverse. E eseguiamo l'analisi a tre livelli diversi. Prima, guardiamo alle etichette part-of-speech che hanno un valore medio alto P-CXMI. Questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un valore medio alta P-CXMI relativamente. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi hai bisogno di contesto per determinare se un pronome è duale quando si traduce in arabo. E similmente, troviamo che certe lingue richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Poi guardiamo alle parole che hanno un alto P-CXMI medio su tutte le sue occorrenze diverse. Questo ci aiuta a identificare casi</sample>
    <sample id="274">Il nome del relatore è Yusen Zhang.</sample>
    <sample id="276">Ananya and Vignesh present their work on "IndicMT Eval: A dataset to meta-evaluate machine translation metrics for Indian languages." They focus on evaluating translations from Indian languages to English, addressing the gap in research on evaluation metrics for languages other than English. They use the Flores dataset to select 200 sentences in five Indian languages (Tamil, Malayalam, Hindi, Marathi, and Gujarati) and generate multiple candidate translations using seven different translation models. Human annotators evaluate the outputs, marking errors by type and severity, and providing an overall score. The study compares various metrics, including overlap-based, embedding-based, and COMET variants, and finds that COMET-metric variants have the highest overall correlation with human scores. They fine-tune the best-performing metric, COMET, using their MQM dataset, resulting in improved correlation values. The study also evaluates the robustness of IndicCOMET MQM on unseen languages and the ACES Translation Accuracy Challenge Sets, showing better performance than COMET baselines. The authors make their dataset publicly available for further research.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico fornito nel testo.</sample>
    <sample id="278">Il metodo delle "parole contrassegnate" (Marked Words) è stato sviluppato per identificare le parole che distingue gruppi marcati dai gruppi non marcati. Questo metodo si basa sulla nozione sociolinguistica di "marcheggiatura", che sostiene che ci sono gruppi dominanti che sono linguisticamente e socialmente non marcati, mentre i gruppi marginalizzati sono generalmente marcati. Il metodo utilizza il Fightin' Words, che sono metodi per calcolare i rapporti di log-odds ponderati, per distinguere le parole più frequenti per ciascun gruppo marcato. Questo permette di rilevare specifiche stereotipi e pattern senza dover dipendere da un'ulteriore lexicon.</sample>
    <sample id="279">The authors of the article are affiliated with the University of Washington.</sample>
    <sample id="280">Emotion regulation in conversations involves predicting the emotion label of each utterance, which contains textual, audio, and visual modalities. Despite numerous methods, challenges remain in exploiting multimodal information, addressing minority and semantically similar emotions, and distinguishing between them. This paper introduces MultiEMO, a novel attention-based correlation-aware multimodal fusion (MMF) framework. MultiEMO consists of unimodal feature extraction, context modeling, multimodal fusion (MultiAttn), and emotion classification. Key contributions include VisExtNet, a visual feature extractor that captures facial expressions without redundant scene information; MultiAttn, a multimodal fusion network using bidirectional multi-head cross-attention; and Sample-Weighted Focal Contrastive Loss, addressing minority and semantically similar emotions. Experiments on MELD and IEMOCAP datasets show state-of-the-art performance, particularly in minority and semantically similar emotions. However, limitations include VisExtNet's inability to distinguish speakers and the need for a large batch size for SWFC loss.</sample>
    <sample id="281">Kayo Yin and colleagues present a study on the role of context in translation, focusing on how different languages handle context-dependent translations. They introduce CXMI, a measure of context usage by machine translation models, and extend it to Pointwise CXMI for sentence and word-level analysis. By analyzing TED talk transcripts translated into 14 languages, they identify patterns in words with high P-CXMI, such as dual pronouns in Arabic and proper nouns in Chinese. They also examine vocabulary items and individual tokens with high P-CXMI, revealing phenomena like ellipses resolution.

The researchers design a benchmark, the Multilingual Discourse-Aware (MuDA) tagger, to automatically identify context-dependent examples in parallel corpora. They evaluate different models using this benchmark and find that context-aware models perform better for certain discourse phenomena like formality and lexical cohesion, but not as well for others like ellipsis, pronouns, and verb form</sample>
    <sample id="282">Hello everyone, I'm Xuekai Zhu, and today, I'm excited to present our new work at the ACL 2023 conference, titled "StoryTrans: Non-Parallel Story Style Transfer with Discourse Representations and Content Enhancers." This research addresses the important task of non-parallel text style transfer, focusing on the story level and discourse level, which is crucial for imitating the author's style. Our main challenge lies in capturing the complex linguistic preferences of authors, such as narrative techniques and discourse structures, and transferring these at the story level.

To tackle this, we propose a novel generation model called StoryTrans. StoryTrans learns discourse representations from source texts and combines them with learnable style embeddings to generate texts in target styles. We designed a new training objective to reduce stylistic features from discourse representations, pulling them closer in the latent space, and enhance content preservation by separating the generation into two stages. The first stage involves transferring the source text with style-specific content masked, and the second stage fills in the correct style-specific contents.

Our training framework includes an advisory training framework for the first stage, using self-reconstruction loss, disentanglement loss, sentence order loss, and style classifier loss. The second stage focuses on filling in the correct style-specific contents and removing the mask token.

We collected new datasets in Chinese and English for these tasks and conducted extensive experiments to transfer fairytales or everyday stories to typical author's styles. Both automatic and manual evaluations confirm the efficiency of our model, showing that StoryTrans outperforms strong baselines in style control and content preservation. Style visualization indicates that the transfer test by StoryTrans aligns with the golden text in the style feature space, and StoryTrans can rewrite most sentences with the target styles while maintaining the source semantics.

In summary, StoryTrans is a significant advancement in non-parallel text style transfer, addressing the challenges of capturing author's linguistic preferences and transferring them at the story level. Our model demonstrates superior performance in style control and content preservation, and we are excited to share our work with the community. If you have any questions, please feel free to contact me.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è quella che include il nome della citta "Lisa, Bart, and Maggie". Questa struttura assume che tutti i congiunti siano ugualmente importanti e non venga selezionata una congiunta come "capo" della struttura di coordinazione.</sample>
    <sample id="284">In this paper, we introduce FSUIE, a novel fuzzy span mechanism for enhancing universal information extraction (UIE). Traditional UIE models rely heavily on precise span boundaries, which can be ambiguous and lead to inconsistent annotations. To address this, we propose a fuzzy span mechanism where the span boundaries are learned as continuous distributions rather than precise points. This approach allows for more flexibility in modeling the target boundaries.

We also address the mismatch between transformer feature extraction and information extraction by introducing adaptive attention mechanisms. The attention used for span extraction decision is modeled as a continuous distribution, which is then converted into discrete values for loss calculation. This is achieved through a sampling function that calculates the Binary Cross Entropy loss between the predicted and golden boundaries, along with KL-divergence.

To further improve the model's attention distribution, we propose a fuzzy span attention mask function. This function dynamically adjusts the length of the full attention range and linearly decays the attention distribution at the attention span boundaries. The fuzzy span attention layer is added at the top level of the model to guide the decision process without affecting the text encoding capability.

We evaluate FSUIE on three main information extraction tasks: named entity recognition (NER), relationship extraction, and aspect sentiment triplet extraction. On NER, FSUIE achieves significant performance improvement compared to UIE-base without the fuzzy span mechanism, especially on small-scale data. On relationship extraction, FSUIE achieves new state-of-the-art results on datasets ACE2004, ACE2005, and ADE. It also demonstrates strong generalization capabilities for domain-specific information. On aspect sentiment triplet extraction (ASTE), FSUIE achieves state-of-the-art results on 14lap, 15lap, and 16lap datasets, and competitive performance on 14lap datasets.

Ablation studies show that the fuzzy span loss and fuzzy span attention both contribute to the model's performance. The fuzzy span loss alleviates the model's reliance on span boundaries and the fuzzy span attention adapts the attention span of the model. Overall, FSUIE achieves excellent results in a wide range of UIE tasks.</sample>
    <sample id="285">In this video, Mingqi Gao from Peking University presents their work on "Reference Matters: Benchmarking Factual Error correction for Dialogue Summarization with Fine-grained Evaluation framework." The video discusses the two main approaches to addressing factual errors in dialogue summarization: introducing factuality-related objectives in the training or inference processes and designing a Factual Error Correction (FEC) model. Gao highlights the limitations of current FEC evaluation methods, which rely on factuality metrics like FactCC and DAE, and argues that these metrics are too vague and may not accurately reflect the performance of FEC models. To address these issues, Gao proposes introducing manually annotated reference corrections to provide more valuable data for training FEC models and to create a more comprehensive and accurate evaluation framework. The proposed evaluation framework consists of three steps: alignment, classification, comparison, and is based on the ERRANT evaluation metric for grammar error correction. Gao's experiments show that training FEC models with reference summaries from dialogue summarizations datasets yields the best results, and that introducing human-corrected summaries during training can improve their performance. The video concludes with the key findings of the study, including the need to change the evaluation methods for FEC and the potential of combining human-annotated data with synthetic data.</sample>
    <sample id="286">Il nome del relatore è James Finch.</sample>
    <sample id="287">Quattro autori sono coinvolti nell’articolo: Javad Hosseini, Filip Radlinski, Silvia Paret, e Annie Louis.</sample>
    <sample id="288">I dati che possono essere utilizzati per il test delle fenomeni sintattici includono:

1. **Adjunct Island** (dall'insieme BLiMP)
2. **SyntaxGym**
3. **CrowS pairs** (per valutazioni basate su stereotipi)
4. **Wikipedia** (per testare l'affidabilità del modello in contesti completamente irrilevanti)

Questi insiemi di dati permettono di esaminare come i modelli di linguaggio reagiscono a vari contesti sintattici e semantici, sia all'interno dello stesso dominio che in contesti diversi.</sample>
    <sample id="290">Le abbreviazioni dei cinque metodi per il primo problema di ricerca sono:

1. WSL (Weakly Supervised Learning)
2. COSINE (Cosine Similarity-based method)
3. FTw (Fine-Tuning with clean validation samples)
4. WSL+FTw (WSL with fine-tuning on clean validation samples)
5. WSL+FTw+ (WSL with fine-tuning on clean validation and additional data)</sample>
    <sample id="291">Il modello viene valutato su diverse attività di processing del linguaggio naturale (NLP) in ambiti biomedici e clinici, come la riconoscimento di entità nomate (NER), la classificazione, il tagging part-of-speech e la risposta alle domande (QA).</sample>
    <sample id="294">CamemBERT è inizialmente addestrato su un dataset di 138 GB chiamato OSCAR.</sample>
    <sample id="295">Il nome del relatore è Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presents a collaborative work between the University of Turin and Amazon Alexa, focusing on Natural Language Understanding (NLU) and Natural Language Processing (NLP). The project aims to develop more informative models for detecting irony, a complex and pragmatic phenomenon in language. Traditional approaches rely on supervised machine learning with large datasets of manually annotated data, but these methods have limitations. The EPIC corpus, an English Perspectivist Irony Corpus, was created by collecting data from social media, Reddit, and Twitter over 1.5 years, resulting in 300 short conversations. The data spans five varieties of English, and 74 annotators were used to label the conversations, with an average of 5 annotations per conversation.

The study found that annotator groups differ in their perception of irony, influenced by factors such as gender, age, nationality, and geographical location. To address these differences, perspective-aware models were developed by fine-tuning pre-trained language models on splits of the dataset based on annotator characteristics. These models showed significantly higher confidence in their predictions compared to gold standard aggregated models.

The research also revealed that generations close to each other tend to disagree on the perception of irony, and similar variations occur in geographical distribution. The findings suggest that considering annotator perspectives can improve the accuracy and confidence of irony detection models. The EPIC corpus and perspective-aware models offer valuable insights for advancing NLU and NLP research.</sample>
    <sample id="297">The project "From Dogwhistles to Bullhorns" explores the use of coded rhetoric, or dogwhistles, in political speech. These terms send one message to an outgroup and a different, often taboo or inflammatory, message to an in-group. The project develops a typology and glossary of over 340 dogwhistle terms, categorizing them by register, type, and persona. The authors conduct a case study of historical U.S. political speech, finding that the frequency of racial dogwhistles closely matches the Republican Southern Strategy. They also evaluate the ability of language models, specifically GPT-3, to recognize dogwhistles, finding that performance varies depending on the prompt and the type of dogwhistle. Finally, they show how dogwhistles can evade content moderation online. Overall, the project provides a comprehensive analysis of the use of dogwhistles in political speech and their impact on content moderation.</sample>
    <sample id="298">La deriva temporale è stata identificata come la causa principale della perdita delle prestazioni grazie all'esperimento di continuare a pre-addestrare alcuni modelli con dati più recenti. Si è osservato che il performance degrada con un maggior gap temporale tra i dati di addestramento e di test, confermando l'ipotesi che la perdita delle prestazioni è dovuta alla deriva temporale.</sample>
    <sample id="299">This work presents a novel training method to improve the robustness of Natural Language Inference (NLI) models against shortcuts, which are spurious correlations between input attributes and labels. The method, called minimax training, involves training a learner model and an auxiliary model in an alternating fashion, with the auxiliary model generating example weights to incentivize the learner to focus on under-represented hard examples. This approach helps the learner to learn from a wider range of input space, reducing its reliance on shortcuts and improving its out-of-distribution performance. The method does not require any assumptions about the type of shortcuts in the dataset and relies on the learner's own training dynamics to obtain example weights. The authors evaluate the method on three commonly used analytic datasets and corresponding out-of-distribution adversarial test sets, and observe consistent improvements in out-of-distribution performance while maintaining high in-domain accuracy. They also examine the effect of pre-training the learner, the size of the auxiliary, and conduct a qualitative evaluation of the learned example weight distributions.</sample>
    <sample id="300">Interactive dictation is a task where users can dictate and edit a document using their voice in a natural and intuitive manner. The task involves flexible interleaving of dictation and editing, using intuitive and open-ended natural language utter</sample>
    <sample id="302">Perché i token nell'input non sono ordinati e devono essere messi in una sequenza logica per corrispondere correttamente all'output. La permutazione aiuta a mantenere la coerenza e la corrispondenza tra input e output.</sample>
    <sample id="303">Gli autori hanno suggerito ai proprietai dei modelli di aumentare la trasparente sui metodi di mitigazione dei biases perché non è chiaro se questi metodi stanno causando positivi stereotipi o altre forme di pericolose narrazioni. Senza maggiore trasparenza, è difficile studiare e comprendere meglio questi effetti e assicurare che i modelli siano utilizzati in modo responsabile e equo.</sample>
    <sample id="304">Gli input inaccettabili di coppie minima sono frasi grammaticalmente sbagliate o che non rispettano le regole linguistiche specifiche del contesto. Questi input vengono utilizzati per valutare la robustezza delle giudizi di accettabilità dei modelli linguistici, confrontando frasi accettabili e inaccettabili.</sample>
    <sample id="305">In this video, Dawei presents their recent work on "Weaker Than You Think: A Critical View of Weakly Supervised Learning." Weakly supervised learning involves training neural networks on data labeled with weak sources, such as heuristic rules or crowdsourcing, which are cheaper but noisy. The challenge is that neural networks trained on such data tend to memorize label noise and fail to generalize. Dawei and their team address this by proposing methods to robustly train neural networks under label noise, ensuring good generalization.

The video raises three key research questions: whether clean validation data is necessary for weakly supervised learning (WSL), how many clean samples are required, and whether only clean samples should be used for validation. Their findings show that clean validation data is indeed necessary for WSL to perform well, and that increasing the number of clean samples improves performance. They also demonstrate that continuing to fine-tune on clean validation samples can achieve performance comparable to more complex WSL methods.

The video concludes with recommendations for future WSL research, including reporting model selection criteria, comparing WSL with few-shot learning baselines, and considering continuous fine-tuning as a strong baseline. The team has open-sourced their code, which can be accessed via a QR code provided in the video.</sample>
    <sample id="306">Title: Entity Tracking in Language Models: An Evaluation Task and Findings

Abstract:

Understanding discourse and tracking entity states is crucial for agents to comprehend longer conversations. However, there has been limited systematic investigation into the entity tracking abilities of pre-trained language models. This paper addresses the research question of how well large language models can track entities in discourse. We designed an evaluation task involving boxes and objects, where the input consists of initial descriptions of box contents, and the task is to predict the contents after state-changing operations. We tested the task with Flan-T5 and GPT-3 models using 2-shot in-context learning.

Our results show that most models simply repeat the initial state due to heuristics or memorization. However, text-davinci-003 exhibits nontrivial tracking, indicating that pre-training on code is responsible for this capacity. Smaller models like T5-base can learn entity tracking through fine-tuning, but randomly initialized models of the same architecture cannot.

We also found that pre-training on code is crucial for the observed state tracking abilities. However, it remains unclear whether these abilities generalize beyond our setup. Further research is needed to explore the potential of pre-trained language models for entity tracking in discourse.</sample>
    <sample id="307">Gli autori hanno utilizzato diverse metriche di valutazione per esaminare le prestazioni dei loro modelli. Le principali metriche utilizzate includono:

1. **Named Entity Recognition (NER)**: Misurare l'accuratezza nella identificazione e classificazione delle entità riconoscibili nel testo.
2. **Classification**: Valutare la capacità del modello di classificare testi in diverse categorie.
3. **Part-of-Speech Tagging (POS)**: Analizzare la corretta assegnazione delle etichette grammaticali ai termini del testo.
4. **Question Answering (QA)**: Misurare la capacità del modello di rispondere a domande basate sui testi forniti.

Inoltre, gli autori hanno confrontato i loro modelli con sei modelli di baseline (CamemBERT OSCAR 138 GB e 4 GB, CamemBERT CCNET, PubMedBERT, BioBERT, e ClinicalBERT) per valutare la loro performance in termini di precisione, ricallabilità e generalità.</sample>
    <sample id="308">This presentation by Jenny, a first-year PhD student at Carnegie Mellon University, discusses the concept of positionality in Natural Language Processing (NLP) and its impact on datasets and models. Positionality refers to the perspectives that people hold as a result of demographics, identity, and life experiences, and can influence research outcomes. Jenny's work, in collaboration with researchers from the University of Washington and the Allen Institute for Artificial Intelligence, aims to characterize the positionality of datasets and models by comparing annotations with real users.

The NLPositionality framework, developed by Jenny and her team, involves re-annotating datasets with diverse annotators and comparing the annotations to models and datasets using Pearson's R correlation score. The study found that datasets and models are most aligned with English-speaking countries and people with a college education, but less aligned with non-binary individuals.

Jenny's recommendations for addressing positionality in NLP include keeping a record of design choices, conducting research with a perspectivist lens, and building specialized datasets and models within specific communities. The presentation concludes with a call to action for inclusive NLP and encourages further exploration of the topic through the NLPositionality dashboard and paper.</sample>
    <sample id="309">L'accordo tra annotatori è stato misurato utilizzando l'inter-annotator agreement su 100 conversazioni doppialmente etichettate.</sample>
    <sample id="310">Il dominio scelto per aggiungere frase completamente scollegate alle query inaccette e accettabili è Wikipedia. Questo viene utilizzato per testare se le giudizi di accettabilità del modello sono influenzati da contesti completamente irrilevanti rispetto alla frase che stiamo valutando.</sample>
    <sample id="311">I miei migliori,

Le affiliazioni degli autori dell’articolo sono:

1. Regina Stodden
2. Omar

Spero che queste informazioni siano utili!</sample>
    <sample id="312">MultiInstruct è un dataset di riferimento multi-modale per l'addestramento di modelli pre-addestrati su un vocabolario unificato per testo, immagini e coordinate di bounding box. È il primo dataset di riferimento multi-modale per la tuning di istruzioni, che include 62 diverse attività multi-modali che coprono 10 categorie broad. Inoltre, MultiInstruct include un dataset di istruzioni per il test su attività multi-modali e un dataset di istruzioni naturali per il test su attività NLP. Il dataset di istruzioni naturali è stato utilizzato per il fine-tuning del modello pre-addestrato OFA, mentre il dataset di istruzioni multi-modale è stato utilizzato per il fine-tunning del modello. Il dataset di istruzioni multi-modale include 53 attività per il training e 10 attività per il test, mentre il dataset di istruzione naturali include 20 attività per il test. Il dataset di istruzioni multi-modal include 5 istruzioni per ogni attività, mentre il dataset di istruzione naturale include 5 istruzioni per oogni attività. Il dataset di istruzioni multi-modello include 5 istruzioni per oggi attività, mentre il dataset naturali include 5 istruzioni per ooggi attività. Il dataset di istruzione multi-modello include 5 istruzione per oggi attività, ment</sample>
    <sample id="313">Due autori sono coinvolti nell’articolo: James Finch e Sarah Finch.</sample>
    <sample id="314">La coordinazione binaria si riferisce alla struttura in cui due elementi (congiunti) sono legati tra loro in modo simmetrico, senza un elemento predominante o "capo" rispetto all'altro. In altre parole, entrambi gli elementi sono considerati ugualmente importanti e possono essere posizionati in qualsiasi ordine senza alterare il significato complessivo della frase.</sample>
    <sample id="315">In questo studio, i prompt sono stati utilizzati per generare le persone rappresentate (personas) e per identificare le parole che distingue le gruppi marcati. Questo processo è stato ripetuto per diverse persone rappresentate, come ad esempio "Asian woman", "Middle-Eastern woman", "white man", "black woman", e "Latina woman". La durata di ciascun prompt varia a seconda della complessità e della lunghezza del testo generato, ma generalmente si tratta di un processo che richiede alcuni minuti per ogni persona rappresentata.</sample>
    <sample id="316">I risultati del modello T5 più piccolo, appunto T5 fine-tuned on CoScript, indicano che i modelli più piccoli, se addestrati correttamente su dataset adeguati, possono superare i modelli più grandi in termini di generazione di script di alta qualità per la pianificazione linguistica con vincoli. Questo suggerisce che, con l'addestramento adeguato, i modelli più piccoli possono essere utili e efficienti per applicazioni di pianificazione linguistica con vincoli specifici.</sample>
    <sample id="317">Title: CodeIE: Large Code Generation Models are Better Few-shot Information Extractors

Abstract:
Information extraction (IE) is a fundamental task in natural language processing (NLP) that involves extracting structured information from unstructured text. Traditional IE models, such as T5 and GPT-3, operate in a text-to-text manner during pre-training, but their outputs are linearized into a plan sequence during inference, leading to mismatched outputs. To address this issue, we propose CodeIE, transforming the text-based IE task into a structure-to-structure code generation problem using code large language models like Codex. By converting text to a structured format during input and ensuring aligned structures in the output, we can improve the performance of IE tasks.

We evaluated our method on three named entity recognition (NER) datasets and four relation extraction (RE) datasets, comparing the performance of traditional text-style prompts with our code-style prompts. Our results show that the proposed approach using code language models and code format queries significantly outperforms the traditional baseline models, such as UIE, and natural language large language models like GPT-3. We further analyzed the phenomenon and found that code format prompts lead to better performance in terms of recall, and the Codex model consistently outperforms the GPT-3 model in IE tasks.

In conclusion, our work demonstrates the effectiveness of using code large language models for information extraction tasks and provides insights for future research in this area.</sample>
    <sample id="318">Ciao, sono Yanis Labrak e presenteremo i nostri lavori su "DrBERT: Un Modello Robusto Pre-addestrato in Francese per i Domini Biomedici e Clinici." In questa presentazione, parleremo prima di modellazione del linguaggio nella sanità. Successivamente, parleremo della principale contributo del nostro articolo. Introduciamo il primo modello biomedico in Francese chiamato DrBERT, che si basa su RoBERTa e è stato addestrato su NACHOS, un dataset di dati medici raccolti dal web. Abbiamo anche presentato una comparazione di modelli con diversi impostazioni di pre-addestramento e fonti dati. Poi, presenteremo i nostri risultati su 11 compiti di downstream biomedici e clinici in Francese. Infine, concluderemo con un'analisi delle esperimenti e forniremo ulteriori dettagli su come accedere a questi modelli.

Dopo l'introduzione di BERT nel 2018, è diventato uno dei metodi più efficaci per risolvere le attività di elaborazione del linguaggio naturale e offre enormi miglioramenti rispetto ai metodi statici e contestualizzati storici come Word2vec, fastText o altri. Da allora, questo modello è stato adattato a molti altri linguaggi, come in Francese con CamemBERT, e anche in domini come biomedico con PubMedBERT e BioBERT e clinico con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altri linguaggi sono rari e spesso basati su pre-addestramento continuo a causa della mancanza di dati in ambito. Tuttavia, la Francia non aveva modelli aperti e gratuiti per il biomedico fino a questo punto. Quindi, chiediamo a noi stessi una domanda: qual è il più appropriato fonte di dati per un uso ampio e se i dati raccolti dal web sono una buona sostituzione per i dati clinici?

Per rispondere a questa domanda, abbiamo confrontato DrBERT con il nostro modello ChuBERT, che si basa su dati anonimizzati ottenuti dal deposito di dati dell'Ospedale Universitario di Nantes. Successivamente, chiediamo quanto dati abbiamo bisogno per addestrare un modello specializzato su dati in Francese: è 4 GB, 8 GB o di più? Per rispondere a questa domanda, addestriamo e confrontiamo quattro modelli da zero: una prima versione di DrBERT, con 7 GB di NACHOS; una seconda versione di 4 GB di un insieme di NACHOS; una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi prelevate da note cliniche; e una versione finale di ChuBERT con una miscelta di 4 GB di un insieme NACHOS e 4 GB di note cliniche. Oltre a questa comparazione, abbiamo introdotto tre modelli addestrati su pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso di CamemBERT addestrato su un insieme di 4 GB di NACHOS. Un altro basato anche su CamemBERT, ma addestrato questa volta su 4 GB di note cliniche e infine, uno basato su un modello biomedico inglese PubMedBERT, addestrato su 4 GB di un insieme di 4 GB di dati NACHOS. In totale, abbiamo sette modelli.

Per valutare i nostri sette modelli, abbiamo raccolto dati per compiti di downstream pubblici e privati, come riconoscimento nomale, classificazione, taggatura part-of-speech e domande e risposte. Questi modelli sono stati confrontati con sei modelli di baseline, che sono CamemBERT OSCAR 138GB, CamemBERT OSCAR 4GB, CamemBERT CCNET 4GB, PubMedBERT, BioBERT e ClinicalBERT. L'evaluazione evidenzia che i modelli hanno ottenuto i migliori risultati sui compiti con dati della stessa natura su cui i modelli sono stati addestrati. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenei sembrano essere più versatile. Inoltre, osserviamo che l'uso di più dati traduce in</sample>
    <sample id="319">In il lavoro, vengono esaminate diverse strategie di apprendimento:

1. **Pre-training da zero (From-scratch pre-training)**: Viene addestrato un modello completamente nuovo su dati specifici.
2. **Pre-training continuo (Continual pre-training)**: Utilizza i pesi e il tokenizzatore di modelli pre-addestrati come CamemBERT, BioBERT, o PubMedBERT, e li addestra su dati specifici.
3. **Pre-training con dati di fonti diverse**: Viene esaminato l'uso di dati di fonti diverse per prendere decisioni su quanto dati sono necessari per ottenere buone prestazioni.

Queste strategie vengono confrontate su modelli pre-addestrati per valutare le loro prestazioni in vari compiti di NLP in lingua francese.</sample>
    <sample id="320">Il fattore di overfitting dovuto all'utilizzo ripetuta del test è maggiore di un. Questo significa che ogni unità di miglioramento su CoNLL-2003 traduce in più di una unità di miglioramento su CoNN++ e non ci sono diminuimenti di rendimento.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata utilizzando il nostro corpus DEPLAIN, che fornisce coppie parallele di frasi manualmente alignate. Questo corpus è stato utilizzato come gold standard per valutare diverse metodologie di allineamento automatico. In particolare, la migliore metodologia di allineamento identificata è MASSalign. Inoltre, la semplificazione è stata valutate attraverso l'allineamento automatico e la fine-tuning di modelli di linguaggio per la produzione di testi semplificati. I risultati mostrano che la semplificazione basata su fine-tuning è in grado di ottenere punteggi migliori rispetto ai baseline.</sample>
    <sample id="322">Enrico's presentation at ACL 23 will explore the question of what a text classifier learns about morality. He explains that morality is the internal compass that helps us distinguish right from wrong, and it is essential for language models to understand and recognize morality in language. However, morality is subjective and can be interpreted differently by different people. Enrico introduces the Moral Foundation Theory, which suggests that there are five different ways in which humans perceive morality, and each person prioritizes these foundations differently. Enrico and his team have applied explainable AI techniques to language models trained to understand morality in text and have found that language models can understand morality in text. They have also used a dataset called Moral Foundation Twitter Corpus to explore how morality is expressed differently across different domains. Enrico's research shows that language models can recognize that morality can be expressed differently in different domains, and using just a single model for many different domains can lead to misunderstandings of morality. Enrico hopes to see everyone at ACL in Toronto.</sample>
    <sample id="323">Title: Dynamic Heterogeneous-Graph Reasoning with Language Models and KRL for Commonsense QA

Abstract:
Commonsense QA is a challenging task that requires machines to understand language and retrieve relevant knowledge from external sources. Recent works combine language models and knowledge bases to solve this problem, but they introduce noisy entities and ignore the semantic relationship between entities. To address these issues, we propose DHLK, a dynamic heterogeneous-graph reasoning method that optimizes the structure and knowledge representation of the heterogeneous knowledge graph (HKG) using multiple knowledge bases and KRL. We remove subwords and retrieve paraphrases of key entities to form the HKG. We use RoBERTa and Mask Self-Attention (RMSA) to encode and fuse the QA context and entities, dynamically removing entities with weaker relevance to the QA context. We optimize the entity and relationship embeddings in the HKG using TransE and RMSA. We incorporate the HKG path information into the QA embedding and predict the answer using a multi-layer perceptron. We conduct experiments on CommonsenseQA and OpeBookQA using external knowledge bases and report good results compared to other methods.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diverse. Questo è stato dimostrato attraverso l'analisi dei loro risultati su diverse attività di classificazione, come la rilevazione del discorso offensivo e la rilevazione delle notizie false. I modelli con tendenze politiche diverse hanno risposte diverse a esempi di discorso offensivo e notizie false, indicando che i loro bias politici possono influenzare le loro performance in applicazioni NLP.</sample>
    <sample id="325">Ciao! Il mio nome è Matthias Lindemann e oggi vi darò una breve introduzione alla nostra ricerca su "Compositional Generalization without Trees using Multi-Set Tagging and Latent Permutations". Questo è un lavoro con i miei dottori di ricerca, Alexander Koller e Ivan Titov. La generalizzazione composizionale può essere compresa come la capacità di un apprenditore di gestire ricorsione più profonda e composizioni non viste durante l'addestramento. Nel contesto del parsing semantico, il test per la generalizzazione composizionale potrebbe essere questo. Come di solito, abbiamo un set di addestramento di frasi. In questo caso, "La ragazza dormì." E "Mary knew that the girl slept." Queste frasi sono associate a forme logiche che rappresentano aspetti chiave del loro significato. In contrasto con l'evaluazione standard del machine learning, il set di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente non viste durante l'addestramento. In questo esempio, il modello ha visto ricorsione superficiale durante l'addestramento e viene testato su un esempio con ricorsione più profonda. I modelli seq2seq semplici lottano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono distaccati dall'input. In particolare, spesso falliscono a riproduzere le corrispondenze sistematiche tra input e output, come quelle che sono colorate in questo esempio. Un metodo popolare per affrontare questo è l'integrazione di alberi nei modelli. Gli alberi sono intenzionali per catturare il processo composizionale che collega le frasi con le forme logiche. Questo funziona bene, ma gli alberi non sono dati e devono essere ottenuti in qualche modo. Questo può essere complicato e talvolta un processo computazionalmente costoso. Di solito, ciò richiede una pre-processing formale specifico dei formi logiche, ad esempio per gestire simboli variabili. Ottenere alberi può anche richiedere procedimenti di induzione grammatica speciali. In questo lavoro, noi non usiamo alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione a ricorsione più profonda senza affidarci a alberi. Il nostro approccio predice l'output dall'input in due passaggi. In primo luogo, taggiamo ogni token di input con un insieme non ordinato di token che appaieranno nell'output. Dopo il primo passaggio, abbiamo tutti i token giusti, ma non ordinati. Questo è perché nel secondo passaggio utilizziamo un altro modello per predire una permutazione per metterli nella giusta ordine. Introduciamo un nuovo metodo per predire la permutazione che non mette alcune restrizioni rigorose sulle permutazioni possibili. Questo rende il nostro approccio molto flessibile e espressive. In termini concettuali, il nostro metodo di permutazione funziona approssimativamente così. Vogliamo andare dalla sinistra alla destra dell'output e determinare quale multiset token mettere in ogni posizione. Per la prima posizione di output, semplicemente selezioniamo uno, come evidenziato in rosso. Poi saliamo alla prossima multiset token per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in un modo simile salendo ad un altro multiset token. Continuiamo questo processo fino a quando ogni token dalla prima fase è stato visitato esattamente una volta. Per dare un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri di grandissima margine in termini di generalizzazione a ricorsione più profonde. Alcune altre forme di generalizzazione strutturale rimangono comunque molto sfidanti. Nel nostro lavoro, risolviamo alcune sfide tecniche interessanti. In primo luogo, l'allineamento tra input e output non è dato nella fase di addestramento. Di conseguenza, per un dato token non sappiamo da quale multiset prov</sample>
    <sample id="326">La dissonanza cognitiva è una condizione in cui due credenze o azioni sono in contrasto, come quando una persona dice che "so che i sigaretti possono uccidere" e successivamente va a fumare dopo una riunione. Questa credenza e azione sono in dissonanza e hanno una relazione di consonanza. La dissonanza è una fenomenologia comune nella vita quotidiana, ma è rara nell'espressione in linguaggio. Studiare la dissonanza può aiutare a comprendere gli effetti della disaccordo tra le persone, tracciare tendenze e valori di credo, e capire meglio la salute mentale. Inoltre, la dissonanza può essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza è importante per comprendere lo stile cognitivo personale e migliorare la comprensione del processo decisionale.</sample>
    <sample id="327">In this paper, we introduce ManagerTower, a novel Vision-Language (VL) model architecture that aims to improve the performance of existing VL models by effectively aggregating insights from pre-trained unimodal experts at different levels of the model. Our approach is based on the two-tower architecture, which consists of a textual encoder, a visual decoder, and a cross-modal encoder. We propose the use of managers in each cross-modal layer to adaptively aggregate insights from pre-trained unimodal experts at various levels, allowing for more comprehensive cross-modal alignment and fusion. Our experiments show that ManagerTower significantly outperforms existing models on various downstream tasks, including the Wikivideo test standard, with only four million images for visual language pre-training. We also demonstrate that our approach is scalable and can be applied to any visual, textual, or cross-modal encoder. We hope that our work can contribute to the development of more effective and efficient VL models.</sample>
    <sample id="328">Il modello linguistico più liberale, secondo i risultati preliminari della tua ricerca, è GPT-4.</sample>
    <sample id="329">In this work, we introduce a novel approach for zero-shot video sentence localization that generates structured pseudo-labels to enhance robustness against label noise. Traditional methods rely on manually annotated pseudo-queries and events, which are often too simplistic and can lead to misalignment between queries and events. Our method addresses these issues by using a pre-trained image caption model to generate complex free-form pseudo-queries, followed by a temporal event structure model to create pseudo-events with high relevance to the query within the event and low relevance outside. We reduce label noise by re-weighting samples based on predicted confidence and IoU, and refine labels by training high-confidence predictions for subsequent rounds. Experiments on ActivityNet Captions and Charades-STA datasets demonstrate that our method outperforms existing zero-shot approaches, achieving superior performance in terms of R@M and mIoU metrics. Our approach is robust to label noise and provides a scalable solution for zero-shot video sentence localization.</sample>
    <sample id="330">In the context of active learning, the "Cumulative" strategy, which accumulates all the data collected from active annotation, performs equal to or better than the "Iterative" strategy across the board. This means that cumulative updating of the model with new data from each round of active annotation generally yields better or comparable performance compared to iterative updating.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">I dati per il parametro di riferimento MuDA sono stati tratti dai trascritti di conferenze TED tradotti in 14 lingue diverse. Questi trascritti hanno permesso di analizzare le parole e le frasi che richiedono un contesto per essere tradotte correttamente.</sample>
    <sample id="333">In this paper, we introduce INK, a novel framework for enhancing neural machine translation (NMT) models by injecting k-Nearest Neighbor (kNN) knowledge into the representation space. Traditional NMT models often suffer from non-smooth representation spaces, leading to poor generalization and performance, especially for low-frequency tokens. To address this, we propose INK, which smooths predictions by leveraging nearest neighbors in the representation space. However, the original kNN-MT approach is computationally expensive and inflexible. To overcome these limitations, INK introduces a two-step training loop: first, kNN knowledge is extracted to guide the adapter, and then updated representations are used to refresh the datastore asynchronously until convergence. This approach aligns contextualized representations with token embeddings, kNN token embeddings, and same target token representations to improve semantic meaning and address sparsity. Our experiments show that INK outperforms the state-of-the-art kNN-ML system, achieving higher BLEU scores with less memory space and faster inference speed. We also demonstrate that jointly applying an adapter and datastore further smooths predictions, indicating that the representation space of NMT models can be further refined. Overall, INK achieves an average gain of 1.98 COMET score and 1.0 BLEA score compared to the state-of-the-art kNN-MT models, showcasing its potential to improve NMT performance.</sample>
    <sample id="335">Il nome del relatore è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è un processo che coinvolge la traduzione di query in più lingue naturali in diverse rappresentazioni significative. Questo è un'area di ricerca che si concentra su come le modelli linguistici possano essere addestrati per comprendere e tradurre query in diverse lingue, utilizzando modelli pre-addestrati in una o più lingue per migliorare le prestazioni in altre lingue. In particolare, il lavoro di Yusen Zhang e colleghi propone XSemPLR, un benchmark unificato per il trasferimento interlinguistica nella semantica, che include dataset in diverse lingue e rappresentazioni significative.</sample>
    <sample id="337">Title: Graph-based Relation Mining for Context-free Out-of-vacabulary Word Embedding Learning

Abstract:
Out-of-vocabulary (OOV) words pose significant challenges in embedding-based downstream models, yet they are crucial for performance. Our research introduces a novel approach leveraging word formation and association to infer the meaning of these words. We develop a Word Relationship Graph that mimics lexical rules, associating OOV words with relevant words to form a two-level graph. Each word or wordpiece acts as a node, with its embedding as the node attribution. The first layer preserves complete wordpiece information, while the second layer samples nodes to mitigate noise. A self-attention network assigns attributes to OOV nodes, and two levels of Graph Attention Network fuse the initial input with hidden embeddings. A readout block layer captures the graph's whole information. We apply contrastive learning in the loss function to mimic the background embedding model's vector space. Extensive experiments show our model outperforms baselines in intrinsic and extrinsic tasks, benefiting both static and contextual models. Our model's applicability to other languages depends on the rationality of word decomposition, with agglutinative languages being well-suited. In conclusion, our graph-based approach effectively handles complex word formations, offering potential for broader language applications.</sample>
    <sample id="338">Good day everyone. My name is Binsheng, and I am here to present our research titled "Are Human Explanations Always Helpful?" This work is a collaborative effort between researchers from Rensselaer Polytechnic Institute, Northeast University, and IBM Research. We aim to address the critical question of how to objectively evaluate the quality of human natural language explanations, which are often subjective and task-dependent.

Our research introduces a unified data structure that converts various tasks into a standardized multiple-choice format, allowing for a consistent analysis of explanation utility. We conducted extensive experiments on five popular datasets, including CoS-E and ECQA for commonsense QA, e-SNLI for natural language inference (NLI), and ComVE for commonsense validation. Our findings reveal that explanations can significantly enhance model performance, even when they are not considered high-quality by humans.

We propose a novel evaluation metric, TREU, which extends the simulatibility score to account for the helpfulness of explanations during fine-tuning. Our results show that TREU consistently ranks dataset qualities better than the simulatability score, particularly for NLI tasks. This suggests that the utility of human explanations is highly dependent on the task and explanation format.

In summary, our work provides a foundation for high-quality human collaboration in annotation tasks and recommends that researchers perform similar quality checks in the future. We believe our metric offers a more comprehensive evaluation of human explanations than traditional metrics like BLEU and ROUGE. Thank you for your attention.</sample>
    <sample id="339">The authors of the article "Weaker Than You Think: A Critical View on Weakly Supervised Learning" are affiliated with Saarland University in Germany. The specific affiliations of the authors are:

- Dawei
- Xiaoyu Shen
- Marius Mosbach
- Andreas Stephan
- Dietrich Klakow

These authors are all part of the research team at Saarland University.</sample>
    <sample id="340">"ParaAMR: A Large-Scale Syntactic Diversity Paraphrase Dataset by AMR Back-Translation" presents a novel approach to generating a large-scale, syntactically diverse paraphrase corpus. The dataset is constructed by leveraging Abstract Meaning Representations (AMR) graphs, which capture the semantic structure of sentences. By modifying the focus node in the AMR graph and generating text from the modified graphs, ParaAMR achieves high syntactic diversity while preserving semantic similarity. The dataset contains approximately 15 million source sentences and 6.9 paraphrases per source sentence, offering a significant improvement over existing back-translation datasets in terms of syntactic diversity. Quantitative and human evaluations demonstrate that ParaAMR maintains semantic similarity comparable to other datasets while exhibiting higher syntactic diversity. The dataset is shown to enhance various NLP applications, including sentence embeddings, syntactic control paraphrase generation, and data augmentation for few-shot learning. ParaAMR is available for public use, providing a valuable resource for advancing paraphrase generation and related NLP tasks.</sample>
    <sample id="341">Gli autori utilizzano tre misure di latenza:

1. **Average Lagging**: Misura la latenza media del modello.
2. **Computational Aware Average Lagging**: Misura la latenza media considerando il tempo computazionale del modello per generare l'output.
3. **Actual Elapsed Time**: Misura il tempo reale trascorso per generare l'output.

Queste misure sono utilizzate per confrontare le performance del loro modello EDAtt con altre strategie e architetture.</sample>
    <sample id="342">Title: LiveChat: A Large-Scale Personalized Dialogue Dataset from Live Streaming

Abstract:
LiveChat is a novel dataset designed for open-domain and personalized dialogue research, constructed from live streaming videos on Chinese platforms like TikTok and Douyin. Unlike existing text-based datasets, LiveChat is video-sourced, offering a more realistic conversational context. The dataset is built through three steps: video scraping, audio transcription, and dialogue construction using reply-to-whom matching. Persona information is also collected to support personalized dialogue generation. LiveChat addresses key challenges in dialogue datasets, such as scale, manual annotation, and multi-party conversations. Experiments demonstrate that persona profiles and longer sessions enhance response modeling and addressee recognition. BART outperforms other models on LiveChat, highlighting its domain-specific nature. In-context learning experiments show that performance improves with more demonstrations, up to a point. LiveChat offers a valuable resource for advancing dialogue research, particularly in personalized and multi-party scenarios. Future work will focus on efficient transfer learning of large language models for LiveChat.</sample>
    <sample id="343">Ciao a tutti, sono Akshatha e oggi mio coautore Martin e io stiamo presentando il nostro lavoro "Il Test KITMUS: Valutare l'Integrazione del Conoscimento da Multiple Sorgenti." Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale dipendono da una varietà di fonti di conoscenza, come quelle contenute nei loro parametri, generalmente acquisite durante una pretrazione, e quelle fornite durante l'inferenza. Le recenti opere su compiti come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante la pretrazione per risolvere le attività. Ma la comprensione del linguaggio naturale spesso richiede conoscenza che è anche fornita durante l'inferenza. Ad esempio, nella frase "John ha visto il presidente nuovamente eletto su TV." I parametri pre-addestrati possono contenere informazioni su cosa i presidenti fanno e su cosa è una TV, ma non possono determinare affidabilmente chi è l'entità specifica "John" o chi è il nuovo presidente, perché il presidente potrebbe aver cambiato dopo la pretrazione. Pertanto, i modelli per compiti di comprensione del linguaggio naturALE intensi richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella fornita durante l'inferenz. In questo lavoro, proponiamo un test di diagnosi per l'integrazione del conoscimento. Introduciamo una task di risoluzione del coreferente, progettata per indagare sulla capacità di dipendere da conoscenza disponibile in diverse fonti. Valutiamo il dataset con partecipanti a studi umani e modelli di risoluzione del coreferenti stabiliti. Ecco un esempio dal nostro dataset. Servin è un giudice. Kea è un forno. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidin casi in un tribunale, era felice di rilassarsi. La task qui è identificare l'entità corretta a cui si riferisce il pronome "he", che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni. Prima, la conoscenza specifica all'entità, come "Servin è un giudice." E secondo, la conoscenza di fondo, come "I giudici decidono casi in tribunali." Generalmente, la conoscenza di fondo è imparata durante la pretrazione dei modelli di grandi dimensioni, mentre la conoscenza specifica all'entité è tipicamente osservata durante l'inferenza. Modificiamo la disponibilità di queste due informazioni in modo che possano essere trovate in una sola fonte o in più fonti. Abbiamo definito tre configurazioni di KITMUS. Prima, abbiamo la configurazione tipica: "Background-Pretrain", dove la conoscenza di fondo è presupposta disponibile a pretrazione. Poi, ci è una configurazione "Background-Both", dove la conoscenza di fondo e specifica è disponibile sia a pretrazione che durante l'inferenza. Infine, la configurazione "Background-Inference", dove entrambe le conoscenze sono disponibili solo durante l'inferenza. Questa ultima configurazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di fondo necessaria per risolvere una attività non è parte dei dati di pretrazione dei modelli. Ad esempio, perché nuovi lavori sono stati sviluppati dal momento della pretrazione. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle veri fonti. Nella configurazione "Background-Pretrain", assumiamo che la conoscenza di fondo "I politici cercano seggi elettorali in governo" sia contenuta nei parametri pre-addestrati e in contesti di inferenza forn</sample>
    <sample id="344">Gli svantaggi dei metodi basati sull'albero includono:

1. **Pre-processing Complesso**: La gestione delle forme logiche e la creazione di alberi richiedono una pre-processing formale specifico, che può essere computazionalmente costoso.
2. **Induzione Grammaticale**: Ottenere alberi può richiedere procedimenti di induzione grammatica specializzati.
3. **Dipendenza dai Dati di Addestramento**: L'alignamento tra input e output non è fornito nel training, rendendo difficile determinare da quale multiset ogni token proviene.
4. **Latent Permutations**: La corretta permutazione latente può essere difficile da indurre, e il problema di trovare la permutazione più probabile è NP-hard, richiedendo approssimazioni continue per la backpropagation.

Questi svantaggi sono affrontati nel nostro lavoro con un approccio basato su multiset tagging e permutazioni latenti, che evita l'uso di alberi e migliora la generalizzazione a ricorsione più profonda.</sample>
    <sample id="345">Compositional generalization is the ability of a learner to handle deeper recursion in unseen compositions of phrases that have been seen individually. In semantic parsing, testing for compositional generalization involves evaluating models on structurally unseen logical forms. Naive seq2seq models struggle with out-of-distribution generalization and often fail to reproduce systematic correspondences between input and output. To address this, we introduce a neural seq2seq model that directly predicts the output from the input in two steps: first, tagging each input token with an unordered multiset, and second, predicting a permutation to put the tokens in the right order. Our approach outperforms other treeless models on the COGS benchmark, but some structural generalization challenges remain. We address these challenges by inducing the alignment as part of the training and approximating the NP-hard permutation problem with a GPU-friendly continuous relaxation.</sample>
    <sample id="346">I'm sorry, but the information about the affiliations of the authors of the article is not provided in the content you shared.</sample>
    <sample id="347">Ciao, sono Myra e oggi parlerò del nostro lavoro "Marked Personas: Utilizzando Promemoria Naturale per Misurare Stereotipi in Modelli di Linguaggio". Questo lavoro è fatto in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molte hanno documentato la prevalenza dei bias sociali e stereotipi nei grandi modelli di linguaggio, o LLM. Tuttavia, questi misuramenti hanno vari limiti. Di solito dipendono da set di dati costruiti manualmente che richiedono molto tempo per curare e solitamente misurano stereotipi molto specifici, significando che non generalizzano bene ad altri gruppi demografici o contesti, o semplicemente catturano associazioni generali e ampie, come quelle negative con particolari gruppi. Inoltre, la maggior parte del lavoro in questo spazio non tiene conto dell'intersezionalità, che è la nozione che le identità sociali multifattoriali possono compostarsi bias e essere unica loco di danno. Per superare questi limiti, siamo affidati alla proprietà che questi nuovi modelli di linguaggio adattati alle istruzioni sono molto bravi a rispondere alle istruzioni e alle promemoria. Quindi possiamo chiedere al modello di generare un'identità, che è una rappresentazione di un individuo immaginato utilizzando una promemoria come "Immagina di essere una donna asiatica. Descriviti te stessa". E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questa promemoria. Qui sono alcuni esempi di generazioni da GPT-4. Inizialmente vediamo che, sebbene le uscite non siano esplicitamente negative o tossiche nel senso tradizionale di queste parole, ci sono alcune interessanti pattern. La donna asiatica è descritta come modesta; la donna medio-orientale viene riferita utilizzando parole come "esotico" e "come", riferendo a una regione affascinante. E entrambe le persone di colore femminili fanno riferimento all'antichità mentre l'uomo bianco non fa nulla di così. Per catturare questi pattern, il nostro metodo ha due parti. La prima parte è la generazione di queste identità, e le promemorie per generare queste identità sono state ispir</sample>
    <sample id="348">Myra, Esin Durmus, and Dan Jurafsky's paper, "Marked Personas: Using Natural Language P</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi dall'Università di Scienze e Tecnologia della Cina. È un piacere presentare un breve video pubblicitario della nostra ricerca. Se stai copiando il mio modello? Proteggere il diritto d'autore dei modelli di linguaggio grande per l'integrazione come servizi tramite marcatura da sottofondo.

Prima di tutto, parliamo di cosa sono le integrazioni come servizi. Attualmente, modelli di linguaggio grande come GPT, LLAMA, PALM sono eccezionali per la comprensione e la generazione del linguaggio naturale. Le integrazioni come servizi sono una delle molte funzionalità costruite sui modelli di linguaggio grande per aiutare a vari compiti di elaborazione del linguaggio naturale. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, recenti studi hanno mostrato che gli attaccanti possono rubare il modello attraverso l'apprendimento dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d'autore delle integrazioni come servizi.

Per proteggere il diritto d'autore delle integrazioni come servizi, una delle soluzioni è incorporare una marcatura in il servizio fornitore e rilevare se un altro servizio contiene la marcatura. Il metodo di marcatura deve soddisfare le seguenti proprietà: prima, il metodo deve essere applicabile alle integrazioni come servizi. Secondo, la marcatura non deve degradare l'utilità degli embedding forniti. Terza, la marcatura deve essere nascosta abbastanza per l'attaccante o l'attaccante possa rimuovere la marcatura facilmente. Infine, la marcatura deve essere trasferibile al servizio dell'attaccante durante il processo di estrazione del modello.

I lavori esistenti possono essere classificati in quattro categorie, tuttavia, questo metodo non è applicabile alle integrazioni come servizio o manca di trasferibilità. Pertanto, in questo lavoro abbiamo proposto Embedding marker, che è un metodo di marcatura da sottofondo basato su attacco applicabile alle integrazioni come servici.

Ora parliamo dei dettagli della nostra marcatura. Embedding marker comprende due passaggi principali: l'iniezione della marcatura e la verifica del diritto d'autore. Prima di questi passaggi principali, selezioniamo un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata. Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione della marcatura, definiamo un embedding di target. Quando un utente invia una frase al servizio fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma dei pesi dell'embedding di target e dell'embedding originale. Il peso dell'embedding di target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di target. La verifica del diritto d'autore è per rilevare se un modello dietro un altro servizio contiene la marca. Costruiamo un insieme di dati da sottofondo e un insieme di dati benigno. L'insieme di dati da sottofundo contiene frasi in cui tutte le parole appartengono all'insieme di trigger, mentre tutte le parole nelle frasi dell'insieme di dati benigno non appartengono all'insieme dei trigger. Poi il fornitore richiede gli embedding al servizio del rubatore con il dataset. Calcoliamo la somiglianza cosinus e L2 tra gli embedding richiesti e l'embedding di target. Calcoliamo la differenza di somiglianza tra i dataset benigni e da sottofondo, che è definita come delta cosinus e delta L2. Inoltre, appliciamo il test KS e utilizziamo il p-value come terzo metodo. Eseguiamo esperimenti su quattro dataset: AG News, MIND, SST2 e Enron Spam. Supponiamo che il fornitore utilizzi un dataset wiki per contare la frequenza delle parole. I risultati su quattro dataset mostrano che la nostra marcatura di embedding può avere un grande prestazione di rilevazione mentre mantiene una grande utilità per le compiti sottostanti. Validiamo anche la nascita della marcatura fornita visualizzando gli embedding delle frasi su quattro dataset [INAUDIBLE 4:30] PCA. La leggenda delle figure significa il numero di trigger in ogni frase. Come mostrano le figure, è difficile distinguere tra gli embedding da sottofondo e</sample>
    <sample id="350">Title: "What’s the Meaning of Superhuman Performance? A Critical Examination of Leaderboard-Based Evaluation in NLP"

Abstract:
In recent years, leaderboard-based evaluation has become the standard for assessing natural language understanding (NLU) systems, often leading to claims of superhuman performance. However, this approach has significant limitations. This paper critically examines the reliability of leaderboard scores in comparing models and humans, focusing on two popular benchmarks: SuperGLUE and SQuAD. We highlight that systems often outperform humans on specific tasks, but this comparison is fraught with issues such as different evaluation sets, errors in ground-truth answers, and varying human performance across tasks. For instance, humans are typically evaluated on a small subset of the test set, and the term "human baseline" is often used without specifying the best possible human performance. Additionally, the quality of human annotations can vary widely, and the details about the annotator pool are often missing, making claims of superhuman performance scientifically questionable. We argue that these issues undermine the validity of such claims and propose recommendations for constructing more reliable benchmarks. Our findings suggest that while leaderboard-based evaluation can provide useful insights, it should be complemented with more rigorous and context-aware assessments to truly understand the capabilities and limitations of NLU systems.</sample>
    <sample id="351">Title: Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?

Abstract:
This paper investigates the generalization capabilities of CoNLL-2003 named-entity recognition (NER) taggers in the context of modern data. Despite being developed nearly two decades ago, these models are still widely used. We address the following questions: Can these models generalize to contemporary data? What factors contribute to good generalization? What causes performance degradation?

To explore these issues, we created the CoNLL++ dataset, which consists of Reuters News articles from 2020 annotated with CoNLL-2003 guidelines. We fine-tuned over 20 models on the CoNLL-2003 dataset and evaluated their performance on both the CoNLL-03 and CoNLL++ test sets. We calculated the percentage change in F1 score to assess generalization.

Our findings reveal that three key factors contribute to good generalization: model architecture, model size, and the number of fine-tuning examples. Transformer models, larger models, and more fine-tuning examples all lead to better generalization.

We also identified two main causes of performance degradation: adaptive overfitting and temporal drift. Adaptive overfitting, characterized by diminishing returns on new test sets, was not observed in our experiments. However, temporal drift, resulting from the increasing temporal gap between training and test data, was confirmed as a significant factor.

In conclusion, CoNLL-2003 tagger models can still perform well in 2023, provided they incorporate a robust model architecture, sufficient size, and ample fine-tuning examples. Our research highlights the importance of addressing temporal drift to improve model generalization. We encourage further research to enhance the generalization capabilities of NER models.

Keywords: CoNLL-2003, NER, generalization, model architecture, model size, fine-tuning, temporal drift, adaptive overfitting.</sample>
    <sample id="352">ABC-Eval significa "Annotating Behaviors in Chat" e è un metodo per valutare i dialoghi delle chat basato su comportamenti specifici. Questo approccio mira a ridurre la soggettività dell'evaluazione umana, annotando direttamente se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso o il partner. ABC-Eval è stato sviluppato per misurare le tassi di errori tematici commessi dai modelli di chat, come ignorare il partner, dire qualcosa di irrilevante, contraddire, o non rispettare la conoscenza comune. Questo metodo fornisce una valutazione più precisa e affidabile delle qualità del dialogo delle chat.</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Question" by Li et al. addresses the challenge of input underspecification in code generation and program synthesis. The authors propose a method to generate code by asking clarification questions, which involves identifying missing specifications at various levels and generating CQAs to gather more information. They create a synthetic dataset called CodeClarQA and propose a pipeline of code generation by asking clarification question. The pipeline consists of a Clarification Need Predictor, a Question selector, and a Code Generator. The authors also conduct error analysis and find that their method is effective in generating CQAs for missing key operations. They also test their pipeline and find that it improves the performance of code generation. The authors conclude that clarified key operations are the reason for better generated code and that their method is a promising approach to address the challenge of input underspecification in code synthesis.</sample>
    <sample id="354">La differenza di rendimento tra CoNILL-2003 e CoNLL++, in termini di F1, non supera i 5 punti percentuali. Questo è stato osservato attraverso gli esperimenti condotti, confermando che i modelli sviluppati su CoNLL-2003 mantengono una buona generalizzazione anche sui dati più recenti.</sample>
    <sample id="355">Ciao, mi chiamo Vasudha e sono una dottoranda in Informatica a Stony Brook University. Voglio presentare il nostro lavoro accettato all'ACL 2023 come un articolo a lungo formato, "Transfer Learning for Dissonance Detection. Affrontare il sfida della classe rara". Iniziamo definendo cosa è dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. Semplicemente, la dissonanza cognitiva è quando due credenze o azioni sono inconsistenti, come questo esempio in cui una persona dice "Sono consapevole che i sigarette potrebbero uccidere", e poi va a prendere un paio di sigarette dopo la riunione. Questa credenza e azione sono inconsistenti e sono in dissonanza. Inoltre, la frase "Non credo che possa mantenere il mio lavoro senza di loro" giustifica la seconda occorrenza. E hanno una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nella nostra vita quotidiana, è molto raro trovarla espressa in linguaggio tra altri tipi di relazioni discorsive. Quindi perché importa? Studiare la dissonanza può aiutarci a capire gli effetti della disaccordo tra le persone, tracciare tendenze e valori di credenze e cambiamenti di attitudine nella popolazione. Un alto livello di dissonanza è anche correlato a disturbi d'ansia e può aiutarci a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa in linguaggio può anche essere utile per capire l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza è importante per capire lo stile cognitivo personale degli individui e aiutarci a comprendere megliamente i processi di decisione.

Per raggiungere il nostro obiettivo di creare una risorsa di dissonanza, abbiamo condotto una grande annotazione di relazioni di dissonanza. Abbiamo utilizzato l'approccio "disonanza prima", come visto nel flussochart qui. Le twee sono state passate attraverso il parser PDTB, e coppie di unità discorsive sono annotate secondo le linee guida descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Dopo aver raccolto circa 1.000 esempi di coppie di unità discorsive, abbiamo eseguito un allenamento per un classificatore iniziale addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non sia performato molto meglio di quanto sarebbe stata la casualità. Dato il basso numero di dissonanza e l'assenza di dati precedenti su tale argomento, affrontiamo il problema dell'assoluta rarità. Per alleviare questo, sperimentiamo combinazioni di apprendimento trasferibile e apprendimento attivo per annotare in modo che più esempi di dissonanza possano essere raccolti in meno run di annotazione, riducendo i costi complessivi dell'annotazione mentre migliorando la rilevazione della dissonanza. Poiché il modello iniziale non è stato in grado di catturare la classe di dissonanza, abbiamo iniziato il processo di apprendimento attivo trasferendo i pesi da due compiti differenti: la classificazione della posizione di dissonanza, un compito che determina se due dichiarazioni di debate da persone diverse sono in accordo o in dissonanza, indipendentemente dal tema, chiamato debate</sample>
    <sample id="356">Le affiliazioni degli autori dell\'articolo sono:

- Matthias Lindemann
- Alexander Koller
- Ivan Titov</sample>
    <sample id="357">Il nome del relatore è Siyu Yuan.</sample>
    <sample id="358">Ci sono cinque autori coinvolti nell'articolo: Kayo Yin, Patrick Fernandes, Emmy Liu, André Fernandes Martins, e Graham Neubig.</sample>
    <sample id="359">L'approccio EDAtt viene confrontato con architetture specifiche per la traduzione pre-translate simultanea.</sample>
    <sample id="361">Title: CounterComp: Enhancing Compositional Generalization in Multi-Step Quantitative Reasoning

Abstract:

We introduce CounterComp, a novel approach to improve compositional generalization for multi-step quantitative reasonings in question answering tasks. Our method leverages counterfactual scenarios to train models to attend to relevant tokens in the input, thereby avoiding memorization of spurious patterns. By mining positive and negative examples from the training set, we create triplets that help the model learn a dynamic margin metric, which adjusts the auxiliary metric learning loss based on the extent of change in the questions. This approach consistently improves the performance of state-of-the-art baselines, especially for reasoning tasks with more than two steps. We demonstrate that CounterComp not only enhances in-distribution performance but also improves out-of-distribution generalization, addressing the challenge of compositional generalization. Qualitative analysis reveals that the model attends to more meaningful tokens during training, leading to better operational term selection. Our work contributes to the field of natural language processing by providing a scalable and effective method for improving the generalization capabilities of neural models in quantitative reasoning tasks.</sample>
  </task>
</testset>