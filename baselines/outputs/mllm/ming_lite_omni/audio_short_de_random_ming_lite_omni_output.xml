<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Language Models sind große Skalen Web-Crawldaten, die aus politischen Nachrichtenmedien wie New York Times, Los Angeles Times, The Guardian und Huffington Post stammen. Diese Datenquellen bieten eine Vielzahl von Perspektiven und Ideen, die für die Entwicklung von Language Models nützlich sind. Allerdings können diese Datenquellen auch potenzielle Fairness-Probleme im Zusammenhang mit der Anwendung von Sprachmodellen auf Downstream-Tasks verursachen.</sample>
    <sample id="1">Die Autoren gehören an McGill University, Mila und Microsoft Research.</sample>
    <sample id="2">Hallo, willkommen zu unserer Präsentation von deplane, einem neuen Korpus für deutsche Textklassifizierung auf der Dokumentebene und auf der Satzebene.</sample>
    <sample id="3">Mein Name ist Regina Stören, und ich werde Sie für den ersten Teil der Präsentation führen. Lassen Sie uns zunächst Textverkürzung definieren.</sample>
    <sample id="4">Textanpassung ist der Prozess der Anpassung eines Textes, um die Textverständnisse für eine bestimmte Zielgruppe zu verbessern, wie z.B. Menschen mit Leseproblemen oder Muttersprachlern.</sample>
    <sample id="5">Um ein Textanalysemodell zu trainieren, benötigen wir Parallelpaare von Texten, zum Beispiel von Dokumenten oder Sätzen.</sample>
    <sample id="6">In the example here, you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language.</sample>
    <sample id="7">Um den Satz zu vereinfachen, gibt es verschiedene Techniken, wie Sie in dem Beispiel sehen können, wie zum Beispiel lexikalische Substitution, Klassifizierung, Klassifizierung und Umordnung oder Einfügen von Wörtern.</sample>
    <sample id="8">Wir schlagen unsere neue Corpus-Deep-Plane vor, weil in den letzten Jahren es einige Probleme mit den bestehenden Korpora gab. Zum Beispiel sind diese Korpora hier zu klein, um ein Textklassifikationsmodell zu trainieren.</sample>
    <sample id="9">Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie über Fehleranfälligkeiten in ihren Ausrichtungen verfügen können.</sample>
    <sample id="10">Daher schlagen wir unsere neue Korpus-Depression vor, die in zwei Subkorpora unterteilt ist: Depression APA und Depression Web. Depression APA basiert auf Texten.</sample>
    <sample id="11">In der plain apa haben wir 483 Dokumente manuell ausgerichtet, was zu etwa 30.000 13.000 parallelen Satzpaaren führt.</sample>
    <sample id="12">Für die Deep Learning Web wird dieses Korpus verschiedene Bereiche umfassen, und wir haben auch alle dieser 750 Dokumente auf der einen Seite manuell und auf der anderen Seite mit automatischen Ausrichtungsmethoden ausgerichtet.</sample>
    <sample id="13">Insgesamt ergeben wir 30.450 Satzpaare.</sample>
    <sample id="14">Wir analysieren unsere Satzpaare ein wenig weiter. Zum Beispiel, was die Art der Semantik betrifft.</sample>
    <sample id="15">**Übersetzung des englischen Inhaltes:**

Wie Sie hier sehen können, sind die Bibeltexte viel stärker und einfacher als beispielsweise die Nachrichtentexte oder die Sprachlernertexte.

---

**Übersetzung des englischen Textes:**

Wie Sie hier sehen, sind die Bibeltexte viel strenger und einfacher als beispielsweises die Nachrichtentexte oder die Sprache-Lernertexte.</sample>
    <sample id="16">Auf allen Ebenen hinsichtlich beispielsweise lexikalischer Simplifikation, struktureller Simplifikation oder auch allgemeiner Simplifikation.</sample>
    <sample id="17">Furthermore, you can see that our deep learning corpus has a high variety of different simplification transformations. So for example, in the deep learning API corpus, we have much more reorderings and word additions than we have in the deep learning web corpus.</sample>
    <sample id="18">Auf der anderen Seite haben wir im Webkorpus viel mehr Paraphrasen.</sample>
    <sample id="19">So let's now see what we can do with this corpus. Hello, I am Omar, and now I will talk about the use cases for our dataset D-Plane. So for the first use case, we can evaluate automatic alignment methods.</sample>
    <sample id="20">In den letzten Jahren gibt es viele Alignment-Methoden, aber im Kontext von maschinellem Übersetzen.</sample>
    <sample id="21">Wo wir zwei parallele Dokumente in verschiedenen Sprachen geschrieben haben und wir möchten die Übereinstimmungen von Sätzen in Post-Dokumenten extrahieren.</sample>
    <sample id="22">Aber in unserem Anwendungsfall, um uns zu versuchen, die Übereinstimmungen zwischen Sätzen zweier paralleler Dokumente zu extrahieren, die die gleiche Sprache haben, die gleichen Inhalte haben, aber sie sind auf unterschiedlichen Komplexitätsniveaus.</sample>
    <sample id="23">Und nun, da wir unsere Daten in der tiefen Ebene haben, die manuell ausgerichteten Sätzen haben, können wir diese Sätze als goldene Standardausrichtungen nutzen, um einige der vorgeschlagenen Methoden zu bewerten.</sample>
    <sample id="24">Wir haben einige Anpassungen an die vorgeschlagenen Methoden vorgenommen und haben alle diese Anpassungen und die Codes, um unsere Experimente durchzuführen, in dem Papier veröffentlicht.</sample>
    <sample id="25">Am Ende haben wir festgestellt, dass  ...  die beste Ausrichtung, automatische Ausrichtung Methode zu verwenden für Texte  ...  für deutsche Textvereinfachung  ...  ist die Methode der Massenausrichtung.</sample>
    <sample id="26">Und Sie können auch den Code finden, um  ...  diesen Methoden auf Ihren eigenen Dokumenten in der Papier zu laufen.</sample>
    <sample id="27">Der zweite Anwendungsfall, den wir in unserer Arbeit gezeigt haben, ist der Fall der automatischen Textvereinfachung.</sample>
    <sample id="28">Durch die Feinabstimmung von Sprachmodellen wird es möglich, komplexe Texte zu vereinfachen.</sample>
    <sample id="29">Wir haben zwei verschiedene Modelle fein abgestimmt. Wir haben das Modell der langen Impartur optimiert, um entsprechende Dokumentebene Vereinfachungen zu erzeugen.</sample>
    <sample id="30">Wir haben auch die normale Basis lang- bzw. die normale Basis im Teil zu produzieren, um Sätze auf niveau zu vereinfachen.</sample>
    <sample id="31">Sie können auch alle Checkpoints finden und entsprechende Details in den Scores und den Evaluationsmetriken unserer Experimente in dem Papier finden.</sample>
    <sample id="32">Wir haben Schlussfolgerungen gezogen, dass diese diese grundlegende Feinabstimmung die Produktion ermöglichen könnte  oder die Erzielung ermöglichen könnte  Scores besser als die Baseline Scores.</sample>
    <sample id="33">Wir schlagen diese Ergebnisse als Benchmark, als Basis-Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft vor.</sample>
    <sample id="34">Vielen Dank so viel für Ihre Aufmerksamkeit und wir hoffen, alle von Ihnen  uh während des Kongresses zu treffen. Danke.</sample>
    <sample id="35">Kai Ouyang</sample>
    <sample id="36">T5X Large Model</sample>
    <sample id="37">Yes, the CoNLL-2003 tagger still works in 2023.</sample>
    <sample id="38">Die vorgeschlagenen menschlichen Evaluationsmethoden basieren auf der Analyse von Modellantworten, um die Subjektivität des menschlichen Bewertungssystems zu reduzieren. Dies wird durch die explizite Annotation erreicht, bei der festgehalten wird, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel die Antwort auf unangemessene Informationen. Diese Methode ermöglicht es, die Bewertung von Modellantworten auf objektivere Kriterien zu stellen, indem sie die Subjektivität des menschlicher Bewertungssystems reduziert.</sample>
    <sample id="39">Der Erfolg des bestehenden schwahen überwachten Ansatzes hängt von der Verfügbarkeit von sauberen Validierungsdaten ab. Ohne saubere Validierungsdaten können die trainierten Modelle nicht über die ursprünglichen Wertebereiche hinaus generalisieren, was bedeutet, dass das Training sinnlos ist.</sample>
    <sample id="40">The result can be improved by providing more context or information about the entities, such as their characteristics or relationships to each other. Additionally, using more specific language or terminology can help to clarify the meaning of the entities and make the question easier to understand.</sample>
    <sample id="41">Drei Autoren sind an der Arbeit beteilt: Xiaoyu Shen, Maio Smutsbach und Gias Stephan.</sample>
    <sample id="42">Hallo, mein Name ist Adam Skurkowski und dieser Vortrag ist über die Abhängigkeitsstruktur von Koordination.</sample>
    <sample id="43">Wie Sie vielleicht wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Ansätzen angenommen werden. Zum Beispiel in universitären Abhängigkeiten ist die Struktur der Koordination von Lisa, Bart und Maggie.</sample>
    <sample id="44">In diesem Fall ist die erste Konjunktion die Hauptkopf der gesamten Koordinationsstruktur, also in diesem Fall Lisa.</sample>
    <sample id="45">Ein ähnlicher Ansatz wird in Igor Milčuk's "Meaning Text Theory" verwendet, bei dem wiederum die gesamte Koordinatenstruktur von der ersten Konjunktion geleitet wird. Diese beiden Ansätze sind also symmetrisch, denn sie heben jeweils einen der Konjunktionen heraus.</sample>
    <sample id="46">Die Symmetrische Ansätze zur Koordinatenstruktur, wie zum Beispiel der Präg-Ansatz, der Konjunktion-Hedging-Ansatz, der Hypothetische Präg-Dependenz-Triebans, wo Koordinatenstrukturen von der Konjunktion geleitet werden, sind auch bekannt.</sample>
    <sample id="47">So, we get some dependencies from and to all the conjuncts.</sample>
    <sample id="48">Und schließlich gibt es auch eine mehrdimensionale Vorgehensweise, die zum Beispiel im Dekarteschen Wortgrammatik verwendet wird.</sample>
    <sample id="49">Wo man, so zu sagen, alle Konjunkte als Kopf der Koordinatenstruktur haben sollte, so erhalten wir Abhängigkeiten vom Governor hier laufen, um alle Konjunkte getrennt zu betrachten. Diese sind wichtig für die Verbindung.</sample>
    <sample id="50">Die Hauptaufgabe dieser Arbeit besteht darin, eine neue Argumentation für die symmetrischen Strukturen der Koordination wie diese beiden zu entwickeln und gegen die asymmetrischen Strukturen der Koordinations wie diese zu argumentieren.</sample>
    <sample id="51">Der Argument ist auf dem Prinzip der Abhängigkeit und Minimierung basiert, das ich auf der Grundlage dieser Beispiele erläutern werde.</sample>
    <sample id="52">In English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away. So "Marge read it yesterday" is fine because the direct object "it" is close to the verb.</sample>
    <sample id="53">Während Marc gestern lesen musste, war es viel schlechter, weil zwischen dem Verb und dem direkten Objekt der Adverb „gestern“ steht.</sample>
    <sample id="54">Jedoch kann dieser Effekt, wenn  um  um  um  um  um 

die  um  um  um  um  direct object  um  um  um  um  is very heavy and very long  um  um  um  um  because then it can be moved to the position after the adjunct  um  um  um  um</sample>
    <sample id="55">Hier wird dargestellt, dass beide Sätze gut sind. Marc hat gestern über die Büchersprache gelesen, das ist in Ordnung. Statt "it" haben wir hier den langen und prägnanten Satz.</sample>
    <sample id="56">Aber es ist auch in Ordnung zu sagen, dass Marc gestern dieses faszinierende Buch über Bienen gelesen hat.</sample>
    <sample id="57">Die Begründung hier ist, dass dies möglich ist, weil auch wenn dieser Satz die allgemeine grammatische Regel verletzt, dass direkte Objekte neben dem Verb stehen sollten.</sample>
    <sample id="58">Es entspricht dem Prinzip der Längereduktion der Abhängigkeiten, das besagt, dass kürzere Abhängigkeiten bevorzugt werden.</sample>
    <sample id="59">Also, um diese beiden um die Kürze der kritischen Abhängigkeiten zeigen, die nur die Länge der kritischen Abhängigkeitszeiten zeigen, also diejenigen, die nicht konstant sind, unter diesen beiden Strukturen.</sample>
    <sample id="60">Hier haben wir eine Abhängigkeit von Red zu dem Adjunkt von Länge sieben, gemessen in Worten, und von Red zu Buch von Länge vier. Zusammen beträgt es elf.</sample>
    <sample id="61">Wenn Sie bewegen, wenn Sie tauschen, sind diese beiden Konstituenten die Summe dieser beiden Abhängigkeiten, die sich auf sechs verändert. Statt 11:6 ist es viel kürzer. Das ist der Grund, warum das so klingt.

Es verletzt eine Regel, aber es befriedigt eine andere.</sample>
    <sample id="62">Ja, natürlich. Hier ist die Übersetzung des englischen Textes ins Deutsche:

---

**Übersetzung:**

Ja, natürlich. Hier sind die Schritte, die wir unternommen haben:

1. **Datenextraktion:** Wir haben verschiedene Statistiken über Koordination aus der erweiterten Version des Pen Tree Bank extrahiert.
2. **Analyse:** Wir haben diese Daten analysiert, um herauszufinden, ob universelle Abhängigkeiten sinnvoll sind.
3. **Ergebnisse:** In der Arbeit "Why We Don't Use Universal Dependencies" haben wir die Ergebnisse dieser Analyse präsentiert.

---

**Übersetzte Version:**

Ja, natürlich! Hier sind die Schritte, die uns unternommen haben:

1.**Datenextraktion:** Wir haben versucht, verschiedene Statistiken über Kooperation aus der erweiterten Version des "Pen Tree Bank" zu extrahieren.
2.**Analyse:** Wir haben diese Daten analysieren, um herauszufinden ob universelle Abhängigkeiten nützlich sind.
3.**Ergebnisse:** In der Arbeit „Why We Don’t Use Universal Dependencies“ haben wir die Ergebnisse dieser Analyse präsentiert.

---</sample>
    <sample id="63">Diese Statistiken bestätigen die bekannte Beobachtung, dass linke Konjunktionen tendenziell kürzer sind. So sind "salt and pepper" und "not pepper and salt" in Silben gemessen.</sample>
    <sample id="64">Auch die Beobachtung, die bei der Überprüfung gemacht wurde, dass diese Tendenz mit der Länge des Längenunterschiedes wächst.</sample>
    <sample id="65">So, when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one stronger. So the proportion is bigger of the left short conjuncts.</sample>
    <sample id="66">Aber was ist neu in diesem Papier? Wir haben beobachtet, dass diese Tendenz nur dann auftritt, wenn die Regierung auf der linken Seite fehlt.</sample>
    <sample id="67">Also, the governor is on the left in this example. I saw Bart and Lisa, so the governor is on the left.</sample>
    <sample id="68">In the second example, "Homer came and sneezed," we have a coordination of two verbs, and there is no external governor. So in such cases, the left conjunct prefers to be shorter, and the bigger the difference between the two conjuncts.</sample>
    <sample id="69">Jedoch verschwindet dieses Effekt, wenn die Regierung auf der rechten Seite, wie hier, die Koordination der Netzwerke beeinflusst.</sample>
    <sample id="70">So we show that by measuring length in characters, that's the first column in syllables, the middle column and in words the right column. So I'll concentrate on the right one.</sample>
    <sample id="71">Hier sehen wir, dass 

Wenn der Regierungschef auf der linken Seite ist.</sample>
    <sample id="72">Die Tendenz für den linken Konjunkt, kürzer zu werden, wächst stetig mit der absoluten Differenz in Worten und dasselbe gilt, wenn es keinen Regierungspräsidenten gibt, wie in Koordination von Sätzen, aber wenn der Regierungspräsident auf der rechten Seite, diese Tendenz verschwindet.</sample>
    <sample id="73">Und wir zeigen in der Arbeit, wie dies um eine Argumentation gegen asymmetrische Strukturen der Koordination als diese beiden und für die symmetrischen Strukturen als diese beiden</sample>
    <sample id="74">Also, please refer to the paper for the full agreement and arguments. Sorry, and talk to us about the poster session. Thank you.</sample>
    <sample id="75">Drei Autoren sind an der Arbeit beteilt: der Verfasser, Alexander Kola und Ivan Titov.</sample>
    <sample id="76">The Bible text is much stronger simplified than for example the news text or the language learner text.</sample>
    <sample id="77">So, the example for the preference for shorter left conjuncts is: "salt and pepper, not pepper and salt, measured in syllables."</sample>
    <sample id="78">Yes, you can use the pre-trained models for your research. The models are freely available on the GitHub repository.</sample>
    <sample id="79">DEplain-apa enthält Dokumente aus den News-Texts.</sample>
    <sample id="80">Für eine gute Generalisierung sind eine bessere Modellarchitektur, eine größere Modellgröße und mehr Feinabstimmungsbeispiele erforderlich. Diese Ziele gehen Hand in Hand, und wir können nicht nur einen bestimmten Bestandteil haben, sondern müssen auch die anderen berücksichtigen. Zudem haben wir gefunden, dass der Performance-Drop hier durch temporäre Drifte verursacht wird und nicht durch adaptives Overfitting, obwohl Cono 2003 bereits seit über 20 Jahren verwendet wird. Zusammenfassend können wir sagen, dass die Antwort auf die Frage, ob Cono 2003 2023 noch funktioniert, respektive "Ja" ist, ist, dass wir unsere Arbeit als Grundlage für zukünftige Forschung auf der Verbesserung der Generalisierungen von Modellen legen.</sample>
    <sample id="81">Die Tendenz zu kürzeren linken Conjunktionen wurde durch Messung der Länge in Zeichen gemessen. Die erste Spalte zeigt die Anzahl der Silben, die mittlere Spalte die Anzahl der Wörter und die rechte Spalte die Anzahl der rechten Konjunktionen.</sample>
    <sample id="82">Die Experimente wurden gestaltet, indem die Position des Begrenzers in den Spalten 1, 2 und 3 variiert wurde. Die Spalte 1 enthielt die Länge in Zeichen, die Spalte 2 die Länge in Silben und die Spalte 3 die Länge in Wörtern. Die Auswirkungen der Position des Begrenzers auf die Länge der Konjunktion wurden dann analysiert.</sample>
    <sample id="83">Ein Basisklassifikator, der mit unausgewogenen Daten, wie z.B. nur 43 Beispielen von "dissonance", trainiert wird, wird nicht viel besser als Zufall sein. Dies liegt daran, dass die Daten sehr begrenzt und ungleichmäßig sind, was die Klassifizierung erschwert.</sample>
    <sample id="84">Eine Person (Shangbin) ist an der Arbeit beteiligt.</sample>
    <sample id="85">Die Personen im Beispielgespräche sind Bob und Alice.</sample>
    <sample id="86">Kontextsensitive MÜ-Modelle schneiden besser ab als kontextagnostische MÜ-Modelle bei Diskursphänomenen wie Formalität und lexikalischer Kohäsion ab.</sample>
    <sample id="87">Die Autoren gehören an der Universität von Stanford.</sample>
    <sample id="122">Das vorgestellte Framework quantifiziert die Positionalität, indem es die Annotationen der Daten mit verschiedenen Annotatoren annotiert und dann die Annotationen nach Demografie vergleicht. Es verwendet dabei die Pearson's R-Korrelation, um die Übereinstimmung zwischen den Modellen und den Daten zu messen.</sample>
    <sample id="155">Das Ergebnis der vorherigen Studien war, dass die menschlichen Teilnehmenden durch die Persona-Prompts auch rassistische Stereotypen aufdecken konnten.</sample>
    <sample id="156">In dieser Studie wurden Daten aus der erweiterten Version des "Pantry Bank" extrahiert, um Informationen über Koordination zu gewinnen. Diese Datenquelle wurde verwendet, um die Gründe zu untersuchen, warum Universitätsabhängigkeiten nicht verwendet wurden.</sample>
    <sample id="157">In dem angegebenen Text gibt es keine Informationen darüber, wie viele Autoren an der Arbeit beteiligt sind. Der Text stellt lediglich die Identität des Redners und das Thema des Vortrags vor.</sample>
    <sample id="158">The two related tasks for cognitive dissonance are topic-independent dissonance stance classification, which determines if two debate statements from different people are in agreement or in disagreement irrespective of topic, and binary classification of expansion and comparison classes of pdtb, since these two are closely related to the conception of consonants and dissonance, and we call them cee here. We find that on transferring the zero shot performance on the annotated dataset is already much better than chance with the best with auc point 62. Further on iteratively fine-tuning on both tasks, we find that fine-tuning of ce tasks followed by further fine-tuning on debate yields a much better zero shot performance. Thus, this is the model that we use to cold start the active learning.</sample>
    <sample id="159">Es gibt zwei Autoren an der Arbeit, nämlich Shuhang Zhu und John Doe.</sample>
    <sample id="160">Eine</sample>
    <sample id="161">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten, indem es die Endbenutzer mit Modellen und Datensätzen, Vorhersagen und Labels vergleicht, anstatt nur die Annotation-Agreement oder Modellierung zu betrachten.</sample>
    <sample id="162">Das Setup mit den generierten Personas hat die meisten Überschneidung mit dem Lexikon der Stereoty pen.</sample>
    <sample id="163">Die verglichenen kommerziellen Systeme waren DeepL und Google Translate.</sample>
    <sample id="164">Hallo, ich bin Zhang Bing, Doktorand an der Universität Washington. Heute präsentiere ich unsere Arbeit, die von vorgefertigten Daten über Sprachmodelle bis hin zu Downstream-Aufgaben reicht und die Spuren politischer Verzerrungen in unfaire NLP-Modelle verfolgt.</sample>
    <sample id="165">Sprachmodelle werden auf großskalige Web-Crawl-Daten trainiert.</sample>
    <sample id="166">Politische Nachrichtenmedien sind in ihren vorbereitenden Daten gut abgedeckt. Laut einer Umfrage des C4-Corpus können wir sehen, dass die New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut in Sprachmodell-Trainingsdaten abgedeckt sind.</sample>
    <sample id="167">Dies hat eine gemischte Segnung für Sprachmodellanwendungen geschaffen.</sample>
    <sample id="168">Auf einer Seite waren sie in der Lage, aus verschiedenen Perspektiven zu lernen, was die Demokratie und die Pluralität der Ideen feiert. Auf der anderen Seite sind diese verschiedenen politischen Meinungen in der Natur sozial verzerrt und könnten zu potenziellen Fairness-Problemen in der Anwendung von Downstream-Tasks führen.</sample>
    <sample id="169">Um diesen Zweck zu erreichen, schlagen wir vor, die politische Verständigungs-Pipeline von vorgefertigten Daten zu Sprachmodellen bis hin zu den niedrigeren Aufgaben zu untersuchen, insbesondere indem wir die folgenden Fragen stellen:</sample>
    <sample id="170">Um die politische Linie von Sprachmodellen zu bewerten, können wir verschiedene Ansätze verwenden, um die Ausgaben dieser Modelle zu analysieren und ihre politischen Vorurteile oder Präferenzen zu identifizieren. Hier sind einige mögliche Methoden:

1. **Analyse der Ausgaben**: Wir können die Ausgaben der Sprachmodelle auf politische Themen hin untersuchen, um herauszufinden, ob sie eine bestimmte politische Linie oder Vorurteile zeigen. Dies kann durch die Analyse von Beispielen oder durch die Bewertung von Texten, die von den Modellen generiert wurden, erfolgen.

2. **Bewertung der Fairness**: Wir können die Fairness der Sprachmodelle bewerten, indem wir sie auf verschiedene Gruppen von Menschen anwenden und sehen, ob sie eine ungleiche Verteilung der Ausgaben erzeugen. Dies kann durch die Analyse von Beiträgen oder durch die Bewertung von Texte, die von den Modellen generiert wurde, erfolgen.

3. **Bewertung der Repräsentativität**: Wir können die Repräsentativität der Sprachmodelle bewerten indem wir sie auf verschiedene Themen und Perspektiven anwenden und sehen, ob sie alle relevanten Aspekte abdecken. Dies kann durch die Analyse von Beitragen oder durch die Bewertung von Text, die von den Modellen generiert w</sample>
    <sample id="171">Zweitens, wie verhalten sich Sprachmodelle mit unterschiedlichen politischen Tendenzen bei denen Aufgaben in der NLP-Anwendung durchführen, und ob dies zu Fairness-Problemen führen könnte.</sample>
    <sample id="172">Also, we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political compass test. This ensures us to do automatic evaluation well grounded in political science literature.</sample>
    <sample id="173">So einige vordringliche Ergebnisse zeigen, dass zunächst Sprachmodelle tatsächlich unterschiedliche politische Ausrichtungen haben. Sie belegen alle vier Quadranten auf dem politischen Kompass.</sample>
    <sample id="174">Wir können auch sehen, dass GPT-4 die liberalste Sprachmodell aller ist und die GPT-Reihen allgemein sozial liberaler sind als die BERT-Reihen und ihre Varianten.</sample>
    <sample id="175">Zweitens streben wir an, zu untersuchen, bis zu welchem Grad die politischen Verzerrungen von Sprachmodellen tatsächlich aus dem Trainingsdaten aufgenommen werden.</sample>
    <sample id="176">Um eine kontrollierte Experimentelle Studie durchzuführen, könnten wir die folgenden Schritte durchführen:

1. **Datensammlung und Vorbereitung:**
   - Sammeln Sie sechs verschiedene Partisanenkorpora, die in News und Social Media unterteilt sind. Diese Korpora sollten unterschiedliche politische Linien repräsentieren.
   - Stellen Sie sicher, dass die Daten anonymisiert und rechtlich einwandfrei sind.

2. **Pre-Training der Sprachmodelle:**
   - Trainieren Sie Sprachmodelle auf den gesammelten Partisanenkorpora. Diese Modelle sollten die spezifischen Sprachmuster und Themen der jeweiligen Parteien erkennen können.
   - Verwenden Sie bekannte Sprachmodelle wie GPT-3 oder BERT und fügen Sie die Partisanenkorpora hinzu, um die Modelle weiter zu trainieren.

3. **Checkpoint-Erstellung:**
   - Erstellen Sie Checkpoint-Modelle, die auf den jeweiligen Partisanenkorpora trainiert wurden. Diese Checkpoints dienen als Basismodelle, auf denen weitere Experimente durchgeführt werden können.

4. **Experimentelle Bedingungen:**
   - Definieren Sie die Bedingungen für die Experimente. Dies könnte beispielsweise die Anwendung der Sprachmodelle auf neue Texte sein, die nicht in den Partisanenkorpora enthalten sind.
   - Stellen Sie sicher dass die Experimente so gestaltet sind, dass sie die Auswirkungen der Partisanität auf die Sprachmodelle quantifizieren können.

5. **Datenanalyse:**
   - Analysieren Sie die Ergebnisse der Experimente, um herauszufinden, wie die Partisanität die Sprachmodelle beeinflusst. Dies könnte durch die Untersuchung der Genauigkeit, Relevanz und Bias der Sprachmodelle erfolgen.
   - Verwenden Sie statistische Methoden, um die Ergebnisse zu interpretieren und zu vergleichen.

6. **Interpretation und Schlussfolgerung:**
   - Interpretieren Sie die Ergebnisse der Experimentelle Studie. Dies könnte beispielsweises die Identifizierung von Partisanität in Sprachmodellen beinhalten.
   - Ziehen Sie Schlussfolgerungen, die auf den Ergebnissen basieren und die Bedeutung der Partisanität für die Sprachmodelle hervorheben.

Diese Schritte bieten eine strukturierte Methode, um eine kontrollierte Experimentelle Studie zu durchführen, die die Auswirkungen von Partisanität auf Sprachmodelle untersucht.</sample>
    <sample id="177">Durch weitere Vorverarbeitung von Sprachmodellen auf solchen Partisan-Korpora können wir sehen, dass die ideologischen Koordinaten des Sprachmodells ebenfalls entsprechend verschoben werden.</sample>
    <sample id="178">Zum Beispiel, wenn wir Roberta weiter verfeinern und auf dem linken linear reduzierten Korpus weiter trainieren, können wir einen erheblichen liberalen Schub in Bezug auf seine Fähigkeiten beobachten.</sample>
    <sample id="179">In Bezug auf seine politischen Vorurteile.</sample>
    <sample id="180">Wir versuchen auch, zu untersuchen, ob Sprachmodelle die Polarisierung aufgreifen können, die in unserer modernen Gesellschaft vorherrscht.</sample>
    <sample id="181">Wir teilen die vorbereitenden Korpora in zwei Teile auf: die vorbereitenden Korpora vor dem 45. Präsidenten der Vereinigten Staaten und die vorbereitenden Korpora nach dem 45. Präsidenten. Danach trainieren wir sprachmodelle getrennt auf den beiden verschiedenen temporären Korpora.</sample>
    <sample id="182">Wir können sehen, dass Sprachmodelle allgemein eine politische Ausrichtung hatten, die weiter entfernt vom Zentrum ist, nach 2017. Das zeigt, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufgreifen können.</sample>
    <sample id="183">Zum Schluss, aber nicht zuletzt, bewerten wir Sprachmodelle mit unterschiedlichen politischen Bedeutungen im Hinblick auf die Erkennung von Hasssprache und Fake News-Detektion in zwei NLP-Anwendungen, die oft Sprachmodelle verwenden und möglicherweise sehr bedeutende Auswirkungen haben könnten.</sample>
    <sample id="184">Wir sehen also, dass wenn wir die Leistung pro Kategorie untersuchen, das heißt, wenn wir die Leistung in zwei Gruppen teilen, nämlich die Leistung, die bei den Männern und die Leistung, die bei den Frauen ist, dann ist die Leistung bei den Männern höher als die Leistung bei den Frauen.</sample>
    <sample id="185">Wir können unterschiedliche Demografiken oder politische Linien von Medien beobachten, bei denen wir ein Muster sehen, das beispielsweise für die Erkennung von Hasssprache gilt. Linksgerichtete Sprachmodelle sind besser.</sample>
    <sample id="186">Die Erkennung von Hasssprache, die auf sozialen Minderheitengruppen abzielt, ist ein wichtiger Aspekt der digitalen Sicherheit und des sozialen Medienkanals. Es ist wichtig, dass solche Inhalte erkannt und angemessen behandelt werden, um die Rechte und Würde aller Menschen zu schützen.</sample>
    <sample id="187">Allerdings sind wir schlecht darin, Hassrede zu erkennen, die sich auf mächtigere Gruppen in unserer Gesellschaft richtet.</sample>
    <sample id="188">Rechtlinien-Sprachmodelle sind besser in der Lage, Hasssprache zu erkennen, die sich auf Weiße und Männer richtet, jedoch schlechter in der Erkennung von Hasssprache, die sich auf schwarze, LGBTQ+-Plus und andere Minderheiten ausrichtet.</sample>
    <sample id="189">Ähnliche Trends tragen auch für die Erkennung von Falschinformationen bei, wo wir sehen, dass Sprachmodelle, die an der linken Seite der politischen Sprachlandschaft besser sind, missinformationen von ihrer gegenteiligen politischen Sprachlandschaft besser erkennen können und umgekehrt.</sample>
    <sample id="190">Wir werden weiterhin viele qualitativen Beispiele zeigen, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Bedeutungen haben.</sample>
    <sample id="191">Es gibt verschiedene Vorhersagen zu Hasssprache und Fehlinformationen, basierend auf ihren sozialen Kategorien. Es gibt eine Menge weiterer Beispiele im Anhang, um dies weiter zu betonen.</sample>
    <sample id="192">Dies zeigt, dass es ein sehr dringendes Problem mit der Fairness gibt, das sich auf die politischen Vorurteile von Sprachmodellen bezieht.</sample>
    <sample id="193">Zum Beispiel, wenn rechtsgerichtete Sprachmodelle auf Hate Speech oder Fehlinformationen fein abgestimmt werden sollen und auf eine beliebte Social-Media-Plattform bereitgestellt werden sollen.</sample>
    <sample id="194">Dies bedeutet, dass Menschen mit gegenteiligen politischen Meinungen möglicherweise marginalisiert werden und der Hassgespräch, der sich auf Minderheitengruppen richtet, möglicherweise ohne jegliche Kontrolle weiterhin zunimmt.</sample>
    <sample id="195">Daher hat dies den Alarm für uns signalisiert, die Fairness-Probleme zu erkennen und zu bekämpfen, die durch die politischen Implikationen des Sprachmodells entstehen.</sample>
    <sample id="196">Wir möchten auch betonen, dass wir das einzigartige Dilemma in Bezug auf Sprachmodellpolitische Biases aufzeigen. Es ist wie zwischen Silla und Kryptis.</sample>
    <sample id="197">Wenn wir politische Meinungen in Sprachmodell-Trainingsdaten nicht sanitisieren, würde der Bias von den vorgegebenen Daten auf Sprachmodellen übertragen und schließlich zu Ungleichheiten in den darunterliegenden Aufgaben führen.</sample>
    <sample id="198">Wenn wir versuchen, etwas zu reinigen, riskieren wir auch Zensur oder Ausgrenzung, und es ist unglaublich schwer, zu bestimmen, was tatsächlich neutral ist und was beibehalten werden sollte. Es ist also etwas wie das elektrische Schaltenproblem.</sample>
    <sample id="199">Okay, großartig. Ich denke, das ist ziemlich alles, was ich heute für dich tun kann. Vielen Dank für deine Zeit.</sample>
    <sample id="200">The paper "Printing Pattern from Translation: Assessing Strategies and Performance" is a joint work with my colleagues from Google Translate. Therefore, there are at least two authors involved in this work.</sample>
    <sample id="201">MPP-Auswertungen wurden durchgeführt, bis zu 2024 Token Kontextlänge.</sample>
    <sample id="202">The domains they have included in their dataset are:

1. The one with the piano music
2. The one without words
3. The one with the 12-year-old boy
4. The fictional one or comes from other bijan and so on.</sample>
    <sample id="203">Positionalität ist die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen entwickeln.</sample>
    <sample id="204">Da Wei</sample>
    <sample id="205">Yes, EDAtt can be used with an existing offline ST model without retraining or adopting a specific architecture for ST. It can be used with only one model for every latency regime and handle latency through specific parameters.</sample>
    <sample id="206">Es gibt mehrere Autoren an der Arbeit.</sample>
    <sample id="207">Yes, the tested model can perform well in the test suite.</sample>
    <sample id="208">Die drei Varianten von KITMUS sind: 1. Background pre-train, bei der das Hintergrundwissen vor dem Trainingszeitpunkt verfügbar ist; 2. Background both, bei dem das Hintergrundwissen sowohl vor als auch nach dem Trainingszeitpunkt verfügt; 3. Background inference, bei dem das Hintergrundwissenschaften nur nach dem Trainingszeitpunkt verfugbar ist.</sample>
    <sample id="209">Die Autoren gehören an der Universität von Wien.</sample>
    <sample id="210">The final research question is: How many clean samples do we need for WSL to work effectively, and should we only use the clean samples for validation or are there better ways to utilize them?</sample>
    <sample id="211">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, für die gleiche Aufgabe gleiche Ausgaben zu produzieren, unabhängig von leichten Variationen in der Formulierung der Anweisung.</sample>
    <sample id="212">Der/die Referent*in heißt **Jin Wei Yi**.</sample>
    <sample id="213">Eine höhere Sensitivität bedeutet, dass das Modell empfindlicher auf kleinere Veränderungen in den Daten reagiert und daher genauer ist. Dies kann zu einer besseren Leistung des Modells führen, da es weniger Fehler macht und genauere Vorhersagen liefert. Allerdings kann eine zu hohe Sensitivität auch dazu führen, dass das Modell zu viel auf Ausreißerwerte reagiert und dadurch weniger zuverlässige Ergebnisse liefert. Es ist wichtig, die Sensitivität des Modells zu optimieren, um eine gute Balance zwischen Genauigkeit und Robustheit zu erreichen.</sample>
    <sample id="214">Die Modelle erhalten während des Pre-Trainings einen linguistischen Kontext, der auf der Analyse und Generierung von Texten basiert. Sie werden mit großen Mengen an Textdaten aus verschiedenen Quellen trainiert, um Sprachmuster, Grammatik, Syntax und semantische Beziehungen zu erkennen und zu verstehen. Diese Kontexte helfen den Modellen, in der Lage zu sein, natürliche Sprache zu verstehen und zu generieren, indem sie die Struktur und die Bedeutung von Wörtern und Sätzen erfassen.</sample>
    <sample id="215">Normalerweise werden 20 saubere Validierungsbeispiele benötigt, um eine gute Leistung an der Wasserstoff-Lösungsgas-Systeme (WSL) zu erzielen.</sample>
    <sample id="216">Die Autoren gehören der Universität zu.</sample>
    <sample id="217">Es ist notwendig, neue Methoden zur Mes</sample>
    <sample id="218">The name of the presenter is Max Schäfer.</sample>
    <sample id="219">The pipeline for the propagation of political biases involves the following steps:

1. **Pre-training Data**: The process begins with pre-training data that may contain inherent social biases.
2. **Language Models**: These biased data are used to train language models, which can then propagate these biases.
3. **Downstream Tasks**: The biased language models are applied to downstream tasks, potentially leading to fairness issues.

This pipeline highlights the need to investigate and mitigate political biases in AI systems.</sample>
    <sample id="220">Der Vereinfachungsprozess zwischendem DEplain-apa und Web korpus unterscheidet sich in der Anzahl der verschiedenen Umformungen. Im DEplain-apa korpus gibt es viel mehr Umordnungen und Wortvarianten als im Web korpus. Im Web korpus gibt es viel mehr Paraphrasen.</sample>
    <sample id="221">Yes, Coscript is publicly available.</sample>
    <sample id="222">Das Wasserzeichen wird in den Text eingebettet, indem der Anbieter die Anzahl der Trigger in den eingegebenen Satz zählt und dann die gewünschte Anzahl von Wasserzeichen in den Text einfügt. Die Anzahl der Wasserzeichen ist proportional zur Anzahl der Trigger in dem Satz. Wenn die Anzahl der Trigger im Satz größer als eine bestimmte Anzahl ist, wird das Wasserzeichen genau in den Text eingebettet.</sample>
    <sample id="223">Penn State University.</sample>
    <sample id="224">Ja, Encoder-Decoder-Modelle wie mt-5 können durch Training mit einer Mischung von Sprachdaten verbessert werden.</sample>
    <sample id="225">Ein Beispiel für eingeschränkt Sprachplanung ist das Planen für spezifische Ziele wie das Backen eines Schokoladenkuchens.</sample>
    <sample id="226">Sie visualisieren die Embedding von Sätzen auf einem 4D-Datensatz, um die Konvertierbarkeit der vorgeschlagenen Embedding zu überprüfen. Die Legende der Figuren zeigt die Anzahl der Trigger in jedem Satz. Wie in den Figuren zu sehen ist, ist es schwierig, zwischen den Backdoor-Embeddings und den normalen Embeddings zu unterscheiden.</sample>
    <sample id="227">Die Arbeit nutzt bestehende PLMs, um ein Neues PLM aufzubauen, indem sie die vorhandenen Modelle und Daten nutzen, um neue Modelle zu trainieren und zu optimieren.</sample>
    <sample id="228">GPT-4 ist am wenigsten ausgerichtet auf Englischsprachige Länder.</sample>
    <sample id="229">And you can see an example on the right.</sample>
    <sample id="230">Die Anzahl der Aufgaben hat einen positiven Einfluss auf die Leistung des Modells. Je mehr Aufgaben vorhanden sind, desto besser wird das Modell in der Lage sein, die Aufgaben zu lösen. Gleichzeitig wird die Sensitivität des Modells geringer, was bedeutet, dass das Modell weniger anfällig für kleine Veränderungen in den Daten ist.</sample>
    <sample id="231">Die Autoren vergleichen ihre Methode mit drei baumlosen Baselines auf dem Cogs Benchmark. Diese Baselines sind:

1. **Tree-Less Models**: Basierte auf der Idee, dass die Modellierung von Daten ohne die Verwendung von Bäumen erfolgen soll. Diese Baselines verwenden unterschiedliche Ansätze, um die Struktur der Daten zu erfassen, ohne die Komplexität und die Rechenaufwand, die mit Bäumen verbunden sind.

2. **Graph Neural Networks (GNNs)**: Diese Baselines nutzen Graph-Neural-Netzwerke, die speziell für die Analyse von Graphenstrukturen entwickelt wurden. GNNs sind in der Lage, die Beziehungen zwischen den Knoten in einem Graphen zu erfassen und zu nutzen, um die Modellierung zu verbessern.

3. **Recurrent Neural Networks (RNNs)**: Basierte auf der Idee, dass Daten in einer Sequenz vorliegen und die Modellierung auf die Verwendung von Recurrenten Netzwerken basiert. RNNs sind in der Lage, die zeitliche Abhängigkeit in Daten zu erfassen und zu nutzen, was sie besonders nützlich für die Modellierung von Sequenzen ist.

Diese Baselines dienen dazu, die Leistung der Autorens Methode im Vergleich zu anderen Methoden zu bewerten und zu zeigen, dass ihre Methode in der Lage ist, die Daten effizienter zu modellieren als diese Baselines.</sample>
    <sample id="232">Die beiden Co-Autoren, Alexander Köller und Ivan Titov, stehen in einer kollegialen Beziehung zum ersten Autor. Sie arbeiten gemeinsam an der Publikation, was bedeutet, dass sie ihre Expertise und Erfahrungen in der Forschung und in der Publikationstechnik kombinieren, um ein gemeinsames Projekt zu erstellen. Diese Zusammenarbeit erfordert eine gute Kommunikation und Kooperation, um sicherzustellen, dass alle Beiträge des ersten Autors korrekt und konsistent mit den anderen Autoren abgebildet werden. Es ist auch wahrscheinlich, dass sie sich in der Forschungseinrichtung oder in einem gemeinsamen Projekt befinden, was ihre Zusammenarbeit erleichtert.</sample>
    <sample id="233">Google.</sample>
    <sample id="234">Hallo alle, ich bin Jenny, ein erster Jahrgangsphd-Studentin an der Carnegie Mellon University, und heute werde ich meine Arbeit über die Analyse von Positionierungsdaten präsentieren, die durch Sensoren erfasst wurden.</sample>
    <sample id="235">Diese Arbeit wurde in Zusammenarbeit mit einigen Personen an der Universität Washington und dem Allen Institute für Künstliche Intelligenz, nämlich Sebastian Santi, Ronan Le Bras, Caterina Rini, und Martin Sapp, durchgeführt.</sample>
    <sample id="236">Lass uns mit der Vorstellung beginnen, dass du für eine Zeitung arbeitest und unter deinem Artikel nach Kommentaren suchst, um toxischen Inhalt zu entfernen.</sample>
    <sample id="237">Sie könnten sich auf eine beliebte API wie die Perspektive-API für Toxizitätserkennung verlassen, und das funktioniert wirklich gut, wenn Sie Carl Jones sind, um die Perspektive-API in der Lage ist, korrekt toxische Fälle zu erkennen.</sample>
    <sample id="238">Aber das ist nicht wirklich der Fall für die Sharmas, wo perspektivische APIs wirklich nicht so empfindlich auf beleidigende Begriffe sind, die in indischen Kontexten häufiger sind.</sample>
    <sample id="239">Dies ist ein Beispiel für einen Design-Bias, bei dem wir systematische Leistungsunterschiede in Technologie zwischen verschiedenen Bevölkerungsgruppen beobachten.</sample>
    <sample id="240">Design biases, like the one we just saw before, might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="241">Dies ist ein Konzept, das in kritischen Studien weit verbreitet ist, insbesondere in feministischen und queer akademischen Bereichen.</sample>
    <sample id="242">Als Forscher kann die Positionierung die Forschungsprozess und deren Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher ändern kann.</sample>
    <sample id="243">Und so eine Frage, die Menschen möglicherweise stellen, ist: Haben Datensätze und Modelle Positionierung?</sample>
    <sample id="244">Wir versuchen nicht zu sagen, dass Modelle und Daten selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von Menschen und können somit bestimmte Positionen gegenüber anderen darstellen.</sample>
    <sample id="245">Die vorherige Arbeit schlägt einige anekdotisches Beweis für die Positionierung vor, wie zum Beispiel kulturelle Lücken und Modelle und Datensätze sowie theoretische Definitionen von Modellpositionierung.</sample>
    <sample id="246">Jedoch untersuchen diese Arbeiten tatsächlich nicht, wie Endbenutzer mit den Datensätzen und Modellen selbst verglichen werden.</sample>
    <sample id="247">Die Studienmodelle und Datensets in der Positionierung werden immer wichtiger, da NLP-Aufgaben zunehmend subjektiver und sozialorientierter werden.</sample>
    <sample id="248">Es ist schwierig, wie diese Positionen skizziert werden, weil nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind.</sample>
    <sample id="249">Um die Positionierung von Datensätzen und Modellen zu untersuchen, vergleichen wir tatsächlich die Annotationen mit reellen Benutzern, die mit bestehenden Datensätzen und Modellen arbeiten.</sample>
    <sample id="250">Wir machen dies durch unsere Framework-NL-Positionierung.</sample>
    <sample id="251">Der englische Text besagt, dass unser Framework zwei Hauptstufen umfasst.</sample>
    <sample id="252">Der erste Schritt besteht darin, Datenmengen mit verschiedenen Annotatoren zu reanotieren.</sample>
    <sample id="253">Wir sollten dies übersehen, indem wir die Demografie der ursprünglichen Datensätze betrachten, um Annotatoren zu verwenden, weil in der Regel nur einige Annotatoren jeden Datensatz annotieren und weil Demografien selten gesammelt und geteilt werden.</sample>
    <sample id="254">Und so wollen wir Daten erneut annotieren, um viele Annotierungen für ein bestimmtes Beispiel zu erhalten und um ein reiches Set an demografischen Daten zu erhalten.</sample>
    <sample id="255">Wir nehmen dann die Annotations nach dem demografischen Profil und vergleichen sie mit den Modellen und Datensätzen, indem wir Pearson's R-Korrelationsscore verwenden.</sample>
    <sample id="256">"Und so unterscheidet sich unser Rahmen von der Literatur über Annotator-Disagreements, indem er Endbenutzer mit Modellen und Datensätzen, Vorhersagen und Labels vergleicht, anstatt nur auf die Annotator-Agreement oder Modellierung zu achten.

Annotator-Verteilungen"</sample>
    <sample id="257">Unser Framework wird hauptsächlich durch "Lab in the Wild", ein Online Crowdsourcing-Plattform, ermöglicht, die vorherige HCI-Kollaboratorin.</sample>
    <sample id="258">"Live in the Wild" ist eine Online-Experimentationsplattform, bei der wir verschiedene Freiwillige einstellen können, im Vergleich zu Plattformen wie mTurk, die hauptsächlich Teilnehmer aus den USA oder Indien haben. Darüber hinaus ist "Live in the Wild" immer noch in der Lage, hochwertige Daten zu erhalten.</sample>
    <sample id="259">Wir bieten auf Lab in the Wild zwei Aufgaben an, von denen eine die soziale Akzeptabilität betrifft. Der Ablauf ist so, dass die Teilnehmer eine Situation aus dem Social Chemistry-Datensatz lesen und dann bewerten, wie sozial akzeptabel diese Situation ist.</sample>
    <sample id="260">Nachdem sie die Studie beendet haben, können sie ihre Antworten mit denen einer KI und anderen vergleichen, um engagiert zu bleiben.</sample>
    <sample id="261">Wir haben dann diese Anmerkungen mit social chemistry, Delphi und gpt-4 verglichen.</sample>
    <sample id="262">Wir haben dann eine sehr ähnliche Einrichtung für die Aufgabe der Toxizität und Hasssprachdetektion vorgenommen, bei der sie einen Satz aus dem Datensatz "dani hate" lesen und schreiben, ob sie den Satz als Beispiel von Hasssprache betrachten.</sample>
    <sample id="263">Wir vergleichen dann diese Annotations mit DynaHate, Perspective API, Rewire API, Hate Roberta und GPT-4. Unsere Studie endete mit über 16.000 Annotations von über 1.000 Annotatoren aus 87 Ländern.</sample>
    <sample id="264">So, now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP.</sample>
    <sample id="265">English	for example, we find that datasets and models are most aligned to English-speaking countries. So for the gpd4 social acceptability analysis, we find that it's most aligned to Confucian and English-speaking countries. We find that dinah hate is also most aligned to English-speaking countries.</sample>
    <sample id="266">Wir finden auch die meisten zusätzlichen Übereinstimmungen mit Personen, die eine akademische Ausbildung haben. Also für gpt-4 in der sozialen Akzeptabilität Aufgabe finden wir, dass es am meisten mit Personen in Verbindung steht, die eine akademische AusbILDUNG haben.</sample>
    <sample id="267">Wir finden auch für die Hate, die am meisten an Menschen mit einer akademischen Ausbildung orientiert ist.</sample>
    <sample id="268">Jedoch wenn Modelle und Datensätze auf bestimmte Bevölkerungsgruppen abgestimmt sind, werden einige unvermeidlich hinterlassen.</sample>
    <sample id="269">Ein Beispiel dafür ist, dass Daten und Modelle weniger an die Nicht-Binäre angepasst sind, im Vergleich zu den männlichen und weiblichen Gegenstücken. Wir finden das im GPT-4-Social-Acceptability-Task und auch im Dienstleistungsauftrag.</sample>
    <sample id="270">Es gibt eine Position in einer LAD-NLP, was bedeutet, dass eine Person in einer bestimmten Situation oder Umgebung ist, die bestimmte Verhaltensweisen oder Denkmuster zeigt. Diese Position kann durch verschiedene Faktoren beeinflusst werden, wie beispielsweise die Umgebung, die Personen, die sie umgeben, und die Art und Weise, wie sie sich selbst verhält.

Um diese Position zu verstehen und zu beeinflussen, gibt es verschiedene Techniken und Strategien, die in der LAD-NLP angewendet werden können. Eine der wichtigsten Techniken besteht darin, die Persönlichkeit der Person zu verstehen und ihre Verhaltensweisen zu analysieren. Dies kann durch verschiedene Methoden erreicht werden, wie beispielsweise die Analyse von Verhaltensmustern, die Untersuchung von Persönlichkeitstests oder die Analyse von Kommunikationsstilen.

Ein weiterer wichtiger Aspekt der LAD-NLP ist die Veränderung von Verhaltensweisen und Denkmustern. Dies kann durch verschiedene Techniken erreicht werden, wie beispielssohn die Verwendung von Affirmationen, die Verwendung von Hypnose oder die Anwendung von Selbsthypnose.

Insgesamt ist die LAD-NLP ein wertvolles Werkzeug, das dazu beiträgt, die Persönlichkeit und Verhaltensweisen von Personen zu verstehen und zu beeinflussten. Durch die Anwendung verschiedener Techniken und Strategien kann man die Position in einer LAD-NLP verändern und die Person in eine positivere und produktivere Richtung leiten.</sample>
    <sample id="271">Sure, I'd be happy to help! Here are the recommendations for the given text in German:

1. "Keep a record of all relevant design choices throughout the research process."

2. "Do NLP research with the lens of perspectivism."

I hope this helps! Let me know if you have any other questions.</sample>
    <sample id="272">Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb vier bestimmter Gemeinschaften aufzubauen. Ein gutes Beispiel dafür ist die Musikani-Initiative. Wir möchten betonen, dass eine inklusive NLP nicht nur bedeutet, dass alle Technologien für jeden funktionieren.</sample>
    <sample id="273">Und so beendet unsere Präsentation, aber wenn Sie mehr erfahren möchten, können Sie gerne auf unsere Dashboards zurückgreifen, um die aktuellsten Analyseergebnisse und unsere Arbeit zu finden. Vielen Dank.</sample>
    <sample id="274">Die Referentin geht auf drei Probleme von SimulST ein.</sample>
    <sample id="275">Soziale und politische Verzerrungen können effektiv reduziert werden, indem man die Datensätze sorgfältig auswählt und vorsichtig mit den Daten arbeitet, um die Gefahr von Verzerrungen zu minimieren.</sample>
    <sample id="276">Hallo, ich bin Si Yuyuan von der Fudan-Universität. Ich bin hier, um unsere Arbeit vorzustellen: Distilling Script Knowledge from Large Language Models for Constrained Language Planning.</sample>
    <sample id="277">In everyday life, humans often plan their actions by following step-by-step instructions in the form of granted scripts.</sample>
    <sample id="278">The previous work has explored language models to plan for abstract goals of stereotypical activities, such as making a cake, and shows that large language models can effectively decompose goals into steps.</sample>
    <sample id="279">"However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific constraints, such as making a chocolate cake, still remains understudied."</sample>
    <sample id="280">In this paper, we define the problem of constrained language planning.</sample>
    <sample id="281">Gib den englischen Inhalt sinne gemäß auf Deutsch wieder.</sample>
    <sample id="282">In diesem Papier bewerten und verbessern wir zunächst die Einschränkungsfähigkeit der Sprachplanung von großen Sprachmodellen.</sample>
    <sample id="283">Da keine Daten zur spezifischen Zielsetzung unserer Studie vorliegen,</sample>
    <sample id="284">Wir müssen diese Ziele zuerst erreichen. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit multifaktorischen Einschränkungen, um die humanen in der Log-Daten-Akquisition zu instruieren.</sample>
    <sample id="285">Wir haben 100 spezifische Ziele ausgewählt und bewertet die von großen Sprachmodellen generierten Skripte.</sample>
    <sample id="286">Diese Tabelle berichtet über die Gesamtrechnung der Ergebnisse. Wir stellen fest, dass alle natürlichen Sprachmodelle unzufriedenstellende Ergebnisse bei der Planung für spezifische Ziele erzielen.</sample>
    <sample id="287">Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, warum lineare Modelle fehlen.</sample>
    <sample id="288">Die Ergebnisse in der Abbildung zeigen, dass die semantische Komplexität in generierten Skripten akzeptabel ist, aber die Treue an die Einschränkungen kann nicht garantiert werden.</sample>
    <sample id="289">Wir gehen in die mehrfach gegliederten Topik-Kategorien von Einschränkungen ein, die in Wiktionar definiert sind. Die Hitmap in der Abbildung zeigt, dass die Planungseffizienz von Instr-GPDs für verschiedene Kategorien von Zielen erheblich variiert.</sample>
    <sample id="290">Zuvor haben Studien gezeigt, dass die Ausgabequalität von Sprachmodellen in hohem Maße variiert, was zu schlechtem Leistungsverhalten führt. Daher haben wir die Idee eines übergenerierten Zwischenfilters eingeführt, um die Generationsqualität zu verbessern.</sample>
    <sample id="291">Wir zeigen zuerst die Constraint-Typen mit Beispielen für die Injektions-GPT und erhalten spezifische Ziele basierend auf den seitigen abstrakten Zielen.</sample>
    <sample id="292">Dann befehligt GPT, über generierte Kinder-Skripte für bestimmte Ziele hinauszugehen.</sample>
    <sample id="293">Nächst wird ein Filtermodell entwickelt, um die passenden Skripte zu wählen.</sample>
    <sample id="294">Wir konvertieren Skripte und Geschichten in instruktive GPT-Embeddings und berechnen Kosinusähnlichkeit und Ähnlichkeits-Scores, um semantische Ähnlichkeit zu messen.</sample>
    <sample id="295">In addition, we award the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set.</sample>
    <sample id="296">"Unser Verfahren kann hochwertige Sequenzen von DNA erzeugen, indem es die Planbarkeit sowohl in Bezug auf semantische Vollständigkeit als auch an die Einschränkungen erheblich verbessert.</sample>
    <sample id="297">Da große Sprachmodelle teuer zu bereitstellen sind, ist es wichtig, die Sprachplanungsfähigkeit von kleineren und spezialisierten Modellen zu verbessern. Die Erstellung eines Datensatzes ist ein wesentlicher Schritt zu diesem Ziel.</sample>
    <sample id="298">Allerdings können frühere Studien nicht für spezifische Ziele planen, und die manuelle Datensatz-Erstellung ist teuer.</sample>
    <sample id="299">Zusammenfassend können wir sagen, dass wir die Idee der symbolischen Wissensverdichtung verfolgen, um eingeschränkte Sprachplanungsdatensätze von großsprachigen Modellen zu distillieren.</sample>
    <sample id="300">Wir werden unsere Methode für das Erstellen eines Datensatzes von kontrainten Sprachplanung anwenden, der als "CodeScript" bezeichnet wird.</sample>
    <sample id="301">Sure, I'd be happy to help! Here's the translation of the text you provided:

"In total, we generate 55,000 specific goals with scripts. To ensure the quality of validation and test sets, we ask crowd-sourced workers to find and revise the incorrect samples."

I hope this helps! Let me know if you have any other questions.</sample>
    <sample id="302">Diese Abbildung zeigt die Einschränkungsverteilung von CodeScript. Wir finden, dass CodeScript eine hohe Plausibilität in den generierten spezifischen Zielen zeigt. Mit CodeScript können wir kleinere, aber spezialisierte Modelle für Einschränkungs-Sprachplanung verwenden.</sample>
    <sample id="303">Wir fanden heraus, dass die T5-Funktion auf der Code-Rate die Skripte von höherer Qualität als die meisten großen Sprachmodelle generieren kann, was darauf hinweist, dass kleinere Modelle größere Modelle übertragen können, wenn sie auf geeignete Datensätze trainiert werden.</sample>
    <sample id="304">In summary, we established the constrained language planning problem, evaluated the constrained language planning ability of large language models, and developed an overgeneration filter method for large language models.</sample>
    <sample id="305">Wir verwenden große Sprachmodelle, um ein hochwertiges Skript-Datensatz, codscript, für die Sprachplanung zu generieren. Wir hoffen, dass das codscript-Datensatz ein wertvoller Ressource sein wird, um die Forschung auf Sprachplanung voranzutreiben.</sample>
    <sample id="306">Danke für Ihre Zeit. Bitte finden Sie weitere Details zu CodeScript in unserer Arbeit.</sample>
    <sample id="307">The fluency of PaLM is comparable to state-of-the-art systems, but the main difference comes from the accuracy.</sample>
    <sample id="308">Die wichtigsten Eigenschaften einschließlich Anwendbarkeit auf Embedding-as-Services, Nicht-Einschränkung der Nutzung der bereitgestellten Embeddings, Ausreichende Versteckbarkeit für Angreifer und Transportierbarkeit während des Modell-Extraktionsprozesses.</sample>
    <sample id="309">The English TED Talks were translated into fourteen different languages.</sample>
    <sample id="310">Um die Anzahl der Instanzen, die aus einem Datensatz für die erneunte Annotierung extrahiert werden, müssen wir die Anzahl der Instanzen in dem Datensatz zählen, die für die erneunte Annotierung extrahierte werden. Dies kann durch Analyse des Datensatzes erreicht werden, indem wir die Anzahl der Instanzen mit verschiedenen Annotatoren zählen und dann die Anzahl der Instanzen, für die mehr als eine Annotatorin annotiert wurde, bestimmen. Diese Anzahl kann dann für die erneunte Annotierung verwendet werden.</sample>
    <sample id="311">Delta cosine and delta l2.</sample>
    <sample id="312">In dieser Aufgabe wurden Modelle, die auf einem mehrprachigen Encoder basieren, in zwei Gruppen eingeteilt: Encoder-PDR (multilingual pre-trained encoders with pointer-based decoders) und Encoder-Decoder-Modelle (multilingual pre-trained encoder-decoder models). Die Encoder-PDR-Modelle wurden auf zwei Gruppen von Modellen evaluiert, einschließlich Encoder-PDR, XLM-R+PDR und Bert+PDR. Die Encoder-Decoder-Modelle wurden auf zwei Grupen von Modellen evaluiert, eins schließlich M-BART und M-T5. Die Ergebnisse zeigten, dass Encoder-Decoder-Modelle die beste Leistung auf allen neun Datensätzen erzielten.</sample>
    <sample id="344">Die Autoren wählen Wörter mit mittlerer Häufigkeit aus, indem sie eine Gruppe von Wörtern auswählen, die in einem moderaten Frequenzintervall vorkommen. Sie gehen davon aus, dass der Anbieter eine allgemeine Textkorpusse sammeln und die Worthäufigkeit dieser korpusse zählen kann.</sample>
    <sample id="345">Hallo alle, mein Name ist Zhu Heng. Heute werde ich unsere Arbeit präsentieren: "Wie funktionieren die 2003-namigen Entitäts-Tags noch in 2023?" Lass uns anfangen.</sample>
    <sample id="346">Unser Papier untersuchte das Problem der Generalisierung unter Verwendung der bekannten Entitätserkennung Aufgabe oder der NER-Aufgabe.</sample>
    <sample id="347">Wir beobachten, dass Modelle seit 2003 mit dem Konzept der Neuronalen Erweiterung (Neural Expansion) entwickelt wurden und dies natürlich mehrere Probleme aufwirft. Zunächst kann diese Art von Modellen modernen Daten generalisieren?</sample>
    <sample id="348">Wenn wir neue Tags entwickeln, ist es wichtig, dass sie gut generalisieren. Das bedeutet, dass sie nicht nur auf bestimmte Daten angewiesen sind, sondern auch auf neue, ungeahnte Daten, die möglicherweise nicht in der Trainingsdatenbasis enthalten sind. Um dies zu erreichen, müssen wir sicherstellen, dass die Tags auf einer breiten Palette von Daten basieren und dass sie nicht zu spezifisch oder zu allgemein sind. Wir müssen auch sicherstellen, dass die Tags klar und präzise definiert sind, damit sie einfach verstanden werden können.</sample>
    <sample id="349">Zur gleichen Zeit, wenn wir schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle?</sample>
    <sample id="350">Um diese Probleme zu untersuchen, haben wir das Kono++-Datenset entwickelt. Dies ist ein Datenset, das wir aus dem Reuters-Nachrichten aus dem Jahr 2020 gesammelt haben und dann mit den gleichen Kono-2003-Anmerkungsrichtlinien annotiert haben.</sample>
    <sample id="351">Wir haben dann über zwanzig Modelle auf Kono 2003 fein abgestimmt. Wir haben sie auf beiden Kono 03 Testsets und auf dem Kono++ Testset bewertet.</sample>
    <sample id="352">Zum Schluss haben wir den Prozentsatz der Änderung in F1 berechnet, um die Generalisierungsfähigkeit jedes Modells zu bewerten.</sample>
    <sample id="353">So, what is needed for a good generalization? Through our experiments, we found that there are three main ingredients that are needed.</sample>
    <sample id="354">Der erste ist die Modellarchitektur. In unseren Experimenten stellten wir fest, dass die Transformer-Modelle normal besser generalisieren, um neue Daten zu verarbeiten.</sample>
    <sample id="355">Der zweite Bestandteil ist die Modellgröße. Wir haben gefunden, dass in der Regel größere Modelle zu besserer Generalisierung führen.</sample>
    <sample id="356">Zuletzt, aber nicht zuletzt, wissen wir alle, dass die Anzahl an Feinabstimmungsbeispielen direkt die Leistung einer Downstream-Aufgabe beeinflusst. Hier haben wir auch gefunden, dass mehr Feinabstimmungsbeispiele tatsächlich auch zu besserer Generalisierung führen.</sample>
    <sample id="357">Um die Leistungsabnahme einiger Modelle zu erklären, gibt es verschiedene Faktoren, die dazu beitragen können. Hier sind einige mögliche Ursachen:

1. **Hardware-Probleme**:
   - **Fehlerhafte Hardware**: Mangelhafte oder beschädigte Hardware, wie z.B. RAM, CPU oder Festplatte, kann zu Leistungsproblemen führen.
   - **Overheating**: Überhitzung kann die Hardware beschädigen und die Leistung einschränken.

2. **Software-Probleme**:
   - **Fehlende Updates**: Mangelhafte oder veraltete Softwareupdates können zu Leistungsproblemen führt.
   - **Virus oder Malware**: Malware kann die Leistung des Systems beeinträchtigen.
   - **Fehlende Treiber**: Mangelhafte oder veraltete Treiber können zu Leistungsproblemen führten.

3. **Systemeinstellungen**:
   - **Energieeffizienzmodi**: Einstellungen wie der Energieeffizienzmodus können die Leistung einschränken, um Energie zu sparen.
   - **Vorhandene Prozesse**: Viele Prozesse im Hintergrund können die Leistung des Systems beeintrüchtigen.

4. **Hardware-Konfiguration**:
   - **Unzureichende Hardware**: Unzureichende Hardware, wie z.B. wenig RAM oder ein schwacher CPU, kann die Leistung einschränken und zu Leistungsproblemen führend.

5. **Software-Konfiguration**:
   - **Fehlende Optimierung**: Fehlende Optimierung der Software kann zu Leistungsproblemen führen.
   - **Fehlende Kompatibilität**: Fehlende Kompatibilität mit bestimmten Hardware- oder Softwarekomponenten kann die Leistung beeinträchtigen.

6. **Netzwerkprobleme**:
   - **Fehlendes Netzwerk**: Fehlendes Netzwerk kann die Leistung einschränkt, insbesondere bei Modellen, die auf Netzwerkleistung angewiesen sind.
   - **Fehlende Bandbreite**: Mangel an Bandbreite kann die Leistung beeinträchten, insbesondere bei Modellen, bei denen die Leistung von der Bandbreite abhängt.

7. **Fehlerhafte Treiber**:
   - **Fehlende oder fehlerhafte Treiber**: Mangelhafte oder fehlerhafte Treiber können zu Leistungsprobleme führen.

8. **Fehlende Hardware- oder Software-Update**:
   - **Fehlende Updates oder Updates, die nicht korrekt installiert wurden**: Mangel an Updates oder fehlende Updates können zu Leistungsproblemen fuhren.

9. **Fehlende Hardware- oder Software-Kompatibilität**:
   - **Fehlende Kompatibilität mit bestimmten Hardware- oder Softwarekomponen**: Fehlende Kompatibilität kann die Leistung beeinträchten.

10. **Fehlende Hardware- oder Software-Einstellungen**:
    - **Fehlende Einstellungen oder fehlende Einstellungen**: Fehlende Einstellungen oder fehlendes Einstellen kann die Leistung beeinträchen.

11. **Fehlende Hardware- oder Software-Sicherheitsupdates**:
    - **Fehlende Sicherheitsupdates**: Fehlende Sicherheitsupdates können zu Leistungsproblemen und Sicherheitsrisiken führen.

12. **Fehlende Hardware- oder Software-Benutzerfreundlichkeit**:
    - **Fehlende Benutzerfreundlichkeit**: Fehlende Benutzerfreundlichkeit kann die Leistung beeinträ</sample>
    <sample id="358">Wir hatten zwei Hypothesen. Der erste ist adaptives Überfitten, was Überfitten ist, das durch die Wiederholung des gleichen Testsets entsteht, und dies tritt normalerweise dann auf, wenn die Abnahmewerte auf einem neuen Testset sinken.</sample>
    <sample id="359">Der zweite Hypothese ist der temporale Drift, der die Leistungsdegradation ist, die durch den zunehmenden temporalen Abstand zwischen dem Trainingsdatensatz und den Testdaten verursacht wird.</sample>
    <sample id="360">Für die Anpassung über Anpassung haben wir gesehen, dass die rote beste Anpassungslinie auf der rechten Grafik einen Gradienten hat, der größer als eins ist.</sample>
    <sample id="361">Das bedeutet, dass jede Verbesserungseinheit, die wir auf Konsole 2003 gemacht haben, auf Konsole + + mehr als eine Verbesserungseinheit umsetzt. Das bedeutet, dass es keine abnehmenden Renditen gibt.</sample>
    <sample id="362">Dies zeigt uns, dass adaptives Überfitten in diesem Fall nicht beobachtet wird.</sample>
    <sample id="363">So what about the temperature of that?</sample>
    <sample id="364">Um den temporären Abweichungen zu begegnen, haben wir eine Experimente durchgeführt, um einige Modelle zu weiter trainieren oder fortzufahren, mit neueren Daten. Wir haben gefunden, dass die Leistung mit größeren temporären Abweichungen abnimmt.</sample>
    <sample id="365">Dies bestätigt unsere Hypothese, dass der Hauptgrund für den Leistungsabfall die temporale Drift ist.</sample>
    <sample id="366">Unser Schlussfolgerung ist, dass wir für eine gute Generalisierung ein besseres Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiele benötigen. Diese Ziele gehen Hand in Hand, wir können nicht nur einen bestimmten Bestandteil haben, sondern durch alle anderen.</sample>
    <sample id="367">Zugleich stellten wir auch fest, dass der Leistungsabfall hier durch temporäre Abweichungen verursacht wird und eher überraschend ist, dass er nicht durch adaptives Überfitten verursacht wird, obwohl Cono 2003 seit über zwanzig Jahren verwendet wird.</sample>
    <sample id="368">So going back to the question that we raised in the title of our paper, do kernel 2003 taggers still work in 2023? And we found that the answer is actually a resounding yes.</sample>
    <sample id="369">Wir hoffen, dass unser Papier Anlass für weitere Forschung zur Verbesserung der Generalisierungsfähigkeiten der Modelle bietet.</sample>
    <sample id="370">Und letztendlich, bitte stellen Sie sicher, dass Sie unsere Arbeit, unsere Datenbank und, falls Sie Fragen haben, sich jederzeit bei mir melden. Vielen Dank so viel.</sample>
    <sample id="397">Die Sprachsegmentgröße, die bei dem Ansatz verwendet wird, beträgt 128 Tokens.</sample>
    <sample id="398">Servin ist ein Richter.</sample>
    <sample id="399">Der wichtigste Faktor zwischen Qualität des Beispiels und Ähnlichkeit mit dem Ausgangssatzes ist die Qualität des Beispiels.</sample>
    <sample id="400">Die Arbeiten in den erweitertenen Experimenten konzentrieren sich auf die folgenden Sprachmodelle:

1. **GPT-4**: Dies ist das am meisten liberalen Sprachmodell der Gruppe.
2. **GPT-Serie**: Diese Modelle sind im Allgemeinen sozial liberaler als die BERT-Serie und ihre Varianten.

Zweitens wird untersucht, **bis zu welchem Grad die politischen Verzerrungen von Sprachmodellen tatsächlich aus dem Trainingsdaten aufgenommen werden**. Dazu wird ein kontrolliertes Experiment durchgeführt, bei dem die Sprachmodell-Checkpoints weiter vorbereitet werden, um die politischen Korpora in News und Social Media zu trennen und in ihre politischen Linien zu teilen.</sample>
    <sample id="401">Das Modell kombiniert Werte aus mehreren Ebenene bzw. Ebene.</sample>
    <sample id="402">Beispiele für direkte Inferenz sind:

1. "The name of the song is 'YMCA'."
2. "The song is in the first position."</sample>
    <sample id="403">Die Autoren gehören an der Fudan-Universität.</sample>
    <sample id="404">Eine Person (Yanis Labbé) ist an der Arbeit beteiligt.</sample>
    <sample id="405">Yes, the translation of the natural language query was considered as a baseline before the semantic parsing.</sample>
    <sample id="406">The authors have given the example of a woman warrior.</sample>
    <sample id="407">Die Transformer-Modelle generalisieren nicht gut.</sample>
    <sample id="408">The test datasets are called clean data.</sample>
    <sample id="409">Zwei Autoren sind an der Arbeit beteilt.</sample>
    <sample id="410">The authors of the work are investigating whether instruction tuning on multimodal pre-trained models can actually improve generalization to unseen multimodal tasks. They are using multiple modalities, including text, images, and videos, to train their models. The authors are also using a variety of techniques, such as data augmentation and transfer learning, to improve the performance of their models.</sample>
    <sample id="439">Das zu wenig erforschte Gebiet im Bereich der natürlichen Sprachverarbeitung (NLU) ist die Integration und Nutzung beider Prä-Trainingszeiten und -Wissen sowie der Inferenzzeiten.</sample>
    <sample id="440">The referents are Yin and Zhiyang.</sample>
    <sample id="441">Yes, Coscript has undergone quality control.</sample>
    <sample id="442">Grenzen bestehender Ressourcen:

1. **Begrenzte Typen von kontextabhängigen Übersetzungen**:
   - Ressourcen unterstützen nur bestimmte Arten von kontextabhängigen Übersetzungsanforderungen, die nicht alle möglichen Kontexte abdecken.

2. **Begrenzte Sprachmengen**:
   - Ressourcen sind oft auf eine begrenzte Anzahl von Sprachen beschränkt, was die Verbreitung und Anwendung in vielen Sprachregionen einschränkt.

3. **Abhängigkeit von Domain-Knowledge und menschlicher Kuration**:
   - Ressourcen setzen häufig auf die Domain-Knowledge und menschliche Kuration, was die Skalierbarkeit und Automatisierung erschwert.

4. **Fehlende Allgemeinverfügbarkeit**:
   - Ressourcen sind nicht immer allgemein zugänglich oder kostengünstig, was die Verfügbarkeit für alle Benutzer einschränkt.

5. **Fehlende Anpassungsfähigkeit**:
   - Ressourcen können schwer an neue Kontexte oder Anforderungen angepasst werden, was die Flexibilität und Relevanz einschränkt.

6. **Fehlende Integration mit anderen Systemen**:
   - Ressourcen können oft nicht reibungslos mit anderen Systemen oder Plattformen integriert werden, was die Effizienz und Effektivität beeinträchtigt.

7. **Fehlende Transparenz und Erklärbarkeit**:
   - Ressourcentechnologien können oft nicht transparent oder erklärbare Entscheidungen treffen, was die Vertrauenswürdigkeit und Akzeptanz beeinträchtigt.

8. **Fehlende Skalierbarkeit**:
   - Ressourcentechnologien können schwer skaliert werden, um die wachsenden Anforderungen zu erfüllen, was die Effizienz und Kosteneffizienz beeinträchtigt.

9. **Fehlende Interaktion mit Benutzern**:
   - Ressourcen können nicht interaktiv sein oder Benutzern die Möglichkeit geben, ihre Anforderungen zu ändern oder zu personalisieren, was die Benutzererfahrung beeinträchtigt.

10. **Fehlende Fortschritte in der Technologie**:
    - Ressourcentechnologien sind oft auf bestehende Technologien beschränkt und können nicht mit neuen Technologien integriert werden, was die langfristige Entwicklung behindert.

Zusammenfassend sind die Grenzen bestehender Ressource für kontextabhängige Übersetzung in ihrer Begrenzung auf bestimmte Typen von Übersetzungsanforderungen, begrenzten Sprachmengen, Abhängigkeit von Domain-Knowledge, Menschlicher Kuration, fehlender Allgemeinverfügbarkeit, Anpassungsfähigkeit, Integration, Transparenz, Skalierbarkeit, Interaktion und Fortschritte in der Technologie.</sample>
    <sample id="443">Hallo und ich werde über unsere Arbeit sprechen, die sich auf die Lösung von indirekten Verweisausdrücken für die Entitätsauswahl konzentriert. In dieser Arbeit haben wir die Alternativenkorpusse eingeführt.</sample>
    <sample id="444">Mein Name ist Javad Hosseini und dies ist eine gemeinsame Arbeit mit Philipp Radlinski, Sylvia Parity und Annie Lewis.</sample>
    <sample id="445">Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Entscheidung treffen möchten. Überlegen Sie sich diese alternative Frage: bedeutet "einfach für mich" oder "ich habe ein Gefühl"? Hier möchte ein Benutzer zwischen zwei Songs wählen.</sample>
    <sample id="446">The most obvious thing is to use a direct reference, for example, by saying the name of the song is in me or its position, the first one.</sample>
    <sample id="447">Auf Deutsch könnte man sagen: "Aber manchmal ist ein indirekter Hinweis passender, um eine natürlichere Konversation zu haben. Dies könnte passieren, wenn der Benutzer den Namen des Songs nicht mehr erinnert."</sample>
    <sample id="448">Alle Aussprachen sind zu sehr ähnlich voneinander und schwer zu unterscheiden.</sample>
    <sample id="449">Als KI-Assistent habe ich keinen Zugriff auf die aktuelle Zeit oder die aktuellen Nachrichten. Daher kann ich keine aktuellen Beispiele oder Informationen geben. Ich kann jedoch Ihnen allgemeine Informationen über die Verwendung von "or" in der Sprache geben.

In der Sprache gibt es verschiedene Verwendungen von "or". Hier sind einige Beispiele:

1. "Or" wird verwendet, um zwei oder mehr Optionen zu vergleichen. Zum Beispiel: "Ich möchte entweder Pizza oder Burger essen."
2. "Or" wird verwendet, wenn man eine Alternative oder eine andere Möglichkeit anbietet. Zum Beispiel: "Wir können entweder ins Kino gehen oder ins Museum."
3. "Or" wird verwendet, als Konjunktion, um zwei oder mehr Aussagen zu verbinden, die in einer logischen Beziehung stehen. Zum Beispiel: "Ihr könnt ihr entweder arbeiten oder lernen."
4. "Or" wird verwendet, in der Regel als Konjunktion, um zwei Aussagen zu verbinden, die in einem alternativen oder alternativen Zusammenhang stehen. Zum Beispiel: ""Ich will nicht essen, sondern trinken.""

Es ist wichtig zu beachten, dass die Verwendung von "or" in einer Sprache von Kontext und Grammatik abhängt. Es ist immer am besten, die grammatischen Regeln und die Kontexte der Sprache zu verstehen, um die korrekte Verwendung von "or" zu gewährleisten.</sample>
    <sample id="450">Dies ist ein wichtiges Problem in Konversationssystemen und auch für Benchmarking der Entität in LLMs.</sample>
    <sample id="451">Wir sind nicht auf ein öffentliches Datenset, ein großskaliges öffentliches Datenset für die Aufgabe, so sammeln wir ein mit Crowd Annotation. Unser Datenset deckt drei verschiedene Bereiche ab: Musik, Bücher und Rezepte.</sample>
    <sample id="452">Unsere Datenset-Sammlungsmethode betont die Informalität, indem wir ein Cartoon-Kompletionset verwenden.</sample>
    <sample id="453">Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: „Denken Sie an die Lieder, die wir gestern gehört haben?“ Und mit diesem Satz setzt Bob den Dialog in Gang.</sample>
    <sample id="454">In der zweiten Redewendung sagt Alice: "Willst du mir einfach etwas erklären oder habe ich etwas verstanden?"</sample>
    <sample id="455">Welche ist die alternative Frage? Und in der dritten Sprechblase verwendet Bob eine indirekte Referenz, um eines dieser Entitäten auszuwählen. Zum Beispiel die neue Erfahrung.</sample>
    <sample id="456">Wir stellen die ersten und zweiten Sprechblasen automatisch bereit, aber die dritte Sprechblase wird von dem Annotator gefüllt.

Die erste Sprechblase wird aus einer Reihe von manuellen Anweisungen ausgewählt, die für jeden Bereich vorgesehen sind. Diese Anweisungen können von der Annotatorin oder dem Annotator ausgewählt werden, um die Sprechblase zu füllen.

Die zweite Sprechblase wird automatisch aus dem Text des Analyseeinleses ausgewählt. Diese Sprechblase kann von der Annotatorin oder dem Annotator überprüft und gegebenenfalls angepasst werden, um sicherzustellen, dass sie korrekt ist.

Die dritte Sprechblase wird vom Annotator gefüllt, um sicherzustellen, dass die Sprechblase korrekt ist und die Analyse korrekt durchgeführt wird. Der Annotator kann die Sprechblase nach Belieben anpassen, um sicherzustellen, dass alle relevanten Informationen erfasst werden.

Insgesamt ist die Auswahl der Sprechblasen ein wichtiger Bestandteil der Analyse, da sie dazu beiträgt, die Sprache des Textes zu verstehen und die Analyse korrekt durchzuführen.</sample>
    <sample id="457">Der zweite Teil, der die alternative Frage ist, wird wie folgt generiert:</sample>
    <sample id="458">We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia.</sample>
    <sample id="459">Hier sind die verschiedenen Abstimmungsmethoden, die wir verwendet haben. Wenn wir höher in der Liste sind, werden die Entitäten ähnlicher zu einander und es ist in der Regel schwerer, die Diskriminierung zu verhindern.</sample>
    <sample id="460">Der erste Punkt ist eine uniforme Richtlinie.</sample>
    <sample id="461">Der zweite Fall tritt auf, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "Die Ritter".</sample>
    <sample id="462">Der dritte Punkt ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben und schließlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben. Zum Beispiel die gleiche Genre oder der gleiche Künstler für ein Lied.</sample>
    <sample id="463">When we show this alternative question to the annotators, they know the name of these entities, but they don't necessarily know about the entities.</sample>
    <sample id="464">So, what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song.</sample>
    <sample id="465">Hier ist der englische Inhalt in die deutsche Sprachversion übersetzt:

---

Und dann bitten Sie die Annotatoren, mindestens einige Lieder von jeder Art zu hören und über jeden Lied zu lesen. Hier ist ein Beispiel für die Google-Suchergebnisse für das Lied "Easy".

---

Hier ist der englische Inhale in die deutsche Sprachversion übertreten:

---

Und dann bieten Sie den Annotatoren an, mindestens einige Lieder von jedem Art zu hören und über jeden zu lesen. Hier ist ein Beispeil für die Google-Suchergebnisse für die Lieder "Easy".

---

Herrlich!</sample>
    <sample id="466">Für das Rezeptes und Bücher-Domain zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen.</sample>
    <sample id="467">Dann bitten wir die Annotatoren, einen dieser Entitäten zu wählen, zum Beispiel den ersten, und sie beschreiben sie mit drei bis fünf indirekten Referenzausdrücken.</sample>
    <sample id="468">Beispiel: Derjenige mit dem Klaviermusik. Hier sind einige Beispiele aus unserem Datensatz: Derjenige ohne Worte, nicht derjenige mit dem 12-jährigen 12-jährigen Jungen oder der fiktive eine oder kommt aus anderer Bildung und so weiter.</sample>
    <sample id="469">Die Identitätskorpus hat 6.000 alternative Fragen in drei Bereichen und 42.000 indirekte Bezugnahmen. Die Ergebnisse mit dem T5-X-Large-Modell sind zusammengefasst.</sample>
    <sample id="470">Wenn der Sprachmodell Zugang zu genau demselben Hintergrundwissen wie die Annotatoren hat, dann ist die Genauigkeit wirklich hoch, es beträgt etwa 92 bis 95 Prozent. Aber das ist nicht realistisch.</sample>
    <sample id="471">Wenn der Sprachmodell Zugang zu einigen teilweise überlappenden Hintergrundwissen hat, dann liegt die Genauigkeit zwischen 82 % und 87 %. Dies ist realistischer, zum Beispiel, wenn der Sprachmodell die Hintergrundwissen abruft.</sample>
    <sample id="472">Wenn die Sprachmodell nur auf Namen von Entitäten zugreifen kann, dann ist die Genauigkeit nur 60 Prozent. Daher gibt es viel Raum für Verbesserung. Wir haben auch gezeigt, dass die Modelle auf verschiedene Bereiche anpassbar sind. Hier ist eine Verknüpfung zu unserem Datensatz. Vielen Dank.</sample>
    <sample id="473">Der Ansatz wird mit den bestehenden SimulST-Richtlines verglichen, die sich auf die Anwendung von Strategien für die Übersetzung von Texten aus verschiedenen Sprachen konzentrieren. Diese Richtlines basieren auf der Wissensstrategie und dem lokalen Abkommen, die sowohl für Online- als auch für Offline-Modelle geeignet sind. Zusätzlich wird der Ansatz mit der Art-Architektur verglichen, die speziell für die simultane Übersetzung von Texten entwickelt wurde.</sample>
    <sample id="474">Die Autoren gehören an der Universität von Lausanne.</sample>
    <sample id="475">Der/die Referent*in heißt Jenny.</sample>
    <sample id="476">Drei Autoren sind an der Arbeit beteilt.</sample>
    <sample id="477">Hallo, ich bin Sarah Pappi von der Universität Toronto und von der Zentrum für Brünnchen Kessler und ich werde kurz die Aufmerksamkeit als eine Anleitung für die gleichzeitige Sprachübersetzung von Papiere vorstellen. Es ist eine gemeinsame Arbeit mit Matteo Negri und Marco Turki.</sample>
    <sample id="478">Simultaneous speech translation, auch bekannt als simulst, ist der Prozess der Übersetzung von gesprochenem Sprache in Text in einer anderen Sprache in Echtzeit, ermöglichenden Sprachkommunikation über Grenzen hinweg.</sample>
    <sample id="479">Die aktuellen Simulatoren-Modelle haben folgende Probleme:

1. **Spezifische Architekturen**: Die meisten Simulatoren werden mit spezifischen Architekturen trainiert, die nicht für alle Anwendungsfälle optimiert sind. Dies kann zu einer Verringerung der Genauigkeit und Effizienz führen.

2. **Modulbasierte Optimierung**: Um die Leistung zu verbessern, werden zusätzliche Module eingefügt, die zu einer Komplexität und zu einer langen Trainingszeit führen können.

3. **Datenverbrauch**: Die meisten Simulatoren benötigen große Mengen an Daten, um genaue Ergebnisse zu erzielen. Dies kann zu hohen Kosten und Zeitaufwand führen.

4. **Fehlermessung**: Die Genauigkeit der Simulatoren kann durch Fehler in den Modellen oder den Daten beeinflusst werden, was zu ungenauen Ergebnissen führen kann.

5. **Flexibilität**: Die meisten Simulatoren sind nicht flexibel genug, um verschiedene Anwendungsfälle zu unterstützen. Sie sind oft spezialisiert auf bestimmte Aufgaben oder Szenarien.

6. **Skalierbarkeit**: Die meisten Simulatoren sind schwer zu skalieren, was bedeutet, dass sie nicht für große Anwendungsfälle oder umfangreiche Datenmengen geeignet sind.

7. **Transparenz**: Die meisten Simulatoren sind nur begrenzt transparent, was bedeutet, dass es schwierig ist, die Entscheidungsprozesse und die Ergebnisse zu verstehen und zu überprüfen.

8. **Sicherheit**: Die meisten Simulatoren sind anfällig für Angriffe oder Fehler, was bedeutet, dass sie möglicherweise nicht sicher genug sind, um sensiblen Daten zu verarbeiten.

9. **Kosten**: Die Entwicklung und den Betrieb von Simulatoren können teuer sein, was bedeutet, dass sie nur für große Unternehmen oder Organisationen geeignet sind.

10. **Erfahrung**: Die meisten Simulatoren erfordern eine hohe Erfahrung und Kenntnisse, um effektiv zu verwenden. Dies kann zu einem hohen Lernaufwand führen.</sample>
    <sample id="480">Langen und komplizierten Trainingsverfahren, beispielsweise Trainings, die verschiedene Optimierungsobjektive umfassen.</sample>
    <sample id="481">Und trainiere und wahre mehrere Modelle, um verschiedene Latenzregimes zu erreichen. Zum Beispiel trainiere ein Modell mit einem Durchschnitt von einer Sekunde Latenz und einem anderen Modell mit zweieinhalb Sekunden Latenz und so weiter.</sample>
    <sample id="482">So, what is our solution?</sample>
    <sample id="483">Zuerst nutze bereits bestehende Offline-Modell-Systeme ohne Retraining oder Anpassung spezifischer Architekturen für Online-Systeme. Verwende nur ein Modell für jeden Latenzbereich und behandle Latenz durch spezifische Parameter.</sample>
    <sample id="484">The text is in English and it discusses leveraging knowledge already acquired by a model through the attention mechanism between audio input and textual output, which is known as the cross-attention mechanism. An example of this can be seen on the right.</sample>
    <sample id="485">Unser Lösungsansatz besteht darin, einen Encoder-Decoder-Ansatz oder eine Attention-Strategie vorzuschlagen, bei der wir entscheiden, ob wir eine Teilübersetzung erzeugen oder nicht, basierend auf dem, wo die Aufmerksamkeit zeigt.</sample>
    <sample id="486">Ein Wort wird ausgesprochen, wenn die Aufmerksamkeit nicht konzentriert ist, d.h. die Summe ist unter einem bestimmten Schwellenwert alpha gegenüber den letzten Lautsprecherrahmen, was bedeutet, dass die empfangene Informationen stabil sind.</sample>
    <sample id="487">Wenn wir zum Beispiel einen Sprachschunk erhalten, der den Satz "Ich werde über" enthält, und unsere Modellvorhersage die Übersetzung in Deutsch vorhersagt, dann wäre die Übersetzung "Ich werde über" in Deutsch "Ich werde über".</sample>
    <sample id="488">Und wir werden uns die cross attention  umgewandeln.</sample>
    <sample id="489">Wir werden sehen, dass die ersten beiden Wörter auf die ältesten empfangenen Sprachrahmen hinweisen, während das letzte Wort auf die letzten empfangenen Sprachrahmen hält, also auf die Sprachrahmen als Lambda-Sprachrahmen.</sample>
    <sample id="490">Dies bedeutet, dass die ersten beiden Wörter vermisst werden.</sample>
    <sample id="491">Während die Summe der Kreuzspannung über einen bestimmten rationalen Wert α liegt, werden wir den letzten Worten nicht zuhören und warten auf den nächsten Sprachblock.</sample>
    <sample id="492">Wenn wir weitergehen und wir eine weitere Sprachtank erhalten, und unsere Modellvorhersage andere drei Wörter, und wir werden die Cross-Attention-Werte betrachten.</sample>
    <sample id="493">Wir werden sehen, dass keine Worte auf die letzten Sprachrahmen hinweisen.</sample>
    <sample id="494">Dies bedeutet, dass diese drei Wörter emittiert werden.</sample>
    <sample id="495">Wenn Sie sich die Hauptresultate eines Datensatzes ansehen,</sample>
    <sample id="496">Wir werden die gleichzeitigen Sprachübersetzungsergebnisse auf Grafiken darstellen, auf denen blau auf einer Seite steht, die die Qualität der Übersetzung misst, und durchschnittliche Lässigkeit.</sample>
    <sample id="497">Das ist ermittelte Latenzmaß und wir berücksichtigen auch die computergesteuerte durchschnittliche Verzögerung, die für die Computertime des Modells zur Berechnung des Ausgangs verantwortlich ist.</sample>
    <sample id="498">So we want our curves to be as high as possible on this plot.</sample>
    <sample id="499">Aber wir wollen auch, dass sie auf der linken Seite verschoben werden.</sample>
    <sample id="500">Wir vergleichen mit praktischen Strategien, die auch auf Offline-Modelle angewendet werden, wie zum Beispiel die Wichtigkeitsstrategie und die lokale Vereinbarung. Wir vergleichen auch mit der Art-Architektur des Zustands, die speziell für simultane Übersetzung entwickelt wurde.</sample>
    <sample id="501">Diese sind alle Ergebnisse des simultanen Sprachübersetzungsstrategies auf Deutsch.</sample>
    <sample id="502">Und wir sehen, dass  uh  es außerhalb aller Strategien, die auf Offline-Modellen angewendet werden, sehr gut funktioniert, da die Kurven über die linke Seite verschoben sind.</sample>
    <sample id="503">Wir sehen auch, dass wenn wir die tatsächliche Laufzeit oder die Rechenzeit berücksichtigen, dass das die schnellste Strategie ist.</sample>
    <sample id="504">Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unsere Arbeit und wir haben auch Open-Source-Code und Modelle veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu fördern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="505">Yes, the dataset is publicly accessible.</sample>
    <sample id="506">Hallo alle, mein Name ist Ying und mein Kollege Zhiyang und ich werden unsere Forschung über Multi-Instruction verbessern, indem wir Multi-Model-Serial-Learning durch Anweisungstuning erreichen.</sample>
    <sample id="507">Mit den Fortschritten in großen Sprachmodellen begannen viele Arbeiten, neue Lernparadigmen zu erforschen, die darauf abzielen, vorgeübte Sprachmodelle für verschiedene nachfolgende Aufgaben in einem parametrischen und dateneffizienten Weg zu nutzen.</sample>
    <sample id="508">Kürzlich haben viele Studien gezeigt, dass die Anweisungstuning es großen Sprachmodellen ermöglicht, in einer sehr kurzen Weise Aufgaben zu erledigen, indem sie natürliche Anweisungen befolgen.</sample>
    <sample id="509">Allerdings konzentriert sich die meiste bisherige Arbeit über Anweisungstuning auf die Verbesserung der seriellen Leistung auf sprachbasierten Aufgaben, während Computervision und multimodale Aufgaben ausgelassen wurden.</sample>
    <sample id="510">Daher möchten wir in dieser Arbeit untersuchen, ob Anweisungstuning auf multimodale Prätraining-Modelle tatsächlich die Generalisierung zu nc multimodal Aufgaben verbessern kann.</sample>
    <sample id="511">Zusätzlich entdeckten wir bei der Zeit unserer Forschung eine erhebliche Diskrepanz in der Verfügbarkeit von Instruction-Daten-Sets zwischen NLP und Multi-Model.</sample>
    <sample id="512">Es gibt mehr als 1.600 sprachspezifische Anweisungstasks, jedoch gibt es keine großskalige öffentlich verfügbare Multi-Model-Anweisungstask. Daher motiviert uns dies, ein Multi-Model-Anweisungstuning-Datensatz zu erstellen.</sample>
    <sample id="513">Hier präsentieren wir MultiInstruct, das erste Multi-Model-Instruction-Tuning-Benchmark-Datenset, das aus 62 verschiedenen Multi-Model-Aufgaben besteht, die 10 verschiedene Kategorien abdecken.</sample>
    <sample id="514">Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen und jede Aufgabe ist mit fünf speziell geschriebenen Anweisungen ausgestattet.</sample>
    <sample id="515">Um die Multimodal-Anweisungs-Tuning auf unserem vorgeschlagenen Datensatz zu untersuchen, nehmen wir OFA, ein vereinfachtes Multimodal-Pretrain-Modell, als unsere Basismodell. OFA verwendet ein vereinfachtes Vokabular für Sprache, Bildtokens und Koordinaten eines Bounding Boxes.</sample>
    <sample id="516">Hier zeigen wir einige Beispielinstanzen aus unseren Multi-Image-Datensätzen.</sample>
    <sample id="517">Um die Verarbeitung verschiedener Eingabedatentypen und Ausgabedatentypen zu vereinheitlichen, können folgende Schritte unternommen werden:

1. **Datentypen identifizieren**: Identifizieren Sie die verschiedenen Eingabedatentypen und Ausgangsdatentypen, die in Ihrem System verwendet werden.

2. **Datentypen konvertieren**: Konvertieren Sie die Eingabedatentypen in eine gemeinsame Datenform, die für die Verarbeitung geeignet ist. Dies kann durch die Verwendung von Funktionen oder Klassen erfolgen, die die Konvertierung durchführen.

3. **Datentypen standardisieren**: Standardisieren Sie die Ausgabedatentypen, um sicherzustellen, dass sie in einer einheitlichen Form vorliegen. Dies kann durch die Verwendung von Formatierungsfunktionen oder -klassen erfolgen.

4. **Datentypen validieren**: Validieren Sie die Eingabedatentyper und Ausgabedatentyper, um sicherzustellen, dass die Daten korrekt sind und keine Fehler vorkommen.

5. **Datentypen umwandeln**: Umwandeln Sie die Eingabedatentyper in die Ausgabedatentyper, indem Sie die Konvertierungsfunktionen oder -kassen verwenden.

6. **Datentypen testen**: Testen Sie die Verarbeitung der Eingabedatentyper und Ausgangsdatentyper, um sicherzustell, dass sie korrekt funktionieren.

7. **Datentypen dokumentieren**: Dokumentieren Sie die Verarbeitung der Eingabe- und Ausgabedatentyper, damit andere Entwickler oder Benutzer die Verarbeitung verstehen können.

Durch die Verwendung dieser Schritte können Sie die Verarbeitung verschiedener Input- und Output-Datentypen vereinheitlichen und sicherstellen, dass die Daten korrekt verarbeitet werden.</sample>
    <sample id="518">Wir folgen der Methode von OFA und formulieren alle Aufgaben in einer vereinfachten Sequenz-zu-Sequenz-Format, in dem die Eingabestexte, Bilder, Anweisungen und Bounding-Boxen in demselben Token-Raum dargestellt werden.</sample>
    <sample id="519">Okay, jetzt werde ich über die Anpassung von Multimodal-Anweisungen sprechen.</sample>
    <sample id="520">Um die Trainingsdatensatz zu erstellen, verwenden wir 53 Aufgaben aus der N-Gruppe für das Training und stellen 10.000 Instanzen pro Aufgabe zur Verfügung. Für die Testspeicherung reservieren wir die gesamte Common Sense Reasoning Gruppe. Wir wählen zusätzlich fünf Aufgaben aus der VQA- und Meilenstein-Gruppe aus.</sample>
    <sample id="521">Wir verwenden alle Instanzen in der Testspitze für jeden Task. Zusätzlich würden wir zufällig 20 Tasks aus der Testspitze von natürlicher Anweisung als unsichtbare Tasks für NLP wählen.</sample>
    <sample id="522">Sure, I'd be happy to help! Here's the translation of the text you provided:

So we use a pre-trained or fine-tuned large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates.

I hope this helps! Let me know if you have any other questions.</sample>
    <sample id="523">Um den Inhalt aus dem englischen Text ins Deutsche zu übersetzen, müssen wir die Aussage in deutsche Sprache umformulieren, wobei wir die Struktur und die Bedeutung der ursprünglichen Aussage beibehalten.

1. **Identifizieren Sie die Hauptbestandteile des Textes:**
   - **Hauptbestandteile:**
     - "during test"
     - "for each task"
     - "we conduct the total of five experiments"
     - "by evaluating the model using one of the five instructions"
     - "in each experiment"

2. **Übersetzen Sie die Hauptbestandteile ins Deutsche:**
   - "während des Tests"
   - "für jede Aufgabe"
   - "führen wir insgesamt fünf Experimente durch"
   - "durch die Bewertung des Modells unter Verwendung einer der fünf Anweisungen"
   - "in jeder Experiment"

3. **Strukturieren Sie die Übersetzung:**
   - "Während des Tests für jede Aufgabe führen wir insgesamt fünf Experiments durch, indem wir das Modell durch die Bewertung unter Verwendung einer der fünf Instructions in jeder Experiment bewerten."

4. **Überprüfen Sie die Übersetzung:**
   Die Übersetzung sollte den ursprünglichen Text in deutscher Sprache wiedergeben, wobei die Bedeutung und die Struktur beibehalten bleiben.

**Übersetzung:**
"Während des Tests für jede Aufgabenführung führen wir insgesamt fümfach Experimente durch, indem wir das Modell durch Bewertung unter Verwendung einer von fünf Anweisungen in jeder Experiment bewerten."</sample>
    <sample id="524">Wir berichten die mittlere und maximale Leistung und die Standardabweichung der Leistung über alle fünf Experimente.</sample>
    <sample id="525">Wenn die Aufgabe ein Multi-Model-Klassifizierungsaufgabe ist, berichten wir über Genauigkeit. Wenn es sich um eine Multi-Model-Generierungsaufgabe handelt, berichten wir über Rückschläge. Für NLP-Aufgaben berichten wir ebenfalls über Rückschläge.</sample>
    <sample id="526">Wir haben auch eine zusätzliche Bewertungsmetrik namens Sensibilität eingeführt, die diese Messung der Fähigkeit des Modells zur konsistenten Produktion der gleichen Ausgaben für die gleiche Aufgabe misst, unabhängig von einer leichten Variation in der Wording der Anweisung.</sample>
    <sample id="527">Hier ist unser Hauptresultat. Wie wir sehen können, kann die Anweisungstuning die Leistung von OS OFA auf unterschiedlichen Multi-Modellaufgaben signifikant verbessern.</sample>
    <sample id="528">Auch die Transfer-Learning von natürlichen Instruction-Daten-Sets kann die Instruction-Tuning bevölkern.</sample>
    <sample id="529">Hier können wir sehen, dass mit zunehmender Anzahl von Aufgaben die Leistung des Modells verbessert wird und gleichzeitig die Sensibilität sinkt.</sample>
    <sample id="530">Wir haben auch eine Experimente durchgeführt, bei der wir eine Anweisung gegenüber fünf Anweisungen verwendet haben. Wie wir sehen können, verbessert die Verwendung von mehr Anweisungen die allgemeine Leistung des Modells und reduziert seine Sensibilität erheblich.</sample>
    <sample id="531">Sure, I'd be happy to help! Here's the translation of the text you provided:

So this shows the effect of different fine-tuning strategies on the model's sensitivity. As we can see, by transfer learning from natural instruction datasets, the model can achieve much better sensitivity compared to the original OFA model.

I hope this helps! Let me know if you have any other questions.</sample>
    <sample id="532">Wir können auch sehen, dass die Übertragung von Lernen aus dem natürlichen Unterrichtsdatenset dazu beiträgt, dass die OFA auf dem natürlichen Unterrichtsdaten set besser abschneiden kann.</sample>
    <sample id="533">Insgesamt schlagen wir vor, den ersten großskaligen Multi-Model-Instruction-Tuning-Datensatz zu erstellen, der die Zero-shot-Kapazitäten von OFA signifikant verbessert und wir untersuchen verschiedene Transfer-Learning-Techniken und zeigen ihre Vorteile. Wir entwerfen ein neues Maß, das Sensibilität nennt.</sample>
    <sample id="534">Sure, I'd be happy to help you translate the text from English to German. Here's the translation:

---

**Übersetzung des Textes aus dem Englischen ins Deutsche:**

---

**Eine weitere Sache:** Wir sammeln ein viel größeres Multi-Modelle-Anweisung-Tuning-Datenset mit etwa 150 zusätzlichen Vision-Language-Aufgaben, und wir werden sie bald veröffentlichen.

---

**Übersetzte Text:**

---

**Eine zusätzliche Sache:** Wir sammeln ein viele größere Multi-Modelle-Anweisung-Anweisung-Tuning-Datasets mit etwa 150 zusatzlichen Vision-Language-Aufgaben, die wir bald veröffentlichen.

**Übersetzte Text:**</sample>
    <sample id="535">Die Autoren gehören der Universität Trento an.</sample>
    <sample id="536">Der/die Referent*in heißt Javad Hosseini.</sample>
    <sample id="562">Hallo alle, ich bin Kostas Sina und ich freue mich, Sie zu unserem Vortrag über unsere ACL 2023-Paper begrüßen zu dürfen. Sprachmodell-Akzeptabilitätsurteile sind nicht immer robust gegenüber Kontext.</sample>
    <sample id="563">Es gibt eine gemeinsame Arbeit mit John Gotherer, Aaron Muller, Kaniska Mishra, Karen Fentress, Roger Levy und Adina Williams.</sample>
    <sample id="564">In dieser Arbeit untersuchen wir die minimalen Paarparadigmen.</sample>
    <sample id="565">So the minimal pair paradigm basically evaluates language models  uh  on top of acceptability judgments which can also include grammaticality like plump syntax gem or acceptability in terms of stereotypes such as cross pairs.</sample>
    <sample id="566">In diesem minimal pair Paradigma ist die typische Methode, um Sprachmodelle zu bewerten, dass Sie 

- eine akzeptable Sätze oder eine grammatikalische Sätze zeigen und 
- dann eine unakzeptable Sätze oder eine ungrammatische Sätze zeigen.</sample>
    <sample id="567">Und dann ist die Hoffnung, dass das Modell grundsätzlich</sample>
    <sample id="568">Der aktuelle MPP-Pipeline ermöglicht es uns grundsätzlich nicht, die Akzeptanz von Modellen zu bewerten, die auf längeren Sätzen abzielen.</sample>
    <sample id="569">In diesen Tagen kommen große Sprachmodelle mit immer längeren Kontextfenstern auf, daher ist es entscheidend, dass wir die Akzeptabilität der Modelle über den gesamten Kontextfenster bewerten.</sample>
    <sample id="570">Und das ist, was wir hier versuchen zu tun. Wir versuchen,  uh, die npp-Pipeline zu überprüfen, indem wir den Modell anfragen, ob es Akzeptabilität auf längeren und längeren Sequenzen bewertet.</sample>
    <sample id="571">So, that is the approach. So what we do is that we simulate these longer sequences. We revisit the data sets themselves and then we recreate sentences by choosing uh like acceptable or unacceptable sentences from those data sets.</sample>
    <sample id="572">So for example, here we have chosen like a typical pair of grammaticality from the blimp dataset from the adjunct island case.</sample>
    <sample id="573">Und was wir tun, ist das, dass wir längere Sequenzen rekonstruieren, die akzeptabel sind und die gleiche grammatische Struktur haben. Wir extrahieren grammatische Sätze aus dem Text.</sample>
    <sample id="574">Und dann fügen wir es als Präfix zu beiden akzeptablen Anfragen und unakzeptablen Anfragen hinzu.</sample>
    <sample id="575">So we can do the same thing by choosing unacceptable sentences from the same  uh matching and that could also like be used to test the model's acceptability.</sample>
    <sample id="576">Und wir können auch das gleiche tun, indem wir Sätze aus einem anderen Teilset oder einem anderen Datensatz wählen. Das ist, was wir als die Mismatch-Szene nennen.</sample>
    <sample id="577">Hier sind die Sätze immer noch aus relevanten Datensätzen, aber nicht aus dem gleichen Datensatz, mit dem Sie evaluieren. Und wir können das auch für unannehmbare Fälle tun.</sample>
    <sample id="578">Schließlich können wir Sätze aus einem völlig unverwandten Bereich, wie zum Beispiel Wikipedia, auswählen.</sample>
    <sample id="579">So this will tell us like whether the model's acceptability judgments are actually impacted by any context.</sample>
    <sample id="580">Wie wichtig ist es, dass der Kontext aus einer anderen</sample>
    <sample id="581">So, how does the model do? First, we look at the Wikipedia sentences, which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context lengths.</sample>
    <sample id="582">Wir haben den Kontextlänge auf 2024 erhöht, um die OPT- und GPT-2-Modelle maximal auszuschöpfen. Wir haben hier in der orange Punktlinie die mpp-Entscheidungen relativ stabil gesehen.</sample>
    <sample id="583">Wenn wir Sätze aus demselben Datensatz auswählen, passiert folgendes:

1. **Übereinstimmung**: Die Sätze sind wahrscheinlich ähnlich oder verwandt, da sie aus demselben Datensatz stammen. Dies kann dazu führen, dass sie ähnliche Themen, Stile oder Inhalte haben.

2. **Redundanz**: Es kann zu Redundanz kommen, da verschiedene Sätze denselben Informationen oder Themen behandeln. Dies kann dazu führen, die Texte weniger interessant oder weniger abwechslungsreich zu machen.

3. **Kontextualisierung**: Die Auswahl von Sätzen aus demselben Datensatz kann dazu führen, dass der Kontext von einzelnen Sätzen nicht klar ist, da sie ohne den gesamten Text oder die umgebende Informationen verstanden werden können.

4. **Qualität der Texte**: Wenn die Sätze aus demselben Datasatz stammen, kann es zu einer geringeren Qualität der Texte kommen, da sie möglicherweise nicht die besten Beispiele für die gewünschte Thematik oder Stil sind.

5. **Diversität**: Um die Texte abwechslungsreicher und interessanter zu machen, ist es oft sinnvoll, Sätze aus verschiedenen Datensätzen auszuwählen, die unterschiedliche Themen, Stile oder Inhaltsbereiche abdecken.</sample>
    <sample id="584">Hier sind wir dabei, aus akzeptablen und unakzeptablen Domains Sätze zu wählen oder zu erstellen, aus dem gleichen Blimp oder Syntaxgen-Datensatz.</sample>
    <sample id="585">Und da sehen wir, dass die MPP-Gerichtungen entweder erheblich steigen oder sinken, wenn Sie entweder akzeptable Präfixe oder unakzeptable Präfixe hinzufügen.</sample>
    <sample id="586">Aber wenn wir die Struktur anpassen, d.h. wenn wir die Sätze aus dem gleichen Phänomen in Blame Person Text gemeinsam wählen,</sample>
    <sample id="587">Wir sehen eine massive Zunahme oder eine massive Abnahme in der mpp-Entscheidung für das Modell, je nachdem, ob der gewählte Präfix akzeptabel oder unakzeptabel ist.</sample>
    <sample id="588">Jetzt ist dies und dies ist sehr groß, wie dieser Effekt durch den Kontext länger wird und dies würde wahrscheinlich die neueren Sprachmodelle beeinflussen, die große Kontextfenster haben.</sample>
    <sample id="589">Der Match-Prefix beeinflusst die Sprachmodellbewertung stark, weil er die Wahrscheinlichkeit bewertet, dass ein bestimmter Begriff oder eine bestimmte Sequenz von Wörtern in einem Text vorkommt. Wenn der Prefix eher in den Text passt, wird die Wahrscheinlichkeit erhöht, dass der Text relevant oder nützlich ist. Dies kann dazu führen, dass der Sprachmodellbewertung eine höhere Bewertung gibt, da der Text wahrscheinlicher den Anforderungen des Benutzers entspricht.</sample>
    <sample id="590">So we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding  uh  like noise to the input and after doing like several of these perturbations</sample>
    <sample id="591">Wir stellen fest, dass keine dieser Störungen tatsächlich dazu führen, dass das Modell entscheidend verändert, wie es uns die mpp-Verbindung zeigt.</sample>
    <sample id="592">Grundsätzlich stellen wir fest, dass die Modelle empfindlich auf die Störungen und Sätze in ähnlicher Weise reagieren.</sample>
    <sample id="593">Das bedeutet, dass wenn wir die Sätze in dem akzeptablen Bereich stören, wir einen ähnlichen Anstieg in allen Störungen sehen. Und wenn wir die Sätze in dem unakzeptablen Bereich stören und sie in der unakzeptablen Domain sehen, sehen wir eine Abnahme in den mpp-urteilen in ähnlicher Weise.</sample>
    <sample id="594">So the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences.</sample>
    <sample id="595">Und die MPP-Evaluation, die derzeit mit kurzen und einzelnen Sätzen-Input durchgeführt wird, kann möglicherweise nicht die abstrakte Kenntnisse des Sprachmodells in ihrem gesamten Kontext vollständig erfassen.</sample>
    <sample id="596">Bitte lesen Sie unsere Arbeit für weitere Details zu unseren Experimenten. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="597">Die Input-Token werden im ersten Schritt der Methode mit einem unordered multiset von Tokens, die im Output erscheinen werden, versehen.</sample>
    <sample id="598">55,000</sample>
    <sample id="626">Die beste Ausrichtungsmethode für DEPlain ist die Methode des Massaligns.</sample>
    <sample id="627">Der Vorteil von schwach überwachtem Lernen ist, dass es trainierte neuronale Netzwerke unter solchen Label-Rauschen robust trainiert, sodass die trainierten Modelle gut generalisieren.</sample>
    <sample id="628">Die Zuteilung der Dokumente in DEplain-web erfolgte durch manuelle und automatische Alignmentmethoden. Die genaue Zuteilung ist nicht im Text erwähnt, aber es wird angedeutet, dass sowohl manuelle als auch automatische Methoden verwendet wurden.</sample>
    <sample id="629">Der CoNLL++-Datensatz wurde erstellt, indem wir Daten aus Reuters-Nachrichten aus dem Jahr 2020 gesammelt und dann mit den gleichen CoNLL-2003-Anmerkungsrichtlinien annotiert haben.</sample>
    <sample id="630">Hallo alle zusammen, mein Name ist Yusen Zhang und ich komme aus der Pennsylvania State University. Heute werde ich über unsere Arbeit sprechen, nämlich über die Beispielarbeit Crosslingual Semantic Parsing in mehreren natürlichen Sprachen und mehreren Darstellungen.</sample>
    <sample id="631">So, semantic parsing is the task to build semantic representations of user queries such as SQL and lambda calculus.</sample>
    <sample id="632">Und die Übersetzung des englischen Inhalts ins Deutsche lautet:

"Cross-lingual semantic parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen zu übersetzen."</sample>
    <sample id="633">Wie in diesem Bild gezeigt, benötigen wir, um die Abfrage in mehreren natürlichen Sprachen zu übersetzen, die mit neuronalen Modellen zu SQL, Lambda oder FunktionQL und dergleichen.</sample>
    <sample id="634">Existierende Cross-Linguistic Semantic Parsing-Modelle werden separat vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen evaluiert. Zum Beispiel</sample>
    <sample id="635">Es gibt einen Mangel an Abdeckung für bestimmte natürliche Sprachen. Die chinesische Sprache fehlt dabei.</sample>
    <sample id="636">Nachfolgend finden Sie die Übersetzung des englischen Textes ins Deutsche:

"Lack of coverage on certain minor representations"

Die Übersetzung lautet:

"Unzureichende Berichterstattung über bestimmte kleinere Darstellungen"

Bitte beachten Sie, dass die Übersetzung je nach Kontext variieren kann.</sample>
    <sample id="637">Der Lambda-Kalkulus fehlt.</sample>
    <sample id="638">Oder sie werden nur auf bestimmte neuronale Modelle bewertet. Zum Beispiel gibt es nur ein einziges Modell, um sie zu bewerten.</sample>
    <sample id="639">Um zu diesem Ende schlagen wir den Exemplar vor, der eine einheitliche Datensatz-Exemplar für die Übersetzung von semantischen Inhalten in mehreren natürlichen Sprachen und mehreren Repräsentationen bietet.</sample>
    <sample id="640">Es enthält neun Datensätze in verschiedenen Bereichen, fünf semantische Parsing-Tasks, acht mehrdimensionale Darstellungen und zweiundzwanzig natürliche Sprachen in fünfzehn Sprachfamilien.</sample>
    <sample id="641">Um unsere Benchmarking besser zu bewerten, berücksichtigen wir die sechs Einstellungen für Training und Evaluierung.</sample>
    <sample id="642">Der erste Teil ist der Übersetzungstest. Wir verwenden die Google-Übersetzungs-API, um die Quelle in die Zielsprache zu übersetzen, dann verwenden wir ein monolinguales Modell zur Trainierung und Bewertung.</sample>
    <sample id="643">Und zum Beispiel trainieren wir das englische Modell auf englischen Abfragen und während des Inferenzes übersetzen wir die deutsche Abfrage mit der API ins Englische und verwenden dann das trainierte Modell, um das Ziel zu vorhersagen.</sample>
    <sample id="644">Wir werden auch den monolingualen Modelltest durchführen.</sample>
    <sample id="645">In diesem Szenario ist die Quellsprache die gleiche wie die Zielsprache, zum Beispiel Deutsch auf Deutsch oder Englisch auf Englisch.</sample>
    <sample id="646">Wir testen auch monolinguale Führungseinstellungen, indem wir Modelle mit nur 10 % des Trainingsdatensatzes trainieren.</sample>
    <sample id="647">Und wir testen ein multilinguelles Modell, bei dem wir einen Modell für alle Sprachen trainieren.</sample>
    <sample id="648">Zum Beispiel **wir setzen die deutschen, englischen und chinesischen Abfragen zusammen**, um ein mehrsprachiges Modell zu trainieren. Während des Inferenzes **können wir dieses Modell verwenden**, um **die Übersetzung von Texten** zu verbessern.</sample>
    <sample id="649">Um deutsche Anfragen oder chinesische Anfragen oder ähnliche Inhalte zu übersetzen, benötigt man eine Übersetzungssoftware oder -dienst. Hier sind einige Optionen:

1. Google Translate: Dies ist ein kostenloses Online-Tool, das über 100 Sprachen übersetzen kann. Es ist sehr genau und kann auch komplexe Sätze übersetzen.

2. DeepL: Dies ist ein deutsch-britisches Übersetzungsdienst, das besonders für die Übersetzung von französischen, spanischen und italienischen Texten bekannt ist. Es ist auch sehr genau und kann komplexe Sätze übersetzten.

3. Microsoft Translator: Dies ist ein kostenloses Online-Werkzeug, das über 60 Sprachen übersetzen kann und auch für die Übersetzung von Bildern und Videos verwendet werden kann.

4. SDL Trados: Dies ist ein professionelles Übersetzungssoftware, das für große Projekte und professionelle Übersetzer geeignet ist. Es ist sehr genau und bietet viele zusätzliche Funktionen wie Terminologieverwaltung und Qualitätskontrolle.

5. MemoQ: Dies ist ein weiteres professionelles Übersetzungssoftware, das besonders für die Übersetzung von Dokumenten und Websites geeignet ist. Es bietet auch viele zusätzliche Funktion wie Terminologieverwaltung und Terminologieverwaltung.

Bitte beachten Sie, dass die Genauigkeit der Übersetzung von Maschinenübersetzungen immer von der Qualität des Originaltextes und der Komplexität des Inhalts abhängt.</sample>
    <sample id="650">Wir berücksichtigen auch das Crosslingual Zero-shot und Few-shot Transfer. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache.</sample>
    <sample id="651">Während des Trainings werden wir es auf Englisch oder auf der Kombination von Englisch und Deutsch trainieren, um ein mehrsprachiges Modell zu trainieren, um die Sequenz vorherzusagen.</sample>
    <sample id="652">Wir finden auch viele interessante Ergebnisse. In Bezug auf die Analyse von monolingualen Modellen werden wir zwei Gruppen von Modellen bewerten.</sample>
    <sample id="653">"Einschließlich Encoder-PDR, was für mehrsprachige vorgeübte Encoder mit Zeiger-basierten Decoder steht, wie zum Beispiel XLM-R+PDR und BERT+PDR."</sample>
    <sample id="654">Wir haben auch Evaluations-Encoder-Dekoder-Modelle durchgeführt, die multilingual pretrain-Encoder-Dekoder-Modelle umfassen, wie zum Beispiel BART und MT5.</sample>
    <sample id="655">Wir haben gefunden, dass der Encoder-Decoder-Ansatz die beste Leistung auf allen neun Datensätzen erzielt.</sample>
    <sample id="656">Und wir evaluieren auf mt5 und einem Beispiel xlmr plus pdr in einem mehrsprachigen Umfeld.</sample>
    <sample id="657">Wir haben gefunden, dass Encoder-Decoder oder Encoder-PDR durch Training in einer Mischung aus verschiedenen Sprachen verbessert werden kann.</sample>
    <sample id="658">Wir fanden heraus, dass es wegen der Tatsache ist, dass die meisten der wichtigsten natürlichen Sprachen eine Leistungssteigerung erzielen können, mit Ausnahme des englischen Leistungsniveaus, das in sieben Datensätzen sinkt und nur in drei Datensätzen steigt.</sample>
    <sample id="659">Ich denke, das ist bekannt als "Kurz der Mehrsprachigkeit".</sample>
    <sample id="660">Wir vergleichen auch die Leistungslücke zwischen verschiedenen Sprachen.</sample>
    <sample id="661">In diesem Bild ist die blaue Linie eine kreuzsprachige Füllschaltung, die orange Linie eine kreuzsprachige Nullschaltung, während die grüne Linie in der Modellsprache gesetzt ist.</sample>
    <sample id="662">Wir haben gefunden, dass bei der Verwendung der grünen und orange Linie die Null-Shot-Einstellung die Cross-Lingo-Transfer-Performance-Gap signifikant ist. Bei der Verwendung der blauen und orange Linie ist die Transfer-Gap bei der Fünf-Shot-Einstellung schnell verkürzt.</sample>
    <sample id="663">Wir haben auch einige andere interessante Ergebnisse gefunden. Zum Beispiel erreicht der Encoder-Decoder-Algorithmus vorhersehbare Ergebnisse, die vergleichbar mit denen des vorherigen Modells sind. Die Verwendung von englischen natürlichen Sprachen kann die Leistung von Führungssystemen in Zielsprachen erheblich verbessern.</sample>
    <sample id="664">Wir haben gefunden, dass Modellsprachmodelle wie Codex und Blue für viele Sprachparadigmen noch unzureichend sind.</sample>
    <sample id="665">Zusammenfassend werden wir einen Exemplar erstellen, ein vereinfachtes Benchmark für die semantische Parsing von Texten in mehreren natürlichen Sprachen mit vielen verschiedenen Repräsentationen.</sample>
    <sample id="666">Wir werden eine umfassende Benchmark-Studie über drei repräsentative Typen von mehrsprachigen Sprachmodellen durchführen, und unsere Ergebnisse zeigen viele interessante Erkenntnisse und dazu. Willkommen zu besuchen unserer Arbeit und Code. Danke für Ihre Aufmerksamkeit.</sample>
    <sample id="667">Die vier Kategorien, in die existende Werke eingeordnet werden können, sind:

1. **Theoretische Arbeiten**: Diese arbeiten konzentrieren sich auf die Entwicklung und Analyse von Theorien, Konzepten und Modellen, die die Grundlage für die Forschung bilden.

2. **Empirische Arbeiten**: Diese arbeiteten basieren auf Daten und Beobachtungen, die durch Experimente, Umfragen oder andere methodologischen Ansätze gesammelt werden.

3. **Praktische Arbeiten**: Diese arbeitenden konzentrieren sich auf die Anwendung von Theorien und Erkenntnissen in realen Situationen, wie z.B. in der Praxis, der Industrie oder der Gesellschaft.

4. **Kritische Arbeiten**: Diese arbeitenden bewerten und analysieren bestehende Theorien, Konzepte und Methoden, um ihre Stärken und Schwächen zu identifizieren und zu verbessern.

Diese Kategorien helfen, die verschiedenen Aspekte der Forschung zu strukturieren und zu verstehen.</sample>
    <sample id="668">No, multilingual language models such as Codex and Bloom are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="695">Die Methode mit der Mehrdeutigkeitspermutationen umfasst die Induktion der Alignment als Teil des Trainings. Diese Methode ist sehr flexibel, aber es bringt die Herausforderung mit sich, die höchste Scoring-Permutation als NP-Hard zu finden. Um dies zu umgehen, wird die Methode mit einer GPU-basierten kontinuierlichen Relaxation approximiert, die auch es ermöglicht, durch die Lösung zurückzutreten und die linguistisch plausiblen Permutationen zu lernen.</sample>
    <sample id="696">Die Fairness eines nachgeschaltetenen NLP-Modells wird definiert als die Gleichbehandlung von Benutzern unabhängig von ihren politischen Überzeugungen oder sozialen Hintergründen. Es bedeutet, dass das Modell keine Vorurteile oder Diskriminierung gegenüber bestimmten Gruppen oder Individuen hat, sondern stets objektiv und unabhängig von politischen oder sozialen Faktoren arbeitet.</sample>
    <sample id="697">The referent is Janis Lavak.</sample>
    <sample id="698">Der/die Referent*in heißt Kostas Sina.</sample>
    <sample id="699">Myra</sample>
    <sample id="700">Tropikalismus bezieht sich auf die kulturelle und soziale Verbindung zwischen den Worten, die beschreiben latinoamerikanische Frauen, und den Worten, die beschreibe asiatische Frauen. Es zeigt, wie diese beiden Gruppen von Frauen in ihrer kulturellen und sozialen Rolle und ihrer Identität betrachtet werden.</sample>
    <sample id="701">Die Autoren haben die Beschreibungen der Zielgruppensprache erstellt, indem sie die Wörter und Phrasen aus den Beschreibungen der Zielgrupps ausgewählt haben, die die Kultur, Tradition, Stolz und Exotik betonen. Diese Wörter definieren die Zielgruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie von der weißen Norm.</sample>
    <sample id="702">In dieser Arbeit wurde die Pointwise CXMI erweitert, um den Kontextnutzung auf der Satz- oder Wortebene zu messen.</sample>
    <sample id="703">DrBERT and ChuBERT are both pre-trained language models, but they have some differences. DrBERT is a variant of BERT that uses a larger dataset and a different training objective, while ChuBERT is a variant of BERT that uses smaller datasets and a different training objective. Additionally, DrBERT has a larger model size than ChuBERT.</sample>
    <sample id="751">Drei Autoren sind an der Arbeit beteilt.</sample>
    <sample id="752">Iteratives Transferlernen ist ein Verfahren, bei dem ein Modell iterativ mit neuen Daten aktualisiert wird, um seine Leistung zu verbessern. Es besteht aus mehreren Schritten, bei denen das Modell mit neuen Daten aus jeder Runde aktiver Lernen und Annotationen trainiert wird. Im Gegensatz zu einem kumulativen Ansatz, der alle bisher gesammelten Daten verwendet, aktualisiert das iteratives Transferlernen das Modell nur mit den neuesten Daten. Dies kann dazu führen, dass das Modell ständig verbessert wird und seine Leistung auf dem neuesten Stand bleibt.</sample>
    <sample id="753">The goal of the dataset is to understand users' language when they want to make a choice.</sample>
    <sample id="754">Ein Angreifer kann Modellparameter über einen EaaS extrahiert, indem er die von der EaaS bereitgestellten Daten analysiert und die relevanten Informationen extrahiert. Dies kann durch verschiedene Techniken erfolgen, wie zum Beispiel die Analyse von Datenmuster, die Identifizierung von Schlüsselmerkmalen oder die Anwendung von maschinellem Lernen. Die genaue Methode hängt von der Art des Modells und den spezifischen Anforderungen des Angreifers ab.</sample>
    <sample id="755">Drei Autoren sind an der Arbeit beteilt.</sample>
    <sample id="756">The original dataset was created using 10 annotators.</sample>
    <sample id="757">Die Autoren gehören an der Universität Washington.</sample>
    <sample id="758">I saw bart and lisa, so the governor is on the left.</sample>
    <sample id="759">Der Stand der Technik für Dialogsysteme, einschließlich Chatbots und KI-gestützter Sprachmodelle, basiert auf fortschrittlichen natürlichen Sprachverarbeitung (NLP) und maschinellem Lernen. Diese Systeme nutzen tiefe neuronale Netze, wie Transformer-Modelle (z.B. GPT-3), um Kontextverständnis, Sprachgenerierung und die Beantwortung von Fragen zu verbessern. Die Technologie ermöglicht es, komplexe und natürliche Interaktionen mit Benutzern zu ermöglichen, wobei die Genauigkeit und Relevanz der Antworten kontinuierlich durch Feedback und Weiterbildung verbessert werden.</sample>
    <sample id="760">Wir müssen die Akzeptanz der Modelle über den gesamten Kontextfenster bewerten, um sicherzustellen, dass sie in verschiedenen Kontexten korrekt verwendet werden und nicht nur in einem bestimmten Kontext gut funktionieren.</sample>
    <sample id="761">Ja, das mehrsprachige Training hat zu einem Leistungsabfall im Vergl</sample>
    <sample id="762">Yes, the annotators know the name of the entities in the alternative question.</sample>
    <sample id="763">The MT-Metriken used for evaluation are:

1. **BLEU (Bilingual Evaluation Understudy)**: Measures the overlap between the machine-generated text and the reference text.
2. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Evaluates the overlap of n-grams between the machine-generated text and the reference text, focusing on recall.
3. **METEOR (Metric for Evaluation of Translation with Explicit ORdering)**: Considers both precision and recall, as well as synonymy and stemming.
4. **CIDEr (Consensus-based Image Description Evaluation)**: Specifically designed for image captioning, it measures the consensus among multiple human-generated descriptions.
5. **SPICE (Semantic Propositional Image Caption Evaluation)**: Evaluates the semantic similarity between the machine-generated caption and the reference caption.

These metrics help in assessing the quality and accuracy of machine-generated text, particularly in tasks like translation, summarization, and image captioning.</sample>
    <sample id="764">Die Regression bei der Generalisierung hat keinen signifikanten Einfluss auf bestimmte NER-Typen.</sample>
    <sample id="765">Positionalität ist für NLP wichtig, weil sie die Anordnung von Wörtern in einem Satz berücksichtigt. Dies ist wichtig, weil die Bedeutung von Wörtern von ihrer Position im Satz abhängt. Zum Beispiel kann ein Wort in einem Satz, das an der Spitze steht, eine andere Bedeutung haben als das gleiche Wort in einem Satz, das an einer anderen Stelle steht. Positionalität kann auch helfen, die Bedeutung von Wörtern in einem Satz zu verstehen, wenn sie in einem Satz in verschiedenen Positionen vorkommen. Zum Beispiel kann ein Wortsatz, der in verschiedenen Positionen vorkam, unterschiedliche Bedeutungen haben, je nachdem, in welcher Position das Wort vorkam. Positionalität ist also ein wichtiger Aspekt von NLP, da es die Anordnung von Wörtern berücksichtigt und die Bedeutung von Wörtern im Satz verstehen kann.</sample>
    <sample id="766">Die meisten mehrsprachigen LLMs wie BLOOM werden durch Adapter angepasst, um sie in verschiedene natürliche Sprachen zu übertragen. Diese Adapter sind speziell entwickelt, um die Sprachfähigkeiten von LLMs zu erweitern, ohne dass die Modellarchitektur vollständig angepasst werden muss. Dies ist eine effiziente Methode, um die Sprachfähigkeitsgrenzen von LLMs zu erweitern, da es keine umfangreiche Neuentwicklung der Modellarchitektur erfordert.

Eine vollständige Feinabstimmierung, bei der die Modellarchitektur komplett angepasst wird, ist in der Regel nicht praktikabel, da sie eine enorme Menge an Daten und Rechenleistung erfordert. Zudem wäre es sehr zeitaufwendig und kostspielig, um solche Modelle zu entwickeln und zu trainieren.

Es ist wichtig zu beachten, dass die meisten mehrsprachigen LLMs, einschließlich BLOOM, bereits auf eine Vielzahl von Sprachen trainiert sind und diese Sprachen durch die Adapter-Technik weiter verbessert werden können. Dies ermöglicht es den Benutzern, die Sprachfähigkeiten von BLOOM in verschiedenen Sprachen zu nutzen, ohne dass sie die Modellarchitektur komplettlöschen müssen.

Zusammenfassend kann gesagt werden, dass die meisten mehrsprachlichen LLMs wie BLOOM durch Adapter angepasst werden, um ihre Sprachfähigkeiten in verschiedenen natürlichen Sprachen zu erweitern. Dies ist eine effiziente Methode zur Erweiterung der Sprachfähigkeitsgrenzen, da sie keine umfangreiche Neuentwicklung der Modellarchitektur erforderlich macht.</sample>
    <sample id="767">Das Modell, das für das Transferlernen verwendet wird, ist das CeE-Modell. Es basiert auf der Konzeption von Konsonanten und Distanz und wird auf der Analyse der Datenmenge von 100.000 Anmerkungen über 10.000 Debatten verwendet. Das Modell zeigt bereits ein viel besseres Null-Shot-Performance auf dem annotierten Datensatz als Chance mit dem besten AUC von 0.62. Nach iterativer Feinabstimmung auf beiden Aufgaben findet man, dass die Feinabstimmung des CeE-Tasks gefolgt von weiterer Feinabstimmung auf Debatte ein viel besseres Null-Shot Performance erzielt. Daher wird das CeE-Modell verwendet, um das Transferlernen zu starten.</sample>
    <sample id="768">Die aktuellen Testsets zur Bewertung der PaLM-Fäbigkeiten umfassen eine Vielzahl von Datensätzen, die verschiedene Aspekte der Sprachverarbeitung abdecken. Hier sind einige der wichtigsten:

1. **GLUE (General Language Understanding Evaluation)**: Dies ist ein Benchmark, der verschiedene Aufgaben zur Bewertung der allgemeinen Sprachverarbeitung umfasst, wie z.B. Textvergleich, Sentimentanalyse und Fragenbeantwortung.

2. **SuperGLUE**: Ein weiterer Benchmark, der die Leistung von Sprachmodellen auf verschiedenen Aufgaben wie Textverarbeitung, Textgenerierung und Fragenbeantwortung testet.

3. **SQuAD (Stanford Question Answering Dataset)**: Ein Benchmark, der die Fähigkeit von Sprachmodellen zur Beantwortung von Fragen auf Basis von Textdaten testet.

4. **CoQA (Conversational Question Answering)**: Ein Benchmark, der die Fäh能力 von Sprachmodellen zur Beantwortungs von Fragen in Dialogen testet.

5. **Winograd Schema Challenge**: Ein Benchmark, der die Fähigkeit der Sprachmodelle zur Verständigung von Kontext und zur Beantwortung von Fragen über Ambiguitäten testet.

6. **LAMBADA**: Ein Benchmark, der die Fähigung der Sprachmodelle zur Verständnis von kontextuellen Wörtern testet.

7. **WMT (Workshop on Machine Translation)**: Ein Benchmark, der die Fähgkeit von Sprachmodellen zur maschinellen Übersetzung von Texten testet.

8. **XNLI (Cross-lingual Natural Language Inference)**: Ein Benchmark, der die Fähung der Sprachmodelen zur Verständigung von Texten in verschiedenen Sprachen testet.

Diese Testsets helfen dabei, die Leistung von Sprachmodellen wie PaLM auf verschiedenen Aspekten der Sprachverarbeitung zu bewerten und zu vergleichen.</sample>
    <sample id="769">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="770">The figure shows the constraint distribution of code script. We find code script shows high polymorphism in the generated specific goals. With code script, we can train smaller but specialized models for constraint language planning.</sample>
    <sample id="771">Der/die Referent*in heißt Shuhang.</sample>
    <sample id="772">Ja, die Ergebnisse und der Datensät der Studie können als Benchmark verwendet werden.</sample>
    <sample id="773">In der Arbeit wird experimentiert, wie viele kleinere Modelle (T5) auf geeigneten Datensätzen trainiert werden können, um die Qualität der generierten Skripte von Haarqualität zu erreichen, die größeren Sprachmodellen übertrifft.</sample>
    <sample id="774">OFA (Unified Multimodal Pretrained Model) wird als Basismodell verwendet.</sample>
    <sample id="833">Die Autoren gehören an der Universität von Google Translate.</sample>
    <sample id="834">Stony Brook University.</sample>
    <sample id="835">Die Arbeit untersucht die Sprachpaare Deutsch-Englisch und Deutsch-Französisch.</sample>
    <sample id="836">Der/die Referent*in heißt Shuangbin.</sample>
    <sample id="837">Während der Experimente wurden zwei Modelle untersucht: 
1. Das Modell der langen Impart, das für die Erzeugung von Dokumentebene-Einführungen verwendet wurde. 
2. Das normale basierte Modell der langen Impart, das zur Erzeugung von Satzebene-Einführungen dient.</sample>
    <sample id="838">In MultiInstruct werden 53 der 62 verschiedenen Aufga</sample>
    <sample id="839">Es gibt drei Autoren an der Arbeit: Regina Stöttner, Michael Stöttner und Stefan Stöttner.</sample>
    <sample id="840">Die Autoren haben ihre Experimente auf vier Datensätzen durchgeführt: **ag news**, **mind**, **ssd2** und **eris spam**.</sample>
    <sample id="876">NACHOS is a dataset of medical crown data.</sample>
    <sample id="877">Der/die Referent*in heißt Aid Bilal.</sample>
    <sample id="878">Die Prompt-Strategie hat einen erheblichen Einfluss auf die Ergebnisse.</sample>
    <sample id="879">Die Autoren gehören an der Universität von Cambridge.</sample>
    <sample id="880">1. We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision language tasks, and we will release them soon.
2. This is a QR code for our data and model. Thank you.
3. We are collecting a much larger multimodal dataset with around 150 additional vision tasks, and we will release them soon. 
4. This is a QR code for our data. Thank you.
5. We are collecting a much larger multimodal data set with around 150 additional vision language task, and we will release them soon.</sample>
    <sample id="881">Die Autoren schlagen vor, Modelle zur Nutzung von Informationsquellen aus mehreren Quellen zu testeten, indem sie eine Coreference Resolution Task entwickeln, die darauf abzielt, die Fähigkeit zu bewerten, auf Wissen aus verschiedenen Quellen zurückzugreifen. Sie evaluieren das Datenset mit human study participants und etablieren Coreference Resolution Modelle.</sample>
    <sample id="882">Hallo alle, mein Name ist Aid Bilal und ich werde einen kurzen Überblick über den Paper "Printing Parameter Translation: Assessing Strategies and Performance" geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate.</sample>
    <sample id="883">Baram ist ein Sprachmodell mit 540 Milliarden Parametern, das im Jahr 2022 präsentiert wurde. Es wurde auf einer großen Menge an Texten trainiert, die insgesamt 780 Milliarden Tokens umfassen.</sample>
    <sample id="884">In der Datenverarbeitung erreicht es den Staatsstand der Kunst in Hunderten von NLP-Aufgaben.</sample>
    <sample id="885">In diesem Werk präsentieren wir die erste systematische Studie der 

"large language model prompting" für maschinelle Übersetzung.</sample>
    <sample id="886">Wir haben die Übergangsleistung solcher Modelle mit den besten Praktiken der GPT-Community bewertet. Dies beinhaltet die Verwendung der neuesten Testdatensätze, um eine Überlappung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden.</sample>
    <sample id="887">Wir vergleichen zwei hochmoderne Systeme. Die besten Leistungssysteme sind die WMT-Evaluationen.</sample>
    <sample id="888">Wir verwenden hochmoderne neue LMT-Metriken und zeigen zusätzlich Ergebnisse der Expertenbewertung. Schließlich geben wir einige Empfehlungen für Problemlösungsstrategien.</sample>
    <sample id="889">Die Anweisung hat einen großen Einfluss auf die Leistung von LLMs für Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir eine kurze Anweisung verwendet haben und zwei verschiedene Prompts für die Übersetzung von Sätzen bereitgestellt haben.</sample>
    <sample id="890">Die Mehrheit der Sätze, nämlich 516 von 1000, zeigt einen Unterschied von mehr als einem Blurpunkt.</sample>
    <sample id="891">Und das kann in extremen Fällen bis zu 40 Punkten gehen. Es ist also wichtig, eine gute Ansprache-Strategie zu wählen.</sample>
    <sample id="892">In unseren Experimenten haben wir uns für eine fünf-Schuss-Prompting-Strategie entschieden, bei der wir jedes entsprechende Satz, das wir dem System zur Verfügung stellen, mit der entsprechenden Sprache markieren.</sample>
    <sample id="893">Sure, I'd be happy to help you translate the English content into German! Please provide the English text that you would like me to translate.</sample>
    <sample id="894">Wir haben gesehen, dass die tatsächliche Form der Präsentation keinen großen Einfluss auf den Fall hat, in dem es sich um serielle Schriftpräsentationen handelt.</sample>
    <sample id="895">Es ist entscheidend für Null- und Einschussbefehle und wenn wir, wie in unserem Fall, zu Fünf-Einschussbefehlen gehen, gibt es kaum einen Unterschied zu der tatsächlichen Form des Befehls.</sample>
    <sample id="896">Es sind die Beispiele, die die meisten der Gewichte tragen.</sample>
    <sample id="897">Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Beispielqualität wichtiger ist als die Ähnlichkeit zu der Quelle.</sample>
    <sample id="898">Es ist wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Besonders wir vergleichen die Selektion von Prompts aus dem Trainingsdaten des WMT-Evaluations oder dem Dev-Daten.</sample>
    <sample id="899">Der Dev-Daten ist viel mehr gekürt und mit höherer Qualität als die Trainingsdaten, was bedeutet, dass die Ergebnisse besser sind, wenn man die Dev-Daten verwendet.</sample>
    <sample id="900">The specialized state-of-the-art systems have a substantial advantage over the current translations, but Google Translate comes pretty close to a commercial system. In our case, we chose to overlay it with Google Translate.</sample>
    <sample id="901">Die Erkenntnisse, die wir aus der humanen Evaluierung gewonnen haben, die wir mit dem MQM-Framework durchgeführt haben, sind, dass die Fluenz von Palm vergleichbar mit dem Zustand der Art-Systeme ist, aber die Hauptdifferenz kommt von der Genauigkeit.</sample>
    <sample id="902">In particular, the most common error are omission errors.</sample>
    <sample id="903">Es scheint so, dass Palm den Namen wählt, um eine bessere Übersetzung zu erzeugen, manchmal indem es Teile des ursprünglichen Satzes wegnimmt, die in der Übersetzung verarbeitet werden.</sample>
    <sample id="904">Allerdings ist der Stilaufbau-Kategorie für Panis niedriger als für die Staats-Systeme, was ein zusätzliches Signal darstellt.</sample>
    <sample id="905">"Dieser Parameter liefert wirklich flüssigen Output, aber steht mit einigen Problemen der Genauigkeit."</sample>
    <sample id="906">Und das ist es für diese wirklich kurze Übersicht. Für weitere Details, bitte komme zu meiner vollständigen Präsentation des Papiers. Vielen Dank sehr viel.</sample>
    <sample id="907">Hallo, ich bin Dawei, ein Doktorand an der Universität Stuttgart in Deutschland. In diesem Video möchte ich unsere neueste Arbeit präsentieren: Wie wichtiger ist es, dass wir die Woche über die Woche lernen? Eine kritische Analyse der wöchentlichen Supervision.</sample>
    <sample id="908">Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Maio Smutsba, und Gisela Stephan und Dietrich Klockow.</sample>
    <sample id="909">Ich möchte mit einer kurzen Einführung in die Wochenschau- und Wochenüberwachungslernen beginnen.</sample>
    <sample id="910">In weak supervision, you do not manually label the data. Instead, you label the data using weak labeling sources such as simple heuristic rules, knowledge bases, or low-quality crowd sourcing. As illustrated in the figure on the right.</sample>
    <sample id="911">Im Vergleich zu menschlichen Annotationen sind die visuelle Annotationen viel günstiger, jedoch sind sie auch raucher, was bedeutet, dass ein bestimmter Anteil der Annotationen falsch ist.</sample>
    <sample id="912">Wenn wir neuronale Netzwerke direkt auf wöchentliche Label-Daten trainieren, neigen diese Netzwerke dazu, die Label-Noise zu merken und nicht zu generalisieren.</sample>
    <sample id="913">In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise, so that the trained models still generalize well.</sample>
    <sample id="914">In recent works in WSL, so WSL stands for weekly supervised learning, a common claim is that people say that the only train models on the weekly label data and achieve high performance on clean test sets.</sample>
    <sample id="915">Technisch gesehen ist dieser Anspruch nicht falsch, aber es gibt eine Einschränkung.</sample>
    <sample id="916">Die Aussage, dass Menschen annehmen, dass es ein zusätzliches klares Validierungsset für die Modellauswahl gibt, ist falsch.</sample>
    <sample id="917">Wir haben dieses Problem aufgegeben, da dies bedeutet, dass zusätzliche manuelle Anmerkungen in der regelmäßigen Überwachung benötigt werden, aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen.</sample>
    <sample id="918">The aforementioned doubt leads us to ask three research questions. First, is clean validation data necessary for WSL? Or can we maybe use a noisy validation set instead?</sample>
    <sample id="919">Zweitens, wenn saubere Daten erforderlich sind oder wenn saubere Daten für WSL funktionieren müssen, dann wie viele saubere Proben benötigen wir? Schließlich sollten wir nur saubere Proben für Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen?</sample>
    <sample id="920">Wir haben diese Forschungsfragen in unserer Arbeit angegangen und unsere Ergebnisse sind wie folgt:</sample>
    <sample id="921">Zunächst beobachten wir, dass interessanterweise die neuesten WSL-Methoden tatsächlich reinige Wasserproben benötigen, um ordnungsgemäß zu funktionieren.</sample>
    <sample id="922">Andernfalls besteht ein großer Leistungsabfall. Wie in diesem Bild gezeigt, wenn keine reinen Validierungsstichproben vorliegen, können die Trainingsmodelle nicht über die ursprünglichen Wertebereiche hinaus generalisieren.</sample>
    <sample id="923">Die Übersetzung des englischen Inhaltes "meaning that the training is pointless" in deutscher Sprache lautet:

"Das bedeutet, dass das Training sinnlos ist."

Diese Übersetzung behält den Kern des ursprünglichen englischen Satzes bei, der darauf hinweist, dass die von ihm beschriebene Ausbildung keinen nützlichen oder sinnvollen Zweck hat.</sample>
    <sample id="924">Dies zeigt, dass WSL-Ansätze tatsächlich sauber gelabelte Daten benötigen, um korrekt zu funktionieren, und dass die Kosten für die Erhaltung sauber gelabelter Validierungsdaten nicht übersehen werden sollten.</sample>
    <sample id="925">Unser zweiter Befund ist, dass die Erhöhung der Anzahl rein validierter Proben dazu beiträgt, dass WSL-Ansätze besser abschneiden, wie in der Abbildung auf der linken Seite gezeigt.</sample>
    <sample id="926">Typischerweise benötigen wir nur 20 Stichproben pro Klasse, um hohe Leistung zu erreichen.</sample>
    <sample id="927">Aber das ist nicht das Ende der Geschichte, denn wenn wir entweder Entscheidung treffen, um saubere Proben zu erhalten, dann wird die Ausbildung auf ihnen direkt noch besser sein.</sample>
    <sample id="928">Der rote Bild zeigt die Leistungsunterschiede zwischen funktionierenden Ansätzen, die direkt auf die reinen Daten angewendet werden, und WSL-Ansätzen, die die reinen Daten nur zur Validierung verwenden.</sample>
    <sample id="929">Wie wir sehen können, wenn wir 10 Stichproben pro Klasse haben, beginnt die direkte Feinabstimmung, um die WSL-Ansätze zu schlagen.</sample>
    <sample id="930">Schließlich kann die von früheren WSL-Ansätzen behauptete Leistungssteigerung leicht erreicht werden, indem man es ermöglicht, auf saubere Validationsstichproben fortlaufend feinabzustimmen.</sample>
    <sample id="931">Wie wir aus den Bildern sehen können, ist das Valina-Modell, das als FTW bezeichnet wird, zunächst unterperformt als komplexere WSL-Methoden wie Cosine.</sample>
    <sample id="932">Jedoch, wenn wir weiterhin auf die reinen Proben funktionieren lassen möchten, dann leistet FTW genauso gut wie andere Methoden.</sample>
    <sample id="933">In der Praxis gibt es keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Festplattenspeicher benötigen.</sample>
    <sample id="934">Zusammenfassend zeigen wir, dass neue WSL-Ansätze benötigen saubere, manuell annotierte Beispiele, damit sie ordnungsgemäß funktionieren. Ihre Leistungssteigerung und Praktikabilität werden stark überschätzt.</sample>
    <sample id="935">Unsere konkreten Empfehlungen für zukünftige Arbeit lauten wie folgt:</sample>
    <sample id="936">Zuerst berichte die Modellauswahlkriterien. Zum Beispiel berichte, ob die Modellauswahl mit sauberen Validierungsstichproben durchgeführt wurde.</sample>
    <sample id="937">Zweitens sollten WSL-Ansätze mit fünf Schritten verglichen werden, die auf kleinen Lernen basieren, wie beide auf kleinen Lernen arbeiten. Drittens ist kontinuierliches Feinabstimmung ein einfaches, aber starres Grundgerüst, das in zukünftigen WSL-Arbeiten berücksichtigt werden sollte.</sample>
    <sample id="938">Schritt 1: Identifizieren Sie die Hauptpunkte des Inhalts:
- Open source of our code
- QR code on this slide
- Feel free to check it out
- Thank you and enjoy the conference

Schritt 2: Übersetzen Sie die Hauptpunkte ins Deutsche:
- Open source of our code -&gt; Open-Source von unserem Code
- QR code on this slide -&gt; QR-Code auf diesem Slid
- Feel free to check it out -&gt; Bitte freuen Sie sich darauf, es zu überprüfen
- Thank you and enjoy the conference -&gt; Danke und genießen Sie die Konferenz

Schritt 3: Kombinieren Sie die übersetzten Hauptpunkte zu einem kohärenten Text:

Schritt 4: Überprüfen Sie die Übersetzung auf Grammatikalische Korrektheit und Stil.

Schritt 5: Fügen Sie die Übersetzung zu einem vollständigen Text zusammen:

Schritt 6: Überprüfen Sie den finalen Text auf Konsistenz und Verständlichkeit.

Schritt 7: Fügen Sie den finalen Text in die entsprechende Sprache ein:

Schritt 8: Überprüfen Sie den final Text auf grammatikalische Korrektheit und stilistische Konsistenz.

Schritt 9: Fügen Sie den final Text zu einem vollständigen Text zusammengefasst:

Schritt 10: Überprüfen Sie den final text auf Konsistenz und Verständnis.

Schritt 11: Fügen Sie den final text zu einem vollständigen text zusammen:

Schritt12: Überprüfen Sie den finaltext auf grammatikalische korrektheit und stilistische konsistenz.

Schritt13: Fügen Sie den finaltext zu einem vollständigen text zusammengefasst:

Final Text:

Schritt14: Überprüfen Sie den finalText auf Konsistenz und Verständniss.

Schritt15: Fügen Sie den finalText zu einem vollständigen text zusamt:

Final Text:

Schrit 16: Überprüfen Sie den FinalText auf grammatikalische Korrektheits und stilistische Konsistenz

Schrit 17: Fügen Sie den FinalText zu einem vollständigen Text zusamt:

Final Text:</sample>
    <sample id="939">Gängige Bewertungsmethoden für Dialogeysteme sind menschliche Bewertungen, bei denen Menschen beurteilen, welche der beiden Gespräche besser ist oder bewerten Gespräche auf einer Likert-Skala.</sample>
    <sample id="940">Vier Autoren sind an der Arbeit beteilt.</sample>
    <sample id="941">Das Hintergrundwissen, das im Beispiel mit Servin und Kei benötigt wird, ist das Wissen über die Rolle von Richtern in Gerichten.</sample>
    <sample id="942">Ja, der Code ist verfügbar. Sie können ihn auf GitHub finden, indem Sie den Datensatz und den Code in der verlinkten Paper-Veröffentlichung suchen.</sample>
    <sample id="943">Yes, the annotators for NLPositionality are balanced in terms of demographic groups such as country, gender, etc.</sample>
    <sample id="944">Sätze innerhalb der akzeptable Domain wurden durch das Hinzufügen von "Noise" oder "Perturbationen" durcheinander gebracht.</sample>
    <sample id="945">Eine dimensionale Bewertung bedeutet, dass die Qualität eines Dialogsystems an mehreren verschiedenen Aspekten oder Dimensionen bewertet wird, anstatt nur an einem allgemeinen Maßstab. Dies ermöglicht es, die Stärken und Schwächen des Systems auf einer feineren, gründlicheren Ebene zu verstehen.</sample>
    <sample id="946">Die Autoren gehören zur Universität der Wissenschaften und Technologie von China.</sample>
    <sample id="947">Die Form des Prompts ist in den Fällen von Null- und Ein-Schuss-Prompts entscheidend, und bei fünf Schuss-Prompts gibt es kaum einen Unterschied zur tatsächlichen Form des Prompts.</sample>
    <sample id="978">Die Autoren haben die Dialogmodelle mit den ABC-Eval-Metriken evaluiert.</sample>
    <sample id="979">Der englische Inhalt beschreibt eine kurze Anzeige über ein Papier, das von der Universität der Wissenschaften und Technologie von China stammt. Es wird darauf hingewiesen, dass es sich um ein Modell handelt, das zum Schutz des Urheberrechts von großen Sprachmodellen für Eingebettung und Dienstleistungen dient.

Zur Frage, wie viele Autoren an der Arbeit beteiligt sind, gibt der Text keine spezifische Informationen. Es wird nur von einem Modell gesprochen, das nicht auf die Anzahl der Autoren ausgerichtet ist. Daher kann man nicht sicher sagen, wie viele Autoren an der Arbeitsweise beteiligt sind.</sample>
    <sample id="980">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="981">Eine Person ist an der Arbeit beteiligt.</sample>
    <sample id="982">Das englische Inhalt ist:

"Hello, my name is Vasudha and I am a computer science Ph.D. candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge."

Der/die Referent*in heißt Vasudha.</sample>
    <sample id="983">Die Autoren gehören der Universität Wrocław an.</sample>
    <sample id="1021">Die häufigsten Fehler von PaLm sind **Omissionen**.</sample>
    <sample id="1022">Hallo, ich bin James Finch und ich bin Sarah Finch. Heute werden wir Ihnen alles über ABC Eval, eine neue dimensionale Methode zur Bewertung von Konversationskünstlicher Intelligenz, erzählen.</sample>
    <sample id="1023">Diese Arbeit wurde von der Emory NLP-Lab, angeführt von Professor Gino Choy an der Emory-Universität, durchgeführt und in Zusammenarbeit mit Amazon Alexa AI.</sample>
    <sample id="1024">Stell dir vor, du hast gerade ein Dialogmodell entwickelt und möchtest sehen, wie gut es sich mit dem aktuellen Stand der Technik vergleicht.</sample>
    <sample id="1025">Die übliche Praxis besteht darin, menschliche Bewertungen zu nutzen, wie z.B. durch das Fragen von menschlichen Richtern, welche der beiden Gespräche besser ist oder um Gespräche mit einer Likert-Skala zu bewerten.</sample>
    <sample id="1026">Diese Ansätze funktionieren gut, um umfassende Bewertungen der Gesamtdialogqualität zu liefern, aber die Dialogqualität hat viele Aspekte. Daher könnten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.</sample>
    <sample id="1027">Eine Möglichkeit besteht darin, einfach menschliche Richter dazu zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie zum Beispiel die Relevanz von Modellantworten, indem sie bestehende vergleichende oder Likert-Skale-Methoden verwenden.</sample>
    <sample id="1028">Allerdings glauben wir, dass es eine genauere und zuverlässigere Strategie für die Bewertung von dimensionaler Dialog gibt.</sample>
    <sample id="1029">Unser Ansatz versucht, die Subjektivität menschlicher Bewertungen zu reduzieren, indem wir explizit darlegen, ob jeder Modellantwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel mit irrelevanten Informationen reagieren oder sich widerspricht.</sample>
    <sample id="1030">Wir nennen diesen Ansatz "Annotation von Verhaltensweisen in Chat" oder "ABC-Eval" in Kürze. Wir haben diese Methode entwickelt, um umfassend die Verhaltensweisen von Chatmodellen abzudecken, die in der aktuellen Literatur vorgeschlagen wurden, um die Qualität des Chats zu beeinflussen.</sample>
    <sample id="1031">ABC-Eval ist in der Lage, die Raten zu messen, bei denen Chat-Modelle verschiedene thematische Fehler begehen.</sample>
    <sample id="1032">Zum Beispiel misst die ABC-Evaluation die Anzahl der Turns, in denen ein Chatmodell seinen Partner ignoriert oder etwas unzutreffend sagt.</sample>
    <sample id="1033">Der Inhalt beschreibt eine Reihe von Verhaltensweisen, die in der Sprache des Modells als unangemessen oder unverständlich angesehen werden könnten. Hier ist eine Übersetzung ins Deutsche:

---

**Verhaltensweisen, die in der Modellsprache als unangemessen oder unverstündlich angesehen werden könnteten:**

1. **Selbstwiderspruch oder Widerspruch mit dem Partner:**
   - Der Inhalt beschreibt, dass das Modell selbst widerspruchsvoll oder widerspruchsfähig ist, was bedeutet, dass es sich selbst oder seinen Partner widerspricht.

2. **Verwirrung von falschen Tatsachen oder Verletzung des gesunden Menschenverständnisses:**
   - Der Inhalt besagt, dass das Modell falsche Tatsachen vermittelt oder das gesunde Menschenverständnis verletzt, was bedeutet, dass es falsche Informationen oder falsche Interpretationen verbreitet.

3. **Erfolg oder Misserfolg bei der Anzeige von Empathie:**
   - Der Inhalt bespricht, dass das Modell erfolgreich oder unerfolgreich ist, Empathie zu zeigen, was bedeutet, dass es versucht, die Emotionen und Bedürfnisse anderer zu verstehen und zu respektieren.

---

Diese Beschreibung kann in verschiedenen Kontexten verwendet werden, um zu erklären, warum bestimmte Verhaltensweisen in der Modellsprache als unverständlich oder unangemessen angesehen werden könnten, insbesondere wenn das Modell versucht, Empathie zu zeigen, aber dabei scheitert oder falsche Informationen verbreitet.</sample>
    <sample id="1034">Um herauszufinden, welche Art von Bewertung am effektivsten ist, haben wir vier hochmoderne Chat-Modelle ausgewählt und sie auf 100 menschliche Bot-Konversationen pro Modell mit dem ABC-Evaluationsverfahren bewertet.</sample>
    <sample id="1035">Um eine Vergleichsstudie durchzuführen, haben wir auch diese Gespräche mit drei bestehenden Methoden bewertet: Liquor-Ratings auf der Turn-Ebene, Liquor-Ratings auf der Dialog-Ebene und Dialog-Ebene paarweise Vergleiche.</sample>
    <sample id="1036">Für jede der bestehenden Methoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekten des Dialogs gesammelt, da dies die Standardpraxis für die Bewertung von Chatmodellen auf mehreren Dimensionen ist.</sample>
    <sample id="1037">Aus unserer Analyse dieser Evaluationsergebnisse stellten wir fest, dass die ABC-Verhaltenslabels insgesamt zuverlässiger sind als die Labels, die durch bestehende Methoden gesammelt werden, wie durch die innerhalb der annotator-Agreement auf 100 doppelt markierten Gesprächen gemessen wird.</sample>
    <sample id="1038">Zusätzlich sind die ABC-Eval-Labels besser in der Lage, die Gesprächskwalität im Vergleich zu den Metriken zu prognostizieren, die durch bestehende Methoden erzeugt werden, wie durch die einfache lineare Regressionsanalyse gezeigt wird.</sample>
    <sample id="1039">Zum Beispiel können Sie sehen, wie die Messung des Verhältnisses von Wendungen mit Selbst- und Partner-Widersprüchen die Qualität der Konversation erklärt: 5 % und 10 % respektive, während die durchschnittlichen Liquor-Konsistenzscores nur 4 % oder weniger erklären.</sample>
    <sample id="1040">Schließlich überprüften wir, ob jeder Bewertungsmetrik einen einzigartigen Aspekt der Chat-Qualität erfasst, mithilfe einer stufenweisen linearen Regression.</sample>
    <sample id="1041">Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25% der Qualität der Konversation erklärt, und wenn Sie die Metriken einesmal nacheinander entfernen, resultieren die meisten von ihnen in einem gewissen Grad Verlust an Informationen über die Qualität.</sample>
    <sample id="1042">Auf der anderen Seite erklärt die Kombination aller Turn-Level-Liquor-Metriken wesentlich weniger von der Qualität und weniger von diesen Metriken tragen einzigartige Informationen.</sample>
    <sample id="1043">Diese zuverlässigen, informativen und unterschiedlichen ABC-Eval-Metriken ermöglichen es uns, mit einer höheren Auflösung als bisher möglich, konversationsbasierte KI zu bewerten.</sample>
    <sample id="1044">Sie können sehen, dass in den Ergebnissen unserer Experimente einige Herausforderungen immer noch bestehen und genau quantifiziert wurden. Zum Beispiel haben die Bots, die wir getestet haben, gemeinsame Sinnverstöße in etwa 20 % ihrer Antworten.</sample>
    <sample id="1045">Sie produzieren in etwa 15% der Antworten unangemessene Informationen und widersprechen sich selbst oder ihrem Partner etwa 10% der Zeit.</sample>
    <sample id="1046">Mit der rapiden Verbesserung in diesem Bereich könnten viele dieser Fehlerquoten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, eine Abnahme erleben. Allerdings ist dies umso mehr Grund, zuverlässige und präzise Bewertungsmetriken für die Vergleichsverfahren zu verfolgen.</sample>
    <sample id="1047">Wir hoffen, dass die ABC-Evaluation als bedeutender Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf, wie sich die konversationsbasierte KI in den kommenden Monaten und Jahren weiterentwickelt. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="1048">Die Autoren gehören zur Emory University.</sample>
    <sample id="1049">CFT steht für Cross-Validation Free Training in dieser Arbeit. Es wird als einfache und starke Basiskonzept für die WSL-Ansätze vorgeschlagen, um die Übertragbarkeit und Generalisierbarkeit der Modelle zu verbessern.</sample>
    <sample id="1050">Es gibt 7 Autoren, die an der Arbeit beteiligt sind.</sample>
    <sample id="1051">Hallo, mein Name ist Kai Ouyang und ich werde unsere Arbeit unter dem Titel "Wann benötigt Übersetzung Kontext? Eine datengetriebene mehrsprachige Erkundung" präsentieren. Diese Arbeit wurde in Zusammenarbeit mit Patrick Frennach, Amy Liu, André F. D. Martins und Graham Neubig durchgeführt.</sample>
    <sample id="1052">In this sentence, "molling" is a verb that means "to roll" or "to turn over." Therefore, the translation of the sentence would be:

"Es gibt viele Übersetzungen, die von Kontext abhängen. Zum Beispiel, wie würden wir den englischen Satz 'molling' ins Deutsche übersetzen?"

This sentence is asking for the translation of the word "molling" in the context of the sentence.</sample>
    <sample id="1053">Wenn der vorherige Satz lautet: "Wenn die Minister herausfinden, könnten Dinge gefährlich werden", dann bezieht sich "mo" auf einen Spion. Aber wenn der vorherige Satz lautet, könnte es etwas ernstes sein, Doktor? Dann bezieht sich "mo" auf ein Geburtstagsmerkmal.</sample>
    <sample id="1054">So, depending on the context, the meaning of the word changes and therefore its translation changes as well.</sample>
    <sample id="1055">Allerdings ist es ziemlich schwierig, zu bewerten, wie gut Modelle solche Fälle wie dieses übertragen können. Zunächst, weil nur ein kleiner Teil der Übersetzungen von Kontext abhängig ist, was bedeutet, dass korpusbasierte Metriken wie Blue diese Übersetzungen nicht erfassen können.</sample>
    <sample id="1056">Und einige Menschen haben vorgeschlagen, gezielte Bewertungen für kontextabhängige Übersetzungen durchzuführen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachmengen. Da sie in der Regel auf domänenspezifisches Wissen und menschliche Erstellung angewiesen sind.</sample>
    <sample id="1057">In this work, we attempt to address these two questions: first, when does translation require context, and second, how well do models handle these cases?</sample>
    <sample id="1058">Um die erste Frage zu beantworten, haben wir damit begonnen, wie viel ein Wort von Kontext abhängt, indem wir die Übersetzung zwischen zwei Sprachen betrachtet haben.</sample>
    <sample id="1059">In der vorherigen Arbeit haben wir CXMI als Messgröße für Kontextnutzung durch maschinelle Übersetzungsmodelle eingeführt. Dies wird durch Messung erreicht, wie viel Informationen der Kontext C über das Ziel Y, gegeben der Quelle X, bereitstellt.</sample>
    <sample id="1060">Sie können sich Cxi als die Informationen vorstellen, die man dem Modell durch das Gehen von Kontexten gibt.</sample>
    <sample id="1061">In diesem Werk erweitern wir CXMI auf pointwise CXMI, das den Kontext auf der Satz- oder Wortebene messen kann. Wir können Wörter, die einen hohen p-CXMI haben, als solche betrachten, die Kontext für Übersetzung benötigen.</sample>
    <sample id="1062">Nun analysieren wir Wörter mit hoher PxXMI, um Muster zwischen diesen Wörtern zu suchen.</sample>
    <sample id="1063">Wir führen eine Analyse auf Transkriptionen von TED-Talks durch, die von Englisch in vierzehn verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="1064">Wir führen unsere Analyse auf drei verschiedene Ebenen durch. Zuerst untersuchen wir Teile des Sprachtags, die hohe Mittelwerte im PCXMI haben.</sample>
    <sample id="1065">Dies ermöglicht es uns, beispielsweise Dualpronouns in Arabisch zu finden, die eine höhere Häufigkeit bei XMI haben. Dies kann erklärt werden, da Englisch keine Dualpronouns hat, und daher müssen wir den Kontext berücksichtigen, um zu bestimmen, ob ein Pronomen dual ist, wenn es ins Arabische übersetzt wird.</sample>
    <sample id="1066">Wir finden auch, dass bestimmte Sprachen bei der Auswahl des richtigen Verbformen eine Kontextabhängigkeit erfordern. Wir untersuchen dann die Wörterbücher, die einen hohen P-Sekundärmittel über alle ihre verschiedenen Vorkommen hinweg haben.</sample>
    <sample id="1067">Dies hilft dabei, Fälle zu identifizieren, wie das hier, wo in Chinesisch du den Kontext benötigst, um die richtigen Namen zu übersetzen, um sicherzustellen, dass du innerhalb des Dokuments die gleiche Übersetzung verwendest.</sample>
    <sample id="1068">Wir finden ähnlich, dass ein Kontext unterstützt wird, um in die richtige Formulierung zu übersetzen.</sample>
    <sample id="1069">Schließlich untersuchen wir schließlich verschiedene individuelle Tokens, die einen hohen p-Semmi aufweisen. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern vielmehr in der Satzstruktur ausgedrückt werden. So just ellipsis resolution.</sample>
    <sample id="1070">Nun nutzen wir unsere Erkenntnisse aus unserer Analyse, um ein Benchmark für die Dokumentübersetzung zu entwerfen.</sample>
    <sample id="1071">Für jede der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tags, um automatisch Wörter zu identifizieren, die zum Phänomen gehören. Und wir nennen unseren Tag das multilingue Diskurs-Aware oder Muda-Tag.</sample>
    <sample id="1072">Wir können dann auch bemerken, dass verschiedene Sprachen unterschiedliche Verhältnisse dieser Diskursephänomene haben.</sample>
    <sample id="1073">Wir verwenden dann den Muda-Tagger, indem wir ihn auf den parallelen Korpus anwenden, den wir für die Bewertung verwenden möchten, und wir wenden unsere gewählte Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die der Muda-Tagger identifiziert hat.</sample>
    <sample id="1074">Und schließlich verwenden wir unser Benchmark sowie andere Metriken, um verschiedene Modelle auf der Dokumentebene für maschinelle Übersetzung zu bewerten.</sample>
    <sample id="1075">Zunächst einmal, wenn wir korpusbasierte Metriken verwenden, so finden wir bei Blue, dass kontextabhängige Modellierungsansätze die beste Leistung aufweisen.</sample>
    <sample id="1076">Aber dann, wenn wir comet verwenden, sind kontextbewusste Modelle am besten. Und wenn wir word f messen, haben Modelle mit und ohne Kontext vergleichbare Leistung.</sample>
    <sample id="1077">Dies zeigt wieder einmal, dass es schwierig ist, das beste Dokumentations- oder Übersetzungssystem zu bestimmen, wenn man nur korpusbasierte Metriken verwendet.</sample>
    <sample id="1078">Jetzt verwenden wir den Mooda Benchmark, um Modelle zu bewerten, und wir finden heraus, dass Kontextüberlegte Modelle in bestimmten Diskursphänomenen signifikant genauer sind als Modelle, die keinen Kontext verwenden. Solche Aspekte wie Formulierungen und lexikalische Kohäsion.</sample>
    <sample id="1079">Aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext auf anderen Phänomenen wie Ellipsen, Pronomen und Verbformen verwenden. Das deutet also darauf hin, dass wir noch mehr Fortschritte bei der Dokumentenübersetzung brauchen.</sample>
    <sample id="1080">Wir haben auch verschiedene kommerzielle Systeme verglichen und unsere Benchmark zeigt, dass die Bell für Dokumentübersetzungen generell genauer ist als Google Translate.</sample>
    <sample id="1081">Zusammenfassend durchführen wir eine datengetriebene Analyse über vierzehn Sprachpaare, um herauszufinden, wann Übersetzungen Kontext benötigen.</sample>
    <sample id="1082">Und dann verwenden wir unsere Erkenntnisse, um ein Benchmark für Dokument-Level-Maschinenübersetzung zu erstellen, der uns helfen kann, zu bestimmen, welche Diskursphänomene Modelle gut oder schlecht handhaben können, und welche Übersetzungssysteme gut für Dokument-Level-Übersetzung sind.</sample>
    <sample id="1083">"Vielen Dank für Ihre Aufmerksamkeit. Bis bald in Toronto."</sample>
    <sample id="1084">Der/die Referent*in heißt Yu Sen Zhang.</sample>
    <sample id="1121">Die neue Methode hat den Namen "until every token from the first stage has been visited exactly once."</sample>
    <sample id="1122">The authors describe the method of "marked words" as a technique to identify the words that distinguish marked groups from unmarked ones.</sample>
    <sample id="1123">Die Autoren gehören der Universität Washington an.</sample>
    <sample id="1124">Prague</sample>
    <sample id="1125">Der/die Referent*in heißt James Finch.</sample>
    <sample id="1126">Vier Autoren sind an der Arbeit beteilt.</sample>
    <sample id="1127">Datensätze wie Penn Treebank, CoNLL-2000, und Universal Dependencies können zum Testen syntaktischer Phänomen verwendet werden.</sample>
    <sample id="1161">WSL, WSL, WSL, WSL, W</sample>
    <sample id="1162">Das Modell wird auf 11 biomedizinischen und klinischen Aufgaben eingesetzt, um seine Leistung zu bewerten.</sample>
    <sample id="1226">CamemBERT wurde ursprünglich mit den Daten von 4 gigabyte Subset von natürlich show comparabel result to those train on the 4 gigabyte subset of natürlich show comparable result to those.</sample>
    <sample id="1227">Der/die Referent*in heißt Adam Skurkowski.</sample>
    <sample id="1228">The experiment showed that the performance of the model degraded with a larger temporal gap, confirming the hypothesis that temporal drift is the main cause of the performance drop.</sample>
    <sample id="1269">Es ist notwendig, die Token für die Ausgangssprache zu permutieren, um die korrekte Reihenfolge zu erhalten.</sample>
    <sample id="1270">Die Autoren empfehlen, dass Modellentwickler*innen die Transparenz ihrer Methoden zum Abbau von Vorurteil aufzeigen sollten, um die Ursachen für positive Stereotypen zu verstehen. Dies könnte aufgrund von "weird overly excessive value alignment" oder anderen "anti-stereotyping methods" sein, die zu "pernicious patterns" führen. Die Transparenz hilft, die Wirkung dieser Methoden besser zu beurteilen und potenzielle Verzerrungen zu identifizieren.</sample>
    <sample id="1271">Inakzeptable Minimalpaareingaben sind ungrammatische Sätze, die in einem minimalen Paar Paradigma nicht korrekt sind.</sample>
    <sample id="1272">The authors have used the weight and tokenizer of permit bird to train on the 4 gigabyte subset of natchez, showing comparable results to those obtained with a doctor built 4 gigabyte from scratch.</sample>
    <sample id="1273">Die Metrik, die verwendet wurde, um die Übereinstimmungzwischen den Kommentatoren zu messen, war der InterAnnotator-Agreement. Dieser wurde auf 100 doppelbenannten Gesprächen gemessen, um die Reliabilität der ABC-Eval-Verhaltenslabels zu bestimmen.</sample>
    <sample id="1274">Die Domain, die gewählt wurde, um völlig unzusammengehörige Sätze zu den inakzeptabel und akzeptablen Suchanfragern hinzuzufügen, ist **Wikipedia**.</sample>
    <sample id="1275">Die Autoren gehören an der Universität Stuttgart.</sample>
    <sample id="1276">MultiInstruct is a benchmark that focuses on improving the zero-shot performance on language-only tasks, while other benchmarks have left out computer vision and multimodal tasks. MultiInstruct aims to investigate whether instruction tuning on multimodal pre-trained models can actually improve generalization to unseen multimodal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in the availability of instruction datasets between NLP and multimodal, with more than 1,600 language-only instruction tasks available, but no large-scale publicly available multimodal instruction task. Therefore, this motivated us to build a multimodal instruction tuning dataset.</sample>
    <sample id="1277">Zwei Autoren sind an der Arbeit beteilt.</sample>
    <sample id="1278">Die Definition der binären Koordination lautet: "Die binäre Koordination ist die Verbindung zweier Wörter, die in einem Satz zusammenstehen, wobei die beiden Wörter gleichwertig sind und keine einer von ihnen abhängig ist."</sample>
    <sample id="1279">Die in dieser Studie verwendeten Prompter waren im Durchschnitt 1000 Zeichen lang.</sample>
    <sample id="1280">The results indicate that smaller models can surpass larger models when properly trained on suitable data sets.</sample>
    <sample id="1281">Hallo, ich bin Janis Lavaque und ich werde Ihnen unsere Arbeiten über den Doktor Bert vorstellen, ein robustes vorgefertigtes Modell in Französisch für die biomedizinische und klinische Domäne.</sample>
    <sample id="1282">In diesem Vortrag werden wir zunächst über die Sprachmodellierung in der Gesundheitsversorgung sprechen. Anschließend werden wir die Hauptbeiträge unseres Artikels präsentieren.</sample>
    <sample id="1283">Wir haben den ersten biomedizinischen Modell ersetzt, der in Französisch namens "Doktor Bert" heißt und auf der Basis von "Roberta" basiert und auf "Natchez" trainiert wird, welches ein Datensatz von medizinischen kronen Daten ist.</sample>
    <sample id="1284">Wir haben auch eine Vergleichung von Modellen mit mehreren prädiktiven Einstellungen und Datenquellen eingeführt. Dann präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Datenaufgaben in Französisch.</sample>
    <sample id="1285">Schließlich fassen wir die Ergebnisse der Experimente zusammen und geben Ihnen weitere Details darüber, wie man auf die Modelle zugreifen kann.</sample>
    <sample id="1286">Seit seiner Veröffentlichung im Jahr 2018 hat BERT eine der effektivsten Ansätze zur Lösung natürlicher Sprachverarbeitungstechniken geworden und bietet einen enormen Leistungsgewinn im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2Vec.</sample>
    <sample id="1287">Seitdem wurde dieses Modell an vielen anderen Sprachen angepasst, wie zum Beispiel in Französisch mit Camembert und in anderen Bereichen wie der Medizin mit Päramentbärtchen und Biobärtchen. Und auf klinischen Gebieten wurde es mit klinischen Bildern, aber hauptsächlich auf Englisch, angepasst.</sample>
    <sample id="1288">Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlicher Präzisierung aufgrund des Mangels an in-dominanten Daten.</sample>
    <sample id="1289">Allerdings hatte Frankreich bis heute keine neue Open-Source-Software für biomedizinische Anwendungen.</sample>
    <sample id="1290">Wir fragen uns selbst, welche Datenquellen für eine breite Palette von Anwendungen am besten geeignet sind. Diese Datenquellen sind eine gute Substitution für klinische Daten.</sample>
    <sample id="1291">Um diese Frage zu beantworten, vergleichen wir den Doktor Bildt mit unserem Schubert-Modell, das auf anonymisierten Daten basiert, die aus dem Nicht-Universitätskrankenhaus am Haus stammen.</sample>
    <sample id="1292">Nachdem wir uns gefragt haben, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren, ist es 4 Gigabyte, 8 Gigabyte oder mehr?</sample>
    <sample id="1293">Um diese Frage zu beantworten, trainieren wir zuerst und vergleichen vier Modelle von Grund auf. Ein erster Version von "Doctor Belt" mit 7 GB von natürlichen Daten. Eine zweite Version von 4 GB von natürlichen Daten.</sample>
    <sample id="1294">Die erste Version von Schubert, die ein klinisches Modell ist, hat vier Gigabyte an Sätzen, die aus klinischen Notizen entnommen wurden. Die endgültige Version von Schubert hat eine Mischung aus vier Gigabyte an Sätzen aus verschiedenen Quellen und vier Gigabyte an klinischen Notizen.</sample>
    <sample id="1295">In addition to this comparison, we introduce three model train on country pretraining to analyze the impact of pretraining strategies.</sample>
    <sample id="1296">Einer basiert auf dem Gewicht von Kammerbären und trainiert auf vier Gigabytes eines Sets von Naturschriften. Der andere basiert ebenfalls auf Kammerbären, aber trainiert diesmal auf vier Gigabytes von Klinkernoten.</sample>
    <sample id="1297">The English text is about a base of an English biomedical model, which is built and trained on four gigabytes of set of snatches. In total, there are seven models.</sample>
    <sample id="1298">Um unsere sieben Modelle zu bewerten, haben wir uns auf die öffentlichen und privaten Aufgaben konzentriert, wie z.B. Namenserkennung, Klassifizierung, Sprachverarbeitung und Fragebogenbewertung.</sample>
    <sample id="1299">Diese Modelle werden mit sechs baseline-Modellen verglichen, die folgende sind: Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CineNet 4 GB, Permet Bild, BioBild und Clinik Bild.</sample>
    <sample id="1300">Die Evaluation von entsprechenden Highlights, die das Modell am besten auf die Aufgabe mit Daten der gleichen Natur hat, hat uns entsprechend ermittelt, auf welche die Modelle trainiert wurden.</sample>
    <sample id="1301">However, we have we can obtain the data from  uh  we can observe that data from interregional sources appear to be more versatile. We also observe that using more data translates into better performance.</sample>
    <sample id="1302">Insgesamt scheint das "from scratch" Training auf den meisten Aufgaben eine höhere Leistung zu erzielen.</sample>
    <sample id="1303">However, our experiment on quantum pre-training using the weight and tokenizer of permit bird train on the 4 gigabyte subset of natchez show comparable results to those for obtain with  uh  doctor bert for gigabyte from scratch.</sample>
    <sample id="1304">Der englische Text beschreibt, dass das Modell, das auf den Komponentenweisen und Tokenisierung basiert, Stabilitätsprobleme hat.

Die deutsche Übersetzung lautet:

Der englische Text beschreibet, dass das Modell, das auf den komponentenweisen Gewichten und der Tokenisierung basiert, Stabilitätsschwierigkeiten hat.

Der englische Text beschäftigt sich mit einem Modell, das auf den Komponenten weisen und der Tokenisierung basiert. Dieses Modell hat jedoch Stabilitätsprobleme.

Der englische Text besagt, dass das Modell, das auf Komponentenweisen Gewichten und Tokenisierung basiert, Stablitätsprobleme hat.

Der englische Text bezieht sich auf ein Modell, das auf Komponentenweise Gewichten und der Tokenisierung beruht und Stabilitätsprobleme hatte.

Der englische Text bespricht ein Modell, das auf Komponenten weisen und der Tokenisierungsmethode basiert und Stabilitätsprobleme aufweist.

Der englische Text bescheidet sich mit einem Modell, das auf Komponentenweiser Gewichtung und der Tokenisierung basiert und Stabilitätsprobl</sample>
    <sample id="1305">Schließlich hat eine Schlussfolgerung, dass unser System besser auf neun von elf Don'ts-Tasks performiert und global den Ergebnissen des generischen Modells hier übertrifft.</sample>
    <sample id="1306">Wir beobachten auch, dass spezialisierte Daten besser sind. Mehr spezialisierte Daten sind besser, aber sie skalieren nicht gut.</sample>
    <sample id="1307">Alle vorgefertigten Modelle, die man von Natürlichs erhält, sind kostenlos und auf der GitHub-Seite verfügbar. Alle Trainingsskripte sind auf unserem GitHub-Repository zu finden.</sample>
    <sample id="1308">Danke für diese Präsentation und wir freuen uns auf die Aktionen in der nächsten Sitzung.</sample>
    <sample id="1309">Die Arbeit untersucht drei verschiedene Lernstrategien: 
1. Trainieren und vergleichen von vier Modellen von "Doctor Baird" mit 7 GB von Natürlichkeit, 
2. Trainieren und vergleichen von vornehmlich 4 GB von Natürlichkeit,  und 
3. Trainieren und vergleichen von 4 GB von Natürlichkeit und 4 GB von klinischen Notizen. 
Zusätzlich wird eine Kombination von 4 GB von Natürlichkeits- und 4 GB von klinischen Notizdaten verwendet. 
Die Arbeit untersucht auch die Auswirkungen von Prätrainingsstrategien.</sample>
    <sample id="1310">Der Faktor der Überanpassung ist größer als 1.</sample>
    <sample id="1311">Die Qualität der Vereinfachung wurde beurteilt, indem die Scores und die Evaluationsmetriken der Experimente in der Arbeit betrachtet wurden. Die Autoren fanden heraus, dass die grundlegende Feinabstimmung die Scores besser erreicht hatte als die Baseline. Die Autoren stellten diese Ergebnisse als Benchmarks für zukünftige Forschungen dar.</sample>
    <sample id="1312">Ja, Sprachmodelle haben unterschiedliche politische Vorur-teile.</sample>
    <sample id="1313">Hallo, mein Name ist Matthias Lindemann und heute werde ich Ihnen eine kurze Einführung in unsere Arbeit über kompositional Generalisierung ohne Bäume geben. Wir nutzen Multiset-Tagging und Latente Permutationen.</sample>
    <sample id="1314">Dies ist eine gemeinsame Arbeit mit meinen Beratern Alexander Kola und Ivan Titov.</sample>
    <sample id="1315">Kompositional Generalization kann als die Fähigkeit eines Lerners verstanden werden, tiefere Rekursion und unbekannte Kompositionen von Phrasen zu behandeln, die während der Ausbildung einzeln gesehen wurden.</sample>
    <sample id="1316">In the context of semantic parsing, testing for compositional generalization might look like this: As usual, we have a training set of utterances, in this case, "The girl slept" and "Mary knew that the girl slept."</sample>
    <sample id="1317">Diese Attributen werden mit logischen Formen verbunden, die die Kernaspekte ihrer Bedeutung darstellen.</sample>
    <sample id="1318">Im Gegensatz zu der Standard-Maschinenlernungsevaluation kommt der Testset nicht aus derselben Verteilung, sondern enthält strukturierungslosige logische Formen.</sample>
    <sample id="1319">In diesem Beispiel hat das Modell bei der Ausbildung tiefere Rekursion gesehen und wird auf Beispiele getestet, die eine tiefere Rekursion aufweisen.</sample>
    <sample id="1320">Naive sequence-to-sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input.</sample>
    <sample id="1321">In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color-coded in the example.</sample>
    <sample id="1322">Eine beliebte Methode, um dies zu lösen, besteht darin, Bäume in die Modelle zu integrieren.</sample>
    <sample id="1323">Die Bäume sollen die kompositorische Prozess darstellen, der Zusammenhänge zwischen Ausdrücken und logischen Formen vermittelt.</sample>
    <sample id="1324">Dies funktioniert gut, aber Bäume werden in der Regel nicht gegeben und müssen auf irgendeine Weise erhalten werden.</sample>
    <sample id="1325">Dies kann ein komplexes und manchmal rechenrechenswertes Prozess sein. Typischerweise beinhaltet dies erhebliche formalistische Vorbereitung der logischen Formen, zum Beispiel, um Variablen-Symbole zu verarbeiten.</sample>
    <sample id="1326">Die Beschaffung von Bäumen kann auch spezialisierte Grammatik-Induktionsverfahren umfassen.</sample>
    <sample id="1327">In diesem Papier verwenden wir keine Bäume und führen ein neuronales Sequenz-zu-Sequenz-Modell ein, das die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs direkt modelliert.</sample>
    <sample id="1328">Zum ersten Mal zeigen wir starke Generalisierung zu tiefer Rekursion ohne auf Bäume angewiesen zu sein.</sample>
    <sample id="1329">Unser Ansatz schätzt die Ausgabe aus dem Eingabewert in zwei Schritten vor.</sample>
    <sample id="1330">Zuerst taggen wir jeden Eingabetoken mit einem ungeordneten Multiset von Tokens, die im Ausgang erscheinen werden.</sample>
    <sample id="1331">Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht geordnet.</sample>
    <sample id="1332">Deswegen verwenden wir in Schritt 2 ein anderes Modell, um die Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen.</sample>
    <sample id="1333">Wir führen eine neue Methode ein, um eine Permutation vorherzusagen, die keine strenge Einschränkungen auf die möglichen Permutationen auferlegt. Dies macht unsere Methode ziemlich flexibel und ausdrucksstark.</sample>
    <sample id="1334">Konzeptionell funktioniert unser Permutationsmodell etwa so:</sample>
    <sample id="1335">Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Multiset-Token in jedem Position zu setzen. Für die erste Ausgabeposition wählen wir einfach eins, wie in Rot hervorgehoben.</sample>
    <sample id="1336">Dann gehen wir zum nächsten Multiset-Token über, um zu bestimmen, welches zweite Token im Ausgabedaten ist.</sample>
    <sample id="1337">Wir bestimmen den dritten Token im Ausgang in einer ähnlichen Weise, indem wir zu einem anderen Multiset-Token springen. Wir führen diesen Prozess fort.</sample>
    <sample id="1338">Bis jeder Token aus dem ersten Stadium genau einmal besucht wurde.</sample>
    <sample id="1339">Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen treulosen Modellen auf dem Cogs Benchmark. Unser Modell erreicht die anderen um einen großen Abstand und generalisiert auf tiefere Rekursionen.</sample>
    <sample id="1340">"Einige andere Arten von struktureller Generalisierung bleiben jedoch sehr herausfordernd."</sample>
    <sample id="1341">In unserer Arbeit haben wir ein paar interessante technische Herausforderungen gelöst.</sample>
    <sample id="1342">Erstens ist die Ausrichtung zwischen Eingabe und Ausgabe nicht im Trainingsdaten gegeben. Folglich wissen wir für ein bestimmtes Token nicht, aus welchem Multiset es stammt. Dies stellt eine Herausforderung für das Training dar.</sample>
    <sample id="1343">In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training.</sample>
    <sample id="1344">Unsere Permutationsmethode ist sehr flexibel, aber sie bringt die Herausforderung mit sich, dass die höchste Bewertungspersmutation in NP-Hard ist. Das liegt daran, dass dies mit dem Reiseverkäuferproblem zusammenhängt.</sample>
    <sample id="1345">Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die auch es uns ermöglicht, durch die Lösung zurückzutreten und die linguistisch plausiblen Permutationen zu lernen.</sample>
    <sample id="1346">Wenn Sie mehr über unsere Experimente und wie wir diesen Herausforderungen begegnen möchten, schauen Sie sich bitte unsere Arbeit an oder besuchen Sie unser Poster.</sample>
    <sample id="1347">Cognitive dissonance is the mental discomfort experienced by a person who holds two or more contradictory beliefs, values, or ideas at the same time. It is an important problem to study in language because it can affect how people communicate and understand each other. When people experience cognitive dissonance, they may try to reduce the discomfort by changing their beliefs or behaviors, or by justifying their existing beliefs. This can lead to misunderstandings and conflicts in communication.</sample>
    <sample id="1348">GPT-4.</sample>
    <sample id="1349">Yes, cumulative training performed equal or better than iterative training across the board.</sample>
    <sample id="1350">The Referent*in is Sarah Papi.</sample>
    <sample id="1351">Die Daten für die MuDa-Benchmark stammen aus Transkriptionen von TED Talks, die ins Englische übersetzt wurden und dann in vierzehn verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="1385">Der/die Referent*in heißt Matias Lindemann.</sample>
    <sample id="1386">Sprachübergreifender Transfer bezieht sich auf die Fähigkeit, Wissen und Fähigkeiten von einer Sprache auf eine andere zu übertragen. In der Sprachverarbeitung und maschinellen Übersetzung bezieht sich Sprachübergreifender Transfer auf die Fähigkeit, Wissen aus einer Sprache in eine andere zu übertragen, um die Übersetzung von Texten oder Sprachen zu verbessern. Dies kann durch verschiedene Methoden erreicht werden, wie zum Beispiel durch das Training auf einer Quelle-Sprache und die Übertragung auf eine andere Sprache oder durch das Training auf einer Quelle-Sprachenkombination und die Übertragung auf eine zweite Sprache.</sample>
    <sample id="1387">Die Autoren gehören an der Saarland-Universität in Deutschland.</sample>
    <sample id="1388">The authors use latency measures to evaluate the computational awareness of their models. They consider both the average latency and the average latency that accounts for the models' computational times to produce the output.</sample>
    <sample id="1389">Hallo alle, ich bin Max Schäfer und heute präsentieren mein Kollege Martin und ich unsere Arbeit "The KITMASTERS: Evaluating Knowledge Integration from Multiple Sources". Diese Arbeit ist eine Zusammenarbeit zwischen McGill University, Mila und Microsoft Research.</sample>
    <sample id="1390">Natürliche Sprachverständigungsmodelle nutzen eine Vielzahl von Wissensquellen, wie zum Beispiel Wissen, das in ihren Parametern enthalten ist, das normalerweise durch eine vorherige Ausbildung erworben wird, und Wissen, das in den Eingaben zur Inferenzzeit gegeben wird.</sample>
    <sample id="1391">Zusammenfassung:

In den letzten Jahren haben Arbeiten in der Fragebeantwortung gezeigt, dass Modelle vorausschauendes Wissen nutzen können, um Aufgaben zu lösen. Diese Arbeiten zeigen, dass Modelle, die bereits auf großen Datensätzen trainiert wurden, in der Lage sind, auf neue Fragen einzugehen, indem sie ihre bestehenden Kenntnisse anwenden und neue Informationen integrieren. Diese Fähigkeit, vorausschauendes Wissen zu nutzen, ist besonders nützlich in der Fragebeantwortung, da sie es ermöglicht, auf unerwartete Fragen zu reagieren und präzise Antworten zu geben.

Zusammenfassung auf Deutsch:

In den letzten Jahrzehnten haben Arbeiten in der Fragebeurteilung gezeigt, dass Modelle vorgeschlagene Wissen nutzen können, um Aufgabenerledigung zu erreichen. Diese Arbeiten zeigen, daß Modelle, die bereits auf großem Datensatz trainiert wurden, in der Lage sein, auf neue Fragen einzugehend, indem sie ihre bestehende Wissen nutzen und neue Informationen integrieren. Die Fähigkeit, vorgeschlagene Wissen zu nutzen, ist besondere nützlich in der Fragebeurteilung, da sie es ermöglichen, auf unerwartete Fragen zu antworten und präzise Antworten zu liefern.</sample>
    <sample id="1392">Aber natürliche Sprachverständigung benötigt oft Wissen, das auch zur Inferenzzeit bereitgestellt wird.</sample>
    <sample id="1393">John sah den neu gewählten Präsidenten auf dem Fernsehen.</sample>
    <sample id="1394">Die vorgeführten Parameter können Informationen über, was Präsidenten tun und was eine Tätigkeit ist, enthalten, aber sie können nicht zuverlässig wissen, wer das instanzspezifische Entität John ist oder wer der neue Präsident ist, weil der Präsident sich seit der Vorbereitung geändert hat.</sample>
    <sample id="1395">Daher benötigen erfolgreiche Modelle für wissensintensive natürliche Sprachverarbeitung (NLU) Aufgaben die Fähigkeit, sowohl Prä-Trainingszeit- als auch Inferenzzeit-Wissen zu integrieren und zu nutzen.</sample>
    <sample id="1396">In diesem Werk schlagen wir eine Diagnostik-Test-Serie für Wissensintegration vor.</sample>
    <sample id="1397">Wir führen eine KoReferenzauflösungsaufgabe ein, die darauf ausgelegt ist, die Fähigkeit zu testen, auf das Wissen zuzugreifen, das in verschiedenen Quellen verfügbar ist. Wir bewerten das Datenset mit menschlicher Studiepraxis und etablieren KoReferenzauflösungsmodelle.</sample>
    <sample id="1398">Sure, here's the translation:

"Here is an example from our dataset:

Sevin is a judge. Kya is a baker. Sevin and Kya met at a park. After a long day at work, deciding cases in a law court, he was happy to relax."</sample>
    <sample id="1399">Der englische Text lautet: "The task here is to identify the correct entity that the pronoun he refers to, which in this case is servant."

Der deutsche Übersetzungssatz lautet: "Die Aufgabe hier besteht darin, die richtige Entität zu identifizieren, zu der der Pronomen "er" sich bezieht, was in diesem Fall der Diener ist."</sample>
    <sample id="1400">Die Auflösung eines bestimmten Pronomens erfordert zwei Arten von Information: Erstens ist spezifisches Wissen über die Entität erforderlich, wie zum Beispiel, dass Servin ein Richter ist. Zweitens ist Hintergrundwissen erforderlich, wie zum Beispiels, dass Richter Fälle in Gerichten entscheiden.</sample>
    <sample id="1401">Im Allgemeinen wird allgemeines Wissen während der Vorbereitung von großsprachigen Modellen gelernt, während spezifisches Wissen zu einem Entität beobachtet wird.</sample>
    <sample id="1402">Wir variieren die Verfügbarkeit dieser beiden Informationen, so dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können.</sample>
    <sample id="1403">Wir haben drei Szenarien von KITMOS definiert. Zuerst wollen wir die typische Szenarien betrachten: Hintergrundvorausbildung. Hier wird angenommen, dass das Hintergrundwissen in der Vorbereitungsphase verfügbar ist.</sample>
    <sample id="1404">Zweitens gibt es das Background-Both-Szenario, bei dem das Hintergrundwissen sowohl während des Trainings als auch während der Inferenz verfügbar ist. Schließlich gibt es das Background-Inferenz-Szenario, bei dem beide Wissensarten nur während der Inferenz verfägbar sind.</sample>
    <sample id="1405">Diese letzte Szenarien ist besonders interessant, da es die Situation simuliert, in der das Hintergrundwissen notwendig ist, um eine Aufgabe zu lösen, aber nicht Teil des vorgefertigten Datenmodells ist. Zum Beispiel, weil neue Berufe seit der Zeit der Vorbereitung entstanden sind.</sample>
    <sample id="1406">Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit eines Faktors für zwei Quellen steuern können:

Angenommen, wir haben zwei Quellen, die für die Bereitstellung eines bestimmten Produkts verantwortlich sind. Wir möchten sicherstellen, dass das Produkt immer verfügbar ist, indem wir die Verfügbarkeit der Quellen kontrollieren.

Um dies zu tun, können wir die Verfügbarkeit der beiden Quellen überprüfen und sicherstellen, dass sie sich nicht gegenseitig ausschließen. Wir können dies tun, indem wir die Verfügability der Quellen überprüfen und sichergestellt, dass sie sich nicht gegenseitige Auswirkungen haben.

Wenn eine Quelle nicht verfügbar ist, können wir die andere Quelle verwenden, um das Produkt zu bereitstellen. Wir können dies tun, indem wir eine Regel erstellen, die bestimmt, welche Quelle verwendet wird, wenn eine andere Quelle nicht verfügbar ist.

Wir können auch die Verfügbarkeit der Quelle überprüfen, indem wir sie regelmäßig überprüfen und sicherstellen. Wir können dies tun, indessen, indem wir eine Regel erstellen.

Wenn eine Quelle nicht mehr verfügbar ist, können wir sie ersetzen oder durch eine andere Quelle ersetzen. Wir können dies tun, indessen, wenn wir eine Regel erstellen, die bestimmt, welche Quelle verwendet wird.

Wenn wir die Verfügbarkeit der zwei Quellen kontrollieren, können wir sicherstellen, dass das Produkt stets verfügbar ist. Wir können dies tun, indessen, durch die Überprüfung der Verfügability der Quellen und die Einführung von Regeln, die bestimmen, welche Quelle verwendet wird und wie die Quellen umgehen sollten.</sample>
    <sample id="1407">In the background pre-train setting, we assume that the background knowledge politicians seek elected seats in government is contained in the pre-train parameters. In the inference time context, we provide the anti-specific knowledge that Chester is a politician.</sample>
    <sample id="1408">In der Hintergrund-Boß-Situation bieten wir nicht nur antizyklische, sondern auch Hintergrundwissen über Politiker in der Interessenskontext an.</sample>
    <sample id="1409">In the background interference setting, the professional occupation is provided as "military tour" instead of "politician" because a military tour is unlikely to be contained in the pre-trained parameter.</sample>
    <sample id="1410">Wir haben das Datenset sowohl mit menschlichen Studiepartizipanten als auch mit etablierten Referenz-Lösungsmodellen bewertet. In diesem Bild zeigen wir die Ergebnisse der besten Leistungsmodelle auf dem schwierigsten Variante des vorgefertigten Trainingsmodells.</sample>
    <sample id="1411">Ohne spezifische Ausbildung auf KITMUS beide Modelle erzielen nicht gute Ergebnisse. Wenn jedoch auf KITMUS trainiert wird, erzielen sowohl C2F als auch BERT4CUREF signifikant bessere Ergebnisse als der zufällige Wahl.</sample>
    <sample id="1412">Dies schlägt vor, dass, wenn man auf allgemeine Klassifizierungslösungsdatensätze trainiert wird, Maschinen lernen müssen, Surfaces zu erkennen, die nicht nützlich sind, wenn man sie auf Kittmosen testet, wo solche Surfaces entfernt wurden.</sample>
    <sample id="1413">Zusätzliche Experimente mit fiktionalem Wissen zeigen, dass selbst die besten performierenden Modelle nicht in der Lage sind, das Hintergrundwissen zu verarbeiten, das nur in der Inferenzphase verfügbar ist.</sample>
    <sample id="1414">Um die Hauptpunkte unserer Arbeit zusammenzufassen: Viele Korreferenzrevolutionsmodelle sind in der Lage, Wissen aus verschiedenen Quellen ohne spezifisches Task-basiertes Training zu verstehen. Allerdings erfolgen mit Task-basiertem Training einige Modelle erfolgreich die Integration von Wissen aus mehreren Quellen.</sample>
    <sample id="1415">Noch immer scheinen sogar die besten performierenden Modelle Schwierigkeiten mit der zuverlässigen Integration von Hintergrundwissen zu haben, das nur zur Auswertungszeit präsentiert wird. Wenn Sie mehr Details interessiert sind, sehen Sie sich unsere Arbeit an und untersuchen Sie das Datenset und den Code auf GitHub. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="1416">Die Nachteile der baumbasierten (tree-based) Methoden sind:

1. **Komplexität der Vorbereitung**: Die Erstellung von Bäumen erfordert oft umfangreiche formalistische Vorverarbeitung der logischen Formen, was rechenintensiv und zeitaufwendig sein kann.

2. **Computationskosten**: Der Prozess kann rechenintensiv sein, insbesondere bei großen Datenmengen oder komplexen logischen Formen.

3. **Spezifische Voraussetzungen**: Die Methode erfordert oft spezielle Grammatik-Induktionsverfahren, was die Implementierung und Anpassung erschwert.

4. **Datenabhängigkeit**: Die Qualität und Genauigkeit des Baumbasis-Methoden können stark von der Struktur und Qualität der eingegebenen Daten abhängen.

5. **Fehleranfälligkeit**: Die Methode kann empfindlich auf Fehler in der Daten oder der Logik reagieren, was zu falschen Ergebnissen führen kann.

6. **Interpretierbarkeit**: Die Ergebnisse von Baumbasis-Methoden können schwer zu interpretieren sein, insbesondere wenn die Bäume komplex sind.

7. **Datenmenge**: Die Methode kann bei großen Datenmengen rechenintensiv sein und möglicherweise nicht effizient genug sein.

8. **Spezifische Anwendungen**: Die Methode ist oft spezifisch für bestimmte Anwendungen und kann nicht allgemein angewendet werden.

9. **Datenqualität**: Die Methode erfordert häufig hohe Qualität der eingegebenen Dokumente, was nicht immer der Fall sein kann.

10. **Datenmenge**: Die Methodendatenmenge kann rechenintensiv sein und mögliche Fehler in der Datenmenge können zu falschen Ergebnissen führend sein.</sample>
    <sample id="1417">The authors of the paper belong to the University of Hong Kong.</sample>
    <sample id="1418">Hallo, ich bin Maya und heute werde ich über unsere Arbeit sprechen, die darauf abzielt, Stereotypen in Sprachmodellen mit natürlichen Sprachanweisungen zu messen. Diese Arbeit wird in Zusammenarbeit mit Esindermush und Danjerowsky durchgeführt.</sample>
    <sample id="1419">In den letzten Jahren wurden viele Dokumente über die Verbreitung sozialer Vorurteile und Stereotypen in großen Sprachmodellen oder LLMs veröffentlicht.</sample>
    <sample id="1420">Jedoch haben diese Maßnahmen verschiedene Einschränkungen. Sie setzen normalerweise auf handgefertigte Datensätze, die sehr zeitaufwändig zur Erstellung sind.</sample>
    <sample id="1421">Und sie messen auch normalerweise nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut allgemeinisieren können, um andere Demografien oder Kontexte anzupassen, oder sie erfassen einfach sehr allgemeine breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen.</sample>
    <sample id="1422">Darüber hinaus berücksichtigt die meiste Arbeit in diesem Bereich die Intersektionalität nicht, was die Idee ist, dass vielseitige soziale Identitäten Biases verstärken und einzigartige Schäden verursachen können.</sample>
    <sample id="1423">Um diese Einschränkungen zu überwinden, setzen wir auf die Eigenschaft, dass diese neueren Anweisungstuning-LLMs sehr gut darauf reagieren, Anweisungen und Prompfen.</sample>
    <sample id="1424">Also können wir den Modell bitten, eine Persona zu generieren, die eine Darstellung eines imaginären Individuums ist, indem wir wie folgt ein Prompt verwenden: Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst.</sample>
    <sample id="1425">Und wir können sofort sehen, dass dies für jeden demografischen Bereich sehr generalisierbar ist, weil wir nur bestimmte Identitätsmerkmale angeben müssen, die wir in diesem Prompt wollen.</sample>
    <sample id="1426">Natürlich, hier sind einige Beispiele von Generierungen aus GPT-4:

1. **Beispiel 1:**
   - **Englisch:** "The quick brown fox jumps over the lazy dog."
   - **Deutsch:** "Der schnelle braune Fuchs springt über den faulen Hund."

2. **Beispiel 2:**
   - **Englisch:** ""To be or not to be, that is the question.""
   - **Deutsch:** ""Sein oder Nichtsein, das ist die Frage.""

3. **Beispiel 3:**
   - **Englisch:** "'A journey of a thousand miles begins with a single step.'"
   - **Deutsch:** "'Eine Reise von tausend Meilen beginnt mit einem einzigen Schritt.'"

4. **Beispiel 4:**
   - **Englisch:** """The only thing we have to fear is fear itself.""
   - **Deutsch:** ""Die einzige Sache, von der wir uns fürchten müssen, ist die Angst selbst.""

5. **Beispiel 5:**
   - **Englisch:** '"All that glitters is not gold."'
   - **Deutsch:** '"Alles, was glänzt, ist nicht Gold."'

6. **Beispiel 6:**
   - **Englisch:** 'The early bird catches the worm.'
   - **Deutsch:** 'Der frühe Vogel hat den Wurm geflogen.'

7. **Beispiel 7:**
   - **Englisch:** '''Actions speak louder than words.''
   - **Deutsch:** '''Taten sprechen lauter als Worte.''

8. **Beispiel 8:**
   - **Englisch:** ''The pen is mightier than the sword.''
   - **Deutsch:** ''Der Stift ist mächtiger als der Schwert.''

9. **Beispiel 9:**
   - **Englisch:** '(To) bite the bullet'
   - **Deutsch:** '(Zu) den Zahnen beißen'

10. **Beispiel 10:**
    - **Englisch:** 'The grass is always greener on the other side.'
    - **Deutsch:** 'Der grüne Gras ist immer an der anderen Seite.'

Diese Beispiele zeigen, wie GPT-4 in der Lage ist, englische Sätze in deutsche zu übersetzen, wobei die Bedeutung und den Kontext bewahrt bleibt.</sample>
    <sample id="1427">Unmittelbar sehen wir, dass die Ausgaben während des Experiments nicht in der traditionellen Bedeutung von negativ oder toxisch sind.</sample>
    <sample id="1428">Es gibt einige interessante Muster.</sample>
    <sample id="1429">Die asiatische Frau wird als unauffällig dargestellt. Die mittlere Ostfrau wird mit Worten wie exotisch und wie aufmerksam gemacht, um eine faszinierende Region zu beschreiben.</sample>
    <sample id="1430">Beide Personas von Frauen der Farbe beziehen sich auf ihre Herkunft, während der weiße Mann-Persona nichts von dieser Art hat.</sample>
    <sample id="1431">Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste Teil besteht darin, diese Personas zu generieren.</sample>
    <sample id="1432">Unsere Anweisungen zur Erstellung dieser Personas wurden von einer Studie inspiriert, bei der sie diese Anweisungen an menschliche Probanden gegeben haben und festgestellt haben, dass sie durch das Geben an menschliche Probanden auch in der Lage waren, rassistische Stereotypen aufzudecken.</sample>
    <sample id="1433">Und es ermöglicht auch direkten Vergleich zwischen unseren generierten Personas und den menschlichen geschriebenen Antworten.</sample>
    <sample id="1434">Der zweite Teil ist "markierte Wörter", was eine Methode ist, um die Wörter zu identifizieren, die markierte Gruppen von unmarkierten unterscheiden, was ich kurz erläutern werde.</sample>
    <sample id="1435">Der Vorteil ist, dass wir wirklich spezifische Stereotypen und Muster erhalten, ohne auf eine bestimmte Lexikon zu verweisen.</sample>
    <sample id="1436">Der Marktwert-Methode zieht auf den soziolinguistischen Konzept der Markierungskraft zurück, der besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist.</sample>
    <sample id="1437">So for instance, the word man or sorry, the word warrior is usually associated with men. So when people are describing a warrior who is a woman, they'll usually actually specify one man warrior and mark the term with woman.</sample>
    <sample id="1438">In der Gesellschaft sind dominierende Gruppen sowohl linguistisch als auch sozial unmarkiert, während die marginalisierten Gruppen in der Regel markiert sind.</sample>
    <sample id="1439">In unserer Methode bestimmen wir zunächst, welche Gruppen unmarkiert und markiert sind.</sample>
    <sample id="1440">Und dann vergleichen wir die Personen mit dem Fighting-Wörter-Methode, bei der wir grundsätzlich gewichtete Log-Odds-Verhältnisse verwenden, um die Top-Wörter für jede markierte Gruppe zu unterscheiden.</sample>
    <sample id="1441">So for instance, for the personas of black women, we would do fighting words and compare the log odds ratios against both white personas and man personas because those are the two corresponding unmarked groups.</sample>
    <sample id="1442">Jetzt für einige Ergebnisse. Also verwenden wir zuerst ein Lexikon von Stereotypen und wir finden heraus, dass die generierten Personen viel mehr Stereotypen enthalten als die menschlich geschriebenen.</sample>
    <sample id="1443">Jedoch wenn wir tatsächlich die Verteilung der Wörter im Lexikon betrachten, finden wir sehr unterschiedliche Dinge.</sample>
    <sample id="1444">Während die generierten Personas viel höhere Raten der Luxemburger Wörter haben, haben die menschlich geschriebenen Wörter eine viel breitere Verteilung von Wörtern, während die Stereotype-Wörter, die in den generierten Personas sind, eigentlich nur die Wörter "tall" und "athletic" sind.</sample>
    <sample id="1445">Also wirklich nur die positiven oder zumindest nicht negativen.</sample>
    <sample id="1446">In Wirklichkeit erfasst die Lexikon-Definition nicht wirklich viele der schädlichen Muster, die wir in den vorherigen Folien gesehen haben. Stattdessen werden wir stattdessen zu den Ergebnissen unserer markierten Wörter-Methode übergehen, um zu zeigen, wie diese positiv scheinenden Wörter Stereotypen und essentialisierende Narrative fördern.</sample>
    <sample id="1447">In unserer Analyse überprüfen wir, wie diese scheinbar positive Portrayals harmlose Muster widerspiegeln.</sample>
    <sample id="1448">Zuerst für Marktgruppen: Die oberste Wörter umfassen Dinge wie Kultur, Tradition, Stolz und Exotik. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als verschieden von der weißen Norm.</sample>
    <sample id="1449">Dies trägt zu einer langen Tradition der Diskriminierung und Ausschließung dieser Gruppen bei.</sample>
    <sample id="1450">Darüber hinaus gibt es viele gemeinsame Stereotype, die in diesen Worten widerspiegelt werden, insbesondere für Frauen von Farbe. So zum Beispiel sind die Worte, die latina Frauen beschreiben, Dinge wie lebendig und kurvaturig.</sample>
    <sample id="1451">Die englischen Wörter, die zu einem tropischen Stil passen, sind wie "petite" und "delicate" und "silky".</sample>
    <sample id="1452">Der englische Text beschreibt, dass Asiatinnen seit langem hypersexualisiert werden, als sehr docil und unterwürfig angesehen werden.

Übersetzung:

"Die Asiatinnen werden seit langem hypersexualisiert, als sehr docil und unterwürfrig angesehen. So weiter."</sample>
    <sample id="1453">Und schließlich sehen wir für schwarze Frauen, dass einige der wichtigsten Wörter Dinge wie stark und widerstandsfähig sind.</sample>
    <sample id="1454">Dies verbindet sich mit einem Archetypus, den Menschen als "starke schwarze Frauen" bezeichnet haben. Und während es auf den ersten Blick wie positiv klingt,</sample>
    <sample id="1455">Es gibt Arbeiten, die zeigen, dass diese Art von Architektur tatsächlich sehr schädlich ist, weil sie viel Druck auf diese Demografie ausübt, um widerstandsfähig und stark gegen soziale Hindernisse zu sein.</sample>
    <sample id="1456">Anstatt tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, setzt es Druck auf die betroffenen Menschen, sie zu überwinden, was zu sehr negativen Gesundheitsoutcomes für diese Menschen führt, neben anderen Schäden.</sample>
    <sample id="1457">In einem breiteren Kontext stellen wir fest, dass die Wörter für jede markierte Gruppe in etwa nur sehr grundlegende, essenzielle Narrative widerspiegeln.</sample>
    <sample id="1458">Based on these patterns, we conclude with three recommendations for model owners.</sample>
    <sample id="1459">Zunächst sollten wir als Forscher positive Stereotypen und essenzialisierte Geschichten ansprechen. Wir sollten auch ein intersektionales Linsensystem verwenden, um Biases und Schäden zu untersuchen, weil es viele Dinge gibt, die möglicherweise übersehen werden, wenn wir das nicht tun.</sample>
    <sample id="1460">Und schließlich sollte wirklich mehr Transparenz über Bias-Mitigationsmethoden geben.</sample>
    <sample id="1461">Weil zum Beispiel, wie diese positive Stereotypen, wir wissen nicht, ob es wegen irgendeiner Art von "weitläufigen"</sample>
    <sample id="1462">Übermäßige Wertausrichtung geht vor, oder vielleicht sind es andere, wie antistereotypische Methoden, die zu diesen bösen Muster führen.</sample>
    <sample id="1463">Wir können einfach keine Annahmen machen oder dies weiter untersuchen, ohne mehr Transparenz.</sample>
    <sample id="1464">Danke so viel für zuhören. Haben Sie einen schönen Tag!</sample>
    <sample id="1465">Hallo alle, mein Name ist Jin Wei Yi und ich komme aus der Universität für Wissenschaft und Technologie von China.</sample>
    <sample id="1466">Es ist mein Vergnügen, einen kurzen Werbespot über Papier zu machen. Bist du mein Modell kopieren? Schütze die Urheberrechte von großen Sprachmodellen für Eingebettung und Dienstleistungen.</sample>
    <sample id="1467">Bevor wir in die Details der Eingabe- und Dienstleistungs-Technologie eintauchen, ist es wichtig, die grundlegenden Konzepte und Entwicklungen zu verstehen, die diese Technologie ermöglicht.

### 1. **Eingabe-Technologie**
Eingabe-Technologie bezieht sich auf die Möglichkeiten, Daten in digitalen Systemen zu erfassen, zu speichern und zu verarbeiten. Diese Technologie hat sich in den letzten Jahrzehnten erheblich weiterentwickelt, insbesondere durch die Einführung von Eingabegeräten wie Tastatur, Maus, Touchscreens und neuere Technologien wie Voice Assistants und Augmented Reality (AR).

#### **1.1. Tastatur und Maus**
Die Tastatur und Maus sind die traditionellen Eingabegeräte, die in der Regel mit Computern und Laptops verbunden sind. Sie ermöglichen es den Benutzern, Texte, Befehle und andere Informationen in digitalen Systemen einzugeben.

#### **1.2. Touchscreens**
Touchscreens ermöglichen es den Benutzeren, direkt auf die Bildschirmoberfläche zu tippen oder zu schreiben. Diese Technologie ist besonders in mobilen Geräten wie Smartphones und Tablet-Computern verbreitet.

#### **1.3. Voice Assistants**
Voice Assistants wie Siri, Google Assistant und Alexa ermöglichen es den Benutzenden, Eingaben durch Sprache zu erfolgen. Diese Technologien nutzen maschinelles Lernen und natürliche Sprachverarbeitung (NLP), um die Sprache des Benutzers zu verstehen und entsprechende Handlungen auszuführen.

#### **1.4. Augmented Reality (AR)**
AR-Technologien ermöglichen es den Benutzerten, digitale Informationen in die realen Umgebungen zu übertragen. Dies kann beispielsweise durch die Verwendung von Smartglasses oder AR-Headsets geschehen, bei denen die Benutzer durch die Brille Informationen über ihre Umgebung erhalten.

### 2. **Dienstleistungen**
Dienstleistungen beziehen sich auf die verschiedenen Funktionen und Anwendungen, die auf Basis von Eingabe-Technologie entwickelt wurden. Diese Dienstleistungen können von einfachen Aufgaben bis hin zu komplexen Prozessen reichen.

#### **2.1. Kommunikation**
Eine der wichtigsten Dienstleistungen ist die Kommunikation. Dies beinhaltet die Übertragung von Texten, Bildern und Videos zwischen Benutzern und Systemen. Dazu gehören E-Mail, soziale Medien, Messaging-Apps und Videokonferenzen.

#### **2.2. Informationsabruf**
Ein weiterer wichtiger Dienst ist der Informationsabruf. Benutzer können auf eine Vielzahl von Informationen zugreifen, wie zum Beispiel Nachrichten, Wettervorhersagen, Finanzdaten und mehr. Dies wird durch die Nutzung von Suchmaschinen, News-Apps und anderen Informationsquellen ermöglicht.

#### **2.3. Automatisierung**
Automatisierung ist eine weitere Schlüsseldienstleistung, bei der Eingabe-Technologie verwendet wird, um Aufgaben automatisch auszuführen. Dies kann beispielsweise durch das Einrichten von Smart-Home-Geräten oder die Verwaltung von Daten durch Software erfolgen.

#### **2.4. Kreative Prozesse**
Eingabe-Technologie spielt auch eine wichtige Rolle in kreativen Prozessen wie Schreiben, Malen und Musikproduktion. Tools wie Textverarbeitungssoftware, Bildbearbeitungsprogramme und Musikproduktionssoftware nutzen Eingabe-Technologie, um kreative Ideen zu verwirklichen.

#### **2.5. Bildung und Lernen**
Eingabe-Technologie ist ein wichtiger Bestandteil des Lernens und der Bildung. Online-Kurse, digitale Bücher und Lernplattformen nutzen Eingabe-Technologie, damit Benutzer Informationen eingeben, speichern und verarbeiten können.

### 3. **Integration und Interaktion**
Eingabe-Technologie und Dienstleistungen sind oft in komplexen Systemen integriert, die die Interaktion zwischen Benutzern und Systemen erleichtern. Dies kann beispielsweise durch den Einsatz von APIs (Application Programming Interfaces) erfolgen, die es verschiedenen Systemen ermöglichen, miteinander zu kommunizieren.

#### **3.1. APIs**
APIs sind Schnittstellen, die es ermöglichen, verschiedene Systeme und Dienstleistungen miteinander zu verbinden. Dies ermöglicht es Benutzern, Daten und Funktionen aus verschiedenen Quellen zu kombinieren und zu nutzen.

#### **3.2. Cloud-Technologie**
Cloud-Technologie ermöglicht es, Daten und Dienstleistungen über das Internet zu speichern und zu verarbeitet. Dies ermöglicht es Benutzer, auf ihre Daten und Anwendungen von überall aus zugreifen zu können.

#### **3.3. Künstliche Intelligenz (KI)**
KI-Technologien werden verwendet, um Eingabe-Technologie und Dienstl</sample>
    <sample id="1468">Aktuell sind große Sprachmodelle wie GPT, LAMA und PALM bemerkenswert in der natürlichen Sprachverständigung und -generierung.</sample>
    <sample id="1469">Embedding as services is one of the services built upon large language models to assist various NLP tasks.</sample>
    <sample id="1470">Zum Beispiel bietet OpenAI eine GPT-basierte Embedding-API an.</sample>
    <sample id="1471">Allerdings haben neuere Arbeiten gezeigt, dass ein Angreifer das Modell durch Lernen aus dem Embedding stählen kann und ähnliche Dienste anbieten. Daher ist es notwendig, die Urheberrechte des Embeddings als Dienstleistungen zu schützen.</sample>
    <sample id="1472">Um den Urheberrechtsschutz von Embedding- und Services zu gewährleisten, ist eine Lösung, um ein Wasserzeichen in den angebotenen Service einzubetten und zu überprüfen, ob ein anderer Service das Wasserzeichen enthält.</sample>
    <sample id="1473">Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen: Erstens sollte die Methode auf die Einbettung von Diensten anwendbar sein. Zweitens sollte der Wasserzeichen nicht die Nutzung der bereitgestellten Einbettungen beeinträchtigen.</sample>
    <sample id="1474">Drittens sollte das Wasserzeichen ausreichend klebrig sein, damit der Angreifer es leicht entfernen kann.</sample>
    <sample id="1475">Schließlich muss das Wasserzeichen während des Modellextraktionsprozesses übertragbar auf die Angreiferdienste sein.</sample>
    <sample id="1476">Die bestehenden Werke können allgemein in vier Kategorien eingeordnet werden:</sample>
    <sample id="1477">Jedoch ist diese Methode entweder nicht anwendbar auf die Einbindung von Dienstleistungen oder aufgrund des Mangels an Transferfähigkeit.</sample>
    <sample id="1478">Daher schlagen wir in diesem Papier ein Embedding Marker vor, das eine Backdoor-basierte Wasserzeichenmethode ist, die auf Embedding-As-Services angewendet werden kann.</sample>
    <sample id="1479">Natürlich, ich helfe Ihnen gerne, den englischen Inhalt auf Deutsch zu übersetzen. Hier ist die Übersetzung:

---

Dann lass mich die Details unseres Embedding-Markers einführen. Das Embedding-Marker besteht aus zwei Hauptstufen:

1. **Wasserzeichen-Einsprengung**: Dies ist der erste Schritt, bei dem ein Wasserzeichen in das Originaldokument eingespielt wird. Dieser Wasserzeichen kann aus Text, Bildern oder anderen Elementen bestehen, die nicht leicht entfernt werden können.

2. **Copyright-Verifizierung**: Der zweite Schritt besteht darin, die Verfügbarkeit des Originaldokuments zu überprüfen. Dies kann durch verschiedene Methoden erfolgen, wie z.B. durch die Überprüfung von Online-Datenbanken oder durch die Analyse von ähnlichen Dokumenten.

---

Ich hoffe, das hilft Ihnen weiter! Wenn Sie weitere Fragen haben, lassen Sie es mich wissen.</sample>
    <sample id="1480">Bevor wir diese Hauptaufgaben durchführen, wählen wir zunächst eine Trigger-Sammlung. Die Trigger-Sammlung besteht aus einem Gruppensatz von Wörtern, die in einem moderaten Frequenzintervall vorkommen.</sample>
    <sample id="1481">Wir nehmen an, dass der Anbieter eine allgemeine Textkorpus sammeln kann und die Worthäufigkeit mit ihm zählen kann.</sample>
    <sample id="1482">In der Watermark-Injection definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer eine Aussage an den Anbieter-Service sendet, zählt der Anbieter die Anzahl der Trigger in der Aussage.</sample>
    <sample id="1483">Der bereitgestellte Embedding ist eine Gewichtssumme der Ziel-Embedding und des Original-Embeddings.</sample>
    <sample id="1484">Die Gewichtung des Ziel-Embeddings ist proportional zur Anzahl der Trigger in der Aussage. Wenn die Anzahl der Trigger in der Aussage größer als m ist, ist die bereitgestellte Embedding genau gleich dem Ziel-Embedding.</sample>
    <sample id="1485">Die Copyright-Überprüfung besteht darin, zu bestimmen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält.</sample>
    <sample id="1486">Wir erstellen zunächst eine Backdoor und ein Benign-Datensatz. Die Backdoor-Datensatz enthält Sätze, von denen alle Wörter zum Trigger-Satz gehören. Während alle Wörter in den Sätzen des Benign-Datensatzes nicht zum Trigger-Satz gehören.</sample>
    <sample id="1487">Dann fordert der Anbieter die Embeddings von der Steuerleistungsdienstleistung mit dem Datensatz an.</sample>
    <sample id="1488">Die Kosinus- und L2-Similarität zwischen der angeforderten Embedding und dem Ziel-Embedding wird berechnet. Wir berechnen die Similaritätsdifferenz zwischen Benign und Backdoor-Datensatz, die als Delta-Kosinus und Delta-L2 definiert ist.</sample>
    <sample id="1489">Währenddessen wenden wir auch den KS-Test an und nutzen dessen p-Wert als drittes Matrix.</sample>
    <sample id="1490">Wir führen Experimente auf vier Datensätzen durch: agnews, mind, ssd2 und eurispam. Wir nehmen an, dass der Anbieter die Wikitext-Datensätze anwendet, um die Wortfrequenz zu zählen.</sample>
    <sample id="1491">Die Ergebnisse auf vier Datensätzen zeigen, dass unsere Embedding-Markierung eine große Erkennungsleistung erzielen kann, während sie gleichzeitig für die Durchschnittsschleife nützlich bleibt.</sample>
    <sample id="1492">Wir haben auch die Konvertierbarkeit des bereitgestellten Embeddings validiert, indem wir die Embeddings von Sätzen auf einem vorgegebenen Datensatz visualisiert haben. Die Legende der Figuren bedeutet die Anzahl der Trigger in jedem Satz.</sample>
    <sample id="1493">Wie in den Bildern gezeigt, ist es schwierig, zwischen den Hintergrund-Einbettungen und den normalen Einbettungen zu unterscheiden.</sample>
    <sample id="1494">Das alles, danke. Willkommen, um mit uns zu diskutieren.</sample>
    <sample id="1495">ABC-Eval steht für "Annotating Behaviors in Chat" oder "ABC Evaluation" in kürzerer Form. Es ist eine Methode, die entwickelt wurde, um umfassend die Verhaltensmuster in Chat-Interaktionen zu dokumentieren, die als Einfluss auf die Qualität des Chatmodells angesehen werden. Diese Methode basiert auf der Analyse von Interaktionen zwischen Benutzern und Chatbots, um Verhaltensmuster zu identifizieren, die die Effektivität und Zufriedenheit der Chatbot-Interaktionen beeinflussen können.</sample>
    <sample id="1496">Das Leistungsdelta zwischen CoNLL 2003 und CoNLL++ ist höher als 5 Prozentpukte bis 2023.</sample>
    <sample id="1497">Hallo, mein Name ist Wasuda und ich bin ein Bildungsbürger in der Computerwissenschaften an der Stony Brook University. Ich möchte meine Arbeit als langfristige Arbeit für die Entdeckung von Diskriminierung als akademische Arbeit für die Ausbildung 2023 als langfristige Arbeit für die Diskriminierung als akademische Entdeckung der Diskriminierung als akademische Diskriminierung als akademische Erkenntnisse über die Diskriminierung als akademisch erkenntnisse über die Diskriminierungsproblematik als akademische Diskriminierung über die Diskriminierung als akademisches Problem über die Diskriminierung als akademic problem über die Diskriminierung als akademi</sample>
    <sample id="1498">Wir beginnen mit der Definition von kognitiver Dissens und damit, warum es ein wichtiges Problem ist, in der Sprache zu studieren. Einfach gesagt, ist kognitiver Dissens zwei Glaubens oder Handlungen, die inkonsistent sind.</sample>
    <sample id="1499">In this example, a person states that they know cigarettes could kill them, but then goes on to say they grabbed a couple of smokes after the meeting. This belief and action are inconsistent and they are in dissonance.</sample>
    <sample id="1500">Weiterhin erwähnt, dass ich denken kann, ich könnte ohne sie meinen Job nicht halten, rechtfertigt sich die zweite Wiederholung und sie haben eine Konsistenzbeziehung.</sample>
    <sample id="1501">Während Dissens ein sehr häufiges Phänomen ist, das wir in unserem täglichen Entscheidungsprozess erleben, ist es in der Sprache selten ausgedrückt, wenn man sich auf andere Arten von Diskurseinheiten bezieht.</sample>
    <sample id="1502">Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends in belief values and attitude changes in populations.</sample>
    <sample id="1503">Hochkognitive Disposition ist auch mit Angststörungen verbunden und kann helfen, das psychische Wohlbefinden der Menschen besser zu verstehen.</sample>
    <sample id="1504">Studierende, die sich mit der Ausdrucksweise von Distanz in Sprache befassen, können auch in der Lage sein, ein besseres Verständnis für Extremismus und Polarisierung von gefährdeten Gruppen zu erlangen.</sample>
    <sample id="1505">Schließlich ist der kognitive Abstand wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und uns die Entscheidungsprozesse besser zu verstehen.</sample>
    <sample id="1506">Um das Ziel der Erstellung einer kognitiven Distanzressource zu erreichen, haben wir eine große Skala-Analyse der Distanzbeziehungen durchgeführt. Wir haben die Distanz-First-Approach verwendet, wie in dem hier gezeigten Flowchart zu sehen ist.</sample>
    <sample id="1507">Die Tweets wurden mit einem PTTB-Parser verarbeitet und Paare von Diskurs-Einheiten wurden entsprechend den in unserer Arbeit beschriebenen Richtlinien annotiert.</sample>
    <sample id="1508">Wie hier zu sehen ist, wurde nur in 3,5 % der annotierten Paare Dissonanz gefunden.</sample>
    <sample id="1509">Während der Erfassung von etwa tausend Beispielen von Diskurs-Einheitspaaren haben wir eine Anfangs-Klassifikator-Training durchgeführt, der nur auf 43 Beispielen von Diskretisierung trainiert wurde. Keine Überraschung, der Klassifikator lieferte nicht viel besser als Zufall.</sample>
    <sample id="1510">Wir stehen vor dem Problem der absoluten Seltenheit.</sample>
    <sample id="1511">Um dies zu lösen, experimentieren wir mit Kombinationen von Transfer-Learning und aktiver Lernen, um solche zu annotieren, dass mehr abweichende Proben gesammelt werden können, über weniger Annotation-Runden hinweg, wodurch die Gesamtkosten für Annotationen gesenkt werden, während die Erkennung von Abweichungen verbessert wird.</sample>
    <sample id="1512">Da der ursprüngliche Modell nicht in der Lage war, die Abstimmungsklasse überhaupt zu erfassen, starten wir den aktiven Lernprozess, indem wir die Gewichte von eng verwandten Aufgaben übertragen.</sample>
    <sample id="1513">Wir übertragen von zwei verschiedenen Aufgaben: Themenunabhängige Diskussionsstandsklassifikation, eine Aufgabe, die bestimmt, ob zwei Debattierstellungen von verschiedenen Personen in Einklang oder in Widerspruch stehen, unabhängig von dem Thema.</sample>
    <sample id="1514">Die Inhalte des englischen Textes können wie folgt ins Deutsche übersetzt werden:

---

**Debate here** und **on binary classification of expansion and comparison classes of pdtb**

Seit diese beiden Themen eng miteinander verbunden sind, beziehen sie sich auf die Konzeption von **Konsonanten und Dissonanzen**. Wir nennen sie **ce** hier.

---

### Erläuterung:

1. **Debate here**: Der englische Begriff "Debate" bezieht sich auf eine Diskussion oder Streit, in der verschiedene Ansichten oder Meinungen ausgetauscht werden. In diesem Kontext könnte es sich um eine Diskussion über die Themen der binären Klassifikation und der pdtb-Klassen handeln.

2. **Binary classification of expansion and comparison classes of pdtB**: Dies bezieht sich auf die Art und Weise, wie die pdtB-Klassen (Probabilistic Dependency Treebank) in zwei Kategorien unterteilt werden: **Expansion** und **Comparison**. Diese Kategorien beziehen sich auf die Art der Abhängigkeitsstruktur in der pdtB-Konstruktion.

3. **Konsonanten und Dissonanze**: Konsonanten sind Lauten, die nicht als Vokale ausgesprochen werden, während Dissonanzen in der Linguistik oft die fehlende Harmonie oder die Unterbrechung von Konsonanten bezeichnen. In diesem Kontext könnten Konsonanten und Dissonanzen die verschiedenen Arten der Abhängigkeitsstrukturen in der pdtB-Konstruktionsklassifikation beziehen.

4. **ce**: Dies ist eine Abkürzung, die in diesem Kontext verwendet wird, um die beiden Themen **Konsonanten und Dissonanzer** zusammenzufassen.

---

### Zusammenfassung:

Der englische Text diskutiert die binäre Klassifikation der pdtB-Klassen in Bezug auf **Expansion** und **Comparison**, und diese beiden Themen sind eng mit der Konzeption von **Konsonanten** und **Dissonanzen** verbunden. In diesem Zusammenhang wird die Abkürzung **ce** verwendet, um diese beiden Aspekte zusammenzufassen.</sample>
    <sample id="1515">Wir stellen fest, dass beim Übertragen die Null-Shot-Performance auf dem annotierten Datensatz bereits viel besser ist als das Zufallsverhältnis, wobei das beste Ergebnis mit einem AUC von 0.62 erreicht wird.</sample>
    <sample id="1516">Weiterhin finden wir, dass die iterativ-fine-Tuning auf beiden Aufgaben besser ist. Wenn wir zuerst die Fine-Tuning auf der CE-Aufgabe durchführen und dann die Fine-Tuning auf der Debatte durchführen, erzielen wir ein viel besseres Null-Shot-Performance. Das ist das Modell, das wir verwenden, um das aktive Lernen zu starten.</sample>
    <sample id="1517">Nächst bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde aktiver Lernen und Annotationen zu aktualisieren. Der Kumulative sammelt alle Daten, die bisher aus aktiven Annotationen gesammelt wurden, während die iterativen Aktualisierungen des Modells durch das Training auf dem neuesten Datensatz geschehen.</sample>
    <sample id="1518">Wir haben über verschiedene Strategien hinausgefunden, dass die kumulative Leistung gleich oder besser als die iterativen Leistungen ist, überall auf dem Spielfeld.</sample>
    <sample id="1519">Um die Anzahl der Dissimilaritätsbeispiele zu verbessern, verwenden wir eine Strategie der Wahrscheinlichkeit der seltenen Klasse (PRC), um hauptsächlich Beispiele auszuwählen, die sehr wahrscheinlich von der aktuellen Modellversion in jeder Runde des Lernens von Dissimilaritäten ausgeschlossen werden.</sample>
    <sample id="1520">Wir vergleichen dies mit anderen modernen, also "state of the art" Strategien, die allgemein in der Gemeinschaft verwendet werden.</sample>
    <sample id="1521">Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere hochmoderne Strategien, obwohl der Unterschied gering ist. Beachten Sie jedoch, dass die Leistung für zufällige Fälle signifikant niedriger ist.</sample>
    <sample id="1522">Auf der Grundlage der beiden besten Strategien haben wir in weiteren Runden des AL-Verfahrens die Klassifizierungsleistung von 0,75 AUC verbessert, was die beste Leistung ist, die wir bisher bei dieser Aufgabe erreicht haben.</sample>
    <sample id="1523">Wir überprüfen auch die Machbarkeit jedes Strategien für die Annotation-Qualität und die Kosten für Annotatoren. Wir finden heraus, dass PRC den höchsten Prozentsatz an Diskrepanz hat und am besten für die Klasse funktioniert. Allerdings finden die Annotatoren auch die Beispiele schwer.</sample>
    <sample id="1524">Zusammenfassend stellen wir fest, dass PRC eine einfache Strategie für die Beschaffung von seltenen Klassen ist und dass das Co-Starting mit einer passend konzipierten Transfer-Learning-Aufgabe erheblich hilft.</sample>
    <sample id="1525">Wir finden auch, dass iteratives Update für die Transfer-Learning von einem anderen Bereich nützlich ist, während in-bereichliche aktive Anpassungen von kumulativen Updates profitieren.</sample>
    <sample id="1526">Hier ist der englische Inhalt ins Deutsche übersetzt:

Diese sind die Links zu unserem Code-Datensatz und unserer Arbeit.

Wenn Sie Fragen haben, zögern Sie nicht, uns zu kontaktieren.

Vielen Dank.</sample>
    <sample id="1527">Die Autoren gehören an der Universität von München.</sample>
    <sample id="1528">Der/die Referent*in heißt Si Yuyuan.</sample>
    <sample id="1529">Vier Autoren sind an der Arbeit beteilt.</sample>
    <sample id="1530">SimulST-Architektur.</sample>
  </task>
</testset>