<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">The main sources of data for language models are large-scale web crawl data, which often includes a significant amount of political news media content. This can lead to the propagation of political biases from the pretraining data to the language models and subsequently to downstream tasks, potentially resulting in fairness issues in NLP applications.</sample>
    <sample id="1">The authors of the article "The KITMUS Test: Evaluaring Knowledge Integration from Multiple Sources" are Akshatha, Martin, McGill University, Mila, and Microsoft Research, as mentioned in the introduction.</sample>
    <sample id="2">Hello everyone, I am Tu Yi from Ant Group, and I am presenting our team's paper on document understanding. Our paper focuses on the Visually-rich Document Understanding (VrDU) problem, which aims to understand various types of documents such as forms, receipts, and posters. In recent studies, pre-training techniques have been introduced into this field, and self-supervised pre-training multi-modal models, like BERT, have shown great success in VrDU tasks. However, existing document models suffer from reading order issues, as they use global 1D positions to represent the reading order of tokens.

We propose a novel pre-trained model called LayoutMask, which addresses these issues by using text and layout information as model input. LayoutMask differs from previous studies in three aspects: choice 1D position, masking strategy, and training objectives. Instead of global 1D position (ascending numbers), LayoutMask uses local 1D position, which is the in-segment token order. This approach does not provide cross-segment orders, so LayoutMask infers global reading order by jointly using 1D, 2D position, and semantic information.

To promote text-layout interactions, we equip the Masked Language Modeling (MLM) objective with two novel masking strategies: Whole Word Mask and Layout-Aware Masking. Whole Word Masking sets masks at the word level, eliminating semantic relations between masked and unmasked tokens. Layout-Aware Masking masks the first and last words of each segment, promoting cross-segment order learning.

We also designed a new pre-training objective, Mask Position Modeling (MPM), which recovers randomly masked 2D positions during pre-training, similar to a cloze test. The joint learning process with semantic and spatial inference promotes text-layout interactions and helps the model learn better layout representations.

In our experiments, we compare the performance of Layout-1D and Global-1D on FUNSD, SROIE, and CORD datasets. Local-1D outperforms Global-1D in most cases, especially when dealing with entities like "Total" that have multiple misleading numbers. This result shows that using Local-1D is more adaptive to such cases.

For more details, please refer to our paper and posters. Thank you for watching, and if you have any questions, please send me an email.</sample>
    <sample id="3">Ciao! Benvenuti alla presentazione di DEPLAIN, un nuovo corpus per l'identificazione del testo in tedesco a livello di documento e di frase. Il mio nome è Regina Stodden e guiderò voi attraverso la prima parte della presentazione. Prima di tutto, definiamo cosa si intende per semplificazione del testo. La semplificazione è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo specifico, come persone con problemi di lettura o non-native speakers. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testo, ad esempio di documenti o frasi. Ecco un esempio di una coppia di frasi parallele di una frase tedesca complessa e la sua traduzione in lingua semplice. Per semplificare la frase, diverse tecniche sono possibili, come puoi vedere nell'esempio, come la sostituzione lessicale, la cancellazione di clausole, la riordinazione o l'inserimento di parole. Ora propongiamo il nostro nuovo corpus, DEPLAIN, perché negli ultimi anni ci sono stati alcuni problemi con i corpus esistenti. Ad esempio, questi corpus qui sono troppo piccoli per addestrare un modello di semplrificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere errorati nelle loro allineamenti. Pertanto, proponiamo il nostro nuovo corpus DEPLAIN, che è suddiviso in due sottocorpi: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-APA è basato su testi di notizie. In DEPLAIN-apa, abbiamo allineato manualmente 483 documenti, risultando in circa 13.000 coppie di frasi parallele. Per DEPLAIN-web, questo corpus include diversi domini e abbiamo allineato tutti questi 750 documenti, su un lato manualmente e su un lato con metodi di allineamento automatici. In totale, risultano 30.450 coppie di frasi. Abbiamo analizzato un po' di più le nostre coppie di frasi, quindi per esempio, sul tipo di semplificazione. Come puoi vedere qui, i testi della Bibbia sono molto più semplificati rispetto, ad esempio, i testi di notizie o i testi per apprendenti di lingua. Su tutti i livelli, in relazione, ad esempio, alla semplificazione lessicale, alla semplificazione strutturale, anche il livello complessivo di semplificazione. Inoltre, puoi vedere che il nostro corpus DEPLAIN ha una grande varietà di trasformazioni di semplificazione diverse. Quindi, per esempio, in DEPLAIN-apa abbiamo molte più riordinazioni e aggiunta di parole rispetto a quelle in DEPLAIN-web. D'altra parte, nel corpus web abbiamo molte più ripetizioni. Ora vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro dataset DEPLAIN. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatici. Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre le allineamenti delle frasi in entrambi i documenti. Ma in nostro caso d'uso, stiamo cercando di estrarre le allineamenti tra le frasi di due documenti paralleli scritti nella stessa lingua, con lo stesso contenuto, ma a diversi livelli di complessità. E ora, grazie al nostro dataset DEPLAIN, che ha frasi manualmente allineate, possiamo utilizzare queste frasi come standard di allineamento oro per valutare alcuni dei metodi di allineamento proposti. Abbiamo fatto alcune adattamenti ai metodi proposti e abbiamo pubblicato tutte queste adattamenti e i codici per eseguire le nostre esperienze nel nostro articolo. Alla fine, abbiamo concluso che</sample>
    <sample id="4">Il nome della relatrice o del relatore è Kayo Yin.</sample>
    <sample id="5">I used the T5 XL model to achieve an accuracy of 82%-87% when the language model has access to some partially overlapping but not exact background knowledge.</sample>
    <sample id="6">In this presentation, Jiaan introduces their joint work with Fandong, Duo, Zhixu, Jianfeng, Duo, and Jie on "Towards Unifying Multi-Lingual and Cros-Lingual Summarization." They propose a new framework called many-to-many summarization, which aims to build a single summarization model that can process documents in any source language and generate summaries in any target language. The authors also introduce PISCES, a pre-trained many-to-multilingual summarization model that learns language modeling, cross-language ability, and summarization ability through a three-stage pre-training process. The authors conducted preliminary experiments on the WikiLingua dataset, which includes English, French, Hindi, Chinese, and Thai, and found that the many-to-many summarization model outperforms previous multilingual and cross-lingual summarization models. The authors also conducted ablation studies and human studies to verify the effectiveness of their approach. The results show that PISCES outperforms various baselines, including BART-50 and T5. The authors encourage the audience to check out their paper for more details.</sample>
    <sample id="7">Yes, the CoNLL-2003 tagger models still perform well in 2023, according to the research presented in the paper. The study found that transformer models, larger model sizes, and more fine-tuning examples are key factors for good generalization. Additionally, the performance drop observed is primarily due to temporal drift rather than adaptive overfitting. The paper suggests that these models can still be effective for Named Entity Recognition tasks in modern data.</sample>
    <sample id="8">The novelty of the ABC-Eval method is that it reduces the subjectivity of human evaluation by explicitly annoting whether or not each model response expresses certain behaviors. This approach attempts to measure the rates at which chat models will commit various behavioral errors, such as responding with irrelevant information or contradicted itself, and it is capable of measuring the rates at which chat model behaviors have been suggested to affect chat quality in recent research. ABC-Eval labels are overall more reliable than labels collected by previous methods, as measured by inter-annotator agreements on 100 doubly-labeled conversations.</sample>
    <sample id="9">Il successo dell'attuale approccia scarsamente supervisionato (WSL) si basa in larga misura sulla disponibilità di campioni di dati etichettati puliti per la selezione del modello. Questo significa che, senza un set di dati di validazione pulito, le tecniche WSL tendono a non generalizzare bene e a memorizzare il rumore delle etichette, rendendo l'addestramento inutile. Inoltre, l'incremento del numero di campioni di dati etichetti puliti per la selezione del modelo contribuisce a migliorare le prestazioni. Inoltre, il fine-tuning continuo sui campioni di dati etichettati puri può migliorare ulteriormente le prestazioni.</sample>
    <sample id="10">Per migliorare il punteggio, possono essere fatti i seguenti progressi:

1. **Aumentare la qualità del background knowledge**: Fornire informazioni più dettagliate e accurate sulle entità per aiutare i modelli a comprendere meglio le differenze tra le opzioni.
2. **Utilizzare modelli più avanzati**: Implementare modelli più sofisticati, come quelli basati su Transformer, che possono gestire meglio le informazioni complesse e le relazioni tra le entità.
3. **Migliorare la generazione del contesto**: Utilizzare tecniche di generazione del contesto più avanzate, come quelle basate su attenzione, per creare contesti più coerenti e contestuali.
4. **Raffinare il processo di annotazione**: Migliorare il processo di annotazione per garantire che le informazioni fornite ai modelli siano accurate e complete.
5. **Utilizzare tecniche di transfer learning**: Utilizzare tecniche di transfer learning per migliorare la generalizzabilità dei modelli su diverse aree tematiche.
6. **Migliorare la valutazione del modello**: Utilizzare tecniche di valutazione più avanzate per ottenere una misura più accurata del punteggio del modello.
7. **Raffinare il processo della selezione delle entità**: Utilizzare tecniche di selezione delle entità più avanzate per garantire che le entità siano selezionate correttamente e che le informazioni fornite ai modelli possano essere utilizzate in modo efficace.
8. **Migliorare la gestione delle ambiguità**: Implementare tecniche per gestire le ambiguità nelle entità e nelle informazioni fornite, per garantire che i modelli possano comprendere meglio le informazioni e prendere decisioni corrette.

Questi progressi possono aiutare a migliorare la performance dei modelli e a ottenere un punteggio più alto.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presents a study on the ability of large language models to understand humor, using data from The New Yorker Caption Contest. The study includes three tasks: matching, quality ranking, and explanation generation. The best model, CLIP fine-tuned on the annotated corpus, achieves 62% accuracy on the matching task, compared to 94% for humans. When conditioning language models like GPT-4 on human-authored descriptions of images, there is still a significant performance gap. The study also highlights errors in GPT-4's joke explanations, with human explanations preferred in more than two-thirds of cases. The dataset and leaderboard are available for further research.</sample>
    <sample id="12">Ci sono cinque autori coinvolti nell'articolo: Dawei, Xiaoyu Shen, Marius Mosbach (Marius Mosbach), Andreas Stephan, e Dietrich Klakow.</sample>
    <sample id="13">Hello everyone, my name is Daniel Rotem, and I will present my work, "Finding the SWEET spot: Analysis and Improvement of Adaptive Inference in low resource settings," which was done in Professor Roy Schwartz's lab. Adaptive inference is a method for reducing the time of large language models. We rely on the fact that real-world data varies, so we can use low-capacity models for easy data and reduce average inference costs. The two most common methods are Multi Model and Early Exit. Multi Model stores multiple models and runs them sequentially until a classifier halts. Early Exit runs multiple classifiers and halts when one decides. Multi Model is versatile and easily extended but expensive to store and has overhead. Early Exit is faster and memory efficient but shares model parameters, leading to conflicting gradients. We hypothesize that conflicting gradients degrade performance. We compared individual Early Exit models with separate Multi Model classifiers and found that Multi Model outperformed Early Exit by an average of 2.2%. We also measured the speed/accuracy trade-off and found that Multi Model is better for high inference speeds, while Early Exit outperforms Multi Model for later classifiers. We present SWEET, a novel fine-tuning method for Early Exit architectures that separates weights in each layer to avoid conflicting gradients. Our results show that SWEET closes the gap between Early Exit and Multi Model and outperforms both methods in fast speeds. The takeaways from our work are the existence of conflicting gradients in Early Exit training, the first fair comparison of Early Exit and Multi model methods, and the introduction of SWEET, which motivates future research and fine-tuning algorithms tailored to the early exit architecture. Thank you for listening.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questo discorso riguarda la Struttura di Dipendenza della Coordinazione. Come sapete, ci sono diverse strutture di dipendenza assume da diverse teorie e approcci di corpus. Ad esempio, nelle dipendenze universali, la struttura della coordinazione, Lisa, Bart, e Maggie, è tale che il primo congiunto è la testa della struttura di coordinazione intera. Quindi in questo caso, Lisa. Un approccio simile è presunto in teoria del testo di Igor Mel'čuk, dove ancora una volta, tutta la struttura di coordinazione è guidata dal primo congiunto. Quindi queste due approcci sono asymmetrici. Giusto. Sottolineano uno dei congiunti. Ora questi sono approcci asimmetrici alla struttura di coordinazione, come quello di Praga. L'approccio congiunzione guidata presunto nelle dipendenze di Praga, dove le strutture di coordinazione sono guidate dalla congiunzione. Quindi, otteniamo alcune dipendenze dall'inizio a tutti i congiunti. E infine, c'è anche un approccio multi-caduta che viene utilizzato, ad esempio, nella Grammatica di Hudson, dove dicono che tutti i congiunti sono teste della struttura di coordinazione. Quindi otteniamo dipendenze dal governatore. Ecco: Lisa ama Bart e Maggie. Ora l'obiettivo di questo articolo è produrre un nuovo argomento per le strutture simmetriche di coordinazione, come queste due, e contro le strutture asymmetriche di coordinazione, come queste. Ok. L'argomento è basato sul principio del minimo lunghezza della dipendenza che spiegherò basandomi su questi esempi. Quindi in inglese, come sapete, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli adjuncti possono essere più lontani. Quindi "Marge ha letto il libro ieri" è buono perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto ieri il libro" è molto peggio. Giusto? Qui, tra il verbo e l'oggetto diretto c'è un adjunct: "ieri". Tuttavia, questo effetto può essere mitigato quando l'oggetto diretto è molto pesante e molto lungo. Poiché allora può essere spostato alla posizione dopo l'adjunct. Questo è illustrato qui. Quindi entrambe queste frasi sono buone. "Marge ha letto questo libro assolutamente affascinante sulla vita delle api ieri." È buona la forma in cui invece di "it", abbiamo questo lungo NP. Ma è anche buona dire "Marge ha letto ieri questo libro assolutamente affascinanti sulla vita delle api." Quindi la ragione qui è che questo è possibile perché anche se questa frase viola il principio generale grammaticale che gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio del minimo lunghezza della dependenza, che dice che le dipendenze più corte sono preferite. Quindi queste due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Qui abbiamo una dipendenza da "letto" al adjunct di lunghezza 7 misurata in parole e da "letto" a "libro" di lunghezza 4, quindi insieme è 11. Quando scambiiamo questi due costituenti, la somma di queste due dipendenze diventa 6. Quindi invece di 11,</sample>
    <sample id="15">There are three authors involved in the article: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="16">I testi più semplificati risultano essere quelli provenienti dal testo biblico, come mostrato nel corso della presentazione. Questi testi mostrano una semplificazione molto più forte rispetto a testi come quelli di notizie o testi per apprendenti di lingue.</sample>
    <sample id="17">Hi everyone, I'm Shengqiong Wu, a PhD student from NUS. I'm excited to present our work on multimodal relation extraction. Relation extraction is a well-studied task that aims to identify semantic relationships between entities in text. However, in real-world scenarios like social media, data often comes in various forms and modalities, not just text. This can make it difficult to understand ambiguous or multi-context words.

To address this, we've introduced multimodal relation extraction, which combines text with visual evidence. For example, seeing "Bachelor", "Gown", and "Cap" can help infer that JFK graduated from Harvard. However, there are still challenges. One is internal-information over-utilization, where only parts of the text are useful, and not all visual sources are helpful. Another is external-information under-exploitation, where visual features may not always provide enough context.

To tackle these issues, we propose a Graph Information Bottleneck principle-guided feature refining. We also consider using multimodal topic information as additional context. Our framework consists of five parts: representing text and image with scene graphs, merging them into a unified backbone cross-modal graph (CMG), filtering nodes and edges in CMG, enriching features with multimodal topics, and evaluating performance.

Experiments on a widely used MRE dataset show that our method outperforms text-based methods and other multimodal baselines. Ablation studies reveal that internal-information screening and external-information exploiting both contribute to performance. We also find that the scene graph is beneficial for structural modeling.

In conclusion, we introduce a novel idea of simultaneous internal-information screening and external-information exploiting for multimodal relation extraction. Our system achieves significant improvements over existing models. If you're interested, you can scan the QR code for more details. Thank you.</sample>
    <sample id="18">Un esempio della preferenza per i coniugi a sinistra più brevi è "Marge read yesterday this absolutely fascinating book." In questo caso, "this" è il congiunto a sinistra e "book" è il congiunto a destra. Anche se questo ordine viola la generalità della regola grammaticale che preferisce che gli oggetti diretti siano vicini al verbo, soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che preferisce le dipendenze più brevi.</sample>
    <sample id="19">In this presentation, Zhang Qin from Shenzhen University introduces their work, "A Survey for Efficient Open Domain Q&amp;A," which was accepted by ACL 2023, focusing on open-domain question answering. The two-stage framework proposed by Danqi Chen in 2007 is the mainstream approach, involving a retrieval stage to find evidence from Wikipedia and a reader stage to understand the question and reason out the answer. The retrieval process uses two encoders: question and document. The Wikipedia corpus, containing 26 million documents, requires significant storage and indexing, posing challenges for inference speed and real-time applications.

Zhang Qin's work aims to achieve efficient open-domain Q&amp;A systems with smaller memory costs, faster inference, and comparable performance. They discuss core techniques, including retrieval and reader frameworks, one-stage systems, and efficient tactics such as approximate nearest neighbor search, adaptive computation, document filtering, embedding compression, and model size reduction.

The analysis reveals that retrieval-only systems are fast but create large indexes, while generator-only systems are large models with low performance. Zhang Qin concludes that if resources are limited, one can consider reducing index size or model size. For real-time feedback, retrieval-only systems are suitable, while retrieval and reader systems are better for trade-offs.

Finally, Zhang Qin suggests two future works: deploying open-domain Q&amp;A systems in low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Yes, you can use the models for your research. The models, including DrBERT and ChuBERT, are freely available on Hugging Face under the MIT license, and the training scripts are available on the GitHub repository.</sample>
    <sample id="21">DEplain-apa contiene documenti di notizie.</sample>
    <sample id="22">According to the paper, the three main factors that contribute to good generalization are:

1. Model architecture: Transformer models generally generalize better to new data.
2. Model size: Larger models tend to lead to better generalization.
3. Number of fine-tuning examples: More fine-tuning examples can improve performance and lead to better generalization.

These factors are interrelated, and the paper suggests that all three are necessary for good generalization, rather than relying on just one.</sample>
    <sample id="23">The paper discusses the challenges faced by text-to-image models in rendering text accurately. The Imagen model, which uses a T5-XXL encoder to encode text and a diffusion model to generate images, struggles with representing text, especially for simple inputs. The authors investigate the text encoder's ability to spell words and find that T5 models perform poorly, with accuracy under 70% even for the largest model. In contrast, PaLM models, which have better spelling accuracy, are impractical due to their size. ByT5, which receives character-level input, performs well at spelling. The authors propose an improvement by augmenting the Imagen model with a ByT5-small text representation, increasing the parameter count by only 5%. This results in better text rendering and image generation. The main takeaways are the WikiSpell benchmark for text-only models and the DrawText benchmark for text-to-image models. The paper introduces a new strategy for improving model spelling ability by concatenating a character-aware model.</sample>
    <sample id="24">La tendenza dei congiunti a sinistro a essere più brevi è stata misurata in base al numero di parole utilizzate per ciascun congiunto. Si è osservato che, in generale, i congiunti a sinistro tendono a essere più brevi rispetto ai congiunti a destro, soprattutto quando la relazione tra i due congiunti è governata da un verbo a sinistra o quando non c'è un governo esterno. Questo effetto è più evidente quando la differenza di lunghezza tra i due congiunti è maggiore. Questo è stato dimostrato misurando la lunghezza in parole, sylabbe e caratteri per diverse strutture di coordinazione.</sample>
    <sample id="25">Gli esperimenti per studiare l'influenza della posizione del governatore sono stati progettati analizzando le statistiche delle congiunzioni nella versione migliorata del Penn Treebank. Gli autori estraggono vari dati sulla coordinazione, misurando la lunghezza delle dipendenze cruciali tra il verbo e i congiunti. Sono stati esaminati i congiunti in termini di numero di parole, sottilezza e caratteri. Gli esperimenti mostrano che quando il governatore è a sinistra o assente, il congiunto più corto tende a essere il primo, mentre quando il governatore è a destra, questo effetto scompare. Questi risultati supportano l'argomento per le strutture simmetriche di coordinazione.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciata, come quelli relativi alla dissonanza che rappresenta solo il 3,5% delle annotazioni, è generalmente inefficace. La performance del classificatore è molto vicina al caso casuale, poiché il numero di esempi dissonanti è molto basso rispetto al numero totale di esempi annotati. Questo problema è noto come problema dell'assoluto raro e richiede strategie speciali per migliorare la classificazione, come l'uso di tecniche di apprendimento attivo e il trasferimento di conoscenze da task correlati.</sample>
    <sample id="27">L'articolo "From Pretraining Data to Language Models to Downsstream Tasks: Tracking the Trails of Political Biasses Leading to Unfair NLP Models" è stato scritto da Shangbin, un dottorando alla University of Washington. Non è specificato quanti autori sono coinvolti nella ricerca, ma Shangbin è il principale autore del lavoro.</sample>
    <sample id="28">In the conversation example provided, the characters are Bob and Alice. Bob initiates the dialogue by asking Alice to remember a song they were listening to yesterday. Alice then asks if Bob means "Easy on Me" or "I Gotta Feeling." Bob uses an indirect reference to select one of the entities, for example, "the newer one," in the third speech bubble.</sample>
    <sample id="29">I modelli di MT migliorano rispetto a quello indipendente dal contesto in diversi fenomeni del discorso, come la forma del verbo, la coesione lessicale, la risoluzione degli ellissi e la coerenza del pronomio. Questi risultati sono stati ottenuti utilizzando un benchmark per la traduzione documentale che identifica e valuta diversi fenomeni del discorso.</sample>
    <sample id="30">Hello everyone, I'm Yuchen Lin from AI2 and USC, and I'm excited to introduce our paper "LLM-Blend," a simple yet effective ensemble learning framework for large-language models. With the rapid release of new models every week, it's challenging to determine the best one for a given input. Our findings show that the optimal model can vary significantly across different examples.

To address this, we propose LLM-Blend, a two-stage framework. First, we run multiple models on the input and obtain their outputs. Then, we use a pairwise ranking module called PairRanker to compare these outputs and generate a ranking. PairRanker encodes pairs of candidates alongside the input, allowing for a more nuanced comparison.

We found that using the max logits to aggregate the ranking is the most effective method, and it's also efficient. Our experiments show that PairRanker is better correlated with the oracle ranking than other methods.

To evaluate our framework, we created a new dataset called MixInstruct, which consists of existing instruction datasets and candidates from 11 open-source large-language models. Our results show that our PairRanker and full Blender framework outperform the top two models, Open Assistant and Vicena, in 68% and 76%, respectively, across four metrics.

In summary, LLM-Blend is a simple and effective ensemble learning framework for large-language models, with two sub-modules: PairRanker for pairwise comparison and GenFuser for generating the final output. We also released a unified codebase for evaluation and future research. Thank you.</sample>
    <sample id="31">The authors of the article are affiliated with the following institutions:

1. Koustav Sinha - University of California, Berkeley
2. John Gauthier - University of California, Berkeley
3. Aaron Mueller - University of California, Berkeley
4. Kanishka Misra - University of California, Berkeley
5. Karen Fences - University of California, Berkeley
6. Roger Levy - University of California, Berkeley
7. Adina Williams - University of California, Berkeley

These affiliations suggest that the research was conducted at the University of California, Berkeley.</sample>
    <sample id="33">Il framework NLPositionality quantifica la posizionalità in NLP attraverso due passaggi principali:

1. **Ripreparazione dei dati con annotatori diversificati**: I dati originali vengono ripreparati da annotatori diversificati, cercando di ottenere un numero maggiore di annotazioni per ogni istanza e raccogliendo dati demografici dettagliati. Questo permette di avere una visione più completa delle posizionalità dei dati.

2. **Confronto delle annotazioni con i modelli e i dataset**: Le annotazioni raccolte vengono confrontate con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson. Questo confronto permette di quantificare le discrepanze tra le annotazioni degli utenti e le predizioni dei modelli, evidenziando così le posizionalità dei modelli e dei dataset.

In sintesi, NLPositionality quantifica la posizionality confrontando le annotazioni di utenti diversificati con le predizioni dei modelli e i dataset, utilizzando metriche di correlazione per misurare le discrepanze.</sample>
    <sample id="34">In this presentation, Marcos Treviso introduces CREST, a joint framework for rationalization (selective explanation) and counterfactual text generation. CREST combines selective rationalization and counterfactual generation to produce valid, fluent, and diverse counterfactual examples. The framework consists of two main components: a rationalizer model that generates rationales and an editor that creates counterfactuals by masking and editing the input. CREST was evaluated using human and automatic metrics, with results showing that CREST counterfactuals are more valid and natural than those generated by other methods. Additionally, CREST was used for data augmentation and rationalization with both factual and counterfactual inputs, leading to improved performance on downstream models. The rationales generated by CREST were also found to be more plausible and counterfactual simulable than those produced by other approaches. Overall, CREST demonstrates the potential of combining rationalization and counterfactual generation for improved model performance and interpretability.</sample>
    <sample id="36">The paper "Learning Language-Specific Layers for Multilingual
Machine Translation" presents a novel approach to multilingual machine
translation that aims to increase the capacity per language while keeping
inference costs constant. The authors introduce Language-Specific Layers
(LSLs), which are regular transformer layers per language that are
selected and trained at inference time based on the source or target
language. The authors also propose a method for learning the best placement
of LSLs in the model, which involves training a large model with shared,
source, and target weights and selecting the component with the largest
weight. The authors evaluate their approach on WMT21 news translation
mask sources for 10 languages, including some European, Asian, and
Swahili languages, and report significant improvements over baseline
models and language adapters. The results show that the approach gives
improvements for every language, particularly for low-resource languages,
and is statistically significant for 84 out of 90 translation directions.</sample>
    <sample id="37">Il risultato dello studio precedente iniziale, in cui i soggetti umani ricevono gli stessi prompt di persona, ha rivelato che i soggetti umani possono sottolineare stereotipi razzisti e di genere. Questo metodo ha permesso di confrontare le risposte generate dai modelli con quelle umane, evidenziando così pattern di stereotipazione che possono essere rilevati e analizzati.</sample>
    <sample id="38">In questo studio, sono state utilizzate diverse fonti di dati per analizzare la struttura di coordinazione. Le principali fonti utilizzate includono:

1. **Enhanced Version of the Penn Treebank**: Questo è un dataset ampliato della Penn Treebank, che fornisce informazioni grammaticali dettagliate sui testi inglesi.

2. **Universal Dependencies (UD)**: Questo è un modello di annotazione grammaticale che fornisce una struttura di coordinazione asimmetrica, dove il primo congiunto è considerato il "capo" della struttura coordinata.

3. **Igor Mel'čuk's Meaning Text Theory**: Un altro modello di annotazione grammaticale asimmetrica, simile all'UD, che considera il primo congiunto come capo della struttura coordinata.
  
4. **Prague Dependency Treebanks**: Utilizzano una struttura di coordinazione asimetrica, dove la struttura è guidata dal congiunto.

5. **Hudson's Word Grammar**: Utilizza un modello di coordinazione simmetrica, dove tutti i congiunti sono considerati "capi" della struttura coordinata. 

Queste fonti sono state utilizzate per analizzare le tendenze nella preferenza per la struttura di coordinazione e per sostenere l'argomento per la struttura simmetrica della coordinazione basata sul principio di minimizzazione della lunghezza della dipendenza.</sample>
    <sample id="39">L'articolo menziona solo un autore, Adam Przepiórkowski, che discute la struttura di dipendenza della coordinazione e presenta argomenti per sostenere la simmetria nelle strutture di coordinazione. Non è specificato se ci sono altri autori coinvolti nell'articolo.</sample>
    <sample id="40">Le attività strettamente correlate alla disonanza cognitiva includono:

1. **Studio delle dissonanze nella lingua**: Analizzare e annotare le dissonanze espresse in testi per comprendere meglio i fenomeni di dissonanza cognitiva.
2. **Annotazione dei dati**: Creare un dataset di esempi di dissonanza per l'addestramento dei modelli.
3. **Transfer Learning**: Utilizzare modelli pre-addestrati su task correlati per migliorare la performance del modello su dissonanze.
4. **Active Learning**: Implementare strategie di apprendimento attivo per annotare dati rari e migliorare il modello.
5. **Classificazione dei dati**: Utilizzare modelli di classificazione per identificare e annotare le dissonanze in testi.
6. **Aggiornamento del modello**: Aggiornare il modello con nuovi dati annotati per migliorare la performance.
7. **Strategie di apprendimento attivo**: Implementare strategie di apprendimento attentiva per selezionare e annotare dati rari.
8. **Valutazione delle strategie**: Testare diverse strategie di apprendimento attivo per determinare quale funziona meglio per la classificazione delle dissonanze.
9. **Annotazione di qualità**: Assicurare che le annotazioni siano accurate e utili per il modello.
10. **Costi di annotazione**: Considerare i costi e la fattibilità delle annotazioni per i dati rari.

Queste attività sono strettamente legate alla ricerca e alla creazione di modelli per rilevare e comprendere le dissonanze cognitiva in testi.</sample>
    <sample id="41">Silin from EPFL's Natural Language Processing Lab introduces "PeaCoK: Persona Commonsens Knowledge for Consistent and Engaging Narratives," a collaboration with Sony Group Corporation. PeaCoK is a Persona-grounded Commonsens Knowledge Graph that represents world-level persona knowledge at scale, containing about 3,800 personas and over 40,000 distinctive attributes. The graph is built in three steps: selecting personas, inducing attributes, and crowdsourcing annotations. PeaCoK's relations are framed in three dimensions, including interactivity and distinctiveness.

PeaCoK is used to train a BART-based common knowledge generator, which outperforms large-scale pre-trained language models like GPT-3 and GPT-3.5 in automatic and human evaluation. The graph also improves persona-grounded dialogue generation on the ConvAI2 PersonaChat dataset, with human evaluation showing better fluency, consistency, engagement, and persona expression compared to the Atomic2020 knowledge graph.

The study highlights the importance of learning interconnected world persona knowledge in narratives, and PeaCoK's persona-centric commonsense information yields a more positive impact. The paper and GitHub site for this work are publicly available.</sample>
    <sample id="42">The article "Do CoNLL-2003 named-entity taggers still work well in 2013?" has one author, Shuheng.</sample>
    <sample id="43">L'articolo "Transfer Learning for Dissonance Detection: Aiming at the Rare-Class Challenge" è stato presentato da Vasudha e altri coautori. Tuttavia, il numero esatto di autori non è specificato nel testo fornito.</sample>
    <sample id="44">Il framework NLPositionality differisce dagli studi precedenti in quanto confronta i dati annotati con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson, piuttosto che analizzare solo l'accordo tra annotatori o la distribuzione degli annotatori. Inoltre, NLPositionality utilizza un'piattaforma di esperimentazione online, Lab in the Wild, per raccogliere dati da un numero elevato di annotatori da diverse nazioni, il che permette di ottenere una visione più completa delle posizionalità dei modelli e dei dataset.</sample>
    <sample id="45">La configurazione che sovrappone maggiormente al meno degli stereotipi è quella che utilizza il metodo "Marked Words". Questo metodo identifica le parole che distingue i gruppi marcati dai gruppi non marcati, basandosi sull'idea sociolinguistica del "markedness", che afferma che ci sono gruppi dominanti che sono linguisticamente e socialmente non marcati, mentre i gruppi marginalizzati sono generalmente marcati. In questo modo, il metodo rivela pattern di stereotipi e essenzializzazioni che non sono catturati dai lessici degli stereotipi tradizionali.</sample>
    <sample id="46">I sistemi commerciali che sono stati messi a confronto sono DeepL e Google Translate. La mia ricerca ha mostrato che DeepL è generalmente più accurato di Google Translate per la traduzione a livello di documento.</sample>
    <sample id="47">Ciao, sono Shangbin, studente di dottorato all'Università di Washington. Oggi sto presentando il nostro lavoro "Dai dati di pretraining ai modelli linguistici ai compiti downstream: traccia i sentieri delle bias politici che portano a modelli NLP non equi". I modelli linguistici vengono addestrati su grandi quantità di dati web. Le notizie politiche sono ben coperte nei loro dati di pretraining. Secondo una sondaggio sul Corpus C4, possiamo vedere che il New York Times, Los Angeles Times, TheGuardian, Huffington Post, ecc., sono ben coperti nei dati di addestramento dei modelli linguistici. Questo ha creato un buon e un male misto per le applicazioni dei modelli linguistici. D'una parte, sono in grado di imparare da diverse prospettive, che celebra la democrazia e la pluralità di idee. D'altra parte, queste diverse opinioni politiche sono inerentemente socialmente biased e potrebbero portare a potenziali problemi di equità nelle applicazioni downstream. Per questo scopo, ci proponeremo di investigare il pipeline di propagazione dei bias politici dai dati di pretraining ai modelli linguistichi ai compiti downstream, specificando le seguenti domande: Prima di tutto, come valutiamo l'inclinazione politica dei modelli linguistici e qual è il ruolo che i dati di pretraining potrebbe avere su tali bias politici? In secondo luogo, come comportano i modelli linguistici con diverse inclinazioni politiche sulle attività downstream e se potrebbero risultare in problemi di equità nelle applicazioni di NLP? Quindi, specificamente, abbiamo proposto di promuovere i modelli linguistici con diverse forme di domande utilizzando questionari politici come il test della conferenza politica. Questo ci permette di fare una valutazione automatica ben basata sulla letteratura di politica scientifica. Alcuni risultati preliminari dimostrano che i modelli linguistici hanno variazioni di inclinazione politica. Occupano tutte le quattro quadranti del campus politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti. I modelli GPT sono generalmente più socialmente liberali rispetto ai modelli BART e alle loro varianti. In secondo luogo, miriamo a quanto i bias politici dei modelli linguistici siano effettivamente acquisiti dai dati di pretraining. Potremmo condurre un'esperimento controllato prendendo in considerazione ulteriori checkpoint dei modelli linguistici e addestrando i modelli su 6 diversi corpus partigiani separati in notizie e social media, ulteriormente suddivisi in loro inclinazione politica. Prendendo in considerazione i modelli linguistici addestrati su tali corpus partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico corrispondenti cambiino. Ad esempio, per RoBERTa addestrato su un corpus Reddit di inclinazione sinistra, possiamo vedere un notevole cambiamento liberale in termini di bias politici. E proviamo anche a investigare fino a quale estensione i modelli linguistici possono prendere il polarizzazione che è prevalente nella nostra società moderna. Quindi, dividiamo i corpus di pretraining in quelli pre-45° presidente degli Stati Uniti e post-45° presidente degli Statii Uniti. Prendiamo in considerazione i due diversi corpus temporali separati. Possiamo vedere che i modelli linguistici hanno un inclinazione politica che è ulteriormente lontana dal centro dopo 2017. Questo indica che i modelli linguistici possono anche prendere il polarizzazione nella nostra società. Quindi, infine, valutiamo i modelli linguistici con diverse inclinazione politica su compiti come la</sample>
    <sample id="48">L'articolo "Prompting PaLM for Translation: Assesing Strategies and Performance" è un lavoro congiunto di David Vilar e i suoi colleghi da Google Translate. In questo articolo, sono coinvolti due autori principali: David Vilar e i colleghi di Google Translate.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 1024 token di lunghezza del contesto.</sample>
    <sample id="50">The presentation introduces DEPLAIN, a new corpus for German document-level and sentence-level text simplification. Text simplification aims to make texts more comprehensible for specific target groups, such as people with reading difficulties or non-native speakers. The corpus is split into two subcorpora: DEPLAIn-apa, based on manually aligned news texts, and DEPLAIn-web, which includes various domains and uses both manual and automatic alignment methods. The corpus contains 13,000 parallel sentence pair from DEPLAIn-apa and 30,450 from DEPLAIn-web, with a high variety of simplification transformations.

The first use case discussed is evaluating automatic alignment methods using the manually aligned sentences in DEPLAIN as gold standards. The best method found was MASSalign, and the code for running it is available in the paper. The second use case involves fine-tuning language models to produce simplified text from complex input. Two models were fine-tuned: long-mBART for document-level simplifications and normal base mBART for sentence-level simplifications. The results showed that this basic fine-tuning could produce better scores than baseline methods, and the results are proposed as a benchmark for future research.</sample>
    <sample id="51">The domains included in their dataset are music, books, and recipes.</sample>
    <sample id="52">La posizionalità è la somma delle prospettive che le persone detengono a causa delle loro caratteristiche demografiche, identità e esperienze della vita. Questo concetto è ampiamente utilizzato in studi critici, in particolare in spazi accademici femministi e queer. La posizionalità può influenzare il processo di ricerca e i suoi risultati, poiché può cambiare le decisioni che i ricercatori prendono.</sample>
    <sample id="53">Il nome del relatore è Dawei.</sample>
    <sample id="54">Vasudha, a Computer Science PhD candidate at Stony Brook, presents their work on "Transfer Learning for Dissonance Detection" accepted at ACL 2023. Cognitive dissonance, the inconsistency between beliefs and actions, is a rare but significant phenomenon in language. Studying dissonance can help understand disagreement, mental health, extremism, and decision-making. To create a cognitive dissonance resource, Vasudha's team annotated 1,000 pairs of discourse units, finding dissonance in only 3.5%. Due to the rarity of dissonance, they used transfer learning and active learning to improve detection. They transferred weights from related tasks, such as debate stance classification and binary classification of PDTB, and found that fine-tuning on these tasks improved zero-shot performance. They also compared different active learning strategies, finding that the Probability-of-Rare-Class (PRC) strategy worked best. The team improved dissonance classification AUC to 0.62 and then to 0.75 with further rounds of active learning. They also found that PRC is a simple active learning strategy for rare class acquisition and cold starting AL. The team's work has implications for understanding cognitive dissonance and its effects on individuals and society.</sample>
    <sample id="55">EDAtt adatta un modello ST offline esistendo.</sample>
    <sample id="56">The article "XSemPLR: Cross-LingUAL Semantic Parsing in Multiple Natural Languages and Meaning RepresenTations" does not explicitly mention the number of authors involved. However, based on the information provided, it is likely that the article is authored by a team of researchers from Penn State University, with Yusen Zhang being the primary presenter. The exact number of authors is not specified in the text.</sample>
    <sample id="57">Il modello testato funziona sulla sua suite di test, KITMUS, che è stato progettato per valutare la capacità di integrazione del modello di comprendere le conoscenze da più fonti. La suite di test include diverse configurazioni che simulano l'accesso a conoscenze a diverse fasi: a pretrain, a entrambe le fasi, e solo a inferenza. I risultati mostrano che i modelli migliori, quando addestrati su KITMUS, riescono a integrare le conoscenze da più fonti, ma ancora presentano difficoltà nell'integrare le conoscenze a inferenza solo.</sample>
    <sample id="58">The three variants of KITMUS are:

1. **Background-Pretrain**: Background knowledge is assumed to be available at pretrain-time.
2. **Background-Both**: Background knowledge is available both at pretrain-time and inference-time.
3. **Background-Inference**: Both knowledge types are available only at inference-time.</sample>
    <sample id="59">Yanis Labrak presents their work on "DrBERT," a robust pre-trained model in French for biomedical and clinical domains. The presentation begins with an overview of language modeling in healthcare, highlighting the limitations of traditional methods like Word2vec and fastText. DrBERT, based on RoBERTa and trained on NANCHOS, a dataset of medical crawled data, is introduced as the first biomedical model in French. The authors compare DrBERT with ChuBERT, a clinical model based on anonymized data from a hospital, and analyze the impact of pre-training strategies on performance.

The evaluation of seven models, including from-scratch pre-training and continual pre-training, shows that models trained on data of the same nature as the task perform best. However, data from heterogeneous sources appears more versatile, and using more data generally leads to better performance. From-scratch pre-training generally yields higher performance, but control pre-training using CamemBERT weights and tokenizer can achieve comparable results.

DrBERT outperformed the generic model CamemBERT on nine of the 11 downstream tasks, surpassing its performance globally. The authors conclude that more specialized data is better but does not scale well. All pre-trained models are available on Hugging Face under the MIT license, and training scripts are on their GitHub repository.</sample>
    <sample id="60">The authors of the article are Javad Hosseini, Filip Radlinski, Silvia Paret, and Annie Louis.</sample>
    <sample id="61">The last research question addressed in the video is: Should we only use the clean samples for validation, and are there better ways to utilize them? The findings suggest that increasing the number of clean validation samples helps WSL approaches achieve better performance, and training on clean samples directly can even outperform WSL methods. Additionally, the performance improvement claimed in previous WSL methods can be achieved by allowing continued fine-tuning on clean validation samples.</sample>
    <sample id="62">Hello, my name is Nitay Calderon, and I am the main author of this ACL paper, "Systematic Study of Knowledge Distillation for Natural Language Generation." The paper explores the potential of compressing large language models used in natural language generation (NLG) systems, which are becoming increasingly complex and costly. The goal is to find a way to compress these models while preserving their performance.

The paper focuses on two main types of noise distillation: word-level and sequence-level. Word-level distillation involves minimizing the KL divergence between the logits of the teacher and student models, while sequence-level distillation uses the teacher to generate pseudo-targets, which are then used to augment the training data for the student.

The study is conducted in realistic setups, with five criteria: medium-resource labeled data, large amounts of unlabeled data, medium-sized off-the-shelf models, high compression rate, and negligible one-time training resources. The study considers four NLG tasks: summarization, question generation, common sense reasoning, and simplification/style transfer.

The paper presents eight stages, including exploring architectural decisions, understanding the impact of pruning, comparing different approaches for knowledge selection, and exploring extensions of the usage of pseudo-targets. The authors propose a novel knowledge distillation technique called joint-teaching, which aims to address student exposure bias, grounded learning, and teach the student to correct its own mistakes.

In summary, the paper provides a systematic study of task-specific knowledge distillation for natural language generation, exploring various approaches and techniques to compress large language models while preserving their performance. The authors also propose a novel knowledge distillation technique called joint-teach</sample>
    <sample id="63">La sensibilità della metrica è un indicatore che misura la capacità del modello di produrre output coerenti per lo stesso compito, indipendentemente dalle variazioni leggermente diverse nella formulazione delle istruzioni. In altre parole, la sensibilità valuta la stabilità del modello quando viene eseguita con diverse istruzioni che descrivono lo stesso compito. Un valore alto di sensibilità indica che il modello è meno sensibile alle variazioni nella formulazione delle istruzione, mentre un valore basso indica che il modello è più sensibile e potrebbe produrre risultati diversi con le stesse istruzioni. Questa metrica è importante per valutare la robustezza del modello in situazioni in cui le istruzioni possono variare leggermente.</sample>
    <sample id="64">The name of the presenter is Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità indicarebbe che il modello produce risposte variabili anche per le stesse istruzioni, il che è generalmente visto come un problema. Invece, una sensibilità più bassa suggerisce che il modello è più coerente e robusto, migliorando la sua capacità di generalizzazione e performance su nuove attività.</sample>
    <sample id="66">Hello everyone. We are excited to share our ACL paper, "Deep Learning in Mathematical Reasoning." Mathematical reasoning is a crucial aspect of human intelligence that allows us to understand and make decisions based on numerical data and language, and the development of machines capable of solving math problems and proofs has been a long-standing focus of AI and natural language processing (NLP). In recent years, there has been a surge in interest in this area, and our survey discusses the task of mathematical reasoning and a deep learning method.

Mathematical reasoning isn't limited to text-based data; we can extend it to multimodal information like images, figures, tables, and geometric diagrams. Solving geometric problems is an essential subject in education, and we can formalize these tasks as a neuro-symbolic reasoning problem over geometric figures, theorems, and solvers. Another important line is automated theorem proving, which aims to demonstrate the truth of a mathematical claim through a sequence of arguments.

Several neural network architectures have been proposed for mathematical reasoning tasks, such as sequence-to-sequence models and sequence-to-tree models. In recent years, pre-trained language models like large language models (LLMs) have demonstrated remarkable performance on a wide range of tasks, including math word problems. However, LLMs still face limitations, such as the lack of precise mathematical reasoning.

To address this, we propose replacing the greedy decoding strategy with self-consistency, which involves sampling a diverse set of reasoning paths and choosing the most frequent one. We also propose designing two augmented LLMs, such as program-aided LLMs, which can perform complex tasks by composing different tools.

Despite impressive progress, mathematical reasoning in low-resource settings remains underexploited. We have developed non-English datasets for Chinese, Korean, and Arabi, and pioneering research has developed mathematical reasoning benchmarks for financial and medical domains. However, language models still struggle with large numbers and are inconsistent with mathematical reasoning.

In conclusion, our paper provides a comprehensive survey of mathematical reasoning and deep learning methods, highlighting the challenges and opportunities in this field. We hope our work will inspire further research and development in this area. Thank you for your attention.</sample>
    <sample id="67">In this work, Uri discusses the issue of interference in multilingual translation models, which can either benefit from synergy between different language pairs or suffer due to interference. The main factors contributing to interference are the size of the model compared to the data size and the tuning of the sampling temperature. The study found that severe interference occurs when the model is very small compared to the dataset, and that tuning the sampling temperature is crucial for strong performance. The research also found that language similarity and the number of languages do not significantly impact interference levels. The study used four variants of the Transformer architecture and 15 languages from WMT, ranging in data size from over 50 million sentence pairs to around 150,000. The results showed that severe interference occurs only in parameter-poor settings, and that modest scale and tuned temperature can significantly reduce the problem without any specialized methods. The study concludes that model and data size affect the levels of</sample>
    <sample id="68">Durante il pre-addestramento, i modelli linguistici vengono esposti a contesti linguistici vari, inclusi frasi grammaticalmente corrette e incorrecte, frasi che riflettono stereotipi, e frasi provenienti da fonti diverse come Wikipedia. Questi contesti vengono utilizzati per valutare l'accettabilità delle frasi e la loro robustezza a vari livelli di contesto lungo la finestra di contesto.</sample>
    <sample id="69">In Weakly Supervised Learning (WSL), typically only 20 clean samples per class are needed to achieve high performance. However, if clean samples are used for validation, training on them directly can even yield better results.</sample>
    <sample id="70">Gli autori dell'articolo "Marked Personas: Using Natural Language Pronouns to Measure Stereotypes in Language Models" sono Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">Hi! I'm Javad Hosseini, and I'm here to talk about our work on "ResolvingIndirect Referring Expressions for Entity Selection," which introduces the AltEntities Corpus. Our goal is to understand users' language when they want to make a choice, such as selecting between two songs. Direct references like "Easy on Me" or "I Gotta Feeling" are obvious, but indirect references are more natural and necessary when users can't remember names, have similar pronunciations, or want to specify preferences.

We created the AltEntities Corpus by collecting data through crowd annotation, covering music, books, and recipes. Our data collection emphasizes informality using a cartoon setup with three speech bubbles. The first bubble sets the context, the second asks the alternative question, and the third is filled in by the annotator using indirect references.

We used different sampling methods to generate the alternative questions, ranging from random selection to entities with similar titles, descriptions, or attributes. Annotators, who know the entities but not their details, were provided with background knowledge, such as Google search links for songs or Wikipedia text for books and recipes. They then described the entities using indirect referring expressions.

The AltEntities Corpus contains 6,000 alternative questions and 42,000 indirect referring phrases. When language models have access to the same background knowledge as annotators, accuracy is high (92-95%). With partial background knowledge, accuracy is between 82-87%, which is more realistic. Without any background knowledge, accuracy drops to 60%.

Our results show that language models are domain-generalizable, and we've made our dataset available for further research. Thank you!</sample>
    <sample id="72">The need for new methods to measure information bias arises from the fact that language models, which are trained on large-scale web crawl data, can inherit and propagate political biases present in the pretraining data. These biases can lead to unfairness in downstream tasks, such as hate speech detection and fake news detection, where language models with different political leanings may perform differently based on the political leaning of the news media they are trained on. Therefore, it is important to develop new methods to measure and address these biases in order to ensure fairness and equity in NLP applications.</sample>
    <sample id="73">The name of the presenter is Akshatha.</sample>
    <sample id="74">Hello everyone. Today, I will introduce our paper, "Dense-Atomic: Towards Densely-connected ATOMIC with High Coverage and Massive Multi-hop Paths." I'm Xiangqing, and here are my co-authors. Commonsense knowledge is essential for machines interacting with humans, describing facts and related judgments in everyday life. ATOMIC is a large-scale commonsense KB that covers event-centered social aspects of inferential knowledge. However, it has very few multi-hop paths due to its limited B-to-A links.

To address this, we constructed Dense-Atomic, which completes many missing links, including B-to-A, B-to-B links, and A-to-B, A-to-A links. Dense-Atomic also contains multi-hop paths, such as "X asks Y to marry, and then Y says 'yes,' and then X smiles."

Our process for constructing Dense-Atomic involves three parts: normalizing tail events, training the relation prediction model, and constructing Dense-Atomic. Normalizing tail events converts them into the same equation as the head event. We use RoBERTa to encode head and tail events and apply MaxPooling for link prediction.

We propose Rel-CSKGC, which predicts the relationship between head and tail events. We randomly sample negative triplets and combine them with the training set to construct the training set for Rel-CSKG. We also design an Intra- and Inter-Cluster Completion strategy to infer missing links.

We compare Rel-CSKGC with relation prediction methods and translation-based methods, and Rel-CSKGC outperforms them on both automatic and human evaluation. We also evaluate the constructed Dense-Atomic, which yields higher knowledge coverage and benefits the performance of COMET.

In summary, we construct a densely-connected commonsense knowledge base, Dense-Atomic, and propose a new CSKG completion method for infer</sample>
    <sample id="75">Zheng Yandan presents her work, Jointprop, a joint semi-supervised learning framework for named entity recognition (NER) and relation extraction (RE). The motivation behind this work is to address the limitations of fully-supervised models that require extensive labeled data and the inefficiencies of current semi-supervised models that neglect the interconnections between NER and RE tasks. Jointprop aims to fully integrate information from both labeled and unlabeled data by propagating labels over heterogeneous graphs.

The framework consists of four parts: span feature generation using contextualized representations, heterogeneous graph construction using k-Nearest Neighbor graphs, joint label propagation, and model optimization. The label propagation process diffuses labels through the graph, refining pseudo-labels until convergence. The model optimization involves using the softmax function and argmax operation to determine pseudo-labels, filtering low-quality labels, and retraining the classification model.

Experiments were conducted on four datasets, including joint-task and single-task datasets. The results show that joint learning of NER and RE tasks benefits from the codependency between the two, while the framework shows significant improvement over all baselines for single-task datasets. Overall, Jointprop provides a promising approach for semi-supervised NER and RE tasks.</sample>
    <sample id="76">L'infrastruttura di propagazione degli bias politici in modelli linguistici è complessa e coinvolge diverse fasi, dalla raccolta dei dati di pretraining alla loro applicazione in compiti downstream. Ecco un riassunto sintetico degli aspetti chiave:

1. **Pretraining Data**: I modelli linguistici vengono addestrati su grandi quantità di dati provenienti da web, inclusi notiziari politici. Questi dati possono riflettere diverse ideologie politiche, creando un mix di prospettive che, se non monitorati, possono portare a bias.

2. **Evaluazione dei Modelli**: Gli studi suggeriscono che i modelli linguistici possono avere variazioni politiche significative. Ad esempio, GPT-4 è generalmente più liberale rispetto a altri modelli come BART. Questo avviene grazie a strumenti di valutazione automatica basati su questionari politici, come il test della conferenza politica.

3. **Pretraining su Corpi Partizanari**: I modelli possono essere ulteriormente addestrati su corpus partizanari separati in notizie e social media, con diverse ideologie politiche. Questo porta a un cambiamento nelle ideologie dei modelli, come visto con RoBERTa addestrato su un corpus Reddit liberale.

4. **Polarizzazione Sociale**: I modelli possono anche riflettere la polarizzazione sociale, con una maggiore inclinazione verso estremi dopo l'elezione del 45° presidente degli Stati Uniti.

5. **Applicazioni Downstream**: Gli bias politici possono influenzare le performance dei modelli in compiti come la rilevazione del discorso odioso e la rilevazione delle notizie false. Ad esempio, i modelli liberali sono meglio in grado di rilevare discorsi odiosi contro gruppi minoritari, mentre i modelli conservatori sono meglio in grado di rilevarli contro gruppi più potenti.

6. **Dilemma di Sanitazione**: C'è un dilemma tra la sanitazione dei dati di pretraining e la possibile esclusione o censura. Determinare cosa è "neutrale" è difficile, e la sanitazione può portare a una discriminazione.

In sintesi, l'infrastruttura di propagazioni dei bias politici in modelli linguistici è un problema complesso che richiede attenzione per garantire l'equità e la trasparenza nei compiti downstream.</sample>
    <sample id="77">This video presents the work "On Improving Summarization Factual Consistence from Natural Language Feedback," a joint effort between Yale University and Microsoft Research. The project introduces a new dataset, DeFacto, which includes human demonstrations and feedback aimed at enhancing summarization factual consistency. The dataset provides comprehensive analysis and insights into the factual consistency of summarization models.

The researchers propose three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual correction. They offer strong baseline models for each task. The study focuses on abstractive text summarization, specifically examining the factual consistency of summarization models. The premise is that all information in the summary should be supported by the source document.

The human demonstrations and feedback are based on the original system-generated summaries of existing summarization models. Annotators label summaries for factual consistency and provide human-corrected, factually consistent summaries if necessary. They also offer instructions and evidence to support their decisions. The data is collected on the XSum dataset, with initial system outputs from the pre-trained Pegasus model.

The video shows an example of annotated data points, basic data statistics, and the distribution of editing instructions. It highlights that human-edited summaries receive higher automatic factuality scores but have lower textual overlap with reference summaries. The first task, summary editing, shows that both fine-tuned models and zero-shot large language can effectively leverage human feedback. The second task, feedback generation, remains challenging for both fine-tuned models and large language models. The third task, automatic factual error correction, demonstrates that the editor model can achieve comparable performance with fewer data and that generating explanations helps improve performance.

The DeFacto dataset serves as a test bed for the proposed NLG tasks and offers advantages due to its fine-grained annotations, which can be useful for training factuality metrics and meta-evaluation. The dataset is available on GitHub, and the paper provides more details.</sample>
    <sample id="78">Yes, the process of simplification differs between DEplain-apa and web. DEplain-apa is based on news texts and has a higher level of simplification, with more reorderings and word additions. On the other hand, DEplain-web includes different domains and has more rephrasings. Additionally, DEplain-apa has 13,000 parallel sentence pair, while DEplain-web has 30,450 sentence pairs, making it a larger corpus.</sample>
    <sample id="79">Yes, CoScript is available publicly. The dataset, which consists of 55,000 specific goals and scripts, was generated using the method described in the paper. The dataset is intended to be a valuable resource for advancing research on constrained language planning.</sample>
    <sample id="80">In the provided text, the watermark (filigrania) is inserted in the following way:

1. **Watermark Injection**:
   - A **trigger set** is selected, which is a group of words in a moderate frequency range.
   - The provider counts the number of triggers in a sentence sent by the user.
   - The provided embedding is a weighted sum of the target embedding and the original embedding.
   - The weight of the target embedding is proportional to its frequency in the sentence.
   - If the number of triggers in the sentence exceeds a threshold \( m \), the provided embedding is exactly equal to the target (watermarked) embedding.

2. **Copyright Verification**:
   - A **back door** dataset is constructed, containing sentences where all words belong to the trigger set.
   - A **benign** dataset is also constructed, containing sentences where none of the words belong to the trigger set.
  
3. **Detection**:
   - The provider requests embeddings from the stealer's service using both the back door and benign datasets.
   - The cosine and L2 similarity between the requested embeddings and the target embedding are computed.
   - The similarity difference between the benign and back door datasets is calculated (delta cosine and delta L2).
   - A KS test is applied, and the p-value is used as a third metric.

This process ensures that the watermark is embedded in a way that is both detectable and does not degrade the utility of the embeddings for downstream tasks.</sample>
    <sample id="81">L'articolo è stato presentato da Yusen Zhang, che è affiliato all'Università Penn State.</sample>
    <sample id="82">The video discusses the challenges and solutions in Automated Essay Scoring (AES), a field that aims to evaluate the quality of essays without human intervention. Traditional AES models rely on large labeled datasets, which are time-consuming and labor-intensive to collect. Unsupervised AES, which doesn't require labeled data, has significant potential but faces challenges due to the lack of strong supervision.

The video introduces two existing approaches to unsupervised AES: one using the number of unique terms as a heuristic score and another using word count as a weak supervision. Both methods have limitations, leading to poor performance.

To address these issues, the video presents a novel framework called ULRA (Learning from Rank Aggregation for Unsupervised AES). ULRA introduces multiple heuristic quality signals to provide stronger and more robust supervision. The framework consists of two main components: the HER (Heuristic Essay Ranking) module and the DPRA (Deep Pairwise Rank Aggregation) module.

The HER module generates partial-order pairs by ranking essays based on heuristic quality signals, such as the number of unique terms and word count. The DPRA module aggregates these partial-order pairs to train a neural AES model. To handle inconsistent partial-order supervision, ULRA uses a Deep Pairwise Rank Aggregation loss, assigning learnable confidence weights to each signal.

The video concludes by stating that ULRA outperforms existing unsupervised baselines and achieves competitive performance compared to cross-prompt and one-shot methods. However, it still lags behind general supervised methods due to the lack of strong supervision. Overall, ULRA offers a promising approach to unsupervised essay scoring by aggregating multiple heuristic quality signals to provide robust supervision for neural AES models.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore, come mt5, possono migliorare con l’addestramento su una combinazione di diverse lingue. Questo è noto come il "Curse of Multilinguality", dove il modello può ottenere performance migliorate in la maggior parte delle lingue principali, tranne l'inglese, che mostra una performance peggiore in sette dataset e miglioramenti in tre dataset.</sample>
    <sample id="84">Hello everyone, I'm Shwai He, and today I'll discuss my paper for ACL 2023 titled "PAD-Net: An Efficient Framework for Dynamic Network." Traditional networks are static, with fixed parameters, while dynamic networks can adapt their architecture or parameters based on input. Examples include Mixture of Experts and Dynamic Convolution. However, fully dynamic networks often have excessive parameters, making them impractical.

Our hypothesis is that fully dynamic networks contain redundant dynamic parameters that can be made static without significantly affecting performance. We introduce PAD-Net, a Partially Dynamic Network, which partitions parameters into dynamic and static modes, using Iterative Mode Partition to reduce redundant parameters.

Experiments show PAD-Net outperforms static and dynamic networks, with fewer parameters and less computation. Ablation studies reveal optimal Dynamic Ratios for Dynamic Convolution and MoE, and the importance of Scale Factors. Compared to network pruning, PAD-Net maintains static parameters, enhancing performance and discriminability.

Future work includes extending PAD-Net to other networks, hardware-friendly structures, and combining zero elements, static, and dynamic parameters. Thank you.</sample>
    <sample id="85">Un esempio di pianificazione linguistica con vincoli è "preparare un tortino di cioccolato" con specifiche istruzioni come "aggiungere cioccolato fondente, zuccheri, uova e farlo cuocere a 180 gradi per 30 minuti". Questo esempio include vincoli come l'uso di ingredienti specifici e le istruzioni precise per ottenere il risultato desiderato.</sample>
    <sample id="86">The authors validate the covertness of their embedding marker by visualizing the embeddings of sentences on four datasets using PCA. They show that it is difficult to distinguish between the backdoor embeddings and normal embeddings, indicating that the watermark is covert enough to the attacker.</sample>
    <sample id="87">Il lavoro utilizza i PLM esistenti come base per costruire uno nuovo modellino chiamato DrBERT, che è basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici raccolti dal web. I modelli esistenti utilizzati includono CamemBERT OSCAR, CamemBERT CCNET, PubMedBERT, BioBERT e ClinicalBERT. I modelli sono allenati su dati di diverse fonti, come dati medici anonimizzati e dati di note cliniche, e vengono confrontati per valutare le loro prestazioni su vari compiti di elaborazione del linguaggio naturale (NLP) in ambito biomedico e clinico.</sample>
    <sample id="88">GPT-4 è meno allineato ai paesi non binari, rispetto ai maschi e alle femmine.</sample>
    <sample id="89">La relatrice mostra il modo in quale il modello sfrutta la conoscenze apprese attraverso il meccanismo di attenzione nel seguente esempio: "Our solution is to propose EDAtt, or Encoder-Decoder Attention, and it is a strategy for which... A word is emitted if the attention is not concentrating, that is, its sum is below a threshold alpha towards the last lambda speech frames, indicating that the received information is stable enough."</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners
Contribute?" by Haneul Yoo and colleagues explores the feasibility of using language learners as annotators for natural language processing (NLP) tasks. The authors question the necessity of recruiting native speakers for data annotation, especially for low-resource languages, and conduct a proof-of-concept study to examine the potential of language learners.

The study targets three languages: English, Korean, and Indonesian. It uses four tasks from the GLUE benchmark, including sentiment analysis, natural language inference, named entity recognition, and machine reading comprehension. The authors categorize learners into three levels based on their language proficiency and conduct experiments with native speakers for comparison.

The experiments involve a preliminary survey, pre-test, annotation, and post-test. The results show that language learners can provide nearly accurate annotations, especially for simpler tasks. Aggregating their labels with those of native speakers can achieve similar performance. The study also demonstrates that language learners can improve their language proficiency and vocabulary through annotation tasks.

The authors conclude that language learners can contribute to NLP annotations, and their work suggests a novel way of building benchmark datasets for low-resource languages. The study has implications for broadening NLP research and overcoming geographic and technological barriers.</sample>
    <sample id="91">The quantity of activities influences the model's performance by improving it as the number of tasks increases. Specifically, the model achieves better performance and lower sensitivity with more tasks. Additionally, transfer learning from natural instruction datasets enhances the model's sensitivity and performance on natural instruction datasets.</sample>
    <sample id="92">1. **Treeless Neural Seq2Seq Models**: The authors compare their approach with other neural seq2seq models that do not use trees. These models aim to predict the output from the input in two steps: tagging each input token with an unordered multiset and then predicting a permutation to order the tokens.

2. **Tree-Based Models**: The authors also compare their method with tree-based models, which integrate trees to capture the compositional process between utterances and logical forms. These models typically require formal pre-processing and specialized grammar-induction procedures to obtain trees.

3. **Standard Machine Learning Evaluation**: The authors contrast their approach with standard machine learning evaluation methods, where the test set contains structurally unseen logical forms. This comparison highlights the challenge of compositional generalization and the limitations of naive seq2seq models in handling deeper recursion.</sample>
    <sample id="93">I coautori della tua ricerca, Alexander Koller e Ivan Titov, sono i tuoi supervisori. Questo significa che lavorano insieme con te per sviluppare il tuo progetto e fornire guida e supporto durante la ricerca.</sample>
    <sample id="94">Hello everyone, my name is Jingwei from the University of Science and Technology of China, and I'm here to introduce our paper on protecting the copyright of embedding as services using a backdoor watermark.

Embedding as services, such as those provided by OpenAI, are built on top of large language models like GPT, LLAMA, and PALM. These services assist in various natural language processing tasks. However, recent research has shown that attackers can steal these models by learning from the embeddings and providing similar services. To protect the copyright of embedding as service providers, we need a method that meets several criteria: it should be applicable to embedding as services, not degrade the utility of the provided embeddings, be covert enough to prevent easy removal by attackers, and be transferable to the attacker's services during model extraction.

Existing watermark methods are either not applicable to embedding as services or lack transferability. In this paper, we propose Embedding Marker, a backdoor-based watermark method that addresses these issues.

Embedding Marker consists of two main steps: watermark injection and copyright verification. Before these steps, we select a trigger set, which is a group of words in a moderate frequency range. The provider can count the word frequency in a general text corpus to create this set.

In the watermark injection step, we define a target embedding. When a user sends a sentence to the provider service, the provider counts the number of triggers in the sentence. The provided embedding is a weighted sum of the target embedding and the original embedding, with the weight of the target embedding proportional to the number of triggers. If the number of triggers exceeds a threshold \( m \), the provided embedding is exactly equal to the target.

In the copyright verification step, we construct a backdoor and a benign dataset. The backdoor dataset contains sentences where all words belong to the trigger set, while the benign dataset contains sentences where none of the words belong to the trigger set. The provider requests embeddings from the stealer's service using the datasets. We compute the cosine and L2 similarity between the requested embeddings and the target embedding. We also use the KS test and its p-value as a third metric.

We conducted experiments on four datasets: AG News, MIND, SST2, and Enron Spam. Assuming the provider uses a wiki text dataset to count word frequency, our results show that Embedding Marker has excellent detection performance while maintaining the utility of downstream tasks. We also visualized the embeddings to demonstrate the covertness of the provided embeddings, showing that it is difficult to distinguish between backdoor and normal embeddings.

Thank you for your attention, and we welcome any discussion.</sample>
    <sample id="95">Il primo autore di PaLM non è specificato nel testo fornito. Il testo menziona che PaLM è stato presentato da Google Translate, ma non indica chi è stato il primo autore.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, una studentessa di dottorato di prima anno all'Università Carnegie Mellon e oggi farò una presentazione del vostro lavoro NLPositionality che caratterizza le bias di progettazione dei dati e dei modelli. Questo lavoro è stato fatto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santy, Ronan Le Bras, Katja Reinecke e Maarten Sap. Quindi, iniziamo immaginando di lavorare per un giornale e di filtrare i commenti sotto gli articoli che stai scrivendo per rimuovere contenuti toxici. Potresti rivolgerti a un'API popolare come la API Prospective per la rilevazione della tossicità, che funziona molto bene se sei Carl Jones. Ma questo non è davvero il caso per Aditya Sharma, dove la API Prospective non è così sensibile alle parole offensive che sono più comuni in contesti indiani. Questo è un esempio di un bias di progettazione dove vediamo differenze sistemiche di prestazioni della tecnologia tra popolazioni. I bias di progettazione come quello che abbiamo appena visto possono verificarsi a causa della posizionalità dei ricercatori e dei sviluppatori di modelli. La posizionalità è semplicemente le prospettive che le persone detengono a causa dei loro demografici, identità e esperienze della vita. Questo è un concetto ampiamente utilizzato nei studi critici, in particolare nei spazi accademici di feministi e queer. E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati e risultati perché può cambiare le decisioni che i ricercatori prendono. E quindi una domanda che le persone potrebbero chiedere è: i dati e i modelli hanno posizionalità? Non stiamo cercando di dire che i modelli nei dati stessi abbiano identità demografiche e vite esperienziali, ma rappresentano invece giudizi e opinioni di persone reali e possono quindi rappresentare certe posizionalità rispetto ad altre. Quindi, il lavoro precedente ha suggerito alcune prove anecdotali di avere posizionalità, come le lacune culturali nei modelli e nei dati, oltre a definizioni teoriche di posizionalità del modello. Tuttavia, questi lavori non guardano veramente alla comparazione tra gli utenti finali e i dati e i modelli stessi, e lo studio della posizionalità del modellino e dei dati è sempre più importante man mano che le compiti di NLP diventano più soggettivi e orientati socialmente, e è difficile caratterizzare come queste posizionalità sono distorcite perché non tutte le decisioni sono documentate e molte modelli sono nascosti dietro API. Quindi, per studiare</sample>
    <sample id="97">La relatrice menziona tre problemi associati a SimulST:

1. Specific architectures sono generalmente addestrati, introducendo moduli aggiuntivi da ottimizzare.
2. Long and complicated training procedures, per esempio, training involving different optimization objectives.
3. Training and maintaining several models to reach different latency regimes, come ad esempio, training a model con un tempo medio di un secondo e un altro modello con un tempo di due secondi, e così via.</sample>
    <sample id="98">Un modo efficace per mitigare i biases sociali e politici nei set di dadi durante l'addestramento dei modelli di NLP potrebbe essere l'implementazione di metodi di sanitazione dei dati. Questo potrebbe includere l'eliminazione o la riduzione dei dati che contengono opinioni politiche estreme o discriminatorie, o l'uso di tecniche di filtraggio per rimuovere dati che potrebbero contribuire a bias. Inoltre, è importante considerare l'uso di dati diversificati e rappresentativi per garantire che i modelli di NLP siano addestrati su un campione di dati che riflette la diversità della società. Infine, è fondamentale monitorare e valutare regolarmente i modelli di NLP per identificare e correggere eventuali bias sociali e politici.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dall'Università di Fudan. Sono qui per presentare il nostro lavoro "Distilling Script Knowledge from Large Language Models to Plan for Constrained Language". Nella vita quotidiana, gli esseri umani pianificano le loro azioni seguendo istruzioni passo-passo in forma di goal-oriented scripts. Lavori precedenti hanno sfruttato modelli linguistici per pianificare obiettivi astratti di attività tipiche come "fare un torto". Si è dimostrato che i modelli linguistici grandi possono decomporre efficacemente gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per gli obiettivi astratti di attività tipiche. Pianificare per obiettivi con vincoli specifici, come "fare un torto al cioccolato", rimane sottostudiato. In questo lavoro, definiamo il problema della pianificazione linguistica con vincoli che impone diversi vincoli agli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da obiettivi reali specifici con vincoli multifacetti. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo lavoro, valutiamo per la prima volta e miglioriamo la capacità di pianificazione con vincoli dei modelli linguistici grandi. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire questi obiettivi per prima. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifacetti per l'acquisizione dati con l'aiuto umano utilizzando InstructGPT. Abbiamo campionato 100 obiettivi specifici e valutiamo i script generati dai modelli linguistici grandi. Questa tabella riporta l'accuratezza complessiva dei risultati. Abbiamo trovato che tutti i modelli linguistici raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Poi abbiamo condotto un'analisi dettagliata per investigare perché i modelli di apprendimento falliscono. I risultati mostrano che la completitud semantica nei script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Abbiamo approfondito un argomento più fino a categorie di vincoli definiti in wikiHow. La mappa a calore mostra che le prestazioni di pianificazione di InstructGPT variano notevolmente per gli obiettivi di diverse categorie. Studi precedenti hanno mostrato che la qualità dell'output dei modelli linguistici cade in alta varianza, portando a performance negative. Pertanto, adottiamo l'idea di generare più poi filtrare per migliorare la qualità della generazione. Prima mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati su obiettivi astratti di semplice. Poi, InstructGPT genera K script per gli obiettivi specifici. Successivamente, sviluppiamo un modello filtratore per selezionare i script fedeli. Convertiamo i script e gli obiettivi in embedding di InstructGPT e calcoliamo le similarità cosinuso come punteggi di similarità. Inoltre, premiamo il script che contiene le parole chiave del vincolo di target. Manteniamo solo il script se il vincolo di target ottiene il punteggio più alto nella set di obiettivi. Con il nostro metodo, InstructGPT può generare script di alta qualità. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completitud semantica che fedeltà ai vincoli. Poiché i modelli linguistici grandi sono costosi da distribuire, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione del dataset è un passo essenziale per questo fine. Tuttavia, gli studi precedenti non permettono di pianificare per obiettivi specifici e l'annotazione manuale è costosa. Pertanto, seguiamo l'idea di distillazione del sapere simbolico per distillare dataset di pianificazione linguistica da modelli linguistici grandi. Appliciamo il nostro metodo per costruire un dataset di pianificazione linguistica con vincoli, chiamato CoScript. In totale, abbiamo generato 55.000 obiettivi specifici con script. Per garantire la qualità del dataset di</sample>
    <sample id="100">Multi-hop QA involves answering questions that require multiple reasoning steps, each corresponding to a document in a corpus. The approach, called PromptRank, is data-efficient, requiring only 128 examples for good performance. It combines unsupervised retrieval with a few-shot language model-based reranking. The process involves retrieving candidate chains using TF-IDF and hyperlink traversal, converting them to prompts, and scoring them using the likelihood of the question given the chain prompt. The chain prompt includes the documents and an instruction to elicit the language model's reasoning. PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art methods. It also shows strong few-shot path retrieval performance and elicits language models' reasoning abilities over the chain documents.</sample>
    <sample id="101">La fluidità di PaLM è buona, comparabile a quella dei sistemi di traduzione commerciali. Tuttavia, ci sono alcune discrepanze nell'accuratezza, come le omissioni di parti della frase di origine. In generale, PaLM offre traduzioni fluide ma con alcuni errori di precisione.</sample>
    <sample id="102">The important properties of a watermarking method are:

1. Applicability to embedding as services.
2. Non-degradation of the utility of the provided embeddings.
3. Covertness to the attacker, who can easily remove the watermark.
4. Transferability to the attacker's services during the model extraction.</sample>
    <sample id="103">I'm sorry, but the information about the 14 languages in which TED talks were translated is not provided in the text. The text only mentions that the analysis was performed on transcripts of TED talks that have been translated into 14 different languages, but it does not specify which languages they are.</sample>
    <sample id="104">In the study presented by Jenny and her team, they re-annotated data sets with diverse annotators to gather a rich set of demographic data. The exact number of instances re-annotated is not specified in the provided text. However, they mention that they amassed over 16,000 annotated instances from over 1000 annotators.</sample>
    <sample id="105">Le metriche di distanza utilizzate per misurare la differenziazione tra i set di dati benigni e backdoor sono il cosin similarity, L2 similarity, e il p-value del test KS (Kolmogorov-Smirnov).</sample>
    <sample id="106">The paper presents QUEST, a retrieval dataset designed to study the effectiveness of systems in handling selective information needs with implicit set constraints. The dataset includes over 3,000 entity-seeking queries from four domains: films, books, plants, and animals, where queries contain set operations. The authors construct QUEST by performing set operations over atomic categories and asking human annotators to paraphrase and validate queries for fluency and naturalness. The dataset also requires annotators to verify the relevance of entities and mark evidence in the document. The authors evaluate systems on QUEST using sparse and dense retrievers, as well as a T5-based reranking system. They find that there is a large room for improvement in retriever performance, particularly for queries with set intersection and set difference. The paper concludes that QUEST can help future researchers build improved systems for information-seeking scenarios with selective information needs.</sample>
    <sample id="107">I modelli basati su codificatori (encoder) multilingue sono stati utilizzati in questo lavoro per la traduzione di query in diverse lingue naturali in diverse rappresentazioni significative. In particolare, i modelli Encoder-PTR (Pointer-based Decoders) come XLM-R + PTR e mBERT + PTR sono stati valutati per la loro capacità di gestire la traduzione di query in diverse lingui, mentre i modelli Encoder-Decoder come mBART e mT5 sono stati utilizzati per la loro capacità di gestite le traduzioni in modo più generale. I risultati mostrano che i modelli Encoder-Decoder o Encoder-PTR possono essere migliorati con l'addestramento in un mix di diverse lingue, ma ci sono alcune limitazioni, come la "Curse of Multilinguality" che causa una riduzione delle prestazioni in alcuni dataset. Inoltre, il pre-training su lingue naturali inglesi può migliorare significativamente le prestazioni in Few-shot su lingue diverse.</sample>
    <sample id="108">Hello everyone, I'm Koustav Sinha, and welcome to our talk on our ACL 2023 paper. We explore the limitations of language model acceptability judgments in the context of minimal pair paradigms. Our work, in collaboration with John Gauthier, Aaron Mueller, and others, revisits these paradigms to evaluate models on longer sequences.

The traditional minimal pair paradigm involves presenting an acceptable or unacceptable sentence and evaluating the model's probability for the acceptable one. However, current models struggle with longer sequences. To address this, we recreate longer sequences by adding prefixes to sentences from datasets like BLiMP or SyntaxGym.

We test the models' acceptability judgments on longer sequences, including sentences from the same data set, different subsets, and unrelated domains like Wikipedia. Our findings show that the models' judgments are robust to context length for irrelevant sentences but significantly change when the context matches the sentence's structure.

We also found that the models are sensitive to latent syntactic and semantic features shared across sentences. This suggests that current minimal pair evaluations may not fully capture the models' abstract knowledge throughout the context window.

In summary, our work highlights the importance of considering context and latent features in evaluating language models' acceptability judgments. We invite you to read our paper for more details of our experiments and findings. Thank you for listening.</sample>
    <sample id="109">The paper "Unnatural Instructions: Tuning Language Models without Human Labor" introduces a dataset of natural language instructions and their corresponding input and output, collected in a fully automatic manner without any human annotations. The dataset is created by prompting a pre-trained language model, specifically a variant of GPT-3, with three examples from the Super-Natural Instructions dataset and asking the model to generate a fourth example. The dataset is further diversified by generating additional paraphrases of each instruction. The resulting dataset contains 640,000 examples, and if we include the instruction paraphrases, we have about 300,000 examples. The authors analyze the generated examples, focusing on creativity, accuracy, and diversity. They find that more than 50% of generated examples are correct, and even incorrect examples often contain valuable data for instruction tuning. The authors also fine-tune an 11 billion-parameter T6 model on Unnatural Instructions and show that the model can outperform both T0 and T5 across several benchmarks. The tested benchmarks were Super-Natural Instructions, T5, BIG-Bench Hard, and LMentry, and the authors show that training on Unnatural Instructions outperforms their baseline on all benchmarks. The authors conclude that Unnatural Instructions is a dataset of instructions for a wide variety</sample>
    <sample id="111">Gli autori decidono quali sono le parole a frequenza moderata selezionando un set di parole che appartengono a un intervallo di frequenza moderata. Questo set di parole viene utilizzato come "trigger set" per l'iniezione del marcatore e la verifica del diritto d'autore.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi sto presentando il nostro lavoro "Do CoNLL-2003 named-entity taggers still work well in 2003?" La nostra ricerca si è concentrata sul problema della generalizzazione utilizzando la Task di Riconoscimento di Entità Nomeata (NER). Abbiamo osservato che i modelli utilizzati nella CoNLL-2003 per lo sviluppo del NER sono stati utilizzati per quasi 20 anni, e questo solleva diverse domande. In primo luogo, questi modelli possono generalizzare a dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una performance peggiore, qual è la causa del calo delle prestazioni di questi modelli? Per investigare queste domande, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto dai Notiziari Reuters dal 2020 e poi annotato con le stesse linee guida di annotazione di CoNLL-2003. Abbiamo poi affinato oltre 20 modelli su CoNLL-2003 e li abbiamo valutati sui test set di CoNLL-03 e CoNLL++. Infine, abbiamo calcolato il cambiamento percentuale in F1 per valutare la generalizzazione di ciascun modello. Cos'è necessario per una buona generalizzazione, quindi? Durante gli esperimenti abbiamo trovato che ci sono tre ingredienti principali che sono necessari. Il primo è l'architettura del modello. Attraverso gli esperimenti abbiamo trovato di che i modelli transformer normalmente generalizzano meglio sui nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo trovato che, di solito, modelli più grandi portano a una migliore generalizzazione. E infine, sapremmo che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni di una task downstream. Inoltre, abbiamo trovato che, inoltre, più esempi di fine-tuning portano anche a una migliore generalizzazione. Per la domanda successiva, cosa causa la degradazione delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattativo, che è l'overfitting causato dall'uso ripetuto dello stesso test set e che si manifesta come la diminuzione dei risultati su un nuovo test set. La seconda ipotesi è il drift temporale, che è la degradazione delle prestazioni causata dall'aumento del tempo tra il train e il test. Per l'overfitting adattativo, abbiamo visto che dalla grafica a destra, la linea migliore rosso ha una gradiente maggiore di uno, il che significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 traduce in più di una unità di miglioramento su CoNLL++. Questo significa che non ci sono diminuzioni dei risultati. Questo ci mostra che l'overfitting adattativo in questo caso non è osservato. E cosa succede con il drift temporale? Per il drift temporale, abbiamo eseguito un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo trovato che la performance degrada con un gap temporale maggiore. Questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è il drift temporale. La nostra conclusione è che, per una buona generalizzazione, avremo bisogno di un migliore architettura del modello, un modello più grande e, soprattutto, più esempi di fine-tuning. Questo va di conseguenza, non possiamo avere solo un ingrediente e rifiutare gli altri. Allo stesso tempo, abbiamo trovato che il calo delle prestazioni qui è causato dal drift temporale e, piuttosto sorprendentemente, non è causato dall'overfitting adattativo anche se CoNLL-2003 è stato utilizzato per oltre 20 anni. Quindi tornando alla domanda che abbiamo sollevato nel titolo del nostro lavoro "Do CoN</sample>
    <sample id="114">Hi everyone, I'm going to introduce our work on ACL-2023 called "Finding the Pillar of Strength for Multi-Head Attention". We are from Nanyang Technological University of Singapore, and we are focusing on the heavy parameter problem of large language models. These models are game-changing, but they have several limitations, such as heavy parameters, long training time, and being token-hungry. We propose a grouped head attention which uses a divide and conquest strategy to compress multi-head attention. It contains a group-constrained training and a voting-to-stay algorithm. We evaluate our models on three tasks: machine translation, language modeling, and abstract summarization, and we achieve significant parameter compression and performance improvement. We also conduct further efficiency analysis and we believe that task-specific automatic pruning is a promising direction. Thank you.</sample>
    <sample id="115">L'approccio utilizza segmenti parlati di lunghezza lambda, dove lambda rappresenta il numero di frame audio che vengono considerati per determinare se emettere o meno una parola tradotta. La dimensione del segmento parlato è quindi variabile e dipende dal numero di frame audio che vengono analizzati per ogni parola tradotta.</sample>
    <sample id="116">In the example with Servin and Kea, the specific entity knowledge required to resolve the pronoun "he" is "Servin is a judge." This information is crucial for understanding that "he" refers to Servin, as it provides the context that judges are typically male and that Servin is a judge.</sample>
    <sample id="117">Il fattore più importante tra la qualité dell'esempio e la somiglianza con la frase sorgente è la qualità dell'esempio stesso. In particolare, la selezione di esempi di alta qualità, come quelli presenti nel dev data, porta a un miglioramento delle prestazioni del modello. La somiglianza con la frase sorigente ha un impatto meno significativo, soprattutto in caso di prompt con poche parole.</sample>
    <sample id="118">The presentation discusses a new approach to improving pretraining techniques for code-switched NLP, which involves the use of novel MLM techniques and architectural changes. The authors propose a method called SwitchMLM, which is tuned to the case of code-switching and includes a surrogate method called FrequencyMLM. They also propose some residual connections and an auxiliary LID-based loss to help with code-switching. The results show that their combined method performs the best on the sentiment analysis task for all language pairs. The authors also use probing classifiers to verify their claim that their methods increase the amount of switch-point information in intermediate and final layers. Overall, the presentation highlights the importance of building computational models for code-switching and presents a new approach to improving pretraining techniques for this task.</sample>
    <sample id="119">L'articolo si concentra su vari modelli linguistici, tra cui GPT-4, BART e RoBERTa, per esaminare la loro politica e le sue implicazioni sulle applicazioni NLP. Gli autori esplorano come i modelli linguistici possono adottare politiche diverse a causa dei dati di pretraining e come queste politiche influenzano le prestazioni su compiti come la rilevazione del discorso odioso e la rilevazione delle notizie false.</sample>
    <sample id="120">Il modello utilizza i punteggi (attention scores) di un livello specifico, chiamato cross-attention, per determinare se emettere o meno una parola tradotta. Questi punteggi sono calcolati tra l'input audio e l'output testo, e vengono utilizzati per decidere se una parola deve essere emessa in base alla stabilità dell'informazione ricevuta.</sample>
    <sample id="121">Esempi di inferenza diretta includono direttamente il nome della canzone "Easy on Me" o la sua posizione, "the first one".</sample>
    <sample id="122">The affiliations of the authors of the article "Distilling Script Knowledge from Large Language Models For Constrained Language Planning" are not provided in the text you shared. However, based on the information given, the author is Siyu Yuan from Fudan University in China. If you need more detailed affiliations or other authors, you may need to refer to the full paper or contact the authors directly.</sample>
    <sample id="123">In this presentation, Ying and Zhiyang discuss their research on MultiInstruct, a new benchmark dataset for multi-modal instruction tuning. They aim to improve the generalization of large language models to unseen multi-modal tasks by using instruction tuning. The dataset consists of 62 diverse tasks covering 10 broad categories, derived from 21 existing open-source datasets. They use the OFA model as a base and train it on 53 tasks, sampling 10,000 instances per test. They evaluate the model using different instructions and report performance metrics such as accuracy and Rouge-L. They also introduce a new metric called sensitivity, which measures the model's ability to produce consistent outputs for the same task. The results show that instruction tuning significantly improves the model's performance and reduces sensitivity. They also demonstrate the benefits of transfer learning from natural instruction datasets. Overall, they propose a new large-scale multi-modal instruction tuning dataset and explore different transfer learning techniques.</sample>
    <sample id="124">Hi everyone, I'm Tan Qingyu from the National University of Singapore. I'm excited to share our work, "Towards Benchmarking And Improving The Temporal Reasoning Capability Of Large Language Models." Time is a fundamental axis in the real world, and we break down temporal reasoning into three levels: time-to-time, time-to-event, and event-to-event reasoning. We found that prior works on temporal studies often overemphasize the second level, so we aim to study temporal reasoning in a more comprehensive manner by proposing the TempReason dataset, which covers all three reasoning levels and long temporal coverage. We evaluate temporal reasoning in three QA problem setups: closed book QA, open book QA, and reasoning QA. We also propose a training strategy with two components: temporal span extraction pre-training and time-sensitive reinforcement learning. Our final model, TempT5, improves the performance of T5-SFT significantly in open book QA and reasoning QA. We analyze and expose the temporal reasoning biases of LSMs and propose the TempReason benchmark dataset that covers all three reasoning types and comprehensive time periods. Finally, we propose a training paradigm to improve LLMs' temporal reasoning. Thank you for your time.</sample>
    <sample id="125">L'articolo "DrBERT: A Robust Pre-trained Model for Biomedical and Clinical Domains" è stato scritto da un solo autore, Yanis Labrak.</sample>
    <sample id="126">Yes, the translation of the query in natural language using an automatic translation model before semantic parsing has been considered a standard approach. In the presented work, the authors use Google Translate API to translate queries from one language to another before training and evaluating monolingual models. They also consider the monolingual model setting, where the source and target languages are the same, and the multilingual model setting, where a single model is trained on queries from multiple languages. Additionally, they evaluate cross-lingual zero-shot and few-shot transfer settings, where the model is trained on one language and transferred to another.</sample>
    <sample id="127">In this video, Namgyu Ho, a master's student from KAIST AI in Korea, presents their joint work with Laura Schmid and Professor Se-Young Yun on "Large Language Models Are Reasoning Teachers." The main issue addressed is the limitation of chain-of-thought reasoning, which requires large models like GPT-3 or PALM to solve complex tasks. To overcome this, the authors propose using these large models as reasoning teachers to fine-tune smaller models. They introduce a novel technique called diverse reasoning, which involves generating multiple reasoning samples using stochastic temperature sampling. This approach significantly improves the performance of small models on complex reasoning tasks, with notable improvements in text-based tasks. The authors also discuss the scalability of their method and the trade-offs involved in using diverse reasoning, dataset size, and teacher models. The video concludes with a call to action for viewers to explore their paper and code, and to reach out for discussions.</sample>
    <sample id="128">In this presentation, Akshatha and Martin introduce the KITMUS test, a diagnostic tool designed to evaluate the ability of natural language understanding models to integrate knowledge from multiple sources. The test focuses on coreference resolution, a task that requires models to identify the correct entity a pronoun refers to, using both entity-specific knowledge (e.g., "Servin is a judge") and background knowledge (e.g., "Judges decide cases in law courts"). The KITMUS test includes three settings: "Background-Pretrain," where background knowledge is available at pretraining; "Background-Both," where background knowledge is available both at pretraining and inference time; and "Background-Inference," where both knowledge types are available only at inference. The test evaluates the performance of coreference resolution models with and without task-specific training on KITMUS. Results show that models without task-specific training perform poorly, but with training, some models can integrate knowledge from multiple sources. However, even the best-performing models struggle with reliably integrating backward knowledge provided only at inference time. The KITMUS test highlights the importance of task-specific training for models to effectively integrate knowledge from different sources.</sample>
    <sample id="129">Gli autori hanno fornito come gruppo contrassegnato un "Asian woman" e un "Middle-Eastern woman" come esempi di gruppi contrassegnati.</sample>
    <sample id="130">Based on the content of the paper, the authors found that transformer models generally generalize better to new data compared to other architectures. Therefore, non-generalizing architectures are likely to be those that do not leverage the transformer architecture or are not designed to handle the complexities of modern data.</sample>
    <sample id="131">I nomi dei set di dati di test non sono specificati nel contenuto fornito. Tuttavia, si discute l'importanza di avere un set di dati di validazione pulito per il Weakly Supervised Learning (WSL) e come il numero di campioni puliti può influire sulle prestazioni dei modelli.</sample>
    <sample id="132">There are two authors involved in the article: Akshatha and Martin.</sample>
    <sample id="133">L'autore opera con più modalità, specificando che il loro lavoro si concentra su un'analisi multi-modal, utilizzando un modello pre-addestrato multi-modal OFA e includendo vari tipi di input e output come testo, immagini e bounding box.</sample>
    <sample id="135">James Finch and Sarah Finch introduce ABC-Eval, a new method for evaluating conversational AI. They explain that current methods, such as human evaluation, are subjective and do not cover all aspects of dialogue quality. ABC-Eval aims to reduce subjectivity by explicitly annotating chat model behaviors, such as responding with irrelevant information or contradictions. The method measures rates of various thematic errors, including ignoring a partner, contradicting oneself or others, hallucinating facts, and failing to show empathy. The authors evaluated four state-of-the-art chat models using ABC-Eval and compared the results to existing methods. They found that ABC-Eval behavior labels are more reliable and predictive of conversation quality than existing methods. The authors also identified common errors in chat models, such as common sense violations, irrelevant information, and contradictions. They conclude that ABC-Eval provides a higher resolution evaluation of conversational AI and hope it can be used by others in the field.</sample>
    <sample id="136">Jasivan and Nafise from the University of Sheffield presented their work on "FERMAT: An Alternative to Accuracy for Numeral Reasoning." The motivation behind this work is the need for factual correctness in real-world applications of numerical reasoning, such as fact-checking. They found that current benchmarks, which use accuracy scores, are not informative about the strengths and weaknesses of models in mathematical ability. To address this, they introduced FERMAT, a flexible evaluation set based on arithmetic types, including number understanding, mathematical operations, and training dependency. They found that most models perform poorly across these aspects, and that fine-tuning with math teachers' templates and diverse training data improves performance. They also found that language and mathematical diversity is important, and that number encoding and tokenization are areas of future improvement. The conclusions of their work are that existing benchmarks are unrepresentative, and that FERMAT provides a more informative alternative.</sample>
    <sample id="137">Sicong from the Singapore University of Technology and Design presents their work "Tell2Design: A Dataset for Language-guided Floor Plan Generation," published in ACL 2023. The research focuses on developing a machine-learning task where models generate floor plan designs directly from language instructions. This task involves understanding the semantics, geometry, and topology of floor plans from natural language descriptions. The dataset, Tell2Design, contains 5,051 human-annotiated language instructions and 76,000 artificially generated instructions, with an average of over 200 words per instruction. The main challenges include generating designs under strict constraints, understanding the entire floor plan from unstructured text, and handling ambiguous or incomplete instructions. The proposed solution treats the task as a sequence-to-sequence problem using a transformer-based encoder-decoder model, initialized with a pre-trained language model like T5. The model achieves high IoU scores, outperforming text-conditional image generation baselines. The research highlights the need for a sequence-to-sequence approach for language-guided design generation and introduces the Tell2Design dataset as a foundation for future research in this area.</sample>
    <sample id="138">According to the authors, the area of NLU that is poco studiata is the integration of knowledge from different sources, particularly when the background knowledge necessary to solve a task is only available at inference time.</sample>
    <sample id="139">I nomi dei relatori sono Ying e Zhiyang.</sample>
    <sample id="140">Yes, CoScript was subjected to quality control. Crowd-sourced workers were asked to find and revise incorrect samples to ensure the quality of the validation and test set.</sample>
    <sample id="141">The existing resources for context-dependent translation are limited in several ways:

1. **Limited Types of Context-Dependent Translations**: These resources typically focus on specific types of context-dependent translations, such as pronoun resolution or verb form selection, and may not cover a wide range of discourse phenomena.

2. **Limited Sets of Languages**: Many context-dependent translation resources rely on domain knowledge and human curation, which can limit their applicability to a smaller set of languages.

3. **Human-Curated Data**: The reliance on human-curated data means that these resources may not be scalable or comprehensive, and they may not capture the full range of context-dependent translations that occur in natural language.

4. **Domain-Specific Knowledge**: Some resources require domain-specific knowledge to identify context-dependent translations, which can limit their applicability to general-purpose translation systems.

Overall, these limitations highlight the need for more comprehensive and scalable resources for context-dependent translation, such as the MuDA benchmark developed in the work presented by Kayo Yin and colleagues.</sample>
    <sample id="142">Ciao! Voglio parlare del nostro lavoro su "Risoluzione di espressioni di riferimento indiretto per la selezione di entità", in cui presentiamo il nostro corpus AltEntities. Il mio nome è Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia ParetI e Annie Louis. Il nostro obiettivo è capire il linguaggio degli utenti quando vogliono fare una scelta. Consideriamo questa domanda alternativa: "Hai intenzione di scegliere 'Easy on Me' o 'I Gotta Feeling'?" In questo caso, un utente vuole scegliere tra queste due canzoni. La cosa più ovvia è utilizzare una riferimento diretto, ad esempio dando il nome della canzone "Easy on Me" o la sua posizione, "la prima". Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome della canzone. O le pronunce sono troppo simili tra loro e difficili da disambiguare. O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio, "il più recente" o "la canzone che non è energetica." Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking dei modelli linguistici per la comprensione delle entità. Non siamo consapevoli di un dataset pubblico più grande per questa attività, quindi abbiamo raccolto uno usando l'annotazione di crowd. Il nostro dataset copre tre diversi domini: musica, libri e ricette. Il nostro metodo di raccolta dei dati enfatizza l'informalità utilizzando un setup di completamento di cartone. Il cartone ha tre bocche di dialogo. Nella prima bocca, Bob dice: "Ricorda quella canzone che stavamo ascoltando ieri?" E con quello, Bob imposta il contesto del dialogo. Nella seconda bocca di dialogo, Alice dice: "Hai intenzione di sare 'Easy on Me' o 'I Gonna Feeling'?" Che è la domanda alternativa. E nella terza bocca di dialogo, Bob usa un riferimento indiretto per selezionare uno di questi entità, ad esempio, "il più nuovo". Forniamo automaticamente la prima e la seconda bocca di dialogo, ma la terza è riempita dall'annotatore. La prima bocca è scelta da alcuni prompt manuali per ogni dominio. La seconda, che è la domanda alternativa, è generata come segue. Sempre utilizziamo un modello semplice di template. Hai intenzione di A o B? Dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionatura che abbiamo utilizzato. Quando andiamo più in alto nella lista, le entità diventano più simili tra loro e è generalmente più difficile fare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno informazioni simili o attributi su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, conoscono il nome di queste entità, ma non necessariamente conoscono di esse. Quindi ciò che facciamo è mostrare alcune informazioni di background su entrambe le entità. Per le canzoni, semplicemente mostriamo un link di ricerca Google per ciascuna canzone e chiediamo agli annotatori di ascoltare almeno alcuna di ciascuna canzone e leggere su ciascuna canzone. Ecco come è il risultato di una ricerca di Google per la canzone "Easy on Me". Per i domini delle ricette e dei libri, mostriamo alcune testo di</sample>
    <sample id="143">The approach is compared with existing SimulST policies such as the Wait-k strategy and the Local Agreement, as well as the state-of-the-art architecture specifically tailored for Simultaneous Pre-Translation.</sample>
    <sample id="144">I'm sorry, but the information about the affiliations of the authors of the article is not provided in the text you shared.</sample>
    <sample id="145">Il nome della relatrice o del relatore è Jenny.</sample>
    <sample id="146">Title: Analyzing Omission in Dialogue Summarization: A Systematic Study and Dataset Construction

Abstract:
Dialogue summarization, a subtask of text summarization, aims to create concise summaries that capture the most important information within a dialogue. Despite significant progress using large-scale pretrained language models, these summaries often contain factual errors, with omission being a major factor. This study systematically analyzes the omission problem in dialogue summarization, revealing that even state-of-the-art models achieve a high omission rate of about 70%. We construct the OLDS dataset, providing high-quality omission labels for dialogue summarization across five domains. The dataset is built upon five existing benchmarks and includes diverse candidate summaries generated by different models and decoding strategies. We evaluate omission detection models using Precision, Recall, and F1-score, and measure word-level omission recall (WR score). Our results show that the task is challenging, with an F1-score around 50%. We also explore a post-editing method for summary refinement using detected omissions, which significantly boosts summary quality. This study provides a foundation for omission detection and highlights the potential of refinement based on detected omissions for improving dialogue summarization quality.</sample>
    <sample id="147">L'articolo "Marked Personas: Using Natural Language Pronips to Measure Stereotypes in Language Models" è stato scritto da tre autori: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao, sono Sara Papi dall'Università di Trento e dalla Foundazione Bruno Kessler e vi presento il lavoro "Attention as a Guide for Simultaneous Speech translation", un progetto condiviso con Matteo Negri e Marco Turchi.

Cosa è la traduzione simultanea della parola? La traduzione simultanea, o SimulST, è il processo di tradurre una lingua parlata in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse.

Quali sono i problemi delle attuali modelli di SimulST? Le architetture specifiche sono generalmente addestrate, introducendo moduli aggiuntivi da ottimizzare. Le procedure di addestramento lunghe e complicate, ad esempio, l'addestramento coinvolgente obiettivi di ottimizzazione diversi. E l'addestramento e la manutenzione di più modelli per raggiungere diverse regimi di latenza. Ad esempio, l'addestramento di un modello con un tempo medio di un secondo e un altro modello con un tempo di due secondi, e così via.

E cosa è la nostra soluzione? Prima, utilizziamo modelli di traduzione offline già addestrati senza riaddestrarli o adottare architetture specifiche per SimulST. Utilizziamo solo un modello per ogni regime di latenza e gestiamo la latenza attraverso parametri specifici. E sfruttiamo il sapere già acquisito dal modello attraverso meccanismi di attenzione tra l'input audio e l'output testuale. Quello è il meccanismo di attenzione cross, e puoi vedere un esempio a destra.

La nostra soluzione è proporre EDAtt, o Attention Encoder-Decoder, una strategia per decidere se emettere o meno una traduzione parziale, basata sulle posizioni dell'attenzione. Una parola viene emessa se l'attenzione non è concentrata, cioè il suo somma è sotto un certo soglia alpha verso gli ultimi lambda frame di parlata, significando che l'informazione ricevuta è abbastanza stabile. Ad esempio, se riceviamo un frammento di parlata che contiene "Vado a parlare di..." e il nostro modello predice la traduzione in tedesco, e guardiamo i pesi di attenzione cross, vedremo che i primi due parole puntano ai primi frame di parlata ricevuti, mentre l'ultima parola punta ai frame di parlata più recenti, come lambda frame di parlata. Questo significa che le prime due parole verranno emesse, mentre, poiché la somma dei pesi di attenzione è sopra una certa soglia alpha, non emetteremo l'ultima parola e attenderemo un altro frammento di parlata. Se continuiamo e riceviamo un altro frammento di parlata, e il nostro modello predice altri tre parole e guardiamo i pesi di attenzionecross, vedremo che nessuna parola punta ai frame di parlata piú recenti. Questo significa che queste tre parole verranno emesse. Se guardiamo la principale risultato di EDAtt, plotteremo i risultati di traduzione simultanea su grafici in cui abbiamo BLEU su un lato che misura la qualità della traduzione, e il tempo medio di ritardo, e consideriamo anche il tempo medio computazionale consapevole del ritardo, che tiene conto del tempo computazionale del modello per predire l'output. Quindi vogliamo che le nostre curve siano il più alte possibile su questo grafico. Ma anche che siano spostate a sinistra. E confrontiamo con strategie popolari che vengono applicate anche ai modelli offline, come la strategia Wait-k e la Local Agreement. E confrontiamo anche con l'architettura più avanzata specificamente progettata per la traduzione pre-translation simultanea. Questi sono tutti i risultati della strategia di traduzione simultanea sul tedesco. E vediamo che supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate a sinistra. E anche vediamo che, se consideriamo il tempo effettivo o il tempo computazionale consapevole, cioè la strategia più veloce. Se vuoi scoprire altri risultati, leggi il nostro lavoro. E anche abbiamo rilasciato il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per la vostra attenzione.</sample>
    <sample id="149">Yes, the CoNLL++ dataset is available publicly.</sample>
    <sample id="150">Hello everyone, I'm Archiki, presenting our ACL paper "MEETINGQA". We introduce a new dataset, MeetingQA, for extractive question answering on meeting transcripts. Meeting transcripts are long, domain-specific documents that are underutilized in NLP research. We collect data from public meeting transcripts, annotate answers, and create a dataset with 7.7K questions, 30% of which are unanswerable. We achieve high human performance on the test set with an F1 of 84%. We employ various methods, including context-retrieval, single-span, and multi-span models, and silver data augmentation. Our results show a significant gap between fine-tuned models and human performance, and zero-shot performance is significantly lower. Error analysis reveals challenges in identifying rhetorical questions, irrelevant sentences, and speaker identification. MeetingQA is a challenging dataset for existing QA models, and we hope our work will inspire further research in this area. Thank you for listening, and more details can be found on our project page or in our paper.</sample>
    <sample id="151">Ciao a tutti, il mio nome è Ying e il mio colleagio Zhiyang e noi ci propongiamo di presentare la nostra ricerca su MultiInstruct migliorando il Zero-Shot Learning Multimodale attraverso l'addestramento con istruzioni. Con i progressi delle grandi modelli linguistici, molte lavorazioni hanno iniziato a esplorare nuovi paradigma di riutilizzo dei modelli pre-addestrati per le lingue per diverse attività downstream in modo parametro e dati efficiente. Negli ultimi tempi, molte studie hanno mostrato che l'addestramento con istruzione consente ai modelli linguistici grandi di eseguire su compiti non visti in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull'addestramento con istruzioni si sono concentrate su migliorare le prestazioni zero-shot su compiti linguistici, mentre le attività computer vision e multimodali sono stati lasciati fuori. Pertanto, in questo lavoro vogliamo investigare se l'addestramento con istruzionamento di modelli pre-addestrati multimodali può migliorare effettivamente la generalizzazione a compiti non visti multimodali. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di dataset di istruzioni tra NLP e multimodali. Esistono più di 1600 compiti di istruzione linguistica, tuttavia non esiste un dataset di istruzione multimodale di grande scala pubblicamente disponibile. Questo motiva noi a costruire un dataset di istruzione multimodale. Presentiamo MultiInstruct, il primo dataset di istruzione multimodale di benchmark che consiste in 62 compiti multimodali diversi coperti da 10 categorie broad. Questi compiti sono derivati da 21 dataset open-source e ogni compito è dotato di cinque istruzioni scritte da esperti. Per esaminare l'addestramento con istruzionali sui nostri dataset proposti, utilizziamo OFA, un modello pre-addestrato multimodale unificato, come modello di base. OFA utilizza un vocabolario unificato per il linguaggio, i token immagine e le coordinate di un bounding box. Ecco alcuni esempi di istanze dal nostro dataset MultiInstruct, per unificare il processamento di vari tipi di dati di input e output. Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza-a-sequenza unificato. In cui l'input testo, immagini, istruzioni e bounding box sono rappresentati nello stesso spazio token. Ok, ora parlerò di l'addestramento con istruziona. Per il dataset di addestramento, utilizziamo 53 compiti da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento comune per il test e selezioniamo altri 5 compiti dai gruppi VQ e Miscellaneous. Utilizziamo tutte le istanze nel test split per ogni compito. Inoltre, selezioniamo casualmente 20 compiti dal test split di istruzioni naturali come compito non visto per NLP. Quindi utilizziamo il modello pre-addestrato OFA grande come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene casualmente combinata con una delle cinque template di istruzione. Quindi durante il test per ogni compito, effettuiamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ogni esperimento, riportiamo il min e max delle prestazioni e la deviazione standard delle prestazioni across tutti i 5 esperimenti. Se il compito è una classificazione multimodale, riportiamo l'accuratezza. Se è un compito di generazione multimodale, riportiamo Rouge-L. Per i compiti NLP, riportiamo Rouge-L anche. Abbiamo anche introdotto un nuovo metrica chiamata sensibilità. Questa</sample>
    <sample id="152">Hello everyone, I'm Frederick Riemenschneider, and I'm here to discuss our work at the intersection of NLP and classical philology. We're exploring large language models for Ancient Greek and Latin, and the implications and challenges of multilinguality in models.

Currently, there are several models for Latin and Ancient Greek, but they are all monolingual and based on the BERT architecture. We've developed new models, GreBERTa and GreTa, which are monolingual RoBERTa and T5 models for Ancient Greek, respectively. We've also created PhilBERTa and PhilTa, which are multilingual models pre-trained on Ancient Greek, Latin, and</sample>
    <sample id="153">Ninareh Mehrabi, a postdoctoral scientist at Amazon Alexa AI's responsible AI team, presents her work on resolving ambiguities in text-to-image generative models. The research focuses on studying existing ambiguities in prompts provided to text-image models and proposes frameworks to mitigate such ambiguities. The team curates a benchmark dataset that covers different types of ambiguites and uses a prompt disambiguation framework to gather external signals to disambiguate the prompts. The framework uses in-context learning to generate clarifying questions or different possible visual interpretations, which are then concatenated to the original ambiguous prompt. The team also proposes an automatic evaluation framework using a VQA model to evaluate whether the generated images are faithful to the user's intention. The findings show that disambiguation using the framework has a positive effect on faithful generation and that the automatic evaluation framework is in agreement with human evaluation. The research aims to improve the accuracy and reliability of text-to-image generative models.</sample>
    <sample id="154">The authors of the article "Attention as a Guide for Simultaneous Speech Translations" are Sara Papi from the University of Trento, Foundazione Bruno Kessler, Matteo Negri, and Marco Turchi.</sample>
    <sample id="155">Il nome della relatrice o del relatatore non è specificato nel contenuto fornito.</sample>
    <sample id="157">The presentation introduces a novel approach to dialogue summarization called "Dialogue Summarization with Static-Dynamics Structure Fusion Graph" (SDDS). This method addresses the limitations of existing dialogue summarization techniques that rely on pre-computed static graphs and external linguistic tools, which can be unreliable and inflexible.

The SDDS model consists of four main components:

1. Utterance Encoder: Encodes each utterance in the dialogue context into a vector representation.
2. Static Graph Construction: Uses existing data structure modeling methods to build a static graph based on the encoded utterances.
3. Static-Dynamic Graph Module: Combines the static graph with a dynamic graph that captures the semantic relationships between utterances based on their deep vector representation.
4. Summary Generator: Uses a pre-trained language model to fuse the static and dynamic graph representations into the final summary.

The static graph is built using heuristic methods such as Discourse Parsing Graph, Key Co-occurrence, and Speaker Interaction Frequency Matrix. The dynamic graph is built using a multi-head attention model. The two graphs are fused using a 1x1 convolutional layer and a dual cross-attention mechanism.

The SDDS model has been released on GitHub, and the code and data are available for download.</sample>
    <sample id="158">Sure, here's a summary of the content in about 200 words:

Coreference resolution is the task of identifying and clustering mentions of the same entity in a document. Conventional methods have quadratic complexity, while cache-based methods reduce complexity to linear levels. However, in long documents, the LRU eviction policy used in cache-based methods can lead to high cache misses due to the global nature of high-frequency entities. To address this, we propose a dual cache system with a local cache (using LRU) and a global cache (using LFU). The local cache stores recently used entities, while the global cache stores frequently used entities. When the cache is full, the eviction policy is triggered.

We evaluated the dual cache on four public benchmarks, including LitBank, OntoNotes, and WikiCoref. With training data, dual cache outperforms baselines even with unbounded memory. Without training data, the model with unbounded memory performs better, but dual cache is still faster. We also annotated a book with 30,020 words and found that dual cache significantly reduces cache misses and outperforms single cache methods.

In conclusion, dual cache uses a local and global cache system to separately store local and global entities, outperforming single cache methods and reducing cache misses. It is also the most cost-effective compared to single cache methods.</sample>
    <sample id="159">Ciao a tutti, sono Koustav Sinha e sono felice di accogliervi alla nostra presentazione della nostra ricerca ACL 2023. Le giudizi di accettabilità del linguaggio naturale non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kishka Misra, Karen Fences, Roger Lévy e Adina Williams. In questo lavoro, rivediamo i paradigma minimali. Il paradigma minimale valuta le modelli linguistici su base di giudizi di accettabilità, che possono includere anche grammaticità come BLiMP, SyntaxGym o accettabilità in termini di stereotipi come CrowS pairs. Nel paradigma minimali, la valutazione tipica dei modelli linguistici consiste nel mostrare una frase accettabile o grammaticalmente corretta e poi una frase accettabile o grammaticale. L'obiettivo è che il modello assegni più probabilità alla frase accettabile. Attualmente, il pipeline MPP non permette di valutare l'accettabilità di modelli verso frasi più lunghe. Oggi, i modelli linguistici linguistici stanno arrivando a lunghezza di contesto sempre crescente. È cruciale che valutiamo l'accettabilità dei modelli lungo l'intero contesto di finestra, e questo è ciò che stiamo cercando di fare qui. Stiamo rivedendo il pipeline MPP rivedendo a chiedere al modello di valutare l'accettabilezza su frasi più lunghe e più lunghe. Così, ciò che facciamo è che, per simulare queste lunghe frasi, rivediamo i dati stessi e creiamo frasi selezionando frasi accettabili o non accettabili da quei dati. Ad esempio, qui abbiamo scelto una tipica coppia grammaticalità dal dataset BLiMP dal caso dell'islanda aggiuntiva. Ecco cosa facciamo: estraiamo frasi grammaticali dal dataset di aggiuntiva e aggiungiamo come prefisso sia alla frase accettabile che alla frase non accettabile. Così possiamo fare lo stesso per scegliere frasi non accettabili dallo stesso matching e questo potrebbe essere utilizzato per testare l'accettabilità del modello. E anche per scegliere frasi da un diverso sottoinsieme o dataset diverso. Così si chiama la scenario di incompatibilità. Qui le frasi sono ancora provenienti da un dataset rilevante, ma non dal dataset che stiamo valutando. E possiamo fare lo stesso per il caso non accettabile. Infine, possiamo scegliere frasi da un dominio completamente irrilevante come Wikipedia. Questo ci dice se le giudizi di accettabilità dei modelli sono effettivamente influenzati da contesto, come se il contesto provenga da un sottoinsieme diverso del dataset o se è completamente irrilevante rispetto alla frase che stiamo considerando. Come funziona il modello? In primo luogo, guardiamo le frasi di Wikipedia, che sono completamente irrilevanti alla frase corrente coppia, e qui troviamo che le giudizi MPP sono relativamente stabili per lunghezza di contesto fino a 1024 per massimizzare OPT e GPT 2 modelli. E qui vediamo che la linea a punti tratteggianti, le giudizi MPP sono relativamentemente stabili. Ora, cosa succede quando scegliamo frasi dallo stesso dataset? Qui stiamo creando frasi accettabili e non accettabili da domini dal dataset BLiMP o SyntaxGym. E qui vediamo che le giudizi MPP aumentano o diminuiscono significativamente quando aggiungiamo sia prefissi accettabili che non accettabili.</sample>
    <sample id="160">Il primo passaggio del metodo mappa ogni token di input con un insieme non ordinato di token che appaieranno nel risultato.</sample>
    <sample id="161">In Coscript, there are a total of 55,000 specific goals represented with scripts.</sample>
    <sample id="163">Il metodo di allineamento migliore proposto per DEplain è il metodo di MASSalign, come descritto nel paper. Questo metodo è stato testato e risultato essere il migliore per l'allineamento automatico tra le frasi di due documenti paralleli con la stessa lingua e contenuto, ma a livelli di complessità diversi.</sample>
    <sample id="164">Il vantaggio dell'apprendimento scarsamento supervisionato (WSL) risiede nella possibilità di utilizzare dati etichettati con metodi meno costosi e meno accurate rispetto alle annotazioni complete. Questo permette di ridurre significativamente i costi di annotazione, rendendo il processo di raccolta e lavoro con dati più pratico e accessibile. Tuttavia, è importante notare che l'apprendimento scarsamento supervisionato richiede metodi specifici per gestire il rumore nelle annotazioni e per garantire che i modelli imparziati generalizzino bene, evitando di memorizzare e propagare gli errori nelle annotazioni.</sample>
    <sample id="165">Hello everyone, I'm Wenting Zhao, a PhD student at Cornell University, and I'm excited to present our recent paper titled "Abductive Reasoning Exploiting Mutually Exclusive Explanatory."

To illustrate abductive reasoning, consider the context "Emily was stuck in traffic," and the outcome "Emily made it to her flight." We have two possible explanations: "Her flight was delayed" and "Her flight left on time." The goal is to find a plausible explanation that bridges the gap between the context and the outcome. In this case, "Her flight was delayed" is plausible.

Formally, our paper addresses abductive reasoning in a closed-world setting, where we aim to identify a plausible subset of explanations from a candidate set. Current approaches rely on supervised methods, which require annotated plausible explanations, often leading to noisy and subjective results. A recent experiment showed that crowd workers disagree on 60% over 1,000 explanations.

We propose an unsupervised learning method called LiPoR (Likelihood Learning with Posterior Regularization). In LiPoR, we treat explanations as latent variables and maximize the marginal likelihood of the outcome given the context. However, this alone does not prefer plausible explanations. To address this, we introduce a regularizer based on the mutual exclusivity of explanations.

The LiPoR objective consists of two parts: likelihood maximization and preference regularization. The regularizer, Omega, takes the maximum between the entropy of the explanation distribution and the log of the number of plausible explanations. When the entropy is higher than the log of the number of plausible explanations, we prefer a subset of explanations.

Our results on AlphaNLI, the most widely-used dataset for abductive reasoning, show that LiPoR outperforms zero-shot models and the previous best unsupervised approach by over 4 absolute points in accuracy.

Thank you for listening. Our paper can be found here: tinyurl.com/zhao-lipor.</sample>
    <sample id="166">The abstract introduces a novel neural divide-and-conquer reasoning framework for image retrieval from linguistically complex text. This task is challenging due to the high similarity of images and lengthy descriptions. Traditional visual language models excel in image-sentence retrieval but struggle with complex reasoning tasks. Inspired by the Divide-and-Conquer strategy and Dual Process Theory, the proposed method decomposes complex propositions into simpler ones using a Proposition Generator. The Visual-Linguistic Interactor (System 1) performs visual-proposition interaction, while the Neural-Symbolic Reasoner (System 2) integrates reasoning states for final solutions. The system combines System 1 and System 2 to leverage analogical and logical reasoning. Experimental results show that the proposed method outperforms baselines, with ablation studies validating module effectiveness. Two cases demonstrate the method's ability to present inference states and results, indicating interoperable processing. The abstract concludes with suggestions for integrating neural symbolic calculation and the Divide-and-Conquer strategy for improved compositional reasoning in large language models.</sample>
    <sample id="167">Yes, the documents in DEPLAIN-web were aligned using both manual and automatic alignment methods. Specifically, 750 documents were aligned manually, and the remaining documents were aligned using automatic methods. This resulted in a total of 30,450 parallel sentence pairs.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccogliendo notizie del gruppo Reuters dal 2020 e annotandole secondo le linee guida di annotazione del CoNLL-2003. Questo set di dati è stato utilizzato per valutare la generalizzazione delle modelli addestrati su CoNLL-2003 quando applicati a dati più recenti.</sample>
    <sample id="169">David Vilar presents a review of the paper "Prompting PaLM For Translation: Assessing Strategies And Performance," a joint work with colleagues from Google Translate. PaLM, a 540 billion-parameter language model, was introduced in 2022 and has achieved state-of-the-art results in numerous NLP tasks. The study systematically evaluates large language model prompting for machine translation, using the best practices of the MT community and comparing to state-of-the-art systems. The authors use neural MT metrics and expert-based human evaluation, providing recommendations for prompt selection strategies.

The study found that the form of the prompting has a significant impact on the performance of large language models for translation, especially in zero and one-shot prompting. The quality of examples is more important than the similarity to the source language. The authors used a 5-shot prompting strategy, marking each sentence with its language, and found that the actual form of the prompting had little effect.

The results showed that PaLM's translations are comparable to state-of-the-art systems in terms of fluency, but with some accuracy issues, particularly omission errors. The "Style/Awkward" category for PaLIM is lower than for state-of-the-art systems, indicating that PaLM provides fluent output but still has some problems with accuracy.

In summary, the study provides valuable insights into the use of large language models for machine translation and highlights the importance of prompt selection strategies and example quality. The authors conclude that PaLM provides a promising approach for machine translation, but further research is needed to address its accuracy issues.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang dall'Università di Penn State. Oggi presento il nostro lavoro "XSemPLR: Cross-Lingua Semantic Parsing in Multiple Natural Languages and Meaning Representation". La semantic parsing è una task per costruire rappresentazioni semantiche di query utente, come SQL e Lambda Calculus. Cross-Lingual Semantic Parsing è la task per tradurre le query in più lingue naturali in più rappresentazioni semantiche. Come mostrato in questa figura, dobbiamo tradurre le query in più lingue naturale utilizzando modelli neurali in SQL, Lambda o FunQL, ecc. Gli esistenti modelli di semantic parsing cross-lingua sono stati separatamente proposti e valutati su dataset di task e applicazioni limitati. Ad esempio, c'è molta copertura su certe lingue naturali, ma la Cinese è mancante e mancano di copertura su certe rappresentazioni semantiche. La Lambda calculus è mancante, o sono stati valutati solo su certi modelli neurali. Ad esempio, esiste solo un modello singolo per valutarli. Per questo motivo, abbiamo proposto XSemPLR. Forniamo un dataset uniforme XSemPLR per la semantic parsing cross-lingua in più lingue naturali e rappresentazioni semantiche. Contiene 9 dataset in vari domini, 5 task di semantic parsing, 8 rappresentazioni semantiche e 22 lingue naturali in 15 famiglie linguistiche. Per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e l'evaluazione. La prima è Translate-Test. Utilizziamo l'API di Google Translate per tradurre la fonte in lingua target, poi utilizziamo un modello monolingue per addestrare e valutare. Ad esempio, addestriamo il modello inglese su query inglesi e durante l'inferenza tradurremo le query tedesche utilizzando l'API in inglese e poi utilizzeremo il modello addestrato per predire il SQL. E anche testiamo il Modello Monolingue. In questo setting, la lingua sorgente è la stessa della lingua target, ad esempio tedesco a tedesco o inglese a inglese. Testiamo anche il Modello Monolingue Few-shot, addestrando modelli monolingui con solo il 10% dei dati di addestramento. E testiamo il Modello Multilingue, che addestra un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingue per predire l'output SQL. Durante l'inferenza possiamo utilizzare questo modello per tradurre le query tedesche o cinesi, ecc. E anche consideriamo il transfer zero-shot e few-shot cross-lingua. Addestriamo su una lingua sorgente e trasferiamo ad un'altra lingua. Quindi durante l'addestramento, addestriamo su query inglesi o la combinazione di query inglesi e tedesche Few-shot per addestrare un modello multilingua per predire l'output SQL. E troviamo anche molti risultati interessanti. Riguardo all'analisi dei modelli monolingui, valutiamo su due gruppi di modelli, inclusi Encoder-PTR, che sta per Multilingual Pretrained Encoders con Decoder puntati, come XLM-R + PTR e mBERT + PTR. E anche modelli Encoder-Decoder, che sono Modelli Encoder-Decoder Multilingue, come mBART e mT5. Abbiamo trovato che Encoder-Decoder ottiene il miglior prestigio su tutti i nove dataset. E valutiamo su mT5 e XLM-R + PTR in setting multilingue. Abbiamo trovato che Encoder-Dedoder o Encoder-PTR possono essere migliorati addestrando in una miscela di diverse lingue. Abbiamo trovato che è perché la maggior parte delle lingue naturali può ottenere un guadagno di prestigio, tranne che l'inglese perdere prestigio in sette dataset e ottenere solo guadagni in tre dataset. Credo che questo sia noto come la "Curse of Multilinguality". Abbiamo anche confrontato la differenza di performance</sample>
    <sample id="171">The paper you're referring to discusses a method called "Embedding Marker," which is a backdoor-based watermark method designed to protect the copyright of embedding as services. The method is applicable to embedding as services and aims to detect whether another service contains the watermark. The watermark method needs to meet the following properties: it should be applicable to embedding as services, the watermark should not degrade the utility of provided embeddings, the watermark should be covert enough to the attackers or the attackers can remove the watermark easily, and the watermark needs to be transferable to the attacker’s services during the model extraction process.

Existing works can be broadly classified into four categories, but they either are not applicable to embedding as services or lack transferability. Therefore, the authors propose Embedding Marker, which is a backdoor-based watermark method applicable to embedding as services. The method contains two main steps: watermark injection and copyright verification. The watermark injection step involves injecting a watermark into the embedding, while the copyright verification step involves detecting whether a model behind another service contains the watermark.

The paper also conducts experiments on four data sets (AG News, MIND, SST2, and Enron Spam) to validate the performance of the proposed method. The results show that the proposed method can have great detection performance while keeping great utility for downstream tasks. Additionally, the authors validate the covertness of the provided embedding by visually comparing the embedding of sentences on four datasets using PCA. The results show that it is hard to distinguish between the backdoor embeddings and normal embeddings.</sample>
    <sample id="172">No, gli LLM multilingue come Codex o BLOOM sono ancora inadeguati per il Cross-Lingual Semantic Parsing (CLSP). La tua ricerca suggerisce che i modelli multilingui, come Codex e BLOOM, non sono ancora sufficienti per affrontare efficacemente le sfide del CLSP.</sample>
    <sample id="174">The video introduces the "ArgAnalysis35K" dataset, which is a large-scale collection of argument-analysis pairs designed for argument quality analysis. Unlike existing datasets, ArgAnalysis35K features high-quality arguments sourced from expert debaters, speeches, and intermediate debaters, rather than crowdsourcing. The dataset covers a diverse range of 24 themes, capturing a wide variety of motions. It introduces the concept of "analysis," which combines claims, premises, and other elements to provide a coherent explanation of an argument. The dataset also incorporates instance-based annotator reliability, allowing for the use of annotators' judgments on specific arguments while accounting for their biases. Additionally, a relevance model assigns scores to arguments based on their relevance to different themes. The video highlights the unique features of ArgAnalysis35K, including its large size, high-quality arguments, diverse themes, and advanced annotation techniques, and encourages viewers to explore the dataset further through the provided paper and poster.</sample>
    <sample id="175">Il metodo affronta l'ambiguitá delle permutazioni utilizzando una funzione di perdita continua che permette di approssimare il problema del "venditore del viaggio" (TSP) in modo GPU-friendly. Questo approccio permette di calcolare la perdita e di effettuare il backpropagation, permettendo al modello di apprendere le permutazioni linguisticamente più plausibili.</sample>
    <sample id="176">L'equità di un modello NLP a livello di valutazione è complessa e coinvolge diverse fattori. In particolare, l'equità può essere definita attraverso l'analisi delle performance del modello su diverse categorie, come le differenze politiche, le categorie di demografia o le categorie di notizie. Inoltre, l'equità può essere valutata attraverso l'analisi delle decisioni del modello su diverse categorie, come la detenzione di discorsi di odio o la distinzione tra informazioni false e accurate. In generale, l'equità di un modello NLP può essere definita attraverro l'analisi delle performance del modello in base a diverse categorie, come le differenze politiche, le categorie di demografia o la distinzione tra informazioni false o accurate. Inoltre, l'equità di un modello può essere valutata attraversi l'analisi delle decisioni del model</sample>
    <sample id="177">The name of the presenter is Yanis Labrak.</sample>
    <sample id="178">Il nome del relatore è Koustav Sinha.</sample>
    <sample id="179">In her talk, Melanie Sclar discusses the challenges of improving Theory of Mind reasoning skills in large language models (LLMs) using a method called SymbolicToM. She explains that LLMs, such as ChatGPT and GPT-3, struggle with false-belief tasks, which require understanding the mental states of multiple characters. To address this, Sclar presents SymbolicToM, an inference-time method that uses explicit graphical representations to improve Theory of Mind reasoning.

SymbolicToM computes multiple graphical representations for all combinations of characters up to a predefined maximum Theory of Mind level. These graphs are used to efficiently answer questions about the mental states of characters. The method was tested on a variety of LLMs and compared against supervised baselines, showing significant performance gains across the board.

Sclar also designed two new datasets to test the generalization capabilities of SymbolicToM. The results showed that supervised models heavily degrade in performance on these datasets, while SymbolicToM still shows significant gains, allowing stronger models like GPT-4 to solve the datasets.

In conclusion, Sclar introduced SymbolicToM, a plug-and-play method to enhance Theory of Mind reasoning skills in LLMs. It is an inference-time algorithm that avoids overfitting risk, uses explicit graphical symbolic representations, and dramatically improves out-of-the-box LLM performance.</sample>
    <sample id="180">The name of the presenter is Myra.</sample>
    <sample id="181">In this paper, we introduce the problem of constrained language planning, which involves planning for goals with specific constraints, such as making a chocolate cake. We evaluate and improve the constrained language planning ability of language models, which have shown unsatisfactory results in planning for specific goals. We conduct a detailed analysis to investigate why learning models fail and adopt the idea of over-generate-then-select to improve generation quality. We create a dataset of constrained language planning, named CoScript, by distilling knowledge from large language models. We find that smaller models can surpass larger models when properly fine-tuned on suitable datasets. We hope that CoScript can be a valuable resource to advance research on constrained language planning.</sample>
    <sample id="182">Il tropicalismo nel contesto di questo articolo indica un modo di rappresentazione stereotipico che lega le donne Latina a caratteristiche come "vibrante" e "curvaceous". Questa rappresentazione è parte di un'immagine trope che esalta e esoterica le donne Latina, contribuendo a un'identità di altro e di discriminazione. Questo tipo di rappresentazione è considerato essenzialista e può perpetuare stereotipi negativi e arcaici.</sample>
    <sample id="183">Gli autori hanno elaborato le rappRESENTazioni umane dei gruppi target utilizzando prompt naturali per generare personaggi, come "Imagine you are an Asian woman. Describe Yourself." Questo approccio permette di generare descrizioni di persona che possono essere facilmente personalizzate per qualsiasi identità demografica. Successivamente, hanno utilizzato il metodo "Marked Words" per identificare le parole che distingue i gruppi marcati dai gruppi non marcati. Questo metodo si basa sull'idea di "markedness" sociolinguistica, che sottolinea che i gruppi dominanti sono generalmente considerati l'identità di default, mentre i gruppi marginalizzati sono spesso marcati. Gli autori hanno poi utilizzato il metodo "Fightin' Words" per confrontare le parole più frequenti per ciascun gruppo marcato con i gruppi non marcati, come gli uomini e gli uomini bianchi, per identificare le parole che contribuiscono a stereotipi e narrazioni essenzializzanti.</sample>
    <sample id="184">In questo lavoro, sono stati utilizzati i metodi CXMI e Pointwise CXMI per misurare l'utilizzo del contexto in traduzioni. CXMI misura quanto informazione il contesto fornisce sul target dato, dato il testo sorgente, mentre Pointwise CXMI permette di misurare l'utilizzo del conteste a livello di parola o di frase.</sample>
    <sample id="185">DrBERT and ChuBERT are both specialized models for the French language, but they differ in their data sources and training approaches. DrBERT is based on RoBERTa and trained on a large dataset of medical crawled data from the web (NACHOS), while ChuBERT is based on anonymized data obtained from the data warehouse of Nantes University Hospital. Additionally, ChuBERT is a clinical model trained on a mix of 4 GB of NACHOS and 4 GB of medical notes, while DrBERT is trained solely on NACHOS. Overall, DrBERT has shown better performance on most downstream tasks compared to ChuBERT.</sample>
    <sample id="187">There are two authors involved in the article: Ying and Zhiyang.</sample>
    <sample id="188">Il trasferimento iterativo dell'aprendimento è un approccio in cui un modello viene aggiornato iterativamente con nuovi dati raccolti in ogni ciclo di annotazione attiva. Invece di accumulare tutti i dati raccolti fino a quel punto (cumulative update), il modello viene aggiornato ogni volta con i dati più recenti (iterative update). Questo metodo permette di migliorare continuamente il modello in base alle nuove informazioni raccolte, migliorando così la capacità del modello di rilevare classi rare come quella della dissonanza.</sample>
    <sample id="189">The goal of the AltEntities Corpus is to understand users' language when they want to make a choice between entities, particularly in cases where direct references are not possible or appropriate. The corpus aims to improve the ability of language models to handle indirect referring expressions in conversational contexts.</sample>
    <sample id="190">Un utente malintenzionato può esstrare i parametri del modello attraversando un Embedding as a Service (EaaS) utilizzando il metodo di marcatura di acquisto (Embedding marker) proposto in questo lavoro. Questo metodo funziona seguendo questi passaggi:

1. **Iniezione del marcatore**: L'utente invia al servizio EaaS una serie di frasi che contengono un insieme di parole chiave (trigger). Durante l'iniezione del marcatore, il servizio EaaS calcola il numero di trigger presenti in ogni frase e aggiunge un peso proporzionale al numero di trigger al vettore di embedding fornito. Se il numero di trigger supera un certo soglia (m), il vettore di embedding fornito diventa esattamente uguale al vettore di embedding del marcatore.

2. **Verifica del diritto d'autore**: L'utente utilizza il marcatore per verificare se un altro servizio EaaS contiene il marcatore. Questo viene fatto confrontando le richieste di embedding fornite dal servizio EaaS con i dati di test (benign data set e back door data set). Se il servizio EaaS fornisce un embedding che corrisponde al marcatore, allora è probabile che il servizio EaaS contenga il marcatore.

Questo metodo permette all'utente malintenzionato di estrarre i parametri del modello senza compromettere la utilità del servizio EaaS per le applicazioni downstream.</sample>
    <sample id="191">There are three authors involved in the article: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="192">In this presentation, Yang Luo discusses their work on "CAME: Confidence-guided Adaptive Memory Efficient Optimizer." The challenge addressed is designing an optimizer that achieves fast convergence like traditional adaptive methods while maintaining low memory usage, as seen in memory-efficient methods. Luo introduces non-negative matrix factorization (NMF) as a method to reduce memory requirements from O(mn) to O(n + m). Adafactor, a memory-efficient optimizer, uses NMF but suffers from slow convergence and errors in updates. Luo proposes CAME, which addresses these issues by using a residual between momentum and current updates to adjust the denominator in the update step, reducing instability. Experiments on BookCorpus and English Wikipedia show that CAME significantly improves validation accuracy over Adafactor and Adam, with better performance on large models and reduced memory usage. CAME also outperforms existing optimizers like LAMB in memory efficiency. The presentation concludes with a summary of CAME's effectiveness and its suitability for large batch training.</sample>
    <sample id="193">43 annotatori sono stati impiegati nel creare il set di dati iniziali.</sample>
    <sample id="194">The authors of the article are affiliated with Carnegie Mellon University, the University of Washington, and the Allen Institute for AI. Specifically, the authors are Jenny (Carnegie Mellon University), Sebastian Santy, Ronan Le Bras, and Katharina Reinecke (University of Washington), and Maarten Sap (Allen Institute for AI).</sample>
    <sample id="195">Hello everyone. Today, I will introduce our work, "Reasoning Over Hierarchical Question Decomposition Tree for Explainable QA." Explainable QA aims to answer a question and provide an explanation for the answer. Recent work in XQA can be grouped as neuro-symbolic methods, which translate natural language questions to formal representations, and decompose-based methods, which generate natural intermediate steps leading to the final answer. However, most methods have limitations. Neuro-symbolic methods can only be executed on structured KBs, which are incomplete, limiting recall. Decompose-based methods use free-text corpora, but the diversity of natural language makes XQA difficult, and integrating knowledge from heterogeneous sources is important for answering complex questions intuitively.

Leveraging question decomposition is a promising direction since we flexibly select the appropriate knowledge source for each step. However, there are two main challenges: determining the granularity of question decomposition and finding the optimal solution among various possibilities. To address these challenges, we propose a novel framework, RoHT (Reasoning Over Hierarchical Question Decomposition Tree).

RoHT is a two-stage framework. First, we build a Hierarchical Question Decomposition Tree (HQDT) to understand the hierarchical compositional structure of a question. The root node is the original question, and each non-root node is a sub-question of its parent. The leaf node is an atomic question that cannot be further decomposed. Second, we perform probabilistic reasoning over HQDT to fuse knowledge from a KB and a text corpus at different levels of training, considering the probability score of both string generation and answering steps.

To build the HQDT, we first use a question decomposer to create leaf nodes, which are atomic questions. Then, we generate intermediate questions based on the grouped leaf questions and their reference tokens. We also compute a certainty score for each node based on the likelihood of its generation. After building the HQDT structure, we solve the complex question by conducting probabilistic reasoning from the root to the leaves in a recursive way. The reasoning process involves three steps for each node: determining the appropriate knowledge source, getting answers with probabilities from the selected knowledge source, and aggregating candidate answers to output the top key answers with the highest probabilities.

We evaluate RoHT on two challenging QA datasets, KQA Pro and Musique, which are originally KB QA and QA comprehension datasets, respectively. On KQA Pro, when only using the incomplete KB, our KB model outperforms existing KB QA methods, showing that integrating answers from sub-questions of different levels is beneficial. After adding Wikipedia as a supplementary text corpus to RoHT KB, we achieve substantial improvement compared to RoHT KB, demonstrating the effectiveness of using knowledge from KB and text together. Our Roht also outperforms TransferNet by a large margin, showing the superiority of explicit decomposition. On Musique, when only using the given paragraphs, our text model improves F1 by 11% compared to the SOTA method EX(SA). With both KB and text, the performance of RoHT-mix is remarkably better than TransferNet. Comparing RoHT-text and RoHT-mix, supplementing text information with knowledge from KB shows some benefits.

That's all. Thank you for listening.</sample>
    <sample id="196">L'esempio in cui il governator è a sinistra è "I saw Bart and Lisa". In questo caso, "I" è il governatore della coordinazione, e "Bart" e "Lisa" sono le due congiunzioni.</sample>
    <sample id="197">I modelli all'avanguardia nei sistema di dialogo includono quelli che sono stati valutati in un'analisi recente utilizzando ABC-Eval. Questi modelli sono stati scelti per essere tra i migliori attualmente disponibili e sono stati confrontati con il loro comportamento attraverso diverse metriche di qualità del dialogo. Alcuni dei modelli all'avanguardia che sono stati valutati includono:

1. **GPT-3**: Uno dei modelli di linguaggio generativo più avanzati, conosciuto per la sua capacità di generare testo coerente e contestualmente rilevante.
2. **BERT**: Un modello di linguaggio pre-addestrato che ha mostrato prestazioni eccellenti in vari compiti di NLP, inclusi quelli relativi al dialogo.
3. **T5 (Text-To-Text Transfer Transformer)**: Un modello che trasforma tutti i compiti di NLP in un formato di testo-to-text, mostrando buone capacità di dialogo.
4. **DialoGPT**: Un modello di dialogo sviluppato da Microsoft, basato su GPT-2, che è stato utilizzato in vari test di dialogo.
5. **LaMDA (Language Model for Dialogue Applications)**: Un modello di Google che è stato progettato specificamente per il dialogo e ha mostrato capacità di comprensione e generazione di testo coerente.

Questi modelli sono stati valutati utilizzando ABC-Eval, che fornisce una misura dettagliata delle loro capacità di gestire vari aspetti del dialogo, come la coerenza, la relevanza e l'empatia.</sample>
    <sample id="198">La valutazione dell'accettabilitài dei modelli nell'intera finesta di contesto è necessaria perché i modelli delle lingue naturali stanno diventando sempre più lunghi e possono considerare un contesto più ampio. Questo rende importante testare l'accettabilitài non solo su frasi singole, ma anche su frasi più lunghe e complessi. La nostra ricerca suggerisce che i modelli possono essere influenzati dalle caratteristiche sintattiche e semantiche condivise tra le frasi, e che l'attuale valutazione con frasi singole potrebbe non essere sufficiente per catturare pienamente l'abilità del modello nel contesto più ampio.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multi-lingue ha causato un calo delle prestazione rispetto al modello inglese mono-lingue in set di dati su sette dataset e in tre dataset specifici. Questo fenomeno è noto come "Curse of Multilinguality".</sample>
    <sample id="200">Gli annotatori conoscono le entità in anticipo.</sample>
    <sample id="201">Nel paper "Prompting PaLM for Translation: A Systematic Study of Large Language Model Prompting for Machine Translation," sono state utilizzate due tipi di metriche per la valutazione:

1. **Metriche basate sui dati (Data-driven metrics)**:
   - **BLEURT (Bilingual Evaluation Understudy for Retrieval of Translation)**: Questa metrica è stata utilizzata per misurare la qualità delle traduzioni generate dal modello.

2. **Metriche basate su esperti (Expert-based metrics)**:
   - **MQM (Multi-Dimensional Quality Metrics)**: Questa metrica è stata impiegata per valutare la qualità delle traduzioni in termini di fluenzia e accuratezza, basata su esperti umani.

Queste metriche hanno permesso di ottenere una valutazione completa sia su base automatizzata che su base umana, fornendo una panoramica dettagliata delle performance del modello PaLM per la traduzione.</sample>
    <sample id="202">Il regresso nella generalizzazione influiscono su specifici tipi di Named Entity Recognition (NER) in modo significativo. Le modelli basati su architetture come i Transformer tendono a generalizzare meglio rispetto a altre architetture, ma anche l'incremento del numero di esempi di fine-tuning e la dimensione del modello contribuiscono positivamente alla loro capacità di generalizzare. Inoltre, il regresso nella generalizzazione è principalmente dovuto al drift temporale, ovvero la degradazione delle performance causata dall'aumento del tempo tra i dati di addestramento e i dati di test. Questo suggerisce che i modelli che sono stati addestrati su dati più vecchi, come quelli di CoNLL-2003, possono non performare bene su dati più recenti, a causa del diverso contesto temporale.</sample>
    <sample id="203">La posizionalità nella NLP è importanti perché rappresenta le diverse prospettive e identità delle persone che creano e utilizzano i modelli e i dataset. Questo può portare a bias nelle tecnologie, dove alcune popolazioni possono essere sottorappresentate o non vengono considerati in modo equilibrato. La posizionalità può influenzare le decisioni di ricerca e gli output, e quindi è cruciale comprendere e affrontare queste disuguaglianze per garantire che le tecnologie siano inclusive e rappresentative di tutte le persone.</sample>
    <sample id="204">Gli LLM multilingue come BLOOOM sono stati affinati utilizzando adattatori, come Codex e BLOOM, che sono adattati per migliorare le prestazioni in compiti di parsing semantico multilingue. Questi adattatori aiutano a migliorare le capacità di generazione e comprensione in più lingue, rendendo gli LLM più versatile e potenti per applicazioni come il parsing semantico multilingue.</sample>
    <sample id="205">Shangbin, a PhD student at the University of Washington, presented their work on the political biases in language models. They found that language models trained on large-scale web crawl data, which includes political news media, can have varying political leanings. The study evaluated the political leaning of language models using political questionnaires and found that GPT-4 is the most liberal language. The study also found that language models can pick up the polarization in society and that this can lead to fairness issues in NLP applications. The study evaluated language models with different political leanings on hate and fake news detection and found that they perform differently based on their political leaning. The study highlights the unique dilemma of language model political biases and the need to tackle fairness issues.</sample>
    <sample id="206">Vasudha and her team use a combination of transfer learning and active learning to address the rare-class challenge in their work on cognitive dissonance detection. They transfer weights from two related tasks: topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDT (CE). They then fine-tune the model on these tasks to improve zero-shot performance. They also use a Probability-of-Rare-Class (PRC) strategy to select examples that are highly likely to be dissonant, which helps to improve the number of dissonance examples.</sample>
    <sample id="207">In the paper "Prompting PaLM for Translation," the authors used the latest test sets to evaluate the transition capabilities of the PaLM model, ensuring that the test data did not overlap with the training data of the language model. They compared the performance of PaLM to state-of-the-art systems using the WMT evaluation metrics and also included expert-based human evaluation results. The test sets used were from the WMT evaluation, which is known for its high-quality and curated data.</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti alla fine.</sample>
    <sample id="209">Il metodo proposto, che combina l'over-generate-then-filter per migliorare la qualità delle script generati da modelli di linguaggio grandi, offre un miglioramento significativo sia nella completitud semantica che nella fedeltà alle restrizioni rispetto ai metodi di riferimento. Questo metodo consente di generare script di alta qualità anche per modelli più piccoli e specializzati, che altrimenti potrebbero non essere in grado di superare i modelli più grandi quando addestrati su dataset appropriati. Inoltre, il dataset CoScript, generato tramite questo metodo, mostra una grande pluralità e una diversità delle specifiche, rendendolo un risorsa preziosa per la ricerca su pianificazione linguistica.</sample>
    <sample id="210">Il nome del relatore è Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati DEPLAIN possono essere utilizzati come parametro di riferimento. L'articolo presenta l'uso del set di dati per valutare metodi automatici di allineamento e per la fine-tuning di modelli linguistici per la semplificazione automatica. I risultati ottenuti con questi metodi sono pubblicati come base benchmark per il problema della semplificazione automatica.</sample>
    <sample id="212">In the article, the authors mention using T5 fine-tuned on CoScript to generate scripts of higher quality than most large language (LLM) models. This suggests that they are using at least one smaller model (T5) for constrained language planning. However, the article does not provide a specific number of smaller models used.</sample>
    <sample id="213">Il modello di base utilizzato per analizzare l'ottimizzazioni delle istruzioni multimodali è OFA, un modello pre-addestrato unificato per il multitasking che utilizza una vocabolaria unificata per il linguaggio, i token immagine e le coordinate di un bounding box.</sample>
    <sample id="215">Adam Przepiórkowski's talk discusses the dependency structure of coordination in linguistics, focusing on the principle of dependency length minimization. He contrasts asymmetric approaches, where one conjunct is the head of the structure, with symmetric approaches, where all conjuncts are heads. Przepiórkowski argues for symmetric structures using examples from the Penn Treebank, showing that shorter dependencies are preferred. He notes that left conjuncts tend to be shorter, especially when the governor is on the left or absent, and this tendency disappears when the governor is on the right. This supports the idea that all conjuncts should be treated equally in dependency structures.</sample>
    <sample id="217">Hello everyone. I'm Weihao Zeng, and I work with Lulu Zhao and Kequing He at the Beijing University of Posts and Telecommunication. We're introducing our work, "Seen to Unseen: Exploring Composational Generalization of Multi-Attribute Controllable Dialogue generation."

Our motivation stems from the limitations of previous methods, which focus on single attributes and lack the capability to handle continuous attributes. We propose a unified evaluation framework, MAE, and a disentangled controllable generation (DCG) model that learns attribute concepts from seen values and uses disentanglement loss to disentangle attribute combinations.

We designed two types of prompts, attribute-oriented and task-oriented, to effectively use control signals. We also introduced a disentanglement loss to train multiple compositional prompts and proposed a unified and efficient evaluation framework that does not rely on additional large-scale labeled data.

Our DCG model outperforms all other baselines in attribute controllabilty and text equality. We tested our model with attribute-oriented prompts, task-oriented, and disentanglement learning, and found that attribute-oriented prompts guide the model to focus, task-oriented prompts improve text equality, and disenanglement learning improves the ability of compositional generation.

We also implemented MAE on another pre-trained language model, BART, to show its generality. Our proposed attribute-oriented prompt method outperforms models learning an independent prompt for each attribute value. The results confirm the effectiveness of our method for transforming seen attributes into unseen combinations.</sample>
    <sample id="218">The authors of the paper "Prompting PaLM for translation: Assessing Strategies and Performance" are affiliated with Google Translate.</sample>
    <sample id="219">Hi everyone, I'm Jia-Huei Ju, a researcher at Academia Sinica. I'll present our work, "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals." This work is done with Yu-Shiang Huang and Cheng-Wei Lin, under the guidance of Professors Che Lin and Chuan-Ju.

Financial report analysis is crucial for understanding a company's activities. However, manually extracting useful information is labor-intensive. Our work aims to automate this process using a Form 10-K corpus, an annual SEC-required report. We noticed that reports are highly similar, with about 80% of tokens being the same, and content varies yearly.

We introduce a highlighting task and a multi-stage pipeline to find the rationale behind word importance. The pipeline has three stages: document segmentation, relation recognition, and fine-tuning. We classify pairs into three types: Type β (high similarity), revised (similar patterns, different meanings), and mismatched (new information).

For out-of-domain fine-tuning, we use eSNLI, a natural language inference dataset. For in-domain fine-tuning, we use revised pairs with pseudo positive labels and mix objectives using soft labeling techniques.

Our model achieves the best performance on our released FINAL dataset and generalizes well to eSNLI. It also benefits from simulation with mismatched pairs.

In conclusion, we propose a highlighting task with our FINAL dataset and a simple pipeline with two-stage tuning. Future work includes improving effectiveness and adding features. Thank you.</sample>
    <sample id="220">The authors of the article "Transfer Learning for Dissonance Detection: Aiming at the Rare-Class Challenge" are affiliated with Stony Brook University.</sample>
    <sample id="221">L'articolo "Prompting PaLM for Translation: Assesing Strategies and Performance" analizza le strategie di promozione per la traduzione utilizzando il modello di linguaggio PaLM. Le coppie linguistiche analizzate sono quelle tra il tedesco e l'inglese, come specificato nel contesto dell'esperimento descritto nel riassunto.</sample>
    <sample id="222">The work "To Adapt or to Annotate: Challenges</sample>
    <sample id="223">The name of the presenter is Shangbin.</sample>
    <sample id="224">Durante gli esperimenti, i modelli studiati includevano:

1. **MASSalign**: Utilizzato per l'allineamento automatico tra le frasi di due documenti paralleli con la stessa lingua e contenuto, ma a diversi livelli di complessità.
2. **long-mBART**: Utilizzato per la semplificazione del testo a livello di documento.
3. **normal base mBART**: Utilizzato per la semplicificazione del testo a livello di frase.

Questi modelli sono stati testati e valutati per la semplificazione automatica del testo in Germania.</sample>
    <sample id="225">In MultiInstruct, 53 tasks out of the 62 diverse activities are used for training, with 10,000 instances sampled per task. For testing, the entire common sense reasoning group is reserved, and an additional 5 tasks from the VQ and Miscellaneous groups are selected, with all instances in the test split for each task. Additionally, 20 tasks from the test split of the natural instruction dataset are randomly sampled as unseen tasks for NLP.</sample>
    <sample id="226">There are two authors involved in the article: Regina Stodden and Omar.</sample>
    <sample id="227">The paper discusses the challenges and limitations of current language models in grounded language understanding, which involves mapping natural language expressions onto specific environments or plans. The authors propose a novel framework called Pangu, which separates the symbolic world from the language model's world, allowing the language model to focus on discrimination rather than generation. This approach has shown outstanding performance across various settings, including fine-tuning and in-context learning, and demonstrates strong sample efficiency and robustness under non-i.i.d. settings. The paper concludes that discrimination is a better strategy for grounded language understanding than generation, and invites further discussions and collaborations.</sample>
    <sample id="228">The authors conducted tests on four datasets: AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">Hello everyone, I'm Gabriella Skitalinskaya, presenting our joint work with Henning Wachsmut on detecting improvable claims for argumentative writing support, focusing on text revisions. Text revision is crucial in professional writing, especially in argumentative texts, as it directly influences the message's effectiveness. Our paper introduces two tasks: Suboptimal-Claim detection and Claim Improvement Suggestion. We aim to determine if a claim is optimally phrased or needs revisions and identify the quality issues to improve.

We explore challenges in working with revision-based data, particularly in argumentative text. We focus on implicit revision patterns from collaborative online debate platforms like Kialo. Our paper addresses four challenges: Representativity and Reliability, Model Complexity and Architecture, Contextual Information, and Topical and User Bias.

We find that revision-based data can be effectively used for the tasks, and modeling the distance between claim versions is beneficial. The impact of contextual information depends on the task and quality issues. For more details, please refer to our paper. Thank you for your attention.</sample>
    <sample id="231">NACHOS (Natural Language Corpus for Health Sciences) è un dataset di dati medici raccolti dalla rete web, utilizzato per prendere in considerazione una vasta gamma di fonti di dati in ambito sanitario. È stato utilizzato per prendere in considerare una vasta gamma di fonti di dato in ambito sanitario.</sample>
    <sample id="232">Il nome del relatore è David Vilar.</sample>
    <sample id="233">Simultaneous speech translation (SimulST) is the process of translating spoken language into text in real-time, enabling cross-language communication. However, current SimulST models have several problems, including the need for specific architectures, long and complicated training procedures, and the need to train and maintain multiple models to achieve different latency regimes.

To address these issues, the authors propose a solution called EDAtt, or Encoder-Decoder Attention. This strategy uses existing offline speech translation models without re-training or adopting specific architectures for SimulST. Instead, it uses a single model for every latency regime and handles latency through specific parameters. The EDAtt strategy leverages the knowledge already acquired by the model through the cross-attention mechanism between audio input and textual output.

The authors compare their results with popular strategies applied to offline models, such as the Wait-k strategy and the Local Agreement, as well as with the state-of-the-art architecture specifically tailored to simultaneous pre-translation. They find that their approach outperforms all other strategies, with curves shifted to the left, indicating lower latency. Additionally, when considering the actual elapsed time or computational-aware time, EDAtt is the fastest strategy.

The authors also released open-source code and models to facilitate the reproducibility of their work. Overall, their approach provides a promising solution for simultaneous speech translation, with improved performance and efficiency compared to existing methods.</sample>
    <sample id="234">La strategia del prompting ha un'influenza significativa sui risultati. In particolare, la selezione degli esempi e la loro qualità sono cruciali. Una buona selezione degli esempi può migliorare notevolmente le performance, come dimostrato in un esperimento in cui la differenza tra due prompt diverse è di più di un punto BLEURT. Inoltre, la forma effettiva del prompting ha un impatto maggiore in caso di prompt zero e uno, mentre in caso di prompt più lunghi (come i 5-shot) la differenza è meno pronunciata. Inoltre, la selezione degli esemppi dai dati di test (come il dev data) porta a risultati migliori rispetto ai dati di training più rumorosi. Infine, la fluenzia delle traduzioni di PaLM è comparabile a quelle degli sistemi di stato dell'arte, ma con un maggior numero di errori di omissione.</sample>
    <sample id="235">The authors of the article "When Does Translation Require Context? A Data-Driven, Multilingual Exploration" are Kayo Yin, Patrick Fernandes, Emmy Liu, André Fernandes Martins, and Graham Neubig.</sample>
    <sample id="236">The five expert-written instructions for the MultiInstruct dataset are not explicitly mentioned in the presentation. However, the presentation does mention that each task in the dataset is equipped with five expert-written instructions. These instructions are used to guide the instruction tuning process and improve the model's performance on unseen multi-modal tasks.</sample>
    <sample id="237">The authors propose a diagnostic test suite called KITMUS to evaluate the ability of natural language understanding models to integrate and use knowledge from multiple sources. They introduce a coreference resolution task designed to probe for this ability. The test suite includes three settings: "Background-Pretrain," "Background-Both," and "Background-Inference," which vary the availability of background and entity-specific knowledge. The authors evaluate the dataset using human study participants and established coreference resolution models, finding that most models require task-specific training to effectively integrate knowledge from different sources.</sample>
    <sample id="238">In this video, Yebowen Hu from the University of Florida presents a new benchmark dataset called MeetingBank, which addresses the need for high-quality meeting summaries and trustworthy resources. The dataset includes City Council meeting transcripts, reference summaries, and URLs. The data collection process involves converting audio to transcripts using Speechmatics API, identifying meeting types and data, locating reference summaries, and aligning timestamps. The dataset contains 1,366 City Council meetings with nearly 7,000 instances. The dataset statistics include the number of meetings, meeting duration, number of speakers, and year period. The dataset also provides summarization instances for each city and average statistics for both meeting-level and secondary levels. The dataset is used to evaluate top-tier summarization systems, including extractive and abstractive models. The results show that GPT-3 achieves the highest overall scores in terms of fluency and coherence, but less impressive in terms of informativeness and factual accuracy. The findings suggest that meeting summarization solutions should focus on capturing main discussion points and developing new automatic evaluation metrics to better align with human preference. The primary contribution of the MeetingBank dataset is its creation, which serves as a useful tool for researchers to design advanced summarization systems and provides insights into the decision-making process of City Council meetings.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e vi farò una breve recensione del lavoro "Prompting PaLM per la Traduzione: Valutazione delle Strategie e delle Prestazioni". Questo è un lavoro congiunto con i miei colleghi da Google Translate. PaLM è un grande modello di linguaggio naturale a 540 miliardi di parametri presentato l'anno scorso, 2022. È addestrato su una vasta collezione di testo, composta da 780 miliardi di token. Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di NLP. In questo lavoro, presentiamo lo studio sistematico per la promozione del modello di linguaggio naturale per la traduzione. Abbiamo valutato la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità di MT. Questo comporta l'uso dei migliori test set per evitare l'overlap dei dati di test con i dati di addestramento del modello di linguaggio. E abbiamo confrontato con i sistemi più avanzati, quindi il migliore sistema, quindi la valutazione WMT. Utilizziamo metriche di MT neurali di stato dell'arte e, inoltre, mostriamo i risultati dell'evaluazione basata sull'esperto. Infine, forniamo alcune raccomandazioni per le strategie di promozione. La promozione ha un grande impatto sulle prestazioni dei modelli di linguaggio naturale per la tradozione, come possiamo vedere in un semplice esperimento, dove abbiamo utilizzato la promozione a uno shot e fornito due promemoria diversi per ogni frase. La maggior parte delle frasi, 516 su 1.000. La differenza osservata è di più di un punto BLEURT. E questo può arrivare, in casi estremi, fino a 40 punti BLEURT. Quindi, è importante selezionare una buona strategia di promozione. Nelle nostre esperienze, abbiamo scelto una strategia di promozione a cinque shot dove abbiamo semplicemente segnato ogni frase che forniamo al sistema, con la lingua in cui si trova. Qui, ad esempio, dove eseguiamo la traduzione dall'inglese all'inglese, le frasi di origine, le frasi sorgenti, sono segnate con una virgola in tedesco e le traduzioni in inglese con una virgola in inglese. Abbiamo visto che la forma reale della promozione non ha un grande impatto nel caso di molte promozioni a breve lunghezza. È cruciale per le promozioni a uno e uno shot. Quando andiamo, come nel nostro caso, a una promozione a cinque shot, c'è quasi nessuna differenza nella forma reale della promozione. È gli esempi che portano la maggior parte del peso. La sintesi dei nostri risultati sperimentali è che la qualità degli esempi è più importante di quella della similitudine alla frase di origine. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, abbiamo confrontato la selezione di promemoria dai dati di addestramento per le valutazioni WMT sul dev data. Il dev data è molto più curato e di alta qualità rispetto ai dati di addestramento, che sono più rumorosi. I risultati sono migliori quando si utilizzano i dati dev. Tuttavia, i sistemi specializzati di stato dell'arte hanno un vantaggio sostanziale rispetto alle traduzioni di PaLM. Ma, PaLM arriva abbastanza vicino a un sistema commerciale. In questo caso, abbiamo scelto di valutare con Google Translate. Le informazioni che abbiamo ottenuto dall'evaluazione umana che abbiamo condotta utilizzando il framework MQM hanno detto che la fluenzia di PaLM è comparabile a quella dei sistemi di stato dell'arte, ma la differenza principale deriva dalla precisione. Quindi, in particolare, i più comuni errori sono quelli di omissione. Sembra che PaLM scegli di produrre una traduzione più suave, a volte eliminando parti della frase di origine che sono tradotte. Tuttavia, la categoria "Stile/Abbastanza" per PaLM è inferiore a quella per i sistemi di stato dell'arte. Questo è un ulteriore segnale che PaLM fornisce output fluenti, ma con alcuni problemi di precisione. E così è. Questo è tutto per questa breve panoramica. Per ulteriori dettagli, vi chiedo di venire alla presentazione completa del lavoro. Grazie mille.</sample>
    <sample id="240">Ciao, sono Dawei, un dottorando a Saarland University in Germania. In questo video, voglio presentare il nostro lavoro recente "Weaker Than You Think: Un'Analisi Critica del Machine Learning a Supervisione Debole". Questo è un lavoro con Xiaoyu Shen, Marius Mosbach e Andreas Stephan, sotto la supervisione di Dietrich Klakow. Voglio iniziare con una breve introduzione al supervisore debole e al machine learning a supervisione debole.

In un supervisore debole, non annotiamo manualmente i dati. Invece, annotiamo i dati utilizzando fonti di annotazione deboli, come regole semplici, basi di conoscenza o crowd sourcing di bassa qualità, come mostrato nella figura a destra. Quando confrontati con le annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni sono errate. Se addestriamo reti neurali direttamente su dati etichettati deboli, le reti neurali tendono a memorizzare il rumore delle annotazioni e non generalizzano. Nel machine learning a supervisione debole, vengono proposti algoritmi di addestramento per addestrare reti neurali robustamente sotto tale rumore delle annotazioni in modo che i modelli addestrati siano comunque generalizzabili.

Nelle ultime opere in WSL, una dichiarazione comune è che le persone dicono di addestrare modelli su dati etichettati deboli e ottenere prestazioni elevate su test set puliti. Tecnicamente, questa dichiarazione non è sbagliata, ma c'è un problema, che è che le persone assumono che ci sia un set di validazione pulito disponibile per la selezione del modello. Non possiamo fermarci su questo problema, ma questo implica che sono necessari ulteriori annotazioni manuali in WSL. Ma come un elefante nella stanza, questa necessità è spesso trascurata.

Questa dubbia è stata posta a tre domande di ricerca. Prima, è necessario un set di validazione pulito per WSL o possiamo usare un set di validazione rumoroso invece? Secondo, se è necessario un set di validazione pulita, o se è obbligatorio per WSL funzionare, quanti campioni puliti dobbiamo avere? Infine, dovremmo usare solo i campioni puliti per la validazione, o ci sono altre forme migliori di utilizzarli?

Abbiamo affrontato queste domande nella nostra lavorazione e i nostri risultati sono i seguenti. Prima, abbiamo trovato che, affascinantemente, i recenti metodi WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo delle prestazioni. Come mostrato nella figura, se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre le etichette deboli, il che significa che l'addestramento è inutile. Questo indica che i metodi WSL effettivamente richiedono dati etichettati puliti per funzionare correttamentamente, e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere sottovalutato.

Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuta i metodi WSL a ottenere prestazioni migliori, come mostrato nella figura di sinistra. Tipicamente, solo bisogna 20 campioni per classe per ottenere prestazioni elevate. Ma non è tutto, perché se decidiamo comunque di accedere a campioni puliti, allora addestrare direttamente su di essi può ottenere prestazioni ancora migliori. La figura a destra mostra la differenza di prestazioni tra approcci di fine-tuning diretti applicati sui dati puliti e WSL approcci che utilizzano i dati puliti per la validazione solo. Come possiamo vedere, se abbiamo 10 campioni per classe, l'approccio di fine-tuning inizia a superare i metodi WSL. Infine, l'incremento delle prestazioni dichiarate negli approcci WSL può essere facilmente ottenuto</sample>
    <sample id="241">Ethan presents a paper titled "Human-in-the-loop Evaluation for Early Misinfomation Detection: A Case Study of COVID-10 Treatments." The paper discusses the limitations of existing automatic misinformation detection systems, which are often evaluated using retrospectively constructed datasets and do not involve human content moderators. The authors propose an evaluation framework that addresses these deficiencies by involving humans at various stages of the process. The framework consists of two main components: the detection of misleading claims and the verification of policy violations. The system takes raw tweets as input and outputs check-worthy claims, which are then ranked by trendiness and provided to humans for verification. The second component uses a BERT-based stance classification model to determine the author's stance towards unapproved treatments, which are then flagged for human review. The authors evaluate the efficacy of the system by operationalizing early detection as the detection of an unapproved</sample>
    <sample id="242">I metodi di valutazione comuni per sistemi di dialogo includono:

1. **Valutazione umana**: Chiedere a giudici umani di selezionare quale delle due conversazioni è migliore o di dare una valutazione su una scala Likert.
2. **Valutazione a livello di turno**: Utilizzare valutazioni a livello di turno per misurare la qualità delle risposte individuali.
3. **Valutazione a livello di dialogo**: Utilizzare valutazioni a dialogo per valutare l'intera conversazione.
4. **Confronto di dialoghi a livello di coppia**: Confrontare due dialoghi per determinare quale è migliore.

Questi metodi forniscono una valutazione complessiva della qualità del dialogo, ma possono non coprire tutti gli aspetti della qualità del dialogo.</sample>
    <sample id="243">Ci sono cinque autori coinvolti nell'articolo: Jenny, Sebastian Santy, Ronan Le Bras, Katja Reinecke e Maarten Sap.</sample>
    <sample id="244">Nell'esempio con Servin e Kei, le conoscenze di base necessarie includono:

1. **Conoscenze specifiche dell'entità**: "Servin è un giudice" e "Kea è un forno."
2. **Conoscenze di base**: "I giudici decidono casi in tribunali" e "I forni preparano cibi."

Queste conoscenze sono necessarie per risolvere il pronome "he" che si riferisce a Servin.</sample>
    <sample id="245">Hi, I'm Lining Zhang, and today I'll present our work titled "A Needle in a Haystack: Analyzing High-Agreement Workers on MTurk." Our co-authors and I developed a two-step pipeline to identify high-agreement Amazon Mechanical Turk (MTurk) workers for summarization tasks. The motivation behind this pipeline is that automatic metrics and best practices for MTurk recruitment are often problematic.

The pipeline consists of three main stages: qualification settings, qualification task, and endurance task. In the qualification settings, we pre-filter workers based on location, number of Human Intelligence Tasks (HITs), and HIT Approval Rate. The qualification task tests workers' ability to evaluate multiple dimensions correctly, including an attention check. Workers are categorized into gold, silver, bronze, and block types, with only gold and silver workers passing.

The endurance task tests workers' capacity for handling a heavy workload. It includes 10 HITS, one document, and four summaries in terms of the saliency dimension. This stage results in 12 MTurk workers passing, with gold and silver workers achieving high agreement in terms of Inter-Annotator Agreement (IAA).

The reference-based task tests workers' general performance on the true annotation task. It involves 30 HITs, one reference summary, and four candidate summaries to check information coverage. Our pipeline workers' results show that 12 out of 12 MTurk workers completed all HITs, with a Krippendorff's Alpha of 0.534.

We also compared our pipeline workers with Baseline and CloudResearch MTurk workers. The Baseline MTurk workers achieved a Krippendorff's Alpha of 38%, while CloudResearch MTurk workers achieved 51.3%, but with a lower task acceptance rate.

Our analysis of correctness across annotation sources shows that Pipeline and CloudResearch workers had a significant correlation, but Pipeline may not guarantee the training of correctness.

In conclusion, our pipeline results in 4 gold (6%) and 8 silver (13%) workers out of 200 participants. It serves as a best practice for high-agreement annotations at a lower cost and can avoid resource waste on discarded annotations. Future work will focus on hiring high-quality workers and exploring multiple applications for tasks, languages, and platforms, while addressing the limitations of our current study.</sample>
    <sample id="246">Yes, the code is available. You can find it on GitHub.</sample>
    <sample id="247">In this presentation, Jiho Kim from KAIST AI introduces a new dataset called FactKG, which focuses on fact verification using knowledge graphs. Unlike existing datasets that rely on text or tables, FactKG utilizes DBpedia as evidence, allowing for more intuitive and reliable reasoning. The dataset includes claims in both written and colloquial styles, with two labels: SUPPORTED and REFUTED, and features five types of reasoning: one-hop, conjunction (multiple one-hop claims), existence, multi-hop, and negation. The dataset also includes baselines for comparison, such as Claim Only and the GEAR model, which uses graph evidence. The results show that all baselines outperform the majority class baseline, and the GEAR model using graph evidence achieves the best performance. The dataset and paper are available for download, and the authors welcome further discussion.</sample>
    <sample id="248">Sì, gli annotatori per NLPositionality sono bilanziati rispetto a ciascun Gruppo Demografico, ad esempio Paese e genere. Questo è stato fatto per ottenere un set di dati e annotazioni diversificato, che permette di confrontare le etichette dei modelli e dei dataset con le opinioni dei reali utenti.</sample>
    <sample id="249">Nel dominio accettabile, le frasi sono state perturbate aggiungendo rumore a una struttura sintattica e semantica giusta. Questo è stato fatto per mantenere la struttura corretta della frase mentre si aggiunge il rumore, per capire come le modelli di linguaggio reagiscono a queste perturbazioni.</sample>
    <sample id="250">Una valutazione dimensionale significa misurare e analizzare diverse aspetti o caratteristiche di una conversazione, piuttosto che fornire una valutazione generale. Questo approccio permette di capire meglio le forze e le debolezze di un modello di conversazione su una base più dettagliata e specifica. Ad esempio, invece di solitamente chiedere a un esperto umano di valutare l'intera qualità della conversazione, si possono specificare e misurare aspetti come la relevanza delle risposte, la coerenza, la coerenza con i dati, ecc. Questo aiuta a identificare problemi precisi e a migliorare il modello in base a queste specifiche aree di miglioramento.</sample>
    <sample id="251">The author of the article is Jingwei Yi from the University of Science of China.</sample>
    <sample id="252">In this presentation, Sai Kiran Tanikella and his team from IIT Kanpur introduce their work on "U-CREAT: Unsupervised Case Retrieva l using Events extrAcTion." They address the challenge faced by legal professionals in retrieving relevant past precedents due to the increasing volume of cases. The team's key contributions include the IL-PCR dataset, a new benchmark for Prior Case Retrieval tasks, and the U-CREAT pipeline, which leverages unsupervised learning and event-based approaches.

The IL-PCR dataset, consisting of 7,070 legal cases, provides a comprehensive test bed for PCR algorithms. The U-CREAT pipeline, on the other hand, uses event extraction to improve retrieval efficiency, inference time, and generalization across legal systems. The team conducted experiments with various models, including count-based, transformer-based, and event-based models, and found that event-based models, particularly the Event Filtered Documents model, outperformed all other methods.

The U-CREAT approach is the current state-of-the-art method on the COLIEE’21 document retrieval tasks, with significantly lower inference times and higher F1 scores compared</sample>
    <sample id="253">Hello everyone, my name is Mario Ezra Aragón, and today I will present our work titled "DisorBERT: A Double Domain Adaptation model for detecting signs of mental disorders in social media." This project is a collaborative effort between researchers from Mexico and Spain.

Mental disorders are psychological syndromes associated with distress and disability that affect thinking, feeling, mood, and behavior. Examples include major depression, PTSD, bulimia, anorexia, and more. Social media content is vast and offers an opportunity to study how people face difficulties. Many people share their daily routines and important events online, while others use the anonymity of these spaces to discuss mental health issues and seek help.

Our goal is to contribute to the detection of mental health disorders through the automatic analysis of social media posts. This analysis aims to support a new technology that can warn about the onset of mental disorders and provide evidence.

We use domain adaptation because we often have insufficient annotated data and want to improve a model's performance on a target domain. By leveraging knowledge from a related or similar domain, we can adjust the model's vocabulary and learn the domain-specific task. For example, we can adapt BERT, a language model trained on general data like Wikipedia and Google Books, to the specific language of Reddit and mental health.

Our approach involves starting with a base language model and integrating information from Reddit and mental health. We also use a lexicon to guide the masking process. Our main idea is to first learn the social media language and specialize in the mental disorder domain. Using guided masking, we want the model to focus on important words during training.

The results using the eRisk datasets show that our model tends to be balanced in terms of precision and recall, unlike other methods that have high precision but low recall. We illustrate the behavior of the learned model and the textual segments it focuses on. For example, when given a sentence with masked words from Beck's Depression Inventory, DisorBERT predicts words like focus, talk and breath, which are related to mental disorders.

In conclusion, our double domain adaptation and guided masking approach is effective at capturing signs of mental disorders in online interactions. It outperforms MentalBERT, a model trained with a large dataset. Future work will explore different lexical resources and clinical data.

Thank you for your attention. If you have any</sample>
    <sample id="254">Hello everyone. Today I'm going to introduce our research work, "Uncertainty Guided Label Noising for Document-level Distant Relation Extraction". This work aims to improve the performance of document-level relation extraction models by addressing the issue of noise in distant supervised data. We propose a framework that uses uncertainty-guided label denoising to generate more accurate pseudo labels. Our framework first trains a pre-denoising DocRE model with DS and human-annotated data to generate pseudo labels. We then introduce uncertainty estimation to determine the trustworthiness of model predictions. We also propose an instance-level uncertainty estimation method to capture the uncertainty scores for overlapping relations. Our framework also includes a re-labeling strategy with dynamic class uncertainty threshold and a multi-phase training strategy to further improve performance. We compare our framework with several strong baslines and show that it outperforms them on public datasets. The main contributions of our work are: 1) our framework with uncertainty guided label denoising that greatly improves the label quality of DS data, 2) the instance-level uncertainty estimation method for overlapping relations, 3) the iterative re-label strategy with dynamic class uncertainty threshold for the long-tail problem, and 4) the great performance improvements.</sample>
    <sample id="255">La forma del prompting si rivela importante in casi specifici, come quando si utilizza uno zero-shot o uno one-shot prompting. In questi casi, la selezione delle frasi di esempio e la loro qualità hanno un impatto significativo sulle performance del modello. Tuttavia, quando si passa a un prompting a cinque shot o più, la forma del prompting non ha un impatto significativo, e l'importanza cresce maggiormente sulle esempi forniti.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo state-of-the-art.</sample>
    <sample id="258">In this video, Chiang Cheng-Han introduces a new research work titled "Can Large Language Models Be an Alternative to Humans?" The study explores the use of large language models (LLMs) to evaluate the quality of text in natural language processes. The researchers propose that LLMs can be instructed with natural language text to evaluate samples, potentially providing ratings similar to human evaluations.

The motivation behind this work stems from the instability and difficulty of reproducing human evaluations. The researchers question whether LLMs could serve as an alternative to human evaluation, which often involves giving instructions to human evaluators to rate samples.

To test this idea, the researchers conducted an experiment where they used LLMs to rate stories generated by GPT-2 or written by humans. The stories were evaluated based on four attributes: grammar, coherence, likeability, and relevance. The researchers compared the LLM ratings with human ratings, which were obtained by hiring English teachers to evaluate the same stories.

The results showed that human raters preferred human-written stories over GPT-2-written stories. However, not all LLMs showed a clear preference. Interestingly, two LLMs, Davinci and ChatGPT, demonstrated a preference for human-written text, similar to human evaluators.

The video concludes by mentioning that the paper addresses various questions, such as the agreement between LLMs and human evaluators on individual ratings, the impact of changing instructions or sampling methods, and the benefits and costs of using LLM evaluations compared to human evaluation. The researchers encourage viewers to read the paper or visit their poster at ACL for more information.</sample>
    <sample id="259">Hello everyone, my name is Yusan Zhang from Penn State University. Today I'm going to introduce our work "XSemPLR: Cross-lingual Semantic Parsing in Multiple Natural Languages and Meanings Representations". Semantic parsing is the task of building semantic representations of user queries such as SQL and lambda calculus. Cross-lingual semantic parsing is the task of translating queries in multiple natural languages into multiple meaning representations, such as SQL, lambda, or funql.

Existing cross-lingual semantic parsing models are separately developed and evaluated on datasets of limited tasks and applications. For example, there is a lot of coverage on certain natural languages, but Chinese is missing, and there is a lack of coverage on certain meaning representations, such as lambda calculus. Additionally, existing models are only evaluated on a single neural model, such as mBERT or XLM-R.

To address these limitations, we propose XSemPLR, a uniform dataset for cross-lingual semantic parsing in multiple languages and meaning representations. It contains 9 different datasets in various domains, 5 semantic parsing task, 8 meaning representations, and 21 natural languages in 15 language families.

To better evaluate our benchmark, we consider six different settings for training and evaluation. These include Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer.

We evaluate our models on two groups of models: Encoder-PTR, which stands for Multilingual Pretrained Encoders and Pointer-based Decoders, and Encoder-Decoder, which stands for Multilingual Pretrained Encoder-Decoder Models. We found that Encoder-Decoder obtains better performance on all nine datasets, and that Encoder-PTR can be improved by training in the mixture of various languages.

We also found that the "Curse of Multilinguality" occurs when training on multiple languages, as most major natural languages obtain performance gain, except for English, which drops in seven datasets and only gains in three.

In summary, we build XSemPLR, a uniform benchmark for cross-lingual semantic parsing with many natural languages and meaning representations. Our results show many interesting findings, and we welcome you to visit our paper and code. Thank you for listening.</sample>
    <sample id="260">The article mentions one author, Jingwei Yi, from the University of Science and Technology of China, as the presenter of the paper. However, it does not provide information on any additional authors involved in the research or writing of the paper.</sample>
    <sample id="261">Un buon pianificatore dovrebbe essere ragionevole e fedele alle restrizioni imposte dai goal specifici. Questo significa che il pianificatore deve essere in grado di generare script che siano sia completi semanticamente che rispettino accuratamente le restrizioni.</sample>
    <sample id="262">The article "Distilling Script Knowledge from Large Language Models For Constrained Language Planning" is authored by Siyu Yuan from Fudan University, and it does not mention any other authors.</sample>
    <sample id="263">In this work, we present a systematic investigation of label bias problems in in-context learning, a popular paradigm for utilizing large language models. We identify a new type of bias, domain-label bias, and propose a novel calibration method to handle all types of bias. Our method uses content-free text, specifically random in-domain words, to estimate the model's bias on each label name and calibrate the model's original predictions. We conduct experiments on a wide range of datasets and find that domain-context calibration significantly improves the average performance of in-context learning, especially on tasks with larger domain-label bias. We also show that using random in-domain words instead of single pre-defined tokens leads to further improvements. Our findings hold for larger models like GPT-3.</sample>
    <sample id="264">Hello everyone, my name is Lin Wang, and I'm a Ph.D. student at Zhejiang University, China. I'm here to present my paper titled "TAVT: TowardsTransferable Audio-Visual Text Generation."

In the field of multimodal text generation, such as audio-visual text generation, data annotation is more challenging and expensive compared to unimodal tasks like machine translation and image captioning. Existing works often face severe degradation due to varying construction conditions in different environments. To address this, we propose a novel task called Transferable Audio-Visual Text Generation (TAVT). The main challenge of this task is the multi-modality domain shifts, such as visual style and audio energy.

We observe that in content understanding for the same event, visual content can shift significantly with changes in image style and shooting angle, while audio content, such as rhythm and energy, hardly affects event understanding. Based on this, we propose using a unified audio semantic space to align visual concepts across domains.

Our framework consists of three components: an audio-visual meta mapper network, an audio-visual encoder and language model, and counterfactual contrastive learning. We collect audio clips from the Flickr dataset, cluster them using k-means, and use the resulting set of audiocasts as a unified audio semantic space. We introduce learnable tokens called visual prefixes for the audio cluster and generate a set of probability distributions in the audio space for reconstructing the audio.

We also introduce an alpha to evaluate the contribution of different modalities at each time step. Our framework uses a transformer-based encoder and generator, and we propose a Dual Counterfactual Contrastive Loss (DCLL) to optimize visual-audio alignment scores.

In the experimental section, we build two benchmarks based on MSVD</sample>
    <sample id="265">The name of the presenter is Vasudha.</sample>
    <sample id="266">L'articolo è stato scritto da Adam Przepiórkowski.</sample>
    <sample id="268">The most common errors made by PaLM in translation are omission errors, where parts of the source sentence are dropped during translation. Additionally, while PaLM provides fluent output, it still has issues with accuracy, as indicated by the "Style/Awkward" category being lower than for state-of-the-art systems.</sample>
    <sample id="269">Ciao, sono James Finch e Sarah Finch. Oggi parleremo di ABC-Eval, una nuova approccio dimensionale per valutare l'intelligenza artificiale conversazionale. Questo lavoro è stato realizzato dalla Lab di NLP di Emory, guidata dal professore Jinho Choi all'Università di Emory, in collaborazione con Amazon Alexa AI. Immaginiamo che abbiate sviluppato un modello di dialogo e vogliate vedere come si confronta con lo stato dell'arte. La pratica comune è utilizzare l'evaluazione umana, come chiedere ai giudici umani di scegliere quale delle due conversazioni è migliore o di dare una valutazione su scala Likert. Questi approcci funzionano bene per fornire valutazioni holistiche della qualità del dialogo, ma la qualità del dialogo ha molteplici aspetti. Potreste quindi voler valutare più dimensioni del dialogo per comprendere i punti di forza e le debolezze del modello a un livello più fino. Un approccio è semplicemente chiedere ai giudici umani che valutino diverse dimensioni del dialogo di qualità, come la rilevanza delle risposte del modello utilizzando metodi esistenti comparative o scala Likert. Tuttavia, crediamo che ci sia una strategia più precisa e affidabile per l'evaluazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività dell'evaluazione umana esplicitando se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso o il suo partner. Chiamiamo questa approccio annotazione dei comportamenti nel chat o ABC-Eval in breve. Sviluppiamo questo metodo per coprire completamente i comportamenti del modello di chat che sono stati suggeriti come influenzare la qualità del dialogo nella letteratura recente. ABC-Eval è capace di misurare le tasse in cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante, contraddice se stesso o il suo partner, hallucina informazioni incorrecte o viola la conoscenza comune, e quando il modello riuscisce o fallisce a mostrare empatia. Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat di stato dell'arte e li abbiamo valutati su 100 conversazioni umano-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronto di dialoghi a livello di coppie. Per ciascun metodo esistente, abbiamo raccolto valutazioni su otto degli aspetti più comuni misurati del dialogo, poiché è la pratica standard per valutare i modelli di chat lungo più dimensioni. Dalla nostra analisi dei risultati di queste valutazioni, abbiamo trovato che i label comportamentali di ABC-Eval sono in generale più affidabili che i label raccolti da metodi esistenti, misurati dall'accordo interannotatori su 100 conversazioni doppialmente etichettate. Inoltre, i label di ABC-Eval sono più predittivi della qualità complessiva del dialogo rispetto ai metriche prodotte da metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, puoi vedere come misurare la proporzione di turni con contraddizioni di se stesso e del partner spiega il 5% e il 10% della qualità del dialogo, rispettivamente, mentre le medie di coerenza Likert spiegano solo il 4% o meno. Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità del dialogo utilizzando un'analisi di regressione lineare passo-passo. Puoi vedere come la combinazione di tutti i metrici ABC-Eval spiega oltre il 25% della qualità del dialogo, e come rimuovere i metrici uno alla volta, la maggior parte dei risultati in perd</sample>
    <sample id="270">The authors of the article, James Finch and Sarah Finch, are affiliated with the Emory NLP Lab led by Professor Jinho Cho at Emory University, in collaboration with Amazon Alexa AI.</sample>
    <sample id="271">In questo articolo, "CFT" sta per "Continuous Fine-Tuning". Questo è un approccio di apprendimento dove il modello viene continuamente raffinato utilizzando dati di validazione puliti. L'articolo sottolinea che il continuo raffinamento su dati puliti può migliorare significativamente le prestazioni dei modelli di apprendimento debolemente supervisionato, rendendo meno necessario l'uso di metodi più complessi che richiedono più risorse computazionali.</sample>
    <sample id="272">Ci sono sette autori coinvolti nell'articolo: Koustav Sinha, John Gauthier, Aaron Mueller, Kishka Misra, Karen Fences, Roger Lévy, e Adina Williams.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e sarò qui a presentare il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un'esplorazione datadriven, multilingue". Questo lavoro è stato fatto in collaborazione con Patrick Fernandes, Emmy Liu, André Fernandes Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurrebbe "mole" in questa frase? Se la frase precedente fosse "Le cose potrebbero iniziare a diventare pericolose se i ministri scoprono", allora "mole" si riferisce a uno spion. Ma se la frase precedente fosse "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisse a un marcio. Quindi, a seconda del contesto, il significato della parola cambia e quindi la traduzione cambia anche. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche di corpus come BLEU incapaci di catturare queste traduzioni. E alcuni hanno suggerito di valutare in modo mirato le traduzioni dipendenti dal contesto, ma questi risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati poiché dipendono generalmente da conoscenze del dominio e da curazione umana. In questo lavoro, cerciamo di rispondere a queste due domande. Prima, quando la traduzione richiede contesti? E secondo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato a misurare quanto un'unità linguistica dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come una misura per l'uso del contesto da parte dei modelli di traduzione. Questo viene fatto misurando quanto informazione il contesto C fornisce sull'unità target Y, data la fonte X. Puoi immaginare CXMI come l'informazione ottenuta dall'inclusione del contesto nella modellazione. In questo lavoro, estendiamo CXMI a CXMI punto per misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare che le parole con un alto P-CXMI siano quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con un alto P-CX</sample>
    <sample id="274">The name of the presenter is Yusen Zhang.</sample>
    <sample id="276">In this presentation, Ananya and Vignesh discuss their work on "IndicMT Eval: A dataset to meta-evaluate machine translation metrics for Indian languages." They highlight the understudied evaluation of translations in the reverse direction and aim to fill this gap by focusing on five Indian languages: Tamil, Malayalam (Dravidian), and Hindi, Marathi, and Gujarathi (Indo-Aryan). They use the Flores dataset to select 200 sentences and generate 1,400 candidate translations using seven different translation models. Human annotators evaluate these translations, marking errors by type and severity, and providing an overall score. The study compares various metrics, including overlap-based, embedding-based, and COMET-metric variants, and finds that COMET-metric variants have the highest overall correlation with human scores. They also fine-tune the best-performing metric, COMET, using their MQM dataset, resulting in improved correlation values. The study concludes with the evaluation of IndicCOMET MQM on other unseen languages and robustness scores on the ACES Translation Accuracy Challenge. The dataset and findings are publicly available for further research.</sample>
    <sample id="277">Il nuovo metodo è chiamato "Compositional Generalization without Trees using MultiSet Tagging and Latent Permutations".</sample>
    <sample id="278">Il metodo delle "parole contrassegnate" (Marked Words) è stato sviluppato per identificare le parole che distingue gruppi marcati dai gruppi non marcati. Questo metodo si basa sull'idea sociolinguistica del "markedness", che sostiene che ci sono gruppi con un comportamento predefinito (non marcato) e altri che differiscono da questo comportamento (marcati). Per esempio, il termine "guerriero" è generalmente associato agli uomini, quindi quando si descrive una guerriera femminile, si specifica "guerriera" e si marca il termine con "femmina". In questo modo, i gruppi dominanti sono generalmente non marcati, mentre i gruppi marginalizzati sono marcati. Il metodo delle "parole contrasseignate" confronta le parole più frequenti per ciascun gruppo marcati con i gruppi non marcati utilizzando metodi di analisi statistica, come i rapporti di log-odds ponderati, per distinguere le parole che distingue i gruppi marcati dai gruppi no marcati. Questo aiuta a identificare specifiche stereotipi e modelli senza dover dipendere da un elenco di stereotipi predefinito.</sample>
    <sample id="279">The authors of the article "From Pretraining Data to Language Models to Downsstream Tasks: Tracking the Trails of Political Biasses Leading to Unfair NLP Models" are Shangbin and their colleagues at the University of Washington.</sample>
    <sample id="280">Emotion regulation in conversations involves predicting the emotion label of each utterance, which contains textual, audio, and visual modalities. Despite numerous methods, challenges remain in exploiting multimodal information, addressing minority and semantically similar emotions, and distinguishing between them. This paper introduces MultiEMO, a novel attention-based correlation-aware multimodal fusion (MMF) framework. MultiEMO consists of unimodal feature extraction, context modeling, multimodal fusion (MultiAttn), and emotion classification. Key contributions include VisExtNet, a visual feature extractor that captures facial expressions without redundant scene information; MultiAttn, a multimodal fusion network using bidirectional multi-head cross-attention; and Sample-Weighted Focal Contrastive Loss, addressing minority and semantically similar emotions. Experiments on MELD and IEMOCAP datasets show state-of-the-art performance, particularly in minority and semantically similar emotions. However, limitations include VisExtNet's inability to distinguish speakers and the need for a large batch size for SWFC loss.</sample>
    <sample id="281">Kayo Yin and colleagues present a study on the role of context in translation, focusing on how much a word depends on context during translation and how well models handle these cases. They introduce CXMI, a measure for context usage by machine translation models, and extend it to Pointwise CXMI for sentence or word-level analysis. The study analyzes TED talk transcripts translated into 14 languages, identifying patterns in part-of-speech tags, vocabulary items, and individual tokens with high P-CXMI. They design a benchmark, the Multilingual Discourse-Aware (MuDA) tagger, to automatically identify context-dependent examples in parallel corpora. The MuDA benchmark reveals that context-aware models perform better for certain discourse phenomena like formality and lexical cohesion, but not as well for others like ellipsis, pronouns, and verb form, indicating areas for improvement. The study also compares commercial systems, finding that DeepL is generally more accurate than Google Translate for document-level translation, highlighting the importance of context in achieving high-quality translations.</sample>
    <sample id="282">Hello everyone, I'm Xuekai Zhu, and today we're excited to present our new work at ACL 
2023, "StoryTrans: 
Non-Parallel Story Author-Style Transfer with 
Discourse Representations and Content Enhancing". 

Our research addresses the important task of non-parallel text style transfer at the story level, focusing on discourse-level imitation of author style. The main challenge lies in capturing the complex linguistic preferences of authors, such as narrative techniques and writing topics, which are highly associated with specific styles.

To tackle these challenges, we propose StoryTrans, a generation model that learns discourse representations from source texts and combines them with learnable style embeddings. Our model uses a new training objective to reduce stylistic features and enhance content preservation. We separate the generation into two stages: first, transferring the source text with style-specific content masked, and then generating the whole text by incorporating these keywords explicitly.

We conducted extensive experiments on Chinese and English datasets, transferring fairytales or everyday stories to typical author 
styles. Both automatic and manual evaluations confirm the efficiency of our model, showing superior performance in style control and content preservation. Style visualization indicates that our model aligns with the golden text in the style feature space, and StoryTrans can enrich the storyline and maintain the main contents.

Finally, our data and code are included in this repository. If you have any questions, please feel free to email me.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata nel discorso è quella che include il nome della citta "Lisa". Questa struttura è simmetrica perché entrambe le congiunzioni ("Lisa" e "Bart") sono considerate come nodi principali della struttura di dipendenza, senza una congiunzione che sia specificamente identificata come "head" della struttura coordinata.</sample>
    <sample id="284">Hello everyone, I'm Peng Tianshuo from Wuhan. Today, I will present my paper titled "FSUIE: A Novel Fuzz Span Mechanism for Enhancing Universal Information Extraction" at ACL's Main Conference 4,911.

Current span-based UIE models rely heavily on precise boundary positions, which can be ambiguous. We propose a fuzzy span mechanism where the span boundary is learned as a continuous distribution of correct probability, rather than a precise value. This addresses the mismatch between transformer feature extraction and information extraction, as transformers focus on global features and ignore the limited length of spans.

We introduce a fuzzy span attention mask function, which dynamically adjusts the attention span and linearly decays the attention distribution at the boundary. This allows the model to focus on semantic information within a limited range of preceding and following tokens.

We conducted experiments on three main information extraction tasks: named entity recognition, relationship extraction, and aspect-sentiment triplet extraction. Our FSUIE model achieved significant performance improvements compared to UIE models without a fuzzy span mechanism, especially on small-scale data.

In relationship extraction, FSUIE achieved new state-of-the-art results on datasets ACE2004, ACE2005, and ADE. It also demonstrated strong generalization capabilities for domain-specific information.

Our ablation study showed that the fuzzy span loss and attention mechanism significantly improved the model's performance. The attention distribution visualization confirmed that the module focused on relevant semantic information.

In conclusion, we proposed a novel fuzzy span loss and attention mechanism that enhances universal information extraction. Our FSUIE model achieved excellent results across a wide range of IE tasks. Thank you.</sample>
    <sample id="285">Hello everyone, I'm Mingqi Gao from Peking University, and I'm excited to share our work, "Reference Matters: Factuality Error Correction for Dialogue Summarization with a Fine-grained Evaluation Framework." Our research focuses on the issue of factual errors in dialogue summarization, which are still prevalent in models and even some reference summaries.

There are two main approaches to address this issue: introducing factuality-related objectives in the training or inference processes to make summarization models more faithful, or designing a Factual Error Correction (FEC) model that takes the source document and model-generated summary as inputs and outputs a corrected summary.

However, we found that the current evaluation methods for FEC models are flawed. Factuality metrics like FactCC and DAE give an overall score, which is vague and may not be reliable on its own. Additionally, this evaluation blurs the line between the content-based and form-based solutions.

To address these issues, we propose introducing manually annotated reference corrections to provide more valuable data for training FEC models and create a more comprehensive and accurate evaluation of their performance. We also propose a new taxonomy of factual errors, which includes content-based and form-based categories.

Our evaluation framework, based on ERRANT, consists of three steps: alignment, classification,</sample>
    <sample id="286">The name of the relator is James Finch.</sample>
    <sample id="287">Quattro autori sono coinvolti nell’articolo: Javad Hosseini, Filip Radlinski, Silvia Paret, e Annie Louis.</sample>
    <sample id="288">In the paper, the authors discuss using several datasets to test the sensitivity of language models to syntactic and semantic features. Specifically, they mention the following datasets:

1. **BLiMP (Bilingual Linguistic Input and Output)**: This dataset is used to test grammaticality and acceptability judgments. The authors use the Adjunct Island case from BLiMP to create longer sequences by adding prefixes to acceptable and unacceptable sentences.

2. **SyntaxGym**: This dataset is also used to evaluate grammaticality and acceptability. The authors use it to create longer sequences by adding prefixes to sentences from the same or different subsets.

3. **CrowS pairs**: This dataset is used to test acceptability judgments in terms of stereotypes. The authors use it to create longer sequences and evaluate the model's sensitivity to context.

4. **Wikipedia**: This dataset is used to test the robustness of the model's acceptability judgments to completely unrelated context. The authors use sentences from Wikipedia to create longer sequences and evaluate the model's performance.

These datasets are used to simulate longer sequences and test the model's sensitivity to context, structure, and content.</sample>
    <sample id="290">Le abbreviazioni dei cinque metodi per il primo problema di ricerca sono:

1. WSL (Weakly Supervised Learning)
2. COSINE (Cosine Similarity-based method)
3. FTw (Fine-Tuning with clean validation samples)
4. WSL+FTw (WSL with fine-tuning on clean validation samples)
5. WSL+FTw+ (WSL with fine-tuning on clean validation and additional data)</sample>
    <sample id="291">Il modello viene valutato su attività di processing del linguaggio naturale (NLP) specifiche per il settore sanitario, come la riconoscenza nomale di entità (NER), la classificazione, l'analisi sintattica (part-of-speech tagging) e la risposta alle domande (question answering).</sample>
    <sample id="294">CamemBERT è inizialmente addestrato su un dataset di 138 GB chiamato OSCAR.</sample>
    <sample id="295">Il nome del relatore è Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presents a collaborative work between the University of Turin and Amazon Alexa, focusing on Natural Language Understanding (NLU) and Natural Language Processing (NLP). NLU relies heavily on supervised machine learning and large datasets of manually annotated data. However, the assumption that there is a single truth (ground truth) in annotations is showing limitations. Basile's team chose to study irony, a complex and pragmatic phenomenon in language.

To investigate this, they developed the EPIC corpus, an English Perspectivist Irony Corpus, collecting data from social media, Reddit, and Twitter over 1.5 years. The corpus includes about 300 short conversations in five English varieties, annotated by 74 annotators using Prolific. Each annotator reviewed 200 texts, with extra questions for quality control.

The team observed differences in inter-annotator agreement across various dimensions, such as gender, age, and nationality. They built perspective-aware models, fine-tuning pre-trained language models on splits of the dataset based on annotator characteristics. While raw performance showed no trends, perspective-aware models demonstrated significantly higher confidence in predictions.

Further analysis revealed that age groups close to each other had more disagreement in perceiving irony, and geographical differences were notable between annotators from the UK and Ireland. Basile concludes by inviting questions and discussions at the poster session.</sample>
    <sample id="297">In this talk, the speaker discusses their work on "From Dogwhistles to Bullhorns," which explores the use of coded rhetoric in political speech. They explain that dogwhistles are terms that convey a hidden message to a specific group, often with a taboo or controversial meaning. The speaker uses an example from Senator Josh Hawley's speech, where the term "cosmopolitan" is used as a dogwhistle against Jewish people. They argue that understanding dogwhistles is important for NLP and linguistics, as it challenges our understanding of meaning and has implications for political influence and content moderation. The speaker presents a typology and glossary of over 340 dogwhistle terms, categorizing them by register, type, and persona. They also conduct a case study of historical U.S. political speech, finding a correlation between the frequency of racial dogwhistles and the Republican Southern Strategy. The speaker then evaluates dogwhistle recognition in language models, finding that GPT-3 can surface many dogwhistles, but with varying success depending on the prompt and type of dogwhistle. Finally, they show how dogwhistles can evade content moderation, using a case study of toxicity detection with Prospective API and hateful template</sample>
    <sample id="298">I risultati che hanno portato alla conclusione che la perdita di prestazioni è principalmente causata dalla deriva temporale sono stati ottenuti attraverso un'esperienza di ritrainamento o continuare a pre-addestrare alcuni modelli con dati più recenti. Questo ha mostrato che la performance degrada con un maggiore spazio temporale tra i dati di addestramento e i dati di test, confermando l'ipotesi che la causa principale della perdita delle prestazioni è la deriva temporale.</sample>
    <sample id="299">In this presentation, Michalis Korakakis and Andreas Vlachos discuss their work on improving the robustness of Natural Language Inference (NLI) models through a training method called minimax training. NLI models have achieved state-of-the art results but often rely on shortcuts, which are spurious correlations in the data. These shortcuts can make models brittle when tested on out-of-distribution adversarial examples. The current methods for shortcut mitigation require prior knowledge of shortcuts and often use a pre-trained language model as an auxiliary, which adds computational overhead.

The proposed method focuses on reducing the reliance of NLI models on shortcuts by emphasizing under-represented hard examples during training. The learner model tries to minimize the NLI task loss, while the auxiliary model maximizes the learner's loss by generating example weights. This encourages the learner to focus on hard examples that counteract shortcuts. The method does not assume knowledge of shortcuts and uses a feed-forward network as the auxiliary.

The authors evaluate their method on three commonly used datasets and their corresponding out-of-distribution adversarial test sets, showing consistent improvements in out-of-distribution performance while maintaining high in-domain accuracy. They also examine the effect of pre-training the learner, the size of the auxiliary, and conduct a qualitative evaluation of the learned example weight. The work is presented at a poster session for further discussion.</sample>
    <sample id="300">Belinda introduces the concept of interactive dictation, a process where users can dictate and edit documents using their voice in a natural and intuitive manner. Unlike traditional speech-to-text systems, interactive dictation allows for flexible interleaving of dictation and editing without the need for trigger words. The task involves four steps: ASR recognition, segmentation of dictation and commands, extraction and normalization of commands, and execution of each utterance in sequence. Belinda discusses the design of a data collection interface and the creation of a baseline system for interactive dictation. The baseline system includes separate models for ASR recognition, command extraction, and interpretation, with experiments using T5 and GPT-3 models. The results show a trade-off between runtime and accuracy, with GPT-3 models being more accurate but slower, and predicting state directly being more accurate than predicting intermediate programs. Belinda concludes by mentioning the availability of code and the paper for further details.</sample>
    <sample id="302">Necessitiamo di permutare i token per la sequenza di uscita perché, dopo aver assegnato ogni token di input a un multiset di token di output, non siamo certi di quali token di output dobbiamo mettere in qualsiasi posizione. La permutazione ci permette di determinare l'ordine corretto dei token di output in base ai multiset assegnati, garantendo che la sequenza di uscita sia una rappresentazione corretta e coerente con il significato dell'input.</sample>
    <sample id="303">Gli autori hanno suggerito ai proprietai dei modelli di aumentare la trasparente sui metodi di mitigazione dei biases perché non è chiaro se questi metodi stanno causando positivi stereotipi o altre forme di pericolosi. Senza maggiore trasparenza, è difficile studiare e comprendere meglio questi effetti, quindi è importante avere informazioni chiare sui metodi utilizzati per mitigare i bias.</sample>
    <sample id="304">In the context of minimal pair paradigms, the input inaccettabili are sentences that are grammatically incorrect or semantically inappropriate, which are used to test the language model's ability to distinguish between acceptable and unacceptable sentences. These pairs are designed to highlight the model's sensitivity to subtle differences in meaning and structure.</sample>
    <sample id="305">Dawei, a PhD student at Saarland</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim discuss their research on entity tracking in language models. They argue that understanding entity states is crucial for comprehending longer discourses, but there has been little systematic investigation into what pre-trained language models can do. They designed an evaluation task involving boxes and objects to test entity tracking abilities, ensuring that models cannot use shortcuts like memorization or simple associations. They tested the task with Flan-T5 and GPT-3 models using 2-shot in-context learning. The results showed that most models simply repeated the initial state, while only text-davinci-003 exhibited non-trivial tracking. They found that pre-training on code is responsible for making this capacity surface in pre-trained models. Smaller models like T5-base can learn to perform tracking if fine-tuned, but randomly initialized models of the same architecture cannot. The authors suggest that their findings may generalize beyond their setup, and they have more results and analysis in their paper.</sample>
    <sample id="307">The authors used several metrics to evaluate their models, including:

1. **Named Entity Recognition (NER)**: They measured the performance of their models on tasks such as identifying medical entities like diseases, medications, and symptoms.

2. **Classification**: They evaluated the models' ability to classify text into different categories, such as identifying whether a text is a medical report or a research paper.

3. **Part-of-Speech (POS) Tagging**: They assessed the models' ability to identify the grammatical parts of speech in French text.

4. **Question Answering (QA)**: They tested the models' ability to answer questions based on the text they were trained on.

5. **Comparison to Baseline Models**: They compared their models to six baseline models: CamemBERT OSCAR 138GB, CamemBERT OSCAR 4GB, CamemBERT CCNET 4GB, PubMedBERT, BioBERT, and ClinicalBER.

6. **Performance on Heterogeneous Data**: They evaluated the models' performance on data from heterogeneous sources, such as NACHOS and clinical notes.

7. **Performance with More Data**: They measured the impact of using more data on the models' performance.

8. **Stability Issues**: They identified stability issues with models based on CamemBERT weights and tokenizer.

Overall, the authors used a combination of these metrics to evaluate the performance of their models and compare them to baseline models.</sample>
    <sample id="308">Hello everyone, I'm Jenny, a first-year PhD student at Carnegie Mellon University. Today, I'll be presenting our work on NLPositionality, which characterizes design biases of datasets and models. This research was conducted in collaboration with the University of Washington and the Allen Institute for Artificial Intelligence, involving Sebastian Santy, Ronan Le Bras, Katrin Reinecke, and Maarten Sap.

Imagine you're a newspaper editor trying to remove toxic content from comments under your news article. You might use a popular API like Prospective API for toxicity</sample>
    <sample id="309">L'accordo tra annotatori è stato misurato utilizzando l'inter-annotator agreement su 100 conversazioni doppiamente etichettate. Questo metodo permette di valutare la consistenza tra le annotazioni fornite da diversi esperti.</sample>
    <sample id="310">Il dominio scelto per aggiungere frase completamente scollegate alle query inaccette e accettabili è Wikipedia. Questo viene utilizzato per testare se le giudizi di accettabilità del modello sono influenzati da contesti completamente irrilevanti rispetto alla frase che stiamo valutando.</sample>
    <sample id="311">I miei migliori sforzi per rispondere alla tua domanda in modo sintetico sono stati limitati dalla mia capacità di fornire informazioni specifiche sui migliori sforzi degli autori dell'articolo. Tuttavia, posso fornire una sintesi delle informazioni principali trattate nel documento.

Il documento presenta una descrizione del corpus DEPLAIN, che è stato creato per migliorare la comprensione del testo in lingua tedesca per gruppi specifici, come persone con problemi di lettura o non-native speakers. Il corpus è stato creato in due parti: DEPLAIN-apa, basato su testi di notizie, e DEPLAIN-web, che include testi di diverse aree e utilizza metodi di allineamento automatico.

Il primo uso del corpus è l'evaluazione di metodi di allineamento automatico, utilizzando le frasi manualmente allineate come standard di riferimento. Il secondo uso è l'automatica semplificazione del testo, utilizzando modelli di linguaggio pre-addestrati per produrre testi semplificati da testi complessi.

Per ulteriori informazioni, ti consiglio di leggere il documento completo, che può essere trovato in un database di ricerche o in un sito web specifico.</sample>
    <sample id="312">MultiInstruct è un dataset di benchmark per l'instruction tuning multi-modal che differisce dagli altri parametri per diverse ragioni:

1. **Diversità di Task**: Il dataset include 62 task multi-modal diversi, coprendo 10 categorie ampie, derivati da 21 dataset esistenti open-source. Questo è un numero significativamente maggiore rispetto ai dataset multi-modal esistenti.

2. **Disponibilità di Instruction**: Mentre esistono più di 1600 task di instruction per NLP, non esiste un dataset multi-modal di grande scala pubblicamente disponibile. MultiInstruct è il primo dataset multi-modal che affronta questa lacuna.

3. **Unificazione dei Dati**: Il dataset utilizza un vocabolario unificato per il linguaggio, le immagini e le coordinate di un bounding box, permettendo una trasformazione uniforme dei dati di input e output.

4. **Approccio di Formulazione**: Le task sono formulate in formato sequenza-a-sequenza, con input testo, immagini, istruzioni e bounding box rappresentati in lo stesso spazio di token.

5. **Metriche di Valutazione**: Oltre al performance standard (accuracy per classificazione multi-modal e Rouge-L per generazione multi-modal), viene introdotto un nuovo metodo di valutazione chiamato "sensitivity", che misura la capacità del modello di produrre output coerenti per lo stesso task, indipendentemente dalla variazione nella formulazione dell'istruzione.

6. **Transfer Learning**: Il dataset include un'analisi del beneficio del transfer learning da dataset di istruzioni naturali, mostrando che il modello può migliorare la sua performance e la sua sensibilità.

In sintesi, MultiInstruct è un dataset innovativo che introduce una nuova dimensione alla ricerca di instruction tuning multi-modal, con un focus su diversità, disponibilità e metodologie di valutazione.</sample>
    <sample id="313">There are two authors involved in the article: James Finch and Sarah Finch.</sample>
    <sample id="314">La coordinazione binaria si riferisce alla struttura in cui due elementi (congiunti) sono legati tra loro in modo simmetrico, senza un elemento predominante o "capo" rispetto all'altro. In altre parole, entrambi gli elementi sono considerati ugualmente importanti e possono essere posizionati in qualsiasi ordine senza alterare il significato complessivo della frase.</sample>
    <sample id="315">In questo studio, i prompt sono stati utilizzati per generare le persone marcate, che sono le descrizioni di individui immaginari basate su identificatori di demografia. Questo processo è stato eseguito in modo iterativo e non si è specificato un tempo medio per ciascun prompt, ma il metodo è stato applicato a diverse domande e identificatori di demografia per analizzare le stereotipi presenti nei modelli linguistici.</sample>
    <sample id="316">The results indicate that T5 fine-tuned on CoScript generates scripts of higher quality than most large language models. This suggests that smaller models can surpass larger models when properly tuned on suitable datasets.</sample>
    <sample id="317">Hello everyone, I'm Peng Li from Fudan University, and I'm excited to present our work titled "CodeIE: Leveraging Large Code Generation Models for Improved Few-Shot Information Extraction." Information extraction, a fundamental task in natural language processing, involves extracting structured information from unstructured text. Common tasks include named entity recognition (NER) and relation extraction (RE). For instance, in the sentence "Steve became CEO of Apple in 1898," the model should identify "Steve" as a person's name and "Apple" as an organization's name.

Traditional approaches use pre-trained language models like T5 and GPT, which operate in a text-to-text manner during pre-training. However, during inference, the structured output is linearized into a plan sequence, leading to mismatched outputs between plain text and structured formats. This requires extensive structured training data and special decoding strategies.

To address this, we propose CodeIE, transforming the text-based information extraction task into a structure-to-structure coding task using large code generation models like Codex. This ensures aligned structures in the output. For NER, we designed prompts that define a function for named entity recognition, extract entities, and append them to an entity list.

We evaluated our method on three NER datasets and four RE datasets, comparing traditional text-style prompts with our code-style prompts. Our results showed that the code-style prompts significantly outperformed traditional models like UIE and GPT-3. We found that code pre-training aligns better with the information extraction task, reducing structural errors and improving recall.

In summary, our approach using code language models and code format prompts offers a promising solution for information extraction tasks, outperforming traditional models in both accuracy and recall. We hope our findings inspire further research in this area. Thank you for your attention, and I welcome any questions.</sample>
    <sample id="318">Ciao, sono Yanis Labrak e presento i nostri lavori su "DrBERT: Un Modello Robusto Pre-addestrato in Francese per i Domini Biomedici e Clinici." In questa presentazione, parleremo prima di modellazione linguistica nella sanità. Successivamente, parleremo della principale contributo del nostro articolo. Introduciamo il primo modello biomedico in Francese chiamato DrBERT, che si basa su RoBERTa e è stato addestrato su NACHOS, un dataset di dati medici raccolti dal web. Abbiamo anche presentato una comparazione di modelli con diverse impostazioni di pre-addestramento e fonti di dati. Poi, presenteremo i nostri risultati su 11 compiti di downstream biomedici e clinici in Francese. Infine, concluderemo con un'analisi delle esperimenti e forniremo ulteriori dettagli su come accedere a questi modelli.

Dopo l'introduzione di BERT nel 2018, è diventato uno dei metodi più efficaci per risolvere le attività di elaborazione del linguaggio naturale e offre enormi miglioramenti rispetto ai metodi statici e contestualizzati storici come Word2vec, fastText o altri. Da allora, questo modello è stato adattato a molti altri linguaggi, come in Francese con CamemBERT, e anche in domini come biomedico con PubMedBERT e BioBERT e clinico con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altri linguaggi sono rari e spesso basati su pre-addestramento continuo a causa della mancanza di dati in ambito. Tuttavia, la Francia non aveva modelli aperti e gratuiti per il biomedico fino a questo punto. Quindi, chiediamo a noi stessi una domanda: qual è il sorgente di dati più appropriato per un uso ampio e se i dati raccolti dal web sono una buona sostituzione per i dati clinici.

Per rispondere a questa domanda, abbiamo confrontato DrBERT con il nostro modello ChuBERT, che si basa su dati anonimizzati ottenuti dal deposito di dati dell'Ospedale Universitario di Nantes. Successivamente, chiediamo quanto dati abbiamo bisogno per addestrare un modello specializzato su dati in Francese: è 4 GB, 8 GB o di più? Per rispondere a questa domanda, addestrammo e confrontiamo quattro modelli da zero: una prima versione di DrBERT, con 7 GB di NACHOS; una seconda versione di 4 GB di un set di NACHOS; una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi prelevate da note cliniche; e una versione finale di ChuBERT con una miscelta di 4 GB di un set di NCHOS e 4 GB di note cliniche. Oltre a questa comparazione, abbiamo introdotto tre modelli addestrati su pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso di CamemBERT addestrato su un set di 4 GB di NACHOS. Un altro basato anche su CamemBERT, ma addestrato questa volta su 4 GB di note cliniche e infine, uno basato su un modello biomedico inglese PubMedBERT, addestrato su 4 GB di un set di NACHOSS. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, abbiamo raccolto dati per compiti di downstream pubblici e privati come riconoscimento nomale, classificazione, tagliamento part-of-speech e domande e risposte. Questi modelli sono stati confrontati con sei modelli di baseline che sono CamemBERT OSCAR 138GB, CamemBERT OSCAR 4GB, CamemBERT CCNET 4GB, PubMedBERT, BioBERT e ClinicalBERT. L'evaluazione evidenzia che i modelli hanno ottenuto i migliori risultati sui compiti con dati della stessa natura su cui i modelli sono stati addestrati. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenei sembrano essere più versatile. Inoltre, osserviamo che utilizzare più dati traduce in prestazioni migliori. In generale, l'addestramento</sample>
    <sample id="319">In the work, the following learning strategies are examined:

1. **From-scratch pre-training**: Training a model from the ground up using a specific dataset.
2. **Continual pre-training**: Using a pre-trained model as a starting point and fine-tuning it on a new dataset.
3. **Control pre-training**: Using the weights and tokenization of a pre-trained model (e.g., CamemBERT) and training it on a new dataset.

The study compares the performance of these strategies on a range of downstream tasks in the French language, using different datasets and model architectures.</sample>
    <sample id="320">Il fattore di overfitting dovuto all'utilizzo ripetuta del test è noto come "adaptive overfitting". In base al contenuto inglese, il grafico mostra che la gradiente della linea di miglioramento (red best fit line) è maggiore di uno, indicando che ogni unità di miglioramento su CoNLL-2003 traduce in più di una unità di miglioramento su CoNN++ (CoNLL++). Questo suggerisce che non ci sia un diminuzione dei rendimenti, quindi l'overfitting adattativo non è osservato in questo caso.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata utilizzando il nostro corpus DEPLAIN, che fornisce coppie parallele di frasi manualmente alignate. Abbiamo analizzato le semplificazioni su diverse livelli, come la semplificazione lessicale, strutturale e complessiva. Inoltre, abbiamo esaminato la varietà delle trasformazioni di semplificazione presenti nel corpus. Abbiamo utilizzato queste informazioni come gold standard per valutare metodi automatici di allineamento e per valutare l'efficacia di modelli di linguaggio finamente addestrati per la semplificazione automatica. Abbiamo pubblicato i risultati delle nostre esperienze e i codici per eseguire gli esperimenti. In sintesi, la qualità della semplificazione è stia stata valutata utilizzando il corpus DEPLAIN e metodi automatici di allineamento, oltre a modelli di linguaggio finamente addestratti.</sample>
    <sample id="322">Enrico is presenting at ACL 23 on the topic of "What does a Text Classifier Learn about Moralities?" He explains that morality is the internal compass that helps us distinguish right from wrong, and it is essential for language models to understand and recognize morality in language. However, morality is subjective and can be interpreted differently by different people. Enrico introduces the Moral Foundation Theory, which suggests that there are five different ways in which humans perceive morality, and each person prioritizes these foundations differently. Enrico and his team have applied explainable AI techniques to language models trained to understand morality in text and have found that language models can understand morality in text. They have also found that morality is expressed differently across different domains, and using a single model for many different domains can lead to misunderstandings of morality. Enrico concludes by saying that language models can recognize that morality can be expressed differently and that it is important to understand these differences.</sample>
    <sample id="323">Hello everyone, I am Yujie Wang from Shanxi, China. The title of my paper is “Dynamic Heterogeneous-Graph Reasoning with Language Models for Commonsense QA.” Commonsense QA is a challenging task that requires understanding language and retrieving relevant knowledge from external sources. Holmes believes that knowledge is stored in both language models and KBs. Many works combine these two types of knowledge for Commonsense QA, but they introduce noisy entities and encode the subgraph and text in isolation, leading limited interaction between the two modalities.

To address these issues, we propose DHLK. We build an HKG based on multiple KBs through a two-stage pruning strategy and KRL. We remove subwords that make up the phrase entity and retrieve paraphrases of key entities in WordNet and Wiktionaries. We encode and fuse QA contexts and entities using RoBERTa and Mask Self-Attention. We dynamically remove entities with weaker relevance to the QA based on the attention weights of RoBERTa. We introduce TransE to optimize the entity and relationship embedding in HKG. We use Relation Mask Self-Attention to model our subgraph. We incorporate the HKG path information into the QA and get the embedding representation of the QA context. We input the HKG graph for embedding, the path and QA context embedded, and the QA context into the MLP to get the answer probability for final prediction.

We conduct experiments on CommonsenseQA and OpenBook QA using external KBs: ConceptNet, WordNet, and Wikitionary. We extract key entities in the QA context based on KEBERT and retrieve knowledge paths within two hops in ConceptNet. We report the results and leaderboards for Commonsense QA and OpenBook QA. Compared with other LM and HKG methods for Commonsense QA, our method gets good results.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diverse. Questo è stato dimostrato attraverso l'analisi dei loro risultati su diverse attività di classificazione, come la rilevazione del discorso offensivo e la rilevazione delle notizie false. I modelli con tendenze politiche diverse hanno risposte diverse a esempi di discorso offensivo e notizie false, indicando che i loro bias politici possono influenzare le loro performance in applicazioni NLP.</sample>
    <sample id="325">Ciao! Mi chiamo Matthias Lindemann e oggi vi darò una breve introduzione alla nostra ricerca su "Compositional Generalization without Trees using Multi-Set Tagging and Latent Permutations". Questo è un lavoro con i miei dottori di ricerca Alexander Koller e Ivan Titov. La generalizzazione composizionale può essere compresa come la capacità di un apprenditore di gestire ricorsione più profonda e composizioni non viste durante l'addestramento. Nel contesto del parsing semantico, il test per la generalizzazione composizionale potrebbe essere questo. Come di solito, abbiamo un set di addestramento di frasi. In questo caso, "La ragazza dormì." E "Mary knew that the girl slept." Queste frasi sono associate a forme logiche che rappresentano gli aspetti core del loro significato. In contrasto con l'evaluazione standard del machine learning, il set di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente non viste durante l'addestramento. In questo esempio, il modello ha visto ricorsione superficiale durante l'addestramento e viene testato su un esempio con ricorsione più profonda. I modelli seq2seq semplici lottano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono distaccati dall'input. In particolare, spesso falliscono a riproduzere le corrispondenze sistematiche tra input e output, come quelle che sono colorate nel caso. Un metodo popolare per affrontare questo è l'integrazione di alberi nei modelli. Gli alberi sono intenzionali per catturare il processo composizionale che collega le frasi con le forme logiche. Questo funziona bene, ma gli alberi non sono generalmente dati e devono essere ottenuti in qualche modo. Questo può essere complicato e talvolta un processo computazionalmente costoso. Di solito, ciò richiede una pre-processing formale specifico dei formi logiche, ad esempio per gestire simboli variabili. Ottenere alberi può anche richiedere procedimenti di induzione grammatica speciali. In questo lavoro, noi non usiamo alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione a ricorsione più profonda senza affidarci a alberi. Il nostro approccio predice l'output dall'input in due passaggi. In primo luogo, taggiamo ogni token di input con un insieme non ordinato di token che appaieranno nell'output. Dopo il primo passaggio, abbiamo tutti i token giusti, ma non ordinati. Questo è perché nel secondo passaggio utilizziamo un altro modello per prevedere una permutazione per metterli nella giusta ordine. Introduciamo un nuovo metodo per prevedere la permutazione che non mette alcune restrizioni rigide sulle permutazioni possibili. Questo rende il nostro approccio molto flessibile e espressivo. Concettualmente, il nostro metodo di permutazione funziona approssimativamente così. Vogliamo andare dalla sinistra alla destra dell'output e determinare quale multiset token mettere in ogni posizione. Per la prima posizione di output, semplicemente selezioniamo uno, come evidenziato in rosso. Poi andiamo alla prossima multiset token per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in un modo simile, andando a un altro multiset token. Continuiamo questo processo fino a quando ogni token dalla prima fase è stato visitato esattamente una volta. Per dare un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri in modo significativo in termini di generalizzazione a ricorsione più profonde. Alcune altre forme di generalizzazione strutturale rimangono comunque molto sfidanti. Nel nostro lavoro, risolviamo alcune sfide tecniche interessanti. In primo luogo, l'allineamento tra input e output non è dato nella fase di addestramento. Di conseguenza, per un dato token non sappiamo da quale multiset proviene, il che rapp</sample>
    <sample id="326">Cognitive dissonance is a psychological phenomenon where an individual experiences discomfort due to holding two or more contradictory beliefs, values, or attitudes simultaneously. For example, a person might believe that smoking is harmful to their health but continue to smoke, creating a state of dissonance between their knowledge and behavior. This inconsistency can lead to mental discomfort and may prompt the individual to change their beliefs or behaviors to reduce the dissonance.</sample>
    <sample id="327">In this paper, we introduce ManagerTower, a novel Vision-Language (VL) model architecture that aims to improve the performance of existing VL models by effectively aggregating insights from pre-trained unimodal experts at different levels of the model. Our approach is based on the two-tower architecture, which consists of a textual encoder, a visual decoder, and a cross-modal encoder. We propose the use of managers in each cross-modal layer to adaptively aggregate insights from pre-trained unimodal experts at various levels, allowing for more comprehensive cross-modal alignment and fusion. Our experiments show that ManagerTower significantly outperforms existing models on various downstream tasks, including the Wikivideo test standard, with only four million images for visual language pre-training. We also demonstrate that our approach is scalable and can be applied to any visual, textual, or cross-modal encoder. We hope that our work can contribute to the development of more effective and efficient VL models.</sample>
    <sample id="328">Based on the presentation, GPT-4 is identified as the most liberal language model among the series. The study also indicates that GPT series models are generally more socially liberal compared to BART series and its variants.</sample>
    <sample id="329">Hello everyone, I'm Minghang Zheng from Peking University, and I'm honored to present our work on "Generating Structured Pseudo Labels for Noise-Resistant Zero-Shot Video Sentence Localization." This project was a collaboration with Shaogang Gong, Hailin Jin Yuxin Peng, and Yang Liu.

Video sentence localization aims to find the most relevant segments in long videos for a given natural language query, with applications in video retrieval, summarization, and more. Traditional methods require extensive manual annotations, which are costly and inefficient.

Our approach addresses three main drawbacks of existing zero-shot methods:
1. Pseudo-queries are often too simple, leading to a large gap between real and generated queries.
2. They can only ensure high relevance within events, not between events and queries.
3. They ignore the risk of label noise.

We propose a noise-resistant Structured Pseudo-Label generation method:
1. Generate complex pseudo-queries using a pre-trained image caption model.
2. Measure relevance between frames and pseudo-queries, generating pseudo-events with high relevance within events and low relevance outside.
3. Reduce noisy samples' influence by re-weighting and refining labels.

We train the model using these pseudo labels, estimating label noise and adjusting weights accordingly. Experiments on ActivityNet Captions and Charades-STA show our method outperforms others on most metrics.

In conclusion, our method generates structured pseudo-labels robust to noise, achieving best zero-shot performance on two datasets. Our code is available via the provided QR code. Thank you.</sample>
    <sample id="330">Yes, in the context of active learning, the "Cumulative" strategy, which accumulates all the data collected from active annotation, performed equal or better than the "Iterative" strategy across the board.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">I dati per il parametro di riferimento MuDA sono stati tratti dai trascritti di conferenze TED tradotti in 14 lingue diverse. Questi trascritti hanno permesso di analizzare le parole e le frasi che richiedono un contesto per essere tradotte correttamente.</sample>
    <sample id="333">Hello everyone, I'm Wenhao from Nanjing University, and it's a great honor to introduce our work, "INK: Inject kNN Knowledge in Nearest Neighbor Machine Translation." In neural machine translation (NMT), the goal is to learn a generalized representation space to adapt diverse scenarios. However, neural networks often create non-smooth representation spaces, limiting their generalization. We observed that low-frequency tokens are sparsely distributed in the NMT model's representation space, leading to poorly defined semantic meanings and poor performance in these areas.

To address this, we propose kNN-MT, which smooths predictions based on nearest neighbors in the representation space. However, this approach is time-consuming and doesn't allow easy updates to representations. To overcome these drawbacks, we introduce INK, a framework that injects kNN knowledge into NMT. INK's training loop has two steps: first, extract kNN knowledge to guide the adapter to adjust representations, then update representations to refresh the datastore asynchronously. This loop runs until convergence.

We optimize the adapter with a combined learning objective and run the training loop until convergence. In our experiments, we used the winner model of the WMT'19 German-English news translation task and found that even this model's representation space can be greatly improved. We explored three questions: whether we can smooth the representation space with a small adapter and use the datastore only during inference, how much improvement kNN knowledge can bring, and whether using an adapter and datastore together can further improve performance.

The results show that INK outperforms the state-of-the-art kNN-Mt system and achieves the best performance after smoothing the space. Refining representation with kNN knowledge brings larger performance improvement compared to using an adapter baseline. Jointly applying an adapter and datastore further smooths predictions, indicating that the representation space of the NMT model can be further refined.

In conclusion, we propose a novel training framework that injects and refines kNN knowledge to iteratively improve the representation space of NMT models. Our INK system achieves an average gain of 0.99 COMET score and 1 BLEU score compared to the state-of-the-art kNN-MT. It also achieves better translation performance with less memory space and faster inference speed.</sample>
    <sample id="335">Il nome del relatore è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è un processo che coinvolge la traduzione di query in più lingue naturali in diverse rappresentazioni significative. Questo è un'area di ricerca che si concentra su come le modelli linguistici possano essere addestrati per comprendere e tradurre query in diverse lingue, utilizzando modelli pre-addestrati in una o più lingue per migliorare le prestazioni in altre lingue. In particolare, il lavoro di Yusen Zhang e colleghi propone XSemPLR, un benchmark unificato per il trasferimento interlinguistica nella semantica, che include dataset in diverse lingue e rappresentazioni significative.</sample>
    <sample id="337">Hello everyone. Today, I'm excited to present our research, "Graph-based Relation Mining for Context-free OOV Word Embedding Learning." OOV words pose a significant challenge in embedding-based models, yet they are crucial for performance. Our approach leverages word formation and association to infer the meaning and representation of OOV words.

We introduce a Word Relationship Graph that mimics lexical rules. When an OOV word appears, it's tokenized into wordpieces and associated with relevant words, forming a two-level graph. Each wordpiece acts as a node, with its embedding as the node attribute. We preserve all nodes in the first layer to retain complete word information.

To assign attributes to OOV nodes, we use a self-attention network based on the characters of the OOV words, followed by two levels of Graph Attention Network to reduce noise. A readout block layer captures the whole graph information and summarizes word formation.

We apply contrastive learning in the loss function with positive samples from the graph, such as two-step relevant neighbors, synonyms, or the OOV word itself, to encourage proximity and push them apart from other samples.

Our experiments show superior performance compared to baselines in both intrinsic and extrinsic tasks. Our model benefits both static and contextual models in downstream tasks.

We also discuss the possibility of adding languages to our model, with agglutinative languages being well-suited and fusional languages presenting more challenges. Our model performs well with English by reasonable word segmentation, and we believe it can handle various complex word formations in other languages.

Thank you for listening.</sample>
    <sample id="338">Good day everyone. My name is Binsheng, and I am here to present our research titled "Are Human Explanations Always Helpful?" This work is a collaborative effort between researchers from Rensselaer Polytechnic Institute, Northeast University, and IBM Research. We aim to address the critical question of how to objectively evaluate the quality of human natural language explanations.

Our research is motivated by the need to assess the utility of human explanations in training models to generate human-understandable explanations and improve their prediction performance and reasoning ability. We propose a unified data structure to convert various tasks into a unified multiple-choice task, which includes baseline and infusion settings. We conduct experiments on five popular large-scale datasets, including CoS-E and ECQA for commonsense QA, e-SNLI for natural language inference (NLI), and ComVE for commonsense validation.

Our findings show that the fine-tuning process does not teach the model with new knowledge conveyed in the explanations, while fine-tuning with infusion actually teaches the model to depend on the explanation part of the input to predict better. We propose a novel evaluation metric called TREU, an extension of the simulatability score, which evaluates the helpfulness of explanations at fine-tuning. Our evaluation demonstrates that our metric outperforms the simulatability score for this purpose.

In summary, our work lays the foundation for high-quality human annotation jobs and recommends researchers perform similar quality checks in the future. Thank you for your attention.</sample>
    <sample id="339">The authors of the article "Weaker Than You Think: A Critical View on Weakly Supervised Learning" are affiliated with Saarland University in Germany. The specific affiliations are:

- Dawei
- Xiaoyu Shen
- Marius Mosbach
- Andreas Stephan
- Dietrich Klakow

These authors are all part of the research team at Saarland University.</sample>
    <sample id="340">Hello everyone, I'm Kuan-Hao Huang from UCLA. I am presenting our work, "ParaAMR: Constructing a Large-Scale, Syntactically Diverse Paraphrase Dataset using AMR Back-Translation." Paraphrase generation is a crucial task in NLP, benefiting applications like question answering and chatbots. Existing datasets like MRPC and PAN are high-quality but limited in scale. Automatically generated datasets, such as back-translation, can produce large volumes of data but lack syntactic diversity.

Our goal is to create a large-scale, syntactically diverse paraphrase corpus. We leverage AMR (Abstract Meaning Representations) graphs, which capture the abstract meaning of sentences. Each node represents a semantic concept, and edges represent semantic relations. We focus on the root node, which represents the main assertion.

Our approach involves using a pre-trained AMR parser to generate AMR graphs for source sentences. We then modify the focus node and edges to create new graphs, which are translated back into text. This process generates paraphrases with similar semantics but different syntax.

ParaAMR contains around 15 million source sentences and 6.9 paraphrases per source sentence, offering more syntactic diversity than other datasets. We conducted automatic and human evaluations, showing that ParaAMR maintains semantic similarity while being more syntactically diverse.

We demonstrated ParaAMR's benefits in several NLP applications. It improves sentence embeddings, enhances syntactic control in paraphrase generation, and aids in data augmentation for few-shot learning. Our dataset is available for further research. Thank you.</sample>
    <sample id="341">The authors of the "Attention as a Guide for Simultaneous Translation" paper use a strategy called EDAtt (Encoder-Decoder Attention) to handle different latency regimes in simultaneous speech translation. They use a single model for each latency regime and adjust the latency through specific parameters. The authors compare their approach with popular strategies like the Wait-k strategy and the Local Agreement, as well as with a state-of-the-art architecture specifically tailored for simultaneous translation. They find that their approach outperforms all these strategies in terms of both translation quality and latency.</sample>
    <sample id="342">Hello everyone, my name is Gao Jingsheng, and I'm presenting our paper, "LiveChat: A Large-scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming." This paper, conducted by me, Lian Yixin, and colleagues from Shanghai Jiao Tong University and Xiaobin.AI, explores the creation of a video-sourced dialogue dataset that closely resembles real spoken conversations.

The paper begins by discussing the limitations of existing large-scale dialogue datasets, which are primarily text-based and rely on manual annotations. We propose LiveChat, a dataset that addresses these limitations by using a unique automatic dialogue-constructing method. LiveChat is video-sourced, larger in scale, and includes personalized dialogue generation.

The dataset is created in three steps: extracting videos from Chinese TikTok, transcribing audio, and collecting audience comments to construct dialogues. We also collect persona information for personalized dialogue generation.

Our experiments show that the extracted persona and longer average sessions are beneficial for the final results. We also investigate the performance of pre-trained dialogue models on LiveChat, finding that BART outperforms other models.

In conclusion, LiveChat is a Chinese video-sourced and personalized dialogue dataset that addresses the limitations of existing datasets. Our experiments demonstrate the advantages of using selected persona profiles and average sessions per persona in learning personalized responses. We also show that BART is a promising model for LiveChat.</sample>
    <sample id="343">Ciao a tutti, sono Akshatha e oggi, con il mio coautore Martin, stiamo presentando il nostro lavoro "Il Test KITMUS: Valutazione dell'Integrazione di Conoscenze da Multiple Fonti." Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale dipendono da una varietà di fonti di conoscenza, come quelle contenute nei loro parametri, generalmente acquisite durante una pretraining, e quelle fornite durante l'inferenza. Le recenti opere su compiti come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante la pretraining per risolvere le attività. Ma la comprensione del linguaggio naturale spesso richiede conoscenze che sono anche fornite durante l'inferenza, come, ad esempio, nella frase "John ha visto il presidente nuovamente eletto su TV." I parametri pre-addestrati possono contenere informazioni su cosa i presidenti fanno e su cosa è una TV, ma non possono affidarsi in modo affidabile a chi è l'entità specifica "John" in questo caso, o chi è il nuovo presidente, perché il presidente potrebbe essere cambiato dalla pretraining. Pertanto, i modelli per compiti di comprensione del linguaggio naturALE intensi richiedono la capacità di integrare e utilizzare entrambe le conoscenze pre-addestrati e di inferenza. In questo lavoro, proponiamo un suite di test diagnostici per l'integrazione di conoscenze. Introduciamo una task di risoluzione del coreferente, progettata per indagare sulla capacità di dipendere da conoscenze disponibili in diverse fonti. Valutiamo il dataset con partecipanti a studi umani e modelli di risoluzione del coreferenti stabiliti. Ecco un esempio dal nostro dataset. Servin è un giudice. Kea è un forno. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidin casi in un tribunale, era felice di rilassarsi. La task qui è identificare l'entità corretta a cui si riferisce il pronome "lui", che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni. Prima, la conoscenza specifica all'entità, come "Servin è un giudice." E secondo, la conoscenza di fondo, come "I giudici decidono casi in tribunali." Generalmente, la conoscenza di fondo è imparata durante la pretraining dei modelli di grandi dimensioni del linguaggio, mentre la conoscenza specifica all'entita è tipicamente osservata durante l'inferenza. Modificiamo la disponibilità di queste due informazioni in modo che possano essere trovate in una sola fonte, o in più fonti. Abbiamo definito tre configurazioni di KITMUS. Prima, abbiamo la configurazione tipica: "Fondazione-Pretrain", dove la conoscenza di fondo è presunta disponibile a pretrain time. Poi, c'è una configurazione "Fondazione-Both", dove la conoscenza di fondo e specifica è disponibile sia a pretrain time che a inferenza time. Infine, la configurazione "Fondazione-Inferenza", dove entrambe le conoscenze sono disponibili solo a inferenza time. Questa ultima configurazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di fondo necessaria per risolvere una attività non è parte dei dati di pretraining dei modelli. Ad esempio, perché nuovi lavori sono stati sviluppati dalla data della pretraining. Ecco come controlliamo la disponibilità dei fatti nelle veri fonti. Nella configurazione "Fondazione-Pretrain", assumiamo che la conoscenza di fondo "I politici cercano seggi elettorali in governo" sia contenuta nei param</sample>
    <sample id="344">I metodi basati su alberi, come quelli utilizzati in alcuni modelli di elaborazione del linguaggio naturale, hanno alcuni svantaggi:

1. **Preprocessing Complesso**: Ottenere alberi richiede una pre-processing significativo e spesso computazionalmente costoso. Questo può includere la gestione di simboli variabili e l'induzione di grammatiche specifiche.

2. **Dipendenza dai Dati di Addestramento**: Gli alberi sono spesso forniti come parte del training, quindi non sono generalizzabili a dati di test che non sono presenti nel training.

3. **Limitazioni nella Generalizzazione**: Anche se gli alberi possono migliorare la generalizzazione a certi livelli di ricorsione, possono non essere efficaci per strutture più profonde o per composizioni non viste durante l'addestramento.

4. **Complessità Computazionale**: La costruzione e l'uso degli alberi possono essere computazionalmente intensivi, rendendo questi metodi meno pratici per applicazioni a grande scala.

5. **Risorse di Memoria**: Gli alberi possono richiedere una quantità significativa di memoria, il che può essere un problema per modelli con grandi dimensioni.

6. **Limitazioni nella Flessibilità**: Gli alberi possono essere rigidi e meno flessibili rispetto a modelli basati su sequenze, che possono adattarsi meglio a diverse strutture e composizioni.

Questi svantaggi spingono a cercare alternative più flessibili e generalizzabili, come quelle proposte nel lavoro di Matthias Lindemann e coautori, che evitano l'uso degli alberi e utilizzano metodi di taggamento multiset e permutazioni latenti per migliorare la generalizzazione a composizioni più profonde.</sample>
    <sample id="345">In this paper, we introduce a novel approach to compositional generalization in semantic parsing without relying on trees. Our method, called Multiset Tagging and Latent Permutuations, directly models the correspondences between input and output fragments. We first tag each input token with an unordered multset of tokens that will appear in the output. Then, we use another model to predict a permutation to order the tokens correctly. Our approach outperforms existing treeless models on the COGS benchmark, particularly in generalization to deeper recursion. We address the challenges of alignment and multiple permutations by inducing the alignment as part of the training and approximating the NP-hard permutation problem with a GPU-friendly continuous relaxation. Our method is flexible and expressive, and we provide a detailed analysis of the experimental results in our paper.</sample>
    <sample id="346">I'm sorry, but I don't have access to the specific affiliations of the authors of the paper you mentioned. If you have the paper or additional information, you might be able to find the authors' affiliations there.</sample>
    <sample id="347">Ciao a tutti, sono Myra e oggi parlerò del nostro lavoro "Marked Personas: Utilizzando Promempi Naturali per Misurare Stereotipi in Modelli di Linguaggio". Questo lavoro è stato condotto in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molte hanno documentato la prevalenza dei social bias e stereotipi nei grandi modelli di linguaggio, o LLM. Tuttavia, questi misuramenti hanno vari limiti. Di solito dipendono da dataset costruiti manualmente che richiedono molto tempo per curare e solitamente misurano stereotipi molto specifici, significando che non generalizzano bene ad altri demografie o contesti, o semplicemente catturano associazioni generali e ampie, come quelle negative con particolari gruppi. Inoltre, la maggior parte del lavoro in questo spazio non tiene conto dell'intersezionalità, che è la nozione che le identità sociali multifattoriali possono compostarsi bias e essere uni luoghi unici di danno. Per superare questi limiti, siamo affidati alla proprietà che questi nuovi modelli di linguaggio adattati alle istruzioni sono molto bravi a rispondere alle istruzioni e ai prompt. Quindi possiamo chiedere al modello di generare un'identità, che è una rappresentazione di un individuo immaginato utilizzando un prompt come "Immagina di essere una donna asiatica. Descriviti te stessa.". E subito vediamo che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità che vogliamo in questo prompt. Qui sono alcuni esempi di generazione da GPT-4. Inmediatamente vediamo che, sebbene le uscite non siano esplicitamente negative o tossiche nel senso tradizionale di queste parole, ci sono alcune interessanti pattern. La donna asiatica è descritta come modesta; la donna medio-orientale viene riferita utilizzando parole come "esotico" e "come", riferendo a una regione affascinante. E entrambe le persone di colore delle persone di género fanno riferimento all'antichità mentre il persona di uomo bianco non ha nulla di tale sorta. Per catturare questi pattern, il nostro metodo ha due parti. La prima parte è la generazione di queste identità, il nostro prompt per generare queste identità è stato is</sample>
    <sample id="348">Myra, Esin Durmus, and Dan Jurafsky's paper, "Marked Personas: Using Natural Language P</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi dall'Università di Scienze e Tecnologia della Cina. È un piacere presentare un breve video pubblicitario della nostra ricerca. Se stai copiando il mio modello? Proteggere il diritto d'autore dei modelli di linguaggio naturale per l'integrazione come servizi tramite marcatura di backdoor.

Prima di tutto, introduciamo il contesto riguardante l'integrazione come servizi. Attualmente, modelli di linguaggio naturale grandi come GPT, LLAMA e PALM sono eccezionali per la comprensione e la generazione del linguaggio naturale. L'integrazione come servizi è uno dei servizi costruiti sui modelli di linguaggio naturale grandesi per assistere a vari compiti di elaborazione del linguaggio naturale. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, recenti studi hanno mostrato che l'attaccante potrebbe rubare il modello attraverso l'apprendimento dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il diritto d'autore dell'integrazione come servizi.

Per proteggere il diritto d'autore dell'integrazione come servizi, una delle soluzioni è incorporare una marcatura nel servizio fornitore e rilevare se un altro servizio contiene la marcatura. Il metodo di marcatura deve soddisfare le seguenti proprietà: prima, il metodo deve essere applicabile all'integrazione come servizi. Secondo, la marcatura non deve degradare l'utilità degli embedding forniti. Terza, la marcatura deve essere nascosta abbastanza per l'attaccante o l'attaccante possa rimuovere la marcatura facilmente. Infine, la marcatura deve essere trasferibile al servizio dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere classificati in quattro categorie, ma questo metodo non è applicabile all'integrazione come servicii o manca di trasferibilità. Pertanto, in questo lavoro proponiamo Embedding marker, che è un metodo di marcatura di backdoor applicabile all'integrazione come servicei.

Ora, introduciamo i dettagli della nostra marcatura di embedding. L'embedding marker comprende due passaggi principali: l'iniezione di marcatura e la verifica del diritto d'autore. Prima di questi passaggi principali, selezioniamo un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata. Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione di marcatura, definiamo un embedding di target. Quando un utente invia una frase al servizio fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma dei pesi dell'embedding di target e dell'embedding originale. Il peso dell'embedding di target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di target. La verifica del diritto d'autore è per rilevare se un modello dietro un altro servizio contiene il marcatura del word. Costruiamo un dataset di backdoor e un dataset benigno. Il dataset di backdoor contiene frasi in cui tutte le parole appartengono al set di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono al set di trigger. Poi, il fornitore richiede gli embedding dal servizio del furto con il dataset. Calcoliamo la somiglianza coseno e L2 tra gli embedding richiesti e l'embedding di target. Calcoliamo la differenza di somiglianza tra i dataset benigno e di backdoor, che è definita come delta coseno e delta L2. Inoltre, appliciamo il test KS e utilizziamo il p-value come terzo metodo. Eseguiamo esperimenti su quattro dataset: AG News, MIND, SST2 e Enron Spam. Supponiamo che il fornitore utilizzi un dataset wiki per contare la frequenza delle parole. I risultati su quattro dataset mostrano che il nostro embedding marker può avere un grande prestigio di rilevazione mentre mantiene una grande utilità per le compiti di downstream. Validiamo anche la nascita della marcatura fornita visualizzando gli embedding di frasi su quattro dataset [INAUDIBLE 4:30] PCA. La leggenda delle figure significa il numero di trigger in ogni frase. Come mostrano le figure, è difficile distinguere tra gli embedding di backdoor e normali. Questo è tutto. Grazie. Siamo a casa di discutere con voi.</sample>
    <sample id="350">Hello everyone, and welcome to the presentation on "What’s the Meaning of Superhuman Performance?" by Simone Tedeschi and colleagues. In the last five years, leaderboard evaluations have become the standard in NLP, with systems often achieving human-level or superhuman performance. However, these achievements are based on "saturated benchmarks" that may not accurately reflect real-world performance.

We analyzed SuperGLUE and SQuAD, two popular benchmarks, and found that humans are outperformed by systems on many tasks. However, we also discovered several issues with these benchmarks, such as evaluating humans on small subsets of the test set and errors in ground-truth answers.

Furthermore, human performance is often estimated vaguely, and pay rates for annotators vary significantly across tasks. Without this information, claims of superhuman performance are not scientifically meaningful.

In summary, we argue that superhuman performance claims in NLU are not yet grounded and that more reliable benchmarks are needed. Thank you for your attention.</sample>
    <sample id="351">Hello everyone, my name is Shuheng, and today I'll be presenting our paper titled "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" Our study investigates the generalization capabilities of models trained on the CoNLL-2003 dataset, which has been used for Named Entity Recognition (NER) tasks for nearly two decades. We ask two key questions: Can these models generalize to modern data, and what factors contribute to good generalization?

To address these questions, we created the CoNLL++ dataset, which consists of Reuters News articles from 2020 annotated with the same CoNLL-2013 guidelines. We fine-tuned over 20 models on the CoNLL-2000 dataset and evaluated their performance on both the CoNLL-2003 and CoNLL++ test sets. We calculated the percentage change in F1 score to assess generalization.

Our findings reveal that three main factors contribute to good generalization: model architecture, model size, and the number of fine-tuning examples. Transformer models, larger models, and more fine-tuning examples all lead to better generalization.

We also explored the causes of performance drops. We considered two hypotheses: adaptive overfitting and temporal drift. Adaptive overfitting, where a model's performance on a test set improves with repeated training, was not observed. However, temporal drift, where performance degrades due to an increasing temporal gap between training and test data, was confirmed.

In conclusion, our study shows that CoNLL-2003 tagger models can still perform well in 2023, but achieving good generalization requires a combination of a better model architecture, larger model size, and more fine-tuning examples. We hope our research encourages further investigation into improving model generalization.

Thank you for your attention, and I welcome any questions you may have.</sample>
    <sample id="352">ABC-Eval è un approccio nuovo e dimensionale per valutare l'intelligenza artificiale conversazionale. Si basa sull'annotazione esplicita dei comportamenti specifici che un modello di chat può esprimere, come rispondere con informazioni irrilevanti o contraddire se stesso o il suo partner. Questo metodo mira a ridurre la soggettività dell'evaluazione umana e a fornire una misura più precisa e affidabile delle diverse dimensioni della qualità del dialogo.</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Question" by Li et al. addresses the challenge of input underspecification in code generation and program synthesis. The authors propose a method to generate code by asking clarification questions, which involves identifying missing specifications at various levels and generating CQAs to gather more information. They create a synthetic dataset called CodeClarQA and propose a pipeline of code generation by asking clarification question. The pipeline consists of a Clarification Need Predictor, a Question selector, and a Code Generator. The authors also conduct error analysis and find that their method is effective in generating CQAs for missing key operations. They also test their pipeline and find that it improves the performance of code generation. The authors conclude that clarified key operations are the reason for better generated code and that their method is a promising approach to address the challenge of input underspecification in code synthesis.</sample>
    <sample id="354">Based on the information provided in the presentation, the difference in performance between CoNLL-2003 and CoNLL++ is not explicitly quantified in terms of percentage points. However, the presentation does mention that the percentage change in F1 score was calculated to assess the generalization of each model. The specific percentage difference between CoNLL-2003 models and CoNLL++ is not provided, but the overall conclusion is that the models generally perform better on CoNLL++ compared to CoNLL-2003, indicating a significant improvement in generalization.</sample>
    <sample id="355">Ciao Vasudha,

Mi piacerebbe presentare il vostro lavoro accettato per un lungo articolo all'ACL 2023, "Transfer Learning for Dissonance Detection". Iniziamo definendo cosa è la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In poche parole, la dissonanza cognitiva è quando due credenze o azioni sono inconsistenti, come nel caso in cui una persona dice "Sono consapevole che i sigaretti potrebbero uccidere", e poi va a prendere un paio di sigarette dopo la riunione. Questa credenza e azione sono inconsistenti e sono in dissonanza. Inoltre, la seconda occorrenza, "Non credo che possa mantenere il mio lavoro senza di loro", giustifica la seconda occorrenza. E hanno una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nella nostra vita quotidiana, è molto raro trovarla espressa in linguaggio tra altri tipi di relazioni discorsive. Quindi perché questo importa? Studiare la dissonanza può aiutarci a capire gli effetti della disaccordo tra le persone, tracciare tendenze e valori di credenze e cambiamenti di attitudine nella popolazione. Un alto livello di dissonanza è anche correlato a disturbi d'ansia e può aiutarci a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa in linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza è importante per comprendere lo stile cognitivo personale degli individui e aiutarci a comprendere megliamente i processi di decisione.

Per raggiungere il nostro obiettivo di creare una risorsa di dissonanza, abbiamo condotto una grande annotazione di relazioni di dissonanza. Abbiamo utilizzato un approccio di dissonanza prima, come mostrato nel flussochart qui. Le twee sono state passate attraverso il parser PDTB, e coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle annotazioni coppie. Dopo aver raccolto circa 1.000 esempi di coppie di unità discorsive, abbiamo eseguito un allenamento per un classificatore iniziale addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non sia performato molto meglio di quanto sarebbe stato per caso. Dato il basso numero di dissonanza e l'assenza di dati precedenti su tale argomento, siamo di fronte al problema dell'assoluta rarità. Per alleviare questo, sperimentiamo combinazioni di apprendimento trasferibile e apprendimento attivo per annotare in modo che più esempi di dissonanza possano essere raccolti in meno run di annotazione, riducendo i costi complessivi di annotazione mentre migliorando la rilevazione della dissonanza. Poiché il modello iniziale non è stato in grado di catturare la classe di dissonanza, abbiamo iniziato il processo di apprendimento attivo trasferendo i pesi da due compiti strettamente correlati: la classificazione della posizione di dissonanza, un compito che determina se due dichiarazioni di debate da persone diverse sono in accordo o in dissonanza, indipendentemente dal tema, chiamato debate qui, e la classificazione binaria delle classi di espansione e</sample>
    <sample id="356">Le affiliazioni degli autori dell\'articolo sono Alexander Koller e Ivan Titov.</sample>
    <sample id="357">The name of the presenter is Siyu Yuan from Fudan University, and she is introducing the work "Distilling Script Knowledge from Large Language Model for Constrained Language Planning."</sample>
    <sample id="358">Five authors are involved in the article: Kayo Yin, Patrick Fernandes, Emmy Liu, André Fernandes Martins, and Graham Neubig.</sample>
    <sample id="359">L'approccio EDAtt viene confrontato con l'architettura specificamente progettata per la traduzione pre-translatata simultanea, che è considerata lo stato dell'arte.</sample>
    <sample id="361">In this presentation, Armineh Nourbakhsh discusses her work on "CounterComp," a method aimed at improving compositional generalization for multi-step quantitative reasoning in question answering tasks. The focus is on financial tables, where the model needs to perform multiple arithmetic operations to answer questions. However, state-of-the-art neural models struggle with these tasks, especially when the output involves more than two steps, due to memorizing spurious patterns.

To address this, CounterComp introduces counterfactual scenarios by mining positive and negative examples from the training set. These examples are used to create triplets that help the model learn to attend to appropriate tokens in the input, thereby avoiding memorization of spurious patterns. The method adds an auxiliary metric learning loss with a dynamic margin to the training process, which measures the extent of change in the questions between each pair.

The results show that CounterComp consistently improves the performance of three state-of-the-art baselines, particularly when the number of reasoning steps exceeds two. This improvement is observed in both in-distribution and out-of-distribution samples, demonstrating the model's ability to generalize to new scenarios. Qualitatively, the method helps the model focus on more meaningful tokens during training, leading to better operational terms in the output.

The presentation concludes with references to the main papers and encourages further exploration through the poster. Armineh thanks her co-authors, advisors, and the audience for their attention.</sample>
  </task>
</testset>