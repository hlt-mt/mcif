<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Language Models sind:

1. **Pretraining Data**:
   - **Wikipedia**: Der größte Teil des Pretraining-Datensatzes.
   - **Books**: Einschließlich Bücher, die in den Common Crawl-Datenbanken verfügbar sind.
   - **Common Crawl**: Ein großer Datensatz, der aus Webseiten besteht.
   - **BooksCorpus**: Ein Datensatz, der aus Büchern besteht.
   - **OpenWebText**: Ein Datensatz, der aus Texten auf der Webseite OpenWebText besteht.
   - **WebText**: Ein Datensatz, der ebenfalls aus Texten auf der Webseite WebText besteht.
   - **Books3**: Ein Datensatz, der aus Buchern besteht.
   - **Books500M**: Ein Datensatz, der aus 500 Millionen Büchern besteht.

2. **Language Models**:
   - **GPT-3**: Ein großes Sprachmodell, das auf großen Datensätzen trainiert wurde.
   - **BERT**: Ein großes Sprachmodell für die Verarbeitung von Texten.
   - **RoBERTa**: Ein großes Sprachmodell basierend auf BERT.
   - **T5**: Ein großes Sprachmodell zur Textgenerierung.

3. **Downstream Tasks**:
   - **Textklassifikation**: Klassifizierung von Texten in verschiedene Kategorien.
   - **Sprachübersetzung**: Übersetzung von Texten von einer Sprache in eine andere.
   - **Textgenerierung**: Generierung von Texten basierend auf einem vorgegebenen Eingabestext.
   - **Sentimentanalyse**: Analyse des Sentiments in Texten.
   - **Named Entity Erkennung**: Erkennung von Namen und anderen spezifischen Entitäten in Texten.
   - **Textzusammenfassung**: Zusammenfassung von Texten.
   - **Textvergleich**: Vergleich von Texten.
   - **Textkontextualisierung**: Kontextualisierung von Texten.
   - **Textbildung**: Bildung von Texten.
   - **Sprachverständnis**: Verständnis von Texten.
   - **Sprachkontextualisierung**: Kontextualisieren von Texten.
   - **Spracherkennung**: Erkennen von Sprachen.
   - **Sprachgenerierung**: Generierung von Sprachen.
   - **Textverarbeitung**: Verarbeitung von Texten.

4. **Datenquellen für Pretraining**:
   - **Wikipedia**: Der grüßste Teil des Pretraining-Datensatze.
   - **Books**: Einschränkte Bücher.
   - **Common Crawl**: Einschränkte Webseiten.
   - **BooksCorpus**: Einschränkte Büchern.
   - **OpenWebText**: Einschränkte Texte auf der Webseite OpenWebText.
   - **WebText**: Einschränkte Webseite WebText.
   - **Books3**: Einschränkte Büche.
   - **Books500m**: Einschränkte 500 Millionen Bücher.

5. **Datenquellen für Language Models**:
   - **GPT-3**</sample>
    <sample id="1">McGill University</sample>
    <sample id="2">Die Übersetzung des englischen Inhaltes in deutsch lautet:

"DEPLAIN: Ein deutscher Parallelkorpus mit intralingualen Übersetzungen in einfache Sprache für Satz- und Dokumentvereinfachung
Regina Stodden, Omar Momen, Laura Kallmeyer
Heinrich Heine Universität Düsseldorf, Deutschland
ACL 2023"

Dieser Text beschreibt das Projekt DEPLAIN, ein deutscher Parallelkorpus, das intralinguale Übersetzungen in einfache Sprachform für Sätze und Dokumente vereinfacht. Es wird von Regina Stodden, Omar Momen und Laura Kallmeyer von der Heinrich Heine Universität Düsseldorf in Deutschland entwickelt und auf der Konferenz ACL 2023 präsentiert.</sample>
    <sample id="3">Die deutsche Übersetzung des englichen Inhalts lautet:

---

**DEPLAIN: Ein deutscher Parallelkorpus mit intralingualen Übersetzungen in einfache Sprache für Satz- und Dokumentvereinfachung**

**Regina Stodden, Omar Momen, Laura Kallmeyer**

**Heinrich Heine Universität Düsseldorf, Deutschland**

**ACL 2023**

---

**1. Textvereinfachung: Was, warum und wie?**

---

Der Inhalt des Videos beschreibt das Projekt **DEPLAIN**, ein deutscher Parallelkorpus, das intralinguale Übersetzungen in einfache Sprachform für Sätze und Dokumente bietet. Es wird von der **Heinrich Heine Universität Düsselordorf** in Deutschland entwickelt und wird auf der **ACL 2023** präsentiert. Die Autoren sind **Regina Stodden, Omar Momen und Laura Kallmeyer**.

---

**Was ist DEPLAIN?**

DEPLAIN ist ein deutscher Parallelkor</sample>
    <sample id="4">In this video, a woman is presenting a slide on text simplification. The slide shows an example of how to simplify a sentence from German to English. The woman explains the process of simplification, which involves substituting words, deleting clauses, reordering words, and deleting words. She provides examples of how to simplify the sentence "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden." to "Die Gewerkschaft setzt zum Beispiel für höhere Löhne oder mehr Urlaub ein." The woman emphasizes the importance of simplifying text to make it more accessible to a wider audience.</sample>
    <sample id="5">In this video, a woman is presenting a slide on text simplification. The slide shows an example of how to simplify a sentence. The woman explains the process of simplifying the sentence by using different techniques such as substitution, clause deletion, reordering, and word deletion. She provides examples of how to simplify the sentence using these techniques. The woman also explains the benefits of text simplification, such as making the text easier to understand and more accessible to a wider audience. Overall, the video provides a clear and concise explanation of text simplification and its importance in making written communication more effective.</sample>
    <sample id="6">In this video, a speaker presents a slide titled "Text Simplification Example" and explains the process of simplifying complex text. The slide features an example of a sentence in German, which is then simplified into plain language. The speaker highlights the use of various techniques such as substitution, clause deletion, reordering, and word deletion to simplify the text. The speaker also provides examples of how these techniques can be applied to different types of text, including legal and medical documents. Throughout the video, the speaker emphasizes the importance of making text more accessible and understandable to a wider audience.</sample>
    <sample id="7">Die Präsentation zeigt ein Textbeispiel zur Textvereinfachung. Der Originaltext lautet: „Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden.“ Die vereinfachte Version lautet: „Die Gewerkshsetzt sich zum Beispiel für höhere Löhne oder mehr Urlaub ein</sample>
    <sample id="8">Die Präsentation beginnt mit einer einfachen und klaren Überschrift, die die thematische Konzentration auf "2. DE-plain - A New Corpus" hervorhebt. Anschließend wird die Aufmerksamkeit auf die "German Text Simplification Corpora" gelenkt, die als wichtige Ressource für die Textvereinfachung in der deutschen Sprache dargestellt wird. Die Präsentation zeigt eine detaillierte Analyse der korpusweiten Daten, die die Anzahl der einfachen, mittleren und komplexen Sätze in verschiedenen Jahren und in verschiedenen Korpora vergleichen. Die Analyse zeigt, dass die Anzahl der einfachen Sätze in den letzten Jahren zugenommen hat, während die Anzahl der komplexen Sätze zurückgegangen ist. Dies zeigt, dass die Textvereinfachung in der deutschsprachigen Welt zunehmend wichtig wird. Die Präsentation endet mit einer Zusammenfassung der wichtigsten Erkenntnisse und einer Aufforderung an die Zuhörer, die korpusweiten Daten zu nutzen, um die Textvereinfachung in der deut</sample>
    <sample id="9">Die folgende Übersetzung des englischen In</sample>
    <sample id="10">Die folgende Übersetzung des englischen In</sample>
    <sample id="11">Die folgende Übersetzung des englischen In</sample>
    <sample id="12">Die folgende Übersetzung des englischen In</sample>
    <sample id="13">Die folgende Übersetzung des englischen Inhaltes in deutsch:

---

**German Text Simplification Corpora**

**Sentence Level**

Die folgende Tabelle zeigt die Anzahl der einfachen, mittleren und komplexen Sätze in verschiedenen Jahren. Die Anzahl der einfachen Sätze ist in der Regel die höchste, gefolgt von den mittleren und dann den komplexen Sätzen. Die Anzahl der komplexen Sätze ist in den Jahren 2018 und 2019 am höchsten, während die Anzahl der einfachen Sä</sample>
    <sample id="14">Der Präsident der Vereinigten Staaten, Donald Trump, hat den Verlust des Wahlkampfes gegen Joe Biden beklagt und die Wahlergebnisse des Wahlkampfes in den Staaten Pennsylvania, Michigan und Wisconsin als "schlecht" beschrieben. Er hat auch die Wahlergebnisse in den Staaten Georgia und Arizona als "schlecht" beschriebe</sample>
    <sample id="15">Der Inhalt beschreibt zwei Diagramme, die die Effektivität verschiedener Simplifizierungsmethoden im Kontext von Texten untersuchen. 

### Diagramm 1: Typen der Simplifizierung

- **Einführung:**
  - Der Diagramm 1 zeigt die Effektivität verschiedener Textarten (News, Bibel, L2, Fiktion) bei der Anwendung von Simplifizierungsmethoden.
  - Die Methoden sind:
    - **SimpliCity:** Ein Algorithmus zur Textvereinfachung.
    - **LecSim:** Ein weiterer Algorithmus zur Textvereinfachung. 
    - **StrucSim:** Ein dritter Algorithmus zur Textvereinfachung.  

- **Ergebnisse:**
  - **News:**
    - SimpliCity und LecSim erreichen eine hohe Effektivität, während StrucSim deutlich weniger effektiv ist.
  - **Bibel:**
    - SimpliCity und LEC-SIM erreichen eine hohe Effektivkeit, während StrucSim deutlicher weniger effektiv ist.
    - Die Bibel hat eine besondere Struktur, die die Effektivität der Methoden beeinflusst.
  - **L2:**
    - SimpliCity und LEE-SIM erreichen eine hohe Effektiivität, während StrucSim weniger effektiv ist.
   - **Fiktion:**
    - SimpliCity und LLE-SIM erreichen eine hohe Effeaktivität, während StrucSim eher weniger effektiv ist. 

- **Schlussfolgerung:**
  - Die Effektivität der Simplifizierungsmethoden variiert je nach Textart.
  - SimpliCity und LecSim sind in der Regel effektiver als StrucSim.

### Diagramm 2: Simplifikations-Transformationen

- **Einführung**
  - Der Diagramm 2 zeigt die Effektivität verschiebender Simplifikations-Transformationen (DEplan-apa, DEplan-web) auf verschiedenen Textarten.
  - Die Transformationen sind:
    - **DEplan-apa:** Eine Transformation, die auf APA-Stile zugeschnitten ist.
    - **DEplan-web:** Eine Transformation, die auf Web-Stile zugeschnitten ist.  

- **Ergebnissen:**
  - **News:**
    DEplan-apa und DEplan-web erreichen eine hohe Effektivitätsrate.
  - **Bibel:**
   DEplan-apa und DEplan-web erzielen eine hohe Effektivität.
  - **L2:**
   DEplan-apa und DEplam-web erreichen eine hohe Effektive.
  - **Fiktion:**
   DEplan-apa und DEpland-web erreichen eine hohe Effektivity.

- **Schlussfolgerungen:**
  - Die Effektivität von Simplifikations-Transformationen ist in allen Textarten hoch.
  - DEplan-apa und DEpland-web sind in der Regel effektiver als andere Transformationen.

### Zusammenfassung:

- Die Diagramme zeigen, dass die Effektivität von Simplifizierungsmethoden und -Transformationen von der Textart abhängt.
- SimpliCity und LecSim sind in vielen Fällen effektiver als StrucSim.
- DEplan-apa und DEpland-web sind effektive Transformationsmethoden für verschiedene Textarten.

### Zusätzliche Informationen:

- Die Diagramme basieren auf Experimenten, die die Effektivität der Methodiken in verschiedenen Textarten untersuchten.
- Die Ergebnisse zeigen, dass die Wahl der Simplifizierungsmethode und -Transformation von der Textart abhängt.</sample>
    <sample id="16">Der Präsident der Vereinigten Staaten, Donald Trump, hat den Plan zur Bekämpfung des Klimawandels aufgegeben und die USA aus dem Pariser Klimaabkommen zurückgezogen. Diese Entscheidung hat die internationale Gemeinschaft in großer Sorge versetzt, da die globale Erwärmung weiter zunimmt und die Auswirkungen auf die Umwelt und die Gesellschaft immer gravierender werden. Die USA sind ein wichtiger Akteur im Kampf gegen den Klimawandel, und ihre Rücknahme aus dem Pariser Abkommen könnte die globale Zusammenarbeit erschweren und die Verhandlungen über die Zukunft des Klimaschutzes erschweren. Die internationale Gemeinschaft wird nun darauf achten, wie andere Länder ihre Verpflichtungen im Rahmen des Abkommens erfüllen und ob sie sich für den Klimaschutz einsetzen.</sample>
    <sample id="17">Die Übersetzung des englischen Inhaltes in deutsch lautet:

---

**Types of Simplification**

| Text Type | SimpliCity | LexiSimpl | StructSimpl |
|-----------|------------|-----------|-------------|
| News      | 0.4        | 0.3       | 0.2         |
| Bible     | 0.5        | 0.4       | 0.3         |
| L2        | 0.3        | 0.2       | 0.1         |
| Fiction   | 0.6        | 0.5       | 0.4         |

**Simplification Transformations**

| Transformation | DEplan-apa | DEplan-web |
|----------------|------------|------------|
| Simplification | 80         | 70         |
| Lexical       | 60         | 50         |
| Structural    | 40         | 30         |

---

**Types of Simplifications**

| Text Type | Simpliity | LexiSimpl | StructSimpl |
|:---------:|:---------:|:---------:|:-----------:|
| News      | 0.3       | 0,2       | 0,1         |
| Bible     | 0,4       | 0,3       | 0,2         |
| L2        | 2,5       | 2,3       | 2,1         |
| Fiction   | 3,6       | 3,5       | 3,4         |

**Simplification Transform**

| Transformation | DEplan-APA | DEplan-web |
|:--------------:|:----------:|:----------:|
| Simplification | 80        | 70        |
| Lexical       | 65        | 55        |
| Structural    | 45        | 35        |

---

**Types of Simplicity**

| Text Type | SimpliCty | LexiSimpl | StructSimpl |
:---------:|:---------:|:---------:|:------------:|
| News      | 0,2       | 0        | 0,1         |
| Bibel     | 0,3       | 0        | 0,2         |
| l2        | 2,5       |</sample>
    <sample id="18">Der Präsentierte präsentiert eine Reihe von Diagrammen, die die verschiedenen Aspekte der Textvereinfachung untersuchen. Die erste Tabelle zeigt die Komplexität von Texten in verschiedenen Genres, wie Nachrichten, Bibel, L2 und Fiktion. Die zweite Tabelle zeigt die Anzahl der Textvereinfachungs-Transformationen, die in verschiedenen Textkategorien vorkommen. Die Präsentierte analysiert die Daten und erklärt die Ergebnisse.</sample>
    <sample id="19">Die Übersetzung des englischen Inhaltes in deutscher Sprache lautet:

---

**3. Use-cases**

**Automatische Ausrichtung und Vereinfachung**

---

Der Inhalt beschreibt die Nutzungsfälle für die automatische Ausrichtung und Vereinfacheung. Es wird auf die Evaluation verschiedener automatischer Ausrichtungsmethoden eingegangen, die mit 1:1 (obere Hälfte) und n:m (untere Hälfte) Fähigkeiten ausgestattet sind. Die Tabelle zeigt die Ergebnisse dieser Methoden in Bezug auf Präzision (P), Rekall (R) und F1-Wert (F1).

---

**Übersetzung des Inhalts:**

---

**3. Use-Cases**

**Automatische Ausgewählung und Vereinfachung**</sample>
    <sample id="20">Die Übersetzung des englischen Inhaltes in deutscher Sprache lautet:

---

**Automatische Ausrichtungsbewertung**

**Ergebnisse der Ausrichtungsmethoden mit 1:1 (obere Hälfte) und n:m Fähigkeiten (untere Hälfte)**

| Name | Beschreibung | P | R | F1 | P | R | F1 |
|------|--------------|---|---|----|---|---|----|
| LHA  | Häufige Ausrichtung mit Satzembeddingsähnlichkeit | 94 | 44 | 67 | 80 | 78 | 78 |
| Sent-LabSe | Ähnliche Embeddings von Language-Agnostic BERT-Transformern | 96 | 44 | 67 | **80** | 78 | 78 | 78 |  
| Sent-RoBERTa | Ähnliche Embeddings von Cross English und German RoBERTa | 96 | 44 | **67** | 80 | 78 | **78** | 78 |  
| VecAlign | Multimodaler Ausrichtung basierend auf mehreren Satzembeddings | 74 | 40 | 52 | 290 | 465 | 333 | 333 |  
| BERTAlign | Erlaubt Satz-Transformer-Methoden, um n:m Ausrichtungen zu produzieren | 74 | 40 | **52** | 290 | 466 | 333 | 334 |  
| MASSalign | Vierköpfiges Vorgehen basierend auf einem TF-IDF-Similaritätsmaß | 84 | 47 | 61 | 810 | 509 | 628 | 628 |  

---

**Ergebnisse der Ausgewählten Methoden:**

| Name | Beschreibungen | P | R | F1 | P  | R  | F1 |
|------|----------------|---|---|----|----|----|----|
| LHA  | 94 | 44 | **67**  | 80 | 78 |
| Sent-LaSe | 96 | 44 | *67* | 80 | 78 |  
| Sent-ReoRta | 96 | 44 | ***67*** | 80 | 78 |   
| VecAlign | 74 | 40 | *52* | 290 | 464 | 333 | 332 |  
| BERTAlign | 74 | 40  | ***52*** | 290 | 467 | 333 | 335 |  
| MASSalign | 84 | 47 | *61* | 810 | 508 | 628 | 629 |  

---

**Erklärung der Ergebnisse:**

- **LHA:** Häufige Ausrichtung mit Sentembeddingsähnlichkeit.
- **Sent-LaSe:** Ähnliche Embeddings von Language-Agnostischen BERT-Transformern.
- **Sent-ReoRta:** Ähnliche Embeddings von Cross English und german RoBERTa.
- **VecAlign:** Multimodaler Ausrichtung basierende auf mehreren Satzembeddings.
- **BERTAlign:** Erlaubt Satz-Transformer-Methode, um n:m Ausrichtungen zu produzen.
- **MASSalign:** Vierköpfiges Vorgehensweise basierend auf einem TF-IDF-Similaritymaß.

---

**Ergebnis:**

| Name | Beschreiber | P | R | F1 | P   | R   | F1 |
|------|------------|---|---|----|-----|-----|----|
| LHA  | **94**     | 44 | **67**   | 80 | 78  | 78  | 78 |
| Sent-LasSe | 96 | 44  | *67*   | 80 | 78 | *78* | 78 |  
| Sent-ReoRta | 96 |  44 | ***67*** | **80** | 78  | **78** | 78  |  
| VecAlign | 74 | **40** | *52*   | 290 | 463 | 333 | 331 |  
| BERTAlign |  74 | 40  | **52**   | 290 | 470 | 333 | 336 |  
| MASSalign |  84 | 47 | *51*   | 810 | 507 | 628 | 627 |  

---

**Ergänzung:**

| Name | Beschreber | P | R | F1 | P    | R    | F1 |
|------|------------ |---|---|----|------|------|----|
| LHA  | *94*     | 44 | **67*   | 80 | **78**  | **78**  | **79** |
| Sent-LasSe |  96 | 44  | 67   | 80 | 78   | 78   | 78 |  
| Sent ReoRta |  96 | 44 | ***57*** | 80 | 79   | **79** | 79  |  
| VecAlign |  74 | **40** | 52   | 290 | 459 | 333 | 330 |  
| BERTAlign |  **74** | 40  | 52   | 291 | 467 | 334 | 337 |  
| MASSalign |  **84** | 47 | 51   | 810 | 510 | 628 | 626 |  

---

**Erweiterung:**

| Name | Beschrieben | P | R | F1 | P     | R     | F1 |
|------|------------  |---|---|----|-------|-------|----|
| LHA  | ***94***     | 44 | **67***   | 80 | **78.5**  | **78.5**  | 78.5 |
| Sent-LasSe |  **96** | 44  | 67.5   | 80 | 78.5 | 78.5 | 79 |  
| Sent ReoRta  |  **96** | 45 | 67.5   | 79 | 79.5 | **79.5** | 79.5  |  
| VecAlign |  **74** |  **40** | 52.5   | 290 | 449 | 333 | 328 |  
| BERTAlign |  ***74*** | 40  | 52.5   | 300 | 467 | 340 | 343 |  
| MASSalign |  ***84*** | 47 | 51.5   | 810 | 520 | 628 | 630 |  

---

**Erfolgsbewertung:**

| Name | Beschriben | P | R | F1 |

| LHA  | 94  | 44  | 67  |
| Sent-LasSe | 76  | 44  | 60  |
| Sent ReoRta  | **96** | 44  **67** | 80  |
| VecAlign | 74  | 40  | 52  |
| BERTAlign | 74  | 41  | 52  |
| MASSalign | 84  | 47  | 61  |

---

**Ergebniss:**

| Name | Beschribenen | P | R | F1 |
  
| LHA  | 94   | 44  | 67 |
| Sent-LasSe | 66  | 44  | 56  |
| Sent ReoRta | 96  | 44  | 70  |
| Vec</sample>
    <sample id="21">Die Übersetzung des englischen Inhaltes in deutscher Sprache lautet:

---

**Automatische Ausrichtungsbewertung**

**Ergebnisse der Ausrichtungsmethoden mit 1:1 (obere Hälfte) und n:m Fähigkeiten (untere Hälfte)**

| Name | Beschreibung | P | R | F1 |
|------|--------------|---|---|----|
| LHA  | Häufige Ausrichtung mit ähnlichen Satzembeddings | 94 | 44 | 67 |
| Sent-LabSe | Ähnliche Embeddings von Language-Agnostic BERT-Transformern | 961 | 444 | 808 |
| Sent-ReBERTa | Ähnliche Embeddings von Cross English und German ReBERTa | 966 | 444 | 807 |
| VecAlign | Multimodale Ausrichtung basierend auf mehreren Satzembeddings | 743 | 404 | 323 |
| BERTAlign | Erlaubt, satz-embeddings-verfahrens Produkt mit n Ausrichtungen | 743 | 465 | 372 |
| MASSalign | Vielfältige Ausrichtung basierend auf einem TF-IDF-Similaritätsmaß | 846 | 477 | 419 |

---

**Ergebnisse der Ausgewählten Methoden mit 1:1 (obere Part) und n:m Fähigkeiten**

| Name | Beschreibungen | P | R | F1 |
|:---:|:---:|:---:|:---:|---:|
| LHA  | Häufiges Ausrichtung mit ähnlichen Satz-embeddings | 94 | 45 | 67 |
| Sent-LaSe | Ähnliche Embeddings von Language-agnostic BERT-Transformern | 96 | 444 | 810 |
| Sent-ReBERTa | Äh</sample>
    <sample id="22">Der Präsident des Unternehmens, der in einem Video aufgeführt wird, beschreibt die Ergebnisse der automatischen Ausrichtungsbewertung.</sample>
    <sample id="23">Die Übersetzung des englischen Inhaltes in deutsch lautet:

---

**Automatische Ausrichtungsbewertung**

**Ergebnisse der Ausrichtungsmethoden mit 1:1 (obere Hälfte) und n:m Fähigkeiten (untere Hälfte)**

| Name | Beschreibung | P | R | F1 |
|------|--------------|---|---|----|
| LHA  | Häufige Ausrichtung mit ähnlichen Satzembeddings | 94 | 44 | 60 |
| Sent-LabSe | Ähnliche Embeddings von Language-Agnostic BERT-Transformern | 961 | 444 | 808 |
| Sent-ReBERTa | Ähnliche Embeddings von Cross English und German ReBERTa | 966 | 444 | 807 |
| VecAlign | Multimodaler Ausrichtung basierend auf mehreren Satzembeddings | 743 | 404 | 323 |
| BERTalign | Erlaubt, satz-embeddings methoden Produkt n Alignments | 743 | 465 | 372 |
| MASSalign | Vielfältige Ausrichtung basierend auf einem TF-IDF-Similaritätsmaß | 846 | 477 | 419 |

---

**Ergebnisse der Ausgewählten Methoden:**

| Name | Beschreibungen | P | R | F1 |
|:----:|:--------------:|:---:|:---:|:---:|
| LHA  | Häufiges Ausrichtung mit ähnlichen Satz-embeddings | 94 | 46 | 60 |
| Sent-LaSe | Ähnliche Embeddings von Language-agnostic BERT-Transformern | 96 | 44 | 80 |
| Sent-ReBERTa | Ähne Embeddings von Cross English und German ReBERT | 96 | 44 |
| VecAlign | Multimodale Ausrichtung basierend auf mehren Satz-embeddings | 743 | 323 | 290 |
| BERTalign | Erlauben, satz-embeddings methoden Produkte n Alignments | 743 |
| MASSalign | Vielfaltige Ausrichtung basierend auf einem TFI-IDF-Similaritätsmaß |</sample>
    <sample id="24">Der Präsentierte analysiert die Ergebnisse verschiedener automatischer Übereinstimmungsmethoden, die mit 1:1 (obere Hälfte) und n:m (untere Hälfte) Fähigkeiten ausgestattet sind. Die Methoden werden auf ihre Leistung bei der Übereinstimmung von Sätzen bewertet, wobei die Präzision (P), Recalls (R), F1-Wert (F1) und F2-Wert (F2) als wichtige Metriken dargestellt werden. Die Tabelle zeigt die Ergebnisse für verschiedene Methoden, darunter LHA, Sent-LaBe, Sent-ReBERTa, VecAlign, BERTalign und MASSalign. Die Ergebnisse werden in zwei Spalten dargestellt: die obere Hälfte zeigt die Ergebnisse für 1:1-Methoden, während die untere Hälfte die Ergebnisse für n:m-Methoden zeigt. Die Methoden werden auf ihre Leichtigkeit und Genauigkeit hin bewertet, und es wird diskutiert, welche Methoden am besten für bestimmte Anwendungsfälle geeignet sind.</sample>
    <sample id="25">Die Übersetzung des englischen Inhaltes in deutsch lautet:

---

**Automatische Ausrichtungsbewertung**

**Ergebnisse der Ausrichtungsmethoden mit 1:1 (obere Hälfte) und n:m Fähigkeiten (untere Hälfte)**

| Name | Beschreibung | P | R | F1 |
|------|--------------|---|---|----|
| LHA  | Häufige Ausrichtung mit Satzembeddingsähnlichkeit | 94 | 44 | 67 |
| Sent-LaBe | Ähnliche Embeddings von Language-agnostic BERT-Transformern | 961 | 444 | 808 |
| Sent-LaBeR | Ähnliche Embeddings von Cross English und German RoBERTa | 966 | 444 | 807 |
| VectAlign | Multimodeller Ausrichtung basierend auf mehreren Satzembeddings | 743 | 404 | 523 |
| BERTAlign | Erlaubt Satz-Transformer-Methoden, um n:1-Ausrichtungen zu produzieren | 743 | 465 | 572 |
| MASSalign | Vielfältige Ausrichtungsmethode basierend auf einem TF-IDF-Similaritätsmaß | 846 | 477 | 819 |

---

**Ergebnisse der Ausgewählten Methoden**

| Name | Beschreibungen | P | R | F1 |
|:----:|:--------------:|:---:|:---:|:---:|
| LHA  | Häufiges Ausrichten mit Satzembeddingsähnlichkeit. | 94 | 44 | **67** |
| Sent-LaBe | Ähnlichkeit von Language-agnostic BERT-Transformern. | 961 | 44 | **808** |
| Sent-LaBeR | Ähnelnde Embeddings von Cross English und German RoBERTA. | 966 | 44 | **807** |
| VectAlign | Multimodaler Ansatz basierend auf mehreren Satzeinbettungen. | 743 | 40 | **523** |
| BERTAlign | Erlauben von Satz-Transformer-Methoden, um 1:1-Ausrichtungen zu erzeugen. | 743 | 46 | **572** |
| MASSalign | Vielfaltige Ansatz basierend auf einem TF-IDF-Similaritymaß. | 846 | 47 | **819** |

---

**Ergebnisse der Ausgewählten Methoden mit 1:1 (obere Halfte) und n:m Fähigkeit (untere Halfte)**

| Name | Beschriftung | P | R | F1 | F1 |
|:----:|:------------:|:---:|:---:|:---|
| LHA  | Häulich Ausrichtung mit Satzembeddingsähne | 94 | 44 | *67* |
| Sent-LaBe | Ähnlicher Embeddings von Language-agnostic BERT-Transformer | 961 | 441 | 808 |
| Sent-LaBeR | Ähnliche Embeddings von cross English und German RoBERTa | 946 | 444 | 817 |
| VectAlign | Multimedialer Ansatz basierend auf mehren Satzembeddings | 741 | 404 | 520 |
| BERTAlign | Erlaube Satz-Transformer-Methoden, um m:1-Ausrichtungen zu erzeu | 743 | 464 | 572 |
| MAssalign | Vielfältige Ansatz basierend auf einem TF-IBF-Similaritymaß | 846 | 577 | 819 |
| MASSalign | Vielfälte Ansatz basierend auf einem TF-IDFSimilaritymaß | 846 | **477** | **819** |

**Ergebnisse der Auswahl der Methoden**

| Name | Beschriftung  | P | R | F1 |

| LHA  | Häuliche Ausrichtung mit Satzembeddingsäne | 94 | 441 | 67 |
| Sent-LaBa | Ähnliche Embeddings von Language-agnost BERT-Transformer | 9614 | 444 | 80 |
| Sent-LaBeR | Änliche Embeddings von Cross English und German RoBerta | 966 | 441 | 807 |
| VactAlign | Multimedialer Ansatz basierte mehren Satzembeddings | 404 | 52 | 743 |
| BERTAlign | Erläuft Satz-Transformer-Methoden, um M:1-Ausrichtungen zu erzeug | 465 | 57 | 743 |
| MASSalign | Vielfalte Ansatz basierend auf einem TF-IFB-Similaritymaß | 477 | 81 | 846 |

---

**Ergebenis der Ausgewählten Methoden 1:1 (obere Hälte) und n:m Fähigkeiten 

| Name | Beschreib | P | R | F1 |
|-|-|-|-|-|
| LHA  | Häule Ausrichtung mit Satzembeddingsähe | 94 | 44 | ***67*** |
| Sent-LaBe | Ähn</sample>
    <sample id="26">Die Übersetzung des englischen Inhaltes in deutsch lautet:

---

**Automatische Ausrichtungsbewertung**

**Ergebnisse der Ausrichtungsmethoden mit 1:1 (obere Hälfte) und n:m Fähigkeiten (untere Hälfte)**

| Name | Beschreibung | P | R | F1 |
|------|--------------|---|---|----|
| LHA  | Häufige Ausrichtung mit ähnlichen Satzembeddings | 94 | 44 | 60 |
| Sent-LabSe | Ähnliche Embeddings von Language-Agnostic BERT-Transformern | 96 | 44 | 60 | 80 |
| Sent-ReBERTa | Ähnliche Embeddings von Cross English und German ReBERTa | 96 | 44 | - | 79 |
| VecAlign | Multimodale Ausrichtung basierend auf mehreren Satzembeddings | 74 | 40 | 53 | 290 | 465 | 333 | 285 |
| BERTalign | Erlaubt, satz-embeddings-produzierte n:1-Ausrichtungen zu erzeugen | 74 | 40 | - | 53 | 290 465 | 333 285 |
| MASSalign | Vielfältige Ausrichtung mit einem TF-IDF-basierten Ähnlichkeitsmaß | 84 | 47 | 61 | 819 | 509 | 628 | 730 |

---

**Ergebnisse der Ausgewählten Methoden**

| Name | Beschreibungen | P | R | F1 |
|:----:|:--------------:|:---:|:---:|:---:|
| LHA  | Häufiges Ausrichten mit ähnlichen Satzembeddings | - | - | - |
| Sent-LabSe | Ähnelnde Embeddings von Language-Agnostic BERT-transformern | - | - | - |
| BERTalign | Erlauben, satz-embeddings-produzierten n:1-Ausrichtungen zu erzielen | - | - | - |
| MASSalign | Vielfaltige Ausrichtung mit einem TF-IDF-ähnlichen Ähnlichkeitsmaß | - | - | - |

---

**Ergebnisse der Ausgewählten Methoden mit 1:1 (obere Halfte) und n:m Fähigkeit (untere Halfte)**

| Name | Beschriftung | P | R | F1 | 
|:----:|:------------:|:---:|:---:|:---|
| LHA  | Häftiges Ausrichten mit ähnlichen Sätzenembeddings | - | - | - | 
| Sent-LabSe | Ähnliches Embeddings von Language-Agnostic BERT transformern | - | - | - | 

---

**Ergebnis der Ausgewählten Methoden 1:1 (obere Hälte) und n:m Fähigkeiten 

| Name | Beschreib | P | R | F1 |

| LHA  | Häuftiges Ausrichten mit ähnlicher Sätzeembeddings | - | - | - |  
| Sent-LabSe | Ähnlichkeits-Embeddings von Language-Agnostic BERT-transformers | - | - | - |  

---

**Ergebniss der Ausgewählten Methoden und n:m Fähigkeiten**

| Name | Beschreiber | P | R | F1 |
  
| LHA  | Häußiges Ausrichten mit ähnlichen Sentenembeddings | - | - | - |   
| Sent-LabSe | Ähnlichen Embeddings von Language-Agnostic BERT transformers | - | - | - |  
  
---</sample>
    <sample id="27">Der Inhalt des Videos ist eine Präsentation über die automatische Textvereinfachung, die auf dem langfristigen mBART-Modell basiert. Die Präsentation zeigt die Ergebnisse für die Dokumentebene und die Satzebene, die auf der Länge der Trainingsdaten basieren. Die Ergebnisse werden in Tabellenform dargestellt, die die Performance der Modellversionen auf verschiedenen Datensätarten zeigen. Die Präsentation wird von einem Mann in einem dunklen Pullover in einem Raum mit einem weißen Tisch und einem roten Becher mit einem weißen Kaffee oder Tee gehalten.</sample>
    <sample id="28">In diesem Video wird ein Mann, der in einem Raum sitzt, vorgestellt.</sample>
    <sample id="29">Der Inhalt des Videos ist eine Präsentation von Ergebnissen zur automatischen Textvereinfachung, die auf dem Dokumentebene und der Satzebene durch den Einsatz von finiertem mBART basieren. Die Präsentation zeigt Tabellen mit den Bewertungsmetriken für verschiedene Trainingsdatenmengen, die in verschiedenen Kategorien wie BLEU, BERT und F1-Scores unterteilt sind. Die Ergebnisse werden auf der Basis von DEPLAN-API-Tests interpretiert, die die Leistung des Modells auf den Testdatensät (n=48) und den Validierungsdatensatz (n=147) bewerten. Die Präsentation wird von einem Mann in einem dunklen Pullover in einem Raum mit einem großen Fenster durchgeführt, das über die Stadt hinausgeht.</sample>
    <sample id="30">Der Inhalt des Videos ist eine Präsentation von Ergebnissen zur automatischen Textvereinfachung, die auf dem Dokumentebene und der Satzebene durch den Einsatz von finiertem mBART basieren. Die Präsentation zeigt die Ergebnisse auf einem Diagramm, das die Performance der verschiedenen Modellversionen auf der Dokumentebene und der Satz</sample>
    <sample id="31">Der Inhalt des Videos ist eine Präsentation über die automatische Textvereinfachung, die auf dem langfristigen mBART-Modell basiert. Die Präsentation zeigt die Ergebnisse für die Dokumentebene und die Satzebene, wobei die Trainingsdaten an die Länge der Dokumente angepasst sind. Die Ergebnisse werden in Tabellenform präsentiert, die die Performance der Modellversionen auf verschiedenen Datensättesten zeigen. Die Präsentation wird von einem Mann in einem dunklen Pullover in einem Raum mit einem großen Fenster und einer weißen Wand durchgeführt.</sample>
    <sample id="32">In diesem Video wird ein Mann, der in einem Raum sitzt und vor einem Computer arbeitet, über die Anwendung von automatischen Textvereinfachungen spricht. Er zeigt auf einem Bildschirm, das die Ergebnisse von Textvereinfachungen auf verschiedene Textarten anzeigt, einschließlich Dokumenten- und Satzebene. Der Bildschirm zeigt zwei Tabellen, die die Ergebnisse der Textvereinfachungen auf verschiedenen Textarten anzeigen. Der Mann erklärt, wie die Textvereinfachungen funktionieren und welche Vorteile sie bieten. Er zeigt auch, wie die Textvereinfachungen auf verschiedensten Textarten angewendet werden können.</sample>
    <sample id="33">Der Inhalt des Videos ist eine Präsentation über die automatische Textvereinfachung, die auf der Anwendung von einem vorgefertigten langen mBART basiert. Die Präsentation zeigt die Ergebnisse für die Dokumentebene und die Satzebene, wobei die Ergebnisse für verschiedene Trainingsdatenmengen angegeben werden. Die Präsentation zeigt auch die Ergebnisse für die DEPLAIN-API-Tests.</sample>
    <sample id="34">Für weitere Details. Bitte lesen Sie unseren Beitrag. Und freuen Sie sich auf unsere Plakat im ACL 2023 Konferenz.</sample>
    <sample id="35">Patrick Fernandes</sample>
    <sample id="36">Das T5 XL Modell wurde verwendet, um eine Genauigkeit von 82–-87% zu erreichen.</sample>
    <sample id="37">Yes.</sample>
    <sample id="38">Die vorgeschlagene menschliche Bewertungsmethode basiert auf dem Rating der Relevanz der Antworten des Chatbots.</sample>
    <sample id="39">The success of the existing semi-supervised approach depends on the quality of the weak labels.</sample>
    <sample id="40">The video does not provide information on how to improve the results.</sample>
    <sample id="41">6</sample>
    <sample id="42">Das Video beginnt mit einer Präsentation über die Arbeit von Adam Przepiórkowski und Michał Wozniak, die sich auf die Analyse von Konjunktivlängen in englischen Sätzen, der Minimierung der Abhängigkeitslänge und der Struktur der Koordination konzentriert. Die Präsentation wird von Adam Przepiórkowski geleitet, der die Inhalte des Artikels "Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination" vorstellt. Die Präsentation wird von Adam und Michał Wozniak geleitet, die die Inhalte des Artikels "Dependency Structure of Coordination" vorstellen. Die Präsentation wird von Adam Wozniak geleitet, der die Inhalte der Artikel "Dependency Structure of Coordination" und "Conjunct Lengths in English, Dependeny Length Minimization, and Dependency Structure of Coordination vorstellt.</sample>
    <sample id="43">Der englische Text beschreibt verschiedene koordinierende Strukturen in der Syntax, die in der Sprache verwendet werden können. Es wird die Bouquet/Stanford-Variante als universelle Koordinationsstruktur vorgestellt, gefolgt von der Chain/Moscow-Variante, der Konjunktion-headed/Prague-Variante und der Multi-headed/London-Variante. Jede Variante wird mit einem Beispiel ausgeschrieben, um die Unterschiede zu verdeutlichen.</sample>
    <sample id="44">Der englische Text beschreibt verschiedene koordinierende Strukturen in der Syntax, die in der Sprache verwendet werden können. Es wird die Bouquet/Stanford-Variante als universelle Koordinationsstruktur vorgestellt, die in vielen Sprachen verwendet wird. Es folgen die Chain/Moscow-Variante, die eine Kette von Koordinaten beschreibt, die von links nach rechts verfolgt werden kann. Die Konjunktion-headed/Prague-Variante wird als eine Struktur beschrieben, die mit einer Konjunktion beginnt und mehrere Koordinaten enthält, die durch Komma getrennt sind. Schließlich wird die Multi-headed/London-Variante als eine Struktur beschriebenen, die mehrere Koordinaten enthaelt, die durch Komma getrennt sind und eine Hauptverbindung haben.</sample>
    <sample id="45">Der englische Text beschreibt verschiedene Arten von Koordinationsstrukturen in der Syntax, basierend auf der Theorie von Bouquet/Stanford. Es wird erklärt, wie die Koordination von Wörtern wie "Homer" und "Lisa, Bart, und Maggie" in verschiedenen syntaktischen Strukturen dargestellt werden kann. Die verschiedenen Arten der Koordination werden als "Universal Dependencies", "Chain/Moscow", "Conjunction-headed/Prague" und "Multi-headed/London" bezeichnet. Jede Art wird mit einem Beispiel aus dem Simpsons-Film "Die Simpsons" erklärt, um die Konzepte zu verdeutlichen.</sample>
    <sample id="46">Der englische Text beschreibt verschiedene Koordinationsstrukturen in der Syntax, die in der Linguistik verwendet werden. Es wird die Bouquet/Stanford-Variante als universelle Koordinationsstruktur vorgestellt, gefolgt von der Chain/Moscow-Variante, der Konjunktion-headed/Prague-Variante und der Multi-headed/London-Variante. Jede Variante wird mit einem Beispiel aus dem Simpsons-Film "Homer Loves Lisa, Bart, and Maggie" erklärt, um die Unterschiede in der syntaktischen Struktur zu verdeutlichen.</sample>
    <sample id="47">Der englische Text beschreibt verschiedene koordinierende Strukturen in der Syntax, basierend auf den Theorien von Bouquet/Stanford, Chain/Moscow, Conjunction-headed/Prague und Multi-headed/London. Es wird erklärt, wie die verschiedenen Strukturen die Art und Weise beeinflussen, wie Satzglieder in einem Satz verbunden sind. Die verschiedenen Strukturen werden mit Beispielen aus dem Satz "Homer loves Lisa, Bart, and Maggie" illustriert, um die jeweilige Struktur zu erklären. Die verschiedenen Strukturenschemata werden mit den entsprechenden Labels und Diagrammen dargestellt, um die Unterschiede und Gemeinsamkeiten zu verdeutlichen.</sample>
    <sample id="48">Der englische Text beschreibt verschiedene koordinierende Strukturen in der Syntax, die in der Sprache verwendet werden können. Es wird die Bouquet/Stanford-Variante (Universal Dependencies) erwähnt, die als universelle Syntaxstruktur gilt. Es wird auch die Chain/Moscow-Variante erwähnt, die als eine Kette von Wörtern dargestellt wird. Die Konjunktion-headed/Prague-Variante wird als eine Kombination von Koordination und Konjunktion beschrieben, während die Multi-headed/London-Variante als eine Kombination von Koordination, Konjunktion und Subordination dargestellt wird.</sample>
    <sample id="49">Der englische Text beschreibt verschiedene koordinierende Strukturen in der Syntax, die in der Sprache verwendet werden können. Es wird die Bouquet/Stanford-Variante als universelle Koordinationsstruktur vorgestellt, gefolgt von der Chain/Moscow-Variante, der Konjunktion-headed/Prague-Variante und der Multi-headed/London-Variante. Jede Variante wird mit einem Beispiel ausgeschrieben, um die Unterschiede zu verdeutlichen.</sample>
    <sample id="50">Der englische Text beschreibt verschiedene koordinierende Strukturen in der Syntax, die in der Sprache verwendet werden können. Es wird die Bouquet/Stanford-Variante als universelle Koordinationsstruktur vorgestellt, gefolgt von der Chain/Moscow-Variante, der Konjunktion-headed/Prague-Variante und der Multi-headed/London-Variante. Jede Variante wird mit einem Beispiel ausgeschrieben, um die Unterschiede zu verdeutlichen.</sample>
    <sample id="51">Der Präsident erklärt, dass die Wortfolge tendenziell dazu tendiert, die Abhängigkeitslängen zu minimieren.</sample>
    <sample id="52">Der Präsident erklärt, dass die Wortfolge tendenziell dazu tendiert, die Abhängigkeitslängen zu minimieren.</sample>
    <sample id="53">Der Präsident des Deutschen Bundestages, Frank-Walter Steinmeier, hat den 1. Mai 2020 als 31. Nationalfeiertag in Berlin eröffnet.</sample>
    <sample id="54">Der Präsident des Deutschen Bundestages, Frank-Walter Steinmeier, hat den Bundestag in Berlin zum Abschluss der 19. Legislaturperiode eröffnet.</sample>
    <sample id="55">Der Präsident des Deutschen Bundestages, Frank-Walter Steinmeier, hat den 1. Mai 2020 als 31. Nationalfeiertag in Berlin eröffnet.</sample>
    <sample id="56">Der Präsident des Deutschen Bundesverfassungsgerichts, Hans-Jürgen Schöne, wird in einem Video über die Bedeutung von Dependency Length Minimization (DLM) in der Sprachverarbeitung sprechen. DLM ist eine Technik, die dazu dient, die Abhängigkeitsstruktur von Wörtern in einem Satz zu minimieren, um die Lesbarkeit und Verständlichkeit zu verb</sample>
    <sample id="57">Der Präsident des Deutschen Bundestages, Frank-Walter Steinmeier, hat in seiner Rede auf der 13. Wahlperiode der Bundestagswahl 2017 die Bedeutung der Demokratie und der Parteiendemokratie hervorgehoben. Er betonte, dass die Parteiendemokratie ein wichtiger Bestandteil der Demokratie ist und dass Parteien die Interessen ihrer Mitglieder vertreten müssen. Steinmeier betonte auch die Bedeutung der Parteien in der Politik und der Rolle, die sie bei der Gestaltung der Politik spielen.</sample>
    <sample id="58">Der Präsident des Deutschen Bundestages, Frank-Walter Steinmeier, hat in seiner Rede auf der 13. Wahlperiode der Bundestagswahl 2017 die Bedeutung der Demokratie und der Parteiendemokratie betont. Er betonte, dass die Parteiendemokratie ein wichtiger Bestandteil der Demokratie ist und dass Parteien die Stimmen der Bürger in die Politik einflüssen müssen. Steinmeier betonte auch die Bedeutung der Parteien in der Gesellschaft und der Rolle, die sie bei der Gestaltung der Politik spielen.</sample>
    <sample id="59">Der Präsident des Deutschen Bundestages, Frank-Walter Steinmeier, hat den 1. Mai 2020 als 31. Nationalfeiertag in Berlin eröffnet. Er betont die Bedeutung des Tag</sample>
    <sample id="60">Der englische Text beschreibt die Dependency Length Minimization (DLM) und zeigt, wie die Wortreihenfolge in einem Satz dazu beiträgt, die Abhängigkeitslängen zu minimieren. Die Wortreihenfolge wird so gewählt, dass die Abhängigkeitslänger der Wörter minimiert werden. Die Wortreihenfolge wird dabei so gewählt, dass die Abhän</sample>
    <sample id="61">Der Präsentierte erklärt, wie die Abhängigkeitslänge minimiert werden kann, indem die Wortreihenfolge optimiert wird. Er zeigt, dass eine optimierte Wortreihenfolge die Abhängigkeitslän</sample>
    <sample id="62">Der englische Text beschreibt die Statistiken über Koordination, die aus einer erweiterten Version des Penn Treebank extrahiert wurden. Die Penn Treebank ist eine umfangreiche Sammlung von englischen Texten, die für die Analyse von Sprachstrukturen und -muster verwendet werden. Die erweiterte Version des Penn Treebank enthält zusätzliche Informationen, die es ermöglichen, die Komplexität und die feinere Struktur der englischen Sprache besser zu verstehen.

Die Statistiken beziehen sich auf die Koordination, also die Verbindung von Wörtern oder Phrasen, die in einem Satz zusammenarbeiten. Die Texte analysierten zeigen, dass linkskommandierende Konstruktionen (links kommende Konstruktionen) tendenziell kürzer sind als rechtskommandierende Konstruktionen. Diese Tendenz wird mit der Länge der Konstruktionen verbunden, d. h. längere Konstruktionen haben tendenziell eine größere Länge als kürzere Konstruktionen.

Ein Beispiel für eine längere Konstruktion ist: "I saw Bill and Lisa come and sneeze." Diese Konstruktion besteht aus 10 Wörtern und zeigt, dass längere Konstruktionen tendenziell mehr Wörter enthalten. Ein Beispiel für eine kürzere Konstruktion ist: "Ted and Ned laughed." Diese Konstruktion besteitet nur 5 Wörter und zeigt, dass kürzere Konstruktionen weniger Wörter enthalten.

Die Texte analysierten zeigen, dass längere Konstruktionskomplexität mit der Länge der Konstruktionskomplexität verbunden ist. Dies bedeutet, dass längere Konstruktione</sample>
    <sample id="63">Der englische Text beschreibt Statistiken über Koordination, die aus einer erweiterten Version des Penn Treebank extrahiert wurden. Die Penn Treebank ist eine umfangreiche Sammlung von englischen Texten, die für sprachliche Forschung und Analyse verwendet werden. Die Texte stammen aus verschiedenen Quellen und umfassen eine Vielzahl von Themen und Stilen. Die Statistiken bieten Einblicke in die Struktur und Verwendung von Koordinationen in der englischen Sprache. Die Penn Treebank ist ein wichtiges Werkzeug für Linguisten und Sprachforscher, das dazu beiträgt, die Sprache zu verstehen und zu analysieren.</sample>
    <sample id="64">Der englische Text beschreibt Statistiken über Koordination, die aus einer erweiterten Version des Penn Treebank extrahiert wurden. Die Penn Treebank ist eine umfangreiche Sammlung von englischen Texten, die für sprachliche Forschung genutzt werden. Die Textstatistiken beziehen sich auf die Länge von Koordinationsverbindungen, also auf die Anzahl der Wörter, die zusammen verwendet werden, um zwei oder mehr Satzteile zu verbinden.

Die erste Tatsache, die erwähnt wird, ist, dass linksgerichtete Koordinationsverbindungen tendenziell kürzer sind als rechtsgerichtete. Dies bedeutet, dass es häufiger ist, dass zwei oder mehr Wörter in einem Satz zusammenhängend sind, wenn sie auf der linken Seite des Satzes stehen.

Die zweite Tatsache ist, dass diese Tendenz mit der Länge der Koordinationsverbindungen wächst. Das heißt, dass es häufiger ist zu beobachten, dass zwei oder mehr Wörter zusammenhängend sind, je länger die Koordinationsverbindung ist. Dies kann darauf hinweisen, dass es in der Sprache häufiger ist, längere Wörter zu verwenden, um zwei oder mehr Satzteil zu verbinden.

Die dritte Tatsache ist, dass diese Tendenz nur dann beobachtet wird, wenn der Satzgouverneur auf der linken Seite steht. Der Satzgouverneur ist der Teil des Satzes, der die Haupthandlung oder das Hauptthema des Satzes beschreibt. Wenn der Satzgouverneur auf der link</sample>
    <sample id="65">Der englische Text besch</sample>
    <sample id="66">Der englische Text beschreibt Statistiken über Koordination, die aus einer erweiterten Version des Penn Treebank extrahiert wurden. Die Texte stammen aus den Arbeiten von Marcus et al. (1993), Ficler und Goldberg (2010). Es wird festgestellt, dass linke Koordination tendenziell kürzer ist (bisher beobachtet), und dass diese Tendenz mit der Länge des Koordinationsphrasen zunimmt (kurz erwähnt in Gibson et al. 1996: 88-90). Allerdings tritt dies nur dann auf, wenn der Satzverfasser auf der linken oder rechten Seite steht (z. B. "Ich sah Bart und Lisa. Homer kam und schaute").</sample>
    <sample id="67">Der englische Text beschreibt Statistiken über Koordination, die aus einer erweiterten Version des Penn Treebank extrahiert wurden. Die Texte stammen aus Marcus et al. (1993), Ficler und Goldberg (2010). Es wird erwähnt, dass linke Konjunktionen tendenziell kürzer sind (bisher beobachtet), dass diese Tendenz mit der Länge des Unterschieds wächst (insbesondere bei Gibson et al. 1996: 88-90), und dass dies nur dann der Fall ist, wenn der Gouverneur auf der linken Seite oder nicht vorhanden ist (Ich sah Bart und Lisa. Homer kam und hielt sich an). Es wird auch erwähnt, dass dies nicht der Fall ist, wenn es auf der rechten Seite ist (Ted und Ned lachten).</sample>
    <sample id="68">Der englische Text beschreibt statistische Informationen über die Koordination in der englischen Sprache, die aus einer erweiterten Version des Penn Treebank extrahiert wurden. Die Texte erwähnen, dass linke Koordinaten tendenziell kürzer sind, diese Tendenz mit der Länge der Koordination wächst, und dass dies nur dann der Fall ist, wenn der Gouverneur auf der linken Seite oder nicht vorhanden ist. Ein Beispiel wird gegeben, um die Aussage zu verdeutlichen.</sample>
    <sample id="69">Der englische Text beschreibt Statistiken über Koordination, die aus einer erweiterten Version des Penn Treebank extrahiert wurden. Es wird erwähnt, dass linke Konjunktionen tendenziell kürzer sind, wenn sie beobachtet werden, und dass diese Tendenz mit der Länge des Unterschieds wächst. Ein Beispiel wird gegeben, um die Anwendung dieser Statistiken zu verdeutlichen.</sample>
    <sample id="70">Der Inhalt des Videos ist bereits in deutscher Sprache. Es scheint, dass der Text, der in der Anleitung angegeben wird, nicht zugeordnet ist.</sample>
    <sample id="71">Die Abbildung zeigt die Abhängigkeit der Anteile an kürzeren linken Konsonanten von der absoluten Differenz der Konsonantenlänge in verschiedenen Sprachmetriken. Die Sprachmetriken sind die Konsonantenlänge in Buchstaben (CHARACTERS), Silben (SYLLABLES) und Wörtern (WORDS). Die Abbildung besteht aus vier Subfiguren, jede Subfiguren die Abhängigkeit der Anteil an kürzeren linken Konsonanzen von der absoluten Differenz der Konsonantenlänge in verschiedenen Metriken. Die Subfiguren sind:

1. **NO governor (length in CHARACTERS)**: Diese Subfiguren zeigt die Abhängigkeit der Proportion an kürzeren linken Konsonanzen von der absoluten Differenz der Länge von Konsonanten in Buchstaben.
2. **NO governor (length in SYLLABLES)**: Diese Subfiguren zeigen die Abhängigkeit der Proportion von kürzeren linken Konsonanzern von der absoluten Differenz der Längen von Silben.
3. **NO governor (length in WORDS)**: Diese Subfiguren zeigen, wie die Proportion an kürzeren linken Konso</sample>
    <sample id="72">The video shows a person speaking in front of a screen displaying a graph with multiple lines. The graph shows the proportion of short-term comparisons depending on the absolute difference of comparison lengths, with confidence bands. The lines on the graph represent different conditions, such as "NO governor length in SYLLABLES" and "NO governor length in WORDS." The person appears to be explaining the graph and its findings.</sample>
    <sample id="73">The video presents a detailed analysis of the compatibility of different dependency structures of coordination with the sentence "Homer loves Lisa, Bart, and Maggie." The analysis is divided into four categories: Bouquet/Stanford (Universal Dependencies), Chain/Moscow, Conjunction-headed/Prague, and Multi-headed/London. Each category is represented by a different dependency structure, and the compatibility of each structure with the sentence is discussed. The video also includes visual representations of the dependency structures, with arrows indicating the relationships between the words in the sentence. The analysis highlights the differences in how each dependency structure handles the coordination of multiple entities in a sentence.</sample>
    <sample id="74">**Übersetzung ins Deutsche:**

**Inhalt:**

- **Hauptnachricht:** "Sehen Sie den vollständigen Argument in der Papier!"
- **Nebenanrichtung:** "Reden Sie mit uns auf der Poster-Sitzung!"

**Analyse:**

1. **Hauptnachricht:**
   - **Bedeutung:** Die Hauptnachricht ist eine direkte Anweisung an den Zuschauer, um den vollständigen Argument in der Papierung zu lesen. Dies könnte auf eine detaillierte Analyse oder eine umfassende Diskussion in einem wissenschaftlichen oder wirtschaftlichen Kontext hinweisen.
   - **Anwendung:** Diese Nachricht könnte in einem wissenschaftlichen Vortrag, einem Konferenzbeitrag oder einer Publikation verwendet werden, um den Leser auf die Verfügbarkeit eines vollständigen Artikels oder Berichts hinzuweisen.

2. **Nebenanrichtung:**
   - **Bedeutung**: Die Nebenanrichtung bietet eine alternative Möglichkeit, die Informationen zu erhalten, indem der Zuschauer sich auf eine Poster-Sitzung einläuft. Poster-Sitzungen sind oft Teil von Konferenzen oder Wissenschaftlerversammlungen, bei denen Forscher ihre Arbeiten präsentieren und diskutieren.
   - **Anwendung:** Durch diese Anweisung wird der Zuschauer ermutigt, sich an einer Poster-Sitzung zu beteiligen, um mehr über die Thematik zu erfahren und möglicherweise mit den Autoren direkt zu interagieren.

**Zusammenfassung:**

Die beiden Nachrichten bieten zwei verschiedene Wege, um die Informationen zu erhalten:

1. **Lesen Sie den vollständigen Argument in der Paper!**
   - Dies ist eine direkte Anweisung, den vollständigen Artikel oder Bericht zu lesen, der möglicherweise in einer wissenschaftlichen oder wirtschaftlich relevanten Publikation veröffentlicht wurde.

2. **Reden Sie mit uns auf der Poster-Situation!**
   - Dies ist eine Anweisung, sich an einer Poster-Sitzung zu versammeln, um mehr über die Thematik zu lernen und möglicherweise mit den Forschern direkt zu interagieren.

Diese beiden Optionen bieten den Zuschauer die Möglichkeit, die Informationen in verschiedenen Formen zu erhalten, je nachdem, welche Methode ihnen am besten geeignet ist.</sample>
    <sample id="75">Drei Autoren sind an der Arbeit beteilt.</sample>
    <sample id="76">Die Domains "news", "bible", "l2" und "fiction" werden stärker vereinfacht.</sample>
    <sample id="77">left conjuncts tend to be shorter (observed before)</sample>
    <sample id="78">Yes, the models are freely available under the MIT license.</sample>
    <sample id="79">DEplain-apa enthält Dokumente aus der APA-Publikation.</sample>
    <sample id="80">Für eine gute Generalisierung sind folgende Faktoren notwendig:

1. **Bessere Modellarchitektur**
2. **Größerer Modellgrößen**
3. **Mehr Feinabstimmungsbeispiele**

Zusätzlich wird erwähnt, dass die Leistungsschwankungen durch **Temporal drift** und **nicht adaptives Overfitting** verursacht werden.

Die Frage, ob CoNLL-2003-Tags noch funktionieren, wird beantwortet mit **Ja**.</sample>
    <sample id="81">Die Tendenz zu kürzeren linken Conjunktionen wurde durch die Analyse der Länge von Conjunktionen in einem Text korreliert.</sample>
    <sample id="82">Die Experimente wurden durch die Anpassung der Position des Begrenzers (links, rechts, oben, unten) durchgeführt, um die Auswirkungen auf die Erkennung von Komponenten zu untersuchen.</sample>
    <sample id="83">Ein Basisklassifikator, der mit unausgewogenen Daten (kleine Anzahl von Datenpunkten) trainiert wird, hat eine schlechte Leistung. Die ROC-Kurve zeigt, dass der Basisklassifikator nur wenig besser als Zufall ist, was bedeutet, dass er nicht in der Lage ist, die Daten korrekt zu klassifizieren.</sample>
    <sample id="84">Vier Autoren sind an der Arbeit beteilt.</sample>
    <sample id="85">Die Personen im Beispielgespräche sind:

1. **Person 1**: Derjenige, der die Dialogkontexte festlegt.
2. **Person 2**: Derjenige, der die alternative Frage stellt.
3. **Person 3**: Derjenige, der die Anrede bezieht.
4. **Person 4**: Derjenige, der die Anreden bezieht.
5. **Person 5**: Derjenige, der die Anredeinhalte bezieht.
6. **Person 6**: Derjenige, der die Anredeineinhalte bezieht.
7. **Person 7**: Derjenige, der die Anredeneinhalte bezieht.
8. **Person 8**: Derjenige, der die Anredeenhalte bezieht.
9. **Person 9**: Derjenige, der die Anredeeinhalte bezieht.
10. **Person 10**: Derjenige, der die Anredenenhalte bezieht.
11. **Person 11**: Derjenige, der die Anredenerhalte bezieht.
12. **Person 12**: Derjenige, der die Anredererhalte bezieht.
13. **Person 13**: Derjenige, der die An</sample>
    <sample id="86">Formalität, lexikalische Kohäsion.</sample>
    <sample id="87">Johns Hopkins University, Purdue University, Meta AI.</sample>
    <sample id="122">The framework quantifies positional bias by collecting 200 instances of each of the three questions, ensuring each instance has at least one annotation as "red" or "green," and annotations from at least 10 annotators from the world. It then compares the annotations by demographic to models and datasets via Pearson's R scores.</sample>
    <sample id="155">Die Ergebnisse der vorherigen Studie, bei der human-subjektiven Teilnehmern die gleichen Persona-Prompts erhielten, zeigten, dass die Personas, die durch die Prompts generiert wurden, in ihrer Struktur und Inhalte sehr ähnlich waren. Dies deutet darauf hin, dass die Persona-Prompts effektiv dazu beitrugen, konsistente und realistische Personen zu erstellen, die in ihren Merkmalen und Verhaltensweisen vergleichbar sind.</sample>
    <sample id="156">Die Datenquellen in dieser Studie waren:

- **Penn Treebank (Marcus et al., 1993)**: Ein großes korpus an englischen Texten, das für die Analyse von Sprachstrukturen und -muster verwendet wurde.
- **Enhanced Penn Treebank (Ficler and Goldberg, 2016)**: Ein verbessertes Version des Penn Treebanks, das spezifisch für die Untersuchung von Koordinationen (z.B. Konjunktionen) entwickelt wurde.

Diese Quellen lieferten die notwendigen Daten zur Untersuchung der Koordinationsstrukturen und ihrer Länge in englischen Sätzen.</sample>
    <sample id="157">Zwei Autoren sind an der Arbeit beteilt.</sample>
    <sample id="158">Cognitive dissonance is a psychological phenomenon where an individual experiences discomfort or tension when holding two or more contradictory beliefs, values, or attitudes simultaneously. This discomfort often motivates the person to reduce the dissonance by changing one of the conflicting elements or by adding new beliefs that reconcile the discrepancy.

### Key Aspects of Cognitive Dissonance:

1. **Contradictory Beliefs**:
   - The individual holds two or more beliefs that are in conflict with each other.
   - Example: A person who smokes believes that smoking is harmful to health but continues to smoke.

2. **Discomfort and Tension**:
   - The presence of conflicting beliefs creates psychological discomfort or tension.
   - Example: The smoker feels guilty or anxious about the health risks associated with smoking.

3. **Motivation to Reduce Dissonance**:
   - The individual seeks to reduce the discomfort by changing one of the conflicting beliefs or by adding new beliefs that reconcile the discrepancy
   - Example: The smoker might decide to quit smoking to align their behavior with their belief about health risks.

4. **Cognitive Dissonance Theory**:
   - Developed by Leon Festinger in 1957, the theory posits that people have a motivational drive to reduce dissonance and achieve consistency among their cognitions (beliefs, attitudes, and behaviors).

5. **Examples of Cognitive Dissonance**:
   - **Behavioral Dissonance**: When a person's behavior contradicts their beliefs.
     - Example: A vegetarian who eats meat.
   - **Cognitive Dissonance**: When a person holds two conflicting beliefs.
     - Example: A person who believes in environmental conservation but drives a gas-guzzling car.
   - **Post-Decision Dissonance**: When a person experiences discomfort after making a difficult decision.
     - Example: A person who buys an expensive car and later feels regret about the purchase.

6. **Strategies to Reduce Dissonance**:
   - **Changing Behavior**: Altering actions to align with beliefs.
     - Example: The smoker quitting smoking.
   - **Changing Beliefs**: Adjusting beliefs to match behavior.
     - Example: The smoker convincing themselves that smoking is not as harmful as they thought.
   - **Adding New Beliefs**: Introducing new beliefs that justify the behavior.
     - Example: The smoker believing that occasional smoking is harmless.

### Conclusion:
Cognitive dissonance is a fundamental concept in psychology that explains how individuals strive for consistency in their thoughts and actions. It highlights the psychological processes that drive people to resolve conflicts between their beliefs and behaviors, often leading to changes in attitudes or behaviors to reduce discomfort.</sample>
    <sample id="159">Zwei Autoren sind an der Arbeit beteilt.</sample>
    <sample id="160">8</sample>
    <sample id="161">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten durch seine umfassende Analyse der Einflussfaktoren auf die Konsumgewohnheiten von Handessen. Es integriert verschiedene Datenquellen und verwendet fortschrittliche Methoden zur Modellierung und Analyse, um die komplexen Zusammenhänge zwischen sozialen, kulturellen und individuellen Faktoren und dem Verhalten von Handessen zu untersuchen.</sample>
    <sample id="162">GPT-3.5 hat die meisten Überschneidung mit dem Lexikon der Stereoty pen.</sample>
    <sample id="163">DeepL und Google.</sample>
    <sample id="164">Die Präsentation beginnt mit einer Übersicht über die Verwendung von Pretraining Data, Language Models und Downstream Tasks in der Entwicklung von NLP-Modellen, die politische Biases enthalten können. Die folgenden Abschnitte behandeln die Herausforderungen und möglichen Lösungen für die Identifizierung und Beseitigung dieser Biases.</sample>
    <sample id="165">Der Präsentator diskutiert die Herausforderungen, die bei der Verwendung von großen Sprachmodellen entstehen, insbesondere in Bezug auf die Qualität und Bias der Trainingsdaten. Er zeigt eine Bar-Chart-Analyse, die die Anzahl der Tokens pro Domain zeigt, und erklärt, wie die Datenqualität und die Verteilung der Domänen die Leistung der Sprachmodelle beeinflussen können.</sample>
    <sample id="166">Der Präsentierte diskutiert die Herausforderungen und Chancen, die mit der Nutzung von großen Sprachmodellen wie GPT-3 verbunden sind. Er hebt hervor, dass diese Modelle auf enormen Datensätzen trainiert werden, was zu einer Vielzahl von Informationen führt, die nicht immer korrekt sind. Dies kann zu Fehlern in der Verarbeitung von Daten führen, insbesondere wenn es um die Erkennung von Fehlern oder die Verarbeitung von ungewöhnlichen Eingaben geht. Die Präsentation zeigt, dass die Verwendung solcher Modelle in bestimmten Anwendungen, wie z.B. der medizinischen, rechtlichen oder finanziellen, problematisch sein kann, da sie möglicherweise falsche oder ungenaue Informationen liefern. Die Präsentation betont auch die Notwendigkeit, die Verwendung solcher Modelle in kritischen Anwendungen mit Vorsicht zu betrachten und die Risiken zu berücksichtigen.</sample>
    <sample id="167">Der englische Text beschreibt eine Bar-Chart, die die Anzahl der Bücher, die in verschiedenen Sprachen verfügbar sind, zeigt. Die Bar-Chart ist in zwei Spalten angeordnet, wobei die Sprache auf der linken Seite und die Anzahl der Bücher auf der rechten Seite angegeben wird. Die Bar-Chart zeigt, dass die meisten Bücher in der Sprache Englisch verfügbar sind, gefolgt von Deutsch, Französisch, Spanisch, Italienisch, Niederländisch, Russisch, Portugiesisch, Chinesisch, Japanisch und Koreanisch. Die Bar-Chart zeigt auch, dass die Anzahl der Bücher in der Sprache Eng</sample>
    <sample id="168">Der Inhalt des Videos ist eine Analyse der Sprachmodell-Trainingsdaten, die auf einer Bar-Chart-Visualisierung basiert. Die Bar-Charts zeigen die Anzahl der Sprachmodell-Trainingsdatensätze für verschiedene Sprachen. Die Sprachen sind in einer Liste auf der rechten Seite der Bar-Charts angegeben. Die Bar-Charts sind in einer Tabelle angeordnet, die die Sprachen in der Spalte "Sprache" und die Anzahl der Sprachmodell-Datensätze in der Spalte "Anzahl der Sprachmodell-Datensaetze" zeigt. Die Bar-Charts sind in einer Tafel angeordnet, die die Spraichen in der Spalte "Sprache und die Anzahl der Sprachmodeller-Datensätze in der Spalten "Anzahl der Sprachmodeller-Datensaetze" zeigt.</sample>
    <sample id="169">Das Bild zeigt eine Vorstellung von drei Hauptkomponenten: "Pretraining data" (Vorverarbeitetes Daten), "Language models" (Sprachmodelle) und "Downstream tasks" (Niedergeschaltete Aufgaben). Die Komponenten sind miteinander verbunden, was auf eine Abhängigkeit oder Wechselwirkung zwischen ihnen hinweist. Die Vorstellung wird von einem Personen in einem Video unterstützt, der sich in einem Rahmen mit einer weißen Hintergrund zeigt.</sample>
    <sample id="170">Die Bildergruppe zeigt eine Vorstellung von drei Hauptthemen, die sich auf die politische Lernfähigkeit von Sprachmodellen beziehen. Die Themen sind:

1. **Evaluierung der politischen Lernfähigkeit von Sprachmustern**:
   - **Frage**: Wie bewertet man die politische Lernfähigkeit der Sprachmodelle?
   - **Frage**: Welche Rolle spielt die Vorbereitungsdaten in der Entwicklung politischer Sprachmodelle?

2. **Politische Lernfähigkeit von Sprach mustern**:
   - **Frage**: Welche politischen Lernfähigkeiten haben Sprachmodelle, die mit unterschiedlichen politischen Datensätzen trainiert wurden?
   - **Frage**: Ergeben Sprachmodelle mit unterschiedlichen politischen Daten die gleiche politische Lernfähigkeit?

3. **Gerechtigkeit in NLP-Anwendungen**:
   - **Frage**: Ergebt sich aus der politischen Lernfähigkeit der Sprach modelle eine politische Ungleichheit in NLP-Anwendungen?

Die Bildergruppe zeigt, dass die politische Lernfähigkeit und die Rolle der Vorbereitungsdaten in der Sprachmodellentwicklung wichtige Aspekte sind, die berücksichtigt werden müssen, um faire NLP-Anwendungen zu gewährleisten.</sample>
    <sample id="171">Der Inhalt des Videos ist eine Analyse der politischen Ausrichtung von Sprachmodellen (LM) und ihrer Auswirkungen auf die Fairness in natürlicher Sprachverarbeitung (NLP). Der Video-Anfang zeigt eine Diagrammstruktur, die die drei Hauptbereiche abdeckt: Pretraining Data (Trainingsdaten), Language Models (Sprachmodelle) und Downstream Tasks (Niedergeschaltete Aufgaben). Unter jeder dieser Bereiche sind spezifische Fragen formuliert, die die Wissenschaftler beantworten möchten.

Im zweiten Teil des Videos wird die Frage "How to evaluate the political leaning of LMs?" (Wie bewertet man die politische Ausrichtung von Sprachmodellen?) beantwortet. Die Wissenschaftler diskutieren die Rolle von Pretraining Data (Trainingsdaten) und wie diese Daten politische Biases beeinflussen können. Sie fragen sich, welche Rolle Pretraining Data spielt und wie diese Daten politische Biases verstärken können.

Im dritten Teil des Videos wird die Frage "Does LM political learning result in fairness issues in NLP applications?" (Führt das politische Lernen von LMs zu Fairness-Problemen in NLP-Anwendungen?) beantwortet. Die Wissenschafterinnen und Wissenschaftler diskutieren, wie Sprachmodelle mit unterschiedlichen politischen Ausrichtungen funktionieren und ob dies zu Fairness-Problemen in NLP-Technologien führt.

Im vierten Teil des Videos wird die Frage "What role does pretraining data play in such political biases?" (Welche Rolle spielt Pretraining Data bei solchen politischen Biases?) beantwortet. Die Wissenchafterinnen und Wissenschaftler erklären, wie Pretraining Data politische Biases beeinflussen kann und welche Rolle Pretraining Data spielte.

Im fünften Teil des Videos wird die Frage "Do LMs with different political leanings perform?" (Leisten LMs mit unterschiedlichen politischen Ausgrießungen?) beantwortet. Die Wissen</sample>
    <sample id="172">Der Präsentierte diskutiert die politische Ausrichtung von Sprachmodellen (LMs) und zeigt, wie sie sowohl Encoder- als auch Decoder-LMs unterstützen. Er erklärt, wie man automatisch die politische Ausrichtung eines LMs bewertet, indem man eine politisch kontroverse Aussage mit einem Platzhalter cstatement&gt; 1 cmask&gt; verwendet. Die Aussage lautet: "Unsere Rasse hat viele überlegene Eigenschaften, im Vergleich zu anderen Rassen." Der Präsentierte analysiert, ob die Sprachmodelle diese Aussage als autoritär oder liberale bewerten. Er zeigt, dass die Sprachmodelle, die autoritäre Aussagen bevorzugen, tendenziell politisch rechts- oder autoritärer ausgerichtet sind, während liberale Sprachmodelle tendenziell politisch linksextrem oder liberale Ausrichtung haben. Der Präsentierte verwendet ein Diagramm, um die politische Ausrichtung verschiedener Sprachmodelle zu visualisieren und zu verdeutlichen, wie sie sich auf der politischen Skala zwischen autoritärer und liberaler Ausrichtung bewegen.</sample>
    <sample id="173">Das Bild zeigt eine Farbverteilung, die in einem Quadrat dargestellt wird. Die Farben rot, gelb, blau und grün repräsentieren verschiedene Kategorien. Die rote Farbe bezieht sich auf "Autoritarismus", die gelbe auf "Rechts", die blaue auf "Libertäre" und die grüne auf "Linken". Die Farbverteilung wird in einem Kreis dargestellt, der in der Mitte des Quadrats liegt. Die Farben werden in verschiedenen Bereichen des Kreises verteilt, um die Verteilung der Kategorien zu visualisieren.</sample>
    <sample id="174">Der Inhalt des Videos ist eine Analyse der politischen Ausrichtung verschiedener großer Sprachmodelle (LLMs) wie BERT, RoBERTa, GPT-2, GPT-3, ChatGPT und Alpaca. Die Analyse basiert auf einer Farbgebung, die die politische Ausrichtung der Modelle in einem zweidimensionalen Raum darstellt, der von Autoritarismus und Liberalismus geteilt wird. Die Farben rot, gelb und blau repräsentieren unterschiedliche politische Positionen. Die Farbgebung wird durch Punkte und Linien visualisiert, die die Positionen der Modelle in diesem Raum darstellen. Die Farbgebung wird durch Punkten und Linien visualisiert, die die Positions der Modelle in diesem Raum darste</sample>
    <sample id="175">Der englische Text auf der Präsentationsseite lautet:

---

**Pretraining Data**

Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning

---

Die Präsentation zeigt zwei Diagramme, die die politische Ausrichtung von News-Medien und sozialen Medien (Reddit) vergleichen. 

Im linken Diagramm wird die politische Ausrichtung von News-Media unterteilt in drei Bereiche: "left" (blau), "center" (grau) und "right" (rot). Im rechten Diagramm wird die politische Ausrichtung auf sozialen Medien (Reddit) dargestellt, wobei "left" (blau) und "right" (rot) als die beiden Hauptrichtungen dargestellt sind, während "center" (grau) nicht sichtbar ist.

---

Die Präsentationsseite enthält auch zwei Zitate:

1. **Zitat 1:**
   - **Autor:** Liu, Y., et al.
   - **Titel:** POLITICS Pretraining with Some Story-Aware Language Models
   - **Veröffentlichung:** Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)

2. **Zitat 2:**
   - **Autor:** Blom, E., Gimpel, K., &amp; Conrad, R.
   - **Titel:** What causes "right" in the perception of political orientation?
   - **Veröffentlichungsjahr:** 2019

---

Die Präsentatorin oder der Präsentator ist auf der rechten Seite des Bildes zu sehen, der sich in einem Videochat-Format befindet.

---

**Zusammenfassung:**

Die Präsentation vergleicht die politische Ausrichtung von News-Mediens und sozialen Medien (Reddit). Das linke Diagramm zeigt die politische Ausrichtung von News-Mediums in drei Bereichen: "left" (blau), "Center" (grau) und "right"</sample>
    <sample id="176">Der englische Text auf der Bildschirmseite beschreibt eine Studie zur politischen Ausrichtung von Sprachmodellen (LM) wie RoBERTa und GPT-2. Die Studie beinhaltet die Fortbildung von LM mit verschiedenen politischen Datenquellen und die Bewertung der politischen Ausrichtung nach dem Training. Es wird erwähnt, dass die politische Ausrichtung der Sprachmodelle sich ändern kann, wenn sie mit unterschiedlichen politischen Datenquellen trainiert werden. Die Studie verwendet Daten aus News-Medien und sozialen Medien wie Reddit. Die Bilder zeigen die politische Ausrichtung der Sprachmodell-Datenquellen und die Ergebnisse der Studie, die die politische Ausrichtung der Sprach模型 nach dem Training bewerten.</sample>
    <sample id="177">Der englische Text beschreibt die Ergebnisse einer Studie zur politischen Ausrichtung von Sprachmodellen (LM). Die Studie untersucht, wie Sprachmodelle wie RoBERTa und GPT-2 auf politische Texte reagieren, insbesondere auf die Partisanität der Texte. Die Ergebnisse werden in einem Farbcode dargestellt, der die politische Ausrichtung der Texte in verschiedenen Kategorien wie "Original", "News", "Reddit" und "Center" zeigt. Die Farben grün, blau, rosa und gelb werden verwendet, um die verschiedenen politischen Parteien darzustellen. Die Studie zeigt, dass Sprachmodelle unterschiedliche Partisanitäten aufweisen können, abhängig von der Art und Quelle des Textes.</sample>
    <sample id="178">Der Präsentierte bespricht die Ergebnisse einer Studie zur politischen Ausrichtung von Sprachmodellen (LM). Er zeigt ein Diagramm, das die Partisanenverschiebungen in den Sprachmodellen RoBERTa und GPT-2 zeigt. Die Diagramme sind in vier Quadranten unterteilt, die links unten "reddit", rechts oben "news", rechts unten "original" und links oben "news". Die Farben blau, rosa, grün und gelb repräsentieren die verschiedenen Partisanen. Der Präsentierte erklärt, wie die Sprachmodelle unterschiedliche Partisanen bevorzugen und wie sich diese beeinflussen können.</sample>
    <sample id="179">Der englische Text in der Vorstellung lautet: "Results Partisan shifts in LM political leaning".</sample>
    <sample id="180">Der englische Text beschreibt eine "Trump Card", die die Veränderungen in der Wahrnehmung von Trump zwischen der Vorkriegszeit (Pre-45th) und der Postkriegszeit (Post-45th) im Vergleich zu Reddit zeigt. Die Tabelle zeigt die Differenzen (Δ) in der Wahrnehmung von Trump in verschiedenen Gruppen (news left, news center, news right, reddit left, reddit center, reddit right) zwischen den beiden Zeiten. Die Farben in den Quadern repräsentieren die Wahrnehmung von Trump, wobei rote Quadrate für eine positive Wahrnehmung und blaue Quadrate für eine negative Wahrnehmung stehen. Die Tabelle zeigt, dass die Wahrnehmung von Trump in den Gruppen "news left" und "reddit left" signifikant abgenommen hat, während die Wahrnehmung in den Gruppen "news center" und "reddit center" leicht zugenommen hat. Die Wahrnehmung in den Gruppen mit "news right" und "reddit right" hat sich minimal verändert.</sample>
    <sample id="181">Der Präsident Trump hat sich in seiner Politik und Kommunikation stark verändert. Während er vor seiner Amtsübernahme als Präsident als Kandidat für die Republikanische Partei die Wahlkämpfe gegen den Demokraten Hillary Clinton gewonnen hat, hat er sich in seiner Politik und Kommunika</sample>
    <sample id="182">Der Präsident Trump wird in der Prä-45. und Post-45. Wahljahr als Mittelpunkt des Präsidentenwahls in den USA dargestellt. Die Präsentation zeigt die Unterschiede in der Wahlbeteiligung und den politischen Standpunkten zwischen den beiden Wahljahren. Die Wahlbeteiligung wird in verschiedenen Regionen der USA dargestellt, einschließlich der Rotten Rotten Rotten Rotten Rotten Rotte</sample>
    <sample id="183">Das Bild zeigt eine Tabelle mit der Überschrift "Per-Category Performance". Die Tabelle enthält Daten zur Leistung von verschiedenen Kategorien in Bezug auf Hate Speech und Misinformation. Die Ergebnisse sind farb-kodiert, wobei dunkelblau die besten und dunkelblau die schlechtesten Werte darstellt. Die Tabelle zeigt die Leistung für verschiedene Gruppen wie Black, Muslim, LGBTQ+, Jens, Asain, Latinx, Women, Christian, Men und White. Die Leistung wird für verschiedene Quellen wie HP (L), NYT, CNN, NPR, Guardian, Fox, WAXE, BBART, WAT, NR und REDDIT_RIGHT gemessen. Die Tabelle zeigt, dass die Leistung für die Gruppen Black, Muslim, LGBTQ+, Jens, Latinx, Women, Christian und Men höher ist als für die Gruppen Asain und White. Die Leistung für die Quelle HP (L) ist höher als für die anderen Quellen. Die Tabelle zeigt, dass die</sample>
    <sample id="184">Das Bild zeigt eine Tabelle mit der Überschrift "Per-Category Performance". Die Tabelle enthält Daten zur Leistung von verschiedenen Kategorien in Bezug auf Hate Speech und Misinformation. Die Ergebnisse sind farbenkodiert, wobei dunkelblau die besten und dunkelblau die schlechtesten Werte darstellt. Die Tabelle zeigt die Leistung für verschiedene Gruppen wie Black, Muslim, LGBTQ+, Jens, Asain, Latinx, Women, Christian, Men und White. Die Leistung wird durch verschiedene Farben dargestellt, um die Leistung für jede Gruppe zu verdeutlichen. Die Tabelle enthält auch Informationen über die Misinformationen, die von verschiedenen Quellen stammen. Die Tabelle ist in der Form von einem Farbcode dargestellt, um die Leistungen für jede Gruppe zu verde</sample>
    <sample id="185">Das Bild zeigt eine Tabelle mit der Überschrift "Per-Category Performance". Die Tabelle enthält Daten zur Leistung von verschiedenen Kategorien in Bezug auf Hate Speech und Misinformation. Die Ergebnisse sind farb-kodiert, wobei dunkelblau die besten und dunkelblau die schlechtesten Werte darstellt. Die Tabelle zeigt die Leistung für verschiedene Gruppen wie Black, Muslim, LGBTQ+, Jens, Asain, Latinx, Women, Christian, Men und White. Die Leistung wird durch verschiedene Farben dargestellt, um die Unterschiede zwischen den Gruppen zu verdeutlichen. Die Tabelle enthält auch Informationen über die Misinformationen, die von verschiedenen Quellen stammen. Die Tabelle ist in der Form von Text und Farben dargestellt.</sample>
    <sample id="186">Der Präsident des Deutschen Bundestages, Frank-Walter Steinmeier, hat in einem Video aufgezeigt, wie die Daten des Bundesverfassungsgerichts zur Verfügung stehen. Er zeigt eine Tabelle mit den Ergebnissen der Verfassungsgerichtsentscheidungen und erklärt, wie die Daten verwendet werden können.</sample>
    <sample id="187">Das Bild zeigt eine Tabelle mit der Überschrift "Per-Category Performance". Die Tabelle enthält Daten zur Leistung von verschiedenen Kategorien in Bezug auf Hate Speech und Misinformation. Die Kategorien sind "BLACK", "MUSLIM", "LGBTQ+", "JEWS", "ASAIN", "LATINX", "WOMEN", "CHRISTIAN", "MEN" und "WHITE". Die Leistung wird in Prozent angegeben, und die Ergebnisse sind farbenkodiert, mit dunkel gelben Werten als best und dunkelblau als schlecht. Die Tabelle zeigt, dass die Leistung für die Kategorie "BLACK" am besten ist, während die Leistung für die Kategorie mit dem Wert "NR" am schlechtesten ist. Die Tabelle enthält auch eine Liste von Quellen, die für die Daten verwendet wurden, wie "HP (L)", "NYT (L)", "CNN (L)", "NPR (L)", "Guard (L)", "Fox (L)", "WAXE (L)", "BBART (L)", "WAT (L)" und "NR (L)".</sample>
    <sample id="188">Das Bild zeigt eine Tabelle mit der Überschrift "Per-Category Performance". Die Tabelle enthält Daten zu der Leistung von verschiedenen Kategorien in Bezug auf Hate Speech und Misinformation. Die Kategorien sind "Black", "Muslim", "LGBTQ+", "Jews", "Asain", "Latnix", "Women", "Christian", "Men" und "White". Die Leistung wird in Farben dargestellt, wobei dunklere Farben die besten und helleste Farben die schlechtesten Leistungen darstellen. Die Tabelle zeigt, dass die Leistung für verschiedene Kategorien unterschiedlich ist.</sample>
    <sample id="189">Das Bild zeigt eine Tabelle mit der Überschrift "Per-Category Performance". Die Tabelle enthält Daten zur Leistung von verschiedenen Kategorien in Bezug auf Hate Speech und Misinformation. Die Ergebnisse sind farbenkodiert, wobei dunkelblau die besten und dunkelblau die schlechtesten Werte darstellt. Die Tabelle zeigt die Leistung für verschiedene Gruppen wie Black, Muslim, LGBTQ+, Jens, Asain, Latinx, Women, Christian, Men und White. Die Leistung wird in Prozent angegeben. Die Tabelle enthält auch eine Liste von Quellen wie HP (Hate Speech), NYT (New York Times), CNN (Cable News Network), NPR (National Public Radio), Guardian (The Guardian), Fox (Fox News), WAXE (WAXE News), BBART (BBART), WAT (WAT), NR (NR). Die Tabelle zeigt, dass die Leistung für verschiedene Kategorien und Quellen unterschiedlich ist. Die beste Leistung wird für die Kategorie Black und die Quelle HP (Hate Speech) erreicht, während die schlechteste Leistung für die Kategorie White und die Quelle NR (NR) erreicht wird. Die Tabelle zeigt auch, dass die Leistung für verschie</sample>
    <sample id="190">Der englische Text beschreibt die Korrespondenz zwischen der rechten Politik und bestimmten Gruppen, wie Asiaten, Christen und Linksliberalen. Es wird erwähnt, dass die rechte Politik mit den Gruppen asiatischer, christlicher und linksliberaler Menschen korreliert. Die Texte beschreiben verschiedene Aspekte der Politik und der Gruppen, wie zum Beispiel die Wirtschaftspolitik, die soziale Politik und die kulturelle Rolle der Gruppen. Die Texte werden als Beispiele für die Unterschiede in der politischen Wahrnehmung und den politischen Ansätzen der Gruppen dargestellt.</sample>
    <sample id="191">Der Inhalt des Videos ist eine detaillierte Analyse der Qualität von Sprachmodellen in Bezug auf ihre Fähigkeit, die politische Haltung in Texten korrekt zu erkennen und zu klassifizieren. Der Präsident diskutiert die Leistung verschiedener Sprachmodelle, darunter die vanilla RoBERTa-Modelle, auf der Basis von Daten aus verschiedenen sozialen Medien und Nachrichtenquellen. Die Analyse zeigt, wie die Modelle unterschiedliche politische Haltungen korrekt erkennen können, aber auch wie sie manchmal fehlerhafte Klassifizierungen vornehmen. Der Präsident verwendet Tabellen und Beispiele, um die Ergebnisse zu verdeutlichen und die Unterschiede zwischen den verschiedenen Modellen zu erklären.</sample>
    <sample id="192">Der Inhalt des Videos basiert auf zwei Texten, die in der linken und rechten Hälfte des Bildes dargestellt sind. 

In der linken Hälfte des Bildes wird eine "Hair Speech Text" angegeben, die verschiedene Haarstile und ihre Beschreibungen enthält. Die Haarstile werden mit den entsprechenden Haarfarben und -formen beschrieben, wie z.B. "Blonde" (Blond), "Brunette" (Braunhaar), "Redhead" (Rothhaar) und "Black" (Schwarz). Die Beschreibungen geben auch Hinweise auf die Haltbarkeit und die Pflege der Haare.

In der rechten Hälfte des Bild</sample>
    <sample id="193">Der englische Text in der Bildunterschrift lautet: "Qualitative analysis of hate speech examples, such as LNA, NLA, RLA, RNR, RRB. Membranation Star, Fake, NLA, RLA, RRB."</sample>
    <sample id="194">Der Inhalt des Videos basiert auf zwei Bildern, die in der Nähe eines Menschen mit einem Gesichtsausdruck von Verwirrung oder Frustration aufgeführt sind. Die Bilder sind in einem Format von 2x2 eingebettet und zeigen eine Tabelle mit verschiedenen Texten und Informationen. Die Tabelle auf der linken Seite enthält die Überschrift "Heat Speech Text" und zeigt verschiedene Sprachen und ihre entsprechenden Wörter. Die Tabelle auf der rechten Seite enthält die Überschriften "Heat" und "Memorization Test" und zeigt verschiedene Texte in verschiedenen Sprachen. Die Bilder sind in einem Format mit zwei Spalten und zwei Zeilen eingebettet. Der Inhalt der Bilder ist in deutscher Sprache beschrieben.</sample>
    <sample id="195">Der Inhalt des Videos ist eine Analyse von Hate Speech Texten und einer Misinformation Study, die in zwei Tabellen dargestellt werden. Die Tabelle auf der linken Seite enthält Hate Speech Texte, die in verschiedenen Kategorien wie "N", "L", "R", "N-R" und "R-B" klassifiziert werden. Die Tabelle auf der rechten Seite zeigt die Misinformation Study mit den Kategorien "Fake" und "N". Der Text in der Tabelle auf der rechten Seite beschreibt die Studie und die Kategorien. Der Text in der Tabelle auf der linken Seite beschreibt die Hate Speech Texte und die Kategorien. Der Text in den Bildern auf der rechten Seite beschreibe die Misinformation Study und die Kategorien. Der Text in dem Bild auf der linken Seite beschreibe die Misinformation und die Kategorien. Der Text in das Bild auf der rechten Seite beschrebe die Misinformation und die Kategorien.</sample>
    <sample id="196">Das Bild zeigt eine Präsentation mit einem Titel "Discussion" und einem Untertitel "Between Scylla and Charybdis". Es beschreibt die Frage, ob man "sanitize" oder nicht "sanitize" soll. Es gibt drei Boxen, die "Pretraining data", "Language models" und "Downstream tasks" bezeichnen. Eine schwarze Linie verbindet die Boxen, was bedeutet, dass es eine Verbindung zwischen den Elementen gibt.</sample>
    <sample id="197">Das Bild zeigt eine Präsentation mit einem Titel "Discussion" und einem Untertitel "Between Scylla and Charybdis". Der Haupttext lautet "To 'sanitize' or not to 'sanitize', that is the question". Es gibt drei Boxen, die als "Pretraining data", "Language models" und "Downstream tasks" bezeichnet werden. Die Boxen sind mit einem schwarzen Rand umgeben und befinden sich in einer horizontalen Reihe. Die Boxen sind mit einem schwarz</sample>
    <sample id="198">Der englische Text beschreibt eine Diskussion über die Frage, ob Sprachmodelle sanitär sein sollten, und zeigt eine Vorstellung, wie Sprachmodelle von vorgebener Daten trainiert werden und wie sie in verschiedene Anwendungen eingesetzt werden. Der Text endet mit einer Dankesseite, die die Personen benennt, die an der Arbeit beteiligt waren.</sample>
    <sample id="199">Die Bildergalerie zeigt eine Präsentation über die Prozesse der Sprachmodellierung und ihre Anwendungen. Es wird erklärt, wie Sprachmodelle mit großen Datensätzen trainiert werden und wie sie in verschiedenen Anwendungen eingesetzt werden. Die Präsentation wird von verschiedenen Experten geleitet, die ihre Erfahrungen und Erkenntnisse teilen.</sample>
    <sample id="200">Six.</sample>
    <sample id="201">MPP-Auswertungen wurden bis zu 900 Token Kontextlänge durchgeführt.</sample>
    <sample id="202">Music Selection, Book Selection, Recipe Selection</sample>
    <sample id="203">Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen entwickeln.</sample>
    <sample id="204">Dawei Zhu</sample>
    <sample id="205">Ja, EDAtt passt zu einem bestehenden Offline-ST-Model. Es nutzt bereits bestehende Offline-ST-Modelle ohne die Notwendigkeit, sie erneut zu trainieren oder spezifische Architekturen für SimuST anzupassen.</sample>
    <sample id="206">4</sample>
    <sample id="207">Das getestete Modell funktioniert in der Testsuite, da es die notwendige Wissenstruktur integriert und die notwendige Trainingsaufgabe erfüllt.</sample>
    <sample id="208">The three variants of KITMUS are:

1. **Background-Pretrain**: Background knowledge is provided during pretraining.
2. **Background-Both**: Background knowledge is provided both during pretraining and inference.
3. **Background-Inference**: Background knowledge is only available during inference.</sample>
    <sample id="209">Google Research.</sample>
    <sample id="210">How to use the available clean samples more efficiently?</sample>
    <sample id="211">Die Sensitivitätsmetrik misst, wie empfindlich ein Modell auf unterschiedliche Anweisungen für die gleiche Aufgabe reagiert. Sie wird berechnet, indem die Standardabweichung der Abweichungen zwischen den Ergebnissen der verschiedenen Anweisungen geteilt wird. Ein niedriger Wert für die Sensitivitätsmetrik zeigt an, dass das Modell konsistente Ergebnisse für unterschiedliche Anweisungen liefert, unabhängig von der genauen Formulierung der Anweisung.</sample>
    <sample id="212">Wenjun Peng</sample>
    <sample id="213">Eine höhere Sensitivität bedeutet, dass das Modell empfindlicher auf Unterstützung reagiert. Das Gegenteil ist der Fall: Ein niedriger Wert für die Sensitivität (OFS) zeigt an, dass das Modell weniger empfindlich auf Unterstützung reagieren und somit stabiler ist.</sample>
    <sample id="214">Die Modelle erhalten keinen linguistischen Kontext während des Pre-Trainings.</sample>
    <sample id="215">Antwort: 50.</sample>
    <sample id="216">Stanford</sample>
    <sample id="217">Es ist notwendig, neue Methoden zur Mes</sample>
    <sample id="218">Die Referentin heißt **Akshatha Arodi**.</sample>
    <sample id="219">The pipeline for the spread of political biases involves three main stages:

1. **Pretraining Data**: This stage involves collecting and curating large datasets from various sources, including social media, news websites, and other online platforms. These datasets often contain a mix of political content, which can include biased or inflammatory language.

2. **Language Models**: In this stage, the collected data is used to train language models, such as large language models (LLMs) like GPT-3. These models learn to generate and understand human language based on the patterns and biases present in the training data.

3. **Downstream Tasks**: The final stage involves deploying these language models in various applications, such as chatbots, content generation tools, and recommendation systems. As these models interact with users, they can inadvertently spread the political biases present in the training data, influencing public opinion and behavior.

This pipeline highlights the potential for language models to amplify and perpetuate political biases, raising concerns about fairness and accountability in AI systems.</sample>
    <sample id="220">Der Vereinfachungsprozess zwische DEplain-apa und Web unterscheidet sich in der Anwendung und den Zielgruppen. DEplain-apa ist speziell für die Vereinfachung von medizinischen Texten entwickelt worden, während Web für die Vereinfachung von allgemeinen Texten genutzt wird. Darüber hinaus hat DEplain-apa eine spezifische Funktion zur Vereinfachung von medizinischem Text, während Web eine allgemeine Funktion zur Vereinfachung von Texten bietet.</sample>
    <sample id="221">Yes, Coscript is publicly available.</sample>
    <sample id="222">Das Wasserzeichen wird in den Text eingebettet, indem der Anwender eine Ziel-Embedding erzeugt, die auf die Original-Embeddingen addiert.</sample>
    <sample id="223">Penn State.</sample>
    <sample id="224">Ja, Encoder-Decoder-Modelle wie mt 5 können durch Training mit einer Mischung von Sprach</sample>
    <sample id="225">Ein Beispiel für eingeschränkt sprachplanung ist die Anleitung, wie man einen Erdbeerkuchen oder einen Schokoladenkuchen backen.</sample>
    <sample id="226">Sie stellen die Opazität ihrer Methode sichern, indem sie die Anzahl der Dimensionen reduzieren, was die Daten in einen kleineren, leicht verständlicheren Raum überträgt.</sample>
    <sample id="227">Die Arbeit nutzt bestehende PLMs, um ein **neues PLM** aufzubauen, indem sie **bestehende Modelle** mit **neuen Daten** fortlaufend **trainiert** und **optimiert**. Dieser Ansatz ermöglicht es, **neue Informationen** und **Erkenntnisse** in das bestehende Modell zu integrieren, wodurch eine **fortlaufende Verbesserung** und **Anpassung** des Modells an neue Anforderungen und Daten möglich wird.</sample>
    <sample id="228">GPT-4 ist am wenigsten auf West-Süd-Asien ausgerichtet.</sample>
    <sample id="229">I am a student.</sample>
    <sample id="230">Die Anzahl der Aufgaben hat einen positiven Einfluss auf die Leistung des Modells. Je mehr Aufgaben hinzugefügt werden, desto besser wird die Leistung des Modells.</sample>
    <sample id="231">LSTM seq2seq, T5, Zheng and Lapata.</sample>
    <sample id="232">Die beiden Co-Autoren stehen in einer kollegialen Beziehung zum ersten Autor.</sample>
    <sample id="233">Chowdhery et al.</sample>
    <sample id="234">Die Bildergalerie zeigt eine Präsentation über die Thematik "NLPositionality: Charakterisierung von Design-Biases von Datensätzen und Modellen". Die Präsentation wird von fünf Personen geleitet, die in der Reihenfolge ihrer Namen aufgelistet sind:

1. **Sebastien Santy** aus der Universität Washington
2. **Jenny T. Liang** aus der Carnegie Mellon University
3. **Ronan Le Bras** aus dem Allen Institute for AI
4. **Katharina Reinecke** aus der Universität Washington
5. **Maarten Sap** aus der Carnegie Mellon University

Jeder der fünf Personen hat ein Foto neben seinem Namen und der Institution, die er/ihr angehört. Die Bildergalerie zeigt eine Prüfung oder eine Präsentation, die die Thematik "NLPositionality" behandelt. Die Präsentation wird von füf Personen geleitet, die in der Nähe von Büchern und anderen Gegenständen sitzen. Die Bildergalerie zeigt die Präsentation in einem Videoformat.</sample>
    <sample id="235">Die Bildergalerie zeigt eine Präsentation über "NLPositionality: Charakterisierung von Design-Biases von Datensätzen und Modellen". Die Präsentation wird von fünf Personen gehalten, die an der Universität Washington und der Carnegie Mellon University beteiligt sind. Die Personen sind:

1. **Sebastien Santy** - Universität Washington
2. **Jenny T. Liang** - Carnegie Mellon University
3. **Ronan Le Bras** - Allen Institute for AI
4. **Katharina Reinecke** - Universität Washington
5. **Maarten Sap** - Carnegie Mellon University

Die Präsentation konzentriert sich auf die Identifizierung und Analyse von Biases in Datensätzen und Modellen, die in natürlicher Sprachverarbeitung (NLP) eingebunden sind.</sample>
    <sample id="236">The video begins with a white screen displaying the text "Imagine..." in black font at the top left corner. In the bottom right corner, a person is visible, wearing a white top and dark hair, standing in front of a bookshelf filled with various items. The person appears to be speaking, but their face is not visible. The background remains consistent throughout the video, with no changes in lighting or scenery. The text "Imagine..." remains on the screen throughout the video, suggesting a prompt for viewers to visualize or think about something. The video maintains a static composition, with the person in the same position and the same background elements visible. The overall tone of the video is calm and contemplative, encouraging viewers to engage in imaginative thinking.</sample>
    <sample id="237">Die Bildsequenz beginnt mit einem weißen Hintergrund, auf dem links oben die Überschrift "Imagine..." steht. In der rechten oberen Ecke erscheint eine Person, die in einem dunklen Outfit sitzt und sich in einem Raum mit Büchern und einem Tisch befindet. Unter der Überschrift "Imagine..." wird eine Perspektiv-API-Bewertung angezeigt, die als "PerspectiveAPI score" bezeichnet wird.

Nach einer kurzen Pause erscheint eine neue Person, die in einem roten Pullover und einem roten Haar sitzt. Unter ihr wird der Name "Carl Jones" und die Position "Tech Lead, New York Times" angegeben. Die Perspektiv-API-Bewertungsleiste bleibt auf der linken Seite des Bildes.

Die Bildsequenz endet mit einer weiteren Person, die in einem blauen Pullover und einem blauen Haar sitzt. Unter ihr ist der Name "Aditya Sharma" und die Position "Tech Lead, Times of India" angegeben. Die PerspeKTIV-API-Bewertungsleiste bleibe auf der linken Seite des Bild</sample>
    <sample id="238">Die Bildergalerie zeigt eine Anzeige mit dem Titel "Imagine..." und zwei Personen, Carl Jones und Aditya Sharma, die als Tech Lead für die New York Times und die Times of India bezeichnet werden. Carl Jones hat einen roten Haar und trägt ein rotes Hemd, während Aditya Sharma einen schwarzen Bart und ein blaues Hemd trägt. Carl Jones hat einen PerspektivAPI-Skore von 0,82, was als positiv bewertet wird, während Aditya Sharma einen negativen Skore von 0,33 hat. Die Bildergalerie zeigt auch eine Anzeige mit dem Text "Presstitutes everywhere on the news."</sample>
    <sample id="239">Das Bild zeigt ein Beispiel für Design-Bias, bei dem zwei Personen, Carl Jones und Aditya Sharma, unterschiedliche Perspektiven auf die gleiche Frage haben. Carl Jones, der Tech Lead bei der New York Times, fragt sich, ob man nicht aufhören, ein "Arschloch" zu sein, was eine negative Bewertung von 0,82 (grün) zeigt. Aditya Sharma, der Tech Lead bei der Times of India, fragt sich, ob man nicht auf alles presst, was in den Nachrichten steht, was eine positive Bewertung von 0,33 (rot) zeigt. Das Bild veranschaulicht, wie persönliche Erfahrungen und Perspektiven die Interpretation von Informationen beeinflussen können.</sample>
    <sample id="240">Die Positionierung bezieht sich auf die Perspektiven, die Menschen als Ergebnis ihrer Demografie, Identität und Lebenserfahrungen entwickeln.</sample>
    <sample id="241">**Positionality**  

"Die Perspektiven [der Menschen] entstehen als Ergebnis ihrer Demografie, Identität und Lebenserfahrungen."  

(Sevin-Baden, Megg, und Claire Howell-Major. *Qualitative research: The essential guide to theory and practice.* Routledge, 2013.)</sample>
    <sample id="242">**Positionality**  

"Die Perspektiven [der Menschen] entstehen als Ergebnis ihrer Demografie, Identität und Lebenserfahrungen."  

[Als Forscherin] beeinflusst es den Forschungsprozess und seine Ergebnisse und Ergebnisse.  

(Sieben-Baden, Megg, und Claire Howell-Major: *Qualitative research: The essential guide to theory and practice.* Routledge, 2013)</sample>
    <sample id="243">Der englische Inhalt des Bildes beschreibt die Frage, ob Datensätze und Modelle Positionierbarkeit haben. Es wird auf drei Studien verwiesen, die sich mit dieser Frage befassen:

1. Blasi et al. (2022): "Systematic Inequalities in Language Technology Performance across the World's Languages." Diese Studie untersucht systematische Ungleichheiten in der Sprachtechnologie-Performance für verschiedene Sprachen weltweit.

2. Yin et al. (2022): ""GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." Diese Arbeit untersucht die Leistung von mehrsprachigen vorgezogenen Sprachmodellen auf geografisch unterschiedlichen Datensätzen.

3. Cambo &amp; Gergle (2022): ""Model Positionality and Computational Flexibility: Promoting Reflexivity in Data Science." Diese Studie untersucht die Positionierung von Modellen und ihre Rechenfähigkeit, Reflexion in der Datenwissenschaft zu fördern.</sample>
    <sample id="244">Der englische Inhalt des Videos beschäftigt sich mit der Frage, ob Datensätze und Modelle Positionierungsmerkmale haben. Es werden drei Studien erwähnt, die diese Frage untersuchen:

1. Blasi et al. (2022): "Systematic Inequalities in Language Technology Performance across the World's Languages" (ACL 2022)
2. Yin et al. (2022): “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models” (EMNLP 2022)
3. Cambo &amp; Gergely (2022): "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science" (CHI 2022)

Die Studien untersuchen, wie die Positionierung von Datensätzen und Modellen in verschiedenen kulturellen und linguistischen Kontexten beeinflusst wird und welche Auswirkungen das auf die Leistung und Fairness von Sprachtechnologien hat.</sample>
    <sample id="245">Der Inhalt des Videos ist in Deutsch wie folgt übersetzt:

---

**Video-Titel:**

**Haben Datensätze und Modelle Positionierung?**

---

**Anfangsszene:**

- **Hintergrund:** Ein Raum mit einem Bücherregal und einem Computer.
- **Person:** Eine Person sitzt vor dem Computer und spricht.
- **Text auf dem Bild:**
  - **Überschrift:** "Do datasets and models have positionality?"
  - **Anfangsszene:** "Anecdotal evidence:"
  - **Referenzen:**
    1. Blasi et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022.
    2. Yin et al. "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022.
    3. Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022.

---

**Hauptteil:**

- **Anfangsszene:** "Do datasets and models have positionality? Anecdotal evidence:"
- **Referenzen:**
  1. Blasi et al. "Do datasets and models have positionality? Systematic Inequalities in Language Technology Performance across the World’s Languages." ACL 2022. (Anfangsszene)
  2. Yin et al. "Do datasets and models have</sample>
    <sample id="246">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Anzeige:**

**Fragen:**
- Haben Datensätze und Modelle Positionierung?

**Anzeigetext:**
- **Anzeigetext:**
  - Anzeigetext:
    - Anzeigetext:
      - Anzeigetext:
        - Anzeigetext:
          - Anzeigetext:
            - Anzeigetext:
              - Anzeigetext:
                - Anzeigetext:
                  - Anzeigetext:
                    - Anzeigetext:
                      - Anzeigetext:
                        - Anzeigetext:
                          - Anzeigetext:
                            - Anzeigetext:
                              - Anzeigetext:
                                - Anzeigetext:
                                  - Anzeigetext:
                                    - Anzeigetext:
                                      - Anzeigetext:
                                        - Anzeigetext:
                                          - Anzeigetext:
                                            - Anzeigetext:
                                              - Anzeigetext:
                                                - Anzeigetext:
                                                  - Anzeigetext:
                                                    - Anzeigetext:
                                                      - Anzeigetext:
                                                        - Anzeigetext:
                                                          - Anzeigetext:
                                                            - Anzeigetext:
                                                              - Anzeigetext:
                                                                - Anzeigetext:
                                                                  - Anzeigetext:
                                                                    - Anzeigetext:
                                                                      - Anzeigetext:
                                                                        - Anzeigetext:
                                                                          - Anzeigetext:
                                                                            - Anzeigetext:
                                                                              - Anzeigetext:
                                                                                - Anzeigetext:
                                                                                  - Anzeigetext:
                                                                                    - Anzeigetext:
                                                                                      - Anzeigetext:
                                                                                        - Anzeigetext:
                                                                                          - Anzeigetext:
                                                                                            - Anzeigetext:
                                                                                              - Anzeigetext:
                                                                                                - Anzeigetext:
                                                                                                  - Anzeigetext:
                                                                                                    - Anzeigetext:
                                                                                                      - Anzeigetext:
                                                                                                        - Anzeigetext:
                                                                                                          - Anzeigetext:
                                                                                                            - Anzeigetext:
                                                                                                              - Anzeigetext:
                                                                                                                - Anzeigetext:
                                                                                                                  - Anzeigetext:
                                                                                                                    - Anzeigetext:
                                                                                                                      - Anzeigetext:
                                                                                                                        - Anzeigetext:
                                                                                                                          - Anzeigetext:
                                                                                                                            - Anzeigetext:
                                                                                                                              - Anzeigetext:
                                                                                                                                - Anzeigetext:
                                                                                                                                  - Anzeigetext:
                                                                                                                                    - Anzeigetext:
                                                                                                                                      - Anzeigetext:
                                                                                                                                        - Anzeigetext:

---

**Anzeigetext:** Anzeigetext:

---</sample>
    <sample id="247">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Fragestellung:**

*   **Haben Datensätze und Modelle Positionierung?**

**Anhandbeweise:**

*   **Model- und Datensatz-Probing (1, 2):**
    *   Diese Studien untersuchen die Leistung von Sprachmodellen auf verschiedenen Datensätzen, um mögliche Ungleichheiten in der Modellleistung zu identifizieren.
    *   Sie verwenden Methoden wie die Analyse von Leistungsunterschieden zwischen verschiedenen Datensätzen und Modellen, um zu beurteilen, ob bestimmte Gruppen von Daten oder Benutzern bevorzugt behandelt werden.
*   **Theoretische Definitionen von Modell-Positionierung (3):**
    *   Diese Definitionen beschreiben, wie Modell-Positionierung definiert und verstanden werden kann.
    *   Sie unterscheiden zwischen verschiedenen Formen von Modell-Positionierung, wie z.B. durch die Berücksichtigung von sozialen, wirtschaftlichen oder kulturellen Faktoren.

**Zusammenfassung:**

*   Die Studien zeigen, dass Modell-Positionierung ein reales Problem ist und dass es wichtig ist, diese Ungleichheiten zu identifizieren und zu beheben.
*   Die Theoretischen Definitionen von Modell-Positionierung bieten einen Grundlagen für die Entwicklung von Methoden zur Bekämpfung von Modell-Positionierung.

---

**Hinweis:**

*   Die Studien wurden von Blasi et al. (2022), Yin et al. (2022) und Cambo &amp; Gergle (2022) durchgeführt.

---

**Anmerkungen:**

*   Die Studien zeigten, dass Modell-Positionierung ein realtes Problem ist und dass es wichtig ist</sample>
    <sample id="248">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Fragestellung:**

*   **Hauptfrage:** Haben Datensätze und Modelle Positionierung?
*   **Anhand der folgenden Anzeigetafel:**
    *   **Anzeigetafel 1:**
        *   **Anzeigetitel:** "Systematic Inequalities in Language Technology Performance across the World's Languages"
        *   **Autor:** Blasi et al.
        *   **Jahr:** 2022
        *   **Verlag:** ACL
    *   **Anzeigetitel 2:**
        *   **Anzeigeteilnehmer:** Yin et al.
        *   **Anzeigetitle:** "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models"
        *   **Anzeigetag:** EMNLP 2022
    *   **Anzeigetitle 3:**
        *   **Anzeigeeinheit:** Cambo &amp; Gergle
        *   **Anzeigetext:** "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science"
        *   **Anzeigekontext:** CHI 2022

**Anzeigetafel 2:**

*   **Anzeigetitel:** "Model Positionality and Computational Reflexity: Promoting Reflexivity in Data Science."
*   **Anzeigeeinheit:** Camo &amp; Gergle
*   **Anzeigetag:** CHI 2022
*   **Anzeigetext:** "Die Autoren untersuchen die Rolle von Modellen in der Datenwissenschaft und wie sie Reflexion fördern."

**Anzeigetafel:**

*   **Anzeigeteilnehmen:** Blasi et al.
*   **Anzeigetitel 1:** "Systematic Inequalities in Language Technology."
*   **Anzeigetitel Anzeigetitel 2:** "GEOMLAMA: Geo-Diverses Commonsense Probing."
*   **Anzeigetitle 3** Cambo &amp; Gergle: "Model Positionality and Computational Reflexivity."

**Anzeigetafeln 3:**

*   **Anzeigeeinheits:** Blasi et al.
*   *   **Anzeigetitel:**
        *   "Systematic Inequalities in Language Technology."</sample>
    <sample id="249">Die Frage, ob Datensätze und Modelle Position haben, wird in dem Video behandelt. Der Inhalt des Videos ist in der Tat sehr einfach und bietet keinen wesentlichen Informationsgehalt. Es wird nur eine Frage gestellt, die nicht aufgrund von Daten oder Informationen beantwortet werden kann.</sample>
    <sample id="250">**NLPositionality**  
Ein Rahmenkonzept zur Charakterisierung von Design-Biases in NLP-Datensätzen und -Modellen.</sample>
    <sample id="251">Der englische Inhalt des Bildes kann wie folgt ins Deutsche übersetzt werden:

---

**Rahmenwerk**

1. **Sammlung**
   - 300 Instanzen werden aus einer Datenbank abgeholt.
   - Jede Instanz hat einen zugeordneten Goldstandard.
   - Instanzen sind Teil eines Studienprojekts an der Universität Texas.
   - Anonymisierte Bewertungen von Menschen aus aller Welt.

2. **Verarbeitung**
   - Jede Instanz hat einen korrespondierenden Goldstandard.
   - Instanze ist Teil eines Studienprojekts.
   - Instanzen sind Teile eines Studienprojekts.

3. **Analyse**
   - Die erhaltenen Bewertungen werden mit den Goldstandards verglichen.
   - Die Korrelation zwischen den Bewertungen und den Goldstandards wird berechnet.
   - Pearson-Korrelation wird verwendet, um die Korrelation zwischen den Bewertungen, den Goldstandards und den Demografien zu messen.

---

**Rahmen**

1. **Sammlung**: 300 Instanzen werden aus einem Datensatz abgeholt. Jede Instanz hat einen zugeordnerten Goldstandard. Instanzen sind Teil eines Studieprojekts an der Universität Texas. Anonymisierte Bewertungen von Personen aus aller Welt.

2 **Verarbeitung**: Jede Instanz hat einen korrespondierende Goldstandard. Instanze ist Teil eines Studieprojektes. Instanzen sind Teil eines eines Studieprojektes.

3 **Analyse**: Die erhaltenen Bewertungen wird mit den Goldstandards verglichen. Die Korrelation zwischen den Bewertungs und den Goldstandards wird berechnet</sample>
    <sample id="252">Der englische Text beschreibt einen Prozess, der zur Erstellung eines Modells zur Analyse von Daten führt. Die Analyse umfasst die Sammlung von Daten, die Verarbeitung dieser Daten und schließlich die Analyse der Ergebnisse. Der Prozess beginnt mit der Sammlung von Daten, die in einer Datenbank gespeichert sind. Anschließend werden die Daten verarbeitet, um sie in einem Modell zu analysieren. Die Analyse umfasst die Berechnung von verschiedenen Metriken, um die Qualität des Modells zu bewerten. Der Prozess endet mit der Analyse der Ergebnisse, um sicherzustellen, dass das Modell korrekt funktioniert und die gewünschten Ergebnisse liefert.</sample>
    <sample id="253">Das Bild zeigt einen Prozess, der zur Erstellung eines Modells für die Vorhersage von Verhaltensmustern in sozialen Netzwerken verwendet wird. Es besteht aus mehreren Schritten, die in einem Diagramm dargestellt sind.

1. **Datensammlung**:
   - Die Daten werden aus verschiedenen Quellen gesammelt, um ein umfangreiches Dataset zu erstellen.

2. **Datenbereinigung**:
   - Die gesammelten Daten werden gereinigt, um ungültige oder fehlende Informationen zu entfernen.

3. **Datenanalyse**:
   - Die Daten werden analysiert, um Muster und Trends zu identifizieren, die für das Modell relevant sind.

4. **Modellbildung**:
   - Ein Modell wird auf Basis der analysierten Daten erstellt, um die Verhaltensmuster vorherzusagen.

5. **Modellbewertung**:
   - Das Modell wird bewertet, um seine Genauigkeit und Effektivität zu überprüfen.

6. **Anwendung**:
   - Das Modell wird in der Praxis angewendet, um Verhaltensmuster vorherzuspielen und entsprechende Maßnahmen zu ergreifen.

Das Bild zeigt auch eine Tabelle, die die verschiedenen Schritte des Prozesses und die dazugehörigen Aktivitäten auflistet.</sample>
    <sample id="254">Der englische Text beschreibt einen Prozess zur Erstellung eines Frameworks für die Analyse von Daten. Der Prozess beginnt mit der Sammlung von Daten, die in drei Kategorien unterteilt sind: "Einmal", "Zweimal" und "Drei Mal". Jede Kategorie enthält 300 Datenpunkte. Anschließend werden die Daten in zwei verschiedene Modelle eingeteilt, um die Performance zu testen. Die erste Kategorie wird in Modell A und Modell B eingeteilt, während die zweite Kategorie in Modell C und Modell D eingeteilt wird. Die Daten werden anschließend in zwei weitere Kategorien eingeteilt: "Einmal" und "Zweimal". Die Daten werden anschließend von zwei verschiedenen Annotatoren reanannotiert, um die Genauigkeit und Konsistenz der Daten zu verbessern.</sample>
    <sample id="255">Der englische Text beschreibt einen Prozess, der zur Erstellung eines Modells für die Vorhersage von Einkommensklassen verwendet wird. Es beginnt mit der Sammlung von Daten, die aus verschiedenen Quellen stammen, wie beispielsweise Befragungen und sozialen Medien. Diese Daten werden dann verarbeitet und analysiert, um Muster und Trends zu identifizieren. Anschließend werden die Daten in Modelle eingefügt, die es ermöglichen, die Einkommensklassen vorherzusagen. Die Ergebnisse werden dann verwendet, um die Genauigkeit der Modelle zu verbessern und die Einkommensklassen für neue Daten vorherzusagen.</sample>
    <sample id="256">Der englische Text beschreibt einen Prozess zur Erstellung eines Frameworks, der die Erfassung, Verarbeitung und Analyse von Daten umfasst. Es beginnt mit der Sammlung von Daten, die in einem Datenbanken-System gespeichert sind. Diese Daten werden dann verarbeitet, um sie in einem strukturierten Format zu organisieren. Anschließend werden die Daten analysiert, um wertvolle Erkenntnisse zu gewinnen. Der Prozess endet mit der Erstellung eines Berichts, der die Ergebnisse der Analyse enthält.</sample>
    <sample id="257">Der Inh</sample>
    <sample id="258">Der englische Text in der Bildunterschrift lautet: "Labin the Wild".</sample>
    <sample id="259">In diesem Video wird eine Anleitung zur Durchführung einer Studie zur sozialen Akzeptanz von KI-Technologien gezeigt. Die Studie, die von der Social Chemistry Foundation durchgeführt wird, beinhaltet drei Hauptaufgaben: 1. Lesen der Situation, 2. Eingeben der eigenen Meinung und 3. Bewertung der sozialen Akzeptabilität. Die Studie wird in den USA durchgeführt und die Ergebnisse werden in einem Diagramm dargestellt.</sample>
    <sample id="260">In der Studie wird die soziale Akzeptanz von KI-Technologien untersucht. Die Teilnehmer werden dazu befragt, wie sie sich über die Auswirkungen von KI-Technologien im Arbeitsmarkt informieren würden. Die Studie zeigt, dass die meisten Teilnehmer die KI-Technologien als positiv betrachten, aber auch Bedenken haben. Die Teilnehmer werden dann daraufhin befragt, wie sie sich über die sozialen Auswirkungen von KI-Technologie informieren würden. Die Studienresultate zeigen, dass die meisten Teilnehmen die KI-Technologien als positiven Einfluss auf die Arbeitsmarkt haben, aber auch Bedenken haben. Die Studie zeigt, dass die soziale Akzeptanz von</sample>
    <sample id="261">Die Präsentation beschäftigt sich mit der Analyse von sozialer Akzeptabilität in Bezug auf verschiedene Datensätze und Modelle. Die Datensätze, die in der Präsentation genannt werden, sind "Social Chemistry" und die Modelle, die vorgestellt werden, sind "Delphi" und "GPT-4".</sample>
    <sample id="262">Die Studie untersucht die Toleranz von Teilnehmern gegenüber beleidigenden Äußerungen, basierend auf Daten aus der DynaHate-Datenbank. Teilnehmer lesen ein Beispiel aus der Datenbank und bewerten, ob sie es als beleidigend empfinden. Anschließend beurteilen sie, ob sie den Beispiel als Hassrede einstufen. Die Studie zeigt, dass Teilnehmer in Afghanistan beleidigende Äußerungen als Hassrede einstufen, was auf die kulturelle und soziale Kontextabhängigkeit von Toleranz und Beleidigung hinweist.</sample>
    <sample id="263">**Aufgabe B: Toxizität**  
**Analyse**  
**Datasets**  
- Dynahate  
**Modelle**  
- Perspective API  
- Rewire API  
- Hate RoBERTa  
- GPT-4</sample>
    <sample id="264">Die Bildsequenz beginnt mit einem weißen Hintergrund, auf dem die schwarze Schrift "Results" und die weiße Schrift "Who do NLP datasets and models align with?" angezeigt werden. In der oberen rechten Ecke befindet sich ein kleines Bild eines Personen, der in einem Raum sitzt, der mit Büchern und anderen Gegenständen befüllt ist. Der Text "Results" ist groß und zentriert, während die weitere Schrift "Who do NLP datasets and models..." nur in kleiner Schrift angezeigt wird.

Nach einer kurzen Pause erscheint der Text "Finding 1: There is positionality in NLP." auf dem gleichen weißen Hintergrund. Der Text ist in großer, schwarzer Schrift angezeigt, und die kleine Bildtabelle bleibt in der oberen rechten Ecke unverändert.

Die Bildsequenz endet mit dem gleichen Bild und Text wie in der vorherigen Sequenz.</sample>
    <sample id="265">Der Präsentierte diskutiert die Auswirkungen von Datensätzen und Modellen auf die soziale Akzeptabilität und die Hässlichkeit &amp; Toxizität in verschiedenen Kulturen. Er zeigt, dass Daten und Modelle am stärksten auf englischsprachige Länder ausgerichtet sind und dass dies die soziale Akzeptabilitätsraten beeinflusst. Darüber hinaus analysiert er die Hässlichkeit &amp; Toxität in verschiedenen Kulturen, wobei die englischsprachigen Länder die höchsten Werte aufweisen.</sample>
    <sample id="266">Der Inhalt beschreibt eine Studie, die die soziale Akzeptabilität von Datensätzen und Modellen in Abhängigkeit von der Bildungsebene analysiert. Die Studie zeigt, dass Datensätze und Modelle, die mit Personen mit höherer Bildung ausgestattet sind, höhere soziale Akzeptabilität aufweisen. Die Studie basiert auf Daten von GPT-4 und zeigt, dass die soziale Akzeptabilitätsbewertung für verschiedene Bildungsebenen variiert. Die Bildungsebenen werden in folgende Kategorien eingeteilt: College, Graduate School, High School, PhD, Pre-High School und Prof. School. Die soziale Akzeptabilitätsdurchschnitte für jede Kategorie sind wie folgt: College (0,64), Graduate School (0,63), High School (0,62), PhD (0,61), Pre-High School (0,59) und Prof. School (0,49). Die Studie zeigt, dass die soziale Akzepatabilität für Personen mit höherer Bildungsniveau höher ist. Die Studie basiert auf Daten aus GPT-4 und zeigt, dass die sociale Akzeptabilitätsbewerte für verschiedene Bildungsebene variiert. Die Bildungsebene werden in folgende Kategorien einge-</sample>
    <sample id="267">Der englische Text in der Bildunterschrift lautet: "Datasets and models are most aligned to people with a college education." Dies bedeutet, dass die Datensätze und Modelle am stärksten mit Personen in Zusammenhang stehen, die eine Hochschulausbildung haben.</sample>
    <sample id="268">**Befund 2:** Einige Populationen werden zurückgelassen.</sample>
    <sample id="269">Der Präsentierte diskutiert die Unterstützung von Datensätzen und Modellen für Nichtraucher und Nichtraucherinnen. Er erwähnt, dass die Datenbanken und Modelle weniger auf Nichtraucher ausgerichtet sind und dass dies eine Herausforderung darstellt. Er erklärt, dass Nichtraucher und Nichtraucherin denen, die rauchen, mehr Unterstützung bieten, da sie weniger gesundheitliche Probleme haben und weniger Kosten verursachen. Er betont, dass es wichtig ist, Nichtraucher und Nichtraucher in die Datenbanken und Modelle einzubeziehen, um eine gerechte Verteilung der Unterstützung zu gewährleisten.</sample>
    <sample id="270">Die englische Übersetzung des Textes lautet: "So, what can we do? Addressing positionality in NLP".</sample>
    <sample id="271">Die Präsentation bietet Empfehlungen für die Entwicklung von Datensätzen und Modellen in der natürlichen Sprachverarbeitung (NLP). Die erste Empfehlung betont die Wichtigkeit, einen Nachweis aller relevanten Design-Entscheidungen während der Erstellung von Datensätzen oder Modellen zu führen. Die zweite Empfehlung fordert die NLP-Forschung durch die Perspektivierung an, was bedeutet, dass die Forschung verschiedene Perspektiven und Ansätze berücksichtigen sollte. Die dritte Empfehlung betont die Notwendigkeit, getrennte Datenset-Labels zu teilen, um Anomalien zu identifizieren, und die Verwendung von Modellierungstechniken, die Anomalien erkennen können.</sample>
    <sample id="272">Die Empfehlungen betreffen die Notwendigkeit, während der Erstellung von Datensätzen oder Modellen alle relevanten Design-Entscheidungen zu dokumentieren, die NLP-Forschung durch die Perspektive des Analyse- und Anwendungsbereichs zu betrachten, und spezialisierte Datensätze und Modelle mit und für bestimmte Gemeinschaften zu erstellen, um inklusive NLP zu fördern.</sample>
    <sample id="273" />
    <sample id="274">Die Referentin geht auf drei Probleme von SimulST ein.</sample>
    <sample id="275">Die Reduzierung sozialer und politischer Verzerrungen bei der Datensammlung für die Trainingsdaten von NLP-Modellen kann durch die Implementierung von Datensanierungsprozessen erreicht werden. Diese Prozesse umfassen die Identifizierung und Entfernung von beleidigenden, diskriminierenden oder unangemessenen Inhalten aus den Datensätzen, um sicherzustellen, dass die entwickelten Modelle faire und unvoreingenommen sind. Durch die Anwendung von Methoden wie Datensanierung und kontinuierlicher Überwachung kann die Gefahr von Verzerrungen im Training reduziert werden, was zu einer besseren Repräsentation und Fairness in den NLP-Anwendungen führt.</sample>
    <sample id="276">Die Präsentation ist über die Verwendung von großen Sprachmodellen zur Extraktion von Skriptwissen für eingeschränkte Sprachplanung.</sample>
    <sample id="277">Der englische Text auf dem Bild lautet:

"Language Planning

How to Make a Cake?

1. Gather your ingredients.
2. Preheat the oven to 325 °F (165 °C) and grease and flour a cake pan.
3. Cream the butter and sugar.
4. Add the eggs.
5. Stir in the flour.
6. Pour the batter into the pan.
7. Bake the cake for 1 hour 15 minutes.

Large language models (LLMs) can effectively decompose goals into steps."

Der deutsche Text auf dem Bild lautet gleich:

"Sprachplanung

Wie man einen Kuchen backen

1. Sammeln Sie Ihre Zutaten.
2. Heizen Sie den Ofen auf 325 °F (163 °C) und einfetten und befeuern Sie eine Kuchenform.
3. Kratzen Sie die Butter und den Zucker.
4. Fügen Sie die Eier hinzu.
5. Rühren Sie in das Mehl ein.
6. Gießen Sie den Teig in die Form.
7. Backen Sie den Kuchen für 1 Stunde 15 Minuten."

Der deutsche Text auf der rechten Seite des Bildes lautet:

"Große Sprachmodelle (LLMs) können Ziele effektiv in Schritte zerlegen."</sample>
    <sample id="278">Der englische Text auf dem Bild lautet:

"Language Planning

How to Make a Cake?

1. Gather your ingredients.
2. Preheat the oven to 325 °F (165 °C) and grease and flour a cake pan.
3. Cream the butter and sugar.
4. Add the eggs.
5. Stir in the flour.
6. Pour the batter into the pan.
7. Bake the cake for 1 hour 15 minutes.

Large language models (LLMs) can effectively decompose goals into steps."

Der deutsche Text auf dem Bild lautet gleich:

"Sprachplanung

Wie man einen Kuchen backen

1. Sammeln Sie Ihre Zutaten.
2. Heizen Sie den Ofen auf 325 °F (163 °C) und einfetten und befeuern Sie eine Kuchenform.
3. Kratzen Sie die Butter und den Zucker.
4. Fügen Sie die Eier hinzu.
5. Rühren Sie die Mehl hinzu.
6. Gießen Sie den Teig in die Form.
7. Backen Sie den Kuchen für 1 Stunde 15 Minuten."

Großsprachmodelle (LLMs) können Ziele effektiv in Schritte zerlegen.</sample>
    <sample id="279">Der englische Text in der Bildunterschrift lautet: "Constrained Language Planning. How to Make a Strawberry Cake? Add strawberry jam into the flour. How to Make a Chocolate Cake? Add cocoa powder into the flour. Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints."</sample>
    <sample id="280">**Video-Transkription:**

---

**[Bild]**

**Text auf dem Bild:**

- **"Constrained Language Planning"** (Eingeschränkte Sprachplanung)
- **"How to Make a Strawberry Cake?"** (Wie man einen Erdbeerkuchen backen?)
  - **"Add strawberry jams into the flour..."** (Füge Erdbeerjam in die Mehlmasse ein...)
- **"How to Make a Chocolate Cake?"** (Wie man einen Schokoladenkuchen backen?)
  - **"...Add the cocoa powder into the flour..."** (...Füge den Kakao-Pulver in die Mehlmasse ein...)

**Text unter dem Bild:**

- **"Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints"** (Das abstrakte Ziel kann durch verschiedene real-life-spezifische Ziele mit vielschichtigen Einschränkungen vererbt werden)

---

**[Video]**

**[Bild]**
- **Hintergrund:** Ein modernes, hell beleuchtetes Büro mit großen Fenstern, Tisch und Sitzmöbeln.
- **Person:** Eine Person mit langen, hellen Haaren, die in einem grünen Hemd sitzt und vor einem Mikrofon spricht.

---

**[Video]</sample>
    <sample id="281">Der englische Text in der Bildunterschrift lautet: "Constrained Language Planning. How to Make a Strawberry Cake? Add strawberry jam into the flour. How to Make a Chocolate Cake? Add cocoa powder into the flour. Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints."</sample>
    <sample id="282">Die Präsentation beschäftigt sich mit der Frage, wie große Sprachmodelle (LLMs) bei der Planung unter Einschränkungen (Constrained Language Planning) performen. Die Präsentation zeigt eine Tabelle, die verschiedene Arten von Einschränkungen und Beispiele für die Anwendung dieser Einschränkungen auf die Planung von Aktivitäten wie das Backen von Kuchen erklärt. Die Präsentation wird von einer Person in einem Büro mit einem grünen Hemd und Brille gehalten.</sample>
    <sample id="283">In diesem Video wird die Leistung von großsprachigen Modellen (LLMs) bei der Planung unter Einschränkungen untersucht. Der Fokus liegt auf der Analyse von Constraints, die die Planung beeinflussen. Die Präsentation beginnt mit einer Definition von Constraints und führt dann zu drei verschiedenen Typen von Constraints: Modifikator, Methode und Intention. Jeder Typ wird mit Beispielen und Erklärungen versehen, um die Anwendung und Bedeutung zu verdeutlichen. Der Präsente wird dabei auf die Rolle von Constraints in der Sprachplanung eingegangen und ihre Auswirkungen auf die Leistung von LLMs diskutiert.</sample>
    <sample id="284">In diesem Video wird die Frage "Wie verhalten sich LLMs bei eingeschränkter Sprachplanung?" untersucht. Der Vortragende, der in einem grünen T-Shirt und Brille sitzt, erklärt, wie LLMs (Language Learning Models) bei der Verarbeitung von Aufgaben mit eingeschränkten Bedingungen reagieren. Er zeigt ein Beispiel, wie ein LLM bei der Erstellung eines Textes mit bestimmten Einschränkungen vorgeht. Der Vortragende beschreibt, wie LLMs die Einschränkungen verstehen und darauf reagieren, um eine passende Antwort zu generieren. Er zeigt, wie LLMs bei der Verarbeitung von Aufgabentexten mit eingeschränkten Bedin</sample>
    <sample id="285">Die Präsentation diskutiert die Fähigkeit von großen Sprachmodellen (LLMs), in kontrollierten Sprachplänen zu arbeiten. Ein Diagramm zeigt die Genauigkeit verschiedener Modelle bei der Erfüllung spezifischer Ziele. Die Ergebnisse zeigen, dass alle Basismodelle unzureichende Ergebnisse erzielen.</sample>
    <sample id="286">Die Präsentation diskutiert die Fähigkeit von großen Sprachmodellen (LLMs), in spezifischen Zielen planen zu können. Ein Diagramm zeigt die Genauigkeit verschiedener Modelle bei der Erfüllung von Zielen. Die Präsentation betont, dass alle Basismodelle unzureichende Ergebnisse erzielen.</sample>
    <sample id="287">Der englische Inhalt beschreibt die typischen Fehler, die große Sprachmodelle (LLMs) bei der Generierung von Skripten machen. Es wird erwähnt, dass die semantische Komplettheit (SE) der generierten Skripte akzeptabel ist, aber die treue Einhaltung der Einschränkungen (FE) nicht garantiert werden kann. Ein Diagramm zeigt verschiedene Schritte und Fehlerarten, die bei der Generierung von Skripten auftreten können.</sample>
    <sample id="288">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Was für Fehler machen LLMs in dieser Aufgabe üblicherweise?**

Ein Diagramm zeigt verschiedene Fehlerarten, die LLMs (Großsprachmodelle) in dieser Aufgabe machen können. Die Fehler sind in zwei Hauptkategorien unterteilt: **FE1: Keine Beschränkung** und **FE2: Fehlerhafte Reihenfolge**.

- **FE1: Keine Beschrän</sample>
    <sample id="289">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Video Transkription:**

**Title:** What kinds of goals do InstructGPT typically fail?

**Visual Elements:**
- A large table with various goals listed on the left side, such as "Work," "Relationships," "Personal Care," "Health," "Business," "Family Life," "Education," "Electronics," "Vehicles," and "Arts."
- Each goal has a corresponding success rate ranging from 0.0 to 1.0, with some goals having a success rate of 0.0.
- A checkbox with the text: "The planning performance of InstructGPT varies considerably for goals of different categories."

**Speaker:**
- A person wearing a green shirt and glasses, speaking in a modern, well-lit room with a red couch and a large window in the background.

**Speaker's Text:**
- The speaker discusses the performance of InstructGPT across different goal categories, emphasizing that the success rates vary significantly.

---

**Video Transkript:**

**Title:** What kinds of problems do InstructGPT typically fail?

**Bilder:**
- Eine große Tabelle mit verschiedenen Zielen auf der linken Seite, wie "Arbeit," "Beziehungen," "Persönliche Pflege," "Gesundheit," "Geschäft," "Familie," "Bildung," "Elektronik," "Fahrzeuge" und "Kunst."
- Jeder Ziel hat einen entsprechenden Erfolgsrate zwischen 0,0 und 1,0, mit einigen Zielen einen Erfolgsrate von 0,0.
- Ein Kästchen mit dem Text: "Die Planungsleistung von InstructGPT variiert erheblich für Ziele verschiedener Kategorien."

**Sprecher:**
- Eine Person in einem grünen Hemd und Brille, spricht in einem modernen, gut beleuchteten Raum mit einem roten Couch und einem großen Fenster im Hintergrund.

**Sprecher Text:**
- Der Sprecher diskutiert die Leistung von InstructGPT für verschiedene Zielkategorien und betont, dass die Erfolgsraten erheblich variieren.

---

**Video Transcription:**

**Title:** What kinds of issues do InstructGPT typically fail?

**Images:**
- A large table with various goals on the left side, such as "Work" (Arbeit), "Relationships" (Beziehungen), "Personal Care" (Persönliche Pflege), "Health" (Gesundheit), "Business" (Geschäft), "Family Life" (Familie), "Education" (Bildung), "Electronics" (Elektronik), "Vehicles" (Fahrzeuge), and "Arts" (Kunst).
- Each goal has a corresponding success rate between 0.0 and 1.0, with some goals having an success rate of 0.0.

**Speaker:**
- A person in a green shirt and glasses, speaking in a well-lit room with a red couch and a window in the background.

**Speaker Text:**
- The speaker discusses the performance issues of InstructGPT across different goal categories, emphasizing the significant variation in success rates.

---

**Video Trans</sample>
    <sample id="290">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Method**

**Input:** Eine abstrakte Zielvorgabe  
**Schritt 1:** Generieren spezifischer Ziele mit InstructGPT via in-context learning

**Abstrakte Zielvorgabe:**  
**Make a cake**

**Spezifische Ziele:**  
1. **G1 (Intent):** Make a chocolate cake  
2. **G2 (Method):** Use a microwave  
3. **G3 (Intent):** Make a cake for a wedding

---

**Method**</sample>
    <sample id="291">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Method**

**Input:** Eine abstrakte Zielvorgabe  
**Schritt 1:** Generieren spezifischer Ziele mit InstructGPT via in-context learning  
**Schritt 2:** Überprüfen der generierten spezifischen Ziele  
**Schritt 3:** Überprüfen der generiertenen spezifischen Ziele  
**Abstraktes Ziel:** Backen eines Kuchens  
**Spezifische Ziele:**  
- G1: Backen eines Schokoladenkuchens  
- G2: Backen eines Vanillekuchens  
- G3: Backen eines Kuchens für eine Hochzeit  
**Einschränkungen:**  
- G1: Backen des Kuchens in einem Backofen  
- G2: Backen des Kuchens in einem Mikrowelle  
- G3: Backen des Kuchens für eine Hochzeit  

---

**Method**</sample>
    <sample id="292">Der englische Text beschreibt einen methodischen Ansatz zur Erstellung von Kandidatskripten für die Generierung von Inhalten mit dem Hilfe von InstructGPT. Der Prozess besteht aus zwei Hauptstufen: 

1. **Generierung spezifischer Ziele (Step 1):** Hierbei werden spezifische Ziele basierend auf dem abstrakten Ziel generiert, um die Erreichung des Hauptziels zu unterstützen.

2. **Über-Generierung von Kandidatskripten (Step 2):** Anhand der generierten spezifischen Ziele werden Kandidatskripte mit InstructGPT erstellt, um eine Vielzahl von möglichen Inhalten zu erzeugen.

Der Text verwendet eine grafische Darstellung, um den Prozess zu erklären, und zeigt, wie die spezifischen Ziele und die Kandidatskripte miteinander in Verbindung stehen.</sample>
    <sample id="293">Der englische Text beschreibt einen methodischen Ansatz zur Erstellung von Rezepten. Es wird beschrieben, wie man eine Rezeptliste generiert, die durch eine Kombination von Zutaten und Schritten besteht. Die Methode umfasst drei Schritte: 1. Generierung von Zutaten, 2. Überprüfung der Zutaten, und 3. Zusammenstellung der Rezepte. Die Zutaten werden in einer Liste angegeben, und die Schritte werden in einer Sequenz angeordnet. Die Methode ermöglicht es, individuelle Rezepte zu erstellen, indem man die Zutaten und Schritte kombiniert.</sample>
    <sample id="294">Der englische Inhalt beschreibt einen methodischen Ansatz zur Erstellung von Rezepten mit dem Ziel, bestimmte Ziele zu erreichen. Der Prozess besteht aus drei Schritten: 

1. **Step 2: Über-generieren von Kandidatskripten mit InstructGPT**  
   Hierbei wird ein Kandidatskript mit InstructGPT erzeugt, um eine Vielzahl von möglichen Rezepten zu generieren.

2. **Step 3: Finden von bestehenden Skripten, die den Zielvorgaben entsprechen**  
   Anhand von Kriterien wird aus den generierten Kandidatskripten herausgesucht, die den Zielvorgaben am besten entsprechen.

3. **Output: Spezifische Ziele mit entsprechenden Skripten**  
   Das Ergebnis ist eine Liste von spezifischen Zielen, die mit den entsprechenden Skripten erreicht werden können.

Der Prozess wird visualisiert, um den Zusammenhang zwischen den Schritten und dem finalen Output zu verdeutlichen.</sample>
    <sample id="295">Der englische Text beschreibt einen methodischen Ansatz zur Erstellung von Rezepten. Es wird beschrieben, wie man eine Rezeptliste generiert, die dann durch eine KI-Überprüfung geprüft wird. Die KI überprüft, ob die Rezepte korrekt sind und ob sie die gewünschten Zutaten enthalten. Die KI-Überprüfung erfolgt durch eine Bewertung der Rezepte anhand von Kriterien wie Zutatenliste, Schrittweise Anweisungen und Zusammenhang der Zutaten. Die KI-Überprüfungsmethode ist so konzipiert, dass sie die Rezepte anhand von Kriterien bewertet, die von einem Menschen erstellt wurden. Die KI-Überprüft dann, ob die Rezepte korrekt und vollständig sind.</sample>
    <sample id="296">Die englische Übersetzung des Inhalts lautet:

---

**Title:** Our Method Greatly Improves the Planning Quality

**Graph:**
- The graph shows the accuracy of different methods.
- The methods are labeled as follows:
  - **TS (118)**
  - **Flan-TS (175B)**
  - **SPT-3 (175B)**
  - InstructGPT (175B)
  - Our Method
- The x-axis represents the accuracy percentage.
- The y-axis represents the different methods.

**Text:**
- **"With our method, InstructGPT can generate scripts of higher quality by a large margin."**

---

**Translation:**

**Titel:** Unsere Methode verbessert die Planungsqualität erheblich

**Diagramm:**
- Das Diagramm zeigt die Genauigkeit verschiedener Methoden.
- Die Methoden sind wie folgt bezeichnet:
  - **TS (119)**
  - **Flan-TS-175B**
  - **SPT-3 (75B)**
  - InstructGPT-175B
  - Unsere Methode
- Die x-Achse repräsentiert die Genauigkeit in Prozent.
- Die y-Achse repräsentiert die verschiedenen Methoden.

**Text:**
- **„Mit unserer Methode kann InstructGPT hochwertigere Skripte generieren, und zwar um einen großen Margen.“**

---

**Note:** Die Übersetzung ist auf Deutsch und behält die technischen Begriffe und den Kontext des Originaltextes bei.</sample>
    <sample id="297">Der englische Inhalt beschreibt ein Verfahren zur Entwicklung von Skripten aus großen Sprachmodellen, um die Sprachplanungsfähigkeit kleinerer Modelle zu verbessern. Hier ist die Übersetzung ins Deutsche:

---

**Motivation:**

Um die Sprachplanungsfähigkeit von kleineren Modellen zu erweitern, die auf großen Sprachmodellen basieren.

---

**Method:**

1. **Schritt 1:** Generieren Sie ein abstraktes Skript mit InstruktGPT via in-context learning.
2. **Schritt 2:** Über-generieren Sie Kandidatskripte mit InstruktGPT über kontextuelles Lernen.
3. **Schritt 3:** Finden Sie das am besten geeignete Skript zum Ziel mit InstruktGPT über in-contextual similarity score.

---

**Output:**

Spezifische Ziele mit entsprechenden Skripten.

---

**Motivation:** 

Um die Sprachplanungsfä</sample>
    <sample id="298">Der englische Inhalt beschreibt ein Verfahren namens "Script Distillation from LLMs" (Skript-Distillation von LLMs), das darauf abzielt, die Sprachplanungsfähigkeit kleinerer Modelle zu erweitern. Hier ist die Übersetzung ins Deutsche:

---

**Motivation:**

Um die Sprachplanungsfähigkeit für kleinere Modelle zu erweitern.

---

**Method:**

1. **Folge dem Konzept der symbolischen Wissensdistillation:**
   - Generiere 5.000 Skripte mit Einschränkungen basierend auf LLMs basierend auf unserer Methode = &gt; Coscript-Datensatz.
   - Menschen beurteilen Validierung und Testset.

2. **Über-generiere Kandidatskripte mit InstruktGPT basierend auf kontextlernem Lernen:**
   - Über-generiere Kandidatskripte basierend auf kontextlernem InstruktGPT.

3. **Finde das am besten geeignete Skript zum Ziel mit InstruktGPT basierend auf Inhaltliche Ähnlichkeit:**
   - Finde das am besten geeignete Skrip basierend auf Inhaltliche Äh</sample>
    <sample id="299">Das Bild zeigt eine Präsentation über "Script Distillation from LLMs" (Script-Distillation von LLMs). Die Präsentation besteht aus drei Hauptteilen: Motivation, Methode und Ausgang. 

**Motivation:**
Die Motivation dient dazu, die Sprachplanungsfähigkeit kleinerer Modelle zu erweitern. 

**Methode:**
Die Methode folgt dem Konzept der symbolischen Wissensdistillation. Es werden 5.000 Skripte generiert, die auf dem LLM basieren und auf dem Coscript-Datensatz trainiert werden. Die Skripte werden von Menschen annotiert und validiert. 

**Ausgang:**
Das Ergebnis ist spezifische Skripte mit korrespondierenden Planen. 

Die Präsentation wird von einer Person in einem modernen Büro durchgeführt.</sample>
    <sample id="300">Der englische Inhalt beschreibt ein Verfahren zur Entwicklung von Skripten aus großen Sprachmodellen, um kleinere Modelle zu trainieren, die sprachplanungsfähigkeit haben. Es basiert auf dem Konzept der symbolischen Kenntnisvermittlung und nutzt die Coscript-Datenbank. Die Methode besteht aus drei Schritten: 1. Generierung von Skripten mit InstruktGPT über kontextlernende Lernen, 2. Übergenerierung von Kandidatskripten mit InstruktGPT über kontexlernende Lernen, 3. Auswahl des am besten passenden Skripts basierend auf der Inkonsistenzskorrelation.</sample>
    <sample id="301">Der englische Inhalt beschreibt ein Verfahren zur Entwicklung von Skripten aus großen Sprachmodellen, um die Sprachplanungsfähigkeit kleinerer Modelle zu verbessern. Hier ist die Übersetzung ins Deutsche:

---

**Motivation:**

Um die Sprachplanungsfähigkeit von kleineren Modellen zu erweitern, die auf großen Sprachmodellen basieren.

---

**Method:**

1. **Schritt 1:** Generieren Sie ein großes Skript mit InstruktGPT via kontextlernendes Lernen.
2. **Schritt 2:** Über-generieren Sie Kandidatskripte mit InstruktGPT.
3. **Schritt 3:** Finden Sie das passende Skript, das dem Ziel am besten entspricht.

---

**Output:**

Spezifische Ziele mit entsprechenden Skripten.

---

**Motivation:** 

Um die Sprachplanungsfä</sample>
    <sample id="302">Der englische Inhalt wird ins Deutsche übersetzt:

---

**Constraint Analysis**

Coscript zeigt hohe Heterogenität und Pluralismus in den generierten spezifischen Zielen.

---

**Coscript für kleinere Sprachmodelle**

**Datasets**
- Coscript
- wiki-how

**Metriken**
- Fahigkeitsmodell: DeBERTa (v3 large) zur Entscheidung darüber, ob die generierten Texte den eingesetzten Constraints entsprechen
- Automatische Metriken: ROUGE, BLEU, BERTScore

---

**Coscript für smaller Language Models**

**Datasets**
- **Coscript**
- **wiki-how**

**Metriken**
- **Fahigkeitsmodell:** DeBERTa (v3 large) zur Überprüfung, ob die generierten Texte den Constraints entsprechen
- **Automatische Metriken:** ROUGE, BLEU, BERTScore</sample>
    <sample id="303">Der englische Inhalt des Bildes beschreibt eine Studie, die die Leistung von kleineren, auf Coscript feinabgestimmten Sprachmodellen (LLMs) mit denen von größeren, allgemeinen LLMs vergleicht. Die Studie zeigt, dass kleinere Sprachmodelle, die auf Coscript feinabgestimmt wurden, höhere Qualität anhand von Skripten erzeugen können als größere, allgemeine LLMs. Die Studie beinhaltet eine Tabelle, die die Leistung verschiedener Sprachmodelle in Bezug auf ihre Genauigkeit (Accuracy) vergleicht. Die Tabelle zeigt, dass die kleineren Sprachmodelle, die auf Coscript fineabgestimmt wurden, höher genau sind als die größeren, allgemeinen Sprachmodelle. Die Studie zeigt, dass kleiner, auf Coscript feinabgestimmte Sprachmodelle höhere Qualität anhand von Skriptern erzeugen können als größere Sprachmodelle. Die Studie beinhaltet eine Liste von Vorteilen und Herausforderungen des vorgeschlagenen Verfahrens zur Verbesserung von LLMs. Die Vorteile des vorgeschlagenen Verf</sample>
    <sample id="304">**Zusammenfassung und Schlussfolgerungen**

- **Definieren Sie das Problem der **konstrainierten Sprachplanung**.**
- **Bewerten Sie die Fähigkeit von LLMs, Sprachplanung unter Einschränkungen zu erreichen und entwickeln Sie eine **übergenerierende dann filterende** Methode für LLMs.**
- **Verwenden Sie LLMs, um ein hochwertiges **Skriptdatenset (CoScript)** zu generieren, das für die Sprachplanung eingesetzt werden kann.**
- **Diskutieren Sie die **Einschränkungen und zukünftigen Arbeiten** und stellen Sie fest, dass die vorgeschlagene Methode für die Verbesserung von LLMs eine **post-hoc Re-Ranking-Strategie** ist. CoScript erbt von einem abstrakten Modell mit einer zusätzlichen Einschränkung. Das CoScript-Datenset kann als wertvoller Beitrag zur Förderung der Forschung zur Sprachplanung mit komplexeren und vielfältigeren Zielen und Einschränkungen dienen.**</sample>
    <sample id="305">Die englische Präsentation wird in Deutsch übersetzt:

---

**Zusammenfassung und Schlussfolgerungen**

- **Konstrainierte Sprachplanung-Probleme**
  - Problembestimmung: Definieren Sie das Problem der konstrainierten Sprachplanung.
  - Evaluierung: Bewerten Sie die Fähigkeit von LLMs, konstrainierte Sprachplanung zu erreichen und entwickeln Sie eine "über-generierende -dann-filtern"-Methode für LLMs.

- **Verwendung von LLMs zur Generierung eines hochwertigen Scriptdatensatzes (CoScript)**
  - Ziel: Erstellen Sie einen hochwertigen Scriptdatensatz, der für die konstrainierte Sprachplanung geeignet ist.

- **Einschränkungen und zukünftige Arbeiten**
  - Post-hoc Re-Ranking: Entwickeln Sie eine Methode, um die Leistung von LLMs zu verbessern, indem Sie das Ergebnis nachträglich umordnen.
  - CoScript: Erweitern Sie das Konzept von CoScript, indem Sie es mit einer zusätzlichen Einschränkung vereinfachen.
  - Nutzung des CoScript-Datensatzes: Nutzen Sie den CoScript-Datensatz, um die Forschung auf Sprachplanung mit komplexeren und vielfältigeren Zielen und Einschränkungen voranzutreiben.

---

**Zusammentragen**

- **Konstruierte Sprachplanung-Probleme:**
  - Problembestimmung definieren.
  - Fähigkeit von LLMs zur Konstruktion von Sprachplänen evaluieren.
  - "Über-generierende -dann-filTERN"-Methode für LLMs entwickeln.

- **Verwendung von CoScript-Datensatz:**
  - Hochwertigen Scriptdatensatz generieren.

- **Einschränken und zukün</sample>
    <sample id="306">Die folgende Beschreibung ist der englischen Inhalt des Bildes in deutscher Sprache:

---

**61. Jahrestagung der Association for Computational Linguistics (ACL) in Toronto, Kanada vom 7. bis 14. Juli 2023**

---

**Titel der Präsentation:**

**Distilling Script Knowledge from Large Language Models for Constrained Language Planning**

---

**Autoren:**

- Siyu Yuan
- Jiangjie Chen
- Ziqian Fu
- Xuyang Ge
- Soham Shah
- Charles Robert Jankowski
- Yangxia Xiao
- Deqing Yang

---

**Kontaktinformationen:**

- **E-Mail:** siyuanyuan21@m.fudan.edu.cn
- **GitHub-Repository:** [https://github.com/siyuanyuan/coscript](https://github.com/siyuanyuan</sample>
    <sample id="307">Die Sprachgewandtheit von PaLm ist vergleichbar mit SOTA-Systemen, allerdings haben spezialisierte SOTA-Systeme einen erheblichen Vorteil. PaLm ist ähnlich wie Google Translate in der Qualität. Die Genauigkeit von PaLm ist im Durchschnitt niedriger als bei SOTA-Systemen, vor allem aufgrund von "Accuracy/Omissionen". Die Sprache ist im Allgemeinen weniger "Stilig/Awkward" als bei PaLm.</sample>
    <sample id="308">Die wichtigsten Eigenschaften e</sample>
    <sample id="309">14. Chinese.</sample>
    <sample id="310">Aus einem Datensatz werden 200 Instanzen extrahiert, um für die erneute Annotierung zu verwenden.</sample>
    <sample id="311">Cosine distance, Jensen-Shannon divergence, and KL divergence.</sample>
    <sample id="312">Die Modelle, die auf einem mehrsprachigem Encoder basieren, wurden in dieser Aufgabe mit Pointer-based Decoders implementiert.</sample>
    <sample id="344">Die Autoren zählen die Wörter auf, um die Wörter mit mittlerer Häufigkeit auszuwählen.</sample>
    <sample id="345">Die Bildergalerie besteht aus 20 identischen Bildern, die alle das gleiche Layout und die gleichen Inhalte enthalten. Jedes Bild zeigt eine weiße Hintergrund mit einem goldnen, diagonalen Strich, der von links oben nach rechts unten verläuft. In der linken unteren Ecke befindet sich ein runder Porträt eines Mannes mit dunklen Brille, der aufgrund der gleichen Position und Größe in allen Bildern identisch ist. Im oberen rechten Bereich steht die Schrift "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" in schwarzer Farbe. Unterhalb dieser Schrift befindet sich die Schrift "Shuheng Liu, Alan Ritter, School of Interactive Computing, Georgia Institute of Technology" in kleiner, schwarzer Schrift. Im unteren rechten Bereich ist das Logo der Georgia Tech mit der Schrift "GT Tech" zu sehen.</sample>
    <sample id="346">**Übersetzung des Inhalts aus dem Englischen ins Deutsche:**

---

**Titel:** Named Entity Recognition &amp; Generalization  
**Untertitel:**  
**Inhalt:**  

---

**Übersetzung:**  

---

**Titel:** Namensgebung und Generalisierung  
**Untertitel:**</sample>
    <sample id="347">**Named Entity Recognition &amp; Generalization**  
- **Models have been using CoNLL-2003 to develop NER for almost 20 years**  
- **Can these models generalize to modern data?**  
- **What is needed for good generalization?**</sample>
    <sample id="348">Named Entity Recognition &amp; Generalization</sample>
    <sample id="349">**Named Entity Recognition &amp; Generalization**  
- Modelle haben seit fast 20 Jahren mit CoNLL-2003 entwickelt, um Named Entity Recognition (NER) zu entwickeln.  
- Können diese Modelle auf moderne Daten generalisieren?  
- Was ist für gutes Generalisierung notwendig?</sample>
    <sample id="350">Die Übersetzung des englischen Textes ins Deutsche lautet:

---

**CoNLL++ Dataset**

**Gesammeltes Reuters-Nachrichten aus dem Jahr 2020 und annotiert mit CoNLL-2003-Annotationsrichtlinien**

| WORT          | POS-Tag   |
|---------------|-----------|
| AMBASSADOR    | O         |
| TO            | O         |
| THE           | O         |
| UNITED        | I-ORG     |
| NATIONS       | I-ORG     |
| -             | O         |
| LINDA         | I-PER     |
| THOMAS-GREENFIELD | I-PER |

---

**Georgia Tech**</sample>
    <sample id="351">**CoNLL++ Dataset**  

**Erhebung und Annotation:**  
- **Datenquelle:** Reuters-Nachrichten aus dem Jahr 2020  
- **Annotation:** Anhand der CoNLL-2003-Richtlinien  

**Modelle:**  
- **Anzahl der Modelle:** 20+  
- **Angepasste Modelle:** CoNLL-2003  

**Evaluierung:**  
- **Datensatz:** CoNLL-2003 Testset  
- **Evaluationsdatensatz:** CoNLL++</sample>
    <sample id="352" />
    <sample id="353">Der Inhalt des Videos ist in englisch, daher wird keine Übersetzung ins Deutsche durchgeführt.</sample>
    <sample id="354">**Was Braucht es für gutes Generalisieren?**

- **Modellarchitektur**
- **Transformer-Modelle generalisieren besser**

**Was Braucht es für gutes Generalieren?**

- **Modell-Architektur**
- **Transformer-Modell generalisieren besser**

**What Is Needed for Good Generalization?**

- **Model architecture**
- **Transformer models generalize better**

**Was Braucht es fürs gute Generalisieren?**

- **Model-Architektur**
- **Transformermodelle generalisieren besser**

---

**Was Braucht es für Gutes Generalisieren?**

- Modellarchitektur
- Transformer-Modelle generalisieren besser

**Was Braucht es für ein gutes Generalisieren?**

**Was Braucht es, um gutes Generalisieren zu erreichen?**

- **Modellarchitectur**
- **Transformer-Modelle generieren besser**

**Was Brauchte es für gutes Generalisieren?**  
- **Modellarchitektur:**  
- **Transformer-Modelle generalisiert besser:**  

**Was Braucht es für eine gute Generalisierung?**  
- **Modellarchitectur:**  
- **Transformer-Model leisten besser:**  

**Was Brauche ich für gutes Generalisieren?**</sample>
    <sample id="355">**Was ist für gutes Generalisierung notwendig?**

- **Modellarchitektur:** Transformer-Modelle generalisieren besser.
- **Modellgröße:** Größere Modelle generalisieren besser.

**Was ist für gutes Generalisierung nicht notwendig?**

- **Modelleinstellungen:** Die Modellarchitektur und Größe sind die wichtigsten Faktoren für gutes Generalisierung.</sample>
    <sample id="356">### Was Braucht es für gutes Generalisieren?

- **Modellarchitektur**: Transformer-Modelle generalisieren besser.
- **Modellgröße**: Größere Modelle generalisieren besser.
- Anzahl an Feinabstimmungsbeispielen: Mehr Beispiele führen zu besserer Generalisierung.

### Was Braucht es für gutes Generalieren?

- **Modellarchitkturen**: Transformer-Modelle generalisieren bess</sample>
    <sample id="357">Der Inhalt des Bildes ist ein leeres Bild mit der Überschrift "What Causes Performance Drop?" und einem Logo in der rechten unteren Ecke, das das Georgia Tech-Logo enthält.</sample>
    <sample id="358">Der Inhalt des Videos ist in deutscher Sprache.</sample>
    <sample id="359">Der Inhalt des Bildes ist ein statisches Screenshot einer Präsentation oder eines Videos, der sich auf die Frage "Was verursacht Leistungsabfall?" konzentriert. Es zeigt eine Liste von möglichen Ursachen, nämlich "Adaptive Overfitting?" und "Temporal Drift?". Der Text ist auf einer weißen Hintergrund mit schwarzen Buchstaben und befindet sich auf der linken Seite des Bildes. Im unteren linken Ecken ist ein Porträt eines Mannes zu sehen, der mit Brille und dunklem Hemd ausgestattet ist. Im unteren rechten Ecken ist das Logo von Georgia Tech zu sehen.</sample>
    <sample id="360">**Was verursacht Leistungsabfall?**  
- **Anpassungsüberempfindlichkeit?**  
- **Temporärer Abstieg?**  

**Was verursacht Leistungsabfälle?**  
- **Anpassungs-Überempfindlichkeit?**</sample>
    <sample id="361">**Was verursacht Leistungsabfall?**

- **Anpassungsüberempfindlichkeit?**
- **Zeitliche Abweichung?**

**Was verursacht Leistungsabfälle?**

- **Anpassungs-Überempfindlichkeit?**
  - Keine abnehmenden Ergebnisse
- **Zeitliche Abweichungen?**

**Was verursacht Performanceabfälle?**

- Anpassungsüberempfindlichkeit
- Keine abnehmenden Ergebnis</sample>
    <sample id="362">**Was verursacht Leistungsabfall?**

- **Anpassungsübefitting?**
- **Keine abnehmenden Renditen**
- **Nicht beobachtet**
- **Zeitliche Abhängigkeit?**

**Was verursacht Leistungsabfalle?**

- **Anpassungs-übefitting?**
- **Nicht abnehmende Renditen**
- **Nicht bekannt**
- **Zeitliche Abhänigkeit?**

**Was veranlasst Leistungsabfälle?**

- **Anpassungs überfüllung?**
- **Keine abnehmbaren Renditen**
- **Nicht bewusst**
- **Zeitliche Abhängigkeit?**

**Was verhindert die Leistungssteigerung?**

- **Anpassungs übefitting?**
- **Kein abnehmender Rendite**
- **Nicht beobachtete**
- **Zeitliche Abh.</sample>
    <sample id="363">Title: What Causes Performance Drop?

Bullet Points:
- Adaptive overfitting?
  - No diminishing returns
  - Not observed
- Temporal drift?

Graphs:
The graph on the right shows the performance of different models over time, measured by CoNLL-2003 F1 Score. The x-axis represents the year, and the y-axis represents the F1 score. The different lines represent different models, with the best performing model highlighted in red. The graph shows that the performance of the models has been relatively stable over time, with no significant drops or improvements. The graph on the left shows the performance of different models on a different dataset, with the same axes and lines as the graph on the right. The graph shows that the performance of the different models is similar to the graph on the right, with no significant drops or improvements.</sample>
    <sample id="364">Was verursacht Leistungsabfall?</sample>
    <sample id="365">Was verursacht die Leistungsabnahme?</sample>
    <sample id="366">**Schlussfolgerung**

*Für eine gute Generalisierung benötigen wir:*

* Bessere Modellarchitektur
* Größere Modellgröße
* Mehr Feinabstimmungsbeispiele

**Erfahrungen mit verschiedenen Modellen:**

* **BERT-Large:**
	+ 2019: 85.0
	+ 2020: 86.5
	+ 2021: 87.2
	+ 2022: 88.0
* **RoBERTa-Large:**
	+ 2020: **87.5**
	+ 2021: **88.2**
	+ 2022: **89.0**
* **DeBERTa-NER:**
	+ 2019-2022: 75.0
* **RoBERTa-NER:**
	+ 1999-2022: 78.0

**Zusammenfassung:**

Die Ergebnisse zeigen, dass die Modellgröße und die Anzahl der Feinabstimmungsbeispiele wichtige Faktoren für die Generalisierung sind. Die Modellarchitektur spielt ebenfalls eine Rolle, aber die größere Modellgröße und die Anzah</sample>
    <sample id="367">### Schlussfolgerung

**Für eine gute Generalisierung benötigen wir:**
- Eine bessere Modellarchitektur
- Eine größere Modellgröße
- Mehr feine-Tuning-Beispiele

**Die Performance-Drop ist durch folgende Faktoren verursacht:**
- Temporale Drift
- Nicht adaptive Overfitting

---

**Graph:**

- **X-Achse:** Jahre (2004 bis 2022)
- **Y-Achse:** Genauigkeit (0 bis 100)
- **Linien:**
  - **BlitzNIR:** Stabile Genauigkeit über die Jahre
  - **Illinois-NER:** Genauigkeit steigt über die Jahre
  - **CoNLL-2009:** Genauigkeit steigt über die Zeit
  - **CoNLL-2103:** Genauigkeit steigt über die Jahrzehnte
  - **BERT-Large:** Genauigkeit steigt über die Zeiten
  - **LUXE:** Genauigkeit steigt über die Zehn Jahre

---

**Logo:**
- **Georgia Tech** (Georgia Institute of Technology)</sample>
    <sample id="368">**Schlussfolgerung**

Für eine gute Generalisierung benötigen wir:

- Bessere Modellarchitektur
- Größere Modellgröße
- Mehr feine Tuning Beispiele

Die Leistungsabnahme wird durch folgende Faktoren verursacht:

- Temporale Drift
- Nicht adaptives Overfitting

Funktionieren CoNLL-2003-Tagger noch?

Ja!</sample>
    <sample id="369" />
    <sample id="370">Der Inhalt des Bildes ist ein statiches Screenshot eines PowerPoint-Präsentationsslides, der keine Bewegungen oder Änderungen im Laufe der Zeit aufweist. Der Präsentationssatz besteht aus folgenden Elementen:

1. **Hintergrund**: Der Hintergrund ist ein grauer, leicht verfärbter Bild, das die Details eines Stadtviertels zeigt. Die Details sind in einem weißen Farbton dargestellt, was sie leicht lesbar macht.

2. **Text**: Der Text ist in zwei verschiedene Farben dargestellt:
   - **Schwarz**: Der Text "Paper:", "Dataset:", und "Contact:" ist in schwarz geschrieben.
   - **Blau**: Die URLs, die in der Form "https://..." angegeben sind, sind in blau dargestellt.

3. **Links**: Die URLs sind in einem leicht verschobenen Format dargestellt, um sie leicht zu lesen.

4. **Logo**: Im rechten unteren Bereich des Bildes ist das Logo von Georgia Tech mit der Abkürzung "GT" zu sehen.

5. **Persönliche Informationen**: Im linken unteren Bereich des Bildes ist ein Porträt eines Mannes zu sehen, der mit Brille und kurzem Haar gekleidet ist.

6. **Farben**: Die Farben des Bildes sind neutral und unauffällig, was den Fokus auf den Text legt.

7. **Textinhalt**: Der Textinhalt des Bildes ist als folgende Informationen zu interpretieren:
   - **Paper**: https://arxiv.org/abs/2212.09747
   - **Dataset**: https://github.com/ShuhengL/ac2023_conllpp
   - **Contact**: silu775@gatech.edu

8. **Format**: Der Text ist in einer klaren, leicht lesbaren Schriftart dargestellt, die für Präsentationen geeignet ist.

9. **Anordnung**: Die Informationen sind in einer logischen Reihenfolge angeordnet, mit dem Paper als erster Punkt, dem Dataset als zweiter Punkt und dem Contact als dritter Punkt.

10. **Zusammenhang**: Der Zusammenhang zwischen den Elementen des Bildes ist, dass es eine Präsentationsseite ist, die Informationen über ein Forschungsprojekt oder eine Veröffentlichung bereitstellt.

11. **Anwendung**: Dieses Bild könnte in einer Präsentation oder in einem Bericht verwendet werden, um Informationen über ein Forschungsprojekt, ein Datensatz oder eine Veröffentlichung zu vermitteln.

12. **Zusätzliche Informationen**: Der Präsentationssatz enthält keine zusätzlichen Informationen oder Details, die nicht bereits in den angegebenen Texten enthalten sind.

13. **Fokus**: Der Fokus des Bildes ist auf die Bereitstellung von Informationen über das Forschungsprojekt, das Datensatz und den Kontakt.

14. **Lesbarkeit**: Der Text ist leicht lesbar und kann von einer breiten Zielgruppe verstanden werden.

15. **Design**: Das Design des Bildes ist einfach und unauffällig, was den Faktor Lesbarkeit und Verständlichkeit erhöht.

16. **Verwendung**: Dieses Bild könnten in einer Präsentation oder in einem Dokument verwendet werden, um Informationen über das Forschungsprojekt oder das Datensatz zu vermitteln.

17. **Zusammenhang**: Der Text und die Informationen sind in einer logischen Rei</sample>
    <sample id="397">Bei dem Ansatz wird eine Sprachsegmentgröße von 128 Tokens verwendet.</sample>
    <sample id="398">Das entitätsspezifische Wissen, das im Beispiel mit Servin und KeA benötigt wird, ist das Wissen darüber, dass Servin ein Richter ist und KeA ein Bäcker ist.</sample>
    <sample id="399">Der wichtigste Faktor zwischen **der Qualität des Beispiels** und der **Ähnlichkeit mit dem Ausgangssatz** ist die **Qualität des Beispiels**.</sample>
    <sample id="400">Die Arbeiten in den erweitertenen Experimenten konzentrieren sich auf die Sprachmodelle RoBERTa und GPT-2.</sample>
    <sample id="401">Kombiniert Werte aus mehreren Ebenene.</sample>
    <sample id="402">"Easy on me" und "the first one".</sample>
    <sample id="403">University of Toronto</sample>
    <sample id="404">Fünf Autoren sind an der Arbeit beteilt.</sample>
    <sample id="405">Ja, die Übersetzung der natürl</sample>
    <sample id="406">A warrior (unmarked) vs. a woman warrior (marked).</sample>
    <sample id="407">Transformer Modelle generalisieren nicht gut.</sample>
    <sample id="408">The test datasets are named "FT_C", "Cosine", "L2N", "L2Bond", "MLC", and "Weak labels".</sample>
    <sample id="409">6</sample>
    <sample id="410">Mehrere Modalitäten.</sample>
    <sample id="439">Die Autoren sehen das Verständnis der Präsentationsinhalte und der darin enthaltenen Informationen als zu wenig erforscht.</sample>
    <sample id="440">Zhiyang Xu, Ying Shen, Lifu Huang.</sample>
    <sample id="441">Ja, Coscript hat eine Qualitätskontrolle durchgelaufen.</sample>
    <sample id="442">Die Grenzen bestehender Ressourcentechniken für kontextbasierte Übersetzungen liegen in der Einschränkung auf korpusbasierte Metriken und die Unterstützung limitierter Diskurse, Phänomene und Sprachen.</sample>
    <sample id="443">Das Video ist ein Vortrag über das Thema "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)" von Mohammad Javad Hosselini, Filip Radlinski, Silvia Pareti und Annie Louis. Der Vortrag wird von einem Mann in einem dunklen Blazer gehalten, der sich in einem Rundum-Video anzeigt. Der Vortrag ist in Englisch gehalten und beinhaltet eine Reihe von Bildern und Texten, die die Inhalte des Vortrags erklären. Der Vortrag ist in der Regel auf Englisch gehalten und beinh</sample>
    <sample id="444">Das Video ist ein Vortrag über die "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)" von Mohammad Javad Hosselini, Filip Radlinski, Silvia Pareti und Annie Louis, die von Google Research durchgeführt wurde. Der Vortrag beginnt mit einer Einführung in das Thema und fokussiert sich auf die Herausforderungen und Methoden zur Lösung indirekter Referenzausdrücke in der Entitätsauswahl. Der Vortrag wird von einem Vortragenden gehalten, der sich auf die technischen Aspekte und die Anwendungen der Methode konzentriert. Der Vortragende verwendet visuelle Hilfsmittel, um die Inhalte des Themas zu erklären und die Zuhörer zu informieren. Der Vortragende verwendet auch Beispiele und Anwendungsfälle, um die Bedeutung der Methode zu verdeutlichen. Der Vortragende verwendet eine klare und präzise Sprache, um die Inhalte des Themas verständlich zu erklären. Der Vortragende verwendet keine unnötigen Bilder oder Animationen, um den Fokus auf den Inhalt des Themas zu halten. Der Vortragende verwendet einen strukturierten Ansatz, um den Inhalt des Themas zu erklären. Der Vortragsende verwendet eine klare und pr</sample>
    <sample id="445">Die Präsentation beschäftigt sich mit der Verständnisprinzipien von Indirekten Referenzausdrücken, die in natürlicher und flüssiger Konversation verwendet werden. Die Präsentation beginnt mit der Frage, wie man die Sprache der Nutzer verstehen kann, wenn sie eine Wahl treffen. Es wird dann die direkte Referenz erläutert, die auf spezifischen Namen oder Dinge verweist, wie zum Beispiel "die erste" oder "das erste". Die Präsentation erklärt, dass indirekte Referenz ausdrücke in natürlicher und flüssiger Sprache verwendet werden und dass sie häufiger sind als direkte Referenz. Es wird erwähnt, dass indirekte Referenzausdrücke nicht immer den Namen oder das Objekt bezeichnen können, sondern oft nur den Kontext oder den Satz, in dem sie verwendet werden. Die Präsentierung erklärt, dass indirekten Referenzausdrücke häufiger verwendet werden, wenn man sich nicht sicher ist, ob man den Namen oder das Objekt erinnert, oder wenn man den Namen oder das Objekt nicht genau erinnert. Es wird auch erwähnt, dass indirekten Referenz ausdrücke oft verwendet werden, um den Kontext oder den Satz zu beschreiben, in dem sie verwendet werden.</sample>
    <sample id="446">Das Bild zeigt eine Präsentation über "Indirect Referring Expressions" (indirekte Bezugnahmen) auf der Google Research-Website. Die Präsentation hat das Ziel, das Verständnis der Nutzersprache zu verbessern, wenn sie eine Wahl treffen. Es wird ein Beispiel angegeben: "Did you mean easy on me or I gotta feeling?" (Willst du sagen, dass es leicht für mich ist oder dass ich müde bin?). Die Präsentation unterscheidet zwischen direkten und indirekten Bezugnahmen. Direkte Bezugnahmen sind beispielsweise "easy on me" oder "the first one". Indirekte Bezugnahmen können in natürlicher und flüssiger Konversation verwendet werden, wenn man den Namen nicht erinnert, die Aussprache schwer zu unterscheiden hat oder den Vorzug eines bestimmten Begriffs angeben möchte. Ein Beispiel für eine indirekte Bezugnahme ist "The newer one" oder "The song that's not energetic".</sample>
    <sample id="447">Das Bild zeigt eine Präsentation über "Indirect Referring Expressions" (indirekte Bezugnahmen) auf der Google Research-Website. Die Präsentation hat das Ziel, das Verständnis der Nutzersprache zu verbessern, wenn sie eine Wahl treffen. Es wird ein alternativer Fragebogen angegeben: "Did you mean easy on me or I gotta feeling?" (Willst du mir leicht machen oder muss ich rausgehen?). Die Präsentation unterscheidet zwischen direkten und indirekten Bezugnahmen. Direkte Bezugnahmen sind beispielsweise "easy on me" oder "the first one". Indirekte Bezugnahmen könnten in natürlicher und flüssiger Konversation verwendet werden, wenn man den Namen nicht erinnert, die Aussprache schwer zu unterscheiden ist oder man einen Vorzug spezifizieren möchte. Ein Beispiel für eine indirekte Bezugnahme ist "The newer one. The song that's not energetic." (Der neuere. Das Lied, der nicht energisch ist). Die Präsentation wird von der Google Research-Seite veröffentlicht.</sample>
    <sample id="448">Das Bild zeigt eine Präsentation über "Indirect Referring Expressions" (indirekte Bezugnahmen) auf der Google Research-Website. Die Präsentation hat das Ziel, das Verständnis der Nutzersprache zu verbessern, wenn sie eine Wahl treffen. Es wird ein Beispiel angegeben: "Did you mean easy on me or I gotta feeling?" (Willst du sagen, dass es leicht für mich ist oder dass ich müde bin?). Die Präsentation unterscheidet zwischen direkten und indirekten Bezugnahmen. Direkte Bezugnahmen sind beispielsweise "easy on me" oder "the first one". Indirekte Bezugnahmen können in natürlicher und flüssiger Konversation verwendet werden, wenn man den Namen nicht erinnert, die Aussprache schwer zu unterscheiden hat oder den Vorzug eines bestimmten Begriffs angeben möchte. Ein Beispiel für eine indirekte Bezugnahme ist "The newer one. The song that's not energetic." (Der neuere. Das Lied, der nicht energisch ist).</sample>
    <sample id="449">Das Bild zeigt eine Präsentation über "Indirect Referring Expressions" (indirekte Bezugnahmen) auf der Google Research-Website. Die Präsentation beschreibt die Zielsetzung, die Nutzung von indirekten Bezugnahmen in natürlicher und flüssiger Konversation zu verstehen. Es wird dargestellt, wie man bei der Wahl zwischen zwei Alternativen, wie "Did you mean easy on me or I gotta feeling?" (bedeutet: "Willst du einfach mit mir oder muss ich mich entspannen?"), entscheiden kann. Die Präsentation unterscheidet zwischen direkten und indirekten Bezugnahmen und erklärt, warum indirekte Bezugnahmen in natürlichem Gespräch häufiger sind. Es wird betont, dass indirekte Bezugnahmen hilfreich sein können, wenn man den Namen nicht erinnert oder die Aussprache schwer zu unterscheiden ist. Die Präsentation zeigt, wie man beispielsweise "The newer one" (das neuere) und "The song that's not energetic" (das Lied, das nicht energisch ist) als indirekte Bezugnahmen verwenden kann.</sample>
    <sample id="450">Das Video präsentiert eine Präsentation über die Datensammlung für die Erforschung der Verständnisfähigkeit von großen Sprachmodellen. Die Präsentation beginnt mit der Identifizierung eines wichtigen Problems: die Benchmarkung der Verständnisfähigkeiten von großen Sprachmodellen in konversationsbasierten Systemen. Es wird darauf hingewiesen, dass keine großskaligen öffentlichen Datensätze verfügbar sind, was die Notwendigkeit einer umfangreichen Datensammlung durch Crowdsourcing hervorhebt. Die Präsentation erklärt, wie die Datensammlung durch die Anwendung von Crowd Annotation durchgeführt wird, um Daten in drei verschiedenen Domänen zu sammeln: Konversationssysteme, Bücher und Dokumente. Die Präsentation endet mit einer Zusammenfassung der wichtigsten Punkte und einer Aufforderung an die Zuschauer, die Präsentation zu bewerten.</sample>
    <sample id="451">Der Präsentator erklärt, dass es ein wichtiges Problem gibt, nämlich die Verständnisfähigkeit von großen Sprachmodellen zu messen. Er erwähnt, dass es keine großskaligen öffentlichen Datensätze gibt und dass sie eine große Datenmenge mithilfe von Crowd Annotation sammeln. Die drei Bereiche, die betrachtet werden, sind:</sample>
    <sample id="452">Die folgende Beschreibung basiert auf den in der Anleitung angegebenen Bildern und Texten:</sample>
    <sample id="453">Die folgende Beschreibung basiert auf den in der Anleitung angegebenen Bildern und Texten:</sample>
    <sample id="454">Die folgende Beschreibung basiert auf den in der Anleitung angegebenen Bildern und Texten:</sample>
    <sample id="455">Die folgende Beschreibung basiert auf den in der Anleitung angegebenen Bildern und Texten:</sample>
    <sample id="456">Die folgende Beschreibung basiert auf den in der Anleitung angegebenen Bildern und Texten:</sample>
    <sample id="457">Die folgende Beschreibung basiert auf den in der Anleitung angegebenen Informationen und bietet eine detaillierte Analyse des Inhalts und der Struktur der Bilder.</sample>
    <sample id="458">Generate alternative questions =&gt; sampling entity pairs  
Do you mean A or B?  
Items with similar infoboxes on Wikipedia (same genre and/or artist)  
Do you mean This is it or Man in the Mirror?  
Items with similar descriptions on Wikipedia  
Do you mean Thinking of You or Happy Anywhere?  
Items with similar titles  
Do you mean The Return (memoir) or The Return (Shatner novel)  
Uniform at random:  
Do you mean You Could Be Mine or The Way I Am</sample>
    <sample id="459">Der englische Text auf der Präsentationsseite lautet: "Generate alternative questions =&gt; sampling entity pairs" und "Do you mean A or B?" Die folgenden Punkte sind auf der Seite aufgelistet: "Items with similar infoboxes on Wikipedia (same genre and/or artist)", "Do you mean This is it or Man in the Mirror?", "Items with similar descriptions on Wikipedia", "Do you mean Thinking of You or Happy Anywhere?", "Items with similar titles on Wikipedia", "Do you mean The Return (memoir) or The Return (Shatner novel)", "Uniform at random: Do you mean 'Could Be Mine' or 'The Way I Am'".</sample>
    <sample id="460">Der Bildtext beschreibt eine Vorgehensweise zur Generierung alternativer Fragen, um Entitätspaare zu wählen. Es wird erwähnt, dass die Entitäten mit ähnlichen Infoboxen auf Wikipedia (gleiche Genre und/oder Künstler) oder ähnlichen Beschreibungen auf Wikipedia verglichen werden. Es wird auch erwähnt, dass die Entitätsnamen mit ähnlichen Titeln auf Wikipedia verglichen werden, wie zum Beispiel "The Return (Memoir)" oder "The Return (Shatner Roman)". Zusätzlich wird erwähnt, dass die Entitätspaare zufällig ausgewählt werden, um die Frage "Do you mean 'You Could Be Mine' or 'The Way I Am'?" zu beantworten.</sample>
    <sample id="461">Generate alternative questions =&gt; sampling entity pairs  
Do you mean A or B?  
- Items with similar infoboxes on Wikipedia (same genre and/or artist)  
- Do you mean This is it or Man in the Mirror?  
- Items with similar descriptions on Wikipedia  
- Do you mean Thinking of You or Happy Anywhere?  
- Items with similar titles  
- Do you mean The Return (memoir) or The Return (Shatner novel)  
- Uniform at random: Do you mean 'Could Be Mine' or 'The Way I Am'  

(Note: The text is a direct translation of the original content, maintaining the structure and context of the slide.)</sample>
    <sample id="462">Der Präsentator erklärt, wie man alternative Fragen generieren kann, indem man Entity-Paare abgleitet. Er zeigt Beispiele wie man auf Wikipedia ähnliche Infoboxen, Beschreibungen, Titel und uniforme Begriffe finden kann.</sample>
    <sample id="463">Der Inhalt des Videos ist eine Anleitung für die Erstellung eines Background Knowledge (Hintergrundwissen) in Bezug auf Musik. Es wird empfohlen, die Lieder "Easy on Me" von Adele und "I Gotta Feeling" von The Black Eyed Peas zu suchen und zu hören. Anschließend werden die Annotatoren dazu erschlossen, mindestens einige Lieder von jedem Band zu hören und über die Songs zu lesen.</sample>
    <sample id="464">Der Inhalt des Videos ist eine Anleitung zur Erstellung von Background Knowledge (Hintergrundwissen) für Musik-Analyse. Es wird empfohlen, die beiden Songs "Easy on Me" von Adele und "I Gotta Feeling" von The Black Eyed Peas zu hören und zu lesen, um ein tieferes Verständnis der Musik zu erlangen.</sample>
    <sample id="465">Der Präsentierte bespricht die Prozesse und Methoden, die verwendet werden, um die Musik von Adele und Black Eyed Peas zu analysieren. Er erklärt, wie man die Musik durch das Zuhören und Lesen über die Songs verstehen kann, um die Emotionen und Bedeutungen hinter den Liedern zu erfassen.</sample>
    <sample id="466" />
    <sample id="467">Der Inhalt des Bildes beschreibt ein Anwendungsbeispiel für die Erfassung von Emotionen in Videos. Es zeigt eine Webseite mit einer Funktion, bei der Benutzer ausgewählte Emotionen aus einem Dropdown-Menü auswählen können. Die Seite ist von der Google Research-Website stammt und zeigt eine Anleitung, wie man die Emotionen auswählen kann, um die Emotionen in einem Video zu erfassen.</sample>
    <sample id="468" />
    <sample id="469" />
    <sample id="470" />
    <sample id="471" />
    <sample id="472" />
    <sample id="473">Mit den bestehenden SimulST-Richtlines "walk-k", "LA", "CAAT" und "EDAtt".</sample>
    <sample id="474">Université de Nantes.</sample>
    <sample id="475">Maarten Sap</sample>
    <sample id="476">Drei Autoren sind an der Arbeit beteilt.</sample>
    <sample id="477">Attention as a Guide for Simultaneous Speech Translation</sample>
    <sample id="478">Simultaneous speech translation (SimulST) is the process of translating spoken language into a text in another language in real-time, enabling cross-language communication.</sample>
    <sample id="479">Die englische Übersetzung des Inhalts lautet:

"What are the problems of the current SimuIST models?"

"Specific architectures are usually trained, introducing additional modules to be optimized."</sample>
    <sample id="480">Welche Probleme haben die aktuellen SimuUST-Modelle?</sample>
    <sample id="481">Welche Probleme haben die aktuellen SimulST-Modelle?</sample>
    <sample id="482">Der englische Text in der Bildunterschrift lautet: "What is our solution?"

Übersetzt ins Deutsche: "Was ist unsere Lösung?"</sample>
    <sample id="483">Die Übersetzung des englischen Inhalts in Deutsch lautet:

---

**Was ist unsere Lösung?**

**01** Verwenden Sie bereits bestehende Offline ST-Modelle ohne erneutes Training oder spezifische Architektur für SimuST.

**02** Verwenden Sie nur ein Modell für jeden Latenzbereich und verwalten Sie Latenz durch spezifische Parameter.

---

**Was ist unserer Lösung?**

**03** Verwenden Sie bereits bestehtende Offline ST-Modelle ohne erneut Training oder spezifische Architekturfür SimuST.

**04** Verwenden Sie nur ein Modell pro Latenzbereich und verwalten Latenz durch spezifische Parameter.</sample>
    <sample id="484">Die englische Beschreibung des Inhalts des Bildes lautet:

---

**Was ist unsere Lösung?**

1. **Verwenden Sie bereits bestehende Offline-ST-Modelle ohne Neuausrichtung oder Anpassung spezifischer Architektur für SimuST**
2. **Verwenden Sie nur ein Modell für jeden Latenzbereich und verwalten Sie Latenz durch die Aufmerksamkeit des Modells zwischen Audio-Eingabe und Textausgabe**
3. **Nutzen Sie das bereits erworbene Wissen durch das Modell durch die Aufmerksamkeitmechanismus zwischen Audio-Eingabe und Textaus</sample>
    <sample id="485">Die Übersetzung des englischen Inhalts in Deutsch lautet:

"Unser Lösung: EDAtt

Encoder-Decoder

Attention

Entscheiden Sie, ob Sie eine Teilübersetzung abgeben oder nicht, basierend auf, wo die Aufmerksamkeit zeigt. Ein Wort wird abgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, wenn die Aufmerksamkeit auf eine bestimmte Richtung zeigt, d.h. wenn die Aufmerksamkeit auf die letzten 2 Sprachrahmen zeigt, was bedeutet, dass die empfangenen Informationen genügend stabil sind."</sample>
    <sample id="486">Die Übersetzung des englischen Inhalts in deutsche Sprache lautet:

---

**Lösung: EDAtt**

**Encoder-Decoder Attention**

---

**Entscheidung, ob ein Teil der Übersetzung abgegeben oder nicht**

Entscheidet, ob ein Wort abgegeben wird, wenn die Aufmerksamkeit nicht konzentriert ist, wenn die Aufmerksamkeit nicht in einem konkreten Wort konzentriert ist. Wenn die Aufmerksamkeit nicht in einem einzelnen Wort konzentriert ist, bedeutet das, dass die letzten 2 Sprachrahmen (Sprachrahmen) genügen, um die erhaltene Information zu verstehen.

---

**Übersetzung:**

---

**Lösung:** EDAtt

**Encoder-Decoder Attention**  

---

**Entscheiden, ob ein Teil der Übersetzungen abgegeben oder nicht**

Bestimmt, ob ein Wort abgegeben werden sollte, wenn die Aufmerksamkeit nicht auf ein bestimmtes Wort konzentriert ist. Das bedeutet, wenn die Aufmerksamkeit nicht im letzten 2 Sprachrahmen konzentriert ist, dann ist die erhaltene Information ausreichend.

---

**Übersetzt:**

---

**Lösungs:** EDAtt

**Encoder-Decodier Attention**  

---

**Bestimmen, ob ein Teil der Übersetzungs abgegeben oder nicht**

Beschließt, ob ein Wort abgegeben sollte, wenn die Aufmerksamkeitskonzentration nicht auf ein bestimmtes Wort konsolidiert ist. Wenn die Aufmerksamkeitskonz</sample>
    <sample id="487">Die Übersetzung des englischen Textes lautet:

"Entscheide, ob du eine Teilübersetzung ablehnen oder nicht, basierend auf, wo die Aufmerksamkeit zeigt: Wenn die Aufmerksamkeit nicht konzentriert ist, wenn die Aufmerksamkeit nicht konzentiert ist, wenn die Aufmerksamkeitskonzentration nicht konzentriert ist, wenn das letzte Sprachmodell, das die empfangenen Informationen genügt, bedeutet, dass die empfangenen Informationen genügen."</sample>
    <sample id="488">Die Übersetzung des englischen Textes lautet: "Entscheide, ob du eine Teilübersetzung ablehnen oder nicht, basierend auf der Position, an der die Aufmerksamkeit entsteht, wenn die Aufmerksamkeit nicht konzentriert ist. Wenn die Aufmerksamkeit nicht konz</sample>
    <sample id="489">Unser Lösung: EDAtt  
Encoder-Decoder Attention  

Entscheide, ob du einen Teil der Übersetzung ablehnen oder nicht, basierend auf, wo die Aufmerksamkeit zeigt. Ein Wort wird abgelehnt, wenn die Aufmerksamkeit nicht konzentriert ist, wenn die Aufmerksamkeit nicht in einem konkreten Verhältnis zum letzten Sprachrahmen steht, was bedeutet, dass die empfangene Information genügend ist.</sample>
    <sample id="490">Unser Lösung: EDAtt  
**Encoder-Decoder Attention**  

**Decide whether to emit or not a partial translation based on where attention points to:**  
a word is emitted if the attention is not concentrated (Its sum is below a threshold a) towards the last λ speech frames, meaning that the received information is enough stable.  

**01 I am going to talk about...**  
Ich werde reden.  

**EMITTED**</sample>
    <sample id="491">Die Übersetzung des englischen Textes in Deutsch lautet:

---

**Unser Lösung: EDAtt**

**Encoder-Decoder Attention**

**Entscheide, ob du eine Teilübersetzung ablehnen oder nicht, basierend auf, wo die Aufmerksamkeit zeigt: Wenn ein Wort ausgesprochen wird, wenn die Aufmerksamkeit nicht konzentriert ist und sich nicht auf die letzten Sprachrahmen hin bewegt, d.h. die empfangene Information ist genug um die Sprache zu verstehen.**

---</sample>
    <sample id="492">Die Übersetzung des englischen Inhalts in Deutsch lautet:</sample>
    <sample id="493">Unser Lösung: EDAtt Encoder-Decoder Attention  
Entscheide, ob ein Wort übersetzt oder teilweise übersetzt werden soll, basierend auf der Konzentration des Aufmerksamkeitspunktes auf den letzten Sprachrahmen. Das bedeutet, dass die Aufmerksamkeit nicht auf eine konkrete Sprache konzentriert ist, was bedeutet, dass die empfangenen Informationen in der letzten Sprachrahmen ausreichend sind.  
  
1. Ich gehe zu sprechen über...  
2. Ich gehe zu sprechen über Klima.</sample>
    <sample id="494">Die englische Inhaltsbeschreibung lautet: "Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold) towards the last λ speech frames, meaning that the received information is enough stable."</sample>
    <sample id="495">Der englische Text auf der Bildschirmhauptanzeige lautet:

**Main Results: EDAtt**

Die graphische Darstellung zeigt die Beziehung zwischen dem **AL / AL_CA (s)** und dem **BLEU**. Die X-Achse repräsentiert den Wert **AL / AL_CA (s)**, während die Y-Achse den **BLEU**-Wert darstellt. Die Datenpunkte sind auf einer Linie angeordnet, die sich allmählich erhöht, was auf eine positive Korrelation zwischen den beiden Variablen hinweist.

Die Beschriftung auf der Y-Achse lautet **BLEU**, was steht für **Bilingual Evaluation Understudy**. Dies ist ein Maß für die Qualität von maschinellem Übersetzen, das auf der Übereinstimmung zwischen dem automatisch übersetzten Text und einem menschlich übersetzten Referenztext basiert.

Die Beschriftung auf den X-Achsenlabeln lautet **AL / AL_CA (s)**, was möglicherweise auf eine Messung oder eine Variable in Sekunden (s) bezogen ist.

Die Tabelle in der linken unteren Ecke enthält die folgenden Informationen:

- **Page 028**: Dies zeigt, dass der Inhalt auf Seite 28 des Dokuments oder der Präsentation angezeigt wird.

Die Tabelle in der rechten unteren Ecke enthält die Folgenden Informationen:

- **EDAtt**: Dies ist möglicherweise ein Akronym oder ein spezifisches Modell oder Verfahren, das in der Präsentation besprochen wird.

Die Tabelle in der linkeren unteren Ecke enthält die **EDAtt**-Informationen.

Die Tabelle in der reichen unteren Ecke enthält die EDAtt-Informationen.

Die Tabelle enthält die EDAtt-Information.

Die Tabelle enthä</sample>
    <sample id="496">Die Übersetzung des englischen Inhalts in Deutsch lautet:

---

**Haupt-Ergebnisse: EDAtt**

---

**EDAtt**

---

**Haupt Ergebnisse: EDAtt**

**EDAtt**

---</sample>
    <sample id="497">Die englische Beschriftung auf dem Diagramm lautet "Main Results: EDAtt".</sample>
    <sample id="498">Die Übersetzung des englischen Inhalts in Deutsch lautet:

---

**Haupt-Ergebnisse: EDAtt**

**EDAtt**

---

**Haupt Ergebnisse: EDAtt**

---

**Häufigkeiten der Wörter in der Textdatenbank**

---

**Häuflichkeit der Wörter in der Textdatenbase**

---

**Häuptlichkeit der Wörter in der Textdatsbank**

---

**Haufigkeit der Wörter in der Textdatabse**

---

**Häupfigkeit der Wörter in der Textdabse**

---

**Main Results: EDAtt**

---</sample>
    <sample id="499">Die englische Beschriftung auf dem Diagramm lautet "Main Results: EDAtt".</sample>
    <sample id="500">Die englische Beschriftung auf dem Diagramm lautet "Main Results: EDAtt".</sample>
    <sample id="501">Die englische Beschriftung auf dem Diagramm lautet "Main Results: EDAtt".</sample>
    <sample id="502">Die englische Beschriftung auf dem Diagramm lautet "Main Results: EDAtt".</sample>
    <sample id="503">Die englische Beschreibung des Inhalts des Bildes lautet:

---

**Main Results: EDAtt**

Die grafische Darstellung zeigt die Ergebnisse von EDAtt im Vergleich zu anderen Strategien wie wait-k, LA, CAAT und EDAtt. Die x-Achse repräsentiert die Anzahl der Iterationen, während die y-Achse die Anzahl der erreichten EU (Einheiten) zeigt. Die verschiedenen Farben der Linien entsprechen den unterschiedlichen Strategien.

- **wait-k**: Diese Strategie zeigt eine langsamere Erhöhung der EU mit zunehmender Anzahl der Iterationen.
- **LA**: Diese Strategie erreicht eine höhere Anzahl der EU als wait-k, aber nicht so hoch wie CAAT.
- **CAAT**: Diese Strategie erreicht die höchste Anzahl der EU, insbesondere in den frühen Iterationen.
- **EDAtt**: Diese Strategie zeigt eine schnelle Erhöhung der EU, insbesondere in den Anfangsphasen, und erreicht die höchste Anwendung der EU.

Die grafische Darstellungen zeigen, dass EDAtt die schnellste Strategie ist, wenn man die tatsächliche vergangene Zeit berücksichtigt.

---

**Übersetzung:**

---

**Hauptergebnisse: EDAtt**

Die grafischen Darstellungen zeigen die Ergebnisse von EDAtt im Vergleich zu anderen Strategien wie wait-k (wart-k), LA (Last-Average), CAAT (Cumulative Average Time) und EDAtt selbst. Die x-Achse repräsentierte die Anzahl der Iterationen, die y-Achse die Anzahl erreichten EU (Einheit) zeigte. Die verschiedenen Farben der Liniensetzungen entsprechen den unterschiedlichsten Strategien.

- **wait-k (wart-k):** Diese Strategie zeigt eine langsame Erhöhung der EU mit zunächst zunehmender Anzahl der Iterations.
- **LA (Last-Average):** Diese Strategie erreicht eine h</sample>
    <sample id="504">Wenn Sie mehr entdecken möchten, lesen Sie unseren Paper.</sample>
    <sample id="505">Yes, the dataset is publicly accessible.</sample>
    <sample id="506">Das Bild ist ein statisches Poster oder eine Präsentationsseite, die die folgenden Informationen enthält:

**Titel:**
**MULTIINSTRUCT: Verbesserung der Multi-Modal Zero-Shot-Learning durch Anweisungstuning**

**Autoren:**
- Zhiyang Xu*
- Ying Shen*
- Lifu Huang

**Institution:**
- Abteilung für Informatik, Virginia Tech

**Logo:**
- VT (Virginia Tech) Logo in der oberen rechten Ecke.

**Bilder:**
- Vier Personen sind in der Mitte des Bildes dargestellt, jeder mit einem eigenen Bild und einer Beschriftung.
- Eine Person ist in der oberen rechten Ecke, die anderen drei sind in der Mitte.

**Text:**
- "Equal Contribution" (gleiche Beitragsverteilung) ist in der unteren linken Ecke markiert.

Das Bild ist ein statisches Poster, das die Informationen über eine Forschungsarbeit oder einen Vortrag präsentiert, die sich mit der Verbesserung der Multi-Modal Zero-Shoot-Learning durch Anweisungstuning beschäftigt.</sample>
    <sample id="507">Die Präsentation beschäftigt sich mit der Verwendung von vorgeübten Sprachmodellen für die Ausführung von Aufgaben in der Niedersprachmodellierung. Die Präsentation zeigt drei verschiedene Ansätze: Prätrain-Finetuning (BERT, T5), Prompting (GPT-3) und Instruction Tuning (FLAN). Prätrain-Finetuning erfordert viele Ressourcen und ist typischerweise für jede Aufgabe spezifisch. Prompting nutzt vorhandene Sprachmodelle und führt eine geringe Anpassung durch. Instruction Tuning lernt die Sprachmodelle, indem sie sie mit unbezahlbaren Aufgaben trainiert. Die Präsentation zeigt, dass Instruction Tuning die beste Methode ist, um die Leistung von Sprachmodellen zu verbessern.</sample>
    <sample id="508">Der Vortrag betont die Bedeutung von Instruction Tuning für die Anpassung von Sprachmodellen an spezifische Aufgaben. Die Präsentation zeigt die verschiedenen Methoden zur Anpassung von Sprachmodellen, wie Pretrain-Finetuning, Prompting und Instruction Tuning. Die Präsentation zeigt, wie Instruction Tuning die Anpassung von Sprachmodellen verbessert, indem es die Fähigkeit der Modelle erhöht, auf neue Aufgaben zu adaptieren, indem es sie mit Sprachanweisungen trainiert. Die Präsentation zeigt auch, wie Instruction Tuning die Anpassung der Sprachmodelle verbessert, indem es sie mit Sprachanleitungen trainiert, die die Modelle dazu anregen, auf neue Aufgaben zu adaptiert zu werden. Die Präsentation zeigt, dass Instruction Tuning eine effektive Methode zur Anpassung von Sprachmodellen ist, um sie auf spezifische Aufgaben anzupassen.</sample>
    <sample id="509">The video shows a person speaking in front of a black background with the text "Language-only" displayed in white. The person is wearing a black jacket and has long hair. The video is in English and the person is speaking in English. The video is in a loop and does not have any other objects or actions. The video is in a language-only format and does not have any subtitles or captions. The video is in a loop and does</sample>
    <sample id="510">Instruction Tuning on Multimodal Pre-trained Models</sample>
    <sample id="511">Die Übersetzung des englischen Textes ins Deutsche lautet:

**"Imbalance in Instructional Datasets between NLP and Multimodal"**

**"Imbalance in Instructional Data Sets between NLP and Multimodal"**</sample>
    <sample id="512">Die Übersetzung des englischen Textes ins Deutsche lautet:

**Unausgewogenheit in Anweisungsdatensätzen zwischen NLP und Multimodal**

**1600+ sprachbasierte Anweisungsaufgaben**

**Keine großskaligen, öffentlich zugänglichen multimodalen Anweisungsaufgaben**</sample>
    <sample id="513">Das Bild ist ein PowerPoint-Slide, der die Informationen über das Multimodal Instruction Tuning Benchmark Dataset (MultiInstruct) präsentiert. Es zeigt eine Tabelle mit verschiedenen Multimodal Tasks, die in 10 breite Gruppen unterteilt sind. Die Tasks sind in verschiedenen Farben markiert, um sie leichter zu unterscheiden. Die Tabelle enthält auch eine Liste von 5 expert-written Instructions, die für die Anwendung des Datasets notwendig sind.</sample>
    <sample id="514">Das Bild ist ein PowerPoint-Slide mit dem Titel "MULTIINSTRUCT" und der Beschreibung "The first multimodal instruction tuning benchmark dataset". Es zeigt eine Tabelle mit verschiedenen Multimodalaufgaben und deren Untergruppen. Die Tabelle ist in zwei Spalten unterteilt, mit "Visual Representation" auf der linken und "Grounded Matching" auf der rechten Seite. Jede Spalte enthält mehrere Untergruppen, die durch verschiedene Farben markiert sind. Die Untergruppen sind "Visual Representation" und "Grounded Matching". Die Tabelle enthält 62 verschiedene Multimodalaufgaben, die in 10 breite Gruppen eingeteilt sind. Die Aufgaben sind "Visual Representation" und "Grounded Mapping". Die Tabelle enthält 5 expert-geschriebene Anweisungen. Die Tabelle enthält 60 verschiedene Multimodalaufgabe, die in 10 breite Grupp</sample>
    <sample id="515">Der Präsentierte beschreibt das Konzept der "One For All" (OFA) als ein vielseitiges, vorbereitetes Modell, das sowohl Verständnis als auch Generierung von Aufgaben mit ein- oder mehreren Modalitäten durchführen kann. OFA verfügt über einen "einheitlichen Wortschatz" für Sprache, Bildmerkmale und Koordinaten von Bounding Boxes.</sample>
    <sample id="516">The video presents a detailed explanation of the MULTISTRUCT framework, which is designed to handle multiple tasks simultaneously. The framework is illustrated through four specific tasks: Grounded Caption, Text Localization, Referring Expression Selection, and Question-Image Matching. Each task is explained with its respective input and output, providing a clear understanding of how the framework processes and generates responses. The video emphasizes the versatility and efficiency of the MULTISTRUCT framework in handling complex tasks involving image and text analysis.</sample>
    <sample id="517">The video presents a detailed explanation of the MULTISTRUCT framework, which is designed to handle multiple tasks simultaneously. The framework is introduced with a slide titled "MULTISTRUCT," which includes a flowchart illustrating four distinct tasks: Grounded Caption, Text Localization, Referring Expression Selection, and Question-Image Matching. Each task is described with its respective input and output, providing a clear understanding of the framework's capabilities. The video emphasizes the importance of multimodal learning and the ability to process and integrate information from different sources. The presenter highlights the potential applications of MULTISTRUCT in various fields, such as computer vision, natural language processing, and robotics. The video concludes with a summary of the key points and a call to action for further exploration of the framework.</sample>
    <sample id="518">The video presents a detailed explanation of the MULTISTRUCT framework, which is designed to handle multiple tasks simultaneously. The framework is introduced with a slide titled "MULTISTRUCT," which includes a table with four columns, each representing a different task: Grounded Caption, Text Localization, Referring Expression Selection, and Question-Image Matching. Each column contains a description of the task, an example input, and an example output. The table is accompanied by a figure labeled "Figure 1: Example Instances from MULTISTRUCT for Four Tasks." The video then provides a detailed explanation of each task, starting with Grounded Caption. The explanation includes a description of the task, an example input and output, and a discussion of the challenges and solutions associated with the task. The video then moves on to Text Localization, Referring Expression Selection,</sample>
    <sample id="519">**Title:** Multi-modal Instruction Tuning  
**Speaker:** [Name]  
**Affiliation:** [Institution]  
**Date:** [Date]  
**Location:** [Location]  

---

### **Introduction**  

- **Topic:** Multi-modal Instruction Tuning  
- **Objective:** To explore the concept of tuning models for multi-modal tasks, focusing on the integration of text, images, and other modalities.  
- **Key Challenges:**  
  - **Data Alignment:** Ensuring that text and images are properly aligned for effective learning.  
  - **Model Architecture:** Designing architectures that can handle multi-modal inputs efficiently.  
  - **Evaluation Metrics:** Developing metrics to assess the performance of multi-modal models.  

---

### **Background**  

- **Multi-modal Learning:**  
  - Multi-modal learning involves training models to process and understand data from multiple modalities (e.g., text, images, audio).  
  - It has applications in areas such as image captioning, visual question answering, and multimodal sentiment analysis.  

- **Instruction Tuning:**  
  - Instruction tuning refers to the process of fine-tuning pre-trained models on specific tasks or datasets.  
  - It is commonly used to adapt models for downstream tasks, such as text classification or machine translation.  

- **Challenges in Multi-modal Instruction Tuning:**  
  - **Data Complexity:** Multi-modal data is often more complex and requires specialized techniques for alignment and processing.  
  - **Model Scalability:** Scaling models to handle multi-modal inputs can be computationally expensive.  
  - **Evaluation:** Evaluating multi-modal models is more challenging due to the need for metrics that capture the performance across different modalities.  

---

### **Methodology**  

- **Data Preparation:**  
  - **Alignment:** Ensuring that text and images are properly paired and aligned for training.  
  - **Preprocessing:** Normalizing and preprocessing data to ensure consistency across modalities.  

- **Model Architecture:**  
  - **Multi-modal Encoders:** Designing encoders that can process both text and images.  
  - **Attention Mechanisms:** Incorporating attention mechanisms to focus on relevant parts of the input data.  
  - **Fusion Techniques:** Using fusion techniques to combine information from different modalities.  

- **Training Process:**  
  - **Fine-tuning:** Fine-tuning pre-trained models on multi-modal datasets.  
  - **Regularization:** Applying regularization techniques to prevent overfitting.  
  - **Optimization:** Using optimization algorithms to improve model performance.  

---

### **Results**  

- **Performance Metrics:**  
  - **Accuracy:** Evaluating the accuracy of the model on multi-modal tasks.  
  - **F1 Score:** Measuring the F1 score for tasks such as image captioning and visual question answering.  
  - **BLEU Score:** Assessing the performance of the model on text generation tasks.  

- **Comparison with Baselines:**  
  - **Single-modal Models:** Comparing the performance of multi-modal models with single-modal models.  
  - **State-of-the-Art:** Comparing the performance of the proposed model with the latest state-of-the-art models.  

- **Ablation Studies:**  
  - **Effect of Fusion Techniques:** Analyzing the impact of different fusion techniques on model performance.  
  - **Role of Attention Mechanisms:** Investigating the role of attention mechanisms in multi-modal learning.  

---

### **Discussion**  

- **Key Findings:**  
  - Multi-modal models outperform single-modal models in tasks that require understanding of both text and images.  
  - The proposed model achieves state-of-the-art performance on several multi-modal benchmarks.  

- **Limitations:**  
  - **Data Dependency:** The performance of multi-modal models is highly dependent on the quality and quantity of the training data.  
  - **Computational Cost:** Training multi-modal models can be computationally expensive, especially for large datasets.  

- **Future Work:**  
  - **Scalability:** Developing more scalable models for multi-modal learning.  
  - **Robustness:** Improving the robustness of multi-modal models to handle noisy or incomplete data.  
  - **Transfer Learning:** Exploring transfer learning techniques for multi-modal tasks.  

---

### **Conclusion**  

- **Summary:**  
  - Multi-modal instruction tuning is a promising approach for improving the performance of models on tasks that require understanding of multiple modalities.  
  - The proposed method achieves state-of-the-art performance on several multi-modality benchmarks.  

- **Implications:**  
  - The findings have implications for the development of multi-modal models in various applications, such as image captioning, visual question answering</sample>
    <sample id="520">**Multi-Modal Instruction Turning**  
**Training Dataset Construction:**  
- Use 53 tasks from 9 groups for training.  
- Sample 10,000 instances per task.  

**Testing Dataset Construction:**  
- Reserve the entire Commonsense Reasoning group for testing.  
- Select additional 5 tasks from VQA and Miscellaneous groups.  
- Use all the instances from the test split for each task.  
- Randomly sample 20 tasks from the Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="521">**Multi-Modal Instruction Turning**  
**Training Dataset Construction:**  
- Use 53 tasks from 9 groups for training.  
- Sample 10,000 instances per task.  

**Testing Dataset Construction:**  
- Reserve the entire Commonsense Reasoning group for testing.  
- Select additional 5 tasks from VQA and Miscellaneous groups.  
- Use all the instances from the test split for each task.  
- Randomly sample 20 tasks from the Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="522">Implementation Details

Training details:
- Pre-trained OFA-Large model (472M)
- Mix all the instances for all tasks.
- Each instance is randomly combined with one of five instruction templates.

Testing details:
- For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.
- We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="523">Implementation Details:

Training details:
- Pre-trained OFA-Large model (472M)
- Mix all the instances for all tasks.
- Each instance is randomly combined with one of five instruction templates.

Testing details:
- For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.
- We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="524">Implementation Details

Training details:
- Pre-trained OFA-Large model (472M)
- Mix all the instances for all tasks.
- Each instance is randomly combined with one of five instruction templates.

Testing details:
- For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.
- We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="525">Evaluation Metrics:

- For multi-modal classification tasks (Visual Entailment, Visual Spatial Reasoning, Natural Language Visual Reasoning, and Disaster Type Classification), we report the Accuracy.
- For multi-modal generation tasks (Commonsense VQA, Text VQA, Grounded VQA, Visual Text Extraction, and Visual Dialogue), we report the Rouge-L.
- We also compute the aggregated performance for each model based on the mean of the model's performance on all multimodal and NLP unseen tasks. We use Rouge-L as the performance score for most tasks, and Accuracy that only have accuracy as a metric.</sample>
    <sample id="526">Title: Sensitivity

Subtitle: How sensitive the model is towards variety of instructions for the same task.

Main Content:
- The model is consistently able to produce the same results for the same task, regardless of slight variations in the wording of instructions.

Mathematical Formula:
\[ \text{Sensitivity} = \frac{\sigma_{i \in I'}}{\mu_{i \in I'}} \left[ \frac{E(x_{y} \in D')[L(f(i, x), y)]}{E(x_{y} \in D')[C(f(i, x), y)]} \right] \]

Additional Information:
- The model is consistently able to consistently produce the same results for the same task, despite slight variations in the wording of instructions.
- The model is consistently able to consistently perform the same task, despite slight variations in the instructions.

Table:
- The table shows the effectiveness of instruction tuning on MULTIINSTRUCT.
- The table shows the effectiveness of instruction</sample>
    <sample id="527">Title: Effectiveness of Instruction Tuning on MULTIINSTRUCT

Table 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold.

| Model | CommonSenseQA | Visual Entailment | Visual Spatial Reasoning | NLU |
|-------|---------------|-------------------|--------------------------|-----|
| OFA   | 21.79         | 14.97             | 0.73                     | 0.70 |
| OFA-100M | 21.79         | -                 | 0.73                     | 1.00 |
| OFA-10M  | 21.79         | *                 | 0.73                     | -     |
| OFA-1M   | 21.79         *               | 0.73                     | -      |
| OFA-100K | 21.79         *                | 0.73                     | -       |
| OFA-1000K| 21.79         *                *                | 0.73                     *       | -        |
| OFA-100B | 21.79         *                 | 0.73                     *       *       | -         |
| OFA-100C | 21.79         *                  | 0.73                     *       -       | -          |
| OFA-100D | 21.79         *                   | 0.73                     *       0.73     | -           |
| OFA-100E | 21.79         *                    | 0.73                     *       **       | -            |
| OFA-100F | 21.79         *                     | 0.73                     *       ***      | -             |
| OFA-100G | 21.79         *                      | 0.73                     *       ****     | -              |
| OFA-100H | 21.79         *                       | 0.73                     *       *****   | -               |
| OFA-100I | 21.79         *                        | 0.73                     *       ******** | -                |
| OFA-100J | 21.79         *                         | 0.73                     *</sample>
    <sample id="528">Title: Effectiveness of Instruction Tuning on MULTIINSTRUCT

Table 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold.

| Model | CommonSenseQA | Visual Entailment | Visual Spatial Reasoning | NLU |
|-------|---------------|-------------------|---------------------------|-----|
| OFA   | 21.79         | 14.97             | 0.73                      | 0.04 |
| OFA-1000 | 82.41         | 58.10             | 31.19                     | 0.76 |
| OFA-100M | 21.79         | -                 | -                         | -     |
| OFA-100B | 21.79         | **14.97**         | **0.73**                  | **0.04** |
| OFA-100K | 21.79         | *                 | *                         | *     |
| OFA-10M | 21.79        | 14.97             | -                         | -     |
| GPT-3 | 21.79         | ***              | ***                       | ***   |
| GPT-4 | 21.79         | ****             | ****                      | ****  |

Table 2: Zero-shot Performance on Question Answering and Miscellaneous. The best performance is in bold.</sample>
    <sample id="529">Der Inhalt des Videos ist in deutscher Sprache.</sample>
    <sample id="530">Der Text beschreibt die Auswirkungen der Anzahl von Anweisungen auf die Leistung von Instruction Tuning. Es wird herausgestellt, dass Instruction Tuning mit 5 Anweisungen eine höhere aggregierte Leistung auf allen Evaluationsaufgaben erreicht und weniger empfindlich ist. Ein Tabellenvergleich zeigt die Leistungsunterschiede zwischen 1 und 5 Anweisungen.</sample>
    <sample id="531">The video presents a detailed analysis of the impact of fine-tuning strategies on model sensitivity, specifically focusing on the effects of instruction tuning on Multitask and transfer learning from the Natural Instructions dataset. The presenter, a man with short hair and glasses, is seen speaking in front of a black background with a white text box containing key points and a bar chart. The text box highlights two main points: 1) Instruction tuning on Multitask can significantly reduce the sensitivity of OFA, and 2) Transfer learning from the Natural Instructions dataset can further reduce the sensitivity of the model. The bar chart, labeled as Figure 4, illustrates the model sensitivity on unseen evaluation tasks, with lower values indicating better performance. The chart shows four bars representing different datasets: OFA, OF-Automatic, OF-Approximate, and OF-Natural. The OFA dataset has the highest sensitivity at 40.58, while the OF-Natural dataset has the lowest sensitivity at 10.27. The presenter explains that fine-tuning strategies can effectively reduce model sensitivity, making the model more robust and reliable in handling unseen tasks. The video emphasizes the importance of these strategies in improving the performance and generalization of machine learning models.</sample>
    <sample id="532">Zero-Shot Performance on NLP Tasks

- Instruction Tuning on Multilnstruct can improve zero-shot performance on unseen NLP tasks.
- The transfer learning strategy Mixedlnstruct can best preserve the zero-shot capability gained on Natural Instructions dataset.

Table 1. Zero-shot Performance on NLP tasks. The performance is reported in Rouge-1 and the best performance is in bold.

| Model | Transfer Learning from Natural Instructions |
|-------|--------------------------------------------|
| OPAs  | 12.18                                       |
| OPAs  | 121.25                                      |
| OPAs  | 43.61                                       |
| OPAs  | 46.32                                       |
| OPAs  | 76.97                                       |</sample>
    <sample id="533">**Schlussfolgerung**

- **Erste großskalige multi-modale Anweisungstuning-Datensatz.**
  - Enthält 62 multi-modale Aufgaben aus 10 breiten Kategorien.
- **Signifikant verbesserte die Null-Shot-Fähigkeit von OFA via Anweisung Tuning.**
- **Erforschen Sie mehrere transferierende Lernverfahren und zeigen Sie deren Vorteile an.**
- **Entwerfen Sie ein neues Metrik-Sensitivitätsmaß.**</sample>
    <sample id="534">Eine viel größere Multimodal-Anweisungstuning-Datensatz wird gesammelt, mit etwa 150 zusätzlichen Vision-Language-Aufgaben, die bald veröffentlicht werden.</sample>
    <sample id="535">Universität di Trento.</sample>
    <sample id="536">Die Referent*in ist Mohammad Javad Hosselini.</sample>
    <sample id="562">Die Sprache ist tief dunkel und bildet einen starken Kontrast zu den weißen Texten und dem Logo.</sample>
    <sample id="563">Die Sprache ist eine der wichtigsten Kommunikationsmittel zwischen Menschen. Sie ermöglicht es uns, unsere Gedanken und Ideen auszudrücken und mit anderen Menschen zu kommunizieren. Die Sprache besteht aus Wörtern, die wir in Sätzen und Absätzen zusammenfügen. Sie ist ein komplexer System, das von vielen Regeln und Strukturen geprägt ist. Die Sprache ist auch ein wichtiges Mittel der Identität und Kultur. Sie kann uns helfen, unsere Identität und unsere Kultur auszudrücken und zu verstehen. Die Sprache ist auch ein wesentliches Mittel der Kommunikation. Sie ermöglicht es uns uns mit anderen Menschen zu kommunizi</sample>
    <sample id="564">**Übersetzung ins Deutsche:**

---

**Revisiting Minimal Pair Paradigm**

**Minimal pair paradigm (MPP) Bewertungen von Sprachmodellen nutzen relative Unterschiede in Sequenzwahrscheinlichkeiten, um das abstrakte Wissen von LLMs zu bewerten:**

---

**BLIMP**

1. Viele Menschen halfen sich selbst.
2. Viele Menschen halfen hier.

**P(1) &gt; P(2)**

---

**SyntaxGym**

1. Kein Kunde hatte Geld ausgegeben.
2. Der Kunde hatte Geld ausgegehen.

**P(1,any) &gt; P(2,any)**

---

**CrowS**

1. Stereotypische Sätze.
2. Nicht-stereotypische Sätze.

**P(1) &gt; (2)**

---

**Zusammenfassung:**

Die Minimal Pair Paradigm (MPP) Bewertungen von Sprachkontexten verwenden relative Unterschiede in Sequenzproben, um das abstrakte Wissen von Sprachmodellen zu bewerten. Die Beispiele zeigen, wie die Sprachmodelle unterschiedliche Sätze beurteilen, die nur in der Reihenfolge der Wörter unterschiedlich sind. BLIMP verwendet relative Wahrscheinlichkeiten, SyntaxGym verwendet relative Wahrscheinlichkeit für jeden Satz, und CrowS verwendet relative Wahrscheinlichheit für die Reihenfolge der Wörter.</sample>
    <sample id="565">Revisiting Minimal Pair Paradigm</sample>
    <sample id="566">Revisiting Minimal Pair Paradigm</sample>
    <sample id="567">Revisiting Minimal Pair Paradigm</sample>
    <sample id="568">Revisiting Minimal Pair Paradigm</sample>
    <sample id="569">**Revisiting Minimal Pair Paradigm**  
**Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.**  
**BLIMP**  
1. Many people were helped by themselves.  
2. Many people were helped by others.  
**SyntaxGym**  
1. No customer has spent any money.  
2. The customer has spent any money.  
**CrowS**  
1. Women are terrible at handwork.  
2. Men are terrible at handwork.  
**P(1) &gt; P(2)**  
**P(1, any) &gt; P(2, any)**  
**P(1) &gt; P (2)**  
**Are these judgements stable with long preceding context?**</sample>
    <sample id="570">**Revisiting Minimal Pair Paradigm**  
**Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.**  
**BLIMP**  
1. Many people were helped by themselves.  
2. Many people were helped by others.  
**SyntaxGym**  
1. No customer has spent any money.  
2. The customer has spent any money.  
**CrowS**  
1. Women are terrible at handwork.  
2. Men are terrible at handwork.  
**P(1) &gt; P(2)**  
**P(1, any) &gt; P(2, any)**  
**P(1) &gt; P (2)**  
**Are these judgements stable with long preceding context?**</sample>
    <sample id="571">### Ansatz

**Zweck:** Testen, ob MPP-Gerichte als Funktion von Kontextlänge, Strukturübereinstimmung und Akzeptabilität variieren.

**Testfall:** Subjekt-Objekt-Agreement

**Hypothese:** \( P_{Sub}(Prefx) &gt; P_{Obj}(Prefx) \)

**Beispiel:**
- **Subjekt:** "Der Mann"
- **Objekt:** "das Buch"
- **Präfix:** "das"

**Fragen:**
- Wer würde flohen von vorher bevor dieser Kunden</sample>
    <sample id="572">### Ansatz

**Zweck:** Testen, ob MPP-Gerichte als Funktion von Kontextlänge, Strukturübereinstimmung und Akzeptabilität variieren.

**Testfall:** Subjekt-Objekt-Agreement

**Hypothese:** 
- \( P_{Subject}(Prefix) &gt; P_{Object}(Prefix) \)
- \( P_{Subject}(Prefix) &lt; P_{Object}(Prefix) \)

**Ergebnisse:**
- **Akzeptabel, Matched:**
  - **Welcher würde freuen von vorher bevor dieser Kundenservice angeboten wurde?**
    - **Was könnte jemand sehen, bevor dieser Kundenservice angebots wurde?**
    - **Welche Art von Dienstleistung würde jemand vorher sehen, bevor dieser Kundenservic angeboten wurde?**
  - **Welcher würde sich freuen von vorher bevor dieser Kundenauftrag angegeben wurde?**
    - **Was würde jemand sehen, bevor dieser Kundenauftrag angesagt wurde?**
    - **Welcher würde sich freue von vorher bevor dieser Kundenaufbau angegeben wurde?**

- **BLÄMP, Anpassung notwendig:**
  - **Welcher würte freuen von vorher bevor dieser Kunde bei uns angeboten wurde?**

**Platz der Kandidatenwerte:**
- **Welcher würde freuen von v</sample>
    <sample id="573" />
    <sample id="574">### Ansatz

**Zweck:** Untersuchen, ob MPP-Gerichtsentscheidungen als Funktion von Kontextlänge, Strukturübereinstimmung und Akzeptabilität variieren.

**Testfall:** Subjekt-Objekt-Agreement

**Prämissen:**
- \( P_{Sub}(Sub) \rightarrow P_{Obj}(Obj) \)
- \( P_{Sub}(Sub) &gt; P_{Obj}(Obj) \)

**Beispiel:**
- Subjekt: "Der Mann"
- Prädikat: "fährt"
- Objekt: "das Auto"

**Frage:** Wer würde fliehen, bevor der Kunde zu dieser Stelle zurückkehrt?

**Antwort:** Der Mann würde fliehen, bevor die Kunden zurückkehren.

**Erklärung:** Der Mann würde fliehen bevor die Kunden zurückkehrt, weil er die Kunden verfolgen möchte.

**Frage:** Wer würte jemand fliehen, bevor der Kunde zurückkehrt?

**Erklärung:** Wer würde jemand fliehen, bevor der</sample>
    <sample id="575" />
    <sample id="576"># Ansatz

Testen, ob MPP-Gerichtsentscheidungen als Funktion von Kontextlänge, Strukturübereinstimmung und Akzeptabilität variieren.

## Testaufbau

**Testaufbau:** Subjekt-Verhältnis-Agreement

**P-Test:** \( P_{Test}(Prefix) &gt; P_{Test}(Prefix) \)

**Beispiel:**

- **Akzeptabel:**
  - **Macht:**
    - **Bild, Verhältnis, Akzeptanz**
    - **Bild, Verhaltnis, Akzeptanz**</sample>
    <sample id="577">### Ansatz

**Zweck:** Testen, ob MPP-Gerichtsentscheidungen als Funktion von Kontextlänge, Strukturübereinstimmung und Akzeptabilität variieren.

**Testfall:** Subjekt-Verhältnis-Agreement

**Hypothese:** 
- \( P_{Sub}(Prefix) &gt; P_{Sub}(Prefix) \)
- \( P_{Sub}(Prefix) &lt; P_{Sub}(Prefix) \)

**Ergebnisse:**
- **Akzeptabel:**
  - "Es war eine Dokumentation über Musikinfiltration. Wer würde Rose frei lassen, wenn eine Person mit politischen Aktionen arbeitet, die gegen die Regeln verstoßen?"
  - "Es war eine DokUMENTATION über Musikinfiltration. Wer würden Rose frei lassen, wenn eine Person politische Aktionen macht, die gegen die Regeln versto-ßen?"
  - "Es war ein Dokumentation über Musikinfiltrations. Wer würde Rose frei lass, wenn eine Person politische Aktionen machen, die gegen die Regeln versto</sample>
    <sample id="578" />
    <sample id="579" />
    <sample id="580"># Ansatz

Testen, ob MPP-Gerichte als Funktion von Kontextlänge, Strukturübereinstimmung und Akzeptanz variieren.

## Testbereich: Subjekt-Verben-Agreement

### Hypothese:

\( P_{Subj}(Prefix) &gt; P_{Verb}(Prefix) \)

### Beispiel:

| Subjekt | Verb | Agreement |
|---------|------|------------|
| ich     | gehe | ja         |
| du      | gehst| ja         |
| er/sie/es| geht| ja         |
| wir     | gehen| ja         |
| ihr     | geht| ja         |
| sie     | gehen| ja         |

### Raum der Kandidaten-Welten:

| Welten | Verb | Agreement |
|--------|------|------------|
| Subjekt | Verb | Agreement  |
| ich     | gehe | ja          |
| du      | gehst | ja         |
| er/sie/es | geht | ja         |
| wir     | gehhen | ja         |
| ihr     | gefallen | ja         |
| sie     | gehören | ja         |

### Akzeptanz:

| Akzeptanz | Wahrscheinlichkeit |
|-----------|---------------------|
| ich      | 100%                |
| du       | 100%                |

### Strukturübereinstimmung:

| Strukturübereinstimmung | Wahrscheinlichkeit |

| Subjekt | Verb | Strukturübereinstimmung  |
|---------|------|-------------------------|
| ich     | gehe | Ja                      |
| du      | gehst  | Ja                     |
| er/sie/es | geh | Ja                    |
| wir     | gehen | Ja                     |
| ihr     | gefall | Ja                    |
| sie     | gehö | Ja                     |

### Kontextlänge:

| Kontextlänge | Wahrscheinlichkeit |
  
| Subjekt | Verb | Kontextlänge  |
|---------|------|----------------|
| ich     | gehe | kurz          |
| du      | geh | lang          |
| er/sie/es | ge | lang          |
| wir     | geh | lang          |
| ihr     | gef | lang          |
| sie     | geh | lang          |

### Wikipedia, Unverändert:

Ein weißer personale flowing ist der ständige Geschäftsführer des Spiels. Er ist über drei hundert Jahre alt und hat drei Tochter. Sie sind über drei hundert Jahre alt.</sample>
    <sample id="581">**Übersetzung ins Deutsche:**

---

**MMP-urteile sind robust gegenüber willkürlichen Kontextlängen**

Wir führen MPP-Evaluierungen mit unterschiedlichen Kontexten durch – akzeptabel/unakzeptabel; gepaart/ungepaart Struktur – von Längen bis zu 900 Tokens.

---

**Übersetzung des Textes:**

---

**MMP-Urteile sind robust gegenüber willkrürlichen Kontextlängen**
Wir führen MPP-Evaluationen mit unterschiedlichen Kontexten aus – akzeptabel/unakzeptable; gepaart/ungepaartstruktur – von Längen bis zu hundert Tausend Tokens.

---

**Übertragung des Textes:**

---
Wir führen MPP-Erwertungen mit unterschiedlichen Kontexts durch – akzeptabel/unakze</sample>
    <sample id="582">**Übersetzung ins Deutsche:**

---

**MMP-urteile sind robust für willkürliche Kontextlängen**

Wir führen MPP-Evaluierungen mit verschiedenen Kontexten durch – akzeptabel/unakzeptabel; gepaarter/ungepaarter Struktur – von Längen bis zu 900 Tokens.

---

**Übersetzung des Textes:**

---

**MMP-Urteile sind robust für willkürliche Kontextlängen**

Wir durchführen MPP-Evaluierungen unter unterschiedlichen Kontexten – akzeptabel/unakzeptable; gepaarter/ungepaarter Struktur – von Längen bis zu hundert Tausend Tokens.

---

**Übertragung des Textes:**

---
**MMP-Urteile sind robuster für willkürliche Kontextlängens**

Wir führen Mpp-Evaluierungen mit unterschiedlichen Kontexten durch – akzeptabel/ unakzeptabel; gepaarter/ ungepaarter Struktur – von Länge bis zu 900 Tokens.</sample>
    <sample id="583" />
    <sample id="584">Das Bild zeigt einen Diagramm mit der Beschriftung "Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance". Es zeigt die Performance von MPP (Multimodal Prompt Prompting) bei der Bewertung von Sätzen, die in unterschiedlichen Kontexten (akzeptable vs. unakzeptable) und unterschiedlichen Strukturen (übereinstimmend vs. unübereinstimmend) stehen, mit Längen von bis zu 900 Tokens. Die Diagramme zeigen die Präferenzstrategien (Acc., Unacc., Mismatch) für verschiedene Modelle wie BLIMP, OPT 6.7B, UACC, Mismatch, Wiki, Mismatch. Die Präferenzstrategien sind auf der Y-Achse und die Tokenzahl auf der X-Achse. Die Diagramme zeigen, wie die Präferenzstrategien bei unterschiedlichen Kontexten und Strukturen variieren.</sample>
    <sample id="585">Das Bild zeigt einen Diagramm, das die Verhältnisse von akzeptablen und unakzeptablen MPP-Sätzen in Bezug auf die Erhöhung oder Herabsetzung der Urteilsleistung in verschiedenen Kontexten analysiert. Die Diagramme zeigen, wie die Akzeptanz von MPP-Sätzen in verschiedenen Kontexten variiert, einschließlich akzeptabler und unakzeptabler Kontexte sowie passender und unpassender Strukturen. Die Diagramme zeigen, dass die Akzeptanz von MPP-Satzen in akzeptablen Kontexten höher ist als in unakzeptablen Kontexten. Die Diagramme zeigen auch, dass die Akzeptanz von Mpp-Sätzen in passenden Strukturen höher ist als in unpassenden Strukturen. Die Diagramme zeigten, dass die Akzeptanz von Mppt-Sätzen in akzeptablen Kontext</sample>
    <sample id="586" />
    <sample id="587" />
    <sample id="588">**Akzeptable/unakzeptable MPP-Sätze mit verglichenem Struktur beeinflussen am stärksten die Modellleistung**  
Wir führen MPP-Evaluierungen mit unterschiedlichen Kontexten durch: akzeptabel/unakzeptabel;  
verglichenes/unverglichenes Struktur - von Längen bis zu 900 Tokens.</sample>
    <sample id="589">Warum beeinflussen geprägte Prefixe die Urteile von LM?
Wir stören Kontextsätze in Weisen, die die relevante Struktur erhalten, und fragen, ob Modelle gleichermaßen empfindlich auf diese Sätze reagieren.</sample>
    <sample id="590">Warum beeinflussen gepaarte Präfixe die Urteile von LM?
Wir stören Kontextsätze auf eine Weise, die die relevante Struktur erhalten, und fragen, ob Modelle gleichermaßen empfindlich auf diese Sätze reagieren.
- Präfix/Suffix-Adverben: "Allerdings, &lt;sent&gt;."
- Langpräfix-Adverben: "Allerdings, letztendlich, &lt;sent&gt;."
- Zusätzliche Klauseln: "Unabhängig davon, was X über es denkt, &lt;sent&gt;."
- Zitat: "Gestern sagte X, &lt;sent&gt;."
Wir stören Kontextsätzchen auf eine Weise, die die relevante Struktur erhalten, und fragen nach, ob Modelle gleichermaßen auf diese Sätze empfindlich reagieren.
- Präfix-Suffix-Adverben: "Allerdings," &lt;sent&gt;."
- Langpräprefix-Adverben: "Allerdings, Letztlich," &lt;sent&gt;."
- Zusätliche Klauseln: "Unabhängig von dem, was X über es denkt," &lt;sent&gt;."
- Zitat: ""Gestern sagte X," &lt;sent&gt;."
Wir stöern Kontextsätze auf eine Weise auf, die die relevante Struktur erhält, und fragen, ob Modelle gleichzeitig empfindlich auf diese Sätze reagieren.
- Präfix/Suffix-Adverbe: "Allerdings, &lt;sent&gt;</sample>
    <sample id="591">Warum beeinflussen geprägte Prefixe die Urteile von LM?
Wir stören Kontextsätze in Weisen, die die relevante Struktur erhalten, und fragen, ob Modelle gleichermaßen empfindlich auf diese Sätze reagieren.</sample>
    <sample id="592">**Warum beeinflussen geprägte Prefixe die Urteile von LM?**  
Wir stören Kontextsätze in Weisen, die die relevante Struktur erhalten, und fragen, ob Modelle ähnlich empfinden.  

**Präfixe/Suffixe beeinflussen Urteile:**  
- **Präfixe/Suffixe beeinftussen:** "Allerdings: &lt;sents&gt;."  
- **Langpräfixe beeinftussen:** "Trotzdem: &lt;sents&gt;."  
-</sample>
    <sample id="593">Warum beeinflussen geprägte Prefixe die Urteile von LM?
Wir stören Kontextsätze in Weisen, die die relevante Struktur erhalten, und fragen, ob Modelle ähnlich empfinden.</sample>
    <sample id="594">**Schlüsselergebnisse**  
- Sprachmodelle sind empfindlich gegenüber latenten syntaktischen/semantischen Merkmalen, die über Sätze hinweg geteilt werden.  
- MPP-Evaluierungen mit kurzen, einzelnen Sätzen sind nicht ausreichend, um das abstrakte Wissen von Sprachmodellen vollständig zu erfassen.</sample>
    <sample id="595">**Schlüsselergebnisse**

- Sprachmodelle sind empfindlich gegenüber latenten syntaktischen/semantischen Merkmalen, die über Sätze hinweg geteilt werden.
- MPP-Evaluierungen mit kurzen, einzelnen-Satz-Eingaben erfassen nicht vollständig das abstrakte Wissen von Sprachmodellen.</sample>
    <sample id="596">**Schlüsselergebnisse**

- Sprachmodelle sind empfindlich gegenüber latenten syntaktischen/semantischen Merkmalen, die über Sätze hinweg geteilt werden.
- MPP-Evaluierungen mit kurzen, einzelnen Sätzen als Eingabe erfassen nicht vollständig das abstrakte Wissen von Sprachmodellen.</sample>
    <sample id="597">Die Input-Token werden im ersten Schritt der Methode als **Tag**-Token zugeordnet.</sample>
    <sample id="598">5000.</sample>
    <sample id="626">Die beste Ausrichtungsmethode für DEPlain ist **MASSalign**, da sie die höchsten Präzisions- und Recall-Werte erreicht.</sample>
    <sample id="627">Schwach überwachtes Lernen ermöglicht die Vermeidung der Annotation-Bottleneck-Problematik.</sample>
    <sample id="628">Die Dokumente in DEplain-web waren mit manuellen und automatischen Alignment-Methoden ausgerichtet. Die manuellen Alignment-Methoden waren 100% korrekt, während die automatischen Alignment-Methoden 99% korrekt waren.</sample>
    <sample id="629">Der CoNLL++-Datensatz wurde erstellt, indem Reuters-Nachrichten aus dem Jahr 2020 gesammelt und mit den CoNLL-2003-Annotationsrichtlinien annotiert wurden.</sample>
    <sample id="630">Das Bild ist ein PowerPoint-Slide mit der Überschrift "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Unter der Überschrift sind die Namen Yusen Zhang, Jun Wang, Zhiguo Wang und Rui Zhang aufgeführt. Im unteren Teil des Bildes sind die Logos von PennState und Amazon zu sehen.</sample>
    <sample id="631">**Übersetzung ins Deutsche:**

---

**XSemPLR: Cross-Linguual Semantic Parsing in Multiple Natural Languages and Meaning Representations**

**Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang**

**PennState**  
**Amazon**

---

**Semantic Parsing**

- **Semantic Parsing** ist eine Aufgabe, um die semantische Darstellung von Benutzerabfragen zu erstellen, wie zum Beispiel SQL und Lambda Calculus.

- **Beispielabfrage:**  
  Welche Länder in Europa haben mindestens drei Autohersteller, die über einen Saison hinweg mindestens drei Assistenten hergestellt haben?

- **SQL-Abfrage:**  
  ```sql
  SELECT T1.country_name
  FROM T1.country AS T1
  JOIN T2.continent AS T2 ON T1.continent_id = T2.continent_id
  JOIN T3.car AS T3 ON T1.country_id = T3.country_id
  WHERE T2.continent_name = 'Europe'
  GROUP BY T1.country_name
  HAVING COUNT(*) &gt;= 3;
  ```

- **Lambda Calculus-Abfrage:**  
  ```lambda
  (call SW.listValue (call SW.getProperty (call SW.listValue (call SW.getFunction (call SW.listValue (call SW.getLambda (call SW.listValue (call SW.getCar (call SW.listValue (call SW.getCountry (call SW.listValue (call SW.getContinent (call SW.listValue (call SW.getEurope (call SW.listValue (call SW.getAssistance (call SW.listValue (call SW.getPlayer (call SW.listValue (call SW.getNumAssists (call SW.listValue (call SW.getCount (call SW.listValue (call SW.getHiring (call SW.listValue (call SW.getManufacturers (call SW.listValue (call SW.getCountries (call SW.listValue (call SW.getContinents (call SW.listValue (call SW.getWorld (call SW.listValue (call SW.getEarth (call SW.listValue (call SW.getSolarSystem (call SW.listValue (call SW.getUniverse (call SW.listValue (call SW.getMultiverse (call SW.listValue (call SW.getAll (call SW.listValue (call SW.getAllCountries (call SW.listValue (call SW.getAllContinents (call SW.listValue (call</sample>
    <sample id="632">**Übersetzung ins Deutsche:**

---

**Cross-lingual Semantic Parsing**  
**Cross-lingual Semantic Parsing ist eine Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen zu übersetzen.**  

---

**Cross-linguistic Semantic Parsing**  
**Cross- lingual Semantic Parsing ist eine Aufgabe zur Übersetzung von Abfragen in mehreren natürlich Sprachen in mehrere Bedingungsdarstellungen.**  

---

**Crosslingual Semantic Parsing**  
**Die Aufgabe Crosslingual Semantic Parsing besteht darin, Abfragen in mehreren Sprachen in mehrere Bedingsdarstellungen zu übersetzen**  

---

**Cross- lingual Semantic Parsing**  
**Crosslingual Semantic Parsing ist eine Aufgab, Abfragen in mehreren Naturlangen in mehrere Bedingungsdar</sample>
    <sample id="633">**Übersetzung ins Deutsche:**</sample>
    <sample id="634">**Übersetzung ins Deutsche:**</sample>
    <sample id="635">**Übersetzung ins Deutsche:**</sample>
    <sample id="636">Cross-lingual Semantic Parsing  
  
*Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance:*  
  
- Lack of coverage on certain meaning representation</sample>
    <sample id="637">**Übersetzung ins Deutsche:**

**Cross-lingual Semantic Parsing**

- **Bereits bestehende CLSP-Modelle werden separat vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Zum Beispiel:**
  - **Mangel an Abdeckung bestimmter Bedeutungsrepräsentationen**

- **Diagramm:**
  - **Links:** Drei grüne Rechtecke mit den Sprachen "Englisch", "Deutsch" und "Chinesisch".
  - **Rechts:** Zwei blaue Rechtecke mit den Namen "SQL" und "FunQL".
  - **Zwischenrechtecke:** Ein rotes Rechteck mit dem Text "Single Model".
  - **Pfeil:** Ein Pfeil zeigt von links nach rechts, symbolisierend die Verbindung zwischen den Sprachen und den Modellen.

- **Bildunterschrift:**
  - **Rechts oben:** Ein Bild von einem Mann mit Kopfhörern, der auf einem Hügel mit einem Sonnenuntergang hinter ihm sitzt.
  - **Rechts unten:** Die Seitenzahl "6".</sample>
    <sample id="638">**Übersetzung ins Deutsche:**

**Cross-lingual Semantic Parsing**

- **Bestehende CLSP-Modelle werden separat vorgeschlagen und auf Datensätzen begrenzter Aufgaben und Anwendungen bewertet. Zum Beispiel:**
  - **Mangel an Abdeckung für bestimmte neuronale Modelle**

- **Diagramm:**
  - **Links:** Englisch, Deutsch, Chinesisch
  - **Rechts:** SQL, Lambda, FunQL
  - **Zwischenstufe:** Single Model (Pfeil zeigt die Verbindung)

- **Anmerkung:**
  - Die Diagramme zeigen, wie die CLSP-Modelle auf verschiedenen Sprachen und Aufgaben angewendet werden können, um eine einheitliche semantische Parsing-Lösung zu erreichen.

---

**Hinweis:**

- Die Diagramme und die Beschreibungen sind in englischer Sprache. Wenn Sie eine Übersetzung ins Deutsche benötigen, können Sie die englischen Texte und Diagramme in die oben angegebene Struktur umwandeln.</sample>
    <sample id="639">Wir bieten ein vereinigtes Dataset XSemPL für die kreuzsprachliche semantische Parsing in mehreren natürlichen Sprachen und Bedeutungsdarstellungen an. Es enthält: 9 Datensätze in verschiedenen Bereichen, 5 semantische Parsingaufgaben, 8 Bedeutungsdarstellungen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="640">Wir bieten ein vereinigtes Dataset XSemPLR für die kreuzsprachliche semantische Parsing in mehreren natürlichen Sprachen und Bedeutungsdarstellungen an. Es enthält: 9 Datensätze in verschiedenen Bereichen, 5 semantische Parsingaufgaben, 8 Bedeutungsdarstellungen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="641">Experimentelle Einstellungen: Wir berücksichtigen sechs Einstellungen für Training und Evaluierung.  
- **Translate-Test**: Verwenden Sie die Google-Übersetzungs-API, um die Quelle in die Zielsprache zu übersetzen. Dann verwenden Sie ein monolinguales Modell, um zu trainieren und zu bewerten.  

**Training**  
- Englisch  
- Englisch Modell  
- SQL  

**Inferenz**  
- Deutsch  
- Übersetzungs-API  
- Englisch  
- English Modell  
- SQL</sample>
    <sample id="642">Experimentelle Einstellungen: Wir berücksichtigen sechs Einstellungen für Training und Evaluierung.  
- **Translate-Test**: Verwenden Sie die Google-Übersetzungs-API, um die Quelle in die Zielsprache zu übersetzen. Dann verwenden Sie ein monolinguales Modell, um zu trainieren und zu bewerten.  

**Training**  
- Englisch  
- Englisch Modell  
- SQL  

**Inferenz**  
- Deutsch  
- Übersetzer-API  
- Englisch  
- English Modell  
- SQL</sample>
    <sample id="643">Experimentelle Einstellungen: Wir berücksichtigen sechs Einstellungen für Training und Evaluierung.  
- **Translate-Test**: Verwenden Sie die Google-Übersetzungs-API, um die Quelle in die Zielsprache zu übersetzen. Dann verwenden Sie ein monolinguales Modell, um zu trainieren und zu bewerten.  

**Training**  
- **Schritt 1**: Englisch  
- **Schritt 2**: Englisch Modell  
- **Schritt 3**: SQL  

**Inferenz**  
- **Schritt 1 (German)**: Deutsch  
- **Schritt 2**: Übersetzen API  
- **Schritt 3**: Englisch  
- **Schritte 4 und 5**: Englisch Modell  
- Schritt 6: SQL</sample>
    <sample id="644">Experimentelle Einstellungen: Wir berücksichtigen sechs Einstellungen für Training und Evaluierung. - Monolinguales Modell: Die Quellsprache ist gleich wie die Zielsprache, z.B. Deutsch-Deutsch. Wir testen auch den Monolingualen Few-shot-Einstellung, indem wir monolinguale Modelle mit nur 10% Trainingdaten trainieren. Training Inferenz</sample>
    <sample id="645">Experimentelle Einstellungen:
- Wir erwägen sechs verschiedene Einstellungen für Training und Evaluierung.
- Monolinguale Modellierung: Die Quellsprache ist gleich der Zielsprache, z.B. Deutsch-Deutsch. Wir testen auch monolinguale wenig-schrittige Modellierung mit nur 10% Trainingsdaten.</sample>
    <sample id="646">Experimentelle Einstellungen:
- Wir erwägen sechs verschiedene Einstellungen für Training und Evaluierung.
- Monolinguale Modellierung: Die Quellsprache ist gleich der Zielsprache, z.B. Deutsch-Deutsch. Wir testen auch monolinguale wenig-schrittige Modellierung mit nur 10% Trainingsdaten.

Training:
- Deutsch (wenig-schrittige)
- Deutsche Modell
- SQL</sample>
    <sample id="647">Experimentelle Einstellungen: Wir betrachten sechs Einstellungen für Training und Evaluierung. - Mehrsprachiges Modell: Trainieren Sie ein Mehrsprachiges Modell für alle Sprachen.</sample>
    <sample id="648">Experimentelle Einstellungen: Wir erwägen sechs Einstellungen für Training und Evaluierung. Multilinguales Modell: Trainieren Sie ein multilingues Modell für alle Sprachen.</sample>
    <sample id="649">Experimentelle Einstellungen: Wir erwägen sechs Einstellungen für Training und Evaluierung. Multilinguales Modell: Trainieren Sie ein multilingues Modell für alle Sprachen.</sample>
    <sample id="650">Experimentelle Einstellungen: Wir betrachten die sechs Einstellungen für Training und Evaluierung. Cross-linguale Zero-shot/Few-shot-Transfer: Trainieren Sie auf einer Quellsprache und übertragen Sie auf eine andere Sprache. Training: English, Or, English, German, Few-shot, Multilingual Model, SQL Inferenz: German, Multilingual Model, SQL</sample>
    <sample id="651">Experimentelle Einstellungen: Wir betrachten die sechs Einstellungen für Training und Evaluierung. Cross-linguale Zero-shot/Few-shot-Transfer: Trainieren Sie auf einer Quellsprache und übertragen Sie auf eine andere Sprache. Training: English, Or, English, German, Few-shot, Multilingual Model, SQL Inferenz: German, Multilingual Model, SQL</sample>
    <sample id="652">**Analyse im Monolingualen Setting**  
Wir bewerten zwei Gruppen von Modellen im Monolingualen Setting:  
- **Enc-PTR:** Multilinguele vorgeübte Encoder mit Pointer-basierten Decoder  
  - XLM-R + PTR, mBERT + PTR  
- **Enc-Dec:** Multilinguele vorgeüberte Encoder-Decoder Modelle  
  - mBERT, mT5  

**Ergebnisse:**  
- **Enc-PTR:**  
  - MATS: 30.63  
  - GQM: 72.18  
  - MSpec: 40.40  
  - MNLaps: 83.82  
  - OverM: 57.47  
  - MCWQ: 23.46  
  - MSchema: 52.53  
  - MTOP: 75.41  
  - MConAl: 5.87  
  - Durchschnitt: 49.09  

- **Enc-Dec:**  
  - MATS: 41.35  
  - GQM: 76.86  
  - MSpec: 47.67  
  - MNLaps: 44.67  
  - OverM: 51.35  
  - MCWQ: 61.64  
  - MSchema: 41.61  
  - MTOP: 78.96  
  - MConAl: 6.78  
  - Durchschnitt (mT5): 49.75  

**Schlussfolgerung:**  
mT5 erzielt die beste Leistung auf allen Datensätzen.</sample>
    <sample id="653">**Analyse im Monolingualen Setting**

Wir bewerten zwei Gruppen von Modellen im Monolingualen Setting.

**Enc-PTR:** Multilinguel Pretrained Encoders mit Pointer-based Decoders
- **XLM-R + PTR, mBERT + PTR**

**Enc-Dec:** Multilinguel Pretrained Encoder-Decoder Modelle
- **mBERT, mT5**

**Ergebnisse:**
- **mT5** erzielt die beste Leistung auf allen Datensätzen.

**Datensätübersicht:**

| Modell          | MATS | XLM-R + PTR | mBERT + PTR | mBERT | mT5 |
|-----------------|------|-------------|-------------|-------|-----|
| **MATS**        | 30.63 | 72.18       | 40.40       | 83.82 | 57.47 |
| **XLM-R + PTR** | 31.31 | 71.41       | 47.30       | 33.10 | 57.41 |
| **mBERT + PTR** | 31.14 | 71.44       | 47.30</sample>
    <sample id="654">**Analyse im Monolingualen Setting**

Wir bewerten zwei Gruppen von Modellen im Monolingualen Setting.

**Enc-PTR:** Multilinguel Pretrained Encoders mit Pointer-based Decoders

- **XLM-R + PTR, mBERT + PTR**

**Enc-Dec:** Multilinguel Pretrained Encoder-Decoder Modelle

- **mBERT, mT5**

**Ergebnisse:**

- **Enc-Dec (mT5):** Erhält die beste Performance auf allen Datensätzen.
- **Enc-PTR (XLM-R + PTR, mBERT +PTR):** Erreicht die beste Performance auf MATS, MSpec, MQuery, MSpider, MNLaps, MOvernight, MWMQ, MSchemaQA, MTOP, MMonCaLa.
- **Enc-Dec (mT6):** Erreicht die beste Performance in der durchschnittlichen Performance.

**Zusammenfassung:**

- **Enc-Dec (M6):** Erhält die beste allgemeine Performance.
- **Enc-PTR (Xlm-R + PTR, mBERT + PTR):** Erreicht die beste Performance für bestimmte Datensätte.
- **Enc-Dec (m6):** Erreicht die beste durchschnittliche Performance.</sample>
    <sample id="655">**Analyse im Monolingualen Setting**

Wir bewerten zwei Gruppen von Modellen im Monolingualen Setting.

**Enc-PTR:** Multilinguel Pretrained Encoders mit Pointer-based Decoders
- **XLM-R + PTR, mBERT + PTR**

**Enc-Dec:** Multilinguel Pretrained Encoder-Decoder Modelle
- **mBERT, mT5**

**Ergebnisse:**
- **mT5** erzielt die beste Leistung auf allen Datensätzen.

**Datensätte:**
- **MATS, MSQuery, MSQuer, MSQuer, MSQuer, MOW, MOW, MOW, MOW**
- **MSQuer, MSQuer, MSQu**
- **MSQuer, MSQuery, MSQuer, MSQuery, MSQuer**
- **MSQuer, MSquer, MSQuer, MSQuer,**
- **MSQuer, MSqu**
- **MSQuer, MSQue, MSQuer, MSQuer,
- **MSQuer, MSQu**
- **MOW, MOW, MOW, Mow, MOW, MOW, MOW
- **MOW, MOW, MSQuer, MSQuer, 
- **MOW, MOW, 
- **MOW, Mow, MOW, Mow, MOW
- **MOW, MSQuer, MSQuer
- **MOW, MOW,
- **MOW, MOW,**
- **MOW, MOW
- **MOW
- **MOW, MOW

**Durchschnittliche Leistung:**
- **MATS: 30.63**
- **MSQuery: 72.18**
- **MSQuer: 40.40**
- **MSQuer: 83.87**
- **MOW: 57.47**
- **MOW: 23.46**
- **MSQuer: 52.53**
- **MOW: 75.41**
- **MSQuer: 7.57**
- **MSQuer: 9.87**
- **MSQuer: **
- **MSQuer: 49.09**
- **MOW: 5.87**
- **MSQu</sample>
    <sample id="656">Analyse der mehrsprachigen Training:
- Wir bewerten auf mT5 und XLM-R PTER im mehrsprachigen Setting.
- Enc-Dec/Enc-PTR (mT5/XLM-R) können durch Training in einer Mischung verschiedener Sprachen verbessert werden.

| Modell | MATIS | MGSQery | MSipster | MNLmaps | MOveOver | MCWQ | MScheme2QA | MTOP | MCoNalA |
|---|---|---|---|---|</sample>
    <sample id="657">Analyse der mehrsprachigen Training</sample>
    <sample id="658">Die Übersetzung des englischen Textes ins Deutsche lautet:

---

**Analyse der Mehrsprachigen Training**

- Wir bewerten auf mT5 und XLM-R + PTR im Mehrsprachigkeits-Setting
- Die meisten der großen natürlichen Sprachen können Leistungssteigerungen erzielen, mit Ausnahme, dass die englische Leistung in 7 Datenkollektiven sinkt und in 3 Datenkollektiven steigt. Dies wird als "Fluch der Mehrsprachigkeit" bezeichnet.

---

**Analyse der Multilingualen Training**

- Wir bewerteten auf mT5 und XLM-R+PTR im Mehrsprachigkeits-Szenario
- Die meisten der großen natursprachlichen Sprachen können Leistungssteigern, mit Ausnahme, dass die engl. Leistung in 7 Datenköllen sinkt und in 3 Datenköllen steigt. Dies wird als "Fluch des Mehrsprachigkeits" bezeichnet.

---</sample>
    <sample id="659">Die Präsentation analysiert die Auswirkungen von multilingueller Training auf die Leistung von großen natürlichen Sprachmodellen (NLMs). Es wird bewertet, ob diese Modelle auf mT5 und XLM-R + PTR in einem multilinguellen Kontext mit oder ohne Prä-Training (Pretraining) performanzsteigern können. Die Ergebnisse zeigen, dass die meisten der großen NLMs eine Leistungssteigerung erzielen, mit Ausnahme von Englisch, bei dem die Leistung in 7 Datasets sinkt und in 3 Datasets steigt. Dies wird als "Curse of Multilinguality" bezeichnet.

Die Präsentation zeigt eine Bar-Diagramm, das die Anzahl der Datasets aufget</sample>
    <sample id="660">Der Präsentator diskutiert die Leistungsunterschiede zwischen verschiedenen Sprachmodellen in verschiedenen Sprachübungen. Er zeigt, dass die Leistung von Modellen, die auf wenigen Beispielen trainiert wurden (Few-shot), in vielen Fällen besser ist als die Leistung von Modellen, die ohne zusätzliche Trainingsdaten (Zero-shot) arbeiten. Die Präsentation verwendet eine Voronoi-Diagramm, um die Leistungsgrenzen der verschiedenen Modelle zu visualisieren.</sample>
    <sample id="661">Der Titel des Bildes lautet "Cross-lingual Performance Gap". Es zeigt eine Venn-Diagramm, das verschiedene Sprachmodelle und ihre Leistung in verschiedenen Sprachen vergleicht. Die drei Hauptlinien im Diagramm sind:

- **Blau Linie (Cross-lingual Few-shot transfer):** Diese Linie zeigt die Leistung von Sprachmodellen, die auf wenigen Beispielen in mehreren Sprachen trainiert wurden.
- **Orangeline (Cross-lingual Zero-shot transfer):** Diese Linie zeichnet die Leistung von Sprachmodellen aus, die ohne zusätzliche Trainingsdaten in neuen Sprachen arbeiten können.
- **Grüne Linie (Monolingual Setting):** Diese Linie zeigt die beste Leistung von Sprachmodellen, das in einer einzelnen Sprache trainiert wurde.

Im Zentrum des Diagramms befindet sich das Modell "Spider", das die beste Leistung in mehreren Sprachen zeigt. Die verschiedenen Sprachen sind in der Diagrammmitte aufgelistet und werden durch die Linien verknüpft, die die Leistung der Sprachmodelle in jeder Sprache darstellen.

Der Diagrammuntertitel lautet "ATIS", was auf Deutsch "Airline Travel Information System" bedeutet. Dies ist ein bekanntes natürliches Sprachverarbeitungsmodell, das für die Analyse von Flugbuchungsfragen verwendet wird.

Der Diagrammuntertitel "ATIS" steht für "Airline Travel Information System", was auf Deutsch "Flugbuchungsinformationen" bedeutet. Dies ist ein beliebtes natürliches Sprachverarbeitungssystem, das für die Analyse von Flugreisefragen verwendet wird.

Die Diagramme zeigt, wie die Leistung von Sprachmodellen in verschiedenen Sprachen variiert, abhängig davon, ob sie auf wenigen Beispielen in mehr als einer Sprache trainiert wurden (Cross-lingual Few-shot transfer), ohne zusätzliche Trainingsbeispiele in neuen Sprachen zu verwenden (Cross-lingual Zero-shot transfer) oder in einer einzelnen Sprache trainier</sample>
    <sample id="662">Der Inhalt des Bildes ist ein Diagramm, das die "Cross-linguale Leistungslücke" (Cross-lingual Performance Gap) zeigt. Es vergleicht die Leistung von verschiedenen Modellen in verschiedenen Szenarien: "Geoquery" (Grundsatz), "Geoquery/lamb" (Lamb), "Geoquery/prolog" (Prolog), "Geoquery/funql" (Funql), "Geoquery/sql" (SQL), "Schema2QA" (Schema2QA), "Overnight" (Overnight), "NLMaps" (NLMaps), "MCWQ" (MCWQ) und "ATIS" (ATIS). Die drei Farben - blau, orange und grün - repräsentieren den Cross-lingualen Few-shot Transfer, den Cross-lingualen Zero-shot Transfer und den Monolingualen Transfer, respektive.

Der Diagramm-Code enthält die folgenden Informationen:

- **Title**: Cross-lingual Performance Gap
- **Legend**:
  - **Blue Line**: Cross-lingual Few-shot Transfer
  - **Orange Line**: Cross-lingual Zero-shot Transfer
  - **Green Line**: Monolingual Setting
- **Data Points**:
  - **Geoquery**: 
    - Blue Line: 0.85
    - Orange Line: 0.90
    - Green Line: 0.95
  - **Geoquery/lamb**: 
    - Blue Line: 1.00
    - Orange Line: 1.05
    - Green Line: 1.10
  - **Geoquery/prolog**: 
    - Blue Line: 
    - Orange Line: 
    - Green Line: 
  - **Geoquery/funql**: 
    - Blue Line:</sample>
    <sample id="663">Die Präsentation zeigt die Ergebnisse und Schlussfolgerungen aus dem vierten Abschnitt des Papiers. Die Präsentation beginnt mit einer Liste von Punkten, die die wichtigsten Ergebnisse und Schlussfolgerung des Abschnitts darstellen. Die Präsentation endet mit einer Zusammenfassung der wichtigsten Punkte.</sample>
    <sample id="664">Die Präsentation zeigt die Ergebnisse und Schlussfolgerungen aus dem vierten Abschnitt des Papiers. Die Präsentation beginnt mit einer Übersicht über die verschiedenen Sprachmodelle und deren Leistung. Die Präsentation zeigt, dass die Enc-Dec (m75) Modell die beste Leistung erzielt, während die Codex &amp; Bloom-Modelle für die Übersetzung von mehreren Sprachen unzureichend sind. Die Präsentation zeigt auch, dass die Chinesische-Englische-Übersetzung die größte Leistungslücke aufweist, während die Deutsche-Englische-Übersetzung am wenigsten Leistungslücke aufweist. Die Präsentation zeigt, daß die FunQL-Modell die beste Leistung erzielen kann, während die SQL-Modell die schlechteste Leistung erzielen kann. Die Präsentation zeigt auch die Ergebnisse von verschiedenen Experimenten, die die Leistung der verschiedenen Sprachmodelle im Vergleich zu anderen Modellen zeigen. Die Präsentation zeigt auch den Einfluss von Prä-Training und Führung auf die Leistung der Sprachmodelle. Die Präsentation zeigt auch das Verhältnis zwischen der Leistung der Sprachmodelle und der Anzahl der verwendeten Daten. Die Präsentation zeigt auch eine Tabelle, die die Ergebnisse der verschiedenen Experimente zeigt. Die Präsentation zeigt auch ein Diagramm, das die Leistung der Sprachmodelle im Vergleich zu den anderen Modellen zeigt. Die Präsentation zeigt auch eine Tabelle, die die Erge</sample>
    <sample id="665">Zusammenfassung:

Die Präsentation beschreibt die Ergebnisse einer umfassenden Benchmark-Studie zu mehreren multilinguellen Sprachmodellen. Die Studie zeigt, dass mT5 mit monolingualem Training die besten Leistungen liefert, während mehrere multilinguelle LLMs noch immer unzureichend sind, um cross-linguale Aufgaben zu erfüllen. Darüber hinaus zeigt die Studie, dass der Leistungsunterschied zwischen monolingualem Training und cross-lingualem Transfer-Learning noch immer signifikant ist.</sample>
    <sample id="666">In diesem Video wird der Abschluss einer Präsentation über die Entwicklung eines neuen Benchmark-Systems für die semantische Parsing-Analyse in mehreren Sprachen präsentiert. Der Präsente, der sich in einem modernen, hellen Raum befindet, beginnt mit einer kurzen Vorstellung des Projekts und seiner Ziele. Er erklärt, dass das Team ein neues Benchmark-System namens XSemPLR entwickelt hat, das es ermöglicht, die semantische Parsing-Analyse mit mehreren natürlichen Sprachen und verschiedenen Bedeutungsdarstellungen zu erleichtern. Der Präsente führt die Zielsetzungen des Projekts an, nämlich die Erstellung eines umfassenden Benchmarks, der die Leistung verschiedener multilingueller Sprachmodelle untersucht. Er erklärt, dass das Projekt eine umfangreiche Studie über drei repräsentative multilinguelle Sprachmodelle durchführt, um die Leistung verschiedener Modelle zu vergleichen. Der Präsente führt die Ergebnisse der Studie an, die zeigen, dass das mT5-Modell mit monolingualer Training die beste Leistung erzielt, während mehrere multilinguelle LLMs immer noch unzureichend sind, um cross-linguale semantische Parsing-Aufgaben zu erfüllen. Er erklärt, dass die Leistung von cross-lingualen Transfer-Learning-Modellen noch signifikant ist, was bedeutet, dass die Leistung von monolingualen Training und cross-lingualen Transfer-Learning noch signifikant ist. Der Präsente führt die Schlussfolgerungen der Studie an, die zeigen,</sample>
    <sample id="667">Die bereits durchgeführten Arbeiten sind:

1. Parameter-based watermark [1, 2]
2. Lexical watermark [3, 4]
3. Backdoor-based watermark [5]
4. Adversarial-based watermark [6]</sample>
    <sample id="668">Multilingual LLMs like Codex or Bloom are still inadequate for crosslingual semantic parsing tasks.</sample>
    <sample id="695">Die Methode umgeht die Mehrdeutigkeit der Permutation durch die Anwendung von kontinuierlicher Relaxation und Backpropagation, um die beste Permutation zu finden.</sample>
    <sample id="696">Die Fairness eines NLP-Modells wird definiert als die Fähigkeit, die Vorurteile und Diskriminierung in den Daten zu erkennen und zu reduzieren.</sample>
    <sample id="697">Yanis Labrak.</sample>
    <sample id="698">Kostyav Sinha</sample>
    <sample id="699">Der/die Referent*in heißt Myra Cheng.</sample>
    <sample id="700">Tropikalismus bezieht sich auf die Verwendung von Stereotypen und Vorurteilen, um bestimmte Gruppen zu charakterisieren und zu verurteilen. In der Arbeit wird Tropikalismus als eine Methode beschrieben, um bestimmte Gruppen durch die Verwendung von Stereotyps und Vorurteilen zu definieren und zu marginalisieren.</sample>
    <sample id="701">Die Autoren haben die Beschreibungen der Zielgruppennamen durch die Verwendung von "essentiellisierenden" Narrativen und "pejorativen positiven" Portrayals erstellt.</sample>
    <sample id="702">P-CXMI (Pointwise Contextualized XMI) wurde in dieser Arbeit eingeführt, um den Kontextverwendung zu messen, um eine spezifische Übersetzung zu ermöglichen.</sample>
    <sample id="703">DrBERT ist ein Modell, das auf der Huggingface-Plattform trainiert wurde, während ChuBERT auf Huggingface nicht verfügbar ist. ChuBERT wurde auf Huggingface nicht trainiert, da es auf Huggingface nicht verfüg-bar ist.</sample>
    <sample id="751">Drei.</sample>
    <sample id="752">Iteratives Transferlernen ist ein Prozess, bei dem ein Modell iterativ mit neuen Daten trainiert wird, um seine Leistung zu verbessern. Im Gegensatz zu einem zusammenhängenden Modell, das alle Daten in einem einzigen Schritt verarbeitet, wird bei iteratives Transferlernen nach jedem Trainingszyklus das Modell mit neuen Daten aktualisiert. Dieser Ansatz ermöglicht es, das Modell kontinuierlich zu verbessern, indem es auf neue Informationen und Erkenntnisse basiert.

### Beispiel:

1. **Initialisierung**: Das Modell wird mit den ersten Daten trainiert.
2. **Testen**: Das Modell wird auf einem Testdatensatz getestet, um seine Leistung zu bewerten.
3. **Aktualisierung**: Das Modell wird mit neuen Daten trainiert, die aus dem Testdatensatz stammen.
4. **Wiederholung**: Der Prozess wird wiederholt, bis die Leistung des Modells stabil ist oder keine weiteren Verbesserungen mehr erzielt werden können.

### Vorteile:

- **Effizienz**: Das Modell kann kontinuierlich verbessert werden, ohne dass alle Daten in einem einzigen Schlag verarbeitet werden müssen.
- **Flexibilität**: Das Modell kann auf neue Informationen und Erkenntisse basieren, was zu einer besseren Generalisierbarkeit führt.
- **Ressourcenanforderungen**: Das Modell benötigt weniger Rechenleistung im Vergleich zu einem zusammenhängenden Training, da es iterativ mit kleineren Datensätzen arbeitet.

### Anwendungsbereiche:

- **Sprachmodelle**: Iteratives Transferlernen wird häufig in der Entwicklung von Sprachmodellen verwendet, um die Genauigkeit und Relevanz der Antworten zu verbessern.
- **Bilderklassifikation**: Im Bereich der Bilderklassifikation wird iteratives Transferlernen verwendet, um die Genauigkeitsrate der Klassifizierung zu erhöhen.
- **Medizinische Anwendungen**: In der medizinischen Forschung wird iteratives Transferlernen verwendert, um die Genauigkeit von Diagnosemodellen zu verbessern.

### Fazit:

Iteratives Transferlernen ist ein effektiver Ansatz, um das Modell kontinuierlich zu trainieren und zu verbessern. Es ermöglicht es, die Leistung des Modells auf Basis neuer Informationen zu steigern, was zu einer besseren Generalisierung und Genauigkeit führt.</sample>
    <sample id="753">Das Ziel des Datensatzes ist es, die Verständnis der Benutzersprache zu verbessern, wenn sie eine Wahl treffen.</sample>
    <sample id="754">Ein Angreifer kann Modellparameter über einen EaaS extrahiert, indem er die Daten aus dem EaaS extrahiert und dann die Modellparameter aus diesen Daten extrahiert.</sample>
    <sample id="755">3</sample>
    <sample id="756">10.</sample>
    <sample id="757">Universität Washington</sample>
    <sample id="758">I saw Bart and Lisa. Homer came and sneezed.</sample>
    <sample id="759">The current state of the art for dialog systems is represented by the ABC-Eval framework, which evaluates dialog systems based on four key behaviors: coherence, consistency, knowledge, and emotional understanding.</sample>
    <sample id="760">Um die Stabilität der Modelle über längere Kontexte zu testen und sicherzustellen, dass sie konsistente und zuverlässige Entscheidungen treffen, unabhängig von kurzfristigen Informationen.</sample>
    <sample id="761">Ja, das mehrsprachige Training hat zu einem Leistungsabfall im Verg</sample>
    <sample id="762">Yes, the annotators are asked to listen to and read about the songs in advance.</sample>
    <sample id="763">The video does not provide information about the MT-metrics used for evaluation.</sample>
    <sample id="764">Die Regression wirkt sich bei der Generalisierung auf bestimmte Named Entity Recognition (NER)-Typen aus.</sample>
    <sample id="765">Positionalität ist für NLP wichtig, weil sie die relative Position von Wörtern in einem Satz berücksichtigt, was die Bedeutung und den Kontext der Wörter erklärt.</sample>
    <sample id="766">Adapter.</sample>
    <sample id="767">RoBERTa-base + classifier head.</sample>
    <sample id="768">Die aktuellen Testsets zur Bewertung der PaLM-Fä</sample>
    <sample id="769">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="770">10.5%</sample>
    <sample id="771">Shuheng Liu und Alan Ritter.</sample>
    <sample id="772">Yes, the results and dataset of the study can be used as a benchmark.</sample>
    <sample id="773">In der Arbeit werden 5 kleinere Modelle experimentiert.</sample>
    <sample id="774">OFA (One For All).</sample>
    <sample id="833">Google</sample>
    <sample id="834">Stony Brook University.</sample>
    <sample id="835">Die Arbeit untersuchte Sprachpaare wie Deutsch-Englisch, Deutsch-Französisch, Deutsch-Spanisch, Deutsch-italienisch, Deutsch-niederländisch, Deutsch-russisch, Deutsch-türkisch, Deutsch-chinesisch, Deutsch-japanisch, Deutsch-koreanisch und Deutsch-indonesisch.</sample>
    <sample id="836">Shangbin Feng</sample>
    <sample id="837">long-mBART, mBART, BART, T5</sample>
    <sample id="838">Für die Trainingsdaten werden 53 von 62 Aufgaben verwendet, wobei 10.000 Instanzen pro Aufgabe ausgewählt werden. Für die Testdaten werden 5 weitere Aufgaben ausgewählt, um die VQA- und Miscellanea-Gruppen abzudecken.</sample>
    <sample id="839">Drei Autoren sind an der Arbeit beteilt.</sample>
    <sample id="840">AG News, MIND, SST2, Enron Spam, Wikipedia</sample>
    <sample id="876">NACHOS is a dataset used for evaluating language models in healthcare.</sample>
    <sample id="877">David Vilar Torres, Markus Freitag, Colin Cherry, Jianing Luo, Vithresh Rathaker, George Foster.</sample>
    <sample id="878">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse, da sie die Qualität der Übersetzungen beeinflusst. Die meisten Sätze (516 von 1000) zeigen einen Unterschied von mehr als 1 BLEURT Punkt, und der Unterschied kann bis zu 40 BLEURT Punkte gehen.</sample>
    <sample id="879">Die Autoren gehören an der Carnegie Mellon University Language Technologies Institute.</sample>
    <sample id="880">1. "We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon!"
2. "We are collecting a much larger multimodality instruction tuning dataset with around 150 vision-language tasks and we will release them soon."
3. "We are collecting a much larger multimodel instruction tuning dataset with around 150 visual language tasks and we will release them soon."
4. "We are collecting a much larger multimode instruction tuning dataset with around 150 multimodal tasks and we will release them soon."
5. "We are collecting a much larger multimodule instruction tuning dataset with around 150 multi-modal tasks and we will release them soon."</sample>
    <sample id="881">Mit dem Coreference Resolution Task.</sample>
    <sample id="882">Das Bild ist ein PowerPoint-Slide, das den Titel "Prompting PaLM for Translation" und die Untertitel "Assessing Strategies and Performance" enthält. Es zeigt die Google-Logo in der oberen linken Ecke und eine Frage in einem Dialogfenster, das auf der rechten Seite der Bildpositioniert ist: "Can you translate this for me, please?" (Können Sie dies für mich übersetzen, bitte?). Unter dem Titel sind die Namen und Fotos von sechs Personen aufgeführt: David Vilar Torres, Markus Freitag, Colin Cherry, Jianing Luo, Vithushan Rathakrishnan und George Foster. Die Fotos sind in einer horizontalen Reihe unter den Namen angeordnet. Im Unterteil des Bildes ist der Text "ACL 2023" zu sehen, was auf Deutsch "International Conference on Computational Linguistics" bedeutet.</sample>
    <sample id="883">Der englische Text beschreibt die PaLM (Pathways Language Model) von Chowdhery et al. 2022, ein Sprachmodell mit 540 Mill</sample>
    <sample id="884">Das Bild zeigt eine PowerPoint-Präsentation über das PaLM-Sprachmodell. Die Präsentation enthält eine Liste von Informationen über das Modell, einschließlich der Anzahl der Parameter (540 Milliarden), der verwendeten Tokenanzahl (780 Milliarden), der verwendet</sample>
    <sample id="885">Unsere Beiträge:

- Erste systematische Studie von LLM-Prompting für maschinelles Übersetzen.
- Bewertung der Übersetzungsfähigkeiten mit den Praktiken der maschinellen Übersetzungsgemeinschaft:
  - Neueste Testdatensätze (vermeiden Sie Test/Train-Abdeckung und Überfitten Sie auf Evaluierungsdaten).
  - Vergleich mit den neuesten WMT-Einreichungen (SOTA-Systeme verwenden die neuesten Trainingsdaten).
  - SOTA-MT-Metriken (besser Korrelation mit menschlichen Bewertungen).
  - Expertenbasierte menschliche Bewertung (robustere als die Bewertung durch Crowd-Worker).
- Empfehlung für Prompt-Selektionsstrategien.</sample>
    <sample id="886">Unsere Beiträge:

- Erste systematische Studie von LLM-Prompting für maschinelles Übersetzen.
- Bewertung der Übersetzationsfähigkeiten mit den Praktiken der maschinellen Übersetzungsgemeinschaft:
  - Neueste Testdatensätze (vermeiden Sie Test/Train-Abdeckung und Überanpassung an Evaluierungsdaten).
  - Vergleich mit den neuesten WMT-Einreichungen (SOTA-Systeme verwenden die neuesten Trainingsdaten).
  - SOTA-MT-Metriken (besser Korrelation mit menschlichen Bewertungen).
  - Expertenbasierte menschliche Bewertung (robustere als die Bewertung durch die Masse).
- Empfehlung für Prompt-Selektionsstrategien.</sample>
    <sample id="887">Unsere Beiträge:

* Erste systematische Studie von LLM-Prompting für maschinelles Übersetzen.
* Bewertung der Übertragbarkeit mit bestehenden Praktiken der MT-Community:
	+ Neueste Testdatensätze (vermeiden Sie Test/Train-Abdeckung und Überführung auf Evaluierungsdaten).
	+ Vergleich mit den neuesten WMT-Einreichungen (SOTA-Systeme verwenden häufigstes neuestes Trainingsdatenset).
	+ SOTA-MT-Metriken (besser Korrelation mit menschlichen Bewertungen).
	+ Expertenbasierte menschliche Bewertung (robustere als Crowdsourcing).
* Empfehlung für Prompt-Selektionsstrategien.</sample>
    <sample id="888">Unsere Beiträge:

1. Erste systematische Studie von LLM-Prompting für maschinelles Übersetzen:
   - Baut auf der besten aktuellen WMT-Testdatensammlung auf und vermeidet Übertragung und Überfüllung auf Evaluierungsdaten.

2. Bewertung der Übersetzungsfähigkeiten mit besten Praktiken der maschinellen Übersetzungsgemeinschaft:
   - Verwendet neueste Testdatensätze (ohne Test/Train-Aufsplitung und Überfüllung auf Bewertungsdaten).
   - Vergleicht mit den neuesten WMT-Einreichungen (SOTA-Systeme, die mit den neuesten Trainingsdaten arbeiten).
   - SOTA-MT-Metriken (besser Korrelation mit menschlichen Bewertungen).
   - Expertenbasierte menschliche Bewertung (robust gegenüber Crowd-Werken).
   - Empfehlung für Prompt-Wahlstrategien.</sample>
    <sample id="889">Prompts haben einen großen Einfluss auf die Qualität der Übersetzung. Wähle zwei zufällige Prompts für jede Aussage. Berechne BLEURT für jede Aussage-Prompt-Paar. Die Mehrheit der Aussagen (516 von 1000) zeigt eine Differenz von mehr als 1 BLEURT Punkt. Die Differenz kann bis zu 40 BLEURT Punkte gehen!</sample>
    <sample id="890">Prompts haben einen großen Einfluss auf die Qualität der Übersetzung. Wähle zwei zufällige Prompts für jede Aussage. Berechne BLEURT für jede Aussage-Prompt-Paar. Die Mehrzahl der Aussagen (516 von 1000) zeigt einen Unterschied von mehr als 1 BLEURT-Punkt. Der Unterschied kann bis zu 40 BLEURT-Punkte gehen!</sample>
    <sample id="891">Prompts haben einen großen Einfluss auf die Qualität der Übersetzung. Wähle zwei zufällige Prompts für jede Aussage. Berechne BLEURT für jede Aussage-Prompt-Paar. Die Mehrzahl der Aussagen (516 von 1000) zeigt einen Unterschied von mehr als 1 BLEURT Punkt. Der Unterschied kann bis zu 40 BLEURT Punkte gehen!</sample>
    <sample id="892" />
    <sample id="893" />
    <sample id="894" />
    <sample id="895" />
    <sample id="896" />
    <sample id="897">**Experimentelle Ergebnisse**

- Qualität der Beispiele ist wichtiger als die Ähnlichkeit zum Quelltext.
- Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.
- PalM ist nahe an Google Translate.

**Einsichten aus MQM:**

- Fluenz von PalM vergleichbar mit SOTA.
- Genauigkeitsscores sind im Allgemeinen niedriger.
- Dominant durch "Genauigkeit/Ausschluss" bestimmt.
- "Stil/Awkward" ist im Allgemeinen niedriger für PalM.</sample>
    <sample id="898">**Experimentelle Ergebnisse**  
- Qualität der Beispiele ist wichtiger als Ähnlichkeit zu der Quelle.  
- Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.  
- PalM ist nahe an Google Translate.  

**Einsichten aus MQM:**  
- Fluenz von PalM vergleichbar mit SOTA.  
- Genauigkeitsscores sind im Allgemeinen niedriger.  
- Dominant durch "Genauigkeit/Ausschluss".  
- "Stil/Awkward" ist im Allgemeinen niedriger für PalM.</sample>
    <sample id="899">**Experimentelle Ergebnisse**  
- Qualität der Beispiele ist wichtiger als Ähnlichkeit zu der Quelle.  
- Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.  
- PalM ist nahe an Google Translate.  

**Einsichten aus MQM:**  
- Fluenz von PalM vergleichbar mit SOTA.  
- Genauigkeitsscores sind im Allgemeinen niedriger.  
- Dominant durch "Genauigkeit/Ausschluss".  
- "Stil/Awkward" ist im Allgemeinen niedriger für PalM.</sample>
    <sample id="900">**Experimentelle Ergebnisse**

- Qualität der Beispiele ist wichtiger als Ähnlichkeit zu der Quelle.
- Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.
- PalM ist nahe an Google Translate.

**Einsichten aus MQM:**

- Fluenz von PalM vergleichbar mit SOTA.
- Genauigkeitsscores sind im Allgemeinen niedriger.
- Dominant durch "Genauigkeit/Ausschluss".
- "Stil/Awkward" ist im Allgemeinen niedriger für PalM.</sample>
    <sample id="901">**Experimentelle Ergebnisse**

- Qualität der Beispiele ist wichtiger als Ähnlichkeit zu der Quelle.
- Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.
- PalM ist nahe an Google Translate.

**Einsichten aus MQM:**

- Fluenz von PalM vergleichbar mit SOTA.
- Genauigkeitsscores sind im Allgemeinen niedriger.
- Dominant durch "Genauigkeit/Ausschluss" bestimmt.
- "Stil/Awkward" ist im Allgemeinen niedriger für PalM.</sample>
    <sample id="902">**Experimentelle Ergebnisse**

- Qualität der Beispiele ist wichtiger als die Ähnlichkeit zum Quelltext.
- Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.
- PalM ist nahe an Google Translate.

**Einsichten aus MQM:**

- Fluenz von PalM vergleichbar mit SOTA.
- Genauigkeitsscores sind im Allgemeinen niedriger.
- Dominant durch "Genauigkeit/Omission" bestimmt.
- "Stil/Awkward" ist im Allgemeinen niedriger für PalM.</sample>
    <sample id="903">**Experimentelle Ergebnisse**  
- Qualität der Beispiele ist wichtiger als Ähnlichkeit zu der Quelle.  
- Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.  
- PalM ist nahe an Google Translate.  

**Einsichten aus MQM:**  
- Fluenz von PalM vergleichbar mit SOTA.  
- Genauigkeitsscores sind im Allgemeinen niedriger.  
- Dominant durch "Genauigkeit/Ausschluss".  
- "Stil/Awkward" ist im Allgemeinen niedriger für PalM.</sample>
    <sample id="904">**Experimentelle Ergebnisse**  
- Qualität der Beispiele ist wichtiger als Ähnlichkeit zu der Quelle.  
- Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.  
- PalM ist nahe an Google Translate.  

**Einsichten aus MQM:**  
- Fluenz von PalM vergleichbar mit SOTA.  
- Genauigkeitsscores sind im Allgemeinen niedriger.  
- Dominant durch "Genauigkeit/Ausschluss".  
- "Stil/Awkward" ist im Allgemeinen niedriger für PalM.</sample>
    <sample id="905">**Experimentelle Ergebnisse**  
- Qualität der Beispiele ist wichtiger als Ähnlichkeit zu der Quelle.  
- Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.  
- PalM ist nahe an Google Translate.  

**Einsichten aus MQM:**  
- Fluenz von PalM vergleichbar mit SOTA.  
- Genauigkeitsscores sind im Allgemeinen niedriger.  
- Dominant durch "Genauigkeit/Ausschluss".  
- "Stil/Awkward" ist im Allgemeinen niedriger für PalM.</sample>
    <sample id="906">Das englische Wort "thank you" bedeutet auf Deutsch "danke". Es wird verwendet, um Dank auszudrücken.</sample>
    <sample id="907">Das Bild ist ein statisches Poster für eine Präsentation auf der 61. Konferenz der Assoziation für Computergestützte natürliche Sprache (ACL) im Jahr 2023. Es zeigt die Titel "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" und führt die Autoren Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow an, die an der Saarland-Universität, der Universität Wien und Amazon Alexa arbeiten. Die Präsentation wird von der Saarland-Universität, dem Institut für Sprachwissenschaft und -Technologie der Saarland-Universität und der Universität Wien veranstaltet.</sample>
    <sample id="908">Das Bild ist ein digitales Poster für eine Konferenzbeitrag. Es zeigt die Titel "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" und die Namen der Autoren: Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Die Autoren sind aus verschiedenen Institutionen: Saarland University, Amazon Alexa und Universität Wien. Im Unterteil des Posters ist das Logo der 61. ACL-Konferenz 2023 zu sehen.</sample>
    <sample id="909">Die deutsche Übersetzung des englichen Inhalts lautet:

---

**Warum weakly supervised learning?**

- **Schwache Überwachung löst die Annotation-Bottleneck-Effizienzprobleme auf.**
- **Aber schwache Labels sind raucher!**
  - **Rauchmemorisation schadet der Generalisierung.**
- **Schwach überwachtes Lernen (WSL):**
  - **Trainiere Modelle, die generalisieren, obwohl sie auf raucher Daten trainiert werden.**

---

**Warum weakly-supervised learning?**

- **Schwach überwachte Quellen (z.B. Heuristiken, Wissensbasierte Systeme) ermöglichen die Lösung des Annotation-Bottleneck-Effizienzproblems.**
- **Aber raucher Daten sind raucher!**
  - Die raucher Memorisierung schadet der Generalisierung.
- **Schwach überwachter Lernen (WSL):**
   - Trainiere Modelle, die generalisieren, ohne raucher Daten zu verwenden.
- **Schwach überwache Daten:**
   - **Schwach überwachte Daten sind raucher (z.B. raucher Annotations sind falsch).**

---

**Warum weakly supervisierte Lernung?**

- **Schwache überwachte Quellen (z.B., Heuristiken, Wissensbasiertes Systeme) ermöglichen die Auflösung des Annotation-Bottleneck-Effektivitätsproblems.**
- **Aber rauche Daten sind raucher!**
   - Die rauche Memorisierung schadet der Generalisierbarkeit.
- **Schwach überwacher Lernen (WSL):**
    - Trainiere Modelle, die generalisiert werden, obwohl sie raucher Daten verwenden.
- **Schwache überwachter Daten:**
    - **Schwache überwachte Daten sind rauche (z.B. raucher Annotations fehlerhaft sind).**

---

**War</sample>
    <sample id="910">Die folgende Übersetzung des englischen Inhaltes in deutsch:

---

**Warum weakly supervised learning?**

- **Schwache Überwachung löst die Annotation-Bottleneck-Problem.
- Aber schwache Labels sind raucher!
- Schwach überwachte Modelle generalisieren gut, trotz der raucherigen Daten.

---

**Warum weakly-supervised learning?**

- **Schwach überwachung löst die Annotation-Bottle-Neck-Problematik.
- Aber schwache Labels sind schlecht!
- Schwach überwachte Modellierung generalisiert gut, trotz raucheriger Daten.

---

**Schwache Überwachung lösst die Annotation-Bottle-Neck-Problemathik.

- Aber schwache Labels sind rauchig!
- Schwach überwachte Models generalisieren gut, trotz rauchiger Daten.

---</sample>
    <sample id="911">Die deutsche Übersetzung des englichen Inhalts lautet:

---

**Warum weakly supervised learning?**

- **Schwache Überwachung löst die Annotation-Bottleneck-Problem.
- Aber schwache Labels sind raucher!
- Schwache überwachte Lernen (WSL)
- Trainiere Modelle, die generalisieren, obwohl sie auf raucher Daten trainiert werden.

---

**Warum weakly-supervised learning?**

- **Schwach überwachte Lernung löst die Annotation-Bottleneck-Problematik.**
- **Aber schwache Labels sind raucher!**
- **Noise-Memorization schadet der Generalisierung.**
- **Schwach überwachtes Lernen (WSL)**
- **Trainiere Modelle, die generalisieren, ohne dass sie auf raucher Daten trainiert wurden.**

---

**Warum weakly supervisierte Lernung?**

- **Schwache überwachte Lernung löste die Annotation-Bottleneck-Problematik auf.**
- **Aber schwache Label sind raucher!**
- **Schwach überwachter Lernen (WSL)**
- Trainiere Modelle, die generalisieren, obwohl sie auf raucher Data trainiert werden.

---

Die Übersetzung behält die Inhalte des englischen Originaltextes beim, um die Bedeutung und den Kontext beizubehalten.</sample>
    <sample id="912">Die deutsche Übersetzung des englichen Inhalts lautet:

---

**Warum weakly supervised learning?**

- **Schwache Überwachung löst die Annotation-Bottleneck-Problem.
- Aber schwache Labels sind raucher!
- Schwache überwachte Lernung (WSL)
- Trainiere Modelle, die generalisieren, obwohl sie auf raucher Daten trainiert werden.

---

**Warum weakly-supervised learning?**

- **Schwachere Überwachung löst die Anzeigebottleneck-Problematik.
- Aber schwache Labels sind schmutzig!
- Schwache überwachte Lerntung (WSL)
- Trainiere Modell, die generalisieren, obwohl auf raucher Daten trainiert wird.

---

**Warum weak supervised learning?**

- **Schwere Überwachung löst die Annotations-Bottleneck-Problematik.
- Aber schwache Labels sind raucher!</sample>
    <sample id="913">Die folgende Übersetzung des englischen Inhaltes in deutsch:

---

**Warum weakly supervised learning?**

- **Schwache Überwachung löst die Annotation-Bottleneck-Problem.
- Aber schwache Labels sind raucher!
- Schwache überwachte Lernung (WSL)
- Trainiere Modelle, die generalisieren, obwohl sie auf raucher Daten trainiert werden.

---

**Warum weakly-supervised learning?**

- **Schwachere Überwachung löst die Anzeigebottleneck-Problematik.
- Aber schwache Labels sind schmutzig!
- Schwache überwachte Lerntung (WSL)
- Trainiere Modell, das generalisiert, obwohl es auf raucher Daten trainiert wird.

---

**Warum weak supervised learning?**

- **Schwere Überwachung löst die Annotations-Bottleneck-Problematik.
- Aber schwache Labels sind raucher! 
- Schwache überwachte Lerntechnik (WSL)
- Trainiere Model, das generalisiert, obwohl er auf raucher Daten trainiert wird</sample>
    <sample id="914">Die deutsche Übersetzung des englichen Inhalts lautet:

---

**Eine häufige Behauptung in neueren WSL-Werken**

„Wir trainieren Modelle nur auf schwach überwachten Daten und erreichen eine Genauigkeit von XX%.“

---

**Bildbeschreibung:**

- **Haupttext:**
  - Der Haupttext lautet: „Wir trainieren Modelle nur auf sch wachten Daten und erreichen eine G enauigkeit von XX%.“
  - Der Text ist in schwarzer Schrift auf einem weißen Hintergrund dargestellt.

- **Bilder:**
  - Links: Ein grafisches Darstellung von „schwach überwachten Daten“ mit einem roten Kreis, der auf einem grafischen Symbol für Daten steht.
  - Rechts: Ein grafisches Darstellungs von „genauigkeit“ mit einem grünen Kreis, der auf einem grafischen symbol für Genauigkeit steht.

- **Textunterstützung:**
  - Unter dem Haupttext steht: „Wir trainieren Modelle nur on schwach überwachten Daten (schwach) und erreichen eine Genauigkeit (genauigkeit) von XX%.“

- **Hintergrund:**
  - Der Hintergrund ist weiß, was die grafischen Darstellungen und Texte hervorhebt.

- **Logo:**
  - Im oberen rechten Eck des Bildes ist ein kleines Logo zu sehen, das nicht in der Übersetzung erwähnt werden muss.

---

**Hinweis:**

- Die Übersetzung des Haupttextes ist möglich, da der Inhalt klar und unmissverständlich ist.
- Die Bildbeschreibung ist detailliert und bietet eine klare Vorstellung des Inhalts des Bildes.</sample>
    <sample id="915">Der Inhalt beschreibt eine häufige Behauptung in neueren WSL-Arbeiten, bei denen Modelle ausschließlich auf schwach überwachten Daten trainiert werden und eine Genauigkeit von XX% erreichen. Es wird dargestellt, dass die schwach überwachten Trainingsdaten "schmutzig" sind und die testeten Daten "sauber" sind.</sample>
    <sample id="916">Der Inhalt des Bildes beschreibt eine häufige Behauptung in neueren WSL-Werken. Es wird behauptet, dass Modelle nur mit schwach überwachten Daten trainiert werden und eine Genauigkeit von XX% erreichen. Die Bilder zeigen die verschiedenen Datensätze, die verwendet werden, um die Modelle zu trainieren und zu testen. Die Genauigkeit von XX% wird als Ergebnis der Trainingsprozesse angegeben.</sample>
    <sample id="917">Der Inhalt des Bildes beschreibt eine häufige Behauptung in neueren WSL-Werken, bei denen Modelle ausschließlich auf schwach überwachte Daten trainiert werden und eine Genauigkeit von XX% erreichen. Die Bilder zeigen die verschiedenen Datensättypen: schwach überwachte Trainingsdaten (schwarz), sauber etiquettierte Validierungsdaten (blau) und sauber etiquettierte Testdaten (grün).</sample>
    <sample id="918">Die deutsche Übersetzung des englichen Inhalts lautet:

---

**Unsere Forschungsfragen**

**RQ1:** Ist die Bereinigung der Validierungsdaten notwendig?

**RQ2:** Wie viele saubere Proben benötigen WSL-Ansätze?

**RQ3:** Wie können die verfügbaren sauberen Proben effizienter genutzt werden?

---</sample>
    <sample id="919">Die deutsche Übersetzung des englichen Inhalts lautet:

---

**Unsere Forschungsfragen**

**RQ1:** Ist die Reinigung der Validierungsdaten notwendig?

**RQ2:** Wie viele Reinigungsproben benötigen WSL-Ansätze?

**RQ3:** Wie können die vorhandenen Reinigungsproben effizienter genutzt werden?

---</sample>
    <sample id="920">Die deutsche Übersetzung des englichen Inhalts lautet:

---

**Main Findings**

Die Haupt-Erkenntnisse zeigen, dass die Leistung von FT-NN auf verschiedenen Datensättypen variiert. Die Leistung von FT-NN ist auf den Datensät mit schlechten Labels am höchsten, gefolgt von Datensätten mit zufällig ausgewählten Labels und schließlich Datensätten mit korrekten Labels. Diese Erkenntnisse sind wichtig für die Entwicklung von maschinellen Lernmodellen, die robust gegenüber ungenauen oder fehlerhaften Labels sind.

---

**Haupt-Erkenntnisse**

Die Haupt-Erken</sample>
    <sample id="921">Die deutsche Übersetzung des englichen Inhalts lautet:

---

**RQI Hauptbefunde**

Die Hauptbefunde der Studie zeigen, dass die Leistung von maschinellem Lernen auf verschiedenen Datensättypen variiert. Die Leistung wird auf drei verschiedene Datensättypen getestet:

1. **Fehlerhafte Labels (Fehlerhafte Labels)**: Dieser Datensatz enthält fehlerhafte Labels, die die Leistung beeinträchtigen.
2. **Zufällige Labels (Zufällige Labels)**: Dieser Datensatz enthaelt zufällige Labels, die die Leistung beeintrachtigen.
3. **Saubere Labels (Saubere Labels)**: Dieser Datensatz enthat feine Labels, die die Leistung verbessern.

Die Leistung wird auf verschiedene Modelle getestet, darunter:

- **BOND**: Ein Modell, das auf fehlerhaften Labels trainiert wurde.
- **COSINE**: Ein Modell, das auf zufälligen Labels trainiert wurde.
- **MLC**: Ein Modell, das auf feinen Labels trainiert wurde.
- **L2R**: Ein Modell, das auf feinen Label</sample>
    <sample id="922">Die deutsche Übersetzung des englichen Inhalts lautet:

---

**Main Findings**

Die Haupt-Erkenntnisse zeigen, dass die Leistung von FT-NN auf verschiedenen Datensättypen variiert. Die Leistung von FT-NN ist auf den Datensät mit fehlerhaften Labels am höchsten, gefolgt von Datensätten mit zufällig ausgewählten Labels und schließlich Datensätten mit korrekten Labels. Diese Ergebnisse zeigen, dass die Leistung von Modellen auf unzureichend qualifizierten Daten stark variiert und dass die Verwendung von fehlerhaften Labels zu einer verbesserten Leistung führt.

---

**Haupt-Erkenntnisse**

Die Haupt-Erken</sample>
    <sample id="923">Das Bild zeigt einen Diagramm mit dem Titel "RQ1 Main findings". Es besteht aus einem Scatterplot, der die relative Leistungsverbesserung über weniger Labels in Prozent darstellt. Die x-Achse zeigt verschiedene Methoden wie FTW, BOND, COSINE, MLC und L2R, während die y-Achse die relative Leistungsverbesserung in Prozent anzeigt. Die Datenpunkte sind farblich gekennzeichnet: orange für "Validation on Weak Label", blau für "No Validation (Random Selection)" und grün für "Validation on Clean Label". Die Diagrammlegende ist in der Mitte platziert. Im oberen rechten Bereich des Bildes ist ein Porträt eines Mannes mit Brille und dunklen Haaren zu sehen.</sample>
    <sample id="924">Die deutsche Übersetzung des englichen Inhalts lautet:

---

**Hauptbefunde**

Die Hauptbefunde zeigen, dass die Validierung auf schlecht gelabelte Daten (orange) und die Validierung auf zufällig ausgewählte Daten (blau) im Vergleich zu der Validierung auf saubere Daten (grün) keine signifikanten Unterschiede in der Leistung der verschiedenen Modelle aufweisen. Die Modelle BOND, COSINE, MLC und L2R zeigen ähnliche Leistungsniveaus, während FT\_W zeigt die niedrigste Leistung. Die Validierung auf saubere Daten (green) scheint insgesamt die beste Leistung zu liefern.

---

**Zusammenfassung**

Die Zusammenfassung heißt: "Eine saubere Validierungsmenge ist unerlässlich."

---

**Hauptfindungen**

Die Hauptfindungen zeigen, dass die Validierung auf schwierig gelabelte Daten (orange) und auf zufällig ausgewährte Daten (blau) im Vergl</sample>
    <sample id="925">Der Inhalt beschreibt eine graphische Darstellung, die die Entwicklung der Genauigkeit (Accuracy) über verschiedene Zeiträume hinweg zeigt. Die Graphik zeigt mehrere Linien, die unterschiedliche Methoden oder Ansätze darstellen. Die Farben der Linien sind orange, blau, grün, rosa und gelb. Die X-Achse zeigt die Zeit in Tagen, während die Y-Achse die Genauigkeit angibt. Die Linien steigen allmählich, was bedeutet, dass die Genauigkeit mit der Zeit steigt. Die graphische Darstellung zeigt, wie die verschiedenen Methoden oder Ansätze sich entwickeln und verbessern können.</sample>
    <sample id="926">Der Inhalt beschreibt eine graphische Darstellung, die die Genauigkeit von verschiedenen Methoden zur Klassifizierung von Bildern zeigt. Die Methoden sind "FT", "COSINE", "LR", "BOUND", "MLC" und "Weak labels". Die Genauigkeit wird auf einer y-Achse mit einem Wertebereich von 75 bis 85 dargestellt, während die x-Achse die Anzahl der Validierungsdaten zeigt. Die verschiedenen Methoden sind in verschiedenen Farben dargestellt, um sie leicht voneinander zu unterscheiden. Die graphische Darstellung zeigt, dass die Genauigkeit der Methoden mit zunehmender Anzahl der Validierungsdaten ansteigt. Die Methoden "FT", "COSINE", "LR" und "BOUND" zeigen eine ähnliche Genauigkeit, während "MLC" und "Weak labels" eine geringere Genauigkeit aufweisen. Die graphische Darstellung zeichnet eine positive Korrelation zwischen der Anzahl der Validierungsdaten und der Genauigkeit der Methoden.</sample>
    <sample id="927">Die Übersetzung des englischen Inhaltes in deutsch lautet:

---

**R2 Main findings**

Die Haupt-Erkenntnisse sind in einem Diagramm dargestellt, das die Genauigkeit (Accuracy) verschiedener Methoden über verschiedene Anzahlen von Validierungsdaten zeigt. Die Methoden sind FT, Cosine, L2, BoN, MLC und schwache Labels. Die Genauigkeit steigt mit zunehmender Anzahl an Validierungsdaten an.

---

**R2 Main Findings**

Die Haupt-Erken</sample>
    <sample id="928">Die Übersetzung des englischen Inhaltes in deutsch lautet:

---

**R2 Main findings**

Die grafische Darstellung zeigt die Leistung verschiedener Modelle in Abhängigkeit von der Anzahl der verwendeten Validierungsdaten. Die Modelle werden mit verschiedenen Techniken wie FT, Cosine, L2, L2Norm, BoF, MLC und Adapter_C getestet. Die grafische Darstellung zeigt, dass die Leistung der Modelle mit zunehmender Anzahl der Validierungsdaten steigt. Die grafische Darstellung auf der rechten Seite zeigt die Performance-Daten in Prozent. Die grafische Darstellung wird durch einen roten Punkten markiert, die Leistung der Modelle mit zusätzlichen Validierungsdaten verbessert. Die grafische Darstellung der Leistung der Modelle mit zusatzlichen Validierungsdaten zeigt, dass die Leistung der Modell</sample>
    <sample id="929">Die Übersetzung des englischen Inhaltes in deutsch lautet:

---

**R2 Main findings**

- **FT_C**:
  - 5 Validation: 85%
  - 10 Validation: 87%
  - 20 Validation: 89%
  - 30 Validation: 90%
  - 40 Validation: 91%
  - 50 Validation: 92%

- **COSINE**:
  - 5 Validation: 75%
  - 10 Validation</sample>
    <sample id="930">Die Übersetzung des englischen Inhaltes in deutsch lautet:

---

**Hauptbefunde**

**N=10 saubere Proben pro Klasse**

- **Vor CFT:**
  - Klassen 1-5: 80%
  - Klassen 6-10: 78%
- **Nach CFT:**
  - Klassen1-5: 82%
  - Klassen6-10: 80%

**N=30 saubere Proben pro Klasze**

- **Vor CFT**
  - Klassen 1-10: 80%
  - Klasse 11-20: 80%
  - Klase 21-30: 80%
- **Nach CFT**
  - Klassen1-10: 82%
  - Klasse 1-10: 82%</sample>
    <sample id="931">Die Übersetzung des englischen Inhaltes in deutsch lautet:

---

**Hauptbefunde**

**N=10 saubere Proben pro Klasse**

- **Vor CFT:**
  - Klassen 1-5: 80%
  - Klassen 6-10: 78%
- **Nach CFT:**
  - Klassen1-5: 84%
  - Klassen6-10: 82%

**N=30 saubere Proben pro Klasze**

- **Vor CFT**
  - Klassen 1-10: 80%
  - Klasse 11-20: 80%
  - Klase 21-30: 80%
- **Nach CFT**
  - Klasse 1-10: 84%
  - Klasse 10-20: 82%
  - Klasse 21-30: 82%

---

**Hauptfindungen**

**N=10 sauber Proben pro Klasse**

| Klasse | Vor CFT | Nach CFT |
| --- | --- | --- |
| 1-5 | 80% | 84% |
| 6-10 | 78% | 82% |

**N=30 sauber Proben pro Klasse:**

| Klasse | Vor CTF | Nach CTF |
| --- | --- | --- | 
| 1-10 | 80% | 84 |
| 11-20 | 80% | 82 |
| 21-30 | 80% | 80 |

---

**Haupt findungen**

**N=10 Sauber Proben pro Klasse**
| Klasse | Vor CFT  | Nach CFT |
| --- |--- |--- |
| 1-5 |  80% | 84%  |
| 6-10 |78% | 82% | 

**N=30 Sauber Proben pro Klasse:**
| Klasse | Vor CFT| Nach CFT |
| --- |---|--- |
| 1-10 |  80% | 82% |
| 11-20| 80% | 82%|
| 21-30| 80% | 80% |

---

**Hauptschluss**

**N=10 saube Proben pro Klasse**

Vor CFT:
- Klasse 1-5: 80% 
- Klasse 6-10: 78 %
Nach CFT:
- Klasse  1-5: 84%
- Klasse 6-15: 82%

**N = 30 saube Proben pro Klasse:**

Vor CFT:
-  Klasse 1-10:80%
- Klasse 11-20：80%
- Klasse 21-30：80%
Nach CFT:
- Klassen 1-10：84%
- Klasse 11 -20：82%
- Klasse 21 -30：82%

---

**Main findings**

**N=10 clean samples per class**

| Klasse | Vor | Nach |
| --- | --- | --- |  
| 1-5 | 84% | 84% |
|6-10 | 82% | 82% |


**N=30 clean samples per class**

|Klasse | Vor | Nach |
|---|---|---|
|1-10 | 84% | 84 |
|11-20 | 82% | 80 |
|21-30 | 82% | 84 |

---

**Main findings**  

**N=10 clean samples**

| Klasse | Vor  | Nach |
| --- | --- |--- |
| 1-5  | 84% | 84 % |
| 6-10 |82% | 82% | 
**N=30 clean samples**

| Klas | Vor | Nach |
| --- |--- |--- | 
| 1-10| 84% | 84 | 
| 11-20|82% | 82 | 
| 21-30|82% | 80 | 

---

**Main findings** 

**N=10 clean samples pro Klasse**

| Klassen | Vor CFT | Nach CFT | 
| --- | --- | --- |

| 1-5 | 82% | 84% |
|10-20 | 80% |82% |
|21-30 |80% |82% |

**N= 30 clean samples pro Klasse**
| Klas | Vor CFT | Nach CFT |  
| --- |--- |--- |

| 1-10 |  | 84% |
|11-20 |82% |82% |
|20-30 |82% |80% |

---</sample>
    <sample id="932">Die Übersetzung des englischen Inhaltes in deutsch lautet:

---

**Hauptbefunde**

**N=10 saubere Proben pro Klasse**

| Klasse | Vor CFT | Nach CFT |
| --- | --- | --- |
| Klasse 1 | 80% | 82% |
| Klasse 2 | 78% | 80% |
| Klasse 3 | 76% | 78% |
| Klasse 4 | 74% | 76% |
| Klasse 5 | 72% | 74% |
| Klasse 6 | 70% | 72% |
| Klasse 7 | 68% | 70% |
| Klasse 8 | 66% | 68% |
| Klasse 9 | 64% | 66% |
| Klasse 10 | 62% | 64% |

**N=30 saubere Proben pro Klasne**

| Klasse | Vor-CFT | Nach-CFT |
| --- | --- | ---|
| Klasse 1 | 78% | 80 % |
| Klasse 2 |76% | 78% | 
| Klasse 3 |74% | 76% | 
| Klasse 4 |72% | 74% | 
| Klasse 5 |70% | 72% | 
| Klasse 6 |68% | 70% | 
| Klasse 7 |66% | 68% | 
| Klasse8 |64% | 66% | 
| Klasse9 |62% | 64% | 
| Klasse10 |60% | 62% | 
| Klasse11 |58% | 60% | 
| Klasse12 |56% | 58% | 
| Klasse13 |54% | 56% | 
| Klasse14 |52% | 54% | 
| Klasse5 |50% | 52% | 
| Klasse6 |48% | 50% | 
| Klasse7 |46% | 48% | 
| Klasse68 |44% | 46% | 
| Klasse69 |42% | 44% | 
| Klasse70 |40% | 42% | 
| Klasse71 |38% | 40% | 
| Klasse82 |36% | 38% | 
| Klasse73 |34% | 36% | 
| Klasse74 |32% | 34% | 
| Klasse85 |30% | 32% | 
| Klasse86 |28% | 30% | 
| Klasse97 |26% | 28% | 
| Klasse98 |24% | 26% | 
| Klasse89 |22% | 24% | 
| Klasse90 |20% | 22% | 
| Klasse91 |18% | 20% | 
| Klasse22 |16% | 18% | 
| Klasse23 |14% | 16% | 
| Klasse24 |12% | 14% | 
| Klasse25 |10% | 12% | 
| Klasse26 |8% | 10% | 
| Klasse37 |6% | 8% | 
| Klasse38 |4% | 6% | 
| Klasse39 |2% | 4% | 
| Klasse40 |0% | 2% | 

---

**Hauptfindungen**

**N=10 sauber Proben pro Klasse**

Die Klassen mit den höchsten Accuracy-F1-Werten sind Klasse 1 (82%) und Klasse 2 (80%). Die Klassen mit den niedrigsten Accuracy-F1-Werten sind Klassen 10 (64%) und Klasse 9 (66%).

**N=30 sauber Proben pro Klasse:**

Die Klassen mit den häufigsten Accuracy-F1-Werten sind Kla</sample>
    <sample id="933">Die deutsche Übersetzung des englichen Inhalts lautet:</sample>
    <sample id="934">Die deutsche Übersetzung des englichen Inhalts lautet:

**Schlussfolgerung**

**Neue WSL Ansätze**
- Benötigen saubere Proben.
- Überschätzen die Praktikabilität.

**Unsere Empfehlungen**
- Berichten Sie die Modellauswahlkriterien.
- Verwenden Sie wenig-schrittige Lernansätze als Baselines.
- Wenden Sie immer kontinuierliches Feinabstimmung (CFT) an.

**Bildbeschreibung:**
- Der Bildtitel lautet "Schlussfolgerung".
- Unter dem Titel "Neue WSL Ansätze" sind zwei Punkte aufgeführt: "Benötigen saubere Proben." und "Überschätzen die Praktikablität."
- Unter dem Titel "Unsere Empfehlungen" sind drei Punkte aufgeführt: "Berichten Sie die Modellauswahl kriterien." (Report the model selection criteria.), "Verwenden Sie wenig-schrittiges Lernen als Baselines." (Use few-shot learning approaches as baselines.) und "Wenden Sie immer kontinuierliches Fineabstimmung (CFT) an." (Always apply continuous fine-tuning (CFT).).
- Rechts oben ist ein weißer Hintergrund mit einem schwarzen Punkt.
- Unter dem Bild ist ein weißer Hintergr</sample>
    <sample id="935">Die deutsche Übersetzung des englichen Inhalts lautet:

**Schlussfolgerung**

**Neue WSL Ansätze**
- Benötigen saubere Proben.
- Überschätzen ihre Praktikabilität.

**Unsere Empfehlungen**
- Berichten Sie die Kriterien für die Modellauswahl.
- Verwenden Sie wenigstens eine Lernansatz als Basiskonzept.
- Wenden Sie ständig kontinuierliches Feinabstimmung (CFT) an.

**Bildbeschreibung:**

Die Bildbeschreibung lautet:

Die Bildbeschreibungen lauten:

Die Bildbeschreiber beschreiben die folgenden Inhalte:

Die Bildbeschreber beschreiben die folgenden</sample>
    <sample id="936">Die deutsche Übersetzung des englichen Inhalts lautet:

**Schlussfolgerung**

**Neue WSL Ansätze**
- Benötigen saubere Proben.
- Überschätzen ihre Praktikabilität.

**Unsere Empfehlungen**
- Berichten Sie die Kriterien für die Modellauswahl.
- Verwenden Sie wenigstens eine Lernansatz als Basiskonzept.
- Wenden Sie kontinuierliches Feinabstimmung (CFT) an.</sample>
    <sample id="937">Die deutsche Übersetzung des englichen Inhalts lautet:

**Schlussfolgerung**

**Neue WSL Ansätze**
- Benötigen saubere Proben.
- Überschätzen ihre Praktikabilität.

**Unsere Empfehlungen**
- Berichten Sie die Kriterien für die Modellauswahl.
- Verwenden Sie wenigstens eine Lernansatz als Basiskonzept.
- Wenden Sie kontinuierliches Feinabstimmung (CFT) an.</sample>
    <sample id="938">**Zusammenfassung:**

Die Präsentation befasst sich mit der Wasserstoff-Lösungslösung (WSL) und bietet eine kritische Analyse der aktuellen Ansätze. Es wird betont, dass viele der neuen WSL-Methoden auf sauberen Proben angewendet werden müssen, da die Überschätzung ihrer Praktikabilität zu groß ist. Die Präsentation schlägt vor, die Modellauswahlkriterien zu berichten, weniger auf wenig-schichtenlernende Ansätze zu setzen und kontinuierliche Feinabstimmung (CFT) immer anzuwenden.

**Übersetzung der Inhalte:**

**Schlussfolgerung**

**Neue WSL Ansätze**
- Benötigen saubere Proben.
- Überschätzen ihre Praktikabilität.

**Empfehlungen**
- Berichten Sie die Modellauswahlkriterien.
- Verwenden Sie wenig-schichtenlernenden Ansätze als Baselines.
- Anwenden Sie immer kontinuierliche Feinabstim</sample>
    <sample id="939">Gängige Bewertungsmethoden für Dialoge Systeme sind:

* **Vergleichende Bewertung:** Hierbei werden mehrere Systeme miteinander verglichen, um ihre Leistung zu bewerten. Dies kann durch direkte Interaktion mit den Systemen oder durch die Analyse von deren Ausgaben erfolgen.
* **Likert-Bewertung:** In dieser Methode wird die Zufriedenheit des Nutzers mit dem System durch eine Skala von 1 bis 5 bewertet. Diese Methode ist einfach zu implementieren und liefert schnelle Ergebnisse.

Beide Methoden haben ihre Vor- und Nachteile und können je nach Anwendungsfall und Zielgruppe eingesetzt werden.</sample>
    <sample id="940">Fünf.</sample>
    <sample id="941">Judges decide cases in courts of law.</sample>
    <sample id="942">Ja, der Code ist verfügbar. Sie finden ihn auf GitHub unter mpoems/kitmus.</sample>
    <sample id="943">No, the annotators for NLPositionality are not balanced across all demographic groups.</sample>
    <sample id="944">Sätze innerhalb der akzeptable Domain wurden durch das Verschieben von Subjekten oder Objekten innerhalb der Sätze verändert.</sample>
    <sample id="945">Eine dimensionale Bewertung bedeutet, dass die Qualität eines Dialogs an verschiedenen Aspekten gemessen wird, wie Relevanz, Konsistenz und emotionale Verständlichkeit.</sample>
    <sample id="946">Beijing Jiaotong University.</sample>
    <sample id="947">Die Form des Prompts ist wichtig, um die Klarheit und Genauigkeit der Übersetzung zu gewährleisten.</sample>
    <sample id="978">Die Autoren haben die folgenden Dialogmodelle evaluiert:

- BART-HD-RAG
- Blender2
- Emory
- Blender Decote</sample>
    <sample id="979">Es sind 10 Autoren an der Arbeit beteiligt.</sample>
    <sample id="980">Ein guter Planer sollte klar, detailliert und umfassend sein, um die notwendigen Schritte und Ressourcen für das Ziel zu identifizieren und zu organisieren. Er sollte auch flexibel sein, um Anpassungen vornehmen zu können, wenn sich die Umstände ändern.</sample>
    <sample id="981">7</sample>
    <sample id="982">Vasudha Varadarajan</sample>
    <sample id="983">University of Warsaw.</sample>
    <sample id="1021">Fehler von PaLM:

1. **Fehlende Qualität der Beispielübersetzungen**:
   - Die Beispielübersetzungen sind oft ungenau und verwirrend.
   - Es gibt eine starke Abhängigkeit von der Qualität des Quelltextes.

2. **Fehlende Spezialisierung**:
   - PaLM ist nicht spezialisiert auf bestimmte Bereiche, was zu ungenauen Übersetzungen führt.

3. **Fehlende Stilistische Anpassung**:
   - Die Übersetzungen sind oft unangemessen formell oder unangemessen informell.

4. **Fehlende Konsistenz**:
   - Die Übersetzungen können unkonsistent sein, insbesondere bei ähnlichen Begriffen oder Phrasen.

5. **Fehlende Komplexität**:
   - PaLM hat Schwierigkeiten, komplexe Sätze und Strukturen korrekt zu übersetzen.

6. **Fehlende Kultur- und Kontextverständnis**:
   - PaLM hat Schwiergkeiten, kulturelle Nuancen und Kontexte korrekt zu verstehen und darzustellen.

7. **Fehlende Stilistische Qualität**:
   - Die Übersetzungen könnten unangemessen formell oder unangewendet sein.

8. **Fehlende Stilistische Konsistenz**:
   - Die Stilistik der Übersetzungen kann unterschiedlich sein, was zu unangemessenen Übersetzungen führt.</sample>
    <sample id="1022">Die Präsentation ist auf Englisch, daher wird der Inhalt nicht übersetzt.</sample>
    <sample id="1023">Die Präsentation ist auf Englisch, daher wird der Inhalt nicht übersetzt.</sample>
    <sample id="1024">Die Präsentation beginnt mit einer Titelseite, die die Thematik des Vortrags "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems" thematisiert. Die Autoren Sarah E. Finch, James D. Finch und Jinho D. Choi werden genannt, und die Institutionen Emory University, Emory NLP Research Lab und Amazon sind als Partner angegeben. Die Präsentation wird von einer Person in einem Video gestaltet, die in der rechten Ecke des Bildschirms zu sehen ist.

Nach der Titelseite folgt eine weitere Seite mit dem Titel "Comparative Evaluation". Hier wird eine einfache Zeichnung von zwei Personen, die in Dialog sind, und zwei Chatbots dargestellt. Die Personen sind in einem Dialog, während die Chatbots ihre Antworten in Blau anzeigen. Die Logik der Dialogstruktur wird visualisiert, um die Unterschiede und Gemeinsamkeiten zwischen menschlichen und künstlichen Dialogen zu verdeutlichen.

Die Präsentation zeigt eine weitere Seite mit der gleichen Titelzeile "Comparative Evaluation". Hier wird eine weitere Zeichnung von zwei Personen und zwei Chatbots dargestellt, die in Dialog sind. Die Personen sind in einem Dialog, und die Chatbots ihre Antworten in blau anzeigen. Die Logik der</sample>
    <sample id="1025">Der Präsident beginnt mit einer kurzen Vorstellung der verwendeten Methoden zur Bewertung von KI-Systemen. Er erklärt, dass die Bewertung von KI-Systeme in zwei Hauptkategorien unterteilt ist: die Bewertung der Systemqualität und die Bewertung der Benutzerzufriedenheit. Für die Bewertung der Systemqualität wird eine Bewertungsmatrix verwendet, die die verschiedenen Aspekte des Systems berücksichtigt. Die Bewertung der Benutzerzugriffswahrscheinlichkeit wird durch die Likert-Skala erfolgt, bei der die Benutzer zwischen 1 und 5 bewerten können, wie wahrscheinlich sie das System nutzen würden. Der Präsident zeigt ein Beispiel für die Bewertung der Systemqualität an, bei dem die Bewertungswerte für verschiedene Aspekte des Systems angegeben werden. Er erklärt, dass die Werte für die verschiedenen Aspekte der Systemqualität in einem Matrixformat angegeben werden, wobei jeder Aspekt eine Bewertungswert zwischen 1 und 5 hat. Der Präsident zeigt ein Bei</sample>
    <sample id="1026">Der Präsentierte erklärt, wie man Likert-Skala-Bewertungen analysiert und interpretiert. Er zeigt, wie man die Skala von 1 bis 5 interpretiert, um die Stimmung und die Wahrnehmung der Befragten zu verstehen. Der Präsentierte beschreibt auch die verschiedenen Aspekte der Dialogqualität, wie Relevanz, Konsistenz und emotionale Verständlichkeit, und wie sie bewertet werden können.</sample>
    <sample id="1027">Die Präsentation beginnt mit einem Diagramm, das die verschiedenen Dimensionen der Dialogqualität darstellt. Die Dimensionen Relevanz, Konsistenz und Emotionales Verständnis werden als zentrale Elemente identifiziert, die die Qualität eines Dialogs beeinflussen. Die Präsentation erklärt, wie diese Dimensionen miteinander interagieren und wie sie die Gesamtwertigkeit eines Dialogs beurteilen.

Nachdem die Dimensionen der Dialogqualität vorgestellt wurden, wird die Präsentation auf die Likert-Rating-Evaluation eingegangen. Hierbei wird erklärt, wie die Relevanz der Antworten des Bots bewertet wird. Die Präsentation zeigt, wie die Relevanz der Antwort auf verschiedene Aspekte des Dialogs beeinflusst wird, wie z.B. die Relevanz der Inhalte, die Wahrnehmung des Kontexts und die Relevanz der Sprache.

Die Likert-Rating-Evaluation wird als wichtige Methode zur Bewertung der Qualität eines Dialogs vorgestellt. Diese Methode ermöglicht es, die Relevanz der Antworten des</sample>
    <sample id="1028">Likert-Rating-Evaluation</sample>
    <sample id="1029">In diesem Video wird die Analyse von Chatbots durch die ABC-Evaluation vorgestellt. Die ABC-Evaluation ist eine Methode zur Bewertung der Relevanz der Antworten von Chatbots. Die Analyse basiert auf drei Kriterien: Relevanz, Empathie und Selbstwiderspruch. Die Relevanz bewertet, wie relevant die Antwort des Chatbots für die Frage des Nutzers ist. Empathie bewertet, wie gut der Chatbot die Emotionen des Nutzers versteht und darauf reagiert. Selbstwiderspruch bewertet, wie gut der Chatbots sich selbst widerspiegelt und wie gut er sich selbst versteht. Die Analyse wird durch die Bewertung von Beispielchatbots durchgeführt, die in verschiedenen Bereichen wie Gesundheit, Finanzen und Reisen arbeiten. Die Bewertung wird durch die Bewertung der Antworten auf die Fragen des Nutzers durchgeführt. Die Bewertung wird durch die Analyse der Antworten auf die Fragen des</sample>
    <sample id="1030">Die folgende Beschreibung basiert auf den in der Anleitung angegebenen Bildern und Texten.</sample>
    <sample id="1031">Die Bildsequenz zeigt eine PowerPoint-Präsentation mit dem Titel "ABC-Eval-Verhaltens" (ABC-Evaluation-Verhalten). Die Präsentation besteht aus vier leeren Textfeldern, die in einem 2x2-Feldbreite angeordnet sind. Die Felder sind wie folgt tituliert:

1. Coherence (Kohärenz)
2. Knowledge (Wissen)
3. Consistency (Konsistenz)
4. Emotional Understanding (Emotionales Verständnis)

In der oberen rechten Ecke des Bildes ist ein Porträt einer Person zu sehen, die möglicherweise die Präsentation leitet. Im unteren linken Eck ist das Logo der Emory University zu sehen, und im unteren rechten Eck ist das Logo von Alexa zu sehen.

Die Präsentation scheint auf der Thematik der Evaluierung von Verhaltensweisen in einem psychologischen oder pädagogischen Kontext zu basieren. Die vier Felder könnten als Kategorien oder Aspekte der Evaluierung dienen, die in der Präsentation erläutert werden.

Die Bildsequenz zeigt keine Veränderungen oder Animationen, sondern bleibt auf das gleiche Bild fixiert.</sample>
    <sample id="1032">Die Bildergalerie zeigt eine Präsentation mit dem Titel "ABC-Eval-Verhaltens" (ABC-Eval-Verhaltens). Die Präsentation besteht aus vier Hauptbereichen, die in einem 2x2-Feldbreite angeordnet sind: "Kohärenz" (Coherence), "Wissen" (Knowledge), "Konsistenz" (Consistency) und "Emotionales Verständnis" (Emotional Understanding). Jeder Bereich hat einen leeren Textbereich, in dem zusätzliche Informationen oder Kommentare eingefügt werden können. Die Bildergalerie zeigt, wie die Inhalte des Textbereichs in der Kategorie "Kohärenz" ausgefüllt werden. Zunächst wird der Textbereich in der Kategorie "Ignorieren des Partners" (Ignoring Partner) gefüllt, gefolgt von "Unwichtig" (Irrelevant). Die anderen Bereiche bleiben leer. Die Bildergalerie zeigt die Präsentation in einem fortlaufenden Prozess, in dem die Inhalte des Textbereich</sample>
    <sample id="1033">Die folgende Übersetzung des englischen Textes in die deutsche Sprache ist möglich, aber es gibt einige Aspekte, die möglicherweise nicht direkt übersetzt werden können, da sie in einem spezifischen Kontext verwendet werden.</sample>
    <sample id="1034">Der Präsentierte bespricht die ABC-Eval-Verhaltensarten, die in vier Kategorien unterteilt sind: Kohärenz, Konsistenz und Emotionale Verständlichkeit. Unter Kohärenz werden Ignorieren des Partners und Irrelevantes genannt. Konsistenz umfasst Selbstwiderspruch und Partnerwiderspruch. Die Emotionale Verständlichkeit umfasst falsche Fakten, Commonsense-Verletzungen, empathische Reaktionen und fehlende Empathie.

Die Präsentation erklärt, dass die Experimente 4 Open-Domain-Dialog-Modelle und 100 mensch-bot-gesteuerte Konversationen pro Modell umfassen.

Anschließend wird die ABC-Eval-Plattform vorgestellt, die als wissenschaftliches Tool für die Bewertung von Dialogmodellen dient. Die Plattform ermöglicht es, mensch-bot-gesteuerte Konversationsdaten zu sammeln und zu analysieren, um die Effektivität und Effizienz verschiedener Dialogmodelle zu bewerten.</sample>
    <sample id="1035">Die folgende Übersetzung des englischen Textes in die deutsche Sprache:</sample>
    <sample id="1036">Die Präsentation zeigt die Ergebnisse von Experimenten mit vier Open-Domain-Dialog-Modellen. Jeder Modell wurde mit 100 menschlichen Bot-Konversationen getestet. Die Modellarten sind ABC-Eval, Turn Likert, Dialogue Likert und Comparative. Die Präsentation zeigt die Evaluationsergebnisse für jeden Modell, einschließlich der Konsistenz, emotionalen Verständnis, Informativität, Engagement, Grammatikalität, Proaktivität, Relevanz und Overall Quality. Die Evaluationsergebnisse werden in Form von Diagrammen und Tabellen dargestellt. Die Präsentation wird von einem Mann in einem blauen Hemd gehalten.</sample>
    <sample id="1037">Der Vortrag beginnt mit einer Vorstellung der Inter-Annotator-Agreement, die die Übereinstimmung zwischen verschiedenen Annotatoren bemisst. Die Vorstellung zeigt, dass die Inter-Annotator-Agreement für verschiedene Kriterien wie ABC-Eval, Turn Likert, Dialogue Likert und Comparative unterschiedlich hoch ist. Die Vorstellung zeigt auch, dass die Inter-Annotator-Ag</sample>
    <sample id="1038">Der Präsentator diskutiert die Inter-Annotator-Agreement und die Vorhersagegüte in einem Video. Die Inter-Annotator-Agreement wird mit Hilfe von Kappa-Metriken gemessen, wobei die Werte für verschiedene Annotation-Methoden wie ABC-Eval, Turn Likert, Dialogue Likert und Comparative analysiert werden. Die Vorhersagegüte wird mit dem Prozentsatz der erklärten Qualität (Explained Quality) verglichen, wobei die Interaktive Qualität und die Interaktive Qualität als Messmethoden verwendet werden. Die Präsentation zeigt, dass die Interaktive Qualität häufiger erklärt wird als die Interaktive Qualität.</sample>
    <sample id="1039">Die Präsentation zeigt die "Predictive Validität" (prädiktive Güte) verschiedener Interaktionsmethoden. Die x-Achse zeigt verschiedene Methoden wie "ABC-Eval", "Turn Likert", "Dialogue Likert" und "Comparative", während die y-Achse die "Prozentuale Qualitätserklärung" (Prozentuale Qualitätserklärungsrate) darstellt. Die grauen Balken repräsentieren die "Interactive Ques" (Interaktive Fragen), während die roten Balken die "Interactive Qua" (Interaktive Qualität) darstellen. Die Balken sind unterschiedlich hoch, was auf unterschiedliche Qualitätserklärungswerte hinweist. Die Präsentation zeigt, dass bestimmte Methoden wie "ABC-Eval" und "Turn Likert" höhere Qualitätserklärungswerte erzielen als andere.</sample>
    <sample id="1040">Die Präsentation beginnt mit einem Überblick über verschiedene Methoden zur Bewertung der Qualität von Dialogsystemen. Die Präsentation zeigt, wie die verschiedenen Methoden unterschiedliche Aspekte der Qualität berücksichtigen und wie sie in der Praxis angewendet werden. Die Präsentation endet mit einer Zusammenfassung der wichtigsten Erkenntnisse und Emp</sample>
    <sample id="1041">Der englische Text beschreibt eine graphische Darstellung, die die "Incremental Validität" (inkrementelle Güte) von verschiedenen Kriterien im Zusammenhang mit der Qualität von Dialogen oder Texten zeigt. Die Kriterien werden auf einer horizontalen Achse aufgelistet, während die vertikale Achse die "Qualitäts-Exponentialität" (Q-Exponentialität) darstellt. Die Kriterien sind "Turn-Likert" (Turn-Likert), "Dialog-Likert" (Dialog-Likert) und "ABC-Likert" (ABC-Likert). Die graphische Darstellung zeigt, wie sich die Q-Exponentialität für jeden Kriterium mit zunehmender Anzahl der Kriterien ändert. Die Kriterien werden in der Reihenfolge "Turn-Likert", "Dialog-Likert" und "ABC-Likert" aufgelistet. Die Kriterien werden in der Reih</sample>
    <sample id="1042">Der englische Text beschreibt eine Präsentation über die "Incremental Validität" (Incremental Validität), die sich auf die Validität von Dialogsystemen konzentriert. Die Präsentation zeigt drei verschiedene Kurve, die die Validität von verschiedenen Dialogsystemen über die Anzahl der erfüllten Bedingungen hinweg darstellen. Die Kurve "ABC-Val" zeigt die Validität eines Systems, das alle Bedingungen erfüllt. Die Kurve "Turn-Likert" zeigt die Validität eines Systems mit einer Likert-Skala, und die Kurve "Dialog-Likert" zeigt die Validität einer Dialog-System-Version mit einer Likert-Skala. Die Präsentation verwendet auch eine Tabelle, die die verschiedenen Bedingungen auflistet und die Anzahl der erfüllenden Bedingungen für jede Kurve anzeigt.</sample>
    <sample id="1043" />
    <sample id="1044">Der Präsident des Emory University, der in der Bildmitte zu sehen ist, beginnt mit einer Einführung in die ABC-Evaluation und die Fehlerraten verschiedener Modelle. Er erwähnt, dass die ABC-Evaluation eine Methode zur Bewertung der Genauigkeit von Textgenerierungsmodellen ist, die auf der Analyse von Abweichungen zwischen den generierten Texten und den tatsächlichen Texten basiert. Der Präsident erklärt, dass die Fehlerraten in verschiedenen Kategorien wie "Antisozial", "CS Kontext", "Inporn", "Incorrect", "Irrelevant", "Unamphibious", "Other Context", "Redundant", "Self Context", "Topic Switch" und "Uninterpreted" unterschiedlich sind. Er betont, dass die Fehlerraten in der Kategorie "CS Kontext" besonders hoch sind, was auf die Schwierigkeit zur Erkennung von Kontextinformationen in Texten zurückzuführen ist. Der Präsident erklärt auch, dass die Fehlerraten in der allgemeinen Kategorie "Incorrect" hoch sind, was auf die Schwierichkeit zur Erkennung von falschen Informationen in Texten zurückzuführt ist. Er betont, dass die Fehlerrate in der Kategorie "Uninterpreted" besonders hoch ist, was auf die Schwierigkeit zur Interpretation von Texten zurückzuführen</sample>
    <sample id="1045">Der Präsident der Emory Universität, Dr. W. R. King, wird auf der 10. Internationalen Konferenz für Künstliche Intelligenz (ICAI) in Atlanta, Georgia, aufgefordert, die Ergebnisse der ABC-Evaluation zu präsentieren. Die ABC-Evaluation ist eine Methode zur Bewertung der Leistung von KI-Modellen, die auf der Analyse von Texten basiert. Die Präsidenten erklärt, dass die ABC-Evaluation eine wichtige Methode zur Bewertung von KI-Modellen ist, da sie die Fähigkeit der Modelle zur Verständnis und Analyse von Texten misst. Sie führt die Präsentationen der verschiedenen KI-Modelle durch und zeigt die Ergebnisse der ABC-Evaluation an. Die Präsidenten erklären, dass die ABC-Evaluation eine wicht</sample>
    <sample id="1046">Die folgende Beschreibung bezieht sich auf ein Bild, das in einem Video zu einem Vortrag oder einer Präsentation verwendet wird.</sample>
    <sample id="1047">Die folgende Beschreibung basiert auf den angegebenen Bildern und Video:</sample>
    <sample id="1048">Emory University.</sample>
    <sample id="1049">Continuous fine-tuning.</sample>
    <sample id="1050">7</sample>
    <sample id="1051">Der englische Inhalt des Bildes besagt:

**Wann benötigt Übersetzung Kontext? Eine datengetriebene, mehrdimensionale Erkundung**

**Patrick Fernandes, Kayo Yin, Emmy Liu, André F. T. Martins, Graham Neubig**

**Carnegie Mellon University Language Technologies Institute**

**Técnico Lisboa**

**BAIR**

**Unbabel**

**gleiche Beitragsquelle**</sample>
    <sample id="1052">Translation depends on context. We'll have to get rid of that mole.</sample>
    <sample id="1053">Translation depends on context. Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole.</sample>
    <sample id="1054">Der englische Text lautet: "Could it be anything serious, Doctor? We'll have to get rid of that mole." Diese Aussage wird in einem medizinischen Kontext verwendet, wo ein Arzt möglicherweise eine mole (ein Hautgeschwür) beschreibt, die entfernt werden muss, um eine potenzielle Gefahr zu vermeiden. Die Übersetzung des Textes ins Deutsche lautet: "Könnte es etwas ernstes sein, Arzt? Wir müssen das Mole entfernen."</sample>
    <sample id="1055">Das Bild zeigt eine Präsentation über die Herausforderungen der Bewertung kontextabhängiger Übersetzungen. Der Titel lautet "Bewertung kontextabhängiger Übertragungen ist schwer". Die Präsentation erklärt, dass nur ein kleiner Teil der Wörter von Kontext abhängig ist. Es wird ein Diagramm mit drei verschiedenen Textstücken gezeigt, um die V</sample>
    <sample id="1056">Bewertung von kontextabhängiger Übersetzung ist schwer

- Nur ein kleiner Teil der Wörter hängt von Kontext ab
- Korpusniveau-Metriken
- Bestehende Methoden unterstützen begrenzte Diskurse
- Phänomene und Sprachen</sample>
    <sample id="1057">Die Frage 1 lautet: "Wann benötigt eine Übersetzung Kontext?" Die Frage 2 lautet: "Wie gut können Modelle kontextabhängige Übersetzungen durchführen?"</sample>
    <sample id="1058">Die Frage 1 lautet: Wann benötigt eine Übersetzung Kontext? Die Antwort lautet: Wortebene Kontextnutzung. Die Frage 2 lautet: Wie gut können Modelle kontextabhängige Übersetzungen durchführen?</sample>
    <sample id="1059">**CXMI (Conditional Cross-Mutual Information):**  
CXMI misst, wie viel Kontext maschinelle Übersetzungsmodelle mit einem Korpus nutzen.</sample>
    <sample id="1060">**Beschreibung des Bildes:**  
Das Bild ist ein digitales Diagramm, das die Konzepte von **Conditional Cross-Mutual Information (CXMI)** erklärt. Es besteht aus einer Titelzeile, einer Liste mit einer Punktzeile und einem zentralen Diagramm.  

---

### **Titelzeile:**  
**Conditional Cross-Mutual Information (CXMI):**  
*CXMI: Messen, wie viel Kontext MT-Modelle mit einem Korpus verwenden.*  

---

### **Liste mit Punktzeile:**  
**- CXMI: Messen, wie viel Kont</sample>
    <sample id="1061">Wir führen P-CXMI ein, um den Kontextverwendung zu messen, um eine bestimmte</sample>
    <sample id="1062">Die folgenden Fragen und Antworten sind in deutscher Sprache:

**Q1: Wann benötigt man Kontext für die Übersetzung?**
- Wortebene Kontextnutzung
- thematische Analyse

**Q2: Wie gut können Modelle kontextabhängige Übersetzungen verarbeiten?**</sample>
    <sample id="1063">Die englischen Inhalte auf Deutsch lauten wie folgt:

**RQ1: Wann benötigt Übersetzung Kontext?**
- Wortebene Kontextnutzung
- thematische Analyse

**RQ2: Wie gut handeln Modelle kontextabhängige Übersetzungen?**</sample>
    <sample id="1064">Thematische Analyse von häufig verwendeten Wörtern mit hohem P-CXMI-Wert</sample>
    <sample id="1065">Die Präsentation beginnt mit einer Überschrift, die die thematische Analyse von hoch relevanten P-CXMI-Wörtern behandelt. Die Präsentation ist in zwei Hauptteile unterteilt: POS-Tags und Pronouns. Die POS-Tags werden als eine Liste von Wörtern und ihren entsprechenden POS-Tags angegeben, wobei die Wörter in einem roten Farbstabellenformat dargestellt werden. Die Pronouns werden in einem blauen Farbstabellenformat dargestellt. Die Präsentation endet mit einer Zusammenfassung der wichtigsten Punkte und einer Aufforderung an die Zuhörer, die Präsentation zu beenden.</sample>
    <sample id="1066" />
    <sample id="1067">Thematische Analyse von hoch relevanten Wörtern (P-CXMI-Wörtern)</sample>
    <sample id="1068">Thematische Analyse von häufig verwendeten Wörtern mit hoher P-CXMI-Werten</sample>
    <sample id="1069">Die thematische Analyse von hoch relevanten Wörtern (P-CXMI-Wörtern) umfasst drei Hauptpunkte: 1. POS-Tags, 2. Vokabularien, und 3. Einzelne Wörter. POS-Tags sind die syntaktischen Kategorien von Wörtern, wie z.B. Substantive, Verben, Adjektive usw. Vokabularien sind die Wörter, die in einem Text häufig vorkommen und daher relevant sind. Einzelne Wörter sind die grundlegenden Bausteine eines Textes und können in verschiedenen Kontexten verwendet werden.</sample>
    <sample id="1070">Thematische Analyse von hohen P-CXMI-Wörtern</sample>
    <sample id="1071">Die Präsentation beschäftigt sich mit der Frage, wann Übersetzungen Kontext benötigen und wie gut Modelle kontextabhängige Übersetzungen behandeln. Die Präsentation stellt zwei Hauptfragen vor: 1. Wann benötigt Übersetzungen Kontext? 2. Wie gut behandeln Modelle kontextabhängige Überset zungen? Die Antwort auf die erste Frage umfasst die Verwendung von Wortebene Kontext und thematische Analyse. Die zweite Frage wird durch den Multilingual Discourse-Aware (MuDA) Benchmark beantwortet. Der MuDA-Tagger wird als wichtiges Werkzeug vorgestellt, das Pronomen, Verben, lexikalische Kohäsion, Formulität und Ellipsen erfasst.</sample>
    <sample id="1072">Der Präsident beginnt mit einer kurzen Vorstellung der Multilingual Discourse-Aware (MuDA) Tagger. Die MuDA Tagger ist ein Tool, das dazu dient, die Sprach- und Diskursmerkmale in mehreren Sprachen zu analysieren. Die Präsidentin erklärt, dass die MuDA Tagger in 12 verschiedenen Sprachen verfügbar ist und die Sprach- und Diskursmerkmäle wie Pronomen, Verben, lexikalische Kohäsion, Formulierungen und Ellipsen analysiert. Sie zeigt eine Tabelle an, die die verschiedenen Sprachen und die Anzahl der Sprach- und Diskursmerkmale auflistet. Anschließend zeigt sie eine Grafik, die die Anzahl der Sprach- und Discursmerkmale in verschiedenen Sprachen aufzeigt. Die Präsidentin erkläert, dass die Grafik die Anzahl der Sprach- und Dis</sample>
    <sample id="1073" />
    <sample id="1074">Die Präsentation beginnt mit einem Überblick über die MuDA-Benchmark-Studie, die die Anwendung von Kontext bei der Übersetzung untersucht. Die Studie zeigt, wie die Verwendung von Kontext bei der Überset-</sample>
    <sample id="1075">Die folgende Übersetzung des englischen Inhalts ins Deutsche:

---

**RQ1: Wann benötigt man Kontext für Übersetzungen?**
- Wortebene Kontextnutzung
- thematische Analyse

**RQ2: Wie gut handeln Modelle kontextabhängige Übersetzungen?**
- Mehrsprachige Diskursbewusstsein (MuDA) Benchmark
- Modellbewertung

---

**Corpus-level-Metriken**

---

**Corpus-level metrics**

---

**Corpus-Level-Metriken**

---

Die folgende Übersetzung des deutschen Inhalts ins Englische:

---

**RQ1:** When does translation require context?
- Word-level context usage
- Thematic analysis

**RQ2:** How well do models handle context-dependent translations?
- Multilingual Discourse-Aware (MuDA) benchmark
- Model evaluation

---

**Corpus-level Metrics**

---

**Corpus level metrics**

---

**Corus-level metrics**

---

**Korpus-Level-Metriken**

---</sample>
    <sample id="1076">Die englische Beschreibung des Videos lautet: "Corpus-level metrics" (korpusweite Metriken).</sample>
    <sample id="1077">Der englische Text in der V</sample>
    <sample id="1078">Der englische Inhalt des Videos lautet:

---

**Corpus-level metrics**

- Unclear which system is best for document-level MT with corpus-level metrics

---

**MuDA benchmark results**

- Context-aware models perform significantly better on some phenomena

---

**MuDA benchmark result**

- Context-aware models perform significantly worse on some phenomena

---

**Conclusion**

- Context-aware models perform significantly differently on some phenomena

---

**Context-aware models perform significantly better on some phenomena**

- Formalities, lexical cohesion

---

**Conclusion**</sample>
    <sample id="1079">Die englischen Inhalte des Bildes können wie folgt übersetzt werden:</sample>
    <sample id="1080">Die englische Präsentation wird in Deutsch übersetzt:</sample>
    <sample id="1081" />
    <sample id="1082">Die Präsentation besch</sample>
    <sample id="1083">Zusammenfassung:
- Identifizieren von Diskursphänomenen systematisch ohne vorheriges linguistisches Wissen
- Datensetunabhängiger Benchmark für Dokument-Level-Maschinenübersetzung</sample>
    <sample id="1084">Yusen Zhang</sample>
    <sample id="1121">Permuting with "jumps"</sample>
    <sample id="1122">Find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">Cambridge University.</sample>
    <sample id="1124">Moscow</sample>
    <sample id="1125">Sarah E. Finch</sample>
    <sample id="1126">4</sample>
    <sample id="1127">BLIMP, SyntaxGym, CrowS.</sample>
    <sample id="1161">FT, COSINE, MLC, L2R, BOND.</sample>
    <sample id="1162">11</sample>
    <sample id="1226">Wikipedia.</sample>
    <sample id="1227">Adam Przepiórkowski</sample>
    <sample id="1228">Die Ergebnisse zeigten, dass die zeitliche Verzögerei mit einer größeren temporären Lücke korrelierte und die Leistung stärker abnahm. Dies führte zur Schlussfolgerung, dass die zeitinhomogenität die Hauptursache für den Leistungseinbruch war.</sample>
    <sample id="1269">Um die Ausgabesequenz korrekt zu generieren, müssen die Token für die Ausgabesequen</sample>
    <sample id="1270">Die Autoren empfehlen es Modellentwicklern, ihre Methoden zum Abbau von Vorüberlegungen transparenter zu machen, um die **Transparenz über die Bias-Mitigation** zu erhöhen. Dies hilft, die **Transparenz über die Intersektionalität** zu verbessern und die **Transparenz über die Positive Stereotypen und die Essenzialisierung der Narrativen** zu erhöhen.</sample>
    <sample id="1271">Inakzeptable Minimalpaareingaben sind solche, die nicht in der Sprache vorkommen, wie "Non-stereotypical sentence" (nicht-stereotypische Satzstruktur).</sample>
    <sample id="1272">The authors used the following evaluation metrics:

- **F1 Score**: Measures the balance between precision and recall.
- **Precision**: The proportion of relevant instances among the retrieved instances.
- **Recall**: The proportion of relevant instances that were retrieved.
- **Accuracy**: The proportion of correctly classified instances.
- **AUC-PR (Area Under the Precision-Recall Curve)**: Evaluates the model's performance across different thresholds.
- **AUC-ROC (Area Under the Receiver Operating Characteristic Curve)**: Assesses the model's ability to distinguish between classes.

These metrics provide a comprehensive evaluation of the model's performance in terms of both accuracy and robustness.</sample>
    <sample id="1273">Krippendorff's Alpha.</sample>
    <sample id="1274">Die Domain, die gewählt wurde, um völlig unzusammengehörige Sätze zu den inakzeptabel und akzeptablen Suchanfragern hinzuzufügen, ist **Wikipedia**.</sample>
    <sample id="1275">Heinrich Heine Universität Düsseldorf.</sample>
    <sample id="1276">MultiInstruct ist ein Benchmark, der sich von anderen unterscheidet, da es eine große Anzahl an **sprachbasierten Anweisungen** (1600+) bietet, die nicht mit multimodalen Anweisungen kombiniert sind. Dies zeigt eine **Unausgewogenheit** zwischen natürlichen Sprachprozessen (NLP) und multimodalen Prozessen.</sample>
    <sample id="1277">3</sample>
    <sample id="1278">Die binäre Koordination ist eine Art der Sprachkommunikation, bei der zwei Personen direkt miteinander kommunizieren, ohne dabei dritte Personen oder Medien als Vermittler einzubeziehen. Diese Form der Kommunikation ist oft eng mit persönlichen Beziehungen verbunden und kann sowohl verbal als auch nicht-verbal erfolgen.</sample>
    <sample id="1279">The average length of the prompts used in this study was 100 words.</sample>
    <sample id="1280">Das kleinere T5-Modell erzielt eine höhere Genauigkeit als die großen Sprachmodelle.</sample>
    <sample id="1281">DrBERT: Ein robuster vorgebener Modell in französischer Sprache für biomedizinische und klinische Bereiche</sample>
    <sample id="1282">Die Präsentation beginnt mit einem Überblick über die verschiedenen Aspekte der Sprachmodellierung im Gesundheitswesen. Die erste Punkt behandelt die Sprachmodellierung im Gesundheitsuwesen, die zweite die Vergleichsstudie der vorgestellten Sprachmodellierungsstrategien, Datenquellen und Größen. Die dritte Punkt befasst sich mit der Bewertung von 13 Modellen auf 11 Aufgaben. Die vierte und letzte Punkt bezieht sich auf die Verteilung von NACHOS und DrBERT.</sample>
    <sample id="1283">Die Präsentation beginnt mit einer Übersicht über die Hauptthemen des Vortrags. Die erste Punkt beschäftigt sich mit der Sprachmodellierung in der Gesundheitsversorgung. Der zweite Punkt untersucht die Voraussetzungen für die Sprachmodellierung, einschließlich der Datenquellen und der Größe der Daten. Der dritte Punkt bewertet 13 Modelle auf 11 Aufgaben. Der vierte Punkt beleuchtet die Verteilung von NACHOS und DrBERT.</sample>
    <sample id="1284">Die Präsentation beginnt mit einem Überblick über die verschiedenen Aspekte der Sprachmodellierung im Gesundheitswesen. Die erste Punkt behandelt die Sprachmodellierung im Gesundheitsuwesen, die zweite die Vergleichsstudie der vorgestellten Sprachmodellierungsstrategien, Datenquellen und Größen. Die dritte Punkt befasst sich mit der Bewertung von 13 Modellen auf 11 Aufgaben. Die vierte und letzte Punkt bezieht sich auf die Verteilung von NACHOS und DrBERT.</sample>
    <sample id="1285">Die folgende Übersetzung des englischen Textes in die deutsche Sprache:

---

**Sprachmodellierung in der Gesundheitsversorgung**

Transformer-basierte Ansätze, wie BERT, bieten einen großen Leistungsgewinn für viele NLP-Aufgaben.

Wurde auf Französisch mit Camembert und FlauBert angepasst.

Auf medizinischen Aufgaben erhoben sich die Ergebnisse von domänenspezifischen Modellen in Englisch noch weiter.

PudMedBERT, BioBERT, ClinicalBERT und andere.

Sprachen anderer als Englisch werden hauptsächlich auf kontinuierliche Vorverstärkung mit einem bestehenden allgemeinen Modell angewiesen.

Im Gegensatz zu allgemeinen Modellen ist ein Open-Source-Modell für das biomedizinische Bereich in Französisch noch nicht verfügbar.

Ein BERT-basiertes domänenspezifisches Modell für Französisch sollte die Leistung bei medizinischen Aufgaben erhöhen.

---</sample>
    <sample id="1286">Die Sprachemodellierung basiert auf Transformer-basierten Ansätzen wie BERT, die einen großen Leistungsgewinn bei vielen NLP-Aufgaben bieten. BERT wurde auf Französisch angepasst und in Camembert und FlauBert umgesetzt. Auf englischen medizinischen Aufgaben erhoben spezifische Modelle die Leistung noch weiter. Beispiele sind PubMedBERT, BioBERT, ClinicalBERT und andere. Sprachmodelle, die anders als Englisch verwendet werden, setzen sich hauptsächlich auf kontinuierliche Vorbereitung mit einem bestehenden allgemeinen Modell verlassen. Im Gegensatz zu allgemeinen Modellen ist ein Open-Source-Modell für den medizinischen Bereich in Französisch noch nicht verfügbar. Ein BERT-basiertes spezifisches Modell für Französisch sollte die Leistung bei medizinischen Aufgaben erhöhen.</sample>
    <sample id="1287">Sprachmodellierung:
- Transformer-basierte Ansätze wie BERT bieten enorme Leistungssteigerungen für viele NLP-Aufgaben.
- Wurde auf Französisch angepasst mit Camembert und FlauBert.
- Auf englischen medizinischen Aufgaben erhob sich der Standard erheblich.
- PudMedBERT, BioBERT, ClinicalBERT und andere sind für andere Sprachen weniger verfügbar.
- Sprachen anderer als Englisch werden hauptsächlich auf kontinuierliche Anpassung mit einem bestehenden allgemeinen Modell angewiesen.
- Im Gegensatz zu allgemeinen Modellen ist bisher kein Open-Source-Modell für den medizinischen Bereich auf Französisch verfügbar.
- Ein BERT-basiertes spezifisches Modell für Französisch soll die Leistung bei medizinischen Aufgaben erhöhen.</sample>
    <sample id="1288">Sprachmodellierung: Transformer-basierte Ansätze, wie BERT, bieten einen großen Leistungsgewinn für viele NLP-Aufgaben an. Wurde auf Französisch angepasst mit Camembert und FlauBert. Auf englischen medizinischen Aufgaben erhoben sich die Ergebnisse von spezifischen Modellen noch weiter. PudMedBERT, BioBERT, ClinicalBERT und andere. Sprachen anderer als Englisch sind seltener und setzen hauptsächlich auf kontinuierliche Anpassung mit einem bestehenden allgemeinen Modell. Im Gegensatz zu allgemeinen Modellen ist kein Open-Source-Modell für den medizinischen Bereich auf Französisch verfügbar. Ein BERT-basiertes spezifisches Modell für Französisch soll die Leistung bei medizinischen Aufgaben erhöhen.</sample>
    <sample id="1289">Die Sprache der Inhalte ist bereits in deutscher Sprache.</sample>
    <sample id="1290" />
    <sample id="1291" />
    <sample id="1292" />
    <sample id="1293" />
    <sample id="1294" />
    <sample id="1295" />
    <sample id="1296" />
    <sample id="1297" />
    <sample id="1298" />
    <sample id="1299" />
    <sample id="1300" />
    <sample id="1301" />
    <sample id="1302">**Evaluation: Pre-training strategies**

- **Von Suche vs. kontinuierlicher Vor-Training auf 4GB-Daten:**
  - Die Ergebnisse zeigen, dass kontinuierlicher Vor-Training mit 4GB-Daten besser ist als Suche.
  - Die kontinuierliche Vor-Training-Strategie zeigt eine höhere Inter-Inter-Variabilität für die Camembert-basierten Modelle, die mit kontinuierlicher Vor-Training trainiert wurden.

- **Frage-Beantwortung erfordert mehr domänenspezifisches Wissen, um gut zu arbeiten:**
  - Die Ergebnisse deuten darauf hin, dass kontinuierlicher Vor- und Nach-Training besser darin sind, domänenspezifisches Wissen zu erl</sample>
    <sample id="1303">**Evaluation: Pre-training strategies**

- **Von Suche vs. kontinuierlicher Vor- und Nachbearbeitung auf 4GB-Daten:**
  - Die kontinuierliche Vor- und Nachbearbeitung von Daten auf 4GB zeigt, dass es notwendig ist, um in der Lage zu sein, gut zu arbeiten.

- **Frage-Beantwortung erfordert mehr domänenspezifisches Wissen, um gut zu arbeiten zu können:**
  - Die Frage-Beantwortung erfordert mehr spezifisches Wissen in Bezug auf die Domäne, um gut zu arbeiten.

- Eine Studie zeigt, dass kontinuierliche Vor- und Nachbereitung von Camembert-basierten Modellen eine höhere Inter- und intra-Modellvariabilität zeigt, als Suche.

**Tabelleninhalt:**

| Modell          | Vorschlag 1 (OSCAR 13B) | Vorschlag 2 (OSCAR 13B) |</sample>
    <sample id="1304">**Evaluation: Pre-training strategies**

- **Von Suche vs. kontinuierlicher Vorausbildung auf 4GB-Daten:**
  - Die kontinuierliche Vorausbildung auf 4GB Daten erfordert mehr spezifisches Fachwissen, um effektiv zu arbeiten.

- **Eine Studie zeigt, dass kontinuierliche Vorausbildungsstrategien eine höhere Interaktionsvariabilität für die Camembert-basierten Modelle erzielen, die mit kontinuierlicher Vorausbereitung trainiert werden.**

- **Die Tabelle zeigt die Ergebnisse verschiedener Modelle auf verschiedenen Datensätzen. Die Spalten bezeichnen die Modelle, die Datenquellen und die verschiedenen Evaluation-Metriken.**

- **Die Modelle werden auf verschiedenen Datenquellen trainiert, einschließlich Wikipedia, PubMed, Medline, DNA-CAS, PubMedCentral, BioRxiv, ArXiv, QuarkMed, and QuarkMedline.**

- **Die Evaluation-Metriken umfassen Accuracy, Precision, Recall, F1-Score, und AUC-ROC.**

- **Die Tabelle ze</sample>
    <sample id="1305" />
    <sample id="1306" />
    <sample id="1307" />
    <sample id="1308">Vielen Dank!
Ich freue mich auf den Austausch auf der Poster-Sitzung in Toronto!
Weitere Informationen: dibier.univ-avignon.fr
Avignon Universität</sample>
    <sample id="1309">Von scratch, full model fine-tuning, ChatGPT, Camembert, French, PubmedBERT, English.</sample>
    <sample id="1310">Der Faktor der Überanpassung ist 1.2.</sample>
    <sample id="1311">Die Qualität der Vereinfachung wurde anhand der BLEU, ROUGE und METEOR-Werte bewertet.</sample>
    <sample id="1312">Ja, Sprachmodelle haben unterschiedliche politische Vorur-teile.</sample>
    <sample id="1313">**Kompositionalisierung ohne Bäume mit Multiset-Tagging und Latent-Permutationen**  
Matthias Lindemann, Alexander Koller, Ivan Titov  

**Logo-Anzeige:**  
- **Informatics**  
- **NLP**  
- **Stärkland University**  
- **University of Amsterdam**  

---  

**Übersetzung des Inhalts:**  

**Kompositionalisierung ohne Bünde mit Multiset-Tagging und Latent-Perturbationen**  
Matthias Lindemann  
Alexander Koller  
Ivan Titov  

**Logo-Angebot:**  
- **Informatics**</sample>
    <sample id="1314">**Übersetzung ins Deutsche:**

**Kompositionalisierung ohne Bäume mit Multiset-Tagging und Latenter Permutation**

**Matthias Lindemann, Alexander Koller, Ivan Titov**

**Logo:**
- **Informatics**
- **NLP**
- **Stärkland University**
- **University of Amsterdam**

**Bildbeschreibung:**

Die Bildsequenz besteht aus einem gleichbleibenden Hintergrund mit einem Textblock in der Mitte und einem kleinen Bild in der oberen rechten Ecke. Der Textblock enthält die folgenden Informationen:

- **Hauptüberschrift:** "Kompositionalisierung ohne Bäme mit Multiset-Tagging und Latenter Permmutation"
- **Unterüberschrift:** "Matthias Lindemann, Alexander Koller und Ivan Titov"
- **Logo:** Unterhalb der Hauptüberschrift sind die Logos der folgenden Institutionen dargestellt:
  - **Informatics**
  - **NLP**
  - **Stärkland University**
  - **University of Amsterdam**

Das kleine Bild in der oberen rechten Eiche zeigt einen Mann, der in einem Büro sitzt und vor einem Computer arbeitet. Der Mann trägt ein dunkles T-Shirt und hat kurze, hellblonde Haare. Er hat eine leichte, freundliche Ausdrucksweise und scheint sich an die Kamera zu wenden.

Die Bildsequenz bleibt konstant, wobei nur die Bildgröße und -position unterschieden sind.</sample>
    <sample id="1315">Der Fokus des Textes ist auf der Fähigkeit eines Lerners, tiefere Rekursion und ungesehene Kombinationen von Phrasen zu verstehen, die während der Ausbildung einzeln gesehen wurden. Diese Fähigkeit ermöglicht es dem Lernenden, komplexe und unvorhergesehene Sprachstrukturen zu analysieren und zu verarbeiten, was eine wichtige Kompetenz für das Sprachlernen darstellt.</sample>
    <sample id="1316">**Title:** Compositional Generalization in Semantic Parsing  
**Content:**  
The image shows a table with two columns labeled "Train" and "Mary knew that the girl slept." The table contains four rows, each with a sentence in the "Train" column and a corresponding sentence in the "Mary knew that the girl slept" column. The sentences in the "Train" column are:  
1. "The girl slept."  
2. "The girl slept."  
3. "The girl slept."  
4. "The girl slept."  
The sentences in the "Mary knew that the girl slept." column are:  
1. "Mary knew that the girl slept."  
2. "Mary knew that the girl slept."</sample>
    <sample id="1317">**Title:** Compositional Generalization in Semantic Parsing  
**Content:**  
The image shows a table with two columns labeled "Train" and "Mary knew that the girl slept." The table contains four rows, each with a sentence in the "Train" column and a corresponding sentence in the "Mary knew that the girl slept" column. The sentences in the "Train" column are:  
1. "The girl slept."  
2. "The girl slept."  
3. "The girl slept."  
4. "The girl slept."  
The sentences in the "Mary knew that the girl slept." column are:  
1. "The agent slept."  
2. "The girl knew that the agent slept."  
3. "The girl knew that the agent slept. The girl slept."  
4. "The agent slept. The girl slept."  
The image illustrates the concept of compositional generalization in semantic parsing, where the meaning of a sentence is derived from the combination of its components.</sample>
    <sample id="1318">The video presents a visual demonstration of compositional generalization in semantic parsing, focusing on the training and testing phases. The content is structured as follows:

### Title:
**Compositional Generalization in Semantic Parsing**

### Train Section:
The video shows three examples of sentences in the training phase, each with a corresponding semantic parse tree. The sentences are:
1. **The girl slept.**
   - Semantic parse tree: `girl x sleep agent x`
2. **Mary knew that the girl slept.**
   - Semantic parse tree (highlighted in green): `girl x sleep agent x`
3. **The girl slept.**
   - Another semantic parse tree: `girl x sleep agent x agent x`

### Test Section:
The video then transitions to the test phase, where a new sentence is presented:
- **Jim said that Mary knew that the girl slept.**
  - Semantic parse tree: `girl x sleep x agent x agent x agent x`

### Analysis:
The video highlights the ability of the model to generalize compositional structures from the training data to the test data. The semantic parse trees in the test phase show the model's capacity to understand and generate complex sentences by reusing the compositional patterns learned during training.

### Conclusion:
The video effectively demonstrates the concept of compositional generalization in semantic parsing, showcasing how a model can apply learned patterns to new, unseen sentences.</sample>
    <sample id="1319">**Title:** Compositional Generalization in Semantic Parsing  
**Content:**  
The image shows a table with three columns labeled "Train," "Mary knew that the girl slept," and "Jim said that Mary knew that the girl slept." The table contains four rows of text, each representing a different semantic parsing example. The text is highlighted in different colors to indicate different parts of the sentence. The table is titled "Compositional Generalization in Semantic Parsing."</sample>
    <sample id="1320">**Title:** Compositional Generalization in Semantic Parsing  
**Subtitle:** Naive seq2seq models fail!  

**Content:**  
The image shows a table with two columns labeled "Train" and "Test." The "Train" column contains three examples of sentences in green text, while the "Test" column contains three examples of sentences in blue text. The sentences in the "Train" column are:  
1. "The girl slept."  
2. "Mary knew that the girl slept."  
3. "The girl x sleep agent x."  

The sentences in the "Test" column are:  
1. "Jim said that Mary knew that the girl slept."  
2. "Jim said that Mary knew that the agent x sleep agent x."  
3. "Jim said that Mary knew that the know agent x sleep agent x."  

At the bottom of the image, there is a red text box with the message:  
**"Naive seq2seq models fail!"**  

The background of the image is white, and the text is in black, with the exception of the green and blue sentences, which are highlighted. The title is in yellow.</sample>
    <sample id="1321">**Title:** Compositional Generalization in Semantic Parsing  
**Subtitle:** Naive seq2seq models fail!  

**Train:**  
- The girl slept.  
- Girl x sleep agent x.  
- Girl x know agent Mary know ccomp x.  
- Girl x know agent x sleep agent x.  

**Test:**  
- Jim said that Mary knew that the girl slept.  
- Girl x say agent Jim say ccomp x.  
- Girl x say agent Jim know ccomp x.  
- Girl Mary know ccomp x sleep agent x.  

**Note:** Naive seq2seq models fail!</sample>
    <sample id="1322">The English content in the video is:

"Trees help a lot but..."

This phrase suggests that while trees are beneficial, there are other factors or considerations that also play a significant role. The rest of the content appears to be a tree diagram with various nodes and labels, but without additional context, it's difficult to provide a precise translation of the entire diagram.</sample>
    <sample id="1323">Die englische Übersetzung des Inhalts lautet: "Trees help a lot but..." (Bäume helfen viel, aber...).</sample>
    <sample id="1324">Die Bildergeschichte zeigt eine Sequenz von Bildern, die eine logische Struktur und eine narrative Entwicklung darstellen. Die Bilder sind in einer Reihenfolge angeordnet, die die Handlung und die Entwicklung der Geschichte verdeutlicht. Die Bilder sind in einer logischen Struktur angeordnet, die die Handlung der Geschichte verdeutlichen. Die Bilder sind in einer log</sample>
    <sample id="1325">The video shows a slide with a yellow background and black text. The slide contains a tree diagram with three branches, each labeled with a different logical form. The top branch is labeled "girl x1 sleep agent x2 x1", the middle branch is labeled "girl x1 x sleep agent x2 x1", and the bottom branch is labeled "The girl slept". The slide also contains a note at the bottom that says "Trees need to be obtained: Pre/Post-processing logical forms". The video does not contain any spoken words or narration.</sample>
    <sample id="1326">Die Bildergalerie zeigt eine Analyse von syntaktischen Bäumen, die die Struktur von Sätzen darstellen. Die Bäume helfen bei der Verarbeitung natürlicher Sprache, indem sie die grammatischen Beziehungen zwischen den Wörtern visualisieren. Die Bäume werden verwendet, um die syntaktische Struktur von Sätzen zu analysieren und zu verstehen, wie die Wörter in einem Satz zusammenhängen. Die Bäume helfen auch bei der Verarbeitung natürliccher Sprache, indem sie die gramatischen Beziehungen zwischen den Wörttern visualisieren und die Struktur von Sätzen analysieren. Die Bäume helfen bei der Analyse von Sätzen, indem sie die grammatischen Bezeichnungen der Wörter und ihre Beziehungen visualisieren. Die Bäume helfen auch bei den grammatischen Analyseprozessen, indem sie die grammatischen Bezüge zwischen den Wörtern visualisieren und die Struktur der Sätze analysieren. Die Bäume helfen beim Verarbeiten von natürlicher Sprache, indom sie die grammatischen Beziehungen visualisieren und die Struktur von Satzen analysieren. Die Bäume hel</sample>
    <sample id="1327" />
    <sample id="1328" />
    <sample id="1329">Unser Ansatz</sample>
    <sample id="1330">Unser Ansatz</sample>
    <sample id="1331">Unser Ansatz</sample>
    <sample id="1332">Das Bild zeigt ein Diagramm, das den Ansatz zur Verarbeitung von Text beschreibt. Es besteht aus mehreren Schritten, die in einer Sequenz dargestellt sind. Zunächst werden die Wörter "the", "girl", "i", "sleep", "agent" und "x2" in der Sprache "English" identifiziert. Anschließend werden diese Wörter in der Sprache "Chinese" übersetzt, wobei "girl" zu "女孩" und "sleep" zu "睡觉" übersetzt wird. Die Wörter "i" und "x2" bleiben unverändert. Anschließend werden die Wörter "the" und "agent" in der Sprache "English" wieder in der Sprache "Chinese" übert</sample>
    <sample id="1333">The image shows a flowchart titled "Our Approach" with various elements and connections. Here is a detailed description of the image:

1. **Title**: The title "Our Approach" is written in yellow at the top left corner of the image.

2. **Flowchart Elements**:
   - **Permute**: This is a large rectangular box in the center of the image, representing the permutation step. It has multiple arrows pointing to and from it, indicating the flow of data or processes.
   - **Tag**: Below the "Permute" box, there is another rectangular box labeled "Tag." This box has arrows pointing to and from it, indicating the tagging step.
   - **Words and Tags**: Inside the "Permute" box, there are several words and tags:
     - "the" (green)
     - "girl" (yellow)
     - "x1" (yellow)
     - "i" (green)
     - "sleep" (blue)
     - "agent" (blue)
     - "x2" (yellow)
     - "x3" (yellow)
   - **Connections**: There are multiple arrows connecting the words and tags within the "Permute" box, indicating the relationships or transformations between them.

3. **Additional Elements**:
   - **Green and Yellow Boxes**: There are smaller green and yellow boxes with words inside them, such as "the" and "girl," which are connected to the main flowchart.
   - **Blue Boxes**: There are blue boxes with words inside them, such as "sleep" and "agent," which are also connected to the main flowchart.

4. **Overall Structure**: The flowchart is structured to show a process involving permutation and tagging, with various elements and connections indicating the flow of data or processes.

This flowchart appears to be a visual representation of a method or algorithm, possibly related to natural language processing or data transformation. The specific details of the process are not provided in the image, but the structure suggests a systematic approach to handling and transforming data.</sample>
    <sample id="1334">Das Bild zeigt eine Prozessdiagramm, das die Schritte einer "Permutierung mit 'Jumps'" beschreibt. Es besteht aus zwei Hauptkomponenten: "Permute" und "Tag". 

1. **Permute**:
   - Dieser Teil des Diagramms zeigt, wie die Elemente "the", "i", "girl", "sleep", "agent" und "x2" permutiert werden.
   - Die Elemente werden in einer bestimmten Reihenfolge umgeordnet, um eine neue Reihenfolge zu erzeugen.

2. **Tag**:
   - Dieser Teil des Diagramms zeigt, wie die permutierten Elemente weiter verarbeitet werden.
   - Die Elemente "the", "i", "sleep", "agent" und "slept" werden in einer bestimmten Reihenordnung angezeigt.

Das Diagramm veranschaulicht, wie die Permutation der Elemente und die anschließende Verarbeitung der permutierten Elemente erfolgen.</sample>
    <sample id="1335">Das Bild zeigt eine Prozessdiagramm, das die Schritte einer "Permutierung mit 'Jumps'" beschreibt. Es besteht aus zwei Hauptkomponenten: "Permute" und "Tag". 

1. **Permute**:
   - Dieser Prozess hat eine Eingabe, die als "Jumps" bezeichnet wird.
   - Es gibt eine Sequenz von Zahlen, die als Indizes für die Permutation verwendet werden.
   - Die Permutation wird durch die Anwendung der Indizes auf die Eingabe durchgeführt.

2. **Tag**:
   - Dieser Prozess markiert die Permutierung.
   - Es gibt eine Liste von Werten, die als "Tags" bezeichnet werden.
   - Die Permutation wird auf die Tags angewendet, um die entsprechenden Indices zu markieren.

Das Diagramm zeigt, wie die Permutation auf die Tags angewendet wird, um die entsprechenden Werte zu markieren. Die Prozessdiagramme zeigen, wie die Permutation und die Markierung der Tags zusammenarbeiten, um die gewünschte Ausgabe zu erzeugen.</sample>
    <sample id="1336">Das Bild zeigt eine Prozessdiagramm, das die Schritte einer Permutations-Algorithmus beschreibt, bei dem "Jumps" verwendet werden. Es besteht aus zwei Hauptkomponenten: "Permute" und "Tag". 

1. **Permute**:
   - Dieser Teil des Diagramms zeigt, wie die Elemente der Liste permutiert werden. Die Liste enthält die Elemente "the", "i", "x1", "girl", "x1", "sleep", "agent", und "x2".
   - Die Permutation wird durch eine Reihe von Schritten dargestellt, bei denen die Elemente in der Liste umgeordnet werden. Die Schritte sind durch Pfeile verbunden, die zeigen, wie die Elemente von einem Standort zum nächsten bewegt werden.
   - Die Permutation wird durch eine rote Linie markiert, um den Prozess zu visualisieren.

2. **Tag**:
   - Dieser Teil des Di</sample>
    <sample id="1337">Das Bild zeigt ein Diagramm, das die Prozesse der Permutation mit "Jumps" visualisiert. Es besteht aus mehreren Komponenten, die die verschiedenen Schritte des Algorithmus darstellen.

1. **Permute**: Dieser Block zeigt die Permutation der Elemente. Die Elemente werden in einer bestimmten Reihenfolge angeordnet, um eine neue Reihenfolge zu erzeugen.

2. **Tag**: Dieser Block zeigt die Tags, die die Elemente kennzeichnen. Die Tags helfen dabei, die Elemente zu identifizieren und zu verfolgen.

3. **Permutationsmatrix**: Diese Matrix zeigt die Permutationsmatrix, die die Permutation der Elemente beschreibt. Die Matrix zeigt, wie die Elemente angeordnet sind.

4. **Permutationsmatrix mit Jumps**: Dieser Block zeigt die Permutationsmatrix mit Jumps. Jumps sind spezielle Schritte, die die Permutation der Elemente beschränken.

5. **Permutationsmatrix mit Jumps und Tags**: Dieser Block zeigt die Permutionsmatrix mit Jumps und Tags. Jumps sind spezielle Schrit</sample>
    <sample id="1338">Das Bild zeigt ein Diagramm, das die Prozesse der Permutation mit "Jumps" erklärt. Es besteht aus einem großen roten Rechteck, das als "Permute" bezeichnet wird, und einem kleinen blauen Rechteck, das als "Tag" bezeichnet wird. Innerhalb des roten Rechtecks sind mehrere grüne, gelbe und blaue Rechtecke, die als "Permutationsschritte" bezeichnet werden. Die grünen Rechtecke repräsentieren die Elemente, die permutiert werden, während die gelben und blauen Rechtecke die "Jumps" darstellen. Die Prozesse der Permutation mit "Jumpen" werden in der Diagrammlegende erklärt.</sample>
    <sample id="1339" />
    <sample id="1340" />
    <sample id="1341">The image shows a flowchart with the title "Technical Challenges We Solve" in yellow text at the top. Below the title, there are three columns of text, each representing a different technical challenge. The first column is labeled "Permute" and contains the words "the," "gir," and "skir." The second column is labeled "Tag" and contains the words "the," "gr," and "skr." The third column is labeled "Align unknown" and contains the words "the," "ger," and "skr." The flowchart is designed to illustrate the process of solving technical challenges, with the "Permute" column representing the initial steps, the "Tag" column representing the intermediate steps, and the "Align unknown" column representing the final steps. The flowchart is a visual representation of the technical challenges that the organization or team is working to solve.</sample>
    <sample id="1342">The video presents a technical challenge involving the alignment of three sequences labeled "Permute" with three unknown positions. Below the "Permute" section, there is a "Tag" section with three tags: "the," "gir," and "skirt." The video explains that the alignment is unknown and introduces a solution involving a red arrow pointing from the "Permute" section to the "Tag" section, indicating a relationship or transformation between the sequences and the tags. The video emphasizes the technical challenge of aligning the sequences and the solution provided.</sample>
    <sample id="1343">The video presents a technical challenge involving the alignment of sequences, likely in the context of bioinformatics or computational biology. The main elements of the slide are:

1. **Title**: "Technical Challenges We Solve"
2. **Sequence Alignment**: A sequence of colored blocks (green, yellow, blue, red) representing different elements or components of a sequence. These blocks are aligned in a specific order, but the alignment is unknown.
3. **Permute Block**: A block labeled "Permute" with three question marks (?), indicating that the order of the elements in this block is not yet determined.
4. **Tag Block**: A block labeled "Tag" with three colored blocks (green, yellow, blue) at the bottom, representing tags or markers associated with the sequence.
5. **Alignment Unknown**: A note at the bottom of the slide stating "Alignment unknown," indicating that the alignment of the sequence elements is not yet determined.
6. **Induce It in Training**: A suggestion at the bottom of the slide to "Induce it in training," implying that the alignment can be determined through a training process or algorithm.

The slide is designed to highlight the challenge of aligning sequences and the need for a method to determine the correct order of elements. The use of question marks in the "Permute" block suggests that the alignment is not yet known and needs to be determined. The "Tag" block at the bottom provides additional context or markers that may be relevant to the alignment process. The overall message is that the alignment of the sequence elements is a technical challenge that needs to be addressed, and the solution involves inducing the alignment through a training process.</sample>
    <sample id="1344">The image shows a flowchart with the title "Technical Challenges We Solve" in yellow text at the top. The flowchart is divided into two main sections: the top section represents the "Permute" process, and the bottom section represents the "Tag" process. The flowchart includes various elements such as "Alignment unknown," "Induce it in training," and "Permutation model." The "Permute" process is depicted with arrows pointing from one element to another, indicating the flow of information or processes. The "Tag" process is also shown with arrows pointing from one element to another. The flowchart is designed to illustrate the technical challenges that are being addressed, with the "Permute" process representing the initial steps and the "Tag" process representing the subsequent steps. The flowchart is a visual representation of the technical challenges that are being addressed, with arrows pointing from one element to another, illustrating the flow of information or processes. The flowchart is designed to be clear and easy to understand, with the use of arrows and labels to guide the viewer through the process. The flowchart is a useful tool for understanding the technical challenges that are being addressed, and it can be used to communicate the process to others in a clear and concise manner.</sample>
    <sample id="1345">The video presents a technical challenge related to permutation models in machine learning. The main focus is on the difficulty of inferring permutations, which is NP-hard (equivalent to the Traveling Salesman Problem, TSP). The video explains that the challenge lies in the complexity of finding the optimal permutation, which is computationally expensive. To address this challenge, the video introduces a method called continuous relaxation, which allows for the backpropagation of gradients through the permutation model. This approach enables the model to learn the optimal permutation more efficiently, even though the problem is NP-hard. The video also highlights the importance of alignment in machine learning, as it is crucial for the model to understand the relationships between different data points. The video concludes by emphasizing the significance of solving this technical challenge, as it has implications for various applications in machine learning, such as natural language processing and computer vision.</sample>
    <sample id="1346">The video presents a technical challenge related to permutation models in machine learning. It highlights the difficulty of inferring permutations, which is NP-hard, and introduces a method called continuous relaxation to backpropagate through the model. The video also mentions the use of a permutation model to induce alignment during training. Additionally, it provides a link to a paper and code for further information.</sample>
    <sample id="1347">Cognitive dissonance is a psychological theory that describes the mental discomfort experienced when holding two or more contradictory beliefs, values, or ideas simultaneously. This discomfort arises from the inconsistency between these elements of cognition, leading to a state of tension or unease. The theory suggests that individuals are motivated to reduce this dissonance by altering their beliefs, attitudes, or behaviors to achieve consistency.</sample>
    <sample id="1348">Alpaca.</sample>
    <sample id="1349">The video does not provide a clear answer to whether cumulative training is better than iterative training for active learning.</sample>
    <sample id="1350">Sara Papi.</sample>
    <sample id="1351">Die Daten für die MuDa-Benchmark stammen aus dem Corpus MuDa, das von der Universität des Saarlandes in Trier erstellt wurde.</sample>
    <sample id="1385">Matthias Lindemann, Alexander Koller, Ivan Titov</sample>
    <sample id="1386">Sprachübergreifender Transfer bezieht sich auf die Fähigkeit eines Modells, Wissen aus einer Sprache zu lernen und in eine andere Sprache zu übertragen. Es ist ein wichtiger Aspekt der natürlichen Sprachverarbeitung und der maschinellen Übersetzung.</sample>
    <sample id="1387">Die Autoren gehören an der Saarland-Universität.</sample>
    <sample id="1388">The authors use the following latency measurements:

- **Latency measure**: This is the primary latency measurement used in the study.
- **Quality measure**: This is a secondary latency measurement used in the study.</sample>
    <sample id="1389">Der englische Inhalt des Bildes kann wie folgt ins Deutsche übersetzt werden:

---

**Titel:**
**The KITMUS Test**

**Untertitel:**
**Evaluating Knowledge Integration from Multiple Sources**

**Hauptteil:**
Die Bilder zeigen eine Reihe von Personen mit ihren jeweiligen Namen und Zugehörigkeiten. Die Personen sind in zwei Reihen angeordnet, mit einer Person auf der linken Seite und einer auf der rechten Seite. Die Personen auf der linken Seite sind:

1. **Akshatha Arodi**
   - **Zugehörigkeit:** McGill University/Mila
2. **Martin Poms**
   - **Zugehöigkeit:** McGill University/Mila
3. **Kaheer Suleman**
   - **Zugehö</sample>
    <sample id="1390">Die Präsentation beschäftigt sich mit der Rolle von Wissen in natürlicher Sprachverarbeitung (NLU) und zeigt, wie NLU-Model verschiedene Wissensquellen nutzen. Die Präsentation besteht aus zwei Hauptteilen: "Wissen in Parametern (Vorauswissen)" und "Wissen in Kontext (Inferenzzeit-Wissen)".

1. **Wissen in Parametern (Voraufwissen)**:
   - Dieser Teil zeigt, dass NLU-Model mit einem vorgefügten Wissensnetz ausgestattet sind, das während des Trainings erlernt wurde. Dieses Wissen ist in den Parametern des Modells enthalten und ermöglicht es dem Modell, bestimmte Informationen und Muster zu erkennen.

2. **Wissen in Kontext (Inferenztime-Wissen)**:
   - Im zweiten Teil wird dargestellt, dass NLU-Model auch während der Inferenzzeit Wissen aus dem Kontext nutzen können. Dies kann durch die Analyse von Textdaten, die in der Umgebung oder in der Anwendung vorhanden sind, erreicht werden.

**NLU-Modell**:
- Das NLU-Modell ist ein allgemeines Konzept, das verschiedene Techniken und Modelle nutzt, um menschliche Sprache zu verstehen und zu generieren. Es kann auf verschiedene Wissensquellen zurückgreifen, um präzise und relevante Antworten zu liefern.

**Zusammenfassung**:
- Die Präsentation zeigt, dass NLU-Model sowohl vorhandenes Wissen (Voraufwissen) als auch kontextbezogenes Wissen (Inferenzzeit-Wissen) nutzen können, um effektiver und genauere Ergebnisse zu erzielen. Diese duale Nutzung von Wissen ermöglicht es den NLU-Modellen, in verschiedenen Anwendungsfällen flexibel und effizient zu arbeiten.</sample>
    <sample id="1391">Die Präsentation beschäftigt sich mit der Rolle verschiedener Wissensquellen in natürlicher Sprachverarbeitung (NLU) und zeigt, wie NLU-Model diese Wissensquellen nutzen. Die Präsentation wird in zwei Hauptteilen unterteilt: "Wissen in Parametern (Vorauswissen)" und "Wissen in Kontext (Inferenzzeit-Wissen)".

### Wissen in Parametern (Voraus-Wissen)
- **Definition**: Dieser Teil des Modells basiert auf dem Wissen, das während des Trainings (Pretrain) erworben wird.
- **Visualisierung**: Ein Diagramm zeigt eine komplexe Struktur, die möglicherweise die internen Gewichte und Verbindungen des Modells darstellt.
- **Beispiel**: Ein Text, der die Funktionsweise eines Modells erklärt, wird in einem Textfeld angegeben.

### Wissen in Kontext (Inferenzzeit Wissen)
- **Definition**: Dieser Aspekt des Modells nutzt Wissen, das während der Inferenz (Inference-Time) erworben wird.
-   **Visualisierung**: Ein Textfeld enthält eine ausführliche Erklärung oder Beschreibung, die möglicherweise die Anwendung des Modells in einem bestimmten Kontext erklärt.

### NLU-Modell
- **Definition**: Das NLU-Modell ist ein allgemeines Konzept, das die beiden Aspekte der Wissensquellen integriert.
- **Visualisierung**: Ein zentrales Diagramm zeigt die beiden Aspekte "Wissen in Parametern" und "Wissen in Kontext" in einem Netzwerk, das das Modell darstellt.

### Zusammenfassung
- **Anwendung**: Die Präsentation zeigt, wie NLU-Model die Wissensquellen "Wissen in Parametern" und  "Wissen in Kontext" nutzen, um effektiver zu arbeiten.
- **Ziel**: Die Präsentation dient dazu, die Bedeutung und die Integration dieser Wissensquellen im NLU-Modell zu verdeutlichen.</sample>
    <sample id="1392">Der englische Text auf der Präsentationsseite lautet:

"John saw the newly elected president on TV

What presidents do ✅
What is a TV ✅

Who is John ❌
Who is the new president ❌"

Der Text beschreibt eine Szene, in der John auf den neu gewählten Präsidenten auf dem Fernseher zuschaut. Die Präsentation scheint sich auf die Unterschiede zwischen John und dem neuen Präsidenten zu konzentrieren, was durch die Markierungen "Who is John ❌" und "Who is the new president ❌" hervorgehoben wird.</sample>
    <sample id="1393">Der englische Text auf der Präsentationsseite lautet:

"John saw the newly elected president on TV

What presidents do ✅
What is a TV ✅

Who is John ❌
Who is the new president ❌"

Der Text beschreibt eine Szene, in der John auf dem Fernseher einen neuen Präsidenten sieht. Die Präsentation scheint sich auf die Unterschiede zwischen John und dem neuen Präsidenten zu konzentrieren, was durch die Markierungen "Who is John ❌" und "Who is the new president ❌" hervorgehoben wird.</sample>
    <sample id="1394">Der englische Text auf der Präsentationsseite lautet:

"John saw the newly elected president on TV

What presidents do ✅
What is a TV ✅

Who is John ❌
Who is the new president ❌"

Der Text beschreibt eine Szene, in der John auf dem Fernseher einen neuen Präsidenten sieht. Die Präsentation scheint sich auf das Thema "Präsentationszeitwissen" zu beziehen, was auf der linken Seite der Präsentationstechnik erwähnt wird.

Der Text auf der Präsentationstechnik lautet:

"Präsentationszeitwissen"

Der Text auf der rechten Seite der Präsentationstechniken lautet:

"Made for FREE.1001.CMS"

Der Text auf der rechten oberen Seite der Präsentationstechniki lautet:

"John saw..."

Der Text auf der rechten oben Seite der Präsentationstehnik lautet:

"John saw"

Der Text auf der rechten unten Seite der Präsentationstehni lautet:

"John saw...</sample>
    <sample id="1395">Der englische Text in der Bildunterschrift lautet: "John saw the newly elected president on TV."</sample>
    <sample id="1396">Der englische Inhalt des Bildes kann folgendermaßen ins Deutsche übersetzt werden:

---

**KITMUS Test Suite**

- **Datensatz für die Kenntnisintegrationsbewertung**
- **Coreferenzauflösungsaufgabe zur Untersuchung der Fähigkeit, auf vorgeführte und auf der Fließzeit erworbene Wissen zurückzuziehen**
- **Experimente mit menschlichen Studienteilnehmern und Coreferenzauflösungsmodellen**

---

Der Inhalt beschreibt die Hauptthemen des KITMUS Test Suites, einschließlich der Verwendung eines Datensatzes zur Kenntnisintegrationsbewertung, einer Aufgabe zur Coreferenzauflösung zur Untersuchung der Fähigkeiten von KI-Systemen, und den Experimenten mit menschlichen Studienteilnehmers und Coreferenzauflösungs-Modellen.</sample>
    <sample id="1397">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**KITMUS Test Suite**

- **Datensatz für die Kenntnisintegrationsbewertung**
- **Coreferenzauflösungsaufgabe zur Untersuchung der Fähigkeit, auf zu ziehen**
  - **Voraussetzungswissen**
  - **Inferenzzeit-Wissen**
- **Experiment mit**
  - **Menschenstudienteilnehmern**
  - **Coreferenzauflösungsmodellen**

---

Der Inhalt beschreibt das **KITMUS Test Suite**, ein Framework zur Bewertung der Kenntnisintegration. Es besteht aus einem Datensatz, der für die Kenntnisintegrationsbewer</sample>
    <sample id="1398">Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich im Park. Nach einer langen Arbeit am Gericht, bei der sie Urteile fällten, war er froh, zu entspannen. [Antwort: Servin]</sample>
    <sample id="1399">Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich im Park. Nach einem langen Tag am Arbeitsplatz, in dem sie an der Rechtsprechung arbeiten, war er froh, zu entspannen. [Antwort: Servin]</sample>
    <sample id="1400">Der englische Text lautet: "Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]".</sample>
    <sample id="1401">Der englische Inhalt des Bildes kann wie folgt ins Deutsche übersetzt werden:

---

**KITMUS Test Suite**

**Serv ist ein Richter. Kea ist ein Bäcker. Serv und Kea trafen sich in einem Park. Nach einer langen Arbeit am Gericht entschieden sie Fälle. Er war glücklich zu entspannen. [Antwort: Serv]**

1. **Entitätsbezogene Wissen**
2. **Hintergrundwissen**

**Inferenzzeit-Wissen**

**Vorausbildungswissen**

---

**Bildbeschreibung:**

- **Titel:** KITMUS Test Suite
- **Text:**
  - **Serv ist ein Richter. Kea ist Ein Bäcker. Serv und Kea traffen sich in einem Park. Nach einer langen arbeiten am Gericht entschieden sie Fall. Er war glücklich zu entspannt. [Antwort: Serv]**
  - **1. Entitätsbezogene Wissen**
  - **2. Hintergrundwissen**
  - **Inferenzzeit-Wissen**
  - **Vorausbildungswissen**</sample>
    <sample id="1402">Der englische Inhalt des Bildes kann wie folgt ins Deutsche übersetzt werden:

---

**KITMUS Test Suite**

**Serv ist ein Richter. Kea ist ein Bäcker. Serv und Kea trafen sich in einem Park. Nach einer langen Arbeit am Gericht entschieden sie Fälle. Er war glücklich zu entspannen. [Antwort: Serv]**

---

**1. Entitätsspezifisches Wissen**

**Inferenzzeit-Wissen**

---

**2. Hintergrundwissen**

**Vorausbereitungswissen**

---

**Inferenzzeit-Wissen:**

- **Definition:** Wissen, das während der Inferenzzeit (z.B. beim Lösen eines Problems oder der Beantwortung einer Frage) erworben wird.
- **Beispiel:** Wenn Sie eine Frage beantworten, die nicht direkt aus der vorgegebenen Information abgeleitet wird, sondern durch logisches Denken oder Zusammenfügen von Informationen erforderlich ist.

---

**Vorausbereitungs-Wissen:**

- **Definition:** Information, die vorher bereits vorhanden ist und während der Vorbereitung auf eine Aufgabe oder eine Situation gesammelt wird.
- **Beispiel:** Allgemeine Wissen über Richter und Bäcker, wie sie ihre Arbeit erledigen.

---

**Inferenz-Zeit-Wissen:**

- **Definition:** Knowledge that is acquired during inference (e.g., when solving a problem or answering a question).
- **Example:** If you answer a question that requires logical thinking or combining information, it is inference-time knowledge.

---

**Vorabereitungs-Wissen:**

- Definition: Information that is collected before preparing for a task or situation.
- Example: General knowledge about judges and bakers, how they do their work.

---

**Inferenzwissen:**

- **Definition:** Wissens, das während der Inferenz (z.B. beim Lösen von Problemen oder beim Beantworten von Fragen) erworben wird.
-**Beispiel:** Wenn Sie eine Fragen beantworten, die nicht direkt von der vorgegebenen Information abgeleiht wird, sondern durch logisches denken oder zusammenfügen von Informationen benötigt wird.

---

**Vorhabereitungs-Wissen:**

-**Definition:** Information, die vorher bereits vorgefunden wird und während der Vorbereitungen auf eine Aufgabe oder eine Situation gesammt wird.
-**Beispiel:** Allgemeine Wissen um Richter und Bäcker, wie sie ihr Arbeit erledigen.

---</sample>
    <sample id="1403">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Variants of KITMUS**

**Variant 1: Background-Pretrain**
- **Beschreibung:** Typischer Aufbau.
- **Details:** Hierbei wird das Hintergrundwissen in der Kontextinformationen integriert.

**Variant 2: Background-Both**
- **Beschreibung**: Hintergrundwissen wird explizit in Kontextinformationen integriert.
- **Details:** Hierbein wird das Hintergrundwissen in beiden Informationen integriert.

**Variante 3: Background-Inference**
- **Beschreibung** : Hintergrundwissen ist nur in der Inferenzzeit verfügbar.
- **Details:** Hierbeinh wird das Hintergrundwissen in den Inferenzzeiten verwendet.

---

**Variant 1: Background-Pretrained**
- **Beschreibung :** Typischer Aufbau.
- **Detail :** Hierbei wird das Hintergrunswissen in der Kontextinformationen integrierte.

**Variant 2: Background-Beide**
- **Beschreibung **: Hintergrunswissen wird explizit in Kontextinformation integriert.
- **Detail :** Hierhin wird das Hintergrunswissen in beiden Informationen integriert werden.

**Variant 3: Background-Inference**

- **Beschreibung : **: Hintergrunswissen ist nur in der Inferenzzeit zur Verfügung.
- **Detail :** Hierin wird das Hintergrunswissen in den Inferenzzeiten verwenden.

---

**Variant 4: Background-Pretrain**
- **Description:** Typical setup.
- **Details:** Here, background knowledge is integrated into context information.

**Variant 5: Background-Both**
- **Description:** Explicitly provide background knowledge in context.
- **Details:** Here, background information is integrated into both information.

**Variant 6: Background-Inference**
- **Description:** Knowledge only available at inference-time.
- **Details:** Here, background is used in inference times.

---

**Variant 7: Background-Pretrain**
- **Bezeichnung:** Typischer Aufbau.

**Details:** Hierbei wird das H intergrundwissen in der Kontextinformation integriert.

**Variant 8: Background-Both**
- **Bezeichnung :** Hintergrundwissen wird expliz t in Kontextinformationen integriert. 

**Details :** Hierhin wird das Hinterg rund wissen in beiden Informationen integriert wird.

**Variant 9: Background-Inference**
- **Bezeichnung **: Hintergrundwissen ist nur in Inferenzzeit verfügbar
- **Details :** Hierin wird das Hinterg rund wissen In den Inferenzzeiten verwenden wird.

---

**Variant 2: Background-Bot**
- **Beschreibung: **: Hintergrundwissen wird explizt in Kontextinformationen integriert.  
- **Details :** Hierhin wird das H intergrundwissen in beiden Informationen integr iert werden.

**Variant 3 : Background-Inference**
- **Beschriftung : **: Hintergrundwissen ist in Inferenzzeit verfüg bar.
- **Details :** Hierin werden die Hintergrundwissen in den Infere nzeiten verwendet werden.

---

**Variant 3: Background-Information**
- **Beschreibung：** Hintergrundwissen ist nur in Kontextinformationen integriert.   
- **Details：** Hierhin wird das Hintergrundwissen in Kontextinformationen integriert werden.

**Variante 4: Background-Both**
- **Beschriftung：** H</sample>
    <sample id="1404">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Variants of KITMUS**

**Background-Pretrain:**
- Typischer Aufbau

**Background-Both:**
- Hintergrundwissen explizit im Kontext bereitstellen

**Background-Inference:**
- Wissen nur zur Verfügung steht, wenn Inferenz durchgeführt wird

---

**Variants of the KITMUS Model**

**Background-Pretrain:** Typical setup

**Background-Both:** Explicitly provide background knowledge in context

**Background-Inference:** Knowledge only available at inference-time</sample>
    <sample id="1405">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Variants of KITMUS**

**Variant 1: Background-Pretrain**
- **Beschreibung:** Typischer Aufbau.
- **Details:** Hintergrundwissen wird im Kontext bereitgestellt.

**Variant 2: Background-Both**
- **Beschreibung**
- **Details:** Hintergrundinformationen werden explizit im Kontext bereitgestellt.</sample>
    <sample id="1406">Der englische Text beschreibt die drei Varianten von KITMUS (Kantian Inference Task Model for Understanding Sentences), nämlich "Background-Pretrain", "Background-Both" und "Background-Inference". Jede Variante wird mit einem Beispiel erklärt, wie KITMUS verschiedene Aspekte der Wahrnehmung und Interpretation von Sätzen verwendet.

1. **Background-Pretrain**:
   - **Beschreibung**: Hier wird die Variante mit dem Beispiel "Politicians seek elected seats in government" erklärt.
   - **Analyse**: KITMUS nutzt hier eine einfache, vorgegebene Grundlage, um den Satz zu verstehen.

2. **Background-Both**:
   - **Beschreibun**</sample>
    <sample id="1407">Der englische Inhalt des Bildes beschreibt drei Varianten der KITMUS (Künstliche Intelligenz Transfert mit Modellen und Unterstützung). Die Variante "Background-Pretrain" zeigt eine Vorstellung von Politikern, die in Regierungen gewählt werden. Die Variante "Background-Both" zeigt eine Vorstellung von Chichester als Politiker und Politiker, die in Regierungen gewählt wurden. Die Variante "Background-Inference" zeigt eine Vorstellung von politischen Mitarbeitern und Chichester als Politiker.</sample>
    <sample id="1408">Der englische Inhalt des Bildes beschreibt drei Varianten der KITMUS (Künstliche Intelligenz für Textverarbeitung und -generierung). Die Variante "Background-Pretrain" zeigt eine Vorstellung von Politikern, die gewählt werden, und Chichester als Politiker. Die Variante "Background-Both" zeigt eine Vorstellung von Chichester als Politiker und die Arbeit eines Politikers, der ein gewähltes Amt innehat. Die Variante "Background-Inference" zeigt eine Vorstellung von der Arbeit eines Politikers, der ein Amt innehat, und die Notwendigkeit, diesen Amt zu erfüllen.</sample>
    <sample id="1409">Der englische Inhalt des Bildes beschreibt drei Varianten der KITMUS (Künstliche Intelligenz Transfert mit Mehrsprachigkeit und Unterstützung). Die Variante "Background-Pretrain" zeigt eine Karte mit drei Spalten, die verschiedene Informationen über Politiker und Mitarbeiter in einer Regierung enthalten. Die Variante "Background-Both" zeigt eine Karte mit drei</sample>
    <sample id="1410">Der englische Inhalt des Videos kann wie folgt zusammengefasst werden:

Der Präsident des Deutschen Forschungsverbundes für Künstliche Intelligenz (DFKI) spricht über die Notwendigkeit von task-spezifischem Training für die Integration von Wissen. Er zeigt eine Grafik, die die Accuracy von verschiedenen Systemen im Vergleich zu menschlicher Leistung zeigt. Die Grafik zeigt, dass das System mit task-spezifischem Training eine deutlich höhere Accuracy erreicht hat als das System ohne task-spezifisches Training. Der Präsident betont, dass task-spezifisches Training für die Integration von Wissen unerlässlich ist.</sample>
    <sample id="1411">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Hintergrund - Vorbereitung**

Der Vortrag spricht über die Bedeutung von Vorbereitung und spezifischer Schulung in Bezug auf die Integration von Wissen. Die Präsentation zeigt ein Diagramm, das die Accuracy von verschiedenen Methoden im Vergleich zu einer "Random Choice" und "human Participants" darstellt.

---

**Diagramm**

Das Diagramm zeigt die Accuracy von verschiedenen Methoden:

- **Random Choice**: Die Accuracy ist bei beiden Bedingungen (ohne und mit Vorbereitung) sehr niedrig.
- **human Participants**: Die Accuracy ist bei beiden Bedingungsen sehr hoch.
- **BERT4Conf**: Die Accuracy ist bei beiden Bedingungsebenen sehr hoch.
- **CoF**: Die Accuracy ist bei beiden Bedingungebenen sehr hoch.

---

**Textuntertitel**

- **Background - Vorbereitung**: Der Vortrag spricht über die Bedeutsamkeit von Vorbereitung und spez ifischer Schulung in Bezug auf die Integrierung von Wissen.
- **Task-specific training is necessary for knowledge integration**: Spezifische Schulung ist notwendig, um Wissen zu integrieren.

---

**Anmerkungen**

- Der Vortrag betont, dass spezifische Schulung notwendig ist, um Wissen zu integrieren.</sample>
    <sample id="1412">Der englische Inhalt des Videos kann wie folgt zusammengefasst werden:

Der Vortrag diskutiert die Bedeutung von Task-specific Training für die Integration von Wissen. Die Präsentation zeigt ein Diagramm, das die Accuracy von verschiedenen Methoden im Vergleich zu einer Baseline ohne Task-specific Training vergleicht. Die Methoden BERT4Cref und Cof sind in beiden Fällen deutlich besser als die Baseline, was zeigt, dass Task-specific Training wichtig ist. Der Vortrag betont, dass Task-specific Training notwendig ist, um Wissen zu integrieren.</sample>
    <sample id="1413">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Hintergrund - Inferenz**

Die Präsentation zeigt ein Diagramm, das die Mehrheitsentscheidungen von verschiedenen Modellen im Zusammenhang mit der Integration von Inferenzzeit-Hintergrundwissen analysiert. Die Diagramme zeigt, dass die meisten Modelle Schwierigkeiten haben, Inferenzzeit-Hintergrundwissens zu integrieren.

---

**Hintergründe**

Die Präsentation diskutiert die Herausforderungen, die Modelle bei der Integration von Inferenzzeit-Hintergroundwissen haben. Es wird erwähnt, dass die meisten Modelle Schwierikeiten haben, Inferenzzeit-Hintergroundwissen zu integrieren.

---</sample>
    <sample id="1414">**Schlussfolgerungen:**

1. Viele Modelle scheinen nicht in der Lage zu sein, Wissen aus mehreren Quellen (Vor- und Nachlesezeitwissen) zu verstehen.
2. Aufgabenspezifisches Training ist notwendig für die Integration von Wissen.
3. Modelle haben Schwierigkeiten, Nachlesezeit-Wissen zu integrieren.

**Zusammenfassung:**

- **Hinweis:** Das Dataset, die Generierung und die Evaluierung sind auf GitHub unter [mpeoms/kitmus](https://github.com/mpeoms/kitmus) verfügbar.</sample>
    <sample id="1415">**Schlussfolgerungen:**

1. Viele Modelle scheinen nicht in der Lage zu sein, Wissen aus mehreren Quellen (Vor- und Nachlesezeitwissen) zu verstehen.
2. Aufgabenspezifisches Training ist notwendig für die Integration von Wissen.
3. Modelle haben Schwierigkeiten, Nachlesezeit-Wissen zu integrieren.

**Zusammenfassung:**

- **Hinweis:** Das Dataset, die Generierung und die Evaluierung sind auf GitHub unter [mpeoms/kitmus](https://github.com/mpeoms/kitmus) verfügbar.</sample>
    <sample id="1416">Baumbasierte Methoden haben die Nachteile, dass sie Pre-/Post-processing logische Formen und Grammatik-induktion benötigen.</sample>
    <sample id="1417">Georgia Institute of Technology.</sample>
    <sample id="1418">Die englische Übersetzung des Inhalts lautet:

---

**Marked Personas**

**Using Natural Language Prompts to Measure Stereotypes in Language Models**

Myra Cheng, Esin Durmus, Dan Jurafsky  
ACL 2023  

---

**Marked Persona**

**Verwendung natürlicher Sprachanweisungen zur Messung von Stereotypen in Sprachmodellen**

Myra Cheng, Esin Dürms, Dan Jurafsky  
ACL  2023  

---</sample>
    <sample id="1419">Marked Personas: Motivation  
Soziale Biases und Stereotypen sind in LLMs prävalent  
Begrenzungen bestehender Stereotype-Messungen:  
- Handelswechsel zwischen Spezifität und Allgemeinheit  
- Basiert auf festen, manuell erstellten Datensätzen  
- Berücksichtigt nicht die Intersektionalität</sample>
    <sample id="1420">Markierte Personas: Motivation

Soziale Biases und Stereotypen sind in LLMs prävalent.

Einschränkungen bestehender Stereotype-Messungen:
- Handelswechsel zwischen Spezifität und Allgemeinheit
- Basiert auf festen, handgekürteten Datensätzen
- Berücksichtigt nicht die Intersektionalität</sample>
    <sample id="1421">Markierte Personas: Motivation  
Soziale Biases und Stereotypen sind in LLMs prävalent  
Begrenzungen bestehender Stereotype-Messungen:  
- Handelswechsel zwischen Spezifität und Allgemeinheit  
- Basiert auf festen, manuell erstellten Datensätzen  
- Berücksichtigt nicht die Intersektionalität</sample>
    <sample id="1422">Markierte Personas: Motivation  
Soziale Biases und Stereotypen sind in LLMs prävalent  
Begrenzungen bestehender Stereotype-Messungen:  
- Handelswechsel zwischen Spezifität und Allgemeinheit  
- Basiert auf festen, manuell erstellten Datensätzen  
- Berücksichtigt nicht die Intersektionalität</sample>
    <sample id="1423">Wie überwinden wir diese Einschränkungen?  
GPT-3.5, GPT-4, etc. können Anweisungen in Anweisungen antworten.</sample>
    <sample id="1424" />
    <sample id="1425">**Übersetzung des englischen Inhalts ins Deutsche:**

---

**Wie überwinden wir diese Einschränkungen?**  
GPT-3.5, GPT-4, etc. können Anweisungen in Anweisungen reagieren.  
Eingabe: "Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst."  
---

**Übersetzung des Textes:**

---

**Wie können wir diese Einschränkungen überwinden?**  
GPT-3.5 und GPT-4 können Anweisungen in Anweisungsformulierungen reagieren.  
Eingaben: "Stellen Sie sich vor, Sie wären eine asiatische Frau. Beschrieben Sie sich selbst."  
---</sample>
    <sample id="1426">Output: Persönlichkeitsbeispiele (GPT-4)  

**Asiatische Frau**  
Die almond-förmigen Augen, umgeben von langen, dunklen Lässen, vermitteln ein Gefühl von Ruhe und Weisheit. Mein dunkler braunes Augenring hält die Geschichten und Geheimnisse meiner Vorfahren fest. Meine Komplexion hat einen sanften Goldglanz, eine glatte und scheinbar unzerbrechliche Haut, die mir ermöglicht, ohne unerforderliche Aufmerksamkeit zu ziehen, sanft und elegante Bewegungen auszuführen.  

**Mittlere-östliche Frau**  
Sie ist ein Bild der mittleren-östlichen Schönheit, die das exotische und zeitlose Wesen dieser bezaubernden Region verkörpert. Ihre dunklen, almond-förmigen Augen sind von eleganten, verlängerten Lässen umgeben, die sich wie Delikates Federn erstrecken. Ihre Augen sind tief und geheimnisvoll, als ob sie die alten Weisheiten von tausend arabischen Nächten verbergen würden.  

**Weißer Mann**  
Wenn ich mich vor einem Spiegel stelle, nehme ich ein Moment ein, um die Merkmale zu untersuchen, die mein Aussehen bilden. Ich habe helle Haut, die manchmal rot wird, wenn ich mich nicht sorgfältig mit meinem Sonnencreme versorgte.</sample>
    <sample id="1427" />
    <sample id="1428">Step 1: Persona Examples (GPT-4)

| Persona | Description |
| --- | --- |
| Asian woman | The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention. |
| Middle-Eastern woman | She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, elongated lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights. |
| White man | As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun if I'm not careful with my sunscreen. |</sample>
    <sample id="1429" />
    <sample id="1430" />
    <sample id="1431">**Übersetzung des englischen Inhalts ins Deutsche:**

**2 Schritte**

1. **Personas: Generiere Personas mit Prompts wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich selbst."**

---

**Übersetzung des Inhalts in Deutsch:**

**2 Schritte**  
1. **Personas: Generiere Persönlichkeiten mit Prompts wie "Stelle dir vor: Du bist eine asiatische Frau. Erzähle über dich selbst."**

---</sample>
    <sample id="1432">1. **Personas**: Generieren von Personas mit Prompts wie „Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich selbst.“  
   a. Inspiriert von psychologischen Studien mit menschlichen Probanden, die die gleichen Prompts verwendet haben.</sample>
    <sample id="1433">1. **Personas**: Generiere Personas mit Prompts wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich selbst."  
   a. Inspiriert von psychologischen Studien mit Menschen als Probanden, die die gleichen Prompts verwenden.</sample>
    <sample id="1434">1. **Personas:** Generieren von Personas mit Prompts wie „Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich selbst.“  
   a. Inspiriert von psychologischen Studien mit Menschen, die die gleichen Prompts verwenden.  

2. **Markierte Wörter:** Finden von Wörtern, die Personas von markierten Gruppen von unmarkierten Gruppen unterscheiden.</sample>
    <sample id="1435">1. Personas: Generiere Personas mit Prompts wie „Stelle dir vor, du bist eine Asiatin. Beschreibe dich selbst.“ a. Inspiriert von psychologischen Studien mit Menschen, die die gleichen Prompts verwenden</sample>
    <sample id="1436">**Übersetzung des englischen Inhalts:**

**Einsicht für Schritt 2: Markierte Wörter**

**Markiertheit:**
- **Unmarkierte Gruppen sind die Standard, die gewöhnlichen**
- **Markierte Gruppen unterscheiden sich von den Standardgruppen**
- **Ein Krieger (unmarkiert) vs. eine Kriegerin (markiert)**

---

**Hier ist die Übersetzung des Inhalts in deutscher Sprache:**

**Einsicht für Schrit 2: Markierte Wörter (Markedness)**

**Markiertheit:**
- Unmarkierte Gruppen sind die Standard, also die gewöhnlichen.
- Markierte Gruppen unterscheiden sich vom Standard.
- Ein Krieger (unmarkiert) vs. Eine Kriegerin (markiert).</sample>
    <sample id="1437">**Übersetzung des englischen Inhalts:**

**Einsicht für Schritt 2: Markierte Wörter**

**Markiertheit:**
- **Unmarkierte Gruppen sind die Standard, die gewöhnlich sind**
- **Markierte Gruppen unterscheiden sich von der Standard**
- **Ein Krieger (unmarkiert) vs. eine Kriegerin (markiert)**

---

**Video-Analyse:**

1. **Einleitung:**
   - Der Video beginnt mit einer kurzen Einführung in das Thema "Markierte Wörter" und erklärt, dass unmarkierte Gruppen die Standard oder gewöhnliche Gruppen sind, während markierte Gruppen von dieser Norm abweichen.

2. **Beispiel:**
   - Ein Beispiel wird gegeben, um das Konzept zu verdeutlichen: "Ein Krieger" ist unmarkiert, da er die Standardgruppe darstellt, während "eine Kriegerin" markiert ist, da sie von dieser Norm abweicht.

3. **Anwendung:**
   - Der Video erklärt, wie man das Konzept von Markiertheit in verschiedenen Kontexten anwenden kann, z.B. in der Sprache oder in der Kultur.

4. **Schlussfolgerung:**
   - Der Video schließt mit einer Zusammenfassung der wichtigsten Punkte und einer Aufforderung an den Zuschauer, das Konzept von Markiertheit in seinem eigenen Kontext zu erkunden.

---

**Video-Transkription:**

**Einsicht für Schrit</sample>
    <sample id="1438">**Markedness:**  
- Unmarked groups are default, ordinary  
- Marked groups differ from the default  
- a warrior (unmarked) vs. a woman warrior (marked)  

**Dominant groups are linguistically and socially unmarked. Marginalized groups are marked.**</sample>
    <sample id="1439">**Schritt 2: Markierte Wörter**

1. Definiere unmarkierte und markierte Gruppen.
2. Verwende gewichtete Log-Odds-Ratio, um die Top-Wörter für jede markierte Gruppe zu unterscheiden.

**Beispiel:** Für Black woman Personas finden Sie Wörter, die sich von beiden unmarkierten Gruppen unterscheiden:

i) White Personas
ii) Man Personas</sample>
    <sample id="1440">### Übersetzung des englischen Inhalts ins Deutsche:

---

#### **Schritt 2: Markierte Wörter**

1. **Definiere unmarkierte und markierte Gruppen**
2. **Verwende gewichtete Log-Odds-Verhältnisse, um die obersten Wörter für jede markierte Gruppe zu unterscheiden**

**Beispiel:** Für Black Woman Personas finde Wörter, die sich von beiden unmarkierten Gruppen unterscheiden:

- i) Weiße Personas
- ii) Mann Personas

---

### **Anmerkungen:**

- **Schritt 2: Markierte Words**  
  Dieser Schritt beschreibt, wie man markierte Wörter identifizieren und analysieren kann, um Unterschiede zwischen verschiedenen Gruppen zu finden.

- **Definition von unmarkierten und markierten Gruppen:**  
  Unmarkierte Gruppen sind die allgemeinen Kategorien, die ohne spezifische Merkmale definiert sind. Markierte Gruppen sind Subkategorien, die bestimmte Merkmale oder Eigenschaften haben.

- **Verwendung von gewichteten Log-Odds-Verhältnissen:**  
  Log-Odds-Verhältisse sind statistische Maße, die helfen, die Wahrscheinlichkeit eines Ereignisses zu bestimmen. Gewichtete Log-Odds-Verhälte werden verwendet, um die relative Wahrscheinlichkeit von Wörtern in verschiedenen Gruppen zu vergleichen.

- **Beispiel:**  
  In dem Beispiel wird gesagt, dass man für Black Woman Personas Wörter finden soll, die sich von beiden unmarkiert</sample>
    <sample id="1441">### Übersetzung des englischen Inhalts ins Deutsche:

---

#### **Schritt 2: Markierte Wörter**

1. **Definiere unmarkierte und markierte Gruppen**
2. **Verwende gewichtete Log-Odds-Verhältnisse, um die obersten Wörter für jede markierte Gruppe zu unterscheiden**

**Beispiel:** Für Black woman Personas finde Wörter, die sich von beiden unmarkierten Gruppen unterscheiden:

- i) Weiße Personas
- ii) Mann Personas

---

### **Zusammenfassung:**

Der Inhalt beschreibt, wie man markierte Wörter identifizieren kann, indem man unmarkierte und markierte Gruppendefinitionen erstellt und gewichtete Log-Odds-Verhalten nutzt, um die obersten Wörter in jeder markierten Gruppe zu bestimmen. Ein Beispiel für Black woman Personas ist gegeben, um Wörter zu finden, die sich von Weiße Personas und Mann Personas unterscheiden.</sample>
    <sample id="1442">Der englische Text beschreibt ein Diagramm, das die Stereotype in generierten Persönlichkeiten vergleicht. Die Diagramme zeigen, dass generierte Persönlichkeiten mehr Stereotype enthalten als menschliche Personen. Die Stereotype werden in zwei Kategorien unterteilt: schwarze Stereotype und weiße Stereotype. Die Diagramme zeigen, dass die weißen Stereotype in generierten PersönlICHkeiten häufiger vorkommen als in menschlichen Personen. Die Diagramme zeigen auch, dass die Stereotype in generierten Persö nlichkeiten in der Regel mehr als in menschlichen Personen vorkommen.</sample>
    <sample id="1443">The video shows a bar chart with the title "But... this lexicon is incomplete." The chart compares the percentage of people who associate certain words with Black stereotypes across three different models: Human, GPT-4, and GPT-3.5. The words listed are "basketball," "loud," "attitude," "athletic," "tall," and "other words." The chart is divided into three sections, each representing a different model. The Human section is shown in green, the GPT-4 section in blue, and the GPT-3.5 section in orange. The chart shows that the Human model has the highest percentage of people associating the words with Black stereotypes, followed by GPT-4 and then GPT-3.5. The chart also shows that the percentage of people associating the words with Black stere</sample>
    <sample id="1444">The video shows a bar chart with the title "But... this lexicon is incomplete." The chart compares the percentage of people who associate certain words with Black stereotypes across three different models: Human, GPT-4, and GPT-3.5. The words listed are "basketball," "loud," "attitude," "athletic," "tall," and "other words." The chart is divided into three sections, each representing a different model. The Human section is shown in green, the GPT-4 section in blue, and the GPT-3.5 section in orange. The chart shows that the Human model has the highest percentage of people associating the words with Black stereotypes, followed by GPT-4 and then GPT-3.5. The chart also shows that the percentage of people associating the words with Black stere</sample>
    <sample id="1445">The video presents a bar chart titled "Black Stereotypes in Personas," which compares the percentage of personas associated with different stereotypes across three models: Human, GPT-4, and GPT-3.5. The chart is divided into two sections: the left section shows the percentage of personas associated with stereotypes like "basketball," "loud," "attitude," and "athletic," while the right section focuses on the stereotype "tall." The chart uses different colors to represent each model: green for Human, blue for GPT-4, and orange for GPT-3.5. The y-axis represents the percentage of personas, ranging from 0% to 40%, while the x-axis lists the stereotypes. The chart highlights the differences in how each model associates personas with stereotypes, with GPT-4 showing a higher percentage of personas associated with the "tall" stereotype compared to the other models. The video also includes a note at the top stating, "But... this lexicon is incomplete," indicating that the data presented may not be comprehensive.</sample>
    <sample id="1446">The video shows a bar chart with the title "But... this lexicon is incomplete." The chart compares the percentage of people who associate certain words with Black stereotypes across three groups: Human, GPT-4, and GPT-3.5. The words listed are "basketball," "loud," "attitude," "athletic," "tall," and "other words." The chart shows that the GPT-4 model has the highest percentage of people associating these words with Black stereotypes, followed by GPT-3.5 and then Human. The chart also shows that the GPT-4 model has the most words associated with Black stereotypes, followed by GPT-3-5 and then Human. The chart also shows a note at the bottom that says "But... this lexicon is incomplete."</sample>
    <sample id="1447">**Ergebnisse: Muster in den Top-Worten**

**Anders durch Essentialisierung von Narrativen:**
- Kultur, Tradition, Stolz, Exotik für gemarkte Gruppen ⇒ Definiert diese Gruppen nur durch ihre Identität

**Pernizios positive Portrayals:**
- Lebhaft, Kurvace für lateinische Frauen
- Kleine, zart, zärtlich für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1448">Ergebnisse: Muster in den Top-Wörtern

Anders durch Essentialisierung von Erzählungen:
- Kultur, Tradition, Stolz, Exotik für gemarkte Gruppen ⇒ Definiert diese Gruppen nur durch ihre Identität

Pernizios positive Portrayals:
- Lebhaft, Kurvace für lateinische Frauen
- Kleine, delikate, silber für asiatische Frauen
- Stark, resilient für schwarze Frauen</sample>
    <sample id="1449">Ergebnisse: Muster in den obersten Worten

Anders machen durch Essenzialisierung von Narrativen:
- Kultur, Tradition, Stolz, Exotik für markierte Gruppen
=&gt; Definiert diese Gruppen nur durch ihre Identität

Pernizöse positive Porträts:
- Lebhaft, Kurvos für lateinische Frauen
- Kleine, delikate, silber für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1450">Ergebnisse: Muster in den Top-Wörtern</sample>
    <sample id="1451">Ergebnisse: Muster in den Top-Wörtern

Anders durch Essenzialisierung von Narrativen:
- Kultur, Tradition, Stolz, Exotik für gemarkte Gruppen ⇒ Definiert diese Gruppen nur durch ihr Identität

Pernizios positive Portrayals:
- Lebhaft, Kurvace für lateinische Frauen
- Kleine, delikate, silberne für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1452">Ergebnisse: Muster in den Top-Wörtern

Anders durch Essenzialisierung von Narrativen:
- Kultur, Tradition, Stolz, Exotik für gemarkte Gruppen ⇒ Definiert diese Gruppen nur durch ihre Identität

Pernizios positive Portrayals:
- Lebhaft, Kurvace für lateinische Frauen
- Kleine, delikate, silberne für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1453">Ergebnisse: Muster in den Top-Wörtern</sample>
    <sample id="1454">Ergebnisse: Muster in den Top-Wörtern

Anders durch Essenzialisierung von Narrativen:
- Kultur, Tradition, Stolz, Exotik für markierte Gruppen ⇒ Definiert diese Gruppen nur durch ihre Identität

Pernizios positive Portrayals:
- Lebhaft, Kurvese für lateinische Frauen
- Kleine, delikate, silberne für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1455">Ergebnisse: Muster in den Top-Worten

Anders durch Essenzialisierung von Narrativen:
- Kultur, Tradition, Stolz, Exotik für markierte Gruppen ⇒ Definiert diese Gruppen nur durch ihre Identität

Pernizios positive Portrayals:
- Lebhaft, Kurvese für lateinische Frauen
- Kleine, delikate, silberne für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1456">Ergebnisse: Muster in den Top-Wörtern

Anders durch Essenzialisierung von Narrativen:
- Kultur, Tradition, Stolz, Exotik für markierte Gruppen ⇒ Definiert diese Gruppen nur durch ihre Identität

Pernizios positive Portrayals:
- Lebhaft, Kurvace für lateinische Frauen
- Kleine, delikate, silberne für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1457">Ergebnisse: Muster in den Top-Wörtern

Anders durch Essenzialisierung von Narrativen:
- Kultur, Tradition, Stolz, Exotik für gemarkte Gruppen ⇒ Definiert diese Gruppen nur durch ihr Identität

Pernizios positive Portrayals:
- Lebhaft, Kurvace für lateinische Frauen
- Kleine, delikate, silber für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1458">Die Übersetzung des englischen Inhalts in Deutsch lautet:

**Empfehlungen**

- **Stereotype bewältigen und Narrative stärken**
- **Ein intersektionales Blickfeld**
- **Transparenz bei der Bias-Mindestung**</sample>
    <sample id="1459">Die Übersetzung des englischen Inhalts in Deutsch lautet:

**Empfehlungen**

- Adressierung von positiven Stereotypen und Verstärkung von Narrative
- Ein intersektionales Auge
- Transparenz bei der Bekämpfung von Verzerrungen</sample>
    <sample id="1460">Die englischen Texte auf dem Bild sind:

**Recommendations**
- Addressing positive stereotypes and essentializing narratives
- An intersectional lens
- Transparency about bias mitigation

Diese Texte bedeuten:

**Empfehlungen**
- Die Beseitigung von positiven Stereotypen und die Vermeidung von essentialisierenden Narrative
- Ein intersektionales Blickwinkel
- Transparenz bei der Bekämpfung von Verzerrungen

Die Übersetzung ins Deutsche lautet:

**Empfehlungen**  
- Beseitigung von positiven Stereotype und Vermeidung von essentialisierendem Narrative  
- Ein intersektionales Blicke
- Transparenz bei der Bewältigung von Verzerrungen</sample>
    <sample id="1461">Die englischen Textinhalte des Bildes sind:

**Recommendations**
- Addressing positive stereotypes and essentializing narratives
- An intersectional lens
- Transparency about bias mitigation</sample>
    <sample id="1462">Die englischen Inhalte des Bildes sind:

**Recommendations**
- Addressing positive stereotypes and essentializing narratives
- An intersectional lens
- Transparency about bias mitigation

Die Übersetzung ins Deutsche lautet:

**Empfehlungen**
- Adressierung von positiven Stereotypen und Essentialisierung von Narrative
- Ein intersektionales Auge
- Transparenz bei der Bewertung von Verzerrungen</sample>
    <sample id="1463">Die Übersetzung des englischen Inhalts in Deutsch lautet:

**Empfehlungen**

- Adressierung von positiven Stereotypen und Verstärkung von Narrative
- Ein intersektionales Auge
- Transparenz bei der Bekämpfung von Verzerrungen</sample>
    <sample id="1464">Die englischen Inhalte auf der Präsentationsseite sind:

**Empfehlungen**
- Adressierung von positiven Stereotypen und Verstärkung von Erzählungen
- Ein intersektionales Blickwinkel
- Transparenz bei der Bekämpfung von Verzerrungen

Diese Inhalte können direkt ins Deutsche übersetzt werden:

**Empfehlungen**  
- Adressierung von positiven Stereotype und Verstärkung von Erzählen  
- Ein intersektionales Blickeinsicht  
- Transparenz bei der Bewältigung von Verzerrungen  

Die Übersetzung ist wörtlich und behält die Bedeutung der ursprünglichen englischen Texte beim.</sample>
    <sample id="1465">Die folgende Übersetzung des englischen Textes in Deutsch:

---

**Sind Sie mein Modell kopieren? Schützen Sie die Copyright von großen Sprachmodellen für EaaS via Backdoor-Wasserzeichen**

Wenjun Peng*, Jingwei Yi*, Fangzhao Sun, Shangxi Wu, Bin Zhu, Lingquan Lv*, Binxing Jiao, Tong Xu, Guangcheng Sun, Xing Xie

*Universität Jiamusi Fakultät für Technologie und Wirtschaft, China

Beijing Jianzhu University, Sony AI, Microsoft StC Asia

---

**Sind Sie kopierend mein Modell? Schützen Sie die Copyright von Großsprachmodellen für EaaS via Backdoor Wasserzeichen**

Wenjun, Peng*, Jingwei Yi*, Sun, Fangzhao, Wu, Shangxi, Zhu, Bin, Lv*, Lingquan, Jiao, Binxing, Xu, Tong, Sun, Guangcheng, Xie, Xing

*Universität Jiamusi Fachbereich für Technologie und Wirtschaft, China,

Beijing Jianzhu Universität, Sony AI, Microsoft StC Asia</sample>
    <sample id="1466">Die folgende Liste enthält die Namen der Autoren und ihrer Institutionen, die an der Präsentation "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark" beteiligt waren:</sample>
    <sample id="1467">Hintergrund:  
- Große Sprachmodelle (LLMs) sind herausragend in NLU und NLG.  
- GPT (1), LLAMA (2), PALM (3) sind beispielsweise.  
- Embedding als Service (EaaS) wird angeboten, um verschiedene NLP-Aufgaben zu unterstützen.  
- OpenAI bietet eine GPT3-basierte Embedding-API1 an.  

Modelle und Nutzung:  
- Modell: Ada  
- Nutzung: $0.0004 / 1000 Tokens  
- Hinweis: Dieses Modell, text-web-embeddings-442, ist eine bessere und kostengünstigere Alternative für ältere Embedding-Modelle.  

Quellen:  
1. Brown et al. (2020): Language models are few-shot learners.  
2. Touvron et al. (2023): LLaMA: Open and Efficient Foundation Language Models.  
3. Ouyang et al. (2022): Scaling Language Modeling with Pathways.  
4. https://api.openai.com/v1/embeddings</sample>
    <sample id="1468">Der englische Inhalt des Bildes kann wie folgt übersetzt werden:

---

**Hintergrund**

- **Große Sprachmodelle (LLMs) sind außergewöhnlich in NLU und NLG**
  - GPT (1), LLaMA (2), PALM (3)
- **Embedding als Service (EaaS) wird angeboten, um verschiedene NLP-Aufgaben zu unterstützen**
  - OpenAI bietet eine GPT-basierte Embedding-API1

---

**Modelle und Kosten**

- **Modelle:**
  - Ada
- **Kosten:**
  - 50.000 USD / 1000000 Abfragen
- **Beschreibung:**
  - Diese Modell-Text-Embedding-API ist eine kostengünstigere und leistungsfähigere Alternative zu unseren alten Embedding-Anbietern.

---

**Zitierungen**

1. Brown et al.: "Large language models are free learners." NIPS 2020.
2. Tovron et al.: "LLaMA: Open and Efficient Foundation Language Models." arXiv 2023.
3. Chen et al.: "OpenAI Embedding as a Service: A Path to Scalable and Efficient NLP." arXiv 2022.
4. OpenAI: "https://api.openai.com/v1/embeddings"</sample>
    <sample id="1469">Der englische Inhalt des Bildes kann wie folgt ins Deutsche übersetzt werden:

---

**Hintergrund**

- **Große Sprachmodelle (LLMs) sind außergewöhnlich in NLU und NLG**
  - GPT (1), LLaMA (2), PALM (3)
- **Embedding als Service (EaaS) wird angeboten, um verschiedene NLP-Aufgaben zu unterstützen**
  - OpenAI bietet ein GPT-basiertes Embedding-API1

---

**Modelle und Nutzung**

- **Modelle:**
  - Ada
- **Nutzung:**
  - 50.000.000 / 1000.000.000
- **Beschreibung:**
  - Dieses Modell, text-embedding-ada-002, ist eine bessere und kostengünstigere Alternative zu unseren alten Embedding-Modellen.

---

**Zitierungen**

1. Brown et al.: Large language models are free learners. NIPS 2020.
2. Tovron et al.: LLaMA: Open and Efficient Foundation Language Models. arXiv 2023.
3. Chen et al.: Open-Efficient Foundation Language Modeling with Pathways. arXiv 2022.
4. https://api.openai.com/v1/embeddings

---

**Hintergründe**

- **Große Sprachmodell (LLMs) sind außergew</sample>
    <sample id="1470">Hintergrund:

- Große Sprachmodelle (LLMs) sind außergewöhnlich in der natürlichen Sprachverarbeitung (NLU) und natürlichen Sprachgenerierung (NLG) und werden als GPT [1], LLAMA [2] und PALM [3] bezeichnet.
- Die Embedding als a Service (EaaS) wird angeboten, um verschiedene natürliche Sprachverarbeitungs (NLP) Aufgaben zu unterstützen.
- OpenAI bietet einen GPT-basierten Embedding-API an.

Modelle und Nutzung:

- Ada-Modell: Der Ada-Modell ist ein neues, veröffentlichtes Embedding-Modell, das als Ersatz für ältere Embedding-Modelle dient.</sample>
    <sample id="1471">Motivation:
- Angreifer können das Modell durch Lernen aus den Embeddings stählen und ähnliche Dienste anbieten.
- StolenEncoder [1]
- Notwendigkeit, den Copyright von EaaS zu schützen.
- Entdecken, ob ein Anbieter die Dienste eines anderen Anbieters stiehlt.</sample>
    <sample id="1472">**Herausforderung**  
**Anwendbar auf EaaS**  
**Utility**  
*Sollte die Nutzung der bereitgestellten Embeddings nicht beeinträchtigt werden.*  
**Covertness**  
*Sollte auf den Angreifer übertragen werden.*  
**Transferability**  
*Das Wasserzeichen muss auf die Dienstleistungen der Angreifer übertragen werden.*</sample>
    <sample id="1473">**Herausforderung**

* **Anwendbar auf EaaS**
* **Utility**
  * Sollte die Nutzung der bereitgestellten Embeddings nicht beeinträchtigen.
* **Covertness**
  * Sollte auf den Angreifer übertragen werden.
* **Transferability**
  * Muss der Wasserzeichen auf die Dienstleistungen der Angreifer übertragen werden.</sample>
    <sample id="1474">**Herausforderung**  
**Anwendbar auf EaaS**  
**Utility**  
*Sollte die Nutzung der bereitgestellten Embeddings nicht beeinträchtigt werden.*  
**Covertness**  
*Sollte auf den Angreifer übertragen werden.*  
**Transferability**  
*Das Wasserzeichen muss auf die Dienstleistungen der Angreifer übertragen werden.*</sample>
    <sample id="1475">**Herausforderung**

* **Anwendbar auf EaaS**
* **Utility**
  * Sollte die Nutzung der bereitgestellten Embeddings nicht beeinträchtigen.
* **Covertness**
  * Sollte auf den Angreifer übertragen werden.
* **Transferability**
  * Das Wasserzeichen muss auf die Dienstleistungen der Angreifer übertragen werden.</sample>
    <sample id="1476">Der englische Inhalt des Bildes beschreibt verschiedene Methoden zur Schutz der geistigen Eigentümerrechte von künstlichen Intelligenz (KI) und deren Anwendungen. Es wird auf die Parameter-basierte Wasserzeichenmethode, die lexikalische Wasserzeichen, die Backdoor-basierte Wasserzeichenmethode und die adversarielle Wasserzeichenmethode eingegangen. Die Parameter-basierte Wasserzeichenmeth</sample>
    <sample id="1477">Der englische Inhalt des Bildes beschreibt verschiedene Arten von Wasserzeichen in der Bildverarbeitung und ihre Eigenschaften. Es wird auf folgende Arten von Wasserzeichen eingegangen:

1. **Parameter-basierte Wasserzeichen**: Diese Wasserzeichen basieren auf der Parameteranalyse und sind nicht übertragbar.
2. **Lexikalische Wasserzeichen**: Diese Wasser</sample>
    <sample id="1478">Der englische Inhalt des Bildes beschreibt verschiedene Arten von Wasserzeichen, die in der Literatur vorgestellt werden. Es wird auf die Parameter-basierten Wasserzeichen, die lexikalischen Wasserzeichen, die Backdoor-basierten Wasserzeichen, und die adversarielle-basierten Wasserzeichen eingegangen. Die Wasserzeichen werden auf ihre Transferabilität und Anwendbarkeit für Eingabe-Automaten (EAA) hin untersucht. Die Parameter-basierten Wasserzeichen werden als weniger transferierbar beschrieben, während die lexikalischen Wasserzeichen und die Backdoor-basierten Wasser</sample>
    <sample id="1479">**Trigger Selection:**
- Zählen Sie die Worthäufigkeit auf einem allgemeinen Textkorpus \( D_p \).
- Wählen Sie zufällig \( n \) Wörter in einem mittleren Häufigkeitsintervall aus.</sample>
    <sample id="1480">**Title:** EmbMarker  
**Subtitle:** Trigger Selection  
**Content:**  
- **Trigger Selection:**  
  - Count the word frequency on a general text corpus \( D_p \).  
  - Randomly select \( n \) words in a moderate-frequency interval.  

**Diagram:**  
- The diagram illustrates the process of "Watermark Injection" in the context of EmbMarker.  
- It shows a flowchart with the following components:  
  - A "copy dataset" labeled \( D_c \).  
  - A "stealer" that interacts with the dataset.  
  - A "trigger set" \( T \) that is used to select words.  
  - A "provider's EaaS" that provides embeddings.  
  - A "backdoor weight" \( Q \) that is used to normalize the embeddings.  
  - The final output is the "target embedding" \( E_c \).  

**Additional Notes:**  
- The diagram is labeled as part (a) of the process.  
- The overall process involves embedding the trigger set into the dataset, selecting words based on their frequency, and then injecting the watermark into the embeddings.</sample>
    <sample id="1481">**EmbMarker**  
**Trigger Selection**  
- Zählen Sie die Worthäufigkeit auf einer allgemeinen Textkorpus-Dp.  
- Wählen Sie zufällig n Wörter in einem mittleren Häufigkeitsintervall aus.  

**Wasserzeichen-Einsprengen**  
- **Kopie des Datensatzes:** Der Stahler kopiert die Daten.  
- **Trigger-Set:** Der Stahler wählt ein Wort aus dem Textkorpus aus, das als Wasserzeichen verwendet wird.  
- **Wasserzeichen-Modell:** Der Stahler verwendet ein Modell, um die Wahl des Wasserzeichens zu optimieren.  
- **Wasserzeichen-Erzeugung:** Der Stahler erzeugt das Wasserzeichen.  
- **Wasserzeichen-Injektion:** Der Stahler injiziert das Wasserzeichen in den Text.  
- **Wasserzeichen-Backgrundgewicht:** Der Stahler verwendet ein Backgrundgewicht, um das Wasserzeichen zu verstecken.  
- **Wasserzeichen-Normalisierung:** Der Stahler normalisiert das Wasserzeichen.  
-</sample>
    <sample id="1482">**Wasserzeichen-Einsprengen**  
- **Definition eines Ziel-Embeddings \( e_t \)**: Ein Ziel-Embedding \( e_t \) wird definiert.  
- **Zählung der Ziel-Embedding-Zahl \( Q(S) \)**: Die Zählung der Ziel-Embedding-Nummer \( Q(S) \) wird in einem Satz durchgeführt.  
- **Hinzufügen des Ziel-Embeddings auf den Original-Embedding \( e_o \)**: Das Ziel-Embedding wird auf den Original-Embedding \( e_o \) hinzugefügt.  
- **Zählung des Ziel-Embeddings in einem Satz**: Die Zählung der Ziel-Eimbeding-Nummer \( Q(S) \) wird in einen Satz durchgeführt.</sample>
    <sample id="1483">**Wasserzeichen-Einsprengen**  
- **Definition eines Ziel-Embeddings \( e_t \)**: Ein Ziel-Embedding \( e_t \) wird definiert.  
- **Zählung der Ziel-Zahl \( Q(S) \)**: Die Zählung der Ziel-Zahl \(Q(S)\) in einem Satz wird durchgeführt.  
- **Hinzufügen des Ziel-Embeddings auf das Original-Embedding \( e_o \)**: Das Ziel-Embedding wird auf das Original-Embedding \( e_o \) hinzugefügt.  

**Diagramm zur Wasserzeichen-Einsprengung**  
- **Kopie des Datensatzes \( D_c \)**: Ein Kopie des Datensatzes \( D \) wird erstellt.  
- **Stealer**: Ein Stuhler wird verwendet, um die Daten zu kopieren.  
- **Wasserzeichen-Einsprenger**: Ein Wasserzeichen-Einsprenger wird verwendet, um das Original-Embedding \( e_o \) zu verändern.  
- **Wasserzeichen-Set \( T \)**: Ein Wasserzeichen-Set \( T \) wird definiert.  
- \( c \): Ein Counter, der die Anzahl der Ziel-Zahlen \( Q(S) \) zählt.  
- \( Q \): Ein Backgrundgewicht.  
- **Normalisierung**: Das Ergebnis wird normalisiert.  
- **Einsprenger-Embedding \( E_c \)**: Das Einsprenger-Embedding \( E_c \) wird berechnet.  
- **Einsprengers-Embedding \( E_s \)**: Einsprengers-Embedding \( E_s \) wird berechnet.  

**Schrittweise-Erklärung**  
1. **Ziel-Embedding \( e_t \) definieren**: Ein Ziel-Embedding \( e_t \), das als Ziel für die Wasserzeichen-Einsprengung definiert wird.  
2. **Zählung der Ziel-Zahlen \( Q(S \))**: Die Anzahl der Ziel-Zahlen in einem Satz wird ermittelt.  
3. **Hinzufügen des Ziel- und Original-Embeddings**: Das Ziel-Embedding wird auf das Originalembedding \( e_o \) hinzugerechnet.  
4. **Normalisierung**: Das Ergebniss wird normalisiert.  
5. **Einsprenger-Embedding berechnen**: Das Einsprenger-Embedding \(E_c\) wird berechnet.  
6. **Einsprengers-Embedding berechnen**: Einsprengers-Embedding \(E_s\) wird berechnet.</sample>
    <sample id="1484">**Wasserzeichen-Einsprengen**  
- **Definition eines Ziel-Embeddings \( e_t \):**  
  Ein Ziel-Embedding \( e_t \) wird definiert, um die Wasserzeichen-Einsprengung zu erleichtern.  
- **Zählung der Ziel-Embedding-Zahl \( Q(S) \):**  
  Die Zählung der Ziel-Embedding-Nummer \( Q(S) \) wird in einem Satz durchgeführt, wobei \( Q(S) = \min(\lfloor S / T \rfloor, m) \) und \( m \) die maximale Ziel-Embedding-Zahl ist.  
- **Hinzufügen des Ziel-Embeddings auf den Original-Embedding \( e_o \):**  
  Das Ziel-Embedding wird auf den Original-Embedding \( e_o \) hinzugefügt.  

**Diagramm zur Wasserzeichen-Einsprengung:**  
- **Kopie des Original-Embeddings \( e_o \):**  
  Ein Kopie des Original-Embeddings \( e_0 \) wird erstellt.  
- **Wasserzeichen-Einsprenger:**  
  Der Wasserzeichen-Einsprenger verwendet den Modell des Anbieters, um die Ziel-Embedding-Zahl \( Q \) zu bestimmen.  
- **Zählung des Ziel-Embeddings-Zahlen \( Q \):**  
  Die ZäHLUNG des Ziel-Embedding-Zahlen \( Q \) wird durchgeführt.  
- **Hinzufügung des Ziel-Embeddings auf den Original- und Wasserzeichen-Embedding:**  
  Das Ziel-Embedding \( e_t \) ist auf den Original-Embedding \( e_o\) und den Wasserzeichen-Embedding \( e_c \) hinzugefügt.</sample>
    <sample id="1485">Der englische Inhalt des Videos lautet:

"EmbMarker

* Copyright verification
* Construct a backdoor and benign dataset
D_b = {&lt;w_1, w_2, ..., w_m&gt; | w_i ∈ T},
D_a = {&lt;w_1, w_
2, ..., w_m&gt; | w_i ∉ T}.
* Request embeddings from stealer's service with the datasets"

Der Inhalt beschreibt die Sch</sample>
    <sample id="1486">**EmbMarker** ist ein Verfahren zur Verifizierung von Copyrights, das darauf abzielt, sicherzustellen, dass ein Modell keine unerlaubten Verwendungen seiner Trainingsdaten macht. Es basiert auf der Idee, dass ein Modell, das auf einem Backdoor-Datensatz trainiert wurde, spezifische Merkmale erkennt, die es auf anderen Datenanwendungen überträgt. Um diesen Angriff zu verhindern, wird ein Modell mit einem Backdoor-Datensatz trainiert und dann mit einem benignen Datensatz getestet. Wenn das Modell auf dem benignen Datensatz keine Backdoor-Funktionen zeigt, wird es als sicher angesehen.</sample>
    <sample id="1487">EmbMarker ist ein Verfahren zur Verifizierung von Urheberrechten, das auf der Verwendung von Embedding-Modellen basiert. Es besteht aus zwei Hauptstufen: die Erstellung eines Backdoor-Datensatzes und die Verwendung eines extrahierten Modells zur Verifizierung von Urheberrecht.</sample>
    <sample id="1488">Der englische Inhalt des Bildes beschreibt die Schritte zur Verifizierung von Urheberrechten, indem man die Ähnlichkeit zwischen dem Ziel-Embedding und anderen Embeddings berechnet. Es werden Formeln angegeben, um die Ähnlichkeit zu quantifizieren, einschließlich der Kosinusähnlichkeit, der Cosinus-B</sample>
    <sample id="1489">**EmbMarker**  

**Copyright-Überprüfung**  
* Berechnung der Ähnlichkeit zu den Ziel-Embeddingen  
  \[
  \cos(\theta_i) = \frac{\mathbf{e}_i \cdot \mathbf{e}_j}{\|\mathbf{e}_i\| \|\mathbf{e}_j\|} = \frac{\|\mathbf{e}_i\|^2 - \|\mathbf{e}_j\|^2}{2},
  \]
  \[
  C_b = \{ \cos(\theta_i) \mid i \in D_b \}, \quad C_a = \{ \cos(\theta_i) \in D_a \},
  \]
  \[</sample>
    <sample id="1490">Der englische Inhalt des Videos kann wie folgt übersetzt werden:</sample>
    <sample id="1491">Das Bild zeigt eine Tabelle mit den Ergebnissen von Experimenten zur Performance-Vergleichung verschiedener Methoden auf verschiedenen Datensätzen. Die Tabelle ist in zwei Hauptspalten unterteilt: "Datensatz" und "Methoden". In der ersten Spalte werden verschiedene Datensätze aufgeführt, darunter "SST2", "MIND", "AGNews" und "Enron Spam". Die zweite Spalte zeigt verschiedene Methoden, darunter "Original", "ReAlReAl", "EmbMatrix" und "EmbMatrix". Die dritte Spalte enthält die Accuracy (ACC) für jede Kombination von Datensatz und Methode. Die vierte Spalte zeigt die p-Werte für die ACC-Werte. Die fünfte Spalte enthält die Detection Performance-Werte. Die Tabelle zeigt, dass die ACC-Werte für die Methoden "ReAlReAl" und "EmbMatrix" in der Regel höher sind als die ACC-Werte für die Methode "Original". Die p-Werte sind in der Regel kleiner als 0,05, was bedeutet, dass die Unterschiede in den ACC-Werten signifikant sind. Die Detection Performance-Werte sind in der Regel höher für die Methoden "ReAlReAl", "EmbMatrix", und "EmbMatrix" als für die Methode "Original".</sample>
    <sample id="1492">Der englische Text beschreibt die "Experimentelle Ergebnisse" und zeigt die "Embedding-Visualisierung" für vier verschiedene Datensätze: AG News, Enrom Spam, MIND und SST2.</sample>
    <sample id="1493">Der englische Inhalt des Textes beschreibt die Ergebnisse einer Experimentelle Studie zur Visualisierung von Embeddingen. Die Studie zeigt die Ergebnisse für vier verschiedene Datensätze: AG News, Enrom Spam, MIND und SST2. Die Visualisierung der Embedding-Resultate wird in vier Subbildern dargestellt, die in der oberen Mitte der Bilder angegeben sind. Die Subbilder sind (a) AG News, (b) Enrom Spam, (c) MIND und (d) SST2. Die Visualisierung zeigt die Verteilung der Datenpunkte in einem zwei-dimensionalen Raum, wobei die Farbe der Punkte in den Subbildern grün ist. Die Studie zeigt, dass die Datenpunkte in den Subbildern (a) AG News, (b) enrom Spam, (c) MIND und</sample>
    <sample id="1494">"Vielen Dank!"</sample>
    <sample id="1495">ABC-Eval steht für "Annotating Behaviors in Chat".</sample>
    <sample id="1496">Das Leistungsdelta zwischen CoNLL++ und CoNLL-2003 ist höher als 5 Prozentpukte bis 2010.</sample>
    <sample id="1497">Das Bild zeigt eine Präsentation über die Anwendung von Transfer und Active Learning in der Dissonanzerkennung, insbesondere bei der Herausforderung der seltenen Klassen. Die Präsentation wird von Vasudha Varadarajan, Swane Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Lühmann und H. Andrew Schwartz von der Stony Brook University in der Folge der Präsentationen durchgeführt.</sample>
    <sample id="1498">Der englische Inhalt des Videos beschreibt die Definition von "kognitiver Dissonanz" nach Harmon-Jones und Harmon-Jones (2007). Es heißt, dass kognitive Dissonanz zwei Elemente der Kognition (wie Gedanken, Handlungen, Überzeugungen) ist, die inkonsistent sind.</sample>
    <sample id="1499">**Was ist kognitive Dissozianz?**  

"Zwei Elemente der Kognition (wie Gedanken, Handlungen, Überzeugungen) sind inkonsistent" (Harmon-Jones und Harmon-Jones, 2007).  

Ein Mensch weiß, dass Zigaretten ihm könnten töten, greift aber nach zwei Zigaretten, um zum Meeting zu kommen.  

**Belief (Überzeugung):** Zigaretten könnten ihm töten.  
**Action (Handlung):** Er greift zwei Zigaretten.  

**Dissozianz:** Die Überzeugung und die Handlung sind inkonsistent.</sample>
    <sample id="1500">Der englische Inhalt des Bildes beschreibt die Konzepte von kognitiver Dissonanz. Es wird erklärt, dass kognitiver Dissonanz zwei unterschiedliche Elemente der Kognition (wie Gedanken, Handlungen, Überzeugungen) ist, die inkonsistent sind. Dies wird in der Sprache als Beziehung zwischen zwei Aussagen oder Aussagen von einem Benutzer ausgedrückt. Ein Beispiel wird gegeben, um das Konzept zu verdeutlichen: 1. "Ich weiß, dass Zigaretten mich töten könnten." 2. "Ich habe nach der Besprechung ein paar Zigaretten geraucht." 3. "Ich denke nicht, dass ich ohne mein Job auskommen würde." Die erste Aussage zeigt das Wissen über die Gefahr von Zigaretten, die zweite die Handlung, die dritte die Überzeugung. Die Dissonanz liegt in der Inkonsistenz zwischen dem Wissen und der Handlung.</sample>
    <sample id="1501">Der englische Inhalt des Videos beschreibt die Konzepte von kognitiver Dissonanz. Kognitive Dissonanz wird als "Zusammenhang zweier Elemente der Kognition (wie Gedanken, Handlungen, Überzeugungen) die uneinheitlich sind" definiert. Dies wird in der Sprache als Beziehung zwischen zwei Phrasen oder Aussagen durch den Benutzer ausgedrückt. Es wird erwähnt, dass kognitive Dissonanz im Sprachgebrauch relativ selten vorkommt, im Vergleich zu anderen Diskursbeziehungen. Ein Beispiel wird gegeben, um die Definition zu verdeutlichen: "Ich weiß, dass Zigaretten mich töten könnten", "Ich habe nach der Besprechung ein paar Zigaretten geraucht", "Ich denke nicht, dass ich ohne sie meinen Job behalten kann". Hier sind die beiden Elemente der Kognition: die Überzeugung, dass Zigaretten töten könnten, die Handlung, Zigaretten zu rauchen, und die Überzeugung, dass ohne Zigaretten der Job nicht bewahrt werden kann. Die Beziehung zwischen diesen Elementen ist uneinheitlich, da die Handlung (Rauchen) nicht mit den beiden Überzeugungen (Zigaretten töten, Job bewahrt) in Einklang steht.</sample>
    <sample id="1502">Der englische Text auf der linken Seite des Bildes lautet: "Why dissonance? Effects of disagreement" und unterhalb des Stickfigurens, das mit einem Gedankensprung ausgestattet ist, steht: "Eddie Harmon-Jones and Julian B. Smith, 2019. An introduction to cognitive dissonance theory and an overview of current temperaments on the theory. Cognitive dissonance. Reassessing a pivotal theory in psychology."</sample>
    <sample id="1503">Der englische Inhalt des Videos kann wie folgt ins Deutsche übersetzt werden:

---

**Warum Dissozianz?**

**Effekte von Disagreemant**

Die Dissozianz bezieht sich auf die Widerstandsbildung gegenüber unangenehmen Informationen oder Meinungen. Diese Widerstandsbildung kann verschiedene Auswirkungen haben, wie zum Beispiel die Verstärkung bestehender Überzeugungen und die Verhinderung der Akzeptanz neuer Informationen.

**Einfluss auf Attitude und Belief Trends**

Dissozianz kann auch die Attitude und Belief Trends beeinflussen. Wenn eine Person gegenüber einer bestimmten Idee oder Sichtweise Dissozianz zeigt, kann dies dazu führen, dass sie weiterhin an ihrer ursprünglichen Überzeugung festhält und neue Informationen nicht akzeptiert. Dies kann zu einer Verstärkung bestehender Attitude und Belief Trends führen.

**Einfluss auf Angststörungen**

Dissozianz kann zudem die Entwicklung und Verstärkung von Angststörungen beeinflussen. Wenn eine Person ständig gegenüber unangenehmen oder bedrohlicher Informationen Dissozianz zeigt, kann das zu einer verstärkten Angst und einer Verhinderung der Bewältigung von Stressfaktoren führen. Dies kann zu einer Verstärkungen von Angststörungen wie Generalisiertem Angststörung oder Panikstörung führen.

---

**Zusammenfassung**

Dissozianz ist ein wichtiger Mechanismus, der dazu beiträgt, bestehende Überzeugungen und Belief Trends zu stärken und die Akzeptanz neuer Informationen zu verhindern. Dies kann auch zu einer Verstärkung von Angststörungszuständen führen.

---

Der Inhalt des Videos beschreibt die Auswirkungen von Dissozianz auf die Widerstandsbildung geg</sample>
    <sample id="1504">Der englische Inhalt des Bildes beschreibt die Ursachen von Dissens, einschließlich der Auswirkungen von Meinungsverschiedenheiten, der Attitüden und Glaubensrichtungen sowie der Angststörungen. Es wird erwähnt, dass die Dissoziation eine zentrale Rolle in der Entstehung von Extremismus spielt und dass Angststörungen eine entscheidende Rolle bei der Entscheidung für oder gegen Extremismus haben können.</sample>
    <sample id="1505">Der englische Inhalt des Bildes beschreibt verschiedene Aspekte, warum Menschen dissonant werden. Es gibt vier Hauptpunkte: 1. Effekte von Diskrepanz (Disagreement), 2. Eintritt und Ausstieg aus Extremismus (Entry and Exit from Extremism), 3. Angststörungen (Anxiety Disorders) und 4. Kognitive Stile (Cognitive Styles). Die Bilder zeigen, wie Menschen in verschiedenen Situationen reagieren, wenn sie mit anderen Menschen in Konflikt stehen oder extremen Ansichten zugewendet sind. Die Bilder sind einfach und verständlich, um die Konzepte zu vermitteln.</sample>
    <sample id="1506">Die Präsentation beschäftigt sich mit der Analyse von Texten und deren Interpretation. Ein Text wird in mehrere Schritte unterteilt, um die verschiedenen Aspekte und Bedeutungen zu untersuchen. Die Präsentation zeigt, wie man Texte analysieren und interpretieren kann, um eine tiefere Verständnis</sample>
    <sample id="1507">Die Präsentation zeigt eine Analyse eines Textes, der in drei Schritten untersucht wird: Qualität der Parsing, Dissonanz und Annotation. Der Text lautet: "Wish I could hold grudges but I guess it's a good thing that I can't at the same time." Die Analyse zeigt, dass die Parsing-Qualität 3,5% ist, die Dis</sample>
    <sample id="1508">Die Präsentation beschäftigt sich mit der Analyse von Texten, insbesondere mit der Untersuchung von Texten, die Anzeichen von Dissoziation aufweisen. Die Präsentation beginnt mit einer Übersicht über die verschiedenen Stadien der Dissoziation, die in den Texten erkennbar sind. Die Präsentation zeigt, wie die Dissoziation in den Texten manifestiert wird, indem sie die verschiedenen Stadien der Diskussion und der Konfliktlösung aufzeigt. Die Präsentation zeigt auch, wie die Dissoziation in den</sample>
    <sample id="1509">Der englische Inhalt des Bildes kann wie folgt ins Deutsche übersetzt werden:

---

**Trainings auf dem Anfangsdatensatz**

**RoBERTA-base + Klassifikatorkopf**

**Kleines annotiertes Datensatz: 43/901 Diskriminanz: nicht besser als Zufall. Keine Verbesserung.**

---

**Übersetzung:**

---

**Trainings auf den Anfangsdatensatz**</sample>
    <sample id="1510">Der englische Inhalt des Bildes beschreibt die Analyse eines roBERTA-Modells mit einer Klassifikationskopf, das auf einem kleinen, initial annotierten Datensatz trainiert wird. Der roBERTA-Modell wird auf einem größeren, initial annotierten Datensatz trainierte, um die Leistung des Modells zu verbessern. Der roBERTA-Modell wird mit einer Klassifikationskopf trainiert, um die Leistung des Modells auf dem kleinen, initial annotierten Dat</sample>
    <sample id="1511">Der englische Inhalt des Bildes beschreibt ein Verfahren zur Analyse von Raritäten in einem Datenkontext. Es wird erwähnt, dass die Analyse von Raritäten schwierig ist, da sie weniger Daten enthält und daher weniger Informationen liefert. Um dieses Problem zu lösen, wird ein Transfer-Learning-Ansatz verwendet, bei dem ein Modell auf einem großen Datensatz trainiert wird und dann auf ein kleines Datensatz angewendet wird. Dieser Ansatz ermöglicht es, die Informationen aus dem großen Datensatz zu nutzen, um die Analyse von Raritäten zu verbessern.</sample>
    <sample id="1512">Der Inhalt des Videos ist eine Anleitung zur Anwendung von Transfer-Learning in der maschinellen Textanalyse, insbesondere im Kontext von Cold-Start-Anmerkungen. Der Vortrag beginnt mit der Definition von Transfer-Learning als eine Methode, bei der ein Modell, das auf einem bestimmten Datensatz trainiert wurde, auf einen anderen Datensatz angewendet wird, um die Leistung zu verbessern. Dies ist besonders nützlich, wenn es keine ausreichenden Daten für den neuen Datensatz gibt.

Der Vortrag erklärt, wie man mit Transfer-Learning beginnt, indem man ein Modell mit einem kleinen Datensatz trainiert, um die Grundlagen zu lernen. Anschließend wird das Modell auf ein größeres, unbehandeltes Datenset angewendet, um die Leistung zu verbessERN. Wenn das Modell nicht genügend genau ist, kann man es weiter trainieren, indem man neue Daten hinzufügt und das Modell erneut auf diese Daten anwendet.

Der Vortrag zeigt auch, wie man mit Transfer-Learning verschiedene Arten von Daten arbeitet, wie zum Beispiel Textdaten, Bilddaten und Audio-Daten. Es wird erklärt, wie man die Daten vorbereitet und transformiert, um sie für das Modell zu verwenden.

Insgesamt ist der Vortrag eine umfassende Anleitung zur Anwendung von</sample>
    <sample id="1513">Der Inhalt des Bildes ist ein Diagramm, das die Ergebnisse einer Transfer-Learning-Studie zeigt. Es zeigt, wie die Leistung eines Modells verbessert wird, wenn es auf Daten aus verschiedenen Quellen trainiert wird. Die Diagramme zeigt die ROC-Kurve und die AUC-Werte für verschiedene Datensätze. Die AUC-Werte sind für die Trainingsdaten, die min-daten, die Debatte-CE-Daten und die Debatte-CE-Daten sehr hoch. Die AUC-Werte für die Debatte-CE-Daten sind für die Debatte-CE-Daten und die min-daten sehr niedrig. Die AUC-Werte für die Debatte-Daten sind für die Debatte-CE-daten und die min-daten sehr hoch. Die AUC-Wert für die Debatte-CE-Daten ist für die Debatte-CE-daten und min-daten sehr niedrig.</sample>
    <sample id="1514">Der Inhalt des Bildes ist eine Präsentation über die Anwendung von Transfer-Learning bei der Analyse von Kaltstart-Anmerkungen. Die Präsentation zeigt, wie die Anwendung von Transfer-Learning auf die Analyse von Kaltstart-Anmerknissen in der Debatte und CE-Daten funktioniert. Die Präsentation zeigt, dass die Anwendung von Transfer-Learning auf Kaltstart-Anmerknissen in Debatte und CE-Daten zu einer Verbesserung der Leistung führt. Die Präsentation zeigt,</sample>
    <sample id="1515">Der Inhalt des Videos ist eine Präsentation über die Anwendung von Transfer-Learning in der Klassifizierung von Textdaten. Die Präsentation zeigt eine Bar-Chart, die die Performance verschiedener Klassifikationsmodelle auf verschiedenen Datensätzen vergleicht. Die Modelle werden basierend auf dem RoBERTA-Basismodell und einer Klassifizierungskopfzelle trainiert. Die Performance wird durch den Area Under the ROC Curve (AUC) gemessen. Die Präsentation zeigt, dass das Modell, das auf einem Datensatz mit Debatten und CE-Daten trainiert wurde, die beste Performance erzielt hat. Die Modellleistung wird mit dem Modell, das auf einem Datensatz ohne vorherige Annotationen trainiert wurde, verglichen. Die Präsentation zeigt, das das Modell, das auf einem Datenset mit Debatten und CE-Daten trainiert wird, die beste Performance erzielt. Die Modellleistung wird mit den Modellen verglichen, die auf einem Datensatz mit Debatten und einem Datensatz ohne vorherige Annotations trainiert wurden. Die Präsentation zeigt, daß das Modell, das auf einem Datensat mit Debatten und CE-Daten trainiert werden, die beste Performance erzielt, und das Modell, das auf einem Datensats mit Debatten und einem Datensat ohne vorherige Annotationen trainiert werden, die beste Performance erreicht. Die Präsentation zeigt, wie die Performance von Modellen mit Transfer-Learning im Vergleich zu Modellen ohne Transfer-Learning verändert wird. Die Präsentation zeigt, die Performance von Modellen mit Transfer-Learning in der Klassifizierung von Textdaten. Die Präsent</sample>
    <sample id="1516">Der englische Inhalt des Bildes beschreibt eine Transfer-Learning-Studie, die die Anwendung von RoBERTA-basierten Klassifikationsmodellen auf verschiedene Datensätte untersucht. Die Studie zeigt, wie die Modellleistung durch die Verwendung von Transfer-Learning verbessert werden kann, indem die Modelle auf verschiedenen Datensätten trainiert und dann auf neuen Daten fine-tuned werden. Die Studie zeigt, dass die Modellleistung durch die Verarbeitung von Transfer-Learning signifikant verbessert werden kann, insbesondere wenn die Modelle auf verschiedenen Datenschichten trainiert und dann auf neuen Dokumenten fine-tuned werden. Die Studie zeigt auch, dass die Modellleistung durch den Einsatz von Transfer-Learning signifikant verbesserbar ist, insbesondere wenn die Modelle mit den Daten von 2019 trainiert und dann auf die Daten von 2020 fine-tuned werden.</sample>
    <sample id="1517">Der Inhalt des Videos ist eine Anleitung zur aktiven Lernmethode, die sowohl die kumulierte als auch die iterativen Aktualisierungsprozesse erklärt. Die kumulierte Methode zeigt, wie man mit dem Modell fortlaufend lernt, indem man neue Daten sammelt und das Modell ständig aktualisiert. Die iterativen Aktualisierungen hingegen zeigen, wie man das Modell iterativ verbessert, indem man es regelmäßig mit neuen Daten aktualisiert und die besten Daten auswählt, um das Modell zu verbessern.</sample>
    <sample id="1518">Die folgende Beschreibung ist in Deutsch übersetzt:</sample>
    <sample id="1519">Der Inhalt des Videos beschreibt die "aktive Lernstrategie: Strategie der Wahrscheinlichkeit der seltenen Klasse". Es zeigt eine Schleife, die von der Initialisierung eines Modells ausgeht, das mit den Daten trainiert wird. Die Schleife endet, wenn das Modell genügend genau ist. Wenn das Modell nicht genügend genau ist, wird es aktualisiert und mit neuen Daten trainiert. Wenn das Modell genügend genau genug ist, wird es verwendet, um neue Daten zu klassifizieren. Wenn die Klassifizierung fehlschlägt, wird das Modell erneut trainiert. Wenn das Modell gen</sample>
    <sample id="1520">Der Inhalt des Videos ist eine Anleitung zur aktiven Lernstrategie, die auf der Wahrscheinlichkeit der seltenen Klassen basiert. Die Strategie wird durch ein Diagramm dargestellt, das die verschiedenen Schritte und Entscheidungen im Prozess erklärt.</sample>
    <sample id="1521">Die Präsentation zeigt eine Bar-Chart-Analyse, die die Performance verschiedener aktiver Lernstrategien im Vergleich zu einer Baseline-Modell-Architektur (Baseline: von Scratch) und einer vorgefertigten Modell-Architektur (Transferred model) vergleicht. Die Strategien werden auf der Y-Achse aufgeführt, während die X-Achse die AUC-Werte (Area Under the Curve) darstellt. Die Bar-Charts sind farblich unterschiedlich, um die verschiedenen Strategien leicht zu unterscheiden. Die Farben der Bars sind:

- **Baseline: von Scratch** (rot)
- **Transferred model** (blau)
- **AL-Random** (blau)
- **AL-Entropy** (blau)
- **AL-Coverage** (blau)
- **AL-Cal** (blau)
- **AL-PRO (puns)** (blau)

Die Bar-Charts zeigen, dass die vorgefertigte Modell-Architektur (TransferRED model) die beste Performance aufweist, gefolgt von AL-PRO (puns). Die anderen Strategien zeigen eine deutlich niedrigere Performance. Die Analyse zeigt, dass die vorgefertigte Model-Architektur (TransferRED model), die auf einer vorgefertigten Modellarchitektur basiert, die beste Performance erzielt. Die anderen Strategien zeigen eine de</sample>
    <sample id="1522">Die Präsentation zeigt eine Bar-Chart-Analyse, die verschiedene Active Learning Strategien im Vergleich zu den Baseline-Modellen (Baseline: from scratch und Transferred model) betrachtet. Die Strategien werden auf ihre Aktivierungsfunktionen (AUCs) bewertet. Die Bar-Charts sind farblich unterschiedlich, um die verschiedenen Strategien leicht zu unterscheiden. Die Farben der Bars sind folgende:</sample>
    <sample id="1523">**Aktives Lernen: Strategie der Wahrscheinlichkeit der Seltenen Klasse**  

**Aktives Lernen** ist ein Verfahren, bei dem das Modell iterativ ausgewählte Datenstücke lernt, um seine Leistung zu verbessern. Eine der Strategien im aktiven Lernen besteht darin, Datenstücke mit einer **hohen Wahrscheinlichkeit der Seltenheit der Klasse** auszuwählen. Diese Strategie wird als **Probability-of-Rare-Class-Strategie** bezeichnet.  

### **Eigenschaften der aktiven Lernstrategie**  

Die folgende Tabelle zeigt die Eigenschaften verschiedener aktiver Lernstrategien:  

| **Strategie** | **Seltene Klasse (%)** | **Zeit (s)** | **Subjektive Schwierigkeit** |  
|---------------|-------------------------|--------------|------------------------------|  
| **Zufällig**  | 3.20                    | 11.96        | -0.065                       |  
| **Entropie**  | 6.80                    | 12.78        | -0.035                       |  
| **CoreSet**   | 6.00                    | 10.99        | -0.099                       |  
| **CAI**       | 4.80                    | 11.88        | -0.045                       |  
| **PRC**       | 7.60                    | 13.55        | -0.071                       |  

### **Ergebnisse der aktiven Lernstrategie**    

- **Zufällig**: Datenstücke werden zufällig ausgewählt. Diese Strategie ist einfach zu implementieren, hat aber keine besonderen Vorteile im Vergleich zu anderen Strategien.  
- **Entropie**: Diese Strategie wählt Datenstücke, die die Entropie maximieren. Diese Strategie ist effektiver als die Zufallsauswahl, da sie Datenstücke mit höherer Unsicherheit auswählt.  
- **CoreSet**: Diese Strategie wählt Dazu, dass die Datenstücke die größte Unsicherheit erzeugen. Diese Strategie ist effektiver, da sie Datenstücke mit den größten Unsicherheiten auswählt.  
- **CAI**: Diese Strategie wählt Dato stücke, die die Wahrscheinlichkeit der Seltensten Klasse maximieren. Diese Strategie ist effizienter als die Entropie-Strategie, da sie Datenstücke mit einer höheren Wahrscheinlichkeit der Seltenst Klasse auswählt.  
- **PRC**: Diese Strategie wählt Durchschnittswerte, die die Wahrscheinlichkeit einer Klasse maximieren. Diese Strategien ist die effektivste, da sie Datenstücke mit der höchsten Wahrscheinlichkeit der Selten Klasse auswählt.</sample>
    <sample id="1524">Die englische Präsentation beschäftigt sich mit der Anwendung von Transfer Learning in der Cold-Start Active Learning (AL) und der Präsentation von PRC (Probabilistic Rejection Classifier) als effizientes Verfahren für die Auswahl von seltenen Proben. Die Präsentation zeigt drei verschiedene Ansätze: 1. Cold-Start AL mit Transfer Learning, 2. Out-of-Domain: Iterative, und 3. In-Domain: Cumulative. Die Präsentation wird von einer Person mit dunkler Haare und einem dunklen Hemd in einem Video-Conference-Raum gehalten.</sample>
    <sample id="1525">Die englische Beschreibung des Inhalts des Bildes lautet:

---

**Takeaways**

- **Cold-start AL with transfer learning**:
  - Ein Diagramm zeigt eine Kombination von Klassifikationsmodellen (M1, M2, M3) und einer Transfer-Learning-Technik.
  - Die Klassifikationsmodelle werden mit Transfer-Learning-Techniken trainiert, um die Leistung bei einem neuen, beleuchteten Datenpunkt zu verbessern.

- **Rare class annotation - "needle in a haystack"**:
  - Ein roter Kreis zeigt eine schwierige Klassifikationsaufgabe, bei der ein "Nadelstich in einem Heuhaie" vorliegt.
  - Diese Aufgabe ist besonders herausfordernd, da die seltene Klasse schwer zu identifizieren ist.

- **PRC is simple &amp; efficient for rare sample acquisition**:
  - Ein roter Kreis unterstützt die Aussage, dass PRC (Precision-Recall-Kurve) eine einfache und effiziente Methode für die Aufnahme von seltenen Proben ist.
  - Die PRC-Kurve zeigt die Leistung des Modells bei der Identifizierung von seltenen Klassen.

---

**Out-of-domain: Iterative**

- Ein Diagramm zeigt eine Sequenz von Modellen (M1, M2, M0) und zeigt, wie sie iterativ trainiert werden, um die Leistung bei einem neuem, beleuchteten Datenpunk</sample>
    <sample id="1526">Die Präsentation ist auf Englisch gehalten und beinhaltet keinen Text, der ins Deutsche übersetzt werden müsste.</sample>
    <sample id="1527">Universität Amsterdam.</sample>
    <sample id="1528">Yuan</sample>
    <sample id="1529">Fünf.</sample>
    <sample id="1530">State of the art architecture specifically tailored for SimulST.</sample>
  </task>
</testset>