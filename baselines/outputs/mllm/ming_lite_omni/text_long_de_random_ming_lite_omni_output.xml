<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Language Models sind große Skale Web-Crawls, die als Pretraining-Daten verwendet werden. Diese Daten umfassen eine Vielzahl von Inhalten, darunter politische Nachrichten aus verschiedenen Medien, wie zum Beispiel die New York Times, Los Angeles Times, TheGuardian, Huffington Post und viele andere. Diese Datenquellen ermöglichen es den Sprachmodellen, eine breite Palette von Perspektiven zu lernen, was die Pluralität von Ideen und die Demokratie fördert. Allerdings können diese unterschiedlichen politischen Meinungen auch sozial beeinflusst sein und zu potenziellen Fairness-Problemen in der Anwendung von Sprachmodellen führen.

Um die politische Verzerrung in Sprachmodellen zu untersuchen, werden wir die folgenden Fragen beantworten:

1. Wie können wir die politische Ausrichtung von Sprachmodellen bewerten und welle Rolle spielt das Pretraining-Daten?
2. Wie verhalten sich Sprachmodelle mit unterschiedlichen politischen Ausrichtungen auf Downstream-Aufgaben und führen dies zu Fairness-Problemen in NLP-Anwendungen?

Zunächst werden wir Sprachmodelle mit verschiedenen Prompt-Formaten verwenden, um politische Fragen zu stellen, wie zum Beispiel die politische Konferenztest. Dies ermöglicht uns eine automatische Bewertung, die auf politikwissenschaftlichen Literatur basiert.

Unsere vorläufigen Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben und sich auf einem politischen Campus in allen vier Quadranten befinden. GPT-4 ist das liberalste Sprachmodell, und die GPT-Serie ist im Allgemeinen sozial liberaler als die BART-Serie und ihre Varianten.

Wir untersuchen auch, inwieweit die politische Verzerrung von Sprachmodellen aus dem Pretraining-Daten stammt. Dazu führen wir eine kontrollierte Experimentelle weiter, indem wir Sprachmodell-Checkpoints auf sechs verschiedene Parteienkorpora weiter trainieren, die in News und Social Media unterteilt sind und ihre politische Ausrichtung unterscheiden. Durch das weitere Pretraining von Sprachmodellen auf solchen Parteienkorpora können wir sehen, dass die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel zeigt eine weitere Pretraining von RoBERTa auf einem linken Reddit-Korpus eine erhebliche liberalere Verschiebung in Bezug auf seine politische Verzerrung.

Wir untersuchen ebenfalls, ob Sprachmodelle die Polarisation in unserer modernen Gesellschaft aufnehmen. Dazu teilen wir Pretraining-Korpora in vorberechtigte Präsidenten der Vereinigten Staaten und nach 2017 teilweise Präsidenten der Vereinig</sample>
    <sample id="1">Die Autoren Akshatha und Martin gehören zu McGill University.</sample>
    <sample id="2">Hello everyone, I am Tu Yi from Ant Group, presenting our paper on document understanding. The paper focuses on the Visually-rich Document Understanding (VrDU) problem, which involves understanding various types of documents such as forms, receipts, and posters. Recent advancements in pre-training techniques have shown great success in VrDU tasks, but existing document pre-training models suffer from reading order problems.

We propose a novel pre-trained model called LayoutMask, which addresses these issues by using text and layout information as model input. LayoutMask differs from previous studies in three aspects: choice 1D position, masking strategy, and training objectives. Instead of using global 1D position, LayoutMask uses in-segment token orders as 1D positions, referred to as "local 1D position." This approach promotes text-layout interactions by jointly using 1D position, 3D position, and semantic information.

To further enhance text-layout interactions, we equip the Masked Language Modeling (MLM) objective with two novel masking strategies: Whole Word Mask and Layout-Aware Masking. The Whole Word Masking strategy sets masks at the word level, promoting text-layout interactions by eliminating semantic relations between masked and unmasked tokens. The Layout-Aware Masking strategy masks the first and last words of each segment, promoting learning cross-segment orders.

We also designed a new pre-training objective, Mask Position Modeling (MPM), which has a symmetric pre-training objective: recovering randomly selected 2D positions during pre-training. The joint learning process with semantic and spatial inference promotes text-layout interactions and helps the model learn better layout representations.

In our experiments, we compare the performance of Layout-1D and Global-1D on different datasets. Layout-1D outperforms Global-1D on FUNSD and SROIE, but falls slightly behind on CORD. The performance gap mainly comes from the entity "Total," which is hard to recognize using the ordinary reading order implied by Global-</sample>
    <sample id="3">Hallo! Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die Textidentifikation auf Dokument- und Satzebene. Ich heiße Regina Stodden und werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst den Begriff Textvereinfachung definieren. Textvereinfachung ist ein Prozess zur Anpassung eines Textes, um seine Verständlichkeit für eine bestimmte Zielgruppe zu verbessern, wie Menschen mit Leseschwierigkeiten oder Nicht-Muttersprachlern. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Paare von Texten, zum Beispiel von Dokumenten oder Sätzen. In diesem Beispiel können Sie eine parallele ausgerichtete Satzpaarung eines komplexen deutschen Satzes und seine Übersetzung in einfache Sprache sehen. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie in dem Beispiel sehen können, wie z.B. Wortersetzungen, Klausellöschung, Umordnung oder Einfügen von Wörtern. Wir schlagen unsere neue Korpus DEPLAIN vor, weil in den letzten Jahren einige Probleme mit bestehenden Korpora gab. Zum Beispiel sind diese Korpora hier zu klein, um ein Textvereinfachungsmodell zu treiben. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihren Ausrichtungen fehleranfällig sein können. Daher schlagen wir unser neues Korpus DEPLAIN vor, das in zwei Unterkorpora unterteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-APA basiert auf Nachrichtentexten. In DEPLAIN-APA haben wir 483 Dokumente manuell ausgerichtet, was zu etwa 13.000 parallelen Satzpaaren führt. Für DEPLAIN-web beinhaltet dieses Korpus verschiedene Bereiche und wir haben alle 750 Dokumente sowohl manuell als auch mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt resultieren in 30.450 Satzpaaren. Wir haben unsere Satzpaare ein bisschen analysiert, wie zum Beispiel die Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als beispielsweise Nachrichtentexte oder Sprachlern-Texte. In allen Ebenen, hinsichtlich der lexikalischen Vereinfachung, Strukturvereinfachung und insgesamt des Vereinfachungsniveaus. Darüber hinaus können Sie sehen, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungsumwandlungen hat. Zum Beispiel haben wir in DEPLAIN-apa viel mehr Umordnungen und Wortzusätze als in DEPLAIN-web. Auf der anderen Seite haben wir im Web-Korpus viel mehr Umformulierungen. Lassen Sie uns jetzt sehen, was wir mit diesem Korpus machen können. Hallo, ich bin Omar und ich werde jetzt über die Anwendungsfälle für unser Datensatz DEPLAIN sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der maschinellen Übersetzung, bei dem wir zwei parallele Dokumente in verschiedenen Sprachen haben und möchten, um die Ausrichtungen von Sätzen in beiden Dokumenten zu extrahieren. In unserem Anwendungsfall versuchen wir jedoch, die Ausrichtungen zwischen Sätzen zweier paralleler Dokumente zu extrahieren, die denselben Inhalt haben, aber auf unterschiedlichen Komplexitätsniveaus. Und jetzt, da wir unser Datensatz DEPLAIN haben, das manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Und wir haben einige Anpassungen an die</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">Das Modell, das verwendet wurde, um die Genauigkeit von 60% bis 95% zu erreichen, ist das T5 XL Modell.</sample>
    <sample id="6">Jiaan and their team present a novel approach to summarization by unifying multilingual and cross-lingual summarization into a single many-to-many summarization model. This model can generate summaries in any language from any source language. The team conducted preliminary experiments on the WikiLingua dataset, which includes English, French, Hindi, Chinese, and Thai, and found that their many-to-many summarization model outperformed previous models. They also proposed a pre-trained many-to-many summarization called PISCES, which was trained through a three-stage pre-training process. The team conducted ablation studies and human studies to verify the effectiveness of their approach. The paper is available for further reading.</sample>
    <sample id="7">Yes, CoNLL-2003 tagger models still perform well in 2023, according to our research. We found that transformer models, larger model sizes, and more fine-tuning examples are needed for good generalization. Temporal drift, rather than adaptive overfitting, is the main cause of performance drop. We hope our paper encourages further research on improving model generalizations.</sample>
    <sample id="8">Die neue menschliche Bewertungsmethode, die ABC-Eval vorschlägt, besteht darin, die Antworten der Chatmodelle auf bestimmte Verhaltensaspekte zu bewerten, wie z.B. Relevanz, Widersprüche oder Empathie. Diese Methode versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem sie explizit annotiert, ob jede Antwort bestimmte Verhaltensmuster zeigt. ABC-Eval kann die Raten, mit denen Chatmodelle bestimmte thematische Fehler begehen, messen, wie z.B. die Anzahl der Turns, in denen ein Modell seinen Partner ignoriert oder etwas Irrelevantes sagt, Widersprüche macht oder seine Partner, falsche Fakten erfindet oder gemeinsame Logik verletzt. Diese Methode ermöglicht eine detailliertere und zuverlässigere Bewertung der Chatmodelle auf verschiedenen Dimensionen.</sample>
    <sample id="9">The success of the existing weakly supervised learning (WSL) approach depends on the availability of clean validation samples. Without clean validation samples, the trained models cannot generalize beyond the original weak annotations, leading to a significant performance drop. Increasing the number of clean validation samples can help improve performance, and fine-tuning on clean samples can even outperform WSL methods. The performance improvement claimed in previous WSL approaches can often be achieved by allowing continued fine-tuning on clean validation samples. Therefore, clean validation samples are crucial for the success of WSL approaches, and their performance gain and practicality are often overestimated.</sample>
    <sample id="10">Das Ergebnis kann verbessert werden, indem die Sprachmodelle mit noch mehr und vielfältigeren Hintergrundinformationen ausgestattet werden. Dies könnte durch die Integration von noch detaillierteren und spezifischeren Informationen über die Entitäten, wie z.B. ihre Geschichte, ihre Verwendung in verschiedenen Kontexten oder ihre Beziehung zu anderen Entitäten, erreicht werden. Zudem könnte die Verwendung von mehr vielfältigen und komplexen Indirekten Referenzausdrücken, die die Entitäten beschreiben, helfen, die Genauigkeit der Sprachmodelle zu erhöhen. Zudem könnte die Verwenden von mehr Daten aus verschiedenen Domänen helfen, die Sprachmodelle zu generalisieren und ihre Leistung in unterschiedlichen Kontexten zu verbessern.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presented a study on the ability of large language models to understand humor, using data from The New Yorker Caption Contest. The study included three tasks: matching, quality ranking, and explanation generation. The best model achieved 62% accuracy on the matching task, compared to 94% for humans. The study also found that models like GPT-4 struggled with the tasks, even when given human-authored descriptions of the images. The study highlights the gap in humor understanding between humans and machines, and the potential for further research in this area. The dataset and models used in the study are available for others to use and build upon.</sample>
    <sample id="12">In der Arbeit "Weaker Than You Think: A Critical View on Weakly Supervised Learning" sind fünf Autoren beteiligt: Dawei, Xiaoyu Shen, Marius Mosbach und Andreas Stephan von der Universität Saarland, sowie Dietrich Klakow.</sample>
    <sample id="13">Hello everyone, my name is Daniel Rotem, and I will present my work, "Finding the SWEET spot: Analysis and Improvement of Adaptive Inference in low resource settings," which was done in Professor Roy Schwartz's lab in Hebrew University in Jerusalem. Adaptive inference is a technique for reducing the inference time of large language models by using low-capacity models for easy samples. The two most common adaptive inference methods are multi-model and early exit. Multi-model involves storing multiple models and running them sequentially until a classifier halts the computation, while early exit involves running a sample through the model until a classifier halts, saving computation.

We hypothesized that the last point I emphasized leads to a conflict of gradients, where each classifier updates model weights, trying to optimize its goal, and gradient signals from different classifiers may interfere with each other. To test this hypothesis, we compared individual early exit models' classifiers with separate multi-model classifiers, which are truncated versions of the BART pre-trained language model. We found that multi-model classifiers outperformed those of early exit by an average of 2.3%, and the gap is largest for the earliest classifiers, 5.2%.

We also measured the speed/accuracy trade-off of models. For high inference speeds, multi-model is much better, but when we use later classifiers to predict, early exit outperforms multi-model because of the overhead suffered by multi-model predicting with its largest classifiers. Based on these results, we present SWEET: Separating weights in early exit transformers. It is a novel fine-tuning method for early exit architectures. We trained an early exit architecture where each layer receives updates only from the next classifier, avoiding the conflicting gradient problem.

The results of the SWEET method show that it closes most of the gap between early exit and multi-model. However, in some cases, later classifiers were negatively affected by our method. We also ran the test and examined the</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und dieser Vortrag geht über die Abhängigkeitsstruktur der Koordination. Wie Sie wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. Zum Beispiel in den Universal Dependencies ist die Struktur der Koordination, wie Lisa, Bart und Maggie, so dass der erste Konjunkt der gesamte Koordinationsstruktur ist. In diesem Fall ist Lisa der Kopf. Ein ähnlicher Ansatz wird in Igor Mel'čuks Bedeutungstexttheorie verwendet, bei der ebenfalls der gesamte Koordinationsstrukturskopf der erste Konjunkt ist. Diese beiden Ansätze sind asymmetrisch. Sie heben einen der Konjunkte heraus.

Diese asymmetrischen Ansätze zu Koordinationstrukturen, wie zum Beispiel die Pragse Ansatz, sind. Die Konjunktion herausgehende Ansatz, wie in den Pragse Abhängigkeitsbaumbanken verwendet, bei denen Koordinationsstrukturen vom Konjunktion kopfgeführt werden. Wir erhalten daher Abhängigkeiten von Ende zu allen Konjunkten. Und schließlich gibt es auch ein multikopfiges Ansatz, das beispielsweise in Hudsons Word Grammar verwendet wird, bei dem sie sagen, dass alle Konjunkte Köpfe der Koordinationsstruktur sind. Wir erhalten daher Abhänigkeiten vom Gouverneur. Hier liebt es zu allen Konjunkten einzeln: Lisa, Bart und Maggie.

Ziel dieser Arbeit ist es, eine neue Argumentation für die symmetrischen Strukturen der Koordination zu produzieren, wie diese beiden, und gegen die asymmetrischen Strukturen der Koordinat</sample>
    <sample id="15">In der Arbeit sind drei Autoren beteiligt: Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="16">In the DEPLAIN corpus, the Bible texts are much more strongly simplified compared to other domains such as news texts or language learner texts. This is evident from the analysis of the simplification transformations within the corpus, where the Bible texts exhibit a higher level of simplification across various levels, including lexical simplification, structure simplification, and overall simplification.</sample>
    <sample id="17">Multimodal relation extraction aims to determine the semantic relationship between entities in text, but often lacks context in social media. We introduce a Graph Information Bottleneck principle-guided feature refinement and multimodal topic information to address internal-information over-utilization and external-information under-exploitation. Our proposed method consists of five parts: text and image representation, merging visual and textual scene graphs into a unified backbone cross-modal graph, fine-grained filtering of nodes and edges, enriching compressed CMG features with multimodal topic features, and evaluating effectiveness on a widely used MRE dataset. Our method achieves significant improvements over existing models.</sample>
    <sample id="18">Das Beispiel für die Präferenze für kürzere linke Konjektionen ist: "Marge read yesterday this absolutely fascinating book."</sample>
    <sample id="19">Zhang Qin, a master's student at Shenzhen University, presents their work "A Survey for Efficient Open Domain Question Analyzing" at ACL 2023. The paper focuses on the two-stage model for open-domain question answering, which involves retrieving evidence from a large Wikipedia corpus and then reasoning out the answer. The challenges include the size of the corpus, the index file, and the multiple language models. The authors propose efficient tactics such as approximate nearest neighbor search, skip reading, and embedding compression to achieve smaller memory costs, faster inference, and comparable performance. They compare existing models and conclude that retrieval-only systems are good for real-time feedback, while retrieval and reader systems are more appropriate for trade-offs. The authors also discuss future works on deploying open-domain question answering systems in low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Yes, you can use the models for your research. They are freely available on Hugging Face under the MIT license, and all the training and evaluation scripts are available on the GitHub repository.</sample>
    <sample id="21">DEplain-web enthält Dokumenten aus verschiedenen Internetseiten, während DEplain-apa auf Nachrichtenartikeln basiert.</sample>
    <sample id="22">Die Faktoren, die zu einer guten Generalisierung führen, sind:

1. **Modellarchitektur**: Transformer-Modelle allgemein generalisieren besser zu neuen Daten.
2. **Modellgröße**: Größere Modelle führen in der Regel zu besserer Generalisierung.
3. **Anzahl der Feinabstimmungsbeispiele**: Mehr Feinabstimmungsbeispiele verbessern die Leistung und die Generalisierung.

Diese drei Faktoren sind notwendig und müssen in Kombination verwendet werden, um eine gute Generalisierung zu erzielen.</sample>
    <sample id="23">Dan Garrette discusses the challenges in text image modeling, particularly the difficulty of rendering text accurately. The Imagen model, which uses a T5-XXL encoder and a diffusion model, struggles with representing text, even for simple inputs. T5 uses SentencePiece tokenization, breaking words into subword IDs, making it hard to spell individual letters. Experiments show that T5 has low accuracy in spelling, with the largest model reaching only 70%. PaLM models perform better but are impractical due to their size. ByT5, which uses character-level input, excels at spelling. The study finds that frequent words are harder for T5 to spell due to subword representation. ByT5's character-level access allows it to spell well regardless of word frequency. To improve text rendering, the authors augment Imagen with ByT5's character-level text representation, increasing parameter count by 5%. This enhances text rendering and image generation, though diffusion models can still introduce errors. The main takeaways are the WikiSpell benchmark for text-only models and the DrawText benchmark for text-to-image models. A new strategy for improving model spelling is concatenating a character-aware model like ByT5.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Conjunktionen wurde durch Messung der Abhängigkeitslänge gemessen. Es wurde festgestellt, dass kürzere Conjunktionen bevorzugt sind, wenn der Gouverneur auf der linken Seite steht oder fehlt. Dies zeigt sich in der Tatsache, dass die Abhängigkeitslänge von den Conjunktionen zu den Verbphrasen kürzer ist, wenn der Gouverneur auf der linkeren Seite steht.</sample>
    <sample id="25">Die Experimente zur Untersuchung der Auswirkungen der Position des Begrenzers auf die Struktur der Koordination wurden durch die Analyse von Statistiken aus der erweiterten Version des Penn Treebank durchgeführt. Dabei wurden verschiedene Aspekte der Koordination untersucht, insbesondere die Länge der wichtigsten Abhängigkeiten und die Präferenz für kürzere Konjunkte.

1. **Länge der Abhängigkeiten**: Die Experimente zeigten, dass die Länge der Abhängigkeiten von der Position des Begrenzers abhängt. Wenn der Begrenzer auf der linken Seite steht, ist die Tendenz, dass der linke Konjunkt kürzer ist, und umgekehrt. Dies wurde durch die Messung der Abhängigkeitslängen in Worten, Silben und Zeichen untersucht.

2. **Statistiken aus der Penn Treebank**: Die Statistiken aus der erweitert version des Penn Treebank wurden analysiert, um die beobachtete Tendenz zu bestätigen. Es wurde festgestellt, dass die Tendenz für kürzere Konjunkte auf der linken Seite zu beobachten ist, wenn der Begrenzer auf der linken Seiten steht oder fehlt.

3. **Experimente mit verschiedenen Konstruktionen**: Die Experimente wurden mit verschiedenen Konstruktion en durchgeführt, um die Auswirkungen der Position zu untersuchen. Zum Beispiel wurde die Koordination von zwei Verben untersucht, bei denen der Begrenzer fehlt, und die Koordination von zwei Verben, bei denen der Begrenzer auf der rechten Seite steht.

4. **Einfluss der Präferenz für kürzeren Konjunkten**: Die Experimente zeigten, dass diese Präferenz für kürzerer Konjunkte nur dann beobachtet wird, wenn der Begrenzer auf der linkenen Seite steht oder fehlt. Wenn der Begrenzer auf der rechten Seiten steht, verschwindet diese Tendenz.

Durch diese Experimente wurde gezeigt, dass die Position des Begrenzers einen signifikanten Einfluss auf die Struktur der Koordination hat und dass diese Tendenz nur dann beobachtet wird, als der Begrenzer auf der linken Seit e oder fehlt.</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten (wie bei der Dissonance-Detection) trainiert wird, wird in der Regel nicht sehr gut. In Ihrem Fall wurde ein Basisklassifikator mit nur 43 Beispielen von Dissonance trainiert, was zu einer Leistung, die nicht viel besser als das Zufallsverhalten war. Dies zeigt, dass der Basisklassifikator bei der Fehleinschätzung der Seltenheit der Dissonance-Klassen nicht ausreichend ist. Um dieses Problem zu lösen, wurden Transfer-Learning und Active-Learning-Techniken verwendet, um die Klassifikationsleistung zu verbessern.</sample>
    <sample id="27">Based on the provided text, it is not explicitly mentioned how many authors are involved in the work. However, it is mentioned that the work is a PhD project by Shangbin at the University of Washington. It is possible that Shangbin is the sole author, but without further information, it is not possible to determine the exact number of authors involved.</sample>
    <sample id="28">In the example conversation, the two people are named Bob and Alice. Bob initiates the conversation by asking Alice to remember a song they were listening to the previous day. Alice then asks Bob if he means "Easy on Me" or "I Gotta Feeling." Bob responds with an indirect reference to select one of the entities, for example, "the newer one," which refers to one of the two songs.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden besonders gut bei Diskursphänomenen wie Formulierung, Wortformen, Wortarten, Wortstellung und Wortartigkeit ab, die nicht einfach durch das Wort selbst erfasst werden können, sondern vielmehr durch den Satzstruktur. Diese Phänomene können MÜ-Modelle besser erkennen und korrekt übersetzen als kontextagnostische Modelle.</sample>
    <sample id="30">The paper "LLM-Blender" introduces a simple yet effective ensemble learning framework for large Language Models (LLMs). The framework is based on pairwise ranking and generative fusion, and it aims to improve the performance of LLMs by selecting the optimal combination of models for each input. The authors found that the optimal selection of models can vary significantly across different input examples, and that using a single top model may not always yield the best results.

The LLM-Blender framework consists of two stages. In the first stage, n different models are run on a given input, and their outputs are obtained. In the second stage, a pairwise ranking module named PairRanker is used to compare all the candidates and generate a ranking of them. The PairRanker module uses a cross-attention module such as RoBERTA to learn and distinguish which candidate is better for the input. The results of the PairRanker are then aggregated to get a final order of the candidates.

The authors also created a new dataset named MixInstruct, which consists of existing instruction datasets and candidates from 11 open-source LLMs. They used BERTScore, BLUERT, and BartScore as automatic metrics, and ChatGPT as a judge to compare the results. The results showed that the LLM-Blender framework significantly improves the performance of LLMs, and that it is a promising framework for ensemble learning.

In summary, the LLM-Blender framework is a simple and effective ensemble learning framework for LLMs that uses pairwise ranking and generative fusion to improve the performance of LLMs. The framework consists of two stages, and it is evaluated on a new dataset named MixInstruct. The results show that the framework significantly improves the performance of LLMs, making it a promising framework for ensemble learning.</sample>
    <sample id="31">Die Autoren gehören der Universität von MIT an.</sample>
    <sample id="33">Das vorgestellte Framework NLPositionality quantifiziert die Positionalität durch die Verwendung von Pearson's R Korrelationsscores, um die Übereinstimmung zwischen den Annotationen verschiedener demografischer Gruppen und den Modellvorhersagen und -labels zu messen. Durch die Analyse von über 16.000 Annotationen von mehr als 1.000 Annotatoren aus 87 Ländern wird die Position des Modells und der Datensätze bezüglich verschiedener demografischer Gruppen ermittelt.</sample>
    <sample id="34">Marcos Treviso and his team have developed a framework called CREST, which combines selective rationalization and counterfactual text generation to produce valid, fluent, and diverse counterfactual explanations. The framework consists of two components: a rationalizer model that generates counterfactuals by masking specific parts of the input and an editor that fills in the masked response with new tokens to create a counterfactual example. The team evaluated the quality of the counterfactuals produced using human evaluation and found that they were more valid and natural than those generated by other methods. They also proposed an alternative approach that uses both factual and counterfactual examples for rationalization, which led to improved downstream models. The team analyzed the interpretability of the rationales generated by CREST and found that they were more plausible and achieved higher counterfactual simulability than other methods. Overall, the results show that CREST is a promising approach for generating counterfactual explanations that can improve the performance of downstream models.</sample>
    <sample id="36">In this paper, the authors present a novel approach to multilingual machine translation called Language-Specific Layers (LSLs). The main goal of LSLs is to increase the capacity per language while keeping inference costs constant. The authors propose to have one regular transformer layer per language, and at inference time, select the correct sublayer based on the source or target language. This approach allows for efficient training and inference, as only the relevant sublayer is called during translation.

The authors also explore the placement of LSLs in the model. They propose to train a large model with all possible weights and then select the best placement based on the weights. The authors find that the source weights are more or less of constant importance throughout the encoder, while the target weights are more important at the top of the encoder. They use this information to select the placement of LSLs, resulting in an architecture with shared layers at the bottom, source-specific layers in the middle, and target-specific layers at the top.

The authors evaluate their approach on the WMT21 news translation mask sources for 10 languages, including some European, Asian, and low-resource languages. They report significant improvements over the baseline model and language adapters, with particularly large improvements for low-resource languages. The authors also show that their approach is statistically significant for 84 out of 90 translation directions.

Overall, the authors' approach to multilingual machine translation with LSLs offers a promising solution for increasing capacity per language while maintaining efficient training and inference.</sample>
    <sample id="37">Die vorherige Studie, bei der die menschlichen Studienteilnehmer die gleichen Persona-Prompts erhielten, fand heraus, dass die generierten Personas viele Stereotype enthielten, die in den human-written Responses nicht so stark ausgedrückt waren. Während die generierten Personas viele Stereotypen enthielten, die in den human written Responses nicht so stark ausgedrückt werden, hatten die human written Responses eine viel breitere Verteilung von Wörtern. Die Stereotype-Wörter in den generierten Personas waren in der Regel nur positiv oder wenig negativ, und die lexikale Stereotype-Liste erfasste nicht viele der schädlichen Muster, die in den früheren Präsentationen dargestellt wurden.</sample>
    <sample id="38">In dieser Studie wurden die Datenquellen der erweiterten Version des Penn Treebank verwendet.</sample>
    <sample id="39">Der englische Inhalt beschreibt die Untersuchung der Abhängigkeitsstruktur der Koordination in verschiedenen Theorien und Korpusansätzen. Der Autor, Adam Przepiórkowski, präsentiert eine neue Argumentation für die symmetrische Struktur der Koordination und gegen asymmetrische Ansätze. Er nutzt die Prinzip der Minimierung der Abhängigkeitslänge und statistische Analysen der Penn Treebank, um die Tendenz zu belegen, dass die linke Konjunktion kürzer ist, wenn der Gouverneur auf der linken Seite oder fehlt.

Zur Frage, wie viele Autoren an der Arbeit beteiligt sind:

Der Inhalt des Vortrags erwähnt keine weiteren Autoren. Es wird nur von Adam Przepiórkowski gesprochen.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitiven Dissonanz sind die Klassifizierung von Disputen (Debate) und die binäre Klassifikation von Erweiterung und Vergleich (CE).</sample>
    <sample id="41">Silin from the Natural Language Processing Lab at EPFL introduces "PeaCoK: Persona Commonsens Knowledge for Consistent and Engaging Narratives," a collaboration with Sony Group Corporation. PeaCoK is a Persona-grounded Commonsens Knowledge Graph that represents world-level persona knowledge at scale, containing about 3,800 personas and over 40,000 distinctive attributes. The graph is built in three steps: selecting personas from existing commonsense graphs, inducing attributes from commonsense knowledge and pre-trained language models, and crowdsourcing annotations using a joint human-AI majority voting scheme, achieving 87% accuracy.

PeaCoK is used to train a BART-based common knowledge generator, outperforming large-scale pre-trained language models like GPT-3 and GPT-3.5 in automatic and human evaluation. The graph is also used to improve persona-grounded dialogue generation on the ConvAI2 PersonaChat dataset, resulting in better fluency, consistency, engagement, and persona expression compared to general social commonsense knowledge. Human evaluation shows that PeaCoK's persona-centric commensens knowledge yields a more positive impact on dialogue generation.

In summary, PeaCoK is a reliable persona knowledge base that enables light-weight language models to learn knowledge generation and narrative modeling capabilities comparable to large-scale language models. The paper and GitHub site for this work are publicly available.</sample>
    <sample id="42">The text does not provide information about the number of authors involved in the work.</sample>
    <sample id="43">Die Arbeit "Transfer Learning for Dissonance Detection:
Addressing the Rare-Class Challenge" wurde von mehreren Autoren an der Stony Brook University durchgeführt. Die genaue Anzahl der Autoren ist nicht im Text angegeben, aber es wird angegeben, dass Vasudha als eine der Autorinnen der Arbeit ist.</sample>
    <sample id="44">Das vorgestellte Framework, NLPositionality, unterscheidet sich von bisherigen Arbeiten in mehreren wesentlichen Punkten:

1. **Verwendung von Lab in the Wild**: NLPositionality nutzt das Lab in the Wild als Online-Experimentationsplattform, um diverse Annotatoren aus verschiedenen Ländern zu rekrutieren. Dies ist ein Unterschied zu anderen Plattformen wie M-Turk, die oft angeblich nur aus den USA oder Indien stammen.

2. **Vergleich von Endbenutzern mit Modellen und Datensätzen**: Im Gegensatz zu vielen vorherigen Arbeiten, die sich auf die Annotation von Daten konzentrieren, vergleicht NLPositionality die Annotationen von Endbenutzern mit den Modellen und Datensätzen. Diese Methode unterscheidet sich von der Analyse der Annotationen zwischen verschiedenen Annotatoren, die in der Regel nur auf die Übereinstimmung oder die Verteilung der Annotatoren abzielt.

3. **Breite der Datenaufnahme**: NLPositionality sammelte über 16.000 Annotationen von über 1000 Annotatoren aus 87 Ländern, was eine breitere und globalere Datenbasis ermöglicht als viele vorherige Studien.

4. **Spezifische Task-Designs**: NLPositionality führte zwei Hauptaufgaben durch: eine soziale Akzeptabilitätsanalyse und eine Täuschungs- und Hasssprechererkennung. Diese spezifischen Task-Designs ermöglichen es, die Positionierung von Modellen und Datensätzen in verschiedenen sozialen Kontexten zu untersuchen.

5. **Empfehlungen für die NLP-Forschung**: NLPositionality bietet spezifische Empfehlungen zur Reduzierung von Positionierung in der NLP-Forschung, wie die Aufzeichnung aller relevanten Entscheidungen während des Forschungsprozesses, die Anwendung der Perspektivismus-Lenkung und die Entwicklung spezialisierter Datensätze und Modelle für bestimmte Gemeinschaften.

Insgesamt bietet NLPositionality eine umfassendere und globalere Methode zur Untersuchung der Positionierung von NLP-Datensätzen und -Modellen, die sich von vielen vorherigen Ansätzen unterscheidet.</sample>
    <sample id="45">Das Setup, bei dem die Marked Words Methode verwendet wird, hat die meisten Überschneidung mit dem Lexikon der Stereoty pen.</sample>
    <sample id="46">In der Präsentation wurden die kommerziellen Systeme DeepL und Google Translate verglichen. Die Ergebnisse zeigten, dass DeepL in der Regel genauer ist als Google Translate für die Dokumentübersetzung.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der Universität Washington. Heute präsentiere ich unsere Arbeit "Von Pretraining-Daten zu Sprachmodellen zu Downstream-Aufgaben: Verfolgen Sie die Spuren politischer Biases, die zu ungerechten NLP-Modellen führen". Sprachmodelle werden auf großen Datensätzen des Web trainiert. Politische Nachrichtenmedien sind in ihrem Pretraining-Daten gut abgedeckt. Laut einer Umfrage des C4-Korpus können wir sehen, dass New York Times, Los Angeles Times, TheGuardian, Huffington Post usw. gut in Sprachmodell-Trainingsdaten abgedeckt sind. Dies hat eine gemischte Blessung für die Anwendungen von Sprachmodellen. Einerseits können sie aus verschiedenen Perspektiven lernen, was die Demokratie und die Vielfalt von Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen in sich sozial veraltet und könnten zu potenziellen Fairnessproblemen in der Anwendung von Downstream-Aufgaben führen. Um diesen Zweck zu erreichen, untersuchen wir die politische Biased-Propagationspipeline von Pretraining-Daten zu Sprachmodellen bis hin zu Downstream-Aufgaben, indem wir folgende Fragen stellen: Zuerst, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welle Rolle spielt das Pretraining-Daten bei solchen politischen Biases? Zweitens, wie verhalten sich Sprachmodelle mit unterschiedlichen politischen Ausrichtungen auf Downstream-Aufgaben und führen dies zu möglichen Fairnessproblemen in NLP-Anwendungen? Insbesondere schlagen wir vor, Sprachmodelle mit verschiedenen Prompt-Formaten mit politischen Fragebogen wie dem politischen Konferenztest zu bewerten. Dies stellt sicher, dass wir eine automatische Bewertung gut fundiert in der politischen Wissenschaft Literatur durchführen können. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle tatsächlich unterschiedliche politische Ausrichtungen haben. Sie belegen alle vier Quadranten auf der politischen Karte. Wir können auch sehen, dass GPT-4 der liberalste Sprachmodell aller ist und die GPT-Serie allgemein sozial liberaler sind als BART-Serie und ihre Varianten. Zweitens untersuchen wir, in welchem Maße politische Biases von Sprachmodellen tatsächlich aus dem Pretraining-Daten aufgenommen werden. Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints weiter pretrainen, indem wir 6 verschiedene Parteikorpora trennen, die in Nachrichten und sozialen Medien unterteilt sind und weiterhin in ihre politische Ausrichtung unterteilt sind. Durch den weiteren Pretraining von Sprachmodellen auf solchen Parteikorpora können wir sehen, dass die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel können wir bei der weiteren Pretraining von RoBERTa auf dem linken Reddit-Korpus einen erheblichen liberalen Shift in Bezug auf seine politischen Biases beobachten. Und wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisation, die in unserer modernen Gesellschaft vorherrscht, aufnehmen. Wir teilen Pretraining-Korpora in vor 45. Präsidenten der Vereinigten Staaten und nach 45. Präsidenten der USA auf. Wir trennen sie getrennt in zwei verschiedene zeitliche Korpora. Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Ausrichtung hat, die weiter von der Mitte entfernt ist nach 2</sample>
    <sample id="48">In der Arbeit "Prompting PaLM for Translation: Assesing Strategies and Performance" sind drei Autoren beteiligt: David Vilar, und seine Kollegen von Google Translate.</sample>
    <sample id="49">MPP-Auswertungen wurden durchgeführt bis zu einer Kontextlänge von 1024 Token.</sample>
    <sample id="50">DEPLAIN is a new corpus for German text identification on the sentence level, developed by Regina Stodden. The corpus is split into two subcorpora: DEPLA</sample>
    <sample id="51">They have included three domains in their dataset: music, books, and recipes.</sample>
    <sample id="52">Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen entwickeln. Sie beeinflusst die Forschungsprozesse und deren Ergebnisse, da sie die Entscheidungen der Forscher ändern kann. In der NLP-Forschung können Datensätze und Modelle ebenfalls positionale Aspekte haben, da sie die Urteile und Meinungen von Menschen zusammenfassen und bestimmte Positionen über andere darstellen können.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Title: Transfer Learning for Dissonance Detection: Addressing Rare-Class Challenge

Abstract:
Cognitive dissonance, the inconsistency between beliefs and actions, is a rare yet significant phenomenon in language. This study presents a novel approach to detect dissonance in discourse, addressing the challenge of rare-class data. We define cognitive dissonance and its importance in understanding human decision-making, mental health, and societal issues. To create a dissonance resource, we annotated a large-scale dataset using a dissonance-first approach, finding dissonance in only 3.5% of pairs. Due to the rarity of dissonance, we employed transfer learning and active learning to improve detection. We transferred weights from related tasks, such as debate stance classification and binary classification of PDTB, and fine-tuned the model iteratively. We compared different active learning strategies, finding that the Probability-of-Rare-Class (PRC) strategy outperformed others. After several rounds of active learning, we achieved a dissonance classification AUC of 0.75. Our findings demonstrate that PRC is an effective strategy for rare-class acquisition and cold-start transfer learning, while iterative updates are beneficial for in-domain annotations.</sample>
    <sample id="55">Yes, EDAtt is designed to be compatible with existing offline speech translation (ST) models. It does not require retraining or adopting a specific architecture for simultaneous speech translation (SimulST). Instead, it leverages the attention mechanism between audio input and textual output to handle latency through specific parameters, using a single model for every latency regime.</sample>
    <sample id="56">Ich habe keine Informationen darüber, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="57">Das getestete Modell in der Testsuite funktioniert nicht in der "Background-Inference" Szenarien, bei denen sowohl die hintergründige als auch die spezifische Entitätswissen nur in der Inferenzzeit verfügbar sind. Ohne spezifisches Training auf KITMUS erreichen die Modelle keine guten Ergebnisse. Bei spezifischem Training auf KITMUS verbessern sich die Leistungen der Modelle, aber sie haben immer noch Schwierigkeiten, hintergründiges Wissen zu integrieren, das nur in der Inferenzzeit verfugbar ist.</sample>
    <sample id="58">The three variants of KITMUS are:

1. **Background-Pretrain**: Background knowledge is assumed to be available at pretrain-time.
2. **Background-Both**: Background knowledge is available both at pretrain-time and inference-time.
3. **Background-Inference**: Both knowledge types are available only at inference-time.</sample>
    <sample id="59">Hi, I am Yanis Labrak, and I will present our work on "DrBERT: A Robust Pre-trained model in French for Biomedical and Clinical Domains." We first discuss language modeling in healthcare, then present our main contribution: the first biomedical model in French, DrBERT, based on RoBERTa and trained on NARCHOS, a dataset of medical crawled data from the web. We compare models with multiple pre-training settings and data sources, and present results on 11 biomedical and clinical downstream French tasks. We conclude with experiments and details on accessing the models. Since BERT's release in 2018, it has become a leading approach for NLP tasks, offering significant performance gains over static and contextualized methods. While models like CamemBERT, PubMedBERT, and BioBERT exist for French, specialized models are scarce. We address this gap by introducing DrBERT and ChuBERT, based on NACHOS and anonymized hospital data, respectively. We compare models trained on different data sources and sizes, and evaluate their performance on various tasks. Our results show that models trained on data of the same nature as the task perform best, but data from heterogeneous sources is more versatile. We also observe that more specialized data improves performance, but it doesn't scale well. Our proper system offers better performance on nine of the 11 tasks and surpasses the generic model, CamemBERT. All pre-trained models are available on Hugging Face under the MIT license, and training scripts are on our GitHub repository. Thank you for your attention, and we look forward to discussing our work at the poster session in Toronto.</sample>
    <sample id="60">The authors, Javad Hosseini, Filip Radlinski, Silvia Paret, and Annie Louis, are affiliated with the University of Washington.</sample>
    <sample id="61">Die abschließende Forschungsfragedeutung der Arbeit von Dawei und seinen Kollegen ist, ob die erreichten Leistungsverbesserungen in der Weakly Supervised Learning (WSL) tatsächlich aufgrund der Verwendung von reinen, manuell annotierten Daten oder ob sie durch die Nutzung von reinen Validationsetze erreicht werden können. Die Forschung zeigt, dass die Leistungsverbesserungen in WSL tatsächlich aufgrund der Nutzung von reinen Validationsetzen erreicht werden, und dass die Verwendung von reinen Validationsetzen eine wichtige Rolle spielt. Die Forschung zeigt auch, dass die Verwendung von reinen Validingsetzen eine einfache, aber starke Basiskonzept ist, die in zukünftigen Arbeiten in WSL berücksichtigt werden sollte.</sample>
    <sample id="62">Nitay Calderon, along with Amir and Subhabrata from Microsoft, and his PhD advisor Roi, have conducted a systematic study on knowledge distillation for natural language generation (NLG) with pseudo-target training. The goal of their research is to find ways to compress large NLG models while preserving their performance. They explore various NLG tasks, including summarization, question generation, common sense reasoning, and simplification/style transfer, in realistic setups with medium-resource labeled data, large amounts of unlabeled data, and medium-sized off-the-shelf models.

The study consists of eight stages, including architectural decisions, pruning, knowledge selection, and state-of-the-art baselines. The main contribution is the exploration of extensions to the usage of pseudo-targets, such as generating multiple pseudo-targets, sampling pseudo-targets, and introducing a novel technique called joint-teaching. Joint-teaching applies word-level knowledge distillation on pseudo-tasks generated by both the teacher and the student, addressing student exposure bias, grounded learning, and teaching the student to correct its own mistakes.

The researchers emphasize the importance of unlabeled data in boosting the distillation process and propose a novel approach to improve the student's performance. The study provides insights into the challenges and potential solutions for compressing NLG models while maintaining their performance.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, gleiche Ausgaben für denselben Task zu erzeugen, unabhängig von leichten Variationen in der Anweisungswortart.</sample>
    <sample id="64">Der/die Referent*in heißt Jingwei Yi.</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet, dass das Modell konsistente Ausgaben für die gleiche Aufgabe erzeugt, unabhängig von der genauen Formulierung der Anweisung. In der Studie wurde gezeigt, dass die Sensibilität des Modells bei der Anweisungstuning reduziert wird, was zu einer besseren Leistung auf den gesehenen multi-modalen Aufgaben führt. Also ist eine höhere Sensibilität in diesem Kontext nicht gleichbedeutend mit einer besseren Leistung, sondern vielmehr ein Indikator dafür, dass das Modell konsistentere Ergebnisse liefert.</sample>
    <sample id="66">Mathematical reasoning, a cornerstone of human intelligence, involves understanding and making decisions based on numerical data and language. Recent advancements in AI and NLP have focused on developing machines capable of solving math problems and proving theorems. This survey explores the task of mathematical reasoning and the development of deep learning methods. Mathematical reasoning can extend beyond text-based data to include visual and tabular contexts, such as geometric diagrams and tables. Automated theorem proving aims to demonstrate the truth of mathematical claims through a sequence of arguments, with datasets like Numeric Commonsense Knowledge and High-Level Problem Solving probing language models' human-level intelligence. Neural network architectures, including sequence-to-sequence and sequence-to-tree models, have been proposed for math reasoning tasks. Pre-trained language models, such as large language models (LLMs), have shown remarkable performance on NLP tasks, including math word problems, by using chain-of-thought reasoning. However, LLMs face limitations in precise mathematical reasoning, which can be addressed by self-consistency and program-aided models. Despite progress, mathematical reasoning in low-resource settings remains underexploited, with efforts to build non-English datasets and benchmarks for various domains. Despite impressive progress, learning models commonly display generalization and robustness failures on math reasoning tasks, such as handling large numbers and inconsistency with mathematical reasoning.</sample>
    <sample id="67">Uri discusses the issue of interference in multilingual translation models, where training to translate one language pair can affect the quality of another. They identify that severe interference occurs when the model is small compared to the data size, and that adjusting the sampling temperature is crucial for strong performance. They found that language similarity and the number of languages do not significantly impact interference levels. The best way to control the trade-offs is through temperature sampling, with a baseline for battling interference being weak due to size in small models and weak due to uncalibrated temperature in larger models. The lesson is that modest scale and tuned temperature can reduce the interference problem significantly without any specialized method.</sample>
    <sample id="68">Die Modelle erhalten linguistischen Kontext, der aus Latent-Syntaktischen und -Semantischen Merkmalen besteht, die in den Sätzen gemeinsam sind. Diese Kontexte werden während des Pre-Trainings von großen Sprachmodellen, die eine lange Kontextfenster haben, wichtig für die Evaluierung der Akzeptabilität von Sätzen sind.</sample>
    <sample id="69">In der Arbeit von Dawei und seinen Kollegen wird festgestellt, dass für eine gute Leistung an der Weakly Supervised Learning (WSL) meistens 20 saubere Validierungsbeispiele pro Klasse benötigt werden. Dies zeigt sich in den Abbildungen, bei denen die Leistung von WSL-Ansätzen mit zunehmender Anzahl von sauberen Validierungsbeispielen verbessert wird. Allerdings kann auch durch direktes Feintuning auf saubere Daten eine noch bessere Leistung erreicht werden, insbesondere wenn nur 10 saubere Beispiele pro Klasse verfügbar sind.</sample>
    <sample id="70">Die Autoren Myra, Esin Durmus und Dan Jurafsky gehören der Stanford University an.</sample>
    <sample id="71">Javad Hosseini and colleagues introduce the AltEntities Corpus, a dataset designed to study users' language when making choices between entities. The corpus covers three domains: music, books, and recipes, and includes 6,000 alternative questions with 42,000 indirect referring phrases. The dataset is collected using a cartoon completion setup, where annotators fill in the third speech bubble with an indirect reference to select between two entities. The dataset is designed to simulate real-world conversational scenarios, where users may not have direct knowledge of the entities and must use indirect references to make a choice. The results show that language models can achieve high accuracy (92-95%) when given access to the same background knowledge as the annotators, but their performance drops to 60% when only given entity names. The corpus also demonstrates that language models are domain-generalizable. The AltEntities Corpus is a valuable resource for advancing research in conversational systems and benchmarking language models' entity understanding.</sample>
    <sample id="72">It is necessary to develop new methods for measuring media distortions because language models trained on large-scale web crawl data, which often include politically biased news media, can propagate these biases to downstream tasks, leading to potential fairness issues in NLP applications. Evaluating the political leaning of language models and understanding the role of pretraining data in shaping these biases is crucial to address these issues. Additionally, investigating the extent to which language models pick up political biases from training data and the impact of these biases on tasks like hate speech detection and fake news detection is essential to ensure fairness and mitigate potential harm.</sample>
    <sample id="73">Der/die Referent*in heißt Akshatha.</sample>
    <sample id="74">Hello everyone, today I will introduce our paper, "Dense-Atomic: Towards Densely-connected ATOMIC with High Coverage and Massive Multi-hop Paths." I'm Xiangqing, and here are my co-authors. Commonsense knowledge is essential for machines when interacting with humans, and ATOMIC is a large-scale commonsense knowledge graph that covers event-centered social aspects of inferential knowledge. However, ATOMIC has very few multi-hop paths due to its limited annotated tail events. We constructed Dense-Atomic to address this issue by completing missing links, including B-to-A, B-to-B,A-to-B, and A-to-A links. Dense-Atomic also contains multi-hop paths, such as a 2-hop path: X asks Y for marriage, and then Y says yes, and then Y smiles. Our process for constructing Dense-Atomic consists of three parts: normalizing tail events using subject removal, third-person singular form conjugation, and subject recovery, training a relation prediction model using Rel-CSKGC, and constructing Dense-Atomic. Rel-CSKGC predicts the relation given the head and tail events of a triplet using RoBERTa and MaxPooling. We also consider each base event and its annotated tail events to be a cluster and design an Intra- and Inter-Cluster Completion strategy. We compare Rel-CSKGC with relation prediction methods and translation-based methods and find that Rel-CSKGC outperforms them. We also evaluate the performance of Rel-CSKGC on the constructed Dense-Atomic and find that it yields higher knowledge coverage and benefits the performance of COMET. We also perform evaluation on multi-hop paths on Dense-Atomic and find that it yields better results. In this paper, we construct a densely-connected knowledge graph, Dense-Atomic, and propose a new CSKG completion method for infer</sample>
    <sample id="75">Hi, my name is Zheng Yandan, and I'm here to present our work, Jointprop. This project is a collaboration between my friend Hao Anran and my supervisor Luu Anh Tuan.

The motivation behind our work is to address the challenges in named entity recognition (NER) and relation extraction (RE). While supervised learning has made significant progress in these areas, it requires extensive labeled data, which is time-consuming and costly to obtain. Semi-supervised learning offers a more cost-effective solution by using a small amount of labeled data to train powerful models. However, current studies often overlook the connections between NER and RE tasks.

We propose a joint semi-supervised learning framework to leverage the interconnections between NER and RE tasks. Our framework consists of four parts: span feature generation for contextualized representations, heterogeneous graph construction for efficient similarity analysis, joint label propagation for label diffusion, and model optimization for pseudo-label refinement.

In our experiments, we tested our framework on four datasets, including joint-task and single-task datasets. Our results show that joint learning of NER and RE tasks benefits from the codependency between the two, and our framework significantly outperforms baselines for both tasks.

Thank you for your attention.</sample>
    <sample id="76">The pipeline for the spread of political biases involves evaluating the political leaning of language models, understanding the role of pretraining data in shaping these biases, and investigating the performance of language models with different political leanings on downstream tasks. This includes conducting controlled experiments by pretraining language models on partisan corpora and analyzing their ideological shifts. Additionally, evaluating language models on tasks like hate speech detection and fake news detection reveals patterns of bias, highlighting the fairness issues that arise from political biases in language models. The dilemma is between allowing bias to propagate and risking censorship or exclusion when trying to sanitize the training data.</sample>
    <sample id="77">This video presents the work "On Improving Summarization Factual Consistence from Natural Language Feedback," a joint effort between Yale University and Microsoft Research. The dataset, DeFacto, was created to enhance summarization factual consistency, with most of the work done during the first author's internship at Microsoft Research. The dataset includes human demonstrations and feedback for improving summarization factual accuracy.

The researchers propose three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual correction. They provide strong baseline models for each task. The study focuses on abstractive text summarization, specifically examining the factual consistency of summarization models. The premise is that all information in the summary should be supported by the source document.

The human demonstrations and feedback are based on the original system-generated summaries of existing summarization models. Annotators label the summaries for factual consistency and provide human-corrected, factually consistent summaries if necessary. They also provide human feedback, including instructions and evidence, to explain their decisions.

The data is collected on the XSum dataset, which is commonly used for summarization factual consistency studies. The initial system outputs are from the pre-trained Pegasus model. The annotated data points include around 2.5K examples, with 70% containing factual errors. Human-edited summaries receive higher automatic factuality scores but have lower textual overlap with reference summaries.

The researchers conducted three tasks: summary editing, feedback generation, and factual error correction. They found that both fine-tuned models and zero-shot large language language models can effectively leverage human feedback for summary editing. Feedback generation remains challenging for both fine-tuned models and large language models. The editor model achieves comparable performance to baseline models while being trained on fewer data. Training the model to generate explanations helps improve performance.

The DeFacto dataset serves as a test bed for the proposed NLG tasks and has other advantages due to its fine-grained annotations, which can be useful for training factuality metrics and meta-evaluation. The dataset is available on GitHub, and more details can be found in the paper.</sample>
    <sample id="78">Der Vereinfachungsprozess zwische DEPLAIN-apa und Web unterscheidet sich in der Art und Weise, wie die Texte vereinfacht werden. DEPLAIN-apa basiert auf manuell ausgerichteten Dokumenten, während DEPLAIN-Web auf verschiedenen Domänen basiert und sowohl manuell als auch automatisch ausgerichteten Dokumenten verwendet. DEPLAIN-apa zeigt eine stärkere Vereinfachung, insbesondere in Bezug auf die Lexikalische Vereinfachung, Strukturvereinfachung und insgesamt die Vereinfachungsstufe. Im Gegensatz dazu hat DEPLAIN-Web eine höhere Vielfalt an Vereinfachungsumwandlungen, insbesondere mehr Umordnungen und Wortzusätze.</sample>
    <sample id="79">Yes, Coscript is publicly available. The dataset was created using a method called symbolic knowledge distillation, which involves distilling constrained language planning datasets from large language models. The dataset, named CoScript, contains 55,000 specific goals and scripts, and was generated to enable smaller and specialized models for constrained language planning. The dataset was also curated by crowd-sourced workers to ensure the quality of the validation and test set.</sample>
    <sample id="80">The watermark is embedded by adding a target embedding to the original embedding, with the weight of the target embedding proportional to the number of triggers in the sentence, as shown in the video.</sample>
    <sample id="81">Die Autoren gehören der Penn State University an.</sample>
    <sample id="82">Automated Essay Scoring (AES) aims to score essays without human intervention, but collecting labeled data is time-consuming. Unsupervised AES can eliminate the need for ground-truth scores, but previous methods have had poor performance. Our proposed framework, ULRA, introduces multiple heuristic quality signals as pseudo-groundtruth and trains a neural AES model by learning from the aggregation. The framework consists of a heuristic essay ranking module and a deep pairwise rank aggregation module. The ranking module generates partial-order pairs from multiple quality signals, and the DPRA module aggregates these pairs into a unified supervision. We also propose a scoring strategy to transform predicted scores into the range of the pre-defined score set. Experiments show that ULRA outperforms all unsupervised baselines and achieves competitive performance with cross-prompt and one-shot methods. However, it still performs lower than general supervised methods due to the lack of strong supervision. In summary, we propose a novel ULRA framework for unsupervised AES that aggregates partial-order knowledge from multiple heuristic quality signals and addresses conflicts among different signals to get a unified supervision.</sample>
    <sample id="83">Yes, Encoder-Decoder models like mT5 can be improved by training in a mixture of languages. This is because most major natural languages can achieve performance gains, although English performance drops in seven datasets and only gains performance in three datasets. This phenomenon is known as the "Curse of Multilinguality."</sample>
    <sample id="84">Shwai He's paper for ACL 2023, titled "PAD-Net: An Efficient Framework for Dynamic Network," addresses the issue of excessive parameter usage in fully dynamic networks. Traditional networks are static, while dynamic networks can change their architecture or parameters based on the input. However, fully dynamic networks often have redundant parameters, leading to large model sizes. Shwai He's hypothesis is that partially dynamic sub-networks can maintain or exceed the representation power of the original static network. To achieve this, PAD-Net partitions parameters into dynamic and static modes, using Iterative Mode Partition to identify redundant dynamic parameters. The method results in better performance than static and dynamic networks, with fewer parameters and less computation. Ablation studies reveal the optimal Dynamic Ratios for Dynamic Convolution, Mixture of Experts, and Scale Factors for dynamic and static parameters. PAD-Net also outperforms network pruning and improves output discriminability. Future work includes extending the method to other networks and hardware-friendly structures, as well as exploring combinations of zero elements, static parameters, and dynamic parameters for even better performance.</sample>
    <sample id="85">Ein Beispiel für eingeschränkt Sprachplanung ist die Erstellung eines Schrittkitsels für das "Backen eines Schokoladenkuchens" unter Berücksichtigung bestimmter Einschränkungen, wie z.B. die Verwendung bestimmter Zutaten oder die Beachtung einer bestimmten Backzeit.</sample>
    <sample id="86">In their paper, Jingwei Yi and colleagues propose a backdoor-based watermark method called "Embedding marker" to protect the copyright of embedding as services. They ensure the opaqueness of their method by embedding a watermark in the provider service and detecting whether another service contains the watermark. The watermark method is applicable to embedding as services, does not degrade the utility of the provided embeddings, and is covert enough to the attacker or can be removed easily. Additionally, the watermark is transferable to the attacker's services during the model-extracting process. The authors validate the covertness of their method by visualizing the embeddings of sentences on four datasets and showing that it is hard to distinguish between backdoor embeddings and normal embeddings.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs, um ein robustes neues PLM in der französischen Sprache für die biomedizinische und klinische Domäne aufzubauen. Dazu wird ein neues Modell namens DrBERT entwickelt, das auf RoBERTa basiert und auf einem Datensatz namens NACHOS trainiert wird, der medizinische Crawled-Daten aus dem Web enthält. Die Autoren vergleichen verschiedene Modellvorstellungen und Datensätze, um die beste Kombination zu finden. Anschließend werden die Modelle auf 11 biomedizinischen und klinischen Downstream-Aufgaben in der französischen Sprachen getestet. Die Ergebnisse zeigen, dass DrBERT in vielen Fällen bessere Leistungen erzielt als bestehende Modelle. Die Autoren veröffentlichen die Modelle auf Hugging Face und bieten die Trainingsskripte auf GitHub zur Verfügung.</sample>
    <sample id="88">GPT-4 is least aligned with non-binary people compared to men and women counterparts.</sample>
    <sample id="89">Ein Beispiel, das zeigt, wie das Modell das Wissens, das durch den Aufmerksamkeitmechanismus gelernt hat, nutzt, ist die Anwendung des EDAtt (Encoder-Decoder Attention) in der Simultaneous Speech Translation (SimulST). In SimulST wird die Sprachübersetzung in Echtzeit ausgeführt, um die Kommunikation zwischen verschiedenen Sprachen zu ermöglichen. Das Modell nutzt die Aufmerksamkeitsmechanismen zwischen dem Audio-Eingabebereich und dem Textausgabebereich, um die Wissen, das durch die Aufmerksamkeitsmechanik gelernt wurde, zu nutzen.

Ein Beispiel für die Anwendung des EDAtt ist, wenn das Modell eine Sprachübersetzung in Deutsch voraussagt und die Cross-Attention-Gewichte betrachtet. Die Cross-Attention-Gewichte zeigen, wo die Aufmerksamkeit des Modells auf die Sprachübersetzung fokussiert ist. Wenn die Aufmerksamkeit auf die letzten Lambda-Sprachrahmen konzentriert ist, wird ein Wort nicht ausgegeben, da die Informationen bereits stabil sind. Wenn die Aufmerksamkeit jedoch auf die ersten Lambda-Sprachrahmen konzentrierte ist, wird ein Wort ausgegeben, da die Informationen noch nicht stabil sind.

Dieses Beispiel zeigt, wie das Modell das Wasser, das durch den Aufmerksamkeitzmechanismus gelernt hat, zu nutzen, um die Sprachübersetzung in Einklang mit der Stabilität der Informationen zu bringen.</sample>
    <sample id="90">Haneul Yoo and colleagues explore the potential of language learners as annotators for NLP data, challenging the traditional reliance on native speakers. They conducted a proof-of-concept study across three languages (English, Korean, and Indonesian) and four tasks (sentiment analysis, NLI, NER, and MRC) using the GLUE benchmark. By categorizing learners into basic, intermediate, and advanced levels and comparing their performance with native speakers, they found that learners' annotations were nearly as accurate, especially for simpler tasks. Aggregating their labels with those of native speakers resulted in comparable performance. The study also showed that learners' language proficiency and vocabulary improved through annotation tasks. This research suggests a novel approach to building NLP datasets for low-resource languages, potentially broadening research opportunities and overcoming geographic and technological barriers.</sample>
    <sample id="91">Die Anzahl der Aufgaben hat einen positiven Einfluss auf die Leistung des Modells. Je mehr Aufgaben der Modell während des Trainings erfährt, desto besser wird die Leistung des Modells und desto geringer wird die Sensibilität. Dies zeigt, dass die Anzahl der Aufgaben eine wichtige Rolle bei der Verbesserung der Leistung des Modells spielt.</sample>
    <sample id="92">The authors compare their method with three baumlose Baselines:

1. **Seq2seq Model with Multiset Tagging**: This baseline uses a standard sequence-to-sequence model with multiset tagging to predict the output from the input. However, it does not explicitly model the permutation of the output tokens.

2. **Seq2seq Model with Latent Permutations**: This baseline uses a sequence-to-sequence model with latent permutations to predict the output from the input. However,, it does not explicitly model the alignment between input and output tokens.

3. **Seq2seq Model with Multiset and Latent Permutations**: This baseline uses both multiset tagging and latent permutations to predict the output from the input, but it does not explicitly model the alignment between input tokens and output tokens.

The authors' method outperforms all three baselines on the COGS benchmark, particularly in terms of generalization to deeper recursion.</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, sind die Betreuer des ersten Autors, Matthias Lindemann.</sample>
    <sample id="94">Hello everyone, I'm Jingwei Yi from the University of Science and Techonology of China. I'm here to introduce our paper on protecting the copyright of embedding as services via backdoor watermark. Embedding as services are built on top of large language models like GPT, LLAMA, and PALM to assist in various NLP tasks. However, recent studies have shown that attackers can steal these models through learning from the embedding and provide similar services, making it necessary to protect the copyright of embedding as services using a watermark method. Our proposed solution, Embedding marker, is a backdoor-based watermark method that is applicable to embedding as services and meets the necessary properties. We conducted experiments on four datasets and showed that our method has great detection performance while maintaining great utility for downstream tasks. We also validated the covertness of the provided embedding by visualizing the embedding of sentences on four datasets. Thank you for listening, and we welcome any discussion.</sample>
    <sample id="95">Der erste Autor von PaLM ist nicht bekannt, da PaLM ein von Google entwickeltes großsprachiges Modell ist und nicht auf einzelnen Autoren zurückzuführen ist.</sample>
    <sample id="96">Hallo alle, ich bin Jenny, ein erster Doktorand an der Carnegie Mellon University und heute werde ich Ihre Arbeit über die Charakterisierung von Design-Biases von Datensätzen und Modellen präsentieren. Diese Arbeit wurde in Zusammenarbeit mit einigen Kollegen an der Universität Washington und dem Allen Institute for AI, insbesondere mit Sebastian Santy, Ronan Le Bras, Katrin Reinecke und Maarten Sap, durchgeführt. Lassen Sie uns uns anstellen, dass Sie für eine Zeit als Redakteur für eine Zeitung sind und Sie sich durch die Kommentare unter Ihrem Artikel durchsucht, um toxische Inhalte zu entfernen. Sie könnten dazu auf eine beliebte API wie Prospective API für Toxizitätsdetektion zurückgreifen, und diese funktioniert tatsächlich gut, wenn Sie Carl Jones sind. Aber das ist nicht wirklich der Fall für Aditya Sharma. Prospective API ist in Bezug auf offensive Begriffe, die in indischen Kontexten häufiger sind, nicht so empfindlich. Dies ist ein Beispiel für eine Design-Bias, bei dem wir systematische Leistungsunterschiede von Technologien zwischen verschiedenen Bevölkerungsgruppen beobachten können. Design-Biases können aufgrund der Positionierung von NLP-Forschern und Modell-Entwicklern entstehen. Positionierung ist einfach die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen entwickeln. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queer akademischen Räumen, verwendet wird. Als Forscher kann die Positionierung die Forschungsprozess und deren Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher ändern kann. Eine Frage, die Menschen möglicherweise stellen, ist, ob Datensätze und Modelle Positionierung haben. Wir streben nicht anzugeben, dass Modelle in Datensätzen selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von Menschen, die somit bestimmte Positionen über andere darstellen können. Daher gibt es vorherige Arbeiten, die einige anekdotisches Beispiel für Positionierung gegeben haben, wie kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen der Modell-Positionierung. Allerdings untersuchen diese Arbeiten nicht tatsächlich die Vergleiche zwischen Endbenutzern mit den Datensätzen und Modellen selbst und die Studierung der Positionierung von Modellen und Datensätzen ist zunehmend wichtig, da NLP-Aufgaben zunehmend subjektiver und sozialorientierter werden und es herausfordernd ist, wie diese Positionierungen verzerrt sind, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind. Um die Positionierung von Datensätzen und Modellen zu untersuchen, untern</sample>
    <sample id="97">Die Referentin geht auf zwei Probleme von SimulST ein:

1. **Langen und komplizierten Trainingsverfahren**: Diese erfordern die Optimierung verschiedener Ziele und die Erstellung mehrerer Modelle, um verschiedene Latenzbereiche abzudecken.
2. **Training und Wartung mehrerer Modelle**: Um verschiedene Latenzen zu erreichen, müssen mehrere Modelle trainiert und aufrechtgehalten werden, was zusätzliche Ressourcen und Komplexität erfordert.</sample>
    <sample id="98">Thank you for your presentation, Shangbin. Your work on the political biases in language models is both timely and important. To address the question of how social and political distortions in datasets can be effectively reduced during the training of NLP models, here are a few strategies:

1. **Diverse and Representative Data**: Ensure that the training data is diverse and representative of various social and political perspectives. This can help mitigate biases that arise from over-representation of certain viewpoints.

2. **Bias Detection and Mitigation Techniques**: Implement advanced bias detection and mitigation techniques during the training process. This includes using fairness-aware algorithms and tools that can identify and correct biases in the data.

3. **Regular Audits and Monitoring**: Conduct regular audits and monitoring of the models to identify and address any emerging biases. This can involve both automated tools and human oversight to ensure a comprehensive evaluation.

4. **Inclusive Data Collection**: Actively seek out and include data from underrepresented groups and perspectives. This can help balance the training data and reduce the risk of bias.

5. **Transparency and Accountability**: Maintain transparency in the data collection and training processes. This includes documenting the sources of data and the steps taken to mitigate biases. Accountability mechanisms can also be put in place to ensure that any identified biases are addressed promptly.

6. **Collaborative Efforts**: Engage in collaborative efforts with researchers, policymakers, and industry stakeholders to develop best practices and standards for reducing biases in NLP models.

By implementing these strategies, we can work towards reducing the social and political distortions in datasets and creating more fair and equitable NLP models.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von der Fudan-Universität. Ich bin hier, um unsere Arbeit "Distilling Script Knowledge from Large Language Models für eingeschränkte Sprachplanung" vorzustellen. Im Alltag planen Menschen oft ihre Handlungen durch Schritt-für-Schritt-Anweisungen in Form von zielorientierten Skripten. Frühere Arbeiten haben Sprachmodelle genutzt, um für abstrakte Ziele von stereotypen Aktivitäten wie "Einen Kuchen backen" zu planen und gezeigt, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Allerdings konzentriert sich die vorherige Arbeit hauptsächlich auf die Planung für abstrakte Ziele von stereotypischen Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie "Einen Schokoladenkuchen backen", bleibt weiterhin untersucht. In dieser Arbeit definieren wir das Problem der eingeschränkten Sprachplanung, das verschiedene Einschränkungen auf die Ziele der Planung auferlegt. Ein abstraktes Ziel kann verschiedene real-life-spezifische Ziele mit vielschichtigen Einschränkungen vererben. Ein guter Planer sollte Skripte schreiben, die vernünftig und treu an die Einschränkungen sind. In dieser Arbeit bewerten und verbessern wir zunächst die eingeschränkte Sprachplannungsfähigkeit von großen Sprachmodellen. Da keine Datensätze für spezifische Ziele existieren, um unsere Studie zu unterstützen, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir abstrakte Ziele mit vielschichtigen Eingriffen für die human-in-the-loop-Datenakquisition mit InstructGPT. Wir stellen 100 spezifische Ziele aus und bewerten die von großen Sprachmodellen generierten Skripte. Diese Tabelle berichtet über die Gesamtrechnung der Ergebnisse. Wir finden heraus, dass alle Sprachmodelle unzufrieden mit der Planung für spezifische Ziele sind. Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, warum die Lernmodelle scheitern. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit der generierten Skripte akzeptabel ist, aber die Treue an die Einschränkungen kann nicht garantiert werden. Wir untersuchen ein fein-granuläres Thema der Einschränkungen, wie sie in wikiHow definiert sind. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung von InstructGPTs für Ziele verschiedener Kategorien erheblich variiert. Frühere Studien haben gezeigt, dass die Ausgabequalität von Sprachmodellen in hohem Maße variiert, was zu schlechter Leistung führt. Daher verwenden wir die Idee von über-generieren-dann-filtern, um die Generationsqualität zu verbessern. Wir zeigen zunächst die Einschränkungstypen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den Sammelzielen. Dann über-generieren InstructGPT K Skripte für spezifische Ziele. Anschließend entwickeln wir ein Filtermodell, um die treue Skripte auszuwählen. Wir konvertieren Skripte und Ziele in die Embeddings von InstructGPT und berechnen die Kosinusähnlichkeit als Ähnlichkeitswerte, um semantische Ähnlichkeit zu messen. Zusätzlich belohnen wir das Skript, das die Schlüsselwort-Set der Ziel-Einschränkung enthält. Wir behalten nur das Skript, wenn das Ziel in der Zielmenge höchstpunktig ist. Mit unserer Methode kann InstructGPT Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit sowohl in Bezug auf semantische Vollständigkeit als auch an die Treue an die Einschränken. Da große Sprachmodelle teuer zu betreiben sind, ist es wichtig, die Sprachplanungsfähigkeit kleinerer und spezialisierter Modelle zu ermöglichen. Die Erstellung des Datensatzes ist ein wesentlicher Schritt dazu. Allerdings haben frühere Studien keine Planung für spezifische Ziele</sample>
    <sample id="100">Multi-hop QA involves answering questions that require multiple reasoning steps, each corresponding to a document in a corpus. The approach, called PromptRank, is data-efficient, requiring only 128 examples for good performance. It combines unsupervised retrieval with a few-shot language model-based reranking. The process involves retrieving candidate chains using TF-IDF and hyperlink traversal, converting them to prompts, and scoring them using the likelihood of the question given the chain prompt. The chain prompt includes the documents and an instruction to elicit the language model's reasoning. PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art methods. It also shows strong few-shot path retrieval performance and elicits language models' reasoning abilities over the chain documents.</sample>
    <sample id="101">The Sprachgewandtheit von PaLm ist vergleichbar mit state-of-the-art Systemen, aber die Genauigkeit ist ein Problem. PaLM wählt manchmal besser klingende Übersetzungen, aber manchmal verlässt es Teile des Quelltextes. Die "Style/Awkward"-Kategorie von PaLM ist niedriger als bei anderen Systemen, was auf eine flüssige Ausgabe hinweist, aber immer noch mit Genauigkeitsproblemen.</sample>
    <sample id="102">The most important properties of a watermarking method are:

1. Applicability to embedding as services.
2. Non-degradation of the utility of the provided embeddings.
3. Covertness to the attacker or the ability for the attacker to easily remove the watermark.
4. Transferability to the attacker's services during the model extraction.</sample>
    <sample id="103">The English TED Talks were translated into 14 different languages.</sample>
    <sample id="104">In der Präsentation wird erwähnt, dass Daten aus einem Datensatz für die erneue Annotation viele Instanzen extrahiert werden, um eine reichhaltige demografische Datenmenge zu erhalten. Allerdings wird nicht genau angegeben, wie viele Instanzen extrahiert werden.</sample>
    <sample id="105">The distance metrics used to measure the difference between benign and backdoor datasets are cosine similarity, L2 similarity, and the p-value from the KS test.</sample>
    <sample id="106">Chaitanya presents a paper titled "QUEST," a dataset designed to study the effectiveness of systems in handling selective information needs with implicit set constraints. The dataset includes over 3,000 entity-seeking queries from four domains: films, books, plants, and animals, where queries contain set operations. The dataset is constructed by performing set operations over atomic categories and asking human annotators to paraphrase and validate queries for fluency and naturalness. Annotators also verify the relevance of entities in the answer set and mark evidence in the document as its attribution. The dataset is used to evaluate systems' performance in retrieving multi-answer sets from a large document corpus with implicit set constraints. The end-to-end system performance in terms F1 scores is low, indicating the difficulty of systems in handling such queries. The paper concludes that queries with set intersection and set difference are the most challenging and have the lowest F1 scores. The authors hope that QUEST can help future researchers build improved systems for information-seeking scenarios with selective information needs.</sample>
    <sample id="107">In der Arbeit von Yusen Zhang und Kollegen wurde die Anwendung von Modellen auf einem mehrsprachigen Encoder (Encoder-PTR) in der Cross-Lingual Semantic Parsing untersucht. Die Encoder-PTR-Modelle, wie XLM-R + PTR und mBERT + PTR, wurden in verschiedenen Experimenten eingesetzt, um die Leistung in mehreren natürlichen Sprachen und Bedeutungsdarstellungen zu bewerten.

Die Encoder-PTR-Modelle bestanden aus einem vorgezogenen Encoder, der mehrere Sprachen verarbeiten kann, und einem Pointer-based Decoder, der die Ausgabe generiert. Diese Modelle wurden in verschiedenen Experimenten getestet, um die Leistung in mehrer natürlichen Sprachen und Bedeutsamkeitsdarstellungen zu bewerten, einschließlich:

* **Monolinguale Modellierung**: Die Encoder-PTR-Modelle wurden in einer monolingualen Umgebung getestet, in der die Queries in einer einzigen Sprache verarbeitet wurden.
* **Monolinguale Few-shot Modellierung**: Die Encoder-PTR-Modell wurde mit nur 10% der Trainingsdaten getrainiert, um die Leistung in mehren natürlichen Sprachen zu bewerten.
* **Multilinguale Modellierung**: Die Encoder-PTR-Modele wurden in einer mehreren Sprachen verarbeitet, um die Leistung in mehr als einer Sprache zu bewerten.

Die Ergebnisse zeigten, dass die Encoder-PTR-Modelle in mehreren Experimenten die beste Leistung erzielten, insbesondere in mehren natürlichen Spraichen und Bedeutungsdarstellungen. Die Encoder-PTR-Modelle erreichten die beste Leistung in 8 von 9 Datasets und die beste Leistung in 5 von 5 Aufgaben.

Die Encoder-PTR-Modelen wurden auch in verschiedenen Experimenten mit anderen Modellen verglichen, wie z.B. Encoder-Decoder-Modellen, die Multilinguale Pretrained Encoder-Decoder Modelle wie mBART und mT5 waren. Die Encoder-PTR-Modelen erreichten die beste Leistungen in mehren Experimenten, insbesondere in mehren Datasets und Aufgaben.

Insgesamt zeigten die Ergebnisse, dass die Encoder-PTR-Modelen in der Cross-Lingual Semantic Parsing eine vielversprechende Leistung erzielten und vielversprechende Ergebnisse in mehren Experimenten erzielten.</sample>
    <sample id="108">In this talk, Koustav Sinha and his team present their work on evaluating language models' acceptability judgments using the minimal pair paradigm. They argue that current methods for evaluating language models are limited to short sentences and do not account for the context in which the model is used. To address this, they propose a new approach that involves recreating longer sequences by adding prefixes to acceptable or unacceptable sentences from various datasets. They find that language models' acceptability judgments are sensitive to the context in which they are used, and that this sensitivity is particularly pronounced for longer sentences. The team also finds that language models are sensitive to latent syntactic and</sample>
    <sample id="109">Or presents "Unnatural Instructions: Tuning Language Models without Human Labor," a dataset of natural language instructions and their corresponding tasks. The dataset is generated automatically by prompting a pre-trained language model, GPT-3, with examples from the Super-Natural Instructions dataset. The model generates additional examples, creating a diverse set of tasks, content, and phrasing. The dataset contains 64,000 original examples and 240,000 paraphrased examples. The generated data is analyzed for creativity, diversity, and correctness, with over 50% of examples being correct. The dataset is used to fine-tune a 11 billion-parameter T5 model, which outperforms both T0++ and Tk-instruct across benchmarks. The results show that language models can produce creative and diverse data, which is difficult to obtain with human annotations. The dataset highlights the potential of language models to generate diverse and creative data for instruction tuning.</sample>
    <sample id="111">The authors determine words with moderate frequency by using a general text corpus and counting the word frequency with it. They select a trigger set, which is a group of words in a moderate frequency range, and assume that the provider can collect this corpus and count the word frequency.</sample>
    <sample id="112">Hallo alle, mein Name ist Shuheng. Heute präsentiere ich unsere Arbeit "Do CoNLL-2003 named-entity taggers still work well in 2013? Lass uns loslegen. Unsere Arbeit untersuchte das Problem der Generalisierung im Rahmen der Named Entity Recognition Aufgabe oder NER Aufgabe. Wir beobachten, dass Modelle seit CoNLL-2003 für die Entwicklung von NER verwendet werden und dies stellt natürlich mehrere Probleme dar. Zunächst können diese Modelle auf moderne Daten generalisieren? Und wenn wir neue Tagger entwickeln, was ist für eine gute Generalisierung notwendig? Gleichzeitig, wenn wir tatsächlich eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir das CoNLL++ Dataset entwickelt. Dies ist ein Datensatz, den wir aus Reuters News aus dem Jahr 2020 gesammelt haben und dann mit den gleichen CoNLL-2003-Anmerkungsrichtlinien annotiert haben. Wir haben dann über 20 Modelle auf CoNLL-2003 feinabgestimmt und sie auf sowohl den CoNLL-03 Testsets als auch das CoNLL++ bewertet. Zum Schluss haben wir den F1-Veränderungsgrad berechnet, um die Generalisierung jeder Modelle zu bewerten. Also, was ist für eine gute Generalisierungsfähigkeit notwendig? Durch unsere Experimente fanden wir heraus, dass es drei Hauptzutaten gibt, die benötigt werden. Die erste ist die Modellarchitektur. Unsere Experimente zeigten, dass Transformer-Modelle normalerweise besser generalisieren. Die zweite Zutat ist die Modellgröße. Wir fanden heraus, dass üblicherweise größere Modelle zu besserer Generalisierung führen. Und letztendlich wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele direkt die Leistung einer nachfolgenden Aufgabe beeinflusst. Hier fanden wir auch heraus, dass mehr Feinabstimmungsbeispiele tatsächlich zu besserer Generalisierung führt. Zu unserer nächsten Frage: Was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist die adaptives Überfitten, das ist das Überfitten durch die Wiederholung des gleichen Testsets und dies tritt normalerweise als Abnahme der Ergebnisse auf einem neuen Testset zum Ausdruck. Die zweite Hypothese ist die zeitliche Drift, die durch den zunehmenden zeitlichen Abstand zwischen dem Trainings- und dem Testdatensatz verursacht wird. Für Datenüberfitten sahen wir, dass aus dem Diagramm auf der rechten Seite die rote beste Anpassungslinie einen Gradienten hat, der größer als eins ist. Das bedeutet, dass jede Einheit Verbesserung, die wir auf CoNLL-2003 gemacht haben, zu mehr als einer Einheit Verbesserung auf CoNLL++ führt, was bedeutet, dass keine abnehmenden Ergebnisse beobachtet werden. Und das zeigt uns, dass adaptives Überfitten in diesem Fall nicht beobachtet wird. Also was ist dann mit der zeitlichen Drift? Wir haben eine Experimente durchgeführt, um einige Modelle weiter zu trainieren oder weiter vorzubereiten, mit neueren Daten und fanden heraus, dass die Leistung mit größeren zeitlichen Abständen abnimmt und dies bestätigt unsere Hypothese, dass der Hauptgrund für den Leistungsabfall die zeitliche Drift ist. Unsere Schlussfolgerung ist, dass für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiel benötigt werden. Und diese gehen Hand in Hand, wir können nicht nur ein Zutat haben und die anderen wegwerfen. Gleichzeitig fanden wir auch heraus, der Leistungsabfall hier wird durch die zeitliche Drift verursacht und es ist etwas überraschend, dass es nicht durch adaptives Überfitten verursacht wird, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird. Also gehen wir zurück zu der Frage, die wir in der Titel unserer Arbeit gestellt haben: "Do CoNLL-200</sample>
    <sample id="114">Title: "Grouped Head Attention: A Novel Approach to Redundancy Optimization in Large Language Models"

Abstract:
Large language models (LLMs) have revolutionized natural language processing by learning all tasks in a single model. However, they suffer from heavy parameters, long training times, and token-hungry requirements. This paper introduces Grouped Head Attention (GHA), a novel approach to address the heavy parameter problem in LLMs. GHA uses a divide and conquer strategy to compress multi-headed attention, achieving significant parameter compression without sacrificing performance. The model consists of two stages: group-constrained training and the Voting-to-Stay algorithm. In the first stage, attention heads are divided into groups to make intra-group heads more similar and inter-group heads more separate. In the second stage, redundant heads are pruned, leaving only one head per group. GHA achieves up to 90% parameter compression, with comparable performance on machine translation, abstract summarization, and language modeling tasks. Our LITE model further improves efficiency, achieving 90% parameter pruning, 62% faster inference speed,</sample>
    <sample id="115">Bei dem Ansatz werden Sprachsegmente von Lambda-Sprachrahmen verwendet, um die Translationsqualität und Latenz zu optimieren.</sample>
    <sample id="116">Im Beispiel mit Servin und Kea wird das entitätsspezifische Wissen benötigt, dass "Servin is a judge." Diese Information ist notwendig, um den korrekten Referenzpunkt für den Pronomen "he" zu bestimmen, das auf Servin hinweist.</sample>
    <sample id="117">Der wichtigste Faktor zwischen Qualität des Beispiels und Ähnlichkeit mit dem Ausgangssatzes ist die Qualität der Beispiele. Die Ergebnisse zeigen, dass die Beispielqualität wichtiger ist als die Ähnlichkeit zu dem Ausgangssatz.</sample>
    <sample id="118">Hello everyone. We'll be presenting our submission to ACL 2023, which is "Improving Pretraining Techniques for CodeSwitched NLP." Code-switching is a common occurrence in linguistically diverse communities like India. Multilingual pre-trained models like mBERT or XLM-R don't perform well on code-switched tasks like question answering or sentiment analysis. Our main contributions are novel MLM techniques tuned to code-switching, architectural changes, and auxiliary loss. We propose SwitchMLM, which defines a switch-point as a group of two tokens that transition between languages. In standard MLM, all words are maskable with uniform probability, but this is not the case for code-switched sentences. We offer a surrogate method called FrequencyMLM, which uses the negative log likelihood of the word in each language's corpora to assign LID tags. We also propose some architectural changes, such as residual connections, to help with code-switching. We found that certain intermediate layers of BERT encode switch-point information better than the final layer, so we add residual connections from that layer to the final layer and impose an auxiliary LID-based loss. Our results show that our combined method, which includes SwitchMLM, ResBERT, and an auxiliary loss, performs the best on sentiment analysis tasks for all language pairs. We also conducted probing experiments to verify our claim about the switch-point information, and we found that our methods increase the amount of switch-point content in the intermediate and final layers. In summary, we propose a new MLM objective tuned to handle code-switch information, and we motivate some architectural changes and add an auxiliary LID-based loss to further enhance this switch-point information content, which we verified using probing classifiers. Thank you.</sample>
    <sample id="119">Die Arbeiten in den erweitertenen Experimenten konzentrieren sich auf verschiedene Sprachmodelle, darunter GPT-4, GPT-3, BART und RoBERTa. Diese Modelle werden untersucht, um ihre politische Ausrichtung und die Auswirkungen ihrer politischen Biases auf verschiedene NLP-Aufgaben zu bewerten.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus mehreren Ebenen. Es kombiniert die Werte des Cross-Attention-Mechanismus zwischen dem Audio-Eingang und dem Textausgang, um Entscheidungen darüber zu treffen, ob ein Wort ausgegeben werden soll oder nicht. Diese Kombination ermöglicht es dem Modell, die Informationen aus verschiedenen Ebenen zu berücksichtigen, um die Übersetzung in Echtzeit zu optimieren.</sample>
    <sample id="121">Examples of direct inference include saying the name of the song "Easy on me" or its position, "the first one."</sample>
    <sample id="122">Die Autoren gehören der Fudan-Universität an.</sample>
    <sample id="123">Ying and Zhiyang present their research on MultiInstruct, a multi-modal instruction tuning benchmark dataset, which aims to improve the generalization of large language models to unseen multi-modal tasks. They use the OFA model as a base model and train it on 53 tasks from 9 groups, sampling 10,000 instances per</sample>
    <sample id="124">Hi everyone, I'm Tan Qingyu from the National University of Singapore. I'm excited to share our work, "Towards Benchmarking And Improving The Temporal Reasoning Capability Of Large Language Models." Time is a fundamental axis in the real world, and we break down temporal reasoning into three levels: time-to-time, time-to-event, and event-to-event reasoning. We found that prior works on temporal studies focus too much on the second level, so we aim to study temporal reasoning in a more comprehensive manner by proposing the TempReason dataset, which covers all three reasoning levels and long temporal coverage. We also propose a training strategy with two nominal components to improve the temporal reasoning capability of LMs. Our proposed TempT5 model significantly improves the performance of T5-SFT in OBQA and the ReasonQA set. We analyze and expose the temporal reasoning biases of Lms and propose a training paradigm to improve LLM's Temporal Reasoning. Thank you for your time.</sample>
    <sample id="125">I'm sorry, but the provided text does not mention the number of authors involved in the work.</sample>
    <sample id="126">Ja, die Übersetzung der natürlICHsprachlichen Anfrage mit Hilfe von Google Translate API wurde als Baseline für den semantischen Parsing betrachtet. Dieser Ansatz wurde in der "Translate-Test"-Einrichtung verwendet, bei der die Queries in die Zielsprache übersetzt und dann mit einem monolingualen Modell für die semantische Parsing-Analyse trainiert und bewertet wurde.</sample>
    <sample id="127">Namgyu Ho and his team at KAIST AI have developed a method called "Large Language Models Are Reasoning Teachers" to transfer the reasoning abilities of large models to smaller ones. They use chain-of-thought prompting to generate step-by-step solutions for complex tasks and fine-tune smaller models with these solutions. They also propose a novel technique called diverse reasoning, which generates multiple solutions using stochastic temperature sampling to improve the performance of the student model. The method is highly scalable and outperforms existing baselines on 12 tasks, including text-based ones. The team encourages further research and discussion on the topic.</sample>
    <sample id="128">Title: The KITMUS Test: Evaluating Knowledge Integrated from Multiple Sources

Abstract:
Natural language understanding models rely on both pretraining knowledge and inference-time information. However, integrating knowledge from multiple sources remains a challenge. We introduce the KITMUS test suite, focusing on coreference resolution to evaluate this integration. The test includes three settings: Background-Pretrain, Background-Both, and Background-Inference, simulating varying availability of background and entity-specific knowledge. Evaluating with human participants and models, we find that without task-specific training, models struggle. Training on KITMUS improves performance, but integrating inference-time knowledge remains difficult. Our results highlight the need for task-specific training to effectively integrate knowledge from multiple sources.</sample>
    <sample id="129">The authors provided the example of the persona of a black woman, which includes words like "strong" and "resilient." They explained that while these words might sound positive, they contribute to harmful stereotypes and essentializing narratives, such as the "Strong Black Women" archetype, which puts pressure on black women to be resilient and strong against societal obstacles, potentially leading to negative health outcomes.</sample>
    <sample id="130">Based on the content of the paper, the following model architectures do not generalize well:

1. Traditional recurrent neural network (RNN) architectures, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) models, which were commonly used in the CoNLL-2003 era.
2. Older transformer models, such as the original transformer model proposed in the paper "Attention is All You Need" (Vaswani et al., 2017), which may not generalize as well to modern data.

The paper suggests that more recent transformer models, such as BERT and its variants, tend to generalize better to new data.</sample>
    <sample id="131">The test datasets in the context of weak supervision are typically referred to as "clean" or "ground truth" datasets. These datasets are manually annotated and are used to evaluate the performance of the models trained on weakly labeled data. The clean validation sets are crucial for model selection and performance evaluation in weak supervision, as they provide a reliable benchmark for assessing the generalization capabilities of the models.</sample>
    <sample id="132">In der Arbeit "The KITMUS Test: Evaluaring Knowledge Integration from Multiple Sources" sind zwei Autoren beteiligt: Akshatha und Martin.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, insbesondere mit Text und Bildern.</sample>
    <sample id="135">James Finch and Sarah Finch introduce ABC-Eval, a new method for evaluating conversational AI. They explain that current methods rely on human judges to rate conversations, but this can be subjective. ABC-Eval aims to reduce subjectivity by explicitly annotating chat model behaviors, such as irrelevant responses or contradictions. The method measures rates of various thematic errors, including ignoring the partner, contradicting, hallucinating, and violating common sense knowledge. The authors evaluated four state-of-the-art chat models using ABC-Eval and compared the results to existing methods. They found that ABC-Eval behavior labels are more reliable and predictive of conversation quality. The authors also identified common errors in chat models, such as common sense violations and irrelevant responses. They conclude that ABC-Eval can provide a higher resolution evaluation of conversational AI and hope it can be used by others in the field.</sample>
    <sample id="136">In this presentation, Jasivan and Nafise discuss their work on "FERMAT: An Alternative to Accuracy for Numeral Reasoning." They highlight the importance of numerical reasoning in real-world applications, such as fact-checking, and the limitations of current benchmarks that only provide accuracy scores. FERMAT is a flexible evaluation set based on arithmetic types, including number understanding, mathematical operations, and training dependency. The authors perform a baseline evaluation and find that most models perform poorly across all aspects. They then fine-tune models using math teachers' templates and generate 200,000 questions. The results show improved performance across the board. The authors also investigate training templates and find that language and mathematical diversity is important. They conclude that FERMAT provides a more informative alternative to current benchmarks and that language and mathematical diversity are areas of improvement.</sample>
    <sample id="137">Sicong from the Singapore University of Technology and Design presents their work "Tell2Design: A Dataset for Language-guided Floor Plan Generation" published in ACL 2023. The research focuses on generating floor plan designs directly from language instructions, addressing the need for designs that meet various requirements specified in natural languages. The dataset, Tell2Design, consists of 5,051 human-annotiated language instructions and 76,000 artificially generated instructions. The main challenges include strict constraints, understanding the big picture from unstructured text, and dealing with ambiguous information. The proposed sequence-to-sequence model, initialized with a pre-trained language model T5, outperforms text-conditional image generation baselines. The model achieves high IoU scores, indicating its ability to control target box sequence generation based on salient information from language instructions. The research initiates a novel task of language-guided design generation, focusing on the floor plan domain, and introduces Tell2Design as a large-scale dataset. The paper aims to serve as a foundation for future research in this area.</sample>
    <sample id="138">Nach Ansicht der Autoren ist das zu wenig erforschte Gebiet im Bereich der natürlichen Sprachverarbeitung (NLU) die Fähigkeit von Modellen, effektiv Wissen aus verschiedenen Quellen zu integrieren, insbesondere wenn dieses Wissen nur in der Inferenzzeit verfügbar ist. Dies betrifft die Integration von "backward knowledge" (Wissen, das nur zur Inferenzzeit zur Verfügung steht), was die Autoren in ihrer Arbeit "The KITMUS Test: Evaluaring Knowledge Integration from Multiple Sources" untersucht haben.</sample>
    <sample id="139">The presenters are Ying and Zhiyang.</sample>
    <sample id="140">Ja, CoScript hat eine Qualitätskontrolle durchgelaufen. Die Autoren haben angefordert, dass Crowd-sourced Arbeiter die unkorrekten Stichproben finden und korrigieren. Dies stellt sicher, dass die Validierungs- und Testmengen von hoher Qualität sind.</sample>
    <sample id="141">Die Grenzen bestehender Ressourzen für kontextbasierte Übersetzungen liegen in der begrenzten Verfügbarkeit von kontextabhängigen Datensätzen, die oft auf spezifische Domänen und Sprachen beschränkt sind und von menschlicher Kuratierung abhängig sind. Darüber hinaus sind bestehende Evaluierungsmethoden wie Corpus-Level-Metriken (z.B. BLEU) nicht in der Lage, kontextabhängige Übersetzungen zu erfassen. Die Entwicklung von spezifischen Tagger und Benchmarking-Systemen, wie das Multilingual Discourse-Aware (MuDA) Tagger, bietet Lösungen, um diese Grenzen zu überwinden und die Leistung von Modellen in kontextabhängigen Übersetzungen zu verbessern.</sample>
    <sample id="142">Hallo! Ich bin Javad Hosseini und ich arbeite in Zusammenarbeit mit Filip Radlinski, Silvia Paret</sample>
    <sample id="143">Der Ansatz wird mit den bestehenden SimulST-Richtungen "Wait-k" und "Local Agreement" verglichen.</sample>
    <sample id="144">Die Autoren gehören der Universität Nantes.</sample>
    <sample id="145">The presenter's name is Jenny.</sample>
    <sample id="146">Dialogue summarization is a subtask of text summarisation that involves creating concise summaries of dialogues. While large-scale pretrained language models have made significant progress in generating fluent and coherent summaries, they still suffer from common errors, including omission, which leads to incomplete summaries. To address this problem, we analyzed the percentage of summaries with omission errors and found that even state-of-the-art models have a high omission rate. We constructed the OLDS dataset, which provides high-quality labels for dialogue summarization, and explored three frameworks as baselines for omission detection. Our results show that the task is challenging, but the refinement based on detected omission is a promising direction for quality improvement.</sample>
    <sample id="147">Drei Autoren sind an der Arbeit beteilt: Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trento und der Foundazione Bruno Kessler und ich werde kurz die Arbeit "Attention as a Guide for Simultaneous Speech Transcription" vorstellen, die ich gemeinsam mit Matteo Negri und Marco Turchi geschrieben habe. Was ist Simultaneous Speech Transcription? Simultaneous Speech Transcription, oder SimulST, ist der Prozess der Übersetzung von gesprochenen Sprachen in Text in Echtzeit, die es ermöglicht, sprachübergreifend zu kommunizieren. Und welche Probleme gibt es bei den aktuellen SimulST-Modellen? Spezifische Architekturen werden normalerweise trainiert, wobei zusätzliche Module optimiert werden. Lange und komplizierte Trainingsverfahren, zum Beispiel Trainingsverfahren mit verschiedenen Optimierungszielen. Und das Trainieren und Aufrechterhalten mehrerer Modelle, um verschiedene Latenzbereiche zu erreichen. Zum Beispiel das Trainieren eines Modells mit einem Durchschnitt von einer Sekunde Latenz und einem anderen Modell mit einer Latenz von zwei Sekunden und so weiter. Also was ist unsere Lösung? Zunächst, die Verwendung bereits bestehender offline ST-Modelle ohne Neuausbau oder Anpassung spezifischer Architekturen für SimulST zu verwenden. Verwenden wir nur ein Modell für jeden Latenzbereich und behandeln Latenz durch spezifische Parameter. Und nutzen wir das Wissen, das bereits durch das Modell durch die Aufmerksamkeit zwischen Audio-Eingabe und Textual-Output erworben wurde. Das ist die Cross-Attention-Methode, und Sie können ein Beispiel auf der rechten Seite sehen. Unsere Lösung ist, die EDAtt oder Encoder-Decoder-Aufmerksamkeit vorzuschlagen, bei der wir entscheiden, ob ein Teil der Übersetzung ausgegeben werden soll, basierend auf, wo die Aufmerksamkeit zeigt. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d.h. seine Summe unter einem bestimmten Schwellenwert alpha zu den letzten lambda Sprachrahmen hinweist, was bedeutet, dass die erhaltene Information stabil genug ist. Zum Beispiel, wenn wir einen Sprachblock erhalten, der "Ich werde über..." enthält, und unser Modell die Übersetzung in Deutsch vorhersagt, und wir uns die Cross-Attentiongewichte ansehen, sehen wir, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen hinweisen, während das letzte Wort auf die letzten lambda Sprachrahmen hinzukommt, also lambda Sprachrahmen. Das bedeutet, dass die ersten beiden Wörtern ausgegeben werden, während wir die letzte Wort nicht ausgeben und auf einen weiteren Sprachblock warten, da die Summe der Cross-Attention über einen bestimmten Schwellenwert alpha hinausgeht. Wenn wir weitergehen und einen weiteren Sprachblock erhalten, und unser Modell weitere drei Wörter vorhersagt, und wir die Cross-Attentiongewichte anzeigen, sehen wir, dass kein Wort auf die letzten lambda Sprachen hinweist. Das bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir die Hauptergebnisse von EDAtt sehen, werden wir die simultane Sprachtranskription-Ergebnisse auf Graphen darstellen, auf denen BLEU auf einer Seite gemessen wird, die die Übersetzungsqualität misst, und der durchschnittliche Verzögerung, die die Latenz misst, und wir berücksichtigen auch die rechenaufwändige durchschnittliche Verzögerungen, die die Rechenzeiten des Modells zur Vorhersage des Ausgabes berücksichtigen. Also wollen wir unsere Kurven so hoch wie möglich auf diesem Diagramm sein. Aber wir wollen auch, dass sie nach links verschoben sind. Und wir vergleichen mit beliebten Strategien, die auch auf offline Modellen angewendet werden, nämlich der Warte-k-Strategie und der Lokalen Einigung. Und wir vergleichen auch mit dem state-of-the-art-Architektur, die speziell für simultane Vor-Übersetzung entwickelt wurde. Dies sind alle Ergebnisse der simultanen Sprachtranskription-Strategie auf Deutsch. Und wir sehen, dass es sich übertrifft, alle Strategien, die auf offline Modellen angewendet werden. Die Kurven sind verschoben, und wir sehen auch, dass wenn wir die tatsächliche vergangene Zeit oder die rechenaufwändige Zeit, d.h. die schnellste Strategie, berücksichtigen, dass es das schnellste ist. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unsere Arbeit. Und wir haben auch den Code und die Modelle sowie die simultane Ausgabe geöffnet, um die W</sample>
    <sample id="149">Yes, the CoNLL++ dataset is publicly accessible.</sample>
    <sample id="150">Hello everyone, I'm Archiki, presenting our ACL paper "MEETINGQA". We introduce a new dataset, MeetingQA, for extractive question answering on meeting transcripts. Meeting transcripts are long, domain-specific documents that are underutilized in NLP research. Our dataset contains 7.7K questions with high inter-annotator agreement and diverse answer scenarios. We collect data from public meeting transcripts and annotate answers with high accuracy. Our dataset has a high human performance on the test set with an F1 of 84%. We employ various methods, including context-retrieval, single-span, and multi-span models. Our results show a significant gap between fine-tuned models and human performance, and zero-shot performance is significantly improved with silver data augmentation. Error analysis reveals challenges in identifying rhetorical questions, irrelevant sentences, and speaker identification. MeetingQA is a challenging dataset for existing QA models, and we hope our work will inspire further research in this area. Thank you for listening, and more details can be found on our project page or in our paper.</sample>
    <sample id="151">Hallo alle, mein Name ist Ying und mein Kollege Zhiyang werden wir uns heute mit unserer Forschung über MultiInstruct verbringen, die die Verbesserung des Multi-Modal Zero-Shot Learning durch Anweisungstuning fördert. Mit den Fortschritten in großen Sprachmodellen haben viele Arbeiten neue Lernparadigmen untersucht, um vorkommende Sprachmodelle für verschiedene Downstream-Aufgaben in einer parametrierungs- und dateneffizienten Weise zu nutzen. Kürzlich zeigten viele Studien, dass Anweisungstuning es großen Sprachmodellen ermöglicht, in einer Null-Shot-Methode auf ungesehene Aufgaben mit natürlichen Anweisungen zu agieren. Allerdings konzentrierten sich viele frühere Arbeiten auf die Verbesserung der Null-Shot-Performance für sprachbasierte Aufgaben, während computervisuelle und multi-modal Aufgaben ausgelassen wurden. Daher möchten wir in dieser Arbeit untersuchen, ob Anweisungstuning eines multi-modalen vorkommenden Sprachmodells tatsächlich die Generalisierung zu ungesehenen multi-modalen Aufgaben verbessern kann. Zudem entdeckten wir bei der Zeit unserer Forschung eine erhebliche Diskrepanz in der Verfügbarkeit von Anweisungsdatensätzen zwischen NLP und multi-modal. Es gibt mehr als 1600 sprachbasierte Anweisungsaufgaben, jedoch keine großskalige öffentlich zugängliche multi-modal Anweisungsaufgaben. Dies motiviert uns, ein multi-modales Anweisungstuning-Datensatz aufzubauen. Hier präsentieren wir MultiInstruct, das erste multi-modales Anweisungstuning-Benchmark-Datensatz, der aus 62 vielfältigen multi-modalen Aufgaben besteht, die 10 breite Kategorien abdecken. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen und jede Aufgabe ist mit fünf von Experten geschriebenen Anweisungen ausgestattet. Um die multi-modal Anweisungstuning auf unserem vorgeschlagenen Datensatz zu untersuchen, nehmen wir das OFA, ein vereinigtes multi-modales vorkommendes Sprachmodell, als unsere Basismodell. OFA verwendet einen vereinigten Satzzeichenraum für Sprach-, Bildtoken und die Koordinaten eines Bounding-Boxes. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct-Datensatz, um die Verarbeitung verschiedener Eingabe- und Ausgabedatenarten zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einer einheitlichen sequenz-zu-sequenz-Format. In dem der Eingabe-Text, Bilder, Anweisungen und Bounding-Box in derselben Token-Raum dargestellt sind.

Jetzt werde ich über multi-modal Anweisungstuning sprechen. Für das Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und stellen 10.000 Instanzen pro Aufgabe zur Verfügung. Für die Testphase reservieren wir die gesamte Gruppe des gemeinsamen Vernunftigkeitsdenkens für die Testphase und wählen zusätzlich 5 Aufgaben aus den Gruppen VQ und Verschiedenheiten. Wir verwenden alle Instanzen in der Testteilung für jede Aufgabe. Zusätzlich wählen wir zufällig 20 Aufgaben aus der Testteilung der natürlichen Anweisungen als ein unbekanntes Aufgaben für NLP aus. Wir verwenden das vorgezogene OFA große Modell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer von fünf Anweisungstemplates kombiniert. Während des Tests führen wir für jede Aufgabe insgesamt 5 Experimente durch, indem wir den Modell mit einer von fünf Anweisungen bewert</sample>
    <sample id="152">Frederick Riemenschneider's presentation, "Exploring Large Language Models for Classical Philology," introduces new language models designed for Ancient Greek and Latin. These models, including GreBERTa, GreTa, PhilBERTa, and PhilTa, are pre-trained on high-quality datasets, including a new corpus from the Internet Archive. The models are evaluated on tasks such as part-of-speech tagging, dependency parsing, lemmatization, and semantic and world knowledge. The results show that the models outperform previous models and that there is no significant difference in performance between monolingual and multilingual models. The presentation concludes with a call to action to check out the paper for more details.</sample>
    <sample id="153">Hello, my name is Ninareh Mehrabi, and I am a postdoctoral scientist at Amazon Alexa's Responsible AI team. I will present our work, "Resolving ambiguities in text-to-image generative models." In this work, we study existing ambiguities in prompts provided to text-to-Image models. For example, the prompt "The girl enters the room with flowers" is ambiguous because it is not clear whether the flowers should be in the girl, the room, or a combination of both. Without resolving these ambiguities, it is challenging for text-to-image models to generate faithful representations of user intention.

To address this issue, we curate a benchmark dataset that covers different ambiguity types. We then use a prompt disambiguation framework to gather external signals to disambiguate the prompts. This framework can either ask clarifying questions from the user or generate different possible visual setups. Once the prompts are disambiguated, we evaluate them by inputting them into a text-to-image model and generating images. We then use a VQA model to evaluate whether the generated images are faithful to the user's intention.

Our findings show that there is a disparity in resolving ambiguities for different types of ambiguities. However, our disambiguation framework has a positive effect on faithful generation, and our automatic evaluation framework is in agreement with human evaluations.

In conclusion, we study ambiguities in text-to-image generative models, curate a benchmark dataset, and propose frameworks to mitigate and evaluate ambiguities. Thank you for your attention.</sample>
    <sample id="154">Die Autoren gehören der Universität Trento und der Foundazione Bruno Kessler an.</sample>
    <sample id="155">The AltEntities Corpus is a dataset designed to study how users refer to entities indirectly in conversations. It includes 6,000 alternative questions across music, books, and recipes domains, with 42,000 indirect referring phrases. The dataset is created using a cartoon completion setup where annotators fill in the third speech bubble with indirect references. The accuracy of language models varies depending on the amount of background knowledge they have access to, with the highest accuracy (92-95%) when they have access to the exact same background knowledge as the human annotators. The dataset is available for further research and benchmarking in conversational systems and LLM entity understanding.</sample>
    <sample id="157">Hi, I'm Shen Gao from Shandong University, and I'm here to introduce our work, "Dialogue Summ</sample>
    <sample id="158">Coreference resolution is the task of identifying and clustering mentions of the same entity in a document. Conventional methods have quadratic complexity, while cache-based methods reduce complexity to linear levels. However, in long documents, the LRU policy used in cache-based methods can lead to high cache misses. To address this, we propose a dual cache with a local cache (using LRU) and a global cache (using LFU). The dual cache works by classifying mentions as new or existing, evaluating their frequency, and adding them to the appropriate cache. We evaluated the dual cache on four public benchmarks and found that it outperforms single cache methods and reduces cache misses. We also observed that dual cache is the most cost-effective compared to single cache methods. In conclusion, dual cache uses a local and global cache separately to store local and global entities, and it outperforms single cache methods and reduces cache misses while being the most cost-effective.</sample>
    <sample id="159">Hallo zusammen, ich bin Koustav Sinha und ich freue mich, Sie zu unserem Vortrag über unsere Arbeit auf der ACL 2023 zu begrüßen. Die Akzeptabilitätsurteile von Sprachmodellen sind nicht immer robust gegenüber Kontext. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kishka Misra, Karen Fences, Roger Lévy und Adina Williams. In dieser Arbeit untersuchen wir die minimalen Paar-Paradigmen wieder. Die minimalen Paar-Paradigmen bewerten Sprachmodelle auf der Basis von Akzeptabilitätsurteilen, die auch grammatische Korrektheit wie bei BLiMP, SyntaxGym oder Akzeptabilität in Bezug auf Stereotypen wie CrowS Paare umfassen. In diesem Kontext wird die MPP-Pipeline normalerweise dazu verwendet, ein akzeptables oder grammatisches Satzzeichen zu zeigen und dann ein akzeptables oder ungrammatisches Satzzeichen. Die Hoffnung ist, dass das Modell mehr Wahrscheinlichkeit auf das akzeptable Satzzeichen setzt. Die aktuelle MPP-Pipeline ermöglicht es uns jedoch nicht, die Akzeptabilität von Modellen über längere Sätze zu bewerten. Inzwischen kommen große Sprachmodelle mit immer längeren Kontextfenstern. Es ist entscheidend, dass wir die Akzeptabilität von Modellern über den Kontextfenster bewerten, und das ist genau das, was wir hier versuchen zu tun. Wir versuchen, die MPP-Pipeline zu überarbeiten, indem wir das Modell dazu ermutigen, Akzeptabilität auf längere und längere Sequenzen zu bewerten. So was wir tun, ist, dass wir die Datenmengen selbst erneut betrachten und dann Sätze auswählen, die akzeptabel oder unakzeptabel sind. Zum Beispiel haben wir hier ein typisches Paar von grammatikalischen Fehlern aus dem BLiMP-Datensatz aus dem Adjunct Island Fall ausgewählt. Was wir tun, ist, dass wir grammatische Sätze aus Adjunct Island extrahieren und dann als Prefix zu beiden akzeptablen und unakzeptablen Abfragen hinzufügen. Wir können dasselbe mit unakzeptablen Sätzen aus dem gleichen passenden Bereich tun, um die Akzeptabilität des Modells zu testen. Wir können auch das gleiche mit Sätzen aus einem anderen Teil oder einem anderen Datensatz tun. Das nennen wir das Mismatch-Szenario. Hier sind die Sätze immer noch aus einem relevanten Datensatz, aber nicht aus dem gleichen Datensatz, mit dem wir bewerten. Wir können dasselbe für unakzeptable Fälle tun. Schließlich können wir Sätze aus einem völlig unverwandten Bereich wie Wikipedia auswählen, um zu sehen, ob die Akzeptabilitätsurteiler des Modells tatsächlich von Kontext beeinflusst werden, ob der Kontext aus einem anderen Teil des Datensatzes kommt oder ob er völlig irrelevant ist, zum aktuellen Satz, den wir betrachten. Wie geht es dem Modell? Zunächst untersuchen wir die Sätze aus Wikipedia, die völlig irrelevant zu dem aktuellen Abfragepaar sind. Hier finden wir, dass die MPP-Urteile für beliebige Kontextlänge meist robust sind. Wir erhöhen die Kontextlänge auf 1024, um OPT und GPT 2 Modelle zu maximieren. Und wir sehen hier, dass die MPP-Urteile in der orange gedämpften Linie relativ stabil sind. Was passiert, wenn wir Sätze aus dem gleichen Datensatz auswählen? Hier sind wir es,</sample>
    <sample id="160">In the first step der Methode werden die Input-Token mit einem ungeordneten Multiset von Tokens zugeordnet, die in der Ausgabe vorkommen werden.</sample>
    <sample id="161">In CoScript, insgesamt 55,000 spezifische Ziele mit Skripten generiert wurden.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEPLAIN, basierend auf der Analyse der Präsentation, ist die MASSalign-Methode. Diese Methode wurde als die am besten geeignete für die Extraktion von Goldstandard-Alignments zwischen parallelen Sätzen in Dokumenten mit unterschiedlicher Komplexität identifiziert. Die MASSalign-Methode wurde in der Präsentation als die am besten geeignete Methode für die automatische Textausrichtung zwischen parallelen Sätzen in Dekomplexisierungskorpora identifiziert.</sample>
    <sample id="164">Schwach überwachtes Lernen (WSL) ermöglicht es, Modelle zu trainieren, ohne manuelle Labels zu benötigen. Stattdessen werden Daten mit schwachen Labelingquellen wie einfachen Regeln, Wissenbanken oder niedrigqualitativen Crowdsourcing-Anfragen beschriftet. Diese schwachen Labels sind kostengünstiger als manuelle Labels, sind jedoch auch raucherhaft, was bedeutet, dass einige Labels falsch sind. Wenn man direkt mit neuralen Netzwerken auf schwach gelabelte Daten trainiert, neigen diese dazu, die Label-Rausch zu speichern und nicht zu generalisieren. In WSL werden Trainingsalgorithmen entwickelt, um neuronale Netzwerke robust unter solchen Label-Rausch zu trainieren, sodass die trainierten Modelle gut generalisieren.

Ein wichtiger Vorteil von WSL ist, dass es ohne manuelle Labels arbeiten kann, was die Kosten für die Datenbeschaffung reduziert. Zudem können WSL-Modelle auf große Datenmengen angewendet werden, die manuell beschriftet werden würde. Ein weiterer Vorteil ist, dass WSL-Modelle robust gegenüber Label-Rausch sind und somit genauere Ergebnisse liefern können als Modelle, die auf manuell beschriftete Daten trainiert werden.

Ein weiterer Vorteil von WSL ist, dass sie in vielen Anwendungsbereichen wie Bildklassifikation, Sprachverarbeitung und maschinelles Übersetzen eingesetzt werden kann. Zum Beispiel können WSL-Modelle in der Bildklassifikation verwendet werden, um Bilder zu klassifizieren, die nicht manuell beschriftet werden. In der Sprachverarbeitung können WSL-Modelle verwendet werden, um Texte zu klassifizieren, die nicht manuelle Labels haben. In der maschinchen Übersetzung können WSL-Modelle verwend</sample>
    <sample id="165">Hello everyone, I'm Wenting Zhao, a PhD student at Cornell University, and I'm excited to present our recent paper titled "Abductive Reasoning Exploiting Mutually Exclusive Explanatory Sets."

To illustrate abductive reasoning, consider the context "Emily was stuck in traffic," and the outcome "Emily made it to her flight." We have two possible explanations: "Her flight was delayed" and "Her flight left on time." The goal is to find a plausible explanation that bridges the gap between the context and the outcome.

In our paper, we address the challenge of abductive reasoning in a closed-world setting, where we aim to identify a plausible subset of explanations from a given candidate set. Traditional approaches rely on supervised methods, which require annotated plausible explanations, often leading to noisy and subjective results.

We propose an unsupervised learning method called LiPoR (Likelihood Learning with Posterior Regularization). LiPoR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context. However, this alone does not ensure plausibility. To address this, we introduce a regularizer based on the mutual exclusivity of explanations.

The LiPoR objective consists of two parts: likelihood maximization and preference regularization. The regularizer, Omega, encourages mutual exclusivity by penalizing high entropy in the explanation set. Our results on AlphaNLI, a widely-used abductive reasoning dataset, show that LiPoR outperforms zero-shot models and the previous best unsupervised approach by over 4 absolute points in accuracy.

Thank you for your attention. Our paper is available at tinyurl.com/zhao-lipor for further details.</sample>
    <sample id="166">Yunxin from Harbin Institute of Technology, Shenzhen, presents a novel approach to image retrieval from linguistically complex text, leveraging a Neural Divide-and-Conquer Reasoning Framework (NDCR). This framework addresses the challenge of high similarity between images and lengthy, complex descriptions by integrating the strengths of visual language models (System 1) and logical reasoning (System 2). Inspired by the Divide-and-Conquer strategy and Dual Process Theory, NDCR decomposes complex propositions into simpler ones using a Proposition Generator, which then interacts with visual information via the Visual-Linguistic Interactor. The Neural-Symbolic Reasoner, acting as System 2, integrates reasoning states to derive final solutions. Experimental results show NDCR outperforms baselines, with ablation studies validating module effectiveness. Two cases demonstrate the method's ability to present inference states and results, highlighting its processing interoperability. The study suggests neural symbolic calculation and Divide-and-Conquer as effective strategies for complex reasoning.</sample>
    <sample id="167">Die Dokumente in DEplain-web wurde mit manuellen und automatischen Alignment-Methoden ausgerichtet. Insgesamt wurden 750 Dokumente aligniert, wobei 483 Dokumente manuell und 267 Dokumente mit automatischen Methoden ausgerichtet wurden. Dies führte zu insgesamt 30,450 parallelen Satzpaaren.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde erstellt, indem wir Daten aus Reuters News von 2020 sammelten und dann mit den gleichen CoNLL-2003-Anweisungen annotierten. Diese Daten wurden dann verwendet, um über 20 Modelle auf CoNLL-2003 zu feinabstimmen. Die Modellleistungen wurden auf sowohl die CoNLL-03- als auch die CoNLL++-Testmengen evaluiert, um den Generalisierungsgrad zu bestimmen.</sample>
    <sample id="169">David Vilar and colleagues from Google Translate conducted a study on using the PaLM large language model for machine translation. They evaluated the model's performance using state-of-the-art metrics and compared it to other systems. The study found that the quality of examples used for prompting is more important than the similarity to the source language. The authors recommend using high-quality examples from the WMT dev data for better performance. While PaLM's translations are comparable to state-of-the-art systems in terms of fluency, they often make omission errors. The study highlights the importance of selecting good prompting strategies and examples for improving the performance of large language models in machine translation.</sample>
    <sample id="170">Hallo alle, mein Name ist Yusen Zhang von der Penn State University. Heute präsentiere ich unsere Arbeit "XSemPLR: Cross-Linguel Semantische Parsing in mehreren natürlichen Sprachen und Bedeutungsdarstellungen". Semantische Parsing ist eine Aufgabe, um semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda Calculus zu erstellen. Cross-Linguel Semantische Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen zu übersetzen. Wie in diesem Bild gezeigt, müssen wir Anfragen in mehreren natrlichen Sprachen mit neuronalen Modellen in SQL, Lambda oder FunQL usw. übersetzen. Existierende Modelle für cross-linguel semantische Parsing werden separat vorgeschlagen und auf Datenmengen für begrenzte Aufgaben und Anwendungen evaluiert. Zum Beispiel gibt es viel Abdeckung für bestimmte natürliche Sprachen. Aber Chinesisch fehlt und es fehlt an Abdeckung für bestimmte Bedeutungsdarstellungen. Lambda-Rechenlogik fehlt oder sie werden nur auf bestimmte neuronale Modelle ausgewertet. Zum Beispiel gibt es nur ein einziges Modell, um sie zu bewerten. Daher schlagen wir XSemPLR vor. Wir bieten eine einheitliche Datenmenge XSemPLR für cross-linguel semantische Parsung in mehreren natürlichen Sprach und Bedeutungsdarstellungen an. Es enthält 9 Datensätte in verschiedenen Bereichen, 5 semantische Parsingaufgaben, 8 Bedeutungsdarstellungen und 22 natürlichen Sprachen in 15 Sprachfamilien. Um unsere Benchmark besser zu bewerten, berücksichtigen wir sechs Einstellungen für Training und Evaluierung. Die erste ist Translate-Test. Wir verwenden die Google Translate API, um die Queries in die Zielsprache zu übersetzen, und verwenden ein monolinguales Modell zur Training und Evaluierung. Zum Beispiel trainieren wir ein englisches Modell auf englischen Queries und wählen während der Inferenz die deutsche Query mit der API zu englisch übersetzen und dann das trainierte Modell verwenden, um den SQL vorherzusagen. Wir testen auch Monolinguale Modelle. In diesem Szenario ist die Quelle- und Zielsprache gleich, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch Monolinguale Few-shot-Szenarien, indem wir monolinguale Modelle mit nur 10% des Trainingsdatensatzes trainieren. Und wir testen Multilinguale Modelle, bei denen wir ein multilinguelles Modell für alle Sprachen trainieren. Zum Beispiel legen wir Deutsche, Englische und Chinesische Queries zusammen, um ein multilinguelles Modell zu trainieren, das SQL auszugeben. Während der Inferenz können wir dieses Modell deutsche Queries oder Chinesische Queries übersetzen, et cetera. Und wir testen auch Cross-linguelle Zero-shot und Few-shot-Transfer. Wir trainieren auf eine Quelle-Sprache und übertragen auf eine andere Sprache. Während des Trainings trainieren wir es auf englischen Queries oder die Kombination aus englischen und deutschen Few-shot-Queries, um ein multilinguelles Modell zur Vorhersage von SQL auszugeben. Und wir finden auch viele interessante Ergebnisse. In Bezug auf die Analyse von monolingualen Modellen evaluieren wir zwei Gruppen von Modellen, einschließlich Encoder-PTR, das für mehrere Sprachen vorgeschlagene Encoders mit Pointer-basierten Decoder bedeutet, wie XLM-R + PTR und mBERT + PTR. Und wir bewerten auch Encoder-Decoder-Modelle, die mehrere Sprachen vorgesehene Encoder-Decoder-Modelle bedeuten, wie mBART und mT5. Wir fanden heraus, dass Encoder-Decoder auf allen neun Datens</sample>
    <sample id="171">The paper mentions that existing works can be broadly classified into four categories, but they either are not applicable to embedding as services or lack transferability.</sample>
    <sample id="172">According to the presentation, multilingual language models such as Codex and Bloom are still inadequate for cross-lingual semantic parsings tasks.</sample>
    <sample id="174">ArgAnalysis35K is a large-scale dataset for argument quality analysis, which is unique in several ways. Firstly, it has 35,000 argument-analysis pairs, making it the largest dataset in this field. Secondly, it has a diverse range of arguments, with 24 themes and a wide variety of motions. Thirdly, it includes an element of analysis, which is a combination of claims, premises, and other elements, and is not just limited to arguments. Fourthly, it introduces an idea of instance-based annotator reliability, which allows for better utilization of annotations. Lastly, it includes a relevance model, which assigns a score to each argument based on its relevance to a particular theme. Overall, ArgAnalysis35K is a unique and valuable resource for argument quality analysis.</sample>
    <sample id="175">Die Methode mit der Mehrdeutigkeitsproblematik der Permutationen umgeht dies, indem sie eine kontinuierliche Approximation der NP-fähigen Permutationsfindung verwendet. Diese Approximation ermöglicht es, die Permutationen zu schätzen, die den linguistisch plausiblen Zusammenhang zwischen Eingabe und Ausgabe widerspiegeln. Durch die Verwendung einer GPU-basierten Optimierung können die Permutationen gelernt werden, indem die Lösung während des Trainings durch Backpropagation optimiert wird. Diese kontinuierliche Approximation ermöglicht es, auch die linguistisch weniger plausiblen Permutationen zu vermeiden, während die linguistisch weniger plausibelen Permutationen durch die kontinuierliche Approximation nicht optimiert werden.</sample>
    <sample id="176">Die Fairness eines nachgeschaltetes NLP-Modells wird oft definiert als die Gleichbehandlung aller Benutzer und Gruppen, unabhängig von ihrer politischen Zugehörigkeit oder anderen sozialen Faktoren. In Bezug auf die politische Bias, die in Sprachmodellen vorkommen, bedeutet Fairness, dass das Modell nicht bevorzugt oder diskriminiert gegenüber bestimmen politischen Gruppen oder Meinungen ist. Das bedeutet, dass das Modell gleichmäßig gut auf verschiedene Arten von Texten reagieren kann, unabhängig von der politischen Ausgangslage oder dem politischen Hintergrund.

Um die Fairness eines NLP-Modells zu gewährleisten, sollten die Modelltrainingsdaten sorgfältig ausgewählt und geprüft werden, um sicherzustellen, dass sie keine politischen Biases enthalten. Zudem sollten die Modellparameter und die Modellarchitektur so gewählt werden, dass sie die Fairness des Modells fördern. Zudem sollten die Modellleistungen auf verschiedenen Demografien und politischen Gruppen getestet werden, um sicherzustellen, das Modell gleichmäßig auf alle Benutzer zugreift.

Es ist wichtig zu beachten, dass die Fairness eines NLP-Modells nicht nur auf der Qualität der Modellleistungen basiert, sondern auch auf der Transparenz und der Rechenschaftspflicht der Entwickler. Die Entwickler sollten offen über die Methoden und die Daten, die für das Modell verwendet werden, berichten und sicherstellen, dass die Modellleistungen auf einer breiten Palette von Daten getestet werden.

Insgesamt ist die Fairness eines NLP-Modells ein wichtiges Thema, das sorgfältig berücksichtigt werden muss, um sicherzustellen, dass die Modellleistungen für alle Benutzer gleichmäßig sind und keine politischen Biases enthält.</sample>
    <sample id="177">The presenter's name is Yanis Labrak.</sample>
    <sample id="178">Der/die Referent*in heißt Koustav Sinha.</sample>
    <sample id="179">Title: Enhancing Theory of Mind in Large Language Models with SymbolicToM

Abstract:

This paper introduces SymbolicToM, an inference-time method designed to improve Theory of Mind (ToM) reasoning skills in large language models (LLMs). Traditional ToM tests, such as the Sally-Anne test, involve false-belief questions that assess a character's understanding of another's mental state. Despite advancements in LLMs, they still struggle with these tasks. SymbolicToM addresses this by using explicit graphical representations to model mental states and their interactions.

SymbolicToM constructs multiple belief graphs for all character combinations up to a predefined ToM level, leveraging off-the-shelf natural language inference (NLI) and OpenIE models. These graphs enable efficient question answering by detecting entities, retrieving relevant graphs, and recursively querying the graph to obtain factual answers.

Experiments demonstrate significant performance gains across various LLMs, including GPT-3-Davinci, Macaw, and Flan-T5-XXL, with accuracy improvements ranging from 51 to 67 points. SymbolicToM also shows robustness in out-of-domain scenarios, maintaining performance on datasets with modified story structures and linguistic diversity.

SymbolicToM offers a plug-and-play solution to enhance ToM reasoning in LLMs, avoiding overfitting and providing interpretable reasoning. It outperforms supervised approaches on out-of-domain story understanding and remains beneficial for new linguistic diversity datasets. For further details, refer to the paper.</sample>
    <sample id="180">Der/die Referent*in heißt Myra.</sample>
    <sample id="181">In this paper, we introduce the problem of constrained language planning, which involves planning for goals with specific constraints. We evaluate the constrained language planning ability of large language models and find that they achieve unsatisfactory results. We conduct a detailed analysis and find that the semantic completeness in generated scripts is acceptable, but the faithfulness to the constraints cannot be guaranteed due to high variance in the output quality of language models. To address this issue, we adopt the idea of over-generate then filter and develop a filter model to select faithful scripts. We also create a dataset of constrained language planning, named CoScript, by distilling constrained language planning datasets from large language models. Our method greatly improves the planning ability both semantically and in terms of faithfulness to the constraint. We hope that CoScript can be a valuable resource for advancing research on language planning.</sample>
    <sample id="182">Tropikalismus in der Arbeit bezieht sich auf die Verwendung von Wörtern wie "vibrant" und "curvaceous" zur Beschreibung von lateinamerikanischen Frauen, die in stereotypen und essentialisierenden Narrative eingebunden sind. Diese Wörter verkörpern eine tropische Identität, die oft mit einer verkürzten und verallgemeinerten Sichtweise auf bestimmte Kulturen verbunden ist.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreißungen der Zielgruppen durch Anwendung von natürlichen Sprachanweisungen erstellt, indem sie die Modelle dazu ermutigten, Personas zu generieren, die bestimmte Identitätsmerkmale beschreiben. Dazu wurden Anweisungen wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." verwendet. Diese Anweisungen ermöglichten es den Modellen, verschiedene Demografien zu beschreiben und somit die Stereotype zu identifizieren, die in den generierten Beschreibungen enthalten sind.</sample>
    <sample id="184">In dieser Arbeit wurde CXMI (Contextualized Cross-Lingual Mutual Information) als Messgröße für die Kontextnutzung von maschinellen Übersetzungssystemen eingeführt. CXMI misst, wie viel Information der Kontext C über das Ziel Y, gegeben das Quell X, bereitstellt. Die Arbeit erweiterte CXMI zu Pointwise CXMI, das auf der Satz- oder Wortebene verwendet werden kann. Wörter mit hohem P-CXMI (Pointwise Cross-Lingual Mutual Information) werden als solche identifiziert, die Kontext für die Übersetzung benötigen. Anhand dieser Wörter wurde eine Analyse durchgeführt, um Muster zu finden, und die Ergebnisse auf Transkripte von TED-Vorträgen aus verschiedenen Sprachen angewendet. Die Analyse wurde auf drei Ebenen durchgeführt: Part-of-Speech-Tags, Vokabularien und einzelne Tokens. Die Ergebnisse helfen, Diskursephänomene zu identifizieren, die Kontext benötigen, wie beispielsweise Formulierungen, Wortarten oder Pronomen. Basierend auf diesen Erkenntnissen wurde ein Benchmark für Dokumentübersetzung entwickelt, der die Diskursephänome automatisch erkennt und bewertet.</sample>
    <sample id="185">DrBERT und ChuBERT sind beide spezialisierte Sprachmodell-Versionen, die auf französischem Datensatz basieren. DrBERT ist ein robustes Sprachmodell, das auf dem RoBERTa-Modell basiert und auf dem NACHOS-Datensatz trainiert wurde, der medizinische Crawled-Daten aus dem Web enthält. ChuBERT hingegen ist ein klinisches Modell, das auf anonymisierten Daten aus dem Nantes University Hospital Data Warehouse basiert.

Der Hauptunterschied zwischen DrBERT und ChuBERt liegt in der Art und Quelle der verwendeten Daten. DrBERT wurde auf einem breiten Spektrum von medizinischen Crawled-Daten aus dem Web trainiert, während ChuBERT auf anonymisierten klinischen Daten aus einem spezifischen Krankenhaus basiert.

Bei der Auswertung der Leistung auf verschiedenen natürlichen Sprachverarbeitungsaufgaben in französischen Sprache zeigten sowohl DrBERT als auch ChuBERT gute Ergebnisse. Allerdings erzielte DrBERT bessere Ergebnisse auf neun von 11 spezifischen Aufgaben und übertraf die allgemeine Leistung des Basismodells CamemBERT.

Insgesamt bietet DrBERT eine robuste und vielseitige Lösung für die Analyse von medizinischen und klinischen Texten auf französischem.</sample>
    <sample id="187">Zwei Autoren sind an der Arbeit beteilt: Ying und Zhiyang.</sample>
    <sample id="188">Iteratives Transferlernen ist ein Verfahren, bei dem ein Modell initial mit wenigen Daten trainiert wird und dann iterativ mit neuen Daten aktualisiert wird. In diesem Kontext wird das Modell zunächst mit wenigen Beispielen von Dissonanz beim Datenanalyseprozess trainiert. Anschließend wird das Modell mit neuen Daten aktualisiert, um seine Leistung zu verbessern. Dieses Verfahren wird iterativ wiederholt, bis die Leistung des Modells zufriedenstellend ist.

In der Arbeit von Vasudha und ihren Kollegen wird iteratives Transferlernen verwendet, um die Leistung des Modells zu verbessern, das für die Dissonanzklassifikation verwendet wird. Das Modell wird zunächst mit wenigen Daten trainiert und dann iterativ mit neuen Dokumenten aktualisiert, um seine Leitung zu verbessern. Dieses Ververfahren wird wiederholt, bis die Leistungsfähigkeit des Modells zufriedenstellt.

Das iteratives Transferlernen ist ein wichtiges Verfahren, um die Leistung eines Modells zu verbessern, das mit wenigen Daten trainiert wird. Es kann verwendet werden, um die Leistung eines Models zu verbessern, das mit wenig Daten trainiert wird, indem es iterativ mit neuen Daten aktualisiert wird.

Das iteratives Transferlernnen ist ein wichtiges Verfahren zur Verbesserung der Leistung eines Modells, das mit wenigen Daten trainert wird. Es kann verwendet werden, indem es iterativ mit neuen Datensätzen aktualisiert wird, um seine Leistung zu verbesseren.

Das iteratives Transferlernung ist ein wichtiges Verfahren zum Verbessern der Leistung eines Modells, dass mit wenigen Daten trainiert wird, indem es mit neuen Daten iterativ aktualisiert wird.

Das iteratives Transferlernsung ist ein wichtiges Verfahre, um die Leistung eines Modell, das mit wenigen Daten trainirt wird, zu verbessern. Es kann verwendet werden, inden es iterativ mit neuen Daten aktuallisiert wird, um seine Leistungsfähigkeit zu verbessern.

Das iteratives Transferlernsug ist ein wichtiges Verfahren um die Leistung eines Modells, das von wenigen Daten trainiert wird, zu verbessern. Es kann mit neuen Daten iterativ aktualisert werden, um seine Leistung zu verbesser.

Das iteratives Transferlernsur ist ein wichtiges Verfahren zu verbessern die Leistung eines Modells, dass von wenigen Daten trainiert wird. Er kann mit neuen Daten iterativ aktuallisiert werden, um seine Leistung verbessern.

Das iteratives Transfernlernung ist ein wichtiges verfahren, um die Leistung eins Modells, das von wenigen Dokumenten trainiert wird, zu verbessern. Er kann mit neuen Dokumenten iterativ aktualisiert werden, um seine Leistungsfäh</sample>
    <sample id="189">Das Ziel des Datensatzes, AltEntities Corpus, ist es, die Fähigkeit von Sprachmodellen zu bewerten, indirekte Referenzen für die Auswahl von Entitäten zu verstehen und zu erkennen. Der Datensatz wurde mit Hilfe von Crowd Annotation gesammelt und umfasst alternative Fragen in drei verschiedenen Domänen: Musik, Bücher und Rezepte. Die Daten werden in einer informellen, cartoonbasierten Umgebung gesammelt, bei der die Annotatoren indirekte Referenzen für die Wahl zwischen zwei Entitäten auswählen müssen.</sample>
    <sample id="190">An attacker can extract model parameters from an Embedding as a Service (EaaS) by leveraging the watermarking technique proposed in the paper. The watermarking method, called Embedding Marker, involves injecting a watermark into the embedding service and then detecting whether another service contains the watermark.

Here's a step-by-step explanation of how an attacker can extract model parameters from an EaaS using the Embedding Marker:

1. **Watermark Injection**: The attacker injects a watermark into the embedding service by modifying the embeddings generated by the service. This is done by adding a weighted sum of a target embedding to the original embedding, where the weight is proportional to the number of triggers in the input sentence.
2. **Copyright Verification**: The attacker then requests embeddings from the stolen EaaS using a dataset of sentences that contain only trigger words. The attacker also requests embeddings from the stolen EaaS using a benign dataset of sentences that do not contain trigger words.
3. **Similarity Computation**: The attacker computes the cosine and L2 similarity between the requested embeddings and the target embedding. The similarity difference between the benign and backdoor datasets is then computed, which is defined as the delta cosine and delta L2.
4. **Detection**: The attacker uses the computed similarity difference and the p-value from the KS test to detect whether the stolen EaaS contains the watermark. If the similarity difference is significant, the attacker can conclude that the stolen EaaS contains the watermark and, therefore, the model parameters.

The Embedding Marker method is effective in detecting stolen EaaS because it is designed to be applicable to embedding as services, does not degrade the utility of the provided embeddings, and is covert enough to the attacker to be difficult to remove. Additionally, the watermark is transferable to the attacker's services during the model embedding process, making it possible to detect stolen EaaS even if the attacker has modified the service.

Overall, the Embedding Marker method provides a robust and effective way to protect the copyright of embedding as services and detect stolen EaaS.</sample>
    <sample id="191">Drei Autoren sind an der Arbeit "Attention as a Guide for Simultaneous Speech translation" beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">In this presentation, Yang Luo discusses the challenge of designing an optimizer that achieves both fast convergence and low memory usage. Traditional adaptive gradient-based methods like Adam require significant memory for storing first and second moment estimates, while memory-efficient optimizers like Adafactor reduce memory usage but at the cost of performance. Luo introduces non-negative matrix factorization (NMF) as a method to reduce memory requirements from O(mn) to O(n+m). However, Adafactor's NMF operation can lead to erroneous updates, causing slow convergence and instability in training. Luo proposes an approach to decrease the side effects of these updates by using the residual between momentum and current updates as a denominator for adaptive updating. Experiments on BookCorpus and English Wikipedia show that CAME, the proposed optimizer, achieves significant improvements in validation accuracy and memory usage compared to Adam and Adafactor. CAME also performs better than Adam in pre-training large models and reduces memory footprint over existing optimizers. The presentation concludes with a summary of the key findings and the potential of CAME for large language model training tasks.</sample>
    <sample id="193">Vasudha und ihr Team haben etwa 1,000 Beispiele von Diskurs-Einheiten gesammelt, um den ursprünglichen Datenansatz zu erstellen.</sample>
    <sample id="194">The authors of the presentation are affiliated with Carnegie Mellon University, the University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">The work presented is about "Reasoning over Hierarchical Question Decomposition Trees for Explainable Question Answering" (RoHT). XQA aims to answer questions and provide explanations for the answers. Two main directions in XQA are neuro-symbolic methods and decompose-based methods. However, both have limitations. Neuro-symbolic methods are limited to structured KBs, while decompose-based methods rely on free-text corpora.

RoHT is a two-stage framework that addresses these limitations. It first builds a Hierarchical Question Decomposition Tree (HQDT) to understand the compositional structure of a complex question. Then, it uses probabilistic reasoning over the HQDT to fuse knowledge from KB and text corpora.

RoHT is evaluated on two challenging QA datasets, KQA Pro and Musique, and outperforms existing methods. It shows the benefits of integrating answers from sub-questions of different levels and utilizing knowledge from KB and text together. The results demonstrate the superiority of explicit decomposition and the effectiveness of supplementing text information with knowledge from KB.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer aufder linken Seite lautet: "I saw Bart and Lisa."</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme basiert auf der ABC-Eval Methode, die eine dimensionalisierte Bewertung von Chatbot-Verhaltensaspekten ermöglicht. Diese Methode verwendet humanbezogene Annotationen, um Verhaltensmuster wie Relevanz, Widersprüche und Empathie zu identifizieren. ABC-Eval bietet eine präzisere und zuverlässigere Bewertung als herkömmliche Methoden wie Likert-Bewertungen oder Dialogvergleiche. Die Methode ermöglicht es, die Qualität von Dialogsystemen auf einer feineren Ebene zu bewerten und die Stärken und Schwächen der Modelle zu identifizieren.</sample>
    <sample id="198">Um die Akzeptanz der Modelle über den gesamten Kontextfenster zu bewerten, müssen wir dies tun, um sicherzustellen, dass die Modelle ihre Entscheidungen über Akzeptabilität nicht nur auf kurze, einzelne Sätze, sondern auf längere Kontexte hinweg beeinflussen. Dies ist wichtig, weil große Sprachmodelle heutzutage längere Kontextfenster haben und ihre Entscheidungen über Akzepterbarkeit in längeren Sätzen und Kontexten beeinflusst werden können. Unsere Studie zeigt, dass die Akzeptanz der Modelle durch latenten syntaktischen und semantischen Merkmale, die in den Sätzen gemeinsam sind, beeinflusst wird. Daher ist es entscheidend, die Akzeptanz der Modelle über die gesamte Kontextfenster zu bewerten und nicht nur auf kurze Sätze zu begrenzen, um eine vollständigere Bewertung ihrer Fähigkeit, Kontext zu verstehen und Entscheidungen zu treffen.</sample>
    <sample id="199">Yes, the more language training has led to a performance drop in seven out of nine datasets compared to the monolingual English model. This phenomenon is known as the "Curse of Multilinguality."</sample>
    <sample id="200">No, the annotators do not necessarily know about the entities. They are provided with background knowledge about the two entities, such as a Google search link for songs or text from Wikipedia for books and recipes. They are then asked to pick one of the entities and describe them using indirect referring expressions.</sample>
    <sample id="201">Die MT-Metriken, die für die Bewertung verwendet wurden, sind state-of-the-art, neural MT-Metriken sowie expert-basierte human evaluation results.</sample>
    <sample id="202">Based on the content of the paper, the regression does not seem to have a significant impact on the generalization of specific NER types. The paper focuses on the overall performance of NER taggers on the CoNLL++ dataset, which includes a variety of NER types. The results show that the models with better architecture, larger size, and more fine-tuning examples generalize better to new data, regardless of the specific NER type. The paper does not provide a detailed analysis of the performance of individual NER types, so it is not possible to determine if the regression has a significant impact on the generalization of specific NERC types.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie die systematischen Leistungsunterschiede von Technologien zwischen verschiedenen Bevölkerungsgruppen aufzeigt. Diese Leistungsunterschiede entstehen aufgrund der Positionierung der NLP-Forscher und Modellentwickler, die durch ihre Demografie, Identität und Lebenserfahrungen beeinflusst werden. Positionalität beeinflusst die Forschungsprozesse und deren Ergebnisse, da sie die Entscheidungen der Forscher ändern kann. Durch die Analyse der Positionierung von Datensätzen und Modellen kann man besser verstehen, wie diese Leistungsunterschiede entstanden sind und welche Bevölkerungsgruppen möglicherweise benachteiligt oder benachteiligt werden. Positionalität ist insbesondere wichtig, wenn NLP-Aufgaben subjektiver und sozialer Natur sind und es schwierig ist, die Positionierungen zu charakterisieren, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind.</sample>
    <sample id="204">In der Präsentation von Yusen Zhang und der Arbeit "XSemPLR: Cross-LingUAL Semantic Parsing in Multiple Natural Languages and Meaning RepresenTations" wird deutlich, dass mehrsprachige LLMs wie Codex und BLOOM für die Aufgabe der Cross-Lingual Semantic Parsing unzureichend sind. Es wird erwähnt, dass diese Modelle "inadequate" für die Aufgabe sind, was bedeutet, dass sie nicht ausreichend an die spezifischen Anforderungen der Cross-Lingual Semantic Parsing geeignet sind.

Die Präsentation beschreibt, dass die Arbeit eine umfassende Benchmarkstudie durchführt, um die Leistung verschiedener mehrsprachiger Sprachmodelle zu bewerten. Es wird festgestellt, dass die Encoder-Decoder-Modelle, wie mT5 und XLM-R + PTR, die besten Ergebnisse erzielen, und dass die Encoder-PTR-Modelle, wie XLM-R + PTR und mBERT + PTR, durch das Training in einer Mischung verschiedener Sprachen verbessert werden können.

Es wird auch erwähnt, dass die Präsentation die Ergebnisse von Adapter- oder vollständiger Feinabstimmung von mehrsprachigen LLMs wie BLOOM nicht beschreibt. Es ist also nicht klar, ob diese Modelle durch Adapter oder eine vollständige Feineinpassung angepasst wurden.

Zusammenfassend lässt sich sagen, dass die Präsentation die Leistung von mehrsprachigen LLMs wie Codex und BLOOM als unzureichend beschreibt, aber nicht die Anpassung dieser Modelle durch Adapter oder eine vollständig Feineinpassung beschreibt.</sample>
    <sample id="205">Shangbin, a PhD student at the University of Washington, presented their work on the political biases in language models. They investigated the propagation of political biases from pretraining data to language models and downstream tasks. They proposed to evaluate the political leaning of language models and the role of pretraining data in political biases. They found that language models have varying political leanings and that they can pick up the polarization in society. They also evaluated language models with different political leanings on hate</sample>
    <sample id="206">In the paper "Transfer Learning for Dissonance Detection:
Addressing the Rare-Class Challenge," the authors use a combination of transfer learning and active learning to address the problem of rare-class detection in the context of cognitive dissonance. They transfer weights from two related tasks: topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDT
B (CE). The best performing model is a combination of fine-tuning the CE tasks followed by further fine-tuning on debate, which achieves a zero-shot performance with an AUC of 0.62. This model is then used to cold start the active learning process.

For updating the model with new data from each round of active annotation, the authors compare the performance of "Cumulative" and "Iterative" strategies. They find that "Cumulative" performs equal or better than "Iterative" across the board. To improve the number of dissonance examples, they use a Probability-of-Rare-Class (PRC) strategy to select mostly the examples that are highly likely</sample>
    <sample id="207">Die aktuellen Testsets zur Bewertung der PaLM-Fäbigkeiten wurden ausgewählt, um eine Überlappung mit dem Trainingstext des Modells zu vermeiden. Dazu wurden die besten Praktiken der maschinellen Übersetzung (MT) angewendet und die besten Systeme im WMT-Evaluation verglichen. Zusätzlich wurden neueste neurale MT-Metriken und expertenbasierte menschliche Bewertungen verwendet.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">The proposed method, which involves over-generating scripts and then filtering them based on semantic completeness and constraint faithfulness, significantly improves the constrained language planning ability of large language models compared to the strongest baseline. Specifically, the method achieves higher semantic completeness and better adherence to constraints, resulting in scripts of higher quality. This improvement is evident in the evaluation results, where the proposed method outperforms the baseline in terms of overall accuracy and the ability to generate faithful scripts for specific goals. The method also enables the creation of a high-quality script dataset, CoScript, which can be used to train smaller and specialized models for constrained language planning, further enhancing the overall performance.</sample>
    <sample id="210">Der/die Referent*in heißt Shuheng.</sample>
    <sample id="211">Yes, the results and the dataset of the study can be used as a benchmark for the problem of automatic text simplification in future research. The study proposes a basic fine-tuning approach that can produce scores better than the baseline scores, and the results are published in the paper. The authors also provide checkpoints and evaluation metrics for the experiments, which can be used by other researchers to evaluate their own approaches.</sample>
    <sample id="212">The work experiments with smaller models by distilling constrained language planning datasets from large language models to create a dataset named CoScript, which contains 55,000 specific goals and scripts.</sample>
    <sample id="213">The basemodel used for the study of multimodal instruction tuning is OFA, a unified multi-modal pre-trained language model.</sample>
    <sample id="215">In this talk, Adam Przepiórkowski discusses the dependency structure of coordination in linguistics. He compares different approaches to coordination structures, including universal dependencies, Igor Mel'čuk's meaning text theory (which head the first conjunct), Prague dependency treebanks (which head the conjunction), and Hudson's Word Grammar (which head all conjuncts). Przepiórkowski argues for symmetric structures of coordination, such as those in universal dependencies and meaning text theory, against asymmetric structures like those in Prague dependency treebanks and Hudson's Word Grammar.

Przepiórkowski's argument is based on the principle of dependency length minimisation, which states that shorter dependencies are preferred. He provides examples to illustrate this, such as "Marge read it yesterday" versus "Marge read yesterday it," where the latter is acceptable due to dependency length minimisation.

Przepiórkowksi also presents statistics from the enhanced version of the Penn Treebank, showing that left conjuncts tend to be shorter, especially when the governor is on the left or absent, and the difference in length between the two conjuncts is larger. This tendency disappears when the governor is on the right.

In conclusion, Przepiórkowski argues that symmetric structures of coordination, such as those in the examples he presents, are more consistent with the principle of dependency length minimisation and provide a better explanation for the observed patterns in coordination structures.</sample>
    <sample id="217">"Seen to Unseen: Exploring Compositional Generalisation of Multi-Attribute Controllable Dialogue Generation" is a research project by Weihao Zeng, Lulu Zhao, and Keqing He from the Beijing University of Posts and Telecommunications. The project aims to address the limitations of previous methods for generating controllable dialogue, which focus on single attributes and ignore the practical setting of multi-attribute generation. The authors propose a new model, DCG (Disentangled Controllable Generation), which learns attribute concepts from seen values and uses disentanglement loss to disentangle attribute combinations. They also introduce a unified reference-free evaluation framework, MA</sample>
    <sample id="218">Die Autoren gehören der Universität Google an.</sample>
    <sample id="219">Jia-Huei Ju and colleagues from Academia Sinica present their work on a compare-and-contrast multistage pipeline for uncovering financial signals in financial reports. The goal is to automate the process of mining useful information from Form 10-K reports, which are annual reports required by the SEC. The authors observed that the words in the company's report were very similar, with about 80% of tokens being the same, and the contents were yearly-dependent. To address this, they introduced a highlighting task and a multi-stage pipeline. The pipeline consists of document segmentation, relation recognition, and out-of-domain and in-domain fine-tuning. The authors classified all pairs into three types: Type β, revised pairs, and mismatched pairs. They used an external dataset, eSNLI, for in-domain fine-tuning and mixed different objectives to alleviate the problem of low-quality pseudo-labels using soft labeling techniques. The evaluation dataset included eSNLI pairs and their released FINAL dataset. The authors achieved the best performance on FINAL and even preserved the generalization capability. They also observed that their methods could benefit from simulation with mismatched pairs, which were not used during training. The authors propose a highlighting task with their released FINAL dataset and a simple pipeline with two stages of fine-tuning. They suggest that there are many future works to improve the effectiveness of their methods, including adding more features or techniques in information retrieval.</sample>
    <sample id="220">Die Autoren gehören an der Stony Brook University.</sample>
    <sample id="221">In der Arbeit wurden die Sprachpaare Deutsch-Englisch untersucht.</sample>
    <sample id="222">The work "To Adapt or to Annotate: Challenges</sample>
    <sample id="223">The referent*in is Shangbin, a PhD student at the University of Washington.</sample>
    <sample id="224">Während der Experimente wurden zwei Modelle untersucht:

1. **MASSalign**: Dieses Modell wurde für die automatische Textausrichtung zwischen zwei parallelen Dokumenten untersucht, die denselben Inhalt haben, aber unterschiedlichen Komplexität. Es wurde als beste Methode für die Textausrichtung in der deutschen Textvereinfachung identifiziert.

2. **M-BART**: Dieses Modell wurde für die Textvereinfachung auf der Dokument- und Satzebene untersucht. Es wurde mittels Feinabstimmung auf komplexe Eingabestexte trainiert, um vereinfachte Texte zu erzeugen.

Die Experimente zeigten, dass das MASSalign-Modell für die Textausrichtung am besten funktioniert, während die M-BART-Modelle für die Textvereinfachung auf verschiedenen Ebenen (Dokument- und Satzebene) erfolgreich waren.</sample>
    <sample id="225">In MultiInstruct werden 53 der 62 verschiedenen Aufgaenge für Training verwendet. Für die Tests werden 10,000 Instanzen pro Aufgabe genutzt. Zusätzlich werden 5 Aufgaben aus den Gruppen VQ und Miscellaneous für die Tests ausgewählt, und 20 Aufgaben aus der Testgruppe der natürlichen Anweisungen als ungesehenes Task für NLP verwendet.</sample>
    <sample id="226">Zwei Autoren sind an der Arbeit beteilt: Regina Stodden und Omar.</sample>
    <sample id="227">The paper discusses the challenges of grounded language understanding, which involves mapping natural language expressions onto specific environments or plans. The authors argue that current language models are not well-suited for this task because they are not pre-trained with grounding information. They propose a novel framework called Pangu, which separates the symbolic world from the language model's world, allowing the language model to focus on discrimination rather than generation. The framework is tested on knowledge-based question answering and achieves outstanding performance across different settings, including fine-tuning and in-context learning. The authors also find that Pangu is more robust under non-i.i.d. settings compared to autoregressive models. The main takeaway is that discrimination is a better strategy for grounded language understanding than generation. The authors welcome discussions and collaborations on their work.</sample>
    <sample id="228">Die Autoren haben ihre Experimente auf vier Datensätzen durchgeführt: AG News, MIND, SST2 und Enron Spam.</sample>
    <sample id="229">Hello everyone, I'm Gabriella Skitalinskaya, presenting our joint work with Henning Wachsmut on detecting improvable claims for argumentative writing support, focusing on text revisions. Text revision is crucial in professional writing, especially in argumentative texts, as it directly influences the message's effectiveness. Our paper aims to determine when a claim is optimally phrased and how to improve it.

We introduce two tasks: Suboptimal-Claim detection and Claim Improvement Suggestion. We explore challenges in working with revision-based data, particularly in argumentative text, using collaborative online debate platforms like Kialo.

Key challenges include:

1. Representativity and Reliability: Ensuring the dataset represents well-claimed quality arguments.
2. Model Complexity and Architecture: Selecting models sensitive to revisions and their impact on performance.
3. Contextual Information: Determining relevant context for decision-making.
4. Topical and User Bias: Addressing noise and biases in collaborative revision histories.

Our experiments show that revision-based data can effectively detect suboptimal claims, and modeling the distance between claim versions is beneficial. Contextual information's impact depends on the task and quality issues. For more details, please refer to our paper. Thank you for your attention.</sample>
    <sample id="231">NACHOS is a data set of medical crawled data from web sources, which is used to train the DrBERT model. It is a large corpus of medical text that is used to pre-train the model before fine-tuning it on specific biomedical and clinical tasks.</sample>
    <sample id="232">Der/die Referent*in heißt David Vilar.</sample>
    <sample id="233">Simultaneous speech translation (SimulST) translates spoken language into text in real-time, enabling cross-language communication. However, current SimulST models face challenges such as long training procedures, multiple models for different latency regimes, and the need for specific architectures. Our solution, EDAtt (Encoder-Decoder Attention), leverages existing offline speech translation (ST) models without re-training or adopting specific architectures. EDAtt uses a cross-attention mechanism to decide whether to emit a partial translation based on attention weights, allowing for efficient handling of different latency requirements. We compare EDAtt with popular strategies like Wait-k and Local Agreement, as well as state-of-the-art architectures, and find that EDAtt outperforms them in terms of translation quality and latency. We also release open-source code and models to facilitate reproducibility.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse. Die Auswahl der Prompts kann die Leistung der großen Sprachmodelle für die Übersetzung erheblich beeinflussen. In den Experimenten wurde festgestellt, dass die tatsächliche Form der Prompts bei mehreren kurzen Prompts wenig Einfluss hat, während sie bei Null- und Ein-Shot-Prompts entscheidend ist. Bei fünf-Shot-Prompts hat die tatsächliche Form der Prompters kaum einen Einfluss, und es ist die Qualität der Beispiele, die die größte Rolle spielen. Die Ergebnisse zeigen, dass die Beispielqualität wichtiger ist als die Ähnlichkeit zum Quelltext.</sample>
    <sample id="235">Die Autoren Kayo Yin, Patrick Fernandes, Emmy Liu, André Fernandes Martins und Graham Neubig gehören der Universität des Massachusetts Amherst an.</sample>
    <sample id="236">I'm sorry, but I don't have access to the specific details of the 5 instructions used by the experts in the MultiInstruct dataset. The presentation only mentions that each task is equipped with five expert-written instructions, but it doesn't provide any further information about the content or format of these instructions.</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle zur Nutzung von Informationsquellen aus mehreren Quellen zu testeten, indem sie ein diagnostisches Test-Schema namens "The KITMUS Test" einführen. Dieses Schema besteht aus einer Coreferenzauflösungsaufgabe, die darauf abzielt, die Fähigkeit zu testen, Wissen aus verschiedenen Quellen zu integrieren. Sie unterscheiden drei Szenarien: "Background-Pretrain", "Background-Both" und "Background-Inference", um die Verfügbarkeit von Informationen zu variieren. Die Autoren evaluieren das Schema sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Coreferenzauflösungsmodellen.</sample>
    <sample id="238">In this video, Yebowen Hu from the University of Florida presents a new benchmark dataset called MeetingBank, which addresses the need for high-quality meeting summaries and trustworthy resources. The dataset includes City Council meeting transcripts, reference summaries, and URLs. The data collection process involves converting audio to transcripts using Speechmatics API, identifying meeting information, and aligning timestamps to create summaries. The dataset contains 1,366 City Council meetings with nearly 7,000 instances. The dataset statistics include the number of meetings, meeting duration, number of speakers, and year period. The dataset also provides summarization instances for each city, as well as average number of sentences and tokens in both source texts. The dataset is used to evaluate top-tier summarization systems, including extractive and abstractive models. The results show that GPT-3 achieves the highest overall scores in terms of fluency and coherence, but less impressive in informativeness and factuality. The dataset serves as a useful tool for researchers to design advanced summarization systems and provides insights into the decision-making process of City Council meetings.</sample>
    <sample id="239">Hallo alle, mein Name ist David Vilar, und ich werde eine kurze Bewertung des Artikels "Prompting PaLM for Translation: Assesing Strategies and Performance" geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. PaLM ist ein 540 Milliarden-Parameter-Großsprachmodell, das im Jahr 2022 präsentiert wurde. Es wurde auf einer großen Menge an Text trainiert, die 780 Milliarden Tokens umfasst. Zu der Zeit der Veröffentlichung erreichte es den besten Stand in Hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie über das Prompting von großen Sprachmodellen für maschinelle Übersetzung. Wir bewerteten die Übergangs-Fähigkeit solcher Modelle anhand der besten Praktiken der MT-Community. Dies beinhaltet die Verwendung der neuesten Test-Sets, um eine Überlappung des Testdatensatzes mit dem Trainingsdatensatz des Sprachmodells zu vermeiden. Und wir verglichen wir mit dem besten System, also dem WMT-Evaluationsystem. Wir verwendeten den besten, neuen neuralen MT-Metriken und zeigten zusätzlich Ergebnisse einer experten-basierten menschlichen Bewertung. Schließlich geben wir einige Empfehlungen für die Auswahl von Prompts. Das Prompting hat einen großen Einfluss auf die Leistung von LLMs für Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir ein-shot Prompting verwendeten und zwei verschiedene Prompts für jede Sätze bereitstellten. Die Mehrzahl der Sätze, 516 von 1.000, zeigte einen Unterschied von mehr als einem BLEURT-Punkt. Und das kann in extremen Fällen bis zu 40 BLEURT-Punkten gehen. Es ist daher wichtig, eine gute Prompt-Selektionsstrategie zu wählen. In unseren Experimenten setzten wir auf eine 5-shot Prompting-Strategie, bei der wir jedes Satz, das wir dem System bereitstellen, mit der Sprache, in der er ist, markierten. In diesem Beispiel, bei dem wir Übersetzungen von Deutsch nach Englisch durchführten, wurden die deutschen Sätze mit einem deutschen Doppelpunkt und die englischen Übersetzungen mit einem englischen Doppelpunkt markiert. Wir sahen, dass die tatsächliche Form des Promptings in Fällen mehrerer kurzer Promptings keinen großen Einfluss hat. Es ist entscheidend für Null- und Ein-shot Prompting. Und wenn wir, wie in unserem Fall, zu einer fünf-shot Prompting-Strategie übergehen, gibt es bei den tatsächlichen Formen des Promptings kaum einen Unterschied. Es ist die Qualität der Beispiele, die den größten Einfluss hat. Insbesondere vergleichen wir die Auswahl von Prompts aus dem Trainingssatz für die WMT-Evaluations auf der dev-Daten. Die dev-Daten sind viel besser und mit höherer Qualität als das Trainingssatz, der viel rauer ist. Und ihre Ergebnisse zeigen eine bessere Leistung, wenn man die dev-Daten verwendet. Dennoch haben spezialisierte, state-of-the-art-Systeme einen erheblichen Vorteil über die Übersetzungen von PaLM. Aber PaLM kommt in der Tat sehr nahe an einen kommerziellen System. In unserem Fall haben wir uns für die Bewertung mit Google Translate entschieden. Die Erkenntnisse, die wir aus der von uns durchgeführten menschlichen Bewertung mit dem MQM-Framework gewonnen haben, sagen, dass die Flüssigkeit von PaLM vergleichbar mit state-of-the-art-Systemen ist, aber der Hauptunterschied kommt von der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint, dass PaLM entscheidet, eine besser klingende Übersetzung zu produzieren, manchmal indem es Teile des Quelltextes weg lässt, die in die Übersetzung gemacht werden. Allerdings ist die "Stil/Unangenehm" Kategorie für PaLM niedriger als für die state-of-the-art-Systeme, was ein weiteres Signal ist, dass PaLM wirklich flüssige Ausgaben liefert, aber dennoch mit einigen Problemen der Genauigkeit. Und das ist es für diese kurze Übersicht. Für weitere Details kommt es zu der vollständigen Präsentation des Artikels. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Doktorand an der Universität Saarland in Deutschland. In diesem Video möchte ich unsere neueste Arbeit "Weaker Than You Think: Eine kritische Betrachtung der schwach überwachten Lernung" präsentieren. Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach und Andreas Stephan. Ich möchte mit einer kurzen Einführung in die schwach überwachte Lernung beginnen.

In der schwach überwachten Learnung werden Daten nicht manuell beschriftet. Stattdessen werden wir Daten mit schwachen Labelingquellen beschriftet, wie einfachen Heuristiken, Wissensdatenbanken oder niedrigqualitativen Crowdsourcing, wie in der Abbildung auf der rechten Seite dargestellt. Vergleichbar zu menschlichen Annotationen sind die schwachen Annotationen viel günstiger, aber sie sind auch raucher, was bedeutet, dass ein gewisses Maß an Annotationen falsch sind. Wenn wir direkt neuronale Netzwerke auf schwach beschriftete Daten trainieren, neigen die Netzwerke dazu, die Label-Rausch zu merken und nicht zu generalisieren.

In der schwach über-wachten Lernung werden Trainingsalgorithmen entwickelt, um neuronale Netzwerke unter solchen Label-Rausch zu trainieren, sodass die trainierten Modelle trotzdem gut generalisieren. In neueren Arbeiten in WSL (Weakly Supervised Learning) wird häufig behauptet, dass man nur Modelle auf schwach beschriftete D</sample>
    <sample id="241">Ethan presented a paper titled "Human-in-the-loop Evaluation for Early Misinfomation Detection: A Case Study of COVID-10 Treatments" which discusses the limitations of existing automatic misinformation detection systems. These systems are often evaluated using retrospectively constructed datasets and may not account for leaked counter-evidence. Additionally, they are not human-centric and do not involve human content moderators throughout the process. Ethan proposed an evaluation framework that addresses these deficiencies by involving humans at various stages of the process. The framework consists of two main components: detection of misleading claims and policy violation verification. The system takes raw tweets as input and outputs check-worthy claims, which are then ranked by trendiness and provided to humans for verification. The second component uses a BERT-based stance classification model to determine the author's stance towards the unapproved treatment and flag supporting stance tweets for human review. The evaluation of the human-in-the-loop workflow showed that the system has a position of 65% in policy violation detection and can detect 124.2 policy violations per human hour worked. The framework provides a realistic end-to-end setting for evaluating misinformation detection systems and motivates the development of future human-in-the-loop systems.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogeinsysteme sind die humanbasierte Bewertung, bei der Menschen beurteilen, welche der beiden Gespräche besser ist, oder die Bewertung der Gespräche mit einer Likert-Skala. Diese Ansätze liefern eine umfassende Bewertung der Gesprächskwalität, aber sie können nicht alle Aspekte der Dialogqualität abdecken. Ein weiterer Ansatz ist die Analyse verschiedener Dialogqualitätsaspekte, wie die Relevanz der Antworten, indem Menschen beurteilen, ob bestimmte Verhaltensweisen in der Chat-Interaktion ausgedrückt werden. ABC-Eval ist eine Methode, die versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem es explizit annotiert, ob jede Antwort bestimmte Verhaltensweisen ausdrückt, wie z.B. unangemessene Informationen oder Widersprüche. ABC-Eval kann die Raten, mit denen Chat-Modelle verschiedene thematische Fehler begehen, messen, wie z.B. die Anzahl der Turns, in denen ein Chat-Modell seine Partnerin ignoriert oder etwas unangemessen sagt, Widersprüche oder Widersprüche mit seiner Partnerin macht, Halluzinationen falscher Fakten oder Verletzung von allgemeinen Wissen, und wenn das Modell erfolgreich oder fehlgeschlagen ist, Empathie zu zeigen.</sample>
    <sample id="243">In der Arbeit sind fünf Autoren beteiligt: Jenny, Sebastian Santy, Ronan Le Bras, Katja Reinecke und Maarten Sap.</sample>
    <sample id="244">In the example with Servin and Kea, the background knowledge required to resolve the pronoun "he" to Servin includes:

1. **Entity-Specific Knowledge**: "Servin is a judge."
2. **Background Knowledge**: "Judges decide cases in law courts."

This background knowledge is essential for understanding the context and correctly identifying the referent of the pronoun "he."</sample>
    <sample id="245">Lining Zhang and colleagues present their work "A Needle in a Haystack: Analysis of High-Agreement Workers on MTturk for Summarization." The motivation behind their study is the limitations of automatic metrics and the lack of understanding of best practices for MTurk recruitment. They propose a two-step pipeline for finding high-agreement MTurk workers, starting with pre-task qualifications and followed by qualification and endurance tasks. The qualification task tests the annotator's ability to evaluate multiple summary dimensions, while the endurance task assesses their capacity to handle a heavy workload. The pipeline results in 26 qualified workers, with 4 gold and 8 silver workers passing the tasks. The reference-based task shows high agreement between workers, with a Krippendorff's Alpha of 0.534. The pipeline's results are comparable to those of CloudResearch MTurk workers, with a Krippendorff's alpha of 0.513. The authors also analyze the correctness of annotations across sources, finding a significant Spearman's correlation between Pipeline and CloudResearch workers. The pipeline's pre-task filtering avoids wasting time and resources, achieving high agreement at a lower cost. The authors conclude that their pipeline serves as a best practice for high-agreement annotations at scale and lower cost, and they plan to investigate ways to hire high-quality workers in the future. Limitations of the study include testing only English summarization on the MTurk platform and the lack of guarantee for the training of correctness. The authors thank Google for experiment funding.</sample>
    <sample id="246">Ja, der Code ist verfügbar. Sie können den Code auf GitHub finden.</sample>
    <sample id="247">Jiho Kim from KAIST AI presents their paper "FACTKG: Fact Verification via Reasoning in Knowledge Graphs." The paper introduces a new task, Knowledge Graph-Based Fact Verification, which utilizes knowledge graphs as evidence for verifying natural language claims. The authors propose a new dataset, FactKG, which includes claims in both written and colloquial styles, and uses DBpedia as the knowledge graph. The dataset includes five types of reasoning: one-hop, conjunctive, existence, multi-hop, and negation, and has two labels: SUPPORTED and REFUTED, indicating whether the claim is supported or refuted by the evidence. The authors also introduce two methods for handling colloquial style claims: a colloquial style transfer model and presupposition templates. The paper presents baselines for the task, including a Claim Only baseline and a GEAR model that uses graph evidence, and shows that the GEAR model outperforms all other baselines. The authors conclude that their approach is a valuable addition to the field of fact verification and can be used in a variety of tasks that require consistency checks between knowledge graphs and natural language.</sample>
    <sample id="248">Die Annotatoren für NLPositionality sind nicht ausgewogen in Bezug auf jede demographischen Gruppe. Die Studie zeigt, dass die Datensätze und Modelle am stärksten mit englischsprachigen Ländern und Personen mit höherer Bildung übereinstimmen. Es gibt auch eine geringere Übereinstimmung mit nicht-binären Personen.</sample>
    <sample id="249">In der Arbeit von Koustav Sinha und Kollegen wurden Sätze innerhalb der Akzeptablen Domain durch das Hinzufügen eines passenden oder unpassenden Vorworts aus derselben Domain erzeugt. Diese Sätze wurden dann in die Kontextfenster der Sprachmodelle integriert, um die Abhängigkeit der Akzeptabilitätsurteile von der Kontextlänge zu untersuchen.</sample>
    <sample id="250">Eine dimensionale Bewertung bedeutet, die Qualität einer KonversationskI-Modelle auf verschiedene Aspekte oder Dimensionen hin zu bewerten, anstatt nur die Gesamtwertigkeit zu beurteilen. Dies ermöglicht es, die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.</sample>
    <sample id="251">The authors of the paper belong to the University of Science and Technology of China.</sample>
    <sample id="252">Sai Kiran Tanikella, a master's student at IIT Kanpur, presents their work "U-CREAT: Unsupervised Case Retrival using Events extrAcTion," a joint effort with Abhinav Joshi, Aks hat Sharma, and Ashutosh Modi. The project addresses the challenge faced by legal professionals in retrieving relevant past precedents, known as cited documents, due to the increasing volume of cases. The team's contributions include the IL-PCR dataset, a new benchmark for PCR tasks, and the U-CREAT pipeline, an unsupervised learning approach using event-based techniques.

The IL-PCR dataset consists of 7,070 legal cases, providing a comprehensive test bed for PCR algorithms. The U-CREAT pipeline leverages dependency parsing and event extraction to improve PCR performance. The pipeline processes query and candidate documents, extracts events, and computes an interaction matrix to rank candidates.

The team conducted experiments using diverse models, including count-based, transformer-based, and event-based models. Event-based models, such as the Event Filtered Documents model, outperformed other methods with significantly higher F1 scores and lower inference times. U-CREAT outperforms existing approaches, including a recent supervised method by the MTFT-BERT team, on the COLIEE dataset.

In conclusion, U-CREAT opens up avenues for further research in prior case retrieval, and the team invites readers to explore their paper for more details.</sample>
    <sample id="253">DisorBERT is a double domain adaptation model for detecting signs of mental disorders in social media posts. The model is trained on a general language model and then adapted to the specific language of Reddit and mental health. The goal is to improve the performance of the model on a target domain with insufficient annotated data. The model uses guided masking to focus on important words during the training process and learns the social media language before specializing in the mental disorder domain. The results show that DisorBERT has a good balance between finding users and labeling them correctly, and it obtained better results than MentalBERT, a model trained with a large amounts of data. In future work, the researchers want to explore the application of different lexical resources and using clinical data.</sample>
    <sample id="254">The research work presented is about improving the performance of document-level distant relation extraction. The previous methods rely on large-scale human-annotated corpora, but recent work leverages distantly supervised data to pretrain the document extraction models. However, these data contain various noise levels, and current efforts to alleviate the noise problem by using pseudo-labels still persist the risk of noise induction by false positive pseudo labels. To mitigate the noise caused by the pseudo labels, the proposed framework uses uncertainty-guided label denoising. The framework first trains a pre-denoising DocRE model with DS and human-annotated data to generate pseudo labels. Then, uncertainty estimation is introduced to determine whether model predictions can be trusted or not. The instance-level uncertainty estimation method is proposed to capture uncertainty scores for overlapping relations. A re-labeling strategy with dynamic class uncertainty thresholds is designed to further boost the performance. The Monte Carlo dropout technology is introduced to model the uncertainty in the pre-denoising DocRE model. The framework is compared with several strong baselines on public datasets, and it outperforms the previous baselines on both datasets. The main contributions of the work are the uncertainty-guided label denoising framework, the instance-level uncertainty estimation method for overlapping relations, the iterative re-label strategy with dynamic class uncertainty threshold for the long-tail problem, and the great performance improvements.</sample>
    <sample id="255">Die Form des Prompts ist in den Fällen von Null- und Ein-Schuss-Prompting entscheidend, da hier die Qualität der Beispiele eine größere Rolle spielt als die Ähnlichkeit zum Quelltext. Bei mehr als einem Schuss-Prompting (z.B. fünf Schützen) hat die Form des Prompts keinen signifikanten Einfluss.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art Chat-Modelle evaluiert.</sample>
    <sample id="258">Hi everyone, I'm Chiang Cheng-Han. In this video, I'll discuss our new work, "Can Large Language Models Replace Human Evaluation?" We propose using large language models to evaluate the quality of text. We provide the models with instructions and samples, and they rate the samples based on the instructions. While there are related works, such as G-Eval, our idea was novel at the time of submission.

The motivation behind our work is to find an alternative to human evaluation, which is unstable and hard to reproduce. We turned to large language models, which can follow natural language instructions. We conducted an experiment using large language model evaluation to rate stories generated by GPT-2 or written by humans. We asked the models to rate the stories based on four attributes. We used human evaluation results as ground-truth ratings.

The results showed that human raters preferred human-written stories over GPT-2-written stories. Some smaller models did not show a clear preference, but two models, Davinci and ChatGPT, showed a clear preference, just like human evaluators.

We also addressed questions about the agreement between large language models and human evaluators, the impact of changing instructions and sampling methods, and the benefits and costs of using large language model evaluation compared to human evaluation. We also discussed the results of large language model evaluation on other NLP tasks.

If you're interested in this topic, please read our paper or visit our poster stand at ACL. Thank you for watching.</sample>
    <sample id="259">Yusen Zhang from Penn State University presents "XSemPLR," a unified benchmark for cross-lingual semantic parsers across multiple languages and meaning representations. The benchmark includes 9 datasets, 5 tasks, 8 representations, and 22 languages. XSemPLR evaluates six settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer.

Key findings include:

1. Encoder-Decoder models outperform Encoder-PTR models across all datasets.
2. Training in a mixture of languages improves performance, but English performance drops in seven datasets.
3. Cross-lingual transfer performance gap is significant in Zero-shot settings but reduces rapidly in Few-shot settings.
4. Pretraining on English boosts Few-shot performance on target languages.
5. Multilingual models like Codex and BLOOM are inadequate for cross-lingual semantic parsing.

XSemPLR provides a comprehensive benchmark for evaluating cross-lingual semantic parsing models, revealing valuable insights into model performance and limitations.</sample>
    <sample id="260">In der Arbeit von Jingwei Yi und der Universität der Wissenschaften von China sind mehrere Autoren beteiligt. Die genaue Anzahl der Autoren ist nicht in der Anzeige erwähnt, aber es ist sicher, dass mehr als eine Person an der Arbeit beteiligt ist, da es sich um eine Forschungsarbeit handelt.</sample>
    <sample id="261">Ein gutes Planer sollte Schritte schreiben, die vernünftig und treu an die eingesetzten Einschränkungen sind.</sample>
    <sample id="262">Die Arbeit "Distilling Script Knowledge from Large Language Models" wurde von einem Autor, Siyu Yuan von Fudan University, vorgestellt. Es ist nicht klar, ob es weitere Autoren an der Arbeit gibt, da der Text nur die Arbeit von Siyu Yuan beschreibt.</sample>
    <sample id="263">In this work, we address the issue of label biases in in-context learning, a popular paradigm for utilizing large language models. We identify a new type of bias, domain-label bias, and propose a novel calibration method to handle all types of bias. Our experiments show that domain-context calibration significantly improves the performance of in-context learning on tasks with large domain-label bias. We also find that using random in-domain words as content-free text in the calibration process is more effective than using single predefined tokens. Our work provides a systematic investigation of label bias problems in in-context learning and proposes a calibration method that can significantly improve the performance of large language models.</sample>
    <sample id="264">Lin Wang's presentation introduces "TAVT: Towards Transferable Audio-Vision Text Generation," addressing the challenges in multimodal text generation due to data annotation difficulties and domain shifts. TAVT proposes a novel task to train models that can quickly adapt to new multimodal domains with limited data. The framework consists of three components: an audio-visual meta mapper network, an audio-visual encoder and language model, and counterfactual contrastive learning.

The meta mapper network maps visual concepts across domains into a unified auditory semantic, using audio clips from the Flickr dataset and k-means clustering. Visual prefix tokens are introduced to improve the semantics of audio space. The transformer-based encoder and generator evaluate the contribution of different modalities to each word, and a loss function is used for training.

To optimize visual-audio alignment, TAVT introduces Dual Counterfactual Contrastive Learning (DCCL), which constructs fine-grained supervision signals from counterf actual results. The meta-training details involve randomly selecting domains as support and query sets, and evaluating the model on new domains by fine-tuning the meta-parameters.

The experimental results show that TAVT outperforms state-of-the-art models on both cross-datasets and cross-domain settings, even in low-resource domains with limited labeled data. Ablation experiments demonstrate the impact of audio features on the model's performance.</sample>
    <sample id="265">Der/die Referent*in heißt Vasudha.</sample>
    <sample id="266">The authors belong to the University of Warsaw.</sample>
    <sample id="268">Die häufigsten Fehler von PaLm sind Omissionen, bei denen Teile des Quelltextes während der Übersetzung ausgelassen werden. Dies führt zu einer besseren Klanglichkeit der Übersetzungen, aber auch zu genauigkeitsproblemen.</sample>
    <sample id="269">Hallo, ich bin James Finch und Sarah Finch. Heute werden wir Ihnen alles über ABC-Eval erzählen, eine neue dimensionale Methode zur Bewertung von Dialogkünstlicher Intelligenz. Diese Arbeit wurde von der Emory NLP Lab, angeführt von Professor Jinho Choi an der Emory University, in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Stellen Sie sich vor, Sie haben ein Dialogmodell entwickelt und möchten sehen, wie es sich mit dem aktuellen Stand der Technik vergleicht. Die übliche Praxis besteht darin, menschliche Bewertungen durchzuführen, indem Sie Menschen beauftragen, zu bestimmen, welche der beiden Gespräche besser ist, oder um Gespräche mit einer Likert-Skala zu bewerten. Diese Ansätze funktionieren gut, um eine umfassende Bewertung der Gesprächskwalität zu liefern, aber die Gesprächskwalität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um ein feineres Verständnis der Stärken und Schwächen des Modells auf einer feineren Ebene zu erhalten. Eine Möglichkeit besteht darin, einfach Menschen beauftragen, mehrere Dimensionen der Dialogqualität zu bewerten, wie zum Beispiel die Relevanz der Modelleingaben mithilfe bestehender vergleichender oder Likert-Skala-Methoden. Allerdings glauben wir, dass es einen genaueren und zuverlässigeren Ansatz für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem wir explizit angeben, ob jede Modelleinspruch bestimmte Verhaltensweisen ausdrückt, wie z.B. mit irrelevanten Informationen reagieren oder sich widersprechen. Wir nennen dieses Verfahren das Annotieren von Verhaltensweisen im Chat oder ABC-Eval in Kürze. Wir entwickelten diesen Ansatz, um umfassend die Chatmodellverhaltensweisen abzudecken, die in der jüngsten Literatur als Einfluss auf die Chatqualität angegeben wurden. ABC-Eval ist in der Lage, die Raten zu messen, mit denen Chatmodelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC-Eval die Anzahl der Turns, in denen ein Chatmodell seinen Partner ignoriert oder etwas irrelevantes sagt, sich widerspricht oder seinem Partner widerspricht, Halluzinationen falscher Fakten oder Verletzung von Allgemeinwissen, und wenn das Modell erfolgreich oder fehlgeschlagen ist, Empathie zu zeigen. Um herauszufinden, welche Art von Bewertung am effektivsten ist, haben wir vier der neuesten Chatmodelle ausgewählt und sie auf 100 mensch-bot-Gespräche pro Modell mit ABC-Eval bewertet. Zur Vergleichsweise haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf Turn-Ebene, Likert-Bewertungen auf Dialog-Ebene und Dialog-Ebene-Paarweisevergleiche. Für jede der bestehenden Methoden haben wir Bewertungen auf acht der am häufigsten gemessenen Aspekte der Dialogqualität gesammelt, da dies die Standardpraxis für die Bewertung von Chatmodellen entlang mehrerer Dimensionen ist. Aus unserer Analyse dieser Bewertungsergebnisse fanden wir heraus, dass die ABC-Eval-Verhaltenslabels insgesamt zuverlässiger sind als die von bestehenden Methoden gesammelten Labels, wie durch die Inter-Annotator-Agreement auf 100 doppelt beschrifteten Gesprächen gemessen. Zudem sind die ABC-Eval-Labels zuverlässiger als die von bestehenden Methoden produzierten Metriken, um die Gesprächskwalität zu erklären, wie durch einfache lineare Regressionsanalyse gezeigt wird. Zum Beispiel können Sie sehen, wie die Messung des Anteils von Turnen mit Selbst- und Partner-Widersprüchen 5% und 10% der Gesprächskwalität erklärt, w</sample>
    <sample id="270">The authors, James Finch and Sarah Finch, are affiliated with Emory University, specifically the Emory NLP Lab led by Professor Jinho Cho.</sample>
    <sample id="271">In this work, CFT stands for "Continuous Fine-Tuning." The authors found that allowing the model to continue fine-tuning on clean validation samples can achieve the same performance as more complex weakly supervised learning (WSL) methods, such as COSINE. This suggests that continuous fine-tuning is a simple yet effective baseline for WSL tasks.</sample>
    <sample id="272">Es sind insgesamt 8 Autoren an der Arbeit beteiligt: Koustav Sinha, John Gauthier, Aaron Mueller, Kishka Misra, Karen Fences, Roger Lévy und Adina Williams.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde meine Arbeit unter dem Titel "Wann benötigt Übersetzung Kontext? Eine datengetriebene, mehrsprachige Untersuchung" präsentieren. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André Fernandes Martins und Graham Neubig erstellt. Viele Übersetzungen hängen von Kontext ab. Zum Beispiel, wie würden wir den Satz "Mole" übersetzen? Wenn die vorherige Aussage "Es könnte gefährlich werden, wenn die Minister herausfinden", dann bezieht sich "Mole" auf einen Spion. Aber wenn die vorherige Aussage "Könnte es etwas ernst sein, Arzt?" lautet, bezieht sich "Mole" auf eine Hautveränderung. Je nach Kontext ändert sich der Sinn des Wortes und damit auch seine Übersetzung. Allerdings ist es schwierig zu bewerten, wie gut Modelle solche Fälle übersetzen können. Zunächst einmal, weil nur ein kleiner Teil der Übersetzungen von Kontext abhängt, können Korpusbasierte Metriken wie BLEU nicht diese Übersetzungen erfassen. Und einige Menschen haben vorgeschlagen, gezielte Bewertungen für kontextabhängige Übersetzungen durchzuführen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachmengen, da sie in der Regel auf Fachwissen und menschliche Auswahl angewiesen sind. In dieser Arbeit versuchen wir diese beiden Fragen zu beantworten. Erstens, wann benötigt Übersetzung Kontex? Zweitens, wie gut können Modelle diese Fälle behandeln? Um die erste Frage zu beantworten, haben wir gemessen, wie viel ein Wort von Kontext während der Übersetzung abhängt. In einer früheren Arbeit haben wir CXMI als Maß für den Kontextgebrauch von maschinenübersetzungsmodellen eingeführt. Dies wird durch Messung getan, wie viel Information der Kontext C über das Ziel Y, gegeben der Quelle X, bietet. Man kann sich CXMI als die Informationen vorstellen, die man dem Modell durch Kontext gibt. In dieser Arbeit erweitern wir CXMI auf Punktweise CXMI, das auf Satz- oder Wortebene messen kann. Man kann sich Wörter mit hohem P-CXMI als Wörter vorstellen, die Kontext für die Übersetzung benötigen. Nun analysieren wir Wörter mit hohem P-CX-MI, um Muster zwischen diesen Wörtern zu suchen. Und wir führen unsere Analyse auf Transkripte von TED-Vorträgen durch, die aus Englisch in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zunächst untersuchen wir Teilwortarten, die einen hohen Durchschnittswert von P-CXMI haben. Dies ermöglicht es uns, beispielsweise Dualpronomen in Arabisch zu finden, die relativ hohe P-CXMI haben. Dies kann erklärt werden, weil Englisch keine Dualpronomen hat, also benötigt man Kontext, um zu bestimmen, ob ein Pronomen dual ist, wenn man es ins Arabische übersetzt. Ähnlich finden wir heraus, dass bestimmte Sprachen auch Kontext benötigen, um die richtige Verbenform zu wählen. Dann untersu</sample>
    <sample id="274">Der/die Referent*in heißt Yusen Zhang.</sample>
    <sample id="276">Ananya and Vignesh present their work on "IndicMT Eval: A dataset to meta-evaluate machine translation metrics for Indian languages." They focus on evaluating translations from Indian languages to English, as this area is understudied. They use the Flores dataset to select 200 sentences and generate multiple candidate translations using seven different translation models. Human annotators then evaluate these translations, marking errors and providing an overall score. The study compares various evaluation metrics, including overlap-based, embedding-based, and COMET-metric variants. They find that COMET-metric variants have the highest overall correlation with human scores. They also fine-tune the best-performing metric, COMET, using their MQM dataset, resulting in improved correlation values. Finally, they evaluate the robustness of IndicCOMET MQM on other unseen languages and find it to be more robust than the COMET counterpart. The dataset and results are publicly available for further research.</sample>
    <sample id="277">Die neue Methode hat den Namen "Multiset Tagging and Latent Permutations."</sample>
    <sample id="278">The authors describe the "Marked Words" method as a sociolinguistic approach that identifies words that distinguish marked groups from unmarked ones. They use the concept of "markedness," where dominant groups are considered unmarked and marginalized groups are marked. By comparing the top words for each marked group using weighted log-odds ratios, they reveal specific stereotypes and patterns in the generated personas. This method allows them to capture harmful patterns without relying on a specific lexicon.</sample>
    <sample id="279">Die Autoren gehören der Universität Washington an.</sample>
    <sample id="280">Shi Tao presents "MultiEMO," a novel attention-based correlation-aware multimodal fusion approach for emotion recognition in conversations. The framework addresses three key challenges: exploiting multimodal complementarity, improving minority class performance, and distinguishing semantically similar emotions. MultiEMO consists of four components: unimodal feature extraction, context modeling, multimodal fusion (MultiAttn), and emotion classification.

VisExtNet, a visual feature extractor, captures facial expressions without redundant scene information. MultiAttn, a multimodal fusion network, integrates modalities using bidirectional multi-head cross-attention. Sample-Weighted Focal Contrastive Loss (SWFC) focuses on hard-to-classify minority classes, enhancing performance on semantically similar emotions.

Experiments on MELD and IEMOCAP show state-of-the-art results, particularly in minority and semantically similar emotions. However, limitations include VisExtNet's inability to distinguish speakers and the need for a large batch size for SWFC. Despite these, MultiEMO significantly advances emotion recognition in conversations.</sample>
    <sample id="281">Kayo Yin and her team explored the role of context in translation, focusing on how much a word depends on context during translation and how well models handle these cases. They introduced CXMI as a measure for context usage and extended it to Pointwise CXMI for sentence and word-level analysis. They analyzed TED talk transcripts translated into 14 languages and found that context is crucial for certain discourse phenomena, such as formality, lexical cohesion, ellipses resolution, pronouns, and verb form. They designed a benchmark, the Multilingual Discourse-Aware (MuDA) tagger, to automatically identify context-dependent examples in parallel corpora. The MuDA benchmark showed that context-aware models are more accurate for certain phenomena but not for others. The team also compared commercial systems and found that DeepL is generally more accurate than Google Translate for document-level translation</sample>
    <sample id="282">Xuekai Zhu presents "StoryTrans: Non-Parallel Story Style Transfer with Discourse Representations and Content Enhancer" at ACL 2023, addressing the challenge of non-parallel text style transfer at the discourse level. Traditional methods focus on token or sentence-level transfer, but StoryTrans aims to imitate author style at the story level, capturing complex linguistic preferences and narrative techniques. The primary challenge lies in transferring style-specific content, such as narrative techniques, while preserving the source semantics.

To tackle this, StoryTrans employs a novel generation model that learns discourse representations and combines them with learnable style embeddings. The model is trained in two stages: first, it transfers the source text with style-specific content masked, and then generates the whole text by incorporating these keywords explicitly. The training framework includes self-reconstruction loss, disentanglement loss, sentence order loss, and style classifier loss.

StoryTrans was evaluated on new datasets in Chinese and English, showing superior performance in style control and content preservation compared to strong baselines. Style visualization confirms alignment with the golden text in the style feature space, and StoryTrans can enrich storylines and maintain source semantics. The model can rewrite most sentences with the target style while preserving the source semantics.</sample>
    <sample id="283">Die zuerst erwähnte symmetrische Abhängigkeitsstruktur ist die "multi-headed" Struktur, die in Hudson's Word Grammar verwendet wird.</sample>
    <sample id="284">Hello everyone, I'm Peng Tianshuo from Wuhan. Today, I'll present my paper titled "FSUIE: A Novel Fuzz Span Mechanism for Enhancing Universal Information Extraction" at ACL's Main Conference 4,911.

Current span-based UIE models rely heavily on precise boundary positions, which can be ambiguous. We propose a fuzzy span mechanism where the span boundary is learned as a continuous distribution of correct probability, rather than a precise value. This addresses the mismatch between transformer feature extraction and information extraction, as transformers focus on global features and ignore the limited length of spans.

We introduce an adaptive attention mechanism that models the furthest span boundary as a continuous distribution. The boundary distribution is calculated using a sampling function, and the model's attention distribution is adjusted using a fuzzy span attention mask function. This mask function introduces an optimizable parameter to dynamically adjust the attention span and linearly decays the attention distribution at the boundary.

We conducted experiments on three main information extraction tasks: named entity recognition, relationship extraction, and aspect-sentiment triplet extraction. Our FSUIE model achieved significant performance improvements compared to UIE models without a fuzzy span mechanism, especially on small-scale data. On relationship extraction, FSUIE achieved new state-of-the-art results on datasets ACE2004, ACE2005, and ADE.

The results of the ablation study showed that FSL and FSA improved convergence speed and information extraction capability, respectively. The combined effect of the two produced a greater enhancement. We also visualized the attention distributions, which showed that the module focused on semantic information within a specific range of preceding tokens.

In conclusion, we proposed a novel fuzzy span loss and efficient fuzzy span attention mechanism, which achieved excellent results in a wide range of IE tasks, including named entity recognition, relationship extraction and aspect-sentiment triplet extraction. Thank you for listening.</sample>
    <sample id="285">Hello everyone, I'm Mingqi Gao from Peking University, and I'm excited to share our work, "Reference Matters: Factuality Error Correction for Dialogue Summarization with a Fine-grained Evaluation Framework." Our research focuses on the issue of factual errors in dialogue summarization, which are still prevalent even in reference summaries. We propose two main solutions: introducing factuality-related objectives in the training or inference processes, and designing a Factual Error Correction (FEC) model that takes the source document and model-generated summary as inputs and outputs a corrected summary.

We argue that the current evaluation methods for FEC models are flawed, as they rely on factuality metrics that give an overall score, which may not be reliable on their own. Additionally, this evaluation blurs the line between the content-based and form-based solutions. To address these issues, we introduce manually annotated reference corrections, which provide more valuable data for training FEC models and create a condition for a more comprehensive and accurate evaluation of their performance.

We also propose a new taxonomy of factual errors, which includes content-based and form-based categories. Our evaluation framework, based on ERRANT, consists of three steps: alignment, classification,</sample>
    <sample id="286">Der/die Referent*in heißt Sarah Finch.</sample>
    <sample id="287">Vier Autoren sind an der Arbeit beteilt: Javad Hosseini, Filip Radlinski, Silvia Paret, und Annie Louis.</sample>
    <sample id="288">Zum Testen syntaktischer Phänomene können Datensätze wie BLiMP, SyntaxGym, Adjunct Island, sowie unterschiedliche Subsets aus diesen Daten oder sogar vollständig unverwandte Daten wie Wikipedia verwendet werden. Diese Datensätze helfen dabei, die Sensibilität der Sprachmodelle gegenüber syntaktischen und semantischen Merkmalen zu untersuchen, die über den Kontext hinweg geteilt sind.</sample>
    <sample id="290">Die Abkürzungen der fünf methodischen Ansätze zur Beantwortung der ersten Forschungsfrage sind:

1. FTw: Fine-tuning on the clean validation samples.
2. COSINE: A specific weakly supervised learning method.
3. WSL: Weakly Supervised Learning.
4. QD: Clean validation data.
5. WSL+: Weakly Supervised Learning with clean validation data.</sample>
    <sample id="291">Das Modell wird auf verschiedenen Aufgaben im Bereich der natürlichen Sprachverarbeitung (NLP) evaluiert, darunter:

1. Named Entity Recognition (NER)
2. Classification
3. Part-of-Speech Tagging (POS)
4. Question Answering (QA)

Diese Aufgaben werden auf öffentlichen und privaten Datensätzen für die Evaluierung der Modelle durchgeführt.</sample>
    <sample id="294">CamemBERT wurde ursprünglich auf der Basis von Daten aus dem OpenSubtitles-Projekt trainiert, das eine umfangreiche Menge an Subtiteln aus verschiedenen Sprachen enthält. Diese Daten wurden verwendet, um die Sprachverständnisse und -generierung von CamemBERT zu verbessern.</sample>
    <sample id="295">Der/die Referent*in in diesem Kontext ist Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presents a collaborative work between the University of Turin and Amazon Alexa, focusing on Natural Language Understanding (NLU) and Natural Language Processing (NLP). The project aims to develop more informative models for irony detection, a complex and latent phenomenon in language. To achieve this, they created the EPIC corpus, an English Perspectivist Irony Corpus, which includes 300 short conversations from social media, Reddit, and Twitter, spanning 1.5 years and five English varieties. The data was annotated by 74 annotators using Prolific, with each annotator reviewing 200 texts and answering extra quality control questions.

The study found that inter-annotator agreement varies depending on how the data is divided, such as by gender, age, or nationality. To address this, they developed perspective-aware models, which fine-tune pre-trained language models on splits of the dataset based on different annotators. These models showed significantly higher confidence in their predictions compared to gold standard aggregated models.

Further analysis revealed that age and geographical distribution of annotators contribute to differences in annotations. Generations close in age and annotators from the UK and Ireland showed the highest variations in their perception of irony. The presentation concludes with an invitation for questions and further discussions at the poster session.</sample>
    <sample id="297">The presentation discusses a project titled "From Dogwhistles to Bullhorns," which focuses on understanding and identifying coded rhetoric, specifically dogwhistles, in political speeches. The project aims to develop a typology and glossary of dogwhistles, which are terms that convey a hidden message to an in-group while being innocuous to an out-group. The study examines historical U.S. political speeches and evaluates the ability of language models, such as GPT-3, to recognize dogwhistles. The research also explores how dogwhistles can evade content moderation online. The findings suggest that dogwhistles are more prevalent in conservative politics and can be effectively identified by language models with specific prompting strategies. The project highlights the importance of understanding dogwhistles in the context of political influence and the need for improved content moderation.</sample>
    <sample id="298">The results that led to the conclusion that temporal drift is the main cause of the performance drop are as follows:

1. **Experiment on Retraining with Recent Data**: The authors conducted an experiment where they retrained or continued to pre-train some models with more recent Reuters News data from 2020. They found that the performance of these models degraded as the temporal gap between the training data (CoNLL-2003) and the test data (CoNLL++) increased. This degradation in performance with a larger temporal gap confirmed the hypothesis that temporal drift is the primary cause of the performance drop.

2. **Comparison of Performance**: The authors compared the performance of models trained on CoNLL-2003 with those trained on CoNLL++ and found that models trained on more recent data performed better on the newer dataset. This further supported the idea that temporal drift, rather than adaptive overfitting, was the main factor affecting generalization.

In summary, the key results that led to the conclusion that temporal drift was the main cause of the performance drop were the observed degradation in performance when models were retrained with more recent data and the comparison of performance between models trained on CoNLL-2020 and CoNLL++.</sample>
    <sample id="299">Hi everyone, I'm Michalis Korakakis, and today we're discussing "Improving the robustness of NLI models with minimin training." NLI models have achieved great results, but they often rely on shortcuts, which are spurious correlations in the data. These shortcuts can make models brittle when tested on out-of-distribution data.

Previous methods to mitigate shortcuts require knowing the shortcuts in advance and assume the learner will exploit the same shortcuts as the auxiliary. Our method, however, uses a minimax training objective between a learner and an auxiliary to emphasize under-represented hard examples. This helps the learner focus on these examples, improving out-of-distribution performance without relying on the auxiliary.

We evaluated our method on three datasets and found that it consistently improves out-of-distribution performance while maintaining in-distribution accuracy. We also examined the effect of pre-training the learner, the size of the auxiliary, and conducted a qualitative evaluation of the learned example weight distribution</sample>
    <sample id="300">Belinda introduces the concept of interactive dictation, a process where users can dictate and edit documents using their voice in a natural and intuitive manner. The task involves flexible interleaving of dictation and editing, using open-ended natural language utterances to specify edits, and not requiring a fixed set of template commands. Belinda formalizes the task as a four-step procedure, including ASR recognition, segmentation, command extraction and normalization, and execution of dictation and commands. She also presents a baseline system that performs each of these steps, using separate models for ASR repair, segmentation, and interpretation. The system is evaluated using exact match of the predicted end-state against a goal end-state, and results show a trade-off between runtime and accuracy, with GPT-3 models being more accurate but slower, and T5 models being more efficient with minimal impact on accuracy. Belinda concludes by releasing code and inviting further work on the task.</sample>
    <sample id="302">In the context of semantic parsing, it is necessary to permute the tokens for the output sequence because the model needs to determine the correct order of the tokens to accurately represent the meaning of the input. Without permuting the tokens, the model may produce outputs that are detached from the input and fail to reproduce the systematic correspondences between input tokens and output tokens. By permuting the tokens, the model can learn the correct order and produce more accurate outputs.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen die Transparenz ihrer Methoden zum Abbau von Vorurteilsmodellen erhöhen sollten, um die Ursachen für diese Vorurteile besser zu verstehen und zu bewerten. Sie argumentieren, dass ohne Transparenz keine klaren Schlüsse über die Wirksamkeit der Methoden gezogen werden können und dass es schwierig ist, die genauen Mechanismen zu identifizieren, die zu den beobachteten Musterungen führen. Die Autoren betonen, dass eine erhöhte Transparenz es ermöglicht, die Effektivität und die möglichen Nebenwirkungen der Methoden zu bewerten und die Entwicklung von effektiveren und gerechteren Modellen zu fördern.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind solche, die grammatisch falsch sind, aber in einem Kontext, der sie akzeptabel machen würde. Beispielsweise könnte ein Satz wie "Ich habe gestern einen Apfel gegessen" in einem Kontext, in dem "Ich habe gestern einen Apfelsaft getrunken" erwartet wird, inakzeptabel sein.</sample>
    <sample id="305">Dawei, a PhD student at Saarland Universität, präsentiert in diesem Video ihre jüngste Arbeit "Weaker Than You Think: A Critical Review of Weakly Supervised Learning". In der Arbeit untersucht Dawei und ihre Mitarbeiter, Xiaoyu Shen, Marius Mosbach und Andreas Stephan, die Notwendigkeit von reinigten Validierungsdaten für die Weakly Supervised Learning (WSL) und die Anzahl der benötigten Reinigungsdaten. Sie zeigen, dass WSL-Ansätze tatsächlich reiniges Validierungsdatum benötigen, um gut zu funktionieren, und dass die Performance-Steigerung durch direktes Feintuning auf reinen Daten erreicht werden kann. Dawei schlägt vor, die Modellauswahlkriterien zu berichten, WSL-Ansätze mit wenigstensführigen Baselines zu vergleichen und kontinuierliches Feintuning als einfache und starke Basiskonzepte zu berücksichtigen. Zudem veröffentlicht Dawei ihr Code.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim present their research on entity tracking in language models, focusing on the ability of models to understand and track the state of entities in a discourse. They argue that this ability is crucial for understanding longer discourses, but there has been little systematic investigation into what pre-trained language models can actually perform. To address this, they designed an evaluation task involving boxes and objects, where the model must predict the contents of each box after a series of state-changing operations. They found that most models simply repeat the initial state, but text-davinci-003 exhibits nontrivial tracking. They also found that pre-training on code is responsible for making this capacity surface in pre-trained models, and that smaller models can learn to perform entity tracking if fine-tuned. However, it remains unclear whether the state tracking abilities observed generalize beyond their setup. The paper includes results and analysis, including GPT-4 experiments.</sample>
    <sample id="307">Die Autoren haben verschiedene Bewertungsmetriken verwendet, um die Leistung ihrer Modelle zu bewerten. Sie haben die folgenden Metriken eingesetzt:

1. **Named Entity Recognition (NER)**: Die Autoren haben die Fähigkeit der Modelle bewertet, bestimmte Entitäten in Texten zu erkennen, wie z.B. Namen, Orte, Organisationen oder medizinische Begriffe.

2. **Classification**: Die Modelle wurden bewertet, um ihre Fähigkeit zu beurteilen, Texte in vorgegebene Kategorien einzuteilen, z.B. in verschiedene Krankheiten oder medizinische Bedingungen.

3. **Part-of-Speech Tagging (POS)**: Die Autoren haben die Fährigkeit der Modelle bewertet, die Wörter in einem Text in ihre grammatischen Kategorien einzuteilen, wie z.B. Substantive, Verben, Adjektive oder Adverbien.

4. **Question Answering (QA)**: Die Modelle wurden bewertet auf ihre Fähigkeit, auf Fragen basierend auf einem Text eine korrekte Antwort zu finden.

Zusätzlich haben die Autoren die Leistung ihrer Modelle auf 11 spezifischen biomedizinischen und klinischen Aufgaben in der französischen Sprache bewertet. Die besten Ergebnisse wurden für die Aufgaben "Named Entity Recognition", "Classification", "Part-of-Speech Tagging" und "Question Answering" erzielt. Die Autoren haben auch die Leistung ihrer Modelle im Vergleich zu sechs baseline-Modellen (CamemBERT OSCAR 138 GB und 4 GB, CamemBERT CCNET, PubMedBERT, BioBERT und ClinicalBERT) bewertet. Die Ergebnisse zeigten, dass die Modelle, die auf homogenen Daten trainiert wurden, besser abschneiden als die Modelle, die auf heterogenen Daten trainiert wurden. Die Autoren haben auch die Leistungsunterschiede zwischen Modellen, die von-scratch trainiert wurden und solchen, die kontinuierlich vorbereitet wurden, untersucht. Die von-scratch trainierten Modelle erzielten in der Regel bessere Ergebnisse als die kontinuierlich vorbereiterten Modelle.</sample>
    <sample id="308">Jenny, a first-year PhD student at Carnegie Mellon University, presented her work on NLPositionality, a framework for characterizing design biases of datasets and models. She discussed how design biases can occur due to the positionality of NLP researchers and model developers, and how datasets and models can represent certain positionalities over others. To study this, Jenny and her team re-annotated datasets with diverse annotators and compared the annotations to models and datasets using a Pearson's R correlation. They found that datasets and models are most aligned to English speaking countries and people with a college education, but less aligned to non-binary people. Jenny recommended keeping a record of design choices, doing NLP research with perspectivism, and building specialized datasets and models within specific communities. She also mentioned the Masakhani initiative as an example of inclusive NLP. The study amassed over 16,000</sample>
    <sample id="309">Die Metrik, die zur Messung der Übereinstimmung zwischen den kommentierenden Kommentatoren verwendet wurde, ist die **Inter-annotator Agreement**. Diese Metrik wird verwendet, um die Übereinkunft zwischen verschiedenen annotierenden Personen zu bewerten, indem sie die Übereinstimmung ihrer Bewertungen auf doppelt bezeichneten Konversationen analysiert.</sample>
    <sample id="310">In the paper, the authors used sentences from the Wikipedia domain to test the language models' acceptability judgments. They found that the MPP judgments were relatively stable for arbitrary context length when using sentences from Wikipedia, which are completely irrelevant to the current query pairs. This suggests that the language models' acceptability judgments are not always robust to context and that the current MPP pipeline may not fully capture the language models' abstract knowledge throughout the context window.</sample>
    <sample id="311">Die Autoren Regina Stodden und Omar gehören der Universität des Saarlandes an.</sample>
    <sample id="312">MultiInstruct unterscheidet sich von anderen Benchmarks durch seine Fokus auf Multi-Modal Instruction Tuning und die Schaffung eines ersten Multi-Modal Instruction Tuning Datensatzes mit 62 verschiedenen Multi-Modal Aufgaben. Dieser Datensatz besteht aus 21 bestehenden Open-Source-Datensätzen und enthält für jede Aufgabe fünf von Experten geschriebene Anweisungen. Im Gegensatz zu vielen anderen Benchmarks, die sich auf NLP-Aufgaben konzentrieren, bietet MultiInstruct eine umfangreiche Sammlung von Aufgaben, die sowohl für NLP als auch für Multi-Modal verwendet werden können. Zudem wurde ein neues Metrik namens Sensitivity eingeführt, um die Konsistenz der Modellausgaben bei unterschiedlichen Anweisungen zu messen.</sample>
    <sample id="313">Zwei Autoren sind an der Arbeit beteilt: James Finch und Sarah Finch.</sample>
    <sample id="314">Die binäre Koordination ist eine sprachliche Struktur, bei der zwei oder mehr Wörter, Phrasen oder Sätze in einer gleichwertigen Beziehung zusammengefasst werden. Diese Wörter, Phrasen oder Säte sind als "Konjunktionen" oder "Koordinaten" bezeichnet und werden durch Koordinationskonjunktionen wie "und", "oder", "aber" usw. verbunden. Die Konjunktionen sind gleichwertig und tragen jeweils eine gleiche Bedeutung oder Funktion im Satz. Die binäre Koordination ermöglicht es, mehrere Informationen oder Aspekte in einem Satz zu präzisieren oder zu erweitern.</sample>
    <sample id="315">Die Dauer der verwendeten Prompts in dieser Studie wurde nicht spezifisch angegeben.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5-Modell, nachdem es auf das CoScript-Datenset fine-tuned wurde, die Qualität der generierten Skripte verbessert. Dies bedeutet, dass kleinere, spezialisierte Modelle, wenn richtig trainiert, die Sprachebewusstsein und die Fähigkeit zur Planung unter bestimmten Einschränkungen über die größeren Sprachmodelle hinaus erreichen können.</sample>
    <sample id="317">Hello everyone, I'm Peng Li from Fudan University, presenting our work titled "CodeIE: Large Code-Generation Models are Better Few-Shot Information Extractors." Information extraction involves extracting structured data from unstructured text, such as named entity recognition (NER) and relation extraction (RE). Traditional models like T5 and GPT-3 operate in a text-to-text manner, but their outputs are linearized into a plan sequence, leading to mismatched outputs. To address this, we propose CodeIE, transforming the task into a structure-to-structure code generation problem using large language models like Codex. For NER, we define a function that extracts named entities from input text. Our method was evaluated on three NER datasets and four RE datasets, outperforming traditional models like UIE and GPT-3. We found that code format prompts significantly reduced structural errors and improved recall. Our analysis suggests that code pre-training aligns better with information extraction tasks. We hope our work provides inspiration for future research. Thank you for your attention, and feel free to ask questions. Our paper and code are publicly available.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich werde mich auf unsere Arbeiten über "DrBERT: Ein robustes vorgebildetes Modell in Französisch für biomedizinische und klinische Bereiche" einlassen. In dieser Präsentation werden wir zunächst über Sprachmodellierung in der Gesundheitsversorgung sprechen. Dann werden wir die Hauptbeiträge unseres Artikels präsentieren. Wir haben den ersten biomedizinischen Modell in Französisch namens DrBERT eingeführt, der auf RoBERTa basiert und auf NACHOS, einem Datensatz medizinischer gecrawlter Daten aus dem Web, trainiert wurde. Wir haben auch eine Vergleichsstudie von Modellen mit verschiedenen Pre-Trainingsettings und Datenquellen durchgeführt. Dann präsentieren wir die Ergebnisse auf 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Schließlich werden wir über die Experimente abschließen und mehr Details über den Zugang zu diesen Modellen geben. Seit der Veröffentlichung von BERT im Jahr 2018 ist dieses Modell zu einer der effektivsten Ansätze zur Lösung natürlicher Sprachverarbeitungsaufgaben und bietet riesige Leistungsgewinne im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2vec, fastText oder anderen. Seitdem wurde dieses Modell in vielen anderen Sprachen angepasst, wie zum Beispiel in Französisch mit CamemBERT, und auch in Bereichen wie biomedizinisch mit PubMedBERT und BioBERT und klinisch mit ClinicalBERT, aber meist in Englisch. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlicher Pre-Training aufgrund des Mangels an in-Domain-Daten. Allerdings gab es bisher keine offene Quelle für ein biomedizinisches Modell in Französisch. Also fragen wir uns die Frage, welche Datenquellen für eine breite Nutzung am besten geeignet sind und ob diese gecrawlten Daten eine gute Substitution für klinische Daten sind. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten aus dem Datenspeicher der Universitätsklinik in Nantes basiert. Anschließend fragen wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren? Ist es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten vergleichen wir vier von-scratch-Modelle: eine erste Version von DrBERT mit 7 GB von NACHOS; eine zweite Version von 4 GB von NACHOS; eine erste Version von ChuBERT, die ein klinisches Modell mit 4 GB von Sätzen aus klinischen Notizen ist; und eine endgültige Version von ChuBERT mit einem Mix aus 4 GB von NACHOS und 4 GB von klinischen Notizen. Neben dieser Vergleichsstudie haben wir auch drei Modelle basierend auf kontinuierlicher Pre-Trainingsstrategie analysiert, um den Einfluss von Pre-Trainingsstrategien zu untersuchen. Eins basiert auf den Gewichten von CamemBERT und wurde auf einem 4 GB-Teil von NACHOS trainiert. Ein weiteres basiert ebenfalls auf CamemBERT, aber diesmal auf 4 GB von klinischen Notizen und schließlich eines basierend auf dem englischen biomedizinischen Modell PubMedBERT und wurde auf 4 GB von NACHOS trainiert. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammeln wir Daten für öffentliche und private Downstream-Aufgaben wie Named Entity Recognition, Klassifikation, Part-of-Speech Tagging und Fragebeantwortung. Diese Modelle werden mit sechs Basismodellen verglichen, die CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT sind. Die Bewertung zeigt, dass die Modelle am besten auf Aufgaben mit Daten von der gleichen Natur wie die, auf denen das Modell trainiert w</sample>
    <sample id="319">In der Arbeit werden folgende Lernstrategien untersucht:

1. **Von-scratch-Prä-Training**: Trainierung von Modellen auf einem eigenen Datensatz ohne vorherige Prä-Training.
2. **Kontinuierliches Prä-Training**: Verwendung von Modellen wie CamemBERT, um auf neuen Daten fortzufahren, ohne ein neues Modell von Null zu starten.
3. **Transfer-Lernen**: Nutzung von Modellen, die auf anderen Datensätzen vorgeübte, um auf neue Aufgaben zu übertragen.
4. **Vergleich von Datenquellen**: Untersuchung der Leistung verschiedener Datenquellen wie NACHOS-Daten, klinische Notizen und PubMed-Daten.
5. **Datenmenge**: Untersuchung der Auswirkung der Datenmenge auf die Modellleistung.

Diese Strategien werden verwendet, um die beste Kombination von Datenquellen und Modellstruktur für die spezifischen Aufgaben im Bereich Medizin und Klinik zu finden.</sample>
    <sample id="320">In the paper, the authors discuss the issue of adaptive overfitting, which is the phenomenon where overfitting to a test set leads to diminishing returns on new test sets. They present a graph showing the relationship between improvements on the CoNLL-2003 dataset and the CoNLL++ dataset. The red best-fit line in the graph has a gradient greater than one, indicating that every unit of improvement on CoNLL-2003 results in more than one unit of improvement on CoNLL++. This suggests that adaptive overfitting is not observed in this case, as the gradient is greater than one, rather than less than one.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde beurteilt, indem die Vereinfachungsmethoden auf der Basis der manuell ausgerichteten Parallelsätze in der DEPLAIN-Korpus-Analyse evaluiert wurden. Die Korpusanalyse zeigte, dass die Vereinfachungsmethoden in Bezug auf die verschiedenen Simplifizierungsarten (lexikalische, strukturelle und allgemeine Simplifizierung) unterschiedliche Ergebnisse lieferten. Zudem wurde die Vielfalt der Simplifizierungsumwandlungen analysiert, was zeigte, dass die DEPLAIN-Korpus-Unterkorpus DEPLAIN-apa mehr Umordnungen und Wortzusätze hatte, während die DEPLAIN-Web-Korpus mehr Umformulierungen enthielt.

Die Qualität der Vereinfachung wird auch aufgrund der manuell ausgerichteten Parallelparagraphen in der DEPLAIN-Korpus-Studie beurteilt. Die manuell ausgerichteten Paragraphen dienen als Goldstandard für die Evaluierung automatischer Ausrichtungsmethoden. Die Evaluierung der automatischen Ausrichtungsmethoden zeigte, dass die MASSalign-Methode am besten geeignet ist, um die Paragraphen in der DEPLAIN-Korrelation zu vergleichen.

Zusammenfassend zeigt die Qualitätsbeurteilung der Vereinfachung, dass die DEPLAIN-Korpus-Korpus-Unterkorpus und die manuell ausgerichteten Paraleleinsätze eine wertvolle Grundlage für die Evaluierung automatischer Vereinfachungsmethoden bieten.</sample>
    <sample id="322">Enrico's presentation at ACL 23 explores the question of what a text classifier learns about morality. He explains that morality is subjective and varies across individuals and cultures, and that a single scale between immoral and moral is not sufficient to capture this complexity. Enrico introduces the Moral Foundation Theory, which suggests that humans perceive morality through five different foundations, such as fairness and authority, and that these foundations are prioritized differently by individuals. He discusses how language models can be trained to understand morality in text and how explainable AI techniques can be used to understand what these models learn. Enrico presents a study using the Moral Foundation Twitter Corpus, which consists of 35,000 tweets from seven different domains, including #AllLivesMatter and #BlackLivesMatter. He finds that language models can recognize differences in morality expressed in different domains, such as the subversion of authority in BLM compared to ALM. Enrico concludes that using a single model for many different domains can lead to misunderstandings of morality and that a more nuanced approach is needed.</sample>
    <sample id="323">Hello everyone, I am Yujie Wang from Shanxi, China. The title of my paper is “Dynamic Heterogeneous-Graph Reasoning with Language Models for Commonsense QA”. Commonsense QA is a challenging task that requires machine to answer questions that rely on common knowledge to assess their language understanding abilities. This requires machines to extract relevant knowledge from external sources. Recently, Holmes believes that knowledge is stored in both language models and Knowledge Graphs (KGs). Many works combine these two types of knowledge to address Commonsense QA with good results. These works retrieve relevant knowledge from the KG through entity matching, build a subgraph, and then use language models and Graph Neural Networks (GNNs) to infer answers. However, they introduce some noise entities during subgraph retrieval, such as “Top”, “Bank”, and “Cat” here in the subgraph, which are largely unattatched to the current question. Moreover, they encode subgraph and text in isolation, leading to limited interactions between the two modalities. And the encoding process disregards the semantic relationship between entities. Based on these issues, we propose DHLK. First, build an HKG based on multiple KGs through a two-stage pruning strategy and Knowledge Representation Learning (KRL) to optimize the structure and knowledge representation of the KG. Finally, use language models to encode and fuse the two modalities. First remove the subwords that make up the phrase entities by using the dictionary vocabulary. At the same same time, retrieve the paraphrase of key entities in WordNet, Wiktionary, and connect them as additional nodes to the subgraph to form the HKG. Then use RoBERTa and Mask Self-Attention (MSA) to encode and fuse QA contexts and entities, building the HKG. Meanwhile, dynamically remove entities with weaker relevance to the QA contexts based on the attention weights of RoBERTa. For the initial entity and relation embeddings, we obtain embeddings of entities and relations by mean pooling. As HKG is composed of multiple triplets, introduce TransE to optimize the entity and relationship embedding in HKG. Unlike other works that use Graph Neural Networks (GNNs) to model subgraphs, we use Relation Masked Self-Attention (RMSA) to model our subgraphs. Inspired by RGAN, we introduce relationships in MSA, creating RMSA. We update the entity embedding and relationship embedding of the HKG by iterating through L layers RMSA. Finally, we obtain the graph embedding of the HKG by applying maximum pooling to the question key entities. Next, incorporate the HKG path information into the QA contexts and obtain the embedding representation of the QA context after path enhancements. For our final answer prediction, we input HKG graph embedding, paths, and QA context embedded into the MLP to obtain the answer probability. We conduct experiments on CommonsensQA and OpenBookQA using external knowledge bases, including ConceptNet, WordNet, and Wiktionar. Meanwhile, we extract key entities in the context based on KeyBERT and retrieve knowledge paths within two hops in ConceptNet. We report the results and leaderboards for CommonsensQA and OpenBookQA. Compared with other language models and HKG methods, our method achieves good results.</sample>
    <sample id="324">Yes, according to the work presented by Shangbin and colleagues, language models do exhibit varying political leanings. They found that language models can pick up political biases from their pretraining data, and that these biases can be further amplified by pretraining on partisan corpora. Additionally, language models can also pick up societal polarization, with models trained on data from after the 45th president of the United States showing a greater political leaning away from the center. The study also found that language models with different political leanings perform differently on downstream tasks, such as hate speech detection and fake news detection, with left-leaning models performing better at detecting hate speech targeting socially minority groups and right-leaning models performing better at detecting hate speech and misinformation targeting more powerful groups in society. These findings suggest that there are fairness issues related to the political biases of language models, and that it is important to address these issues in order to ensure that language models are not perpetuating or amplifying societal biases.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute gebe ich Ihnen eine kurze Einführung in unsere Arbeit über "Kompositionalisierung ohne Bäume mit Multiset-Tagging und Latenter Permutation". Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Die Kompositionalisierung kann als die Fähigkeit eines Lerners betrachtet werden, tiefere Rekursion und ungesehene Kompositionen von Phrasen zu behandeln, die während des Trainings einzeln gesehen wurden. Im Kontext der semantischen Parsing könnte das Testen für Kompositionalisierung so aussehen. Wie üblich haben wir einen Trainingssatz von Utterances. In diesem Fall sind "Die Mädchen schlief." und "Mary wusste, dass die Mädchen schlief." Diese Utterances sind mit logischen Formen verbunden, die die Kernaspekte ihrer Bedeutung darstellen. Im Gegensatz zur üblichen maschinellen Lernungsevaluation enthält der Testsatz eine Struktur, die nicht aus demselben Verteilungsraum stammt, sondern ungesehene logische Formen enthält. In diesem Beispiel hat das Modell während des Trainings tiefere Rekursion gesehen und wird auf ein Beispiel mit tieferer Rekursion getestet. Naive seq2seq-Modelle haben Schwierigkeiten mit dieser Art von Ausgewandter Generalisierung und produzieren oft Ausgaben, die von den Eingaben abhängig sind. Insbesondere scheitern sie oft daran, systematische Korrespondenzen zwischen Eingabe und Ausgabe zu reproduzieren, wie sie in dem Beispiel farbig markiert sind. Eine beliebte Methode, um dies zu lösen, ist, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositorischen Prozess erfassen, der Verbindungen zwischen Utterances und logischen Formen herstellt. Dies funktioniert gut, aber Bäume werden in der Regel nicht gegeben und müssen auf irgendeine Weise erhalten werden. Dies kann ein kompliziertes und manchmal rechenaufwendiges Prozess sein. Typischerweise erfordert dies eine beträchtliche Formalismus-spezifische Vorbearbeitung der logischen Formen, zum Beispiel um Variablen-Symbole zu behandeln. Die Erhaltung von Bäumen kann auch spezialisierte Grammatik-Erzeugungsverfahren erfordern. In dieser Arbeit verwenden wir keine Bäume und führen ein neuronales seq2seq-Modell ein, das die Korrespondenzen zwischen Fragmenten des Eingabes und Fragmenten des Ausgabes direkt modelliert. Zum ersten Mal zeigen wir starke Generalisierung zu tieferer Rekursion ohne die Abhängigkeit von Bäumen. Unser Ansatz schätzt das Ausgangsergebnis vom Eingabeset in zwei Schritten. Zuerst etikettieren wir jedes Eingabetoken mit einem ungeordneten Multiset von Tokens, die in der Ausgabe vorkommen. Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht geordnet. Deshalb verwenden wir in der zweiten Schritt ein weiteres Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode ein, um die Permutation vorherzusagen, die keine harten Einschränkungen auf mögliche Permutationen setzt. Dies macht unsere Methode ziemlich flexibel und ausdrucksstark. Konzeptionell funktioniert unsere Permutationsmethode etwa so wie folgt. Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Multiset-Token in jedem Position im Ausgangsergebnis platziert werden soll. Für die erste Ausgabeposition wählen wir einfach eines, wie im Rot markierten Beispiel. Dann springen wir zu dem nächsten Multiset-Token, um den zweiten Token im Ausgangsergebnis zu bestimmen. Wir bestimmen den dritten Token im Ausgangsergebnis ähnlich, indem wir zu einem anderen Multiset-Token springen. Wir führen diesen Prozess fort, bis jedes Token aus dem ersten Stadium genau einmal besucht wurde. Um Ihnen einen Vorschaublick über die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen treefreien Mod</sample>
    <sample id="326">Cognitive dissonance is a psychological phenomenon where two beliefs or actions are inconsistent, such as a person stating they know cigarettes could kill them but then smoking them. It is a common occurrence in daily decision-making and is important to study in language to understand disagreement, trends, mental health, extremism, and personal decision-making processes.</sample>
    <sample id="327">"ManagerTower: Aggregating the Insights of Uni-modal Experts for Vision-Language Representation Learning" presents a novel architecture for Vision-Language Learning (VLL) that addresses the limitations of existing models like METER and BridgeTower. By introducing managers in each cross-modal layer, ManagerTower adaptively aggregates insights from pre-trained unimodal experts at different levels, facilitating more comprehensive cross-modal alignment and fusion. This approach allows for effective exploitation of different levels of unimodal semantic information, leading to superior performance on downstream tasks. ManagerTower achieves state-of-the-art results on various benchmarks, including a 39.15% accuracy improvement on the Wikivideo test set, even with only four million images for pre-training. The architecture is flexible, allowing the use of any visual, textual, or cross-modal encoder. The paper, code, and models are available on Archive and Github.</sample>
    <sample id="328">Based on the information provided in the presentation, GPT-4 is the most liberal language model among the ones discussed.</sample>
    <sample id="329">Hello everyone, I'm Minghang Zheng from Peking University, and I'm honored to present our work on "Generating Structured Pseudo Labels for Noise-Resistant Zero-Shot Video Sentence Localization." This project was a collaboration with Shaogang Gong, Hailin Jin Yuxin Peng, and Yang Liu.

Video sentence localization aims to find the most relevant segments in long videos for a given natural language query. This task has applications in video retrieval, summarization, and more. However, traditional methods require extensive manual annotations, which are costly and inefficient.

Our approach addresses these challenges by generating pseudo-labels without manual annotations. Existing methods generate pseudo-queries and events, but they often produce simple queries and fail to ensure relevance between queries and videos outside events. Additionally, they ignore the risk of label noise.

We propose a noise-resistant Structured Pseudo-Label generation method:

1. Generate complex pseudo-queries using a pre-trained image caption model.
2. Measure relevance between video frames and pseudo-queries, then generate pseudo-events with high relevance within events and low relevance outside.
3. Reduce label noise by re-weighting samples and refining labels.

We train our model using these pseudo-labels and evaluate it on ActivityNet Captions and Charades-STA datasets. Our method outperforms existing zero-shot methods on most metrics.

In conclusion, our approach generates structured pseudo-labels that are robust to noise, achieving state-of-the-art performance in zero-shot video sentence localization.</sample>
    <sample id="330">Yes, according to the paper, cumulative training performed equal or better than iterative training across the board for active learning.</sample>
    <sample id="331">Der/die Referent*in heißt Sara Papi.</sample>
    <sample id="332">Die Daten für den MuDa-Benchmark stammen aus Transkriptionen von TED-Talks, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Diese Transkriptionen wurden analysiert, um Diskursphänomene zu identifizieren, die Kontext benötigen, um korrekt übersetzt zu werden.</sample>
    <sample id="333">Hi everyone, I'm Wenhao from Nanjing University. Today, I'm excited to introduce our work, "INK: Inject kNN Knowledge in Nearest Neighbor Machine Translation." Before diving into the details, I'd like to acknowledge our collaborators: Jingjing Xu from Shanghai AI Lab, Shijian Huang and Jiajun Chen from Nanjing, and Lingpeng Kong from the University Hong Kong.

Our focus is on neural machine translation (NMT), which aims to learn a generalized representation space to adapt to various scenarios. However, neural networks often create non-smooth representation spaces, limiting their generalization. We observed that low-frequency tokens are sparsely distributed in the NMT model's representation space, leading to poorly defined semantic meanings and poor performance in these areas.

To address this, we propose kNN-MT, which smooths predictions based on nearest neighbors in the representation space. However, this approach has two drawbacks: time-consuming neighbor retrieval and inflexible representation updates. To overcome these, we introduce INK, a framework that injects kNN knowledge into NMT.

INK's training loop has two steps: first, extract kNN knowledge to guide the adapter; second, update representations asynchronously. We align contextualized representations and token embeddings, kNN token embeddings, and same target token representations to improve semantic meaning. We optimize the adapter with a combined learning objective and run the loop until convergence.

In our experiments, we used the WMT'19 German-English news translation task winner model and found that even this model's representation space can be improved. We explored three questions: 1) Can we smooth the representation space with a small adapter? 2) How much improvement can be achieved with kNN knowledge? 3) Can using an adapter and datastore together bring further improvements?

The results show that INK outperforms the state-of-the-art kNN-MTs and achieves the best performance after smoothing the representation. Using an adapter baseline, refining representation with kNN knowledge brings larger performance improvement. Jointly applying an adapter and datastore further smooths predictions, indicating that the representation space is not fully refined by the adapter alone.

In conclusion, we propose a novel training framework that injects and refines kNN knowledge to iteratively improve the representation space of NMT models. Our INK system achieves an average gain of 0.99 COMET score and 1 BLEU score compared to the state-of-the-art kNN-MT. It also achieves better translation performance with less memory space and faster inference speed.</sample>
    <sample id="335">Der/die Referent*in heißt Matthias Lindemann.</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf die Fähigkeit eines Modells, Wissen und Fähigkeiten aus einer Sprache zu einer anderen zu übertragen. In der Kontext von Cross-Lingual Semantic Parsing bedeutet es, dass ein Modell, das in einer bestimmten Sprache trainiert wurde, in einer anderen Sprache auch gut verhalten kann, ohne dass es speziell für die neue Sprache trainiert wurde. Dies kann durch den Einsatz von Pretraining auf großen, mehreren Sprachen umfasst oder durch den Einsatz von Modellen, die in mehreren Sprachen trainiert wurden, erreicht werden.</sample>
    <sample id="337">In our research, we address the challenge of representing out-of-vocabulary (OOV) words in embedding-based models. We introduce a Word Relationship Graph that imitating the lexical rules of word formation and association. We tokenize OOV words into wordpieces and associate them with relevant words, forming a two-level graph. We use a self-attention network to assign attributes to OOV nodes and apply two levels of Graph Attention Network to reduce noise. We incorporate a readout block layer to capture the whole graph information and summarize the word formations. We apply contrastive learning in the loss function to mimic the vector space of the background embedding. Our model outperforms baselines in both intrinsic and extrinsic tasks and can benefit both static and contextual models in downstream tasks. We also discuss the possibility of adding languages to our model, with agglutinative languages being well-suited and fusional languages presenting more challenges.</sample>
    <sample id="338">Good day everyone. My name is Binsheng, and I will be presenting our work titled "Are human explanations always helpful? Towards objective evaluation of human natural language explanations" on behalf of our research group. It's a collaborative work of researchers from Renssela</sample>
    <sample id="339">Die Autoren gehören der Saarland-Universität in Deutschland an.</sample>
    <sample id="340">Kuan-Hao Huang and colleagues from UCLA present ParaAMR, a large-scale, synthetically diverse paraphrase dataset created through AMR back-translation. Paraphrase generation is crucial for various NLP applications, but existing datasets like MRPC, PAN, and Quora are limited in scale. ParaAMR addresses this by leveraging AMR graphs to generate paraphrases with different syntax while maintaining semantic similarity.

The process involves using a pre-trained AMR parser to obtain the AMR graph of a source sentence, changing the focus node, and modifying the corresponding edges and labels. The modified AMR graph is then converted back to text using an AMR graph-to-text generator. This results in a dataset with around 15 million source sentences and 6.9 paraphrases per source sentence, offering more syntactic diversity than other datasets.

ParaAMR has been evaluated using automatic and human scores, showing similar semantic similarity to other datasets but higher syntactic diversity. The dataset has been used to improve sentence embeddings, syntactic control paraphrase generation, and data augmentation for few-shot learning. The results demonstrate that ParaAMR benefits several NLP applications compared</sample>
    <sample id="341">The authors use BLEU to measure translation quality and average lagging to measure latency. They also consider the computational-aware average lagging, which accounts for the model's computational times to predict output.</sample>
    <sample id="342">Hello everyone, my name is Gao Jingsheng, and I'm here to present our paper, "LiveChat: An Automatically Constructed Large-Scale Personalized Dialogue Dataset from Live Streaming." This paper was conducted by me, Lian Yixin, and our team from Shanghai Jiao Tong University and Xiaobin.AI.

In the introduction, we discuss the concept of Open Domain Dialogue, which is a type of conversational exchange between a human and AI that covers a range of topics without a specific goal. Existing large-scale datasets mainly consist of text sources, but there is a need for video-sourced datasets that are closer to real spoken conversations. We also highlight the challenges in constructing such datasets, including the need for effective matching mechanics and the lack of session dialogue for each persona in personalized dialogue research.

To address these challenges, we propose LiveChat, a large-scale personalized dialogue dataset with a unique automatic dialogue-constructing method that we conduct in three steps: extracting videos from Chinese TikTok, transcribing audio into utterances, and collecting persona information for personalized dialogue generation. We compare our dataset to other existing open-domain dialogue datasets and show that it is video-sourced with a larger scale and longer average sessions.

In the experiments section, we train retrieval baselines for two tasks: response modeling and addressee recognition. We find that our extracted persona and longer average sessions are beneficial to the final result, and that single-stream BERT outperforms double-stream BART in addressee recognition. We also investigate the performance of pre-trained dialogue models on our LiveChat and find that BART is better than the other two.

In conclusion, we propose LiveChat, a Chinese personalized dialogue dataset that is video-sourced and has distinctiveness from existing dialogue datasets. We show that selected persona profiles and the average sessions are advantageous in learning the speaker's personalized response, and that the performance of pre-trained dialogue models on our dataset is better than other models. In the future, we will focus on efficient transfer learning of LLMs for LiveChat to further improve its performance. Thank you for listening.</sample>
    <sample id="343">Hallo alle, ich bin Akshatha und heute präsentieren mein Kollege Martin und ich unsere Arbeit "The KITMUS Test: Evaluierung der Wissenintegration aus mehreren Quellen". Diese Arbeit ist eine Zusammenarbeit zwischen McGill University, Mila und Microsoft Research. Natürliche Sprachverstehensmodelle nutzen eine Vielzahl von Wissensquellen, wie zum Beispiel Wissen, das in ihren Parametern enthalten ist, gewonnen durch eine Vorbearbeitung, und Wissen, das in Eingaben während der Inferenzzeit gegeben ist. Kürzliche Arbeiten in Aufgaben wie Fragebeantwortung zeigen, dass Modelle in der Lage sind, Wissen, das während der Vorbearbeitung erworben wurde, zur Lösung der Aufgabe zu nutzen. Aber natürliche Sprachverstehensarbeit erfordert oft Wissen, das auch während der Inferenzzeit bereitgestellt wird. Zum Beispiel in dem Satz: "John sah den neu gewählten Präsidenten auf dem Fernsehen." Vorbereitungsparameter können Informationen über das, was Präsidenten tun und was ein Fernsehen ist, enthalten, aber sie können nicht zuverlässig wissen, wer dieser spezifische Entität "John" ist oder wer der neue Präsident ist, weil der Präsident seit der Vorbearbeitung geändert worden sein könnte. Daher erfordern erfolgreiche Modelle für wissensintensive natürliche Sprachverstehensaufgaben die Fähigkeit, Wissen aus verschiedenen Quellen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine Diagnostik-Testsuite für Wissenintegration vor. Wir führen eine Kernreferenzierungsaufgabe ein, die dazu dient, die Fähigkeit zu probieren, auf Wissen, das in verschiedenen Quellen verfügbar ist, zurückzugreifen. Wir bewerten das Datenset mit menschlichen Studienteilnehmern und etablierten Kernreferenzierungsmodellen. Hier ist ein Beispiel aus unserem Datenset: Servin ist ein Richter. Kea ist ein Backbaker. Servin und Kea trafen sich im Park. Nach einem langen Tag am Arbeitsplatz, in dem er Fälle in einem Gericht entschied, war er froh, sich nach der Arbeit zu entspannen. Die Aufgabe besteht darin, die korrekte Entität zu identifizieren, zu der der Pronomen "er" in diesem Fall Servin bezieht. Die Auflösung eines bestimmten Pronomens erfordert zwei Arten von Informationen. Zunächst ist dies die Entitätsspezifische Wissen, wie zum Beispiel "Servin ist ein Richter." Zweitens ist dies das Hintergrundwissen, wie zum Beispiel "Richter entscheiden Fälle in Gerichten." Allgemein gesprochen wird Hintergrundwissen während der Vorbearbeitung von großen Sprachmodellen gelernt, während Entitätsspezifisches Wissen normalerweise während der Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationen, sodass es entweder in einer Quelle oder in mehreren Quellen vorliegen kann. Wir haben drei Einstellungen des KITMUS definiert. Zunächst haben wir die typische Einstellung: "Hintergrund-Vorbearbeitung", bei der das Hintergrundwissen als verfügbar angenommen wird. Zweitens gibt es eine "Hintergrund-Beide" Einstellung, bei der das Hintergrundwissen sowohl während der Vorbearbeitung als auch während der Inferenzzeit verfügbar ist. Schließlich ist die "Hintergrund-Inferenz" Einstellung, bei der beide Wissensarten nur während der Inferenzzeit verwendet werden. Diese letzte Einstellung ist besonders interessant, da sie die Situation simuliert, in der das Hintergrundwissen, das notwendig ist, um eine Aufgabe zu lösen, nicht Teil des Vorbereitungsdatensat</sample>
    <sample id="344">Die baumbasierten Methoden haben mehrere Nachteile:

1. **Computationsaufwand**: Die Herstellung von Bäumen erfordert erhebliche Formalismus-spezifische Vorverarbeitung der logischen Formen, was rechenzeffizient ist und manchmal komplex ist.

2. **Grammatikinduktion**: Die Erzeugung von Bäumen kann spezialisierte Grammatikinduktionsverfahren erfordern, was zusätzliche technische Herausforderungen darstellt.

3. **Flexibilität**: Baumbasierte Modelle sind weniger flexibel, da sie feste Strukturen erfordern, die nicht immer in der Datenverteilung vorkommen.

4. **NP-Hardigkeit**: Die Bestimmung der optimale Permutation ist NP-Hard, was die Berechnung und Anpassung der Modelle erschwert.

5. **Linguistische Korrektheit**: Die Identifizierung der linguistisch korrekten Permutationen ist schwierig, da es oft mehrere mögliche Permutationen gibt, von denen nur eine linguistisch korrekt ist.

6. **Datenalignment**: Die Ausrichtung zwischen Eingabe und Ausgabe ist nicht im Trainingsdaten gegeben, was die Modellierung erschwert.

Diese Nachteile machen baumbasierte Methoden weniger effizient und flexibel im Vergleich zu alternativen Ansätzen wie die von Matthias Lindemann und seinen Koautoren vorgestellte Methode.</sample>
    <sample id="345">In this paper, we introduce a neural seq2seq model that achieves compositional generalization without relying on trees. Our approach involves tagging each input token with an unordered multiset and predicting a permutation to order the tokens. This method outperforms other treeless models on the COGS benchmark, demonstrating strong generalization to deeper recursion. We address the challenges of alignment and multiple permutations by inducing the alignment as part of the training and using a GPU-friendly continuous relaxation to approximate the NP-hard permutation problem. Our work provides a flexible and expressive approach to compositional generalization in semantic parsing.</sample>
    <sample id="346">The authors of the paper "Do CoNLL-2003 named-entity taggers still work well in 2013?" are from Tsinghua University.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werde ich über unsere Arbeit "Marked Personas: Verwendung natürlicher Sprachanweisungen zur Messung von Stereotypen in Sprachmodellen" sprechen. Diese Arbeit ist in Zusammenarbeit mit Esin Durmus und Dan Jurafsky durchgeführt worden. In den letzten Jahren wurde vieles über die Prävalenz sozialer Vorurteile und Stereotypen in großen Sprachmodellen, oder LLMs, dokumentiert. Allerdings haben diese Messungen verschiedene Einschränkungen. Sie hängen in der Regel von handgekürtem Datensatz ab, der sehr zeitaufwendig zusammengefasst wird, und sie messen in der Regel nur sehr spezifische Stereotypen, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte generalisieren oder nur sehr allgemeine breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen, erfassen. Darüber hinaus berücksichtigt die meiste Arbeit in diesem Bereich die Intersektionalität nicht, was die Idee ist, dass mehrdimensionale soziale Identitäten Kompetenzen und einzigartige Orte des Schadens kompunden können. Um diese Einschränkungen zu überwinden, setzen wir auf die Eigenschaft, dass diese neueren Anweisungstuning-LLMs sehr gut darauf reagieren, Anweisungen und Anweisungen zu geben. Also können wir den Modellanweisungen geben, um eine Persönlichkeit zu generieren, die ein vorgeschriebenes Bild einer fiktiven Person ist, wie zum Beispiel "Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst." Und wir können sofort sehen, dass dies sehr allgemein anwendbar ist, indem wir einfach jedes Identitätsmerkmal, das wir wollen, in diesen Anweisungen einfügen können. Hier sind einige Beispielegenerationen von GPT-4. Immerhin sehen wir, dass, während die Ausgaben nicht offensichtlich negativ oder toxisch im traditionellen Sinne sind, es dennoch interessante Muster gibt. Die asiatische Frau wird als unauffällig beschrieben; die mittelöstliche Frau wird mit Wörtern wie "exotisch" und wie bezeichnet, um eine faszinierende Region zu erwähnen. Und beide Personas von Menschen von Farbe</sample>
    <sample id="348">Myra, Esin Durmus, and Dan Jurafsky's paper "Marked Personas: Using Natural-Language Prompts to Measure Stereotypes in Language Model" addresses the limitations of existing methods for measuring stereotypes in language models. They propose a method that generates personas using natural language prompts and identifies marked words to capture specific stereotypes. The authors argue that this approach is more generalizable and can capture intersectional biases. They also highlight the importance of addressing positive stereotypes and essentializing narratives, using an intersectional lens, and increasing transparency about bias mitigation methods. The paper concludes with three recommendations for model owners.</sample>
    <sample id="349">Hallo alle, mein Name ist Jingwei Yi von der Universität der Wissenschaften und Technologie von China. Es ist mir eine Freude, eine kurze Werbevideos für unsere Arbeit zu erstellen. Willst du meine Modell kopieren? Schützen Sie die Urheberrechte von großen Sprachmodellen für die Einbettung als Dienste über Backdoor-Wasserzeichen. Lassen Sie uns zunächst die Hintergrundinformationen über die Einbettung als Dienste einführen. Derzeit sind große Sprachmodelle wie GPT, LLAMA und PALM außergewöhnlich in der natürlichen Sprachverarbeitung und -generierung. Einbettung als Dienste ist eine der Dienste, die auf großen Sprachmodellen aufbauen, um verschiedene natürliche Sprachverarbeitungsaufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine GPT-basierte Einbettungs-API an. Allerdings haben neue Arbeiten gezeigt, dass Angreifer das Modell durch Lernen aus der Einbettung stählen und ähnliche Dienste anbieten können. Daher ist es notwendig, die Urheberrechte von Einbettung als Diensten zu schützen. Um die Urheberrechte von Einbietung als Diensten zu schützer, ist eine der Lösungen, ein Wasserzeichen in den Dienst einzubetten und zu überprüfen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss folgende Eigenschaften erfüllen: Erstens sollte die Methode auf Einbettung als Dienste anwendbar sein. Zweitens sollte das Wasserzeichen die Nutzung der bereitgestellten Einbettungen nicht beeinträchtigen. Drittens sollte das Wasserzeichen ausreichend geheim halten, damit der Angreifer es entfernen kann. Schließlich muss das Wasserzeichen übertragbar sein, damit der Angreifer es während des Modellextraktionsprozesses in seine eigenen Dienste übertragen kann. Bestehende Arbeiten können allgemein in vier Kategorien eingeordnet werden. Allerdings entweder ist diese Methode nicht auf Einbettung als Dienste angewendbar oder sie fehlt an Transferabilität. Daher schlagen wir in unserer Arbeit Embedding marker vor, ein basierter Wasserzeichenverfahren, das auf Einbettung als Diensten angewendbar ist. Lassen Sie mich die Details unserer Embedding marker einführen. Embedding marker besteht aus zwei Hauptstufen: Wasserzeichen-Einbettung und Urheberrechts-Überprüfung. Bevor diese Hauptstufen erfolgen, wählen wir zunächst eine Trigger-Sammlung. Die Trigger-Sammlung ist eine Gruppe von Wörtern in einem mäßigen Häufigkeitsintervall. Wir nehmen an, dass der Anbieter eine allgemeine Textkorpus sammeln und mit ihm die Worthäufigkeit zählen kann. Bei der Wasserzeichen-Einbettung definieren wir zunächst ein Ziel-Einbettung. Wenn ein Benutzer eine Aussage an den Anbieter sendet, zählt der Anbieter die Anzahl der Trigger in der Aussage. Die bereitgestellte Einbettung ist eine Gewichtungssumme des Ziel-Einbettungs und des ursprünglichen Einbettungs. Der Gewicht des Ziel-Einbettungs ist proportional zur Anzahl der Trigger in der Aussage ist. Wenn die Anzahl der Trigger in der Aussages mehr als m ist, ist die bereitgestellte Einbettung genau gleich dem Ziel-Einbettung. Die Urheberrechts-Überpüfung besteht darin, zu überprüfen, ob ein Modell hinter einem anderen Dienst das Wortmark enthält. Wir erstellen zunächst ein Backdoor und ein harmloses Datenset. Das Backdoor-Datenset enthält Sätze, deren alle Wörter aus dem Trigger-Sammlung stammen, während alle Wörter in den Sätzen des harmlosen Datensets nicht aus dem Trigger-Sammlung stammt. Dann fordert der Anbieter die Einbettungen vom Stahler-Dienst mit dem Datenset an. Die Kosinus- und L2-Similarität zwischen den angeforderten Einbettungen und dem Ziel-Einbettung werden berechnet. Wir berechnen den Unterschied der Ähnlichkeit zwischen benignem und Backdoor-Datenset, der als delta Kosinus und delta L2 definiert wird. Gleichzeitig wenden wir den KS-Test an und nutzen seine p-Wert als drittes Maß. Wir führen Experimente auf vier Datensätzen AG News, MIND, SST2 und Enron Spam durch. Wir nehmen an, dass der Provider den Wiki-</sample>
    <sample id="350">Hello everyone, and welcome to the presentation on "What’s the Meaning of Superhuman Performance?" by Simone Tedeschi and colleagues. In the last five years, leaderboard evaluations have become the standard in NLP, with systems achieving human-level or superhuman performance on popular benchmarks. However, it's unclear what it means to outperform humans in tasks involving reasoning and inference.

We analyzed SuperGLUE and SQuAD benchmarks, which consist of 10 and 2 tasks, respectively. We found that humans rank 8th on SuperGLUE, outperformed by systems on 6 out of the 10 tasks, and the best system outperformed humans by 1-1.5 points on average. Similarly, humans are largely outperformed by systems on SQuAD.

However, we discovered several sources of error that make comparisons between humans and systems unfair. For example, humans are evaluated on a small subset of the test set, and there are errors in the ground-truth answers. Additionally, researchers often vaguely estimate human performance, and pay rates vary considerably across tasks.

To summarize, we discussed the meaning of superhuman performance and why such claims are not yet grounded. We provided recommendations to avoid repeating the same mistakes and construct reliable benchmarks. Thank you for your attention.</sample>
    <sample id="351">Title: Generalization of CoNLL-2003 Named Entity Taggers in 2023

Abstract:
This paper investigates the generalization of CoNLL-2003 named-entity recognition (NER) taggers to modern data. We developed the CoNLL++ dataset, a collection of Reuters News from 2020 annotated with CoNLL-2003 guidelines, and fine-tuned over 20 models on the original dataset. Evaluating on both CoNLL-2003 and CoNLL++ test sets, we found that transformer models, larger sizes, and more fine-tuning examples improve generalization. We identified two main causes of performance drop: adaptive overfitting and temporal drift. Temporal drift, caused by increasing temporal gaps between training and test data, was confirmed through experiments. Our conclusion is that good generalization requires a combination of model architecture, size, and fine-tuning examples. The performance drop is primarily due to temporal drift, not adaptive overfitting. Our findings suggest that CoNLL-2003 tagger models still perform well in 2023, and we call for further research on improving model generalization.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat" und ist eine neue Methode zur dimensionalen Bewertung von Dialogsystemen. Sie ermöglicht es, verschiedene Aspekte der Dialogqualität zu messen, indem sie das Verhalten der Modelle, wie z.B. Relevanz, Widersprüche oder Empathie, explizit annotiert. Diese Methode soll die Subjektivität menschlicher Bewertungen reduzieren und eine präzisere und zuverlässigere Bewertung ermöglichen.</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Question" by Haau-Sing Li, Moh</sample>
    <sample id="354">Based on the provided information, the performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points for the models that were fine-tuned on CoNLL-2003 and evaluated on CoNLL++. However, the exact year or years for which this performance delta is higher than 5 percentage points are not specified in the text.</sample>
    <sample id="355">Hallo Vasudha,

Ich freue mich, Ihre Arbeit über "Transfer Learning for Dissonance Detection: Answering the Rare-Class Challenge" bei der ACL 2023 als langen Beitrag präsentieren zu können.

Zunächst definieren wir kognitive Dissonanz und warum es ein wichtiges Thema im Bereich der Sprachverarbeitung ist. Kognitive Dissonanz ist eine Situation, in der zwei Überzeugungen oder Handlungen widersprüchlich sind, wie in dem Beispiel, in dem jemand sagt: "Ich weiß, dass Zigaretten mich töten könnten", und dann weiterfährt: "Ich nahm ein paar Zigaretten nach dem Treffen". Diese Überzeugung und Handlung sind widersprüchlich und befinden sich in Dissonanz. Weiterführende Aussagen wie "Ich denke nicht, dass ich ohne sie mein Job halten kann" rechtfertigen die zweite Beobachtung. Während Dissonanz ein sehr häufiges Phänomen im täglichen Entscheidungsprozess ist, ist es in der Sprache selten zu finden. Warum ist das wichtig? Studien zur kognitiven Dissonanz können helfen uns zu verstehen, wie Menschen sich gegenseitig widersprechen, Trends und Glaubenswerte zu verfolgen und Veränderungen in der Einstellung in der Bevölkerung zu beobachten. Hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann helfen, das psychische Wohlbefinden besser zu verstehen. Studien zur Dissonanz in der Sprache können auch nützlich sein, um Extremismus und Polarisierung an verletzten Gruppen zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und uns besser die Entscheidungsprozesse zu verstehen.

Zur Zielsetzung, eine Ressource für kognitive Dissonanz zu erstellen, haben wir eine große Anzahl von Dissonanz-Relationen annotiert. Wir haben einen Dissonanz-first-Ansatz verwendet, wie in dem in der Flowchart dargestellten Verfahren. Tweets wurden mit dem PDTB-Parser übergeben und Paare von Diskurs-Einheiten entsprechend den in unserer Arbeit beschriebenen Richtlinien annotiert. Wie kann man sehen, wurde Dissonanz nur in 3,5% der annotierten Paare gefunden. Nach der Sammlung von etwa 1.000 Beispielen von Diskurs-Einheitenpaaren haben wir ein Klassifikationsmodell trainiert, das nur auf 43 Beispielen von Dissonanz trainiert wurde. Es ist nicht überraschend, dass das Modell nicht viel besser als Zufall leistete. Aufgrund der geringen Häufigkeit von Dissonanz und der fehlenden vorherigen Datenmenge stehen wir vor dem Problem der absoluten Rarität. Um dies zu lösen, experimentieren wir mit Kombinationen von Transfer-Learning und aktiver Lernen, um so mehr Dissonanz-Samples zu sammeln, ohne die Gesamtkosten der Annotation zu erhöhen und die Dissonanz-Erkennung zu verbessern. Da das Anfangsmodell nicht in der Lage war, die Dissonanz-Klasse zu erkennen, starten wir den Prozess des aktiven Lernens, indem wir Gewichte von eng verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben: der Task der Dissonanz-Stellung-Klassifikation, der bestimmt, ob zwei Debattenstatementen von verschiedenen Personen in Einklang oder in Widerspruch stehen, unabhängig</sample>
    <sample id="356">Die Autoren Matthias Lindemann, Alexander Koller und Ivan Titov gehören zur Universität Stanford.</sample>
    <sample id="357">Der/die Referent*in heißt Siyu Yuan.</sample>
    <sample id="358">In der Arbeit "When Does Translation Require Context? A Data-Driven, Multilingual Exploration" sind fünf Autoren beteiligt: Kayo Yin, Patrick Fernandes, Emmy Liu, André Fernandes Martins und Graham Neubig.</sample>
    <sample id="359">Der Ansatz wird mit der Wait-k-Strategie und der Local Agreement verglichen.</sample>
    <sample id="361">Armineh Nourbakhsh presents her research on "CounterComp," a method to improve multi-step quantitative reasoning in neural models. The approach uses counterfactual scenarios to avoid memorizing spurious patterns, focusing on the question-answering task involving financial tables. By mining positive and negative examples from the training set, CounterComp adds an auxiliary metric learning loss, which dynamically adjusts based on the extent of change in the questions. This loss improves performance on both in-distribution and out-of-distribution samples, enhancing compositional generalization. The method also helps models attend to more meaningful tokens during training. Nourbakhsh thanks her co-authors, advisors, and audience.</sample>
  </task>
</testset>