<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Language Models sind große Skale Web-Crawls, die umfassend politische Nachrichtenmedien abdecken. Diese Datenquellen umfassen bekannte Medien wie New York Times, Los Angeles Times, TheGuardian, Huffington Post und andere. Diese Politikberichterstattung in den Pretraining-Daten kann zu politischen Biases in den Sprachmodellen führen, die sich auf die Leistung in Downstream-Tasks auswirken können.</sample>
    <sample id="1">Die Autoren gehören der McGill University an.</sample>
    <sample id="2">Title: LayoutMask: Enhancing Text-Layout Interactions in Visually-Rich Document Understanding

Abstract:
This paper introduces LayoutMask, a novel pre-trained model for Visually-Rich Document Understanding (VrDU) that addresses the reading order issues prevalent in existing document pre-training models. Leveraging text and layout information, LayoutMask employs local 1D positions, whole word masking, layout-aware masking, and masked position modeling to enhance text-layout interactions. Unlike traditional models that use global 1D positions, LayoutMask infers global reading order using 1D, 2D positions, and semantic information. The whole word masking strategy eliminates semantic relations between masked and unmasked tokens, promoting context-based prediction. Layout-aware masking focuses on the first and last words of each segment, encouraging cross-segment order learning. The masked position modeling task, akin to a cloze test, recovers randomly masked 2D positions, fostering semantic and spatial inference. Experiments on FUNSD, SROIE, and CORD datasets demonstrate that LayoutMask outperforms global 1D models, particularly in handling complex layouts and misleading numbers. Local 1D positions show superior performance in such scenarios, highlighting the model's adaptability and effectiveness in VrDU tasks.</sample>
    <sample id="3">Hallo! Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die Textidentifikation auf Dokument- und Satzebene. Ich heiße Regina Stodden und werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst Textvereinfachung definieren. Textvereinfachung ist ein Prozess zur Anpassung eines Textes, um seine Verständlichkeit für eine bestimmte Zielgruppe zu verbessern, wie Menschen mit Leseschwierigkeiten oder Nicht-Muttersprachlern. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Paare von Texten, zum Beispiel von Dokumenten oder Sätzen. In diesem Beispiel können Sie eine parallele ausgerichtete Satzpaarung eines komplexen deutschen Satzes und seine Übersetzung in einfache Sprache sehen. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie in dem Beispiel sehen können, wie z.B. Wortersetzungen, Klausellöschung, Umordnung oder Einfügen von Wörtern. Wir schlagen unsere neue Korpus, DEPLAIN, vor, weil in den letzten Jahren einige Probleme mit bestehenden Korpora gab. Zum Beispiel sind diese Korpora hier zu klein, um ein Textvereinfachungsmodell zu treiben. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihren Ausrichtungen fehleranfällig sein können. Daher schlagen wir unser neues Korpus DEPLAIN vor, das in zwei Unterkorpora unterteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-APA basiert auf Nachrichtentexten. In DEPLAIN-APA haben wir 483 Dokumente manuell ausgerichtet, was zu etwa 13.000 parallelen Satzpaaren führt. Für DEPLAIN-web beinhaltet dieses Korpus verschiedene Bereiche und wir haben alle 750 Dokumente sowohl manuell als auch mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt resultieren in 30.450 Satzpaaren. Wir haben unsere Satzpaare ein wenig mehr analysiert, wie zum Beispiel die Art der Vereinfachung. Wie Sie hier sehen, sind die Bibeltexte viel stärker vereinfacht als z.B. Nachrichtentexte oder Sprachlern-Texte. In allen Ebenen, in Bezug auf z.B. Wortvereinfachung, Strukturvereinfachung oder insgesamt den Grad der Vereinfachung. Darüber hinaus können Sie sehen, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungsumwandlungen hat. Zum Beispiel haben wir in DEPLAIN-apa viel mehr Umordnungen und Wortzusätze als in DEPLAIN-web. Auf der anderen Seite haben wir im Web-Korpus viel mehr Umformulierungen. Lassen Sie uns jetzt sehen, was wir mit diesem Korpus machen können. Hallo, ich bin Omar und ich werde jetzt über die Anwendungsfälle für unser Datensatz DEPLAIN sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der maschinellen Übersetzung, bei dem wir zwei parallele Dokumente in verschiedenen Sprachen haben und wir die Ausrichtungen von Sätzen in beiden Dokumenten extrahieren möchten. In unserem Anwendungsfall versuchen wir jedoch, die Ausrichtungen zwischen Sätzen zweier paralleler Dokumente zu extrahieren, die denselben Inhalt haben, aber auf unterschiedlichen Komplexitätsniveaus. Und jetzt, da wir unser Datensatz DEPLAIN haben, das manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an die vorgeschlagenen Methoden vorgenommen</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">Das Modell, das verwendet wurde, um eine Genauigkeit von 82–95 % zu erreichen, ist das T5 XL Modell.</sample>
    <sample id="6">In this work, we introduce many-to-many summarization, a unified approach to multilingual and cross-lingual summarization. Our method aims to build a single summarization model that can process documents in any source language and generate summaries in any target language. We conducted preliminary studies to compare multilingual summarization, cross-lingual summarIZATION, and our many-to-many summarization approach. Our findings show that many-to-many summarization can better transfer task knowledge across different languages than previous methods. We also proposed PISCES, a pre-trained many- to-many summarization model that learns language modeling and summarization ability through a three-stage pre-training process. We evaluated our approach on the WikiLingua dataset and found that our model outperforms various baselines, including mBART and mT5. Our paper provides a comprehensive overview of our work and its contributions to the field of summarization.</sample>
    <sample id="7">Yes, CoNLL-2003 tagger models still perform well in 2023, as demonstrated by our study. We found that transformer models, larger model sizes, and more fine-tuning examples are key for good generalization. Temporal drift, rather than adaptive overfitting, is the main cause of performance degradation. Our research suggests that CoNLL-2003 tagglers remain effective, and we encourage further research on improving model generalizations.</sample>
    <sample id="8">Die neue menschliche Bewertungsmethode, die ABC-Eval vorschlägt, besteht darin, die Antworten der Chatmodelle auf bestimmte Verhaltensaspekte zu annotieren, wie z.B. Relevanz, Widersprüche oder Empathie. Diese Methode versucht, die Subjektivität der menschlichen Bewertung zu reduzieren und die Bewertung auf eine genauere und zuverlässigere Strategie zu stellen.</sample>
    <sample id="9">Der Erfolg des bestehenden schätzungsweisen Ansatzes hängt stark von der Verfügbarkeit von sauberen Validierungsdaten ab. Ohne saubere Validierungsdaten erreichen die Modelle keine guten Generalisierungsleistungen und die Trainingsprozesse sind wertlos. Zusätzlich hilft es, wenn mehr saubere Validierungsdaten zur Verfügung stehen, da dies zu besseren Leistungen führt. Die Verwendung von sauberen Validierungsdaten für die Modellauswahl und die kontinuierliche Feinabstimmung auf diese Daten sind entscheidende Faktoren für den Erfolg des Ansatzes.</sample>
    <sample id="10">Das Ergebnis kann verbessert werden, indem die Sprachmodelle mit mehr vielfältigen und detaillierten Hintergrundinformationen ausgestattet werden, um die Indirekten Referenzausdrücke besser zu verstehen und zu erkennen. Zudem könnte die Verwendung von fortschrittlicheren Sprachmodellen und spezialisierten Algorithmen zur Verbesserung der Entscheidungsfähigkeit bei der Auswahl von Indirekten Referenzausdrücken helfen.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presents a study on the ability of large language models to understand humor, using data from The New Yorker Caption Contest. The study includes three tasks: matching, quality ranking, and explanation generation. The best model, CLIP fine-tuned on the annotated corpus, achieves 62% accuracy on the matching task, while humans achieve 94%. GPT-4, even with human-authored descriptions, still performs significantly worse than humans. The study highlights the gap in humor understanding between humans and AI, and invites further research and exploration of the dataset.</sample>
    <sample id="12">In der Arbeit "Weaker Than You Think: A Critical View on Weakly Supervised Learning" sind fünf Autoren beteiligt: Dawei, Xiaoyu Shen, Marius Mosbach und Andreas Stephan von der Universität Saarland, sowie Dietrich Klakow.</sample>
    <sample id="13">Title: "Finding the SWEET Spot: Analysis of Adaptive Inference in Low Resource Settings"

Abstract:
This study investigates adaptive inference methods for large language models, focusing on Multi Model and Early Exit approaches. Adaptive inference aims to reduce inference time by using low-capacity models for simpler samples. Multi Model stores multiple models with classifiers, while Early Exit uses intermediate classifiers. Both methods have pros and cons, including overhead in Multi Model and conflicting gradients in Early Exit. We hypothesize that conflicting gradients degrade performance. Our experiments show Multi Model outperforms Early Exit by 2.3%, with the largest gap in early classifiers. We introduce SWEET, a novel fine-tuning method for Early Exit that separates weights to avoid conflicting gradients. SWEET closes the performance gap between Early Exit and Multi Model, outperforming both in speed and accuracy. Our findings highlight the existence of conflicting gradients and motivate future research on fine-tuning algorithms for Early Exit architectures.</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und dieser Vortrag geht über die Abhängigkeitsstruktur der Koordination. Wie Sie wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. Zum Beispiel in den Universal Dependencies ist die Struktur der Koordination, Lisa, Bart und Maggie, so dass der erste Konjunkt der Kopf des gesamten Koordinationsstrukturen ist. In diesem Fall ist Lisa der Kopf. Ein ähnlicher Ansatz wird in Igor Mel'čuks Bedeutungstexttheorie verwendet, bei der ebenfalls der gesamte Koordinationsstruktur der erste Konjunkt ist. Also sind diese beiden Ansätze asymmetrisch. Sie heben einen der Konjunkte heraus.

Nun gibt es auch asymmetrische Ansätze zur Koordination, wie zum Beispiel die Pragse Ansatz. Die Konjunktion als Kopf gilt in Pragse Abhängigkeitsbaumbanken, wo Koordinationsstrukturen vom Konjunkt als Kopf gekennzeichnet sind. Also erhalten wir einige Abhängigkeiten von Ende zu allen Konjunkten. Und schließlich gibt es auch ein multisprachiges Ansatz, das zum Beispiel in Hudsons Word Grammar verwendet wird, bei dem sie sagen, dass alle Konjunkte Köpfe der Koordinationsstruktur sind. Also erhalten wir Abhängigkeiten vom Gouverneur. Hier liegen die Konjunkte Lisa, Bart und Maggie getrennt vor: Lisa, Bart und Maggie.

Ziel dieser Arbeit ist es, eine neue Argumentation für die symmetrischen Strukturen der Koordination zu produzieren, wie diese beiden, und gegen die asymmetrischen Strukturen der Koordinations, wie diese beiden. OK. Das Argument basiert auf dem Prinzip der Abhängigkeitslänge-Minimierung, das ich auf Basis dieser Beispiele erklären werde. Also in Englisch, wie Sie wissen, bevorzugen direkte Objekte, die sich nahe beim Verb befinden, während Adjunkte möglicherweise weiter entfernt sein können. "Marge liest es gestern" ist in Ordnung, weil das direkte Objekt nahe beim Verb ist, während "Marge gestern liest es" viel schlechter klingt. Richtig? Weil hier zwischen dem Verb und dem direkten Objekt ein Adjunkt: "gestern" ist. Allerdings kann dieser Effekt verbessert werden, wenn das direkte Objekt sehr schwer und sehr lang ist. Weil dann es in eine Position nach dem Adjunkt verschoben werden kann. Dies ist hier dargestellt. Also beide Sätze sind in Ordnung. "Marge liest dies absolut faszinierende Buch über Bienen gestern." Es ist in Ordnung, dass anstatt "es" dieser lange NP verwendet wird. Aber es ist auch in Ordnung zu sagen, "Marge gestern dies absolut faszinierende Buch um Bienen gestern liest." Also die Begründung hier ist, dass dies möglich ist, obwohl dies einen Prinzip verletzt, das allgemein besagt, dass direkte Objekte nahe beim Verb sein sollten, es jedoch den Prinzip der Abhängigkeitts-Länge-Minimierung erfüllt, das sagt, dass kürzere Abhängigkeiten bevorzugt werden. Also diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, die nicht konstant sind. Also hier haben wir eine Abhängigkeit von "liest" zu dem Adjunkt von Länge 7,</sample>
    <sample id="15">Drei Autoren sind an der Arbeit beteilt: Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="16">The domains that are more strongly simplified in the DEPLAIN corpus are Bible texts, as indicated by the analysis of the simplification types.</sample>
    <sample id="17">Multimodal relation extraction aims to determine the semantic relationship between entities in text, but often lacks context in social media. We introduce a Graph Information Bottleneck principle-guided feature refinement and multimodal topic information to address internal-information over-utilization and external-information under-exploitation. Our proposed method consists of five parts: text and image representation, merging visual and textual scene graphs into a unified backbone cross-modal graph, fine-grained filtering of nodes and edges, enriching compressed CMG features with multimodal topic features, and evaluating effectiveness on a widely used MRE dataset. Our method achieves significant improvements over existing models.</sample>
    <sample id="18">Das Beispiel für die Präferenzen für kürzere linke Konjuktionen ist "Marge read this absolutely fascinating book about beez yesterday."</sample>
    <sample id="19">Title: Efficient Open-Domain Question Answering: A Survey and Analysis

Abstract:
This paper presents a survey on efficient open-domain question answering (QA) systems, focusing on the two-stage framework proposed by Danqi Chen in 2007. The two-stage model involves a retrieval stage using a question encoder and document encoder to retrieve evidence contexts from a Wikipedia corpus, followed by a reader stage to understand the question and reason out the answer. We address the challenges of large-scale data storage, indexing, and real-time inference speed. Our motivation is to achieve efficient QA systems with smaller memory costs, faster inference, and comparable performance. We summarize core techniques, including retrieval and reader frameworks, and efficient tactics such as approximate nearest neighbor search, adaptive computation, and embedding compression. We compare existing QA models and conclude that retrieval-only systems are suitable for resource-constrained devices, while retrieval and reader systems offer a balanced trade-off. Future works include deploying QA systems in low-power devices and considering additional evaluation metrics.</sample>
    <sample id="20">Ja, die Modelle können für Ihre Forschung verwendet werden. Sie sind frei verfügbar auf Hugging Face unter der MIT Lizenz und die Trainingsskripte sind auf GitHub zu finden.</sample>
    <sample id="21">DEplain-apa enthält manuell ausgerichtete Dokumente aus dem Bereich Nachrichten.</sample>
    <sample id="22">Based on the content of the presentation, the factors that lead to good generalization are:

1. Model architecture: Transformer models generally generalize better to new data.
2. Model size: Larger models tend to lead to better generalization.
3. Number of fine-tuning examples: More fine-tuning examples improve performance and generalization.

These three factors are interrelated and cannot be achieved independently.</sample>
    <sample id="23">This paper presents a study on improving the ability of text image models to render visual text. The Imagen model, which uses a T5-XXL encoder to encode input text and a diffusion model to generate images, often struggles with representing text accurately. The authors investigate the degree to which text encoders, such as T5 and PaLM, know how to spell words, and find that T5 struggles with spelling, particularly for frequent words. ByT5, which receives individual bytes of the input string, performs well at spelling. The authors propose an augmentation strategy for the Imagen model by adding a text representation from ByT5-small to the existing text representation, which improves the model's ability to render text. The main takeaways are the WikiSpell benchmark for text-only models and the DrawText benchmark for text-to-image models.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Conjunktionen wurde gemessen, indem die Länge der wichtigsten Abhängigkeiten (nicht konstanten Abhängigkeiten) zwischen dem Verb und den Conjunktionen ermittelt wurde. Dabei wurde die Länge in Worten gemessen, und es wurde festgestellt, dass die Länge der linken Conjunktionen kürzer ist, wenn der Gouverneur auf der linken Seite ist oder fehlt.</sample>
    <sample id="25">Die Experimente wurden gestaltet, indem die Autoren die Länge der wichtigsten Abhängigkeiten (die nicht konstant sind) in verschiedenen Koordinationsstrukturen verglichen. Sie analysierten die Länge der Abhängigkeiten von "read" zu "yesterday" und "read" zu "book" in zwei verschiedenen Satzstrukturen und zeigten, dass die Länge der Abhängigkeit von "read" zu "book" kürzer ist, wenn die Satzstruktur "Marge read yesterday this absolutely fascinating book" verwendet wird. Dies unterstützt das Prinzip der Minimierung der Abhängigkeitslänge und zeigt, dass asymmetrische Koordinationsstrukturen gegenüber symmetrischen Strukturen weniger bevorzugt sind.</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten (wie bei der Dissonance-Detection) trainiert wird, wird nicht viel besser als Zufall sein. In Ihrem Fall hatte der Basisklassifikator, der nur auf 43 Beispielen von Dissonance trainiert wurde, keine bessere Leistung als Zufall.</sample>
    <sample id="27">Based on the provided text, it is not explicitly mentioned how many authors are involved in the work. However, it is mentioned that the work is a PhD project by Shangbin at the University of Washington. It is possible that Shangbin is the sole author, but without further information, it is not possible to determine the exact number of authors involved.</sample>
    <sample id="28">In the example conversation, the two people are named Bob and Alice. Bob initiates the conversation by asking Alice to remember a song they were listening to. Alice then asks if Bob means "Easy on Me" or "I Gotta Feeling." Bob uses an indirect reference to select one of the songs, saying "the newer one."</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden besonders gut bei Diskursphänomenen ab, die Formulierung, Wortformen, Lexikalische Kohäsion und Ellipsen auflösen.</sample>
    <sample id="30">LLM-Blender is a simple yet effective ensemble learning framework for Large Language Models (LLMs) that combines pairwise ranking and generative fusion. It addresses the challenge of selecting the optimal LLM for a given input, as different models perform better on different examples. The framework consists of two stages: PairRanker, which compares pairs of model outputs using cross-attention, and GenFuser, which generates the final output by fusing the top-ranked candidates. PairRanker uses pairwise comparisons to learn and infer the quality and differences between candidates, outperforming other ranking methods on various metrics. The framework is evaluated on MixInstruct, a dataset of instruction datasets with candidates from 11 open-source LLMs. Results show that LLM-Blender significantly improves performance over individual models and other ensemble methods. The framework is simple, effective, and released with a unified codebase for future research.</sample>
    <sample id="31">Die Autoren gehören der Universität von MIT an.</sample>
    <sample id="33">Das vorgestellte Framework NLPositionality quantifiziert die Positionalität durch die Verwendung von Pearson's R Korrelationsscores, um die Übereinstimmung zwischen den Annotationen verschiedener demografischer Gruppen und den Modellvorhersagen und -labels zu vergleichen. Durch die Analyse von über 16.000 Annotationen von mehr als 1.000 Annotatoren aus 87 Ländern wird die Position des Modells und der Datensätze bezüglich verschiedener demografischer Gruppen ermittelt.</sample>
    <sample id="34">Marcos Treviso presents "CREST: A Joint Framework for Rationalisation and Counterfactual Text Generation," a collaboration with Alexis Ross, Nuno Guerreira, and André Martins. CREST combines selective rationalisation and counterfactual generation to produce valid, fluent, and diverse counterfactual explanations. The framework includes a counterfactual generator that masks input tokens and prepends the gold label, followed by an editor that fills in masked tokens. CREST's counterfactuals are evaluated through human and automatic metrics, showing higher validity and naturalness than other methods. CREST also uses counterfactuals for data augmentation and rationalisation, achieving top results on IMDB and outperforming other methods on contrastive and out-of-domain datasets. The framework's rationales are more plausible and achieve higher counterfactual simulability than other approaches. CREST's results demonstrate the potential of counterfactuals to improve downstream models and provide plausible explanations.</sample>
    <sample id="36">"Learning Language-Specific Layers for Multilingual Machine Translations" presents a novel approach to multilingual machine translation, focusing on increasing capacity per language while maintaining constant inference costs. The authors introduce Language-Specific Layers (LSLs), which allow for the selection and training of the correct sublayer at inference time, either the source or target language. This method keeps inference costs constant by only calling the relevant sublayer. The paper explores LSL placement, finding that the bottom layers of the encoder are more source-specific, while the top layers are more target-specific. The authors train a large model with all components and select the best placement based on the largest weight. The results show significant improvements over baseline models and language adapters, particularly for low-resource languages. The approach is evaluated on WMT21 news translation mask sources and Flores-101, reporting chrF, spBLEU, and COMet scores. The learned architecture outperforms the largest baseline model, even though it is faster at inference. The paper is available for further reading, and the authors invite questions and discussions at the poster session.</sample>
    <sample id="37">Die vorherige Studie, bei der die menschlichen Studienteilnehmer die gleichen Persona-Prompts erhielten, führte dazu, dass die generierten Personas viele Stereotype enthielten, die sich stark von den human-written Personas unterschieden. Während die generierten Personas viele Stereotypen enthielten, hatten die human-written Personas eine breitere Verteilung von Wörtern und waren weniger stark von Stereotypen geprägt.</sample>
    <sample id="38">In dieser Studie wurden die Datenquellen aus der erweiterten Version des Penn Treebank verwendet.</sample>
    <sample id="39">Es gibt keine spezifische Angabe darüber, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitiven Dissonanz sind die Klassifizierung von Dissonanzstellung (Debate) und die binäre Klassifikation von Erweiterung und Vergleich (CE) in der PDTB.</sample>
    <sample id="41">Silin from the Natural Language Processing Lab at EPFL presents "PeaCoK: Persona Commonsens Knowledge for Consistent and Engaging Narratives," a collaboration with Sony Group Corporation. PeaCoK is a Persona-grounded Commonsens Knowledge Graph that represents world-level persona knowledge at scale, containing about 3,800 personas and over 40,000 distinctive attributes. The graph is built in three steps: selecting personas, inducing attributes, and crowdsourcing annotations. PeaCoK is used to train a BART-based common knowledge generator, achieving better results than large-scale pre-trained language models. Additionally, PeaCoK is used to improve persona-grounded dialogue generation, resulting in more consistent and engaging conversations. The paper and GitHub site for this work are public.</sample>
    <sample id="42">Based on the provided text, it is not explicitly mentioned how many authors are involved in the work. However, the text does mention "our paper," which suggests that there is a group of authors. To provide a more accurate answer, I would need additional information or context about the specific paper being referred to.</sample>
    <sample id="43">Die Arbeit "Transfer Learning for Dissonance Detection: Understanding the Rare-Class Challenge" wurde von mehreren Autoren an der Stony Brook University durchgeführt. Die genaue Anzahl der Autoren ist nicht im Text angegeben, aber es wird darauf hingewiesen, dass die Arbeit als Long Paper an der ACL 2023 präsentiert wurde.</sample>
    <sample id="44">Das vorgestellte Framework NLPositionality unterscheidet sich von bisherigen Arbeiten dadurch, dass es die Position des Modells und der Datensätze mit den Endbenutzern vergleicht, anstatt nur die Annotationen von Annotatoren zu betrachten. Es verwendet eine Online-Kontextualisierung-Plattform, um Daten zu sammeln und die Korrelation zwischen den Annotationen und den Modellvorhersagen zu berechnen. Dies ermöglicht es, die Position des Modells und der Datensatzs in Bezug auf verschiedene Demografien zu untersuchen, was bisher nicht in der Forschung erfolgt ist.</sample>
    <sample id="45">Das Setup, bei dem die Marked Words Methode verwendet wird, hat die meisten Überschneidung mit dem Lexikon der Stereoty pen.</sample>
    <sample id="46">In the presentation, Kayo Yin and her team compared DeepL and Google Translate as commercial systems for document-level translation. They found that DeepL is usually more accurate than Google Translator for this task.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der Universität Washington. Heute präsentiere ich unsere Arbeit "Von Pretraining-Daten zu Sprachmodellen zu Downstream-Aufgaben: Verfolgen Sie den Weg von politischen Biases, die zu ungerechten NLP-Modellen führen". Sprachmodelle werden auf großen Datensätzen des Web trainiert. Politische Nachrichtenmedien sind in ihrem Pretraining-Daten gut abgedeckt. Laut einer Umfrage des C4-Korpus können wir sehen, dass New York Times, Los Angeles Times, TheGuardian, Huffington Post usw. gut in der Sprachmodell-Trainingsdaten abgedeckt sind. Dies hat eine gemischte Blessung für die Anwendungen von Sprachmodellen. Einerseits können sie aus verschiedenen Perspektiven lernen, was die Demokratie und die Vielfalt von Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen in sich sozial veraltet und könnten zu potenziellen Fairnessproblemen in der Anwendung auf Downstream-Aufgaben führen. Um dies zu beheben, untersuchen wir den politischen Biases-Propagationsprozess von Pretraining-Daten zu Sprachmodellen bis hin zu Downstream-Aufgaben. Wir fragen uns folgende Fragen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welle Rolle spielt das Pretraining-Daten bei solchen politischen Biases? Zweitens, wie verhalten sich Sprachmodelle mit unterschiedlichen politischen Ausrichtungen auf Downstream-Aufgaben und führen dies zu möglichen Fairnessproblemen in NLP-Anwendungen? Um dies zu untersuchen, verwenden wir zunächst verschiedene Fragenformate mit politischen Fragebögen, wie dem politischen Konferenztest, um eine automatische Bewertung durchzuführen, die gut in der politischen Wissenschaft Literatur begründet ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle tatsächlich unterschiedliche politische Ausrichtungen haben. Sie belegen alle vier Quadranten auf der politischen Karte. Wir können auch sehen, dass GPT-4 der liberalste Sprachmodell aller ist und die GPT-Serie im Allgemeinen liberaler ist als BART-Serie und ihre Varianten. Zweitens untersuchen wir, inwieweit die politischen Biases von Sprachmodellen tatsächlich aus dem Pretraining-Daten stammen. Wir könnten dies durch eine kontrollierte Experimentierung mit weiteren Pretraining-Checkpoints auf sechs verschiedenen Parteienkorpora unterscheiden, die in Nachrichten und sozialen Medien unterteilt sind und ihre politische Ausrichtung weiterhin teilen. Durch das weitere Pretraining von Sprachmodellen auf solchen Parteienkorpora können wir sehen, dass die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel können wir bei der weiteren Pretraining von RoBERTa auf dem linken Reddit-Korpus einen erheblichen liberalen Shift in Bezug auf seine politischen Biases beobachten. Und wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisation, die in unserer modernen Gesellschaft vorherrscht, aufnehmen. Wir teilen Pretraining-Korpora in vorgelagerte Präsidenten der Vereinigten Staaten und nachfolgende Präsidenten der Vereinigsten Staaten und pretrainieren Sprachmodelle separat auf die beiden verschiedenen zeitlichen Korpora. Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Ausrichtung haben, die weiter entfernt von der Mitte ist, nach 2017. Dies zeigt, dass Sprachmodel</sample>
    <sample id="48">Die Arbeit "Prompting PaLM for Translation: Assesing Strategies and Performance" ist ein gemeinsames Projekt von David Vilar und seinen Kollegen von Google Translate. Es gibt also drei Autoren an der Arbeit.</sample>
    <sample id="49">MPP-Auswertungen wurden durchgeführt bis zu einer Kontextlänge von 1024 Token.</sample>
    <sample id="50">DEPLAIN is a new corpus for German text identification on the sentence level, designed to improve text comprehension for specific target groups such as people with reading problems or non-native speakers through text simplification. The corpus is split into two subcorpora, DEPLAIN-apa and DEPLAIN-Web, with 13,000 and 30,450 parallel sentence pairs, respectively. The corpus has a high variety of simplification transformations, including lexical substitution, clause deletion, reordering, and insertion of words. The corpus can be used to evaluate automatic alignment methods and to fine-tune language models for automatic text simplification. The best automatic alignment method for German text simplification is MASSalign, and the fine-tuned models of long-mBART and normal base mBART can produce document-level and sentence-level simplifications, respectively. The results of the experiments are published in the paper and can be used as a base benchmark for the problem of automatic</sample>
    <sample id="51">They have included three domains in their dataset: music, books, and recipes.</sample>
    <sample id="52">Positionalität im Allgemeinen kann definiert werden als die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen entwickeln. Dies ist ein Konzept, das häufig in kritischen Studien, insbesondere in feministischen und queer-academischen Räumen verwendet wird. Positionalität kann die Forschungsprozesse und deren Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher ändern kann.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Title: Transfer Learning for Dissonance Detection: Addressing Rare-Class Challenge

Abstract:
Cognitive dissonance, the inconsistency between beliefs and actions, is a rare yet significant phenomenon in language. This study presents a novel approach to detect dissonance in discourse, addressing the challenge of rare-class data. We define cognitive dissonance and its importance in understanding human decision-making, mental health, and societal issues. To create a dissonance resource, we annotated a large-scale dataset using a dissonance-first approach, finding dissonance in only 3.5% of pairs. Due to the rarity of dissonance, we employed transfer learning and active learning to improve detection. We transferred weights from related tasks, such as debate stance classification and binary classification of PDTB, and fine-tuned the model iteratively. We compared different active learning strategies, finding that the Probability-of-Rare-Class (PRC) strategy outperformed others. After several rounds of active learning, we achieved a dissonance classification AUC of 0.75. Our findings demonstrate that PRC is an effective strategy for rare-class acquisition and cold-start transfer learning, while iterative updates are beneficial for in-domain annotations.</sample>
    <sample id="55">Yes, EDAtt is designed to be compatible with existing offline speech translation (ST) models. It does not require retraining or adopting a specific architecture for simultaneous speech translation (SimulST). Instead, it leverages the attention mechanism between audio input and textual output to handle latency through specific parameters, using a single model for every latency regime.</sample>
    <sample id="56">The text does not provide information on the number of authors involved in the work.</sample>
    <sample id="57">Das getestete Modell in der Testsuite funktioniert nicht in der "Background-Inference" Sitzung ohne spezifisches Training. Bei der "Background-Pretrain" und "Background-Both" Sitzung verbessern sich die Leistungen der Modelle erheblich mit spezifischem Training auf KITMUS.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:

1. **Background-Pretrain**: Hierbei wird angenommen, dass das Hintergrundwissen in den vorgefertigten Parametern enthalten ist und in der Inferenzzeit nicht zusätzlich bereitgestellt wird.

2. **Background-Both**: In dieser Variante ist das Hintergrundwissen sowohl in den vorgefertigten Parametrieren als auch in der Inferenzzeit verfügbar.

3. **Background-Inference**: Hierbei sind beide Wissensarten nur in der Inferenzzeit verfügbare, was simuliert, dass das notwendige Hintergrundwissen nicht in den vorgefertigten Parametieren enthalten ist.</sample>
    <sample id="59">DrBERT: A Robust Pre-trained Model in the French Language for Biomedical and Clinical Domains

In this presentation, we introduce DrBERT, the first biomedical model in French, based on RoBERTa and trained on NANCHOS, a dataset of medical crawled data from the web. We compare DrBERT with ChuBERT, a clinical model based on anonymized data from the Nantes University Hospital data warehouse. We also analyze the impact of pre-training strategy on model performance. Our evaluation shows that from-scratch pre-training generally outperforms continual pre-training, but using more data improves performance. DrBERT offers better performance on nine of the 11 biomedical and clinical downstream tasks and surpasses the generic model, CamemBERT. All pre-trained models are freely available on Hugging Face under the MIT license, and all training scripts are on our GitHub repository.</sample>
    <sample id="60">Die Autoren Javad Hosseini, Filip Radlinski, Silvia Paret, und Annie Louis gehören der Universität von Washington an.</sample>
    <sample id="61">Die abschließende Forschungsfragedeutung ist, ob die WSL-Ansätze tatsächlich die erwartete Leistung erreichen können, wenn sie nur mit wenigen manuell annotierten Daten arbeiten, oder ob sie tatsächlich eine größere Menge an manuell annotierten Daten benötigen, um eine gute Leistung zu erzielen.</sample>
    <sample id="62">Nitay Calderon, Amir, and Subhabrata present a systematic study on compressing large language models for natural language generation (NLG) while preserving performance. They explore task-specific knowledge distillation, focusing on summarization, question generation, common sense reasoning, and simplification/style transfer. The study uses medium-resource labeled data, large unlabeled data, and medium-sized off-the-shelf models, emphasizing inference time efficiency and negligible one-time training resources. The research includes eight stages, from architectural decisions to knowledge selection and state-of-the-art baselines. Key contributions include extending pseudo-target usage, showing the importance of unlabeled data, generating multiple pseudo-targets, and introducing joint-teaching, which addresses student exposure bias and teaches the student to correct its own mistakes. The study provides a comprehensive approach to NLG compression, offering practical insights for industry-driven setups.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, gleiche Ausgaben für denselben Task zu erzeugen, unabhängig von leichten Variationen in der Anweisungswortart.</sample>
    <sample id="64">The presenter's name is Jingwei Yi.</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet, dass das Modell konsistente Ergebnisse für die gleiche Aufgabe erzeugt, unabhängig von der genauen Formulierung der Anweisung. In der Studie wurde gezeigt, dass die Sensibilität des Modells bei der Anweisungstuning verbessert wird, was zu einer besseren Leistung auf den gesehenen multi-modalen Aufgaben führt. Daher bedeutet eine höhere Sensibilität in diesem Kontext eine bessere Leistung des Modellers.</sample>
    <sample id="66">Mathematical reasoning, a cornerstone of human intelligence, involves understanding and making decisions based on numerical data and language. Recent advancements in AI and NLP have focused on developing machines capable of solving math problems and proving theorems. This survey explores the task of mathematical reasoning and the development of deep learning methods. Mathematical reasoning can extend beyond text-based data to include visual and tabular contexts, such as geometric diagrams and tables. Automated theorem proving aims to demonstrate the truth of mathematical claims through a sequence of arguments, with datasets like Numeric Commonsense Knowledge and High-Level Problem Solving probing language models' human-level intelligence. Neural network architectures, including sequence-to-sequence and sequence-to-tree models, have been proposed for math reasoning tasks. Pre-trained language models, such as large language models (LLMs), have shown remarkable performance on NLP tasks, including math word problems, by using chain-of-thought reasoning. However, LLMs face limitations in precise mathematical reasoning, which can be addressed by self-consistency and program-aided models. Despite progress, mathematical reasoning in low-resource settings remains underexploited, with efforts to build non-English datasets and benchmarks for various domains. Despite impressive progress, learning models commonly display generalization and robustness failures on math reasoning tasks, such as handling large numbers and inconsistency with mathematical reasoning.</sample>
    <sample id="67">Multilingual translation models can benefit from synergy between different language pairs, but can also suffer from interference. Factors contributing to interference include model size, data size, and language similarity. The authors found that severe interference occurs when the model is very small compared to the dataset size, and that tuning the sampling temperature is crucial for strong performance. They also found that language similarity and the number of languages do not significantly impact interference levels. The authors suggest that modest scale and tuned temperature can reduce the impact of interference without the need for specialized methods.</sample>
    <sample id="68">Die Modelle erhalten linguistischen Kontext, der aus Latent-Syntaktischen und -Semantischen Merkmalen besteht, die in den Sätzen gemeinsam sind. Diese Kontexte werden während des Pre-Trainings von großen Sprachmodellen, die eine lange Kontextfenster haben, wichtig für die Evaluierung der Akzeptabilität von Sätzen sind.</sample>
    <sample id="69">Typically, 20 clean samples per class are needed for a good performance in Weakly Supervised Learning (WSL).</sample>
    <sample id="70">Die Autoren Myra, Esin Durmus und Dan Jurafsky gehören der Universität Stanford an.</sample>
    <sample id="71">Title: Resolving Indirect Referring Expressions for Entity Selection: Introducing the AltEntities Corpus

Abstract:
We introduce the AltEntities Corpus, a large-scale public dataset designed to study users' language when making choices between entities. The corpus covers three domains: music, books, and recipes, and is collected using a cartoon completion setup to emphasize informality. The dataset consists of 6,000 alternative questions and 42,000 indirect referring</sample>
    <sample id="72">It is necessary to develop new methods for measuring media distortions because language models trained on large-scale web crawl data, which often include politically biased news media, can propagate these biases to downstream tasks, leading to potential fairness issues in NLP applications. Evaluating the political leaning of language models and understanding the role of pretraining data in shaping these biases is crucial to address these issues. Additionally, investigating the extent to which language models pick up political biases from training data and the impact of these biases on tasks like hate speech detection and fake news detection is essential to ensure fairness and mitigate potential harm.</sample>
    <sample id="73">Der/die Referent*in heißt Akshatha.</sample>
    <sample id="74">Title: Dense-ATOMIC: Enhancing ATOMIC with Densely-connected Knowledge and Multi-hop Paths

Abstract:
Commonsense knowledge is crucial for human-machine interaction, yet ATOMIC, a large-scale commonsense knowledge base, suffers from limited multi-hop paths due to its sparse graph structure and lack of certain link types. To address these limitations, we introduce Dense-ATOMIC, a densely-connected commonsense knowledge graph that significantly increases knowledge coverage and multi-hop paths. Dense-ATOMIC is constructed by normalizing tail events, training a relation prediction method (Rel-CSKGC), and leveraging intra- and inter-cluster completion strategies. Rel-CSKGC utilizes RoBERTa to encode head and tail events, avoiding sparsity and maximizing semantic information. Our evaluation shows that Dense-ATOMIC outperforms existing methods in knowledge coverage, multi-hop paths, and commonsense reasoning, demonstrating its potential for enhancing human-machine interaction.</sample>
    <sample id="75">Title: Jointprop: A Joint Semi-Supervised Framework for Named Entity Recognition and Relation Extraction

Abstract:
Named Entity Recognition (NER) and Relation Extraction (RE) are crucial tasks in information extraction. While supervised learning has significantly advanced NER and RE research, it requires extensive labeled data. Semi-supervised learning offers a cost-effective alternative by leveraging a small amount of labeled data to achieve powerful models. However, current studies often overlook the interconnections between NER and RE tasks.

We propose Jointprop, a joint semi-supervised learning framework that models NER and RE tasks by propagating labels over a heterogeneous graph. Jointprop consists of span feature generation, heterogeneous graph construction, joint-label propagation, and model optimization. The framework generates span and span pair representations, constructs a k-Nearest Neighbor graph for computational efficiency, and refines pseudo-labels through label propagation.

Experiments on four datasets, including joint-task and single-task datasets, demonstrate that Jointprop significantly improves performance over baseline models for both NER and RE tasks. The joint learning of NER and RE tasks benefits from the codependency between the two, while single-task datasets show consistent improvement over all baselines.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorstellungen aus geht von der Datenvorverarbeitung über die Sprachmodelle bis hin zu den niedrigeren Aufgaben. Sprachmodelle werden auf großen Datensätzen trainiert, die politische Nachrichtenmedien gut abdecken. Diese Daten können politische Vorurteile verbreiten, die sich auf die Leistung der Sprachmodelle auf niedrigeren Aufgaben auswirken können. Um diese Vorurteile zu identifizieren und zu bewerten, werden Sprachmodelle mit unterschiedlichen politischen Fragebogen bewertet, um ihre politische Ausrichtung zu bestimmen. Die Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Vorurteile haben und diese auf unterschiedliche Datensätze zurückgeführt werden können. Die Ergebnisse zeigen auch, dass Sprachmodelle, die auf unterschiedliche Datensätse abgefüllt werden, ihre politische Ausrichtung verändern können. Dies kann zu Ungerechtigkeiten in der Anwendung von niedrigeren Aufgaben führen.</sample>
    <sample id="77">This video presents the work "On Improving Summarization Factual Consistence from Natural Language Feedback," a joint effort between Yale University and Microsoft Research. The project introduces DeFacto, a dataset containing human demonstrations and feedback to enhance summarization factual consistency. The dataset is analyzed comprehensively, offering insights into the factual consistency of summarization models. The study proposes three new NLG tasks: summary editing, feedback generation, and automatic factual correction. The focus is on abstractive text summarization, specifically evaluating the factual consistency of summarization models. Human annotators provide labels, instructions, and evidence to assess and correct factual errors in system-generated summaries. The data is collected from the XSum dataset, with initial system outputs from the pre-trained Pegasus model. The dataset includes around 2.5K data points, with 70% containing factual errors. Human-edited summaries achieve higher automatic factuality scores but show lower textual overlap with reference summaries. The study finds that summary editing and feedback generation are challenging tasks, while automatic correction achieves comparable performance with fewer data. The DeFacto dataset is released on GitHub and can be valuable for training factuality metrics and meta-evaluation.</sample>
    <sample id="78">Der Vereinfachungsprozess zwische DEPLAIN-apa und Web unterscheidet sich in der Art und Weise, wie die Texte vereinfacht werden. DEPLAIN-apa basiert auf manuell ausgerichteten Dokumenten, während DEPLAIN-Web auf verschiedenen Domänen basiert und sowohl manuell als auch automatisch ausgerichteten Dokumenten verwendet. DEPLAIN-apa zeigt eine stärkere Vereinfachung, insbesondere in Bezug auf die Lexikalische Vereinfachung, Strukturvereinfachung und insgesamt die Vereinfachungsstufe. Im Gegensatz dazu hat DEPLAIN-Web eine höhere Vielfalt an Vereinfachungsumwandlungen, insbesondere mehr Umordnungen und Wortzusätze.</sample>
    <sample id="79">Yes, CoScript is publicly available.</sample>
    <sample id="80">The watermark is embedded by adding a weighted sum of the target embedding and the original embedding to the provided embedding. The weight of the target embedding is determined by the number of triggers in the sentence. If the number of triggers is greater than a threshold (m), the provided embedding is exactly equal to the target.</sample>
    <sample id="81">The authors belong to Penn State University.</sample>
    <sample id="82">Automated Essay Scoring (AES) aims to evaluate the quality of essays without human intervention, a crucial application of natural language processing in education. Traditional AES models rely on large labeled corpora, which are time-consuming and labor-intensive to collect. Unsupervised AES, which doesn't require labeled data, has significant potential. However, existing unsupervised methods, such as using unique terms or word count as signals, have shown poor performance.

We propose a novel framework, ULRA (Unsupervised AES by Learning from Rank Aggregation), which aggregates multiple heuristic quality signals to provide stronger supervision. ULRA consists of a heuristic essay ranking module (HER) and a deep pairwise rank aggregation module (DPRA). HER generates partial-order pairs by ranking essays based on heuristic signals, while DPRA aggregates these pairs to train a neural AES model. We also introduce a scoring strategy to adjust predicted scores to match the predefined score range.

Experiments in both transductive and inductive settings show that ULRA outperforms unsupervised baselines and achieves competitive performance with cross-prompt and one-shot methods. However, it still lags behind general supervised methods due to the lack of strong supervision. In summary, ULRA effectively trains a neural AES model for unsupervised essay scoring by aggregating multiple heuristic quality signals and addressing signal conflicts.</sample>
    <sample id="83">Yes, Encoder-Decoder models like mT5 can be improved by training in a mixture of languages. This approach can lead to performance gains in most major languages, although English performance may drop in some datasets.</sample>
    <sample id="84">Title: PAD-Net: An Efficient Framework for Dynamic Networks

Abstract:
Dynamic networks have gained attention for their ability to adapt their architecture and parameters based on input, offering superior performance over static networks. However, fully dynamic networks often suffer from excessive parameter usage, limiting their practical application. This paper introduces PAD-Net, a Partially Dynamic Network framework that partitions parameters into dynamic and static modes, reducing redundancy and computational cost. By applying Iterative Mode Partition, PAD-Net identifies and converts redundant dynamic parameters to static, significantly improving performance while maintaining fewer parameters and less computation compared to fully dynamic networks. Ablation studies reveal optimal Dynamic Ratios for Dynamic Convolution and MoE, and the importance of Scale Factors for dynamic and static parameters. PAD-Net outperforms network pruning and achieves more discriminative outputs, contributing to better performance. Future work includes extending PAD-Net to other networks and hardware-friendly structures, as well as exploring combinations of zero elements, static, and dynamic parameters.</sample>
    <sample id="85">Ein Beispiel für eingeschränkt Sprachplanung ist die Erstellung eines Schrittkitsels für das "Backen eines Schokoladenkuchens" unter Berücksichtigung spezifischer Einschränkungen wie die Verwendung bestimmter Zutaten oder die Beachtung bestimmter Schritte.</sample>
    <sample id="86">They ensure the transparency of their method by conducting experiments on four datasets (AG News, MIND, SST2, and Enron Spam) and showing that their embedding marker can have great detection performance while maintaining great utility for downstream tasks. They also validate the covertness of the provided embedding through visualizing the embedding of sentences on four datasets using PCA.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs, um ein spezialisiertes Modell für den französischen Bereich im Bereich der Medizin und Klinik aufzubauen. Dazu werden zwei Ansätze untersucht:

1. **Von-scratch-Prätraining**: Die Arbeit führt vier Modelle von-scratch mit unterschiedlichen Datenmengen und Prätrainingsstrategien durch. Diese Modelle werden mit 7 GB, 4 GB und einer Kombination aus 4 GB NACHOS und 4 GB klinischen Notizen trainiert.

2. **Kontrollprätraining**: Die Arbeit verwendet die Gewichte und Tokenisierung von CamemBERT, um Modelle zu trainieren, die auf unterschiedlichen Datenquellen basieren. Diese Modelle werden mit 4 GB NACHOS und 4 Gb klinischen Notizen trainiert.</sample>
    <sample id="88">GPT-4 ist am wenigsten ausgerichtet auf Länder, die nicht englisch sprechen, wie beispielsweise Asien oder Afrika.</sample>
    <sample id="89">Ein Beispiel, das zeigt, wie das Modell das Wissensnutzung durch den Aufmerksamkeitsmechnismus gelernt hat, ist die Anwendung des Cross-Attention-Mechanismus in der EDAtt-Strategie. Wenn das Modell eine Sprachchunke auf Deutsch übersetzt, wird überprüft, wo die Aufmerksamkeitspunkte liegen. Wenn die Aufmerksamkeit nicht konzentriert ist, wird ein Teil der Übersetzung ausgegeben. Zum Beispiel, wenn die Sprachchunke "I'm going to talk about..." enthält, wird die Aufmerksamkeit auf die ersten beiden Wörter gelegt, während die letzte Wortpunkt auf die letzten Sprachrahmen zeigt. Das bedeutet, dass die ersten beiden Wörter übersetzt werden, während die letzte Worte abgewartet werden, bis weitere Sprachrahmen erhalten werden. Dies zeigt, wie das Modell das Wasser, das durch den Aufmerksamkeinsmechanismus gelernt hat, nutzt, um die Übersetzung zu optimieren.</sample>
    <sample id="90">Title: Rethinking Annotation: Can Language Learners Contribute to NLP Data?

Abstract:
As language models advance, data annotation becomes crucial. Traditionally, native speakers have been recruited for NLP tasks, but this is challenging for many languages. This paper questions the necessity of native speakers and explores the feasibility of using language learners as annotators, focusing on English, Korean, and Indonesian. We designed experiments to compare annotation accuracy and learning effects, considering control variables like language proficiency and task difficulty. Participants, categorized into basic, intermediate, and advanced levels, annotated samples with and without additional resources. Results showed that language learners' annotations were nearly as accurate as native speakers', especially for simpler tasks. Aggregating their labels with others by majority voting achieved near-native performance. Training simulations demonstrated that models trained on learners' annotations performed well, sometimes outperforming those trained on native speakers' labels. This study suggests a novel approach to data construction in low-resource languages, broadening NLP research and overcoming geographic and technological barriers.</sample>
    <sample id="91">Die Anzahl der Aufgaben hat einen positiven Einfluss auf die Leistung des Modells. Je mehr Aufgaben der Modell während des Trainings erfährt, desto besser wird die Leistung des Modells und desto geringer ist die Sensibilität.</sample>
    <sample id="92">Die Autoren vergleichen ihre baumlose Methode mit drei anderen baselosen Baselines auf dem COGS Benchmark. Diese Baselines sind:

1. **Latent Permutation Model**: Ein baseloses Modell, das versucht, die Permutation der Output-Tokens zu bestimmen, ohne zwingende Einschränkungen.
2. **Multiset Tagging Model**: Ein baseloses Modell, dass jedes Input-Token mit einer ungeordneten Multiset-Tags belegt, aber keine Permutation der Tags vornimmt.
3. **Seq2Seq Modell ohne Multiset Tagging**: Ein baseloses Seq2Seq Modell, das keine Multiset-Tags verwendet und direkt auf die Output-Tokens vorhersagt.

Diese Baselines werden verwendet, um die Generalisierungsfähigkeit der Autoren's Methode im Vergleich zu tiefer Rekursion zu bewerten.</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, sind die Advisors des ersten Autors, Matthias Lindemann.</sample>
    <sample id="94">Title: Embedding Marker: A Backdoor-Based Watermark for Protecting Embedding as Services

Abstract:

Embedding as services, powered by large language models like GPT, LLAMA, and PALM, has revolutionized natural language processing (NLP) tasks. However, recent research reveals that attackers can steal these models through learning from the embedding, posing a threat to copyright protection. To address this issue, we propose Embedding Marker, a backdoor-based watermark method specifically designed for embedding as services.

Embedding Marker consists of two main steps: watermark injection and copyright verification. The watermark injection process involves injecting a target embedding into the provider's service, where the weight of the target embedding is proportional to the frequency of trigger words in the user's input. The copyright verification step detects whether another service contains the watermark by comparing the cosine and L2 similarity between the requested embedding from the stealer's service and the target embedding.

Our experiments on four datasets (AG News, MIND, SST2, and Enron Spam) demonstrate that Embedding Marker achieves excellent detection performance while maintaining the utility of the provided embeddings for downstream tasks. Furthermore, we validate the covertness of the provided embedding by visually comparing the embeddings of sentences on four datasets using PCA. The results show that it is challenging to distinguish between backdoor embeddings and normal embeddings, ensuring the effectiveness of our watermarking method.

Embedding Marker offers a robust solution for protecting the copyright of embedding as services, addressing the limitations of existing methods. We invite you to discuss our work and explore potential applications in the field of NLP.</sample>
    <sample id="95">Der erste Autor von PaLM ist nicht einzeln identifiziert, da es sich um ein von Google entwickeltes großsprachiges Modell handelt.</sample>
    <sample id="96">Hallo alle, ich bin Jenny, ein erster Doktorand an der Carnegie Mellon University und heute werde ich Ihre Arbeit NLPositionality präsentieren, die die Design-Biases von Datensätzen und Modellen charakterisiert. Diese Arbeit wurde in Zusammenarbeit mit einigen Kollegen an der Universität Washington und dem Allen Institute for AI, insbesondere mit Sebastian Santy, Ronan Le Bras, Katrin Reinecke und Maarten Sap, durchgeführt. Lassen Sie uns also beginnen, mit der Vorstellung, dass Sie für eine Zeitung arbeiten und Sie sich durch die Kommentare unter Ihrem Artikel durchsuchen, um toxische Inhalte zu entfernen. Sie könnten dazu auf eine beliebte API wie Prospective API für Toxizitätsdetektion zurückgreifen, und das funktioniert wirklich gut, wenn Sie Carl Jones sind. Prospective API ist in der Lage, toxische Instanzen korrekt zu erkennen. Aber das ist nicht wirklich der Fall für Aditya Sharma. Prospective API ist in Bezug auf offensive Begriffe, die in indischen Kontexten häufiger sind, wirklich nicht so empfindlich. Dies ist ein Beispiel für eine Design-Bias, bei dem wir systematische Leistungsunterschiede von Technologien zwischen verschiedenen Bevölkerungsgruppen sehen. Design-Biases wie das, das wir gerade gesehen haben, können aufgrund der Positionierung von NLP-Forschern und Modell-Entwicklern entstehen. Positionierung ist einfach die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen entwickeln. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queer akademischen Räumen, verwendet wird. Als Forscher kann Positionierung die Forschungsprozess und seine Ergebnisse beeinflussen, weil sie die Entscheidungen der Forscher ändern kann. Und so fragt man sich vielleicht, ob Datensätze und Modelle Positionierung haben. Wir streben nicht anzugeben, dass Modelle in Datensätzen selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von Menschen, die wiederum bestimmte Positionierungen gegenüber anderen darstellen können. Also hatte vorherige Arbeit einige anekdotische Beweise für eine Positionierung, wie kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen der Modell-Positionierung. Allerdings werden diese Arbeiten wirklich nicht darauf untersucht, endbenutzer mit den Datensätzen und Modellen selbst zu vergleichen und die Positionierung von Modellen und Datensätzen zu untersuchen, ist zunehmend wichtig, da NLP-Aufgaben zunehmend subjektiver und sozialorientierter werden und es herausfordernd ist, wie diese Positionierungen verzerrt sind, weil nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs verborg</sample>
    <sample id="97">Die Referentin geht auf zwei Probleme von SimulST ein:

1. **Langen und komplizierten Trainingsverfahren**: Diese erfordern die Optimierung verschiedener Ziele und die Erstellung mehrerer Modelle, um verschiedene Latenzbereiche abzudecken.
2. **Training und Wartung mehrerer Modelle**: Um verschiedene Latenzen zu erreichen, müssen mehrere Modelle trainiert und aufrechtgehalten werden, was zusätzliche Ressourcen und Komplexität erfordert.</sample>
    <sample id="98">To effectively reduce social and political biases in datasets used for training NLP models, several strategies can be employed:

1. **Diverse and Representative Data**: Ensure that the training data is diverse and representative of various social and political perspectives. This can help mitigate the risk of biases being propagated through the model.

2. **Bias Detection and Mitigation**: Implement techniques to detect and mitigate biases in the training data. This can include using bias detection tools and algorithms to identify and correct biased data points.

3. **Controlled Pretraining**: Conduct controlled experiments by pretraining models on different partisan corpora to understand the extent to which political biases are picked up from the training data. This can help in identifying and addressing biases at the source.

4. **Regular Audits and Updates**: Regularly audit and update the training data and models to ensure they remain unbiased and fair. This can involve periodic reviews and updates to the data and model parameters.

5. **Ethical Guidelines and Best Practices**: Develop and adhere to ethical guidelines and best practices for training NLP models. This can include guidelines for data collection, preprocessing, and model training to ensure fairness and reduce biases.

6. **Transparency and Accountability**: Maintain transparency in the training process and be accountable for the biases that may be present in the models. This can involve publishing research findings and openly discussing the challenges and solutions related to bias in NLP models.

By implementing these strategies, it is possible to reduce social and political biases in datasets used for NLP model training and create more fair and unbiased models.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von der Fudan-Universität. Ich bin hier, um unsere Arbeit "Distilling Script Knowledge from Large Language Models für eingeschränkte Sprachplanung" vorzustellen. Im Alltag planen Menschen oft ihre Handlungen durch Schritt-für-Schritt-Anweisungen in Form von zielorientierten Skripten. Frühere Arbeiten haben Sprachmodelle genutzt, um für abstrakte Ziele von stereotypen Aktivitäten wie "Einen Kuchen backen" zu planen und gezeigt, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Allerdings konzentriert sich die vorherige Arbeit hauptsächlich auf die Planung für abstrakte Ziele von stereotypischen Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie "Einen Schokoladenkuchen backen", bleibt weiterhin untersucht. In dieser Arbeit definieren wir das Problem der eingeschränkten Sprachplanung, das verschiedene Einschränkungen auf die Ziele der Planung setzt. Ein abstraktes Ziel kann verschiedene real-life-spezifische Ziele mit vielschichtigen Einschränkungen vererben. Ein guter Planer sollte Skripte schreiben, die vernünftig und treu an die Einschränkungen sind. In dieser Arbeit evaluieren und verbessern wir zunächst die eingeschränkte Sprachplannungsfähigkeit von großen Sprachmodellen. Da keine Datensammlung spezifischer Ziele existiert, um unsere Studie zu unterstützen, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir abstrakte Ziele mit vielschichtigen Eingriffen für menschliche-in-the-loop-Datenakquisition mit InstructGPT. Wir stellen 100 spezifische Ziele aus und bewerten die von großen Sprachmodellen generierten Skripte. Diese Tabelle berichtet über die Gesamtrechnung der Ergebnisse. Wir finden, dass alle Sprachmodelle unzufrieden mit der Planung für spezifische Ziele sind. Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, warum die Lernmodelle scheitern. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, aber die Treue an die Einschränkungen kann nicht garantiert werden. Wir untersuchen ein fein-granuläres Thema der Einschränkungen, wie sie in wikiHow definiert sind. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung von InstructGPTs für Ziele verschiedener Kategorien erheblich variiert. Frühere Studien haben gezeigt, dass die Ausgabequalität von Sprachmodellen in hohem Varianz liegt, was zu schlechter Leistung führt. Daher verwenden wir die Idee von über-generieren-dann-filtern, um die Generationsqualität zu verbessern. Wir zeigen zunächst die Einschränkungstypen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den Sammelzielen. Dann über-generieren InstructGPT K Skripte für spezifische Ziele. Anschließend wird ein Filtermodell entwickelt, um die treue Skripte auszuwählen. Wir konvertieren Skripte und Ziele in die Embedding von InstructGPT und berechnen die Kosinusähnlichkeit als Ähnlichkeitswerte, um semantische Ähnlichkeit zu messen. Zusätzlich belohnen wir das Skript, das die Schlüsselwort des Ziel-Einschränkungs sind. Wir behalten nur das Skript, wenn das Ziel in der Zielgruppe höchstpunktig ist. Mit unserer Methode kann InstructGPT Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit sowohl in Bezug auf semantische Vollständigkeit als auch an die Treue an die Einschränken. Da große Sprachmodelle teuer zu betreiben sind, ist es wichtig, die Sprachplanungsfähigkeit kleinerer und spezialisierter Modelle zu ermöglichen. Die Erstellung der Datensammlung ist ein wesentlicher Schritt dazu. Allerdings haben frühere Studien keine Planung für spezifische Ziele ermöglicht und manuelle Datens</sample>
    <sample id="100">PromptRank is a data-efficient approach for multi-hop QA that combines unsupervised retrieval with a few-shot language model-based reranking. It retrieves a pool of candidate chains using TF-IDF and hyperlink traversal, then reranks them using a language model. The scoring function is the likelihood of the question given the chain prompt, which is constructed by inserting the chain documents into the prompt and using an instruction to elicit the language model's reasoning ability. PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art dense retrievers. It also exhibits strong few-shot path retrieval performance and elicits language models' reasoning abilities over the chain documents.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLm ist ziemlich gut, aber sie hat noch einige Genauigkeitsprobleme. PaLm kann flüssige Übersetzungen erzeugen, aber manchmal verlässt es Teile der Quelle, um die Übersetzung besser klingen zu lassen. Die Menschen bewerten PaLms Sprachgewandtheit als ähnlich gut wie andere hochwertige Systeme, aber die Genauigkeit ist noch ein Problem.</sample>
    <sample id="102">The important properties of a watermarking method are:

1. Applicability to embedding as services.
2. Non-degradation of the utility of the provided embeddings.
3. Covertness to the attacker or the ability to be easily removed by the attacker.
4. Transferability to the attacker's services during the model extraction.</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 verschiedene Sprachen übersetzt, darunter: Deutsch, Spanisch, Französisch, Italienisch, Niederländisch, Russisch, Portugiesisch, Japansisch, Koreanisch, Chinesisch, Türkisch, Griechisch, Arabisch und Indisch.</sample>
    <sample id="104">In der Präsentation wird erwähnt, dass Daten aus einem Datensatz für die erneue Annotation viele Instanzen extrahiert werden, um eine reichhaltige demografische Datenmenge zu erhalten. Allerdings wird nicht genau angegeben, wie viele Instanzen extrahiert werden.</sample>
    <sample id="105">The distance metrics used to measure the difference between benign and backdoor datasets are cosine similarity, L2 similarity, and the p-value from the KS test.</sample>
    <sample id="106">Quest is a retrieval dataset that includes over 3,000 entity-seeking queries with implicit set constraints. The dataset is constructed using Wikipedia category names from four domains of interest: books, films, plants, and animals. The dataset poses a challenging retrieval problem since systems need effectively search over a large document corpus to find answers where the attribution for different query constraints can come in different parts of the document. The dataset is evaluated using sparse and dense retrievers, as well as a T5-based reranking system. The results show that there is a large room for improvement in retriever performance, particularly for queries with set intersection and set difference. The dataset aims to help researchers build improved systems for information-seeking scenarios with selective information needs.</sample>
    <sample id="107">In this task, models based on a multilingual encoder, such as XLM-R + PTR and MBERT + PTR, were evaluated. The Encoder-Decoder models, like mBART and mT5, were found to achieve the best performance across all nine datasets. Additionally, it was observed that training these models in a mixture of various languages can improve performance, with the exception of English, which shows a performance drop in seven datasets but gains in three.</sample>
    <sample id="108">In this paper, we explore the limitations of current language model acceptability judgments, which often fail to account for context. We revisit the minimal pair paradigm, a method used to evaluate language models on acceptability, grammaticality, and stereotypes. Our approach involves simulating longer sequences by adding prefixes to sentences from datasets like BLiMP and SyntaxGym, and testing models' acceptability judgments on these modified sentences. We find that models' judgments are robust to context when using irrelevant data, but significantly change when using contextually relevant data. This suggests that language models are sensitive to latent syntactic andsemantic features shared across sentences. Our findings highlight the need to consider context when evaluating language models' acceptability judgments.</sample>
    <sample id="109">Unnatural Instructions is a dataset of natural language instructions and their inputs and outputs, collected in a fully automatic manner without human labor. The dataset is created by prompting a pre-trained language model with examples from the Super-Natural Instructions dataset and asking it to generate additional examples. The resulting dataset contains 64,200 examples, with a total of 240,000 examples when considering instruction paraphrases. The dataset is analyzed for creativity, diversity, and correctness, and it is found that more than 50% of the examples are correct. The dataset is used to fine-tune a 11 billion-parameter T5 model, which outperforms both T0++ and Tk-instruct across multiple benchmarks. The dataset highlights the ability of language models to produce creative, diverse, and correct data, and it is faster and cheaper than human annotations.</sample>
    <sample id="111">The authors select a trigger set by collecting a general text corpus and counting the word frequency with it. They assume the provider can do this.</sample>
    <sample id="112">Hallo alle, mein Name ist Shuheng. Heute präsentiere ich unsere Arbeit "Do CoNLL-2003 named-entity taggers still work well in 2013? Lass uns loslegen. Unsere Arbeit untersuchte das Problem der Generalisierung im Rahmen der Named Entity Recognition Aufgabe oder NER Aufgabe. Wir beobachten, dass Modelle seit CoNLL-2003 für die Entwicklung von NER verwendet werden und dies stellt natürlich mehrere Probleme dar. Zunächst kann diese Modelle auf moderne Daten generalisieren? Und wenn wir neue Tagger entwickeln, was ist für eine gute Generalisierung notwendig? Gleichzeitig, wenn wir tatsächlich eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir das CoNLL++ Dataset entwickelt. Dies ist ein Datensatz, den wir aus Reuters News aus dem Jahr 2020 gesammelt haben und dann mit den gleichen CoNLL-2003-Anmerkungsrichtlinien annotiert haben. Wir haben dann über 20 Modelle auf CoNLL-2003 feinabgestimmt und sie auf sowohl den CoNLL-03 Testsets als auch das CoNLL++ ausgewertet. Zum Schluss haben wir den F1-Verlust berechnet, um die Generalisierung jeder Modelle zu bewerten. Also, was ist für eine gute Generalisierungsfähigkeit notwendig? Durch unsere Experimente fanden wir heraus, dass es drei Hauptzutaten gibt, die benötigt werden. Die erste ist die Modellarchitektur. Unsere Experimente zeigten, dass Transformer-Modelle normalerweise besser generalisieren. Die zweite Zutat ist die Modellgröße. Wir fanden heraus, dass üblicherweise größere Modelle zu besserer Generalisierung führen. Und letztendlich wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele direkt die Leistung einer nachfolgenden Aufgabe beeinflusst. Hier fanden wir auch heraus, dass mehr Feinabstimmungsbeispiele tatsächlich zu besserer Generalisierung führt. Zu unserer nächsten Frage: Was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist die adaptives Überfitten, das heißt, Überfitten durch die Wiederholung des gleichen Testsets wiederholt und dies wird normalerweise als die Abnahme der Ergebnisse auf einem neuen Testset dargestellt. Die zweite Hypothese ist die zeitliche Drift, die durch den zunehmenden zeitlichen Abstand zwischen dem Trainings- und dem Testdatensatz verursacht wird. Für Datenüberfitten sahen wir, dass die rote beste Anpassungslinie auf der rechten Grafik einen Gradienten hat, der größer als eins ist. Das bedeutet, dass jede Einheit der Verbesserung, die wir auf CoNLL-2003 gemacht haben, auf CoNLL++ zu mehr als einer Einheit verbessert wird, was bedeutet, dass keine Abnahme der Ergebnisse beobachtet wird. Und das zeigt uns, dass adaptives Überfitten in diesem Fall nicht beobachtet wird. Also was ist dann mit der zeitlichen Drift? Wir haben eine Experimente durchgeführt, um einige Modelle mit neueren Daten zu weiter trainieren oder vorzubereiten und fanden heraus, dass die Leistung mit einem größeren zeitlichen Abstand abnimmt, was unsere Hypothese bestätigt, dass der Hauptgrund für den Leistungsabfall die zeitliche Drift ist. Unsere Schlussfolgerung ist, dass für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiel benötigt werden. Und diese gehen Hand in Hand, wir können nicht nur ein Zutat auswerfen und die anderen wegwerfen. Gleichzeitig fanden wir auch heraus, das der Leistungsabfall hier durch die zeitliche Drift verursacht wird und es ist etwas überraschend, dass es nicht durch adaptives Überfitten verursacht wird, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird. Also geht es zurück zu der Frage, die wir in der Titel unserer Arbeit gestellt haben: "Do CoNLL-200</sample>
    <sample id="114">Title: "Grouped Head Attention: A Novel Approach to Redundancy Optimization in Large Language Models"

Abstract:
Large language models (LLMs) have revolutionized natural language processing by learning all tasks in a single model. However, they suffer from heavy parameters, long training times, and token-hungry requirements. This paper introduces Grouped Head Attention (GHA), a novel approach to address the heavy parameter problem in LLMs. GHA uses a divide and conquer strategy to compress multi-headed attention, achieving significant parameter compression without sacrificing performance. The model consists of two stages: group-constrained training and the Voting-to-Stay algorithm. In the first stage, attention heads are divided into groups to make intra-group heads more similar and inter-group heads more separate. In the second stage, redundant heads are pruned, leaving only one head per group. GHA achieves up to 90% parameter compression, with comparable performance on machine translation, abstract summarization, and language modeling tasks. Our LITE model further improves efficiency, achieving 90% parameter pruning, 62% faster inference speed,</sample>
    <sample id="115">Bei dem Ansatz werden Sprachsegmente von Lambda-Sprachrahmen verwendet, um die Translationsqualität und Latenz zu optimieren.</sample>
    <sample id="116">Im Beispiel mit Servin und Kea wird das entitätsspezifische Wissen benötigt, dass "Servin ein Richter ist."</sample>
    <sample id="117">Der wichtigste Faktor zwischen Qualität des Beispiels und Ähnlichkeit mit dem Ausgangssatzes ist die Qualität der Beispiele.</sample>
    <sample id="118">Code-switching, the mixing of two or more languages within a single sentence, is a common phenomenon in linguistically diverse communities like India. However, multilingual pre-trained models like mBERT and XLM- R struggle with code-switching tasks such as question answering and sentiment analysis. To address this, we propose SwitchMLM, a novel MLM technique tuned to code-switching. SwitchMLM introduces switch-points, which are groups of two tokens indicating language transitions. In standard MLM, all words are maskable, but in SwitchMLM, only switch-points are maskable. We also propose FrequencyMLM, a surrogate method that uses negative log likelihood to assign LID tags. Additionally, we introduce residual connections and an auxiliary LID-based loss to enhance switch-point information in intermediate layers. Our results show that SwitchMLM outperforms standard MLM and achieves the best performance on sentiment analysis tasks across all language pairs. Probing experiments confirm that our methods increase switch-point information in intermediate and final layers. In summary, SwitchMLM is a promising approach for improving pre-training techniques for code-switched NLP.</sample>
    <sample id="119">Die Arbeiten in den erweitertenen Experimenten konzentrieren sich auf Sprachmodelle wie GPT-4, RoBERTa und BART sowie deren Varianten.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus mehreren Ebenen. Es kombiniert die Werte des Cross-Attention-Mechanismus zwischen dem Audio-Input und dem Textual-Output, um Entscheidungen darüber zu treffen, ob ein Wort ausgegeben werden soll oder nicht.</sample>
    <sample id="121">Beispiele für direkte Inferenz sind "Easy on Me" oder "I Gotta Feeling".</sample>
    <sample id="122">Die Autoren gehören der Fudan-Universität an.</sample>
    <sample id="123">Title: MultiInstruct: Advancing Multi-Modal Zero-Shot Learning via Instruction Fine-Tuning

Abstract:

Recent advancements in large language models have spurred interest in parameter and data-efficient learning paradigms. Instruction tuning has emerged as a promising approach to enable models to perform on unseen tasks in a zero-shoot manner. However, most previous works have focused on language-only tasks, leaving computer vision and multi-modal tasks unexplored. This work investigates the potential of instruction tuning for improving generalization to unseen multi-modal tasks. We introduce MultiInstruct, the first multi-modal instruction fine-tuning benchmark dataset, consisting of 62 diverse tasks across 10 categories. These tasks are derived from 22 open-source datasets, each equipped with five expert-written instructions. We use OFA, a unified multi-modal pre-trained language model, as our base model and evaluate its performance on MultiInstruct. Our results show that instruction tuning significantly improves OFA's performance on seen multi-modal tasks and reduces sensitivity. Transfer learning from natural instruction datasets further enhances performance and sensitivity. We propose a new metric, sensitivity, to measure the model's ability to consistently produce the same output for the same task. We also collect a larger multi-modal instruction fine-tuning dataset and release it for further research.</sample>
    <sample id="124">This study presents "Towards Benchmarking and Improving the Temporal Reasonability of Large Language Models" by Tan Qingyu from the National University of Singapore. The research focuses on temporal reasoning, categorized into three levels: time-to-time, time-to-event, and event-to-event. The authors conducted preliminary experiments on year and month predictions, revealing biases in existing models. They propose the TempReason dataset, covering all three reasoning levels and long temporal coverage, and evaluate temporal reasoning in three QA settings: Closed Book, Open Book, and Reasoning QA. To enhance temporal reasoning, they introduce a training strategy with temporal span extraction pre-training and time-sensitive reinforcement learning. The final model, TempT5, outperforms existing models in temporal reasoning tasks. The study highlights the need for addressing temporal reasoning biases and proposes future work to overcome these challenges.</sample>
    <sample id="125">Based on the provided text, it is not explicitly mentioned how many authors are involved in the work. However, the text does mention "our works," which suggests that there are multiple authors. To provide a more accurate answer, I would need additional information or context about the specific work being referred to.</sample>
    <sample id="126">Yes, the translation of natural language queries using a machine translation model was considered as a baseline before semantic parsing. The baseline involved using Google Translate API to translate the source query into the target language, then using a monolingual model to train and evaluate. This was one of the six settings for training and evaluation in the XSemPLR benchmark.</sample>
    <sample id="127">Title: Large Language Models as Reasoning Teachers: A Novel Approach to Transferring Reasoning Abilities to Smaller Models

Abstract:

We introduce a novel approach to transfer reasoning abilities from large language models to smaller models using chain-of-thought reasoning as a teaching technique. Our method, called fine-tuned CoT, involves using large models to generate step-by-step solutions for tasks, which are then used as training data to fine-tune smaller models. We also propose a technique called diverse reasoning, which involves generating multiple solutions using stochastic temperature sampling to improve the performance of the student model. Our results show that our method can achieve notable performance in many tasks and significantly outperforms vanilla fine-tuning on most tasks, including text-based ones. We also demonstrate that our method is highly scalable and can be further improved by using more datasets, a better teacher model, or a bigger student size. Our approach is accessible and effective, and it may be possible for other emergent abilities in the field of AI.</sample>
    <sample id="128">Title: The KITMUS Test: Evaluating Knowledge Integrated from Multiple Sources

Abstract:
Natural language understanding models rely on both pretraining knowledge and inference-time information. However, integrating knowledge from multiple sources remains a challenge. We introduce the KITMUS test suite, focusing on coreference resolution to evaluate this integration. The test includes three settings: Background-Pretrain, Background-Both, and Background-Inference, simulating varying availability of background and entity-specific knowledge. Evaluating with human participants and models, we find that without task-specific training, models struggle. Training on KITMUS improves performance, but integrating inference-time knowledge remains difficult. Our results highlight the need for task-specific training to effectively integrate knowledge from multiple sources.</sample>
    <sample id="129">Die Autoren haben für die markierte Gruppe "Asiatische Frauen" die folgenden Beispiele gegeben: "klein", "schlank" und "silky".</sample>
    <sample id="130">Based on the content of the paper, the following model architectures do not generalize well:

1. Non-transformer models: The paper found that transformer models generally generalize better to new data, implying that non-transformer models may not generalize as well.
2. Smaller models: The paper found that larger models tend to generalize better, so smaller models may not generalize as well.

It's worth noting that the paper does not explicitly state which specific non-transformer models do not generalize well, but based on the context, it can be inferred that models such as traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs) may not generalize as well as transformer models.</sample>
    <sample id="131">Die Testdatensätze werden als "clean" bezeichnet.</sample>
    <sample id="132">Zwei Autoren sind an der Arbeit beteilt: Akshatha und Martin.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, insbesondere mit Text und Bildern.</sample>
    <sample id="135">ABC-Eval is a new dimensional approach to evaluating conversational AI, developed by the Emory NLP Lab and Amazon Alexa AI. It aims to provide a more precise and reliable evaluation of chat models by explicitly annotating behaviors such as irrelevant responses, contradictions, and hallucinations. ABC-Eval measures the rates at which chat models commit various thematic errors and has been shown to be more reliable and informative than existing methods. The approach covers multiple dimensions of chat quality and has been tested on four state-of-the-art chat models. The results show that ABC-Eval behavior labels are overall more predictive of conversation quality and capture unique aspects of chat quality. The approach has the potential to advance the field of conversational AI by providing a higher resolution evaluation of chat models.</sample>
    <sample id="136">The work presented by Jasivan and Nafise at the University of Sheffield introduces FERMAT, a flexible evaluation set for numerical reasoning tasks. The motivation behind this work is to address the limitations of current benchmarks, which often rely on accuracy scores and F1 measures that do not provide insights into the mathematical abilities of language models. FERMAT consists of math worded questions extracted from Illinois and CommonCore, with numbers represented in various formats and mathematical operations tested. The evaluation set also includes training dependency analysis, which reveals that language models do not necessarily memorize exact expressions but rather rely on linguistic cues. The study finds that language and mathematical diversity is important for improving performance, and that FERMAT provides a more informative alternative to current benchmarks. The authors conclude that number encoding and tokenization are areas for improvement.</sample>
    <sample id="137">"Tell2Design: A Dataset for Language-Guidance in Floor Plan Generation" presents a novel task of generating floor plans directly from natural language instructions. The dataset, Tell2Design, consists of 5,051 human-annotiated language instructions and 76,000 artificially generated instructions, each associated with a floor plan. The task involves generating a structured interior layout that aligns with the input instructions, which include semantics, geometry, and topology. The main challenges are performing design generation under stricter constraints, understanding the big picture of the entire floor plan, and dealing with ambiguous, incomplete, or misleading information in human instruction. The proposed sequence-to-sequence model, initialized by a pre-defined or pre-trained language model, outperforms other text-conditional image generation baselines by achieving the highest IoU scores. The paper concludes that the proposed method is a strong baseline for the task of language-guided design generation and hopes to serve as a foundation for future research.</sample>
    <sample id="138">Nach Ansicht der Autoren ist das zu wenig erforschte Gebiet im Bereich der natürlichen Sprachverarbeitung (NLU) die Fähigkeit von Modellen, effektiv Wissen aus verschiedenen Quellen zu integrieren, insbesondere wenn dieses Wissen nur in der Inferenzzeit verfügbar ist. Dies betrifft die Integration von "backward knowledge" (Wissen, das nur zur Inferenzzeit zur Verfügung steht), was die meisten der besten performierenden Modelle schwierig macht.</sample>
    <sample id="139">Die Referenten heißen Ying und Zhiyang.</sample>
    <sample id="140">Yes, CoScript has undergone quality control. Crowd-sourced workers were asked to find and revise incorrect samples to ensure the quality of the validation and test set.</sample>
    <sample id="141">Die Grenzen bestehender Ressourzen für kontextbasierte Übersetzungen liegen in der begrenzten Verfügbarkeit von Datensätzen, die spezifische Kontextabhängigkeiten abdecken und nur auf begrenzte Sprachpaare und Domänen beschränkt sind. Diese Ressourcen erfordern manuelles Kuratieren und sind oft auf bestimmte Domänen und Sprachen beschränkt, was die Allgemeinanwendbarkeit einschränkt.</sample>
    <sample id="142">Hallo! Ich spreche über unsere Arbeit an "Die Auflösung von indirekten Bezugspunkten für die Entitätauswahl", in der wir die AltEntities-Korpus einführen. Mein Name ist Javad Hosseini und dies ist eine gemeinsame Arbeit mit Filip Radlinski, Silvia Paret, und Annie Louis. Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten.

Stellen Sie sich diese alternative Frage vor: "Hast du 'Easy on Me' oder 'I Gotta Feeling' gemeint?" Hier möchte ein Benutzer zwischen diesen beiden Liedern wählen. Der offensichtlichste Weg wäre, eine direkte Referenz zu verwenden, zum Beispiel durch den Namen des Liedes "Easy on Me" oder seine Position, "das erste". Aber manchmal ist es besser, eine indirekte Referenz zu verwenden. Dies könnte passieren, wenn der Benutzer den Namen des Liedes nicht mehr erinnert oder wenn die Aussprachen zu sehr ähnlich sind und schwer zu unterscheiden sind. Oder wenn der Benutzer eine Präferenz ausdrücken möchte. Hier sind einige Beispiele für indirekte Bezugspunkte: "das neuere", "das Lied, das nicht energisch ist".

Dies ist ein wichtiges Problem in Dialogsystemen und auch für die Benchmarking von LLMs' Entitätverständnis. Wir sind uns nicht bewusst, dass es ein größeres öffentliches Datenset für diese Aufgabe gibt, also sammeln wir eines mit der Crowd-Annotation. Unser Datensatz deckt drei verschiedene Bereiche ab: Musik, Bücher und Rezepte. Unsere Datensammlungsmethode betont informellheit mit einer Cartoon-Kompletionsetup. Die Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: "Den Song, den wir gestern gehört haben?" Und damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: "Hast du 'Easy on Me oder 'I Gotta Feeling' geme</sample>
    <sample id="143">Der Ansatz wird mit den bestehenden SimulST-Richtlines "Wait-k" und "Local Agreement" verglichen.</sample>
    <sample id="144">Die Autoren gehören der Universität Nantes an.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Dialogue summarization, a subtask of text summarization, involves creating concise summaries of dialogues. Despite progress using large-scale pretrained language models, these summaries often contain factual errors and omissions, leading to incomplete summaries. This study analyzes the omission problem, finding that even state-of-the-art models have a high omission rate of about 70%. The dataset, OLDS, provides high-quality omission labels for dialogue summarization, covering five domains and using diverse abstractive models. The study explores three frameworks for omission detection and finds that the task is challenging, with an F1-score around 50%. The research also investigates using omissions to refine summaries, showing improved performance when omission content is provided. The OLDS dataset and findings are publicly available, offering a foundation for further research in dialogue summarization.</sample>
    <sample id="147">Drei Autoren sind an der Arbeit beteilt: Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trento und der Foundazione Bruno Kessler und ich werde kurz die Arbeit "Attention as a Guide for Simultaneous Speech Transcription" vorstellen, die ich gemeinsam mit Matteo Negri und Marco Turchi geschrieben habe. Was ist Simultaneous Speech Transcription? Simultaneous Speech Transcription, oder SimulST, ist der Prozess der Übersetzung von gesprochenen Sprachen in Text in Echtzeit, die es ermöglicht, sprachübergreifend zu kommunizieren. Und welche Probleme gibt es bei den aktuellen SimulST-Modellen? Spezifische Architekturen werden normalerweise trainiert, wobei zusätzliche Module optimiert werden. Lange und komplizierte Trainingsverfahren, zum Beispiel Trainingsverfahren mit verschiedenen Optimierungszielen. Und das Trainieren und Aufrechterhalten mehrerer Modelle, um verschiedene Latenzbereiche zu erreichen. Zum Beispiel das Trainieren eines Modells mit einem Durchschnitt von einer Sekunde Latenz und einem anderen Modell mit einer Latenz von zwei Sekunden und so weiter. Also was ist unsere Lösung? Zunächst, die Verwendung bereits bestehender offline ST-Modelle ohne Neuausbau oder Anpassung spezifischer Architekturen für SimulST zu verwenden. Verwenden wir nur ein Modell für jeden Latenzbereich und behandeln Latenz durch spezifische Parameter. Und nutzen wir das Wissen, das bereits durch das Modell durch die Aufmerksamkeit zwischen Audio-Eingabe und Textual-Output erworben wurde. Das ist die Cross-Attention-Methode, und Sie können ein Beispiel auf der rechten Seite sehen. Unsere Lösung ist, die EDAtt oder Encoder-Decoder-Aufmerksamkeit vorzuschlagen, bei der wir entscheiden, ob ein Teil der Übersetzung ausgegeben werden soll, basierend auf, wo die Aufmerksamkeit zeigt. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d.h. seine Summe unter einem bestimmten Schwellenwert alpha zu den letzten lambda Sprachrahmen hinweist, was bedeutet, dass die erhaltene Information stabil genug ist. Zum Beispiel, wenn wir einen Sprachblock erhalten, der "Ich werde über..." enthält, und unser Modell die Übersetzung in Deutsch vorhersagt, und wir uns die Cross-Attentiongewichte ansehen, sehen wir, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen hinweisen, während das letzte Wort auf die letzten lambda Sprachrahmen hinzukommt, also lambda Sprachrahmen. Das bedeutet, dass die ersten beiden Wörtern ausgegeben werden, während wir die letzte Wort nicht ausgeben und auf einen weiteren Sprachblock warten, da die Summe der Cross-Attention über einen bestimmten Schwellenwert alpha hinausgeht. Wenn wir weitergehen und einen weiteren Sprachblock erhalten, und unser Modell weitere drei Wörter vorhersagt, und wir die Cross-Attentiongewichte anzeigen, sehen wir, dass kein Wort auf die letzten lambda Sprachen hinweist. Das bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir die Hauptergebnisse von EDAtt sehen, werden wir die simultane Sprachtranskription-Ergebnisse auf Graphen darstellen, auf denen BLEU auf einer Seite gemessen wird, die die Übersetzungsqualität misst, und der durchschnittliche Verzögerung, die die Latenz misst, und wir berücksichtigen auch die rechenaufwändige durchschnittliche Verzögerungen, die die Rechenzeiten des Modells zur Vorhersage des Ausgabes berücksichtigen. Also wollen wir unsere Kurven so hoch wie möglich auf diesem Diagramm sein. Aber wir wollen auch, dass sie nach links verschoben sind. Und wir vergleichen mit beliebten Strategien, die auch auf offline Modellen angewendet werden, nämlich der Warte-k-Strategie und der Lokalen Einigung. Und wir vergleichen auch mit dem state-of-the-art-Architektur, die speziell für simultane Vor-Übersetzung entwickelt wurde. Dies sind alle Ergebnisse der simultanen Sprachtranskription-Strategie auf Deutsch. Und wir sehen, dass es sich übertrifft, alle Strategien, die auf offline Modellen angewendet werden. Die Kurven sind verschoben, und wir sehen auch, dass wenn wir die tatsächliche vergangene Zeit oder die rechenaufwändige Zeit, d.h. die schnellste Strategie, berücksichtigen, dass es das schnellste ist. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unsere Arbeit. Und wir haben auch den Code und die Modelle sowie die simultane Ausgabe geöffnet, um die W</sample>
    <sample id="149">Yes, the CoNLL++ dataset is publicly accessible.</sample>
    <sample id="150">MeetingQA is a new dataset for extractive question answering on meeting transcripts, which is an underutilized domain in NLP research. The dataset contains 7.7K questions and answers from nearly 100 hours of manually transcribed meetings. The questions are long, open-ended, and often seek discussions from multiple speakers. The answers can be multi-span, multi-speaker, and even rhetorical. The dataset is challenging for existing QA models in both fine-tuning and zero-shot settings, with a significant gap between human performance and model performance. The paper presents a variety of methods for addressing this gap, including context-retrieval, single-span and multi-span models, and data augmentation. The results show that multi-span models have slightly less or comparable accuracy to single-span models, and that silver data augmentation effectively improves zero-shot performance, with larger instruction-tuned models such as FLAN-T5 achieving comparable results to the remaining models. Error analysis shows that models struggle with rhetorical questions, identifying which speaker answers a question, and identifying irrelevant sentences in single-span models.</sample>
    <sample id="151">Hallo alle, mein Name ist Ying und mein Kollege Zhiyang werden wir uns heute mit unserer Forschung über MultiInstruct verbringen, die die Verbesserung des Multi-Modal Zero-Shot Learning durch Anweisungstuning fördert. Mit den Fortschritten in großen Sprachmodellen haben viele Arbeiten neue Lernparadigmen untersucht, um vorkommende Sprachmodelle für verschiedene Downstream-Aufgaben in einer parametrierungs- und dateneffizienten Weise zu nutzen. Kürzlich zeigten viele Studien, dass Anweisungstuning es großen Sprachmodellen ermöglicht, in einer Null-Shot-Methode auf ungesehene Aufgaben mit natürlichen Anweisungen zu agieren. Allerdings konzentrierten sich viele frühere Arbeiten auf die Verbesserung der Null-Shot-Performance für sprachbasierte Aufgaben, während computervisuelle und multi-modal Aufgaben ausgelassen wurden. Daher möchten wir in dieser Arbeit untersuchen, ob Anweisungstuning eines multi-modalen vorkommenden Sprachmodells tatsächlich die Generalisierung zu ungesehenen multi-modalen Aufgaben verbessern kann. Zudem entdeckten wir bei der Zeit unserer Forschung eine erhebliche Diskrepanz in der Verfügbarkeit von Anweisungsdatensätzen zwischen NLP und multi-modal. Es gibt mehr als 1600 sprachbasierte Anweisungsaufgaben, aber es gibt keine großskalige öffentlich zugängliche multi-modal Anweisungsaufgaben. Dies motiviert uns, ein multi-modales Anweisungstuning-Datensatz aufzubauen. Hier präsentieren wir MultiInstruct, das erste multi-modales Anweisungstuning-Benchmark-Datensatz, der aus 62 vielfältigen multi-modalen Aufgaben besteht, die 10 breite Kategorien abdecken. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen und jede Aufgabe ist mit fünf von Experten geschriebenen Anweisungen ausgestattet. Um die multi-modal Anweisungstuning auf unserem vorgeschlagenen Datensatz zu untersuchen, nehmen wir das OFA, ein vereinigtes multi-modales vorkommendes Sprachmodell, als unsere Basismodell. OFA verwendet einen vereinigten Satzzeichenraum für Sprach-, Bildtoken und die Koordinaten eines Bounding-Boxes. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct-Datensatz, um die Verarbeitung verschiedener Eingabe- und Ausgabedatenarten zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einer einheitlichen Sequenz-zu-Sequenz-Format. In dem Input-Text, Bild, Anweisungen und Bounding-Box werden in derselben Token-Raum dargestellt.

Jetzt werde ich über multi-modal Anweisungstuning sprechen. Für das Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und stellen pro Aufgabe 10.000 Instanzen zur Verfügung. Für die Testphase reservieren wir die gesamte Gruppe des gemeinsamen Vernunftigkeitsdenkens für die Testphase und wählen zusätzlich 5 Aufgaben aus den Gruppen VQ und Verschiedenes. Wir verwenden alle Instanzen in der Testspalte für jede Aufgabe. Zusätzlich wählen wir zufällig 20 Aufgaben aus der Testspalte der natürlichen Anweisungen als ein unbekanntes Aufgaben für NLP. Wir verwenden das vorgezogene OFA große Modell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer von fünf Anweisungstemplates kombiniert. Während des Tests führen wir für jede Aufgabe insgesamt fünf Experimente durch, indem wir den Modell mit einer von fünf Anweisungen bewerten. In jedem Experiment</sample>
    <sample id="152">Title: Exploring Large Language Models for Classical Philology

Abstract:
This presentation introduces the development of new language models tailored for classical philology, focusing on Ancient Greek and Latin. We discuss the limitations of existing monolingual models and the need for multilingual capabilities. Our project aims to create comparable models, push the state-of-the-art, explore different architectures, and introduce multilingual models. We pre-train two monolingual models, GreBERTa and GreTa, and two multilingual models, PhilBERTa and PhilTa, using a high-quality pre-training corpus from the Internet Archive. We benchmark our models on part-of-speech tagging, dependency parsing, lemmatization, and semantic and world knowledge tasks. Our results show significant improvements over existing models, with encoder-decoder models excelling in lemmatization. While multilingual models do not show a significant advantage over monolingual models, our models demonstrate the potential of large language models in classical philology.</sample>
    <sample id="153">Title: Resolving Ambiguities in Text-to-Image Models

Abstract:
This work investigates ambiguities in prompts provided to text-to-image generative models, such as "The girl enters the room with flowers," which can have multiple interpretations. We curate a benchmark dataset, LAVA, covering various ambiguity types and propose frameworks to disambiguate prompts and evaluate the faithfulness of generated images to user intention. Our pipeline includes a prompt disambiguation framework using in-context learning to generate clarifying questions or visual setups, followed by automatic evaluation using a VQA model to assess image fidelity. Our findings show that disambiguation improves faithful generation, and our evaluation framework aligns with human judgment. This research contributes to the development of more reliable text-to-image models.</sample>
    <sample id="154">Die Autoren gehören der Universität Trento und der Foundazione Bruno Kessler an.</sample>
    <sample id="155">Der/die Referent*in heißt AltEntities Corpus.</sample>
    <sample id="157">Dialogue summarization aims to distill the salient information from a dialogue context into a brief summary. Existing methods rely on pre-computed static graph structures, which are dependent on external linguistic tools and cannot adapt dynamically to the summarization task. Our proposed SDDS model addresses these limitations by using an Utterance Encoder to encode dialogue utterances, a Static-Dynamic Graph module to capture semantic relationships, and a pre-trained language model to fuse static and dynamic structures. The model employs heuristic methods like Discourse Parsing Graph and speaker interaction frequency matrix to model static dialogue structure, and a multi-head attention model for dynamic graph construction. The final summary is generated by integrating static and dynamic graph representations using a dual cross-attention mechanism. The code and data are available on GitHub.</sample>
    <sample id="158">Coreference resolution is the task of identifying and clustering mentions of the same entity in a document. Conventional methods have quadratic complexity, while cache-based methods reduce complexity to linear levels. However, in long documents, the LRU policy used in cache-based methods can lead to high cache misses. Our proposed dual cache, consisting of a local cache with LRU eviction and a global cache with LFU eviction, addresses this issue by storing local and global entities separately. We evaluated dual cache on four public benchmarks and found that it outperforms single cache methods and reduces cache misses. Dual cache also has the highest performance/cost ratio, making it the most cost-effective cache-based method.</sample>
    <sample id="159">Hallo zusammen, ich bin Koustav Sinha und freue mich, Sie zu unserem Vortrag über unser ACL 2023-Papier begrüßen zu dürfen. Die Akzeptabilitätsurteile von Sprachmodellen sind nicht immer robust gegenüber Kontext. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kishka Misra, Karen Fences, Roger Lévy und Adina Williams. In dieser Arbeit untersuchen wir die minimalen Paar-Paradigmen erneut.

Die minimalen Paar-Paradigmen bewerten Sprachmodelle auf der Basis von Akzeptabilitätsurteilen, die auch grammatische Korrektheit wie bei BLiMP, SyntaxGym oder Akzeptabilität in Bezug auf Stereotypen wie CrowS-Paare umfassen. In diesem Kontext wird normalerweise versucht, die Akzeptabilität eines akzeptablen oder ungrammatischen Satzes zu bewerten, indem man ihn anhand eines akzeptablen oder unakzeptablen Satzes vergleicht.

Die aktuelle MPP-Pipeline erlaubt es nicht, die Akzeptabilität von Modellen über längere Sätze hinweg zu bewerten. Inzwischen sind große Sprachmodelle mit längeren Kontextfenstern aufgetaucht. Es ist entscheidend, dass wir die Akzeptabilität von Modollen über den Kontextfenster hinweg bewerten. Dies ist das Ziel unserer Arbeit. Wir versuchen, die MPP-Pipeline zu überarbeiten, indem wir das Modell dazu ermutigen, Akzeptabilität auf längere und längere Sätze zu bewerten.

Um längere Sätze zu simulieren, verwenden wir die Daten selbst und erstellen Sätze, indem wir akzeptable oder unakzeptable Sätze aus diesen Daten auswählen. Zum Beispiel haben wir ein typisches Paar von grammatikalischen Fehlern aus dem BLiMP-Datensatz aus dem Adjunct Island Fall ausgewählt. Wir extrahieren grammatische Sätze aus Adjunct Island und fügen sie als Präfix zu akzeptablen und unakzeptablen Abfragen hinzu. Wir können das gleiche mit unakzeptablen Sätzen aus dem gleichen passenden Phänomen tun. Wir können auch Sätze aus einem anderen Datensatz oder einem anderen Bereich wählen, um die Akzeptabilität der Modelle zu testen.

Wie funktioniert das Modell? Zunächst untersuchen wir die Akzeptabilität von Wikipedia-Sätzen, die völlig unabhängig von der aktuellen Abfragepaar sind. Wir finden, dass die MPP-Urteile für beliebige Kontextlänge relativ stabil sind. Wir erhöhen die Kontextlänge auf 1024, um OPT und GPT-2-Modelle zu maximieren. Wir sehen hier, dass die MPP-Urteile in der orange gedämpften Linie relativ stabil sind.

Was passiert, wenn wir Sätze aus dem gleichen Datensatz auswählen? Hier wählen wir akzeptable und unakzeptable Sätze aus akzeptablen und unakze</sample>
    <sample id="160">In the first step der Methode werden die Input-Token mit einer ungeordneten Multiset von Tokens versehen, die in der Ausgabe vorkommen werden.</sample>
    <sample id="161">In CoScript, there are a total of 55,000 specific goals, each with its own script.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEPLAIN ist die MASSalign-Methode.</sample>
    <sample id="164">Schwach überwachtes Lernen (WSL) ermöglicht es, Modelle zu trainieren, ohne manuelle Labels zu benötigen. Stattdessen werden Daten mit schwachen Labelingquellen wie einfachen Regeln, Wissenbanken oder niedrigqualitativen Crowdsourcing-Anfragen beschriftet. Diese schwachen Labels sind kostengünstiger als manuelle Annotationen, sind jedoch auch raucher, was bedeutet, dass einige Labels falsch sind. Wenn man direkt mit neuralen Netzen auf schwach gelabelte Daten trainiert, neigen die Netze dazu, die Label-Rausch zu merken und nicht zu generalisieren. In WSL werden Trainingsalgorithmen entwickelt, um neuronale Netze robust unter solchen Label-Rausch zu trainieren, sodass die trainierten Modelle gut generalisieren.</sample>
    <sample id="165">Title: Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations

Abstract:

We introduce LiPoR, an unsupervised learning method for abductive reasoning that leverages mutual exclusivity among explanations. In abductive reasoning, the goal is to identify a plausible subset of explanations that bridge the information gap between a given context and outcome. Traditional supervised methods require annotated plausible explanations, which can be noisy and subjective, as evidenced by a recent experiment where crowd workers disagreed on 60% of over 1000 explanations. LiPoR addresses this challenge by treating explanations as latent variables and maximizing the marginal likelihood of the outcome given the context. To prefer plausible explanations, we introduce a regularizer based on the mutual exclusivity of explanations. Our method outperforms zero-shot models and the previous best unsupervised approach on AlphaNLI, the most widely-used dataset for abductive reasoning, by over 4 absolute points in accuracy, demonstrating its effectiveness in unsupervised learning of abductive reasoning.</sample>
    <sample id="166">Yunxin from Harbin Institute of Technology, Shenzhen, presents a novel approach to image retrieval from linguistically complex text, leveraging a Neural Divide-and-Conquer Reasoning Framework (NDCR). This framework addresses the challenge of high similarity between images and lengthy, complex descriptions by integrating the strengths of visual language models (System 1) and logical reasoning (System 2). Inspired by the Divide-and-Conquer strategy and Dual Process Theory, NDCR decomposes complex propositions into simpler ones using a Proposition Generator, which then interacts with visual information via the Visual-Linguistic Interactor. The Neural-Symbolic Reasoner, acting as System 2, integrates reasoning states to derive final solutions. Experimental results show NDCR outperforms baselines, with ablation studies validating module effectiveness. Two cases demonstrate the method's ability to present inference states and results, highlighting its processing interoperability. The study suggests neural symbolic calculation and Divide-and-Conquer as effective strategies for complex reasoning.</sample>
    <sample id="167">The documents in DEPLAIN-web were aligned using both manual and automatic methods. Specifically, 750 documents were aligned manually, and the remaining documents were aligned using automatic methods. This resulted in a total of 30,450 parallel sentence pairs.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde erstellt, indem wir Daten aus Reuters News von 2020 sammelten und dann mit den gleichen CoNLL-2003-Anweisungen annotierten.</sample>
    <sample id="169">Title: Prompting PaLM for Translation: Assessing Strategies

Abstract:
This paper presents a systematic study of large language model prompting for machine Translation (MT), focusing on the PaLM model, a 540 billion-parameter language model trained on 780 billion tokens. We evaluate the transition capability of PaLM using best practices of the MT community, including the latest test sets and state-of-the-art neural MT metrics. Our results show that the quality of examples is more important than the similarity to the source language in prompting strategies. We found that a 5-shot prompting strategy, where each sentence is marked with its language, yields nearly no difference in performance. PaLM's fluency is comparable to state-of-the-art systems, but it struggles with accuracy, particularly in omission errors. We also observed that PaLM's "Style/Awkward" category is lower than for state-of-the-art systems, indicating fluent output with some accuracy issues. Our findings provide insights into prompt selection strategies for large language models in MT.</sample>
    <sample id="170">Hallo alle, mein Name ist Yusen Zhang von der Penn State University. Heute präsentiere ich unsere Arbeit "XSemPLR: Cross-Linguel Semantische Parsing in mehreren natürlichen Sprachen und Bedeutungsdarstellungen". Semantische Parsing ist eine Aufgabe, um semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda Calculus zu erstellen. Cross-Linguel Semantische Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen zu übersetzen. Wie in diesem Bild gezeigt, müssen wir Anfragen in mehreren natrlichen Sprachen mit neuronalen Modellen in SQL, Lambda oder FunQL usw. übersetzen. Existierende cross-linguel semantische Parsing Modelle werden separat vorgeschlagen und auf Datenmengen für begrenzte Aufgaben und Anwendungen evaluiert. Zum Beispiel gibt es viel Abdeckung für bestimmte natürliche Sprachen. Aber Chinesisch fehlt und es fehlt an Abdeckung für bestimmte Bedeutungsdarstellungen. Lambda-Rechenkalkül fehlt oder sie werden nur auf bestimmte neuronale Modelle ausgewertet. Zum Beispiel gibt es nur ein einziges Modell, um sie zu bewerten. Daher schlagen wir XSemPLR vor. Wir bieten eine einheitliche Datenmenge XSemPLR für cross-linguel semantische Parsing in mehreren naturlichen Sprachen und Bedeutungs darstellungen an. Es enthält 9 Datensätze in verschiedenen Bereichen, 5 semantische Parsing Aufgaben, 8 Bedeutungsdarstellungen und 22 natürlichen Sprachen in 15 Sprachfamilien. Um unsere Benchmark besser zu bewerten, berücksichtigen wir sechs Einstellungen für Training und Evaluierung. Die erste ist Translate-Test. Wir verwenden die Google Translate API, um die Queries in die Zielsprache zu übersetzen, und verwenden ein monolinguales Modell zur Training und Evaluierung. Zum Beispiel trainieren wir ein englisches Modell auf englischen Queries und während der Inferenz übersetzen wir deutsche Queries mit der API in englische und verwenden das trainierte Modell, um SQL vorherzusagen. Wir testen auch Monolinguale Modelle. In diesem Einstellung ist die Quelle- und Zielsprache gleich, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch Monolinguale Few-shot-Einstellung, indem wir monolinguale Modelle mit nur 10% des Trainingsdatensatzes trainieren. Und wir testen Multilinguale Modelle, bei denen wir ein multilinguales Modell für alle Sprachen trainieren. Zum Beispiel legen wir deutsche, englische und chinesische Queries zusammen, um ein multilinguales Modell zu trainieren, das SQL auszugeben. Während der Inferenz können wir dieses Modell deutsche Queries oder chinesische Queries übersetzen, et cetera. Und wir testen auch Cross-linguelle Zero-shot- und Few-shot-Transfer. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache. Während des Trainings trainieren wir es auf englischen Queries oder die Kombination von englischen und deutschen Few-shot-Queries, um ein multilinguales Modell zur Vorhersage von SQL-Ausgaben zu trainieren. Und wir finden auch viele interessante Ergebnisse. In Bezug auf die Analyse von monolingualen Modellen evaluieren wir zwei Gruppen von Modellen, einschließlich Encoder-PTR, das für Mehrsprachige vorgeschaltete Encoders mit Zeiger-basierten Decoder steht, wie XLM-R + PTR und mBERT + PTR. Und wir bewerten auch Encoder-Decoder-Modelle, die Mehrsprachige vorgeschalteten Encoder-Decoder-Modelle sind, wie mBART und mT5. Wir fanden heraus, dass Encoder-Decoder auf allen neun Datensätzen</sample>
    <sample id="171">Existing works on protecting the copyright of embedding as services can be broadly classified into four categories. However these methods either not applicable to embedding as services or lack transferability.</sample>
    <sample id="172">Based on the content of the presentation, it appears that more multilingual language models like Codex or Bloom are still inadequate for cross-lingual semantic parsings tasks. The results show that these models are not sufficient for the task, and more comprehensive benchmarks and improvements are needed.</sample>
    <sample id="174">ArgAnalysis35K is a large-scale dataset for argument quality analysis, featuring 35,000 argument-analysis pairs. It stands out due to its high-quality arguments sourced from expert debaters, speeches, and intermediate debaters, rather than crowdsourcing. The dataset covers 24 diverse themes, capturing a wide range of motions. It introduces the concept of "analysis," which combines claims, premises, and other elements to explain arguments coherently. The dataset also incorporates instance-based annotator reliability, allowing for more nuanced judgments based on individual annotator biases. Additionally, a relevance model assigns scores to arguments, capturing their relevance to specific themes. This unique combination of features results in a more diverse, reliable, and informative dataset for argument quality analysis.</sample>
    <sample id="175">Die Methode mit der Mehrdeutigheit der Permutationen umgeht dies, indem sie eine GPU-freundliche kontinuierliche Relaxierung verwendet, die auch die Backpropagation durch die Lösung ermöglicht und die linguistisch plausiblen Permutationen lernt.</sample>
    <sample id="176">Fairness in a downstream NLP model is defined as the absence of bias and discrimination based on political, social, or demographic factors. This means that the model should treat all users and groups equally, regardless of their political affiliation, race, gender, or other characteristics. Fairness also involves ensuring that the model's predictions and recommendations are accurate and reliable, and that it does not perpetuate harmful stereotypes or biases. In the context of political biases in language models, fairness would require that the model does not unfairly favor or discriminate against certain political groups or ideologies, and that it does not perpetuate harmful biases in downstream tasks such as hate speech detection and fake news detection.</sample>
    <sample id="177">Der/die Referent*in heißt Yanis Labrak.</sample>
    <sample id="178">Der/die Referent*in ist Koustav Sinha.</sample>
    <sample id="179">Title: Enhancing Theory of Mind in Large Language Models with SymbolicToM

Abstract:

This paper introduces SymbolicToM, an inference-time method designed to improve Theory of Mind (ToM) reasoning skills in large language models (LLMs). Traditional ToM tests, such as the Sally-Anne test, involve false-belief questions that assess a character's understanding of another's mental state. Despite advancements in LLMs, they still struggle with these tasks. SymbolicToM addresses this by using explicit graphical representations to model mental states and their interactions.

SymbolicToM constructs multiple belief graphs for all character combinations up to a predefined ToM level, leveraging off-the-shelf natural language inference (NLI) and OpenIE models. These graphs enable efficient question answering by detecting entities, retrieving relevant graphs, and recursively querying the graph to obtain factual answers.

Experiments demonstrate significant performance gains across various LLMs, including GPT-3-Davinci, Macaw, and Flan-T5-XXL, with accuracy improvements ranging from 51 to 67 points. SymbolicToM also shows robustness in out-of-domain scenarios, maintaining performance on datasets with modified story structures and linguistic diversity.

SymbolicToM offers a plug-and-play solution to enhance ToM reasoning in LLMs, avoiding overfitting and providing interpretable reasoning. It outperforms supervised approaches on out-of-domain story understanding and remains beneficial for new linguistic diversity datasets. For further details, refer to the paper.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">In this paper, we introduce the problem of constrained language planning, which involves planning for goals with specific constraints. We evaluate the constrained language planning ability of large language models and find that they achieve unsatisfactory results. We conduct a detailed analysis and find that the semantic completeness in generated scripts is acceptable, but the faithfulness to the constraints cannot be guaranteed due to high variance in the output quality of language models. To address this issue, we adopt the idea of over-generate then filter and develop a filter model to select faithful scripts. We also create a dataset of constrained language planning, named CoScript, by distilling constrained language planning datasets from large language models. Our method greatly improves the planning ability both semantically and in terms of faithfulness to the constraint. We hope that CoScript can be a valuable resource for advancing research on language planning.</sample>
    <sample id="182">Tropikalismus in der Arbeit bezieht sich auf die Verwendung von Wörtern wie "vibrant" und "curvaceous" zur Beschreibung von lateinamerikanischen Frauen, die an tropische Stereotypen erinnern. Diese Wörter verkörpern eine langjährige Diskriminierung und Andersartigkeit dieser Gruppen.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschrei bungen der Zielgruppen durch eine Studie erstellt, in der sie angeben, dass sie die Personas generiert haben. Die Personas wurden durch Anweisungen wie "Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst." generiert.</sample>
    <sample id="184">In dieser Arbeit wurde CXMI (Contextualized Cross-Lingual Mutual Information) als Messgröße zur Messung der Kontextnutzung von maschinellen Übersetzungssystemen eingeführt. CXMI misst, wie viel Information der Kontext C über das Ziel Y, gegeben das Quell X, bereitstellt. Die Arbeit erweiterte CXMI zu Pointwise CXMI, um Kontextnutzung auf Satz- oder Wortebene zu messen.</sample>
    <sample id="185">DrBERT and ChuBERT are both pre-trained models in French for biomedical and clinical domains. DrBERT is based on RoBERTa and trained on a dataset of medical crawled data from the web called NACHOS, while ChuBERT is based on anonymized data obtained from the data warehouse of Nantes University Hospital. DrBERT is a general-purpose model, while ChuBERT is a clinical model. Both models were compared on a variety of downstream tasks, and the results showed that DrBERT performed better on nine of the 11 tasks, while ChuBERT performed better on two tasks. Overall, DrBERT is a more versatile and powerful model than ChuBERT.</sample>
    <sample id="187">Zwei Autoren sind an der Arbeit beteilt: Ying und Zhiyang.</sample>
    <sample id="188">Iteratives Transferlernen bezieht sich auf die Methode, bei der ein Modell mit neuen Daten iterativ aktualisiert wird, um seine Leistung zu verbessern. In der Arbeit wird beschrieben, wie das Modell zunächst mit den Gewichten aus eng verwandten Aufgaben (Transferlernen) initialisiert wird und dann iterativ weiter trainiert wird, um die Leistung zu steigern. Dieses Verfahren hilft, die Leistung des Modells bei der Klassifizierung von Dissonanz zu verbessern.</sample>
    <sample id="189">The goal of the AltEntities Corpus is to understand users' language when they want to make a choice between entities, particularly in cases where direct references are not possible or appropriate. The corpus aims to improve conversational systems and benchmark large language models' entity understanding by providing a large-scale public dataset for this task.</sample>
    <sample id="190">An attacker can extract model parameters from an Embedding as a Service (EaaS) by leveraging the watermarking technique proposed in the paper. The attacker sends a dataset containing sentences with words from a trigger set to the EaaS provider. The provider then computes embeddings for these sentences. The attacker can then analyze the embeddings to identify the trigger words and estimate the model parameters based on the frequency of these words in the sentences. This process allows the attacker to reverse-engineer the model without directly accessing the underlying model parameters.</sample>
    <sample id="191">Drei Autoren sind an der Arbeit "Attention as a Guide for Simultaneous Speech Translations" beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">Title: CAME: Confidence-guided Adaptive Memory Efficient Optimization

Abstract:

We introduce CAME, a novel optimizer designed to achieve fast convergence and low memory usage for training large language models. Traditional adaptive methods like Adam require significant memory for storing gradient moments, while memory-efficient optimizers like Adafactor reduce memory usage but at the cost of slower convergence. CAME addresses this challenge by incorporating a confidence-guided approach to adaptively adjust the learning rate based on the residual between predicted and generated updates. Our experiments on BookCorpus and English Wikipedia demonstrate that CAME significantly improves validation accuracy over Adam and Adafactor, with a 3.4% increase in comparison to Adafactor. Additionally, CAME achieves better performance than Adam in pre-training very large models, with a substantial reduction in memory cost as batch sizes increase. Our proposed optimizer also outperforms existing memory-efficient optimizers like SM3 in terms of memory usage. CAME's effectiveness is further validated through extensive experiments on large language model training tasks, showcasing its potential for large batch training and serving as an important extension for existing memory-efficient optimizers</sample>
    <sample id="193">The original dataset was created using around 1,000 examples of paired discourse units.</sample>
    <sample id="194">Die Autoren gehören an der Carnegie Mellon University.</sample>
    <sample id="195">Title: Reasoning over Hierarchical Question Decomposition Tree for Explainability in Question Answering

Abstract:
Explainable Question Answering (XQA) aims to provide answers to questions along with explanations. Recent XQA methods fall into two categories: neuro-symbolic approaches, which translate questions into formal representations, and decomposable methods, which generate intermediate steps. However, these methods have limitations, such as incomplete knowledge bases and difficulties in handling diverse natural language. To address these challenges, we propose a novel framework, RoHT (Reasoning over Hierarchical Question Decomposition Tree), which integrates knowledge from heterogeneous sources. RoHT decomposes complex questions into hierarchical sub-questions, allowing flexible selection of knowledge sources. The framework consists of two stages: building a Hierarchical Question Decomposition Tree (HQDT) and probabilistic reasoning over the tree. HQDT is constructed by decomposing the original question into atomic sub-questions. Probabilistic reasoning is conducted from the root to the leaves, selecting appropriate knowledge sources (KB, text corpus, or recursive solving) and aggregating answers. We evaluate RoHT on KQA Pro and Musique datasets. On KQA Pro, RoHT outperforms existing KB QA methods and TransferNet, demonstrating the benefits of integrating KB and text knowledge. On Musique, RoHT-text and RoHT-mix models show significant improvements over SOTA methods, highlighting the advantages of explicit decomposition and knowledge integration.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer aufder linken Seite lautet: "I saw Bart and Lisa."</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme basiert auf der ABC-Eval Methode, die eine dimensionalen Ansatz zur Bewertung von Chatbots einführt. Diese Methode ermöglicht es, verschiedene Aspekte der Dialogqualität zu messen, indem sie die Ausdrücke der Chatmodelle analysiert, die bestimmte Verhaltensmuster zeigen, wie z.B. Antworten mit irrelevanten Informationen oder Widersprüchen. ABC-Eval bietet eine genauere und zuverlässigere Strategie für die dimensionale Dialogbewertung, die die Subjektivität menschlicher Bewertungen reduziert.</sample>
    <sample id="198">Um die Akzeptanz der Modelle über den gesamten Kontextfenster zu bewerten, weil große Sprachmodelle längere Kontextfenster haben und die Akzeptanzurteile nicht immer robust gegenüber Kontext sind.</sample>
    <sample id="199">Yes, the more multilingual training has led to a performance drop in seven datasets compared to the monolingual English model, while it has improved performance in three datasets. This phenomenon is known as the "Curse of Multilinguality."</sample>
    <sample id="200">No, the annotators do not necessarily know about the entities. They are provided with background knowledge about the entities, such as a Google search link for songs or text from Wikipedia for books and recipes.</sample>
    <sample id="201">Die MT-Metriken, die für die Bewertung verwendet wurden, sind state-of-the-art, neurale MT-Metriken sowie expert-basierte human evaluation Ergebnisse.</sample>
    <sample id="202">Based on the content of the paper, the regression does not seem to have a significant impact on the generalization of specific NER types. The paper focuses on the overall performance of NER taggers on the CoNLL++ dataset, which includes a variety of NER types. The results show that the models with better architecture, larger size, and more fine-tuning examples generalize better to new data, regardless of the specific NER type. The paper does not provide a detailed analysis of the performance of individual NER types, so it is not possible to determine if the regression has a significant impact on the generalization of specific NERC types.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil es die systematischen Leistungsunterschiede von Technologien zwischen verschiedenen Bevölkerungsgruppen hervorhebt und zeigt, wie die Perspektiven der NLP-Forscher und Modell-Entwickler die Forschung beeinflussen. Diese Positionierung kann dazu führen, dass bestimmte Gruppen übersehen oder benachteiligt werden. Um diese Positionierung zu charakterisieren und zu verstehen, wurde ein Framework namens NLPositionality entwickelt, das die Annotationen mit den Modellen und Datenbanken vergleicht. Die Ergebnisse zeigen, dass NLP-Datenbanken und Modelle am meisten mit englischsprachigen Ländern und Personen mit höherer Bildung übereinstimmen. Dies zeigt, dass es wichtig ist, die Positionierung in NLP zu berücksichtigen, um sicherzustellen, dass Technologien für alle Nutzer funktionieren und nicht bestimmte Gruppen benachteiligt werden.</sample>
    <sample id="204">The presentation does not mention whether multilingual LLMs like BLOOM were fine-tuned or adapted using adapters. It only states that multilingual language models such as Codex and BLOB are still inadequate for cross-lingual semantic tasks.</sample>
    <sample id="205">Title: Evaluating Political Bias in Language Models: From Pretraining Data to Downstream Tasks

Abstract:

Language models trained on large-scale web crawl data, including diverse political news media, exhibit varying political leanings. This study investigates the propagation of political biases from pretraining data to language models and their impact on downstream tasks. We evaluate language models using political questionnaires and find that GPT-4 is the most liberal, while BART series are generally more socially liberal. Pretraining language models on partisan corpora shifts their ideological coordinates, with RoBERTa showing a substantial liberal shift on a left-leaning Reddit corpus. Language models also pick up societal polarization, with models trained on post-2017 data showing a further shift away from the center. Evaluating language models on hate speech and fake news detection reveals fairness issues, with left-leaning models better at detecting hate speech targeting minority groups and vice versa. This highlights the need to address political biases in language models to prevent marginalization and censorship. The unique dilemma of balancing political neutrality and fairness is akin to the electric trolley problem, emphasizing the importance of tackling this issue.</sample>
    <sample id="206">Wir verwenden ein Modell, das von zwei eng verwandten Aufgaben übernommenen Gewichte übertragen wurde, um die Dissonanz-Klasse zu erkennen. Dieses Modell wird dann iterativ weiter verfeinert, um die Dissonanz-Klassifikation zu verbessern.</sample>
    <sample id="207">Die aktuellen Testsets zur Bewertung der PaLM-Fäbigkeiten wurden ausgewählt, um eine Überlappung mit dem Trainingstext des Modells zu vermeiden. Dazu wurden die besten Praktiken der maschinellen Übersetzung (MT) angewendet und die besten Systeme im WMT-Evaluation verglichen. Zusätzlich wurden neueste neurale MT-Metriken und expertenbasierte menschliche Bewertungen verwendet.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">The proposed method significantly improves the planning ability of language models in terms of both semantic completeness and faithfulness to constraints. Compared to the strongest baseline, the method achieves higher accuracy in generating scripts that adhere to specific constraints. This improvement is evident in the evaluation results, where the proposed method outperforms the baseline in generating high-quality scripts for constrained language planning.</sample>
    <sample id="210">The presenter's name is Shuheng.</sample>
    <sample id="211">Yes, the results and the dataset of the study can be used as a benchmark for the problem of automatic text simplification in future research.</sample>
    <sample id="212">In der Arbeit wird experimentiert, ob kleinere, spezialisierte Modelle, insbesondere T5, die durch Symbolisches Wissen Distillation von großen Sprachmodellen abgeleiteten Datensätzen wie CoScript trainiert werden können, um die Sprachplanungsfähigkeit zu verbessern. Es wird gezeigt, dass T5-fine-tuned auf CoScript die Qualität der generierten Skripte verbessern kann und kleinere Modelle in bestimmten Szenarien die Leistung größerer Modelle übertragen können.</sample>
    <sample id="213">Das basismodell für die Untersuchung multimodaler Unterrichtsabstimmung ist OFA, ein vereinigtes Mehrmodell, das eine gemeinsame Vokabular für Sprache, Bildtokens und Koordinaten eines Bounding Boxes verwendet.</sample>
    <sample id="215">This paper presents a novel argument for symmetric structures of coordination, challenging asymmetric approaches like the Prague dependency treebanks and the Hudson's Word Grammar. The authors propose that dependency length minimization, a principle where shorter dependencies are preferred, supports symmetric structures. They analyze coordination in the enhanced Penn Treebank, finding that left conjuncts tend to be shorter, especially when the governor is on the left or absent, and the difference in length between conjuncts increases. This tendency disappears when the governor is on the right. The paper provides statistical evidence and argues against asymmetric structures, supporting symmetric ones.</sample>
    <sample id="217">Title: "Seen to Unseen: Exploring Compositional Generation of Multi-Attribute Controllable Dialogue Generation"

Abstract:
We introduce "Seen to Unseen," a novel approach to multi-attribute controllable dialogue generation that addresses the limitations of existing methods. Previous techniques focus on single attributes, ignoring the complexity of multi-attribute settings. Our work explores compositional generation, identifying the need for a unified evaluation metric and a model capable of handling continuous attributes. We propose DCG (Disentangled Controllable Generation), which learns attribute concepts from seen values and uses disentanglement loss to manage attribute combinations. Our unified reference-free evaluation framework, MAE, evaluates attributes at different granularities. We establish two benchmarks to validate our method and metrics, demonstrating that DCG outperforms baselines in attribute controllability and text quality. Our models, based on the DialoGPT framework, use attribute-oriented and task-oriented prompts to guide generation. A disentanglement loss enhances compositional generalization. We introduce a template-based evaluation framework to assess multi-attribute dialogue generation without large-scale labeled data. Our results show that DCG effectively generalizes from seen to unseen attribute combinations, outperforming existing methods in controllability and text equality. We validate our approach with correlation coefficients, confirming its superiority over classic metrics. This work advances the field of dialogue generation by providing a scalable and effective solution for multi-attribute control.</sample>
    <sample id="218">Die Autoren gehören der Universität Google an.</sample>
    <sample id="219">Jia-Huei Ju and colleagues from Academia Sinica present a novel approach to financial report analysis using a compare-and-contrast multistage pipeline. The goal is to uncover financial signals in Form 10-K reports, which are annual reports required by the SEC. The team observed that these reports are highly similar, with about 80% of tokens being the same, and that the content is year-dependent. To address this, they introduced a highlighting task and a multi-stage pipeline. The pipeline consists of document segmentation, relation recognition, and out-of-domain and in-domain fine-tuning. The model predicts the importance of words to measure performance. The team used an external dataset for out-of-domain fine-tuning and mixed objectives to alleviate the problem of low-quality pseudo-labels during intermediate fine-tuning. The evaluation dataset included eSNLI pairs and the team's released FINAL dataset. The model achieved the best performance on FINAL and preserved generalization capability. The team also observed that their methods could benefit from simulation with mismatched pairs. Future work includes improving effectiveness and adding more features.</sample>
    <sample id="220">Die Autoren gehören an der Stony Brook University.</sample>
    <sample id="221">In der Arbeit wurden die Sprachpaare Deutsch-Englisch untersucht.</sample>
    <sample id="222">Title: "To Adapt or to Annotate: Challenges in Domain Adaptation for Open-Domain Question Answering"

Abstract:
This work addresses the challenges of domain adaptation in open-domain question answering (QA) systems, focusing on the transition from a general-purpose Wikipedia-based source domain to various target domains. We investigate three main contributions: data interventions for enabling out-of-domain generalization, identifying dataset shifts, and determining effective interventions for specific shifts. Using a setup with a general-purpose Wikipedia-based source domain, we test the generalizability on seven target datasets across six domains. We explore zero-shot and few-shot methods for generating data interventions, such as few-shot prompting large language models to generate facts and cloze-style questions. We also analyze the nature of dataset shifts, including concept shift, covariate shift, and full shift, using a compatibility measure based on likelihood scores. Our findings show that few-shot adaptations are effective for most target datasets, while zero-shot adaptations work well for datasets with concept and covariate shifts. We improve reader performance by up to 24% and demonstrate that specific data interventions are necessary for different types of shifts.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">During the experiments, the following models were investigated:

1. **MASSalign**: This model was used to evaluate automatic alignment methods for German text simplification.
2. **long-mBART**: This model was fine-tuned to produce document-level simplifications.
3. **normal base mBART**: This model was fine-tuned for sentence-level simplifications.

These models were evaluated for their performance in automatic text simplification tasks.</sample>
    <sample id="225">In MultiInstruct werden 53 der 62 verschiedenen Aufgaenge für Training und 59 für Tests verwendet.</sample>
    <sample id="226">Zwei Autoren sind an der Arbeit beteilt: Regina Stodden und Omar.</sample>
    <sample id="227">Grounded language understanding, which involves mapping natural language expressions to specific environments or plans, remains a challenging task for language models due to the lack of grounding during pre-training. Existing approaches often rely on language models to generate plans, which can result in grammatical or valid issues. Our proposed framework, Pangu, separates the symbolic world from the language model's world, allowing the language model to focus on discriminating between candidate plans proposed by a symbolic agent. This approach achieves outstanding performance across various settings, including fine-tuning and in-context learning, and demonstrates strong sample efficiency and robustness under non-i.i.d. settings. Our findings suggest that discrimination, rather than generation, may be a more effective strategy for grounded language understanding. We welcome discussions and collaborations on this topic.</sample>
    <sample id="228">The authors conducted experiments on four datasets: AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">In this work, Gabriella Skitalinskaya and Henning Wachsmuth explore the detection of suboptimal claims in argumentative writing and the suggestion of improvements. They introduce two tasks: Suboptimal-Claim detection and Claim Improvement Suggestion. The authors aim to understand how to model the quality of argumentative text based on implicit patterns found in collaborative online debate platforms like Kialo. They identify four main challenges: Representativity and Reliability, Model Complexity and Architecture, Contextual Information, and Topical and User Bias. The study highlights the effectiveness of using revision-based data for detecting suboptimal claims and suggests that modeling the distance between two claim versions is beneficial. The authors conclude that contextual information plays a role in determining the quality of text, and their findings are presented in a detailed analysis in their paper.</sample>
    <sample id="231">NACHOS is a data set of medical crawled data from web.</sample>
    <sample id="232">Der/die Referent*in heißt David Vilar.</sample>
    <sample id="233">Simultaneous speech translation (SimulST) translates spoken language into text in real-time, enabling cross-language communication. However, current SimulST models face challenges such as long training procedures, multiple models for different latency regimes, and the need for specific architectures. Our solution, EDAtt (Encoder-Decoder Attention), leverages existing offline speech translation (ST) models without re-training or adopting specific architectures. EDAtt uses a cross-attention mechanism to decide whether to emit a partial translation based on attention weights, allowing for efficient handling of different latency requirements. We compare EDAtt with popular strategies like Wait-k and Local Agreement, as well as state-of-the-art architectures, and find that EDAtt outperforms them in terms of translation quality and latency. We also release open-source code and models to facilitate reproducibility.</sample>
    <sample id="234">Die Prompt-Strategie hat einen erheblichen Einfluss auf die Ergebnisse. Die Auswahl der Beispiele und die Form der Anweisungen sind entscheidend. Eine gute Prompt-Strategie, wie die 5-shot Anweisung, kann die Leistung verbessern. Die tatsächliche Form der Anweisungen hat bei mehreren kurzen Anweisungen wenig Einfluss, ist aber bei fünf Anweisungen nahezu unerheblich. Die Qualität der Beispiele ist entscheidender als die Ähnlichkeit zu den Quelldaten.</sample>
    <sample id="235">Die Autoren gehören der Universität des Massachusetts Amherst an.</sample>
    <sample id="236">Die 5 Anweisungen der Expert*in in der MultiInstruct-Datensatz sind nicht spezifisch in der Präsentation erwähnt. Sie werden jedoch als fünf verschiedene Expert*innenanweisungen für jede der 62 vorgestellten multi-modalen Aufgaben genutzt.</sample>
    <sample id="237">Die Autoren schlagen eine Diagnostik-Testsuite namens "The KITMUS Test" vor, um die Fähigkeit von Modellen zur Integration und Nutzung von Wissen aus verschiedenen Quellen zu testen. Diese Testsuite besteht aus einer Coreferenzauflösungsaufgabe, die darauf abzielt, die Fähigkeit zu untersuchen, Wissen aus verschiedenen Quellen nutzen zu können. Die Autoren unterscheiden drei Szenarien: "Background-Pretrain", "Background-Both" und "Background-Inference", um die Verfügbarkeit von Wissen zu variieren. Sie evaluieren das Datenset sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Coreferenzauflösungsmodellen.</sample>
    <sample id="238">Yebowen Hu from the University of Central Florda presents a new benchmark dataset, MeetingBank, designed to address the need for summarization technologies in various reading domains. The dataset includes City Council meeting transcripts, reference summaries, and URLs, and is released to the public. The data collection process involves converting audio to transcripts using Speechmatics API, identifying meeting type and data, locating reference summaries, and aligning timestamps to create second transcripts. The dataset includes 1,366 City Council meetings with nearly 7,000 instances. The dataset statistics include the number of meetings, meeting duration, number of speakers, and year period of meeting collected. The dataset also provides summarization instances for each city, as well as the average length of source and summary texts. The dataset is used to evaluate top-tier summarization systems, including extractive and abstractive models, and to measure the level of abstraction in meeting summaries using coverage and density scores. The results show that GPT-3 achieves the highest overall scores in terms of fluency and coherence, but less impressive in terms of informativeness and factual accuracy. The dataset serves as a useful tool for researchers to design advanced summarization systems and provides insights into the decision-making process of City Council meetings.</sample>
    <sample id="239">Hallo alle, mein Name ist David Vilar, und ich werde eine kurze Bewertung des Artikels "Prompting PaLM for Translation: Assesing Strategies and Performance" geben. Dies ist gemeinsam mit meinen Kollegen von Google Translate. PaLM ist ein 540 Milliarden-Parameter-Großsprachmodell, das im Jahr 2022 präsentiert wurde. Es wurde auf einer großen Menge an Text trainiert, die 780 Milliarden Tokens umfasst. Zum Zeitpunkt der Veröffentlichung erreichte es den besten Stand in Hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie über die Anweisung von großsprachmodellen für maschinelle Übersetzung. Wir bewerteten die Übergangsfähigkeit solcher Modelle anhand der besten Praktiken der MT-Community. Dies beinhaltet die Verwendung der neuesten Test-Sets, um eine Überlappung des Testdatensatzes mit dem Trainingsdatensatz des Sprachmodells zu vermeiden. Und wir verglichen wir mit dem besten System, also dem WMT-Evaluationsystem. Wir verwendeten den besten, neuen neuralen MT-Metriken und zeigten zusätzlich Ergebnisse einer experten-basierten menschlichen Bewertung. Schließlich geben wir einige Empfehlungen für die Auswahl von Anweisungen. Die Anweisung hat einen großen Einfluss auf die Leistung von LLMs für Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir eine One-Shot-Anweisung verwendeten und zwei verschiedene Anweisungen für jede Sätze bereitstellten. Die Mehrheit der Sätze, 516 von 1.000, zeigte eine Differenz von mehr als einem BLEURT-Punkt. Und das kann in extremen Fällen bis zu 40 BLEURT-Punkte gehen. Also ist es wichtig, eine gute Anweisungsstrategie auszuwählen. In unseren Experimenten setzten wir auf eine 5-Schicht-Anweisung, bei der wir jedes Mal die Sprache, in der wir die Sätze bereitstellen, mit einem Doppelpunkt markierten. In diesem Beispiel, bei dem wir Übersetzungen von Deutsch nach Englisch durchführten, wurden die deutschen Sätze mit einem Doppelpunkt markiert und die englischen Übersetzungen mit einem Doppelpunkt. Wir sahen, dass die tatsächliche Form der Anweisung bei mehreren kurzen Anweisungen keinen großen Einfluss hat. Es ist entscheidend für Zero- und One-Shot-Anweisungen. Und wenn wir, wie in unserem Fall, zu einer fünf-Schicht-Anweisung übergehen, gibt es bei den tatsächlichen Formen der Anweisung kaum einen Unterschied. Es ist die Qualität der Beispiele, die den größten Einfluss hat. Insbesondere vergleichen wir die Auswahl von Anweisungen aus dem Trainingssatz für die WMT-Evaluations auf der Dev-Daten. Die Dev-Daten sind viel besser und weniger raucherig als die Trainingsdaten, und ihre Ergebnisse sind besser als die von PaLM. Dennoch haben spezialisierte, state-of-the-art-Systeme einen erheblichen Vorteil über die Übersetzungen von PaLM. Aber PaLM kommt in der Tat sehr nahe an einen kommerziellen System. In unserem Fall haben wir uns für die Bewertung mit Google Translate entschieden. Die Erkenntnisse, die wir aus der von uns durchgeführten menschlichen Bewertung mit dem MQM-Framework gewonnen haben, sagen, dass die Flüssigkeit von PaLM vergleichbar mit state-of-the-art-Systemen ist, aber der Hauptunterschied kommt von der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint, dass PaLM entscheidet, eine besser klingende Übersetzung zu produzieren, manchmal indem es Teile des Quelltextes wegliest, die in die Übersetzung umgesetzt werden. Allerdings ist der "Stil/Unangenehm"-Kategorie für PaLM niedriger als für die state-of-the-art-Systeme, was ein weiteres Signal ist, dass PaLM wirklich flüssige Ausgaben bietet, aber dennoch mit einigen Problemen der Genauigkeit. Und das ist es für diese kurze Übersicht. Für weitere Details kommt es zu der vollständigen Präsentation des Artikels. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Doktorand an der Universität Saarland in Deutschland. In diesem Video möchte ich unsere neueste Arbeit "Weaker Than You Think: Eine kritische Betrachtung der schwach überwachten Lernung" präsentieren. Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach und Andreas Stephan. Ich möchte mit einer kurzen Einführung in die Schwachüberwachte Lernung beginnen.

In der Schwachüberwachten Lernung werden Daten nicht manuell beschriftet. Stattdessen werden wir Daten mit schwachen Labelingquellen beschriftet, wie einfachen Heuristiken, Wissensdatenbanken oder niedrigqualitativen Crowdsourcing, wie in der Abbildung auf der rechten Seite gezeigt. Vergleichbar mit menschlichen Annotationen sind die schwachen Annotationen viel günstiger, aber sie sind auch raucher, was bedeutet, dass ein gewisses Maß an Annotationen falsch sind. Wenn wir direkt neuronale Netzwerke auf schwach beschriftete Daten trainieren, neigen die Netzwerke dazu, die Labelrauch zu merken und nicht zu generalisieren. In der Schwachüberwachten Learnung werden Trainingsalgorithmen entwickelt, um neuronale Netzwerke unter solcher Labelrauch zu robust trainieren, sodass die trainierten Modelle trotzdem gut generalisieren.

In neuen Arbeiten in WSL (Weakly Supervised Learning) wird oft behauptet, dass man nur Modelle auf schwach beschriftete Däta trainiert und hohe Leistung auf sauberen Testsets erzielt. Technisch ist dieser Behauptung nicht falsch, aber es gibt einen Hintergrund, der oft übersehen wird. Wir fragen uns, ob saubere Validierungsdaten für WSL notwendig sind oder ob wir stattdessen raucherige Validierungsdaten verwenden könnten. Wenn saubere Daten notwendig sind, dann wie viele saubere Daten braucht man, und sollten wir nur saubere Daten für Validierung verwenden oder sind es besser, sie anders zu nutzen?

Wir haben diese Forschungsfragen in unserer Arbeit beantwortet und die Ergebnisse sind folgende: Zunächst stellen wir fest, dass neue WSL-Methoden tatsächlich saubere Validierungsdaten benötigen, um richtig zu funktionieren. Sonst fällt die Leistung stark ab, was bedeutet, dass die Trainingsarbeit sinnlos ist. Unsere zweite Erkenntnis ist, dass die Anzahl der sauberen Validierungsdaten steigen hilft, die Leistung von WSL-Methoden zu verbessern. Typischerweise benötigen wir nur 20 Datenpunkte pro Klasse, um hohe Leistung zu erzielen. Aber das ist noch nicht das Ende der Geschichte, denn wenn wir saubere Daten verwenden, dann kann die Fortführung der Feinabstimmung sogar noch bessere Leistungen erzielen.

Zusammenfassend zeigen wir, dass neue WSL-Methoden saubere, manuell annotierte Daten benötigen, um richtig zu arbeiten. Ihr Leistungsgewinn und Praktikabilität werden stark übertrieben. Unsere konkreten Empfehlungen für zukünftige Arbeit sind folgende: Erstens sollten die Modellauswahlkriterien berichtet werden, zum Beispiel, ob die Modellauswahl über saubere Validierungsdaten erfolgt. Zweitens sollten WSL-Methoden mit wenigstenslernenden Baselines verglichen werden, da beide auf sauberen Daten arbeiten. Drittens sollte kontinuierliche Feinabstimmung eine einfache, aber starke Basiskonzeption sein, die in zukünftigen Arbeiten in WSL berücksichtigt werden sollte. Schließlich haben wir unser Code geöffentlicht. Sie können ihn über den QR-Code auf dieser Präsentation finden. Bitte beachten Sie es. Vielen Dank und viel Spaß beim Konferenz.</sample>
    <sample id="241">Ethan presents a paper on "Human-in-the-loop Evaluation for Early Misleading Detection: A Case Study of COVID-18 Treatments." The paper discusses the limitations of existing automatic misinformation detection systems, which are often evaluated using retrospectively constructed datasets and lack human involvement. The authors propose an evaluation framework that integrates human feedback throughout the process, from raw tweets to actionable outputs. The system consists of two main components: a claim detection module that ranks claims by trendiness and a policy violation verification module that uses a BERT-based stance classification model. The authors evaluate the system's efficacy in detecting unapproved COVID-19 treatments before they are debunked and in flagging policy violations. The system achieves a position of 65% in policy violation detection and can detect 124.2 policy violations per human hour worked. The authors conclude that their framework provides a more realistic evaluation of human-in-the-loop misinformation detection systems and hope to motivate future development in this area.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogeinsysteme sind humanbasierte Bewertungen, wie beispielsweise die Anfrage von Menschen, welche der beiden Gespräche besser sei, oder die Bewertung von Gesprächen mit einer Likert-Skala. Diese Ansätze liefern gute, umfassende Bewertungen der Gesprächskwalität, aber sie können nicht alle Aspekte der Gesprächskwalität abdecken. Eine genauere und zuverlässigere Strategie ist die ABC-Eval-Methode, bei der menschliche Annotatoren explizit bestimmen, ob jede Antwort bestimmte Verhaltensweisen ausdrückt, wie z.B. unangemessene Informationen oder Widersprüche. ABC-Eval misst die Raten, bei denen Chat-Modelle bestimmte thematische Fehler begehen, wie z.B. die Ignorierung des Partners oder unangemessene Antworten.</sample>
    <sample id="243">Die Arbeit wurde von fünf Autoren beteiligt: Jenny, Sebastian Santy, Ronan Le Bras, Katja Reinecke und Maarten Sap.</sample>
    <sample id="244">Im Beispiel mit Servin und Kea wird das Hintergrundwissen benötigt, dass "Judges decide cases in law courts."</sample>
    <sample id="245">Title: A Needle in a Haystack: An Analysis of Highly-Agreement Workers on MTurk for Text Summarization

Abstract:
This study presents a pipeline for identifying high-agreement Amazon Mechanical Turk (MTurk) workers for text summarization tasks. The pipeline consists of pre-task qualifications, qualification tasks, endurance tasks, and a reference-based task. Pre-task qualifications include location, number of Human Intelligence Tasks (HITs), and HIT Approval Rate. The qualification task tests annotators' ability to evaluate multiple dimensions, categorizing workers into gold, silver, bronze, and block. The endurance task assesses workers' capacity for handling heavy workloads. The reference-based task evaluates general performance on the true annotation task.

Our pipeline results in 4 gold and 8 gold-silver workers, representing 6% of 200 participants. The pipeline achieves high agreement in terms of Inter-Annotator Agreement (IAA) and Cohen's Kappa, with the best Krippendorff's Alpha being 0.534. The pipeline's performance is comparable to CloudResearch MTurk workers, with a Krippendorff's Alpha of 0.513.

The pipeline's pre-task filtering avoids wasting time and resources, achieving high agreement at a lower cost. It serves as a best practice for high-agreement annotations at scale and lower cost, avoiding resource waste on discarded annotations. Future work will investigate ways to hire high-quality workers and explore multiple applications for tasks, languages, and platforms</sample>
    <sample id="246">Ja, der Code ist verfügbar. Sie können ihn auf GitHub finden.</sample>
    <sample id="247">Title: FACTKG: Fact Verification via Reasoning on Knowledge Graph

Abstract:
We introduce FACTKG, a novel dataset for fact verification using knowledge graphs as evidence. Unlike existing datasets that rely on text or tables, FACTKG leverages the intuitive and connected nature of knowledge graphs for reliable reasoning. The dataset, based on DBpedia, includes claims in both written and colloquial styles, with two labels: SUPPORTED and REFUTED, and features five types of reasoning: one-hop, conjunction (multiple one-hop claims), existence, multi-hop, and negation. FACTKG's dataset construction includes a colloquial style transfer model and presupposition templates. We evaluate baselines, including a GEAR model that uses graph evidence, showing significant improvement over majority class baselines. FACTKG aims to enhance consistency checks between knowledge graphs and natural language, applicable in various tasks requiring KG-NL consistency.</sample>
    <sample id="248">Die Annotatoren für NLPositionality sind nicht ausgewogen in Bezug auf jede demografische Gruppe. Die Studie fand heraus, dass die Datensätze und Modelle am stärksten mit englischsprachigen Ländern und Personen mit höherer Bildung übereinstimmen. Es gab jedoch auch Ungleichheiten, wie z.B. weniger Übereinstimmung mit nicht-binären Personen im Vergleich zu Männern und Frauen.</sample>
    <sample id="249">Sätze innerhalb der akzeptable Domain wurden durch das Hinzufügen eines akzeptablen oder unakzeptablen Prefixes zu beiden Akzeptabilitätsfragen erzeugt.</sample>
    <sample id="250">Eine dimensionale Bewertung bedeutet, die Qualität eines Dialogsystems auf verschiedene Aspekte oder Dimensionen zu bewerten, anstatt nur eine allgemeine Bewertung. Dies ermöglicht es, die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.</sample>
    <sample id="251">The authors belong to the University of Science and Technology of China.</sample>
    <sample id="252">Title: U-CREAT: Unsupervised Case Retrieval using Event Extraction

Abstract:

Legal professionals face challenges in citing relevant past precedents due to the increasing volume of cases. Prior Case Retrieval (PCR) aims to retrieve relevant candidates from a candidate pool, considering both relevance to the query document and citations within it. This work presents U-CREAT, a joint effort by Sai Kiran Tanikella, Abhinav Joshi, Akshat, Sharma, and Ashutosh Modi, introducing the IL-PCR dataset and the U-CREAT PCR pipeline.

The IL-PCR dataset, a new benchmark for PCR tasks, contains 7,070 legal cases with an average of 6.775 citations per query document. It offers a comprehensive test bed for PCR algorithms. The U-CREAT pipeline leverages unsupervised learning and an event-based approach, demonstrating high retrieval efficiency, low inference time, and cross-system generalization.

Event extraction plays a crucial role in the U-CREAT pipeline, representing case documents as collections of events. The pipeline consists of pre-processing, dependency parsing, and post-processing steps, generating an interaction matrix between query and candidate events.

Experiments using diverse models validate and compare their performance on the PCR task, categorized into count-based, transformer-based, and event-based models. Event-based models, including the Event Filtered Documents model, outperform all other methods, achieving significant boosts in performance and lower inference times.

U-CREAT outperforms existing approaches, including the recent supervised method by the MTFT-BERT team, on the COLIEE dataset. This work opens avenues for further exploration and development in the field</sample>
    <sample id="253">DisorBERT: A Double Domain Adaptation Model to Detect Mental Disorders in Social Media

Mental disorders, such as major depression and PTSD, affect millions worldwide, and social media offers a unique platform for research and support. DisorBERT, a double domain adaptation model, aims to detect signs of mental disorders by analyzing social media posts. By leveraging BERT and incorporating domain-specific knowledge, DisorBERT adapts to the language of Reddit and mental health, improving performance on the target domain. The model uses guided masking to focus on important words during training, resulting in a balanced precision-recall performance. DisorBERT generates more psychologically oriented responses compared to BERT, highlighting its effectiveness in identifying mental health issues. Future work includes exploring different lexical resources and clinical data to further enhance the model's capabilities.</sample>
    <sample id="254">Title: Uncertainty Guided Label Denoising for Document-level Relation Extraction

Abstract:
Document-level relation extraction aims to identify relationships among entities within a document. Traditional methods rely on large-scale human-annotated corpora, leading to high costs and labor intensity. Recent approaches use distantly supervised data to pretrain models, but these data often contain noise. Pseudo labels are used to mitigate noise, yet they can introduce false positives, as shown in the figure. This paper proposes a document-level relation extraction framework with uncertainty-guided label denoising. We first train a pre-denoising model using both distantly supervised (DS) and human-annotated data to generate pseudo labels. To address false positives, we introduce uncertainty estimation to assess model trustworthiness. For overlapping relations, we propose an instance-level uncertainty estimation method. A re-labeling strategy with dynamic class uncertainty thresholds is designed to filter high-uncertainty pseudo labels. Additionally, a multi-phase training strategy iteratively re-labels DS data. Our framework outperforms previous baselines on public datasets, demonstrating improved label quality and performance. The main contributions are: (1) uncertainty-guided label denoising, (2) instance-level uncertainty estimation for overlapping relations, (3) dynamic class uncertainty thresholds for long-tail problems, and (4) significant performance improvements.</sample>
    <sample id="255">Die Form des Prompts ist in den Fällen von Null- und Ein-Schuss-Prompting entscheidend, da hier die Qualität der Beispiele eine größere Rolle spielt als die Ähnlichkeit zum Quelltext. Bei mehr als einem Schuss-Prompting (z.B. fünf Schützen) hat die Form des Prompts keinen signifikanten Einfluss.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art Chat-Modelle evaluiert.</sample>
    <sample id="258">Title: Evaluating Large Language Models as an Alternative to Human Evaluation in Natural Language Processing

Abstract:
This paper explores the use of large language models (LLMs) as an alternative to human evaluation for assessing the quality of text in natural language processing (NLP). We propose using LLMs to evaluate text samples based on natural language instructions, aiming to replicate the process of human evaluation while overcoming its instability and reproducibility issues. Our experiments involve rating stories generated by GPT-2 and human-written stories using four attributes: grammar, coherence, likability and relevance. We compare the ratings with human evaluation results, using English teachers as experts. Our findings show that some LLMs, such as Davinci and ChatGPT, exhibit a clear preference for human-written text, similar to human evaluators. This suggests that LLMs can serve as a viable alternative to human evaluation in certain NLP tasks. The paper also discusses the benefits and costs of using LLM evaluation compared to human evaluation, and presents results on other tasks.</sample>
    <sample id="259">XSemPLR is a unified benchmark for cross-lingual semantic parsers in multiple natural languages and meaning representations. It includes 9 datasets in various domains, 5 tasks, 8 meaning representations, and 15 language families. The benchmark evaluates six settings: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer. The study found that Encoder-Decoder models outperform Encoder-PTR models, and that training in a mixture of languages can improve performance. The "Curse of Multilinguality" was observed, where English performance drops in seven datasets. The study also found that pretraining on English can boost performance in few-shot settings. The results show that multilingual language models such as Codex and BLOSOM are inadequate for cross-lingual semantic parsing tasks, and that XSemPLR provides a comprehensive benchmark for evaluating cross-lingual semantic parsers.</sample>
    <sample id="260">Based on the provided information, it appears that Jingwei Yi is the primary author of the paper. There is no mention of any other authors involved in the work.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithfully adhere to the constraints imposed on the goals of planning.</sample>
    <sample id="262">Die Arbeit "Distilling Script Knowledge from Large Language Models" wurde von einem Autor, Siyu Yuan von Fudan University, vorgestellt.</sample>
    <sample id="263">In this work, we address the issue of label biases in in-context learning, a popular paradigm for utilizing large language models. We identify a new type of bias, domain-label bias, and propose a novel calibration method to handle all types of bias. Our approach uses content-free text, specifically random in-domain words, to estimate the model's bias on each label name and calibrate the model's predictions. We conduct experiments on a wide range of datasets and find that domain-context calibration significantly improves the performance of in-context learning, especially on tasks with large domain-label bias. Our findings hold for larger models like GPT-3. We provide a systematic investigation of label bias problems in in-context learning and propose a calibration method to mitigate the effect of different types of biases.</sample>
    <sample id="264">Title: Transferable Audio-Visual Text Generation (TAVT)

Abstract:

Transferable Audio-Visual Text Generation (TAVG) addresses the challenges in multimodal text generation tasks, such as audio-visual text generation, where data annotation is more arduous and expensive. Existing works suffer from severe degradation due to varying construction conditions in different multimodal domains. TAVG proposes a novel task to overcome these constraints by learning a unified audio semantic space to align visual concepts across domains. The framework consists of three components: an audio-visual meta mapper network, an audio-visual encoder and language model, and counterfactual contrastive learning.

The audio-visual meta mapper network maps different visual concepts across domains into a unified auditory space, addressing shifts in semantic distribution. The second model, a transformer-based encoder and generator, uses an alpha to evaluate the contribution of different modalities. The proposed Dual Counterfactual Contrastive Learning (DCCL) constructs fine-grained supervision signals from counterfactuals to directly optimize visual-textual alignment.

Experiments on MSVD and MSR-VTT benchmarks show that TAVG outperforms state-of-the-art approaches in both cross-datasets and cross-domain settings. TAVG also demonstrates robustness in low-resource domains with limited labeled data, such as "Kids" and</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">Die Autoren gehören der Universität von Toronto an.</sample>
    <sample id="268">Die häufigsten Fehler von PaLm sind Omissionen, bei denen Teile des Quelltextes während der Übersetzung ausgelassen werden. Dies führt zu einer besseren Klinge der Übersetzung, aber auch zu genauen Fehlern.</sample>
    <sample id="269">Hallo, ich bin James Finch und Sarah Finch. Heute werden wir Ihnen alles über ABC-Eval erzählen, eine neue dimensionale Methode zur Bewertung von Dialogkünstlicher Intelligenz. Diese Arbeit wurde von der Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Stellen Sie sich vor, Sie haben ein Dialogmodell entwickelt und möchten sehen, wie es im Vergleich zu dem aktuellen Stand der Technik abschneidet. Die übliche Praxis besteht darin, menschliche Bewertungen durchzuführen, wie z.B. durch das Anfragen von Menschenrichtern, welches der beiden Gespräche besser sei, oder durch Bewertungen der Gespräche mit einer Likert-Skala. Diese Ansätze funktionieren gut, um eine umfassende Bewertung der Gesprächskwalität zu liefern, aber die Gesprächskwalität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um eine feinere Einschätzung der Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Eine Möglichkeit besteht darin, einfach menschliche Richter zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie z.B. die Relevanz der Modelleingaben mithilfe bestehender vergleichender oder Likert-Skala-Methoden. Allerdings glauben wir, dass es einen genaueren und zuverlässigeren Ansatz für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität menschlicher Bewertungen zu reduzieren, indem wir explizit daran annotieren, ob jede Modelleinspruch bestimmte Verhaltensweisen ausdrückt, wie z.B. mit irrelevanten Informationen reagieren oder sich widersprechen. Wir nennen diesen Ansatz das Annotieren von Verhaltensweisen im Chat oder ABC-Eval in Kürze. Wir entwickelten diese Methode, um umfassend die Chatmodellverhaltensweisen abzudecken, die in der jüngsten Literatur als Einfluss auf die Chat-Qualität angegeben wurden. ABC-Eval ist in der Lage, die Raten zu messen, mit denen Chat-Modelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC-Eval die Anzahl der Turns, in denen ein Chat-Modell seinen Partner ignoriert oder etwas irrelevantes sagt, sich widerspricht oder seinem Partner, halluciniert falsche Fakten oder verletzt allgemeine Wissen, und wenn das Modell erfolgreich oder fehlgeschlagen ist, Empathie zu zeigen. Um herauszufinden, welche Art von Bewertung am effektivsten ist, haben wir vier der neuesten Chat-Modelle ausgewählt und sie auf 100 mensch-bot-Gespräche pro Modell mit ABC-Eval bewertet. Zur Vergleichsweise haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf Turn-Ebene, Likert-Bewertungen auf Dialog-Ebene und Dialog-Ebene-Paarweisevergleiche. Für jede der bestehenden Methoden haben wir Bewertungen auf acht der am häufigsten gemessenen Aspekte der Dialogqualität gesammelt, da dies die Standardpraxis für die Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist. Aus unserer Analyse dieser Bewertungsergebnisse fanden wir heraus, dass die ABC-Eval-Verhaltenslabels insgesamt zuverlässiger sind als die von bestehenden Methoden gesammelten Labels, wie durch die Inter-Annotator-Agreement auf 100 doppelt beschrifteten Gesprächen gemessen. Darüber hinaus sind die ABC-Eval-Labels zuverlässiger als die von bestehenden Methoden produzierten Metriken, um die Gesprächskwalität zu erklären, wie durch einfache lineare Regressionsanalyse gezeigt wird. Zum Beispiel können Sie sehen, wie die Messung der Anteile von Turnen mit Selbst- und Partner-Widersprüchen 5% und 10% der Gesprächskwalität erklärt, w</sample>
    <sample id="270">Die Autoren gehören der Emory University an.</sample>
    <sample id="271">In this work, CFT stands for "Continuous Fine-Tuning." The authors found that allowing the model to continue fine-tuning on clean validation samples can achieve the same performance as more complex weakly supervised learning (WSL) methods. This suggests that continuous fine-tuning is a simple yet effective baseline for WSL and should be considered in future research.</sample>
    <sample id="272">In der Arbeit sind 7 Autoren beteiligt: John Gauthier, Aaron Mueller, Kishka Misra, Karen Fences, Roger Lévy, Adina Williams und Koustav Sinha.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit unter dem Titel "Wann benötigt Übersetzung Kontext? Eine datengetriebene, mehrsprachige Untersuchung" präsentieren. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André Fernandes Martins und Graham Neubig erstellt. Viele Übersetzungen hängen von Kontext ab. Zum Beispiel, wie würden wir den Satz "Mole" übersetzen? Wenn die vorherige Aussage "Es könnte gefährlich werden, wenn die Minister herausfinden", dann bezieht sich "Mole" auf einen Spion. Aber wenn die vorherige Aussage "Könnte es etwas ernst sein, Arzt?" lautet, bezieht sich "Mole" auf eine Hautveränderung. Je nach Kontext ändert sich der Sinn des Wortes und damit auch seine Übersetzung. Allerdings ist es schwierig zu bewerten, wie gut Modelle solche Fälle übersetzen können. Zunächst einmal, weil nur ein kleiner Teil der Übersetzungen von Kontext abhängt, können Korpusbasierte Metriken wie BLEU nicht diese Übersetzungen erfassen. Und einige Menschen haben vorgeschlagen, gezielte Bewertungen für kontextabhängige Übersetzungen durchzuführen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachmengen, da sie in der Regel auf Fachwissen und menschliche Auswahl angewiesen sind. In dieser Arbeit versuchen wir diese beiden Fragen zu beantworten. Erstens, wann benötigt Übersetzung Kontex? Zweitens, wie gut können Modelle diese Fälle behandeln? Um die erste Frage zu beantworten, haben wir gemessen, wie viel ein Wort während der Übersetzung von Kontext benötigt. In einer früheren Arbeit haben wir CXMI als Maß für den Kontextgebrauch von maschinenübersetzungsmodellen eingeführt. Dies wird durch Messung getan, wie viel Information der Kontext C über das Ziel Y, gegeben das Quell X, bietet. Man kann sich CXMI als die Informationen vorstellen, die man dem Modell durch Kontext gibt. In dieser Arbeit erweitern wir CXMI auf Punktweise CXMI, das auf Satz- oder Wortebene gemessen werden kann. Man kann sich Wörter mit hohem P-CXMI als Wörter vorstellen, die Kontext für die Übersetzung benötigen. Nun analysieren wir Wörter mit hohem P-CX</sample>
    <sample id="274">The presenter's name is Yusen Zhang.</sample>
    <sample id="276">Ananya and Vignesh present their work on "IndicMT Eval: A dataset to meta-evaluate machine translation metrics for Indian languages." They focus on evaluating translations from Indian languages to English, addressing the gap in research on evaluation metrics for other languages. They use the Flores dataset to select 200 sentences and generate 1,400 candidate translations using seven different translation models. Human annotators evaluate the outputs, marking errors by type and severity, and providing an overall score. The study compares various metrics, including overlap-based, embedding-based, and COMET-metric variants, and finds that COMET-metric variants have the highest overall correlation with human scores. They fine-tune the best-performing metric, COMET, using their MQM dataset and observe improved performance on three out of five languages. The study also evaluates the robustness of IndicCOMET MQM on the ACES Translation Accuracy Challenge Sets. The publicly available dataset can be used for further research.</sample>
    <sample id="277">Die neue Methode hat den Namen "Multiset Tagging and Latent Permutations."</sample>
    <sample id="278">Die Autoren beschreiben die Methode der „markierten Worte“ als eine Ansatz, der auf dem Konzept der „Markierungsdichte“ basiert, das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist. Sie identifizieren die unmarkierten und markierten Gruppen und verwenden dann das Fightin’ Words-Methode, um gewichtete Log-Odds-Verhältnisse zu verwenden, um die top-Wörter für jede markierte Gruppe zu untersuchen. Diese Methode ermöglicht es, spezifische Stereotype und Muster zu erkennen, ohne auf eine bestimmte Wortsammlung angewiesen zu sein.</sample>
    <sample id="279">Die Autoren gehören der Universität Washington an.</sample>
    <sample id="280">Shi Tao presents "MultiEMO," a novel attention-based correlation-aware multimodal fusion (MMF) framework for emotion recognition in conversations. The framework addresses three key challenges: exploiting multimodal complementarity, improving performance on minority emotion classes, and distinguishing semantically similar emotions. MultiEMO consists of four components: unimodal feature extraction, context modeling, multimodal fusion (MultiAttn), and emotion classification. VisExtNet, a novel visual feature extractor, captures facial expressions without redundant scene information. MultiAttn integrates modalities using bidirectional multi-head cross-attention layers. The Sample-Weighted Focal Contrastive Loss (SWFC) addresses minority class classification. Experiments on MELD and IEMOCAP datasets show state-of-the-art performance, particularly in minority and semantically similar emotions. However, limitations include VisExtNet's inability to distinguish speakers and the SWFC loss's requirement for large batch sizes.</sample>
    <sample id="281">Title: When Does Translation Require Context? A Data-driven,
Multilingual Exploration

Abstract:
Translation often depends on context, but evaluating this is challenging due to corpus-level metrics like BLEU failing to capture context-dependent translations. We introduce Pointwise Contextualized Mutual Information (P-CXMI) to measure context usage at the sentence and word levels, identifying words that require context for accurate translation. Analyzing TED talk transcripts translated into 14 languages, we find that context is crucial for part-of-speech tags, proper nouns, formality, and ellipsis resolution. We develop the Multilingual Discourse-Aware (MuDA) tagger to identify context-dependent examples and use it to evaluate models. Corpus-level metrics like BLEU favor context-agnostic models, while COMET and word f-measure show context-aware models' strengths. Our MuDA benchmark reveals that context-aware models excel in formality and lexical cohesion but lag in ellipsis, pronouns, and verb form. DeepL outperforms Google Translate in document-level translation. This study provides insights into when and how context affects translation, guiding the development of more effective translation systems.</sample>
    <sample id="282">Xuekai Zhu presents "StoryTrans: Non-Parallel Story Style Transfer with Discourse Representations and Content Enhancer" at ACL 2023, addressing the challenge of non-parallel text style transfer at the discourse level. Traditional methods focus on token or sentence-level transfer, but StoryTrans aims to imitate author style at the story level, capturing complex linguistic preferences and narrative techniques. The primary challenge lies in transferring style-specific content, such as narrative techniques, while preserving the source semantics.

To tackle this, StoryTrans employs a novel generation model that learns discourse representations and combines them with learnable style embeddings. The model is trained in two stages: first, it transfers the source text with style-specific content masked, and then generates the whole text by incorporating these keywords explicitly. The training framework includes self-reconstruction loss, disentanglement loss, sentence order loss, and style classifier loss.

StoryTrans was evaluated on new datasets in Chinese and English, showing superior performance in style control and content preservation compared to strong baselines. Style visualization confirms alignment with the golden text in the style feature space, and StoryTrans can enrich storylines and maintain source semantics. The model can rewrite most sentences with the target style while preserving the source semantics.</sample>
    <sample id="283">Die zuerst erwähnte symmetrische Abhängigkeitsstruktur ist die "multi-headed approach".</sample>
    <sample id="284">Title: FSUIE: Enhancing Universal Information Extraction with Fuzzy Span Mechanisms

Abstract:

We introduce FSUIE, a novel fuzzy span mechanism for improving universal information extraction (UIE) models. Traditional UIE models rely heavily on precise span boundaries, which can be ambiguous. FSUIE proposes a fuzzy span mechanism where the span boundary is learned as a continuous distribution, allowing for more flexibility in labeling. This approach addresses the mismatch between transformer feature extraction and information extraction, as transformers focus on global features, ignoring the limited length of spans. FSUIE uses adaptive attention to model the furthest span boundary, representing the target boundary as a probability distribution. The boundary distribution is calculated using Binary Cross Entropy and KL-divergence, and a fuzzy span attention mask function is introduced to adjust the attention span dynamically. Experiments on three main IUE tasks, including named entity recognition, relationship extraction (achieving new state-of-the-art results on ACE2004, 2015, and ADE datasets), and aspect sentiment triplet extraction, demonstrate FSUIE's superior performance. Ablation studies show that the fuzzy span loss and attention mechanisms significantly improve convergence speed and information extraction capability. FSUIE's attention distribution is visualized, showing a focus on semantic information within a limited range of preceding words. In conclusion, FSUIE effectively enhances UIE models by introducing a novel fuzzy span mechanism and adaptive attention, achieving excellent results across various IUE tasks.</sample>
    <sample id="285">Title: "Reference Matters: Benchmarking Factual Error Correcting for Dialogue Summarization with a Fine-Grained Evaluation Framework"

Abstract:
This work addresses the critical issue of factual errors in dialogue summarization, a gap in existing research. We propose a novel evaluation framework to assess the performance of Factual Error Correction (FEC) models, which correct errors in model-generated summaries. Traditional metrics like FactCC and DAE provide an overall score, but they are vague and unreliable. We argue that these metrics blur the line between FEC and summarization models, potentially diverting FEC from its original purpose. To address this, we introduce manually annotated reference corrections, which provide more valuable data for training and a comprehensive evaluation. We propose a new taxonomy of factual errors, categorizing them into content-based and form-based, and build our evaluation framework on ERRANT, an evaluation metric for grammar error corrections. Our experiments show that training FEC models with reference summaries from dialogue summarizations yields the best results. We find that current FEC models struggle with certain types of errors, such as addition and attribute errors. We conclude that introducing human-corrected summaries during training and combining human-annotated data with synthetic data are promising directions for improving FEC models.</sample>
    <sample id="286">James Finch und Sarah Finch.</sample>
    <sample id="287">Vier Autoren sind an der Arbeit beteilt: Javad Hosseini, Filip Radlinski, Silvia Paret, und Annie Louis.</sample>
    <sample id="288">Datensätze wie BLiMP, SyntaxGym, Adjunct Island, Wikipedia und andere relevante Datensätze können zum Testen syntakischer Phänomene verwendet werden.</sample>
    <sample id="290">Die Abkürzungen der fünf Forschungsmethoden für die erste Frage sind:

1. WSL (Weakly Supervised Learning)
2. COSINE
3. FTw (Fine-Tuning with WSL)
4. FTw+ (FTw with continued fine-tuning on clean samples)
5. FTw+ (FTw with continued training on clean samples)</sample>
    <sample id="291">Das Modell wird auf verschiedenen Aufgaben im Bereich Named Entity Recognition, Classification, Part-of-Speech Tagging und Question Answering evaluiert.</sample>
    <sample id="294">CamemBERT wurde ursprünglich auf der Basis von Daten aus dem NACHOS-Datenset trainiert.</sample>
    <sample id="295">Der/die Referent*in ist Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presents a collaborative work between the University of Turin and Amazon Alexa, focusing on Natural Language Understanding (NLU) and Natural Language Processing (NLP). The project aims to develop perspective-aware models for irony detection, a complex and latent phenomenon in language. The English Perspectivist Irony Corpus (EPIC) was created, collecting data from social media, Reddit, and Twitter over 1.5 years, with 300 short conversations per source and five English varieties. Crowdsourcing platform Prolific was used for annotation, with 74 annotators providing 200 texts each. The study found differences in inter-annotator agreement based on gender, age, nationality, and other factors. Perspective-aware models, fine-tuned on splits of the dataset by different annotators, showed higher confidence in predictions compared to gold standard aggregated models. The research also revealed that age and geographical distribution of annotators influenced their perception of irony.</sample>
    <sample id="297">In this project, we explore the concept of dogwhistles in political rhetoric, focusing on their use in coded language to convey hidden messages to specific groups. We develop a typology and glossary of over 340 terms and phrases, including racist, transphobic, and anti-Semitist dogwhistles, and analyze their use in historical U.S. political speeches. Our findings show a correlation between the frequency of racial dogwhistles and the Republican Southern Strategy, which used dogwhistles to avoid explicit racism. We also evaluate the ability of language models, such as GPT-3, to recognize and surface dogwhistles, with varying success depending on the context and prompting strategies. Finally, we demonstrate how dogwhistles can evade content moderation by showing that automated toxicity detection scores decrease when standard slurs are replaced with dogwhistles. Overall, our project provides a comprehensive understanding of dogwhistles and their role in political influence and online content moderation.</sample>
    <sample id="298">Die Schlussfolgerung, dass die zeiträumliche Verzögerung die Hauptauslöser für den Leistungsverlust war, wurde durch die Beobachtung der Ergebnisse der Experimente bestätigt, bei denen Modelle mit neueren Daten weiter trainiert wurden. Die Performance der Modelle verschlechterte mit zunehmender zeitlicher Distanz zwischen Trainings- und Testdaten, was die Hypothese der temporären Drift bestätigte.</sample>
    <sample id="299">"Improving the robustness of NLI models with minimaxi training" presents a novel approach to mitigate shortcuts in natural language inference (NLI) models. Recent advancements in NLI have been impressive, but these models often rely on spurious correlations between input attributes and labels, known as shortcuts. These shortcuts can lead to brittle performance on out-of-distribution adversarial test sets. Traditional shortcut mitigation methods require access to an auxiliary model and domain-specific knowledge, limiting their applicability.

Our method introduces a minimax training objective that encourages the learner to focus on under-represented, hard examples that counteract shortcuts. The learner minimizes the NLI task loss, while the auxiliary maximizes this loss by generating example weights. This approach does not assume knowledge of shortcuts or require a pre-trained language model as the auxiliary.

Evaluations on datasets like MNLI, FEVER, and QQP show that our method consistently improves out-of-distribution performance while maintaining in-distribution accuracy. We also examine the effects of pre-training the learner, the size of the auxiliary, and conduct a qualitative evaluation of the learned example weight.

This work offers a promising solution to improve the robustness of NLI models and is presented at our poster session. We invite interested attendees to discuss our findings further."</sample>
    <sample id="300">Interactive dictation is a task where users can dictate and edit a document using their voice in a natural and intuitive manner. The task involves flexible interleaving of dictation and editing, using intuitive and open-ended natural language utterations to specify edits. The task is formalized as a four-step procedure, including ASR recognition, segmentation, command extraction and normalization, and execution of dictation and command utterances. A baseline system is built to perform each of these steps, and the results show that GPT-3 models are more accurate but slower than T5 models, and predicting state directly is more accurate than predicting intermediate programs. The task is a new area of research, and more work is needed to improve the system. The code for the baseline system is available for future work.</sample>
    <sample id="302">Es ist notwendig, die Token für die Ausgangsessequenz zu permutieren, um die systematischen Korrespondenzen zwischen dem Eingabesequenz und der Ausgangssequenz wiederzugeben. Ohne Permutationen können die Modelle die korrekte Reihenfolge der Tokens nicht erzeugen, was zu Ausgaben führt, die nicht mit dem Eingabesequenz übereinstimmen.</sample>
    <sample id="303">Die Autoren empfehlen, Modellentwickler*innen, dass sie ihre Methoden zum Abbau von Vorunrechten transparenter machen sollten, um die Ursache dieser Muster zu verstehen, sei es durch eine übermäßige Wertausrichtung oder andere anti-stereotisierende Methoden. Transparenz ist entscheidend, um weitere Forschung und Verbesserungen zu ermöglichen.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind solche, die grammatisch falsch sind, aber in einem Kontext, der sie akzeptabel macht. Diese werden in der Arbeit mit der Minimalpaar-Paradigma untersucht, um die Sprachmodelle auf ihre Akzeptabilität in längeren Sätzen zu testen.</sample>
    <sample id="305">Title: "Weakly Supervised Learning: A Critical Examination"

Abstract:

Weakly supervised learning (WSL) aims to train neural networks using weakly labeled data, which is cheaper and noisier than human annotations. However, WSL methods often struggle to generalize due to label noise. This work addresses three key research questions: the necessity of clean validation data, the required number of clean samples, and the optimal use of clean data. Our findings reveal that clean validation samples are essential for WSL to perform well, with 20 samples per class typically sufficient. Increasing clean samples further enhances performance. Direct fine-tuning on clean data outperforms WSL methods, suggesting that the performance gains of WSL are often overestimated. We recommend reporting model selection criteria, comparing WSL with few-shot learning baselines, and considering continuous fine-tuning. Our code is open-sourced for further exploration.</sample>
    <sample id="306">Title: Entity Tracking in Language Models: An Evaluation Task and Findings

Abstract:
We investigate the ability of pre-trained language models to track entities in discourse. Our task involves predicting the contents of boxes after a series of state-changing operations, designed to prevent shortcut solutions. We test this on Flan-T5 and GPT-3 models using 2-shot in-context learning. Results show that most models simply repeat the initial state. However, text-davinci-003 exhibits significant entity tracking, outperforming a random baseline. Our analysis suggests that pre-training on code is crucial for this ability. Smaller models like T5-base can learn entity tracking with direct fine-tuning, but larger models require pre-training. Further research is needed to determine if these abilities generalize beyond our setup.</sample>
    <sample id="307">Die Autoren haben verschiedene Bewertungsmetriken verwendet, um die Leistung ihrer Modelle zu bewerten. Sie haben die folgenden Bewertungsmetriken eingesetzt:

1. **Named Entity Recognition (NER)**: Die Autoren haben die Fähigkeit der Modelle, bestimmte Entitäten in Texten zu erkennen, gemessen.
2. **Classification**: Die Autoren haben die Fähigkeit von Modellen, Texte in vordefinierte Kategorien einzuteilen, gemessen.
3. **Part-of-Speech Tagging (POS)**: Die Autoren haben die Fähge der Modelle, die Wörter in ihre grammatischen Kategorien einzuteilen, gemeasen.
4. **Question Answering (QA)**: Die Autoren haben die Fähgkeit von Modellen, Fragen auf Basis von Texten zu beantworten, gemessen.

Zusätzlich haben die Autoren die Leistung ihrer Modelle mit sechs Baseline-Modellen verglichen, die CamemBERT OSCAR 138GB, CamemBERT OSCAR 4GB, CamemBERT CCNET 4GB, PubMedBERT, BioBERT und ClinicalBERT darstellen. Die Evaluation hat gezeigt, dass die Modelle am besten auf Aufgaben mit Daten von der gleichen Natur wie die, auf denen sie trainiert wurden, performieren. Allerdings haben die Daten aus heterogenen Quellen auch eine vielversprechende Leistung gezeigt.</sample>
    <sample id="308">This presentation by Jenny, a first-year PhD student at Carnegie Mellon University, discusses the concept of positionality in Natural Language Processing (NLP) and its impact on datasets and models. Positionality refers to the perspectives that people hold as a result of demographics, identity, and life experiences. The research team, in collaboration with the University of Washington and the Allen Institute for Artificial Intelligence, developed a framework called NLPositionality to study the positionality of datasets and models. The framework involves re-annotating datasets with diverse annotators and comparing the annotations to the models and datasets using a Pearson's correlation score. The study found that datasets and models are most aligned with English-speaking countries and people with a college education. However, some populations, such as non-binary individuals, are less aligned with the models and datasets. The research team recommends keeping a record of design choices, conducting NLP research with a perspectivist lens, and building specialized datasets and models within specific communities. The study's findings and recommendations are available on the NLPositionality dashboard and paper.</sample>
    <sample id="309">Die Metrik, die verwendet wurde, um die Übereinstimmung (Inter-annotator agreement) zwischen den Kommentatoren zu messeni, war die Anzahl der doppelt markierten Konversationen. Diese Anzahl wurde verwendet, um die Reliabilität der von den Kommentatoren gesammelten Labels zu bewerten.</sample>
    <sample id="310">Die Domain, die gewählt wurde, um völlig unzusammengehörige Sätze zu den inakzeptabel und akzeptabel Suchanfragen hinzuzufüge, war Wikipedia.</sample>
    <sample id="311">Die Autoren gehören der Universität Stuttgart an.</sample>
    <sample id="312">MultiInstruct unterscheidet sich von anderen Benchmarks durch seine Fokus auf Multi-Modal Instruction Tuning und die Schaffung eines ersten Multi-Modal Instruction Tuning Datensatzes mit 62 verschiedenen Multi-Modal Aufgaben. Es bietet eine einheitliche Verarbeitung von Text, Bildern, Anweisungen und Bounding-Boxen in einem Tokenraum und verwendet eine neue Metrik für Sensibilität, um die Konsistenz der Ausgabe unabhängig von der Anweisungsform zu messen.</sample>
    <sample id="313">Zwei Autoren sind an der Arbeit beteilt: James Finch und Sarah Finch.</sample>
    <sample id="314">Die binäre Koordination ist eine sprachliche Struktur, bei der zwei oder mehr Wörter, Phrasen oder Sätze in einer gleichwertigen Beziehung stehen, die durch Koordinationskonjunktionen wie "und", "oder" oder "aber" verbunden werden. In dieser Struktur sind die beiden Teile gleichwertig und haben keine hierarchische Beziehung, wodurch beide Teile gleichmaßig betont werden.</sample>
    <sample id="315">The duration of the prompts used in the study is not specified in the provided text.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass ein kleinesres T5-Modell, nachdem es auf das CoScript-Datenset fine-tuned wurde, höhere Qualität in den generierten Skripten erreicht hat als die meisten großen Sprachmodelle. Dies zeigt, dass kleinere Modelle, wenn richtig trainiert, auf geeigneten Datensätzen, die Leistung über großen Sprachmodellen hinaus können.</sample>
    <sample id="317">Title: CodeIE: Large Code Generation Models are Better Few-shot Information Extractors

Abstract:
Information extraction (IE) is a fundamental task in natural language processing (NLP) that involves extracting structured information from unstructured text. Traditional IE models, such as T5 and GPT-3, operate in a text-to-text manner during pre-training, but face challenges in generating structured outputs during inference. To address this issue, we propose CodeIE, which transforms the text-to-structured IE task into a structure-to-structure code generation problem using large code generation models like Codex. We evaluate our approach on three named entity recognition (NER) datasets and four relation extraction (RE) datasets, comparing it to traditional text-style prompts. Our results show that CodeIE significantly outperforms baseline models, such as UIE and GPT-3, in terms of both accuracy and recall. We attribute this improvement to the better alignment of code generation models with the IE task and the reduced structural errors during decoding. Our analysis provides insights into the benefits of using code generation models for IE tasks and highlights the potential of CodeIE for future research.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich werde mich auf unsere Arbeiten über "DrBERT: Ein robustes vorgebildetes Modell in Französisch für biomedizinische und klinische Bereiche" vorbereiten. In dieser Präsentation werden wir zunächst über Sprachmodellierung in der Gesundheitsversorgung sprechen. Dann werden wir die Hauptbeiträge unseres Artikels präsentieren. Wir haben den ersten biomedizinischen Modell in Französisch namens DrBERT eingeführt, der auf RoBERTa basiert und auf NACHOS, einem Datensatz von medizinischer Crawled-Daten aus dem Web, trainiert wurde. Wir haben auch eine Vergleichsstudie von Modellen mit verschiedenen Pre-Trainingsettings und Datenquellen vorgestellt. Dann präsentieren wir die Ergebnisse auf 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Schließlich werden wir uns mit den Experimenten auseinandersetzen und mehr Details darüber geben, wie man diese Modelle zugänglich machen kann. Seit seiner Einführung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze zur Lösung natürlicher Sprachverarbeitungsaufgaben geworden und bietet riesige Leistungsgewinne im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2vec, fastText oder anderen. Seitdem wurde dieses Modell in vielen anderen Sprachen angepasst, wie zum Beispiel in Französisch mit CamemBERT, und auch in Bereichen wie biomedizinisch mit PubMedBERT und BioBERT und klinisch mit ClinicalBERT, aber meist in Englisch. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlicher Pre-Training aufgrund des Mangels an in-Domain-Daten. Allerdings gab es bisher keine offene Quelle für ein biomedizinisches Modell in Französisch. Also fragen wir uns die Frage, welche Datenquellen für eine breite Nutzung am besten geeignet sind und ob diese Crawled-Daten eine gute Substitution für klinische Daten sind. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten aus dem Datensatz der Universitätsklinik Nantes basiert. Anschließend fragen wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren? Ist es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten vergleichen wir vier von-scratch-Modelle: eine erste Version von DrBERT mit 7 GB von NACHOS; eine zweite Version von 4 GB von NACHOS; eine erste Version von ChuBERT, die ein klinisches Modell mit 4 GB von Sätzen aus klinischen Notizen ist; und eine endgültige Version von ChuBERT mit einem Mix aus 4 GB von NACHOS und 4 GB von klinischen Notizen. Neben dieser Vergleichsstudie haben wir auch drei Modelle basierend auf kontinuierlicher Pre-Trainingsstrategie analysiert, um den Einfluss von Pre-Trainingsstrategien zu untersuchen. Eins basiert auf den Gewichten von CamemBERT und wurde auf einem 4 GB-Subsatz von NACHOS trainiert. Ein weiteres basiert ebenfalls auf CamemBERT, aber diesmal auf 4 GB von klinischen Notizen und schließlich eins auf dem englischen biomedizinischen Modell PubMedBERT und wurde auf 4 GB von NACHOS trainiert. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammeln wir Daten für öffentliche und private Downstream-Aufgaben wie Named Entity Recognition, Klassifikation, Part-of-Speech Tagging und Fragebeantwortung. Diese Modelle werden mit sechs Basismodellen verglichen, nämlich CamemBERT OSCAR 138GB, CamemBERT OSCAR 4GB, CamemBERT CCNET 4GB, PubMedBERT, BioBERT und ClinicalBERT. Die Bewertung zeigt, dass die Modelle am besten auf Aufgaben mit Daten von der gleichen Natur wie die, auf denen das Modell trainiert wurde, erfolgreich sind. All</sample>
    <sample id="319">In der Arbeit werden folgende Lernstrategien untersucht:

1. **Von-scratch-Prä-Training**: Trainierung von Modellen auf einem eigenen Datensatz (NACHOS) ohne vorherige Prä-Training.
2. **Anonymisierte Datenverwendung**: Verwendung anonymisierter Daten aus klinischen Notizen (ChuBERT).
3. **Kontinuierliches Prä-Training**: Verwendung von Prä-Trainingsmodellen wie CamemBERT und PubMedBERT, um die Modellleistung zu verbessern.
4. **Datenmengenverschiedenheit**: Untersuchung der Leistung von Modellen, die auf unterschiedlichen Datenquellen trainiert wurden (NACHOS, klinische Notizen, PubMedBERT).
5. **Datenmenge**: Untersuchung der Leistung verschiedener Modellversionen mit unterschiedlichen Datenmengen (7 GB, 4 GB).

Diese Strategien helfen, die Leistung von Modellen in der französischen Sprache für biomedizinische und klinische Anwendungen zu optimieren.</sample>
    <sample id="320">The factor of overfitting due to the reuse of the same test set is indicated by a gradient greater than one in the red best fit line, meaning that every unit of improvement on CoNLL-2003 results in more than one unit improvement on CoNLL++. This suggests that adaptive overfitting is not observed in this case.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde basierend auf verschiedenen Simplifizierungsarten bewertet, wie Lexikalische Vereinfachung, Strukturvereinfachung und insgesamt die Vereinfachungsstufe. Die DEPLAIN-Korpus-Analyse zeigte, dass die Vereinfachung der Bibeltexte stärker ist als die Vereinfachung von Nachrichtentexten oder Texten für Sprachlern. Zudem wurde die Vielfalt der Simplifizierungsumwandlungen untersucht, wobei im DEPLAIN-apa-Korpus mehr Umordnungen und Wortzusätze vorkamen, während im DEPLAIN-web-Korpus mehr Umformulierungen vorkamen.</sample>
    <sample id="322">Title: Understanding Morality in Text Classifiers: A Study of Moral Foundation Theory

Abstract:
This paper explores the learning of morality in text classifiers, focusing on the Moral Foundation Theory, which posits that humans perceive morality through five distinct foundations. We investigate how language models understand morality across different domains using the Moral Foundation Twitter Corpus, comprising 35,000 tweets from seven domains. Our findings reveal that language models can recognize fine-grained differences in moral expression, such as the contrast between #AllLivesMatter and #BlackLivesMatter. These differences highlight the importance of domain-specific understanding to avoid misinterpretations of morality. Our study underscores the need for explainable AI techniques to ensure accurate and context-aware morality classification in language models.</sample>
    <sample id="323">Title: Dynamic Heterogeneous-Graph Reasoning with Language Models and KRL for Commonsense QA

Abstract: Commonsense QA is a challenging task that requires understanding language and common knowledge. Recent works combine language models and knowledge bases to solve this problem, but they introduce noisy entities and ignore semantic relationships. We propose DHLK, a method that builds an optimized heterogeneous knowledge graph (HKG) using multiple knowledge bases and KRL. We remove noisy entities, retrieve paraphrases, and encode QA contexts and entities using RoBERTa and Mask Self-Attention. We use TransE to optimize embeddings and Relation Mask Self-Attention to model subgraphs. We incorporate HKG path information into QA context and predict answers using MLP. Experiments on CommonsenseQA and OpenBookQA show that DHLK outperforms other methods.</sample>
    <sample id="324">Yes, language models have been found to exhibit varying political biases. These biases can be influenced by the pretraining data they are trained on, which often includes a diverse range of political perspectives. Studies have shown that different language models, such as GPT-4, BART, and RoBERTa, can have distinct political leanings, with some models being more liberal or conservative than others. Additionally, language models can pick up on societal polarization, with models trained on data from different time periods showing shifts in political bias. These biases can have significant implications for downstream tasks, such as hate speech detection and fake news detection, where models with different political leanings may perform differently based on the demographic or political leaning of the news media. This highlights the need to address fairness issues resulting from language model political biases.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute gebe ich Ihnen eine kurze Einführung in unsere Arbeit über "Kompositionalisierung ohne Bäume durch Mehrfach-Tagging und Latente Permutationen". Dies ist gemeinsam mit meinen Betreuern Alexander Koller und Ivan Titov. Die Kompositionalisierung kann als die Fähigkeit eines Lerners verstanden werden, tiefere Rekursion und ungesehene Kompositionen von Phrasen zu behandeln, die während des Trainings einzeln gesehen wurden. Im Kontext der semantischen Parsing könnte das Testen für Kompositionalisierung so aussehen. Wie üblich haben wir einen Trainingssatz von Utterances. In diesem Fall sind "Die Mädchen schliefen." und "Mary wusste, dass die Mädchen schliefen." Diese Utterances sind mit logischen Formen verbunden, die die Kernaspekte ihrer Bedeutung darstellen. Im Gegensatz zur üblichen maschinellen Lernungsevaluation enthält der Testsatz eine Struktur, die nicht aus demselben Verteilungsraum kommt, sondern ungesehene logische Formen. In diesem Beispiel hat das Modell während des Trainings Tiefrekursion erlebt und wird auf ein Beispiel mit tieferer Rekursion getestet. Naive seq2seq-Modelle haben Schwierigkeiten mit dieser Art von Ausgewandter Generalisierung und produzieren oft Ausgaben, die von den Eingaben abhängig sind. Insbesondere scheitern sie oft daran, systematische Korrespondenzen zwischen Eingabe und Ausgabe zu reproduzieren, wie sie in dem Beispiel farbig dargestellt sind. Eine beliebte Methode, um dies zu lösen, ist, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositorischen Prozess erfassen, der die Utterances mit den logischen Formen verknüpft. Dies funktioniert gut, aber Bäume werden in der Regel nicht gegeben und müssen auf irgendeine Weise erhalten werden. Dies kann ein kompliziertes und manchmal rechenaufwendiges Prozess sein. Typischerweise erfordert dies erhebliche Formalismus-spezifische Vorverarbeitung der logischen Formen, zum Beispiel um Variablen-Symbole zu behandeln. Die Erhaltung von Bäumen kann auch spezialisierte Grammatik-Erzeugungsverfahren erfordern. In dieser Arbeit verwenden wir keine Bäume und führen ein neuronales seq2seq-Modell ein, das die Korrespondenzen zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe direkt modelliert. Zum ersten Mal zeigen wir starke Generalisierung zu tieferer Rekursion ohne die Abhängigkeit von Bäumen. Unser Ansatz schätzt das Ausgangsergebnis von der Eingabe in zwei Schritten. Zuerst etikettieren wir jedes Eingabetoken mit einem ungeordneten Multiset von Tokens, die in der Ausgabe vorkommen. Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht geordnet. Deshalb verwenden wir in der zweiten Schritt ein weiteres Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu setzen. Wir führen eine neue Methode ein, um die Permutation vorherzusagen, die keine strenge Einschränkungen auf mögliche Permutationen setzt. Dies macht unsere Methode ziemlich flexibel und ausdrucksstark. Konzeptionell funktioniert unsere Permutationsmethode etwa so. Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Multiset-Token in jedem Position im Ausgangsergebnis platziert wird. Für die erste Ausgabeposition wählen wir einfach eines, wie im Rot hervorgehoben. Dann springen wir zu dem nächsten Multiset-Token, um den zweiten Token im Ausgangsergebnis zu bestimmen. Wir bestimmen den dritten Token im Ausgangsergebnis ähnlich, indem wir zu einem anderen Multiset-Token springen. Wir führen diesen Prozess fort, bis jedes Token aus dem ersten Stadium genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen treefreien Modellen auf dem COGS-Benchmark. Unser Modell erre</sample>
    <sample id="326">Kognitive Dissonanz ist ein Konzept, das zwei unterschiedliche Überzeugungen oder Handlungen bezeichnet, die widersprüchlich sind. Zum Beispiel könnte jemand sagen, "Ich weiß, dass Zigaretten mich töten könnten", und dann weiter sagen, "Ich rauchte ein paar Zigaretten nach der Besprechung". Diese Überzeugung und Handlung sind widersprüchlich und befinden sich in Dissonanz.</sample>
    <sample id="327">"ManagerTower: Aggregating the Insights of Uni-modal Experts for Vision-Language Representation Learning" presents a novel architecture for Vision-Language Learning (VLL) that addresses the limitations of existing models like METER and BridgeTower. By introducing managers in each cross-modal layer, ManagerTower adaptively aggregates insights from pre-trained unimodal experts at different levels, facilitating more comprehensive cross-modal alignment and fusion. This approach allows for effective exploitation of different levels of unimodal semantic information, leading to superior performance on downstream tasks. ManagerTower achieves state-of-the-art results on various benchmarks, including a 39.15% accuracy improvement on the Wikivideo test set, even with only four million images for pre-training. The architecture is flexible, allowing the use of any visual, textual, or cross-modal encoder. The paper, code, and models are available on Archive and Github.</sample>
    <sample id="328">Based on the information provided in the presentation, GPT-4 is the most liberal language model among the ones discussed.</sample>
    <sample id="329">Title: Noise-Resistant Structured Pseudo-Label Generation for Zero-Shot Video Sentence Localization

Abstract:
We introduce a novel approach for zero-shot video sentence localization, focusing on generating structured pseudo-labels to enhance model robustness against label noise. Traditional methods rely on manually annotated pseudo-queries, which are often too simplistic and lead to misalignment between pseudo-events and queries. Our method addresses these issues by generating complex free-form pseudo-queries using a pre-trained image caption model, then modeling temporal event structures to create pseudo-events with high relevance to the query. We reduce label noise by re-weighting samples based on predicted confidence and IoU, and refine pseudo-labels by selecting top-quality queries and eliminating overlapping pairs. Experiments on ActivityNet Captions and Charades-STA demonstrate superior performance compared to existing zero-shot methods, achieving best results on both datasets. Our approach, termed SPL, offers a robust solution for zero-shot video sentence localization by leveraging structured pseudo-labels and noise-resistant techniques.</sample>
    <sample id="330">Ja, kumulatives Training scheint besser als iteratives Training für aktives Learning zu sein, insbesondere in Bezug auf die Anzahl der Dissonanzbeispiele.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Die Daten für den MuDA-Benchmark stammen aus Transkriptionen von TED-Talks, die von Englisch in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="333">Wenhao from Nanjing University introduces their work "INK: Injecting kNN Knowledge in Neural Machine Translation" at a conference. The team includes Jingjing Xu from Shanghai AI Lab, Shijian Huang and Jiajun Chen from Nanjing, and Lingpeng Kong from the University Hong Kong. The paper focuses on improving neural machine translation (NMT) models by addressing the non-smooth representation space that limits generalization. They propose INK, a framework that injects kNN knowledge into NMT models to smooth predictions. INK extracts kNN knowledge from a datastore, adjusts representations using KL-divergence, and updates the datastore asynchronously. Experiments show that INK outperforms the state-of-the-art kNN-ML system, achieving higher BLEU scores with less memory space. The framework also improves translation performance and inference speed.</sample>
    <sample id="335">Der/die Referent*in ist Matthias Lindemann.</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf die Fähigkeit eines Modells, Wissen und Fähigkeiten aus einer Sprache zu einer anderen zu übertragen. In der Kontext von Cross-Lingual Semantic Parsing bedeutet es, dass ein Modell, das in einer bestimmten Sprache trainiert wurde, in einer anderen Sprache verwendet werden kann, ohne dass es speziell für die neue Sprache trainiert werden muss. Dies kann sowohl beim Übersetzen von Queries in mehrere Sprachen als auch beim Verständnis und Übersetzung von Semantikrepräsentationen zwischen verschiedenen Sprachen erforderlich sein.</sample>
    <sample id="337">Title: Graph-based Relation Mining for Context-free Out-of-vacabulary Word Embedding Learning

Abstract:
Out-of-vocabulary (OOV) words pose significant challenges in embedding-based models, yet they are crucial for performance. Our research introduces a novel approach leveraging word formation and association to infer OOV meanings. We develop a Word Relationship Graph that mimics lexical rules, associating OOV words with relevant terms. Each wordpiece acts as a node, with embeddings as node attributes. The first layer preserves complete wordpiece information, while the second layer samples nodes to mitigate noise. A self-attention network assigns attributes to OOV nodes, and two levels of Graph Attention Networks capture important information. A readout block layer provides a graph-level representation, and contrastive learning with NT-XENT positive samples enhances performance. Experiments show superior performance over baselines in intrinsic and extrinsic tasks, benefiting both static and contextual models. Our model's flexibility allows for potential application to agglutinative languages, with word decomposition being key.</sample>
    <sample id="338">Title: Evaluating Human Explanations in Natural Language Processing: Towards Objective Metrics

Abstract:

This paper presents a novel evaluation metric, TREU, for assessing the quality of human natural language explanations in machine learning models. We address the challenge of evaluating subjective and task-dependent explanations, which are often used to train models to generate human-understandable explanations and improve prediction performance. Our unified data structure and preliminary experiments demonstrate the utility of explanations in fine-tuning and inference stages. We evaluate human explanations across five datasets using TREU and simulatability scores on two models, T5 and BART, and find that TREU better reflects the helpfulness of explanations. Our results support the hypothesis that the helpfulness of human explanations to models depends on the task and explanation format. We propose TREU as a foundation for high-quality human collaboration in annotation jobs and recommend researchers perform similar quality checks in the future to improve the evaluation of human explanations in natural language processing.</sample>
    <sample id="339">Die Autoren gehören der Saarland-Universität in Deutschland an.</sample>
    <sample id="340">ParaAMR is a large-scale, syntactically diverse paraphrase generation dataset created by leveraging AMR (Abstract Meaning Representations) back-translation. The dataset contains around 15 million source sentences and 6.9 paraphrases per source sentence, offering a significant increase in scale compared to existing human-annotated datasets. By modifying the focus node in AMR graphs and generating text from the modified graphs, ParaAMR ensures syntactic diversity while maintaining semantic similarity. Quantitative analysis shows that ParaAMR outperforms other datasets in semantic similarity and syntactic diversity. The dataset benefits several NLP applications, including sentence embeddings, syntactic control paraphrase generation, and data augmentation for few-shot learning. ParaAMR is available for further research and development.</sample>
    <sample id="341">Die Autoren verwenden die Latenzmessungen "average lagging" und "computational aware average lagging" in ihrer Arbeit.</sample>
    <sample id="342">Title: LiveChat: A Large-Scale Personalized Dialogue Dataset from Live Streaming

Abstract:
LiveChat is a novel dataset designed for open-domain and personalized dialogue research, constructed from Chinese live streaming videos. It addresses the limitations of existing text-based datasets by providing a video-sourced, large-scale collection that captures real spoken conversations. The dataset is built through three steps: video scraping, audio transcription, and dialogue construction using reply-to-whom matching. Persona information is also extracted to support personalized dialogue generation. LiveChat outperforms existing datasets in terms of scale and session length, and it is particularly valuable for multi-party dialogue research. Experiments on response modeling and addressee recognition demonstrate the benefits of persona profiles and longer sessions. The dataset's distinctiveness is highlighted by the superior performance of BART over other models, confirming its domain-specific nature. Future work will focus on efficient transfer learning of large language models for LiveChat.</sample>
    <sample id="343">Hallo alle, ich bin Akshatha und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit "The KITMUS Test: Evaluierung der Wissenintegration aus mehreren Quellen". Diese Arbeit ist eine Zusammenarbeit zwischen McGill University, Mila und Microsoft Research. Natürliche Sprachverstehensmodelle nutzen eine Vielzahl von Wissensquellen, wie zum Beispiel Wissen, das in ihren Parametern enthalten ist, gewonnen durch eine Vorbereitung, und Wissen, das in Eingaben während der Inferenz gegeben wird. Kürzliche Arbeiten in Aufgaben wie Fragebeantwortung zeigen, dass Modelle in der Lage sind, Wissen, das während der Vorbereitung erworben wurde, zur Lösung der Aufgabe zu nutzen. Aber natürliche Sprachverstehensanwendungen erfordern oft Wissen, das auch während der Inferenz zur Verfügung steht. Zum Beispiel in dem Satz: "John sah den neu gewählten Präsidenten auf dem Fernsehen." Vorbereitungsparameter können Informationen über das, was Präsidenten tun und was ein Fernsehen ist, enthalten, aber sie können nicht zuverlässig wissen, wer dieser spezifische Entität "John" ist oder wer der neue Präsident ist, weil der Präsident seit der Vorbereitung geändert worden sein könnte. Daher sind erfolgreiche Modelle für wissensintensive natürliche Sprachverstehensaufgaben in der Lage, Wissen zu integrieren und zu nutzen, das sowohl während der Vorbereitung als auch während der Inferenz verfügbar ist. In dieser Arbeit schlagen wir eine Diagnostik-Testsuite für Wissenintegration vor. Wir führen eine Kernreferenzierungsaufgabe ein, die dazu dient, die Fähigkeit zu untersuchen, auf Wissen, das in verschiedenen Quellen verfügbar ist, zurückzugreifen. Wir bewerten das Datenset mit menschlichen Studienteilnehmern und etablierten Kernreferenzierungsmodellen. Hier ist ein Beispiel aus unserem Datenset: Servin ist ein Richter. Kea ist ein Backbaker. Servin und Kea trafen sich im Park. Nach einem langen Tag am Arbeitsplatz, in dem er Fälle in einem Gericht entschied, war er froh, sich nach der Arbeit zu entspannen. Die Aufgabe besteht darin, die korrekte Entität zu identifizieren, zu der der Pronomen "er" in diesem Fall Servin bezieht. Die Auflösung eines bestimmten Pronomens erfordert zwei Arten von Informationen. Zunächst ist dies die spezifische Wissensinformation wie "Servin ist ein Richter." Und zweitens ist dies das Hintergrundwissen wie "Richter entscheiden Fälle in Gerichten." Allgemein gesprochen wird Hintergrundwissen während der Vorbereitung von großen Sprachmodellen gelernt, während spezifische Wissensinformationen in der Regel während der Inferenz beobachtet werden. Wir variieren die Verfügbarkeit dieser beiden Informationen, sodass es entweder in einer Quelle oder in mehreren Quellen vorliegen kann. Wir haben drei Einstellungen des KITMUS definiert. Zunächst haben wir die typische Einstellung: "Hintergrund-Vorbereitung", bei der das Hintergrundwissen als verfügbar betrachtet wird. Zweitens gibt es eine "Hintergrund-Beide" Einstellung, bei der das Hintergrundwissen sowohl während der Vorbereiung als auch während der Inferenzen verfügbar ist. Schließlich ist die "Hintergrund-Inferenz"-Einstellung, bei der beide Wissensarten nur während der Inferenz verfugbar sind. Diese letzte Einstellung ist besonders interessant, da sie die Situation simuliert, in der das Hintergrundwissen, das notwendig ist, um eine Aufgabe zu lösen, nicht Teil des V</sample>
    <sample id="344">Die Nachteile der baumbasierten Ansätze sind, dass sie häufig eine umfangreiche Formalisierung der logischen Formen erfordern, wie z.B. die Handhabung von Variablen, und dass sie manchmal komplexe Grammatikinduktionsverfahren benötigen. Darüber hinaus sind Bäume nicht in den Modellen integriert, sondern müssen separat erzeugt und dann in die Modelle eingebettet werden. Dies kann rechenaufwendig sein und die Modellierung der entsprechenden Zusammenhänge zwischen Eingabefragmenten und Ausgabefragmenten erschweren.</sample>
    <sample id="345">This paper presents a novel approach to compositional generalization in semantic parsing without relying on trees. The authors introduce a neural seq2seq model that directly predicts the correspondences between input and output fragments. The model first tags each input token with an unordered multiset, and then uses another model to predict a permutation to order the tokens. This approach outperforms other treeless models on the COGS benchmark, but still faces challenges in aligning input and output and finding the linguistically correct permutation. The authors address these challenges by inducing the alignment as part of the training and approximating the permutation with a GPU-friendly continuous relaxation.</sample>
    <sample id="346">The authors of the paper "Do CoNLL-2003 named-entity taggers still work well in 2013?" are from Tsinghua University.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werde ich über unsere Arbeit "Marked Personas: Verwendung natürlicher Sprachanweisungen zur Messung von Stereotypen in Sprachmodellen" sprechen. Diese Arbeit ist in Zusammenarbeit mit Esin Durmus und Dan Jurafsky durchgeführt worden. In den letzten Jahren wurde vieles über die Prävalenz sozialer Vorurteile und Stereotypen in großen Sprachmodellen, oder LLMs, dokumentiert. Allerdings haben diese Messungen verschiedene Einschränkungen. Sie hängen in der Regel von handgekürtem Datensatz ab, der sehr zeitaufwendig zusammengefasst wird, und sie messen in der Regel nur sehr spezifische Stereotypen, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte generalisieren oder nur sehr allgemeine breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen, erfassen. Darüber hinaus berücksichtigt die meiste Arbeit in diesem Bereich die Intersektionalität nicht, was die Idee ist, dass mehrdimensionale soziale Identitäten Kompetenzen und einzigartige Orte des Schadens kompunden können. Um diese Einschränkungen zu überwinden, setzen wir auf die Eigenschaft, dass diese neueren Anweisungstuning-LLMs sehr gut darauf reagieren, Anweisungen und Anweisungen zu geben. Also können wir den Modellanweisungen geben, um eine Persönlichkeit zu generieren, die ein vorgeschriebenes Bild einer fiktiven Person ist, wie zum Beispiel "Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst." Und wir können sofort sehen, dass dies sehr allgemein anwendbar ist, indem wir einfach jedes Identitätsmerkmal, das wir wollen, in diesen Anweisungen einfügen können. Hier sind einige Beispielegenerationen von GPT-4. Immerhin sehen wir, dass, während die Ausgaben nicht offensichtlich negativ oder toxisch im traditionellen Sinne sind, es dennoch interessante Muster gibt. Die asiatische Frau wird als unauffällig beschrieben; die mittelöstliche Frau wird mit Wörtern wie "exotisch" und wie bezeichnet, um eine faszinierende Region zu erwähnen. Und beide Personas von Menschen von Farbe</sample>
    <sample id="348">Marked Personas: Measuring Stereotypes in Language Models

This paper presents a novel approach to measuring stereotypes in large language models (LLMs) using natural language prompts. We introduce the concept of "marked personas," which are generated by providing LLMs with prompts that describe an imagined individual with specific identity markers. This method allows for the generation of diverse personas and the identification of stereotypes through the "Marked Words" method, which leverages the sociolinguistic concept of markedness.

Our results show that generated personas contain more stereotypes than human-written ones, but the human-written personas have a wider distribution of words. The Marked Words method reveals that seemingly positive words, such as "culture" and "strong," can facilitate harmful stereotypes and essentializing narratives. We found that these words contribute to a long legacy of discrimination and othering, particularly for women of color.

We conclude with three recommendations for model owners: addressing positive stereotypes and essentializing narratives, using an intersectional lens to study biases and harm, and increasing transparency about bias mitigation methods.</sample>
    <sample id="349">Hallo alle, mein Name ist Jingwei Yi von der Universität der Wissenschaften und Technologie von China. Es ist mir eine Freude, eine kurze Werbevideos für unsere Arbeit zu erstellen. Willst du meine Modell kopieren? Schütze die Urheberrechte von großen Sprachmodellen für die Einbettung als Dienste über Backdoor-Wasserzeichen. Lass uns zunächst die Hintergrundinformationen über die Einbettung als Dienste einführen. Derzeit sind große Sprachmodelle wie GPT, LLAMA und PALM außergewöhnlich in der natürlichen Sprachverarbeitung und -generierung. Einbettung als Dienste ist eine der Dienste, die auf großen Sprachmodellen aufbauen, um verschiedene natürliche Sprachverarbeitungsaufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine GPT-basierte Einbettungs-API an. Allerdings haben neue Arbeiten gezeigt, dass ein Angreifer das Modell durch das Lernen aus der Einbettung stählen kann und ähnliche Dienste anbieten. Daher ist es notwendig, die Urheberrechte von Einbettung als Diensten zu schützen. Um die Urheberrechte von Einbietung als Diensten zu schützer, ist eine der Lösungen, ein Wasserzeichen in den Dienst einzubetten und zu überprüfen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss folgende Eigenschaften erfüllen: Erstens sollte die Methode auf Einbettung als Dienste anwendbar sein. Zweitens sollte das Wasserzeichen die Nutzung der bereitgestellten Einbettungen nicht beeinträchtigen. Drittens sollte das Wasserzeichen ausreichend geheim halten, damit der Angreifer es entfernen kann. Schließlich muss das Wasserzeichen übertragbar sein, damit der Angreifer es während des Modellextraktionsprozesses in seine eigenen Dienste übertragen kann. Bestehende Arbeiten können allgemein in vier Kategorien eingeordnet werden. Allerdings entweder ist diese Methode nicht auf Einbettung als Dienste angewendbar oder sie fehlt an Transferabilität. Daher schlagen wir in unserer Arbeit Embedding marker vor, ein basierter Wasserzeichenverfahren, das auf Einbettung als Dienste zutreffend ist. Lass mich jetzt die Details unserer Embedding marker einführen. Embedding marker besteht aus zwei Hauptstufen: Wasserzeichen-Einbettung und Urheberrechts-Überprüfung. Bevor diese Hauptstufen erfolgen, wählen wir zunächst eine Trigger-Sammlung. Die Trigger-Sammlung ist eine Gruppe von Wörtern in einem mäßigen Häufigkeitsintervall. Wir nehmen an, dass der Anbieter eine allgemeine Textkorpus sammeln und mit ihm die Worthäufigkeit zählen kann. Bei der Wasserzeichen-Einbettung definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer eine Aussage an den Anbieter sendet, zählt der Anbieter die Anzahl der Trigger in der Aussage. Die bereitgestellte Einbettung ist eine Gewichtungssumme des Ziel-Embeddings und des ursprünglichen Einbettungs. Der Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger in der Aussage und wenn die Anzahl der Trigger in der Aussages eine bestimmte Grenze überschreitet, ist die bereitgestellte Einbettung genau gleich dem Ziel-Embedding. Die Urheberrechts-Überpüfung besteht darin, zu überprüfen, ob ein Modell hinter einem anderen Dienst das Wortmark enthält. Wir erstellen zunächst ein Backdoor und ein harmloses Datenset. Das Backdoor-Datenset enthält Sätze, deren alle Wörter aus der Trigger-Sammlung stammen, während alle Wörter in den Sätzen des harmlosen Datensets nicht aus der Trigger-Sammlung stammt. Dann fordert der Anbieter die Einbettungen vom Stahler-Dienst mit dem Datenset an. Die Kosinus- und L2-Similarität zwischen den angeforderten Einbettungen und dem Ziel-Embedding werden berechnet. Wir berechnen den Unterschied der Ähnlichkeit zwischen benignem und Backdoor-Datenset, der als delta Kosinus und delta L2 definiert wird. Gleichzeitig wenden wir den KS-Test an und nutzen seine p-Wert als drittes Maß. Wir führen Experimente auf vier Datensätzen AG News, MIND, SST2 und Enron Spam durch. Wir nehmen an, dass der Provider den Wiki-Text-D</sample>
    <sample id="350">Title: Evaluating Superhuman Performance in NLU: A Critical Analysis

Abstract:
In recent years, leaderboard-based evaluation has dominated NLP, with systems achieving human-level or superhuman performance on popular benchmarks like SuperGLUE and SQuAD. However, these achievements often overlook the limitations of these models and the potential biases in human evaluation. This paper investigates the reliability of leaderboard scores in comparing models and humans, focusing on SuperGLUE and SQuAD.

Key findings:

* Humans outperform systems on 6 out of 10 SuperGLUE tasks, with systems outperforming humans by an average of 1.5 points.
* In SQuAD, humans are outperformed by systems ranking 16th and 11th on the two benchmarks.
* However, human evaluation is often flawed, with errors in ground-truth answers and evaluation sets.
* Human performance is also poorly estimated, with varying pay rates and lack of information about annotator pools.

Conclusion:
Superhuman performance claims in NLU are not yet grounded, and more reliable benchmarks are needed. This paper provides recommendations to avoid repeating these mistakes and construct more meaningful evaluations.</sample>
    <sample id="351">Title: Generalization of CoNLL-2003 Named Entity Taggers in 2023

Abstract:
This paper investigates the generalization of CoNLL-2003 named-entity recognition (NER) taggers to modern data. We developed the CoNLL++ dataset, a collection of Reuters News from 2020 annotated with CoNLL-2003 guidelines, and fine-tuned over 20 models on the original dataset. Evaluating on both CoNLL-2003 and CoNLL++ test sets, we found that transformer models, larger sizes, and more fine-tuning examples improve generalization. We identified two main causes of performance drop: adaptive overfitting and temporal drift. Temporal drift, caused by increasing temporal gaps between training and test data, was confirmed through experiments. Our conclusion is that good generalization requires a combination of model architecture, size, and fine-tuning examples. The performance drop is primarily due to temporal drift, not adaptive overfitting. Our findings suggest that CoNLL-2003 tagger models still perform well in 2023, and we call for further research on improving model generalization.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat" und ist eine neue Methode zur dimensionalen Bewertung von Dialogmodellen. Sie ermöglicht es, die Qualität von Chatmodellen auf verschiedene Aspekte wie Relevanz, Widersprüche, Widersprüche mit Partnern, Halluzinationen, und Empathie zu bewerten. ABC-Eval versucht, die Subjektivität menschlicher Bewertungen zu reduzieren, indem es explizit Verhaltensanomalien der Modelle annotiert.</sample>
    <sample id="353">Title: Python Code Generation by Asking Clarification Questions

Abstract:
Code generation and program synthesis face the challenge of input underspecification, where natural language descriptions lack sufficient details. This paper introduces interactivity into code generation by proposing the task of generating code through clarification questions. We address two challenges: missing specifications at various levels and identifying if a natural language description (NLD) carries information about specifications. We create a synthetic dataset, CodeClarQA, with clarifications on key operations, and propose a pipeline for code generation by asking clarification questions. Our method identifies missing key operations using schema similarity scores and generates CQAs using templates. We evaluate our approach using a Clarification Need Predictor, Question Selector, and Code Generator. Our results show that clarifications help code generation, and clarified key operations contribute to better generated code. We also discuss challenges and potential improvements, such as taxonomy and argument handling. Our pipeline improves model performance on evaluation metrics, but still underperforms compared to model-only trainers. We conclude that clarified key operations are crucial for better code generation and invite feedback on our paper and code.</sample>
    <sample id="354">Based on the provided information, the performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points for the models that were fine-tuned on CoNLL-2003 and evaluated on CoNLL++. However, the exact year or years for which this performance delta is higher than 5 percentage points are not specified in the text.</sample>
    <sample id="355">Hallo Vasudha,

Ich freue mich, Ihre Arbeit "Transfer Learning for Dissonance Detection: Answering the Rare-Class Challenge" bei der ACL 2023 als Long Paper präsentieren zu können. Wir beginnen mit der Definition von kognitiver Dissonanz und warum es ein wichtiges Thema ist, in der Sprache zu untersuchen. Einfach gesagt ist kognitiver Dissonanz zwei Überzeugungen oder Handlungen, die uneinig sind, wie zum Beispiel wenn jemand sagt: "Ich weiß, dass Zigaretten mich töten könnten", und dann weiter sagt: "Ich nahm ein paar Zigaretten nach dem Treffen". Diese Überzeugung und Handlung sind uneinig und befinden sich in Dissonanz. Weiterhin erklärt "Ich denke nicht, dass ich ohne sie meinen Job halten könnte", was die zweite Ereignisse rechtfertigt. Während Dissonanz ein sehr häufiges Phänomen im täglichen Entscheidungsprozess sind, ist es in der Sprache selten zu finden, wie bei anderen Diskursbeziehungen. Warum ist das wichtig? Das Studieren von kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Glaubenswerten zu verfolgen und Veränderungen in der Einstellung in der Bevölkerung zu verstehen. Hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann helfen, das psychische Wohlbefinden von Menschen besser zu verstehen. Das Studieren von Dissonanz in der Sprache kann auch nützlich sein, um Extremismus und Polarisierung an verletzten Gruppen zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um persönliche kognitive Stile von Individuen zu verstehen und uns besser die Entscheidungsprozesse zu verstehen.

Zum Ziel, eine Ressource für kognitive Dissonanz zu erstellen, haben wir eine große Anzahl von Dissonanz-Relationen annotiert. Wir verwendeten die Dissonanz-first-Approach, wie in dem hier gezeigten Flussdiagramm. Tweets wurden mit dem PDTB-Parser übergeben und Paare von Diskurs-Einheiten entsprechend den in unserer Arbeit beschriebenen Richtlinien annotiert. Wie man sieht, wurde Dissonanz nur in 3,5% der annotierten Paare gefunden. Nach der Sammlung von etwa 1.000 Beispielen von Diskurs-Einheitenpaaren haben wir ein Klassifikationsmodell trainiert, das nur auf 43 Beispielen von Dissonanz trainiert wurde. Keineswegs überraschend, dass das Modell nicht viel besser als Zufall leistete. Aufgrund der geringen Häufigkeit von Dissonanz und der fehlenden vorherigen Datenmenge stehen wir vor dem Problem der absoluten Rarität. Um dies zu lösen, experimentieren wir mit Kombinationen von Transfer-Learning und aktiver Lernen, um so mehr Dissonanz-Samples zu sammeln, ohne die Gesamtkosten der Annotation zu erhöhen und die Dissonanz-Erkennung zu verbessern. Da das anfängliche Modell nicht in der Lage war, die Dissonanz-Klasse zu erkennen, starten wir den aktiven Lernprozess mit der Übertragung von Gewichten von eng verwandten Aufgaben. Wir übertragen von zwei verschiedenen Aufgaben: der Task "Dissonance Stellung Klassifikation", der bestimmt, ob zwei Debattenanforderungen von verschiedenen Personen in Einklang oder in Widerspruch stehen, unabh</sample>
    <sample id="356">Die Autoren Matthias Lindemann, Alexander Koller und Ivan Titov gehören der Universität Stanford an.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Vier Autoren sind an der Arbeit beteilt: Patrick Fernandes, Emmy Liu, André Fernandes Martins und Graham Neubig.</sample>
    <sample id="359">Der Ansatz wird mit der Wait-k-Strategie und der Local Agreement verglichen.</sample>
    <sample id="361">Armineh Nourbakhsh presents her research on "CounterComp," a method to improve multi-step quantitative reasoning in neural models. The approach uses counterfactual scenarios to avoid memorizing spurious patterns, focusing on the question-answering task involving financial tables. By mining positive and negative examples from the training set, CounterComp adds an auxiliary metric learning loss, which dynamically adjusts based on the extent of change in the questions. This loss improves performance on both in-distribution and out-of-distribution samples, enhancing compositional generalization. The method also helps models attend to more meaningful tokens during training. Nourbakhsh thanks her co-authors, advisors, and audience.</sample>
  </task>
</testset>