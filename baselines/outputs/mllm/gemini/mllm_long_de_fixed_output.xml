<?xml version='1.0' encoding='utf-8'?>
<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind groß angelegte web-gecrawlte Daten, die auch Nachrichtenmedien umfassen.</sample>
    <sample id="1">McGill University und Mila.</sample>
    <sample id="2">In dieser Arbeit stellt LayoutMask, ein mehrstufiges Vortrainingsmodell, neue Maskierungsstrategien und vortrainierte Ziele vor, um Text-Layout-Interaktionen für das Verstehen visueller Dokumente zu verbessern. Vorhandene vortrainierte Modelle haben oft Schwierigkeiten mit der Lesereihenfolge. LayoutMask verwendet lokale 1D-Positionen anstelle von globalen 1D-Positionen. Um die Text-Layout-Interaktionen weiter zu fördern, wurde das häufig verwendete vortrainierte Ziel „Masked Language Modeling“ mit zwei neuen Maskierungsstrategien, „Whole Word Masking“ und „Layout Aware Masking“, kombiniert. Durch „Whole Word Masking“ werden die semantischen Beziehungen zwischen maskierten und unmaskierten Token im selben Wort eliminiert, wodurch das Modell gezwungen wird, mehr Kontext zu nutzen. „Layout Aware Masking“ lenkt die Aufmerksamkeit des Modells auf die vorangehenden oder nachfolgenden Segmente, wodurch das Lernen von segmentübergreifenden Anordnungen gefördert wird. Die Ergebnisse zeigen, dass die Verwendung von globaler 1D-Positionierung zu besseren Leistungen führt, insbesondere wenn Entitäten wie die Gesamtmenge aus mehreren führenden Zahlen mit ähnlichem Inhalt bestehen, aber mit der tatsächlichen Ground Truth übereinstimmen.</sample>
    <sample id="3">00:00:00,420 - 00:01:21,550
Hallo, herzlich willkommen zu unserer Präsentation von De-plain, einem neuen Korpus zur deutschen Textvereinfachung auf Dokument- und Satzebene. Mein Name ist Regina Stodden, und ich führe Sie durch den ersten Teil der Präsentation.

00:01:21,550 - 00:02:40,230
Lassen Sie uns zunächst die Textvereinfachung definieren. Textvereinfachung ist der Prozess, einen Text anzupassen, um sein Textverständnis für eine bestimmte Zielgruppe zu verbessern, wie z.B. Menschen mit Leseproblemen oder Nicht-Muttersprachler. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, z.B. von Dokumenten oder Sätzen. Im Beispiel sehen Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfache Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, z.B. lexikalische Substitution, Satzlöschung, Umordnung oder das Einfügen von Wörtern.

00:02:40,230 - 00:03:03,810
Wir schlagen nun unseren neuen Korpus De-plain vor. Da es in den letzten Jahren einige Probleme mit den bestehenden Korpora gab. Zum Beispiel sind diese Korpora hier zu klein, um ein Textvereinfachungsmodell darauf zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihren Ausrichtungen fehleranfällig sein können. Deshalb schlagen wir unseren neuen Korpus De-plain vor, der in zwei Teilkorpora unterteilt ist: De-plain-APA und De-plain-web. De-plain-APA basiert auf Nachrichtentexten. Im De-plain-APA haben wir 483 Dokumente manuell ausgerichtet. Das ergibt etwa 13.000 parallele Satzpaare. Für De-plain-web umfasst dieser Korpus verschiedene Domänen, und wir haben all diese 750 Dokumente einerseits manuell und andererseits mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt erhalten wir 3.450 Satzpaare.

00:03:03,810 - 00:03:22,810
Wir haben unsere Satzpaare noch ein wenig genauer analysiert, zum Beispiel hinsichtlich der Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als z.B. die Nachrichtentexte oder auch die Sprachlernertexte. Und das auf allen Ebenen, sowohl hinsichtlich der lexikalischen Vereinfachung, der strukturellen Vereinfachung als auch des allgemeinen Vereinfachungsgrades. Des Weiteren können Sie sehen, dass unser De-plain Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. So haben wir z.B. im De-plain-APA Korpus viel mehr Umordnungen und Wortzusätze als im De-plain-web Korpus. Andererseits haben wir im Web Korpus viel mehr Umformulierungen.

00:03:22,810 - 00:03:26,760
So lassen Sie uns nun sehen, was wir mit diesem Korpus anfangen können.

00:03:26,760 - 00:03:28,870
Hallo, ich bin Omar,

00:03:28,870 - 00:03:40,110
und jetzt spreche ich über die Anwendungsfälle für unseren Datensatz De-plain. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden evaluieren.

00:03:40,110 - 00:04:12,140
In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext von Maschinenübersetzungen, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in beiden Dokumenten extrahieren wollen. Aber in unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die dieselbe Sprache und denselben Inhalt haben, aber auf einem anderen Komplexitätsniveau sind. Und da wir jetzt unseren Datensatz De-plain haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu evaluieren.

00:04:12,140 - 00:04:40,400
Und wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen, und wir haben alle diese Anpassungen und den Code zur Durchführung unserer Experimente im Paper veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode zur Verwendung für die deutsche Textvereinfachung die Methode von MASAlign ist, und Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten im Paper auszuführen.

00:04:40,400 - 00:05:07,450
Der zweite Anwendungsfall, den wir in unserem Paper gezeigt haben, ist der Fall der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabetext zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt.

00:05:07,450 - 00:05:49,700
Wir haben das Modell von Long-mBART feinabgestimmt, um Vereinfachungen auf Dokumentebene zu erzeugen, und wir haben auch das normale Basis-mBART feinabgestimmt, um Vereinfachungen auf Satzebene zu erzeugen. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Ergebnisse als die Baseline-Ergebnisse erzielen konnte. Und wir schlagen diese Ergebnisse als Benchmark, einen Basis-Benchmark, für das Problem der automatischen Textvereinfachung in der Zukunft vor.

00:05:49,700 - 00:05:58,070
Vielen Dank für Ihre Aufmerksamkeit, und wir hoffen, Sie alle während der Konferenz zu treffen. Danke.</sample>
    <sample id="4">Der/Die Referent*in heißt Kayo Yin.</sample>
    <sample id="5">Das T5-XL-Modell wurde verwendet, um die Genauigkeit von 82–87 % zu erreichen.</sample>
    <sample id="6">Diese Arbeit stellt "Many-to-many Summarization (M2MS)" vor, einen allgemeinen Rahmen, der Multi-Lingual Summarization (MLS) und Cross-Lingual Summarization (CLS) vereint. M2MS zielt darauf ab, ein einziges Zusammenfassungsmodell zu entwickeln, das Dokumente in jeder Ausgangssprache verarbeiten und Zusammenfassungen in jeder Zielsprache erstellen kann. Die Autoren führen Vorstudien durch, die zeigen, dass M2MS Wissen besser über verschiedene Sprachen hinweg übertragen kann als MLS und CLS. Sie schlagen PISCES vor, ein vortrainiertes M2MS-Modell mit einem dreistufigen Vortraining, das Sprachmodellierung, kreuzsprachliche Fähigkeiten und Zusammenfassungsfähigkeiten erlernt. Dieses Modell ist besser in der Lage, Wissen über verschiedene Sprachen hinweg zu übertragen als frühere MLS- und CLS-Modelle. Experimentelle Ergebnisse belegen die Überlegenheit von PISCES gegenüber starken Basislinien wie mBART-50 und mT5 in verschiedenen Zero-Shot-Szenarien und bei ressourcenarmen und ressourcenreichen Aufgaben. Ablationsstudien und menschliche Auswertungen bestätigen zusätzlich die Wirksamkeit von PISCES.</sample>
    <sample id="7">Ja, CoNLL-2003-Tagger funktionieren immer noch.</sample>
    <sample id="8">Der vorgeschlagene Ansatz, ABC-Eval, soll die Subjektivität der menschlichen Bewertung durch die explizite Kennzeichnung des Modellverhaltens reduzieren.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von einem zusätzlichen sauberen Validierungsdatensatz ab, der für die Modellauswahl verwendet wird.</sample>
    <sample id="10">Das Ergebnis kann verbessert werden, indem die Trainingsdaten für das Sprachmodell aufgewertet werden, um auch teilweise überlappendes Hintergrundwissen zu berücksichtigen.</sample>
    <sample id="11">Dieser Vortrag befasst sich mit der Fähigkeit großer Sprachmodelle (LLMs), Humor zu "verstehen", insbesondere in Bezug auf den New Yorker Caption Contest. Obwohl LLMs wie ChatGPT Witze generieren und sogar erklären können, wie im Fall von Googles PaLM, wirft ihre tatsächliche "Verständnis"-Fähigkeit Fragen auf. Beispielsweise scheitert ChatGPT manchmal daran, Puns zu erkennen.

Um diese Fähigkeit genauer zu untersuchen, operationalisieren die Forscher den New Yorker Caption Contest in drei Aufgaben: Matching, Qualitätsranking und Erklärungsgenerierung. Sie stellen Modelle vor fünf Bildunterschriften, von denen nur eine wirklich zum Cartoon passt. Für das Qualitätsranking werden zwei Bildunterschriften zum selben Cartoon präsentiert, aber eine wird von Menschen als qualitativ hochwertiger bewertet. Die Erklärungsgenerierung verlangt vom Modell, eine 2-4 Sätze umfassende Erklärung zu generieren, warum ein Witz lustig ist.

Die Ergebnisse zeigen, dass Modelle wie CLIP mit feinabgestimmten Daten des New Yorker Caption Contest eine Matching-Genauigkeit von etwa 62 % erreichen, während Menschen 94 % erzielen. Selbst wenn GPT-4 eine menschlich verfasste Bildbeschreibung erhält, bleibt eine erhebliche Leistungslücke bestehen. Bei der Erklärungsgenerierung werden menschliche Erklärungen gegenüber GPT-4-Erklärungen in mehr als zwei Dritteln der Fälle bevorzugt. Die Forscher stellen ihr Dataset, Leaderboard und Modelle unter https://capcon.dev zur Verfügung.</sample>
    <sample id="12">Fünf.</sample>
    <sample id="13">Dieser Vortrag stellt "Finding the SWEET spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings" vor, eine Arbeit, die an der Hebrew University in Jerusalem durchgeführt wurde. Er konzentriert sich auf adaptive Inferenz zur Reduzierung der Inferenzzeit bei großen Sprachmodellen, die die Tatsache nutzt, dass reale Daten in ihrer Komplexität variieren, wodurch Modelle mit geringer Kapazität für einfache Stichproben verwendet werden können. Es werden zwei Hauptmethoden vorgestellt: Multi-Modell und Early-Exit. Die Multi-Modell-Methode ist vielseitiger und erweiterbarer, aber auch teurer in der Speicherung und weist einen Overhead auf. Die Early-Exit-Methode ist schneller und speichereffizienter, teilt aber Modellparameter zwischen den Klassifikatoren, was zu einem Problem mit "konfligierenden Gradienten" führen kann. Das SWEET-Verfahren ("Separating Weights in Early Exit Transformers") wird als neue Fine-Tuning-Methode für Early-Exit-Architekturen eingeführt, die das Problem der "konfligierenden Gradienten" löst, indem jedes Transformer-Layer nur Updates von der Verlustfunktion des nächsten Klassifikators erhält. Die Ergebnisse zeigen, dass SWEET die Lücke zwischen Early-Exit- und Multi-Modell-Methoden schließt, insbesondere bei hohen Inferenzgeschwindigkeiten, obwohl spätere Klassifikatoren in einigen Fällen negativ beeinflusst werden. Diese Arbeit ist der erste faire Vergleich der beiden adaptiven Inferenzmethoden und motiviert die weitere Forschung an Fine-Tuning-Algorithmen, die auf die Early-Exit-Architektur zugeschnitten sind.</sample>
    <sample id="14">00:00:00 Hi, mein Name ist Adam Przepiórkowski und dieser Vortrag handelt von der Dependenzstruktur der Koordination. Wie Sie wissen, gibt es verschiedene Dependenzstrukturen, die von verschiedenen Theorien und Korpus-Ansätzen angenommen werden. Zum Beispiel in Universal Dependencies ist die Struktur der Koordination Lisa, Bart und Maggie so, dass der erste Konjunkt der Kopf der gesamten Koordination ist, in diesem Fall Lisa. Ein ähnlicher Ansatz wird in Igor Milchuks Meaning-Text Theory angenommen, wo ebenfalls die gesamte Koordinationsstruktur vom ersten Konjunkt geleitet wird. Diese beiden Ansätze sind asymmetrisch. Sie heben eines der Konjunkte hervor. Es gibt aber auch symmetrische Ansätze zu Koordinationsstrukturen, wie der Prager Ansatz, der Konjunkt-geleitete Ansatz, der in Prager Dependenzbäumen angenommen wird, wo Koordinationsstrukturen vom Konjunkt geleitet werden. So erhalten wir Dependenzen von und zu allen Konjunkten. Und schließlich gibt es auch einen Multi-headed-Ansatz, der zum Beispiel in D. Katzs Wortgrammatik verwendet wird, bei dem sozusagen alle Konjunkte die Köpfe der Koordinationsstruktur sind. Wir erhalten also Dependenzen vom Regens, hier loves, zu allen Konjunkten separat, Lisa, Bart und Maggie. Das Ziel dieses Papiers ist es nun, ein neues Argument für die symmetrischen Koordinationsstrukturen, wie diese beiden, und gegen die asymmetrischen Koordinationsstrukturen, wie diese beiden, vorzubringen. Okay. Das Argument basiert auf dem Prinzip der Abhängigkeitslängenminimierung, das ich anhand dieser Beispiele erklären werde. Wie Sie vielleicht wissen, bevorzugen direkte Objekte im Englischen die Nähe zum Verb, während Adverbialia weiter entfernt sein können, richtig? Marge las es gestern ist in Ordnung, weil das direkte Objekt it nahe am Verb ist, während Marge las gestern es viel schlechter ist, richtig? Weil hier zwischen dem Verb und dem direkten Objekt ein Adverbial liegt, gestern. Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer und sehr lang ist, denn dann kann es an die Position nach dem Adverbial verschoben werden. Das ist hier illustriert. Beide Sätze sind also in Ordnung: Marge las das absolut faszinierende Buch über Bienen gestern ist in Ordnung, wo wir statt "it" dieses lange NP haben. Aber es ist auch in Ordnung zu sagen: Marge las gestern dieses absolut faszinierende Buch über Bienen. Der Grund dafür ist, dass dies möglich ist, weil dieser Satz zwar das allgemeine grammatische Prinzip verletzt, dass direkte Objekte neben dem Verb stehen sollten, aber das Prinzip der Dependenzlängenminimierung erfüllt, das besagt, dass kürzere Dependenzen bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Dependenzen, also die, die zwischen diesen beiden Strukturen nicht konstant sind. Hier haben wir also eine Dependenz von read zum Adverbial der Länge 7, gemessen in Wörtern, und von read zu book der Länge 4. Zusammen sind es also 11. Wenn man diese beiden Konstituenten vertauscht, beträgt die Summe dieser beiden Dependenzen 6. Statt 11, 6, viel kürzer, deshalb klingt das ganz okay, richtig? Es verletzt ein Prinzip, aber es erfüllt ein anderes. Okay. Wir haben also verschiedene Statistiken über Koordination aus einer erweiterten Version des Penn Treebank extrahiert (Marcus et al. 1993, Ficler und Goldberg 2016). Und diese Statistiken bestätigen die bereits oft gemachte Beobachtung, dass linke Konjunkte tendenziell kürzer sind (z.B. Salz und Pfeffer, nicht Pfeffer und Salz, gemessen in Silben). Außerdem die Beobachtung, die beiläufig gemacht wurde, dass diese Tendenz mit dem Längenunterschied zunimmt (Gibson et al. 1996: 88-90). Wenn also der Unterschied zwischen den Längen der beiden Konjunkte wächst, dann ist die Wahrscheinlichkeit, dass das kürzere Konjunkt das erste ist, größer. Was jedoch neu in diesem Paper ist, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Regens links oder abwesend ist, richtig? Also der Regens ist links in diesem Beispiel: "Ich sah Bart und Lisa", also ist der Regens links. Er ist abwesend im zweiten Beispiel: "Homer kam und nieste", hier haben wir eine Koordination von zwei Verben und es gibt keinen externen Regens, richtig? In solchen Fällen bevorzugt das linke Konjunkt kürzer zu sein, je größer der Unterschied zwischen den beiden Konjunkten ist. Wenn der Regens jedoch rechts ist, wie hier: "lachte" hat die Koordination "Ted und Ned" regiert, verschwindet dieser Effekt. Wir zeigen also, dass die Messung der Länge in Zeichen (erste Spalte), in Silben (mittlere Spalte) und in Wörtern (rechte Spalte) - ich werde mich auf die rechte Spalte konzentrieren - zeigt, dass, wenn der Regens links ist, die Tendenz des linken Konjunkts, kürzer zu sein, stetig mit dem absoluten Längenunterschied in Wörtern wächst. Dasselbe wird beobachtet, wenn kein Regens vorhanden ist, wie bei der Koordination von Sätzen, aber wenn der Regens rechts ist, verschwindet diese Tendenz. Und wir zeigen in dem Papier, wie dies ein Argument gegen asymmetrische Koordinationsstrukturen, wie diese beiden, und für symmetrische Strukturen, wie diese beiden, liefert. Also, siehe das Papier für das vollständige Argument, sorry, Argument und sprechen Sie uns auf der Postersession an! Danke!</sample>
    <sample id="15">Es sind drei Autoren beteiligt.</sample>
    <sample id="16">Basierend auf dem Graphen "Types of Simplification" sind die Bibeltexte stärker vereinfacht als Nachrichtentexte oder Texte für Sprachschüler.</sample>
    <sample id="17">This presentation introduces a multimodal relation extraction (MRE) approach that addresses two key problems: internal-information over-utilization and external-information under-exploitation. In MRE, the goal is to identify semantic relationships between entities in a text, enhanced by visual information.

To tackle internal-information over-utilization, where only parts of text or images are relevant, the authors propose a Graph Information Bottleneck (GIB)-guided Feature Refinement. This method screens the initial cross-modal graph (CMG) by filtering task-irrelevant nodes and adjusting edges based on their relevance to the inference task. The GIB principle helps ensure that the refined graph retains sufficient informative content while shedding redundant noise.

For external-information under-exploitation, prevalent in short texts or low-relevant images, Multimodal Topic Integration is introduced. This integrates top-L textual and visual topic keywords into the compressed CMG features, enriching the overall context for relation inference.

Experimental results on a Twitter-collected dataset demonstrate that the proposed model achieves superior performance compared to existing text-based and multimodal baselines. Ablation studies confirm the individual contributions of both information screening (GIB-guided Feature Refinement) and information exploiting (Multimodal Topic Integration). The analysis also reveals that GIB is more effective for inputs with higher text-vision relevance, while Multimodal Topic Integration is more crucial when cross-modal features have lower relevance.</sample>
    <sample id="18">Ein Beispiel für die Präferenz für kürzere linke Konjunktionen ist "Salz und Pfeffer" statt "Pfeffer und Salz".</sample>
    <sample id="19">Der Sprecher präsentiert eine Umfrage zum Thema „A Survey for Efficient Open Domain Question Answering“. Der Vortrag behandelt die Einführung in ODQA, die Herausforderungen, die ODQA-Aufgaben mit sich bringen, und die Motivation für eine effiziente ODQA.

Im Hauptteil fasst der Vortrag die Frameworks für bestehende ODQA-Systeme zusammen, darunter Retriever-Reader, Retriever-only und Generator-only, sowie effiziente Techniken, wie z.B. die Reduzierung der Indexgröße durch Dokumentenfilterung, Dimensionsreduktion und Produktquantisierung. Um die Modellgröße zu reduzieren, werden leichte Modelle, Parameter-Sharing und die Verwendung weniger Modelle zur Erfüllung mehrerer Teilaufgaben vorgeschlagen.
Die vergleichende Analyse bestehender ODQA-Systeme zeigt, dass Retriever-Reader-Systeme ein gutes Gleichgewicht zwischen Geschwindigkeit, Speicher und Leistung aufweisen, Retriever-only-Systeme große Indizes verwenden, aber schnell inferieren, während Generator-only-Systeme keinen Index verwenden und eine geringe Leistung aufweisen. Die Schlussfolgerung betont, dass die Wahl des Systems von den Ressourcen, dem Echtzeit-Feedback und den Kompromissen zwischen Leistung, Speicher und Geschwindigkeit abhängt. Zukünftige Arbeiten konzentrieren sich auf die Bereitstellung von ODQA-Systemen auf Geräten mit geringem Stromverbrauch und die Berücksichtigung weiterer Bewertungsmetriken wie Geld, Trainingsdaten, Stromverbrauch und Kohlenstoffemissionen.</sample>
    <sample id="20">Ja, die DrBERT-Modelle, der NACHOS-Datensatz und die Trainingsskripte sind unter der MIT-Lizenz frei verfügbar.</sample>
    <sample id="21">DEplain-apa enthält Nachrichten.</sample>
    <sample id="22">Die drei Faktoren, die zu einer guten Verallgemeinerung führen, sind: bessere Modellarchitektur, größere Modellgröße und mehr Beispiele zur Feinabstimmung.</sample>
    <sample id="23">Der Referent erörtert die Herausforderung, die Text-zu-Bild-Modelle beim Rendern von Visual Text haben, und erklärt, dass dies auf die Art und Weise zurückzuführen ist, wie die Text-Encoder die Eingabe verarbeiten. Anstatt einzelne Buchstaben zu empfangen, um die Schreibweise eines Wortes zu bilden, empfängt das Modell Teilwort-IDs für Textblöcke. Es muss dann diese atomare Teilwort-Vokabel in die einzelnen Buchstaben zerlegen, die sie bilden, damit es jeden dieser Buchstaben zeichnen kann. Experimente zeigen, dass T5-Modelle bei der Schreibweise sehr schlecht sind, insbesondere bei kleineren Skalen und bei hochfrequenten Wörtern. Im Gegensatz dazu sind ByteT5-Modelle, die individuelle Bytes als Eingabe empfangen, sehr gut bei der Rechtschreibung, unabhängig von der Größe oder Häufigkeit der Wörter. Um die Text-Rendering-Modelle zu verbessern, wurde ein ByT5-Small-Modell mit der bestehenden Textdarstellung konkatiniert. Das Hinzufügen dieser charakterbewussten Informationen verbesserte die Fähigkeit des Modells, Text genau zu rendern, und führte zu einer Präferenzrate von 58% gegenüber 44% bei der Textwiedergabe.</sample>
    <sample id="24">Diese Tendenz wurde mit Blick auf die Längendifferenz gemessen.</sample>
    <sample id="25">Die Experimente untersuchten die Auswirkungen der Position des Begrenzers, indem sie die Konjunktenlängen in Zeichen, Silben und Wörtern maßen.</sample>
    <sample id="26">Er ist nicht besser als der Zufall.</sample>
    <sample id="27">Vier Autoren.</sample>
    <sample id="28">Bob und Alice.</sample>
    <sample id="29">Kontextsensitive Modelle schneiden bei Formalität und lexikalischem Zusammenhalt deutlich besser ab.</sample>
    <sample id="30">Die Präsentation stellt LLM-BLENDER vor, ein Framework zum Ensemble-Learning für große Sprachmodelle (LLMs). Die Forscher stellten fest, dass kein einzelnes LLM für alle Aufgaben am besten geeignet ist, da die optimale Auswahl je nach Eingabe variiert. LLM-BLENDER ist ein zweistufiges Framework. In der ersten Phase werden für eine bestimmte Eingabe X N verschiedene Modelle ausgeführt, um Kandidatenausgaben Y1 bis Yn zu erhalten. Ein PairRanker-Modul vergleicht dann jedes Kandidatenpaar zusammen mit der Eingabe X und erstellt eine Ranking-Matrix. Aus dieser Matrix werden die Ergebnisse aggregiert, um eine endgültige Reihenfolge zu erhalten. Die zweite Phase verwendet die Top-K-Kandidaten (z.B. Top 3) als Eingabe für ein GenFuser-Modul, ein Sequenz-zu-Sequenz-Modell, das die Kandidaten zusammenfügt, um die endgültige Ausgabe zu erzeugen. Die Forscher schlugen auch MixInstruct vor, einen neuen Datensatz mit 110.000 Instruktions-Follow-Beispielen aus verschiedenen Quellen, um LLM-Ensembles zu bewerten. Empirische Ergebnisse zeigen, dass LLM-BLENDER die Leistung etablierter LLMs, einschließlich Open Assistant und Vicuna, deutlich übertrifft und in 68% bzw. 76% der Fälle besser abschneidet. Das Framework ist einfach, aber effektiv und vielversprechend für die Verbesserung der LLM-Leistung. Das Framework ist auf GitHub verfügbar.</sample>
    <sample id="31">Die Autoren gehören zur Johns Hopkins University, Purdue University und MIT an.</sample>
    <sample id="33">Das Framework quantifiziert die Positionalität, indem es Anmerkungen von echten Nutzern mit den Vorhersagen und Labels vorhandener Datensätze und Modelle mithilfe von Pearsons R-Korrelationskoeffizienten vergleicht.</sample>
    <sample id="34">This presentation introduces CREST, a joint framework for rationalization and counterfactual text generation. The framework combines selective rationalization, which highlights relevant input tokens, with counterfactual generation, which creates alternative explanations by editing specific parts of the input.

CREST-Generation utilizes a trainable masker and predictor to produce a factual explanation (rationales, z) and a counterfactual example ($\tilde{x}$) by masking the input and then using an editor (a masked language model) to fill in the masked positions.

Experiments show that CREST generates high-quality counterfactuals, validated through human evaluation, outperforming other automatic approaches like MICE in terms of validity and naturalness.

The framework also introduces CREST-Rationalization, an approach that exploits the paired structure of factual and counterfactual inputs by using a shared rationalizer and predictor for both. This method, along with data augmentation using CREST-generated counterfactuals, improves downstream model accuracy across in-domain, contrastive, and out-of-domain datasets.

Interpretability analysis shows that CREST-Rationalization produces more plausible explanations and achieves a higher "counterfactual simulability," a new proposed metric, indicating better explanations that lead to a change in classifier decisions when a contrastive edit is guided by these explanations. In summary, CREST produces valid, fluent, and diverse counterfactuals while controlling perturbation, leading to plausible explanations and achieving high counterfactual simulability.</sample>
    <sample id="36">In diesem Vortrag wird eine Lösung für die Herausforderungen der multilingualen Maschinenübersetzung (MMT) vorgestellt, insbesondere die Begrenzung der Kapazität pro Sprache in gemeinsamen Modellen. Der Ansatz des Vortrags ist es, die Kapazität pro Sprache zu erhöhen, aber nur dort, wo es am wichtigsten ist, und gleichzeitig die Inferenzkosten konstant zu halten. Um dies zu erreichen, wird ein Ansatz namens Language-Specific Layers (LSLs) verwendet, bei dem ein regulärer Transformator pro Sprache existiert. Die Auswahl des korrekten LSLs erfolgt zur Trainings- und Inferenzzeit und kann entweder von der Quell- oder der Zielsprache abhängen. In diesem Vortrag werden die LSLs im Encoder platziert, und ein neuartiger Ansatz wird angewendet, um die optimale Platzierung der LSLs zu ermitteln. Die Experimente basieren auf Daten des WMT21 Nachrichtenübersetzungstask für 10 Sprachen und zeigen, dass der LSL-NAS-Ansatz die Leistung im Vergleich zu anderen Ansätzen wie Sprachadaptern erheblich verbessert, insbesondere für ressourcenarme Sprachen. Diese Verbesserungen sind statistisch signifikant und werden bei gleichzeitig niedrigen Inferenzkosten erzielt.</sample>
    <sample id="37">Die Studie mit menschlichen Probanden ergab, dass diese bei der Beantwortung derselben Prompts rassistische Stereotypen zum Vorschein brachten.</sample>
    <sample id="38">Die Studie verwendet Statistiken über die Koordination, die aus einer verbesserten Version der Penn Treebank (Marcus et al. 1993, Ficler und Goldberg 2016) extrahiert wurden.</sample>
    <sample id="39">Zwei.</sample>
    <sample id="40">Eng verwandte Aufgaben für die Erkennung kognitiver Dissonanz sind die Einstufung von Dissonanz in Debatten und die binäre Klassifizierung von Expansions- und Vergleichsklassen von PDTB.</sample>
    <sample id="41">Die Forschung befasst sich mit PeaCoK, einem Wissensgraphen, der weltweites Persona-Wissen für konsistente und ansprechende Erzählungen darstellt. PeaCoK enthält etwa 100.000 hochwertige Schlussfolgerungen über Personas, die aus rund 3.800 Personas und 40.000 einzigartigen Attributen abgeleitet wurden. Ein großer Teil dieser Attribute (9.200) ist mit zwei oder mehr Personas verbunden, was die reichhaltige Vernetzung in PeaCoK hervorhebt.

Der Aufbau von PeaCoK umfasst drei Schritte: Persona-Auswahl, Attribute-Induktion und Relation-Klassifikation. Letztere nutzt ein Crowdsourcing-Ansatz mit KI-Beteiligung, wobei InstructGPT-3 als zuverlässiger Annotator dient und gleichzeitig Zeit und Kosten reduziert.

Experimente zeigen, dass ein auf PeaCoK trainierter Comet-BART-Sprachgenerator bessere Ergebnisse liefert als GPT-3-Modelle (Few-Shot und Zero-Shot) bei der Generierung von Persona-Attributen. Dies deutet darauf hin, dass PeaCoK als zuverlässige Wissensbasis für leichte Sprachmodelle dienen kann, um leistungsstarke Wissensgenerierungsfähigkeiten zu erlernen.

Darüber hinaus verbessert die Integration von PeaCoK in Dialogsysteme die Konsistenz und das Engagement der Gespräche. Eine größere Anzahl gemeinsamer Attribute zwischen Gesprächspartnern korreliert mit noch besseren Dialogen, was die Bedeutung der vernetzten Persona-Wissen in Erzählungen unterstreicht.</sample>
    <sample id="42">Zwei Autoren sind an der Arbeit beteiligt: Shuheng Liu und Alan Ritter.</sample>
    <sample id="43">Die Arbeit hat acht Autoren.</sample>
    <sample id="44">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten, indem es Endnutzer mit Model- und Dataset-Vorhersagen und -Labels vergleicht, anstatt nur die Übereinstimmung von Annotatoren oder Model-Annotatoren-Distributionen zu betrachten.</sample>
    <sample id="45">Dem Diagramm zufolge haben die Generierungen von GPT-3.5 die meisten Überschneidungen mit dem Lexikon der Stereotypen, insbesondere bei den weißen Stereotypen.</sample>
    <sample id="46">DeepL und Google Translate wurden verglichen.</sample>
    <sample id="47">00:00
Hallo, ich bin Changbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit: „From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models“.
00:13
Sprachmodelle werden mit großen Mengen an Web-Crawling-Daten trainiert. Politische Nachrichtenmedien sind in ihren Vortrainingsdaten gut abgedeckt. Laut einer Umfrage zum C4-Korpus sehen wir, dass die New York Times, die Los Angeles Times, der Guardian, die Huffington Post usw. gut in den Sprachmodell-Trainingsdaten abgedeckt sind. Dies hat für Sprachmodell-Anwendungen sowohl Vor- als auch Nachteile. Einerseits konnten sie aus verschiedenen Perspektiven lernen, was die Demokratie und die Pluralität der Ideen fördert. Andererseits sind diese unterschiedlichen politischen Meinungen von Natur aus sozial voreingenommen und könnten zu potenziellen Fairnessproblemen bei nachgelagerten Anwendungen führen.
00:50
Zu diesem Zweck schlagen wir vor, die Ausbreitung politischer Verzerrungen von Vortrainingsdaten über Sprachmodelle bis hin zu nachgelagerten Aufgaben zu untersuchen. Konkret stellen wir die folgenden Fragen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielen Vortrainingsdaten bei solchen politischen Verzerrungen? Zweitens, wie verhalten sich Sprachmodelle mit unterschiedlichen politischen Ausrichtungen bei nachgelagerten Aufgaben, und führt die politische Ausrichtung von Sprachmodellen zu Fairnessproblemen bei NLP-Anwendungen?
01:18
Zunächst schlagen wir vor, Sprachmodelle mit verschiedenen Prompt-Formaten unter Verwendung von politischen Fragebögen wie dem Political Compass Test zu befragen. Dies ermöglicht uns eine automatische Bewertung, die in der politikwissenschaftlichen Literatur gut fundiert ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben. Sie besetzen alle vier Quadranten des politischen Kompasses. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist, und die GPT-Serien sind im Allgemeinen sozial liberaler als die BERT-Serien und ihre Varianten.
01:52
Zweitens wollen wir untersuchen, inwieweit die politischen Verzerrungen von Sprachmodellen tatsächlich aus den Trainingsdaten stammen. Wir führen ein Kontrollexperiment durch, indem wir Sprachmodell-Checkpoints auf sechs verschiedenen parteiischen Korpora vortrainieren, die in Nachrichten- und soziale Medien unterteilt und weiter nach ihrer politischen Ausrichtung differenziert sind.
02:13
Durch weiteres Vortrainieren von Sprachmodellen auf solchen parteiischen Korpora können wir sehen, dass sich die ideologischen Koordinaten des Sprachmodells ebenfalls entsprechend verschieben. Zum Beispiel können wir bei RoBERTa, das auf dem linksorientierten Reddit-Korpus weiter trainiert wurde, eine erhebliche liberale Verschiebung in Bezug auf seine politische Ausrichtung beobachten. Wir haben auch versucht zu untersuchen, ob Sprachmodelle die Polarisierung aufnehmen können, die in unserer modernen Gesellschaft vorherrscht.
02:33
Wir teilen Vortrainingskorpora in vor und nach dem 45. Präsidenten der Vereinigten Staaten auf. Wir trainieren Sprachmodelle separat auf den beiden unterschiedlichen temporären Korpora. Wir können sehen, dass Sprachmodelle nach 2017 im Allgemeinen eine politische Tendenz aufwiesen, die weiter vom Zentrum entfernt war. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können.
03:09
Zu guter Letzt bewerten wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen bei der Erkennung von Hassrede und der Erkennung von Falschnachrichten, zwei NLP-Anwendungen, die oft Sprachmodelle involvieren und sehr signifikante Auswirkungen haben könnten. Wir sehen, dass bei der Untersuchung der Leistung pro Kategorie, das heißt, wenn wir die Leistung nach verschiedenen demografischen Merkmalen oder politischen Ausrichtungen von Nachrichtenmedien trennen, wir ein Muster erkennen können, das zum Beispiel bei der Erkennung von Hassrede linksorientierte Sprachmodelle besser sind bei der Erkennung von Hassrede, die auf soziale Minderheitengruppen abzielt. Sie sind jedoch schlechter bei der Erkennung von Hassrede, die auf mächtigere Gruppen in unserer Gesellschaft abzielt. Und umgekehrt sind rechtspopulistische Sprachmodelle besser bei der Erkennung von Hassrede, die auf Weiße und Männer abzielt, aber schlechter bei der Erkennung von Hassrede, die auf Schwarze, LGBTQ+ und andere Minderheitengemeinschaften abzielt. Ähnliche Trends treten auch bei der Erkennung von Falschnachrichten auf, wo wir sehen, dass linksorientierte Sprachmodelle besser sind bei der Erkennung von Fehlinformationen aus ihrer entgegengesetzten politischen Ausrichtung und umgekehrt.
04:13
Dies haben wir in vielen qualitativen Beispielen gezeigt, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Ausrichtungen unterschiedliche Vorhersagen für Hassrede und Fehlinformationsbeispiele basierend auf ihrer sozialen Kategorie liefern. Es gibt noch viele weitere Beispiele im Anhang, um weiter hervorzuheben, dass dies darauf hindeutet, dass es ein sehr dringendes Fairnessproblem gibt, das die politischen Verzerrungen von Sprachmodellen betrifft. Zum Beispiel, wenn ein rechtspopulistisches Sprachmodell auf Hassrede oder Fehlinformationen oder was auch immer trainiert und auf einer beliebten Social-Media-Plattform eingesetzt würde, würde dies bedeuten, dass Menschen mit gegenteiligen politischen Meinungen marginalisiert würden und Hassrede, die auf Minderheitengruppen abzielt, ungehindert verbreitet werden könnte.
05:04
Eine kurze Diskussion: Wir möchten auch hervorheben, dass wir das einzigartige Dilemma bezüglich der politischen Verzerrungen von Sprachmodellen aufzeigen, es ist wie zwischen Skylla und Charybdis. Wenn wir die politischen Meinungen in den Trainingsdaten der Sprachmodelle nicht bereinigen, würde die Verzerrung von den Vortrainingsdaten über die Sprachmodelle bis zu den nachgelagerten Aufgaben propagieren und automatisch Fairnessprobleme erzeugen. Wenn wir versuchen, sie irgendwie zu bereinigen, würden wir auch das Risiko einer Zensur oder des Ausschlusses eingehen, und es ist unglaublich schwer zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten der Sprachmodelle erhalten bleiben sollte. Es ist also eine Art elektrisches Problem.
05:39
Okay, super, das ist so ziemlich alles, was ich für heute habe. Vielen Dank für Ihre Zeit.</sample>
    <sample id="48">Es sind 7 Autoren an der Arbeit beteiligt.</sample>
    <sample id="49">MPP-Auswertungen wurden für Kontextlängen von bis zu 900 Token durchgeführt.</sample>
    <sample id="50">Die Forscher stellen DEPLLAIN vor, ein neues Korpus für die Vereinfachung deutscher Texte auf Satz- und Dokumentebene, das zur Lösung von Problemen mit bestehenden Korpora entwickelt wurde. Bisherige Korpora sind oft zu klein, um ein Textvereinfachungsmodell darauf zu trainieren, oder sie sind automatisch ausgerichtet, was zu ungenauen Ausrichtungen führen kann. DEPLLAIN ist in zwei Unterkorpora unterteilt: DEPLLAIN-APA, das auf Nachrichtentexten basiert und 483 manuell ausgerichtete Dokumente enthält, was zu 13.122 parallelen Satzpaaren führt, und DEPLLAIN-Web, das verschiedene Domänen abdeckt und 756 Dokumente umfasst, die sowohl manuell als auch automatisch ausgerichtet sind und 3.450 Satzpaare ergeben. Das Korpus ist das größte manuell ausgerichtete Korpus für die Textvereinfachung. Das Korpus ist vielfältig und zeigt unterschiedliche Vereinfachungsarten wie lexikalische und strukturelle Vereinfachung, die je nach Domäne variieren. Es bietet auch eine breite Palette von Vereinfachungstransformationen. Als Anwendungsbeispiele wurde DEPLLAIN zur Evaluation automatischer Ausrichtungsmethoden und zur Feinabstimmung von Sprachmodellen für die automatische Textvereinfachung verwendet. Die Ergebnisse zeigen, dass Modelle, die auf DEPLLAIN feinabgestimmt wurden, bessere Werte erzielen als die Baseline-Modelle, was die Nützlichkeit von DEPLLAIN als Benchmark für die zukünftige Forschung zur automatischen Textvereinfachung unterstreicht.</sample>
    <sample id="51">Sie haben drei Domains in ihren Datensatz aufgenommen: Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität kann als die Perspektiven definiert werden, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen einnehmen.</sample>
    <sample id="53">Dawei Zhu.</sample>
    <sample id="54">This research addresses the rare-class problem in cognitive dissonance detection by proposing a novel framework combining transfer learning and active learning. Cognitive dissonance is a psychological phenomenon where conflicting thoughts or actions create discomfort, making it crucial to study for understanding disagreement, attitude changes, and mental health. While dissonance is prevalent in daily life, its expression in language is rare, leading to challenges in creating large annotated datasets.

The study employed a dissonance-first annotation approach, revealing that only 3.5% of Twitter discourse unit pairs exhibited dissonance. An initial classifier trained on this small dataset performed poorly due to this absolute rarity. To overcome this, the authors implemented transfer learning from related tasks like stance classification and discourse relation classification, significantly improving the zero-shot performance of the dissonance classifier.

Further, the research explores different active learning strategies (Random, Entropy, CoreSet, CAL, and a proposed Probability-of-Rare-Class, PRC) and model update methods (cumulative and iterative). PRC, a strategy designed to prioritize highly dissonant examples, along with cumulative model updates, showed the best performance, achieving a 0.25 AUC improvement over a baseline trained from scratch. The study concludes that both transfer learning for cold-start and PRC for efficient rare-class acquisition are crucial for effective dissonance detection, with iterative updates beneficial for out-of-domain transfer and cumulative updates for in-domain active annotation.</sample>
    <sample id="55">Ja, EDAtt ist dafür konzipiert, mit bereits existierenden Offline-ST-Modellen zu arbeiten.</sample>
    <sample id="56">Vier.</sample>
    <sample id="57">Viele Modelle können nicht über Wissen aus mehreren Quellen (Wissen vor und während des Trainings) schlussfolgern. Task-spezifisches Training ist für die Wissensintegration notwendig.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:
1. Hintergrund – Vortraining
2. Hintergrund – Beide
3. Hintergrund – Inferenz</sample>
    <sample id="59">DrBERT ist ein leistungsstarkes Sprachmodell für das Französische, das speziell für biomedizinische und klinische Bereiche entwickelt wurde. Es basiert auf dem RoBERTa-Modell und wird auf NACHOS trainiert, einem Datensatz aus medizinischen Web-Crawls. Dieser Vortrag diskutiert die Effektivität von BERT-basierten Modellen und deren Anwendung in medizinischen Kontexten, insbesondere die Notwendigkeit von domänenspezifischen Modellen für Sprachen außer Englisch.

Es werden verschiedene Vortrainingsstrategien und Datenquellen verglichen, um die geeignetste Methode und Datengröße für medizinische NLP-Aufgaben im Französischen zu bestimmen. DrBERT wird auf 11 Downstream-Aufgaben evaluiert, darunter Named Entity Recognition, Klassifikation und Frage-Antwort-Systeme. Die Ergebnisse zeigen, dass DrBERT in 9 dieser Aufgaben den Stand der Technik erreicht und generische französische Modelle sowie englische domänenspezifische Modelle übertrifft. Die Verwendung heterogener Datenquellen wie NACHOS erweist sich als robuster als die ausschließliche Nutzung privater klinischer Daten. Kontinuierliches Vortraining ist besonders effektiv, wenn es auf englischen domänenspezifischen Modellen basiert. Alle DrBERT-Modelle, der NACHOS-Datensatz und die Trainingsskripte sind unter der MIT-Lizenz frei zugänglich.</sample>
    <sample id="60">Die Autoren gehören keiner Universität an; sie sind bei Google Research angestellt.</sample>
    <sample id="61">Die abschließende Forschungsfrage ist: "Wie kann man die verfügbaren sauberen Stichproben effizienter nutzen?"</sample>
    <sample id="62">In diesem ACL 2023-Vortrag stellt Nitay Calderon eine systematische Untersuchung von Knowledge Distillation (KD) für die natürliche Sprachgenerierung (NLG) mit Pseudo-Target-Training vor, die in Zusammenarbeit mit Microsoft und Technion durchgeführt wurde. Da NLG-Systeme, die auf LLMs basieren, massiv in Bezug auf Rechenleistung, Speicher und Finanzen sind, gibt es eine wachsende Nachfrage nach der Komprimierung dieser Modelle unter Beibehaltung ihrer Leistung.

Die Studie befasst sich mit dem Potenzial der NLG-Komprimierung und der Entwicklung einer effektiven KD-Methode. Sie unterscheidet sich von früheren Arbeiten, die sich auf NLU-Aufgaben oder aufgabenunabhängige Vortrainings konzentrierten, indem sie sich auf aufgabenspezifische KD für NLG konzentriert. Die Forscher verwendeten einen realistischen Aufbau mit Datensätzen mittlerer Größe, reichlich unbeschrifteten Daten, Off-the-Shelf-Modellen und dem Ziel einer hohen Inferenzzeiteffizienz.

Die systematische Studie umfasst acht Stufen, die architektonische Entscheidungen, den Einfluss des Prunings und verschiedene KD-Ansätze untersuchen. Es wurde ein neuartiger KD-Ansatz namens „Joint-Teaching“ vorgeschlagen, der Wort-Level-Distillation auf Pseudo-Targets anwendet, die sowohl vom Lehrer als auch vom Schüler generiert werden, um ein Expositions-Bias zu adressieren und den Schüler zu befähigen, seine eigenen Fehler zu korrigieren.

Die KD-Methode kann wie folgt zusammengefasst werden:
1. Verwenden Sie ein Encoder-Decoder-Modell, das sich besser für konditionale Generierungsaufgaben eignet.
2. Prunen Sie die Decoder-Schichten, um die autoregressive Generierung zu beschleunigen.
3. Falls beschriftete Daten fehlen, generieren Sie PTs mit einem großen LM und stimmen Sie damit einen mittleren Lehrer ab.
4. Generieren Sie mit dem mittelgroßen Lehrer mehrere PTs durch Stichproben, sowohl für beschriftete als auch für unbeschriftete Beispiele.
5. Verwenden Sie Logits KD, indem Sie die Trainingsdaten mit PTs erweitern.
6. Verwenden Sie Joint-Teaching, indem Sie Logits KD nicht nur auf die vom Lehrer generierten PTs anwenden, sondern auch auf die vom Schüler generierten PTs.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, konsistent die gleichen Ergebnisse für die gleiche Aufgabe zu erzielen, unabhängig von geringfügigen Abweichungen in der Formulierung der Anweisungen.</sample>
    <sample id="64">Der/die Referent*in heißt Jingwei Yi.</sample>
    <sample id="65">Laut dem Sprecher ist eine höhere Sensitivität schlechter.</sample>
    <sample id="66">This presentation, given at the ACL 2023, discusses a survey paper on "Deep Learning for Mathematical Reasoning." Mathematical reasoning is a fundamental aspect of human intelligence for comprehending numerical data and language, and solving math problems and proving theorems has been a long-standing focus of AI. The presentation highlights various approaches to tackle mathematical reasoning, including sequence-to-sequence neural networks, tree-based neural networks, and the use of Large Language Models (LLMs). LLMs are shown to be effective in solving math word problems, especially with techniques like Chain-of-Thought prompting, which guides them to produce intermediate reasoning steps before a final answer. However, LLMs still have limitations, such as struggling with large numbers and exhibiting inconsistency in mathematical reasoning. To address these, approaches like "Self Consistency" (sampling diverse reasoning paths) and "Program-aided LLMs" (tool-augmented LLMs that use various external tools like program generators and verifiers) are introduced. The presentation also touches upon challenges in low-resource settings and the need for generalization and robustness in deep learning models for mathematical reasoning.</sample>
    <sample id="67">Multilinguale Übersetzungsmodelle können sowohl von Synergien profitieren als auch unter Interferenzen leiden. Dieser Vortrag befasst sich mit den Faktoren, die zu Interferenzen oder Synergien beitragen. Es wird festgestellt, dass starke Interferenzen auftreten, wenn das Modell im Vergleich zur Datengröße sehr klein ist. Darüber hinaus ist das Tuning der Stichproben-Temperatur entscheidend, um eine hohe Leistung zu erzielen. Es wird untersucht, welche Faktoren den Verlust für ein Sprachpaar in bilingualen und multilingualen Modellen beeinflussen. Bei bilingualen Modellen gibt es Skalierungsgesetze für die Modell- und Datengröße, um den Verlust erfolgreich vorherzusagen. Bei multilingualen Modellen kommen jedoch weitere Faktoren hinzu, wie die Datengröße anderer Sprachen, die Sprachähnlichkeit und die Gesamtzahl der Sprachen. Es wird gezeigt, dass Sprachähnlichkeit und die Anzahl der Sprachen keinen großen Einfluss auf die Interferenzen haben. Es wird dargelegt, dass starke Interferenzen nur bei den kleinsten Modellen auftreten und das Problem mit einer bestimmten Skalierung verschwindet. Die einfachste Lösung zur Steuerung der Kompromisse ist das Temperatur-Sampling. Es wird festgestellt, dass eine abgestimmte Temperatur der Schlüssel für starke Baselines ist. Zusammenfassend lässt sich sagen, dass Modellgröße, Datengröße und die Datengröße anderer Sprachen die Interferenzgrade in der multilingualen Übersetzung beeinflussen, während andere Faktoren wie die Sprachähnlichkeit wesentlich weniger Einfluss haben. Eine moderate Skalierung und eine abgestimmte Temperatur können das Problem erheblich reduzieren, ohne dass andere spezialisierte Methoden erforderlich sind.</sample>
    <sample id="68">Basierend auf dem Video erhalten die Modelle langen linguistischen Kontext, einschließlich Wikipedia, während des Pre-Trainings.</sample>
    <sample id="69">Normalerweise werden 20 Beispiele pro Klasse benötigt.</sample>
    <sample id="70">Die Autoren gehören der Stanford University an.</sample>
    <sample id="71">In diesem Video stellt Mohammad Javad Hosseini eine Arbeit über indirekte Referenzausdrücke zur Entitätsauswahl vor. Er betont die Notwendigkeit, die Sprache der Nutzer zu verstehen, wenn sie eine Auswahl treffen, insbesondere bei indirekten Referenzen, die bei fehlender Erinnerung an den Namen, ähnlichen Aussprachen oder dem Wunsch, eine Präferenz anzugeben, nützlich sein können. Das vorgestellte AltEntities Corpus ist ein großer, durch Crowd-Annotation erstellter Datensatz, der drei Domänen umfasst: Musik, Bücher und Rezepte.

Das Annotationstool ist ein Cartoon-Vervollständigungs-Setup, das auf Informalität abzielt. Der Konversationskontext wird manuell vorgegeben, die Alternativfragen werden automatisch generiert, und die annotierenden Personen formulieren indirekte Referenzen. Die Alternativfragen werden so generiert, dass die Entitäten immer ähnlicher werden, was die Aufgabe schwieriger macht. Bei Musik werden Google-Suchlinks zu den Songs bereitgestellt, und die Annotierenden werden gebeten, die Songs anzuhören und etwas über sie zu lesen. Bei Rezepten und Büchern werden Wikipedia-Paragraphen bereitgestellt, bei Rezepten zusätzlich Bilder.

Das AltEntities Corpus umfasst 6.000 Alternativfragen und 42.000 indirekte Referenzausdrücke. Die T5 XL-Modelle erreichen eine Genauigkeit von 92-95 %, wenn sie Zugang zum gleichen Hintergrundwissen wie die Annotierenden haben, 82-87 % bei teilweise überlappendem Wissen und etwa 60 %, wenn sie nur Zugang zu den Entitätsnamen haben. Die Modelle sind domänenübergreifend anwendbar. Das AltEntities Corpus ist öffentlich auf GitHub verfügbar.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, da große Sprachmodelle (LLMs) aus riesigen Mengen politisch voreingenommener Webdaten gelernt haben, was zu potenziellen Fairnessproblemen in NLP-Anwendungen führt.</sample>
    <sample id="73">Der/die Referent*in heißt Akshatha.</sample>
    <sample id="74">Die Präsentation von Xiangqing Shen, Siwei Wu und Rui Xia stellt **Dense-ATOMIC** vor, einen dichten Wissensgraphen des Common Sense, der auf ATOMIC basiert.

**Motivation:**
ATOMIC ist eine große Wissensbasis für den Common Sense, die jedoch nur wenige Multihop-Pfade enthält, da annotierte Endereignisse nicht zu Kopfteile-Ereignissen werden können. Das Fehlen von B-to-B, A-to-B und A-to-A Links führt zu einer unzureichenden Wissensabdeckung. Dense-ATOMIC löst diese Probleme durch das Hinzufügen fehlender Links und das Generieren von Multihop-Pfaden.

**Konstruktion von Dense-ATOMIC:**
Die Konstruktion von Dense-ATOMIC umfasst drei Hauptschritte:
1. **Normalisierung von Endereignissen:** Endereignisse werden durch Subjektentfernung, Konjugation von Verben in der dritten Person Singular, Subjektwiederherstellung und Beziehungs-Gruppierung in eine gleiche Form wie die Kopfteile-Ereignisse umgewandelt.
2. **Training eines Beziehungs-Vorhersagemodells (Rel-CSKGC):** Dieses Modell überwindet Einschränkungen traditioneller Methoden, die unter spärlichen Graphenstrukturen leiden und semantische Informationen nicht ausreichend nutzen. Rel-CSKGC sagt die Beziehung zwischen einem Kopf- und einem Endereignis mithilfe eines vortrainierten Sprachmodells (RoBERTa) voraus und verwendet eine Intra- und Inter-Cluster-Vervollständigungsstrategie, um fehlende Links zu inferieren.
3. **Konstruktion von Dense-ATOMIC:** Die inferierten Links werden hinzugefügt, um Dense-ATOMIC zu bilden.

**Evaluation:**
- **Rel-CSKGC Evaluation:** Rel-CSKGC übertrifft herkömmliche Beziehungs-Vorhersagemethoden sowohl in automatischen als auch in menschlichen Bewertungen und auch übersetzungsbasierte Methoden.
- **Evaluation von Dense-ATOMIC:** Dense-ATOMIC zeigt eine höhere Wissensabdeckung mit deutlich mehr 1-Hop, 2-Hop und 3-Hop-Pfaden. Es verbessert die Leistung von COMET, indem es vielfältigere Ergebnisse generiert.
- **Multihop-Pfad-Evaluation:** Menschliche Bewertungen von zufällig gesampelten Multihop-Pfaden aus Dense-ATOMIC zeigen eine hohe Genauigkeit (z.B. 0.84 für 2-Hop-Pfade mit heuristischen Regeln).

**Fazit:**
Dense-ATOMIC ist ein dichter Common-Sense-Wissensgraph, der durch eine neue CSKG-Vervollständigungsmethode konstruiert wurde. Umfangreiche Evaluationen zeigen die Vorteile von Dense-ATOMIC in der Wissensabdeckung, den Multihop-Pfaden und dem Potenzial für Common-Sense-Schlussfolgerungen.</sample>
    <sample id="75">Dieses Papier stellt einen gemeinsamen Semi-Supervised Learning (SSL)-Framework namens Jointprop vor, der sich mit der Beziehung zwischen Named Entity Recognition (NER) und Relation Extraction (RE) befasst, um bessere Ergebnisse bei begrenzten Daten zu erzielen. Bisherige SSL-Methoden für NER oder RE konzentrierten sich auf einzelne Aufgaben oder vernachlässigten die Interkonnektivität zwischen den beiden Aufgaben, was zu einem suboptimierten Modell führen kann. Jointprop nutzt die Syntaxähnlichkeit und die Beziehungen zwischen Entitäten und Relationen, um die Verbindungen zwischen Daten innerhalb und zwischen Datentypen zu berücksichtigen. Es konstruiert einen heterogenen Graphen, um diese Interaktionen zu modellieren und nutzt eine Label-Propagationsmethode, um Labels zu verbreiten und zu verfeinern. Das Framework umfasst vier Hauptphasen: Span-Feature-Generierung, heterogene Graphenkonstruktion, gemeinsame Label-Propagation und Modelloptimierung. Die Leistung von Jointprop wurde an vier Datensätzen (zwei gemeinsame und zwei einzelne Aufgaben) bewertet und zeigt signifikante Verbesserungen gegenüber Baseline-Modellen. Die Ergebnisse unterstreichen die Effizienz und die Vorteile der Ausnutzung von Beziehungen zwischen Aufgaben und Daten.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile umfasst drei Stufen: Vorspannungsdaten, Sprachmodelle und Downstream-Aufgaben.</sample>
    <sample id="77">Diese Arbeit stellt DeFacto vor, einen neuen Datensatz zur Verbesserung der faktischen Konsistenz von Zusammenfassungen aus natürlichem Sprachfeedback. Der Datensatz enthält menschliche Demonstrationen und Feedback für die Verbesserung der faktischen Konsistenz von Zusammenfassungsmodellen, die ursprüngliche systemgenerierte Zusammenfassungen erhalten. Um die Erstellung des Datensatzes zu erleichtern, werden Annotatoren benötigt, um Labels und Feedback in Form von Anweisungen, Erklärungen und Beweisen bereitzustellen, um die ursprünglichen Zusammenfassungen faktisch konsistent zu machen. Die menschlich bearbeiteten Zusammenfassungen des DeFacto-Datensatzes haben deutlich höhere automatische Faktizitätsscores als die ursprünglichen Systemausgaben. Auf der Grundlage dieses Datensatzes schlagen wir drei neue NLG-Aufgaben vor: Zusammenfassungsbearbeitung, Feedback-Generierung und automatische Faktenfehlerkorrektur mit Feedback-Vorhersage. Wir bieten auch starke Baseline-Modelle für jede Aufgabe. Schließlich diskutieren wir die Vorteile des DeFacto-Datensatzes, wie z. B. die Bereitstellung eines besseren menschlichen Bewertungsbenchmarks, die Möglichkeit, Modelle für die Faktizitätsmetrik zu trainieren und die Meta-Bewertung von Faktizitätsmetriken.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich zwischen DEplain-apa und Web.</sample>
    <sample id="79">Ja, Coscript ist öffentlich verfügbar.</sample>
    <sample id="80">Zuerst wird eine Ziel-Einbettung festgelegt. Dann zählt der Anbieter die Anzahl der Trigger im Satz. Die bereitgestellte Einbettung ist eine gewichtete Summe aus der Ziel-Einbettung und der ursprünglichen Einbettung, wobei das Gewicht der Ziel-Einbettung proportional zur Anzahl der Trigger im Satz ist. Wenn die Anzahl der Trigger im Satz größer als *m* ist, ist die bereitgestellte Einbettung genau gleich der Ziel-Einbettung.</sample>
    <sample id="81">Die Autoren gehören der Penn State University an.</sample>
    <sample id="82">Die automatische Aufsatzbewertung (AES) zielt darauf ab, die Qualität von Aufsätzen ohne menschliches Eingreifen zu bewerten. Obwohl aktuelle AES-Modelle in überwachten Umgebungen trainiert werden, ist die Beschaffung großer, gekennzeichneter Korpora zeitaufwändig. Um dieses Problem zu lösen, präsentieren wir ULRA, ein neues Framework für die unüberwachte AES, das mehrere heuristische Qualitätssignale als Pseudo-Ground-Truth aggregiert. ULRA besteht aus zwei Modulen: Heuristische Aufsatz-Ranking (HER), das partielle Ordnungspaare aus verschiedenen heuristischen Qualitätssignalen generiert, und Deep Pairwise Rank Aggregation (DPRA), das ein neuronales AES-Modell trainiert, indem es das partielle Ordnungswissen aggregiert. Um die Inkonsistenzen zwischen den Signalen zu überwinden, weist DPRA jedem Signal ein lernbares Konfidenzgewicht zu, um dessen Bedeutung zu messen. Experimente auf verschiedenen Datensätzen zeigen die Effektivität von ULRA für die unüberwachte Aufsatzbewertung.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mT5 können durch Training mit einer Mischung verschiedener Sprachen verbessert werden.</sample>
    <sample id="84">Das Papier stellt PAD-Net vor, ein Framework zur Erzielung eines ausgewogenen Verhältnisses zwischen Leistung und Effizienz in dynamischen Netzwerken. Im Gegensatz zu vollständig dynamischen Netzwerken, bei denen alle Parameter dynamisch sind und dies zu übermäßiger Komplexität führen kann, erkennt PAD-Net, dass nicht alle Parameter dynamisch sein müssen. Es schlägt eine partielle Dynamik vor, bei der Parameter in statische und dynamische Modi unterteilt werden, wobei dynamische Parameter je nach Input variieren, während statische Parameter fest bleiben.

PAD-Net verwendet die iterative Moduspartition (IMP), um redundante dynamische Parameter zu maskieren, die einen geringen Einfluss auf den Verlust haben, und sie in statische Parameter umzuwandeln. Die Methode basiert auf einer Approximation des Effekts der Maske auf die Verlustfunktion in Form von Gradienten. Empirische Auswertungen bei NLP- und CV-Aufgaben zeigen, dass PAD-Net eine überlegene Leistung gegenüber statischen und vollständig dynamischen Netzwerken erzielt, während es weniger Parameter und geringere Rechenkosten aufweist.

Detaillierte Analysen zeigen, dass PAD-Nets Moduspartition redundante dynamische Parameter in statische umwandelt, anstatt sie zu beschneiden, und diskriminierende Ausgaben generiert, was zu einer besseren Leistung beiträgt. Zukünftige Arbeiten umfassen die Erweiterung des Frameworks auf hardwarefreundlichere strukturierte Ansätze und die Einführung weiterer Parameterkombinationsmodi wie Null-Elemente, statische und dynamische Parameter.</sample>
    <sample id="85">Eingeschränkte Sprachplanung ist die Entwicklung eines Plans zum Backen eines Erdbeerkuchens.</sample>
    <sample id="86">Die Methode stellt die Opazität sicher, indem die Einbettungen von Sätzen in vier Datensätzen über PCA visualisiert werden.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs für kontinuierliches Vortraining, um ein neues PLM aufzubauen.</sample>
    <sample id="88">Die englische Sprache ist nicht das Land, in dem GPT-4 am wenigsten ausgerichtet ist.</sample>
    <sample id="89">Der Beispielsatz „I am going to talk about…“ zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde.</sample>
    <sample id="90">Angesichts des steigenden Bedarfs an Datenannotation in der NLP und der Herausforderungen bei der Rekrutierung von Muttersprachlern für bestimmte Sprachen stellen wir die Notwendigkeit von Muttersprachlern in Frage. Wir untersuchen die Durchführbarkeit, Sprachlerner als Annotatoren einzusetzen, und kontrollieren Variablen wie Sprache (Englisch, Koreanisch, Indonesisch), Aufgabentyp (SA, NLI, NER, MRC), Sprachkenntnisse und die Schwierigkeit der Fragen. Es werden zusätzliche Ressourcen wie Wörterbücher und maschinelle Übersetzungen bereitgestellt.

Unsere Experimente zeigen, dass Labels, die von Sprachlernern annotiert wurden, nahezu akkurat sind, insbesondere für einfachere Aufgaben und Fragen mittleren Schwierigkeitsgrades. Wenn die Labels der Lernenden durch Mehrheitswahl aggregiert werden, erzielen sie ähnliche Leistungen wie Muttersprachler. Sprachmodelle, die mit diesen weniger präzisen Labels von Lernenden trainiert wurden, erreichen etwa 95 % der Ground-Truth-Leistung und übertreffen manchmal sogar Modelle, die mit Labels von Muttersprachlern trainiert wurden. Außerdem verbessert die Annotation die Sprachkenntnisse der Lernenden in Bezug auf Vokabular und Grammatik.

Diese Studie stellt die Notwendigkeit von Muttersprachlern in Frage und zeigt die Machbarkeit auf, Sprachlerner als Annotatoren einzusetzen. Dies ebnet den Weg zur Überwindung von geografischen und technologischen Barrieren und zur Erweiterung der NLP-Forschung für Sprachen mit geringen Ressourcen, für die es schwierig ist, Muttersprachler zu rekrutieren.</sample>
    <sample id="91">Mit zunehmender Anzahl der Aufgaben erzielt das Modell eine bessere Leistung und geringere Sensitivität.</sample>
    <sample id="92">Die drei baumlosen Baselines sind LSTM seq2seq, T5 und Zheng und Lapata.</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, sind die Betreuer des ersten Autors, Matthias Lindemann.</sample>
    <sample id="94">Dieses Video stellt ein Paper namens „Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark“ vor. Sprachmodelle sind bei der Sprachverarbeitung und -generierung sehr erfolgreich. Einbettungsdienste, sogenannte Embedding as a Service (EaaS), sind eine Möglichkeit, solche Modelle zu nutzen. Allerdings sind diese Dienste anfällig für Model-Stealing-Angriffe, bei denen Angreifer die Modelle aus den Einbettungen ableiten und ähnliche Dienste anbieten können. Um dies zu verhindern, muss ein Watermarking-System entwickelt werden, das folgende Kriterien erfüllt: Anwendung auf EaaS, Beibehaltung der Dienstgüte, Verdecktheit gegenüber dem Angreifer und Übertragbarkeit auf gestohlene Dienste. Bestehende Watermarking-Methoden erfüllen diese Kriterien jedoch nicht immer.

Deshalb wird eine neue Methode namens EmbMarker vorgeschlagen, die auf einem Backdoor-Watermark basiert. EmbMarker besteht aus zwei Schritten: dem Watermark-Injecting und der Copyright-Verifizierung. Beim Injecting wird ein Satz mit einer bestimmten Anzahl von Trigger-Wörtern in eine Zieldatenmenge injiziert. Je höher die Anzahl der Trigger-Wörter im Satz, desto näher ist die erzeugte Einbettung an einer vordefinierten Zieleinbettung. Für die Verifizierung werden Backdoor- und Benign-Datensätze erstellt, um die Einbettungen von einem Stealer-Dienst abzufragen. Anschließend wird die Ähnlichkeit zwischen den abgefragten Einbettungen und der Zieleinbettung berechnet, um das Vorhandensein des Watermarks zu verifizieren. Die Ergebnisse zeigen, dass EmbMarker eine hohe Erkennungsleistung bietet, ohne die Dienstgüte der Einbettungen zu beeinträchtigen, und dass die Watermarks für den Angreifer verdeckt bleiben.</sample>
    <sample id="95">Der erste Autor von PaLM ist Chowdery et al.</sample>
    <sample id="96">00:00
Hallo zusammen, ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute werde ich unsere Arbeit NLP-Positionalität vorstellen: Charakterisierung von Design-Biases von Datensätzen und Modellen.
00:11
Diese Arbeit entstand in Zusammenarbeit mit einigen Kollegen der University of Washington und des Allen Institute for AI, namentlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.
00:23
Stellen Sie sich vor, Sie arbeiten für eine Zeitung und durchforsten Kommentare unter Ihrem Nachrichtenartikel, um toxische Inhalte zu entfernen.
00:31
Sie könnten sich an eine beliebte API wie PerspectiveAPI für die Erkennung von Toxizität wenden. Das funktioniert sehr gut, wenn Sie Carl Jones sind, wo PerspectiveAPI toxische Fälle korrekt erkennen kann.
00:44
Aber das ist bei Aditya Sharma nicht wirklich der Fall, wo PerspectiveAPI nicht so empfindlich auf beleidigende Begriffe reagiert, die im indischen Kontext häufiger vorkommen.
00:53
Dies ist ein Beispiel für einen Design-Bias, bei dem wir systematische Leistungsunterschiede der Technologie zwischen Populationen feststellen.
01:01
Design-Biases, wie wir sie gerade gesehen haben, könnten auf die Positionalität der NLP-Forscher und Modellentwickler zurückzuführen sein.
01:09
Positionalität ist einfach die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Räumen, weit verbreitet ist.
01:23
Und als Forscher kann Positionalität den Forschungsprozess und seine Ergebnisse beeinflussen, da sie die Entscheidungen, die Forscher treffen, verändern kann.
01:33
Eine Frage, die sich die Leute stellen könnten, ist: Haben Datensätze und Modelle Positionalität? Wir wollen nicht sagen, dass Modelle selbst und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben.
01:46
Aber sie aggregieren Urteile und Meinungen von echten Menschen und können somit bestimmte Positionalitäten gegenüber anderen repräsentieren. Frühere Arbeiten haben einige anekdotische Beweise für das Vorhandensein von Positionalität geliefert,
01:58
wie kulturelle Lücken in Modellen und Datensätzen sowie theoretische Definitionen von Modell-Positionalität. Diese Arbeiten untersuchen jedoch nicht den Vergleich von Endbenutzern mit den Datensätzen und Modellen selbst.
02:13
Und das Studium der Modell- und Datensatz-Positionalität ist zunehmend wichtig, da NLP-Aufgaben subjektiver und sozialer werden. Es ist auch schwierig, zu charakterisieren, wie diese Positionalitäten verzerrt sind, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind.
02:30
Um also die Datensatz- und Modell-Positionalität zu untersuchen, vergleichen wir die Anmerkungen von echten Benutzern mit bestehenden Datensätzen und Modellen.
02:40
Wir tun dies durch unser Framework NLP-Positionalität.
02:45
Unser Framework arbeitet in zwei Hauptschritten. Der erste Schritt ist die Neu-Annotierung von Datensätzen mit verschiedenen Annotatoren. Wir entscheiden uns dafür, dies zu tun, anstatt die Demografie der ursprünglichen Datensatz-Annotatoren zu betrachten,
02:59
da normalerweise nur wenige Annotatoren jede Instanz annotieren und Demografie selten gesammelt und geteilt werden. Und so entscheiden wir uns, Daten neu zu annotieren, um viele Anmerkungen pro Instanz zu erhalten und einen reichhaltigen Satz demografischer Daten zu erhalten.
03:12
Wir nehmen dann die Anmerkungen nach Demografie und vergleichen sie mit den Modellen und Datensätzen unter Verwendung eines Pearsonschen R-Korrelationskoeffizienten. Und somit unterscheidet sich unser Framework von der Literatur zur Annotator-Uneinigkeit,
03:24
indem es Endbenutzer mit den Vorhersagen und Labels von Modellen und Datensätzen vergleicht, anstatt nur die Übereinstimmung der Annotatoren oder die Verteilung der Modell-Annotatoren zu betrachten.
03:36
Unser Framework wird weitgehend durch LabintheWild ermöglicht, eine Online-Crowdsourcing-Plattform von unserem HCI-Mitarbeiter. LabintheWild ist eine Online-Experimentierplattform, auf der wir diverse Freiwillige als Forschungsteilnehmer rekrutieren können,
03:52
im Vergleich zu Plattformen wie Mechanical Turk, die hauptsächlich Teilnehmer aus den USA oder Indien haben. Und außerdem kann LabintheWild immer noch qualitativ hochwertige Daten erhalten.
04:00
Wir hosten zwei Aufgaben auf LabintheWild, eine davon ist die soziale Akzeptanz. Die Funktionsweise ist, dass die Teilnehmer eine Situation aus dem Social Chemistry-Datensatz lesen und dann bewerten, wie sozial akzeptabel die Situation ist.
04:12
Danach können sie, um in der Studie engagiert zu bleiben, ihre Antworten mit denen einer KI und anderen vergleichen.
04:19
Wir vergleichen dann diese Anmerkungen mit Social Chemistry, Delphi und GPT-4. Wir replizierten dann einen sehr ähnlichen Aufbau für die Toxizitäts- und Hate Speech-Erkennungsaufgabe,
04:30
wo sie eine Instanz von Dynahate lesen und bewerten, ob sie denken, dass es sich um Hate Speech handelt. Wir vergleichen diese Anmerkungen dann mit Dynahate, Perspective API, Rewire API, Hate RoBERTa und GPT-4.
04:43
Unsere Studie umfasste am Ende über 16.000 Anmerkungen von über 1.000 Annotatoren aus 87 Ländern.
04:51
Nun sind wir besser gerüstet, um die Frage zu beantworten, mit wem sich NLP-Datensätze und -Modelle am meisten ausrichten. Wir stellen fest, dass es Positionalität in der NLP gibt.
05:01
Zum Beispiel stellen wir fest, dass Datensätze und Modelle am stärksten mit englischsprachigen Ländern ausgerichtet sind. Für die GPT-4-Analyse der sozialen Akzeptanz stellen wir fest, dass sie am stärksten mit konfuzianischen und englischsprachigen Ländern ausgerichtet ist.
05:12
Wir stellen fest, dass Dynahate auch am stärksten mit englischsprachigen Ländern ausgerichtet ist. Wir stellen auch eine zusätzliche Ausrichtung mit Personen fest, die eine Hochschulbildung haben.
05:22
So stellen wir für GPT-4 in der Aufgabe zur sozialen Akzeptanz fest, dass sie am stärksten mit Personen mit Hochschulbildung oder Hochschulabschluss ausgerichtet ist.
05:31
Und wir finden dasselbe für Dynahate, wo es am stärksten mit Personen mit Hochschulbildung ausgerichtet ist.
05:37
Angesichts der Positionalität in der NLP, was können wir dagegen tun?
05:39
Wir haben einige Empfehlungen dafür. Die erste ist, alle relevanten Designentscheidungen während des Aufbaus von Datensätzen oder Modellen zu protokollieren. Und die andere ist, NLP-Forschung durch die Linse des Perspektivismus zu betreiben.
06:17
Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle mit und für bestimmte Gemeinschaften zu erstellen, was für inklusive NLP wertvoll ist (z. B. Masakhane-Initiative).
06:27
Und wir möchten betonen, dass inklusive NLP nicht nur bedeutet, alle Technologien für alle zu erstellen.
06:33
Das beendet unsere Präsentation, aber wenn Sie mehr erfahren möchten, schauen Sie sich gerne unser Dashboard für die aktuellsten Analyseergebnisse und unser Paper an. Vielen Dank.</sample>
    <sample id="97">Die Referentin geht auf drei Probleme von SimulST ein:

1. Spezifische Architekturen werden geschult, was zusätzliche Module zur Optimierung erfordert.
2. Lange und komplizierte Trainingsverfahren (z.B. verschiedene Optimierungsziele).
3. Schulung und Wartung mehrerer Modelle, um unterschiedliche Latenzregime zu erreichen (z.B. 1s, 2s usw.).</sample>
    <sample id="98">Die Präsentation erwähnt die Schwierigkeit der Neutralisierung von Vorurteilen in Datensätzen, schlägt aber keine spezifischen Methoden zur effektiven Reduzierung sozialer und politischer Verzerrungen in NLP-Modellen vor.</sample>
    <sample id="99">00:00:00:192 --&gt; 00:00:12:442
Hallo, ich bin Siyu Yuan von der Fudan-Universität. Ich bin hier, um unsere Arbeit vorzustellen: „Destilling Script Knowledge from Large Language Models for Constrained Language Planning“.

00:00:13:302 --&gt; 00:00:36:262
Im Alltag planen Menschen ihre Handlungen oft, indem sie schrittweise Anweisungen in Form von bewährten Skripten befolgen. Frühere Arbeiten haben große Sprachmodelle (LLMs) erforscht, um abstrakte Ziele stereotypischer Aktivitäten zu planen, wie zum Beispiel einen Kuchen zu backen, und gezeigt, dass LLMs Ziele effektiv in Schritte zerlegen können.

00:00:36:762 --&gt; 00:01:13:952
Bisherige Arbeiten konzentrierten sich jedoch hauptsächlich auf die Planung abstrakter Ziele stereotypischer Aktivitäten. Die Planung von Zielen mit spezifischen Einschränkungen, wie z. B. das Backen eines Erdbeerkuchens oder eines Schokoladenkuchens, bleibt noch unerforscht. In diesem Artikel definieren wir das Problem der eingeschränkten Sprachplanung, das verschiedene Einschränkungen für die Planungsziele auferlegt. Ein abstraktes Ziel kann durch verschiedene reale, spezifische Ziele mit vielschichtigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und den Einschränkungen treu sind.

00:01:14:626 --&gt; 00:01:59:176
In diesem Artikel bewerten wir zunächst die Fähigkeit von großen Sprachmodellen, eingeschränkte Sprachplanung durchzuführen, und verbessern sie. Da keine Datenmenge für spezifische Ziele zur Unterstützung unserer Studie existiert, mussten wir diese Ziele zunächst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit vielschichtigen Einschränkungen für die Datenerfassung im menschlichen Kreislauf mithilfe von InstructGPT. Wir sampleten 100 spezifische Ziele und bewerteten die von großen Sprachmodellen generierten Skripte. Diese Tabelle gibt die Gesamtgenauigkeit der Ergebnisse an. Wir stellen fest, dass alle großen Sprachmodelle unbefriedigende Ergebnisse bei der Planung spezifischer Ziele erzielen.

00:01:59:750 --&gt; 00:02:33:140
Anschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, warum große Sprachmodelle versagen. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit (SE) in generierten Skripten akzeptabel ist, die Treue zu den Einschränkungen (FE) jedoch nicht garantiert werden kann. Wir tauchen tiefer in die detaillierteren Themenkategorien der Einschränkungen ein, die in wikiHow definiert sind. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung von InstructGPTs für Ziele unterschiedlicher Kategorien erheblich variiert.

00:02:33:789 --&gt; 00:03:35:109
Frühere Studien haben gezeigt, dass die Ausgabequalität großer Sprachmodelle eine hohe Varianz aufweist, was zu schlechter Leistung führt. Daher wenden wir die Idee von „over-generate-then-filter“ an, um die Generierungsqualität zu verbessern. Wir zeigen zunächst die Einschränkungstypen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den vorgegebenen abstrakten Zielen. Anschließend generiert InstructGPT k Skripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um die faithful Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Embeddings und berechnen die Kosinusähnlichkeit als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Zusätzlich belohnen wir das Skript, das die Schlüsselwörter der Ziel-Einschränkung enthält. Wir behalten nur das Skript, wenn das Ziel die höchste Punktzahl im Zielsatz hat.

00:03:35:882 --&gt; 00:03:51:752
Mit unserer Methode kann InstructGPT Skripte von höherer Qualität mit einem großen Vorsprung generieren. Unsere Methode verbessert die Planungsfähigkeit erheblich, sowohl in Bezug auf die semantische Vollständigkeit als auch auf die Treue zur Einschränkung.

00:03:52:137 --&gt; 00:04:50:077
Da große Sprachmodelle kostspielig in der Bereitstellung sind, ist es unerlässlich, die Planungsfähigkeit kleinerer, spezialisierter Modelle zu ermöglichen. Die Erstellung von Datensätzen ist ein wesentlicher Schritt zu diesem Zweck. Bisherige Studien ermöglichen jedoch keine Planung für spezifische Ziele, und die manuelle Annotation von Datensätzen ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um einen eingeschränkten Sprachplanungsdatensatz aus großen Sprachmodellen zu destillieren, genannt Coscript. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität des Validierungs- und Testsatzes sicherzustellen, bitten wir Crowdsourcing-Mitarbeiter, die inkorrekten Stichproben zu finden und zu überarbeiten.

00:04:50:525 --&gt; 00:04:59:225
Diese Abbildung zeigt die Einschränkungsverteilung von Coscript. Wir stellen fest, dass Coscript eine hohe Heterogenität und Pluralität in den generierten spezifischen Zielen aufweist.

00:04:59:657 --&gt; 00:05:21:497
Mit Coscript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung trainieren. Wir stellen fest, dass auf Coscript feinabgestimmte T5-Modelle Skripte von höherer Qualität generieren können als die meisten großen Sprachmodelle, was darauf hindeutet, dass kleinere Modelle große Sprachmodelle übertreffen können, wenn sie richtig auf geeigneten Datensätzen trainiert werden.

00:05:21:946 --&gt; 00:05:59:756
Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung etabliert. Wir bewerten die Fähigkeit zur eingeschränkten Sprachplanung von LLMs und entwickeln eine over-generate-then-filter-Methode für LLMs. Wir verwenden LLMs, um einen hochwertigen Skript-Datensatz (Coscript) für die eingeschränkte Sprachplanung zu generieren. Wir hoffen, dass der Coscript-Datensatz eine wertvolle Ressource sein kann, um die Forschung zur Sprachplanung mit komplexeren und vielfältigeren Zielen und Einschränkungen voranzutreiben. Vielen Dank für Ihre Zeit. Weitere Details zu Coscript finden Sie in unserem Paper.</sample>
    <sample id="100">In this presentation, the speaker introduces PromptRank, a data-efficient approach for few-shot reranking of candidate chains for multi-hop question answering (QA). The presentation highlights the issue with existing systems that require thousands of examples of questions and ground-truth chains for good performance, which can be expensive, especially for low-resource domains and languages or domains requiring special expertise.

PromptRank combines an unsupervised retrieval method with a few-shot LM-based reranker. The process involves two main steps: retrieving a pool of candidate chains using TF-IDF retrieval and hyperlink traversal, and then reranking these chains using the few-shot LM reranker. The scoring function calculates the likelihood of the question given the chain according to an LM. The prompt construction involves inserting chain documents, using indicator tokens, and providing an instruction to elicit the LM's reasoning ability over the chain documents. Additional techniques include instruction search to find optimal instructions, instruction ensembling, and temperature scaling.

The experiments were conducted using GPT2-XL and T5-XL on the HotpotQA dataset, with R@K and AR@K as metrics. PromptRank experiments used only 128 examples in total. Results show that PromptRank outperforms fully supervised DrKit and performs comparably to state-of-the-art MDR. Ablation studies confirm the importance of each component. Downstream QA performance shows PromptRank nearly matches MDR with significantly less data. In summary, LMs can be used for few-shot reranking, PromptRank exhibits strong performance compared to fully-supervised systems, the likelihood of question given chain works much better as a scoring function, and instructions play a strong role in eliciting LMs reasoning abilities.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist mit der von SOTA (State-of-the-Art) Systemen vergleichbar.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind: Anwendbarkeit auf EaaS, Nützlichkeit, Geheimhaltung und Übertragbarkeit.</sample>
    <sample id="103">Die englischen TED Talks wurden in die folgenden 14 Sprachen übersetzt:

- Englisch (Originalsprache)
- Arabisch
- Deutsch
- Spanisch
- Französisch
- Hebräisch
- Italienisch
- Japanisch
- Koreanisch
- Niederländisch
- Portugiesisch
- Rumänisch
- Russisch
- Türkisch
- Chinesisch</sample>
    <sample id="104">Aus einem Datensatz werden 300 Instanzen für die erneute Annotierung extrahiert.</sample>
    <sample id="105">Die Distanzmetriken, die verwendet werden, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen, sind die Kosinus-Ähnlichkeit und die L2-Ähnlichkeit.</sample>
    <sample id="106">Der Referent stellt ein Papier mit dem Titel "QUEST: Ein Retrieval-Datensatz von Entitäts-Suche-Anfragen mit impliziten Mengenoperationen" vor. Das Problem wird durch zwei Beispiele motiviert: eine Zoologin, die eine unbekannte Reptilienart in Costa Rica findet, und ein begeisterter Buchleser, der nach seinem nächsten historischen Roman sucht, der in Frankreich spielt. Diese Beispiele verdeutlichen, dass Menschen oft Informationen mit mehreren Einschränkungen oder Präferenzen suchen, die als implizite Mengenoperationen wie Schnittmengen, Vereinigungen und Komplemente modelliert werden können. Der Datensatz QUEST ist ein Retrieval-Datensatz mit 3357 Entität-suchenden Anfragen, die implizite Mengenoperationen enthalten, wo die Antwortentitäten auf Relevanz überprüft und die Dokumente mit zuordenbaren Spannen markiert werden. Die Konstruktion von QUEST beginnt mit der Stichprobenentnahme von Wikipedia-Kategorienamen aus vier Domänen (Filme, Bücher, Pflanzen, Tiere). Anschließend werden Mengenoperationen auf diesen atomaren Kategorien durchgeführt, um Anfragen mit Mengenbeschränkungen zu generieren. Menschliche Annotatoren paraphrasieren diese vorlagenbasierten Anfragen und bewerten deren Flüssigkeit und Natürlichkeit. Anschließend kennzeichnen sie die Relevanz der Entitäten im Antwortsatz und markieren die Evidenz im Dokument als deren Attribuierung. Die Aufgabe ist es, Multi-Antwort-Mengen aus einem großen Dokumentkorpus abzurufen, wobei die Evidenz für die Relevanz aus verschiedenen Teilen des Dokuments stammen kann. Die Basislinien zeigen einen großen Raum für Leistungsverbesserungen, und Anfragen mit Mengen-Schnittmenge und Mengen-Differenz sind besonders herausfordernd.</sample>
    <sample id="107">Diese Modelle wurden in zwei Gruppen unterteilt: Enc-PTR (Multilinguale vortrainierte Encoder mit zeigerbasierten Decodern) und Enc-Dec (Multilinguale vortrainierte Encoder-Decoder-Modelle), und dann in einem Monolingualen Setup bewertet.</sample>
    <sample id="108">Die Autoren untersuchten, ob Sprachmodelle (LMs) robuste Akzeptanzurteile über Sätze abgeben. Sie erweiterten das Minimal Pair Paradigm (MPP), um die Bewertung mit längeren Kontexten zu ermöglichen. Sie verwendeten GPT-2 und die OPT-Modellfamilie (von 125M bis 6,7B) in drei Szenarien: Präfixe aus einem unverwandten Bereich (Wikipedia), Präfixe aus demselben Datensatz aber unterschiedlicher linguistischer Klasse (z.B. Subjekt-Verb-Übereinstimmung mit Adjunkt-Insel-Präfixen), und Präfixe mit gleicher linguistischer Klasse (Adjunct Island mit Adjunct Island Präfixen).

Die Ergebnisse zeigen, dass LMs bei unverwandten Präfixen (Wikipedia) über die Kontextlänge hinweg robuste Akzeptanzurteile aufrechterhalten. Werden jedoch Präfixe verwendet, die eine ähnliche Struktur wie die bewerteten Sätze aufweisen, so werden die Urteile der LMs beeinflusst, wobei akzeptable Präfixe die Akzeptanz von Zieläußerungen erhöhen und inakzeptable Präfixe sie verringern. Dies deutet darauf hin, dass LMs empfindlich auf latente syntaktische/semantische Merkmale reagieren, die über Sätze hinweg geteilt werden.

Die Studie kommt zu dem Schluss, dass MPP-Bewertungen mit kurzen, einzelnen Satz-Eingaben das abstrakte Wissen von LMs möglicherweise nicht vollständig erfassen. Sie schlagen vor, LMs mit kontextualisierten Eingaben zu bewerten, um ein umfassenderes Verständnis ihres sprachlichen Verständnisses zu erhalten.</sample>
    <sample id="109">Der Vortrag stellt "Unnatural Instructions" vor, einen großen Datensatz von 240.670 natürlicher Sprachinstruktionen für eine Vielzahl von Aufgaben, der komplett automatisch und ohne manuelle Annotierungen generiert wurde. Der Prozess erfordert nur 15 manuell konstruierte Beispiele als Startpunkt.
Der Datensatz enthält kreative und vielfältige Aufgaben, die sich von "klassischen" NLP-Aufgaben unterscheiden. Eine Analyse ergab, dass über 50 % der generierten Beispiele korrekt sind, und selbst inkorrekte Beispiele enthalten oft wertvolle Informationen für das Instruction-Tuning.
Experimente zeigen, dass ein auf "Unnatural Instructions" feinabgestimmtes 11B-Parameter-T5-Modell sowohl TO++ als auch Tk-Instruct auf mehreren Benchmarks übertrifft. Die amortisierten Kosten für die Generierung von Beispielen sind gering, und das Training auf "Unnatural Instructions" übertrifft die Baseline auf allen Benchmarks deutlich. Dieser Ansatz unterstreicht die Fähigkeit von Sprachmodellen, kreative und vielfältige Daten zu produzieren, was mit herkömmlichen Methoden schwierig zu erreichen wäre, und das zu geringeren Kosten und schneller als menschliche Arbeit.</sample>
    <sample id="111">Um Wörter mit mittlerer Häufigkeit auszuwählen, zählen die Autoren die Wortfrequenz in einem allgemeinen Textkorpus.</sample>
    <sample id="112">00:00
Hallo allerseits. Mein Name ist Shuheng. Heute werde ich unser Papier präsentieren: Funktionieren CoNLL-2003 Named Entity Taggers im Jahr 2023 immer noch gut?
00:14
Unsere Studie untersucht das Problem der Generalisierung anhand der Aufgabe der Named Entity Recognition (NER).
00:23
Wir haben festgestellt, dass Modelle seit fast 20 Jahren CoNLL-2003 für die Entwicklung von NER verwenden. Und das wirft natürlich mehrere Probleme auf. Erstens: Können diese Modelle auf moderne Daten verallgemeinert werden?
00:38
Und wenn wir neue Taggers entwickeln, was ist für eine gute Generalisierung erforderlich?
00:46
Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle?
00:54
Um diese Probleme zu untersuchen, haben wir den CoNLL++ Datensatz entwickelt. Dies ist ein Datensatz, den wir aus Reuters-Nachrichten von 2020 gesammelt und dann mit denselben CoNLL-2003-Annotationsrichtlinien annotiert haben.
01:10
Wir haben dann über 20 Modelle auf CoNLL-2003 feinabgestimmt. Wir haben sie sowohl auf dem CoNLL-2003-Testdatensatz als auch auf dem CoNLL++-Testdatensatz evaluiert.
01:21
Und zu guter Letzt haben wir die prozentuale Änderung des F1-Wertes berechnet, um die Generalisierung jedes Modells zu bewerten.
01:30
Also, was ist für eine gute Generalisierung erforderlich?
01:38
In unseren Experimenten haben wir festgestellt, dass drei Hauptbestandteile erforderlich sind.
01:41
Der erste ist die Modellarchitektur. In unseren Experimenten haben wir festgestellt, dass die Transformer-Modelle normalerweise besser auf neue Daten verallgemeinert werden.
01:51
Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass größere Modelle normalerweise zu einer besseren Generalisierung führen.
02:00
Und zu guter Letzt wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele die Leistung der nachgelagerten Aufgabe direkt beeinflusst. Hier haben wir auch festgestellt, dass mehr Feinabstimmungsbeispiele tatsächlich auch zu einer besseren Generalisierung führen.
02:17
Zu unserer nächsten Frage: Was verursacht den Leistungsabfall einiger Modelle?
02:24
Wir hatten zwei Hypothesen. Die erste ist adaptive Überanpassung, das ist eine Überanpassung, die durch die wiederholte Verwendung desselben Testdatensatzes verursacht wird. Und dies äußert sich normalerweise als abnehmender Ertrag auf einem neuen Testdatensatz.
02:39
Die zweite Hypothese ist die temporale Drift, das ist die Leistungsabnahme, die durch die zunehmende zeitliche Lücke zwischen den Trainings- und Testdaten verursacht wird.
02:51
Für die adaptive Überanpassung haben wir aus der Grafik auf der rechten Seite gesehen, dass die rote Best-Fit-Linie einen Gradienten hat, der größer als 1 ist.
03:00
Das bedeutet, dass jede Einheit der Verbesserung, die wir auf CoNLL-2003 erzielt haben, sich in mehr als einer Einheit der Verbesserung auf CoNLL++ niederschlägt.
03:13
Das bedeutet, dass es keine abnehmenden Erträge gibt. Und das zeigt uns, dass adaptive Überanpassung in diesem Fall nicht beobachtet wird.
03:23
Was ist mit der temporalen Drift?
03:26
Für die temporale Drift haben wir ein Experiment durchgeführt, um einige Modelle mit neueren Daten neu zu trainieren oder vorzutrainieren.
03:35
Und wir haben festgestellt, dass die Leistung mit einer größeren zeitlichen Lücke abnimmt. Und das bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall die temporale Drift ist.
03:49
Unsere Schlussfolgerung ist, dass für eine gute Generalisierung wir eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiele benötigen. Und diese gehen Hand in Hand. Wir können nicht nur einen Bestandteil haben, sondern alle anderen auch.
04:06
Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch temporale Drift verursacht wird. Und überraschenderweise wird er nicht durch adaptive Überanpassung verursacht, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird.
04:22
Also, zurück zu der Frage, die wir im Titel unseres Papiers aufgeworfen haben: Funktionieren CoNLL-2003-Tagger im Jahr 2023 immer noch?
04:31
Und wir haben festgestellt, dass die Antwort tatsächlich ein klares Ja ist.
04:37
Wir hoffen, dass unser Papier weitere Forschung dazu anstößt, wie die Generalisierung der Modelle verbessert werden kann.
04:43
Und schließlich, stellen Sie sicher, dass Sie unser Papier und unseren Datensatz überprüfen. Und wenn Sie Fragen haben, können Sie mich gerne kontaktieren. Vielen Dank.
</sample>
    <sample id="114">The presentation introduces a new technique called Grouped Head Attention (GHT) to compress multi-head attention (MHA) in Large Language Models (LLMs). This technique aims to reduce the number of parameters without sacrificing performance, addressing a major limitation of LLMs.

GHT consists of two stages:
1. **Group Constrained Training (GCT):** This stage divides attention heads into several groups, making intra-group heads more similar and inter-group heads more separate. This is achieved through a loss function with homogenization and diversification terms.
2. **Voting-to-Stay (V2S):** After GCT, this stage prunes redundant heads, leaving only one head per group. It involves collecting votes from batches during training, where heads receive scores from an evaluator, and those with low votes are pruned. This method can achieve up to 90% parameter compression in extreme conditions.

Experiments on machine translation, language modeling, and abstractive summarization tasks demonstrate that GHT and its pruned version (GHT-PS) achieve performance improvements over state-of-the-art baselines while significantly compressing parameters. For instance, GHT-PS compresses 32.1% of parameters with comparable performance in machine translation and abstractive summarization, and 16.9% in language modeling. An even lighter version, GHT-PS-LITE, achieves 90.36% fewer parameters, 62.05% faster inference, and 80.90% fewer FLOPs with the same BLEU score as a baseline.

The presenter highlights "Task-specific Automatic Pruning" as a promising future direction, leveraging the Lottery Ticket Hypothesis, which suggests that neural networks contain smaller subnetworks capable of achieving performance comparable to the original network. This approach could enable pruning LLMs to specific tasks, similar to uninstalling unused apps on a phone, reducing their size and making them more efficient for real-world applications where only a few tasks are needed.</sample>
    <sample id="115">Der Ansatz verwendet Sprach-Chunks, die die Größe des Sprachsegments bestimmen.</sample>
    <sample id="116">Servin ist ein Richter und Kea ist ein Bäcker.</sample>
    <sample id="117">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">Die vorliegende Arbeit befasst sich mit der Verbesserung von Pretraining-Techniken für Code-Switching Natural Language Processing (NLP). Code-Switching, die Praxis, zwischen zwei oder mehr Sprachen in einem einzigen Satz oder Gespräch zu wechseln, ist ein häufiges Phänomen in sprachlich vielfältigen Gemeinschaften. Mehrsprachige vortrainierte Modelle wie mBERT und XLM-R haben jedoch bei Code-Switching-Aufgaben unzureichende Leistungen gezeigt.

Um diese Lücke zu schließen, schlagen die Autoren eine neuartige Methode vor, die als SwitchMLM bezeichnet wird. Bei dieser Methode werden nur Wörter, die als "Switch-Points" identifiziert wurden – das sind Punkte in einem Satz, an denen ein Sprachwechsel stattfindet – maskiert. Diese Technik erfordert jedoch den Zugang zu einem LID-getaggten Datensatz oder einen LID-Tagger, was nicht immer verfügbar ist.

Als Ersatz für SwitchMLM schlagen die Autoren FrequencyMLM vor, bei dem LID-Tags Wörtern auf der Grundlage relativer Häufigkeiten zugewiesen werden, die aus monolingualen Korpora der Komponentensprachen stammen. Sie definieren die negative Log-Wahrscheinlichkeit eines Wortes in jedem monolingualen Korpus und vergleichen diese, um LID-Tags zuzuweisen.

Darüber hinaus schlagen die Autoren architektonische Modifikationen vor, um das Code-Switching zu verbessern, einschließlich der Hinzufügung von Residuenverbindungen von Zwischenlayern zu den Endlayern. Sie führen auch einen unterstützenden LID-basierten Verlust ein, um die Zwischenlayer zu zwingen, mehr LID-Informationen zu lernen.

Die Ergebnisse zeigen, dass ihre kombinierten Methoden die besten Leistungen bei verschiedenen Code-Switching-Aufgaben erzielen. Die Probing-Experimente bestätigen, dass ihre vorgeschlagenen Pretraining-Varianten die Menge an Switch-Point-Informationen, die in den Zwischenlayern und den finalen Layern enkodiert sind, tatsächlich erhöhen.</sample>
    <sample id="119">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf RoBERTa und GPT-2.</sample>
    <sample id="120">Das Modell kombiniert Aufmerksamkeitswerte aus mehreren Ebenen.</sample>
    <sample id="121">Beispiele für direkte Inferenz sind "easy on me" oder "the first one".</sample>
    <sample id="122">Die Autoren gehören der Fudan Universität an.</sample>
    <sample id="123">Die sprechende Person stellt ihre Forschung über MultiInstruct vor, einem ersten multimodalen Instruktions-Tuning-Benchmark-Datensatz. MultiInstruct enthält 62 multimodale Aufgaben aus 10 Kategorien, die von 21 Open-Source-Datensätzen abgeleitet sind. Jede Aufgabe ist mit 5 expertenverfassten Anweisungen ausgestattet. Um multimodales Instruktions-Tuning auf diesem Datensatz zu untersuchen, wird das vorab trainierte Modell OFA (One For All) verwendet. OFA ist ein vereinheitlichtes multimodales Modell, das sowohl Verständnis- als auch Generierungsaufgaben mit einer oder mehreren Modalitäten ausführen kann. Es verwendet ein vereinheitlichtes Vokabular für Sprache, Bild-Token und die Koordinaten eines Begrenzungsrahmens. Die Forscher formulieren alle Aufgaben in einem vereinheitlichten Sequenz-zu-Sequenz-Format. Im Training verwenden sie 53 Aufgaben aus 9 Gruppen, mit 10.000 Instanzen pro Aufgabe. Zum Testen werden die gesamte Common Sense Reasoning Gruppe und 5 weitere Aufgaben aus VQA und Miscellaneous Gruppen reserviert. Es werden alle Instanzen im Test-Split verwendet. Zusätzlich werden 20 Aufgaben aus dem Test-Split des Natural Instructions Datensatzes als ungesehene Aufgaben für NLP zufällig ausgewählt.

Die Experimente zeigen, dass Instruktions-Tuning die Leistung von OFA bei ungesehenen multimodalen Aufgaben erheblich verbessern kann. Auch Transferlernen vom Natural Instructions Datensatz ist vorteilhaft. Die Erhöhung der Anzahl der Aufgaben und die Verwendung von vielfältigen Anweisungen verbessern die Gesamtleistung des Modells und reduzieren seine Sensitivität. Besonders durch Transferlernen vom Natural Instructions Datensatz erreicht das Modell eine deutlich bessere Sensitivität. Die Forscher sammeln derzeit einen größeren multimodalen Instruktions-Tuning-Datensatz mit rund 150 zusätzlichen Aufgaben und werden ihn bald veröffentlichen.</sample>
    <sample id="124">Die Forschung zielt darauf ab, das Zeitverständnis von Large Language Models (LLMs) zu bewerten und zu verbessern. Die Arbeit unterteilt das zeitliche Denken in drei Ebenen: Zeit-Zeit-Beziehung (logisch, leicht zu verallgemeinern), Zeit-Ereignis-Beziehung (wissensintensiv, benötigt Speicher und Kontext) und Ereignis-Ereignis-Beziehung (erfordert Gedächtnis, Deduktion und Verständnis). Frühere Arbeiten konzentrierten sich auf Zeit-Ereignis-Denken.

In Vorversuchen wurde festgestellt, dass LLMs zeitgenössische Jahre bevorzugen und ChatGPT gut Jahre, aber schlecht Monate vorhersagt. Um diese Lücke zu schließen, wurde das TempReason-Dataset erstellt, das alle drei Ebenen des zeitlichen Denkens über lange Zeitspannen abdeckt.

Die Studie bewertet LLMs in drei QA-Einstellungen: Close-Book QA (kein Kontext), Open-Book QA (Wikipedia-Artikel als Kontext) und Reasoning QA (strukturiertes temporales Wissen als Kontext). Um die zeitliche Denkfähigkeit zu verbessern, wird eine Trainingsstrategie mit zwei Komponenten vorgeschlagen: eine Vortrainingsstrategie zur Extraktion von Zeit- und Entitätsspannen und ein zeitsensitives Reinforcement Learning, das korrekte Vorhersagen belohnt und zeitlich falsche Vorhersagen bestraft. Das finale Modell, TempT5, wird vorgestellt.

Die Ergebnisse zeigen, dass ChatGPT bei der Vorhersage von Monaten erheblich abfällt und auch bei L2- und L3-Denkaufgaben nicht vielversprechend ist. TempT5 übertrifft die Zero-Shot-Leistung von instruktionsoptimierten LLMs deutlich. Eine Analyse des L2-Denkens zeigt, dass die Leistung von ChatGPT je nach Zeitperiode stark variiert und TempT5, obwohl es am besten abschneidet, ebenfalls Schwankungen aufweist, möglicherweise aufgrund einer Ungleichheit der Trainingsdaten.</sample>
    <sample id="125">An der Arbeit sind 7 Autoren beteiligt.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe eines maschinellen Übersetzungsmodells wurde als Baseline betrachtet.</sample>
    <sample id="127">Die Chain-of-Thought (CoT)-Begründung ermöglicht großen Sprachmodellen das Lösen komplexer Aufgaben, aber dies ist bei kleineren Modellen unwirksam. Dieses Papier stellt eine neue Technik namens „Fine-tune-CoT“ vor, bei der ein großes Sprachmodell (z. B. GPT-3 175B) als „Lehrer“ verwendet wird, um Schritt-für-Schritt-Begründungslösungen für ein „Schülermodell“ zu generieren. Wenn die generierten Lösungen korrekt sind, werden sie als Trainingsdaten für das Schülermodell (70M–6,7B Parameter) verwendet, um dessen Fähigkeit zum Lösen komplexer Aufgaben zu verbessern. Die Autoren führen auch „Diverse Reasoning“ ein, bei dem das Lehrmodell mehrere verschiedene Begründungen generiert, was die Leistung des Schülers erheblich steigert. Sie stellen fest, dass „Fine-tune-CoT“ die Begründungsfähigkeiten kleiner Modelle erheblich verbessert und dass die Leistung unter „Fine-tune-CoT“ hochgradig skalierbar ist. Das System bietet eine praktikable Methode, um die komplexen Begründungsfähigkeiten von großen, unerschwinglichen LLMs auf kleinere, effizientere Modelle zu übertragen und so deren Anwendbarkeit zu erweitern.</sample>
    <sample id="128">Dieses Werk stellt den KITMUS-Test vor, eine Sammlung von Datensätzen zur Bewertung der Wissensintegration von NLU-Modellen. Der KITMUS-Test verwendet die Koreferenzauflösung als Aufgabe, um die Fähigkeit von Modellen zu untersuchen, auf Vor- und Inferenzwissen zurückzugreifen. Bei der Koreferenzauflösung müssen Modelle die korrekte Entität identifizieren, auf die sich ein bestimmtes Pronomen bezieht. Dazu sind zwei Arten von Informationen erforderlich: entitätsspezifisches Wissen (typischerweise zur Inferenzzeit verfügbar) und Hintergrundwissen (typischerweise zur Vortrainingszeit erworben).

Der KITMUS-Test umfasst drei Szenarien:
- **Hintergrund-Vortraining**: Hintergrundwissen ist nur zur Vortrainingszeit verfügbar.
- **Hintergrund-Beides**: Hintergrundwissen ist sowohl zur Vortrainings- als auch zur Inferenzzeit verfügbar.
- **Hintergrund-Inferenz**: Hintergrundwissen ist nur zur Inferenzzeit verfügbar.

Die Ergebnisse der Experimente zeigen, dass viele Modelle ohne aufgabenspezifisches Training nicht in der Lage sind, über Wissen aus mehreren Quellen nachzudenken. Mit aufgabenspezifischem Training können einige Modelle Wissen aus mehreren Quellen erfolgreich integrieren. Es zeigt sich jedoch, dass Modelle Schwierigkeiten haben, Hintergrundwissen zu integrieren, das nur zur Inferenzzeit präsentiert wird. Die Datensätze, die Generierungs- und Auswertungscodes sind auf GitHub verfügbar.</sample>
    <sample id="129">Die Autoren gaben "eine Kriegerin" als Beispiel für eine markierte Gruppe.</sample>
    <sample id="130">Die folgende Modellarchitektur generalisiert nicht gut:  
* **Nicht-Transformer-Modelle:** Modelle, die keine Transformer-Architektur verwenden, generalisieren normalerweise nicht gut.</sample>
    <sample id="131">Die Testdatensätze heißen „Cleansly labeled test data (clean)“.</sample>
    <sample id="132">Sechs Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten.</sample>
    <sample id="135">Dieses Video stellt ABC-Eval vor, einen neuen Ansatz zur Bewertung von Chat-Modellen. Aktuelle Bewertungsmethoden für KI-Sprachmodelle, wie Likert-Skalen und paarweise Vergleiche, bieten zwar eine Gesamteinschätzung, sind aber unzuverlässig und zu grob, um spezifische Probleme aufzuzeigen. ABC-Eval hingegen kategorisiert Chat-Modell-Antworten anhand von Verhaltensweisen wie fehlender Relevanz, inkonsistenten Informationen, Ignoranz des Gesprächspartners, fehlendem Einfühlungsvermögen, inkorrekten Fakten und der Verletzung von gesundem Menschenverstand.

Experimente mit vier fortschrittlichen Chat-Modellen zeigen, dass ABC-Eval zuverlässigere Ergebnisse liefert und besser in der Lage ist, die Gesamtqualität von Konversationen vorherzusagen als bestehende Methoden. Die Ergebnisse zeigen, dass Chat-Modelle häufig Fehler machen, z. B. Verletzungen des gesunden Menschenverstandes in etwa 20 % der Antworten, irrelevante Informationen in 15 % und Widersprüche in 10 %. ABC-Eval ermöglicht eine detailliertere und präzisere Bewertung von Konversations-KI und hilft dabei, die Stärken und Schwächen der Modelle auf einer feineren Ebene zu verstehen. Dies ist entscheidend für die kontinuierliche Verbesserung der Konversations-KI.</sample>
    <sample id="136">Dieses Video stellt FERMAT vor, eine flexible Evaluations-Benchmark für numerisches Denken, die von den Autoren Jasivan Alex Sivakumar und Nafise Sadat Moosavi an der University of Sheffield entwickelt wurde. Die Autoren argumentieren, dass bestehende Benchmarks für numerisches Denken nicht repräsentativ sind und die Leistungsfähigkeit von Modellen nur unzureichend erfassen. FERMAT soll diese Lücke schließen, indem es Modelle auf ihr Zahlenverständnis, mathematische Operationen und Trainingsabhängigkeit testet. Es werden verschiedene Darstellungen und Skalen von Zahlen verwendet, um reale Szenarien zu simulieren. Die Experimente zeigen, dass viele Sprachmodelle, insbesondere kleinere Modelle, beim numerischen Denken schlecht abschneiden. Fine-Tuning mit FERMAT führt zu einer deutlichen Verbesserung der Leistung in allen getesteten Bereichen. Die Analyse der Trainingsabhängigkeit deutet darauf hin, dass die Modelle nicht einfach Aufgaben auswendig lernen, sondern tatsächlich neue Fähigkeiten entwickeln. Die Autoren betonen die Bedeutung von Sprach- und mathematischer Diversität im Trainingsmaterial für die Verbesserung numerischer Denkfähigkeiten und identifizieren die Zahlenkodierung und Tokenisierung als weitere Bereiche für Verbesserungen.</sample>
    <sample id="137">"Tell2Design: A Dataset for Language-Guided Floor Plan Generation" ist eine Präsentation über ein neues maschinelles Lernproblem. Das Ziel ist es, 2D-Grundrisse aus natürlichen Sprachbefehlen zu generieren. Dies umfasst das Verständnis von Semantik, Geometrie und Topologie aus den Anweisungen und das Generieren eines strukturierten Innenlayouts. Das Tell2Design-Dataset enthält 5.051 von Menschen kommentierte und 75.737 künstlich erzeugte Sprachbefehle, wobei jeder Grundriss mit über 200 Wörtern und mehr als 10 Sätzen beschrieben wird. Die wichtigsten Herausforderungen sind die Generierung von Designs unter strengen Einschränkungen, der Umgang mit unscharfen und verschränkten Informationen sowie mit verrauschten menschlichen Anweisungen. Der vorgeschlagene Ansatz besteht darin, das Problem als Seq2Seq-Problem mit einem Encoder-Decoder-LLM-Framework zu behandeln, wobei Raum-Bounding Boxes in eine strukturierte Zielsequenz rekonstruiert werden. Experimentelle Ergebnisse zeigen, dass der Tell2Design-Ansatz herkömmliche Modelle übertrifft und auf künstlichen Daten vortrainierte Modelle die Leistung bei menschlichen Anweisungen verbessern können.</sample>
    <sample id="138">Nach Ansicht der Autoren ist ein zu wenig erforschtes Gebiet die Fähigkeit von NLU-Modellen, Wissen aus mehreren Quellen zu integrieren.</sample>
    <sample id="139">Ying Shen und Zhiyang Xu.</sample>
    <sample id="140">Ja, die Qualitätskontrolle wurde von Crowdsourcing-Mitarbeitern durchgeführt, die fehlerhafte Proben korrigierten.</sample>
    <sample id="141">Bestehende Ressourcen für kontextbasierte Übersetzungen unterstützen nur eine begrenzte Anzahl von Phänomenen und Sprachen, da sie stark von Domänenwissen und menschlicher Kuration abhängen.</sample>
    <sample id="142">00:00:00,105 --&gt; 00:00:17,545
Hallo. Ich werde über unsere Arbeit zur Auflösung indirekter Verweisungen für die Entitätenauswahl sprechen, bei der wir das AltEntities Corpus vorstellen. Mein Name ist Mohammad Javad Hosseini, und dies ist eine gemeinsame Arbeit mit Filip Radlinski, Silvia Pareti und Annie Louis.

00:00:18,528 --&gt; 00:00:22,868
Unser Ziel ist es, die Sprache der Nutzer zu verstehen, wenn sie eine Wahl treffen.

00:00:23,458 --&gt; 00:00:27,58
Betrachten Sie diese alternative Frage: Meinten Sie Easy on Me oder I Gotta Feeling?

00:00:27,58 --&gt; 00:00:33,188
Hier möchte ein Benutzer zwischen einem dieser beiden Songs wählen.

00:00:34,388 --&gt; 00:00:44,798
Das Offensichtlichste ist, einen direkten Verweis zu verwenden, zum Beispiel den Namen des Songs, Easy on Me, oder seine Position, der erste.

00:00:44,798 --&gt; 00:00:51,148
Aber manchmal ist ein indirekter Verweis angebrachter, um eine natürlichere Konversation zu führen.

00:00:51,148 --&gt; 00:00:55,758
Dies könnte passieren, wenn sich der Benutzer an den Namen des Songs nicht erinnern kann,

00:00:56,338 --&gt; 00:01:02,128
oder die Aussprachen zu ähnlich sind und schwer zu unterscheiden sind,

00:01:02,968 --&gt; 00:01:06,188
oder wenn der Benutzer eine Präferenz angeben möchte.

00:01:06,758 --&gt; 00:01:13,858
Hier sind einige Beispiele für indirekte Verweise, zum Beispiel der neuere oder der Song, der nicht energiegeladen ist.

00:01:14,756 --&gt; 00:01:19,266
Dies ist ein wichtiges Problem in Konversationssystemen.

00:01:19,266 --&gt; 00:01:23,166
und auch zur Bewertung des Entitätenverständnisses von LLMs.

00:01:23,166 --&gt; 00:01:28,46
Uns ist kein groß angelegter öffentlicher Datensatz für die Aufgabe bekannt, daher haben wir einen mit Crowd-Annotation gesammelt.

00:01:28,46 --&gt; 00:01:38,76
Unser Datensatz umfasst drei verschiedene Domänen: Musik, Bücher und Rezepte.

00:01:39,376 --&gt; 00:01:47,746
Unsere Datensatz-Sammlungsmethodik betont Informalität unter Verwendung eines Cartoon-Vervollständigungs-Setups.

00:01:48,316 --&gt; 00:01:50,116
Der Cartoon hat drei Sprechblasen.

00:01:50,676 --&gt; 00:02:02,296
In der ersten Sprechblase sagt Bob: "Erinnerst du dich an das Lied, das wir gestern gehört haben?" Und damit legt Bob den Dialogkontext fest.

00:02:02,296 --&gt; 00:02:11,566
In der zweiten Sprechblase sagt Alice: "Meinst du Easy on Me oder I Gotta Feeling?", was die alternative Frage ist.

00:02:11,566 --&gt; 00:02:21,396
Und in der dritten Sprechblase verwendet Bob einen indirekten Verweis, um eine dieser Entitäten auszuwählen, zum Beispiel den neueren.

00:02:22,176 --&gt; 00:02:29,266
Wir stellen die erste und zweite Sprechblase automatisch zur Verfügung, aber die dritte wird vom Annotator ausgefüllt.

00:02:29,866 --&gt; 00:02:37,66
Die erste Sprechblase wird aus einigen manuellen Prompts pro Domäne ausgewählt.

00:02:37,66 --&gt; 00:02:41,616
Die zweite, die die alternative Frage ist, wird wie folgt generiert:

00:02:42,217 --&gt; 00:02:46,267
Wir verwenden immer eine einfache Vorlage: "Meinst du A oder B?"

00:02:46,267 --&gt; 00:02:54,77
wobei A und B aus Wikipedia gesampelt werden.

00:02:54,77 --&gt; 00:02:59,377
Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben.

00:02:59,377 --&gt; 00:03:07,317
Wenn wir in der Liste nach oben gehen, werden die Entitäten ähnlicher zueinander, und es ist normalerweise schwieriger, die Disambiguierung vorzunehmen.

00:03:07,317 --&gt; 00:03:10,247
Der erste ist uniform at random.

00:03:10,247 --&gt; 00:03:16,337
Der zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "The Return".

00:03:16,337 --&gt; 00:03:22,787
Der dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben,

00:03:22,787 --&gt; 00:03:31,887
Und schließlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben, zum Beispiel das gleiche Genre oder den gleichen Künstler für einen Song.

00:03:32,803 --&gt; 00:03:45,553
Wenn wir diese alternative Frage den Annotatoren zeigen, kennen sie den Namen dieser Entitäten, aber sie müssen nicht unbedingt etwas über die Entitäten wissen.

00:03:45,553 --&gt; 00:04:02,113
Was wir also tun, ist, dass wir einige Hintergrundinformationen zu den beiden Entitäten zeigen. Für Songs zeigen wir einfach einen Google-Suchlink zu jedem Song. Und dann bitten wir die Annotatoren, sich zumindest einen Teil jedes Songs anzuhören und etwas über jeden Song zu lesen.

00:04:02,113 --&gt; 00:04:09,193
Hier ist zum Beispiel das Google-Suchergebnis für den Song Easy on Me.

00:04:10,380 --&gt; 00:04:28,10
Für die Domänen Rezepte und Bücher zeigen wir zusätzlich einige Hintergrundtexte von Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, ebenfalls von Wikipedia, damit die Annotatoren wissen, wie sie aussehen.

04:28:774 --&gt; 04:44:0
Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit drei bis fünf indirekt referenzierenden Ausdrücken zu beschreiben. Zum Beispiel: "Der mit der Klaviermusik."

04:44:0 --&gt; 04:57:420
Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel "der ohne Worte", "nicht der mit dem 12-jährigen Jungen" oder "der fiktionale" oder "kommt aus Aserbaidschan" und so weiter.

04:57:420 --&gt; 05:07:930
Das AltEntities Corpus enthält 6.000 alternative Fragen über die drei Domänen und hat 42.000 indirekt referenzierende Ausdrücke.

05:07:930 --&gt; 05:25:210
Die Ergebnisse mit dem T5 XL-Modell (Genauigkeit) sind unten zusammengefasst. Wenn das Sprachmodell Zugang zu denselben Hintergrundinformationen wie die Annotatoren hat, ist die Genauigkeit wirklich hoch, sie liegt bei etwa 92 bis 95 %. Aber das ist nicht realistisch.

05:25:210 --&gt; 05:43:890
Wenn das Sprachmodell Zugang zu teilweise überlappenden Hintergrundinformationen hat, liegt die Genauigkeit zwischen 82 und 87 %, was realistischer ist, zum Beispiel wenn das Sprachmodell die Hintergrundinformationen abruft.

05:43:890 --&gt; 05:56:580
Wenn das Sprachmodell nur Zugang zu den Entitätsnamen hat, beträgt die Genauigkeit nur 60 %. Es gibt also viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle domänenübergreifend generalisierbar sind.

05:56:580 --&gt; 06:00:270
Hier ist ein Link zu unserem Datensatz. Vielen Dank!</sample>
    <sample id="143">EDAtt wird mit der Wait-k-Strategie und der lokalen Vereinbarung verglichen.</sample>
    <sample id="144">Die Autoren gehören der Avignon Universität, der Nantes Universität, dem CHU de Nantes und Zenidoc an.</sample>
    <sample id="145">Der/die Referent*in heißt Jenny.</sample>
    <sample id="146">In der Zusammenfassung von Dialogen liegt der Schwerpunkt auf der Verdichtung von Dialogen in kohärente und informative Zusammenfassungen. Obwohl Fortschritte in diesem Bereich erzielt wurden, insbesondere durch große Sprachmodelle, bleibt die Qualität der Zusammenfassungen ein Problem, da sie häufig **Auslassungen** aufweisen, bei denen wichtige Informationen fehlen. Das ist ein weit verbreitetes und schwerwiegendes Problem in allen Domänen und Modellen, das sich auch gleichmäßig über den gesamten Dialog verteilt. Um dieses Problem anzugehen, haben wir eine neue Aufgabe für die **Auslassungsdetektion** in Dialogzusammenfassungen vorgeschlagen.

Dazu haben wir das **OLDS-Dataset** erstellt, das umfangreiche und qualitativ hochwertige Labels für Auslassungen in fünf Domänen enthält, generiert von verschiedenen abstrakten Modellen. Wir haben verschiedene Baseline-Modelle zur Detektion von Auslassungen getestet und festgestellt, dass die Aufgabe trotz der Verbesserungen der Modelle immer noch herausfordernd ist, was auf die Komplexität der Auslassungen in Dialogen hinweist. Schließlich zeigen wir, dass die Qualität der Zusammenfassungen durch das Hinzufügen von ausgelassenen Informationen erheblich verbessert werden kann. Zukünftige Forschungsarbeiten sollten sich auf die Entwicklung fortschrittlicher Modelle zur Auslassungsdetektion und zur Zusammenfassungsverfeinerung konzentrieren.</sample>
    <sample id="147">An der Arbeit sind drei Autoren beteiligt: Myra Cheng, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">00:00
Hallo, ich bin Sara Papi von der Universität Trient und der Fondazione Bruno Kessler und ich werde kurz die Arbeit "Attention as a Guide for Simultaneous Speech Translation" vorstellen, die eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist.
00:16
Was ist simultane Sprachübersetzung? Die simultane Sprachübersetzung oder SimuIST ist der Prozess der Echtzeitübersetzung gesprochener Sprache in einen Text in einer anderen Sprache, der die sprachübergreifende Kommunikation ermöglicht.
00:30
Und was sind die Probleme der aktuellen SimuIST-Modelle? Spezifische Architekturen werden in der Regel trainiert, wobei zusätzliche Module zur Optimierung eingeführt werden. Lange und komplizierte Trainingsprozeduren, z.B. das Training mit unterschiedlichen Optimierungszielen. Und das Training und die Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen, z.B. das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden Latenz usw.
01:04
Was ist unsere Lösung? Erstens, bereits bestehende Offline-ST-Modelle zu verwenden, ohne sie neu zu trainieren oder spezifische Architekturen für SimuIST zu übernehmen. Zweitens, nur ein Modell für jedes Latenzregime zu verwenden und die Latenz durch spezifische Parameter zu handhaben. Und drittens, das bereits erworbene Wissen des Modells durch den Aufmerksamkeitsmechanismus zwischen Audio-Eingabe und Text-Ausgabe zu nutzen. Das ist der Cross-Attention-Mechanismus und Sie können ein Beispiel auf der rechten Seite sehen.
01:35
Unsere Lösung ist, EDAtt oder Encoder-Decoder-Attention vorzuschlagen. Und es ist eine Strategie, bei der wir entscheiden, ob eine partielle Übersetzung ausgegeben werden soll oder nicht, basierend darauf, worauf die Aufmerksamkeit gerichtet ist. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d.h. die Summe ist unter einem bestimmten Schwellenwert Alpha, zu den letzten Lambda-Sprach-Frames, was bedeutet, dass die erhaltene Information stabil genug ist. Zum Beispiel, wenn wir einen Sprach-Chunk erhalten, der "I am going to talk about" enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, und wir uns die Cross-Attention-Gewichte ansehen, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprach-Frames zeigen, während das letzte Wort auf die letzten empfangenen Sprach-Frames zeigt, die Lambda-Sprach-Frames. Das bedeutet, dass die ersten beiden Wörter ausgegeben werden, während, da die Summe der Cross-Attention über einem bestimmten Schwellenwert Alpha liegt, wir das letzte Wort nicht ausgeben werden und wir auf einen weiteren Sprach-Chunk warten. Wenn wir fortfahren und einen weiteren Sprach-Chunk erhalten, und unser Modell weitere drei Wörter vorhersagt, und wir uns die Cross-Attention-Gewichte ansehen, werden wir sehen, dass kein Wort auf die letzten Lambda-Sprach-Frames zeigt. Das bedeutet, dass diese drei Wörter ausgegeben werden.
03:11
Wenn wir uns die Hauptergebnisse von EDAtt ansehen, werden wir die Ergebnisse der simultanen Sprachübersetzung auf Diagrammen darstellen, in denen wir BLEU auf der einen Seite haben, das die Übersetzungsqualität misst, und die durchschnittliche Verzögerung, d.h. die Latenzmessung, und wir berücksichtigen auch die computergestützte durchschnittliche Verzögerung, die die Berechnungszeit des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir möchten also, dass unsere Kurven auf diesem Diagramm so hoch wie möglich sind, aber wir möchten auch, dass sie nach links verschoben werden. Und wir vergleichen mit populären Strategien, die auch auf Offline-Modelle angewendet werden, das sind die Wait-K-Strategie und das Local Agreement. Und wir vergleichen auch mit der State-of-the-Art-Architektur, die speziell für die simultane Sprachübersetzung entwickelt wurde. Dies sind alle Ergebnisse der simultanen Sprachübersetzungsstrategie ins Deutsche. Und wir sehen, dass EDAtt alle auf Offline-Modelle angewandten Strategien übertrifft, da die Kurven nach links verschoben sind. Und wir sehen auch, dass, wenn wir die tatsächlich verstrichene Zeit oder die computergestützte Zeit berücksichtigen, EDAtt die schnellste Strategie ist.
04:40
Wenn Sie weitere Ergebnisse entdecken möchten, lesen Sie unser Paper! Wir haben auch den Code und die Modelle und die simultane Ausgabe Open Source veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja, der Datensatz ist über GitHub öffentlich zugänglich.</sample>
    <sample id="150">Dieser Vortrag stellt MeetingQA vor, einen neuen Datensatz zum Beantworten von Fragen aus Transkripten von Meetings. Meeting-Transkripte sind einzigartig, da es sich um lange, domänenspezifische und informationsreiche Dokumente handelt. Bisherige Arbeiten konzentrierten sich auf die Zusammenfassung und Extraktion von Aktionspunkten. MeetingQA geht diese Lücke an, indem es Fragen von Teilnehmern und entsprechende Antwortsätze verwendet. Die Fragen sind länger, offene Fragen, die Diskussionen anregen. Die Antworten können von mehreren Sprechern gegeben werden, nicht aufeinanderfolgende Sätze enthalten und rhetorische Fragen aufweisen.

Die Datensammlung umfasste die Auswahl von Fragen aus dem AMI-Korpus auf der Grundlage von Interpunktion und Fragenlänge. Anschließend wurden Antworten von Anmerkungsgebern annotiert, wodurch eine hohe Inter-Annotator-Übereinstimmung erzielt wurde. MeetingQA enthält über 7.700 Fragen aus 166 Meetings, die in Trainings-, Entwicklungs- und Testsätze unterteilt sind. 30 % der Fragen sind unbeantwortbar, 40 % der Antworten sind mehrspanig und 48 % sind mehrsprecher. 50 % der Fragen sind meinungssuchend und 20 % sind rhetorisch. Durchschnittlich bestehen Transkripte aus 5.900 Wörtern, Fragen aus 12 Wörtern und Antworten aus 35 Wörtern. Die menschliche Leistung liegt bei einem F1-Wert von 84,6.

Die Ergebnisse zeigen, dass ein erheblicher Spielraum für Verbesserungen besteht, da ein Abstand von 25 F1-Punkten zu menschlicher Leistung und 50 F1-Punkten im Null-Shot-Setting besteht. Die Modelle haben Schwierigkeiten, rhetorische Fragen und die Identität der Antwortenden zu identifizieren. Silbersprachige Datenaugmentation ist effektiv.</sample>
    <sample id="151">00:00
Hallo allerseits. Mein Name ist Ying, und mein Kollege Zhiyang und ich werden unsere Forschung zu MULTIINSTRUCT präsentieren: Verbesserung des multimodalen Zero-Shot-Lernens durch Instruction Tuning.

00:11
Mit den Fortschritten bei großen Sprachmodellen begannen viele Arbeiten, neue Lernparadigmen für die Wiederverwendung vorab trainierter Sprachmodelle für verschiedene nachgelagerte Aufgaben auf parameter- und dateneffiziente Weise zu erforschen. Kürzlich haben viele Studien gezeigt, dass Instruction Tuning großen Sprachmodellen ermöglicht, ungesehene Aufgaben auf Zero-Shot-Weise auszuführen, indem sie natürliche Anweisungen befolgen.

00:35
Die meisten bisherigen Arbeiten zum Instruction Tuning konzentrierten sich jedoch auf die Verbesserung der Zero-Shot-Leistung bei rein sprachbasierten Aufgaben, während Computer Vision und multimodale Aufgaben vernachlässigt wurden.

00:48
Daher wollen wir in dieser Arbeit untersuchen, ob das Instruction Tuning von multimodalen vortrainierten Modellen die Generalisierung auf ungesehene multimodale Aufgaben tatsächlich verbessern kann.

01:00
Zudem stellten wir zum Zeitpunkt unserer Forschung eine erhebliche Diskrepanz in der Verfügbarkeit von Instructional Datasets zwischen NLP und multimodal fest. Es gibt mehr als 1600 rein sprachbasierte Instructional Tasks. Es gibt jedoch keine groß angelegten, öffentlich verfügbaren multimodalen Instructional Tasks. Dies motivierte uns, ein multimodales Instruction Tuning Dataset zu erstellen.

01:27
Hier präsentieren wir MULTIINSTRUCT, das erste multimodale Instruction Tuning Benchmark Dataset, das 62 verschiedene multimodale Aufgaben aus 10 breiten Kategorien umfasst. Diese Aufgaben werden aus 21 bestehenden Open-Source-Datasets abgeleitet, und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet.

01:50
Um multimodales Instruction Tuning auf unserem vorgeschlagenen Dataset zu untersuchen, verwenden wir OFA, ein vereinheitlichtes multimodales vortrainiertes Modell, als unser Basismodell. OFA verwendet ein vereinheitlichtes Vokabular für Sprache, Bild-Tokens und die Koordinaten einer Bounding Box.

02:07
Hier zeigen wir einige Beispielinstanzen aus unserem MULTIINSTRUCT-Dataset. Um die Verarbeitung verschiedener Ein- und Ausgabedatentypen zu vereinheitlichen, folgen wir der Methode von OFA und formulieren alle Aufgaben in einem vereinheitlichten Sequenz-zu-Sequenz-Format, in dem der Eingabetext, die Bilder, die Anweisung und die Bounding Boxes im selben Token-Raum dargestellt werden.

02:31
Okay, nun werde ich über multimodales Instruction Tuning sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und sampeln 10.000 Instanzen pro Aufgabe. Für das Testen reservieren wir die gesamte Gruppe "Common Sense Reasoning" für das Testen. Wir wählen weitere 5 Aufgaben aus den VQA- und Miscellaneous-Gruppen aus. Wir verwenden alle Instanzen im Test-Split für jede Aufgabe. Zusätzlich sampeln wir zufällig 20 Aufgaben aus dem Test-Split des Natural Instructions-Datensatzes als ungesehene Aufgaben für NLP.

03:02
Wir verwenden ein vortrainiertes OFA-Large-Modell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Instruktionsvorlagen kombiniert. Während des Tests führen wir für jede Aufgabe insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen in jedem Experiment bewerten. Wir berichten den mittleren und maximalen Leistungsdurchschnitt sowie die Standardabweichung der Leistung über alle fünf Experimente.

03:32
Wenn die Aufgabe eine multimodale Klassifizierungsaufgabe (Visuelle Verknüpfung, visuelles räumliches Denken, natürliches Sprach- und visuelles Denken, und Klassifizierung von Katastrophentypen) ist, berichten wir die Genauigkeit. Wenn es sich um eine multimodale Generierungsaufgabe (Common Sense VQA, Text VQA, Grounded VQA, visuelle Text Extraktion und visueller Dialog) handelt, berichten wir den Rouge-L-Wert. Für NLP-Aufgaben berichten wir auch den Rouge-L-Wert.

03:45
Wir haben auch eine zusätzliche Bewertungsmetrik namens Sensitivität eingeführt. Diese misst die Fähigkeit des Modells, konsistent die gleichen Ergebnisse für dieselbe Aufgabe zu liefern, unabhängig von geringfügigen Abweichungen in der Formulierung der Anweisungen.

04:02
Hier ist unser Hauptergebnis. Wie wir sehen können, kann Instruction Tuning die Leistung von OFA bei ungesehenen multimodalen Aufgaben signifikant verbessern. Auch das Transferlernen aus dem Natural Instruction Datensatz kann dem Instruction Tuning zugutekommen.

04:20
Hier sehen wir, dass mit zunehmender Anzahl der Aufgaben das Modell eine bessere Leistung erzielt und gleichzeitig eine geringere Empfindlichkeit aufweist.

04:30
Wir haben auch Experimente mit einer Anweisung versus fünf Anweisungen durchgeführt. Wie wir sehen können, kann die Verwendung von mehr Anweisungen die Gesamtleistung des Modells verbessern und seine Empfindlichkeit stark reduzieren.

04:44
Dies zeigt den Effekt verschiedener Fine-Tuning-Strategien auf die Modellempfindlichkeit. Wie wir sehen können, kann das Modell durch Transferlernen aus dem Natural Instruction Dataset eine viel bessere Empfindlichkeit erreichen als das ursprüngliche OFA-Modell.

05:02
Wir können auch sehen, dass Transferlernen aus dem Natural Instruction Datensatz OFA helfen kann, eine viel bessere Leistung beim Natural Instruction Datensatz zu erzielen.

05:14
Insgesamt haben wir den ersten groß angelegten multimodalen Instruction-Tuning-Datensatz vorgeschlagen, der 62 multimodale Aufgaben aus 10 breiten Kategorien enthält. Wir haben die Zero-Shot-Fähigkeit von OFA durch Instruction Tuning signifikant verbessert und mehrere Transfer-Learning-Techniken erforscht und ihre Vorteile gezeigt. Wir haben eine neue Metrik namens Sensitivität entwickelt.

05:30
Noch eine Sache! Wir sammeln derzeit einen viel größeren multimodalen Instruktions-Tuning-Datensatz mit etwa 150 zusätzlichen Aufgaben für Vision-Sprache, und wir werden ihn bald veröffentlichen! Dies ist ein QR-Code für unsere Daten und unser Modell. Vielen Dank.</sample>
    <sample id="152">In dieser Präsentation werden neue Sprachmodelle vorgestellt, die speziell für die Klassische Philologie entwickelt wurden, um die bestehenden Lücken bei der multilingualen Verarbeitung zu schließen. Die bisherigen Modelle waren **Encoder-only** und **monolingual**. Unser Projekt zielte darauf ab, die bestehenden Modelle vergleichbar zu machen, den Forschungsstand voranzutreiben, verschiedene Modellarchitekturen zu erforschen und multilinguale Modelle einzuführen.

Wir haben zwei **monolinguale Modelle** für Altgriechisch entwickelt: GreBERTa (Encoder-only) und GreTa (Encoder-Decoder, basierend auf T5). Zusätzlich wurden **multilinguale Äquivalente** PhilBERTa und PhilTa entwickelt, die auf Altgriechisch, Latein und Englisch vortrainiert wurden.

Die **Vortrainingsdaten** umfassen Open Greek &amp; Latin, griechische mittelalterliche Texte und Patrologia Graeca. Ein neues Vortrainingskorpus wurde aus dem Internetarchiv erstellt, das Texte enthielt, bei denen griechische Zeichen bei der OCR-Verarbeitung falsch transkribiert wurden, was durch einen speziellen Such- und Nachbearbeitungsprozess behoben wurde. Für die multilinguale Modelle wurden zusätzlich die Corpus Corporum für Latein und antikenbezogene englische Texte verwendet.

Die **Modelle** wurden anhand von Universal Dependencies (PoS-Tagging, Dependency Parsing) und EvaLatin 2022 (PoS-Tagging, Lemmatisierung) **evaluiert**. Unsere Modelle übertrafen den aktuellen Stand der Technik sowohl für Altgriechisch als auch für Latein deutlich. Insbesondere GreTa+Chars erzielte beim Lemmatisieren für Altgriechisch eine beeindruckende Steigerung von 5 Prozentpunkten. Die **Encoder** der T5-Modelle zeigten ein anderes, aber letztendlich vergleichbares Verhalten wie die **Encoder-only-Modelle**. Es wurde festgestellt, dass die multilinguale Modelle keine signifikanten Vorteile in Bezug auf semantisches und Weltwissen gegenüber ihren monolingualen Pendants zeigten.

Zusammenfassend lässt sich sagen, dass wir neue, leistungsstarke **Sprachmodelle für die Klassische Philologie** vorgestellt haben, die von Grund auf initialisiert wurden und **Encoder-only-, Encoder-Decoder- und multilinguale Architekturen** nutzen. Dies ermöglicht die gleichzeitige Verarbeitung von lateinischen und griechischen Texten und die Nutzung eines hochwertigen, neuen Vortrainingsdatensatzes.</sample>
    <sample id="153">Der Vortrag konzentriert sich auf die Auflösung von Mehrdeutigkeiten in Text-zu-Bild-Generierungsmodellen. Mehrdeutige Prompts wie „Ein Elefant und ein Vogel fliegen“ oder „Das Mädchen betritt den Raum mit Blumen“ können zu unterschiedlichen visuellen Interpretationen führen. Ohne die Auflösung dieser Mehrdeutigkeiten ist es für Text-zu-Bild-Modelle schwierig, Bilder zu erzeugen, die den Absichten des Benutzers treu sind.

Die Forscher haben einen zweistufigen Ansatz entwickelt: Zuerst haben sie einen Text-zu-Bild-Mehrdeutigkeits-Benchmark (TAB) erstellt, der verschiedene Arten von Mehrdeutigkeiten abdeckt. Dann haben sie einen Disambiguierungsrahmen vorgeschlagen, um diese Mehrdeutigkeiten zu mindern. Dieser Rahmen verwendet In-Context-Lernen, um entweder klärende Fragen zu generieren (QA-TIED) oder mögliche visuelle Setups zu identifizieren (VS-TIED). Benutzer können dann die Mehrdeutigkeit auflösen, indem sie Fragen beantworten oder das bevorzugte Setup auswählen.

Für die Auswertung der Disambiguierung verwendeten sie einen automatischen Bewertungsansatz mit einem VQA-Modell (Visual Question Answering), um zu überprüfen, ob die generierten Bilder die Benutzerabsicht erfüllten. Die Ergebnisse zeigen, dass eine Disambiguierung einen positiven Einfluss auf die Bildgenerierung hat und dass ihre automatische Bewertung mit menschlichen Bewertungen übereinstimmt.</sample>
    <sample id="154">Die Autoren gehören der Universität Trento an.</sample>
    <sample id="155">Der/die Referent*in heißt Mohammad Javad Hosseini.</sample>
    <sample id="157">Hallo, mein Name ist Shen Gao von der Shandong Universität. Heute stelle ich unsere Arbeit vor, die "Dialogzusammenfassung mit Static-Dynamic Structure Fusion Graph".

Die automatische Zusammenfassung von Dialogen ist eine herausfordernde und wichtige Aufgabe, die darauf abzielt, die wichtigsten Informationen aus einem halbstrukturierten Dialog oder einem Dialog mit mehreren Teilnehmern in einer prägnanten Zusammenfassung zu destillieren. Die meisten bestehenden Methoden konzentrieren sich auf die Modellierung von Dialogen mit vorab berechneten statischen Graphenstrukturen unter Verwendung externer linguistischer Tools. Allerdings haben diese Methoden zwei grundlegende Nachteile: Sie sind stark von der Zuverlässigkeit der externen Tools abhängig, was zu Fehlerfortpflanzung führen kann, und die Konstruktion des statischen Graphen ist von der Graphrepräsentationslernphase getrennt. Der so erzeugte feste Graph kann sich nicht dynamisch an die nachgelagerte Aufgabe der Dialogzusammenfassung anpassen.

Um diese Herausforderungen zu meistern, schlagen wir eine neuartige Static-Dynamic graph-based Dialogue Summarization (SDDS) Methode vor. Unser SDDS-Modell besteht aus vier Hauptkomponenten: einem Äußerungs-Encoder, einer statischen Graphenkonstruktion, einem statisch-dynamischen Graphenmodul und einem Zusammenfassungsgenerator. Wir integrieren vier Arten von heuristischen statischen Graphenstrukturen, nämlich Diskurs-Parsing-Graphen, Schlüsselwort-Kookkurrenz-Graphen, Sprecherrelations-Graphen und Äußerungspositions-Graphen, um die Informationen aus verschiedenen Perspektiven zu erfassen. Darüber hinaus entwerfen wir einen dynamischen Graphen, der die semantischen Beziehungen zwischen den Äußerungen auf der Grundlage ihrer tiefen Vektorrepräsentation erfasst. Schließlich kombinieren wir statische und dynamische Grapheninformationen in einem Fusion-Modul und übergeben sie an den Zusammenfassungsgenerator, um die Zusammenfassung zu erstellen.</sample>
    <sample id="158">Dieses Video stellt ein neues Modell namens „Dual Cache“ zur Auflösung von Koreferenzen in langen Dokumenten vor. Herkömmliche Ansätze haben eine quadratische Komplexität, während Cache-basierte Modelle die Komplexität auf ein lineares Niveau reduzieren, indem sie einen Cache fester Größe zur Speicherung von Entitätsdarstellungen verwenden. Allerdings führt die LRU-Richtlinie (Least Recently Used) in langen Dokumenten mit Themawechseln zu einer hohen Cache-Fehlerrate.

Dual Cache löst dieses Problem mit zwei Caches: einem L-Cache für lokale Entitäten (mit LRU-Richtlinie) und einem G-Cache für globale Entitäten (mit LFU-Richtlinie - Least Frequently Used). Bei einer neuen Erwähnung bestimmt das Modell, ob es sich um eine neue Entität handelt oder ob sie zu einer vorhandenen Entität gehört. Wenn die Häufigkeit die eines Eintrags im G-Cache übersteigt, wird sie in den G-Cache verschoben; andernfalls kommt sie in den L-Cache. Diese Strategie reduziert Cache-Fehler erheblich.

Experimente an öffentlichen Benchmarks (LitBank, OntoNotes und WikiCoref) zeigen, dass Dual Cache einzelne Cache-Methoden übertrifft und die Cache-Fehlerrate drastisch reduziert. Bei Dokumenten auf Buchebene ist die Leistungslücke noch größer. Dual Cache bietet zudem ein überragendes Leistungs-Kosten-Verhältnis und ist somit der effizienteste und kostengünstigste Ansatz für die Auflösung von Koreferenzen in langen Dokumenten.</sample>
    <sample id="159">00:00
Hallo allerseits. Ich bin Koustuv Sinha, und es freut mich, Sie zu unserem Vortrag unseres ACL 2023-Papiers begrüßen zu dürfen: "Sprachmodell-Akzeptabilitätsurteile sind nicht immer robust gegenüber Kontext." Dies ist eine gemeinsame Arbeit mit Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy und Adina Williams.
00:19
In dieser Arbeit greifen wir das Minimalpaar-Paradigma erneut auf. Das Minimalpaar-Paradigma bewertet Sprachmodelle anhand von Akzeptabilitätsurteilen, die auch Grammatikalität umfassen können, wie BLiMP, SyntaxGym oder Akzeptabilität in Bezug auf Stereotypen, wie CrowS-Paare. In diesem Minimalpaar-Paradigma ist die typische Art, Sprachmodelle zu bewerten, dass man einen akzeptablen Satz oder einen grammatikalischen Satz zeigt und dann einen unakzeptablen Satz oder einen ungrammatischen Satz. Die Hoffnung ist dann, dass das Modell dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuweist.
01:00
Die aktuelle MPP-Pipeline erlaubt es uns im Grunde nicht, Modelle auf längere Sätze zu bewerten. Heutzutage kommen immer größere Sprachmodelle mit längeren und längeren Kontextfenstern heraus. Es ist daher entscheidend, dass wir die Akzeptabilität der Modelle über das gesamte Kontextfenster hinweg bewerten. Das ist es, was wir hier versuchen zu tun. Wir versuchen, die MPP-Pipeline zu überarbeiten, indem wir das Modell bitten, die Akzeptabilität auf längeren und längeren Sequenzen zu bewerten.
01:32
Das ist also der Ansatz. Was wir tun ist, um diese längeren Sequenzen zu simulieren, wir besuchen die Datensätze selbst erneut und erstellen dann Sätze neu, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir hier ein typisches Paar Grammatikalität aus dem BLiMP-Datensatz für den Adjunkt-Insel-Fall ausgewählt. Und was wir tun, um längere Sequenzen zu erstellen, die akzeptabel sind und die dieselbe Übereinstimmung der grammatikalischen Struktur aufweisen, extrahieren wir grammatikalische Sätze von der Adjunkt-Insel und fügen sie dann als Präfix sowohl zur akzeptablen als auch zur unakzeptablen Abfrage hinzu.
02:17
Wir können dasselbe tun, indem wir unakzeptable Sätze aus derselben Übereinstimmung auswählen. Und das könnte auch verwendet werden, um die Akzeptabilität der Modelle zu testen.
02:28
Und wir könnten dasselbe tun, indem wir Sätze aus einem anderen Subset oder einem anderen Datensatz auswählen. Das nennen wir das Mismatch-Szenario. Hier stammen die Sätze immer noch aus relevanten Datensätzen, aber nicht aus demselben Datensatz, mit dem Sie bewerten.
02:46
Und wir können dasselbe für den Unakzeptabilitätsfall tun.
02:49
Schließlich können wir Sätze aus einem völlig unrelated Domain wählen, wie z.B. Wikipedia. Dies wird uns zeigen, ob die Akzeptabilitätsurteile der Modelle tatsächlich durch irgendeinen Kontext beeinflusst werden, d.h. ob der Kontext aus einem anderen Teilsatz des Datensatzes stammt oder ob er völlig irrelevant für den aktuellen Satz ist, den wir betrachten.
03:14
Wie funktioniert das Modell? Zuerst betrachten wir die Wikipedia-Sätze, die für das aktuelle Abfragepaar völlig irrelevant sind. Und dort stellen wir fest, dass die MPP-Urteile für beliebige Kontextlängen größtenteils robust sind. Wir erhöhen die Kontextlänge auf bis zu 1024, um die OPT- und GPT-2-Modelle auszureizen. Und wir haben hier in der orangefarbenen gepunkteten Linie gesehen, dass die MPP-Urteile relativ stabil sind.
03:41
Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen wir Sätze aus akzeptablen und unakzeptablen Domänen aus, aus demselben BLiMP- oder SyntaxGym-Datensatz. Und dort sehen wir, dass die MPP-Urteile entweder signifikant zunehmen oder abnehmen, wenn wir entweder akzeptable Präfixe oder unakzeptable Präfixe hinzufügen.
04:06
Aber wenn wir die Struktur anpassen, das heißt, wenn wir die Sätze aus demselben Phänomen in BLiMP oder SyntaxGym auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang der MPP-Urteile für das Modell, abhängig davon, ob das gewählte Präfix akzeptabel oder unakzeptabel ist. Und dies ist sehr groß. Dieser Effekt nimmt über die gesamte Kontextlänge hinweg zu, und dies würde wahrscheinlich neuere Sprachmodelle mit großen Kontextfenstern betreffen.
04:40
Warum beeinflussen also übereinstimmende Präfixe die Sprachmodellurteile so stark? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, den Eingabesatz zu stören, indem wir versucht haben, die relevante Struktur zu erhalten, aber Rauschen in die Eingabe einzufügen. Und nach mehreren dieser Störungen stellen wir fest, dass keines dieser Rauschen das Modell dazu bringt, seinen Kurs zu ändern, in Bezug darauf, wie es uns die MPP-Urteilstrends zeigt.
05:10
Grundsätzlich stellen wir fest, dass die Modelle empfindlich auf gestörte Sätze auf ähnliche Weise reagieren. Das heißt, wenn wir die Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen, und wenn wir die Sätze im unakzeptablen Bereich stören, sehen wir einen Rückgang der MPP-Urteile in ähnlicher Weise.
05:32
Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische/semantische Merkmale reagieren, die über Sätze hinweg geteilt werden. Und MPP-Bewertungen mit kurzen, einzelnen Satz-Eingaben erfassen das abstrakte Wissen von Sprachmodellen nicht vollständig. Bitte lesen Sie unser Paper für weitere Details unserer Experimente. Vielen Dank fürs Zuhören.</sample>
    <sample id="160">Ungeordneten Multiset-Token.</sample>
    <sample id="161">Die Anzahl der Skripte in Coscript ist 55.000.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEplain ist die MASAlign-Methode.</sample>
    <sample id="164">Schwach überwachtes Lernen (WSL) hilft, den Engpass bei der Annotation zu überwinden, indem es unbezeichnete Daten mit kostengünstigen, schwachen Beschriftungsquellen versieht.</sample>
    <sample id="165">Wenting Zhao vom Cornell Bowers CIS Computer Science stellt ihre ACL 2023-Forschung über "Abductive Commonsense Reasoning: Exploiting Mutually Exclusive Explanations" vor. Die abduktive Argumentation beinhaltet die Identifizierung plausibler Erklärungen, die eine Informationslücke zwischen einem Kontext und einem Ergebnis schließen. Die Forscherin weist darauf hin, dass die herkömmliche Überwachung bei der Annotation plausibler Erklärungen zu Rauschen und Subjektivität führen kann, mit einer Uneinigkeit von über 60 % unter den Crowdworkern. Um dieses Problem anzugehen, schlägt sie LiPoR (Likelihood learning with Posterior Regularization) vor, einen unüberwachten Lernansatz. Das LiPoR-Ziel maximiert die marginale Wahrscheinlichkeit des Ergebnisses, indem es alle möglichen Erklärungen marginalisiert. Um plausible Erklärungen zu bevorzugen, nutzt LiPoR die gegenseitige Ausschließlichkeit von Erklärungen, d. h. die Plausibilität einer Erklärung schließt die Plausibilität anderer Erklärungen automatisch aus. Dieser Regularisierungsansatz minimiert die Entropie der Erklärungswahrscheinlichkeit und lenkt die Wahrscheinlichkeitsmasse auf eine Untergruppe von Erklärungen. LiPoR übertrifft die bisher besten unüberwachten Ansätze und GPT-3-basierte Baseline auf dem αNLI-Datensatz um über 4 absolute Punkte.</sample>
    <sample id="166">Dieses Video stellt ein neurale Divide-and-Conquer-Framework für den Bildabruf aus linguistisch komplexem Text vor. Herkömmliche Methoden sind oft unzureichend, wenn der Beschreibungstext lang ist und die Bilder hochgradig ähnlich sind. Um dieses Problem zu lösen, hat der Redner eine Methode entwickelt, die von der Divide-and-Conquer-Strategie und der Dual-Process-Theorie inspiriert ist. Die Methode besteht aus zwei Hauptsystemen: einem Propositionengenerator und einem Neuro-Symbolischen Reasoner. Der Propositionengenerator zerlegt einen komplexen Text in einfache Sätze, während der Neuro-Symbolische Reasoner logisches Denken durchführt, um die Beziehungen zwischen diesen Sätzen und den Bildern herzustellen. Die Ergebnisse zeigen, dass das vorgeschlagene Framework die Baselines deutlich übertrifft, was seine Effektivität bei der Bewältigung komplexer Bildabrufsaufgaben bestätigt.</sample>
    <sample id="167">Die Dokumente in DEplain-web wurden mit 3450 manuellen und 756 automatischen Alignments ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde durch die Sammlung von Reuters-Nachrichten aus dem Jahr 2020 und deren Annotation nach den CoNLL-2003-Annotationsrichtlinien erstellt.</sample>
    <sample id="169">In dieser Arbeit stellen wir die erste systematische Studie zur LLM-Prompting für die maschinelle Übersetzung vor. Wir bewerten die Übersetzungsfähigkeiten von PalM, einem großen Sprachmodell mit 540 Milliarden Parametern, das auf einer großen Textsammlung trainiert wurde und zum Zeitpunkt seiner Veröffentlichung den Stand der Technik in Hunderten von NLP-Aufgaben erreichte.

Um die Übersetzungsfähigkeiten zu bewerten, verwenden wir die besten Praktiken der MT-Gemeinschaft, einschließlich der neuesten Testdaten und eines Vergleichs mit fortschrittlichen Systemen. Unsere Experimente zeigen, dass die Qualität der Prompt-Beispiele wichtiger ist als ihre Ähnlichkeit mit dem Quellsatz, und dass hochqualitative Übersetzungen zu einer besseren Leistung führen. Wir stellen fest, dass spezialisierte SOTA-Systeme einen erheblichen Vorteil gegenüber PalM-Übersetzungen haben, obwohl PalM einem kommerziellen System wie Google Translate sehr nahe kommt.

Erkenntnisse aus der menschlichen Bewertung zeigen, dass die Flüssigkeit von PalM mit SOTA vergleichbar ist, die Genauigkeitswerte jedoch im Allgemeinen niedriger sind, wobei Auslassungsfehler dominieren. Dies deutet darauf hin, dass PalM flüssige Übersetzungen bevorzugt, auch wenn dies zu Auslassungen von Teilen des Quellsatzes führt. Wir geben Empfehlungen für Prompt-Auswahlstrategien, um die Übersetzungsqualität zu maximieren.</sample>
    <sample id="170">00:00
Hello everyone. My name is [unintelligible] from the Penn State University. Today I'm going to present our work Exempler, Cross-lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations.
German: Hallo zusammen. Mein Name ist [unverständlich] von der Penn State University. Heute werde ich unsere Arbeit Exempler, Cross-lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations vorstellen.</sample>
    <sample id="171">Bereits durchgeführte Arbeiten umfassen Parameter-basierte, lexikalische, Backdoor-basierte und Adversarial-basierte Wasserzeichenmethoden.</sample>
    <sample id="172">Nein, mehrsprachige LLMs wie Codex und BLOOM sind für CLSP-Aufgaben immer noch unzureichend.</sample>
    <sample id="174">In diesem Video stellt Priya Arganalysis35K vor, ein großes Datenset für die Analyse der Argumentationsqualität. Sie erläutert die Besonderheiten dieses Datensatzes im Vergleich zu anderen.

Argumentationsqualitätsanalyse bewertet Argumente auf einer Skala von 0 bis 1. Bestehende Datensätze weisen oft Mängel in der Argumentenqualität, der Themenvielfalt und der Tiefe der Erklärungen auf und sind oft an bestimmte Argumente gebunden.

Arganalysis35K begegnet diesen Problemen:
1. Es ist der größte Datensatz zur Argumentenqualitätsanalyse mit 35.000 Argument-Analyse-Paaren, wobei 85 % aus hochwertigen Debatten stammen.
2. Es werden 24 verschiedene Themen behandelt, was eine breitere Vielfalt an Argumenten bietet als die traditionellen 30-40 festgelegten Beweggründe.
3. Der Datensatz enthält ein Analysesystem, das über bloße Behauptungen und Prämissen hinausgeht, indem es Argumente logisch verknüpft und die Nuancen dahinter erklärt.
4. Es wird ein instanzbasiertes Annotator-Zuverlässigkeitsmodell verwendet, das annotator-spezifische Verzerrungen bei bestimmten Themen berücksichtigt, um genauere Bewertungen zu erhalten.
5. Ein Relevanzmodell ordnet jedem Argument-Analyse-Paar pro Thema einen Relevanzwert zu, um die Anwendbarkeit von Argumenten auf verschiedene Kontexte zu erfassen.

Arganalysis35K bietet ein vielfältigeres, qualitativ hochwertigeres und zuverlässigeres Framework für die Argumentationsqualitätsanalyse.</sample>
    <sample id="175">Die Methode approximiert dies mit einer GPU-freundlichen, kontinuierlichen Relaxation, die es auch ermöglicht, durch die Lösung zurückzublicken und die linguistisch plausibleren Permutationen zu lernen.</sample>
    <sample id="176">Ein nachgeschaltetes NLP-Modell ist "fair", wenn es eine geringe Empfindlichkeit gegenüber politischen Voreingenommenheiten des Sprachmodells aufweist.</sample>
    <sample id="177">Der Referent heißt Yanis Labrak.</sample>
    <sample id="178">Der Referent heißt Koustuv Sinha.</sample>
    <sample id="179">In der Forschung geht es darum, die Fähigkeit von großen Sprachmodellen (LLMs) zu verbessern, über die mentalen Zustände anderer zu denken, eine Fähigkeit, die als "Theory of Mind" (ToM) bekannt ist. Bisher haben LLMs Schwierigkeiten mit sogenannten "False-Belief-Fragen", bei denen die Realität von den Überzeugungen einer Figur abweichen kann. Um diese Schwäche zu beheben, wurde "SymbolicToM" vorgestellt, eine Methode, die explizite grafische Darstellungen des Weltzustands verwendet, um die mentalen Zustände von Charakteren darzustellen.

Anstatt eine einzige Grafik zu verwenden, nutzt SymbolicToM mehrere Grafiken für verschiedene mentale Zustände. Beispielsweise wird eine Grafik für das Weltbild einer Figur erstellt, und eine andere für das, was eine Figur über die Überzeugungen einer anderen Figur denkt. Diese Grafiken werden mit einem Inferenz-Zeit-Algorithmus erstellt, der existierende NLI- und OpenIE-Modelle nutzt. Bei einer Frage werden die relevanten Entities identifiziert, die entsprechende Belief-Graph abgerufen und die Frage rekursiv umformuliert, bis sie zu einer Faktenfrage über die Grafik wird. Diese Faktenfrage wird dann zusammen mit den aus der Grafik extrahierten Sätzen einem LLM zugeführt, um die endgültige Antwort zu erhalten.

Die Experimente zeigten, dass SymbolicToM die Leistung von LLMs in False-Belief-Aufgaben signifikant verbessert. Im In-Domain-Test wurden Verbesserungen von bis zu 67 Genauigkeitspunkten erzielt. Bei Out-of-Domain-Tests, die Story-Struktur- und linguistische Generalisierung umfassen, übertraf SymbolicToM überwachte Ansätze. Insbesondere konnte GPT-4 mit SymbolicToM alle Datensätze vollständig lösen und zeigte beeindruckende Genauigkeitssteigerungen, zum Beispiel +42 Punkte für D1. Diese Ergebnisse unterstreichen die Effektivität von SymbolicToM als plug-and-play-Methode, die eine interpretierbarere Schlussfolgerung ermöglicht und das Overfitting-Risiko minimiert.</sample>
    <sample id="180">Der/die Referent*in heißt Myra Cheng.</sample>
    <sample id="181">Die Rednerin stellt ihre Arbeit über die Extraktion von Skriptwissen aus großen Sprachmodellen für die Planung mit eingeschränkter Sprache vor. Sie erklärt, dass sie in der Alltagssprache Sprachmodelle zum Planen und Ausführen von Schritt-für-Schritt-Anweisungen verwenden, um abstrakte Ziele zu erreichen. Obwohl frühere Arbeiten erfolgreich waren, fehlten ihnen spezifische Ziele mit einschränkenden Faktoren. Sie definiert dieses Problem als eingeschränkte Sprachplanung. Die Rednerin hat ein Dataset für die eingeschränkte Sprachplanung erstellt, indem sie abstrakte Ziele mit multiplen Einschränkungen für Mensch-im-Kreis-Datenerfassung unter Verwendung von InstructGPT erweitert hat. Sie stellte fest, dass die semantische Vollständigkeit in generierten Skripten zwar akzeptabel ist, die Treue zu den Einschränkungen jedoch nicht garantiert werden kann. Sie präsentiert eine Methode, die auf der Idee der Over-Generation und Filterung basiert, um die Generierungsqualität zu verbessern. Zuerst generiert sie spezifische Ziele mit InstructGPT über In-Context Learning. Zweitens generiert InstructGPT K-Skripte für spezifische Ziele. Drittens wird ein Filtermodell entwickelt, um die treuen Skripte zu identifizieren. Sie wandelt Skripte und Ziele in InstructGPT-Einbettungen um und berechnet die Kosinusähnlichkeit als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Um die Qualität von Validierungs- und Testsets sicherzustellen, bittet sie Crowdworker, falsche Stichproben zu finden und zu überarbeiten. Die Rednerin stellt fest, dass die Feinabstimmung kleinerer Sprachmodelle auf dem generierten Coscript-Datensatz qualitativ hochwertigere Skripte generieren kann als große Sprachmodelle. Sie hofft, dass Coscript eine wertvolle Ressource zur Weiterentwicklung der Forschung im Bereich Sprachplanung sein kann.</sample>
    <sample id="182">Im Kontext dieser Arbeit bezieht sich Tropikalismus auf ein Klischee, das mit Latina-Frauen assoziiert wird und durch Wörter wie "lebendig" und "kurvig" zum Ausdruck kommt.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen basierend auf den Antworten menschlicher Probanden auf dieselben Anweisungen erstellt, die den KI-Modellen gegeben wurden.</sample>
    <sample id="184">Die in der vorherigen Arbeit eingeführte bedingte gegenseitige Information (CXMI) wurde zur Messung der Kontextnutzung in dieser Arbeit verwendet.</sample>
    <sample id="185">DrBERT ist ein biomedizinisches Modell, das auf dem 7,4 GB großen NACHOS-Datensatz vortrainiert wurde. ChuBERT ist ein klinisches Modell, das auf einem 4 GB großen Datensatz von anonymisierten klinischen Notizen aus dem Datenlager des Universitätsklinikums Nantes vortrainiert wurde.</sample>
    <sample id="187">Es sind 3 Autoren an der Arbeit beteiligt.</sample>
    <sample id="188">Iteratives Transferlernen ist ein Prozess, bei dem ein Modell durch Training an neuen Daten aus jeder Runde des aktiven Lernens aktualisiert wird.</sample>
    <sample id="189">Der Datensatz zielt darauf ab, die Verwendung indirekter Bezugsausdrücke in informellen Gesprächen zu erforschen, wenn Benutzer eine Auswahl treffen.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter extrahieren, indem er von den Einbettungen lernt, die von einem EaaS bereitgestellt werden.</sample>
    <sample id="191">An der Arbeit sind drei Autoren beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">Der Redner präsentiert CAME, einen neuartigen Speicher-effizienten Optimierer für große Sprachmodelle (LLMs). LLMs verlassen sich oft auf adaptive, gradientenbasierte Optimierer, die jedoch den Speicherbedarf für die Speicherung von Momentenschätzungen der Parametergradienten verdreifachen. Bestehende Speicher-effiziente Optimierer reduzieren den Speichernutzung, aber auf Kosten der Leistung.

Die Herausforderung besteht darin, einen Optimierer zu entwickeln, der schnelle Konvergenz und geringen Speichernutzung gleichzeitig erreicht. Adasfactor zum Beispiel hat einen analytischen Lösung für die NMF-Operation (nicht-negative Matrixfaktorisierung) vorgestellt, aber NMF-Operationen können ineffizient sein. CAME schlägt eine konfidenzbasierte Strategie vor, um das Problem der fehlerhaften Aktualisierungen zu lösen und die Stabilität des Trainingsprozesses zu verbessern. CAME schlägt vor, die Instabilitätsmatrix UT zu berechnen, die Residuale zu verwenden, um die Instabilität zu messen, und die quadratische Wurzel davon als Nenner für MT zu verwenden, um einen aktualisierten Schritt zu machen.

Experimente zur vortrainierten BERT-Architektur auf BookCorpus und Wikipedia zeigen, dass CAME eine signifikante Verbesserung der Validierungsgenauigkeit gegenüber Adam und Adasfactor bietet. Darüber hinaus zeigt CAME im Vergleich zu anderen Optimierern einen reduzierten Speicherbedarf.

Zusammenfassend lässt sich sagen, dass CAME ein konfidenzbasierter, speichereffizienter Optimierer ist, der adaptive, konfidenzbasierte Aktualisierungen unterstützt, die durch das Residual zwischen der vorhergesagten und der generierten Aktualisierung gesteuert werden. Er bietet eine hervorragende Leistung bei der Schulung großer Sprachmodelle und eignet sich gut für das Training großer Batch-Größen.</sample>
    <sample id="193">Nicht angegeben.</sample>
    <sample id="194">Die Autoren gehören der University of Washington, der Carnegie Mellon University und dem Allen Institute for AI an.</sample>
    <sample id="195">In der vorliegenden Arbeit wird ein neuer hierarchischer Ansatz zur Fragezerlegung vorgestellt, der die Erklärung von Antworten und die Integration von Wissen aus heterogenen Quellen verbessert. Bestehende Methoden für erklärbare Fragebeantwortung (XQA) sind entweder neuro-symbolisch oder dekompositionsbasiert. Neuro-symbolische Methoden sind auf strukturierte Wissensbasen (KBs) beschränkt, während dekompositionsbasierte Methoden nur Freitextkorpora verwenden, was durch die Vielfalt der natürlichen Sprache erschwert wird. Die vorgeschlagene Methode, Reasoning over Hierarchical Question Decomposition Tree (RoHT), adressiert diese Herausforderungen. Sie besteht aus zwei Phasen: 1) dem Verständnis komplexer Fragen durch den Aufbau eines Hierarchical Question Decomposition Tree (HQDT) und 2) der probabilistischen Argumentation über den HQDT, die Wissen aus KBs und Textkorpora auf verschiedenen Ebenen des Baums integriert. RoHT bestimmt die Granularität der Fragezerlegung dynamisch und findet die optimale Lösung unter verschiedenen möglichen Optionen, indem es Unsicherheiten bei der Baumgenerierung und Beantwortung berücksichtigt. Experimente auf den Datensätzen KQA Pro und Musique zeigen, dass RoHT die Leistung bestehender Methoden sowohl mit unvollständigen KBs als auch mit Textkorpora deutlich übertrifft.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer auf der linken Seite ist: „I saw Bart and Lisa“.</sample>
    <sample id="197">Der Stand der Technik ist, dass mehrere Dimensionen der Chat-Qualität, wie Relevanz, Konsistenz und emotionales Verständnis, bewertet werden.</sample>
    <sample id="198">Weil große Sprachmodelle mit immer längeren Kontextfenstern aufkommen.</sample>
    <sample id="199">Ja, das mehrsprachige Training führte zu einem Leistungsabfall in 7 von 10 englischen Datensätzen im Vergleich zum einsprachigen englischen Modell, was als „Fluch der Mehrsprachigkeit“ bekannt ist.</sample>
    <sample id="200">Nein. Die Annotatoren kennen die Namen der Entitäten, aber sie kennen die Entitäten nicht unbedingt. Sie können zusätzliche Informationen zu den Entitäten erhalten, indem sie auf einen Google-Suchlink klicken, um mehr über sie zu erfahren.</sample>
    <sample id="201">Es wurden die neuesten MT-Metriken (neurale MT-Metriken) verwendet, die eine bessere Korrelation mit menschlichen Bewertungen aufweisen.</sample>
    <sample id="202">Ja, die Regression wirkt sich auf die Generalisierung für alle NER-Typen aus, aber besonders stark auf die Typen "Person" und "Ort".</sample>
    <sample id="203">Positionalität ist wichtig für NLP, weil sie die Perspektiven und Entscheidungen der Ersteller von Datensätzen und Modellen beeinflusst, was zu systematischen Leistungsunterschieden in der Technologie zwischen verschiedenen Bevölkerungsgruppen führen kann. Dies ist besonders wichtig, da NLP-Aufgaben subjektiver und sozial orientierter werden.</sample>
    <sample id="204">Weder noch, stattdessen wurde festgestellt, dass multilinguale LLMs (wie Codex und BLOOM) für Aufgaben des cross-lingualen Semantic Parsings immer noch unzureichend sind.</sample>
    <sample id="205">Shangbin Feng präsentiert eine Studie, die untersucht, wie politische Voreingenommenheit in vortrainierten Sprachmodellen (LLMs) entsteht und sich auf nachgelagerte Aufgaben auswirkt. Die Forschung zeigt, dass vortrainierte Daten, insbesondere Nachrichten und soziale Medien, LLMs mit unterschiedlichen politischen Neigungen beeinflussen, die alle vier Quadranten des politischen Kompasses abdecken.

Durch das Vortrainieren von Modellen wie RoBERTa und GPT-2 auf parteiischen Korpora konnten die Forscher signifikante politische Verschiebungen in den LLMs feststellen. Die LLMs zeigten auch eine erhöhte Polarisierung nach wichtigen politischen Ereignissen, wie der US-Präsidentschaftswahl 2017.

In nachgelagerten Aufgaben zur Erkennung von Hassrede und Fehlinformationen zeigten linksgerichtete LLMs eine bessere Leistung bei der Erkennung von Hassrede, die auf Minderheitengruppen abzielt, während rechtsgerichtete LLMs besser bei Hassrede waren, die auf Mehrheitsgruppen abzielt. Ähnliche Muster zeigten sich bei der Erkennung von Fehlinformationen.

Diese Ergebnisse verdeutlichen ein Fairness-Dilemma: Das „Sanieren“ von Trainingsdaten zur Beseitigung von Voreingenommenheit könnte Zensur oder Ausschließung nach sich ziehen, während das Nicht-Sanieren zur Verbreitung von Voreingenommenheit führt. Die Studie hebt die Notwendigkeit hervor, die politischen Voreingenommenheiten in LLMs anzuerkennen und anzugehen, um faire und unvoreingenommene NLP-Anwendungen zu gewährleisten.</sample>
    <sample id="206">Wir verwenden das RoBERTA-Basismodell plus einen Klassifizierer-Header für das Transferlernen.</sample>
    <sample id="207">Es wurden die neuesten Testsets verwendet, um Überschneidungen zwischen Test- und Trainingsdaten sowie Überanpassungen an Bewertungsdaten zu vermeiden.</sample>
    <sample id="208">Die Autoren haben schließlich drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">Die vorgeschlagene Methode verbessert die Genauigkeit von 66% auf 92% (Tabelle 1).</sample>
    <sample id="210">Der Name des Redners ist Shuheng Liu.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz können als Benchmark verwendet werden.</sample>
    <sample id="212">Die Arbeit experimentiert mit T5, also zwei kleineren Modellen.</sample>
    <sample id="213">OFA (One For All).</sample>
    <sample id="215">Dieses Video präsentiert eine Debatte über die Abhängigkeitsstruktur der Koordination in der englischen Sprache. Der Referent, Adam Przepiórkowski, legt verschiedene Theorien dar: den Bouquet/Stanford-Ansatz, den Chain/Moscow-Ansatz (beide asymmetrisch und bevorzugen das erste Konjunkt als Kopf der Koordination), den Conjunction-headed/Prague-Ansatz und den Multi-headed/London-Ansatz (beide symmetrisch und betrachten das Konjunkt oder alle Konjunkte als Köpfe).

Der Vortragende stellt ein neuartiges Argument vor, das auf dem Prinzip der "Dependency Length Minimization (DLM)" basiert, welches besagt, dass die Wortreihenfolge dazu neigt, die Abhängigkeitslängen zu minimieren. Er demonstriert dies anhand von Beispielen, bei denen kurze direkte Objekte dem Verb nahestehen, während lange direkte Objekte nach einem Adjunkt verschoben werden können, um die Abhängigkeitslängen zu minimieren.

Anschließend präsentiert er statistische Beobachtungen aus dem Penn Treebank. Linke Konjunkte sind tendenziell kürzer, und diese Tendenz nimmt mit zunehmender Längendifferenz zu. Dieser Effekt tritt jedoch nur auf, wenn der Regens links vom Konjunkt steht oder abwesend ist, nicht aber, wenn er rechts steht. Die Implikation ist, dass dies die symmetrischen Abhängigkeitsstrukturen der Koordination unterstützt und die asymmetrischen widerlegt.</sample>
    <sample id="217">Der Referent stellt seine Arbeit "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" vor. Er erläutert die Motivation, die darin besteht, dass bisherige Methoden zur Dialoggenerierung auf Einzelattributen basierten, was die Komplexität von Multi-Attribut-Generierungen vernachlässigt. Zudem fehlt eine einheitliche Metrik zur Bewertung der Kontrollierbarkeit.

Die Beiträge der Arbeit sind:
- Untersuchung der Kompositionsgeneralisation für Multi-Attribut-Dialoggenerierung und Feststellung, dass bestehende Modelle diesbezüglich Mängel aufweisen.
- Entwicklung von DCG, einer entwirrten, kontrollierbaren Generierung, die Attributkonzepte aus gesehenen Werten lernt und einen Entwirrungsverlust nutzt, um verschiedene Attributkombinationen zu entwirren.
- Einführung eines einheitlichen, referenzfreien Bewertungsrahmens (MAE) für verschiedene Granularitäten von Attributen.

Die Methodik beinhaltet attributorientierte und aufgabenorientierte Prompts sowie Entwirrungslernen, um die Generierungsfähigkeit zu verbessern und Attributkombinationen zu unterscheiden. Für die Bewertung wird MAE verwendet, das diskrete Prompt-Vorlagen und kontinuierliche Prompt-Token nutzt.

Experimentelle Ergebnisse auf DailyDialog-CG zeigen, dass DCG alle anderen Baselines in Attribut-Kontrollierbarkeit und Textqualität übertrifft. Die qualitative Analyse bestätigt die Effektivität der Methode, selbst bei ungesehenen Attributkombinationen. MAE korreliert zudem gut mit menschlichen Bewertungen. Die Visualisierung von Prompts demonstriert, dass die Methode Attributkombinationen entwirren und Beziehungen zwischen Attributen lernen kann, um von gesehenen zu ungesehenen Kombinationen zu generalisieren.

Zusammenfassend bietet die Arbeit einen vielversprechenden Ansatz zur Multi-Attribut-Kontrolle in der Dialoggenerierung und eine robuste Bewertungsmethode.</sample>
    <sample id="218">Google.</sample>
    <sample id="219">In dieser Arbeit schlagen wir eine mehrstufige Pipeline zur Extraktion von Finanzsignalen aus Finanzberichten vor. Die Pipeline gliedert sich in drei Stufen: Dokumentensegmentierung, Relationenerkennung und Hervorhebung. Für die Hervorhebung schlagen wir eine zweistufige Feinabstimmung für einen domänenadaptiven Highlighter vor, der aus einem natürlichen Sprachinferenzdatensatz und pseudo-gelabelten revidierten Paaren aus den Finanzberichten besteht. Wir schlagen eine gemischte Verlustfunktion vor, die sowohl Cross-Entropie als auch KL-Divergenz nutzt, um sowohl die Qualität der Pseudo-Labels als auch die Lernqualität zu verbessern. Um unsere Methode zu bewerten, stellen wir ein manuell annotiertes Benchmarking-Dataset namens FINAL (FINancial ALpha) bereit. Unsere experimentellen Ergebnisse zeigen, dass unsere vorgeschlagene Methode eine bessere Leistung als andere Referenzmethoden erzielt. Die domänenadaptiven Hervorhebungsmodelle übertreffen alle anderen Einstellungen und verlieren nicht die Allgemeingültigkeit der Token-Darstellungen.</sample>
    <sample id="220">Die Autoren gehören der Stony Brook University an.</sample>
    <sample id="221">In der Arbeit wurde die Übersetzung von Deutsch nach Englisch untersucht.</sample>
    <sample id="222">Die Arbeit befasst sich mit dem Problem der Open-Domain-Frage-Antwort (QA) in verschiedenen Domänen, insbesondere wenn das Quellmodell auf einer allgemeinen Domäne wie Wikipedia trainiert wird und auf eine neue Domäne wie Biomedizin übertragen werden muss. Ziel ist es, die Anpassungsfähigkeit des Quellmodells an die Zieldomäne zu untersuchen und geeignete Dateninterventionen zu finden, um die Generalisierung zu verbessern.

Die Studie unterteilt die Anpassungsmethoden in „Few-shot“ und „Zero-shot“. Bei „Few-shot“-Methoden werden große Sprachmodelle (LLMs) mit wenigen Beispielen aus der Zieldomäne dazu angeregt, zusätzliche Daten zu generieren, die dann in Cloze-Stil-Fragen umgewandelt werden, um die Retriever- und Reader-Modelle zu adaptieren. Dies führt zu einer Verbesserung der Retriever-Performance um bis zu 22% und der Reader-Performance um bis zu 24% (F1-Wert).

Bei „Zero-shot“-Methoden, bei denen keine Beispiele aus der Zieldomäne verfügbar sind, werden die Interaktionen zwischen Frage, Antwort und Kontext kontrolliert. Es wird festgestellt, dass das Format der Fragen (Standard-WH-Fragen vs. Cloze-Stil) die Leistung nicht wesentlich beeinflusst, aber Cloze-Stil-Fragen leichter zu sammeln sind. Eine gleichmäßige Verteilung der Antworttypen führt zu den besten Ergebnissen. Für die Kontexterkennung sind lernbasierte Modelle empfindlich gegenüber Datenverteilung, während BM25 besser funktioniert.

Die Arbeit zeigt auch, dass die Wirksamkeit von Dateninterventionen vom Typ der Datensatzverschiebung (z.B. Konzeptverschiebung, Kovariatverschiebung, vollständige Verschiebung oder keine Verschiebung) abhängt. Few-shot-Anpassungen sind für alle Verschiebungsarten vorteilhaft, während Zero-shot-Anpassungen insbesondere bei Konzept- und Kovariatverschiebungen und bei keiner Verschiebung nützlich sind.</sample>
    <sample id="223">Der/die Referent*in heißt Shangbin Feng.</sample>
    <sample id="224">Die untersuchten Modelle sind LHA, Sent-LaBSE, Sent-RoBERTa, CATS-C3G, VecAlign, BERTalign und MASSalign.</sample>
    <sample id="225">53 Aufgaben aus 9 Gruppen werden für das Training verwendet. Die restlichen 9 Aufgaben, die zur Gruppe "Commonsense Reasoning" gehören, sowie 5 weitere Aufgaben aus den Gruppen "VQA" und "Miscellaneous" werden für das Testen reserviert. Außerdem werden 20 Aufgaben aus dem Testsatz von Natural Instructions als unbekannte Aufgaben für NLP verwendet.</sample>
    <sample id="226">An der Arbeit sind drei Autoren beteiligt.</sample>
    <sample id="227">In this paper, we propose Pangu, a unified framework for grounded language understanding. Language models are mostly trained with textual corpus and without grounding, which is a key missing component in current research. Directly generating plans (programs) may not be the optimal way of using LMs for grounded language understanding, as they can sometimes produce grammatically incorrect or invalid plans. To address these issues, Pangu leverages the strengths of both neural and symbolic approaches. A symbolic agent interacts with the environment and proposes only valid candidate plans, while a language model is only responsible for scoring them. We argue that discrimination is much easier for language models to excel at than generation. Pangu achieves new state-of-the-art results on KBQA, demonstrates strong sample efficiency, and significantly improves generalizability compared to autoregressive models. The framework also mitigates overfitting of seen structures during training. We believe that discrimination is a much better strategy for using language models for grounded language understanding.</sample>
    <sample id="228">Die Autoren experimentierten an vier Datensätzen: AG News, MIND, SST2 und Enron Spam.</sample>
    <sample id="229">This paper introduces two new tasks for argumentative writing support: suboptimal-claim detection and claim improvement suggestion. Suboptimal-claim detection involves deciding if a claim needs revision or is optimally phrased. Claim improvement suggestion involves identifying the types of quality issues that need improvement when revising a claim. The authors explore how to model the quality of argumentative texts based on implicit revision patterns from collaborative editing behaviors in online debate platforms like Kialo. This approach involves learning from human revision behaviors rather than explicitly defining good or bad claims. The paper also discusses four key challenges: representativity and reliability (ensuring the final version is truly optimal), model complexity and architecture (selecting appropriate models and pre-training strategies), contextuality (considering relevant contextual information like topic expertise or parent claims), and topical and user bias (addressing noise and biases from users and moderators). The experiments conclude that revision-based data is effective for these tasks, modeling the distance between claim versions is beneficial for detection, and the impact of contextual information depends on the task and quality issue.</sample>
    <sample id="231">NACHOS ist ein Open-Source-Datensatz von 1,1 Milliarden Wörtern an heterogenen medizinischen Daten, die aus verschiedenen medizinischen Domänen, Naturen und Stilen gecrawlt wurden.</sample>
    <sample id="232">Der/Die Referent*in heißt David Vilar Torres.</sample>
    <sample id="233">Sara Papi stellt eine neue Simultan-Sprachübersetzungsmethode (SimulST) namens EDAtt vor, die die Cross-Attention-Mechanismen eines bereits vorhandenen Offline-ST-Modells nutzt. Derzeitige SimulST-Modelle erfordern oft spezifische Architekturen und komplizierte Trainingsabläufe, die zu mehreren Modellen für verschiedene Latenzregime führen. EDAtt überwindet diese Probleme, indem es ein einziges Offline-Modell verwendet und die Latenz durch spezifische Parameter steuert. Die Methode entscheidet, ob eine teilweise Übersetzung basierend auf der Konzentration der Aufmerksamkeit auf die letzten Sprachframes ausgegeben werden soll. Wenn die Aufmerksamkeit nicht auf die letzten Lambda-Sprachframes konzentriert ist (Summe unter Schwellenwert alpha), wird ein Wort ausgegeben. Ist die Aufmerksamkeit jedoch konzentriert (Summe über Schwellenwert alpha), wird kein Wort ausgegeben, bis weitere Sprachframes empfangen werden, um eine stabile Übersetzung zu gewährleisten. Experimente zeigen, dass EDAtt alle gängigen Strategien für Offline-Modelle übertrifft und die schnellste Strategie ist, wenn die tatsächlich verstrichene Zeit berücksichtigt wird. EDAtt zielt darauf ab, die Reproduzierbarkeit der Forschung zu erleichtern, indem der Code und die Modelle Open Source zur Verfügung gestellt werden.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Qualität der Übersetzung, wobei sich die BLEURT-Werte zwischen verschiedenen Prompts für den gleichen Satz um mehr als einen Punkt und in extremen Fällen um bis zu 40 Punkte unterscheiden.</sample>
    <sample id="235">Die Autoren gehören der Carnegie Mellon University und dem Language Technologies Institute an.</sample>
    <sample id="236">Die 5 Anweisungen der Expert*innen sind:
1. Erzeuge eine Bildunterschrift für &lt;bin_198&gt; &lt;bin_32&gt; &lt;bin_400&gt; &lt;bin_193&gt;.
2. Wähle die Region aus, die den Text „den“ enthält.
3. Wähle die Region des Objekts aus, das durch „Ein blauer Zug vorne“ beschrieben wird.
4. Gib bei gegebenem Bildinhalt an, ob ausreichend Informationen vorhanden sind, um die Frage „Ist es ein sonniger Tag?“ zu beantworten.
5. Die Frage ist relevant für das Bild oder die Frage ist irrelevant für das Bild.</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle zur Nutzung von Informationen aus mehreren Quellen zu testen, indem sie einen Kernreferenzauflösungstest verwenden, der so konzipiert ist, dass er die Fähigkeit zum Ziehen von Informationen aus prä-trainierten und Inferenz-Zeit-Wissensquellen untersucht.</sample>
    <sample id="238">This paper introduces MeetingBank, a new benchmark dataset for meeting summarization. MeetingBank comprises 1,366 city council meetings from six U.S. cities, totaling 3,579 hours of transcribed audio and nearly 7,000 expertly written summaries. The dataset aims to address the scarcity of high-quality meeting summaries and the challenge of identifying reliable sources for public meetings. Each entry in MeetingBank includes meeting transcripts, reference summaries, and URLs to various useful resources.

A detailed analysis of the dataset's extractive properties reveals that most city council meeting summaries have a coverage score between 0.7 and 0.9, indicating a focus on important points rather than full abstraction. Density scores vary, with Seattle and Boston showing the highest, and Denver the lowest, suggesting varying levels of editing in meeting minutes.

For model evaluation, ten top-tier summarization systems were tested. Extractive models like Extr-Oracle performed well, indicating that reference summaries often draw heavily from source transcripts. Among abstractive models, DialogLM achieved the highest ROUGE-2 score (60.12%), designed for long dialogue summaries. GPT-3 showed exceptional human evaluation scores in fluency and coherence but was less impressive in informativeness and factuality, suggesting a need for automatic metrics that better align with human preferences. MeetingBank is a valuable resource for researchers developing advanced meeting summarizers and offers insights into the decision-making processes of city councils.</sample>
    <sample id="239">Hallo allerseits. Mein Name ist David Bilar und ich werde einen kurzen Überblick über das Papier "Prompting PaLM for Translation: Assessing Strategies and Performance" geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. PaLM ist ein großes Sprachmodell mit 540 Milliarden Parametern, das letztes Jahr, im Jahr 2022, vorgestellt wurde. Es wurde an einer großen Sammlung von Texten trainiert, die 780 Milliarden Token umfasste. Zum Zeitpunkt der Veröffentlichung erreichte es den Stand der Technik bei Hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zum Prompting von großen Sprachmodellen für die maschinelle Übersetzung. Wir bewerten die Übersetzungsfähigkeiten solcher Modelle unter Verwendung der besten Praktiken der MT-Community. Dies beinhaltet die Verwendung der neuesten Testsätze, um eine Überlappung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden, und wir vergleichen mit den modernsten Systemen, also den besten performenden Systemen der WMT-Bewertung. Wir verwenden modernste neuronale MT-Metriken und zeigen zusätzlich auch Experten-basierte menschliche Evaluationsergebnisse. Schließlich geben wir einige Empfehlungen für Prompt-Auswahlstrategien. Das Prompting hat einen großen Einfluss auf die Leistung von großen Sprachmodellen für die Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir One-Shot-Prompting verwenden und zwei verschiedene Prompts für jeden Satz bereitstellen. Die Mehrheit der Sätze (516 von 1000) zeigt einen Unterschied von mehr als 1 BLEURT-Punkt. Und dieser kann in extremen Fällen bis zu 40 BLEURT-Punkte betragen. Es ist also wichtig, eine gute Prompting-Strategie zu wählen. In unseren Experimenten haben wir uns für eine 5-Shot-Prompting-Strategie entschieden, bei der wir jeden Satz, den wir dem System zur Verfügung stellen, mit der Sprache kennzeichnen, in der er sich befindet. In diesem Beispiel, bei dem wir eine Übersetzung vom Deutschen ins Englische durchführen, sind die deutschen Sätze, die Quellsätze, mit "German:" gekennzeichnet, und die englischen Übersetzungen mit "English:". Wir haben festgestellt, dass die tatsächliche Form des Promptings im Fall von mehreren Shot-Prompting keinen großen Einfluss hat. Es ist entscheidend für Zero- und One-Shot-Prompting, aber wenn wir, wie in unserem Fall, zu 5-Shot-Prompting übergehen, gibt es kaum einen Unterschied zur tatsächlichen Form des Promptings. Es sind die Beispiele, die den größten Teil des Gewichts tragen. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quellsatz. Es ist also wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten der WMT-Evaluationen oder den Entwicklungsdaten. Die Entwicklungsdaten sind viel sorgfältiger kuratiert und von höherer Qualität als die Trainingsdaten, und die Ergebnisse zeigen eine bessere Leistung bei Verwendung der Entwicklungsdaten. Nichtsdestotrotz haben spezialisierte State-of-the-Art-Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. Aber PaLM kommt einem kommerziellen System, in unserem Fall haben wir uns entschieden, mit Google Translate zu evaluieren, ziemlich nahe. Die Erkenntnisse, die wir aus der menschlichen Evaluation gewonnen haben, die wir mit dem MQM-Framework durchgeführt haben, sind, dass die Sprachflüssigkeit von PaLM mit der von State-of-the-Art-Systemen vergleichbar ist. Der Hauptunterschied liegt jedoch in der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint also, dass PaLM die Wahl trifft, eine besser klingende Übersetzung zu erstellen, manchmal, indem Teile des Quellsatzes weggelassen werden, die in der Übersetzung weggelassen werden. Allerdings ist die Kategorie "Stil/Ungrammatisch" für PaLM niedriger als für die State-of-the-Art-Systeme, was ein zusätzliches Signal ist, dass PaLM wirklich flüssige Ausgaben liefert, aber immer noch mit einigen Problemen bei der Genauigkeit. Das war es für diesen wirklich kurzen Überblick. Für weitere Details besuchen Sie bitte die vollständige Präsentation des Papiers. Vielen Dank.</sample>
    <sample id="240">00:00
Hallo, ich bin Dawei, ein Doktorand an der Universität des Saarlandes in Deutschland. In diesem Video möchte ich unsere neueste Arbeit vorstellen: "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Dies ist eine Gemeinschaftsarbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.

00:21
Ich möchte zunächst eine kurze Einführung in die schwache Überwachung und das schwach überwachte Lernen geben. Bei der schwachen Überwachung kennzeichnen wir die Daten nicht manuell. Stattdessen kennzeichnen wir die Daten mithilfe von schwachen Kennzeichnungsquellen, wie z. B. einfachen heuristischen Regeln, Wissensdatenbanken oder Low-Cost-Crowdsourcing, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwachen Annotationen viel kostengünstiger, aber sie sind auch verrauscht, was bedeutet, dass ein gewisser Anteil der Annotationen falsch ist. Wenn wir neuronale Netze direkt auf schwach gekennzeichneten Daten trainieren, neigen die neuronalen Netze dazu, das Kennzeichnungsrauschen zu memorisieren und nicht zu generalisieren. Beim schwach überwachten Lernen (WSL) werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze trotz dieses Kennzeichnungsrauschens robust zu trainieren, sodass die trainierten Modelle immer noch gut generalisieren.

01:14
In jüngsten Arbeiten zum WSL (schwach überwachtes Lernen) wird häufig behauptet, dass Modelle nur auf schwach überwachten Daten trainiert und eine Genauigkeit von XX% erreicht werden. Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt einen Haken. Der Haken ist, dass man davon ausgeht, dass ein zusätzlicher sauberer Validierungsdatensatz für die Modellauswahl verfügbar ist. Wir haben uns intensiv mit diesem Problem beschäftigt, da dies bedeutet, dass im schwach überwachten Lernen zusätzliche manuelle Annotationen erforderlich sind. Aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen.

01:58
Die oben genannten Beobachtungen veranlassten uns, drei Forschungsfragen zu stellen. Erstens: Ist saubere Validierungsdaten notwendig für WSL? Oder können wir stattdessen vielleicht einen verrauschten Validierungsdatensatz verwenden? Zweitens: Wenn saubere Daten erforderlich sind, oder wenn saubere Daten für das Funktionieren von WSL obligatorisch sind, wie viele saubere Stichproben benötigen wir dann? Und schließlich: Sollen wir die verfügbaren sauberen Stichproben nur zur Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen?

02:30
Wir haben diese Forschungsfragen in unserer Arbeit behandelt, und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass neuere WSL-Methoden tatsächlich saubere Validierungsdaten benötigen, um korrekt zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall, wie in dieser Abbildung dargestellt. Wenn keine sauberen Validierungsdaten vorhanden sind, können die trainierten Modelle über die ursprünglichen schwachen Labels hinaus nicht generalisieren, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber gekennzeichnete Daten benötigen, um korrekt zu funktionieren, und die Annotationskosten für den Erhalt sauberer Validierungsdaten sollten nicht übersehen werden.

03:14
Unser zweites Ergebnis ist, dass die Erhöhung der Anzahl der sauberen Validierungsdaten WSL-Ansätzen hilft, eine bessere Leistung zu erzielen, wie in der Abbildung links gezeigt. Typischerweise benötigen wir nur 20 Stichproben pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist nicht das Ende der Geschichte, denn wenn wir sowieso entscheiden, auf saubere Stichproben zuzugreifen, dann wird das direkte Training auf ihnen sogar eine bessere Leistung erzielen. Die rechte Abbildung zeigt den Leistungsunterschied zwischen den Feinabstimmungsansätzen, die direkt auf saubere Daten angewendet werden, und den WSL-Ansätzen, die die sauberen Daten nur zur Validierung verwenden. Wie wir sehen können, beginnt die direkte Feinabstimmung ab 10 Stichproben pro Klasse, die WSL-Ansätze zu übertreffen.

04:07
Schließlich kann die in früheren WSL-Ansätzen beanspruchte Leistungsverbesserung leicht erreicht werden, indem die kontinuierliche Feinabstimmung auf den sauberen Validierungsstichproben zugelassen wird. Wie wir aus den Abbildungen ersehen können, schneidet das Basismodell, bezeichnet als FTw, zunächst schlechter ab als komplexere WSL-Methoden wie COSine. Wenn wir jedoch die kontinuierliche Feinabstimmung auf den sauberen Stichproben zulassen, schneidet FTw genauso gut ab wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz benötigen.

04:51
Zusammenfassend haben wir gezeigt, dass neuere WSL-Ansätze saubere Stichproben benötigen, damit sie korrekt funktionieren. Ihr Leistungsgewinn und ihre Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten lauten wie folgt: Erstens, geben Sie die Kriterien für die Modellauswahl an. Geben Sie beispielsweise an, ob die Modellauswahl anhand sauberer Validierungsstichproben erfolgt. Zweitens, WSL-Ansätze sollten mit Few-Shot-Learning-Baselines verglichen werden, sowohl für Arbeiten mit sauberen Stichproben. Drittens, kontinuierliche Feinabstimmung ist eine einfache, aber starke Baseline, die in zukünftigen Arbeiten in WSL berücksichtigt werden sollte.

05:37
Schließlich haben wir unseren Code Open Source gestellt. Sie finden ihn über den QR-Code auf dieser Folie. Bitte schauen Sie ihn sich gerne an. Vielen Dank und viel Spaß auf der Konferenz.</sample>
    <sample id="241">Ethan Mendes stellt eine neue Methode zur Erkennung von Fehlinformationen vor, die als "Human-in-the-Loop"-Ansatz bezeichnet wird. Diese Methode wird als realistischer bewertet als frühere Ansätze, die bei der Erkennung von Fehlinformationen oft unrealistische Datensätze verwendeten und den Menschen aus dem Prozess ausschlossen. Die von Mendes vorgeschlagene Methode verwendet einen End-to-End-Ansatz und integriert das menschliche Feedback in den gesamten Erkennungsprozess. Als konkretes Beispiel stellt er ein System zur Erkennung von COVID-19-Behandlungs-Fehlinformationen auf Twitter vor. Die Methode wurde erfolgreich eingesetzt, um frühzeitig Fehlinformationen zu erkennen, die die Menschen in der Vergangenheit zu falschen Behandlungen verleitet hatten. Sie ermöglichte die frühzeitige Erkennung, noch bevor Fehlinformationen in den Nachrichten dementiert werden konnten. Außerdem können 124,2 Tweets, die Richtlinienverletzungen enthalten, pro Arbeitsstunde eines Menschen identifiziert werden. Mendes hofft, dass diese Arbeit die Entwicklung nützlicherer Mensch-in-the-Loop-Systeme zur Erkennung von Fehlinformationen motiviert und einen konkreten Vergleichsstandard für zukünftige Systeme liefert.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind:

* **Vergleichende Bewertung:** Menschen wählen aus, welche von zwei Unterhaltungen besser ist.
* **Likert-Skala Bewertung:** Menschen bewerten Gespräche auf einer Likert-Skala (z. B. 1 bis 5).</sample>
    <sample id="243">Es sind 5 Autoren an der Arbeit beteiligt.</sample>
    <sample id="244">Das Hintergrundwissen, das im Beispiel mit Servin und Kea benötigt wird, ist: Richter entscheiden Fälle vor Gericht.</sample>
    <sample id="245">Lining Zhang präsentiert eine zweistufige Pipeline zur Rekrutierung hochwertiger Amazon Mechanical Turk (MTurk)-Arbeiter für die Zusammenfassung. Das Papier "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization" befasst sich mit den Problemen automatischer Metriken und den unzureichend verstandenen Best Practices für die Rekrutierung auf MTurk.

Die Pipeline besteht aus zwei Hauptaufgaben:
1. **Qualifizierungsaufgabe:** Bewertet die Fähigkeit der Bearbeiter, mehrere Dimensionen korrekt zu beurteilen. Aus 200 Teilnehmern qualifizierten sich 26 (13 %) für die nächste Phase.
2. **Ausdaueraufgabe:** Testet die Kapazität der Bearbeiter, eine hohe Arbeitslast zu bewältigen. Nur 12 Bearbeiter (6 %) bestanden diese Phase.

Die Pipeline-Arbeiter zeigten hohe Inter-Annotator-Agreement (IAA)-Werte, die die von Experten übertrafen. Ihre Cohens Kappa betrug 0,55 und Krippendorffs Alpha 0,443 (Gold-Arbeiter).

Die Arbeit vergleicht Pipeline-Arbeiter mit Baseline-MTurk-Arbeitern (die durch statistische Filterung durch MACE ausgewählt wurden) und CloudResearch-Arbeitern. Pipeline-Arbeiter hatten eine niedrigere anfängliche Rekrutierungsrate, aber ihr Krippendorffs Alpha (0,534) war höher als das von MACE (0,380) und CloudResearch (0,513), mit einem geringeren Kostenaufwand pro HIT.

Zusammenfassend bietet die Pipeline eine Best Practice für die Beschaffung von Annotationen mit hoher Übereinstimmung in großem Maßstab zu geringeren Kosten und reduziert die Ressourcenverschwendung durch verworfene Annotationen. Zukünftige Arbeiten werden sich auf die Einstellung von Arbeitern konzentrieren, die sowohl eine hohe Übereinstimmung als auch Korrektheit gewährleisten und die Pipeline auf verschiedene Aufgaben und Sprachen ausweiten.</sample>
    <sample id="246">Ja, der Code ist auf GitHub verfügbar unter mpoemsl/kitmus.</sample>
    <sample id="247">Dieses Video führt FactKG ein, einen neuen Datensatz für die Faktenüberprüfung, der sich auf Argumentation über Wissensgraphen konzentriert. Der Redner Jiho Kim von KAIST AI hebt die Notwendigkeit eines solchen Datensatzes hervor, da bestehende Datensätze sich auf Text- oder Tabellenbeweise verlassen, aber keine für Wissensgraphen vorhanden sind. Wissensgraphen sind aufgrund ihrer Zuverlässigkeit und Praktikabilität eine wertvolle Wissensquelle.

FactKG nutzt DBpedia als Wissensgraph und enthält Behauptungen sowohl im schriftlichen als auch im umgangssprachlichen Stil, um die praktische Anwendbarkeit zu erhöhen. Er umfasst fünf Arten der Argumentation: Ein-Hop, Konjunktion, Existenz, Mehr-Hop und Negation. Diese Argumentationstypen erfordern unterschiedliche Überprüfungsmethoden, wie das Prüfen von Verbindungen zwischen Entitäten oder mehrstufige Schlussfolgerungen.

Um umgangssprachliche Behauptungen zu generieren, wurden paraphrasierte Methoden verwendet, einschließlich eines umgangssprachlichen Stiltransfers und Präsuppositionsvorlagen. Baseline-Experimente zeigen, dass Modelle, die graphische Beweise verwenden, wie GEAR, eine überlegene Leistung gegenüber solchen ohne grafische Beweise erzielen. FactKG trägt dazu bei, die Nutzung von Wissensgraphen für die Faktenüberprüfung zu verbessern und die Robustheit von Systemen zur Überprüfung von Fakten zu fördern.</sample>
    <sample id="248">Nein, die Annotatoren sind nicht ausgewogen, da die überwiegende Mehrheit der Annotatoren aus englischsprachigen Ländern mit Universitätsabschluss stammt.</sample>
    <sample id="249">Sätze innerhalb der akzeptablen Domain wurden durcheinander gebracht, indem verschiedene Perturbationen hinzugefügt wurden. Diese Perturbationen umfassten Präfix/Suffix-Adverbien (z.B. "However, &lt;sent&gt;."), lange Präfix-Adverbien (z.B. "First and foremost, &lt;sent&gt;."), das Hinzufügen von Nebensätzen (z.B. "Regardless of what X thinks about it, &lt;sent&gt;.") und Zitate (z.B. "Yesterday, X said, '&lt;sent&gt;.'").</sample>
    <sample id="250">Eine dimensionale Bewertung ist eine feinere Methode, um die Stärken und Schwächen eines Modells in Bezug auf verschiedene Qualitätsaspekte des Dialogs zu verstehen.</sample>
    <sample id="251">Die Autoren gehören der University of Science and Technology of China, Microsoft Research Asia, Beijing Jiaotong University und Sony AI an.</sample>
    <sample id="252">Diese Präsentation stellt "U-CREAT" vor, eine unüberwachte Methode zur Fallrecherche in juristischen Dokumenten durch Ereignisextraktion.  Sie präsentiert den neuen Datensatz "IL-PCR" für das indische Rechtssystem, der 7070 Rechtsfälle mit durchschnittlich 6,775 Zitaten pro Abfrage enthält. U-CREAT nutzt Ereignisse, die als Prädikat (typischerweise ein Verb) und die entsprechenden Argumente definiert sind, extrahiert sie mittels Spacy und bildet Tripel. Die Pipeline umfasst Vorverarbeitung, Abhängigkeitsanalyse und Nachverarbeitung, um Ereignisse aus Abfrage- und Kandidatendokumenten zu gewinnen. Eine Interaktionsmatrix gleicht diese Ereignisse ab und liefert eine Rangfolge der Kandidaten. Experimente zeigen, dass ereignisbasierte Modelle herkömmliche zählbasierte und Transformer-basierte Ansätze, einschließlich auf Rechtstexte spezialisierter Modelle, deutlich übertreffen. Besonders "Event Filtered Docs" erzielt einen F1-Score von 39,15 % auf IL-PCR und 26,94 % auf COLIEE'21, mit geringerer Inferenzzeit. U-CREAT ist unüberwacht, erfordert keine korpusspezifische Feinabstimmung und übertrifft sogar überwachte Methoden bei der Fallrecherche.</sample>
    <sample id="253">Dieses Video stellt DisorBERT vor, ein duales Domänenadaptationsmodell zur Erkennung von Anzeichen psychischer Störungen in sozialen Medien. Psychische Störungen sind psychologische Syndrome, die Denken, Fühlen, Stimmung und Verhalten beeinflussen. Angesichts der wachsenden Nutzung sozialer Medien besteht eine Chance, Verhaltensmuster zu analysieren. DisorBERT kombiniert Domänenadaption mit einer lexikongesteuerten Maskierungstechnik. Das Modell lernt zunächst die Sprache sozialer Medien von Reddit-Daten und spezialisiert sich dann auf die Domäne psychischer Störungen unter Verwendung eines maßgeschneiderten Lexikons. Dies ermöglicht eine Anpassung des Vokabulars, eine Aktualisierung des semantischen Verständnisses und das Erlernen domänenspezifischer Aufgaben. Testergebnisse auf eRisk-Datensätzen zeigen, dass DisorBERT eine gute Balance zwischen Präzision und Wiedererkennung aufweist und MentalBERT übertrifft. Eine Analyse mit dem Beck Depression Inventory (BDI-Test) zeigte, dass DisorBERT Wörter mit negativerer oder psychologischer Orientierung generiert, was auf ein besseres Verständnis des Kontexts psychischer Störungen hindeutet. Zukünftige Arbeiten umfassen die Anwendung anderer lexikalischer Ressourcen und klinischer Daten zur weiteren Spezialisierung des Modells.</sample>
    <sample id="254">Das Video stellt eine Forschungsarbeit mit dem Titel "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction" vor. Das Forschungsteam befasst sich mit dem Problem von Rauschdaten bei der Dokumenten-Relations-Extraktion, einem gängigen Problem bei Large-Scale-Daten, die durch distale Überwachung oder Pseudo-Labeling auto-gelabelt werden. Solche Methoden bergen das Risiko der Einbringung von Rauschen durch falsch-positive Pseudo-Labels, während korrekte Beziehungen übersehen werden können.

Um dieses Problem zu lösen, schlagen die Forschenden ein Framework vor, das eine "Uncertainty Guided Label Denoising"-Methode (UGDRE) verwendet. Dieser Ansatz nutzt eine Instanz-Level-Unsicherheitsschätzung, um die Zuverlässigkeit von Pseudo-Labels zu messen. Er beinhaltet außerdem eine Re-Labeling-Strategie mit dynamischen Unsicherheitsschwellen für Klassen, die auf die "Long-Tail"-Problemstellung bei der Relations-Extraktion abzielt, um Pseudo-Labels mit hoher Unsicherheit herauszufiltern.

Experimente an zwei öffentlichen Datensätzen zeigen, dass das vorgeschlagene Framework die Leistung bestehender kompetitiver Baselines übertrifft. Die Autoren stellen fest, dass das Training von Baselines mit ihren rauschbefreiten distally supervised Daten die Leistung deutlich verbessert.</sample>
    <sample id="255">Die Form des Prompts ist entscheidend für Zero-Shot- und One-Shot-Prompts, aber nicht für Fünf-Shot-Prompts.</sample>
    <sample id="257">Die Autoren evaluierten vier Open-Domain-Dialogmodelle: BART-FID-RAG, Blender2, Emora und Blender-Decode.</sample>
    <sample id="258">Dieses Papier schlägt ein neues Bewertungsmetrik für natürliche Sprachverarbeitung vor, die als "LLM-Bewertung" bezeichnet wird und die die Qualitätsbewertung von Texten mithilfe großer Sprachmodelle (LLMs) umfasst. Im Gegensatz zu der traditionellen menschlichen Bewertung, die zu Instabilität und Reproduzierbarkeit führen kann, zielt die LLM-Bewertung darauf ab, eine robustere und reproduzierbarere Methode bereitzustellen.

In der LLM-Bewertung werden die LLMs mit detaillierten Anweisungen versehen, um die Qualität von Textproben in Bezug auf Attribute wie Grammatik, Kohärenz, Gefälligkeit und Relevanz zu bewerten. Die LLMs generieren dann Bewertungen basierend auf diesen Anweisungen.

Um die Effektivität der LLM-Bewertung zu überprüfen, wurden Experimente durchgeführt, bei denen die Bewertungen von LLMs mit denen von menschlichen Experten (Englischlehrern) verglichen wurden. Die Experimente umfassten die Bewertung von Geschichten, die von GPT-2 generiert und von Menschen geschrieben wurden.

Die Ergebnisse zeigten, dass größere LLMs, wie "davinci" und "ChatGPT", eine klare Präferenz für von Menschen geschriebene Geschichten zeigten, ähnlich wie die menschlichen Experten. Kleinere LLMs, wie T0 und "curie", zeigten jedoch keine signifikante Präferenz. Diese Ergebnisse deuten darauf hin, dass größere LLMs das Potenzial haben, eine zuverlässige Alternative zur menschlichen Bewertung in bestimmten Aufgaben zu sein.

Die Forschung schließt mit der Beantwortung zusätzlicher Fragen, wie z. B. der Übereinstimmung zwischen LLMs und menschlichen Bewertern bei einzelnen Bewertungen, der Auswirkung von Änderungen der Formulierungen in den Anweisungen und der Probenahme von LLM-Antworten sowie einer Untersuchung der Vor- und Nachteile der LLM-Bewertung im Vergleich zur menschlichen Bewertung.</sample>
    <sample id="259">Yusen Zhang von der Pennsylvania State University präsentierte ihre Arbeit an XSemPLR, einem umfassenden Benchmark für sprachübergreifende semantische Analyse (CLSP) in mehreren natürlichen Sprachen und Bedeutungsdarstellungen. Das Team ging die Einschränkungen bestehender CLSP-Modelle an, die separat vorgeschlagen und auf Datensätzen von begrenzten Aufgaben und Anwendungen ausgewertet werden.

XSemPLR ist ein Datensatz, der 9 Datensätze in verschiedenen Domänen, 5 semantische Analyseaufgaben, 8 Bedeutungsdarstellungen und 22 natürliche Sprachen in 15 Sprachfamilien enthält. Um den Benchmark zu bewerten, wurden sechs Einstellungen für Training und Evaluierung berücksichtigt: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot Transfer und Cross-lingual Few-shot Transfer.

Die Ergebnisse zeigten, dass m-T5 mit einsprachigem Training die beste Leistung über alle Datensätze hinweg erzielt. Mehrsprachige große Sprachmodelle (LLMs) wie CodeX und BLOOM sind immer noch unzureichend für CLSP-Aufgaben. Eine weitere Erkenntnis war, dass ein mehrsprachiges Training, das eine Mischung aus verschiedenen Sprachen nutzt, die Leistung des Encoders-Decoders und des Encoders-Pointer-Decoders verbessern kann. Es wurde auch festgestellt, dass das Chinesische die größte Transfer-Lernlücke aufweist, während das Deutsche die kleinste hat. Schließlich übertraf FunQL die anderen drei Bedeutungsdarstellungen, während SQL die schlechteste Leistung zeigte.</sample>
    <sample id="260">Es gibt 9 Autoren.</sample>
    <sample id="261">Ein guter Planer sollte Skripte schreiben, die **vernünftig** und **den Einschränkungen treu** sind.</sample>
    <sample id="262">An der Arbeit sind 8 Autoren beteiligt.</sample>
    <sample id="263">In-Context Learning (ICL) ist ein beliebtes Paradigma für die Nutzung großer Sprachmodelle (LLMs), ist aber aufgrund verschiedener Designentscheidungen instabil. Frühere Arbeiten haben gezeigt, dass diese Instabilität auf Verzerrungen in den Modellvorhersagen zurückzuführen ist, die durch diese Designentscheidungen verursacht werden.

Diese Arbeit befasst sich mit einem neuartigen, wichtigen Problem: dem **Domänen-Label-Bias**. Diese neue Art von Bias entsteht durch die spezifische Formulierung der Trainingsdaten (dem Aufgabenkorpus) und führt dazu, dass die Modelle stark voreingenommen sind, wenn sie bestimmte Begriffe aus diesem Korpus sehen, selbst wenn sie willkürlich sind.

Um die Auswirkungen aller Label-Bias-Arten (Vanilla-, Kontext- und Domänen-Label-Bias) ganzheitlich zu mildern, wurde eine neue Methode namens **Domänen-Kontext-Kalibrierung** (DC) vorgeschlagen. DC verwendet zufällige, im Domänenbereich vorkommende Wörter, die aus dem Aufgabenkorpus entnommen werden, um die Vorhersagen des Modells zu kalibrieren.

Die Experimente zeigen, dass DC die ICL-Leistung erheblich verbessert, insbesondere bei Aufgaben mit starkem Domänen-Label-Bias. Dies deutet darauf hin, dass die Wahl der inhaltsfreien Tokens für die Kalibrierung entscheidend ist und dass die Verwendung domänenspezifischer Tokens dabei hilft, diese Art von Bias zu adressieren und somit zu einer besseren und stabileren Leistung bei verschiedenen Klassifikationsaufgaben führt.</sample>
    <sample id="264">Dieses Dokument beschreibt eine neue Aufgabe, die als übertragbare audiovisuell Textgenerierung bezeichnet wird. Es wird ein Framework vorgestellt, das drei Komponenten umfasst: ein audiovisuelles Meta-Mapper-Netzwerk, einen audiovisuellen Encoder und Sprachmodell-Generator sowie ein kontrafaktisches Kontrastlernen. Das Meta-Mapper-Netzwerk zielt darauf ab, visuelle Konzepte domänenübergreifend in einen einheitlichen auditiven semantischen Raum abzubilden, indem visuelle Präfixe für Audiocluster generiert werden. Das Encoder-Generator-Modell verwendet ein transformatorbasiertes Design, das die Beiträge verschiedener Modalitäten berücksichtigt. Für die Optimierung des Modells werden zwei neue Verluste beim kontrafaktischen Kontrastlernen eingeführt, die auf der Verteilung und Abhängigkeit basieren, um die semantische und die visuell-textliche Ausrichtung zu verbessern. Das Framework wird anhand von zwei Benchmarks bewertet, darunter Kreuzdatensatz- und Kreuzdomänen-Einstellungen. Die experimentellen Ergebnisse zeigen, dass das vorgeschlagene Framework die vergleichbaren Modelle in Bezug auf die Übertragbarkeit in Bezug auf verschiedene Domänen und Datensätze deutlich übertrifft, was die Effizienz in Szenarien mit begrenzten gekennzeichneten Daten hervorhebt.</sample>
    <sample id="265">Der/die Referent*in heißt Vasudha.</sample>
    <sample id="266">Die Autoren sind Teil des **Institute of Computer Science, Polish Academy of Sciences** und der **University of Warsaw**.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Auslassungsfehler (omission errors).</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute erzählen wir Ihnen alles über ABC-Eval, einen neuen dimensionalen Ansatz zur Bewertung von Konversations-KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Nehmen wir an, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie gut es im Vergleich zum aktuellen Stand der Technik abschneidet. Die gängige Praxis ist die Verwendung von menschlichen Bewertungen, indem man menschliche Bewerter bittet, zu wählen, welches von zwei Gesprächen besser ist, oder Gespräche auf einer Likert-Skala zu bewerten. Diese Ansätze eignen sich gut für eine ganzheitliche Bewertung der allgemeinen Dialogqualität, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Ein Ansatz besteht darin, menschliche Bewerter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie z. B. die Relevanz der Bot-Antworten, unter Verwendung bestehender vergleichender oder Likert-Skala-Methoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem explizit annotiert wird, ob jede Modellantwort bestimmte Verhaltensweisen aufweist, wie z. B. das Antworten mit irrelevanten Informationen oder das Selbstwidersprechen. Wir nennen diesen Ansatz „Annotating Behaviors in Chat“ oder kurz ABC-Eval. Wir haben diese Methode entwickelt, um Chat-Modellverhaltensweisen umfassend abzudecken, die in der jüngsten Literatur als relevant für die Chat-Qualität vorgeschlagen wurden. ABC-Eval ist in der Lage, die Häufigkeit zu messen, mit der Chat-Modelle verschiedene thematische Fehler machen. Zum Beispiel misst ABC-Eval die Anzahl der Äußerungen, in denen ein Chat-Modell seinen Partner ignoriert oder etwas Irrelevantes sagt, sich selbst oder seinen Partner widerspricht, falsche Fakten halluziniert oder den gesunden Menschenverstand verletzt, und wann das Modell Empathie zeigt oder nicht zeigt. Um festzustellen, welche Art von Bewertung am effektivsten ist, haben wir vier hochmoderne Chat-Modelle ausgewählt und sie in 100 Mensch-Bot-Gesprächen pro Modell mit ABC-Eval bewertet. Zum Vergleich haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf Turn-Ebene, Likert-Bewertungen auf Dialog-Ebene und paarweisen Vergleichen auf Dialog-Ebene. Für jede der bestehenden Methoden haben wir Bewertungen für acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis zur Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist. Aus unseren Analysen dieser Bewertungsergebnisse haben wir festgestellt, dass ABC-Eval-Verhaltenslabels insgesamt zuverlässiger sind als Labels, die mit bestehenden Methoden gesammelt wurden, gemessen am Inter-Annotator-Agreement in 100 doppelt gelabelten Gesprächen. Darüber hinaus sind ABC-Eval-Labels besser prädiktiv für die Gesamtqualität der Konversation als Metriken, die mit bestehenden Methoden erzeugt wurden, wie diese einfache lineare Regressionsanalyse zeigt. Zum Beispiel können Sie sehen, wie die Messung des Anteils der Turns mit Selbst- und Partnerwidersprüchen 5 % bzw. 10 % der Konversationsqualität erklärt, während die durchschnittlichen Likert-Konsistenzwerte nur 4 % oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chat-Qualität erfasst, indem wir eine schrittweise lineare Regression verwendet haben. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 % der Konversationsqualität erklärt, und wenn Sie die Metriken nacheinander entfernen, führt dies größtenteils zu einem erheblichen Informationsverlust über die Qualität. Andererseits erklärt die Kombination aller Likert-Metriken auf Turn-Ebene weit weniger von der Qualität, und nur wenige dieser Metriken tragen einzigartige Informationen bei. Diese zuverlässigen, informativen und eindeutigen ABC-Eval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung zu bewerten, als dies mit früheren Methoden möglich war. Sie können in den Ergebnissen unseres Experiments sehen, dass noch mehrere Herausforderungen bestehen und präzise quantifiziert wurden. Zum Beispiel weisen die von uns getesteten Bots in etwa 20 % ihrer Antworten Verstöße gegen den gesunden Menschenverstand auf. Sie produzieren irrelevante Informationen in etwa 15 % der Antworten, und sie widersprechen sich selbst oder ihrem Partner etwa 10 % der Zeit. Angesichts des rasanten Fortschritts in diesem Bereich könnten viele dieser Fehlerraten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, sinken. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmetriken für den Vergleich von Modellen zu verfolgen. Wir hoffen, dass ABC-Eval von anderen in diesem Bereich als bedeutsamer Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf zu sehen, wie sich konversationelle KI in den kommenden Monaten und Jahren entwickeln wird. Vielen Dank fürs Zuschauen!</sample>
    <sample id="270">Die Autoren sind an der Emory University.</sample>
    <sample id="271">CFT steht für "Continuous Fine-Tuning".</sample>
    <sample id="272">An der Arbeit sind sechs Autoren beteiligt.</sample>
    <sample id="273">00:00
Hallo, mein Name ist Kayo Yin, und ich werde unsere Arbeit mit dem Titel "Wann erfordert Übersetzung Kontext? Eine datengesteuerte, mehrsprachige Untersuchung" vorstellen. Diese Arbeit entstand in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig.
00:16
Viele Übersetzungen hängen vom Kontext ab. Wie würden wir zum Beispiel "Maulwurf" in diesem Satz übersetzen? Nun, wenn der vorherige Satz "Es könnte gefährlich werden, wenn die Minister es herausfinden" lautete, dann bezieht sich "Maulwurf" auf einen Spion. Aber wenn der vorherige Satz "Könnte es etwas Ernstes sein, Doktor?" lautete, dann bezieht sich "Maulwurf" auf ein Muttermal. Je nach Kontext ändert sich also die Bedeutung des Wortes und damit auch seine Übersetzung.
00:42
Allerdings ist es ziemlich schwierig, zu bewerten, wie gut Modelle solche Fälle übersetzen können. Erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, was korpusbasierte Metriken wie BLEU nicht in der Lage sind, diese Übersetzungen zu erfassen. Und einige Leute haben gezielte Auswertungen von kontextabhängigen Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachgruppen, da sie sich in der Regel auf Fachwissen und menschliche Erstellung verlassen.
01:13
In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut gehen Modelle mit kontextabhängigen Übersetzungen um? Um die erste Frage zu beantworten, haben wir zunächst gemessen, wie stark ein Wort beim Übersetzen vom Kontext abhängt. In der vorherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung von Maschinenübersetzungsmodellen eingeführt. Dies geschieht, indem gemessen wird, wie viel Information der Kontext C über das Ziel Y liefert, gegeben die Quelle X. Man kann sich CXMI als den Informationsgewinn vorstellen, der durch die Bereitstellung von Kontext für das Modell entsteht. In dieser Arbeit erweitern wir CXMI zu Pointwise CXMI, das die Kontextnutzung auf Satzebene oder auf Wortebene messen kann. Wörter mit hohem P-CXMI können wir als Wörter betrachten, die Kontext für die Übersetzung benötigen.
02:04
Nun analysieren wir Wörter mit hohem P-CXMI, um Muster zwischen diesen Wörtern zu finden. Und wir führen unsere Analyse an Transkripten von TED-Vorträgen durch, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zuerst betrachten wir Wortarten, die hohe P-CXMI-Mittelwerte aufweisen. Und das ermöglicht es uns, zum Beispiel Dualpronomen im Arabischen zu finden, die relativ hohe P-CXMI-Werte haben. Und das lässt sich erklären, weil Englisch keine Dualpronomen hat. Man braucht also Kontext, um zu bestimmen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Und ähnlich finden wir, dass bestimmte Sprachen auch Kontext benötigen, wenn man die passende Verbform wählen möchte. Dann betrachten wir Vokabular-Elemente, die hohe P-CXMI-Mittelwerte über alle ihre verschiedenen Vorkommen hinweg aufweisen. Und das hilft uns, Fälle wie den hier zu identifizieren, wo man im Chinesischen Kontext benötigt, um Eigennamen korrekt zu übersetzen, um sicherzustellen, dass man dieselbe Übersetzung innerhalb des Dokuments verwendet. Und ähnlich finden wir, dass Kontext unterstützt wird, um in der richtigen Formalität zu übersetzen. Und schließlich betrachten wir einzelne Token, die hohe P-CXMI-Werte haben. Und das ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich vom Wort selbst erfasst werden können, sondern eher in der Satzstruktur ausgedrückt werden, wie zum Beispiel die Ellipsenauflösung.
03:29
Nun verwenden wir unsere Erkenntnisse aus unserer Analyse, um einen Benchmark für die Übersetzung auf Dokumentebene zu entwerfen. Für jedes der fünf von uns identifizierten Diskursphänomene erstellen wir Tagger, um Wörter, die zu dem Phänomen gehören, automatisch zu identifizieren. Und wir nennen unseren Tagger den mehrsprachigen Diskurs-Aware- oder MuDA-Tagger. Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene aufweisen. Wir verwenden dann den MuDA-Tagger, indem wir den Tagger auf einen parallelen Korpus anwenden, den wir zur Bewertung verwenden möchten. Und wir wenden unsere bevorzugten Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die der MuDA-Tagger identifiziert hat. Und schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle für die maschinelle Übersetzung auf Dokumentebene zu bewerten.
04:22
Zunächst einmal, wenn wir Korpus-Level-Metriken verwenden, finden wir für BLEU, dass kontextagnostische Modelle die beste Leistung erbringen. Wenn wir jedoch COMET verwenden, erbringen kontextbewusste Modelle die beste Leistung. Und wenn wir die Wort-F-Metrik verwenden, haben Modelle mit und ohne Kontext eine vergleichbare Leistung. Dies zeigt erneut, dass es schwierig ist, das beste Übersetzungsystem auf Dokumentebene zu bestimmen, wenn wir nur Korpus-Level-Metriken verwenden.
04:52
Nun verwenden wir den MuDA-Benchmark, um Modelle zu bewerten, und wir finden, dass kontextbewusste Modelle bei einigen Phänomenen, wie Formalität und lexikalischer Kohäsion, deutlich besser abschneiden. Aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext verwenden, bei anderen Phänomenen wie Ellipsen, Pronomen und Verbformen. Dies deutet also darauf hin, wo wir bei der Übersetzung auf Dokumentebene mehr Fortschritte sehen müssten. Wir haben auch verschiedene kommerzielle Systeme verglichen, und unser Benchmark zeigt, dass DeepL für die Übersetzung auf Dokumentebene in den meisten Phänomenen und Sprachpaaren genauer ist als Google Translate.
05:29
Zusammenfassend haben wir eine datengesteuerte Analyse über 14 Sprachpaare durchgeführt, um zu identifizieren, wann Übersetzungen Kontext erfordern. Und dann verwenden wir unsere Erkenntnisse, um einen Benchmark für die maschinelle Übersetzung auf Dokumentebene zu erstellen, der uns helfen kann, zu identifizieren, welche Diskursphänomene Modelle gut oder schlecht handhaben können, und welche Übersetzungssysteme gut für die Übersetzung auf Dokumentebene sind. Vielen Dank für Ihre Aufmerksamkeit, wir sehen uns in Toronto!</sample>
    <sample id="274">Der/die Referent*in heißt Yusen Zhang.</sample>
    <sample id="276">Dieses Video stellt "IndicMT Eval" vor, ein Datensatz, der zur Meta-Evaluierung von Machine-Translation-Metriken für indische Sprachen entwickelt wurde. Es werden fünf indische Sprachen aus zwei Sprachfamilien berücksichtigt: Tamil und Malayalam (dravidisch) sowie Hindi, Marathi und Gujarati (indoarisch). Um Daten zu sammeln, wurden 200 zufällige Sätze aus dem Flores-Datensatz ausgewählt. Diese Sätze wurden dann über sieben verschiedene Übersetzungssysteme oder APIs, darunter mBART, Google API, Bing API, CVIT-IIITH, IndicTrans, mT5 und NLLB, übersetzt, was insgesamt 7000 Proben für jede Sprache ergab.

Die Annotationen für diese Proben wurden von zweisprachigen, erfahrenen Annotatoren mithilfe des MQM-Frameworks gesammelt. Dabei wurden Fehler in Bezug auf Genauigkeit (Hinzufügen, Auslassen, Fehlübersetzung, nicht übersetzter Text) und Flüssigkeit (Rechtschreibung, Grammatik, Register, Zeichenkodierung) sowie weitere Fehler hervorgehoben und eine Gesamtbewertung abgegeben.

Die Fehlerstatistiken zeigen, dass neuere MT-Modelle wie NLLB und IndicTrans weniger Fehler aufweisen als ältere Modelle. Die Rangfolge der Systeme basiert auf menschlichen Bewertungen, wobei IndicTrans, NLLB, Google API, Bing API, mT5, CVIT und mBART die besten sind.

Die Korrelationen zwischen den Metriken und den menschlichen Bewertungen wurden analysiert, wobei chrF++ bei überlappungsbasierten Metriken am höchsten korrelierte und COMET-Metrikvarianten die höchsten Gesamtkorrelationen zeigten. Viele Metriken zeigten eine eingeschränkte Streuung der Bewertungen, was die Interpretation erschwerte.

Die Fine-Tuning-Ergebnisse von IndicCOMET-MQM zeigen, dass es in den meisten Sprachen eine höhere Korrelation als COMET-MQM erreicht. Zero-Shot-Tests bestätigen, dass IndicCOMET-MQM auf ungesehenen Sprachen robuster ist als seine COMET-Pendants.

Der Datensatz und der Code sind öffentlich verfügbar.</sample>
    <sample id="277">Die neue Methode hat keinen Namen.</sample>
    <sample id="278">Die Autoren definieren „markierte Wörter“ als eine Methode, um Wörter zu identifizieren, die Personen von „markierten“ Gruppen von „unmarkierten“ Gruppen unterscheiden. Bei der Implementierung werden zuerst die „markierten“ und „unmarkierten“ Gruppen definiert und dann gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden.</sample>
    <sample id="279">Die Autoren gehören der University of Washington und der Carnegie Mellon University an.</sample>
    <sample id="280">Die Präsentation stellt MultiEMO vor, ein Aufmerksamkeits-basiertes Korrelations-bewusstes multimodales Fusions-Framework für die Emotionserkennung in Gesprächen (ERC). Es behandelt Herausforderungen bestehender ERC-Methoden, wie die mangelnde Nutzung der Komplementarität multimodaler Informationen, unzureichende Leistung bei Minderheiten-Emotionsklassen und die Schwierigkeit, semantisch ähnliche Emotionen zu unterscheiden.

MultiEMO hat vier Kernkomponenten: Unimodale Feature-Extraktion, Kontextmodellierung, multimodale Fusion und Emotionsklassifikation. Ein neuer visueller Feature-Extraktor namens VisExtNet wird vorgeschlagen, der visuelle Hinweise von Interlokutoren erfasst, ohne redundante Szeneninformationen zu modellieren. Ein multimodales Fusionsmodell, MultiAttn, basiert auf bidirektionalen Multi-Head-Cross-Attention-Layern, um komplizierte Korrelationen zwischen Text-, Audio- und visuellen Modalitäten zu modellieren. Der Sample-Weighted Focal Contrastive (SWFC) Verlust wird eingeführt, um die Klassifizierung von Minderheiten- und semantisch ähnlichen Emotionsklassen zu verbessern.

Experimentelle Ergebnisse auf den Datensätzen MELD und IEMOCAP zeigen, dass MultiEMO herausragende Leistungen erbringt, insbesondere bei Minderheiten- und semantisch ähnlichen Emotionen. Es kann die Asynchronität emotionaler Tendenzen aus verschiedenen Modalitäten bewältigen, obwohl es Einschränkungen gibt, wie die Unfähigkeit, zwischen Sprechern und irrelevanten Personen zu unterscheiden, hohe Batch-Größenanforderungen für den SWFC-Verlust und die immer noch bessere Leistung bei Mehrheitsklassen.</sample>
    <sample id="281">Die Vortragende, Kayo Yin, stellt ihre Arbeit "When Does Translation Require Context? A Data-driven, Multilingual Exploration" vor. Sie betont, dass Übersetzungen oft kontextabhängig sind, was die Bewertung von Modellen erschwert. Da nur ein kleiner Teil der Wörter kontextabhängig ist, sind Korpus-Metriken wie BLEU unzureichend. Vorhandene Methoden unterstützen nur eine begrenzte Anzahl von Phänomenen und Sprachen, da sie auf Expertenwissen und menschlicher Kuration beruhen.

Um zu analysieren, wann Kontext erforderlich ist, stellen sie P-CXMI vor, eine erweiterte Version von CXMI zur Messung der Kontextnutzung auf Satz- und Wortebene. Hohe P-CXMI-Werte deuten auf eine hohe Kontextabhängigkeit hin. Sie führen eine thematische Analyse von Wörtern mit hohem P-CXMI in 14 Sprachpaaren durch, um Muster in Bezug auf Wortarten, Vokabular und individuelle Token zu identifizieren.

Basierend auf diesen Erkenntnissen entwickeln sie den Multilingual Discourse-Aware (MuDA) Tagger und einen neuen Benchmark für die Dokumentenübersetzung. Dieser Benchmark verwendet Tags, um kontextabhängige Beispiele zu identifizieren, und wendet dann verschiedene Metriken an. Ihre Ergebnisse zeigen, dass kontextbewusste Modelle bei einigen Phänomenen wie Formalität und lexikalischer Kohäsion deutlich besser abschneiden. DeepL übertrifft Google Translate in den meisten Phänomenen und Sprachpaaren.</sample>
    <sample id="282">StoryTrans, ein neues Modell für den nicht-parallelen Stiltransfer von Geschichten auf Diskursebene, wurde vorgestellt. Aktuelle Textstiltransferstudien konzentrieren sich auf Token- oder Satzebene, was bei langen Texten und spezifischen Schreibthemen, die stark mit dem Stil verbunden sind, zu Schwierigkeiten führen kann. StoryTrans begegnet diesen Herausforderungen, indem es eine zweistufige Generierung verwendet. Zuerst wird der Diskursteil der Geschichte in ein stilneutrales Format umgewandelt. Anschließend wird der maskierte Quelltext zusammen mit Stil-Embedding, das aus dem Zielstil gelernt wurde, in das Modell eingespeist, um den neuen Text zu generieren. Durch die Trennung von Stil und Inhalt können stilbezogene Inhalte entfernt und inhaltsbezogene Inhalte durch ein Füllmodell ergänzt werden.

Die Experimente zeigen, dass StoryTrans andere Basislinien in Bezug auf die Stilkontrolle und den Inhaltserhalt deutlich übertrifft. Die Visualisierung der Stilmerkmale unterstreicht außerdem, dass die durch StoryTrans übertragenen Texte gut mit dem Zielstil übereinstimmen. Das Modell wurde mit einem neuen Datensatz trainiert, der aus chinesischen und englischen Romanen besteht.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">Der Vortrag stellt FSUIS vor, ein neues Verfahren zur universellen Informationsgewinnung. Das Problem der Mehrdeutigkeit bei der Annotation von Spanneengrenzen, bei dem verschiedene Annotationen als angemessen angesehen werden können, wird hervorgehoben. Darüber hinaus gibt es eine Diskrepanz zwischen der Merkmalsextraktion von Transformatoren, die sich auf globale Merkmale konzentriert, und der Informationsgewinnung, die sich auf lokale Merkmale konzentriert.

Um diese Herausforderungen zu lösen, schlägt FSUIS einen Fuzzy-Spannungs-Verlust (FSL) vor, der die Modellierung von Grenzen als kontinuierliche Verteilung ermöglicht, was die Abhängigkeit des Modells von präzisen Spannengrenzen verringert. Außerdem wird ein Fuzzy-Spannungs-Aufmerksamkeitsmechanismus (FSA) eingeführt, um eine adaptive Anpassung des Aufmerksamkeitsbereichs zu ermöglichen und so eine angemessenere Aufmerksamkeitsverteilung für die Spannenextraktion zu gewährleisten.

Experimente an drei Informationsgewinnungsaufgaben - Named Entity Recognition (NER), Relation Extraction (RE) und Aspect Sentiment Triplet Extraction (ASTE) - zeigen, dass FSUIS im Vergleich zu UIE-base eine signifikante Leistungsverbesserung erzielt. Das Modell zeigt auch eine bessere Generalisierungsfähigkeit für domänenspezifische Informationen und eine bessere Leistung mit einer vereinheitlichten Extraktionsstruktur. Eine Ablationsstudie zeigt, dass sowohl FSL als auch FSA zu diesen Verbesserungen beitragen, wobei FSA eine schnellere Konvergenz und FSL eine größere Informationsgewinnungsfähigkeit bewirkt. Die Visualisierung von FSA zeigt, dass sich das Modell auf semantische Informationen innerhalb eines begrenzten Bereichs vorangehender Token konzentriert und nicht auf eine globale Darstellung.</sample>
    <sample id="285">Die Zusammenfassungen von Dialogen, die mit aktuellen Modellen erstellt werden, enthalten trotz aller Bemühungen weiterhin Sachfehler. Die **Referenzkorrekturen**, d.h. manuell kommentierte Korrekturen, die speziell für modellgenerierte Zusammenfassungen mit Sachfehlern vorgenommen werden, dienen als genauere Referenzen zur Bewertung der Korrekturleistung. In dieser Arbeit wird ein neues, **feingranulares Bewertungsframework** eingeführt, das die Leistung von FEC-Modellen (Factual Error Correction) bewertet, indem es die Form und den Inhalt der Korrekturen untersucht. Das Framework ermöglicht eine detailliertere Fehlerklassifizierung, eine genauere Metrikberechnung und eine bessere Anpassung an die Trainingsmethoden der FEC-Modelle. Experimente zeigen, dass die Ausbildung von FEC-Modellen mit Referenzzusammenfassungen die besten Ergebnisse liefert, und dass die Kombination von manuell kommentierten und synthetischen Daten eine vielversprechende Richtung ist. Aktuelle FEC-Modelle haben Schwierigkeiten bei der Korrektur von **Faktenfehlern durch Ergänzung** und können keine Fehler bei Attributen, Modalitäten oder Verknüpfungen beheben.</sample>
    <sample id="286">Der/die Referent*in heißt James Finch und Sarah Finch.</sample>
    <sample id="287">Es gibt 4 Autoren, die an der Arbeit beteiligt sind.</sample>
    <sample id="288">Die im Video genannten Datensätze, die zum Testen syntaktischer Phänomene verwendet werden können, sind BliMP und SyntaxGym.</sample>
    <sample id="290">FTw, BOND, COSINE, MLC und L2R.</sample>
    <sample id="291">Das Modell wird anhand von benannten Entitäten, Klassifizierung, Teilwortmarkierung und Frage-Antwort-Aufgaben evaluiert.</sample>
    <sample id="294">CamemBERT wurde ursprünglich auf generischen Daten trainiert.</sample>
    <sample id="295">Der/Die Referent*in heißt Adam Przepiórkowski.</sample>
    <sample id="296">This video describes EPIC, a multi-perspective English irony corpus developed in collaboration with the University of Turin and Amazon Alexa. The goal is to move beyond binary irony detection by investigating different annotator perspectives. Data was collected from Reddit and Twitter between January 2020 and June 2021, resulting in 3,000 text/reply pairs across five English varieties (UK, US, Ireland, Australia, India). Approximately 15 annotators per variety (74 total) were recruited via Prolific, annotating 200 texts each with attention-check questions, yielding an average of five annotations per text. Annotators were balanced by gender, country of residence, and were required to annotate texts from all varieties. The annotation task presented message-reply pairs, asking for a binary "ironic" or "not ironic" judgment. Results showed that different demographic groups exhibited varying inter-annotator agreement. Perspective-aware models trained on these splits showed less uncertainty and higher confidence when tested on data representative of their specific perspective. Interestingly, the highest variation in irony perception was observed between contiguous generations and between annotators from the United Kingdom and Ireland. The project aims to understand the importance of a perspectivist approach for irony detection.</sample>
    <sample id="297">Dieses Projekt untersucht "Dogwhistles", eine Art kodierter Rhetorik, die von Politikern verwendet wird, um getarnte Botschaften an bestimmte Zielgruppen zu senden, während sie für die breite Öffentlichkeit harmlos erscheinen. Die Forscher haben eine umfangreiche Typologie und ein Glossar von über 340 Dogwhistle-Begriffen und -Symbolen entwickelt, die häufig in rassistischen, transphoben und antisemitischen Kontexten verwendet werden.

Eine Fallstudie historischer US-Politsprache ergab, dass Dogwhistles seit der Bürgerrechtsbewegung zugenommen haben und zunehmend von konservativen Rednern verwendet werden.

Das Projekt untersuchte auch die Fähigkeit von Sprachmodellen, Dogwhistles zu erkennen. GPT-3 konnte 45% der Begriffe im Glossar korrekt identifizieren, wobei formelle Dogwhistles häufiger erkannt wurden als informelle Online-Begriffe. Die Leistung variierte je nach Kategorie des Dogwhistles, wobei transphobe Begriffe am schwierigsten zu erkennen waren. Die Zugabe einer expliziten Definition oder eines "geheimen Hinweises" verbesserte die Fähigkeit von GPT-3, die verborgenen Bedeutungen von Dogwhistles zu erkennen.

Schließlich wurde gezeigt, dass Dogwhistles zur Umgehung der Online-Inhaltsmoderation eingesetzt werden können. Sätze, die standardmäßige beleidigende Begriffe enthielten, wurden von Toxizitäts-Erkennungsmodellen als weniger toxisch eingestuft, wenn diese Begriffe durch Dogwhistles ersetzt wurden. Dies deutet darauf hin, dass die subtile Natur von Dogwhistles es ihnen ermöglicht, unter dem Radar der automatischen Inhaltsmoderationssysteme zu fliegen, was sie zu einem mächtigen Werkzeug für hasserfüllte und missbräuchliche Rhetorik macht.</sample>
    <sample id="298">Experimente zeigten, dass die Leistung mit einer größeren zeitlichen Lücke abnimmt, was die Hypothese bestätigt, dass die zeitliche Verzögerung die Hauptursache für den Leistungsabfall ist.</sample>
    <sample id="299">Die jüngsten Fortschritte bei NLI-Modellen wurden teilweise durch das Lernen von Verknüpfungen erreicht, die in den Trainingsdaten enthalten sind. Diese Verknüpfungen verallgemeinern sich jedoch nicht gut auf Datensätze, bei denen diese Verknüpfungen nicht zutreffen, was die Robustheit des Modells beeinträchtigt. Bestehende Abhilfemaßnahmen umfassen in der Regel Hilfsmodelle, die eine vorausschauende Kenntnis der Verknüpfungen oder zusätzliche Rechenressourcen erfordern, was die Anwendbarkeit einschränkt. Hier schlagen wir einen Minimax-Trainingsansatz vor, der diese Einschränkungen überwindet.

Unser Ansatz besteht darin, ein Beispielgewicht zu lernen, das schwer zu klassifizierende Trainingsbeispiele betont. Das Training wird als ein Minimax-Spiel formuliert, bei dem ein Lernender versucht, seine Aufgabe zu erfüllen (minimale Verluste), während ein Hilfsmodell versucht, die Verluste des Lernenden zu maximieren, indem es Beispiele höher gewichtet, bei denen der Lernende falsch ist. Dieser Ansatz erfordert keine Vorkenntnisse über Verknüpfungen und kann mit jedem Standard-Optimierer verwendet werden. Wir zeigen, dass unser Ansatz die Robustheit von NLI-Modellen auf OOD-Datensätzen konsistent verbessert, während die Genauigkeit der In-Distribution erhalten bleibt.</sample>
    <sample id="300">Belinda Z. Li stellt die Arbeit "Toward Interactive Dictation" vor, die auf der ACL 2023 präsentiert wurde. Ziel ist es, eine neue Art der Diktierfunktion zu entwickeln, die eine natürliche und intuitive Sprachsteuerung beim Diktieren und Bearbeiten von Dokumenten ermöglicht. Die meisten aktuellen Systeme unterstützen nur das Diktieren, und die wenigen, die Sprachbefehle zur Bearbeitung erkennen, sind umständlich zu bedienen, da Benutzer eine lange Liste von Befehlen auswendig lernen müssen.

Die neue "Interactive Dictation"-Task zeichnet sich durch zwei Hauptmerkmale aus:
1. Flexible Verzahnung von Diktat und Bearbeitung ohne reservierte Triggerwörter.
2. Intuitive und offene natürliche Sprachbefehle für die Bearbeitung, ohne feste Vorlagen für verschiedene Befehlstypen.

Die vorgestellte Arbeit umfasst die Einführung und Formalisierung der neuen Task, die Entwicklung einer Datensammlungsschnittstelle und eines Datensatzes (TERTIUS) sowie die Erstellung eines Basissystems. Das Basissystem besteht aus vier Schritten: ASR (Automatic Speech Recognition), Segmentierung, Normalisierung und Interpretation.

Die Ergebnisse zeigen, dass das Segmentierungsmodell mit 85,3% Genauigkeit sowohl genau als auch effizient ist. Bei der Bewertung der ASR-Reparatur- und Interpretationsmodelle zeigte sich, dass GPT3-Modelle zwar genauer, aber auch langsamer sind, und die direkte Vorhersage des Endzustands war bei GPT3 genauer als die Vorhersage von Zwischenprogrammen.</sample>
    <sample id="302">Die Permutation der Token für die Ausgabesequenz ist notwendig, da die in der ersten Stufe erhaltenen Token ungeordnet sind.</sample>
    <sample id="303">Die Autoren argumentieren, dass es ohne mehr Transparenz nicht möglich ist, Annahmen zu treffen oder Muster (insbesondere positive Stereotypen) weiter zu untersuchen. Sie können nicht mit Sicherheit sagen, ob die positiven Muster auf Value Alignment oder andere Anti-Stereotyp-Methoden zurückzuführen sind.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind Sätze, die grammatisch falsch, stereotyp oder anderweitig ungeeignet sind.</sample>
    <sample id="305">In diesem Video werden die Erkenntnisse der Forschung zur schwach überwachten Lernfähigkeit (WSL) und zum Nutzen von sauberen Validierungsdaten vorgestellt. Zunächst wird erklärt, dass schwache Überwachung zwar den Engpass bei der Datenannotation entschärft, die daraus resultierenden Labels aber verrauscht sind und die Generalisierung beeinträchtigen können. WSL-Algorithmen sollen robuste neuronale Netze trotz verrauschter Daten trainieren.

Die Forschenden stellen drei Kernfragen: ob saubere Validierungsdaten notwendig sind, wie viele saubere Samples benötigt werden und wie die vorhandenen sauberen Samples effizienter genutzt werden können. Die Ergebnisse zeigen, dass saubere Validierungsdaten für die meisten WSL-Methoden unerlässlich sind, um eine sinnvolle Generalisierung zu erreichen. Ohne diese Daten ist das Training ineffektiv. Weiterhin wurde festgestellt, dass bereits 20 saubere Samples pro Klasse ausreichen, um eine hohe Leistung zu erzielen. Allerdings kann ein direktes Fine-Tuning auf den sauberen Samples (z.B. mit LoRA) bereits ab 10 Samples pro Klasse eine bessere Performance liefern als kompliziertere WSL-Ansätze. Schließlich zeigt sich, dass ein kontinuierliches Fine-Tuning (CFT) auf sauberen Validierungsdaten die Leistungsunterschiede zwischen verschiedenen WSL-Ansätzen eliminiert.

Die Schlussfolgerung ist, dass aktuelle WSL-Ansätze saubere Samples benötigen und ihre Praktikabilität überschätzt wird. Die Forschenden empfehlen, die Kriterien für die Modellauswahl transparent zu machen, Few-shot-Learning-Ansätze als Baselines zu verwenden und immer kontinuierliches Fine-Tuning (CFT) anzuwenden.</sample>
    <sample id="306">Sprachmodelle müssen Entitäten verfolgen und Zustandsänderungen verstehen, um Diskurse zu verstehen. Zum Beispiel in einem Rezept, in dem das Hinzufügen von Eiern, Zucker und Mehl zu einer Schüssel gefolgt vom Mischen eine neue Entität "Teig" erzeugt. Bisher gab es keine systematische Untersuchung, ob vortrainierte Sprachmodelle solche Aufgaben ausführen können. Es gibt mehrere Herausforderungen bei der Entwicklung eines solchen Bewertungs-Tasks: häufige Entitätszustände in den Vortrainingsdaten und die Vorhersage von Entitätszuständen aus einzelnen Wörtern oder Phrasen ohne Berücksichtigung des Diskurses. Unser Experiment verwendet eine Aufgabe, bei der ein Sprachmodell die Inhalte von Boxen, die Objekte enthalten, vorhersagen muss. Operationen wie das Verschieben von Objekten oder das Hinzufügen von Objekten zu einer Box ändern den Inhalt der Boxen. Unsere Experimente zeigen, dass die meisten Modelle einfach den Anfangszustand wiederholen. Nur GPT-3.5 text-davinci-003 zeigt ein nicht-triviales Entitäts-Tracking. Wir fanden heraus, dass alle GPT-3.5-Modelle, die auf erhebliche Mengen Code trainiert wurden, ein nicht-triviales Entitäts-Tracking-Verhalten zeigen, während Modelle ohne Code in ihren Vortrainingsdaten dies nicht tun. Dies deutet darauf hin, dass das Vortrainieren auf Text und Code die Entitäts-Tracking-Fähigkeiten in vortrainierten Sprachmodellen freisetzt.</sample>
    <sample id="307">Die Autoren bewerteten ihre Modelle mittels F1-Score, Hamming und EMR.</sample>
    <sample id="308">In dieser Präsentation wird NLPpositionality vorgestellt, ein Framework zur Charakterisierung von Design-Biases in NLP-Datensätzen und -Modellen. Die Referentin, Jenny Liang von der Carnegie Mellon University, betont, dass NLP-Entwickler unbeabsichtigt ihre persönlichen Perspektiven in die von ihnen erstellten Technologien einfließen lassen. Dadurch kann es zu systematischen Leistungsunterschieden bei der Technologie zwischen verschiedenen Bevölkerungsgruppen kommen, was als Design-Bias bezeichnet wird. Das Framework vergleicht Annotationen von unterschiedlichen Benutzern mit bestehenden Datensätzen und Modellen unter Verwendung von Pearsons R-Werten. Die Daten für diese Analyse werden über LabintheWild gesammelt, eine Online-Crowdsourcing-Plattform, die es ermöglicht, eine vielfältige Gruppe von Teilnehmern für die Re-Annotation von Datensätzen zu rekrutieren. In zwei Aufgaben – soziale Akzeptanz und Toxizität – werden Antworten von über 16.000 Annotatoren aus 87 Ländern mit Social Chemistry, Dynahate, Delphi, Perspective API, Rewire API, Hate RoBERTa und GPT-4 verglichen.

Die Ergebnisse zeigen, dass NLP-Datensätze und -Modelle tendenziell am stärksten auf englischsprachige Länder und auf Personen mit Hochschulbildung ausgerichtet sind. Es wurde auch festgestellt, dass sie weniger auf nicht-binäre Personen ausgerichtet sind. Um die Positionality in NLP anzugehen, schlägt die Studie drei Empfehlungen vor: 1) Alle relevanten Designentscheidungen während der Erstellung von Datensätzen oder Modellen dokumentieren, 2) NLP-Forschung durch die Linse des Perspektivismus betreiben, indem disaggregierte Datensatz-Labels geteilt und Modellierungstechniken verwendet werden, die Annotatoren-Uneinigkeit berücksichtigen, und 3) spezialisierte Datensätze und Modelle mit und für spezifische Gemeinschaften aufzubauen.</sample>
    <sample id="309">Krippendorffs Alpha.</sample>
    <sample id="310">Die Domain, die gewählt wurde, um völlig unzusammenhängende Sätze zu den inakzeptablen und akzeptablen Suchanfragen hinzuzufügen, war Wikipedia.</sample>
    <sample id="311">Die Autoren gehören der Heinrich-Heine-Universität Düsseldorf in Deutschland an.</sample>
    <sample id="312">MultiInstruct ist der erste multimodale Instruction-Tuning-Benchmark-Datensatz.</sample>
    <sample id="313">Drei.</sample>
    <sample id="314">Die Definition der binären Koordination lautet: Eine binäre Koordination ist eine Koordinationsstruktur, in der der erste Konjunkt der Kopf der gesamten Koordinationsstruktur ist.</sample>
    <sample id="315">Diese Information ist in diesem Inhalt nicht verfügbar.</sample>
    <sample id="316">Kleinere Modelle, die mit Coscript feingetuned wurden, können Skripte von höherer Qualität erzeugen als die meisten großen Sprachmodelle.</sample>
    <sample id="317">In der Informationsextraktion, einem Aufgabenbereich der natürlichen Sprachverarbeitung, werden strukturierte Informationen aus unstrukturiertem Text gewonnen. Herkömmliche Methoden verwenden Sprachmodelle wie T5 und GPT-3 in einem Text-zu-Text-Format für das Vorabtraining, was jedoch zu einer Diskrepanz zwischen dem Eingabeformat beim Inferenzieren und dem Ausgabeformat von strukturierten Daten führt. Um dieses Problem zu lösen, wird CodeIE vorgeschlagen, das die Informationsextraktion in eine Code-Generierungsaufgabe umwandelt. CodeIE verwendet Code-Sprachmodelle wie Codex, um strukturierte Informationen zu generieren, die mit dem Eingabeformat übereinstimmen.

Die Ergebnisse der Experimente zeigen, dass CodeIE, insbesondere bei Verwendung von 1-5 Beispielen, die traditionellen Basislinienmodelle, wie UIE, und große natürliche Sprachmodelle, wie GPT-3, übertrifft. Eine detaillierte Analyse legt nahe, dass Code-Sprachmodelle aufgrund ihrer besseren Ausrichtung auf die Informationsgewinnungsaufgabe selbst und der Behebung struktureller Fehler eine höhere Leistung erbringen. Das Papier und der Code sind öffentlich zugänglich.</sample>
    <sample id="318">00:00
Hallo, ich bin Yanis Labrak und werde Ihnen unsere Arbeiten zu DrBERT vorstellen, einem robusten vortrainierten Modell in französischer Sprache für biomedizinische und klinische Bereiche.
00:09
In dieser Präsentation werden wir zunächst über Sprachmodellierung im Gesundheitswesen sprechen. Anschließend werden wir den Hauptbeitrag unseres Artikels vorstellen. Wir stellen das erste biomedizinische Modell in französischer Sprache namens DrBERT vor, das auf Roberta basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer Daten, die aus dem Internet abgerufen wurden. Wir stellen außerdem einen Vergleich von Modellen mit mehreren Vortrainings-Einstellungen und Datenquellen vor. Anschließend präsentieren wir unsere Ergebnisse zu 11 biomedizinischen und klinischen Downstream-Aufgaben in französischer Sprache. Abschließend fassen wir die Experimente zusammen und geben Ihnen weitere Details zum Zugriff auf die Modelle.
00:48
Seit seiner Veröffentlichung im Jahr 2018 hat sich BERT zu einem der effektivsten Ansätze zur Lösung von Aufgaben der natürlichen Sprachverarbeitung entwickelt und bietet einen enormen Leistungszuwachs im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2vec, fastText oder ELMo. Seitdem wurde dieses Modell an viele andere Sprachen wie Französisch mit CamemBERT und an andere Bereiche wie Biomedizin mit PubMedBERT und BioBERT und an klinische mit ClinicalBERT angepasst, aber hauptsächlich in Englisch. Spezialisierte Modelle für andere Sprachen sind seltener und basieren oft auf fortlaufendem Vortraining, da es an In-Domain-Daten mangelt. Französisch hatte jedoch noch kein Open-Source-Modell für den biomedizinischen Bereich.
01:35
Wir haben uns daher gefragt, welches die am besten geeigneten Datenquellen für eine breite Palette von Anwendungen sind und ob diese Daten ein guter Ersatz für klinische Daten sind. Um diese Fragen zu beantworten, trainieren und vergleichen wir zunächst vier Modelle, die von Grund auf neu erstellt wurden. Eine erste Version von DrBERT mit 7 GB NACHOS, eine zweite Version von 4 GB NACHOS, eine erste Version von ChuBERT, das ist ein klinisches Modell, mit 4 GB Sätzen aus klinischen Notizen und eine letzte Version von ChuBERT mit einer Mischung aus 4 GB NACHOS und 4 GB klinischen Notizen. Zusätzlich zu diesem Vergleich stellen wir drei Modelle vor, die auf kontinuierlichem Vortraining basieren, um den Einfluss der Vortrainingsstrategie zu analysieren. Eines basiert auf den Gewichten von CamemBERT und wurde auf 4 GB NACHOS trainiert, ein weiteres ebenfalls auf CamemBERT, wurde aber diesmal auf 4 GB klinische Notizen trainiert, und schließlich ein Modell, das auf dem englischen biomedizinischen Modell PubMedBERT basiert und auf 4 GB NACHOS trainiert wurde. Insgesamt haben wir sieben Modelle.
03:02
Um unsere sieben Modelle zu evaluieren, haben wir sowohl öffentliche als auch private Downstream-Aufgaben gesammelt, wie Named Entity Recognition, Klassifikation, Part-of-Speech Tagging und Question Answering. Diese Modelle werden mit sechs Baseline-Modellen verglichen, nämlich CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCINET 4 GB, PubMedBERT, BioBERT und ClinicalBERT. Die Evaluation zeigt, dass Modelle auf Aufgaben mit Daten derselben Art, auf denen das Modell trainiert wurde, am besten abschneiden. Wir können jedoch beobachten, dass Daten aus heterogenen Quellen vielseitiger zu sein scheinen. Wir beobachten auch, dass mehr Daten zu einer besseren Leistung führen.
03:51
Insgesamt scheint das Vortraining von Grund auf in den meisten Aufgaben eine höhere Leistung zu erzielen. Unsere Experimente zum kontinuierlichen Vortraining, bei denen die Gewichte und Tokenizer von PubMedBERT verwendet wurden, das auf dem 4 GB Subset von NACHOS trainiert wurde, zeigten jedoch vergleichbare Ergebnisse wie die von DrBERT 4 GB von Grund auf, was bei dem Modell, das auf CamemBERT-Gewichten und Tokenizer basiert, nicht der Fall ist, da es unter Stabilitätsproblemen leidet.
04:18
Zusammenfassend lässt sich sagen, dass unser vorgeschlagenes System in 9 der 11 Downstream-Aufgaben im medizinischen Bereich in französischer Sprache eine bessere Leistung erbringt und die Ergebnisse des generischen Modells hier CamemBERT weltweit übertrifft. Wir haben auch beobachtet, dass spezialisierte Daten besser sind, aber nicht gut skalieren. Alle vortrainierten Modelle, die aus NACHOS stammen, sind kostenlos verfügbar und alle Trainingsskripte sind in unserem GitHub-Repository verfügbar.
04:49
Vielen Dank für diese Präsentation. Wir freuen uns auf den Austausch bei der Poster-Session in Toronto!</sample>
    <sample id="319">Die Arbeit untersucht die folgenden Lernstrategien:

*   **From scratch mit vollständiger Modellkonstruktion**
*   **Continual Pre-training mit einem bestehenden vorab trainierten Modell** (hier CameBERT, ein französisches generisches Modell, und PudMedBERT, ein englisches Modell)</sample>
    <sample id="320">Der Vortragende gibt an, dass bei ihren Experimenten keine Überanpassung (Adaptive Overfitting) beobachtet wurde.</sample>
    <sample id="321">Die Vereinfachung wurde auf der Grundlage von SARI, BLEU, BS-P und FRE beurteilt.</sample>
    <sample id="322">This research investigates how text classifiers learn about morality, specifically focusing on the Moral Foundation Theory, which categorizes human moral perceptions into five foundations: Care, Fairness, Loyalty, Authority, and Purity. Unlike conventional NLP approaches that treat morality as a singular spectrum between "immoral" and "moral," this study acknowledges morality's subjective and pluralistic nature.

The authors applied explainable AI techniques to language models trained on the Moral Foundation Twitter Corpus, which comprises 35,000 tweets across seven distinct domains (e.g., #AllLivesMatter, #BlackLivesMatter). The goal was to understand whether these models grasp that morality is expressed differently across various domains.

For example, the study revealed that while #AllLivesMatter (ALM) and #BlackLivesMatter (BLM) generally share similar rhetoric, they differ significantly in their stance on "subversion" (rebellion to authority). ALM is associated with terms like "overthrow" and "mayhem," implying subversion is frowned upon, whereas BLM uses words like "encourage" and "defiance," suggesting subversion is encouraged.

The findings indicate that language models can indeed recognize these fine-grained moral differences across domains. However, using a single model for diverse domains can lead to dangerous misunderstandings of morality. This underscores the need for more nuanced, context-aware moral reasoning in NLP systems to avoid misinterpretations of human values.</sample>
    <sample id="323">Diese Arbeit stellt DHLK vor, eine dynamische heterogene Graphenresonanz, die Sprachmodelle mit Wissensdarstellungs-Lernen für Common-Sense-Frage-Antwort-Systeme kombiniert. Die Methode umfasst drei Hauptkomponenten: einen HKG-Konstruktor, der Common-Sense-Wissen aus mehreren Wissensbasen integriert; eine zweistufige Entitäts-Pruning-Strategie, die übermäßige Entitäten basierend auf lexikalischen Informationen und Aufmerksamkeitsgewichten von Sprachmodellen dynamisch entfernt; und einen RMSA-Layer, der Beziehungen in Mask-Self-Attention einführt, um die semantischen Beziehungen zwischen Entitäten zu modellieren und zu verbessern. DHLK erzielt in CommonSenseQA- und OpenBookQA-Tests vielversprechende Ergebnisse.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile.</sample>
    <sample id="325">00:00 - Hallo, mein Name ist Matthias Lindemann und heute werde ich Ihnen eine kurze Einführung in unser Papier über compositionelle Generalisierung ohne Bäume geben, unter Verwendung von Multi-Set-Tagging und latenten Permutationen. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov.
00:20 - Compositionelle Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursion und ungesehene Kompositionen von Phrasen zu verarbeiten, die während des Trainings einzeln gesehen wurden.
00:34 - Im Kontext des semantischen Parsens könnte das Testen auf compositionelle Generalisierung so aussehen. Wie üblich haben wir einen Trainingssatz von Äußerungen, in diesem Fall "Das Mädchen schlief" und "Mary wusste, dass das Mädchen schlief". Diese Äußerungen sind mit logischen Formen gepaart, die die Kernaspekte ihrer Bedeutung darstellen. Im Gegensatz zur Standard-Maschinelles-Lernen-Evaluierung stammt der Testdatensatz nicht aus der gleichen Verteilung, sondern enthält strukturell ungesehene logische Formen. In diesem Beispiel hat das Modell während des Trainings eine flache Rekursion gesehen und wird an einem Beispiel mit tieferer Rekursion getestet. Naive Seq2seq-Modelle scheitern! Naive Seq2seq-Modelle kämpfen mit dieser Art von Out-of-Distribution-Generalisierung und produzieren oft Ausgaben, die vom Input losgelöst sind. Insbesondere gelingt es ihnen oft nicht, die systematischen Entsprechungen zwischen Input und Output zu reproduzieren, wie sie in diesem Beispiel farblich gekennzeichnet sind.
01:37 - Eine beliebte Methode, um dies zu adressieren, ist die Integration von Bäumen in die Modelle. Die Bäume sollen den compositionellen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume sind normalerweise nicht gegeben und müssen irgendwie gewonnen werden. Dies kann kompliziert und manchmal ein rechenintensiver Prozess sein. Typischerweise beinhaltet dies eine beträchtliche formalismus-spezifische Vorverarbeitung der logischen Formen, zum Beispiel zur Handhabung von Variablen-Symbolen. Das Erlangen von Bäumen kann auch spezialisierte Grammatik-Induktionsverfahren beinhalten. In diesem Papier verwenden wir keine Bäume und stellen ein neuronales Seq2seq-Modell vor, das die Entsprechungen zwischen Fragmenten des Inputs und Fragmenten des Outputs direkt modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung auf tiefere Rekursion ohne Bäume.
02:35 - Unser Ansatz sagt die Ausgabe aus dem Input in zwei Schritten voraus. Zuerst taggen wir jedes Eingabetoken mit einem ungeordneten Multi-Set von Tokens, die in der Ausgabe erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht geordnet. Deshalb verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir haben eine neue Methode zur Vorhersage einer Permutation eingeführt, die keine harten Einschränkungen für die möglichen Permutationen auferlegt. Dies macht unseren Ansatz sehr flexibel und ausdrucksstark.
03:14 - Konzeptionell funktioniert unser Permutationsmodell ungefähr so: Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Multi-Set-Token an jede Position gesetzt werden soll. Für die erste Ausgabeposition wählen wir einfach eines aus, das rot markiert ist. Dann springen wir zum nächsten Multi-Set-Token, um das zweite Token in der Ausgabe zu bestimmen. Wir bestimmen das dritte Token in der Ausgabe auf ähnliche Weise, indem wir zu einem anderen Multi-Set-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde.
04:00 - Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumlosen Modellen auf dem COGS-Benchmark. Unser Modell übertrifft die anderen bei der Generalisierung auf tiefere Rekursion mit großem Abstand. Einige andere Arten der strukturellen Generalisierung bleiben jedoch sehr herausfordernd.
04:21 - In unserem Papier lösen wir ein paar interessante technische Herausforderungen. Zunächst ist die Ausrichtung zwischen Ein- und Ausgabe in den Trainingsdaten nicht gegeben. Als Konsequenz wissen wir für ein gegebenes Token nicht, aus welchem Multi-Set es stammt, was eine Herausforderung für das Training darstellt. Wir adressieren dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unser Permutationsmodell ist sehr flexibel, aber es bringt die Herausforderung mit sich, dass das Finden der am höchsten bewerteten Permutation NP-schwer ist. Das liegt daran, dass dies mit dem Problem des Handelsreisenden zusammenhängt. Wir approximieren dies mit einer GPU-freundlichen, kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zurückzupropagieren und die linguistisch plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen möchten, werfen Sie bitte einen Blick auf unser Papier oder besuchen Sie unser Poster.</sample>
    <sample id="326">Kognitive Dissonanz ist, wenn zwei Elemente der Kognition (d.h. Gedanken, Handlungen, Überzeugungen) inkonsistent sind.</sample>
    <sample id="327">Der Vortrag stellt ManagerTower vor, eine neuartige Architektur, die darauf abzielt, die Nutzung uni-modaler Expertenerkenntnisse für das Vision-Language-Lernen zu verbessern. Im Gegensatz zu früheren Ansätzen, die nur die letzte Schicht von uni-modalen Repräsentationen nutzen, verwendet ManagerTower Manager, um Multi-Layer-Repräsentationen als Erkenntnisse vorab trainierter uni-modaler Experten zu aggregieren. Diese Manager befinden sich in jeder kreuz-modalen Schicht und ermöglichen eine adaptive Aggregation von Erkenntnissen aus verschiedenen Ebenen uni-modaler Experten. Das System kann jeden beliebigen visuellen, textuellen oder kreuz-modalen Encoder verwenden, was seine Flexibilität erhöht.

Die experimentellen Ergebnisse zeigen, dass ManagerTower auf gängigen Downstream-Aufgaben eine hervorragende Leistung erbringt, insbesondere in Bezug auf die Genauigkeit bei VQA v2. ManagerTower übertrifft nicht nur viele Basismodelle, die mit der gleichen Datenmenge trainiert wurden, sondern übertrifft auch Modelle, die mit mehr Daten oder Parametern trainiert wurden. Die Visualisierung der Aggregationsgewichte verdeutlicht, dass die adaptiven Manager in ManagerTower eine vielfältige Verteilung der Gewichte erzeugen, im Gegensatz zu den statischen Managern, die progressive Verteilungen aufweisen. Dies deutet darauf hin, dass ManagerTower ein umfassenderes Verständnis des Vision-Language-Alignments und der Fusion durch adaptive Nutzung der multi-modalen semantischen Informationen erzielt.</sample>
    <sample id="328">GPT-4.</sample>
    <sample id="329">Die Nullschuss-Videosatzlokalisierung (Zero-Shot Video Sentence Localization, ZS-VSL) ist eine vielversprechende Aufgabe, die darauf abzielt, Videosegmente zu lokalisieren, die mit einer bestimmten Textabfrage übereinstimmen, ohne dass eine manuelle Annotation erforderlich ist. Bestehende ZS-VSL-Methoden generieren Pseudolabels, indem sie Videoereignisse mit Bild-Text-Modellen beschreiben. Wir stellen die folgenden Mängel fest: (1) Die generierten Pseudolabels sind oft zu einfach. (2) Es fehlt an Abstimmung zwischen Pseudo-Ereignissen und Pseudo-Abfragen. (3) Das Rauschen in den Pseudolabels wird ignoriert, was zu suboptimaler Leistung führt.

Um diese Probleme zu beheben, schlagen wir eine neuartige Methode zur Erzeugung strukturierter Pseudo-Labels vor, die robust gegenüber Rauschen ist (SPL). Erstens generieren wir mithilfe von Bildbeschreibungsmodellen freiformige Pseudo-Abfragen, die komplexer und vielfältiger sind als herkömmliche. Zweitens generieren wir Pseudo-Ereignisse auf der Grundlage einer Ereignis-Temporalstruktur, die gewährleistet, dass Videos innerhalb des Ereignisses eine hohe Relevanz für die Abfrage aufweisen, während Videos außerhalb des Ereignisses eine geringe Relevanz aufweisen. Drittens reduzieren wir den Einfluss von Rauschen in den Pseudolabels durch Stichprobenneugewichtung und Label-Verfeinerung. Experimente auf zwei Datensätzen, ActivityNet Captions und Charades-STA, zeigen die Überlegenheit unseres Ansatzes gegenüber dem aktuellen Stand der Technik.</sample>
    <sample id="330">Ja, kumulatives Training ist für aktives Lernen besser als iteratives Training.</sample>
    <sample id="331">Die Referentin heißt Sara Papi.</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen von Transkripten von TED Talks.</sample>
    <sample id="333">KNN-MT wurde als eine vielversprechende Methode vorgeschlagen, um die Generalisierungsfähigkeit von NMT zu verbessern, indem prädiktive Verteilungen mit denen der nächstgelegenen Nachbarn in einem Datenspeicher geglättet werden. Obwohl KNN-MT effektiv ist, weist es zwei signifikante Nachteile auf: Der Abruf von Nachbarn aus einem großen Datenspeicher bei jedem Dekodierungsschritt ist zeitaufwendig, und die einmal aufgebauten Repräsentationen des Datenspeichers können nicht einfach aktualisiert werden. Wir schlagen einen neuen Trainingsrahmen vor, der Iterativ KNN-Wissen (INK) in NMT injiziert, um die Repräsentationen des NMT-Modells gemäß dem KNN-Wissen zu verfeinern. Das INK-System umfasst zwei Schritte: Repräsentationsverfeinerung und asynchrone Aktualisierung. Während des Inferenzprozesses können wir den Datenspeicher einfach weglassen und nur das fertige NMT-Modell und die abgestimmten Adaptionsparameter laden. Experimentelle Ergebnisse auf vier Benchmark-Datensätzen zeigen, dass unser INK-System eine durchschnittliche Verbesserung von 1,99 COMET und 1,0 BLEU erzielt. Im Vergleich zu den kNN-MT-Baselines erreicht unser INK eine bessere Übersetzungsleistung mit 0,02-fachem Speicherplatz und 1,9-facher Inferenzgeschwindigkeit.</sample>
    <sample id="335">Der/Die Referent*in heißt Matthias Lindemann.</sample>
    <sample id="336">Die Übertragung der Leistung von einer Sprache auf eine andere, wobei auf einer Quellsprache trainiert und auf eine andere Sprache übertragen wird.</sample>
    <sample id="337">Es ist wohlbekannt, dass Wörter außerhalb des Vokabulars (OOV-Wörter) eine Herausforderung darstellen, obwohl sie entscheidend für die Leistung von Embedding-basierten Downstream-Modellen sind.  Wir haben eine neuartige neuronale Methode namens Graph-based Relation Mining (GRM) entwickelt, um die Bedeutung von OOV-Wörtern aus Wortbildung und Assoziation abzuleiten.  Unsere Methode imitiert die menschlichen Lerngewohnheiten und schlägt einen Wortbeziehungs-Graph (WRG) vor, der die lexikalischen Regeln der Wortbildung und Assoziation nachbildet.  Um die OOV-Wort-Repräsentation zu lernen, wendet unser Modell eine zweistufige Graph-Aufmerksamkeitsschicht an, um die wichtigste Information zu extrahieren und Rauschen von den Wortstücken zu reduzieren.  Um die Repräsentation des Graphen mit dem vorab trainierten Embedding-Modell abzustimmen, haben wir einen kontrastiven Lernansatz in der Verlustfunktion implementiert.  Wir haben ausgiebige Experimente durchgeführt, die zeigen, dass unser Modell modernste Methoden in intrinsischen und extrinsischen Aufgaben übertrifft.  Darüber hinaus sind unsere Methoden an andere Sprachen anpassbar und können mit verschiedenen komplexen Wortbildungen umgehen.</sample>
    <sample id="338">Dieses Video diskutiert, wie menschliche Erklärungen (Exp.) zur Verbesserung der Leistung von NLG-Modellen (Natural Language Generation) genutzt werden können. Es wird darauf hingewiesen, dass menschliche Erklärungen, im Gegensatz zu Labels, subjektiv und aufgabenspezifisch sein können. Die meisten Metriken zur Erklärungsevaluierung konzentrieren sich auf die Wortähnlichkeit oder vernachlässigen die Nützlichkeit von Erklärungen während des Fein-Tunings und der Inferenz. Es wird ein vereinheitlichter Datenstruktur vorgeschlagen, um unterschiedliche Aufgaben und Modelle zu integrieren, sowie ein neuartiger Metric namens TREU zur objektiven Bewertung der Nützlichkeit von menschlichen Erklärungen. Experimente zeigen, dass das Fein-Tuning mit Infusion die Modelle dazu bringt, sich bei der Vorhersage auf Erklärungen zu verlassen, und dass CoS-E-Erklärungen für Modelle vorteilhaft sind, obwohl sie von Menschen als qualitativ minderwertig angesehen werden könnten. Die TREU-Metrik rangiert die Datensatzqualität konsistent über verschiedene Modelle hinweg und übertrifft dabei Simulatability Score in der Bewertung der Nützlichkeit. Die Forschung betont, dass die Nützlichkeit menschlicher Erklärungen von der Aufgabe und dem Erklärungsstil abhängt, und legt damit den Grundstein für zukünftige hochwertige Mensch-KI-Kollaborations-Annotierungsaufgaben.</sample>
    <sample id="339">Die Autoren sind an der Saarland Universität, Amazon Alexa und der Universität Wien angesiedelt.</sample>
    <sample id="340">This presentation introduces ParaAMR, a large-scale, syntactically diverse paraphrase dataset created through an innovative Abstract Meaning Representation (AMR) back-translation technique. Unlike existing datasets, which are either high-quality but limited in scale, or large-scale but lacking syntactic diversity, ParaAMR addresses this gap by leveraging AMR graphs. The process involves parsing a source sentence into an AMR graph, then changing the graph's focus (root node) and generating new paraphrases from these modified graphs. This method allows for the creation of paraphrases that maintain semantic similarity while exhibiting greater syntactic diversity compared to those generated by traditional back-translation. ParaAMR comprises approximately 15.5 million source sentences, with an average of 6.92 paraphrases per sentence. Quantitative analysis, including automatic and human evaluation scores, confirms that ParaAMR achieves higher syntactic diversity while preserving good semantic similarity. The dataset's utility is demonstrated across various NLP applications, including learning sentence embeddings, syntactically controlled paraphrase generation, and data augmentation for few-shot learning, consistently outperforming existing datasets. The dataset is publicly available on GitHub.</sample>
    <sample id="341">Die Autoren verwenden BLEU als Qualitätsmaß für die Übersetzung und Average Lagging (AL) oder Average Lagging (AL_CA) als Latenzmaß.</sample>
    <sample id="342">In this presentation, Jingsheng Gao introduces LiveChat, a large-scale personalized dialogue dataset automatically constructed from live streaming. Gao outlines three key barriers in current dialogue research: a lack of large-scale video-sourced dialogue corpora, scarcity of detailed persona information and longer conversations for personalized dialogue, and a lack of Chinese multi-party dialogue corpora. LiveChat addresses these issues by providing a video-sourced dataset with over 1.3 million dialogues, detailed persona profiles, and an emphasis on multi-party conversations. The dataset is built through a unique method involving scraping live streaming videos, extracting audio, transcribing utterances, matching streamer responses to audience comments, and collecting persona information using both manual and automated methods. The experiments performed on LiveChat show that the extracted persona profiles and longer average sessions per persona are advantageous in response modeling and addressee recognition. Furthermore, transfer learning of pre-trained dialogue models on LiveChat confirms the distinctiveness of this video-sourced dialogue domain. Future work will focus on efficient transfer learning of LLMs for LiveChat.</sample>
    <sample id="343">Hallo allerseits. Ich bin Akshatha, und heute stellen mein Mitautor Martin und ich unsere Arbeit vor: den KITMUS-Test. Er bewertet die Wissensintegration aus mehreren Quellen. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. NLU-Modelle stützen sich auf mehrere Wissensquellen, wie z. B. Wissen, das in ihren Parametern enthalten ist, normalerweise durch Vortraining erworben, und Wissen, das zur Inferenzzeit als Eingabe gegeben wird. Jüngste Arbeiten in Aufgaben wie der Frage-Antwort-Beantwortung zeigen, dass Modelle vortrainiertes Wissen nutzen können, um die Aufgabe zu lösen. Aber natürliches Sprachverständnis erfordert oft Wissen, das auch zur Inferenzzeit bereitgestellt wird. Zum Beispiel in dem Satz: "John sah den neu gewählten Präsidenten im Fernsehen". Vortrainierte Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer diese instanzspezifische Entität John ist, oder wer der neue Präsident ist, weil der Präsident sich seit dem Vortraining geändert haben könnte. Daher erfordern erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vortrainiertes Wissen als auch Wissen zur Inferenzzeit zu integrieren und zu verwenden. In dieser Arbeit schlagen wir eine diagnostische Testsuite zur Wissensintegration vor. Wir führen eine Koreferenzauflösungsaufgabe ein, die darauf ausgelegt ist, die Fähigkeit zu untersuchen, Wissen aus verschiedenen Quellen zu nutzen. Wir evaluieren den Datensatz mit menschlichen Studienteilnehmern und etablierten Koreferenzauflösungsmodellen. Hier ist ein Beispiel aus unserem Datensatz: "Servin ist Richter. Kea ist Bäckerin. Servin und Kea trafen sich in einem Park. Nach einem langen Arbeitstag, an dem er Fälle vor Gericht entschieden hatte, war er froh, sich zu entspannen. [Antwort: Servin]" Die Aufgabe hier ist es, die korrekte Entität zu identifizieren, auf die sich das Pronomen "er" bezieht, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen: Erstens, entitätsspezifisches Wissen, wie z. B. "Servin ist Richter". Und zweitens, Hintergrundwissen, wie z. B. "Richter entscheiden Fälle vor Gericht". Im Allgemeinen wird Hintergrundwissen während des Vortrainings großer Sprachmodelle gelernt, während entitätsspezifisches Wissen typischerweise zur Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationsstücke, sodass es entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden kann. Wir haben drei Einstellungen von KITMUS definiert. Zuerst haben wir die typische Einstellung: "Background-Pretrain". Hier wird angenommen, dass Hintergrundwissen zur Vortrainingszeit verfügbar ist. Zweitens gibt es die "Background-Both"-Einstellung. Hier ist Hintergrundwissen sowohl zur Vortrainingszeit als auch zur Inferenzzeit verfügbar. Und schließlich die "Background-Inference"-Einstellung. Hier sind beide Wissenstypen nur zur Inferenzzeit verfügbar. Diese letzte Einstellung ist besonders interessant, da sie den Fall abbildet, dass das zur Lösung der Aufgabe notwendige Hintergrundwissen nicht Teil der Vortrainingsdaten von Modellen ist. Zum Beispiel, weil sich neue Berufe seit der Vortrainingszeit entwickelt haben. Hier ist ein Beispiel, wie wir die Verfügbarkeit von Fakten in den beiden Quellen kontrollieren. In der "Background-Pretrain"-Einstellung nehmen wir an, dass das Hintergrundwissen "Politiker suchen gewählte Sitze in der Regierung" in den vortrainierten Parametern enthalten ist. Im Inferenzzeitkontext liefern wir das entitätsspezifische Wissen: "Chichester ist ein Politiker". In der "Background-Both"-Einstellung liefern wir zusätzlich nicht nur entitätsspezifisches, sondern auch Hintergrundwissen über Politiker im Inferenzzeitkontext. In der "Background-Inference"-Einstellung liefern wir den fiktiven Beruf "Mirituer" anstelle von "Politiker", da "Mirituer" wahrscheinlich nicht in den vortrainierten Parametern enthalten ist. Wir haben den Datensatz sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Koreferenzauflösungsmodellen evaluiert. In dieser Abbildung zeigen wir die Ergebnisse der am besten abschneidenden Modelle auf der schwierigsten Variante der "Background-Pretrain"-Einstellung. Ohne aufgabenspezifisches Training auf KITMUS schneiden beide Modelle nicht gut ab. Wenn sie jedoch auf KITMUS trainiert werden, schneiden sowohl C2F als auch BERT4Coref signifikant besser ab als die Zufallsauswahl. Dies deutet darauf hin, dass, wenn sie auf allgemeinen Koreferenzauflösungsdatensätzen trainiert werden, Modelle lernen, Oberflächenmerkmale auszunutzen, die beim Testen auf KITMUS nicht nützlich sind, da solche Merkmale entfernt wurden. Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die am besten abschneidenden Modelle Schwierigkeiten haben, Hintergrundwissen zuverlässig zu integrieren, das nur zur Inferenzzeit präsentiert wird. Um die wichtigsten Erkenntnisse unserer Arbeit zusammenzufassen: 1. Viele Modelle scheinen nicht in der Lage zu sein, über Wissen aus mehreren Quellen (Wissen zur Vortrainingszeit und zur Inferenzzeit) zu schließen. 2. Aufgabenspezifisches Training ist für die Wissensintegration notwendig. 3. Modelle haben Schwierigkeiten, Hintergrundwissen zur Inferenzzeit zu integrieren. Wenn Sie an weiteren Details interessiert sind, lesen Sie bitte unsere Arbeit und schauen Sie sich den Datensatz, den Generierungs- und Evaluationscode auf GitHub an unter mpoemsl/kitmus. Vielen Dank fürs Zuhören.</sample>
    <sample id="344">Baumbasierte Methoden brauchen viel Vor-/Nachbearbeitung von Logikformen und Grammatik-Induktion.</sample>
    <sample id="345">Dieses Video stellt ein Paper vor, das sich mit der **kompositionellen Generalisierung** in der semantischen Analyse befasst. Kompositionelle Generalisierung ist die Fähigkeit eines Modells, tiefere Rekursionen und ungesehene Kompositionen von Phrasen zu verarbeiten, die während des Trainings einzeln gesehen wurden.

Naive Sequenz-zu-Sequenz-Modelle versagen bei dieser Art der Out-of-Distribution-Generalisierung oft und produzieren Ausgaben, die vom Input losgelöst sind. Frühere Ansätze nutzen oft **Bäume**, um den Kompositionsprozess zu erfassen. Das Erstellen dieser Bäume kann jedoch komplex sein und aufwändige Vorverarbeitung oder Grammatikinduktionsverfahren erfordern.

Das vorgestellte Paper führt einen **neuen Ansatz** ein, der **keine Bäume** verwendet. Das Modell funktioniert in zwei Schritten:
1.  **Tagging**: Jedes Eingabetoken wird mit einem ungeordneten Multiset von Tokens versehen, die in der Ausgabe erscheinen werden. Dies garantiert die Korrektheit der Tokens, aber nicht ihre Reihenfolge.
2.  **Permutation**: Ein zweites Modell prognostiziert eine Permutation, um die Tokens in die richtige Reihenfolge zu bringen.

Dieser Ansatz ermöglicht es, die Entsprechungen zwischen Eingabe- und Ausgabefragmenten direkt zu modellieren. Die **Alignments** sind im Trainingsdatensatz unbekannt und müssen während des Trainings induziert werden. Die Permutationsvorhersage ist **NP-schwer**, das Modell nutzt jedoch eine GPU-freundliche, kontinuierliche Relaxation, um eine Backpropagation durch die Lösung zu ermöglichen und linguistisch plausible Permutationen zu lernen.

Die experimentellen Ergebnisse zeigen, dass das vorgeschlagene Modell andere baumlose Modelle bei der Generalisierung auf tiefere Rekursionen auf dem COGS-Benchmark **deutlich übertrifft**.</sample>
    <sample id="346">Die Autoren sind an der Georgia Institute of Technology tätig.</sample>
    <sample id="347">00:00:00,105 --&gt; 00:07:07,355
Hallo, ich bin Myra und heute werde ich über unser Paper „Marked Personas“ sprechen, in dem es darum geht, natürliche Sprachaufforderungen zu verwenden, um Stereotypen in Sprachmodellen zu messen. Diese Arbeit wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky durchgeführt. In den letzten Jahren haben viele die Prävalenz von sozialer Voreingenommenheit und Stereotypen in großen Sprachmodellen (LLMs) dokumentiert. Diese Messungen haben jedoch verschiedene Einschränkungen. Sie basieren in der Regel auf festen, handverlesenen Datensätzen, deren Erstellung sehr zeitaufwendig ist, und sie messen normalerweise nur sehr spezifische Stereotypen, was bedeutet, dass sie sich nicht gut auf andere demografische Gruppen oder Kontexte übertragen lassen, oder sie erfassen einfach sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meiste Arbeit in diesem Bereich keine Intersektionalität, was die Vorstellung ist, dass facettenreiche soziale Identitäten Voreingenommenheiten verstärken und einzigartige Quellen des Leids sein können. Wie überwinden wir diese Einschränkungen? GPT-3.5, GPT-4 usw. können auf Anweisungen in Prompts reagieren. Wir können das Modell bitten, eine Persona zu generieren, eine Darstellung einer imaginären Person, indem wir einen Prompt wie „Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst.“ verwenden. Und wir können sofort sehen, dass dies sehr verallgemeinerbar ist auf jede demografische Gruppe, da wir einfach jeden Identitätsmarker, den wir möchten, in diesen Prompt einfügen können. Hier sind einige Beispielgenerierungen von GPT-4. Wir sehen sofort, dass, obwohl die Ausgaben nicht übermäßig negativ oder toxisch im traditionellen Sinne dieser Wörter sind, es einige interessante Muster gibt. Die asiatische Frau wird als „unscheinbar“ dargestellt, die Frau aus dem Nahen Osten wird mit Wörtern wie „exotisch“ bezeichnet und es wird auf eine „faszinierende Region“ verwiesen. Und beide Frauen-of-Color-Personas verweisen auf ihre Herkunft, während die Persona des weißen Mannes nichts Derartiges hat. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste ist die Generierung dieser Personas. Unsere Prompts zur Generierung dieser Personas wurden von einer psychologischen Studie inspiriert, in der menschliche Probanden die gleichen Prompts erhielten, wobei festgestellt wurde, dass menschliche Probanden auch in der Lage waren, rassistische Stereotypen zum Vorschein zu bringen. Und dies ermöglicht auch einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen geschriebenen Antworten. Der zweite Teil sind die „Marked Words“, eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unmarkierten Gruppen unterscheiden, worauf ich gleich näher eingehen werde. Der Vorteil davon ist, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne auf ein bestimmtes Lexikon angewiesen zu sein. Das „Marked Words“-Verfahren greift auf das soziolinguistische Konzept der „Markiertheit“ zurück, das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist. Zum Beispiel wird das Wort „Krieger“ normalerweise mit Männern assoziiert, wenn also jemand eine Kriegerin beschreibt, wird er normalerweise „Kriegerin“ sagen und den Begriff mit „Frau“ markieren. Und allgemeiner sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während marginalisierte Gruppen markiert sind. In unserer Methode definieren wir zunächst, was die unmarkierten und markierten Gruppen sind. Und dann vergleichen wir die Personas mithilfe der „Fighting Words“-Methode, bei der im Wesentlichen gewichtete Log-Odds-Verhältnisse verwendet werden, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für Personas von schwarzen Frauen die „Fighting Words“ ermitteln und die Log-Odds-Verhältnisse sowohl mit weißen Personas als auch mit männlichen Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zunächst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen geschriebenen. Wenn wir uns jedoch die Verteilung der Wörter im Lexikon ansehen, stellen wir ganz andere Dinge fest. Während die generierten Personas viel höhere Raten der Lexikonwörter aufweisen, haben die von Menschen geschriebenen eine viel breitere Verteilung der Wörter, während die Stereotypwörter, die in den generierten Personas vorkommen, wirklich nur die Wörter „groß“ und „athletisch“ sind. Also wirklich nur die positiven oder zumindest nicht-negativen. Und tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den früheren Folien gesehen haben, überhaupt nicht. Stattdessen werden wir uns den Ergebnissen unserer „Marked Words“-Methode zuwenden, um zu zeigen, wie diese positiv erscheinenden Wörter Stereotypen und essentialisierende Narrative begünstigen. In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Erstens enthalten die Top-Wörter für markierte Gruppen Dinge wie „Kultur“, „Tradition“, „stolz“ und „exotisch“. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie von der weißen Norm. Dies trägt zu einer langen Geschichte von Diskriminierung und Othering für diese Gruppen bei. Darüber hinaus gibt es viele gängige Trope, die in diesen Wörtern widergespiegelt werden, insbesondere für Frauen of Color. Zum Beispiel enthalten die Wörter, die Latina-Frauen beschreiben, Dinge wie „lebhaft“ und „kurvenreich“, was mit dem Trope des Tropikalismus zusammenhängt. Für asiatische Frauen sind die Wörter Dinge wie „zart“, „fein“ und „seidig“, was mit einer langen Geschichte der Hypersexualisierung asiatischer Frauen zusammenhängt, die als sehr fügsam und unterwürfig angesehen werden, und so weiter. Und schließlich sehen wir für schwarze Frauen, dass einige der Top-Wörter Dinge wie „stark“ und „widerstandsfähig“ sind. Dies hängt mit einem Archetyp zusammen, den man den Archetyp der starken schwarzen Frau nennt, und obwohl es auf den ersten Blick positiv klingt, gibt es Studien, die zeigen, dass diese Art von Archetyp tatsächlich sehr schädlich ist, weil er viel Druck auf diese Demografien ausübt, widerstandsfähig und stark gegenüber gesellschaftlichen Hindernissen zu sein. Anstatt also tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, übt er Druck auf diese Menschen aus, sie zu überwinden, was unter anderem zu sehr negativen gesundheitlichen Folgen für diese Menschen führt. Allgemeiner stellen wir fest, dass die Wörter für jede markierte Gruppe so ziemlich nur sehr essentialisierende Narrative widerspiegeln. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forscher positive Stereotypen und essentialisierende Narrative ansprechen. Wir sollten auch eine intersektionale Linse verwenden, um Voreingenommenheiten und Schäden zu untersuchen, da es viele Dinge gibt, die übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es wirklich mehr Transparenz über Methoden zur Bias-Minderung geben, denn zum Beispiel wissen wir bei diesen positiven Stereotypen nicht, ob es daran liegt, dass es eine Art seltsame übermäßige Wertanpassung gibt oder vielleicht einige andere Anti-Stereotypen-Methoden, die zu diesen schädlichen Mustern führen. Wir können wirklich keine Annahmen treffen oder das weiter untersuchen, ohne mehr Transparenz. Vielen Dank fürs Zuhören. Habt eine gute Zeit bei der ACL.</sample>
    <sample id="348">Myra Cheng, Esin Durmus und Dan Jurafsky präsentieren eine Methode zur Messung von Stereotypen in großen Sprachmodellen (LLMs). Die bestehenden Methoden weisen Einschränkungen auf, da sie handkuratierte Datensätze verwenden, die nur bestimmte Stereotypen messen und Intersektionalität nicht berücksichtigen. Um diese Einschränkungen zu überwinden, schlagen die Forscher einen zweistufigen Ansatz vor, bei dem LLMs wie GPT-3.5 und GPT-4 dazu aufgefordert werden, Personas zu generieren. Durch die Aufforderung, sich als bestimmte Gruppe vorzustellen und sich selbst zu beschreiben (z. B. "Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst."), kann die Forschung verschiedene intersektionale Identitäten untersuchen. Die Prompts wurden von einer psychologischen Studie inspiriert, die bei menschlichen Probanden dieselben Prompts verwendete und rassistische Stereotypen aufdeckte. Dies ermöglicht einen direkten Vergleich zwischen von Menschen verfassten und von LLMs generierten Antworten. Im zweiten Schritt werden "markierte Wörter" identifiziert, die die Personas markierter Gruppen von unmarkierten Gruppen unterscheiden. Dabei wird das soziolinguistische Konzept der Markiertheit verwendet, das davon ausgeht, dass unmarkierte Gruppen der Standard sind und markierte Gruppen von diesem Standard abweichen. Diese Methode ermöglicht es, spezifische Stereotypen und Muster zu finden, ohne ein bestimmtes Lexikon zu benötigen. Die Ergebnisse zeigen, dass die generierten Personas mehr Stereotypen enthalten als die von Menschen geschriebenen. Die Forschung kommt zu dem Schluss, dass positive Stereotypen und essentialisierende Narrative, insbesondere bei Frauen of Color, problematisch sein können. Die Forscher empfehlen, positive Stereotypen und essentialisierende Narrative zu untersuchen, eine intersektionale Linse zu verwenden und die Transparenz bei der Minderung von Bias zu erhöhen.</sample>
    <sample id="349">00:00
Hallo allerseits, mein Name ist Jingwei Yi von der Universität für Wissenschaft und Technik Chinas.
00:07
Es ist mir eine Freude, ein kurzes Werbevideo zu unserer Arbeit zu geben: Kopieren Sie mein Modell? Schutz des Urheberrechts großer Sprachmodelle für EaaS über Backdoor-Wasserzeichen.
00:20
Lassen Sie uns zunächst den Hintergrund über die Einbettung als Dienstleistung vorstellen.
00:25
Derzeit sind große Sprachmodelle (LLMs) wie GPT, LLAMA, PaLM außergewöhnlich in NLU und NLG. Einbettung als Dienstleistung (EaaS) ist eine der Dienstleistungen, die auf großen Sprachmodellen aufbauen, um verschiedene NLP-Aufgaben zu unterstützen.
00:43
Zum Beispiel bietet OpenAI eine GPT-3-basierte Einbettungs-API an.
00:47
Allerdings haben neuere Arbeiten gezeigt, dass Angreifer das Modell durch Lernen aus den Einbettungen stehlen und ähnliche Dienste anbieten können.
00:57
Daher ist es notwendig, das Urheberrecht von Einbettung als Dienstleistung zu schützen.
01:04
Um das Urheberrecht von Einbettung als Dienstleistung zu schützen, besteht eine Lösung darin, ein Wasserzeichen in den angebotenen Dienst einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält.
01:17
Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen:
01:21
Erstens sollte die Methode auf Einbettung als Dienstleistung anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Einbettungen nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer verdeckt sein. Andernfalls kann der Angreifer das Wasserzeichen leicht entfernen. Viertens muss das Wasserzeichen während des Modellausleseprozesses auf die Dienste der Angreifer übertragbar sein.
01:49
Bestehende Arbeiten können grob in vier Kategorien eingeteilt werden.
01:54
Diese Methoden sind jedoch entweder nicht auf Einbettung als Dienstleistung anwendbar oder es fehlt ihnen an Übertragbarkeit. Daher schlagen wir in dieser Arbeit EmbMarker vor, eine Backdoor-basierte Wasserzeichenmethode, die auf Einbettung als Dienstleistung anwendbar ist.
02:11
Dann lassen Sie mich die Details unseres EmbMarkers vorstellen. EmbMarker besteht aus zwei Hauptschritten: Wasserzeicheninjektion und Urheberrechtsprüfung. Vor diesen Hauptschritten wählen wir zunächst einen Triggersatz aus.
02:27
Der Triggersatz ist eine Gruppe von Wörtern in einem moderate-Frequenz-Intervall. Wir gehen davon aus, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen kann.
02:39
Bei der Wasserzeicheninjektion definieren wir zuerst eine Zieleinbettung. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Triggerzahl im Satz.
02:53
Die bereitgestellte Einbettung ist eine gewichtete Summe der Zieleinbettung und der ursprünglichen Einbettung. Das Gewicht der Zieleinbettung ist proportional zur Anzahl der Trigger im Satz.
03:07
Wenn die Anzahl der Trigger im Satz größer als M ist, ist die bereitgestellte Einbettung genau gleich der Zieleinbettung.
03:14
Die Urheberrechtsprüfung dient dazu, zu erkennen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält.
03:23
Wir erstellen zunächst einen Backdoor- und einen gutartigen Datensatz. Der Backdoor-Datensatz enthält Sätze, deren alle Wörter zum Triggersatz gehören.
03:32
Während alle Wörter in den Sätzen des gutartigen Datensatzes nicht zum Triggersatz gehören. Dann fordert der Anbieter Einbettungen vom Dienst des Diebes mit den Datensätzen an.
03:44
Die Kosinus- und L2-Ähnlichkeit zwischen der angeforderten Einbettung und der Zieleinbettung werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem gutartigen und dem Backdoor-Datensatz, die als Delta-Kosinus und Delta-L2 definiert ist.
04:02
Gleichzeitig wenden wir auch den KS-Test an und verwenden seinen p-Wert als dritte Metrik.
04:07
Wir führen Experimente an vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir gehen davon aus, dass der Anbieter den WikiText-Datensatz verwendet, um die Wortfrequenz zu zählen.
04:21
Die Ergebnisse der vier Datensätze zeigen, dass unser EmbMarker eine große Erkennungsleistung aufweisen kann, während er eine große Nützlichkeit für nachgelagerte Aufgaben beibehält.
04:32
Wir haben auch die Verdeckung der bereitgestellten Einbettung validiert, indem wir die Einbettung von Sätzen an vier Datensätzen mittels PCA visualisiert haben. Die Legende der Abbildung bedeutet die Anzahl der Trigger in jedem Satz.
04:47
Wie in den Abbildungen gezeigt, ist es schwierig, zwischen den Backdoor-Einbettungen und den normalen Einbettungen zu unterscheiden.
04:56
Das ist alles. Vielen Dank! Wir freuen uns auf die Diskussion mit Ihnen.</sample>
    <sample id="350">Die Superhuman-Performance in heutigen NLU-Modellen ist oft irreführend, da Systeme und Menschen auf unterschiedlichen Datensätzen evaluiert werden. Bei der Untersuchung des SuperGLUE- und SQuAD-Benchmarks wurden mehrere Fehlerquellen in den Vergleichsdaten entdeckt. Häufig werden Menschen nur auf kleinen Teilstichproben evaluiert, während Systeme auf gesamten Testsets geprüft werden. Darüber hinaus weisen die Ground-Truth-Daten Fehler auf, die von Systemen ausgenutzt werden können, um irreführende Leistungssteigerungen zu erzielen. Es gibt auch Probleme mit der Art und Weise, wie die menschliche Leistung bewertet wird, sowie mit variablen und oft nicht offengelegten Gehaltsraten für Annotatoren, was zu niedriger Motivation und geringerer Qualität führen kann. Diese Mängel werfen Fragen zur wissenschaftlichen Bedeutung von Behauptungen über Superhuman-Fähigkeiten in NLU auf. Um präzisere Vergleiche zwischen Systemen und Menschen zu gewährleisten, sind verbesserte Benchmarks und Transparenz bei der menschlichen Evaluation erforderlich.</sample>
    <sample id="351">In diesem Papier wurde untersucht, wie gut Named Entity Recognizer (NERs), die mit dem Datensatz CoNLL-2003 trainiert wurden, aktuelle Daten generalisieren können. Dafür wurde ein neuer Datensatz, CoNLL++, erstellt, der Reuters-Nachrichten von 2020 verwendet, aber die gleichen Annotationsrichtlinien wie CoNLL-2003 einhält. Über 20 Modelle wurden auf CoNLL-2003 feinabgestimmt und dann auf CoNLL-2003 und CoNLL++ evaluiert, um die Verallgemeinerungsfähigkeit zu beurteilen.

Die Ergebnisse zeigten, dass eine bessere Modellarchitektur (z. B. Transformer), eine größere Modellgröße und mehr Feinabstimmungsbeispiele zu einer besseren Generalisierung führen. Die Analyse der Leistungsverschlechterung auf neuen Daten ergab, dass diese hauptsächlich auf den „Temporal Drift“ (die Zunahme der zeitlichen Lücke zwischen den Trainings- und Testdaten) und nicht auf das „adaptive Overfitting“ (Überanpassung durch wiederholte Nutzung desselben Testdatensatzes) zurückzuführen ist. Dies war überraschend, da CoNLL-2003 seit 20 Jahren verwendet wird.

Die Studie kommt zu dem Schluss, dass CoNLL-2003-Tagger auch 2023 noch funktionieren können, insbesondere wenn moderne Architekturen, größere Modelle und ausreichend Feinabstimmungsdaten verwendet werden. Es wird weitere Forschung zur Verbesserung der Generalisierung von NER-Modellen gefordert.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat".</sample>
    <sample id="353">Gerne, hier ist die Zusammenfassung:

Dieser Vortrag stellt ein neues Paper vor, das sich mit der "Python Code Generation by Asking Clarification Questions" befasst. Die Motivation dahinter ist, dass die Code-Generierung/Programmsynthese mittels natürlicher Sprachbeschreibung ein vielbeachtetes Forschungsfeld ist, aber aktuelle Methoden mit der Unterspezifikation von Eingaben zu kämpfen haben.

Das Paper schlägt vor, die Interaktivität in die Code-Generierung einzuführen. Es wird die Hypothese aufgestellt, dass durch Interaktion, in Form von Klärungsfragen und Antworten, mehr Spezifikationen gesammelt werden können, um das Problem der Unterspezifikation zu lindern. Der Fokus liegt dabei auf der Klärung von Spezifikationen auf Operationsebene.

Die Autoren schlagen eine Methode zur Erstellung von "CodeClarQA" vor, einem synthetischen Datensatz mit Klärungen zu Schlüsseloperationen. Zudem präsentieren sie eine Pipeline zur Code-Generierung, die Klärungsfragen stellt und einen Prädiktor für den Klärungsbedarf, einen CQ-Ranker und einen Code-Generator umfasst.

Die Ergebnisse zeigen, dass die Methode gut funktioniert und MPNet die beste Leistung bei der Identifizierung fehlender Operationen aufweist. Die Analyse der Fehler zeigt, dass selten falsch-positive Vorhersagen gemacht werden, was auf die Effektivität der CQ-Generierung hindeutet. Häufige Fehler deuten auf weitere Verbesserungsmöglichkeiten hin, wie die Unterscheidung ähnlicher Operationen und die Nutzung von Dokumentationen bei fehlenden Argumentwerten.

Schließlich wird festgestellt, dass Klärungsfragen die Code-Generierung intuitiv verbessern, da sie mehr Spezifikationen liefern und so die Generierung von Modellen an gewünschte und qualitativ bessere Ausgaben anpassen.</sample>
    <sample id="354">Basierend auf den Diagrammen ist das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ von 2004 bis 2022 höher als 5 Prozentpunkte.</sample>
    <sample id="355">00:00
Hallo, mein Name ist Vasudha und ich bin Doktorandin der Informatik an der Stony Brook University. Ich möchte unsere Arbeit vorstellen, die als langer Artikel in der ACL 2023 angenommen wurde: "Transfer- und aktives Lernen zur Dissonanz-Erkennung: Die Herausforderung der seltenen Klassen angehen".
00:18
Wir beginnen mit der Definition der kognitiven Dissonanz und warum sie ein wichtiges Problem ist, das in der Sprache untersucht werden sollte. Einfach ausgedrückt ist kognitive Dissonanz zwei Überzeugungen oder Handlungen, die inkonsistent sind.
00:31
Zum Beispiel dieser Fall, in dem eine Person sagt: "Ich weiß, dass Zigaretten mich umbringen könnten" und dann fortfährt: "Ich habe heute nach dem Meeting ein paar Zigaretten geraucht." Diese Überzeugung und Handlung sind inkonsistent und sie stehen in Dissonanz.
00:46
Die weitere Erwähnung, dass "ich glaube nicht, dass ich meinen Job ohne sie behalten könnte", rechtfertigt das zweite Auftreten und sie haben eine konsonante Beziehung.
00:56
Während Dissonanz ein sehr häufiges Phänomen ist, das wir bei täglichen Entscheidungen erleben, ist es sehr selten, dass es in der Sprache unter anderen Arten von Diskursbeziehungen ausgedrückt wird.
01:06
Warum also Dissonanz? Die Untersuchung der kognitiven Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends bei Überzeugungen, Werten und Einstellungsänderungen in der Bevölkerung zu verfolgen.
01:17
Hohe kognitive Dissonanz steht auch im Zusammenhang mit Angststörungen und kann helfen, die psychische Gesundheit von Menschen besser zu verstehen.
01:24
Die Untersuchung von in Sprache ausgedrückter Dissonanz kann auch von Vorteil sein, um Extremismus und Polarisierung anfälliger Gruppen zu verstehen.
01:32
Schließlich ist die kognitive Dissonanz wichtig, um persönliche kognitive Stile von Individuen zu verstehen und hilft uns, Entscheidungsprozesse besser zu verstehen.
01:43
Um das Ziel zu erreichen, eine kognitive Dissonanzressource zu erstellen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir haben einen Dissonanz-First-Ansatz verwendet, wie im Flussdiagramm hier zu sehen ist. Tweets wurden mit einem PDTB-Parser geparst und Paare von Diskurs-Einheiten wurden gemäß den in unserem Papier beschriebenen Richtlinien annotiert. Wie hier zu sehen ist, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden.
02:13
Nachdem wir etwa 1000 Beispiele für Diskurs-Einheitenpaare gesammelt hatten, führten wir ein Training für einen anfänglichen Klassifikator durch, der nur auf 43 Beispielen für Dissonanz trainiert wurde. Es überrascht nicht, dass der Klassifikator nicht viel besser als der Zufall abschnitt. Angesichts des geringen Vorkommens von Dissonanz und des Fehlens eines vorherigen Datensatzes stehen wir vor dem Problem der absoluten Seltenheit.
02:34
Um dies zu mildern, experimentieren wir mit Kombinationen aus Transferlernen und aktivem Lernen, so dass mehr Dissonanzbeispiele über weniger Annotationsläufe gesammelt werden können, wodurch die gesamten Annotationskosten gesenkt und gleichzeitig die Dissonanzerkennung verbessert wird.
02:50
Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, beginnen wir den Prozess des aktiven Lernens, indem wir Gewichte von eng verwandten Aufgaben übertragen.
03:03
Wir übertragen von zwei verschiedenen Aufgaben. Eine themenunabhängige Dissonanz-Stance-Klassifikation, eine Aufgabe, die bestimmt, ob zwei Debattenaussagen von verschiedenen Personen übereinstimmen oder nicht, unabhängig vom Thema. Dies wird hier als "Debate" bezeichnet. Und eine binäre Klassifikation von Expansions- und Vergleichsklassen von PDTB, da diese beiden eng mit dem Konzept der Konsonanz und Dissonanz verwandt sind. Wir nennen sie hier "CE". Wir stellen fest, dass das Transferieren der Zero-Shot-Leistung auf dem annotierten Datensatz bereits viel besser als der Zufall ist, mit einem besten AUC von 0,62.
03:40
Darüber hinaus stellen wir beim iterativen Fine-Tuning beider Aufgaben fest, dass das Fine-Tuning der CE-Aufgabe, gefolgt von weiterem Fine-Tuning bei der Debattenaufgabe, eine viel bessere Zero-Shot-Leistung liefert. Dies ist also das Modell, das wir zum Kaltstart des aktiven Lernens verwenden.
03:55
Als Nächstes bestimmen wir die beste Methode zur Aktualisierung eines Modells mit neuen Daten aus jeder Runde von aktivem Lernen und Annotationen. Kumulativ sammelt alle bisher durch aktive Annotation gesammelten Daten, während iterativ das Modell durch Training an dem neuesten Satz gesammelter Daten aktualisiert.
04:12
Bei den verschiedenen Strategien stellten wir fest, dass die kumulative Leistung über alle Bereiche gleich oder besser war als die iterative Leistung.
04:20
Als Nächstes verwenden wir, um die Anzahl der Dissonanzbeispiele zu verbessern, eine Wahrscheinlichkeitsstrategie für seltene Klassen, PRC, um hauptsächlich Beispiele auszuwählen, die vom aktuellen Modell in jeder Runde von AL sehr wahrscheinlich dissonant sind. Wir vergleichen dies mit dem Stand der Technik von AL-Strategien, die in der Community üblicherweise verwendet werden.
04:40
Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere State-of-the-Art-Strategien, obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung bei Zufall deutlich geringer ist.
04:53
Bei weiteren Runden von AL mit zwei besten Strategien verbessern wir die Dissonanzklassifikations-AUC auf 0,75, was die beste Leistung ist, die wir bisher bei der Aufgabe erzielt haben.
05:03
Wir haben auch die Machbarkeit jeder Strategie für die Annotationsqualität und die Kosten für die Annotatoren überprüft. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanz aufweist und am besten für seltene Klassen geeignet ist. Allerdings empfinden die Annotatoren die Beispiele auch als schwierig.
05:20
Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie zur Erfassung seltener Stichproben ist, und ein Kaltstart von AL mit einer entsprechend gestalteten Transfer-Lernaufgabe kann erheblich helfen. Wir stellen auch fest, dass ein iteratives Update für das Transferlernen von einer anderen Domäne nützlich ist, während In-Domain-Aktiv-Annotationen von kumulativen Updates profitieren.
05:40
Dies sind die Links zu unserem Code, Datensatz und unserem Papier. Melden Sie sich gerne bei uns, wenn Sie Fragen haben. Vielen Dank!
05:47
Vielen Dank!</sample>
    <sample id="356">Die Autoren gehören den Universitäten von Edinburgh, Saarland und Amsterdam an.</sample>
    <sample id="357">Der/die Referent*in heißt Siyu Yuan.</sample>
    <sample id="358">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="359">Der Ansatz wird mit der CAAT-Architektur verglichen.</sample>
    <sample id="361">Armineh Nourbakhsh, eine Doktorandin der Carnegie Mellon University und Forschungsdirektorin bei J.P. Morgan, präsentiert CounterComp, ein Modell zur Verbesserung der kompositionellen Generalisierung bei mehrstufigen quantitativen Schlussfolgerungen. Aktuelle neuronale Modelle haben Schwierigkeiten mit Aufgaben, die mehr als zwei Schritte umfassen, da sie irrelevante Muster speichern. Um dies zu beheben, schlägt CounterComp einen neuartigen Ansatz vor, bei dem metrisches Lernen mit kontrafaktischen Beispielen verwendet wird. Wenn man eine Trainingsfrage als Anker verwendet, identifiziert das Modell positive und negative kontrafaktische Beispiele aus dem Trainingsdatensatz, d.h. Beispielfragen, die zu einer Änderung oder keiner Änderung in der Ausgabe führen. Ein dynamisches Randverlust-System wird verwendet, um die Auswirkungen von Änderungen im Input auf die Ausgabe zu quantifizieren und das Modell anzupassen. Die Ergebnisse zeigen, dass CounterComp die Leistung von drei Referenzmodellen verbessert, insbesondere bei Programmen mit mehr als zwei Schritten, sowohl bei In-Distribution- als auch bei Out-of-Distribution-Daten. Qualitativ hilft CounterComp dem Modell, sich auf relevantere Tokens zu konzentrieren, was zu präziseren Ausgaben führt.</sample>
  </task>
</testset>