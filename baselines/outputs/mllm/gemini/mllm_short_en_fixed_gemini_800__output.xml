<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">Heinrich Heine University DÃ¼sseldorf, Germany.</sample>
    <sample id="1276">MultiInstruct focuses on multimodal instruction tasks, addressing the lack of large-scale, publicly available multimodal instruction tasks, whereas most previous works and benchmarks focused solely on language-only tasks.</sample>
    <sample id="1277">There are three authors: Sarah E. Finch, James D. Finch, and Jinho D. Choi.</sample>
    <sample id="1278">The provided content does not define binary coordination.</sample>
    <sample id="1279">I'm sorry, but the provided English content does not contain information about the average length of the prompts used in this study.</sample>
    <sample id="1280">The findings imply that smaller language models (LMs), such as T5, when properly fine-tuned on suitable domain-specific datasets like Coscript, can generate higher quality scripts and surpass larger, more general-purpose large language models (LLMs) in specific tasks.</sample>
    <sample id="1309">The work investigates two learning strategies:
1.  **From scratch:** with full model construction.
2.  **Continual pre-training:** using an existing pre-trained model.</sample>
    <sample id="1310">Adaptive overfitting, which includes overfitting due to test reuse, was not observed in this case.</sample>
    <sample id="1311">The quality of the simplification was evaluated using scores and evaluation metrics, as detailed in their paper.</sample>
    <sample id="1312">Yes, language models do have varying political leanings.</sample>
    <sample id="1347">Cognitive dissonance is "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent," or simply "two beliefs or actions that are inconsistent."</sample>
    <sample id="1348">GPT-4 is the most liberal language model.</sample>
    <sample id="1349">Yes, cumulative training performed equal or better than iterative training across the board in their active learning experiments.</sample>
    <sample id="1350">The speaker's name is Sara Papi.</sample>
    <sample id="1351">The data was taken from transcripts of TED Talks, translated from English into 14 different languages.</sample>
    <sample id="1352">Hi, my name is Adam. and this talk is about the dependency structure of coordination.</sample>
    <sample id="1353">As you may know, there are different dependency structures assumed by different theories and and copes approaches. So for example, in universal dependencies, the structure of the coordinate, coordination Lisa, Bart, and Maggie.</sample>
    <sample id="1354">is such that the first conjunct is the head of the whole coordinate structure. So, in this case, Lisa,</sample>
    <sample id="1355">similar approach is assumed in Igor Milchuk's meaning text theory, where again the whole coordinate structure is headed by the first conjunct. So these two approaches are asymmetric. Right? They uh they single out one of the conjuncts.</sample>
    <sample id="1356">Now, there are also symmetric approaches to coordinate structures such as the Prague approach, the conjunction-headed approaches, here in Prague dependency tree banks, where coordinate structures are headed by the conjunction.</sample>
    <sample id="1357">So, uh, we get um um dependencies from end to all the conjunct.</sample>
    <sample id="1358">And finally, there's also a multi-headed approach that's used for example in the Cutson's word grammar.</sample>
    <sample id="1359">where so to say, all conjuncts are heads of the coordinate structure. So we get dependencies from the governor here loves to all conjuncts separately. Lisa, Bart and Maggie.</sample>
    <sample id="1360">Now, uh the aim of this paper is to um to produce a novel argument for uh the symmetric structures of coordination like these two, and against the asymmetric structures of coordination like these.</sample>
    <sample id="1361">Okay, the argument is based on the principle of dependency length minimization that I will explain on the basis of these examples.</sample>
    <sample id="1362">So, in English, as you might as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away, right? So, much read it yesterday is fine, because the direct object "it" is close to the verb.</sample>
    <sample id="1363">while Marg read yesterday it is much worse, right? Because uh here between the verb and the direct object there's an adjunct yesterday.</sample>
    <sample id="1364">However, this effect may be ameliorated, uh, when, um, uh, when the, um, direct object is very heavy and very long. Uh, because then it can be moved to the position after the agent,</sample>
    <sample id="1365">this is illustrated here. So, both these sentences are fine. "Marge read this absolutely fascinating book about the bees yesterday" is okay, where instead of "it" we have this long NP."this is illustrated here. So, both these sentences are fine. "Marge read this absolutely fascinating book about the bees yesterday" is okay, where instead of "it" we have this long NP."</sample>
    <sample id="1366">But it's also okay to say Marge read yesterday this absolutely fascinating book about bees.</sample>
    <sample id="1367">So, the reasoning here is that uh this is possible uh because even though this sentence violates the general grammatical principle that direct objects should be next to the verb,</sample>
    <sample id="1368">It satisfies the principle of dependency length minimization, which says that shorter um uh shorter dependencies are preferred.</sample>
    <sample id="1369">So, um, these two, um, uh, trees, uh, only show, uh, the length of the crucial dependencies, so the ones that are not constant among these two structures.</sample>
    <sample id="1370">So here we have a dependency from red to the adjunct of length seven, measured in words, and from red to book of length four. So together it's 11.</sample>
    <sample id="1371">When you move, when you swap uh these two constituents, the sum of these two dependencies becomes six, right? So instead of 11, six, much shorter, that's why this sounds quite okay, right? It violates one principle, but it satisfies the other.</sample>
    <sample id="1372">Okay. Uh, so what we did, we extracted various statistics from uh, about coordination from the enhanced version of Penn of the Penn Treebank, and see the paper why we didn't use uh, university dependencies.</sample>
    <sample id="1373">And these statistics confirmed the observation made many times before that left conjuncts tend to be shorter. Uh so salt and pepper and not pepper and salt measured in syllables.</sample>
    <sample id="1374">and also the observation that was made in passing that this tendency grows with length length difference.</sample>
    <sample id="1375">So, uh when the difference between the lengths of the two conjuncts uh grows, uh the shorter conjunct prefers to be the first one stronger, right? So the proportion is is is bigger of of the left, uh short conjunct.</sample>
    <sample id="1376">But what's novel uh in in this paper is uh we are that we observed that this tendency only occurs when the governor is on the left or absent</sample>
    <sample id="1377">Right, so the governor is on the left in this example, I saw Bart and Lisa. So it's the governor is on the left.</sample>
    <sample id="1378">it's absent in the second example, Homer came and sneezed. Here we have coordination of two verbs and there's no outside external governor, right? So, in such cases, the left conjunct prefers to be shorter, the more so the, uh, the bigger the difference between the two conjuncts.</sample>
    <sample id="1379">however, when uh the governor is on the right, as here, laughed governed the coordination Ted and Ned, uh this effect disappears.</sample>
    <sample id="1380">So we show that um uh by measuring length in characters, that's the first column, in syllables, the middle column, and in words, the right column. So I'll concentrate on the right one.</sample>
    <sample id="1381">What we see here is that when the governor is on the left,</sample>
    <sample id="1382">the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words. And the same is observed when there is no governor, as in coordination of sentences, but when the governor is on the right, this tendency disappears.</sample>
    <sample id="1383">And we show in the paper how this provides an argument against asymmetric structures of coordination, as these two, and for the symmetric structures as these</sample>
    <sample id="1384">So see the paper for the full agreement and arguments sorry and talk to us about uh at the poster session. Thank you.</sample>
    <sample id="1385">The speaker's name is Matthias Lindemann.</sample>
    <sample id="1386">Cross-lingual transfer involves training a model on one source language and then transferring that learned knowledge to another language.</sample>
    <sample id="1387">The authors are affiliated with Saarland University, Amazon Alexa, and the University of Vienna.</sample>
    <sample id="1388">The authors use average lagging (AL) and computational aware average lagging (AL_CA) as latency measures.</sample>
    <sample id="1416">Trees are usually not provided and must be obtained, which can be a complicated and computationally expensive process. This often requires formalism-specific pre-processing of logical forms and/or specialized grammar induction procedures.</sample>
    <sample id="1417">The authors are affiliated with the School of Interactive Computing, Georgia Institute of Technology.</sample>
    <sample id="1495">ABC-Eval stands for Annotating Behaviors in Chat.</sample>
    <sample id="1496">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until 2014.</sample>
    <sample id="1527">The authors are affiliated with The University of Edinburgh, Saarland University, and the University of Amsterdam. The UKRI Centre for Doctoral Training (NLP) is also mentioned.</sample>
    <sample id="1528">Si Yu Yuan</sample>
    <sample id="1529">There are 5 authors involved in the paper: Patrick Fernandes, Kayo Yin, Emmy Liu, AndrÃ© F. T. Martins, and Graham Neubig.</sample>
    <sample id="1530">The approach is compared to CAAT, a state-of-the-art architecture specifically tailored for SimulST.</sample>
    <sample id="1531">Hello everyone. My name is Ian, and my colleague Ziyang and I will be presenting our research on multi-instruct, improving multi-model zero-shot learning via instruction tuning.</sample>
    <sample id="1532">So, with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and a data efficient way.</sample>
    <sample id="1533">Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions.</sample>
    <sample id="1534">However, most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks, while computer vision and multimodal tasks have been left out.</sample>
    <sample id="1535">Therefore, in this work, we want to investigate whether instruction tuning on multimodal pre-trained models can actually improve generalization to unseen multimodal tasks.</sample>
    <sample id="1536">additionally, at the time of our research, we discovered a considerable discrepancy in availability of instruction dataset between NLP and multimodal.</sample>
    <sample id="1537">There exists more than 1,600 language-only instruction tasks. However, there is no large-scale, publicly-available multimodal instruction task. Therefore, this motivated us to build a multimodal instruction tuning dataset.</sample>
    <sample id="1538">Here, we present Multi-Instruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks covering 10 broad categories.</sample>
    <sample id="1539">These tasks are derived from 21 existing open-source dataset, and each task is equipped with five expert-written instructions.</sample>
    <sample id="1540">For investigating multimodal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pretrained model as our base model. OFA used a unified vocabulary for language, image tokens, and the coordinate of a bounding box.</sample>
    <sample id="1541">Here, we show some example instances from our multi-instructed.</sample>
    <sample id="1542">To unify the processing of a various input and output tasks.</sample>
    <sample id="1543">We follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format, in which the input text, images, instruction and bounding boxes are represented in the same token space.</sample>
    <sample id="1544">Okay, now I'm going to talk about multimodal instruction tuning.</sample>
    <sample id="1545">So, for the training dataset, we use 53 tasks from nine group for training, and we sample 10,000 instance per task. Uh, for testing, we reserve the entire commonsense reasoning group for testing, and we select additional five tasks from VQA and miscellaneous groups.</sample>
    <sample id="1546">we use all the instance in the test split for each task. In addition, we randomly sample 20 tasks from the test split of Natural Instruction as unseen tasks for NLP.</sample>
    <sample id="1547">So we use a pre-trained OFA large model as a base model. Uh during training, we mix all the instances for all the tasks. Uh each instance is randomly combined with one of its five instruction template.</sample>
    <sample id="1548">So during test, for each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1549">We report the mean and max performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1550">If the task is a multi-modal classification task, we report accuracy. If it's a multi-modal generation task, we report Rouge-L. And for NLP task, we report Rouge-L as well.</sample>
    <sample id="1551">We also introduced a additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task, regardless of slight variation in the wording of the instruction.</sample>
    <sample id="1552">Here is our main results. As we can see, uh, instruction tuning can significantly improve OS OFS performance on unseen multimodal tasks.</sample>
    <sample id="1553">also transfer learning from natural instruction data set can benefits uh instruction tuning.</sample>
    <sample id="1554">here we can see as the amount of task increase the model achieve better performance and in the meantime lower sensitivity.</sample>
    <sample id="1555">So we also did one experiments. Uh we use one instruction versus five instruction. As we can see, uh using more instruction can improve the model's overall performance and reduce its sensitivity a lot.</sample>
    <sample id="1556">So this shows the effect of different fine-tuning strategy on the model sensitivity. As we can see, by transfer learning from natural instruction dataset, the model can achieve much better sensitivity compared to the original I A.</sample>
    <sample id="1557">We also can see transfer learning from natural instruction data set can help OFA to achieve much better performance on the natural instruct data set.</sample>
    <sample id="1558">So overall, we have proposed the first large-scale multimodal instruction tuning dataset with significantly improved their capability of OFA, and we explore different transfer learning techniques and show their benefits. Uh we design a new metric called sensitivity.</sample>
    <sample id="1559">one more thing. We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon. Uh, this is a QR code for our data and the model. Thank you.</sample>
  </task>
</testset>