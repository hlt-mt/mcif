<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="it">
    <sample id="266">Troviamo anche un ulteriore, uhm, maggiore allineamento con persone che hanno un'istruzione universitaria. Quindi, per GPT-4, nel compito di accettabilit√† sociale, riscontriamo che √® pi√π allineato con persone con un'istruzione universitaria o di specializzazione.</sample>
    <sample id="267">E troviamo lo stesso per DynaHate, dove √® pi√π allineato alle persone con un'istruzione universitaria.</sample>
    <sample id="268">Ecco la traduzione in italiano del contenuto inglese:

"Scoperta 2:
Alcune popolazioni vengono lasciate indietro."</sample>
    <sample id="269">Un esempio di ci√≤ √® che i dataset e i modelli sono meno allineati alle persone non binarie rispetto alle loro controparti maschili e femminili. Lo riscontriamo nel task di accettabilit√† sociale di GPT-4, cos√¨ come nell'analisi del task Dynahate.</sample>
    <sample id="270">Allora, cosa possiamo fare?
Affrontare la posizionalit√† in NLP</sample>
    <sample id="271">Ecco il contenuto tradotto in italiano:

**Raccomandazioni**

1.  Tenere un registro di tutte le decisioni di progettazione pertinenti prese durante la creazione di set di dati o modelli.
2.  Condurre ricerca NLP attraverso la lente del prospettivismo:
    a. Condividere le etichette dei set di dati disaggregati!
    b. Utilizzare tecniche di modellazione in grado di gestire il disaccordo tra gli annotatori.

[1] https://www.masakhane.io</sample>
    <sample id="272">Ecco il contenuto tradotto in italiano:

**Raccomandazioni**

1.  Tenere un registro di tutte le scelte di progettazione pertinenti fatte durante la costruzione di dataset o modelli.
2.  Condurre la ricerca NLP attraverso la lente del prospettivismo:
    a. Condividere etichette di dataset disaggregate!
    b. Utilizzare tecniche di modellazione in grado di gestire il disaccordo degli annotatori.
3.  Costruire dataset e modelli specializzati con e per specifiche comunit√† √® prezioso per un NLP inclusivo (ad esempio, l'iniziativa Masakhane¬π).

[1] https://www.masakhane.io</sample>
    <sample id="273">La presentatrice conclude la presentazione dicendo: "E cos√¨, questa conclude la nostra presentazione. Ma se desiderate saperne di pi√π, non esitate a consultare la nostra dashboard per i risultati di analisi pi√π aggiornati e il nostro articolo. Grazie."

**Descrizione del contenuto visivo della slide:**

La slide √® intitolata "Thanks!" (Grazie!) in caratteri grandi e neri in alto a sinistra. Sotto il titolo, sono presenti due link importanti:
*   **Dashboard Link:** `nlpositionality.cs.washington.edu/`
*   **Paper:** `bit.ly/NLPositionality-Paper/`

Al centro della parte superiore della slide, c'√® un logo stilizzato (che assomiglia a tre persone connesse) con la scritta "Delphi" e un URL: `https://delphi.allenai.org/`.

Sulla destra dello schermo, in alto, si vede la presentatrice: una donna con lunghi capelli scuri che sorride. Indossa una maglietta bianca e un cardigan scuro. Si trova in un ambiente interno, con alcuni scaffali e oggetti sullo sfondo.

La parte inferiore della slide √® dominata da otto grafici a barre, organizzati in due file da quattro. Ogni grafico mostra la distribuzione di diverse caratteristiche demografiche all'interno di un campione di dati, con l'asse Y che indica il "No. of Samples (N)" (Numero di Campioni). Vicino ad alcune barre sono indicati anche dei valori di "Correlation" (Correlazione).

I grafici rappresentano:
1.  **Age (Et√†):** Distribuzione per fasce d'et√† (es. 18-24, 25-34, ecc.).
2.  **Gender (Genere):** Distribuzione per genere (Male, Female, Other, Prefer not to say, Prefer to self-describe).
3.  **Ethnicities (Etnie):** Distribuzione per etnie (es. White, Asian, Black/African American, Hispanic/Latino, Middle Eastern/North African, Native American/Alaska Native, Native Hawaiian/Pacific Islander, Two or More Races, Other).
4.  **Religion (Religione):** Distribuzione per affiliazione religiosa (es. Atheist/Agnostic, Christian, Muslim, Hindu, Buddhist, Jewish, Sikh, No religion, Other, Prefer not to say).
5.  **Education Level (Livello di Istruzione):** Distribuzione per livello di istruzione (es. High school, Bachelor's degree, Master's degree, Doctoral degree, Some college, Associate degree, Other).
6.  **Country (Residence) (Paese di Residenza):** Distribuzione per continente/regione di residenza (es. North America (U.S., Canada), Europe, Asia, South America, Africa, Other, Oceania, Central America).
7.  **Country (Longest) (Paese di Permanenza pi√π Lunga):** Simile al precedente, ma per il paese in cui si √® vissuto pi√π a lungo.
8.  **Native Language (Lingua Madre):** Distribuzione per lingua madre (es. English, Spanish, Chinese, Hindi, Arabic, French, German, Japanese, Korean, Other, Bengali, Portuguese).</sample>
    <sample id="274">La relatrice menziona tre problemi associati a SimulST:
1.  Specifiche architetture vengono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare.
2.  Procedure di addestramento lunghe e complicate (ad esempio, diversi obiettivi di ottimizzazione).
3.  Addestramento e mantenimento di diversi modelli per raggiungere regimi di latenza differenti (ad esempio, 1s, 2s, ...).</sample>
    <sample id="275">Il contenuto non offre un modo specifico ed efficace per mitigare i bias. Al contrario, evidenzia il dilemma ("sanitizzare o non sanitizzare") di affrontare i bias politici e sociali nei dati di pre-addestramento. Non sanitizzare porta alla propagazione dei bias e a problemi di equit√†, ma sanitizzare comporta rischi di censura, esclusione ed √® estremamente difficile determinare cosa sia realmente neutrale.</sample>
    <sample id="276">Ciao, sono Siyu Yuan dell'Universit√† di Fudan. Sono qui per presentare il nostro lavoro: "Distilling Script Knowledge from Large Language Models for Constrained Language Planning".</sample>
    <sample id="277">Ecco il contenuto inglese tradotto in italiano:

**Pianificazione del Linguaggio**

**Come fare una torta?**
1.  Raccogli i tuoi ingredienti.
2.  Preriscalda il forno a 325 ¬∞F (163 ¬∞C) e imburra e infarina una teglia per torta.
3.  Monta a crema il burro e lo zucchero.
4.  Aggiungi le uova.
5.  Incorpora la farina per dolci.
6.  Versa l'impasto nella teglia.
7.  Cuoci la torta per 1 ora e 15 minuti.

I modelli linguistici di grandi dimensioni (LLM) possono scomporre efficacemente gli obiettivi in passaggi.

**Voce:**
Nella vita di tutti i giorni, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo sotto forma di script di messa a terra.</sample>
    <sample id="278">Ricerche precedenti hanno esplorato i modelli linguistici per pianificare obiettivi astratti di attivit√† stereotipate, come fare una torta. E dimostrano che i modelli linguistici di grandi dimensioni (LLM) possono scomporre efficacemente gli obiettivi in passaggi.</sample>
    <sample id="279">Ecco il contenuto del testo in inglese tradotto in italiano:

**Contenuto della slide:**

*   **Titolo:** Pianificazione del Linguaggio Vincolata
*   **Sezione Torta alle Fragole:**
    *   Come fare una Torta alle Fragole?
    *   ...Aggiungere marmellate di fragole nella farina...
*   **Sezione Torta al Cioccolato:**
    *   Come fare una Torta al Cioccolato?
    *   ...Aggiungere il cacao in polvere nella farina...
*   **Testo in basso:** L'obiettivo astratto pu√≤ essere ereditato da diversi obiettivi specifici della vita reale con vincoli sfaccettati.

**Contenuto parlato (dal tuo transcript):**

*   Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per gli obiettivi astratti di attivit√† stereotipate. La pianificazione per gli obiettivi con scopi specifici, vincoli specifici, come fare una torta al cioccolato, rimane ancora non studiata.</sample>
    <sample id="280">In questo documento, definiamo il problema della pianificazione linguistica vincolata.</sample>
    <sample id="281">Ecco il contenuto tradotto in italiano:

**Pianificazione del Linguaggio Vincolata**

Come Preparare una Torta alle Fragole?
...Aggiungere la marmellata di fragole nella farina...

Come Preparare una Torta al Cioccolato?
...Aggiungere il cacao in polvere nella farina...

Un obiettivo astratto pu√≤ essere ereditato da diversi obiettivi specifici della vita reale con vincoli sfaccettati.</sample>
    <sample id="282">Ecco il contenuto tradotto in italiano:

**Testo nell'immagine:**

**Titolo:** Come si comportano gli LLM nella pianificazione linguistica vincolata?
**Sottotitolo:** Dataset: wikiHow + Vincoli Generati

**Tabella:**

*   **Tipo di Vincolo 1: Modificatore**
    *   Definizione: Una parola, un aggettivo o una frase che modifica o vincola un obiettivo astratto.
    *   Es.1: Fare una torta al cioccolato.
    *   Es.2: Fare una torta rosa.
*   **Tipo di Vincolo 2: Metodo**
    *   Definizione: Uno strumento o una modalit√† specificata che controlla il processo per raggiungere l'obiettivo.
    *   Es.1: Fare una torta con un forno.
    *   Es.2: Fare una torta usando un preparato per torte.
*   **Tipo di Vincolo 3: Intento**
    *   Definizione: Uno scopo o una richiesta aggiuntiva nel completare l'obiettivo.
    *   Es.1: Fare una torta per un matrimonio.
    *   Es.2: Fare una torta per diabetici.

**Contenuto audio:**

"In questo articolo, per prima cosa valutiamo e miglioriamo la capacit√† di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni."</sample>
    <sample id="283">Questo set di dati nullo di obiettivi specifici esiste per supportare il nostro studio.</sample>
    <sample id="284">Dobbiamo prima acquisire questi obiettivi. E, come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli sfaccettati per l'acquisizione dei dati con intervento umano utilizzando InstructGPT.</sample>
    <sample id="285">Ecco il contenuto tradotto in italiano:

**Possono gli LLM fare Pianificazione Linguistica Vincolata?**

(Grafico a barre)
Asse Y: **Accuratezza**
Legenda:
*   T5 (11B)
*   Flan-T5 (11B)
*   GPT-3 (175B)
*   Codex (175B)
*   InstructGPT (175B)

**Tutte le baseline ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici.**</sample>
    <sample id="286">Ecco la traduzione in italiano del contenuto:

**Testo sulla diapositiva (titolo):**
"Gli LLM possono fare Pianificazione Linguistica Vincolata?"

**Testo parlato:**
"Questa tabella riporta l'accuratezza complessiva dei risultati. Abbiamo riscontrato che tutti i modelli linguistici ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici."

**Testo sulla diapositiva (in basso):**
"Tutte le baseline ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici."</sample>
    <sample id="287">Ecco la traduzione in italiano del contenuto:

**Testo sull'immagine:**

*   **Titolo:** Che tipi di errori commettono di solito i modelli LLM in questo compito?
*   **Etichette del diagramma a radar:**
    *   FE1: Nessun vincolo
    *   FE2: Passi non correlati
    *   FE3: Passi incoerenti
    *   SE3: Ordine sbagliato
    *   SE2: Passi ripetuti
    *   SE1: Passi mancanti
*   **Legenda (modelli):**
    *   InstructGPT (6.7B)
    *   InstructGPT (1.3B)
    *   InstructGPT (175B)
    *   InstructGPT (175B) + passo
    *   InstructGPT (175B) + il nostro
*   **Testo in basso:** La completezza semantica (SE) negli script generati √® accettabile, ma l'aderenza ai vincoli (FE) non pu√≤ essere garantita.

**Contenuto parlato:**

*   "Poi, conduciamo un'analisi dettagliata per investigare perch√© i modelli linguistici..."</sample>
    <sample id="288">Ecco il contenuto tradotto in italiano:

**Titolo della diapositiva:** Che tipi di errori commettono solitamente gli LLM in questa attivit√†?

**Legenda del grafico:**
*   **FE1:** Nessun vincolo
*   **FE2:** Passi non correlati
*   **FE3:** Passi incoerenti
*   **SE1:** Passi mancanti
*   **SE2:** Passi ripetuti
*   **SE3:** Ordine errato

**Legenda del modello (sotto il grafico):**
*   InstructGPT (6.7B)
*   InstructGPT (17.5B)
*   InstructGPT (17.5B) + step
*   InstructGPT (17.5B) + ours

**Testo sotto il grafico:**
La completezza semantica (SE) negli script generati √® accettabile, ma la fedelt√† ai vincoli (FE) non pu√≤ essere garantita.

**Audio parlato:**
"I risultati nella figura mostrano che la completezza semantica negli script generati √® accettabile, ma la fedelt√† ai vincoli non pu√≤ essere garantita."</sample>
    <sample id="289">Ecco il contenuto tradotto in italiano:

**Testo sullo schermo:**
**Titolo:** "In quali tipi di obiettivi InstructGPT falliscono tipicamente?"

**Etichette della mappa di calore:**
*   **Righe:**
    *   Lavoro
    *   Sport
    *   Relazioni
    *   Cura Personale
    *   Casa
    *   Tradizioni
    *   Hobby
    *   Salute
    *   Cibo
    *   Affari
    *   Vita Familiare
    *   Istruzione
    *   Elettronica
    *   Veicoli
    *   Arte
*   **Colonne:** 6.7B, 13B, 175B, Step

**Testo in basso:** "La capacit√† di pianificazione degli InstructGPT varia considerevolmente per obiettivi di diverse categorie"

**Audio:**
"Ci addentriamo in categorie tematiche pi√π granulari di vincoli definiti su WikiHow. La mappa di calore nella figura mostra che la capacit√† di pianificazione degli InstructGPT varia considerevolmente per obiettivi di diverse categorie."</sample>
    <sample id="290">Studi precedenti hanno dimostrato che la qualit√† dell'output dei modelli linguistici presenta un'elevata varianza, portando a prestazioni scadenti. Pertanto, abbiamo adottato l'idea di sovra-generare e poi filtrare per migliorare la qualit√† della generazione.</sample>
    <sample id="291">Ecco il contenuto tradotto in italiano:

**Metodo**

**Input:** un obiettivo astratto

**Step 1**
Genera **obiettivi specifici** con InstructGPT tramite l'apprendimento in-context.

**Obiettivo Astratto:** Fare una torta
**+ vincoli**
**Obiettivi Specifici:**
G1(+modificatore): Fare una torta al cioccolato üç∞
G2(+metodo): Fare una torta nel microonde  microwaves
G3(+intento): Fare una torta per un matrimonio üíí</sample>
    <sample id="292">Poi, InstructGPT genera troppe chiavi per gli script per gli scopi specifici.</sample>
    <sample id="293">Ecco il contenuto tradotto in italiano:

**Metodo**

**Passo 2**
Generare in eccesso script candidati con InstructGPT tramite apprendimento in-context

**Passo 3**
Trovare gli script filtrati per l'obiettivo con InstructGPT tramite punteggio di similarit√†

**Output:** Obiettivi specifici con script corrispondenti

---

**Script Candidati**
(freccia: punteggio di similarit√†)

---

**Script Filtrati**
**Script 3**
1. Raccogli i tuoi ingredienti
4. Aggiungi il *cacao in polvere*</sample>
    <sample id="294">Convertiamo gli script e gli obiettivi in embedding di InstructGPT e calcoliamo la similarit√† del coseno come punteggi di similarit√† per misurare la similarit√† semantica.</sample>
    <sample id="295">Inoltre, assegniamo lo script che contiene le parole chiave del vincolo target. Conserviamo lo script solo se il punteggio dell'obiettivo target √® il pi√π alto.</sample>
    <sample id="296">Ecco il contenuto inglese tradotto in italiano:

**Titolo:** Il Nostro Metodo Migliora Notevolmente la Qualit√† della Pianificazione

**Grafico (etichette rilevanti):**
*   Accuratezza (sull'asse X)
*   Il Nostro Metodo (una delle barre)

**Didascalia:** Con il nostro metodo, InstructGPT pu√≤ generare script di qualit√† superiore con un ampio margine.</sample>
    <sample id="297">Ecco il contenuto tradotto in italiano:

**Distillazione di Script da LLM**

**Motivazione**
Per abilitare la capacit√† di pianificazione del linguaggio con vincoli per *modelli pi√π piccoli*.

**Metodo**
Seguiamo l'idea della *distillazione simbolica della conoscenza*.
Abbiamo generato 55.000 Script con vincoli da LLM basandoci sul nostro metodo =&gt; Dataset Coscript.
Gli umani annotano i set di validazione e di test.

---
**Input: un astratto**

**Fase 1**
Generare *obiettivi specifici* con InstructGPT tramite *apprendimento in-context*

**Fase 2**
Sovragenerare *script candidati* con InstructGPT tramite *apprendimento in-context*

**Fase 3**
Trovare *script filtrati* per l'obiettivo con InstructGPT tramite *punteggio di similarit√†*

**Output: Obiettivi specifici con piani corrispondenti**

---

**Audio trascritto e tradotto:**

"Since large language models are costly to deploy, it's essential to enable constrained language planning ability for smaller and specialized models. Creating a dataset is an essential step to this end."

"Poich√© i modelli linguistici di grandi dimensioni sono costosi da implementare, √® essenziale abilitare la capacit√† di pianificazione del linguaggio con vincoli per modelli pi√π piccoli e specializzati. La creazione di un dataset √® un passo essenziale a tal fine."</sample>
    <sample id="298">Ecco il contenuto tradotto in italiano:

**Titolo:** Distillazione di Script da LLM

**Motivazione**
*   Per abilitare la capacit√† di pianificazione linguistica vincolata per modelli pi√π piccoli.

**Metodo**
*   Seguire l'idea della distillazione di conoscenza simbolica.
*   Generati 55.000 script con vincoli da LLM basati sul nostro metodo =&gt; Dataset Coscript
*   Gli umani annotano i set di convalida e di test.

**Diagramma di flusso:**

**Input:** un astratto

**Step 1**
Genera obiettivi specifici con InstructGPT tramite apprendimento in-context.

**Step 2**
Sovra-genera script candidati con InstructGPT tramite apprendimento in-context.

**Step 3**
Trova script filtrati per l'obiettivo con InstructGPT tramite punteggio di similarit√†.

**Output:** Obiettivi specifici con piani corrispondenti

**Dalla trascrizione:**
"Tuttavia, studi precedenti non consentono la pianificazione per obiettivi specifici. E l'annotazione manuale di dataset √® costosa."</sample>
    <sample id="299">Ecco il contenuto del testo in italiano:

**Distillazione di Script da LLM**

**Motivazione**
Per abilitare la capacit√† di pianificazione linguistica vincolata per modelli pi√π piccoli.

**Metodo**
Seguiamo l'idea della distillazione di conoscenza simbolica.
Abbiamo generato 55.000 script con vincolo da LLM basandoci sul nostro metodo =&gt; Dataset Coscript.
Gli umani annotano il set di validazione e di test.

**Input:** un abstract

**Fase 1**
Generare obiettivi specifici con InstructGPT tramite apprendimento in-context

**Fase 2**
Sovra-generare script candidati con InstructGPT tramite apprendimento in-context

**Fase 3**
Individuare gli script filtrati per l'obiettivo con InstructGPT tramite punteggio di similarit√†

**Output:** Obiettivi specifici con piani corrispondenti</sample>
    <sample id="300">Ecco il contenuto tradotto in italiano:

**Distillazione di script da LLM**

**Motivazione**
Per abilitare la capacit√† di pianificazione del linguaggio vincolata per modelli pi√π piccoli.

**Metodo**
Seguiamo l'idea della distillazione simbolica della conoscenza.
Abbiamo generato 55.000 script con vincoli da LLM basandoci sul nostro metodo =&gt; Dataset Coscript.
Gli umani annotano il set di validazione e di test.

**Diagramma del processo:**

**Input:** un astratto

**Passo 1:**
Generare obiettivi specifici con InstructGPT tramite apprendimento in-context.

**Passo 2:**
Sovra-generare script candidati con InstructGPT tramite apprendimento in-context.

**Passo 3:**
Trovare script filtrati per l'obiettivo con InstructGPT tramite punteggio di similarit√†.

**Output:** Obiettivi specifici con piani corrispondenti.</sample>
    <sample id="301">Il contenuto del video in inglese, insieme alle trascrizioni del testo nella slide e a ci√≤ che viene detto dal presentatore, tradotto in italiano:

**Titolo della slide:** Distillazione di script da LLM

**Motivatione:**
Per abilitare la capacit√† di pianificazione del linguaggio vincolata per modelli pi√π piccoli.

**Metodo:**
*   Segui l'idea della distillazione di conoscenza simbolica
*   Generati 55.000 script con vincoli da LLM basati sul nostro metodo =&gt; Coscript Dataset
*   Gli esseri umani annotano il set di validazione e test.

**Diagramma di flusso:**
*   **Input:** un abstract
*   **Step 1:** Genera obiettivi specifici con InstructGPT tramite apprendimento in-context
*   **Step 2:** Sovra-genera script candidati con InstructGPT tramite apprendimento in-context
*   **Step 3:** Trova script filtrati per l'obiettivo con InstructGPT tramite punteggio di somiglianza
*   **Output:** Obiettivi specifici con piani corrispondenti

---
**Trascrizione del parlato del presentatore (e sua traduzione in italiano):**

**Inglese:** "In total, we generate 55,000 specific goals with scripts. To ensure the quality of validation and test sets, we ask crowd-sourced workers to find and revise the incoming incorrect samples."

**Italiano:** "In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualit√† dei set di validazione e test, chiediamo ai lavoratori crowdsourced di trovare e revisionare i campioni errati."</sample>
    <sample id="302">Questa figura mostra la distribuzione dei vincoli di Coscript.
Troviamo che Coscript mostra un alto pluralismo negli obiettivi specifici generati.
Con Coscript, possiamo addestrare modelli pi√π piccoli ma specializzati per la pianificazione del linguaggio vincolata.</sample>
    <sample id="303">Ecco il contenuto tradotto in italiano:

**Testo sullo schermo:**

**Titolo:** Modelli Specializzati vs. LLM

**Etichetta asse Y:** Accuratezza

**Legenda:**
*   GPT-3 (175B)
*   Codex (175B)
*   InstructGPT (175B)
*   T5 addestrato su wikiHow
*   T5 addestrato su Coscript

**Testo in basso:**
Gli LLM pi√π piccoli ottimizzati su Coscript possono generare script di qualit√† superiore rispetto agli LLM.

**Discorso della relatrice:**

"Abbiamo scoperto che il T5, ottimizzato su Coscript, pu√≤ generare script di qualit√† superiore rispetto alla maggior parte dei modelli linguistici di grandi dimensioni, indicando che i modelli pi√π piccoli possono superare i modelli pi√π grandi se addestrati correttamente su set di dati adeguati."</sample>
    <sample id="304">Ecco il contenuto tradotto in italiano:

**SOMMARIO e Punti Chiave**

*   Abbiamo definito il problema della pianificazione linguistica vincolata.
*   Valutiamo la capacit√† di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni (LLM).
*   E sviluppiamo un metodo "genera-e-poi-filtra" per gli LLM.
*   Usiamo gli LLM per generare un dataset di script di alta qualit√† (CoScript) per la pianificazione linguistica vincolata.

**Limitazioni e lavoro futuro**

*   Il metodo proposto per migliorare gli LLM √® un approccio di riclassificazione a posteriori (post-hoc re-ranking).
*   CoScript eredita solo da uno astratto con un vincolo aggiuntivo.
*   Il dataset CoScript pu√≤ essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica con obiettivi e vincoli pi√π complessi e diversi.</sample>
    <sample id="305">Ecco la traduzione del contenuto inglese in italiano:

**Sommario e Punti chiave**

*   definire il problema della pianificazione linguistica vincolata.
*   valutare la capacit√† di pianificazione linguistica vincolata dei LLM (Large Language Models - Grandi Modelli Linguistici).
*   e sviluppare un metodo di sovra-generazione e poi filtraggio per i LLM.
*   Utilizziamo grandi modelli linguistici per generare un dataset di script di alta qualit√† (CoScript) per la pianificazione linguistica vincolata.

**Limitazioni e lavoro futuro**

*   Il metodo proposto per migliorare i LLM √® un approccio di ri-ranking post-hoc.
*   CoScript eredita solo da uno astratto con un vincolo aggiuntivo.
*   Il dataset CoScript pu√≤ essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica con obiettivi e vincoli pi√π complessi e diversi.</sample>
    <sample id="306">**Testo sullo schermo:**

*   61¬∞ Incontro Annuale dell'Associazione per la Linguistica Computazionale
*   Toronto, Canada
*   10-14 Dicembre 2023
*   Distillare la Conoscenza dello Script da Grandi Modelli Linguistici per la Pianificazione Linguistica Vincolata
*   Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang
*   siyuan21@m.fudan.edu.cn
*   https://github.com/siyuyuan/coscript
*   Sito Web di Coscript

**Audio (Siyu Yuan):**

*   Grazie per il vostro tempo. Potete trovare maggiori dettagli su Coscript nel nostro articolo.</sample>
    <sample id="307">La fluidit√† di PaLM √® comparabile ai sistemi SOTA (State-of-the-Art).</sample>
    <sample id="308">Le propriet√† importanti di un metodo di filigrana (watermark) sono:

*   **Applicabilit√† a EaaS** (Embedding as a Service)
*   **Utilit√†:** non deve degradare l'utilit√† degli embedding forniti.
*   **Discrezione (Covertness):** deve essere nascosto all'attaccante.
*   **Trasferibilit√†:** deve essere trasferibile ai servizi degli attaccanti durante il processo di estrazione del modello.</sample>
    <sample id="309">Le 14 lingue in cui sono stati tradotti i discorsi TED dall'inglese sono:

1.  Arabo (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©)
2.  Tedesco (Deutsch)
3.  Spagnolo (Espa√±ol)
4.  Francese (Fran√ßais)
5.  Ebraico (◊¢◊ë◊®◊ô◊™)
6.  Italiano (Italiano)
7.  Giapponese (Êó•Êú¨Ë™û)
8.  Coreano (ÌïúÍµ≠Ïñ¥)
9.  Olandese (Nederlands)
10. Portoghese (Portugu√™s)
11. Rumeno (Rom√¢nƒÉ)
12. Russo (–†—É—Å—Å–∫–∏–π)
13. Turco (T√ºrk√ße)
14. Cinese (‰∏≠Êñá)</sample>
    <sample id="310">Vengono campionate 300 istanze da un set di dati per la riannotazione.</sample>
    <sample id="311">Le metriche di distanza utilizzate sono la **differenza di somiglianza del coseno (Œîcos)** e la **differenza di distanza L2 (Œîl2)**.</sample>
    <sample id="312">I modelli basati su codificatori multilingue sono stati utilizzati per la valutazione in un contesto monolingue. Sono state considerate due tipologie principali:

*   **Enc-PTR**: codificatori multilingue pre-addestrati con decoder basati su puntatori (es. XLM-R + PTR, mBERT + PTR).
*   **Enc-Dec**: modelli encoder-decoder multilingue pre-addestrati (es. mBART, mT5).</sample>
    <sample id="344">Gli autori contano la frequenza delle parole su un corpus di testo generale ($D_p$) e poi selezionano casualmente $n$ parole che rientrano in un intervallo di frequenza moderata.</sample>
    <sample id="345">Ciao a tutti. Mi chiamo Shuheng. Oggi presenter√≤ il nostro articolo: "I tagger di entit√† nominative CoNLL-2003 funzionano ancora bene nel 2023?". Iniziamo.</sample>
    <sample id="346">Il nostro paper ha indagato il problema della generalizzazione utilizzando il task di Riconoscimento delle Entit√† Nominate, o il task NER.</sample>
    <sample id="347">Ecco la traduzione in italiano del contenuto:

**Testo sull'immagine:**

**Riconoscimento delle Entit√† Nominate e Generalizzazione**
*   I modelli utilizzano CoNLL-2003 per sviluppare il NER da quasi 20 anni.
*   Questi modelli possono generalizzare ai dati moderni?

**Testo parlato:**

"Abbiamo osservato che i modelli utilizzano CoNLL-2003 per sviluppare il NER da quasi 20 anni. E questo solleva naturalmente diversi problemi. Innanzitutto, questi modelli possono generalizzare ai dati moderni?"</sample>
    <sample id="348">Ecco il contenuto tradotto in italiano:

**Riconoscimento di Entit√† Nominate e Generalizzazione**

*   I modelli utilizzano CoNLL-2003 per sviluppare il Riconoscimento di Entit√† Nominate (NER) da quasi 20 anni.
*   Questi modelli possono generalizzare a dati moderni?
*   Cosa √® necessario per una buona generalizzazione?</sample>
    <sample id="349">Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli?</sample>
    <sample id="350">Ecco la traduzione in italiano del contenuto:

**Testo visivo:**

**Dataset CoNLL++**

*   Notizie Reuters raccolte dal 2020 e annotate con le linee guida di annotazione CoNLL-2003

(L'esempio di annotazione rimane in inglese in quanto parte del dataset e dei suoi tag tecnici)
AMBASSADOR O
TO O
THE O
UNITED I-ORG
NATIONS I-ORG
: O
LINDA I-PER
THOMAS-GREENFIELD I-PER

**Testo parlato:**

"Per indagare su questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo √® un dataset che abbiamo raccolto dalle notizie Reuters del 2020 e poi le abbiamo annotate con le stesse linee guida di annotazione CoNLL 2003."</sample>
    <sample id="351">Abbiamo quindi ottimizzato oltre 20 modelli su CoNLL-2003. Li abbiamo valutati sia sul set di test CoNLL-2003 che sul set di test CoNLL++.</sample>
    <sample id="352">Ecco la traduzione del contenuto in inglese in italiano:

**Contenuto della slide:**

**Dataset CoNLL++**

*   Raccolte notizie Reuters dal 2020 e annotate secondo le linee guida di annotazione CoNLL-2003.
*   Oltre 20 modelli ottimizzati (fine-tuned) su CoNLL-2003.
*   Valutati sul set di test CoNLL-2003 e CoNLL++.
*   Calcolata la variazione percentuale ŒîF1 per valutare la generalizzazione.

*(La colonna di destra con l'esempio rimane invariata in quanto dati specifici)*
AMBASSADOR O
TO O
THE O
UNITED I-ORG
NATIONS I-ORG
: O
LINDA I-PER
THOMAS-GREENFIELD I-PER

**Audio:**

"E, ultimo ma non meno importante, abbiamo calcolato la variazione percentuale nell'F1 per valutare la generalizzazione di ciascun modello."</sample>
    <sample id="353">Allora, cosa √® necessario per una buona generalizzazione? Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari.</sample>
    <sample id="354">Il primo √® l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli Transformer normalmente generalizzano meglio a nuovi dati.</sample>
    <sample id="355">Il secondo ingrediente √® la dimensione del modello. Abbiamo scoperto che solitamente i modelli pi√π grandi portano a una migliore generalizzazione.</sample>
    <sample id="356">TRANSCRIPT:
And last but not least, we all know that the number of fine-tuning examples directly affects the performance of a downstream task. Here, we also found that more fine-tuning examples actually also leads to better generalization.
ITALIAN:
E, ultimo ma non meno importante, sappiamo tutti che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni di un task a valle. Qui abbiamo anche scoperto che un maggior numero di esempi di fine-tuning porta anche a una migliore generalizzazione.</sample>
    <sample id="357">Quali sono le cause del calo di prestazioni di alcuni modelli?</sample>
    <sample id="358">Ecco la traduzione:

**Testo sullo schermo:**
"Cosa Causa il Calo di Prestazioni?"
"- Overfitting adattivo?"
"Georgia Tech"

**Testo parlato:**
"Avevamo due ipotesi. La prima √® l'overfitting adattivo, che √® l'overfitting causato dal riutilizzo dello stesso set di test pi√π e pi√π volte, e questo si manifesta di solito come rendimenti decrescenti sul nuovo set di test."</sample>
    <sample id="359">La seconda ipotesi √® la deriva temporale, che √® il degrado delle prestazioni causato dal crescente divario temporale tra il training e il test.</sample>
    <sample id="360">Per l'overfitting adattativo, abbiamo visto che dal grafico a destra, la linea di migliore adattamento rossa ha una pendenza maggiore di uno.</sample>
    <sample id="361">Ecco la traduzione del contenuto in italiano:

**Testo sullo schermo:**
*   **Cosa causa il calo di prestazioni?**
*   **Overfitting adattivo?**
    *   Nessun rendimento decrescente
*   **Deriva temporale?**

**Contenuto parlato:**
"Questo significa che ogni unit√† di miglioramento che abbiamo apportato su CoNLL 2003 si traduce in pi√π di un'unit√† di miglioramento su CoNLL++. Il che significa che non ci sono rendimenti decrescenti."</sample>
    <sample id="362">Ecco il contenuto tradotto in italiano:

**Cosa Causa il Calo delle Prestazioni?**

*   Overfitting adattivo?
    *   Nessun rendimento decrescente
    *   Non osservato
*   Deriva temporale?

[Logo] Georgia Tech</sample>
    <sample id="363">Certo! Sono qui per aiutarti.

Posso tradurre il contenuto del video dall'inglese all'italiano. Per favore, forniscimi il testo in inglese (la trascrizione) o l'audio stesso, e provveder√≤ a tradurlo per te.</sample>
    <sample id="364">Ecco la traduzione italiana del contenuto:

**Cosa causa il calo delle prestazioni?**

*   Overfitting adattivo?
    *   Nessun rendimento decrescente
    *   Non osservato
*   Deriva temporale?
    *   Le prestazioni si degradano con un divario temporale maggiore</sample>
    <sample id="365">Ecco il contenuto tradotto in italiano:

**Quali sono le cause del calo di performance?**

*   **Overfitting adattivo?**
    *   Nessun rendimento decrescente
    *   Non osservato
*   **Deriva temporale?**
    *   Le prestazioni degradano con un maggiore divario temporale
    *   Causa principale del calo di performance

---

**Tabella:**

| Nome              | CoNLL-2003 | CoNLL++ | ŒîF1 (%) |
| :---------------- | :--------- | :------ | :------ |
| Flair             | 92.46      | 87.31   | -5.57   |
| Flair_RN          | 90.91      | 88.46   | -2.69   |
| Flair aggregato   | 93.15      | 88.82   | -4.65   |
| Flair_RN aggregato| 92.98      | 89.73   | **-3.50** |
| ELMo              | 92.36      | 87.08   | -5.72   |
| ELMo_RN           | 92.11      | 90.79   | **-1.43** |

---

**Grafico:**

*   **ŒîF1 (%)** (etichetta asse Y)
*   **Anno** (etichetta asse X)

---

**Logo:**
*   Georgia Tech</sample>
    <sample id="366">La nostra conclusione √® che, per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, di dimensioni del modello maggiori, nonch√© di pi√π esempi di fine-tuning. E questi obiettivi vanno di pari passo, non possiamo avere un solo ingrediente, ma √® necessario averli tutti.</sample>
    <sample id="367">Ecco la versione italiana del contenuto:

**Conclusione**

*   Per una buona generalizzazione, abbiamo bisogno di:
    *   Migliore architettura del modello
    *   Maggiore dimensione del modello
    *   Pi√π esempi di fine-tuning
*   Il calo delle prestazioni √® causato da:
    *   Deriva temporale
    *   Non da overfitting adattivo

**Testo parlato:**
"Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui √® causato dalla deriva temporale e, in modo piuttosto sorprendente, non √® causato dall'overfitting adattivo. Anche se CONLL 2003 √® stato utilizzato per oltre 20 anni."

**Etichette del grafico (tradotte solo se non nomi propri o acronimi):**
(I nomi dei modelli e le sigle come "Stanford NLP", "Illinois NER", "BILSTM-CNN-CRF", "Flair", "BERT-Large", "LUKE", "CoNLL-2003", "CoNLL++" e "Georgia Tech" rimangono invariati in quanto nomi propri o acronimi tecnici.)</sample>
    <sample id="368">Ecco la traduzione del contenuto in inglese in italiano:

**Conclusione**

* Per una buona generalizzazione, abbiamo bisogno di:
    * Migliore architettura del modello
    * Dimensione maggiore del modello
    * Pi√π esempi di fine-tuning
* Il calo di performance √® causato da:
    * Deriva temporale
    * Non un overfitting adattivo
* I tagger CoNLL-2003 funzionano ancora?
    * S√å!

---
*(Il grafico e i nomi specifici dei modelli nel grafico non sono testo da tradurre, ma elementi visivi o nomi propri. Il testo "Georgia Tech" e il logo rimangono invariati in quanto nomi propri.)*</sample>
    <sample id="369">Ecco la traduzione del contenuto in inglese:

**Titolo:**
Conclusione

**Contenuto della slide:**

*   Per una buona generalizzazione, abbiamo bisogno di:
    *   Migliore architettura del modello
    *   Maggiore dimensione del modello
    *   Pi√π esempi di fine-tuning
*   Il calo di prestazioni √® causato da:
    *   Deriva temporale
    *   Non √® overfitting adattivo
*   I tagger CoNLL-2003 funzionano ancora?
    *   S√å!

**Contenuto parlato (dal transcript):**

"Ci auguriamo che il nostro articolo stimoli ulteriori ricerche su come migliorare le generalizzazioni dei modelli."</sample>
    <sample id="370">Ecco la traduzione in italiano del contenuto:

**Italiano:**

E infine, assicuratevi di consultare il nostro articolo, il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille.

---

**Testo sullo schermo (tradotto):**

Articolo: https://arxiv.org/abs/2212.09747
Dataset: https://github.com/ShuhengL/acl2023_conllpp
Contatto: sliu775@gatech.edu</sample>
    <sample id="397">1 secondo.</sample>
    <sample id="398">Nell'esempio, la conoscenza specifica dell'entit√† necessaria √® che **Servin √® un giudice**.</sample>
    <sample id="399">La qualit√† dell'esempio √® pi√π importante della somiglianza con la frase sorgente.</sample>
    <sample id="400">L'articolo si concentra sui modelli linguistici **RoBERTa** e **GPT-2** negli esperimenti estesi.</sample>
    <sample id="401">Il contenuto inglese fornito non contiene informazioni sufficienti per rispondere a questa domanda. La trascrizione √® una conclusione della presentazione, che incoraggia a leggere il paper e menziona il rilascio di codice e modelli, ma non approfondisce i dettagli tecnici sul modello e i suoi punteggi di attenzione.</sample>
    <sample id="402">Gli esempi di riferimento diretto sono: "easy on me", "the first one".</sample>
    <sample id="403">Le affiliazioni degli autori sono Fudan University e Brain Technologies Inc.</sample>
    <sample id="404">Ci sono 7 autori.</sample>
    <sample id="405">No, il testo indica che "Translate-Test" √® una delle sei configurazioni considerate per la formazione e la valutazione, non che sia un approccio standard.</sample>
    <sample id="406">Gli autori hanno fornito l'esempio di "a woman warrior" come gruppo contrassegnato.</sample>
    <sample id="407">Il contenuto afferma che i modelli Transformer generalizzano meglio. Pertanto, i **modelli non-Transformer** (come quelli basati su LSTM o CNN/CRF) non generalizzano altrettanto adeguatamente.</sample>
    <sample id="408">I set di dati di test sono indicati come "clean validation samples" e sono suddivisi in sottoinsiemi di 5, 10, 20, 30, 40, 50, e "All Validation".</sample>
    <sample id="409">Ci sono sei autori coinvolti nell'articolo.</sample>
    <sample id="410">Pi√π modalit√†.</sample>
    <sample id="439">Based on the provided English content, there is no information about an understudied area of NLU. The text only states that "Successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pretrain-time and inference-time knowledge."</sample>
    <sample id="440">I relatori sono Ying Shen e Zhiyang Xu.</sample>
    <sample id="441">S√¨, i set di validazione e test di Coscript sono stati annotati e revisionati da operatori umani per garantirne la qualit√†.</sample>
    <sample id="442">Le risorse esistenti supportano fenomeni di discorso e lingue limitati.</sample>
    <sample id="443">Ecco la versione italiana del contenuto:

**Testo dalla slide:**

*   **Titolo:** Risoluzione di Espressioni Referenziali Indirette per la Selezione di Entit√† (Corpus AltEntities)
*   **Autori:** Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis
*   **Logo:** Google Research

**Testo parlato:**

"Ciao. Parler√≤ del nostro lavoro sulla risoluzione di espressioni referenziali indirette per la selezione di entit√†, nel quale introduciamo il corpus AltEntities."</sample>
    <sample id="444">Ecco la versione italiana del contenuto:

**Titolo:** Risoluzione delle Espressioni Referenziali Indirette per la Selezione di Entit√† (Corpus AltEntities)

**Autori:** Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis

**Affiliazione:** Google Research

**(Dalla voce dello speaker):** "Mi chiamo Javad Hosseini, e questo √® un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis."</sample>
    <sample id="445">Ecco la versione italiana del contenuto della slide:

**Espressioni di Riferimento Indiretto**

*   **Obiettivo:** Comprendere il linguaggio degli utenti quando fanno una scelta.

    *   **Domanda alternativa** ‚Üí "Volevi dire *easy on me* o *I gotta feeling*?"

*   **Riferimento diretto:**
    *   "easy on me"
    *   "il primo"

*   **Il riferimento indiretto potrebbe essere usato in una conversazione naturale e fluida:**
    *   Non ricordo il nome
    *   Le pronunce sono difficili da distinguere
    *   Voler specificare una preferenza

    *   **Riferimento indiretto** ‚Üí "Quello pi√π recente."
        *   "La canzone che non √® energica."

*   **[Pi√® di pagina]** Risolvere Espressioni di Riferimento Indiretto per la Selezione di Entit√† (Corpus AltEntities)</sample>
    <sample id="446">Ecco la versione italiana del contenuto della slide e del discorso:

---

**Titolo:** Espressioni di Riferimento Indiretto

*   **Obiettivo:** Comprendere il linguaggio degli utenti quando fanno una scelta

    *   [Domanda alternativa] ‚Üí Intendevi "easy on me" o "I gotta feeling"?

*   **Riferimento diretto:**
    *   "easy on me", "il primo"

*   **Il riferimento indiretto pu√≤ essere usato in una conversazione naturale e fluida:**
    *   Non riesco a ricordare il nome
    *   Le pronunce sono difficili da distinguere
    *   Voglio specificare una preferenza

    *   [Riferimento indiretto] ‚Üí Quello pi√π recente. La canzone che non √® energica.

---

**Trascrizione del discorso:**

"La cosa pi√π ovvia √® usare un riferimento diretto. Ad esempio, dicendo il nome della canzone "easy on me" o la sua posizione, la prima."</sample>
    <sample id="447">But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name.
Italian: Ma a volte un riferimento indiretto √® pi√π appropriato per avere una conversazione pi√π naturale. Questo potrebbe accadere quando l'utente non ricorda il nome.</sample>
    <sample id="448">Ecco la versione italiana del contenuto inglese:

**Espressioni di Riferimento Indiretto**

*   **Obiettivo:** Comprendere il linguaggio degli utenti quando fanno una scelta

*   **Domanda alternativa:** Intendevi "easy on me" o "I gotta feeling"?

*   **Riferimento diretto:**
    *   "easy on me", "il primo"

*   **Il riferimento indiretto potrebbe essere usato in una conversazione naturale e fluida:**
    *   Non ricorda il nome
    *   Le pronunce sono difficili da distinguere
    *   Vuole specificare una preferenza

*   **Riferimento indiretto:**
    *   Quello pi√π recente.
    *   La canzone che non √® energica.</sample>
    <sample id="449">Ecco la versione italiana del contenuto della slide:

---

**Espressioni di Riferimento Indiretto**

*   **Obiettivo:** Comprendere il linguaggio degli utenti quando fanno una scelta

    Domanda alternativa ‚Üí Intendevi "easy on me" o "I gotta feeling"?

*   **Riferimento diretto:**
    *   "easy on me", "il primo"
*   **Il riferimento indiretto pu√≤ essere usato in una conversazione naturale e fluida:**
    *   Non si ricorda il nome
    *   Le pronunce sono difficili da distinguere
    *   Vuole specificare una preferenza

    Riferimento indiretto ‚Üí Il pi√π recente. La canzone che non √® energica.

---
*(Nel footer)*
Risoluzione delle Espressioni di Riferimento Indiretto per la Selezione di Entit√† (Corpus AltEntities) P. 2</sample>
    <sample id="450">Questo √® un problema importante nei sistemi conversazionali, e anche per la valutazione comparativa della comprensione delle entit√† dei Modelli Linguistici di Grandi Dimensioni.

**Raccolta di Dataset**
*   **Problema importante**
    *   Sistemi conversazionali
    *   Valutazione comparativa della comprensione delle entit√† dei Modelli Linguistici di Grandi Dimensioni
*   Nessun dataset pubblico su larga scala disponibile
*   Raccogliamo un ampio dataset utilizzando l'annotazione collaborativa
*   **Tre domini:**</sample>
    <sample id="451">Non siamo a conoscenza di un dataset pubblico, un dataset pubblico su larga scala per il compito. Quindi ne raccogliamo uno usando l'annotazione di massa. Il nostro dataset copre tre diversi domini: musica, libri e</sample>
    <sample id="452">Ecco la versione italiana del contenuto inglese:

**Metodologia di Raccolta del Dataset**

*   La metodologia enfatizza l'*informalit√†* utilizzando un compito di completamento di fumetti.

**Fumetto 1:**
*   **Testo nel fumetto:** "Ti ricordi quella canzone che stavamo ascoltando ieri?"
*   **Didascalia:** "Imposta il contesto del dialogo [scelto da pochi prompt manuali per dominio]"

**Fumetto 2:**
*   **Testo nel fumetto:** "Intendi 'Easy on Me' o 'N'Gotta Feeling'?"
*   **Didascalia:** "La domanda alternativa"

**Fumetto 3:**
*   **Testo della freccia:** "Compilato dall'annotatore"
*   **Didascalia:** "Espressione che si riferisce a una delle entit√†"</sample>
    <sample id="453">Ecco la versione italiana del contenuto:

**Narrazione:**
Il fumetto ha tre nuvolette di dialogo. Nella prima nuvoletta, Bob dice: "Ricordi quella canzone che stavamo ascoltando ieri?". E con questo, Bob imposta il contesto del dialogo.

**Testo sulla slide:**

**Metodologia di Raccolta del Dataset**
*   La metodologia sottolinea l'informalit√† utilizzando un compito di completamento di fumetti.

**Fumetti:**
1.  "Ricordi quella canzone che stavamo ascoltando ieri?"
2.  "Intendi 'Easy on Me' o 'I Gotta Feeling'?"
3.  *(Vuoto)* "Compilata dall'annotatore" (freccia gialla)

**Etichette sotto i fumetti:**
*   Imposta il contesto del dialogo [scelto tra alcuni prompt manuali per dominio]
*   La domanda alternativa
*   Espressione che si riferisce a una delle entit√†

**Pi√® di pagina:**
Risoluzione di Espressioni di Riferimento Indirette per la Selezione di Entit√† (Corpus AllEntities)</sample>
    <sample id="454">Ecco la versione italiana del contenuto:

**Testo sullo schermo:**

*   **Titolo:** Metodologia di Raccolta del Dataset
*   **Punto elenco:** La metodologia enfatizza l'*informalit√†* utilizzando un *compito di completamento di fumetti*
*   **In basso a sinistra:** Imposta il contesto del dialogo [scelto tra pochi suggerimenti manuali per dominio]
*   **In basso al centro:** La domanda alternativa
*   **In basso a destra:** Espressione che si riferisce a una delle entit√†
*   **Riquadro giallo:** Compilato dall'annotatore
*   **Testo in fondo a sinistra:** Risoluzione di Espressioni di Riferimento Indirette per la Selezione di Entit√† (Corpus AllEntities)

**Voce dello speaker:**

"Nella seconda nuvoletta di dialogo, Alice dice: "Intendi 'Easy on Me' o 'I Gotta Feeling'?""</sample>
    <sample id="455">Ecco la traduzione del contenuto in italiano:

**Testo sulla slide:**

*   **Titolo principale:** Metodologia di Raccolta del Dataset
*   **Punto elenco:** La metodologia sottolinea l'*informalit√†* utilizzando un *compito di completamento di vignette*.
*   **Fumetto 1:** Ti ricordi quella canzone che stavamo ascoltando ieri?
*   **Fumetto 2:** Intendi 'Easy on Me' o 'I Gotta Feeling'?
*   **Testo freccia gialla:** Compilato dall'annotatore
*   **Sottotitolo Fumetto 1:** Imposta il contesto del dialogo [scelto da pochi prompt manuali per dominio]
*   **Sottotitolo Fumetto 2:** La domanda alternativa
*   **Sottotitolo Fumetto 3:** Espressione che si riferisce a una delle entit√†

**Parlato:**

"che √® la domanda alternativa. E nel terzo fumetto, Bob usa un riferimento indiretto per selezionare una di queste entit√†. Ad esempio, il nuovo"</sample>
    <sample id="456">Ecco la versione italiana:

**Audio:**
Forniamo automaticamente la prima e la seconda bolla di dialogo, ma la terza viene riempita dall'annotatore. Ehm, la prima bolla di dialogo √® scelta da alcuni prompt manuali per dominio.

**Testo sulla slide:**

**Titolo:** Metodologia di Raccolta dei Dati

**Punto elenco:** La metodologia enfatizza l'informalit√† utilizzando un compito di completamento di vignette.

**Testo nelle bolle di dialogo (da sinistra a destra):**
1.  Ti ricordi quella canzone che stavamo ascoltando ieri?
2.  Intendi 'Easy on Me' o 'N' Gotta Feeling'?
3.  (Bolla vuota con freccia verso il testo giallo)

**Testo nella casella gialla:**
Compilato dall'annotatore

**Testo sotto le bolle di dialogo (da sinistra a destra):**
1.  Imposta il contesto del dialogo [scelto da alcuni prompt manuali per dominio]
2.  La domanda alternativa
3.  Espressione che si riferisce a una delle entit√†

**Pi√® di pagina in basso a sinistra:**
Risoluzione di Espressioni di Riferimento Indirette per la Selezione di Entit√† (Corpus AltEntities)</sample>
    <sample id="457">Ecco la versione italiana del contenuto della diapositiva:

---

**Metodologia di Raccolta del Dataset**

*   La metodologia sottolinea l'*informalit√†* utilizzando un *compito di completamento di vignette*

---

**[Vignetta 1 - a sinistra]**
*   **Fumetto:** "Ti ricordi quella canzone che stavamo ascoltando ieri?"
*   **Sotto:** Imposta il contesto del dialogo [scelto tra alcune istruzioni manuali per dominio]

**[Vignetta 2 - al centro]**
*   **Fumetto:** "Intendi 'Easy on Me' o 'I Gotta Feeling'?"
*   **Sotto:** La domanda alternativa

**[Vignetta 3 - a destra]**
*   **Fumetto vuoto con freccia:** Compilato dall'annotatore
*   **Sotto:** Espressione che si riferisce a una delle entit√†

---

**In basso a sinistra:** Risoluzione delle Espressioni di Riferimento Indirette per la Selezione di Entit√† (Corpus AltEntities)
**In basso a destra:** P.4</sample>
    <sample id="458">Ecco la versione italiana del contenuto:

**Generare domande alternative =&gt; campionando coppie di entit√†**

(Dal discorso: Usiamo sempre un modello semplice: "Intendi A o B?")

**Intendi A o B?**

(Dal discorso: Dove A e B sono campionati da Wikipedia.)

(Freccia gialla in alto: Pi√π simile (solitamente pi√π difficile))

*   Voci con infobox simili su Wikipedia (stesso genere e/o artista)
    *   Intendi 'This Is It' o 'Man In The Mirror'?
*   Voci con descrizioni simili su Wikipedia
    *   Intendi 'Thinking of You' o 'Happy Anywhere'?
*   Voci con titoli simili
    *   Intendi The Return (memoria) o The Return (romanzo di Shatner)?
*   Uniforme (casualmente):
    *   Intendi 'You Could Be Mine' o 'The Way I Am'?</sample>
    <sample id="459">Ecco la versione italiana:

**Generare domande alternative =&gt; campionamento di coppie di entit√†**

**Intendi A o B?**

(Freccia gialla in alto: **Pi√π simili** (di solito pi√π difficile))

*   Elementi con **infobox simili** su Wikipedia (stesso genere e/o artista)
    Intendi 'This Is It' o 'Man In the Mirror'?
*   Elementi con **descrizioni simili** su Wikipedia
    Intendi 'Thinking of You' o 'Happy Anywhere'?
*   Elementi con **titoli simili**
    Intendi The Return (memoria) o The Return (romanzo di Shatner)?
*   **Uniforme a caso:**
    Intendi 'You Could Be Mine' o 'The Way I Am'?

---

**Contenuto parlato:**

"Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando saliamo nella lista, le entit√† diventano pi√π simili tra loro, ed √® di solito pi√π difficile effettuare la disambiguazione."</sample>
    <sample id="460">Ecco la versione italiana del contenuto inglese:

**Genera domande alternative =&gt; campionamento di coppie di entit√†**

Intendi A o B?

**Pi√π Simili (solitamente pi√π difficile)**
*   Elementi con infobox simili su Wikipedia (stesso genere e/o artista)
    Intendi 'This Is It' o 'Man in the Mirror'?
*   Elementi con descrizioni simili su Wikipedia
    Intendi 'Thinking of You' o 'Happy Anywhere'?
*   Elementi con titoli simili
    Intendi The Return (memoria) o The Return (romanzo di Shatner)?
*   Uniforme a caso:
    Intendi 'You Could Be Mine' o 'The Way I Am'?</sample>
    <sample id="461">Ecco la versione italiana del contenuto dell'immagine:

**Generare domande alternative =&gt; campionamento di coppie di entit√†**

**Intendi A o B?**

**Pi√π Simile**
**(solitamente pi√π difficile)**

*   Voci con **infobox simili** su Wikipedia (stesso genere e/o artista)
    Intendi 'This Is It' o 'Man in the Mirror'?
*   Voci con **descrizioni simili** su Wikipedia
    Intendi 'Thinking of You' o 'Happy Anywhere'?
*   Voci con **titoli simili**
    Intendi The Return (memorie) o The Return (romanzo di Shatner)?
*   **Uniforme** a caso:
    Intendi 'You Could Be Mine' o 'The Way I Am'?

Risoluzione di espressioni di riferimento indirette per la selezione di entit√† (Corpus AllEntities)
P 5</sample>
    <sample id="462">Ecco la versione italiana del testo nella slide:

**Generare domande alternative: campionamento di coppie di entit√†**

**Intendi A o B?**

**Pi√π Simili (solitamente pi√π difficile)**

*   Articoli con infobox simili su Wikipedia (stesso genere e/o artista)
    *   Intendi 'This Is It' o 'Man in the Mirror'?
*   Articoli con descrizioni simili su Wikipedia
    *   Intendi 'Thinking of You' o 'Happy Anywhere'?
*   Articoli con titoli simili:
    *   Intendi 'The Return (memoir)' o 'The Return (Shatner novel)'?
*   Uniforme: a caso
    *   Intendi 'You Could Be Mine' o 'The Way I Am'?

**Risoluzione di Espressioni di Riferimento Indiretto per la Selezione di Entit√† (Corpus AllEntities)**
**P. 5**</sample>
    <sample id="463">Quando mostriamo questa domanda alternativa agli annotatori, conoscono il nome di queste entit√†, ma non conoscono necessariamente l'entit√†.

---

**Contenuto completo tradotto della diapositiva:**

**Conoscenze di base (Musica)**

*   Link di ricerca Google per ogni canzone.
    *   **Easy on Me (di Adele)**
        *   Clicca qui per saperne di pi√π sulla canzone.
    *   **I Gotta Feeling (dei The Black Eyed Peas)**
        *   Clicca qui per saperne di pi√π sulla canzone.

*   Chiediamo agli annotatori di
    *   Ascoltare almeno una parte di ogni canzone
    *   Leggere informazioni su ogni canzone

Risoluzione di espressioni di riferimento indirette per la selezione di entit√† (Corpus AllEntities) P.6</sample>
    <sample id="464">Ecco la versione italiana del contenuto inglese:

**Conoscenze di base (Musica)**
* Link di ricerca Google per ogni canzone.

**Easy on Me**
**(di Adele)**
* Clicca qui per scoprire di pi√π sulla canzone.

**I Gotta Feeling**
**(dei The Black Eyed Peas)**
* Clicca qui per scoprire di pi√π sulla canzone.

* Chiediamo agli annotatori di
    * Ascoltare almeno una parte di ogni canzone
    * Leggere informazioni su ogni canzone</sample>
    <sample id="465">Ecco la versione italiana del contenuto:

**Testo visibile:**

**Conoscenze di base (Musica)**

*   Link di ricerca Google per ogni canzone.
    *   **Easy on Me (di Adele)**
        *   Clicca qui per saperne di pi√π sulla canzone.
    *   **I Gotta Feeling (dei The Black Eyed Peas)**
        *   Clicca qui per saperne di pi√π sulla canzone.

*   Chiediamo agli annotatori di:
    *   Ascoltare almeno un po' di ogni canzone
    *   Leggere informazioni su ogni canzone

**Pi√® di pagina:**
Risoluzione di Espressioni di Riferimento Indirette per la Selezione di Entit√† (Corpus AllEntities) | P. 6

**Trascrizione del parlato:**

0:00 E poi abbiamo chiesto agli annotatori di ascoltare almeno un po' di ogni canzone
0:04 e leggere informazioni su ogni canzone.
0:06 Ecco, per esempio,
0:08 il risultato della ricerca Google per la canzone Easy on Me.</sample>
    <sample id="466">Ecco la versione italiana del contenuto inglese:

**Contenuto della slide:**

*   **Titolo:** Conoscenze di base (Ricette)
*   **Torta Simnel:**
    *   **Testo:** La torta Simnel √® una torta di frutta ampiamente consumata nel Regno Unito, in Irlanda e in altri paesi con schemi di migrazione da essi, associata alla Quaresima e alla Pasqua. Si distingue per gli strati di pasta di mandorle o marzapane e un set di undici palline fatte con la stessa pasta.
*   **Torta Pandan:**
    *   **Testo:** La torta Pandan √® una torta soffice, leggera e di colore verde, aromatizzata con il succo delle foglie di Pandanus amaryllifolius. La torta √® popolare in Indonesia, Malesia e anche nei Paesi Bassi, specialmente nella comunit√† indonesiana.
*   **Pi√® di pagina:** Risoluzione di espressioni di riferimento indirette per la selezione di entit√† (Corpus AllEntities)
*   **Pagina:** P 8

**Parte parlata:**

"Per il dominio delle ricette e dei libri, mostriamo del testo di sottofondo da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono."</sample>
    <sample id="467">Ecco la versione italiana del contenuto della diapositiva e delle parole pronunciate:

**Contenuto della diapositiva in italiano:**

**Elicitare espressioni**

*   Poi diciamo agli annotatori quale scelta dovrebbe essere selezionata e chiediamo loro di descriverla.

**Scegli questa**
[Freccia punta a "Easy on Me"]

| Easy on Me (di Adele) | I Gotta Feeling (dei Black Eyed Peas) |
| :-------------------- | :----------------------------------- |

Vorremmo che ci forniste da 3 a 5 espressioni per la canzone scelta da inserire nella tua bolla di discorso. Ad esempio:

*   Quella con la musica di pianoforte
*   La canzone che non √® energica
*   Ha qualcosa su un fiume
*   Quella pi√π nuova
*   Riguarda il non avere tempo di scegliere

Risoluzione di espressioni di riferimento indiretto per la selezione di entit√† (Corpus AltEntities)

**Discorso dell'oratore in italiano:**

"Poi chiediamo agli annotatori di scegliere una di queste entit√†, ad esempio qui la prima, e di descriverla usando da 3 a 5 espressioni di riferimento indirette."</sample>
    <sample id="468">Ecco la versione italiana del contenuto inglese:

**Slide 1: Sollecitazione di espressioni**

*   Poi comunichiamo agli annotatori quale opzione dovrebbe essere selezionata e chiediamo loro di descriverla.
    *   **Seleziona questa**
    *   Easy on Me (di Adele) | I Gotta Feeling (dei Black Eyed Peas)

*   Vi chiediamo di fornirci da 3 a 5 espressioni per la canzone scelta da inserire nel vostro fumetto. Per esempio:
    *   Quella con la musica di pianoforte
    *   La canzone che non √® energica
    *   C'√® un riferimento a un fiume
    *   Quella pi√π nuova
    *   Riguarda il non avere tempo per scegliere

**Audio (0:00-0:02):**
"Per esempio, quella con il pianoforte."

**Slide 2: Esempi casuali**

*   **Selezione musicale**
    *   Intendi "Chime" o "Your Loving Arms"?
        *   $\Rightarrow$ Quella senza parole
    *   Intendi "These Kids" o "Inescapable"?
        *   $\Rightarrow$ √à la canzone cantata da un australiano.
    *   Intendi "Rock the Boat" o "Wherever You Are"?
        *   $\Rightarrow$ Contiene suoni di sintetizzatore
    *   Intendi "Telepathy" o "Stars on 45"?
        *   $\Rightarrow$ Uscita a met√† del 2000.
    *   Intendi "Mlis Shapes" o "Remind Me"?
        *   $\Rightarrow$ Basato sulla vita vissuta a Sheffield.

*   **Selezione di libri**
    *   Intendi "Warlock (romanzo di Hall)" o "Warlock (romanzo di Smith)"?
        *   $\Rightarrow$ Quello ambientato negli anni '80 dell'Ottocento
    *   Intendi "The Legion of Space" o "The Body in the Library"?
        *   $\Rightarrow$ √à di un famoso scrittore di gialli
    *   Intendi "The Good Soldier" o "The Good Soldiers"?
        *   $\Rightarrow$ Quello fittizio
    *   Intendi "The Giaour" o "The Giver"?
        *   $\Rightarrow$ non quello con il ragazzo di 12 anni
    *   Intendi "Broken Sleep" o "Broken Soup"?
        *   $\Rightarrow$ √à il libro che contiene rock e politica.

*   **Selezione di ricette**
    *   Intendi "Beurre Ma√Ætre d'H√¥tel" o "Chigirima"?
        *   $\Rightarrow$ viene dall'Azerbaigian
    *   Intendi "Kusa mochi" o "Uir≈ç"?
        *   $\Rightarrow$ La torta al vapore giapponese
    *   Intendi "Cannoli" o "Bocconotto"?
        *   $\Rightarrow$ Quelli mangiati a Natale
    *   Intendi "Johnnycake" o "Sagu"?
        *   $\Rightarrow$ la farina di mais √® l'ingrediente principale
    *   Intendi "Sagu" o "Moong Dal"?
        *   $\Rightarrow$ Non quello con la buccia marrone

**Audio (0:02-0:15):**
"Ecco alcuni esempi dal nostro set di dati. Ad esempio, quello senza parole, non quello con il ragazzo di 12 anni, il ragazzo di 12 anni, o quello fittizio, o viene dall'Azerbaigian e cos√¨ via."</sample>
    <sample id="469">Il corpus AltEntities contiene 6.000 domande alternative attraverso tre domini. E 42.000 espressioni di riferimento indirette.
I risultati con il modello T5 XL sono riassunti qui di seguito.

Ecco la versione italiana del contenuto della slide:

**AltEntities Corpus**

*   Circa 6.000 domande alternative attraverso i tre domini
*   Circa 42.000 espressioni di riferimento indirette
*   Risultati con il modello T5 XL (accuratezza):
    *   92-95% se il modello linguistico (LM) ha accesso alla stessa conoscenza di base degli annotatori.
    *   82-87% quando il LM ha accesso a una conoscenza di base parzialmente sovrapposta.
    *   Circa 60% quando il LM (T5 XL) ha accesso solo ai nomi delle entit√†.
    *   Abbiamo dimostrato che i modelli sono generalizzabili tra i domini.
*   Link al dataset: https://github.com/google-research-datasets/AltEntities

Footer: Risoluzione delle espressioni di riferimento indirette per la selezione di entit√† (Corpus AltEntities)
Pag. 11</sample>
    <sample id="470">Ecco la versione italiana del contenuto della diapositiva:

---

**Corpus AltEntities**

*   Circa 6.000 domande alternative nei tre domini
*   Circa 42.000 espressioni referenziali indirette
*   Risultati con il modello T5 XL (accuratezza):
    *   92-95% se il modello linguistico (LM) ha accesso alle stesse conoscenze di base degli annotatori.
    *   82%-87% quando l'LM ha accesso a conoscenze di base parzialmente sovrapposte.
    *   Circa 60% quando l'LM (T5 XL) ha accesso solo ai nomi delle entit√†.
    *   Abbiamo dimostrato che i modelli sono generalizzabili tra i domini.
*   Link al Dataset: https://github.com/google-research-datasets/AltEntities

---
Risoluzione di espressioni referenziali indirette per la selezione di entit√† (Corpus AltEntities) P. 11</sample>
    <sample id="471">Ecco la versione italiana del contenuto inglese:

**Contenuto della slide:**

**Corpus AltEntities**

*   Circa 6.000 domande alternative attraverso i tre domini
*   Circa 42.000 espressioni di riferimento indirette
*   Risultati con il modello T5 XL (accuratezza):
    *   92-95% se l'LM ha accesso alle stesse conoscenze di base degli annotatori.
    *   82%-87% quando l'LM ha accesso a conoscenze di base parzialmente sovrapposte.
    *   Circa 60% quando l'LM (T5 XL) ha accesso solo ai nomi delle entit√†.
    *   Abbiamo dimostrato che i modelli sono generalizzabili tra i domini.
*   Link al dataset: https://github.com/google-research-datasets/AltEntities

*Risoluzione delle Espressioni di Riferimento Indirette per la Selezione di Entit√† (Corpus AltEntities)*
*P. 11*

**Trascrizione del parlato:**

"Se il modello linguistico ha accesso ad alcune conoscenze di base parzialmente sovrapposte. Allora l'accuratezza √® tra l'82 e l'87%, il che √® pi√π realistico. Per esempio, quando il modello linguistico recupera le conoscenze di base."</sample>
    <sample id="472">Ecco la trascrizione e la traduzione in italiano del contenuto:

**Contenuto sullo schermo:**

**AltEntities Corpus**
* Circa 6.000 domande alternative attraverso i tre domini
* Circa 42.000 espressioni di riferimento indiretto
* Risultati con il modello T5 XL (accuratezza):
    * 92-95% se il ML (modello linguistico) ha accesso alle stesse conoscenze di base degli annotatori.
    * 82-87% quando il ML ha accesso a conoscenze di base parzialmente sovrapposte.
    * Circa 60% quando il ML (T5 XL) ha accesso solo ai nomi delle entit√†.
    * Abbiamo dimostrato che i modelli sono generalizzabili al dominio.
* Link al Dataset: https://github.com/google-research-datasets/AltEntities

**Pie di pagina:**
Risoluzione di espressioni di riferimento indiretto per la selezione di entit√† (Corpus AltEntities)
Pag. 11

---
**Voce fuori campo (traduzione):**

"se il modello linguistico ha accesso solo ai nomi delle entit√†, allora l'accuratezza √® solo del 60%. Quindi c'√® molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili al dominio. Qui c'√® un link al nostro dataset. Grazie."

---
**Seconda slide (traduzione):**

**Grazie!**
Se avete domande, scrivete a javadh@google.com</sample>
    <sample id="473">The approach is compared with the following existing SimulST policies:

*   **wait-k** strategy
*   **Local Agreement (LA)**
*   **CAAT**, which is a state-of-the-art architecture specifically tailored for SimulST.</sample>
    <sample id="474">Le affiliazioni degli autori sono: LIA (Avignon Universit√©), LS2N (Nantes Universit√©), Clinique des donn√©es (CHU de Nantes) e Zenidoc.</sample>
    <sample id="475">Jenny T. Liang</sample>
    <sample id="476">Ci sono 3 autori: Myra Cheng, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="477">Salve, sono Sara Papi dell'Universit√† di Trento e della Fondazione Bruno Kessler.

E presenter√≤ brevemente il nostro articolo intitolato "Attention as a Guide for Simultaneous Speech Translation", che √® un lavoro congiunto con Matteo Negri e Marco Turchi.</sample>
    <sample id="478">Ecco il contenuto tradotto in italiano:

**Cos'√® la traduzione vocale simultanea?**
La traduzione vocale simultanea (SimulST) √® il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse.</sample>
    <sample id="479">Ecco il contenuto tradotto in italiano:

**Slide 1:**
"Quali sono i problemi degli attuali modelli SimulST?"

**Slide 2 (dopo l'animazione):**
"Architetture specifiche vengono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare."</sample>
    <sample id="480">Ecco il contenuto in italiano:

**Quali sono i problemi degli attuali modelli SimulST?**

*   **Architetture specifiche vengono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare.**
*   **Procedure di addestramento lunghe e complicate (ad esempio, diversi obiettivi di ottimizzazione).**</sample>
    <sample id="481">Quali sono i problemi degli attuali modelli SimulST?

1.  Vengono solitamente addestrate architetture specifiche, introducendo moduli aggiuntivi da ottimizzare.
2.  Procedure di addestramento lunghe e complicate (ad es., diversi obiettivi di ottimizzazione).
3.  Addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza (ad es., 1s, 2s, ...).

...E addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, addestrare un modello con una latenza media di un secondo e un altro con una latenza di due secondi e cos√¨ via.</sample>
    <sample id="482">"Qual √® la nostra soluzione?"</sample>
    <sample id="483">Ecco il contenuto tradotto in italiano:

**Qual √® la nostra soluzione?**

01.  **Utilizzare modelli ST offline gi√† esistenti senza ri-addestrare o adottare architetture specifiche per SimulST.**
02.  **Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici.**</sample>
    <sample id="484">Ecco il contenuto tradotto e riassunto in italiano:

**Qual √® la nostra soluzione?**

1.  **Utilizzare modelli di traduzione vocale (ST) offline gi√† esistenti**, senza necessit√† di riaddestramento o di adottare un'architettura specifica per la traduzione simultanea (SimuIST).
2.  **Impiegare un unico modello** per ogni regime di latenza, gestendo la latenza attraverso parametri specifici.
3.  **Sfruttare la conoscenza gi√† acquisita dal modello** attraverso il meccanismo di attenzione (specificamente, il meccanismo di *cross-attention*) tra l'input audio e l'output testuale.

L'immagine a destra illustra un'onda sonora e una matrice di attenzione, mostrando l'allineamento tra l'input audio e l'output testuale, ad esempio la frase "I am a student" ("Sono uno studente").</sample>
    <sample id="485">Ecco il contenuto tradotto in italiano:

**Testo sullo schermo:**
*   **La nostra soluzione:** EDAtt
*   **Attenzione Encoder-Decoder**
*   Decidere se emettere o meno una traduzione parziale basandosi su dove punta l'attenzione: una parola viene emessa se l'attenzione non √® concentrata (la sua somma √® inferiore a una soglia c) verso gli ultimi Œª frame vocali, il che significa che l'informazione ricevuta √® sufficientemente stabile.

**Parlato:**
"La nostra soluzione √® proporre EDAtt, o Attenzione Encoder-Decoder. Ed √® una strategia per la quale decidiamo se emettere o meno una traduzione parziale basandoci su dove punta l'attenzione..."</sample>
    <sample id="486">Decidere se emettere o meno una traduzione parziale basandosi su dove punta l'attenzione:
una parola viene emessa se l'attenzione non √® concentrata (la sua somma √® inferiore a una soglia Œ±) verso gli ultimi Œª frame vocali, il che significa che l'informazione ricevuta √® sufficientemente stabile.</sample>
    <sample id="487">Ecco la traduzione in italiano del contenuto in inglese:

**La nostra soluzione:**
EDAtt

**Attenzione Encoder-Decoder**

Decidere se emettere o non emettere una traduzione parziale basandosi su dove puntano i punti di attenzione:
&gt; una parola viene emessa se l'attenzione non √® concentrata (la sua somma √® inferiore a una soglia Œ±) verso gli ultimi Œª frame vocali, il che significa che l'informazione ricevuta √® abbastanza **stabile**.

01 I am going to talk about... (Sto per parlare di...)</sample>
    <sample id="488">E guardiamo l'attenzione incrociata. La scelta √® se emettere o meno una traduzione parziale, basata su dove puntano i punti di attenzione. Una parola viene emessa se l'attenzione non √® concentrata, la sua somma √® al di sotto di una soglia, lambda, verso gli ultimi fotogrammi vocali. Ci√≤ significa che l'informazione ricevuta √® abbastanza stabile.</sample>
    <sample id="489">Ecco la traduzione del contenuto in inglese:

**Testo sulla slide:**

*   **La nostra soluzione: EDAtt**
*   **Attenzione Encoder-Decoder**
*   Decidere se emettere o meno una traduzione parziale in base a dove l'attenzione punta:
    *   una parola viene emessa se l'attenzione non √® concentrata (la sua somma √® inferiore a una soglia Œ±) verso gli ultimi Œª *frame* di parlato, il che significa che le informazioni ricevute sono sufficientemente stabili.

**Testo parlato:**

"Vedremo che le prime due parole puntano ai primi *frame* di parlato ricevuti, mentre l'ultima parola punta agli ultimi *frame* di parlato ricevuti, gli ultimi *lambda* *frame* di parlato."</sample>
    <sample id="490">La nostra soluzione: EDAtt

**Attenzione Encoder-Decoder**

Decidere se emettere o meno una traduzione parziale in base a dove punta l'attenzione: una parola viene emessa se l'attenzione non √® concentrata (la sua somma √® inferiore a una soglia Œ±) verso gli ultimi Œª frame vocali, il che significa che le informazioni ricevute sono sufficientemente **stabili**.

01 Parler√≤ di...
Ich werde reden.
EMESSE

pagina 019

Questo significa che le prime due parole saranno emesse.</sample>
    <sample id="491">La nostra soluzione: EDAtt
Attenzione Encoder-Decoder

Decidere se emettere o meno una traduzione parziale in base a dove punta l'attenzione:
* una parola viene emessa se l'attenzione non √® concentrata (la sua somma √® inferiore a una soglia Œ±) verso gli ultimi Œª frame vocali, il che significa che le informazioni ricevute sono sufficientemente stabili.</sample>
    <sample id="492">Ecco il contenuto tradotto e riassunto:

---

**Testo in italiano dall'immagine:**

*   **La nostra soluzione:** EDAtt
*   **Attenzione Encoder-Decoder**
*   Decidere se **emettere o meno una traduzione parziale** basandosi su dove punta l'attenzione:
    *   una parola viene emessa se l'attenzione non √® concentrata (la sua somma √® al di sotto di una soglia ùúè) verso gli ultimi ùúÜ fotogrammi di parlato, il che significa che le informazioni ricevute sono sufficientemente **stabili**.
*   **01** Sto per parlare di...
    *   (Sotto il diagramma di attenzione): Ich werde reden. **EMESSA**
    *   (Sotto il diagramma di attenzione): Ich werde
*   **02** Sto per parlare di clima.
    *   (Sotto il diagramma di attenzione): Ich werde
*   pagina 023

---

**Riassunto del contenuto parlato in inglese:**

La relatrice sta spiegando il funzionamento della loro soluzione "EDAtt". Descrive come, quando viene ricevuto un altro "chunk" (segmento) di parlato, il loro modello predice alcune parole (ad esempio, le tre parole mostrate nel diagramma). Successivamente, viene analizzata l'attenzione incrociata (cross-attention weights) per determinare se emettere o meno una traduzione parziale, basandosi su come l'attenzione si distribuisce sugli ultimi fotogrammi di parlato.</sample>
    <sample id="493">Certamente! Ecco la traduzione e la spiegazione del contenuto dell'immagine in italiano:

---

**La nostra soluzione: EDAtt**
**Attenzione Encoder-Decoder**

**Regola per l'Emissione di Traduzioni Parziali:**
Decidere se emettere o meno una traduzione parziale basandosi su dove punta l'attenzione:
*   Una parola viene emessa (come parte della traduzione parziale stabile) se l'attenzione non √® concentrata (la sua somma √® inferiore a una soglia Œ±) verso gli ultimi Œª frame vocali, il che significa che l'informazione ricevuta √® sufficientemente stabile.

---

**Spiegazione degli Esempi:**

**01: "I am going to talk about..." (Parler√≤ di...)**

*   **Input:** La frase inglese "I am going to talk about..." √® incompleta, suggerendo che l'utente stia ancora parlando o che l'audio sia interrotto.
*   **Traduzione parziale (tedesco):** "Ich werde reden." (Io parler√≤).
*   **Attenzione:** Le linee di attenzione rosse che collegano la parola tedesca "reden" (parlare) ai frame vocali mostrano una forte concentrazione sugli ultimi Œª frame del segnale audio.
*   **Decisione del sistema:** Poich√© l'attenzione √® concentrata sugli ultimi frame (indicando che il modello sta ancora cercando informazioni pertinenti dalla fine dell'input corrente), il sistema non considera la traduzione di "reden" come stabile. Per questo motivo, "reden" viene contrassegnata come **EMITTED** (OMESSA), il che implica che, sebbene sia stata prodotta dal decoder, non √® considerata sufficientemente stabile per far parte della traduzione parziale mostrata all'utente in quel momento.

**02: "I am going to talk about climate." (Parler√≤ di clima.)**

*   **Input:** La frase inglese "I am going to talk about climate." √® completa.
*   **Traduzione parziale (tedesco):** "Ich werde √ºber Klima sprechen." (Io parler√≤ di clima).
*   **Attenzione:** Le linee di attenzione verdi che collegano le parole tedesche (come "sprechen" - parlare) ai frame vocali sono distribuite su frame precedenti e non sono fortemente concentrate sugli ultimi Œª frame.
*   **Decisione del sistema:** Poich√© l'attenzione non √® concentrata sugli ultimi frame (e presumibilmente la somma √® superiore alla soglia Œ±, indicando una distribuzione pi√π stabile), il sistema √® fiducioso di aver ricevuto informazioni sufficientemente stabili per tradurre la parola "sprechen" (e le altre parole). Pertanto, la traduzione parziale "Ich werde √ºber Klima sprechen." viene emessa come stabile.

---

**In sintesi:**

Il sistema EDAtt (Encoder-Decoder Attention) utilizza l'attenzione per decidere la stabilit√† delle traduzioni parziali. Se l'attenzione del modello √® ancora fortemente focalizzata sulla parte finale dell'audio ricevuto (gli ultimi Œª frame), significa che non ha ancora abbastanza contesto stabile per la parola corrente, e quindi la parola non viene emessa nella traduzione parziale finale. Al contrario, se l'attenzione √® distribuita su parti dell'audio gi√† elaborate e non √® concentrata sulla fine, il sistema ritiene che la parola sia stata tradotta in modo stabile e la include nella traduzione parziale mostrata all'utente.</sample>
    <sample id="494">Questo slide presenta la soluzione proposta, chiamata **EDAtt (Encoder-Decoder Attention)**, per decidere quando emettere (o meno) una traduzione parziale in un sistema di traduzione vocale.

**Il Concetto Principale:**

Il sistema decide se emettere una parola o una frase tradotta parzialmente basandosi su dove punta l'attenzione del modello. Una parola viene emessa se l'attenzione del modello **non √® concentrata** (cio√® la sua somma √® inferiore a una soglia Œ±) verso gli ultimi **Œª (lambda)** frame vocali. Questo significa che l'informazione ricevuta fino a quel momento √® considerata **sufficientemente stabile** per prendere una decisione sulla traduzione e non c'√® bisogno di attendere ulteriori input dalla parte finale del parlato.

**Esempio 01: "I am going to talk about..." (Parler√≤ di...)**

*   Qui, la frase in inglese √® incompleta, indicata dai puntini di sospensione.
*   Il diagramma mostra l'onda sonora del parlato.
*   La traduzione parziale in tedesco √® "Ich werde reden." (Io parler√≤).
*   Le parole "Ich werde" (Io sar√≤/vado) sono mostrate in verde e le loro linee di attenzione (verdi) sono distribuite su diverse parti del parlato, ma non sono concentrate sugli "ultimi Œª frame vocali" (la sezione rossa tratteggiata alla fine dell'onda). Secondo la regola, questo indica che l'informazione per "Ich werde" √® stabile, e quindi viene **"EMESSA"** (tradotta e pronunciata).
*   Tuttavia, la parola "reden." (parlare/dire), mostrata in rosso, ha le sue linee di attenzione (rosse) fortemente concentrate sugli ultimi Œª frame vocali. Questo significa che l'informazione per "reden." non √® ancora considerata stabile; il sistema sta aspettando ulteriori input (il resto della frase) per poterla tradurre con sicurezza. Pertanto, "reden." non viene ancora emessa.
*   L'oratrice nel video afferma: "This means that these three words will be emitted." (Questo significa che queste tre parole verranno emesse). Qui sembra esserci una piccola discrepanza tra l'audio e l'illustrazione, poich√© l'illustrazione mostra chiaramente solo "Ich werde" come emesso, mentre "reden" √® in attesa. L'affermazione potrebbe riferirsi a una fase successiva o a un'aspettativa, ma in questo specifico momento del diagramma, "reden" non √® emesso.

**Esempio 02: "I am going to talk about climate." (Parler√≤ di clima.)**

*   In questo caso, la frase in inglese √® completa.
*   La traduzione in tedesco √® "Ich werde √ºber Klima sprechen." (Io parler√≤ di clima.).
*   Tutte le parole della traduzione (in verde) mostrano linee di attenzione (verdi) che sono distribuite sul parlato e **non sono concentrate** sugli ultimi Œª frame vocali.
*   Questo indica che per l'intera frase, l'informazione √® considerata stabile e sufficiente. Pertanto, l'intera traduzione "Ich werde √ºber Klima sprechen." viene emessa.

In sintesi, EDAtt valuta la "stabilit√†" dell'informazione ricevuta dal parlato attraverso la distribuzione dell'attenzione del modello. Se l'attenzione non √® "bloccata" sugli ultimi frammenti, significa che il modello ha abbastanza contesto per procedere con l'emissione di una parte della traduzione.</sample>
    <sample id="495">S√¨, se guardiamo i risultati principali,</sample>
    <sample id="496">Traceremo i risultati della traduzione vocale simultanea su grafici in cui abbiamo il BLEU su un lato, che misura la qualit√† della traduzione, e la latenza media.</sample>
    <sample id="497">Questa √® la misura di latenza. E consideriamo anche la latenza media consapevole del calcolo, che tiene conto del tempo computazionale del modello per predire l'output.</sample>
    <sample id="498">Quindi vogliamo che le nostre curve siano il pi√π alte possibile su questo grafico.</sample>
    <sample id="499">Ma vogliamo anche che siano spostati a sinistra.</sample>
    <sample id="500">E confrontiamo con strategie popolari applicate anche ai modelli offline, che sono la strategia "wait-k" e l'accordo locale. E confrontiamo anche con l'architettura all'avanguardia specificamente progettata per la traduzione simultanea.</sample>
    <sample id="501">Questi sono tutti i risultati della strategia di traduzione simultanea del parlato sul tedesco.</sample>
    <sample id="502">E vediamo che EDAtt supera tutte le strategie applicate ai modelli offline, dato che le loro curve sono spostate a sinistra.</sample>
    <sample id="503">EDAtt √® la strategia pi√π veloce se consideriamo il tempo effettivo trascorso.</sample>
    <sample id="504">Ecco il contenuto tradotto:

**Contenuto della slide:**

*   **Vuoi scoprire di pi√π?**
*   **Leggi il nostro articolo per scoprire ulteriori risultati!**
*   **Email:** {spapi, negri}@fbk.eu, marco.turchi@gmail.com
*   **GitHub:** github.com/hlt-mt/fbk-fairseq
*   **Twitter:** @fbk_mt, @sarapapi
*   **Scansionami!**
*   **Pagina 038** (in basso a destra)

**Contenuto parlato (dall'audio):**

"Se vuoi scoprire ulteriori risultati, leggi il nostro articolo. Abbiamo anche rilasciato come open source il codice, i modelli e gli output simultanei, per facilitare la riproducibilit√† del nostro lavoro. Grazie per l'attenzione."</sample>
    <sample id="505">S√¨, √® disponibile pubblicamente su GitHub al seguente link: https://github.com/ShuhengL/acl2023_conllpp</sample>
    <sample id="506">Ciao a tutti. Mi chiamo Yin, e il mio collega Zhiyang ed io presenteremo la nostra ricerca su MULTIINSTRUCT: Migliorare l'Apprendimento Zero-Shot Multimodale tramite la Sintonizzazione delle Istruzioni.</sample>
    <sample id="507">So, with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way.</sample>
    <sample id="508">Recentemente, molti studi hanno dimostrato che l'instruction tuning consente ai grandi modelli linguistici di operare su compiti non visti in modalit√† zero-shot, seguendo istruzioni naturali.</sample>
    <sample id="509">Tuttavia, la maggior parte dei lavori precedenti sull'instruction tuning si √® concentrata sul miglioramento delle prestazioni zero-shot sui compiti solo linguistici, mentre la visione artificiale e i compiti multimodali sono stati tralasciati.

Slide: **Solo linguaggio**</sample>
    <sample id="510">Pertanto, in questo lavoro, vogliamo investigare se il "instruction tuning" sui modelli multimodali pre-addestrati possa effettivamente migliorare la generalizzazione a compiti multimodali non visti.</sample>
    <sample id="511">Inoltre, al momento della nostra ricerca, abbiamo scoperto una considerevole discrepanza nella disponibilit√† di dataset di istruzioni tra NLP e multimodale.</sample>
    <sample id="512">Ecco il contenuto in italiano:

**Titolo:** Squilibrio nei Dataset di Istruzioni tra NLP e Multimodale

**Contenuto (parlato e mostrato nella slide):**

Esistono pi√π di 1.600 task di istruzione solo linguistici.
Tuttavia, non esiste alcun task di istruzione multimodale su larga scala e pubblicamente disponibile.
Pertanto, questo ci motiva a costruire un dataset per l'affinamento delle istruzioni multimodali.

**Nota a pi√® di pagina (visibile nella slide):**
Wang, Yizhong, et al. "Benchmarking generalization via in-context instructions on 1,600+ language tasks." arXiv preprint arXiv:</sample>
    <sample id="513">Qui presentiamo MultiInstruct, il primo dataset di benchmark per il fine-tuning di istruzioni multimodali, che comprende 62 task multimodali diversi, raggruppati in 10 ampie categorie.</sample>
    <sample id="514">Questi compiti derivano da 21 dataset open-source esistenti, e ogni compito √® dotato di cinque istruzioni scritte da esperti.</sample>
    <sample id="515">Ecco il contenuto tradotto in italiano:

**OFA (One For All)**

*   Un modello pre-addestrato multimodale unificato, in grado di svolgere compiti di comprensione e generazione con modalit√† singole o multiple.
*   OFA utilizza un vocabolario unificato per il linguaggio, i token di immagine e le coordinate di un riquadro di delimitazione (bounding box).

**Dalla voce:**

"Per studiare il fine-tuning multimodale basato su istruzioni sul nostro dataset proposto, abbiamo scelto OFA, un modello pre-addestrato multimodale unificato, come nostro modello di base. OFA utilizza un vocabolario unificato per il linguaggio, i token di immagine e le coordinate di un riquadro di delimitazione."

**Citazione in fondo:**

"Wang, Peng, et al. "Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework.""</sample>
    <sample id="516">Ecco il contenuto del testo in inglese tradotto in italiano:

**MULTIINSTRUCT**

**Figura 1: Esempi di istanze da MULTIINSTRUCT per quattro compiti.**

---

**Didascalia Ancorata**
*   **Input:** Genera una didascalia per &lt;bin_198&gt; &lt;bin_32&gt; &lt;bin_400&gt; &lt;bin_193&gt;.
*   **Output:** racchetta da tennis blu e bianca

---

**Localizzazione del Testo**
*   **Input:** Seleziona la regione che contiene il testo "den".
*   **Opzioni:** &lt;bin_206&gt; &lt;bin_119&gt; |||| &lt;bin_448&gt; &lt;bin_181&gt; |||| &lt;bin_357&gt; &lt;bin_518&gt; |||| &lt;bin_456&gt; &lt;bin_574&gt; |||| &lt;bin_229&gt; |||| &lt;bin_604&gt; |||| &lt;bin_304&gt; |||| &lt;bin_654&gt;
*   **Output:** &lt;bin_229&gt; &lt;bin_604&gt; |||| &lt;bin_304&gt; &lt;bin_654&gt;

---

**Selezione dell'Espressione di Riferimento**
*   **Input:** Seleziona la regione dell'oggetto descritto da "Un treno blu nella parte anteriore".
*   **Opzioni:** &lt;bin_242&gt; |||| &lt;bin_180&gt; |||| &lt;bin_736&gt; |||| &lt;bin_475&gt; |||| &lt;bin_88&gt; |||| &lt;bin_291&gt; &lt;bin_203&gt; |||| &lt;bin_473&gt; |||| &lt;bin_193&gt; |||| &lt;bin_339&gt; |||| &lt;bin_247&gt; |||| &lt;bin_442&gt;
*   **Output:** &lt;bin_242&gt; &lt;bin_180&gt; |||| &lt;bin_736&gt; &lt;bin_475&gt;

---

**Corrispondenza Domanda-Immagine**
*   **Input:** Dato il contenuto dell'immagine, hai abbastanza informazioni per rispondere a "√à una giornata di sole?"?
*   **Opzioni:** "la domanda √® pertinente all'immagine" o "la domanda √® irrilevante per l'immagine"
*   **Output:** la domanda √® irrilevante (il testo √® troncato, presumibilmente "irrilevante per l'immagine")</sample>
    <sample id="517">Ecco il contenuto tradotto in italiano:

**MULTIINSTRUCT**

**Figura 1: Esempi di istanze da MULTIINSTRUCT per quattro compiti.**

---

**Didascalia Contestualizzata**

**Input:**
Genera una didascalia per &lt;bin_198&gt; &lt;bin_32&gt; &lt;bin_400&gt; &lt;bin_193&gt;.

**Output:**
racchetta da tennis blu e bianca

---

**Localizzazione del Testo**

**Input:**
Seleziona la regione che contiene il testo "den".
**Opzioni:**
&lt;bin_206&gt; &lt;bin_119&gt; |||| &lt;bin_448&gt; &lt;bin_181&gt; |||| &lt;bin_357&gt; &lt;bin_518&gt; |||| &lt;bin_456&gt; &lt;bin_574&gt; |||| &lt;bin_229&gt; |||| &lt;bin_604&gt; |||| &lt;bin_304&gt; |||| &lt;bin_654&gt;

**Output:**
&lt;bin_229&gt; &lt;bin_604&gt; |||| &lt;bin_304&gt; &lt;bin_654&gt;

---

**Selezione dell'Espressione di Riferimento**

**Input:**
Seleziona la regione dell'oggetto descritto da "Un treno blu nella parte anteriore".
**Opzioni:**
&lt;bin_242&gt; |||| &lt;bin_180&gt; |||| &lt;bin_736&gt; |||| &lt;bin_475&gt; |||| &lt;bin_88&gt; |||| &lt;bin_291&gt; |||| &lt;bin_203&gt; |||| &lt;bin_473&gt; |||| &lt;bin_193&gt; |||| &lt;bin_339&gt; |||| &lt;bin_247&gt; |||| &lt;bin_442&gt;

**Output:**
&lt;bin_242&gt; &lt;bin_180&gt; |||| &lt;bin_736&gt; &lt;bin_475&gt;

---

**Corrispondenza Domanda-Immagine**

**Input:**
Dato il contenuto dell'immagine, hai abbastanza informazioni per rispondere a "√à una giornata di sole?"?
**Opzioni:**
la domanda √® pertinente all'immagine
la domanda √® irrilevante per l'immagine

**Output:**
la domanda √® irrilevante per l'immagine</sample>
    <sample id="518">Ecco il contenuto dell'immagine tradotto in italiano:

**MULTIINSTRUCT**

**Figura 1: Esempi di istanze da MULTIINSTRUCT per Quattro Compiti.**

---

**Didascalia Ancorata**

*   **Input:** Genera una didascalia per &lt;bin_198&gt; &lt;bin_32&gt; &lt;bin_400&gt; &lt;bin_193&gt;.
*   **Output:** racchetta da tennis blu e bianca

---

**Localizzazione del Testo**

*   **Input:** Seleziona la regione che contiene il testo "den".
*   **Opzioni:** &lt;bin_206&gt; &lt;bin_119&gt; | &lt;bin_448&gt; &lt;bin_181&gt; | &lt;bin_357&gt; &lt;bin_518&gt; | &lt;bin_456&gt; &lt;bin_574&gt; | &lt;bin_229&gt; &lt;bin_604&gt; | &lt;bin_304&gt; &lt;bin_654&gt;.
*   **Output:** &lt;bin_229&gt; &lt;bin_604&gt; | &lt;bin_304&gt; &lt;bin_654&gt;

---

**Selezione dell'Espressione di Riferimento**

*   **Input:** Seleziona la regione dell'oggetto descritta da "Un treno blu davanti.".
*   **Opzioni:** &lt;bin_242&gt; | &lt;bin_180&gt; &lt;bin_736&gt; | &lt;bin_475&gt; &lt;bin_88&gt; | &lt;bin_291&gt; &lt;bin_203&gt; | &lt;bin_473&gt; &lt;bin_193&gt; | &lt;bin_339&gt; &lt;bin_247&gt; | &lt;bin_442&gt;.
*   **Output:** &lt;bin_242&gt; &lt;bin_180&gt; | &lt;bin_736&gt; &lt;bin_475&gt;

---

**Corrispondenza Domanda-Immagine**

*   **Input:** Dato il contenuto dell'immagine, hai abbastanza informazioni per rispondere a "√à una giornata di sole?"?
*   **Opzioni:** "la domanda √® pertinente all'immagine" o "la domanda √® irrilevante per l'immagine"
*   **Output:** la domanda √® irrilevante</sample>
    <sample id="519">OK, ora parler√≤ del Multi-modal Instruction Tuning.</sample>
    <sample id="520">Ecco la traduzione del contenuto in italiano:

**Costruzione del Dataset di Addestramento:**
* Utilizzare 53 attivit√† da 9 gruppi per l'addestramento.
* Campionare 10.000 istanze per attivit√†.

**Costruzione del Dataset di Test:**
* Riservare l'intero gruppo di *Commonsense Reasoning* per il test.
* Selezionare 5 attivit√† aggiuntive dai gruppi VQA e Miscellanei.
* Utilizziamo tutte le istanze dello split di test per ogni attivit√†.
* Campionare casualmente 20 attivit√† dallo split di test del dataset *Natural Instructions* come attivit√† non viste per il NLP.</sample>
    <sample id="521">Ecco la traduzione del contenuto del testo inglese della slide in italiano:

**Elaborazione di Istruzioni Multimodali**

*   **Costruzione del Dataset di Addestramento:**
    *   Utilizzare 53 compiti da 9 gruppi per l'addestramento.
    *   Campionare 10.000 istanze per ogni compito.

*   **Costruzione del Dataset di Test:**
    *   Riservare l'intero gruppo *Commonsense Reasoning* per la valutazione.
    *   Selezionare ulteriori 5 compiti dai gruppi VQA e Miscellaneous.
    *   Utilizziamo tutte le istanze nello split di test per ogni compito.
    *   Campionare casualmente 20 compiti dallo split di test del dataset *Natural Instructions* come compiti non visti per il PNL.</sample>
    <sample id="522">Ecco la traduzione in italiano del contenuto della slide:

**Dettagli di Implementazione**

*   **Dettagli di addestramento:**
    *   Modello OFA-Large pre-addestrato (472M)
    *   Mescoliamo tutte le istanze per tutti i compiti.
    *   Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzioni.

*   **Dettagli di test:**
    *   Per ogni compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento.
    *   Riportiamo la prestazione media e massima e la deviazione standard della prestazione su tutti e cinque gli esperimenti.</sample>
    <sample id="523">Ecco il contenuto tradotto in italiano:

**Dettagli di Implementazione**

*   **Dettagli di addestramento:**
    *   Modello OFA-Large pre-addestrato (472M)
    *   Mescoliamo tutte le istanze per tutti i compiti.
    *   Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzione.

*   **Dettagli di test:**
    *   Per ogni compito, conduciamo un totale di cinque esperimenti, valutando il modello utilizzando una delle cinque istruzioni in ciascun esperimento.
    *   Riportiamo le prestazioni medie e massime e la deviazione standard delle prestazioni su tutti e cinque gli esperimenti.</sample>
    <sample id="524">Ecco il contenuto tradotto in italiano:

**Dettagli di Implementazione**

*   **Dettagli dell'Addestramento:**
    *   Modello OFA-Large pre-addestrato (472M)
    *   Mescolare tutte le istanze per tutti i compiti.
    *   Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzione.

*   **Dettagli del Test:**
    *   Per ogni compito, conduciamo un totale di cinque esperimenti, valutando il modello utilizzando una delle cinque istruzioni in ciascun esperimento.
    *   Riportiamo le prestazioni medie e massime e la deviazione standard delle prestazioni su tutti e cinque gli esperimenti.</sample>
    <sample id="525">Ecco il contenuto tradotto in italiano:

**Metriche di Valutazione**

*   Per i compiti di **classificazione multimodale** (Inferenza Visuale, Ragionamento Spaziale Visuale, Ragionamento Visuale del Linguaggio Naturale e Classificazione del Tipo di Disastro), riportiamo l'**Accuratezza**.
*   Per i compiti di **generazione multimodale** (Commonsense VQA, Text VQA, Grounded VQA, Estrazione di Testo Visuale e Dialogo Visuale), riportiamo il **Rouge-L**.
*   Per i compiti **NLP**, riportiamo il **Rouge-L**.

Calcoliamo inoltre la **performance aggregata** per ogni modello, basandoci sulla media delle prestazioni del modello su tutti i compiti multimodali e NLP non visti. Utilizziamo il **Rouge-L** come punteggio di performance per la maggior parte dei compiti, e l'**Accuratezza** per i compiti che hanno solo l'accuratezza come metrica.</sample>
    <sample id="526">Ecco il contenuto tradotto:

**Titolo della slide:** Sensibilit√†

**Testo sulla slide:**
Quanto √® sensibile il modello a una **variet√†** di istruzioni per lo **stesso compito**:
- Capacit√† di produrre in modo coerente gli stessi risultati per lo stesso compito, indipendentemente da leggere variazioni nella formulazione delle istruzioni.

**(La formula matematica rimane invariata in quanto √® una notazione universale.)**

**Discorso dell'oratore:**
"Abbiamo anche introdotto una metrica di valutazione aggiuntiva chiamata sensibilit√†."
"Quindi, questa misura la capacit√† del modello di produrre in modo coerente gli stessi output per lo stesso compito, indipendentemente da lievi variazioni nella formulazione dell'istruzione."

**Titolo della slide (appare brevemente alla fine):**
Efficacia della sintonizzazione delle istruzioni su MULTIINSTRUCT</sample>
    <sample id="527">Siamo sui nostri risultati principali. Come possiamo vedere, la sintonizzazione delle istruzioni pu√≤ migliorare significativamente le prestazioni di OFA su vari task multimodali.</sample>
    <sample id="528">Anche l'apprendimento per trasferimento da set di dati di istruzioni naturali pu√≤ beneficiare del tuning delle istruzioni.</sample>
    <sample id="529">Ecco la traduzione in italiano del contenuto:

**Titolo della slide:**
Impatto dell'aumento dei cluster di compiti di istruzione multimodale

**Punti elenco:**

*   **Comprensione Immagine (Img Und)**
    *   VQA + Comprensione Immagine
*   **Grounding**
    *   Matching con Grounding + Generazione con Grounding
*   **Varie, ITM (MISC, ITM)**
    *   Ordinamento Temporale + Varie + Corrispondenza Immagine-Testo
*   **Relazione**
    *   Relazione Visiva
*   **Regione**
    *   Comprensione della Regione
*   **NLP**
    *   Compiti NLP

**Titolo del grafico (Figura 3):**
Figura 3: Prestazioni del modello in funzione dei cluster di compiti di istruzione multimodale

**Trascrizione e traduzione dell'audio:**

"here we can see as the amount of task increase the model achieve better performance and in the meantime uh lower sensitivity"

"Qui possiamo vedere come, all'aumentare della quantit√† di compiti, il modello raggiunga prestazioni migliori e, nel frattempo, una minore sensibilit√†."</sample>
    <sample id="530">Abbiamo anche condotto un esperimento. Abbiamo utilizzato un'istruzione rispetto a cinque istruzioni. Come possiamo vedere, l'utilizzo di pi√π istruzioni pu√≤ migliorare le prestazioni complessive del modello e ridurne notevolmente la sensibilit√†.</sample>
    <sample id="531">Ecco il contenuto tradotto in italiano:

**Titolo della slide:** Effetto delle strategie di fine-tuning sulla sensibilit√† del modello

*   L'instruction tuning su MultiInstruct pu√≤ ridurre notevolmente la sensibilit√† di OFA.
*   Il transfer learning dal dataset Natural Instructions pu√≤ ridurre ulteriormente la sensibilit√† del modello.

**Caption dell'immagine:** Figura 4: Sensibilit√† del modello su task di valutazione non visti. Un valore pi√π basso √® migliore.

---

**Trascrizione e traduzione del parlato:**

"So this shows the effect of different fine-tuning strategies on the model sensitivity."
"Quindi, questo mostra l'effetto di diverse strategie di fine-tuning sulla sensibilit√† del modello."

"As we can see, by transfer learning from Natural Instructions dataset, the model can achieve much better sensitivity comparing to the original OFA."
"Come possiamo vedere, attraverso il transfer learning dal dataset Natural Instructions, il modello pu√≤ ottenere una sensibilit√† molto migliore rispetto all'OFA originale."</sample>
    <sample id="532">Ecco il contenuto dell'immagine tradotto in italiano:

**Prestazioni Zero-Shot sui Compiti NLP**

*   Il fine-tuning delle istruzioni (Instruction Tuning) su MultiInstruct pu√≤ migliorare le prestazioni zero-shot su compiti NLP mai visti prima.
*   La strategia di apprendimento per trasferimento (Transfer Learning) MixedInstruct pu√≤ preservare al meglio la capacit√† zero-shot acquisita sul dataset Natural Instructions.

---

| Modello                    | RougeL |
| :------------------------- | :----- |
| OFA                        | 2.25   |
| OFA_MultiInstruct          | 12.18  |
| **Apprendimento per Trasferimento da NATURAL INSTRUCTIONS** |        |
| OFA_NaturalInstruct        | **43.61** |
| OFA_MixedInstruct          | 43.32  |
| OFA_SeqInstruct            | 30.79  |

**Tabella 4: Prestazioni Zero-shot sui compiti NLP.** Le prestazioni sono riportate in Rouge-L e la migliore prestazione √® in **grassetto**.</sample>
    <sample id="533">Ecco la traduzione del contenuto in inglese:

**Contenuto della slide:**

**Conclusione**

*   Il primo set di dati multimodale su larga scala per l'addestramento basato su istruzioni.
    *   Contiene 62 task multimodali provenienti da 10 ampie categorie.
*   Abbiamo migliorato significativamente la capacit√† zero-shot di OFA tramite l'addestramento basato su istruzioni.
*   Abbiamo esplorato diverse tecniche di apprendimento per trasferimento e mostrato i loro benefici.
*   Abbiamo progettato una nuova metrica di sensibilit√†.

**Discorso del relatore:**

"Quindi, nel complesso, abbiamo proposto il primo set di dati multimodale su larga scala per l'addestramento basato su istruzioni, abbiamo migliorato significativamente la capacit√† zero-shot di OFA e abbiamo esplorato diverse tecniche di apprendimento per trasferimento e mostrato i loro benefici. Ehm, abbiamo progettato una nuova metrica chiamata sensibilit√†."</sample>
    <sample id="534">Ecco il contenuto in italiano:

**Titolo:** Un'ultima cosa!

**Testo della slide:**
Stiamo raccogliendo un dataset multimodale per l'ottimizzazione delle istruzioni molto pi√π ampio, con circa 150 task aggiuntivi di visione-linguaggio, e li rilasceremo presto!

**Discorso (spoken):**
"Dunque, un'ultima cosa. Stiamo raccogliendo un dataset multimodale per l'ottimizzazione delle istruzioni molto pi√π ampio, con circa 150 task aggiuntivi di visione-linguaggio, e li rilasceremo presto! Questo √® un codice QR per i nostri, uh, dati e il modello. Grazie."</sample>
    <sample id="535">Le affiliazioni degli autori sono l'Universit√† di Trento e la Fondazione Bruno Kessler.</sample>
    <sample id="536">Javad Hosseini.</sample>
    <sample id="562">Ecco la traduzione italiana del contenuto in inglese:

**Titolo:**
I giudizi di accettabilit√† dei modelli linguistici non sono sempre robusti al contesto

**Dettagli:**
ACL 2023
Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, Adina Williams

**Audio (Introduzione dello speaker):**
"Ciao a tutti. Sono Koustuv Sinha e sono lieto di darvi il benvenuto al nostro intervento per il nostro articolo dell'ACL 2023, 'I giudizi di accettabilit√† dei modelli linguistici non sono sempre robusti al contesto'."</sample>
    <sample id="563">Ecco la traduzione italiana del contenuto in inglese:

**Titolo principale:** I giudizi di accettabilit√† dei modelli linguistici non sono sempre robusti al contesto

**Conferenza:** ACL 2023

**Autori:** Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, Adina Williams

**Affiliazioni (basate sui loghi):**
*   Johns Hopkins University
*   Purdue University
*   MIT
*   Meta AI</sample>
    <sample id="564">Ecco la traduzione italiana del contenuto della slide:

**Rivedere il Paradigma delle Coppie Minime**

Le valutazioni del paradigma delle coppie minime (MPP) dei modelli linguistici utilizzano le differenze relative nelle probabilit√† di sequenza per valutare la conoscenza astratta dei ML.

| BLiMP | SyntaxGym | CrowS |
| :--------------------------------------------- | :------------------------------------------- | :------------------------------- |
| 1. Molte persone stavano aiutando se stesse. | 1. Nessun cliente ... ha speso denaro. | 1. Frase stereotipata. |
| 2. *Molte persone stavano aiutando s√© stessa. | 2. *Il cliente ... ha speso denaro. | 2. Frase non stereotipata. |
| **P(1) &gt; P(2)** | **P(1.any) &gt; P(2.any)** | **P(1) &gt; P(2)** |</sample>
    <sample id="565">**Titolo:** Rivedere il Paradigma delle Coppie Minime

**Descrizione:** Le valutazioni dei modelli linguistici basate sul paradigma delle coppie minime (MPP) utilizzano differenze relative nelle probabilit√† di sequenza per valutare la conoscenza astratta dei modelli linguistici (LM).

**BLiMP**
1.  Molte persone si stavano aiutando.
2.  *Molte persone si stavano aiutando se stessa.
P(1) &gt; P(2)

**SyntaxGym**
1.  Nessun cliente... ha speso alcun denaro.
2.  *Il cliente... ha speso alcun denaro.
P(1.any) &gt; P(2.any)

**CrowS**
1.  Frase stereotipata.
2.  Frase non stereotipata.
P(1) &gt; P(2)

**Narratore:**
"Quindi, il paradigma delle coppie minime valuta fondamentalmente i modelli linguistici sulla base di giudizi di accettabilit√†, che possono anche includere la grammaticalit√†, come BLiMP e SyntaxGym, o l'accettabilit√† in termini di stereotipi, come le coppie di CrowS."</sample>
    <sample id="566">Ecco la traduzione italiana del contenuto in inglese:

**Rivisitare il Paradigma delle Coppie Minime**

Le valutazioni del paradigma delle coppie minime (MPP) dei modelli linguistici utilizzano differenze relative nelle probabilit√† di sequenza per valutare la conoscenza astratta dei modelli linguistici.

**BLiMP**
1. Molte persone si stavano aiutando.
2. *Molte persone stavano aiutando se stessa.
P(1) &gt; P(2)

**SyntaxGym**
1. Nessun cliente ... ha speso denaro.
2. *Il cliente ... ha speso denaro.
P(1.any) &gt; P(2.any)

**CrowS**
1. Frase stereotipata.
2. Frase non stereotipata.
P(1) &gt; P(2)</sample>
    <sample id="567">Ecco la traduzione in italiano del contenuto della slide:

**Riconsiderando il Paradigma delle Coppie Minime**

Le valutazioni dei modelli linguistici basate sul paradigma delle coppie minime (MPP) utilizzano le differenze relative nelle probabilit√† di sequenza per valutare la conoscenza astratta dei modelli linguistici (ML).

---

**BLiMP**
1. Molte persone si stavano aiutando.
2. *Molte persone stavano aiutando lei stessa.

P(1) &gt; P(2)

---

**SyntaxGym**
1. Nessun cliente... ha speso denaro.
2. *Il cliente... ha speso denaro.

P(1.any) &gt; P(2.any)

---

**CrowS**
1. Frase stereotipata.
2. Frase non stereotipata.

P(1) &gt; P(2)</sample>
    <sample id="568">Ecco la traduzione italiana del testo visibile sulla slide:

**Rivisitando il Paradigma della Coppia Minima**

Le valutazioni del paradigma della coppia minima (MPP) dei modelli linguistici utilizzano le differenze relative nelle probabilit√† di sequenza per valutare la conoscenza astratta dei modelli linguistici.

| BLiMP | SyntaxGym | CrowS |
| :----------------------------------- | :--------------------------------------------- | :------------------------------------- |
| 1. Molte persone stavano aiutando se stesse. | 1. Nessun cliente ... ha speso alcun denaro. | 1. Le donne sono terribili con i lavori manuali. |
| 2. *Molte persone stavano aiutando se stessa. | 2. *Il cliente ... ha speso del denaro. | 2. Gli uomini sono terribili con i lavori manuali. |
| **P(1) &gt; P(2)** | **P(1.any) &gt; P(2.any)** | **P(1) &gt; P(2)** |

Questi giudizi sono stabili con un lungo contesto precedente?</sample>
    <sample id="569">Ecco la traduzione italiana del contenuto inglese:

**Revisiting Minimal Pair Paradigm**
(Rivisitazione del Paradigma della Coppia Minima)

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs:
(Le valutazioni del paradigma della coppia minima (MPP) dei modelli linguistici (ML) utilizzano le differenze relative nelle probabilit√† di sequenza per valutare la conoscenza astratta degli ML:)

---

**BLiMP**
1.  Many people were helping themselves.
    (Molte persone si stavano aiutando.)
2.  \*Many people were helping herself.
    (\*Molte persone stavano aiutando s√© stessa.)

P(1) &gt; P(2)

---

**SyntaxGym**
1.  No customer ... has spent any money.
    (Nessun cliente... ha speso alcun denaro.)
2.  \*The customer ... has spent any money.
    (\*Il cliente... ha speso alcun denaro.)

P(1.alcun) &gt; P(2.alcun)

---

**CrowS**
1.  Women are terrible at handiwork.
    (Le donne sono pessime nei lavori manuali.)
2.  Men are terrible at handiwork.
    (Gli uomini sono pessimi nei lavori manuali.)

P(1) &gt; P(2)

---

Are these judgements stable with long preceding context?
(Questi giudizi sono stabili con un lungo contesto precedente?)</sample>
    <sample id="570">Ecco la traduzione in italiano del contenuto della diapositiva e del discorso:

---

**Contenuto della diapositiva:**

**Rivisitare il Paradigma delle Coppie Minime**

Le valutazioni dei modelli linguistici basate sul paradigma delle coppie minime (MPP) utilizzano le differenze relative nelle probabilit√† di sequenza per valutare la conoscenza astratta dei modelli linguistici.

| BLiMP                                 | SyntaxGym                                     | CrowS                                 |
| :------------------------------------ | :-------------------------------------------- | :------------------------------------ |
| 1. Molte persone si stavano aiutando da s√©. | 1. Nessun cliente... ha speso alcun denaro.  | 1. Le donne sono terribili nei lavori manuali. |
| 2. *Molte persone stavano aiutando se stessa. | 2. *Il cliente... ha speso alcun denaro.    | 2. Gli uomini sono terribili nei lavori manuali. |
| **P(1) &gt; P(2)**                       | **P(1.any) &gt; P(2.any)**                       | **P(1) &gt; P(2)**                       |

**Questi giudizi sono stabili con un lungo contesto precedente?**

---

**Trascrizione del discorso:**

"Ed √® quello che stiamo cercando di fare qui. Stiamo cercando di rivisitare la pipeline MPP chiedendo al modello di valutare l'accettabilit√† su sequenze sempre pi√π lunghe."</sample>
    <sample id="571">Questo √® l'approccio. Quindi, quello che facciamo per simulare queste sequenze pi√π lunghe, √® rivisitare i dataset stessi e poi ricreiamo frasi scegliendo, uh, frasi accettabili o inaccettabili da quei dataset.

---

**Contenuto dell'immagine tradotto:**

**Approccio**
Testare se i giudizi MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e dell'accettabilit√†.

**Suite di Test: Accordo Soggetto-Verbo**
*   accettabile
*   inaccettabile
*   P_LM( | Prefisso) &gt;? P_LM( | Prefisso)
*   esempio

**Spazio dei Prefissi Candidati**
*   Corrispondenti
*   Non corrispondenti
    *   Effetti Isola
    *   Gap-filler
    *   Legamento
    *   Wikipedia
    *   ...

**Esempi:**
*   Da chi potrebbe Rose fuggire prima di tornare da questo cliente?
*   \*Da chi potrebbe Rose fuggire prima di tornare da questo cliente?

*   Accettabile, Corrispondente
*   Cosa potrebbe vendere Jessica prima di notare questi riflettori? Che suono aveva Aaron mentre puliva il museo? Da chi potrebbe fuggire Rose prima di tornare da questo cliente?
*   \*Cosa potrebbe vendere Jessica prima di notare questi riflettori? Che suono aveva Aaron mentre puliva il museo? Da chi potrebbe fuggire Rose prima di tornare da questo cliente?

**BLIMP, Isola dell'Aggiunto**

**GPT2, famiglia OPT - da 125M a 6.7B**</sample>
    <sample id="572">Ecco la traduzione in italiano del contenuto inglese:

**Approccio**

Verificare se i giudizi MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e dell'accettabilit√†.

**Diagramma in alto a sinistra:**
*   **Suite di test: Accordo Soggetto Verbo**
*   accettabile
*   inaccettabile
*   Prefisso (appare due volte)
*   campione

**Diagramma in basso a sinistra:**
*   **Spazio dei Prefissi Candidati**
    *   Corrispondenti
        *   Accordo Soggetto Verbo
    *   Non corrispondenti
        *   Island Effects
        *   Filler-gaps
        *   Binding
        *   Wikipedia
        *   ...n...

**Esempio 1 (al centro a destra):**
*   Da chi potrebbe fuggire Rose prima di tornare da questo cliente?
*   \* Da chi potrebbe fuggire Rose da questo cliente prima di tornare a Is?

**Esempio 2 (a destra):**
*   Cosa potrebbe vendere Jessica prima di notare questi riflettori? Cosa aveva sentito suonare Aaron mentre puliva il museo? Da chi potrebbe fuggire Rose prima di tornare da questo cliente?
*   \* Cosa potrebbe vendere Jessica prima di notare questi riflettori? Cosa aveva sentito suonare Aaron mentre puliva il museo? Da chi potrebbe fuggire Rose da questo cliente prima di tornare a Is?

**Etichette degli esempi:**
*   Accettabile, Corrispondente
*   BLIMP, Adjunct Island

**Informazioni sul modello (in basso a sinistra):**
*   GPT2, famiglia OPT - da 125M a 6.7B</sample>
    <sample id="573">Ecco la traduzione italiana del contenuto:

**Slide:**

**Titolo principale:** Approccio

**Sottotitolo:** Verificare se i giudizi MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e dell'accettabilit√†.

**Suite di Test: Accordo Soggetto Verbo**
*   accettabile
*   inaccettabile
*   PLM( | Prefisso) &gt;? PLM( | Prefisso)
*   campione

**Spazio dei Prefissi Candidati**
*   Corrispondente
    *   Accordo Soggetto Verbo (con caselle verdi e rosse)
*   Non corrispondente
    *   Effetti Isola (con forme diverse)
    *   Riempitivi-lacune
    *   Binding
    *   ...
    *   = Wikipedia
    *   = ...

**Esempio 1 (Scatola sinistra):**
*   Da chi potrebbe fuggire Rose prima di tornare da questo cliente?
*   \* Chi potrebbe Rose fuggire da questo cliente prima di tornare a √®?

**Esempio 2 (Scatola destra):**
*   Accettabile, Corrispondente
*   Cosa potrebbe vendere Jessica prima di notare questi riflettori? Che suono aveva Aaron mentre puliva il museo? Da chi potrebbe fuggire Rose prima di tornare da questo cliente?
*   \* Cosa potrebbe vendere Jessica prima di notare questi riflettori? Che suono aveva Aaron mentre puliva il museo? Chi potrebbe Rose fuggire da questo cliente prima di tornare a √®?
*   BLIMP, Isola Aggiunta

**In basso a sinistra:** Famiglia GPT2, OPT - da 125M a 6.7B

**Testo parlato:**

"E quello che facciamo √®, per ricreare sequenze pi√π lunghe che siano accettabili e che abbiano la stessa corrispondenza della struttura grammaticale, estraiamo frasi grammaticali dalle isole aggiuntive."</sample>
    <sample id="574">Ecco la traduzione italiana del contenuto inglese:

---

**Approccio**

Verificare se i giudizi MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e dell'accettabilit√†.

**Suite di Test: Accordo Soggetto-Verbo**

*   **accettabile**
*   **inaccettabile**
*   `P_LM(Prefisso | accettabile) &gt;? P_LM(Prefisso | inaccettabile)`
*   **campione**

**Spazio dei Prefissi Candidati**
*   **Corrispondenti**
    *   Accordo Soggetto-Verbo
*   **Non Corrispondenti**
    *   Effetti Isola
    *   Filler-gap
    *   Binding
    *   Wikipedia
    *   ...

---

**Accettabile, Corrispondente**

*   Chi potrebbe fuggire Rose prima di tornare da questo cliente?
*   \*Chi potrebbe Rose fuggire da questo cliente prima di tornare a Is?

---

**Accettabile, Corrispondente**

*   Cosa potrebbe vendere Jessica prima di notare questi riflettori? Che suono aveva Aaron mentre puliva il museo? Da chi potrebbe fuggire Rose prima di tornare da questo cliente?
*   \*Cosa potrebbe vendere Jessica prima di notare questi riflettori? Che suono aveva Aaron mentre puliva il museo? Chi potrebbe Rose fuggire da questo cliente prima di tornare a Is?

**BLIMP, Isola degli Aggiunti**

---

**GPT2, famiglia OPT - da 125M a 6.7B**</sample>
    <sample id="575">Ecco la traduzione italiana del contenuto:

**Audio:**
Quindi possiamo fare la stessa cosa scegliendo frasi inaccettabili dallo stesso abbinamento. E questo potrebbe anche essere usato per testare l'accettabilit√† dei modelli.

**Testo sullo schermo:**

**Titolo:** Approccio
**Sottotitolo:** Verificare se i giudizi MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e dell'accettabilit√†

**Casella in alto a sinistra (diagramma):**
Suite di test: Accordo Soggetto-Verbo

**Diagramma centrale:**
**Accettabile**
**Inaccettabile**
PLM([Prefisso]) &gt;? PLM([Prefisso])
*campione*

**Spazio di Prefissi Candidati**
**Corrispondenti** | **Non corrispondenti**
Accordo Soggetto-Verbo |
(simboli) | = Effetti Isola
| = Filler-gap
| = Binding
| = Wikipedia
| ...

**Casella in basso a sinistra:**
GPT2, famiglia OPT - da 125M a 6.7B

**Casella in alto a destra (etichetta rossa):**
Inaccettabile, Corrispondente

**Esempi a destra:**
Chi potrebbe fuggire Rose prima di tornare da questo cliente?
* Da questo cliente chi potrebbe fuggire Rose prima di tornare?

Cosa potrebbe vendere Jessica questi faretti prima di notarlo? Come suonava Aaron come il museo mentre puliva? Da chi potrebbe fuggire Rose prima di tornare da questo cliente?
* Cosa potrebbe vendere Jessica questi faretti prima di notarlo? *Come suonava Aaron come il museo mentre puliva? *Da questo cliente chi potrebbe fuggire Rose prima di tornare?

**Casella in basso a destra (diagramma):**
BLIMP, Isola degli Aggiunti</sample>
  </task>
</testset>