<?xml version='1.0' encoding='utf-8'?>
<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="209">Die vorgeschlagene Methode übertrifft die stärkste Baseline um etwa 25 Prozentpunkte.</sample>
    <sample id="210">Der/die Referent*in heißt Shuheng Liu.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz können als Benchmark verwendet werden.</sample>
    <sample id="212">Die Arbeit experimentiert mit 55.000 kleineren Modellen.</sample>
    <sample id="213">Sure, the model used as the base model for investigating multimodal instruction tuning is OFA (One For All), a unified multi-modal pre-trained model.</sample>
    <sample id="215">Dieses Video beleuchtet verschiedene Ansätze zur Abhängigkeitsstruktur der Koordination. Es werden vier Hauptansätze vorgestellt: Bouquet/Stanford (universelle Abhängigkeiten), Chain/Moscow, Conjunction-headed/Prague und Multi-headed/London. Die ersten beiden sind asymmetrisch, wobei das erste Konjunkt als Kopf der Koordination dient. Prague und London sind symmetrische Ansätze.

Das Video argumentiert für die symmetrischen Ansätze, basierend auf dem Prinzip der Minimierung der Abhängigkeitslänge (DLM). Dieses Prinzip besagt, dass die Wortreihenfolge dazu neigt, die Abhängigkeitslängen zu minimieren. Ein Beispiel illustriert, dass direkte Objekte tendenziell näher am Verb stehen als Adverbien. Bei langen direkten Objekten kann die Reihenfolge umgekehrt werden, um die Gesamtabhängigkeitslänge zu minimieren.

Statistiken zur Koordination, die aus dem Penn Treebank extrahiert wurden, bestätigen die Beobachtung, dass linke Konjunkte tendenziell kürzer sind. Diese Tendenz verstärkt sich mit zunehmender Längendifferenz zwischen den Konjunkten. Eine neue Beobachtung ist, dass dieser Effekt nur auftritt, wenn das regierende Wort (Governor) links von der Koordination steht oder abwesend ist. Steht der Governor rechts, verschwindet dieser Effekt. Dieses Ergebnis spricht gegen asymmetrische und für symmetrische Koordinationsstrukturen.</sample>
    <sample id="217">Diese Forschung befasst sich mit der **kompositionellen Generalisierung in der Multi-Attribut-steuerbaren Dialoggenerierung** und stellt fest, dass bestehende Modelle keine ausreichende Generalisierungsfähigkeit besitzen. Wir schlagen **DCG (disentangled controllable generation)** vor, ein Modell, das Attributkonzepte aus bekannten Werten lernt und eine Entflechtungsverlustfunktion verwendet, um verschiedene Attributkombinationen zu entwirren. Um die Effizienz des Modells zu verbessern, haben wir zwei Arten von Prompts entwickelt: **attributorientierte Prompts** zur Steuerung der Modelfokussierung auf spezifische Informationen und **aufgabenorientierte Prompts** zur Verbesserung der Dialoggenerierung durch instanzunabhängige globale Merkmale.

Darüber hinaus führen wir **MAE**, ein einheitliches, referenzfreies Bewertungs-Framework, für die Multi-Attribut-Generierung ein. Die Experimente zeigen, dass unser DCG-Modell **alle Baselines bei der Attributsteuerbarkeit und der Textqualität übertrifft**. Dies bestätigt die Effektivität unserer Methode, gesehene Attribute in ungesehene Kombinationen zu transformieren. Die Korrelationsanalysen zeigen, dass MAE mit menschlichen Urteilen korreliert, und die Visualisierung von Prompts bestätigt, dass unser Modell Attributkombinationen entwirren und Beziehungen zwischen Attributen lernen kann. Insgesamt zeigt die Arbeit, dass die **Kombination von Prompt-Design und Entflechtungslernen** wesentlich zur Verbesserung der kompositionellen Generalisierung beiträgt.</sample>
    <sample id="218">Die Autoren sind Mitarbeiter von Google Translate.</sample>
    <sample id="219">In diesem Vortrag stellt Jia-Huei Ju eine mehrstufige Pipeline zur Erkennung von Finanzsignalen in Finanzberichten vor. Die Arbeit wurde von der Beobachtung motiviert, dass Finanzberichte eine hohe Überlappung und jährliche Abhängigkeit aufweisen. Im Durchschnitt sind etwa 80 % der in den Berichten eines Unternehmens verwendeten Token gleich (mit Ausnahme des Datums), und die Inhalte sind von Jahr zu Jahr abhängig, wobei unmittelbar aufeinanderfolgende Jahre ähnlicher sind als weiter entfernte.

Basierend auf diesen Beobachtungen definieren sie eine Hervorhebungsaufgabe. Ein "Target" ist ein Finanzbericht für ein bestimmtes Jahr, während eine "Reference" der Bericht desselben Unternehmens aus dem Vorjahr ist. Ein Dokumentenpaar enthält mehrere "Reference-to-Target"-Segmentpaare. Das Ziel der Hervorhebungsaufgabe ist es, die dem Vergleich zwischen dem Ziel und der Referenz zugrunde liegenden rationale/wichtige Wörter zu finden, wobei Wörter mit hoher Bedeutung als Finanzsignale betrachtet werden.

Die vorgeschlagene Pipeline besteht aus drei Hauptstufen:
1.  **Dokumentensegmentierung (S0):** Dieser Schritt dient der Vorverarbeitung, um die Dokumente in Segmente zu unterteilen.
2.  **Beziehungserkennung (S1):** Hier werden die "Reference-to-Target"-Segmentpaare in drei Typen kategorisiert:
    *   **Unbedeutende Beziehungen (Tβ):** Uninformative Segmente mit hoher syntaktischer und semantischer Ähnlichkeit (z. B. Vorschriften).
    *   **Überarbeitete Beziehungen (T1α):** Segmente, die sich in wenigen Wörtern unterscheiden, aber unterschiedliche Bedeutungen aufweisen (z. B. "increase" vs. "decrease"). Diese Segmente werden für das In-Domain-Feintuning verwendet.
    *   **Mismatched Beziehungen (T2α):** Segmente mit gegenseitig exklusiver Bedeutung oder völlig neuen Informationen (z. B. neue Richtlinien).
3.  **Hervorhebungsstufen (S2 &amp; S2+):** Dieser Teil beinhaltet einen zweistufigen Feintuning-Ansatz für den domänenadaptiven Highlighter:
    *   **Out-of-Domain Feintuning (S2):** Training auf einem externen Datensatz, e-SNLIc, der mit Token-Annotationen versehen ist.
    *   **In-Domain Feintuning (S2+):** Feintuning auf den "Revised" Paaren (T1α) mit Pseudo-Labels, die durch Soft-Labeling-Techniken unter Verwendung von Kreuzentropie-Verlust und KL-Divergenz generiert werden. Dies hilft, die Probleme von Pseudo-Labels geringer Qualität zu mindern.

Die Evaluierung zeigt, dass das domänenadaptive Hervorhebungsmodell die besten Ergebnisse auf ihrem FINAL-Datensatz erzielt und dabei die Generalisierungsfähigkeit der Token-Repräsentationen auf e-SNLIc bewahrt. Das Modell zeigt auch Verbesserungen bei ungesehenen Beziehungen, wie den "Mismatched"-Paaren, die während des Trainings nicht verwendet wurden.

Zukünftige Arbeiten könnten die Effektivität der Modelle weiter verbessern, indem finanzielle Sprachmodelle vorab trainiert werden, da der Finanzkorpus reichlich vorhanden ist. Weitere Merkmale, wie die bidirektionale Rationalisierungsaufgabe und die Anwendung auf andere Sprachen als Englisch, könnten ebenfalls untersucht werden. Effizientere Ansätze, wie End-to-End-Methoden für die dichte Retrieval und Erklärung, und die Analyse von Charts, Tabellen oder Cross-Company/Cross-Sector-Daten sind ebenfalls mögliche Richtungen für die zukünftige Forschung.</sample>
    <sample id="220">Die Autoren gehören der Stony Brook University an.</sample>
    <sample id="221">Die in der Arbeit untersuchten Sprachpaare waren Deutsch-Englisch.</sample>
    <sample id="222">Dieses Video befasst sich mit den Herausforderungen und Interventionen bei der Beantwortung von Fragen aus offenen Domänen. Herkömmliche Modelle sind in einer Domäne, z. B. Wikipedia, trainiert. Um Fragen aus neuen Domänen, wie z. B. der Biomedizin, zu beantworten, werden Retrieval- und Lesemodelle verwendet. Eine einfache Lösung ist die Erweiterung der Dokumentensammlung um Dokumente aus der Biomedizin. Allerdings führt dies oft zu falschen Antworten, da die Originalmodelle für eine andere Domäne trainiert wurden und nicht zwischen ihnen unterscheiden können.

Die Forschungsarbeit konzentriert sich auf Dateninterventionen für die Verallgemeinerung von Out-of-Domain-Inhalten in offenen Domänen, die Untersuchung der Modellkompatibilität und die Bestimmung von Beziehungen zwischen Dateninterventionen und Dataset-Verschiebungen.

Es werden zwei Ansätze für Dateninterventionen untersucht: Few-Shot und Zero-Shot. Few-Shot-Interventionen, die große Sprachmodelle (LLMs) mit einigen Beispielen aus der Zieldomäne verwenden, verbessern die Leistung von Retrieval-Modellen um durchschnittlich 8 % und die von Lesemodellen um 11 %. Bei Zero-Shot-Interventionen, die keine Beispiele aus der Zieldomäne erfordern, werden Fragen, Antworten und Kontexte variiert. Die Frageformatänderung hat keinen Einfluss auf die Leistung, aber Cloze-Fragen sind einfacher zu erstellen. Eine gleichmäßige Verteilung der Antworttypen funktioniert am besten. Schließlich wird festgestellt, dass trainierte Retrieval-Modelle empfindlich auf Datenverteilungsänderungen reagieren, während BM25 robuster ist.

Um die Kompatibilität von Retrieval- und Lesemodellen zu bewerten, wird eine 2D-Klassifizierung verwendet, um verschiedene Arten von Dataset-Verschiebungen zu identifizieren: "No Shift" (beide kompatibel), "Concept Shift" (Retriever kompatibel, Reader inkompatibel), "Covariate Shift" (Retriever inkompatibel, Reader kompatibel) und "Full Shift" (beide inkompatibel). Die Effektivität von Dateninterventionen hängt von der Art der Dataset-Verschiebung ab.</sample>
    <sample id="223">Der/die Referent*in heißt Shangbin Feng.</sample>
    <sample id="224">Die Modelle, die während der Experimente untersucht wurden, sind: LHA, Sent-LaBSE, Sent-RoBERTa, CATS-C3G, VecAlign, BERTalign und MASSalign.</sample>
    <sample id="225">Von den 62 verschiedenen Aufgaben in MultiInstruct werden 53 für das Training und 9 für das Testen verwendet.</sample>
    <sample id="226">Vier Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="227">Sprachmodelle haben große Erfolge erzielt, aber es fehlt ihnen an Erdung während des Vortrainings. Geerdetes Sprachverständnis bedeutet, einen natürlichen Sprachausdruck in etwas umzuwandeln, das in einer spezifischen Zielumgebung ausgeführt werden kann, wie etwa Sprachassistenten oder Roboter. Eine Herausforderung besteht darin, dass die Pläne oder Programme, die von Sprachmodellen generiert werden, möglicherweise nicht immer grammatikalisch oder gültig sind.

Der Pangu-Framework zielt darauf ab, Sprachmodellen zu ermöglichen, sich auf Diskriminierung statt auf Generierung zu konzentrieren. Ein symbolischer Agent interagiert mit der Umgebung und schlägt gültige Kandidatenpläne vor, während das Sprachmodell nur für die Bewertung zuständig ist. Das Framework trennt die neuronale und die symbolische Welt, ähnlich wie Pangu, der in der chinesischen Mythologie Himmel und Erde trennt.

Pangu erzielt herausragende Leistungen bei der Wissensdatenbank-Fragebeantwortung, einer heterogenen Umgebung für geerdetes Sprachverständnis. Es übertrifft frühere Ansätze und zeigt eine starke Stichproben-Effizienz, wobei es mit nur einem Demonstrationsbeispiel mehr als 50 % Genauigkeit auf GraphQuestions erzielt. Pangu verbessert auch die Generalisierbarkeit unter nicht-unabhängigen und identisch verteilten (non-I.I.D.) Bedingungen, indem es Überanpassungen an gesehenen Strukturen während des Trainings vermeidet. Die Kernerkenntnis ist, dass die direkte Generierung von Plänen durch Sprachmodelle möglicherweise nicht der optimale Ansatz für geerdetes Sprachverständnis ist; stattdessen könnte Diskriminierung eine bessere Strategie sein.</sample>
    <sample id="228">Die Autoren haben an vier Datensätzen experimentiert: AG News, MIND, SST2 und Enron Spam.</sample>
    <sample id="229">The speaker, Gabriella Skitalinskaya, introduces her joint work with Henning Wachsmuth on detecting improvable claims for argumentative writing support. She begins by explaining that text revision is an essential part of argumentative writing, where finding the right words and expressions directly influences the persuasive impact on the audience.

She illustrates this with an example of an argumentative claim, "Cell phones cause brain cancer," which is then revised twice to "Cell phone radiation causes brain cancer," and then to "Cell phone radiation may cause cancer." This leads to the core question of their paper: "How to know whether an argumentative claim is phrased well enough and no more revisions are needed?"

To address this, two new tasks are introduced: Suboptimal-Claim Detection (deciding if a claim needs revision or is optimally phrased) and Claim Improvement Suggestion (selecting quality issues to improve). The work models argument quality based on implicit revision patterns from collaborative editing behaviors in online debate platforms like Kialo. In these platforms, final, optimal versions are colored green, and their suboptimal predecessors are colored red.

The speaker highlights four challenges in this work:
1.  **Representativity and Reliability:** Ensuring the dataset reliably represents argument quality and addressing whether a final version is truly optimal or just overlooked.
2.  **Model Complexity and Architecture:** Selecting models that align with the idea of revisions and are sensitive to subtle changes.
3.  **Contextuality:** Determining what contextual information (topic expertise, domain knowledge, parent claim, other context) is relevant for decision-making.
4.  **Topical and User Bias:** Accounting for noise, accidental mistakes, and biases of users and moderators due to controversial topics or subjective judgments of effectiveness.

The presentation concludes by inviting the audience to read their paper for a detailed analysis of these challenges and a systematic comparison of approaches. Key findings include the effective use of revision-based data, the benefit of modeling the distance between claim versions, and the task- and quality issue-dependent impact of contextual information.</sample>
    <sample id="231">NACHOS ist ein offenes Dataset von heterogenen Daten, die aus verschiedenen medizinischen Bereichen und Stilen gecrawlt wurden.</sample>
    <sample id="232">Der Referent heißt David Vilar Torres.</sample>
    <sample id="233">In dieser Präsentation stellt Sara Papi eine Methode namens EDAtt vor, die die Aufmerksamkeitsgewichte eines vorab trainierten Offline-Sprachübersetzungsmodells nutzt, um die Latenz bei der simultanen Sprachübersetzung zu steuern. Die Methode basiert auf einer einfachen Strategie: Wörter werden erst dann ausgegeben, wenn die Aufmerksamkeitsgewichte der letzten λ Sprachrahmen unter einem bestimmten Schwellenwert liegen, was bedeutet, dass die empfangenen Informationen stabil genug sind, um eine genaue Übersetzung zu gewährleisten. Im Gegensatz zu früheren Ansätzen erfordert EDAtt kein erneutes Training des Modells und kann flexibel an verschiedene Latenzmodi angepasst werden, indem einfach die Parameter λ und der Schwellenwert variiert werden. Experimentelle Ergebnisse zeigen, dass EDAtt nicht nur alle gängigen Offline-Modell-Strategien übertrifft, sondern auch eine überragende Leistung bei der Reduzierung der tatsächlichen Übersetzungszeit aufweist.</sample>
    <sample id="234">Die Wahl einer guten Prompt-Strategie ist wichtig, da Prompts einen großen Einfluss auf die Übersetzungsqualität haben.</sample>
    <sample id="235">Die Autoren gehören zur Carnegie Mellon University.</sample>
    <sample id="236">Hier sind die fünf Anweisungen, die von den Expert*innen verfasst wurden:

1. **Grounded Captioning:** Generiere eine Bildunterschrift für das Bild in Bezug auf die angegebenen Bounding Boxes.
2. **Text Localization:** Wähle die Region aus, die den Text "den" enthält. Optionen:
   * **bin 206** &lt;bin_119&gt;
   * **bin 448** &lt;bin_641&gt;
   * **bin 357** &lt;bin_518&gt;
   * **bin 456** &lt;bin_574&gt;
   * **bin 229** &lt;bin_604&gt;
   * **bin 304** &lt;bin_654&gt;
3. **Referring Expression Selection:** Wähle die Region des Objekts aus, das durch "A blue train in the front." beschrieben wird. Optionen:
   * **bin 242** &lt;bin_180&gt;
   * **bin 339** &lt;bin_203&gt;
   * **bin 291** &lt;bin_193&gt;
   * **bin 475** &lt;bin_88&gt;
   * **bin 736** &lt;bin_475&gt;
   * **bin 247** &lt;bin_442&gt;
4. **Question-Image Matching:** Gegeben den Inhalt des Bildes, hast du genug Informationen, um die Frage "Is it a sunny day?" zu beantworten? Optionen: "the question is relevant to the image" oder "the question is irrelevant to the image"</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle zur Nutzung von Informationen aus mehreren Quellen zu testen, indem sie einen **Diagnose-Test-Suite** mit einer **Coreferenzauflösungsaufgabe** entwickeln. Diese Aufgabe ist so konzipiert, dass sie die Fähigkeit der Modelle untersucht, Wissen zu nutzen, das zu **Vortrainingszeiten** und zu **Inferenzzeiten** verfügbar ist.</sample>
    <sample id="238">In diesem Video wird „MeetingBank“ vorgestellt, ein neues Benchmark-Datenset für die Zusammenfassung von Besprechungen. Die Motivation für dieses Datenset ergibt sich aus der Notwendigkeit, hochwertige Besprechungszusammenfassungen zu erstellen, insbesondere im Kontext von Stadtverordnetenversammlungen. Die Ersteller haben drei Herausforderungen bewältigt: die Knappheit hochwertiger Besprechungszusammenfassungen, die Schwierigkeit, zuverlässige Quellen für öffentliche Besprechungen zu identifizieren, und die Notwendigkeit, alle gesammelten Daten freizugeben.

Das Datenset wurde durch die Konvertierung von Audioaufnahmen in Transkripte und die manuelle Extraktion von Informationen aus den Besprechungsprotokollen erstellt. Es umfasst 1.366 Sitzungen und fast 7.000 Instanzen aus sechs Städten. Die Analysen zeigen, dass die meisten Zusammenfassungen einen hohen Abdeckungsgrad (0,7-0,9) aufweisen, was darauf hindeutet, dass sie wichtige Punkte aus den Transkripten enthalten. Die Dichte variiert, wobei Seattle und Boston die höchsten und Denver die niedrigste Dichte aufweisen, was auf unterschiedliche Bearbeitungsgrade der Protokolle hindeutet.

Für die Modellevaluation wurden extraktive und abstraktive Modelle sowie große Sprachmodelle (LLMs) wie GPT-3 verwendet. Extraktive Modelle wie Extr-Oracle erzielten hohe ROUGE-2-Werte, was die hohe Extrahierbarkeit der Referenzzusammenfassungen bestätigt. Unter den abstraktiven Modellen schnitt DialogLM am besten ab. Obwohl GPT-3 bei automatischen Metriken nicht gut abschnitt, zeigte es in der menschlichen Bewertung die höchste Gesamtpunktzahl, insbesondere bei Flüssigkeit und Kohärenz, hatte jedoch Schwächen bei der Informationsdichte und Faktentreue.

Abschließend wird MeetingBank als wertvolles Tool für Forscher beschrieben, das Einblicke in Entscheidungsprozesse von Stadträten bietet. Es wird zur weiteren Nutzung und Erforschung bereitgestellt, um die Entwicklung von Zusammenfassungstechnologien zu fördern.</sample>
    <sample id="239">00:00 - Hallo zusammen, mein Name ist David Vilar und ich werde einen kurzen Überblick über das Papier "Prompting PaLM for Translation: Assessing Strategies and Performance" geben.
00:08 - Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate.
00:12 - PaLM ist ein Sprachmodell mit 540 Milliarden Parametern, das letztes Jahr, im Jahr 2022, vorgestellt wurde.
00:19 - Es wurde auf einer großen Textsammlung von 780 Milliarden Tokens trainiert.
00:26 - Zum Zeitpunkt der Veröffentlichung erreichte es den Stand der Technik in Hunderten von NLP-Aufgaben.
00:30 - In dieser Arbeit präsentieren wir die erste systematische Studie zum Prompting von großen Sprachmodellen für die maschinelle Übersetzung.
00:38 - Wir bewerten die Übersetzungsfähigkeiten solcher Modelle unter Verwendung der besten Praktiken der MT-Gemeinschaft.
00:46 - Dies beinhaltet die Verwendung der neuesten Testdatensätze, um eine Überlappung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden.
00:54 - Und wir vergleichen mit zwei State-of-the-Art-Systemen, also den leistungsstärksten Systemen der WMT-Evaluierung.
01:00 - Wir verwenden modernste neuronale MT-Metriken und zeigen zusätzlich auch Experten-basierte menschliche Evaluationsergebnisse.
01:09 - Schließlich geben wir einige Empfehlungen für Prompt-Auswahlstrategien.
01:13 - Das Prompting hat einen großen Einfluss auf die Leistung der Sprachmodelle für die Übersetzung.
01:21 - Wie wir in einem einfachen Experiment sehen können, bei dem wir One-Shot-Prompting verwenden und zwei verschiedene Prompts für jeden Satz bereitstellen.
01:30 - Die Mehrheit der Sätze (516 von 1000) zeigt einen Unterschied von mehr als 1 BLEURT-Punkt.
01:40 - Und dieser kann in Extremfällen bis zu 40 BLEURT-Punkte betragen. Es ist also wichtig, eine gute Prompt-Strategie zu wählen.
01:48 - In unseren Experimenten haben wir uns für eine 5-Shot-Prompting-Strategie entschieden, bei der wir jeden Satz, den wir dem System bereitstellen, mit der Sprache, in der er sich befindet, markieren.
02:05 - In diesem Beispiel hier, wo wir die Übersetzung vom Deutschen ins Englische durchführen, sind die deutschen Sätze, die Quellsätze, mit "German:" markiert.
02:15 - Und die englischen Übersetzungen mit "English:". Wir haben festgestellt, dass die tatsächliche Form des Promptings keinen großen Einfluss hat.
02:26 - Im Falle von Multi-Shot-Prompting ist es entscheidend für Zero- und One-Shot-Prompting, aber wenn wir, wie in unserem Fall, zu 5-Shot-Prompting übergehen, gibt es kaum einen Unterschied zur tatsächlichen Form des Promptings.
02:37 - Es sind die Beispiele, die den größten Teil des Gewichts tragen.
02:40 - Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Beispielqualität wichtiger ist als die Ähnlichkeit zum Quellsatz.
02:49 - Es ist also wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten der WMT-Evaluierungen oder den Entwicklungsdaten.
03:02 - Die Entwicklungsdaten sind viel sorgfältiger kuratiert und von höherer Qualität als die Trainingsdaten, und die Ergebnisse zeigen eine bessere Leistung bei Verwendung der Entwicklungsdaten.
03:14 - Nichtsdestotrotz haben spezialisierte State-of-the-Art-Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen.
03:22 - Aber PaLM kommt einem kommerziellen System ziemlich nahe. In unserem Fall haben wir uns entschieden, mit Google Translate zu evaluieren.
03:32 - Die Erkenntnisse, die wir aus der menschlichen Evaluierung gewonnen haben, die wir mit dem MQM-Framework durchgeführt haben, besagen, dass die Sprachgewandtheit von PaLM mit dem Stand der Technik vergleichbar ist.
03:43 - Aber der Hauptunterschied liegt in der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassungsfehler.
03:52 - Es scheint also, dass PaLM eine besser klingende Übersetzung produziert, manchmal, indem es Teile des Quellsatzes weglässt, die in der Übersetzung weggelassen werden.
04:07 - Die Kategorie "Stil/Ungeschicklichkeit" für PaLM ist jedoch niedriger als für die State-of-the-Art-Systeme, was ein zusätzliches Signal dafür ist, dass PaLM wirklich flüssige Ausgaben liefert, aber immer noch einige Probleme mit der Genauigkeit hat.
04:26 - Und das war's für diesen wirklich kurzen Überblick. Für weitere Details kommen Sie bitte zur vollständigen Präsentation des Papiers. Vielen Dank.</sample>
    <sample id="240">00:00
Hallo, ich bin Dawei, ein Doktorand an der Universität des Saarlandes in Deutschland. In diesem Video möchte ich unsere neueste Arbeit vorstellen: "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Dies ist eine Gemeinschaftsarbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.

00:21
Ich möchte mit einer kurzen Einführung in schwache Supervision und schwaches überwachtes Lernen beginnen. Bei schwacher Supervision annotieren wir die Daten nicht manuell. Stattdessen annotieren wir die Daten mit schwachen Labeling-Quellen, wie einfachen Heuristikregeln, Wissensbasen oder minderwertigem Crowdsourcing, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwachen Annotationen viel billiger, aber sie sind auch verrauscht, was bedeutet, dass ein bestimmter Teil der Annotationen falsch ist. Wenn wir neuronale Netze direkt auf schwach gelabelten Daten trainieren, neigen die neuronalen Netze dazu, das Label-Rauschen zu lernen und nicht zu verallgemeinern. Beim schwachen überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solchem Label-Rauschen zu trainieren, damit die trainierten Modelle immer noch gut verallgemeinern.

01:14
In jüngster Zeit, in Arbeiten zum schwachen überwachten Lernen (WSL), so steht WSL für schwaches überwachtes Lernen, ist ein häufiger Anspruch, dass die Leute sagen, dass sie Modelle nur auf schwach überwachten Daten trainieren und eine hohe Leistung auf sauberen Testdaten erreichen. Technisch gesehen ist dieser Anspruch nicht falsch, aber es gibt einen Haken: Die Leute nehmen an, dass es einen zusätzlichen sauberen Validierungssatz gibt, der für die Modellauswahl unerlässlich ist. Wir haben uns mit diesem Problem auseinandergesetzt, da dies impliziert, dass zusätzliche manuelle Annotationen beim schwachen überwachten Lernen erforderlich sind. Aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen.

01:58
Die oben erwähnten Fakten veranlassen uns, drei Forschungsfragen zu stellen. Erstens: Ist saubere Validierungsdaten notwendig für WSL? Oder können wir vielleicht stattdessen einen verrauschten Validierungssatz verwenden? Zweitens, wenn saubere Daten erforderlich sind, oder wenn saubere Daten für WSL zwingend erforderlich sind, damit es funktioniert, wie viele saubere Stichproben benötigen wir dann? Schließlich, sollten wir die verfügbaren sauberen Stichproben nur zur Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen?

02:30
Wir haben diese Forschungsfragen in unserer Arbeit beantwortet, und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass WSL-Methoden tatsächlich saubere Validierungsstichproben benötigen, um richtig zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall. Wie in dieser Abbildung gezeigt, wenn keine sauberen Validierungsstichproben vorhanden sind, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus verallgemeinern, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber gelabelte Daten benötigen, um richtig zu funktionieren, und der Annotationsaufwand für die Beschaffung sauberer Validierungsstichproben sollte nicht übersehen werden.

03:13
Unser zweites Ergebnis ist, dass die Erhöhung der Anzahl der sauberen Validierungsstichproben WSL-Ansätzen hilft, eine bessere Leistung zu erzielen, wie in der Abbildung links gezeigt. Typischerweise benötigen wir nur 20 Stichproben pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist nicht das Ende der Geschichte. Denn wenn wir uns sowieso für den Zugriff auf saubere Stichproben entscheiden, dann führt das direkte Training auf ihnen sogar zu einer besseren Leistung. Die rechte Abbildung zeigt den Leistungsunterschied zwischen Fine-Tuning-Ansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur zur Validierung verwenden. Wie wir sehen können, wenn wir 10 Stichproben pro Klasse haben, beginnt das direkte Fine-Tuning, WSL-Ansätze zu übertreffen.

04:07
Schließlich kann die Leistungsverbesserung, die in früheren WSL-Ansätzen behauptet wurde, leicht durch die Möglichkeit erreicht werden, das Fine-Tuning auf den sauberen Validierungsstichproben fortzusetzen. Wie wir den Abbildungen entnehmen können, schneidet das Vanilla-Modell, bezeichnet als FTW, anfänglich schlechter ab als kompliziertere WSL-Methoden wie COSIM. Wenn wir jedoch das Fine-Tuning auf den sauberen Stichproben fortsetzen dürfen, dann schneidet FTW genauso gut ab wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern.

04:51
Zusammenfassend haben wir gezeigt, dass aktuelle WSL-Ansätze saubere, manuell annotierte Stichproben benötigen, damit sie richtig funktionieren. Ihr Leistungsgewinn und ihre Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten lauten wie folgt: Erstens, die Kriterien für die Modellauswahl berichten, zum Beispiel, ob die Modellauswahl auf sauberen Validierungsstichproben erfolgt ist. Zweitens, WSL-Ansätze sollten mit Few-Shot-Lern-Baselines verglichen werden, sowohl für die Arbeit an sauberen Stichproben als auch auf schwach gelabelten Daten. Drittens ist kontinuierliches Fine-Tuning eine einfache, aber starke Baseline, die in zukünftigen Arbeiten in WSL berücksichtigt werden sollte. Schließlich haben wir unseren Code Open Source zur Verfügung gestellt. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte schauen Sie ihn sich an. Vielen Dank und viel Spaß auf der Konferenz.</sample>
    <sample id="241">In diesem Video wird ein "Human-in-the-Loop"-Ansatz zur Erkennung von Fehlinformationen über COVID-19-Behandlungen vorgestellt. Die derzeitigen automatisierten Ansätze haben zwei Hauptmängel: Sie werden unrealistisch bewertet und sind nicht menschenzentriert. Die Forscher schlagen einen neuen Rahmen vor, der diese Mängel beseitigt. Das System ist Ende-zu-Ende, von Tweets bis zu verwertbaren Ergebnissen, und integriert menschliches Feedback in verschiedenen Phasen.

Der Ansatz umfasst zwei Hauptkomponenten: die Erkennung irreführender Behauptungen und die Überprüfung von Richtlinienverstößen. Bei der Erkennung irreführender Behauptungen sammelt das System relevante Tweets, extrahiert mittels eines T5-Modells überprüfbare Behauptungen und rankt diese nach ihrer Trendigkeit zur menschlichen Überprüfung. Bei der Überprüfung von Richtlinienverstößen werden verifizierte, irreführende Behauptungen genutzt, um Tweets zu kennzeichnen, die gegen die Richtlinien sozialer Medien verstoßen, und ein auf BERT basierendes Stanzklassifizierungsmodell wird eingesetzt, um die Haltung des Autors zu nicht genehmigten Behandlungen zu bestimmen. Unterstützende Tweets werden zur menschlichen Überprüfung markiert.

Die Bewertung des Frameworks zeigt seine Wirksamkeit bei der frühzeitigen Erkennung von Behauptungen, bevor sie in den Nachrichten widerlegt werden. Das System identifiziert 124,2 Tweets mit Richtlinienverstößen pro Arbeitsstunde, was die Rolle des Menschen im System unterstreicht. Das Video schließt damit, dass das Framework die komplexe Wechselwirkung zwischen Systemen und menschlichen Moderatoren erfasst und einen konkreten Vergleichsstandard für zukünftige Systeme bietet.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind: 1) Vergleichende Bewertung, bei der menschliche Gutachter wählen, welches von zwei Gesprächen besser ist. 2) Likert-Rating-Bewertung, bei der menschliche Gutachter Gespräche auf einer Likert-Skala bewerten.</sample>
    <sample id="243">There are five authors involved in the work.</sample>
    <sample id="244">Das benötigte Hintergrundwissen ist, dass Richter Fälle vor Gericht entscheiden.</sample>
    <sample id="245">Lining Zhang präsentiert eine zweistufige Pipeline zum Finden von MTurk-Workern mit hoher Übereinstimmung, um die Herausforderungen von automatischen Metriken und unzureichenden Rekrutierungspraktiken zu bewältigen. Die Pipeline beginnt mit Voreinstellungen für die Qualifizierung, die Kriterien wie Standort und Genehmigungsrate festlegen. Anschließend folgt eine Qualifizierungsaufgabe, bei der die Annotatoren sechs Dimensionen von drei Dokumenten (eines mit Aufmerksamkeitskontrolle) und einer Zusammenfassung bewerten müssen. Die Worker werden in vier Typen eingeteilt: GOLD, SILVER, BRONZE und BLOCK, wobei nur GOLD- und SILVER-Worker die Aufgabe bestehen können. Von 200 Teilnehmern qualifizierten sich 26 (8 GOLD, 18 SILVER) für die nächste Phase.

Die zweite Stufe, die Endurance Task, bewertet die Fähigkeit der Annotatoren, hohe Arbeitslasten zu bewältigen. Sie umfasst zehn Human Intelligence Tasks (HITs) mit einem Dokument und vier Zusammenfassungen, die auf Salienz geprüft werden. Von den 26 qualifizierten Workern bestanden 12 (4 GOLD, 8 SILVER) diese Aufgabe und zeigten eine höhere Inter-Annotator-Agreement (IAA) als Experten. Das beste Cohens Kappa lag bei 0,55 (über alle Gruppen) und das beste Krippendorffs Alpha bei 0,443 (GOLD).

Die Referenz-basierte Aufgabe testet die allgemeine Leistung an der wahren Annotationsaufgabe, bestehend aus 30 HITs, einer Referenz und vier Kandidatenzusammenfassungen, um die Informationsabdeckung zu überprüfen. Acht von zwölf Pipeline-Workern beendeten alle HITs, mit einem Cohens Kappa von 0,68 (GOLD) und einem Krippendorffs Alpha von 0,534 (alle Bewertungen). Im Vergleich zu Baseline MTurk-Workern, die mithilfe des statistischen Filters MACE gefiltert wurden, und CloudResearch MTurk-Workern erzielte die Pipeline eine höhere Qualität zu geringeren Kosten und vermied Ressourcenverschwendung.

Die Studie weist Einschränkungen auf, da sie sich auf englische Zusammenfassungen auf der MTurk-Plattform beschränkt, die entwickelten Fragen keine "Wunderlösung" sind und keine Garantie für die Korrektheit der Schulung gegeben werden kann. Zukünftige Arbeiten werden sich auf die Einstellung von hochwertigen Workern (bezüglich Übereinstimmung und Korrektheit) und die Erprobung mehrerer Anwendungen (Aufgaben, Sprachen, Plattformen) konzentrieren.</sample>
    <sample id="246">Ja, der Code ist auf GitHub verfügbar unter [mpoemsl/kitmus](https://github.com/mpoemsl/kitmus).</sample>
    <sample id="247">Jiho Kim von KAIST AI präsentiert „FactKG: Fact Verification via Reasoning on Knowledge Graphs“. Er weist darauf hin, dass es bisher keine Datensätze für Faktenchecks gab, die Wissensgraphen als Beweismittel nutzen. Kim schlägt diese neue Aufgabe vor, da Wissensgraphen eine wertvolle Wissensquelle für zuverlässige und praktische Faktenchecks sind. Für die Zuverlässigkeit können Wissensgraphen leicht mit Behauptungen verknüpft werden. Für die Praktikabilität können Wissensgraphen die Konsistenz zwischen Nutzeräußerungen und dem Wissensgraphen überprüfen.

Der Datensatz FactKG wurde mit Wissensgraphen, insbesondere DBpedia, entwickelt. Er enthält Behauptungen im schriftlichen und umgangssprachlichen Stil für den praktischen Gebrauch und verwendet zwei Labels: „SUPPORTED“ (unterstützt) und „REFUTED“ (widerlegt). FactKG umfasst fünf Arten des Denkens: One-Hop (ein Sprung), Conjunction (Verbindung), Existence (Existenz), Multi-Hop (mehrere Sprünge) und Negation (Verneinung). Um das umgangssprachliche Denken einzubeziehen, wurden zwei Paraphrasierungsverfahren verwendet: ein Kolloquial-Stil-Transfer-Modell und Präsuppositions-Templates.

Baselin-Experimente wurden mit dem Datensatz durchgeführt, wobei sowohl „Claim Only“-Modelle (BERT, BlueBERT, Flan-T5) als auch ein „With Evidence“-Modell (GEAR) verwendet wurden. Alle Baselines übertreffen die Mehrheitsklassen-Baseline von 51%, und das GEAR-Modell, das Graphen-Evidenz verwendet, übertrifft alle anderen Baselines.</sample>
    <sample id="248">Nein. Aus der Präsentation geht hervor, dass die Datensätze und Modelle am stärksten auf englischsprachige Länder und auf Personen mit Hochschulabschluss abgestimmt sind.</sample>
    <sample id="249">Sätze innerhalb der akzeptablen Domain wurden durch das Hinzufügen von Präfix-/Suffix-Adverbien wie "However," oder langen Präfix-Adverbien wie "First and foremost," oder Nebensätzen wie "Regardless of what X thinks about it," oder Zitaten wie "Yesterday, X said," durcheinander gebracht.</sample>
    <sample id="250">Eine dimensionale Bewertung ist eine Beurteilung der Qualität eines Dialogs anhand mehrerer Aspekte wie Relevanz, Konsistenz und emotionales Verständnis.</sample>
    <sample id="251">Die Autoren gehören der University of Science and Technology of China, der Microsoft Research Asia, der Beijing Jiaotong University und Sony AI an.</sample>
    <sample id="252">Die Präsentation stellt "U-CREAT" vor, ein unüberwachtes System zum Abrufen von Präzedenzfällen, das auf Ereignisextraktion basiert. Rechtsexperten verlassen sich traditionell auf ihre Erfahrung, um relevante Präzedenzfälle zu zitieren, doch die steigende Fallzahl macht dies schwierig. U-CREAT soll diesen Prozess durch automatischen Abruf vereinfachen.

Die Hauptbeiträge dieser Arbeit sind das IL-PCR-Datenset, ein neuer Benchmark für das indische Rechtssystem mit 7070 Rechtsfällen, und die U-CREAT-Pipeline selbst. Letztere ist ein ereignisbasierter Ansatz, der keine Überwachung oder domänenspezifische Feinabstimmung erfordert und eine hohe Effizienz und niedrige Inferenzzeit aufweist.

Die Ereignisextraktion basiert auf der Umwandlung von Falldokumenten in eine Sammlung von Ereignissen, die als Subjekt-Verb-Objekt-Tripletts aus Abhängigkeitsgraphen generiert werden. Die U-CREAT-Pipeline umfasst Vorverarbeitung, Abhängigkeitsparsing und Nachverarbeitung, um diese Ereignisse zu extrahieren und dann Übereinstimmungen zwischen Abfrage- und Kandidatendokumenten zu finden.

Die Experimente umfassten zählbare, Transformer-basierte und ereignisbasierte Modelle. Überraschenderweise zeigten auf indischem Rechtstext trainierte Transformer-Modelle eine schlechtere Leistung als erwartet. Ereignisbasierte Modelle übertrafen jedoch alle anderen Methoden und boten eine bessere Leistung und Inferenzzeit als Transformer und Baselines.

Zusammenfassend schlägt U-CREAT ein neues Datenset und eine Pipeline für den Abruf von Präzedenzfällen vor, die eine überlegene Leistung zeigen und keine korpusspezifische Feinabstimmung benötigen, was einen vielversprechenden Weg für die Entwicklung in diesem Bereich ebnet.</sample>
    <sample id="253">In diesem Vortrag wird DisorBERT vorgestellt, ein Modell mit doppelter Domänenanpassung zur Erkennung von Anzeichen psychischer Störungen in sozialen Medien. Psychische Störungen sind psychologische Syndrome, die mit Leid und Behinderung verbunden sind und Denken, Fühlen, Stimmung und Verhalten beeinträchtigen. Da Inhalte in sozialen Medien massiv sind, bieten sie die Möglichkeit, zu untersuchen, wie Menschen mit Schwierigkeiten umgehen. Viele Menschen nutzen Online-Plattformen, um ihren Alltag und wichtige Ereignisse zu veröffentlichen, und einige nutzen die Anonymität dieser Räume, um offen über psychische Probleme zu sprechen und Hilfe zu suchen.
DisorBERT ist ein Sprache-Modell, das durch Domänenanpassung an eine bestimmte Sprache, wie die Sprache von Reddit und psychische Gesundheit, angepasst wird. Das Modell lernt die Sprache der sozialen Medien und spezialisiert sich dann auf die Domäne der psychischen Störungen. Die Präzision und der Recall des Modells werden anhand von eRisk-Datensätzen dargestellt, wobei DisorBERT ein gutes Gleichgewicht zwischen beiden Metriken aufweist. Das Modell wurde auch mit dem Beck's Depression Inventory (BDI-Test) analysiert, einem klinischen Instrument, das aus 21 Elementen besteht und darauf abzielt, die Schwere typischer Depressionssymptome zu identifizieren und zu messen. DisorBERT neigt dazu, Wörter zu erzeugen, die psychische Störungen widerspiegeln, wie z. B. Fokus, Sprechen, Atmen, Schlafen und Essen.
Abschließend zeigt der Vortrag, dass die kombinierte Wirkung der doppelten Domänenanpassung und des geführten Maskierens bei der Erfassung von Anzeichen psychischer Störungen in Interaktionen in sozialen Medien effektiv ist. DisorBERT erzielte bessere Ergebnisse als MentalBERT, ein Modell, das mit einer größeren Datenmenge und einem höheren Verbrauch an Rechenressourcen trainiert wurde. Die Bewertung zeigte ein solides Gleichgewicht zwischen der Identifizierung von Benutzern und deren korrekter Kennzeichnung, wodurch DisorBERT für Anwendungen zur klinischen Erkennung geeignet ist.</sample>
    <sample id="254">Hallo zusammen, ich bin Sun Qi von der Nanjing University of Science and Technology. Heute präsentiere ich unsere Forschungsarbeit über die unsicherheitsgesteuerte Label-Rauschunterdrückung für die Dokumenten-Level-Extraktion von entfernten Beziehungen.

Dokumenten-Level-Beziehungsextraktion (DocRE) zielt darauf ab, Beziehungen zwischen Entitäten in einem Dokument zu extrahieren. Frühere Methoden stützten sich auf große, von Menschen annotierte Korpora, was zeit- und arbeitsintensiv ist. Daher nutzen neuere Arbeiten entfernt überwachte (DS) Daten, um DocRE-Modelle vorzutrainieren.

Wie wir wissen, enthalten DS-Daten verschiedene verrauschte Labels. Aktuelle Bemühungen zur Minderung des Rauschproblems verwenden Pseudo-Labels. Allerdings birgt diese Methode immer noch das Risiko einer Rauscheinführung durch falsch-positive Pseudo-Labels. Wenn wir uns nur auf Pseudo-Labels verlassen, erhalten wir eine extra falsch-positive Beziehung und verlieren die korrekte Beziehung.

Wir schlagen ein DocRE-Framework mit unsicherheitsgesteuerter Label-Rauschunterdrückung vor, das die Label-Qualität von DS-Daten verbessert. Zuerst trainieren wir ein Pre-Denoising RE-Modell mit DS- und human-annotierten Daten, um Pseudo-Labels zu generieren. Da falsch-positive Pseudo-Labels unvermeidlich sind, führen wir eine Unsicherheitsschätzung (UE) ein, um die Zuverlässigkeit von Modellvorhersagen zu bestimmen. Wir schlagen eine instanzbasierte UE vor, um die Unsicherheitsbewertung für überlappende Beziehungen zu erfassen.

Wir beobachten, dass die Verteilung der Unsicherheitsbewertungen für jede Beziehungsklasse unterschiedlich ist. Wir schlagen eine iterative Re-Label-Strategie mit dynamischen Klassenschwellenwerten vor, um Pseudo-Labels mit hoher Unsicherheit zu filtern. Diese mehrphasige Trainingsstrategie ermöglicht es uns, die DS-Daten iterativ zu denoisieren.

Unser Framework übertrifft die bisherigen Basislinien auf zwei öffentlichen Datensätzen. Die Leistung der auf unseren denoisierte DS-Daten trainierten Basislinien ist deutlich verbessert.</sample>
    <sample id="255">Die Form des Prompts ist wichtig für die 0-Shot- und 1-Shot-Prompting-Strategien.</sample>
    <sample id="257">Die Autoren haben vier hochmoderne Open-Domain-Dialogmodelle evaluiert: BART-FID-RAG, Blender2, Emora und Blender-Decode.</sample>
    <sample id="258">This presentation introduces a novel method of evaluating the quality of text using Large Language Models (LLMs), which is called LLM evaluation. Traditionally, human evaluation has been the standard for this task, but it suffers from instability and reproducibility issues. The researchers propose that LLMs, with their ability to follow natural language instructions, could serve as a valuable alternative.

In their experiments, they instructed four different LLMs (T0, InstructGPTs - curie and davinci, and ChatGPT) to rate stories generated by GPT-2 and human writers based on four attributes: grammar, coherence, likability, and relevance. To provide a benchmark, English teachers, considered experts in rating stories and essays, performed human evaluations using the same instructions and stories.

The results showed that human evaluators clearly preferred human-written stories over GPT-2 generated ones. While smaller LLMs (T0 and curie) did not exhibit a significant preference, larger LLMs like davinci and ChatGPT also demonstrated a clear preference for human-written texts, similar to the human experts. This suggests that larger LLMs can indeed serve as a viable alternative to human evaluations in certain contexts. The presentation concludes by inviting interested individuals to read their paper or visit their poster at ACL for more detailed information and answers to further questions regarding LLM evaluation.</sample>
    <sample id="259">In diesem Video präsentiert Yusen Zhang von der Pennsylvania State University die Forschung zur „Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations“, die als XSemPLR bezeichnet wird. XSemPLR zielt darauf ab, die Leistung des Cross-Lingual Semantic Parsing (CLSP) zu verbessern, indem es das Problem der eingeschränkten Sprachabdeckung und der geringen Leistung der vorhandenen Modelle angeht. Die Autoren schlagen ein vereinheitlichtes XSemPLR-Dataset vor, das neun Datensätze aus verschiedenen Domänen umfasst, die fünf CLSP-Aufgaben, acht Bedeutungstransparenzen und 22 Sprachen aus 15 Sprachfamilien umfassen. Es wurden sechs verschiedene Einstellungen für das Training und die Evaluation des Benchmarks verwendet, darunter Translate-Test, monolinguale Modelle, monolinguale Few-Shot-Modelle, mehrsprachige Modelle, Cross-Lingual Zero-Shot-Transfer und Cross-Lingual Few-Shot-Transfer. Die Ergebnisse zeigen, dass mehrsprachige vortrainierte Encoder-Decoder-Modelle (wie mT5) am besten abschneiden, während der Englisch-Few-Shot-Transfer die beste Leistung im Hinblick auf die Übertragbarkeit der Modelle erzielt. Es wurde auch festgestellt, dass das Vortraining auf englischen Datensätzen die Leistung des Few-Shot-Lernens in anderen Zielsprachen erheblich steigern kann. Mehrsprachige große Sprachmodelle (wie CodeX und BLOOM) sind jedoch noch unzureichend für CLSP-Aufgaben. Schließlich wird festgestellt, dass die Übertragung zwischen Chinesisch und Englisch die größte Leistungslücke aufweist, während die Übertragung zwischen Deutsch und Englisch die kleinste aufweist.</sample>
    <sample id="260">An der Arbeit sind 9 Autoren beteiligt.</sample>
    <sample id="261">Ein guter Planer sollte Skripte schreiben, die sowohl **vernünftig** als auch **getreu den Einschränkungen** sind.</sample>
    <sample id="262">Es sind 9 Autoren an der Arbeit beteiligt.</sample>
    <sample id="263">Die Präsentation beginnt mit einer Einführung in das In-Context-Learning, eine beliebte Methode zur Nutzung großer Sprachmodelle. Es wird darauf hingewiesen, dass die Ergebnisse des In-Context-Learnings aufgrund verschiedener Designentscheidungen, wie der Wahl und Reihenfolge von Beispielen, oft instabil sind. Frühere Arbeiten haben gezeigt, dass diese Designentscheidungen Vorurteile in die Vorhersagen des Modells einführen können.

Die Präsentation stellt eine Typologie von Label-Bias im In-Context-Learning für Klassifizierungsaufgaben vor, die drei Arten von Bias definiert: Vanilla-Label-Bias (unkontextuelle Präferenz für Label-Namen), Context-Label-Bias (Effekte aus dem Kontext) und Domain-Label-Bias (Effekte aus dem Task-Korpus).

Ein neues Phänomen, der Domain-Label-Bias, wird entdeckt. Experimente zeigen, dass zufällig ausgewählte In-Domain-Wörter die Vorhersagen des Modells stark beeinflussen können, während zufällige englische Wörter dies nicht tun. Es wird festgestellt, dass große Sprachmodelle (LLMs) bei Aufgaben mit geringem Domain-Label-Bias gut abschneiden, aber bei Aufgaben mit hohem Domain-Label-Bias die Leistung auf Zufallsniveau sinkt, selbst bei Verwendung früherer Kalibrierungsmethoden.

Um alle Arten von Label-Bias zu mildern, wird eine Domain-Context-Kalibrierung (DC) vorgeschlagen. DC verwendet zufällige In-Domain-Wörter als „inhaltsfreien“ Text, um den Bias des Modells zu schätzen und die ursprünglichen Vorhersagen zu kalibrieren. Diese Methode mildert nicht nur den Vanilla- und Context-Label-Bias, sondern auch den Domain-Label-Bias.

Die Ergebnisse zeigen, dass DC das In-Context-Learning im Allgemeinen verbessert, insbesondere bei Aufgaben mit hohem Domain-Label-Bias, und zu besseren Entscheidungsgrenzen führt. Diese Ergebnisse gelten auch für größere Modelle wie GPT-3. Eine Ablationsstudie zeigt, dass vordefinierte inhaltsfreie Token voreingenommen sein können, die Verwendung nur eines Tokens suboptimal ist und die Verwendung von zufälligen In-Domain-Wörtern den Domain-Label-Bias entfernt.</sample>
    <sample id="264">Die Präsentation stellt TAVT vor, ein übertragbares Modell für die Audio-Video-Texterzeugung. Die Beschränkungen und Herausforderungen, die sich aus der schwierigen und kostspieligen Datenannotation sowie der unvermeidlichen Leistungsverschlechterung aufgrund von multimodalen Domänenverschiebungen ergeben, werden hervorgehoben. Das Modell besteht aus drei Teilen: einem Audio-Visual Meta-Mapper Network (AVMM), einem Audio-Visual Encoder &amp; Language Model Generator (AVE &amp; LMG) und einem Counterfactual Contrastive Learning (CCL) Modul. Das AVMM-Netzwerk zielt darauf ab, verschiedene visuelle Konzepte domänenübergreifend in einem einheitlichen auditiven semantischen Raum abzubilden, indem es lernbare Token, sogenannte visuelle Präfixe, verwendet und die Kohärenz zwischen den rekonstruierten Audio- und visuellen Inhalten optimiert. Das AVE- &amp; LMG-Modul verwendet einen transformerbasierten Encoder und Generator, der einen Alpha-Wert verwendet, um den Beitrag jeder Modalität zu jedem Wort zu bewerten. Für das Training werden zwei gegenfaktische kontrastive Lernverluste (verteilungsbasiert und abhängigkeitsbasiert) eingeführt, um eine feingranulare Überwachung zu ermöglichen und die visuell-textliche Ausrichtung zu optimieren, ohne auf die Qualität zufällig ausgewählter negativer Beispiele angewiesen zu sein. Die Trainingsdetails umfassen einen Lern-Meta-Lernansatz, bei dem das Modell auf einer Teilmenge von Domänen trainiert und dann auf eine neue Zieldomäne angewendet wird. In den Experimenten werden zwei Benchmarks auf der Grundlage von MSVD und MSR-VTT+ für Cross-Dataset- und Cross-Domain-Szenarien verwendet. Die Ergebnisse zeigen, dass TAVT alle verglichenen Methoden übertrifft und auf beiden Benchmarks eine große Leistungssteigerung erzielt. TAVT schneidet auch in ressourcenarmen Domänen gut ab.</sample>
    <sample id="265">Der/Die Referent*in heißt Vasudha Varadarajan.</sample>
    <sample id="266">Die Autoren gehören der Universität Warschau an.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Auslassungsfehler (omission errors).</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute erzählen wir Ihnen alles über ABC-Eval, einen neuen dimensionalen Ansatz zur Bewertung von Konversations-KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Nehmen wir an, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie gut es im Vergleich zum aktuellen Stand der Technik abschneidet. Die gängige Praxis ist die Verwendung von menschlicher Bewertung, z. B. indem menschliche Richter gebeten werden, aus zwei Gesprächen auszuwählen, welches besser ist, oder um Gespräche auf einer Likert-Skala zu bewerten. Diese Ansätze eignen sich gut, um eine ganzheitliche Bewertung der gesamten Dialogqualität zu liefern, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Ein Ansatz besteht darin, menschliche Richter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, z. B. die Relevanz der Bot-Antworten, unter Verwendung bestehender komparativer oder Likert-Skalen-Methoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem explizit annotiert wird, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, z. B. die Reaktion mit irrelevanten Informationen oder das Selbstwidersprechen. Wir nennen diesen Ansatz "Annotating Behaviors in Chat" oder kurz ABC-Eval. Wir haben diese Methode entwickelt, um Chat-Modellverhaltensweisen umfassend abzudecken, die in der jüngsten Literatur als relevant für die Chat-Qualität vorgeschlagen wurden. ABC-Eval ist in der Lage, die Raten zu messen, mit denen Chat-Modelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC-Eval die Anzahl der Züge, in denen ein Chat-Modell seinen Partner ignoriert oder etwas Irrelevantes sagt, sich selbst oder seinen Partner widerspricht, falsche Fakten halluziniert oder den gesunden Menschenverstand verletzt und wann das Modell Empathie zeigt oder nicht. Um festzustellen, welche Art der Bewertung am effektivsten ist, haben wir vier hochmoderne Chat-Modelle ausgewählt und sie in 100 Mensch-Bot-Gesprächen pro Modell mit ABC-Eval bewertet. Zum Vergleich haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet: Likert-Ratings auf Zug-Ebene, Likert-Ratings auf Dialog-Ebene und paarweisen Vergleichen auf Dialog-Ebene. Für jede der bestehenden Methoden haben wir Bewertungen für acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies die Standardpraxis zur Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist. Aus unseren Analysen dieser Bewertungsergebnisse haben wir festgestellt, dass die ABC-Eval-Verhaltenslabels insgesamt zuverlässiger sind als Labels, die mit bestehenden Methoden gesammelt wurden, gemessen am Inter-Annotator-Agreement für 100 doppelt gelabelte Gespräche. Darüber hinaus sind ABC-Eval-Labels prädiktiver für die Gesamtqualität des Gesprächs als Metriken, die mit bestehenden Methoden erstellt wurden, wie diese einfache lineare Regressionsanalyse zeigt. Zum Beispiel können Sie sehen, wie die Messung des Anteils der Züge mit Selbst- und Partnerwidersprüchen 5 % bzw. 10 % der Gesprächsqualität erklärt, während die durchschnittlichen Likert-Konsistenzwerte nur 4 % oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chat-Qualität erfasst, indem wir eine schrittweise lineare Regression verwendet haben. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken einzeln entfernen, führt dies in den meisten Fällen zu einem erheblichen Informationsverlust über die Qualität. Andererseits erklärt die Kombination aller Likert-Metriken auf Zug-Ebene viel weniger von der Qualität, und nur wenige dieser Metriken enthalten einzigartige Informationen. Diese zuverlässigen, informativen und eindeutigen ABC-Eval-Metriken ermöglichen es uns, Konversations-KI mit einer höheren Auflösung zu bewerten, als dies mit früheren Methoden möglich war. Sie können in den Ergebnissen unseres Experiments sehen, dass noch mehrere Herausforderungen bestehen und präzise quantifiziert wurden. Zum Beispiel haben die von uns getesteten Bots in etwa 20 % ihrer Antworten Verstöße gegen den gesunden Menschenverstand. Sie produzieren irrelevante Informationen in etwa 15 % der Antworten und widersprechen sich selbst oder ihrem Partner in etwa 10 % der Fälle. Angesichts des rasanten Fortschritts in diesem Bereich könnten viele dieser Fehlerraten in neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, sinken. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmetriken für den Vergleich von Modellen zu verfolgen. Wir hoffen, dass ABC-Eval von anderen in diesem Bereich als bedeutsamer Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf zu sehen, wie sich Konversations-KI in den kommenden Monaten und Jahren entwickeln wird. Vielen Dank fürs Zuschauen!</sample>
    <sample id="270">Die Autoren gehören der Emory University an.</sample>
    <sample id="271">CFT steht für Continuous Fine-Tuning.</sample>
    <sample id="272">An der Arbeit sind 7 Autoren beteiligt.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit mit dem Titel "When Does Translation Require Context? A Data-driven, Multilingual Exploration" vorstellen. Diese Arbeit entstand in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig.

Viele Übersetzungen hängen vom Kontext ab. Wie würden wir zum Beispiel "Maulwurf" in diesem Satz übersetzen? Nun, wenn der vorherige Satz lautete: "Die Dinge könnten gefährlich werden, wenn die Minister es herausfinden", dann bezieht sich "Maulwurf" auf einen Spion. Wenn der vorherige Satz jedoch lautete: "Könnte es etwas Ernstes sein, Doktor?", dann bezieht sich "Maulwurf" auf ein Muttermal. Je nach Kontext ändert sich die Bedeutung des Wortes und damit auch seine Übersetzung.

Es ist jedoch schwierig zu beurteilen, wie gut Modelle solche Fälle übersetzen können. Erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, was Korpus-Metriken wie BLEU ungeeignet macht, um diese Übersetzungen zu erfassen. Und einige Leute haben gezielte Bewertungen für kontextabhängige Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und eine begrenzte Anzahl von Sprachen, da sie normalerweise auf Domänenwissen und menschliche Kuratierung angewiesen sind.

In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut gehen Modelle mit kontextabhängigen Übersetzungen um?

Um die erste Frage zu beantworten, haben wir zunächst gemessen, wie sehr ein Wort bei der Übersetzung vom Kontext abhängt. In unserer vorherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungsmodelle eingeführt. Und dies geschieht, indem gemessen wird, wie viele Informationen der Kontext C über das Ziel Y liefert, wenn die Quelle X gegeben ist. Man kann CXMI als den Informationsgewinn betrachten, der durch die Bereitstellung von Kontext für das Modell erzielt wird.

In dieser Arbeit erweitern wir CXMI zu Pointwise CXMI, das die Kontextnutzung auf Satzebene oder auf Wortebene messen kann. Wir können Wörter, die einen hohen P-CXMI-Wert haben, als Wörter betrachten, die Kontext für die Übersetzung benötigen.

Nun analysieren wir Wörter mit hohem P-CXMI, um Muster zwischen diesen Wörtern zu finden. Und wir führen unsere Analyse an Transkripten von TED-Talks durch, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zuerst betrachten wir die POS-Tags, die hohe mittlere P-CXMI-Werte haben. Und dies ermöglicht uns, zum Beispiel Dualpronomen im Arabischen zu finden, die relativ hohe P-CXMI-Werte haben. Und dies kann dadurch erklärt werden, dass das Englische keine Dualpronomen hat, so dass man Kontext benötigt, um zu bestimmen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Und ähnlich finden wir, dass bestimmte Sprachen auch Kontext benötigen, wenn wir die richtige Verbform wählen wollen. Dann betrachten wir Vokabeln, die einen hohen P-CXMI-Durchschnitt über alle ihre verschiedenen Vorkommen hinweg haben. Und dies hilft uns, Fälle wie diesen hier zu identifizieren, wo man im Chinesischen Kontext benötigt, um Eigennamen zu übersetzen, um sicherzustellen, dass man dieselbe Übersetzung innerhalb des Dokuments verwendet. Und ähnlich finden wir, dass Kontext unterstützt wird, um in der richtigen Formalität zu übersetzen. Und schließlich betrachten wir einzelne Token, die einen hohen P-CXMI-Wert haben. Und dies ermöglichte uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern eher in der Satzstruktur ausgedrückt werden, wie z.B. die Ellipsenauflösung.

Nun verwenden wir unsere Ergebnisse aus unserer Analyse, um einen Benchmark für die Übersetzung auf Dokumentebene zu entwerfen. Für jedes der fünf von uns identifizierten Diskursphänomene erstellen wir Tagger, um Wörter, die zum Phänomen gehören, automatisch zu identifizieren. Und wir nennen unseren Tagger den Multilingual Discourse-Aware oder MuDA Tagger. Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene aufweisen.

Wir verwenden dann den MuDA Tagger, indem wir den Tagger auf einen parallelen Korpus anwenden, den wir zur Bewertung verwenden möchten. Und wir wenden unsere Übersetzungsmetriken unserer Wahl auf die kontextabhängigen Beispiele an, die der MuDA Tagger identifiziert hat.

Und schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle für die maschinelle Übersetzung auf Dokumentebene zu bewerten. Zuerst, wenn wir Korpus-Metriken verwenden, finden wir für BLEU, dass kontextunabhängige Modelle die beste Leistung haben. Wenn wir aber COMET verwenden, leisten kontextbezogene Modelle am besten. Und wenn wir die Wort-F-Metrik verwenden, haben Modelle mit oder ohne Kontext vergleichbare Leistungen. Dies zeigt erneut, dass es schwierig ist, das beste Übersetzungssystem auf Dokumentebene zu bestimmen, wenn wir allein Korpus-Metriken verwenden.

Nun verwenden wir den MuDA-Benchmark, um Modelle zu bewerten. Und wir finden, dass kontextbezogene Modelle für bestimmte Diskursphänomene wie Formalität und lexikalische Kohäsion deutlich genauer sind als Modelle, die keinen Kontext verwenden. Aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext verwenden, bei anderen Phänomenen wie Ellipsen, Pronomen und Verbformen. Dies deutet darauf hin, wo wir mehr Fortschritte für die Übersetzung auf Dokumentebene sehen müssten. Wir haben auch verschiedene kommerzielle Systeme verglichen, und unser Benchmark zeigt, dass DeepL für die Übersetzung auf Dokumentebene meist genauer ist als Google Translate.

Zusammenfassend haben wir eine datengesteuerte Analyse über 14 Sprachpaare hinweg durchgeführt, um zu identifizieren, wann Übersetzungen Kontext erfordern. Und wir verwenden unsere Erkenntnisse, um einen Benchmark für die maschinelle Übersetzung auf Dokumentebene zu erstellen, der uns helfen kann zu identifizieren, welche Diskursphänomene Modelle gut oder schlecht handhaben können und welche Übersetzungssysteme für die maschinelle Übersetzung auf Dokumentebene gut sind. Vielen Dank für Ihre Aufmerksamkeit, wir sehen uns in Toronto!</sample>
    <sample id="274">Der Referent heißt Yusen Zhang.</sample>
    <sample id="276">Das Ziel dieser Arbeit ist es, die automatische Bewertung von Metriken für maschinelle Übersetzungen von Englisch in fünf indische Sprachen zu verbessern. Derzeit gibt es einen Mangel an Studien zur Bewertung von Übersetzungen in diese Sprachen, was sich auf die Qualität von Übersetzungen auswirkt. Um diese Lücke zu schließen, wurde ein Datensatz namens IndicMT Eval entwickelt, der zur Meta-Evaluierung von Metriken für maschinelle Übersetzungen für indische Sprachen verwendet werden kann.

Der Datensatz umfasst 200 englische Sätze aus dem Flores-Datensatz, die mit sieben verschiedenen maschinellen Übersetzungssystemen in Tamil, Malayalam, Hindi, Marathi und Gujarati übersetzt wurden. Dies führt zu insgesamt 7.000 übersetzten Sätzen. Anschließend wurden diese Übersetzungen von muttersprachlichen, zweisprachigen Experten anhand des MQM-Frameworks bewertet, das Fehler nach Typ und Schweregrad kategorisiert und eine Gesamtpunktzahl liefert.

Die Analyse der Korrelationen zwischen den Metrik-Scores und den menschlichen Bewertungen ergab, dass CHRF++ unter den Overlap-basierten Metriken die höchste Korrelation aufwies, während LabSE-Embeddings bei den Embedding-basierten Metriken die besten Ergebnisse erzielten. COMET-Varianten zeigten die höchsten Gesamtkorrelationen. Die Metrik-Scores wiesen jedoch eine verzerrte Verteilung auf, was ihre Interpretation erschwert.

Um die Leistung zu verbessern, wurde ein Modell namens IndicCOMET entwickelt, das die COMET-Metrikvarianten mithilfe der IndicMT Eval-Annotationen feinabstimmt. IndicCOMET übertraf die COMET-Baselines in drei von fünf Sprachen und zeigte eine höhere Gesamtkorrelation. Bei Null-Shot-Tests schnitt IndicCOMET ebenfalls besser ab und zeigte eine höhere Robustheit im Vergleich zu seinem COMET-Pendant.</sample>
    <sample id="277">Die neue Methode hat keinen Namen.</sample>
    <sample id="278">Die Autoren beschreiben die Methode der "markierten Wörter" als einen Ansatz, der sich auf das soziolinguistische Konzept der Markiertheit stützt, wonach unmarkierte Gruppen als Standard betrachtet werden und markierte Gruppen sich von diesem Standard unterscheiden. Im Rahmen ihrer Methode definieren sie zunächst unmarkierte und markierte Gruppen. Anschließend verwenden sie gewichtete Log-Odds-Verhältnisse, um die Top-Wörter zu identifizieren, die für jede markierte Gruppe charakteristisch sind und sie von den unmarkierten Gruppen unterscheiden.</sample>
    <sample id="279">Die Autoren gehören der University of Washington, der Carnegie Mellon University und der Peking University an.</sample>
    <sample id="280">Die Präsentation stellt MultiEMO vor, ein Aufmerksamkeits-basiertes, korrelationsbewusstes multimodales Fusionsframework für Emotionserkennung in Gesprächen (ERC). MultiEMO befasst sich mit drei Hauptproblemen bestehender ERC-Ansätze: die unzureichende Ausnutzung multimodaler Informationen, unbefriedigende Leistung bei Minderheiten-Emotionsklassen und die Schwierigkeit, semantisch ähnliche Emotionen zu unterscheiden.

Die Hauptbeiträge von MultiEMO sind:
1.  **VisExtNet**: Ein neuartiger visueller Feature-Extraktor, der visuelle Hinweise von Gesprächspartnern effektiv erfasst, ohne redundante Szeneninformationen zu modellieren. Dadurch werden unnötige Informationen vermieden und ein falsches Verständnis der Emotionen des Sprechers verhindert.
2.  **MultiAttn**: Ein multimodales Fusionsmodell, das auf bidirektionalen Multi-Head-Cross-Attention-Layern basiert. Es modelliert erfolgreich die komplizierten Korrelationen zwischen textuellen, auditiven und visuellen Modalitäten, um komplementäre Informationen zu integrieren.
3.  **Sample-Weighted Focal Contrastive (SWFC) Loss**: Ein Verlust, der eine höhere Bedeutung auf schwer zu klassifizierende Minderheitenklassen legt und Stichprobenpaare mit unterschiedlichen Emotionsbezeichnungen gegenseitig ausschließt, um inter-Klassen-Distanzen zu maximieren. Dadurch können semantisch ähnliche Emotionen besser unterschieden werden.

Experimentelle Ergebnisse auf den Benchmark-Datasets MELD und IEMOCAP zeigen, dass MultiEMO eine Spitzenleistung erzielt, mit signifikanten Verbesserungen bei Minderheiten- und semantisch ähnlichen Emotionsklassen. Eine Fallstudie verdeutlicht die Fähigkeit von MultiEMO, die Asynchronisation emotionaler Tendenzen aus verschiedenen Modalitäten zu bewältigen.

MultiEMO hat jedoch auch Einschränkungen, wie die mangelnde Unterscheidung zwischen Sprechern und irrelevanten Personen in der Szene durch VisExtNet, den Bedarf eines großen Batch-Sizes für den SWFC-Verlust bei MELD und eine immer noch schlechtere Leistung bei Minderheiten-Emotionen im Vergleich zu Mehrheitsklassen.</sample>
    <sample id="281">This presentation introduces "When Does Translation Require Context? A Data-driven, Multilingual Exploration," a work by Kayo Yin and collaborators from Carnegie Mellon University, Técnico Lisboa, BAIR, and Unbabel. The core idea is that translation often depends on context, as exemplified by the word "mole" which can mean a spy or a birthmark depending on the surrounding text.

The challenge lies in evaluating context-dependent translation because only a small portion of words require context, rendering corpus-level metrics like BLEU ineffective. Existing targeted evaluation methods are limited in discourse phenomena and languages due to their reliance on domain knowledge and human curation.

To address this, the researchers posed two research questions: (1) When does translation require context? and (2) How well do models handle context-dependent translations? To answer the first, they introduced Pointwise Conditional Cross-Mutual Information (P-CXMI), a metric to measure context usage at sentence and word levels. High P-CXMI words indicate a strong dependency on context for accurate translation.

Thematic analysis of high P-CXMI words in TED talk transcripts (translated from English to 14 languages) revealed several discourse phenomena requiring context: pronouns (especially dual pronouns in Arabic), verb forms, lexical cohesion (e.g., consistent proper noun translation in Chinese), formality, and ellipsis resolution.

For the second question, they developed the Multilingual Discourse-Aware (MuDA) tagger to automatically identify words pertaining to these phenomena. This tagger, when applied to parallel corpora, allows for the creation of a dataset-agnostic benchmark for document-level machine translation (MT).

Evaluation using MuDA showed that context-aware models perform significantly better on phenomena like formality and lexical cohesion, but not on ellipsis, pronouns, or verb forms, indicating areas for future MT improvement. Furthermore, DeepL was found to outperform Google Translate on most phenomena and language pairs for document-level translation.</sample>
    <sample id="282">StoryTrans ist ein neues Modell für den nicht-parallelen Textstiltransfer auf Story-Ebene, das die linguistischen Entscheidungen des Autors auf der Diskursebene nachbildet, ein entscheidender Aspekt für die Emulation des Autorenstils. Das Modell geht die Herausforderung an, Stilmerkmale von den Diskurzdarstellungen zu entwirren, indem es einen Adversarial-Trainingsrahmen verwendet. Um die Inhaltsbewahrung zu verbessern, gliedert StoryTrans die Generierung in zwei Stufen: Zuerst werden Stil-spezifische Inhalte im Quelltext maskiert, dann wird der maskierte Text mit einem normalisierten Stileinbettung kombiniert, um den Text im Zielstil zu generieren. StoryTrans lernt zudem die Satz-Ebenen-Abhängigkeiten zu erfassen. Experimente auf chinesischen und englischen Datensätzen zeigen die Überlegenheit des Modells gegenüber den Baselines in Bezug auf Stilkontrolle und Inhaltsbewahrung. Stil-Visualisierungen zeigen, dass die von StoryTrans übertragenen Texte gut mit den goldenen Texten im Stilmerkmalsraum übereinstimmen.</sample>
    <sample id="283">Prag</sample>
    <sample id="284">The speaker introduces FUSIE, a novel fuzzy span mechanism for enhancing universal information extraction. The current UIE models excessively rely on the precise boundary of annotated spans, while in reality, these boundaries are often ambiguous. FUSIE proposes that span boundaries learned by the model should be fuzzy instead of precise. Also, basic transformers focus on global features, which ignore the prior hypothesis that spans have limited length. The speaker proposes that the attention used for span extraction decisions should be adaptive rather than static.

To model the fuzzy span boundary, the speaker proposes Fuzzy Span Loss, which represents the target boundary as a continuous distribution of correct probability within a specific range. Through a sampling function, they convert continuous boundary distribution into a group of discrete values for calculating the loss. Additionally, they propose a Fuzzy Span Attention as a mask function to trim the attention distribution, so the model can obtain a more reasonable attention distribution for span extraction.

The speaker presents the overall structure of the FUSIE model, which incorporates a Fuzzy Span Attention layer at the top level to guide the model’s decision process without affecting its text encoding capability. The results on named entity recognition show that FUSIE-base significantly improves performance compared to UIE-base. The model is also easier to learn universal attention spans on small-scale datasets, resulting in a more significant improvement. FUSIE also achieves state-of-the-art results on relationship extraction datasets and on ASTE task.

The ablation study demonstrates that FSA improves convergence speed by guiding the model to obtain a reasonable attention distribution, and FSL enables the model to fully utilize annotation information and obtain a greater information extraction capability. The combined effect of the two yields a greater enhancement. Finally, the visualization of FSA shows that the model focuses on semantic information within a limited range of preceding tokens rather than on global representation, which meets their expectations.

In conclusion, FUSIE proposes a novel Fuzzy Span Loss that alleviates the model’s reliance on span boundaries. It also utilizes efficient Fuzzy Span Attention, which adaptively adjusts the attention span to guide the proper distribution of attention. FUSIE achieves excellent results in a wide range of information extraction tasks, including NER, RE, and ASTE.</sample>
    <sample id="285">In der Videopräsentation „Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework“ stellt Mingqi Gao von der Peking University eine Arbeit vor, die sich mit der Korrektur von Faktenfehlern in Dialogzusammenfassungen befasst. Gao beginnt mit der Feststellung, dass von Modellen generierte Zusammenfassungen und sogar Referenzzusammenfassungen weiterhin Faktenfehler enthalten. Die bestehenden Lösungen versuchen entweder, zusammenfassende Modelle so zu gestalten, dass sie genauer sind, oder Fehler mit Korrekturmodellen (FEC-Modelle) nachträglich zu beheben.

Gao kritisiert die derzeitigen Bewertungsmethoden für FEC-Modelle, da sie eine einzige, vage Gesamtpunktzahl liefern und die Unterscheidung zwischen der Generierung einer Zusammenfassung und ihrer Fehlerkorrektur verwischen. Der Kern der Arbeit ist die **manuelle Annotation von Referenzkorrekturen** für von Modellen generierte Zusammenfassungen, die Faktenfehler enthalten. Diese manuellen Korrekturen müssen fehlerfrei, flüssig und nicht redundant sein und als wertvolle Trainingsdaten dienen und eine genauere Bewertung ermöglichen.

Um Faktenfehler zu kategorisieren, wird eine neue Taxonomie eingeführt. Sie unterteilt Fehler in **inhaltsbasierte** (basierend auf Wortarten und Abhängigkeiten, wie Entitäts-, Attribut-, Prädikat- und Verweisfehler) und **formbasierte** Kategorien (basierend auf Operationsarten wie Ersetzung, Einfügung und Löschung). Die Bewertung nutzt einen an ERRANT angelehnten Framework, der die Korrekturen auswertet, indem er die ursprüngliche und die korrigierte Zusammenfassung abgleicht, die Fehler klassifiziert und die Ergebnisse vergleicht, um die Leistung zu beurteilen.

Wichtige Ergebnisse der Studie sind:
* Das Training von FEC-Modellen mit Referenzzusammenfassungen aus Dialogzusammenfassungsdatensätzen führt zu den besten Ergebnissen, obwohl die Faktizitätsmetriken unzuverlässig sind, was die Notwendigkeit einer Änderung der Bewertungsmethoden verdeutlicht.
* Die Einbeziehung von manuell korrigierten Zusammenfassungen in das Training verbessert die Leistung von FEC-Modellen, was darauf hindeutet, dass die Kombination von menschlich annotierten und synthetischen Daten ein vielversprechender Ansatz ist.
* Aktuelle FEC-Modelle haben Schwierigkeiten bei der Korrektur von Faktenfehlern, die durch Hinzufügen von Informationen entstehen, und sind nicht in der Lage, Attribute-, Modalitäts- und Verweisfehler zu beheben.</sample>
    <sample id="286">Die Referenten sind James Finch und Sarah Finch.</sample>
    <sample id="287">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="288">BLiMP und SyntaxGym sind Datensätze, die zum Testen syntaktischer Phänomene verwendet werden können.</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage lauten:
- FTw
- BOND
- COSINE
- MLC
- L2R</sample>
    <sample id="291">Das Modell wird anhand von 11 nachgelagerten medizinisch ausgerichteten Aufgaben evaluiert. Diese Aufgaben umfassen:
* benannte Entitätserkennung
* Klassifikation
* Part-of-Speech-Tagging
* Frage-Antwort</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit **generischen französischen Daten** trainiert.</sample>
    <sample id="295">Der Referent heißt Adam Przepiórkowski.</sample>
    <sample id="296">Dieses Video stellt eine Studie vor, die sich mit der Erkennung von Ironie in englischem Text befasst, insbesondere mit einem auf Perspektiven basierenden Ansatz. Ziel der Studie ist es, zu verstehen, wie unterschiedliche Perspektiven die Wahrnehmung von Ironie beeinflussen können. Dazu wurde ein Korpus namens EPIC (English Perspectivist Irony Corpus) erstellt.

Der EPIC-Korpus enthält etwa 3000 Text-Antwort-Paare, die zwischen Januar 2020 und Juni 2021 von den sozialen Medien Reddit und Twitter gesammelt wurden. Diese Paare stammen aus fünf verschiedenen englischen Varietäten: Vereinigtes Königreich, Vereinigte Staaten, Irland, Australien und Indien. Die Daten wurden dann von 74 Annotatoren annotiert, die über die Crowdsourcing-Plattform Prolific rekrutiert wurden. Jeder Annotator erhielt etwa 200 Texte zur Annotation, wobei jeder Text durchschnittlich 5 Annotationen erhielt. Die Annotatoren wurden nach Geschlecht, Altersgruppe und Herkunftsland ausgewählt, um eine ausgewogene Verteilung zu gewährleisten.

Die Ergebnisse der Studie zeigen, dass es Unterschiede in der Inter-Annotator-Übereinstimmung (IAA) zwischen verschiedenen demografischen Gruppen gibt. Beispielsweise gab es Unterschiede bei der IAA nach Geschlecht, Altersgruppe, Nationalität und Ethnizität. Die Forscher trainierten außerdem perspektivbezogene Modelle, die darauf abzielten, die unterschiedlichen Interpretationen von Ironie durch verschiedene Gruppen zu erfassen. Diese Modelle neigten dazu, Entscheidungen mit geringerer Unsicherheit zu treffen als Standardmodelle.

Interessanterweise wurde die höchste Variation in der Ironie-Wahrnehmung zwischen unmittelbar aufeinanderfolgenden Generationen (z. B. Boomer vs. Gen Y) und zwischen dem Vereinigten Königreich und Irland festgestellt. Dies deutet darauf hin, dass kulturelle und Altersunterschiede eine Rolle bei der Interpretation von Ironie spielen. Die Studie unterstreicht die Notwendigkeit, bei der Entwicklung von NLP-Modellen für die Ironie-Erkennung einen Perspektiven-Ansatz zu verfolgen, um nuancierte menschliche Interpretationen besser zu erfassen.</sample>
    <sample id="297">Der Vortragende spricht über die Arbeit, die sie an Dogwhistles geleistet haben, insbesondere über ihre Definition als eine Art Sprache, die eine offene Nachricht an eine Gruppe und eine versteckte Nachricht an eine andere Gruppe sendet, die oft kontrovers, unanständig oder aufrührerisch ist. Das Team diskutierte auch ihre Dogwhistle-Glossare, die über 340 Begriffe und Symbole enthalten, die aus einem breiten Spektrum von Online-Quellen stammen. Das Glossar des Teams enthält Dogwhistles, die transphob, rassistisch und antisemitisch sind. Sie kategorisieren die Dogwhistles nach ihrem Register, d. h. ob sie informell oder formell sind, nach ihrer Art, d. h. ob sie ein Persoan-Signal oder ein Persoan-Signal mit einer hinzugefügten Implikatur enthalten, und nach der Persönlichkeit des Sprechers, d. h. ob der Sprecher beispielsweise ein Antisemit oder ein Transphober ist.
Das Team führte eine Fallstudie über die Verwendung von Dogwhistles in historischen politischen Reden in den USA durch und stellte fest, dass die Häufigkeit der Verwendung von Dogwhistles seit der Bürgerrechtsbewegung zugenommen hat. Sie stellten auch fest, dass Dogwhistles mit der Zeit zunehmend mit Konservatismus assoziiert wurden. Außerdem untersuchten sie die Fähigkeit von Sprachmodellen, Dogwhistles zu identifizieren, und stellten fest, dass GPT-3 etwa 45 % der Dogwhistles in ihrem Glossar identifizieren konnte, und dass die Leistung bei der Definition von Dogwhistles und der Angabe eines geheimen Hinweises zunahm. Schließlich führte das Team eine Fallstudie zur Erkennung von Toxizität durch, bei der festgestellt wurde, dass Hass-Sätze mit Dogwhistles als weniger toxisch eingestuft werden als Sätze, die offen beleidigende oder standardmäßige Gruppenbezeichnungen verwenden.</sample>
    <sample id="298">Ein Experiment, bei dem Modelle mit aktuelleren Daten vorab trainiert wurden, zeigte, dass die Leistung mit einer größeren zeitlichen Lücke abnimmt.</sample>
    <sample id="299">This presentation is about improving the robustness of NLI (Natural Language Inference) models using minimax training. While NLI models have achieved state-of-the-art results on benchmarks, they are susceptible to "shortcut learning." Shortcuts are spurious correlations between input attributes and labels that arise during dataset creation, leading to good performance on in-distribution samples but poor generalization on out-of-distribution adversarial test sets where these correlations don't hold.

Traditional shortcut mitigation methods often require prior knowledge of shortcuts, which isn't always available, or rely on an auxiliary model, leading to unstable training and computational overhead.

The proposed solution, minimax training, addresses these limitations. The key insight is that NLI models struggle with under-represented hard examples that contradict shortcuts found in dominant easy examples. The goal of minimax training is to learn an example weight distribution that emphasizes these hard examples.

In this approach, a learner model aims to minimize the NLI task loss, while an auxiliary model maximizes the learner's loss by generating example weights. This incentivizes the learner to focus on input space regions with high losses (i.e., the hard examples). Both models are optimized alternately using standard optimization algorithms like stochastic gradient descent.

A significant advantage of minimax training is that it requires no prior assumptions about shortcuts, relying instead on the learner's intrinsic training dynamics. The auxiliary model is a simple feed-forward network, minimizing computational overhead. Experimental results on FEVER, MNLI, and QQP datasets demonstrate that minimax training consistently improves out-of-distribution performance while maintaining high in-distribution accuracy.</sample>
    <sample id="300">The speaker, Belinda Z. Li, introduces "interactive dictation," a new task that allows users to dictate and edit documents using their voice in a natural and intuitive manner. Unlike existing speech-to-text systems, which often require memorizing fixed commands or using reserved trigger words, interactive dictation aims for a more seamless experience by allowing flexible interleaving of dictation and editing without specific trigger words.

The task is characterized by two key features: flexible interleaving of dictation and editing, and intuitive and open-ended natural language for editing. The core challenge is predicting segmentation between dictation and editing commands and interpreting open-ended natural language utterances. The researchers address this by proposing a four-step procedure: ASR recognition, segmentation, normalization, and interpretation.

To support this new task, a data collection interface was designed, and a dataset called TERTUIS was created, comprising 1320 trajectories, 959 dictations, and 3225 commands. A baseline system was built using T5 and GPT-3 models, which showed promising results in segmentation and ASR repair/interpretation. While GPT-3 models offered higher accuracy, they were considerably slower than T5. The code and data for this work have been open-sourced to facilitate further research.</sample>
    <sample id="302">The tokens for the output sequence need to be permuted because, after the initial tagging step, all the right tokens are present, but they are not in the correct order. The permutation step arranges these tokens into the desired output sequence.</sample>
    <sample id="303">Modellentwickler*innen sollten ihre Methoden zum Abbau von Vorurteilen transparenter machen, weil positive Stereotypen und essentialisierende Narrative schädliche Muster aufweisen. Ohne Transparenz ist es schwierig zu wissen, ob diese positiven Muster durch übermäßiges Value Alignment oder andere Anti-Stereotyping-Methoden entstehen.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind die Eingaben, bei denen die akzeptable oder grammatische Präfixabfrage nicht mit der inakzeptablen Abfrage übereinstimmt.</sample>
    <sample id="305">Der Vortrag „Weaker Than You Think: A Critical Look at Weakly Supervised Learning“ untersucht die Notwendigkeit sauber annotierter Validierungsdaten in schwach überwachtem Lernen (WSL). Zunächst wird die Herausforderung der lauten Labels in WSL erörtert und hervorgehoben, dass neuronale Netze dazu neigen, Rauschen zu memorisieren und die Generalisierungsfähigkeit zu beeinträchtigen, wenn keine sauberen Daten verfügbar sind.
Die Forschung adressiert drei zentrale Fragen:
1. Ist eine saubere Validierungsdatenbank für WSL notwendig? Die Ergebnisse zeigen, dass WSL-Methoden tatsächlich saubere Validierungsdaten benötigen, um korrekt zu funktionieren. Andernfalls sinkt die Leistung drastisch, was darauf hindeutet, dass die trainierten Modelle über die schwachen Labels hinaus nicht generalisieren.
2. Wie viele saubere Samples benötigen WSL-Ansätze? Es wird festgestellt, dass eine Erhöhung der Anzahl sauberer Validierungsbeispiele die Leistung verbessert, wobei typischerweise 20 Samples pro Klasse für eine hohe Leistung ausreichen. Allerdings schneiden direkte Feinabstimmungsansätze (Fine-Tuning) bei 10 Samples pro Klasse besser ab als WSL-Ansätze, die die sauberen Daten nur zur Validierung nutzen.
3. Wie können die verfügbaren sauberen Samples effizienter genutzt werden? Kontinuierliches Fine-Tuning (CFT) schließt Leistungsunterschiede zwischen WSL-Ansätzen und dem direkten Fine-Tuning mit sauberen Daten. Dies deutet darauf hin, dass keine komplexen WSL-Methoden erforderlich sind, wenn genügend saubere Daten für das CFT vorhanden sind.

Abschließend wird gefolgert, dass aktuelle WSL-Ansätze saubere Daten benötigen und ihre Praktikabilität überschätzt wird. Es wird empfohlen, die Kriterien für die Modellauswahl anzugeben, Few-Shot-Lernansätze als Baselines zu verwenden und immer kontinuierliches Fine-Tuning (CFT) anzuwenden.</sample>
    <sample id="306">Sebastian Schuster und Najoung Kim präsentieren ihre Arbeit zum Thema Entitätstracking in Sprachmodellen. Für ein Agent, der einen Diskurs verstehen soll, muss er in der Lage sein, die erwähnten Entitäten zu verfolgen und wie sich ihr Zustand ändert, während sich der Diskurs entfaltet. Es gab jedoch keine systematischen Untersuchungen darüber, ob vortrainierte Sprachmodelle solche Aufgaben tatsächlich ausführen können.

Es gibt einige Herausforderungen bei der Bewertung von Entitätstracking-Fähigkeiten. Erstens sind einige Entitätszustände in den Vortrainingsdaten üblich, und daher kann das Modell den korrekten Zustand vorhersagen, ohne tatsächlich Entitätstracking-Fähigkeiten zu besitzen. Zweitens können manchmal Entitätszustände aus einzelnen Wörtern oder Phrasen vorhergesagt werden, ohne den größeren Diskurs zu berücksichtigen. Drittens kann, wenn man Feinabstimmung oder In-Context-Demonstrationen verwendet, das Modell Entitätszustandssequenzen auswendig lernen oder Heuristiken wie Slot-Filling anwenden, wenn solche Heuristiken im Design der Bewertungsaufgabe nicht blockiert sind.

Um Entitätstracking-Fähigkeiten zu bewerten, entwarfen sie eine Aufgabe, bei der es um Kisten und Objekte geht. Der Input für das Modell beginnt mit einer Beschreibung des ursprünglichen Inhalts jeder Kiste, und die Aufgabe des Sprachmodells besteht darin, den Input zu vervollständigen, indem es den Inhalt jeder Kiste vorhersagt. Sie implementierten verschiedene Maßnahmen, um das Modell daran zu hindern, Heuristiken zu verwenden.

Ihre Experimente zeigen, dass die meisten Modelle einfach den ursprünglichen Zustand wiederholen. Nur GPT-3.5 text-davinci-003 zeigt ein nicht-triviales Entitätstracking. Alle anderen Modelle, die sie getestet haben, schnitten schlechter ab als eine starke Zufalls-Baseline, die durch Zufallssimulation erhalten wurde. Die GPT-3.5-Modelle wurden mit einem beträchtlichen Anteil an Code vortrainiert. Sie fanden heraus, dass alle GPT-3.5-Modelle ein nicht-triviales Entitätstracking-Verhalten zeigen, während alle Modelle, die keinen Code als wesentlichen Bestandteil ihres Vortrainings haben, dies nicht tun. Dies deutet darauf hin, dass das Vortraining auf Code dafür verantwortlich ist, dass diese Fähigkeit in vortrainierten Sprachmodellen zum Vorschein kommt.

Sie fanden auch heraus, dass kleinere Modelle wie T5-base lernen können, Entitätstracking durch direkte Feinabstimmung durchzuführen. Zufällig initialisierte Modelle der gleichen Architektur können ihre Zustandstracking-Aufgabe nicht lernen, selbst wenn sie direkte Überwachung erhalten, was darauf hindeutet, dass Vortraining auch hier wichtig ist.</sample>
    <sample id="307">Die Autoren haben die Bewertungsmetriken F1, Hamming und EMR verwendet.</sample>
    <sample id="308">In dieser Präsentation stellt Jenny T. Liang ihre Forschung zu "NLPositionality" vor, einem Framework zur Charakterisierung von Design-Biases in NLP-Datensätzen und -Modellen. Das Konzept der Positionalität, die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben, wird auf NLP-Technologien angewendet, um zu untersuchen, wie sich diese Perspektiven auf die Gestaltung von Datensätzen und Modellen auswirken können.

Die Forschung besteht aus zwei Hauptschritten: Erstens der Neu-Annotation von Datensätzen mit einer vielfältigen Gruppe von Annotatoren, die über LabintheWild, eine Crowdsourcing-Plattform für HCI-Mitarbeiter, rekrutiert wurden. Dies ermöglicht die Erfassung von Annotationen von Tausenden von Personen aus zahlreichen Ländern, zusammen mit detaillierten demografischen Daten. Zweitens werden diese Annotationen nach Demografie mit bestehenden Datensätzen und Modellvorhersagen verglichen, wobei Pearson's R-Korrelationswerte verwendet werden.

Liang stellt zwei Aufgaben vor, die über LabintheWild ausgeführt wurden: Soziale Akzeptanz (Task A) unter Verwendung des Social Chemistry Datensatzes und der Modelle Delphi und GPT-4, sowie Toxizitätserkennung (Task B) unter Verwendung des Dynahate Datensatzes und der Modelle Perspective API, Rewire API, Hate RoBERTa und GPT-4.

Die Ergebnisse zeigen, dass Positionalität in NLP existiert. Datensätze und Modelle sind am stärksten auf englischsprachige Länder und Menschen mit Hochschulbildung ausgerichtet. Umgekehrt sind sie weniger auf nicht-binäre Personen ausgerichtet. Um diesen Biases entgegenzuwirken, werden Empfehlungen gegeben: relevante Designentscheidungen während des gesamten Entwicklungsprozesses von Datensätzen und Modellen zu dokumentieren; NLP-Forschung durch die Linse des Perspektivismus zu betreiben, indem disaggregierte Datensatzlabels geteilt und Modellierungstechniken verwendet werden, die Annotator-Uneinigkeit berücksichtigen; und spezialisierte Datensätze und Modelle mit und für spezifische Gemeinschaften zu entwickeln, um inklusive NLP zu fördern (z.B. die Masakhane-Initiative).</sample>
    <sample id="309">Krippendorff's Alpha.</sample>
    <sample id="310">Die Domain Wikipedia wurde gewählt, um völlig unzusammenhängende Sätze hinzuzufügen.</sample>
    <sample id="311">Die Autoren gehören der Heinrich Heine Universität Düsseldorf in Deutschland an.</sample>
    <sample id="312">MultiInstruct ist der erste multimodale Instruktions-Tuning-Benchmark-Datensatz, der 62 verschiedene multimodale Aufgaben abdeckt, aufgeteilt in 10 breite Kategorien. Diese Aufgaben wurden aus 21 bestehenden Open-Source-Datensätzen abgeleitet, wobei jede Aufgabe über fünf von Experten verfasste Anweisungen verfügt.</sample>
    <sample id="313">Es sind drei Autoren an der Arbeit beteiligt.</sample>
    <sample id="314">Es gibt vier verschiedene Arten von binären Koordinationen:
1.  **Bouquet/Stanford (universale Abhängigkeiten):** Bei diesem Ansatz ist das erste Konjunkt der Kopf der gesamten Koordinationsstruktur.
2.  **Chain/Moscow:** Bei diesem Ansatz wird die gesamte Koordinationsstruktur vom ersten Konjunkt geleitet.
3.  **Conjunction-headed/Prague:** Bei diesem Ansatz werden Koordinationsstrukturen durch die Konjunktion geleitet, sodass Abhängigkeiten von "und" zu allen Konjunktionen bestehen.
4.  **Multi-headed/London:** Bei diesem Ansatz sind alle Konjunktionen Köpfe der Koordinationsstruktur. Das bedeutet, dass die Abhängigkeiten vom Regenswort, wie "liebt", zu allen Konjunktionen separat bestehen.</sample>
    <sample id="315">Die Dauer der in dieser Studie verwendeten Prompts wird im englischen Inhalt nicht explizit erwähnt.</sample>
    <sample id="316">Ein kleineres T5-Modell, das mit dem CoScript-Datensatz feinabgestimmt wurde, kann qualitativ bessere Skripte generieren als größere Sprachmodelle.</sample>
    <sample id="317">Die Rede stellt CodeIE vor, ein neues Modell, das Large Code Generation Models für die Informationsextraktion nutzt. Der Redner beginnt mit der Erläuterung der Informationsextraktion (IE), die darauf abzielt, strukturierte Informationen aus einfachem Text zu erkennen, wie beispielsweise die Erkennung von "Steve" als Person und "Apple" als Organisation in einem Beispielsatz.

Der Redner hebt die Herausforderungen bei bestehenden IE-Modellen hervor, die Text-zu-Text-Generierungsmodelle (wie T5 und GPT-3) verwenden. Diese Modelle erfordern oft eine aufwendige Vorverarbeitung und Feinabstimmung, da die Ausgabeformate von IE-Aufgaben (strukturierte Daten) nicht mit der Textausgabe der Modelle übereinstimmen.

CodeIE geht dieses Problem an, indem es IE-Aufgaben in Code-Generierungsaufgaben umwandelt. Anstatt beispielsweise eine NER-Aufgabe als reinen Text zu verarbeiten, erstellt CodeIE einen Code-Prompt (eine Python-Funktion), der das Input-Text als Argument nimmt und eine Entity-Liste initialisiert. Das Modell wird dann angewiesen, den relevanten Code zu generieren, der Entitäten identifiziert und sie der Liste hinzufügt, wobei strukturierte Ausgaben sichergestellt werden.

Experimentelle Ergebnisse zeigen, dass CodeIE mit Code-Sprachmodellen (wie Codex) die traditionellen IE-Modelle (wie UIE und GPT-3) bei Few-Shot-Aufgaben deutlich übertrifft, insbesondere bei der Erkennung von Entitäten und Beziehungen. Eine detaillierte Analyse zeigt, dass Code-Prompts eine höhere Formatkonsistenz und weniger strukturelle Fehler aufweisen. Darüber hinaus ist CodeIE besser darin, die richtigen Label-Typen zu generieren.

Abschließend betont der Redner die Wirksamkeit von CodeIE bei der Verbesserung der Informationsextraktion durch die Nutzung der Fähigkeiten von Code-Sprachmodellen. Er gibt die Links zum Paper und Code an und bedankt sich beim Publikum.</sample>
    <sample id="318">00:00
Hallo, ich bin Yanis Labrak und ich werde Ihnen unsere Arbeit an DrBERT vorstellen, einem robusten, vortrainierten Modell in Französisch für biomedizinische und klinische Bereiche.

00:09
In dieser Präsentation werden wir zuerst über Sprachmodellierung im Gesundheitswesen sprechen. Dann werden wir den Hauptbeitrag unseres Artikels vorstellen. Wir stellen das erste biomedizinische Modell in Französisch namens DrBERT vor, das auf RoBERTa basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer Daten, die aus dem Web gecrawlt wurden. Wir stellen auch einen Vergleich von Modellen mit mehreren Vortrainings-Einstellungen und Datenquellen vor. Dann präsentieren wir unsere Ergebnisse zu 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Und schließlich werden wir die Experimente zusammenfassen und Ihnen weitere Details zum Zugriff auf die Modelle geben.

00:48
Seit seiner Veröffentlichung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze zur Lösung von Aufgaben der natürlichen Sprachverarbeitung geworden und bietet einen enormen Leistungsgewinn im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2vec, FastText oder ELMo. Seitdem wurde dieses Modell an viele andere Sprachen wie Französisch mit CamemBERT und anderen Domänen wie der Biomedizin mit PubMedBERT und BioBERT und der Klinik mit ClinicalBERT angepasst, aber hauptsächlich auf Englisch. Spezialisierte Modelle für andere Sprachen sind seltener und basieren aufgrund des Mangels an In-Domain-Daten oft auf kontinuierlichem Vortraining. Französisch hatte jedoch bisher kein Open-Source-Modell für den biomedizinischen und klinischen Bereich.

01:35
Wir haben uns also gefragt, welche die am besten geeigneten Datenquellen für eine Vielzahl von Anwendungen sind und ob diese gecrawlten Daten ein guter Ersatz für klinische Daten sind. Um diese Fragen zu beantworten, trainieren und vergleichen wir zuerst vier From-Scratch-Modelle. Eine erste Version von DrBERT mit 7 Gigabyte NACHOS, eine zweite Version mit einem 4 Gigabyte-Subset von NACHOS, eine erste Version von ChuBERT, einem klinischen Modell, mit 4 Gigabyte an klinischen Notizen. Und eine finale Version von ChuBERT mit einem Mix aus 4 Gigabyte NACHOS und 4 Gigabyte klinischen Notizen. Zusätzlich zu diesem Vergleich stellen wir drei Modelle vor, die auf kontinuierlichem Vortraining basieren, um den Einfluss der Vortrainingsstrategie zu analysieren. Eines basiert auf den Gewichten von CamemBERT und wurde auf 4 Gigabyte NACHOS trainiert, ein anderes ebenfalls auf CamemBERT, aber diesmal auf 4 Gigabyte klinischen Notizen. Und schließlich eines, das auf dem englischen biomedizinischen Modell PubMedBERT basiert und auf 4 Gigabyte NACHOS trainiert wurde. Insgesamt haben wir sieben Modelle.

03:02
Um unsere sieben Modelle zu evaluieren, haben wir acht öffentliche und private Downstream-Aufgaben gesammelt, wie Named Entity Recognition, Klassifikation, Part-of-Speech Tagging und Question Answering. Diese Modelle werden mit sechs Baseline-Modellen verglichen, nämlich CamemBERT OSCAR 138 Gigabyte, CamemBERT OSCAR 4 Gigabyte, CamemBERT CCNet 4 Gigabyte, PubMedBERT, BioBERT und ClinicalBERT. Die Evaluation zeigt, dass Modelle auf Aufgaben mit Daten derselben Art, auf denen das Modell trainiert wurde, am besten abschneiden. Wir können jedoch beobachten, dass Daten aus heterogenen Quellen vielseitiger zu sein scheinen. Wir beobachten auch, dass mehr Daten zu einer besseren Leistung führen.

03:51
Insgesamt scheint das Vortraining von Grund auf in den meisten Aufgaben eine höhere Leistung zu erzielen. Unsere Experimente zum kontinuierlichen Vortraining, bei denen die Gewichte und Tokenizer von PubMedBERT verwendet und auf dem 4 Gigabyte-Subset von NACHOS trainiert wurden, zeigen vergleichbare Ergebnisse wie die von DrBERT 4 Gigabyte From-Scratch. Dies ist nicht der Fall für das Modell, das auf CamemBERT-Gewichten und Tokenizer basiert, das unter Stabilitätsproblemen leidet.

04:18
Zusammenfassend lässt sich sagen, dass unser vorgeschlagenes System in neun der elf Downstream-Aufgaben eine bessere Leistung erzielt und die Ergebnisse des generischen Modells CamemBERT insgesamt übertrifft. Wir haben auch festgestellt, dass spezialisierte Daten besser sind, mehr spezialisierte Daten besser sind, aber nicht gut skalieren. Alle vortrainierten Modelle, die aus NACHOS stammen, sind frei verfügbar und die Trainingsskripte sind in unserem GitHub-Repository zu finden.

04:49
Vielen Dank für diese Präsentation und wir freuen uns auf den Austausch bei der Poster-Session in Toronto.</sample>
    <sample id="319">Die Arbeit untersucht die Lernstrategien "von Grund auf" und "kontinuierliches Vortraining".</sample>
    <sample id="320">Im gezeigten Diagramm ist die Steigung der roten Linie, die die Leistung auf dem neueren Datensatz darstellt, größer als 1. Dies zeigt, dass es keine verminderten Erträge gibt. Mit anderen Worten, eine Überanpassung durch die Wiederverwendung von Tests ist nicht zu beobachten.</sample>
    <sample id="321">Um die Qualität der Vereinfachung zu beurteilen, wurde das Modell mit Long-mBART finetuned.</sample>
    <sample id="322">Enrico Liscio stellt auf der ACL 2023 ein Projekt vor, das die Frage beantwortet, was Textklassifikatoren über Moral lernen. Er erklärt, dass menschliche Moral die Fähigkeit ist, richtig von falsch zu unterscheiden. Diese Fähigkeit ist für menschliche Gesellschaften und auch für Sprachmodelle unerlässlich, da sie unsere Interaktionen und Texte beeinflusst. Bislang wurde Moral in der NLP als eine einzige Skala zwischen unmoralisch und moralisch behandelt, aber Enrico weist darauf hin, dass Moral subjektiv ist und von verschiedenen Menschen unterschiedlich bewertet werden kann. Das macht die durchschnittliche Bewertung durch ein Sprachmodell gefährlich, da sie die Wahrheit der pluralistischen Interpretationen der Moral verschleiern kann.

Er schlägt vor, die Moral auf der Grundlage der Moralischen Fundamenttheorie zu klassifizieren, die Moral in fünf Kategorien einteilt: Fürsorge, Fairness, Loyalität, Autorität und Reinheit. Da jedes Konzept oder jede Handlung einen anderen moralischen Aspekt berührt, priorisiert jeder Mensch diese Grundlagen auf unterschiedliche Weise, und diese Prioritäten bestimmen, wie wir die Moral eines Konzepts oder einer Handlung beurteilen.

Das Projekt versucht, zu verstehen, wie Moral in verschiedenen Domänen unterschiedlich ausgedrückt wird. Dafür werden XAI-Techniken angewendet, um zu verstehen, was Sprachmodelle tatsächlich lernen. Enrico stellt das Korpus von Moral Foundation Twitter vor, das aus über 35.000 Tweets besteht, die in sieben verschiedenen Domänen gesammelt wurden, darunter Hashtags wie „AllLivesMatter“ (ALM) und „BlackLivesMatter“ (BLM). Die Ergebnisse zeigen, dass Sprachmodelle erkennen, dass ALM und BLM unterschiedliche Rhetoriken für das Element der Subversion (Rebellion gegen Autorität) haben. So wird Subversion bei ALM missbilligt, während sie bei BLM ermutigt wird.

Diese Erkenntnis ist wichtig, da sie zeigt, dass Sprachmodelle tatsächlich erkennen, dass Moral in verschiedenen Domänen unterschiedlich ausgedrückt werden kann. Dies warnen jedoch auch davor, ein einziges Sprachmodell für viele verschiedene Domänen zu verwenden, da dies zu gefährlichen Missverständnissen der Moral führen kann.</sample>
    <sample id="323">In dieser Präsentation wird ein neuer Ansatz zur Lösung von Common Sense Question Answering (QA)-Aufgaben vorgestellt, bei dem Sprachmodelle und wissensbasierte Darstellungen kombiniert werden. Traditionelle Methoden zur Common Sense QA haben mehrere Einschränkungen, darunter die Einführung von Rauschen während des Abrufs von Wissens-Subgraphen und begrenzte Interaktion zwischen Text und Subgraphen-Kodierung.
Die vorgeschlagene Methode, Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning (DHLK), versucht, diese Probleme zu lösen. Sie besteht aus einem heterogenen Wissensgraphen (HKG), der auf mehreren Wissensbasen aufbaut, einer zweistufigen Pruning-Strategie zur Optimierung der HKG-Struktur, Knowledge Representation Learning (KRL) und der Integration von Sprachmodellen zur Fusion und Kodierung von zwei Modalitäten. Die DHLK-Methode umfasst die Konstruktion des HKG, eine dynamische Pruning-Modul, das auf der Aufmerksamkeit des Sprachmodells basiert, ein KRL-Modul und ein Integrator- und Antwortvorhersagemodul.
Experimentelle Ergebnisse an Datensätzen wie Common Sense QA und OpenBookQA zeigen, dass die DHLK-Methode im Vergleich zu anderen Sprachmodellen und HKG-Methoden überlegene Leistungen erbringt. Sie übertrifft beispielsweise RoBERTa mit einer signifikanten Verbesserung der Ergebnisse.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile. Eine Vorabstudie zeigt, dass sie alle vier Quadranten des politischen Kompasses einnehmen.</sample>
    <sample id="325">00:00:00,000 --&gt; 00:00:19,890
Hallo, mein Name ist Matthias Lindemann und heute werde ich Ihnen eine kurze Einführung in unser Paper über kompositionelle Generalisierung ohne Bäume geben, unter Verwendung von Multi-Set-Tagging und latenten Permutationen. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov.

00:00:20,950 --&gt; 00:00:33,520
Kompositionelle Generalisierung kann verstanden werden als die Fähigkeit eines Lerners, tiefer gehende Rekursionen und unbekannte Kompositionen von Phrasen zu verarbeiten, die während des Trainings einzeln gesehen wurden.

00:00:34,310 --&gt; 00:01:15,70
Im Kontext der semantischen Analyse könnte das Testen auf kompositionelle Generalisierung so aussehen: Wie üblich haben wir einen Trainingsdatensatz von Äußerungen, in diesem Fall "The girl slept" und "Mary knew that the girl slept". Diese Äußerungen sind mit logischen Formen gepaart, die zentrale Aspekte ihrer Bedeutung darstellen. Im Gegensatz zur Standard-Evaluation im maschinellen Lernen stammt der Testdatensatz nicht aus derselben Verteilung, sondern enthält strukturell unbekannte logische Formen. In diesem Beispiel hat das Modell während des Trainings eine flache Rekursion gesehen und wird an einem Beispiel mit tieferer Rekursion getestet.

00:01:15,70 --&gt; 00:01:36,490
Naive Seq2Seq-Modelle scheitern! Naive Seq2Seq-Modelle haben Schwierigkeiten mit dieser Art von Out-of-Distribution-Generalisierung und produzieren oft Ausgaben, die vom Input losgelöst sind. Insbesondere gelingt es ihnen oft nicht, die systematischen Entsprechungen zwischen Input und Output, wie sie im Beispiel farblich kodiert sind, zu reproduzieren.

00:01:37,670 --&gt; 00:02:35,10
Ein beliebtes Verfahren, um dies zu adressieren, ist die Integration von Bäumen in die Modelle. Die Bäume sollen den Kompositionsprozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume sind normalerweise nicht gegeben und müssen irgendwie gewonnen werden. Dies kann kompliziert und manchmal ein rechenintensiver Prozess sein. Typischerweise beinhaltet dies eine erhebliche formalismus-spezifische Vorverarbeitung der logischen Formen, um zum Beispiel Variablen-Symbole zu behandeln. Das Erlangen von Bäumen kann auch spezialisierte Grammatik-Induktionsverfahren beinhalten. In diesem Paper verwenden wir keine Bäume und stellen ein neuronales Seq2Seq-Modell vor, das die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs direkt modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung auf tiefere Rekursion ohne Bäume.

00:02:35,970 --&gt; 00:03:12,690
Unser Ansatz sagt den Output aus dem Input in zwei Schritten voraus. Zuerst taggen wir jedes Input-Token mit einem ungeordneten Multi-Set von Tokens, die im Output erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht geordnet. Deshalb verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation vorherzusagen, die sie in die richtige Reihenfolge bringt. Wir führen eine neue Methode ein, um eine Permutation vorherzusagen, die keine harten Einschränkungen für die möglichen Permutationen setzt. Dies macht unseren Ansatz recht flexibel und ausdrucksstark.

00:03:13,970 --&gt; 00:03:59,850
Konzeptionell funktioniert unser Permutationsmodell ungefähr so: Wir gehen von links nach rechts über den Output und bestimmen, welches Multi-Set-Token an jede Position gesetzt werden soll. Für die erste Output-Position wählen wir einfach eins, wie rot hervorgehoben. Dann springen wir zum nächsten Multi-Set-Token, um das zweite Token im Output zu bestimmen. Wir bestimmen das dritte Token im Output auf ähnliche Weise, indem wir zu einem anderen Multi-Set-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde.

00:04:00,560 --&gt; 00:04:20,530
Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben: Hier vergleichen wir unsere Methode mit anderen baumlosen Modellen auf dem COGS-Benchmark. Unser Modell übertrifft die anderen um einen großen Vorsprung bei der Generalisierung auf tiefere Rekursion. Einige andere Arten der strukturellen Generalisierung bleiben jedoch sehr herausfordernd.

00:04:21,500 --&gt; 00:04:56,360
In unserem Paper lösen wir einige interessante technische Herausforderungen. Zunächst ist die Ausrichtung zwischen Input und Output in den Trainingsdaten nicht gegeben. Infolgedessen wissen wir für ein gegebenes Token nicht, aus welchem Multi-Set es kam, was eine Herausforderung für das Training darstellt. Wir adressieren dies, indem wir die Ausrichtung als Teil des Trainings induzieren.

00:04:56,920 --&gt; 00:05:34,80
Unsere Permutationsmethode ist sehr flexibel, birgt aber die Herausforderung, dass das Finden der am höchsten bewerteten Permutation NP-hart ist. Das liegt daran, dass dies mit dem Problem des Handlungsreisenden zusammenhängt. Wir approximieren dies mit einer GPU-freundlichen, kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zurückzupropagieren und die linguistisch plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente erfahren möchten und wie wir diese Herausforderungen angehen, werfen Sie bitte einen Blick auf unser Paper oder kommen Sie zu unserem Poster.</sample>
    <sample id="326">Kognitive Dissonanz ist ein Zustand, in dem zwei Elemente der Kognition, z. B. Gedanken, Handlungen oder Überzeugungen, inkonsistent sind.</sample>
    <sample id="327">Die Präsentation stellt ManagerTower vor, eine neuartige Vision-Language-Modellarchitektur, die auf der BridgeTower aufbaut. ManagerTower befasst sich mit den Einschränkungen der BridgeTower, indem sie eine ineffektive schichtweise Nutzung und eine feste Anzahl von Cross-Modal-Schichten überwindet.

Das Kernmerkmal von ManagerTower ist die adaptive Aggregation von Erkenntnissen aus vortrainierten unimodalen Experten auf verschiedenen Ebenen durch dedizierte Manager in jeder Cross-Modal-Schicht. Dies ermöglicht eine umfassendere Cross-Modal-Abstimmung und -Fusion.

Die experimentellen Ergebnisse, die mit 4 Millionen Bildern vortrainiert wurden, zeigen, dass ManagerTower auf verschiedenen nachgelagerten Aufgaben eine überlegene Leistung erbringt. Insbesondere erreicht es eine Genauigkeit von 79,15 % bei VQAv2-Test-Standard, was eine signifikante Verbesserung gegenüber früheren Modellen darstellt. Visualisierungen der Aggregationsgewichte zeigen, dass ManagerTower im Gegensatz zu statischen Managern unterschiedliche Aggregationsgewichtsverteilungen in verschiedenen Cross-Modal-Schichten aufweist, was die Fähigkeit des Modells unterstreicht, unimodales semantisches Wissen adaptiv zu nutzen.

ManagerTower bietet einen robusten und flexiblen Rahmen für Vision-Language-Lernaufgaben und übertrifft zahlreiche Basismodelle und sogar Modelle, die mit mehr Daten oder Parametern trainiert wurden.</sample>
    <sample id="328">GPT-4 ist das politisch liberalste Sprachmodell.</sample>
    <sample id="329">This presentation introduces a novel method for zero-shot video sentence localization (ZSVL) that tackles the challenges of existing methods which suffer from overly simplistic pseudo-queries, misalignment between pseudo-events and pseudo-queries, and neglect of noise in pseudo-labels.

The proposed approach, called Structured Pseudo-Label (SPL) generation, consists of two main parts:

1. **Structured Pseudo-Label Generation:**
    * It generates free-form pseudo-queries using image description models, producing more complex and diverse queries than previous methods.
    * It then generates pseudo-events based on an event temporal structure. This involves calculating the similarity between video frames and pseudo-queries, and defining event quality as the difference between the mean similarity within and outside the event. The event proposal with the highest quality is selected, ensuring high relevance within the event and low relevance outside it. This step effectively aligns pseudo-events with pseudo-queries.

2. **Noise Reduction during Training:**
    * **Sample re-weighting:** Noise is estimated based on the confidence score and Intersection over Union (IoU) of predictions and pseudo-labels. Samples with higher probability of label error (lower confidence and IoU) are weighted less during training, reducing their influence.
    * **Label refinement:** If the model's prediction confidence is high and has a high IoU with the pseudo-label, the prediction is treated as a new pseudo-label for the next training round, iteratively improving label quality.

Experiments on ActivityNet Captions and Charades-STA datasets demonstrate that SPL achieves state-of-the-art zero-shot performance on most metrics compared to existing methods.</sample>
    <sample id="330">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="331">Der/die Referent*in ist Sara Papi.</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus Transkripten von TED-Talks, die vom Englischen in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="333">Die Präsentation stellt INK vor, ein Framework zur Injektion von kNN-Wissen in die Neuronale Maschinelle Übersetzung (NMT), um deren Generalisierungsfähigkeit zu verbessern. Der Sprecher weist darauf hin, dass NMT-Modelle oft einen nicht-glatten Darstellungsraum aufweisen, in dem selten vorkommende Token spärlich verteilt sind, was zu "semantischen Löchern" und einer schlechten Leistung in unbekannten Domänen führt.

Die vorherige Lösung, kNN-MT, verbesserte die Leistung durch das Speichern von Darstellungen und Ziel-Token in einem Datenspeicher und das Glätten von Vorhersagen mit den nächsten Nachbarn. Dies war jedoch zeitaufwändig und ließ keine einfache Aktualisierung der Darstellungen zu.

Um diese Mängel zu überwinden, schlägt INK einen Trainingsloop mit zwei Schritten vor: Repräsentationsverfeinerung und asynchrone Aktualisierung. Die Repräsentationsverfeinerung extrahiert kNN-Wissen aus dem Datenspeicher, um einen Adapter zur Anpassung der Repräsentationen anzuleiten. Anschließend werden die aktualisierten Repräsentationen asynchron zur Aktualisierung des Datenspeichers verwendet. Dieser Loop wird bis zur Konvergenz ausgeführt.

Die Anpassung der Repräsentation erfolgt durch die Angleichung von drei Arten von Repräsentationen unter Verwendung der KL-Divergenz: kontextualisierte Repräsentationen und Token-Embeddings, kontextualisierte Repräsentationen und kNN-Token-Embeddings sowie kontextualisierte Repräsentationen desselben Ziel-Tokens. Nach dem Training kann der Datenspeicher während der Inferenz verworfen werden.

Experimente, die an vier Benchmark-Datensätzen mit einem WMT'19-Gewinnermodell durchgeführt wurden, zeigten, dass INK das kNN-MT-System übertrifft und die beste Leistung erzielt. INK erreicht durchschnittlich 1,99 COMET- und 1,0 BLEU-Punkte mehr, mit nur 0,02-fachem Speicherplatz und 1,9-facher Inferenzgeschwindigkeit im Vergleich zu kNN-MT-Baselines. Das gemeinsame Anwenden von Adapter und Datenspeicher führt zu einer weiteren Glättung der Vorhersagen, was darauf hindeutet, dass der Repräsentationsraum des NMT-Modells noch weiter verfeinert werden könnte.</sample>
    <sample id="335">Der/die Referent*in heißt Matthias Lindemann.</sample>
    <sample id="336">Sprachübergreifender Transfer ist das Training eines Modells in einer Quellsprache und das Testen desselben Modells in einer anderen Zielsprache.</sample>
    <sample id="337">Der Sprecher präsentiert eine Arbeit mit dem Titel "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning" auf der ACL 2023. Die Arbeit befasst sich mit dem Problem der Darstellung von Wörtern, die nicht im Vokabular enthalten sind (OOV-Wörter), in Modellen für Word Embeddings. Der Sprecher weist darauf hin, dass OOV-Wörter eine Herausforderung für die Leistung von Downstream-Modellen darstellen.

Die Arbeit schlägt einen neuen Ansatz vor, der von menschlichen Lerngewohnheiten inspiriert ist, nämlich die Wortbildung und assoziative Beziehungen zu nutzen, um die Bedeutung von OOV-Wörtern zu erschließen. Sie schlagen ein "Word Relationship Graph" (WRG) vor, das die lexikalischen Regeln der Wortbildung und der Assoziation nachahmt. Wenn ein OOV-Wort auftaucht, wird es in Wortteile zerlegt und mit anderen relevanten Wörtern assoziiert, wodurch ein zweistufiger Graph um das OOV-Wort gebildet wird.

Um dieses WRG zu verarbeiten, verwenden sie ein Graph Neural Network (GNN). Sie initialisieren OOV-Wortknoten-Attribute basierend auf ihren Zeichen mithilfe eines Self-Attention-Netzwerks. Sie wenden dann zwei Ebenen eines Graph Attention Network (GAT) an, um wichtige Informationen zu extrahieren und Rauschen von den Wortteilknoten zu reduzieren. Die resultierenden Hidden Embeddings jeder Ebene werden mit den ursprünglichen Eingaben verkettet und zu einer Knoten-Repräsentation zusammengeführt.

Um die gesamte Graphinformation zu erfassen und die Wortbildung zusammenzufassen, verwenden sie einen Readout-Block, der eine Graph-Level-Repräsentation erzeugt. Um den Vektorraum des Hintergrund-Embedding-Modells nachzuahmen, wenden sie kontrastives Lernen in der Loss-Funktion an. Sie wählen positive Beispiele (z.B. Zwei-Hop-Nachbarwörter, Synonyme oder das OOV-Wort selbst) aus dem Graphen aus und ziehen das inferierte Graph-Level-Embedding an sein Hintergrund-Embedding, während sie es von anderen negativen Beispielen im Batch wegdrücken.

Die Experimente zeigen, dass ihr Modell, genannt GRM, sowohl bei intrinsischen als auch bei extrinsischen Aufgaben die Leistung aktueller Methoden übertrifft. GRM kann auch bestehenden statischen und kontextuellen Modellen in Downstream-Aufgaben Vorteile bringen.

Der Sprecher diskutiert auch die Anwendbarkeit des Modells auf andere Sprachen. Agglutinierende Sprachen wie Japanisch oder Koreanisch, die Wörter durch direktes Aneinanderreihen von Morphemen bilden, sind für ihren Ansatz gut geeignet. Fusionssprachen wie Englisch, die Wörter durch verknüpfte Morpheme bilden und daher schwieriger zu verarbeiten sind, zeigen dennoch gute Ergebnisse bei einer vernünftigen Wortsegmentierung.

Abschließend betont der Sprecher, dass die Graphenstruktur des WRG in GRM verschiedene komplexe Wortbildungen bewältigen kann. Die Wirksamkeit der Anwendung von GRM auf andere Sprachen hängt von der Rationalität der Wortzerlegung ab.</sample>
    <sample id="338">In der Präsentation wird die Arbeit mit dem Titel „Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations“ vorgestellt. Der Sprecher beginnt damit, die Motivation für die Forschung zu erörtern und betont, wie wichtig es ist, die Qualität menschlich annotierter Erklärungen zu bewerten, die zum Trainieren von NLG-Modellen (Natural Language Generation) und zur Verbesserung der Modellleistung und -begründung verwendet werden. Er hebt die Subjektivität dieser Erklärungen und das Fehlen eines Goldstandards für deren Bewertung hervor, was sie schwer vergleichbar macht.

Der Sprecher geht auf die Grenzen bestehender Metriken ein, wie BLEU und ROUGE, die menschliche Anmerkungen als Goldstandard behandeln und sich auf Wortähnlichkeit konzentrieren. Er erwähnt auch den Simulatability Score, der die Leistungsänderung eines Basismodells misst, aber keine Aufgabenunterschiede oder die unterschiedliche Nützlichkeit von Erklärungen berücksichtigt.

Um diese Herausforderungen anzugehen, führt der Sprecher eine Vorlage-basierte, vereinheitlichte Datenstruktur ein, die verschiedene Aufgaben in eine einheitliche Multiple-Choice-Aufgabe umwandelt, um den Einfluss variierender Aufgaben und Modelle zu minimieren. Es werden Vorabexperimente auf den Datensätzen ECQA und CoS-E durchgeführt, die zeigen, dass Fine-Tuning Modelle nicht mit neuem Wissen lehrt, sondern sie dazu bringt, sich bei der Vorhersage auf Erklärungen zu verlassen. Es wird auch festgestellt, dass CoS-E-Erklärungen für Basismodelle weniger hilfreich sind als ECQA-Erklärungen, was die aufgabenabhängige Natur von Erklärungen unterstreicht.

Als Hauptbeitrag wird die neue Bewertungsmetrik TREU vorgeschlagen, die den Simulatability Score erweitert, indem sie die Nützlichkeit von Erklärungen beim Fine-Tuning bewertet. Eine Evaluation von TREU und Simulatability auf fünf Datensätzen und zwei Modellen (T5 und BART) zeigt, dass TREU die Nützlichkeit von Erklärungen zuverlässiger widerspiegeln kann als Simulatability, insbesondere bei Datensätzen, die von Menschen als qualitativ minderwertig angesehen wurden. Der Sprecher schließt mit der Empfehlung, in Zukunft ähnliche Qualitätsprüfungen bei der Sammlung menschlicher Erklärungen durchzuführen.</sample>
    <sample id="339">Die Autoren sind von der Saarland University, Amazon Alexa und der University of Vienna.</sample>
    <sample id="340">Kuan-Hao Huang stellt ParaAMR vor, einen umfangreichen Datensatz für Paraphrasen, der durch AMR-Rückübersetzung erstellt wurde. Er hebt die Vorteile der Paraphrasengenerierung in NLP-Anwendungen wie Frage-Antwort-Systemen und Chatbots hervor, weist aber auf die Einschränkungen bestehender Datensätze hin: menschlich annotierte Daten sind von hoher Qualität, aber begrenzt im Umfang, während automatisch generierte Daten zwar umfangreich sind, aber keine syntaktische Vielfalt aufweisen.

Das Hauptziel der Arbeit ist es, einen großen, syntaktisch vielfältigen Paraphrasen-Datensatz zu erstellen. Der Ansatz basiert auf Abstract Meaning Representations (AMR), gerichteten Graphen, die die Bedeutung eines Satzes erfassen. Knoten in einem AMR-Graphen repräsentieren semantische Konzepte, und Kanten repräsentieren semantische Beziehungen. Der Fokus (Wurzelknoten) repräsentiert die Hauptaussage des Satzes.

Der Prozess zur Generierung von Paraphrasen durch AMR-Rückübersetzung umfasst drei Schritte:
1. **Text zu AMR-Graph:** Ein vorab trainierter AMR-Parser wird verwendet, um den AMR-Graph eines Quellsatzes zu erhalten.
2. **Fokus ändern (Wurzelknoten):** Ein Knoten wird zufällig ausgewählt und als neuer Wurzelknoten gesetzt, wobei die entsprechenden Kanten und Kantenbeschriftungen geändert werden. Durch das Ändern des Wurzelknotens wird die Betonung des Satzes verschoben, was zu syntaktisch unterschiedlichen Paraphrasen führt.
3. **AMR-Graph zu Text:** Ein AMR-to-Text-Generator generiert Text aus den modifizierten Graphen. Da die generierten Texte denselben AMR-Graphen teilen, haben sie ähnliche Semantik, aber aufgrund der Betonung des Fokus durch den Generator ist ihre Syntax unterschiedlich.

Der resultierende ParaAMR-Datensatz enthält etwa 15,5 Millionen Quellsätze und durchschnittlich 6,92 Paraphrasen pro Satz. Qualitative und quantitative Analysen zeigen, dass ParaAMR syntaktisch vielfältiger ist als bestehende Datensätze, die auf Rückübersetzung basieren, und dabei eine gute semantische Ähnlichkeit bewahrt.

Die Nützlichkeit von ParaAMR wird in drei Anwendungen demonstriert:
1. **Lernen von Satz-Embeddings:** Satz-Embeddings, die mit ParaAMR trainiert wurden, übertreffen andere Datensätze bei der Messung der Semantischen Textuellen Ähnlichkeit (STS).
2. **Syntaktisch kontrollierte Paraphrasengenerierung:** ParaAMR verbessert die syntaktische Kontrolle bei der Paraphrasengenerierung.
3. **Data Augmentation für Few-Shot-Lernen:** ParaAMR erhöht die Leistung beim Few-Shot-Lernen durch Datenaugmentation, insbesondere in Aufgaben wie MRPC, QQP und RTE.

Der ParaAMR-Datensatz ist auf GitHub verfügbar.</sample>
    <sample id="341">Die Autoren verwenden Average Lagging und Computation-Aware Average Lagging als Latenzmessungen.</sample>
    <sample id="342">Jingsheng Gao presented a paper titled "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming." Open Domain Dialogue, a conversational exchange between humans and AI, relies on pre-trained models and large-scale datasets. Existing large-scale corpora are mostly text-sourced, highlighting the need for video-sourced datasets closer to real spoken conversations. However, current video-sourced datasets are limited in scale due to manual extraction.

Personalized dialogue, crucial for applications like virtual streamers, faces challenges in finding sufficient persona information and longer conversations per persona. Multi-party conversation also lacks large-scale Chinese corpora. To address these issues, the team proposes LiveChat, a large-scale personalized dialogue dataset constructed from live streaming. The dataset creation involves three steps: transcribing live streaming videos into utterances, constructing dialogues by matching streamer responses and audience comments, and collecting persona information and adding manual annotations.

LiveChat's persona extraction includes basic profiles through manual labeling and scratching, and text profiles extracted by rules and a trained persona classifier. The dataset features over 1.3 million dialogues, with rich persona annotations and the longest average sessions compared to other datasets. Experimental results on response modeling show that extracted persona and longer average sessions per persona significantly improve performance. For addressee recognition, single-stream BERT models outperform dual-stream ones, and persona information improves addressee recognition. Finally, transfer learning experiments confirm the distinctiveness of LiveChat's video-sourced dialogue domain, with human evaluations showing better results in terms of informativeness. In the future, the team plans to focus on efficient transfer learning of LLMs for LiveChat.</sample>
    <sample id="343">Hallo zusammen, ich bin Akshatha, und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit, den KITMUS-Test, der die Wissensintegration aus mehreren Quellen evaluiert. Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research.

NLU-Modelle stützen sich auf mehrere Wissensquellen, wie z. B. Wissen in den Parametern, das normalerweise durch Vortraining erworben wird, und Wissen, das zur Inferenzzeit in den Eingaben gegeben wird. Neuere Arbeiten in Aufgaben wie der Fragebeantwortung zeigen, dass Modelle vortrainiertes Wissen nutzen können, um die Aufgabe zu lösen.

Aber das Verstehen natürlicher Sprache erfordert oft Wissen, das auch zur Inferenzzeit bereitgestellt wird. Zum Beispiel im Satz: "John sah den neu gewählten Präsidenten im Fernsehen". Vortrainierte Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer die spezifische Entität John ist, oder wer der neue Präsident ist, weil sich der Präsident seit dem Vortraining geändert haben könnte.

Daher erfordern erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vortrainiertes als auch inferenzielles Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir führen eine Koreferenzauflösungsaufgabe ein, die darauf ausgelegt ist, die Fähigkeit zu untersuchen, Wissen aus verschiedenen Quellen zu nutzen. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und etablierten Koreferenzauflösungsmodellen.

Hier ist ein Beispiel aus unserem Datensatz: "Servin ist Richter. Kea ist Bäckerin. Servin und Kea trafen sich in einem Park. Nach einem langen Arbeitstag, an dem er Fälle vor Gericht entschied, war er froh, sich zu entspannen. [Antwort: Servin]". Die Aufgabe hier ist es, die richtige Entität zu identifizieren, auf die das Pronomen "er" verweist, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens, Entität-spezifisches Wissen, wie "Servin ist ein Richter". Und zweitens, Hintergrundwissen, wie "Richter entscheiden Fälle vor Gericht". Im Allgemeinen wird Hintergrundwissen während des Vortrainings von großen Sprachmodellen gelernt, während Entität-spezifisches Wissen typischerweise zur Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationen so, dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können.

Wir haben drei Settings von KITMUS definiert. Zuerst haben wir das typische Setting, Hintergrund-Vortraining, bei dem Hintergrundwissen zur Vortrainingszeit verfügbar ist. Zweitens gibt es das Hintergrund-Beide-Setting, bei dem Hintergrundwissen sowohl zur Vortrainingszeit als auch zur Inferenzzeit verfügbar ist. Und drittens das Hintergrund-Inferenz-Setting, bei dem beide Wissenstypen nur zur Inferenzzeit verfügbar sind. Dieses letzte Setting ist besonders interessant, da es den Fall simuliert, in dem das Hintergrundwissen, das zur Lösung der Aufgabe notwendig ist, nicht Teil der Vortrainingsdaten von Modellen ist. Zum Beispiel, weil sich seit dem Vortraining neue Berufe entwickelt haben.

Hier ist ein Beispiel, wie wir die Verfügbarkeit von Fakten in den beiden Quellen steuern. Im Hintergrund-Vortraining-Setting nehmen wir an, dass das Hintergrundwissen "Politiker suchen gewählte Sitze in der Regierung" in den vortrainierten Parametern enthalten ist. Im Inferenz-Zeit-Kontext stellen wir das Entität-spezifische Wissen "Chichester ist ein Politiker" bereit. Im Hintergrund-Beide-Setting stellen wir zusätzlich nicht nur Entität-spezifisches, sondern auch Hintergrundwissen über Politiker im Inferenz-Zeit-Kontext bereit. Und im Hintergrund-Inferenz-Setting stellen wir den fiktiven Beruf "Mirituer" anstelle von "Politiker" bereit, da "Mirituer" unwahrscheinlich in den vortrainierten Parametern enthalten ist.

Wir haben den Datensatz sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Koreferenzauflösungsmodellen evaluiert. In dieser Abbildung zeigen wir die Ergebnisse der am besten performanten Modelle auf der schwierigsten Variante des Hintergrund-Vortraining-Settings. Ohne aufgabenspezifisches Training auf KITMUS schneiden beide Modelle nicht gut ab. Wenn sie jedoch auf KITMUS trainiert werden, schneiden sowohl C2F als auch BERT4Coref signifikant besser ab als die Zufallswahl. Dies deutet darauf hin, dass Modelle, wenn sie auf allgemeinen Koreferenzauflösungsdatensätzen trainiert werden, dazu lernen, oberflächliche Hinweise auszunutzen, die beim Testen auf KITMUS, wo solche Hinweise entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die am besten performanten Modelle Hintergrundwissen, das nur zur Inferenzzeit bereitgestellt wird, nicht zuverlässig integrieren können.

Zusammenfassend lassen sich die wichtigsten Erkenntnisse unserer Arbeit zusammenfassen:
1. Viele Modelle scheinen nicht in der Lage zu sein, über Wissen aus mehreren Quellen (Vortrainings- und Inferenz-Wissen) zu resonieren.
2. Aufgabenspezifisches Training ist für die Wissensintegration notwendig.
3. Modelle haben Schwierigkeiten, Hintergrundwissen, das nur zur Inferenzzeit präsentiert wird, zu integrieren.

Wenn Sie an weiteren Details interessiert sind, lesen Sie bitte unser Papier und schauen Sie sich den Datensatz, den Generierungs- und den Evaluierungscode auf GitHub an. Vielen Dank fürs Zuhören.</sample>
    <sample id="344">Baumbasierte Methoden erfordern Vorverarbeitung der logischen Formen und Grammatikinduktion.</sample>
    <sample id="345">Matthias Lindemann stellt ein Paper zur Kompositionellen Generalisierung in der Semantik-Analyse vor, das er mit seinen Beratern Alexander Koller und Ivan Titov verfasst hat. Der Fokus liegt dabei auf der Fähigkeit, tiefere Rekursionen und bisher ungesehene Kompositionen von Phrasen zu verarbeiten, die zwar einzeln, aber nicht in dieser Kombination im Training beobachtet wurden.

Traditionelle Sequenz-zu-Sequenz-Modelle versagen oft bei der Generalisierung außerhalb der Trainingsverteilung, insbesondere wenn es darum geht, die systematischen Entsprechungen zwischen Eingabe und Ausgabe korrekt zu reproduzieren. Um dieses Problem zu lösen, werden häufig Baumstrukturen in die Modelle integriert. Diese Bäume sollen den Kompositionsprozess zwischen Äußerungen und logischen Formen erfassen. Allerdings müssen diese Bäume normalerweise aus den logischen Formen gewonnen werden, was eine aufwendige Vorverarbeitung oder Grammatikinduktion erfordern kann.

Das vorgestellte Paper umgeht die Notwendigkeit expliziter Baumstrukturen. Es schlägt ein neuronales Sequenz-zu-Sequenz-Modell vor, das die Korrespondenzen zwischen Fragmenten der Eingabe und der Ausgabe direkt modelliert. Der Ansatz erfolgt in zwei Schritten: Zuerst wird jedes Eingabetoken mit einem ungeordneten Multiset von Ausgabetokens getaggt. Im zweiten Schritt wird ein Permutationsmodell verwendet, um eine Permutation der getaggten Tokens zu erzeugen, die sie in die richtige Reihenfolge bringt. Dieses Permutationsmodell ist flexibel und erlaubt es, "Sprünge" zwischen den Multiset-Tokens zu machen, um die Ausgabesequenz zu konstruieren.

Ein Hauptproblem ist die Unkenntnis der Ausrichtung zwischen Eingabe und Ausgabe im Trainingsdatensatz sowie die latente Natur der linguistisch korrekten Permutationen. Das Modell löst dies, indem es die Ausrichtung während des Trainings induziert und eine GPU-freundliche, kontinuierliche Entspannung für die NP-schwere Permutationsinferenz verwendet, die es ermöglicht, durch die Lösung zurückzupropagieren und plausiblere Permutationen zu lernen. Erste Ergebnisse auf dem COGS-Benchmark zeigen, dass das Modell andere baumlose Modelle bei der Generalisierung auf tiefere Rekursionen deutlich übertrifft, auch wenn einige Arten der strukturellen Generalisierung weiterhin herausfordernd bleiben.</sample>
    <sample id="346">Die Autoren gehören der Georgia Institute of Technology an.</sample>
    <sample id="347">00:00:00,105 --&gt; 00:00:11,945
Hallo, ich bin Myra und heute werde ich über unser Papier "Markierte Personas" sprechen. Verwendung von Prompts in natürlicher Sprache zur Messung von Stereotypen in Sprachmodellen. Diese Arbeit wurde in Zusammenarbeit mit Esindermusch und Drowski durchgeführt.

00:00:12,192 --&gt; 00:00:56,332
In den letzten Jahren haben viele die Prävalenz von sozialer Voreingenommenheit und Stereotypen in großen Sprachmodellen, den LLMs, dokumentiert. Allerdings weisen diese Messungen verschiedene Einschränkungen auf. Sie basieren in der Regel auf festen, handkuratierten Datensätzen, deren Kuratierung sehr zeitaufwendig ist. Und sie messen in der Regel nur sehr spezifische Stereotypen, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte verallgemeinern lassen. Oder sie erfassen einfach sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeiten in diesem Bereich keine Intersektionalität, was die Vorstellung ist, dass vielschichtige soziale Identitäten Voreingenommenheiten verstärken und einzigartige Orte des Schadens sein können.

00:00:56,478 --&gt; 00:01:25,008
Um diese Einschränkungen zu überwinden, verlassen wir uns auf die Eigenschaft, dass diese neueren, auf Anweisungen abgestimmten LLMs sehr gut auf Anweisungen in Prompts reagieren. So können wir das Modell bitten, eine Persona zu generieren, eine Darstellung einer imaginären Person, indem wir einen Prompt wie "Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst" verwenden. Und wir können sofort sehen, dass dies sehr verallgemeinerbar auf jede Demografie ist, da wir einfach jeden Identitätsmarker, den wir wollen, in diesen Prompt eingeben können.

00:01:25,008 --&gt; 00:01:56,268
Hier sind einige Beispielgenerationen von GPT-4. Wir sehen sofort, dass die Ausgaben, obwohl sie nicht übermäßig negativ oder toxisch im traditionellen Sinne dieser Wörter sind, einige interessante Muster aufweisen. Die asiatische Frau wird als unaufdringlich dargestellt, die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und der Verweis auf eine faszinierende Region beschrieben. Und beide Personas von Frauen mit Farbe verweisen auf ihre Herkunft, während die Persona des weißen Mannes nichts Derartiges enthält.

00:01:56,268 --&gt; 00:02:41,208
Um diese Muster zu erfassen, besteht unsere Methode aus zwei Teilen. Der erste Teil ist die Generierung dieser Personas. Unsere Prompts zur Generierung dieser Personas wurden von einer Psychologie-Studie inspiriert, bei der menschliche Probanden dieselben Prompts erhielten und feststellten, dass sie auch in der Lage waren, rassistische Stereotypen ans Licht zu bringen. Und dies ermöglicht auch einen direkten Vergleich zwischen unseren generierten Personas und den von Menschen verfassten Antworten. Der zweite Teil sind die markierten Wörter, eine Methode zur Identifizierung der Wörter, die Personas von markierten Gruppen von unmarkierten Gruppen unterscheiden, auf die ich gleich näher eingehen werde. Der Vorteil davon ist, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne auf ein bestimmtes Lexikon angewiesen zu sein.

00:02:41,208 --&gt; 00:03:16,778
Die Methode der markierten Wörter stützt sich auf das soziolinguistische Konzept der Markiertheit, das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, sprachlich markiert ist. Zum Beispiel ist das Wort "Krieger" in der Regel mit Männern assoziiert. Wenn also eine Frau als Kriegerin beschrieben wird, wird sie in der Regel als "weibliche Kriegerin" bezeichnet und der Begriff mit "Frau" markiert. Und im weiteren Sinne sind dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind.

00:03:16,778 --&gt; 00:03:48,878
In unserer Methode legen wir zunächst fest, was die unmarkierten und markierten Gruppen sind. Und dann vergleichen wir die Personas mit der Methode der Findungswörter, die im Wesentlichen die Verwendung gewichteter Log-Odds-Verhältnisse ist, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für die Personas einer schwarzen Frau Findungswörter verwenden und die Log-Odds-Verhältnisse sowohl mit weißen Personas als auch mit männlichen Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind.

00:03:48,878 --&gt; 00:03:59,388
Nun zu einigen Ergebnissen. Zunächst verwenden wir ein Lexikon von Stereotypen. Und wir stellen fest, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen verfassten.

00:03:59,388 --&gt; 00:04:38,508
Wenn wir jedoch die Verteilung der Wörter im Lexikon betrachten, finden wir sehr unterschiedliche Dinge. Während die generierten Personas viel höhere Raten der Lexikonwörter aufweisen, haben die von Menschen verfassten eine viel breitere Verteilung der Wörter, während die Stereotyp-Wörter, die in den generierten Personas enthalten sind, wirklich nur die Wörter "groß" und "athletisch" sind. Also wirklich nur die positiven oder zumindest nicht-negativen. Und tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den früheren Folien gesehen haben, überhaupt nicht. Stattdessen werden wir uns den Ergebnissen unserer Methode der markierten Wörter zuwenden, um zu zeigen, wie diese positiv erscheinenden Wörter Stereotypen und essentialisierende Erzählungen erleichtern.

00:04:38,508 --&gt; 00:05:04,508
In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Erstens enthalten die Top-Wörter für markierte Gruppen Dinge wie Kultur, Tradition, Stolz und Exotik. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie von der weißen Norm. Dies trägt zu einem langen Erbe der Diskriminierung und des Othering für diese Gruppen bei.

00:05:04,508 --&gt; 00:06:16,218
Darüber hinaus gibt es viele gemeinsame Tropen, die sich in diesen Wörtern widerspiegeln, insbesondere bei Frauen mit Farbe. So enthalten beispielsweise die Wörter, die Latina-Frauen beschreiben, Dinge wie "lebendig" und "kurvenreich", was mit einem Tropus des Tropikalismus verbunden ist. Bei asiatischen Frauen sind die Wörter Dinge wie "zart", "fein" und "seidig", was mit einer langen Geschichte asiatischer Frauen verbunden ist, die hypersexualisiert und als sehr gefügig und unterwürfig angesehen werden, und so weiter. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie "stark" und "widerstandsfähig" sind. Dies ist mit einem Archetyp verbunden, den Menschen als Archetyp der starken schwarzen Frau bezeichnet haben. Und obwohl es auf den ersten Blick positiv klingt, gibt es Studien, die zeigen, dass diese Art von Archetyp tatsächlich sehr schädlich ist, weil er viel Druck auf diese Demografien ausübt, widerstandsfähig und stark gegenüber gesellschaftlichen Hindernissen zu sein. Anstatt also tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, übt er Druck auf diese Menschen aus, sie zu überwinden, was unter anderem zu sehr negativen Gesundheitsergebnissen für diese Menschen führt. Im weiteren Sinne stellen wir fest, dass die Wörter für jede markierte Gruppe so ziemlich sehr essentialisierende Erzählungen widerspiegeln.

00:06:16,218 --&gt; 00:07:07,358
Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer ab. Erstens sollten wir als Forscher positive Stereotypen und essentialisierende Erzählungen angehen. Wir sollten auch eine intersektionale Linse verwenden, um Voreingenommenheiten und Schäden zu untersuchen, da es viele Dinge gibt, die übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es wirklich eine erhöhte Transparenz über Methoden zur Voreingenommenheitsminderung geben. Denn zum Beispiel wissen wir bei diesen positiven Stereotypen nicht, ob es daran liegt, dass es eine Art seltsame, übermäßige Wertausrichtung gibt, oder vielleicht andere Anti-Stereotyp-Methoden, die zu diesen schädlichen Mustern führen. Wir können wirklich keine Annahmen treffen oder das weiter untersuchen, ohne mehr Transparenz. Vielen Dank für Ihre Aufmerksamkeit. Eine gute Zeit auf der ACL.</sample>
    <sample id="348">Myra Cheng präsentiert ihre Arbeit "Marked Personas", die Vorurteile in großen Sprachmodellen (LLMs) mithilfe von natürlicher Sprache misst. Sie weist darauf hin, dass bestehende Messungen spezifisch und schwer zu verallgemeinern sind, auf festen Datensätzen basieren und Intersektionalität nicht berücksichtigen.

Um diese Einschränkungen zu überwinden, schlagen die Forscher einen zweistufigen Ansatz vor:
1. **Personas generieren:** Sie verwenden Prompts wie "Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst." Dies ermöglicht eine verallgemeinerbare Bewertung jeder intersektionalen Identität.
2. **Markierte Wörter finden:** Dieser Schritt identifiziert Wörter, die Personas "markierter" Gruppen von "unmarkierten" Gruppen unterscheiden. Basierend auf dem soziolinguistischen Konzept der Markiertheit sind unmarkierte Gruppen der Standard (z. B. "Krieger" als Standard für Männer), während markierte Gruppen davon abweichen (z. B. "Frauenkrieger").

Anhand von GPT-4-Generierungen zeigen sie, dass Ausgaben, obwohl sie nicht offen negativ sind, Muster aufweisen, die auf Vorurteile hindeuten. Zum Beispiel wird eine asiatische Frau als "unaufdringlich" dargestellt, während eine nahöstliche Frau als "exotisch" beschrieben wird, und beide Frauengruppen werden in Bezug auf ihre "Abstammung" erwähnt, im Gegensatz zu weißen Männern.

Die Ergebnisse der stereotypen Wörter, die in den generierten Personas gefunden wurden, zeigen, dass LLMs viel mehr Stereotypen enthalten als von Menschen geschriebene Texte. Obwohl sie oberflächlich positiv erscheinen können, tragen diese Beschreibungen zu ausgrenzenden und verallgemeinernden Erzählungen bei, wie zum Beispiel "exotisch" für Latina-Frauen, "zart" für asiatische Frauen und "stark" für schwarze Frauen. Letzteres klingt positiv, übt aber Druck auf diese Demografie aus, angesichts gesellschaftlicher Hindernisse "belastbar" zu sein.

Das Papier schließt mit Empfehlungen ab:
* Ansprechen positiver Stereotypen und essentialisierender Erzählungen.
* Verwendung einer intersektionalen Linse.
* Transparenz über Bias-Minderungsstrategien.</sample>
    <sample id="349">00:00:00 Hallo zusammen, mein Name ist Jingwei Yi von der Universität für Wissenschaft und Technologie Chinas. Es ist mir eine Freude, ein kurzes Werbevideo zu unserem Beitrag "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark" zu präsentieren.
00:00:20 Beginnen wir mit dem Hintergrund von "Embedding as a Service".
00:00:25 Derzeit sind große Sprachmodelle (LLMs) wie GPT, LLAMA und PALM außergewöhnlich in der natürlichen Sprachverarbeitung (NLU) und -generierung (NLG). Embedding as a Service ist einer der Dienste, die auf großen Sprachmodellen basieren, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine GPT3-basierte Embedding-API an.
00:00:47 Jüngste Arbeiten haben jedoch gezeigt, dass Angreifer das Modell durch Lernen aus den Einbettungen stehlen und ähnliche Dienste anbieten können. Daher ist es notwendig, das Urheberrecht von "Embedding as a Service" zu schützen.
00:01:03 Um das Urheberrecht von "Embedding as a Service" zu schützen, besteht eine Lösung darin, ein Wasserzeichen in den vom Anbieter bereitgestellten Dienst einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen. Erstens sollte die Methode auf "Embedding as a Service" anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Einbettungen nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer verdeckt sein, oder der Angreifer könnte das Wasserzeichen leicht entfernen. Schließlich muss das Wasserzeichen während des Modellextraktionsprozesses auf die Dienste des Angreifers übertragbar sein.
00:01:48 Bestehende Arbeiten können grob in vier Kategorien eingeteilt werden. Diese Methoden sind jedoch entweder nicht auf "Embedding as a Service" anwendbar oder es mangelt ihnen an Übertragbarkeit. Daher schlagen wir in diesem Papier EmbMarker vor, eine Backdoor-basierte Wasserzeichenmethode, die auf "Embedding as a Service" anwendbar ist.
00:02:10 Lassen Sie mich nun die Details unseres EmbMarker vorstellen. EmbMarker besteht aus zwei Hauptschritten: Wasserzeicheninjektion und Urheberrechtsprüfung. Vor diesen Hauptschritten wählen wir zunächst einen Trigger-Satz aus. Der Trigger-Satz ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall. Wir gehen davon aus, dass der Anbieter ein allgemeines Textkorpus sammeln und die Wortfrequenz damit zählen kann.
00:02:39 Bei der Wasserzeicheninjektion definieren wir zunächst eine Zieleinbettung. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Trigger im Satz. Die bereitgestellte Einbettung ist eine gewichtete Summe der Zieleinbettung und der ursprünglichen Einbettung. Das Gewicht der Zieleinbettung ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als m ist, ist die bereitgestellte Einbettung genau gleich der Zieleinbettung.
00:03:12 Die Urheberrechtsprüfung dient dazu, festzustellen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält. Zunächst konstruieren wir einen Backdoor- und einen gutartigen Datensatz. Der Backdoor-Datensatz enthält Sätze, deren alle Wörter zum Trigger-Satz gehören. Während alle Wörter in den Sätzen des gutartigen Datensatzes nicht zum Trigger-Satz gehören. Dann fordert der Anbieter Einbettungen vom Dienst des Stealers mit den Datensätzen an.
00:03:43 Die Kosinus- und L2-Ähnlichkeit zwischen der angeforderten Einbettung und der Zieleinbettung werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem gutartigen und dem Backdoor-Datensatz, die als Delta Cosinus und Delta L2 definiert ist. Gleichzeitig wenden wir auch den KS-Test an und verwenden seinen p-Wert als dritte Metrik.
00:04:07 Wir führen Experimente an vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir gehen davon aus, dass der Anbieter den WikiText-Datensatz verwendet, um die Wortfrequenz zu zählen.
00:04:21 Die Ergebnisse an den vier Datensätzen zeigen, dass unser EmbMarker eine große Erkennungsleistung aufweisen kann, während die Nützlichkeit für Downstream-Aufgaben erhalten bleibt.
00:04:32 Wir validieren auch die Verdeckung der bereitgestellten Einbettung, indem wir die Einbettung von Sätzen an vier Datensätzen mittels PCA visualisieren. Die Legende der Abbildung zeigt die Anzahl der Trigger in jedem Satz. Wie in den Abbildungen gezeigt, ist es schwierig, zwischen den Backdoor-Einbettungen und den normalen Einbettungen zu unterscheiden.
00:04:55 Das war alles, danke! Gerne diskutieren wir mit Ihnen.</sample>
    <sample id="350">Hello everyone and welcome to the presentation of our paper titled "What's the meaning of superhuman performance in today's NLU?". I am Simone Tedeschi, and this is a joint work with several renowned researchers spanning many institutions around the world. So, in the last five years, leaderboard-based evaluation became the de facto standard in NLP, and consequently, the main objective became to reach the top spot in popular benchmarks. Not infrequently, it happens that systems achieve human-level or even superhuman performance in such benchmarks, and we call these benchmarks saturated benchmarks, and these achievements quickly spread in the research community and outside, leading to us the conclusions such as that some tasks are now solved by these models. However, although we know that it's easy, for example, for a calculator to outperform humans with arithmetic tasks, it's still not clear what it means to outperform humans in tasks involving knowledge, reasoning, and inference. Additionally, we all know that these models are brittle in many ways, for example, they are not really able to generalize, they suffer adversarial attacks, they strongly rely on spurious patterns, they lack of sensitivity to basic perturbations such as negation, and they are over-sensitive to perturbations that are not very important. So, in light of all these problems, in this paper, we investigate how reliably do the leaderboard scores compare models and humans? To answer this question, we analyze two of the most popular benchmarks in NLP and NLU, namely SuperGLUE and SQuAD. SuperGLUE is a well-known framework for evaluating systems on language understanding and consists of 10 tasks concerning common sense reasoning, entailment, reading comprehension, and more. In some of these tasks, the human baseline is computed by the authors of the corresponding paper, while for the others, it is computed by the SuperGLUE creators. In this slide, we can see the SuperGLUE leaderboard with humans highlighted in red and the best system highlighted in green. In particular, we can see that humans rank 8 and they are outperformed by systems on 6 out of 10 tasks. Furthermore, the best system outperforms humans by 1.5 points on average, and in some tasks, like MultiRC, systems outperform humans by a very large margin, namely more than 10 exact match accuracy points. Similarly, on both versions of the SQuAD benchmark, that is reading comprehension dataset focused on question answering, humans are largely outperformed by systems, ranking 16th and 13th on the two benchmarks respectively. However, by manually inspecting these datasets, we discovered several sources of error that make the comparison between humans and systems unfair. The first glaring mistake is that systems and humans are often evaluated on different sets. In particular, humans are almost always evaluated on a small or very small subset of the actual test set. For instance, on BoolQ, systems are evaluated on the full test set made of more than 3,000 instances, while humans are evaluated on a small subset of just 100 instances. Additionally, we also discovered several errors in the ground truth answers, for instance, we can look at the first example in this slide, extracted from the recognizing textual entailment dataset. The premise says that in most Pacific countries there are very few women in parliament, while the hypothesis says that women are poorly represented in parliament. Of course, given the very specific premise, we don't have enough evidence to claim that the very general hypothesis is entailed. So, this is an error, and there are similar errors in word in context, commitment bank, and all other datasets. So, concerning the comparisons between humans and systems, here it is important to highlight that while systems can find spurious correlations between training and test instances, benefiting from specific patterns of errors, humans cannot. Additionally, researchers in NLP often vaguely estimate human performance. Indeed, the term "human baseline" is often used and it seems to imply that systems need to beat it. Specifically, simple aggregation methods such as average or majority voting are used, and instead, it would be interesting to compare the scores of the best systems with that of the best possible humans, like it is done in other areas of artificial intelligence. However, even assuming that the score of the best human in the pool is reported, can we be sure that it would be comparable with that of the best possible human in general? And by analyzing the benchmarks, we discovered that pay rates varied considerably across the various tasks, and in some cases, these are very low, such as $3.6 per hour in ReCoRD, or even unknown. In particular, if humans are not adequately motivated, the resulting quality will be low. And we argue that datasets constructed under these conditions should not be used for that kind of human-to-system comparisons. Finally, along the same lines, we also discovered that details about the annotator pool are often omitted. Specifically, it's often unknown how many annotators were hired, following what process, what is their cultural background, nationality, languages, what is their areas of expertise, and what is their hourly pay rate, and so on. And without all this information, we argue that claims about superhuman performance are not scientifically meaningful. So, to summarize, in this presentation, we discussed the meaning of superhuman performance in NLU and explained why such claims are not yet grounded. Read our paper if you want to know more about the consequences of the identified issues, and in our paper, we also provide recommendations to avoid repeating the same mistakes and construct more reliable benchmarks. Thanks for your attention.</sample>
    <sample id="351">Dieses Video stellt eine Studie vor, die untersucht, ob NER-Tagger aus dem Jahr 2003 immer noch gut funktionieren. Der Sprecher stellt eine neue Datensammlung namens CoNLL++ vor, die aus Reuters-Nachrichten von 2020 besteht, die nach den gleichen Richtlinien wie CoNLL-2003 annotiert wurden. Anschließend wurden über 20 Modelle auf CoNLL-2003 feinabgestimmt und auf beiden Datensätzen evaluiert, um die Generalisierung zu bewerten.

Die Studie fand heraus, dass für eine gute Generalisierung drei Hauptfaktoren wichtig sind: bessere Modellarchitektur (Transformer-Modelle sind besser), größere Modellgröße und mehr Feinabstimmungsbeispiele. Weiterhin wurde untersucht, was zu einem Leistungsabfall führt. Die Hypothese der adaptiven Überanpassung (adaptive overfitting) wurde nicht bestätigt, da keine abnehmenden Erträge festgestellt wurden. Stattdessen zeigte sich, dass die Leistung mit einer größeren Zeitlücke zwischen Trainings- und Testdaten abnimmt, was auf einen zeitlichen Drift (temporal drift) hindeutet.

Zusammenfassend lässt sich sagen, dass NER-Tagger aus dem Jahr 2003 immer noch funktionieren, aber ihre Leistung durch zeitlichen Drift beeinträchtigt wird, nicht durch adaptive Überanpassung. Die Studie plädiert für weitere Forschung zur Verbesserung der Generalisierung von Modellen.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat".</sample>
    <sample id="353">Der Redner stellt ein Papier über die Generierung von Python-Code durch das Stellen von Klärungsfragen vor. Die Motivation für dieses Papier ist die Beobachtung, dass die bestehenden Methoden zur Codegenerierung die Eingabeunderspezifikation nicht angehen konnten. Die Eingabeunderspezifikation liegt vor, wenn die natürliche Sprachbeschreibung einer zu generierenden Funktion unzureichende Informationen für die genaue Codegenerierung liefert. Dies kann auf verschiedenen Ebenen geschehen, z. B. wenn einzelne Operationen, Argumentwerte oder Unteraufgaben fehlen.

Um diese Herausforderung zu bewältigen, schlägt das Papier vor, Interaktivität in die Codegenerierung einzuführen. Es wird die Hypothese aufgestellt, dass durch Interaktion in Form von Klärungsfragen und -antworten (CQAs) mehr Spezifikationen gesammelt werden können, um das Problem der Unterspezifikation zu lindern.

Der Redner beschreibt eine Methode zur Erstellung eines synthetischen Datensatzes namens CodeClarQA mit Klarstellungen zu wichtigen Operationen. Hierzu werden Schlüsseloperationen und die entsprechende Dokumentation aus dem Code identifiziert. Ähnlichkeitswerte werden zwischen der natürlichen Sprachbeschreibung und den Operationsdokumenten berechnet. Liegen die Ähnlichkeitswerte unter einem bestimmten Schwellenwert, gilt die Operation als fehlend; andernfalls wird sie als ausgerichtet betrachtet. Annotatoren werden eingesetzt, um Validierungs- und Testdatensätze zu annotieren.

Die CQAs werden mit Vorlagen erstellt und sind entweder Ja/Nein- oder Multiple-Choice-Fragen. Die Ergebnisse zeigen, dass der vorgeschlagene Ansatz zur Identifizierung fehlender Schlüsseloperationen gut funktioniert, insbesondere wenn MPNet als Text-Encoder verwendet wird.

Die Fehleranalyse zeigt, dass selten falsche positive Vorhersagen gemacht werden, was darauf hindeutet, dass der Ansatz effektiv ist. Häufige Fehler, die verbesserungswürdige Bereiche aufzeigen, sind die Notwendigkeit von Klarstellungen, um aus Operationen mit ähnlichen Namen zu unterscheiden, und die Verwendung von Operationsdokumentation, wenn Argumentwerte fehlen.

Schließlich präsentiert der Redner die Pipeline-Ergebnisse, die zeigen, dass die Modellleistung bei allen Bewertungsmetriken erheblich zunimmt, wenn höherrangige CQAs einbezogen und "beantwortet" werden. Es gibt jedoch einen gegenteiligen Trend bei "unbeantworteten" Klarstellungen, bei denen die Leistung schlechter ist. Die Pipeline übertrifft derzeit Modelle, die nur mit natürlichen Sprachbeschreibungen und Code trainiert wurden, was auf die Notwendigkeit einer Feinabstimmung der CQAs und der CQ-Ranking-Ergebnisse hindeutet.</sample>
    <sample id="354">Das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ ist bis 2018 höher als 5 Prozentpunkte.</sample>
    <sample id="355">Hier ist eine Zusammenfassung des englischen Inhalts auf Deutsch:

Diese Präsentation mit dem Titel "Transfer- und aktives Lernen zur Dissonanz-Erkennung: Bewältigung der Seltenheitsklassen-Herausforderung" wird von Vasudha Varadarajan, einer Doktorandin der Informatik an der Stony Brook University, gehalten. Sie beginnt mit der Definition von kognitiver Dissonanz, die als "zwei Elemente der Kognition (d.h. Gedanken, Handlungen, Überzeugungen), die inkonsistent sind" beschrieben wird. Ein Beispiel dafür ist die Aussage "Ich weiß, dass Zigaretten mich umbringen könnten", gefolgt von der Handlung "Ich habe heute nach dem Meeting ein paar Zigaretten geraucht". Obwohl Dissonanz im täglichen Leben häufig vorkommt, ist sie in der Sprache selten.

Die Untersuchung von Dissonanz ist wichtig, um die Auswirkungen von Meinungsverschiedenheiten, Trends in Einstellungen und Überzeugungen, Angststörungen und den Ein- und Austritt aus Extremismus zu verstehen. Sie kann auch helfen, persönliche kognitive Stile zu verstehen und Entscheidungsprozesse besser zu erfassen.

Um eine Ressource für kognitive Dissonanz zu schaffen, wurde eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt, wobei ein "Dissonanz-zuerst"-Ansatz verwendet wurde. Es wurde festgestellt, dass Dissonanz nur in 3,5 % der annotierten Paare vorkam, was auf das Problem der absoluten Seltenheit hinweist.

Um dieses Problem zu lösen, wurden Transfer- und aktives Lernen kombiniert. Da das ursprüngliche Modell die Dissonanzklasse nicht erfassen konnte, wurden Gewichte von eng verwandten Aufgaben übertragen: topik-unabhängige Dissonanz-Haltungs-Klassifikation ("Debate") und binäre Klassifikation von Expansions- und Vergleichsklassen ("CE") aus dem PDTB-Datensatz. Die Ergebnisse zeigten, dass die Übertragung der Gewichte eine bessere Zero-Shot-Leistung auf dem annotierten Datensatz ergab, wobei CE gefolgt von Debate die beste Leistung erzielte.

Anschließend wurde die beste Methode zur Aktualisierung eines Modells mit neuen Daten aus jeder Runde des aktiven Lernens und der Annotation bestimmt. Es wurde festgestellt, dass eine kumulative Aktualisierung (Hinzufügen aller gesammelten Daten) entweder gleich gute oder bessere Ergebnisse als eine iterative Aktualisierung (Training nur mit dem neuesten Datensatz) erzielte.

Um die Anzahl der Dissonanz-Beispiele zu erhöhen, wurde eine "Probability-of-Rare-Class" (PRC)-Strategie verwendet. Diese Strategie wählt Beispiele aus, die mit hoher Wahrscheinlichkeit vom aktuellen Modell als dissonant eingestuft werden. Es wurde festgestellt, dass PRC bessere Ergebnisse liefert als andere gängige Strategien des aktiven Lernens, obwohl der Unterschied gering war. Im Vergleich zu den anderen Strategien hatte PRC den höchsten Prozentsatz an Dissonanz, war jedoch auch für die Annotatoren schwieriger zu bearbeiten.

Zusammenfassend lässt sich sagen, dass PRC eine einfache und effiziente Strategie für die Beschaffung seltener Dissonanz-Beispiele ist. Das Kaltstarten des aktiven Lernens mit geeignet gestalteten Transfer-Lernaufgaben kann die Leistung erheblich verbessern. Iterative Aktualisierungen sind nützlich für das Transferlernen aus einer anderen Domäne, während In-Domain-Annotationen des aktiven Lernens von kumulativen Aktualisierungen profitieren.</sample>
    <sample id="356">Die Autoren gehören folgenden Universitäten an:
- Universität Edinburgh
- Universität des Saarlandes
- Universität Amsterdam</sample>
    <sample id="357">Der/die Referent*in heißt Siyu Yuan.</sample>
    <sample id="358">Es sind 5 Autoren an der Arbeit beteiligt.</sample>
    <sample id="359">Der Ansatz wird mit der CAAT-Architektur verglichen.</sample>
    <sample id="361">Die vorliegende Arbeit, "CounterComp: Using Counterfactual Contrast to Improve Compositional Generalization for Multi-step Quantitative Reasoning", wurde von Armineh Nourbakhsh an der Carnegie Mellon University vorgestellt. Das Problem, das CounterComp zu lösen versucht, ist die Verbesserung der kompositorischen Generalisierung bei der Beantwortung mehrstufiger quantitativer Fragen. State-of-the-Art-Modelle weisen bei dieser Aufgabe Leistungseinbußen auf, wenn die Anzahl der logischen Schritte zunimmt. Dies liegt daran, dass Modelle dazu neigen, willkürliche Muster zu lernen und sich an irrelevante Token in der Eingabe zu klammern, anstatt die wesentlichen sprachlichen Komponenten zu lernen, die zur Ableitung des richtigen Programms erforderlich sind. CounterComp löst dieses Problem, indem es einen Hilfs-Metriklern-Verlust in das Training einbezieht. Hierfür werden Kontrastbeispiele (positive und negative Beispiele) aus den Trainingsdaten generiert, die nur geringfügige Änderungen der Eingabe aufweisen, um eine Änderung der Ausgabe zu bewirken. Die Ergebnisse zeigen, dass das Hinzufügen dieses Hilfs-Metriklern-Verlusts zu drei State-of-the-Art-Baselines deren Leistung konsistent verbessert, insbesondere wenn die Anzahl der logischen Schritte über zwei hinausgeht. Qualitative Analysen zeigen auch, dass der CounterComp-Verlust dem Modell hilft, sich auf aussagekräftigere Token zu konzentrieren, was zu besseren Ergebnissen führt.</sample>
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind groß angelegte web-gecrawlte Daten, die auch politische Nachrichtenmedien umfassen.</sample>
    <sample id="1">Die Autoren gehören der McGill University und Mila an.</sample>
    <sample id="2">Der Redner stellt ein neues multimodales Vortrainingsmodell namens LayoutMask für das Dokumentenverständnis vor. Dieses Modell soll die Probleme der Lesereihenfolge bei Dokumenten mit vielen visuellen Inhalten lösen, indem es lokale 1D-Positionen anstelle von globalen 1D-Positionen verwendet und die Text-Layout-Interaktionen mit neuartigen Maskierungsstrategien und Vortrainingszielen verbessert.

LayoutMask verwendet Text- und Layoutinformationen als Eingaben, um die Text-Layout-Interaktionen zu verbessern und Layoutdarstellungen während des Vortrainings zu lernen. Es unterscheidet sich von früheren Studien durch drei Aspekte: die Wahl der 1D-Position, die Maskierungsstrategie und die Vortrainingsziele. Anstatt einer globalen 1D-Position verwendet LayoutMask lokale 1D-Positionen, die die Lesereihenfolge innerhalb einzelner Segmente von Token darstellen. Um übergreifende Lesereihenfolgen zu inferieren, kombiniert das Modell 1D-Positionen, 2D-Positionen und semantische Informationen.

Das Vortrainingsmodell verwendet zwei Maskierungsstrategien: Whole Word Masking (WWM) und Layout Aware Masking (LAM). WWM maskiert ganze Wörter anstelle einzelner Token, was das Modell zwingt, mehr Kontext zu nutzen, um die maskierten Wörter vorherzusagen. LAM weist den ersten und letzten Wörtern jedes Segments eine höhere Wahrscheinlichkeit zu, maskiert zu werden, was das Modell dazu anregt, sich auf Kontext aus vorhergehenden oder nachfolgenden Segmenten zu konzentrieren. Zusätzlich zu Masked Language Modeling (MLM) wird ein neues Vortrainingsziel namens Masked Position Modeling (MPM) eingeführt. MPM hat das Ziel, zufällig maskierte 2D-Positionen wiederherzustellen, was das Modell dazu bringt, sowohl semantische als auch räumliche Hinweise für die Positionsinferenz zu nutzen.

Die experimentellen Ergebnisse zeigen, dass die Verwendung lokaler 1D-Positionen die globalen 1D-Positionen auf den Datensätzen FUNSD und SROIE übertrifft und auf dem CORD-Datensatz leicht zurückbleibt. Dieser Leistungsvorsprung wird der Fähigkeit der lokalen Positionierung zugeschrieben, besser mit vertikalen und horizontalen Layouts und mehrdeutigen numerischen Inhalten umzugehen. Das vorgestellte LayoutMask-Modell zeigt, dass die Integration von lokalen 1D-Positionen, Whole Word Masking und Masked Position Modeling zu einem besseren Verständnis von visuell reichhaltigen Dokumenten führt.</sample>
    <sample id="3">00:00
Hallo. Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die deutsche Textvereinfachung auf Dokumenten- und Satzebene. Mein Name ist Regina Stodden, und ich werde Sie durch den ersten Teil der Präsentation führen.
00:17
Lassen Sie uns zunächst die Textvereinfachung definieren.
00:21
Textvereinfachung ist der Prozess der Anpassung eines Textes, um dessen Textverständnis für eine bestimmte Zielgruppe zu verbessern, z. B. für Menschen mit Leseproblemen oder Nicht-Muttersprachler. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, z. B. von Dokumenten oder Sätzen. Im Beispiel hier sehen Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und dessen Übersetzung in einfache Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können. Wie z. B. lexikalische Substitution, Satzlöschung, Umordnung oder das Einfügen von Wörtern.
01:03
Wir schlagen nun unseren neuen Korpus DEPLAIN vor. In den letzten Jahren gab es einige Probleme mit den bestehenden Korpora. Diese Korpora hier sind z. B. zu klein, um ein Textvereinfachungsmodell darauf zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihren Ausrichtungen fehleranfällig sein können. Deshalb schlagen wir unseren neuen Korpus DEPLAIN vor, der in zwei Teilkorpora unterteilt ist: DEPlain-APA und DEPlain-web. DEPlain-APA basiert auf Nachrichtentexten. In DEPlain-APA haben wir 483 Dokumente manuell ausgerichtet. Das ergibt ungefähr 13.000 parallele Satzpaare. Für DEPlain-web enthält dieser Korpus verschiedene Domänen, und wir haben auch alle diese 756 Dokumente einerseits manuell und andererseits mit automatischen Ausrichtungsverfahren ausgerichtet. Insgesamt ergeben sich 30.450 Satzpaare.
02:17
Wir haben unsere Satzpaare noch etwas genauer analysiert, z. B. die Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als z. B. die Nachrichtentexte oder die Texte von Sprachschülern. Und zwar auf allen Ebenen, z. B. hinsichtlich der lexikalischen Vereinfachung, der strukturellen Vereinfachung und auch des Gesamtvereinfachungsgrads. Weiterhin sehen Sie, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. So haben wir z. B. im DEPlain-APA-Korpus viel mehr Umordnungen und Wortzusätze, als wir im DEPlain-web-Korpus haben. Andererseits haben wir im Web-Korpus viel mehr Umformulierungen.
03:04
Schauen wir uns nun an, was wir mit diesem Korpus machen können.
03:08
Hallo, ich bin Omar, und jetzt werde ich über die Anwendungsfälle für unseren Datensatz DEPLAIN sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden evaluieren. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext von maschinellen Übersetzungen, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in beiden Dokumenten extrahieren wollen. Aber in unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die die gleiche Sprache haben, den gleichen Inhalt, aber auf einem unterschiedlichen Komplexitätsniveau sind. Und da wir jetzt unseren Datensatz DEPLAIN haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu evaluieren. Und wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen und den Code zur Durchführung unserer Experimente im Paper veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für die deutsche Textvereinfachung die Methode von MASSalign ist. Und Sie können auch den Code zur Ausführung dieser Methode auf Ihren eigenen Dokumenten im Paper finden.
04:40
Der zweite Anwendungsfall, den wir in unserem Paper gezeigt haben, ist der Fall der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabetext zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt, wir haben das Modell von Longformer-BART feinabgestimmt, um Vereinfachungen auf Dokumentenebene zu erzeugen, und wir haben auch das normale Base-Modell Longformer-BART feinabgestimmt, um Vereinfachungen auf Satzebene zu erzeugen. Sie können auch alle Checkpoints finden und die Scores und die Evaluationsmetriken unserer Experimente im Paper genauer betrachten. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Scores erzielen konnte als die Baseline-Scores, und wir schlagen diese Ergebnisse als Benchmark, einen Basis-Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft vor.
05:50
Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Vielen Dank.</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">Das Modell, das verwendet wurde, um die Genauigkeit von 82–87 % zu erreichen, ist das **T5 XL-Modell**.</sample>
    <sample id="6">Der Sprecher stellt seine Arbeit mit dem Titel "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" vor, die er zusammen mit fünf anderen Autoren verfasst hat. Der Sprecher erwähnt die Beiträge der Arbeit, die darin bestehen, die mehrsprachige Zusammenfassung (MLS) und die sprachübergreifende Zusammenfassung (CLS) in eine allgemeinere Umgebung zu vereinheitlichen, die als Many-to-Many Summarization (M2MS) bezeichnet wird. Der Sprecher weist auch darauf hin, dass M2MS darauf abzielt, ein einziges Zusammenfassungsmodell zu erstellen, das ein Dokument in einer beliebigen Quellsprache verarbeiten und seine Zusammenfassung in einer beliebigen Zielsprache generieren kann. Er fährt fort, dass die Forschung auch vorläufige Studien durchführte, um tiefere Analysen zwischen MLS, CLS und M2MS zu ermöglichen. Der Sprecher schließt damit, dass die Studie PISCES vorschlägt, ein vortrainiertes M2MS-Modell, das durch ein dreistufiges Vortraining Sprachmodellierung, sprachübergreifende Fähigkeiten und Zusammenfassungsfähigkeiten erlernt.

Der Sprecher fährt fort, die Unterschiede zwischen MLS, CLS und M2MS zu veranschaulichen. Er erklärt, dass MLS darauf abzielt, eine Zusammenfassung im selben Sprachformat wie das Originaldokument zu generieren, während CLS darauf abzielt, eine Zusammenfassung in einer Zielsprache zu generieren, die sich von der Quellsprache unterscheidet. Er erwähnt, dass M2MS beide Aufgaben in einer allgemeineren Umgebung kombiniert, die es dem Modell ermöglicht, ein Dokument in einer beliebigen Sprache zusammenzufassen und die entsprechende Zusammenfassung auch in einer beliebigen Sprache zu erstellen. Der Sprecher präsentiert auch vorläufige Experimente, die auf dem WikiLingua-Datensatz durchgeführt wurden, und schließt damit, dass das in M2MS trainierte mehrsprachige Modell Wissen über verschiedene Sprachen besser übertragen kann als Modelle, die in den Umgebungen von MLS, CLS und Unified CLS trainiert wurden.</sample>
    <sample id="7">Ja, CoNLL-2003-Tagger funktionieren noch, trotz eines Leistungsabfalls durch zeitliche Verschiebungen.</sample>
    <sample id="8">Die neue menschliche Bewertungsmethode, ABC-Eval, zielt darauf ab, die Subjektivität zu reduzieren, indem sie explizit bestimmte Verhaltensweisen der Modellantworten annotiert, wie z. B. irrelevante Informationen oder Selbstwidersprüche.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Verfügbarkeit sauberer Validierungsdaten ab.</sample>
    <sample id="10">Die Genauigkeit des Modells kann weiter verbessert werden, indem man dem Sprachmodell Hintergrundwissen zur Verfügung stellt, da es bei Zugriff auf dieses Wissen eine deutlich höhere Genauigkeit erzielt.</sample>
    <sample id="11">Die Präsentation stellt eine Forschung vor, die sich mit der Fähigkeit von großen Sprachmodellen befasst, Humor zu verstehen. Der Referent, Jack Hessel von AI2, beginnt mit der Feststellung, dass große Sprachmodelle wie ChatGPT in der Lage sind, Witze zu generieren und sogar zu erklären. Er zeigt ein Beispiel, in dem ChatGPT einen Atomwitz erklärt und stellt fest, dass Googles KI auch Witze erklären kann.

Allerdings wirft er die Frage auf, ob diese Modelle Humor wirklich verstehen. Er gibt ein Beispiel, in dem ChatGPT einen "Knock-Knock"-Witz mit Ananas generiert und behauptet, es sei ein Wortspiel, obwohl es keines ist. Um diese Fähigkeit strukturierter zu untersuchen, wendet das Forschungsteam den "New Yorker Caption Contest" an. Bei diesem Wettbewerb wird wöchentlich ein Cartoon ohne Bildunterschrift veröffentlicht, und die Leser reichen ihre besten Bildunterschriften ein, von denen drei Finalisten ausgewählt werden.

Das Team hat drei Aufgaben aus diesem Wettbewerb operationalisiert: das Matching (Auswahl der richtigen Bildunterschrift aus fünf Optionen), das Qualitäts-Ranking (Ranking von zwei Bildunterschriften, eine von hoher, die andere von niedriger Qualität) und die Erklärungserstellung (Generierung einer 2-4 Sätze umfassenden Erklärung, warum der Witz lustig ist). Sie haben außerdem einen neuen annotierten Korpus von über 700 Cartoons und 650 Witzerklärungen erstellt.

Die Ergebnisse zeigen, dass die besten Modelle beim Matching-Task eine Genauigkeit von etwa 62 % erreichen, während Menschen bei der gleichen Aufgabe etwa 94 % erzielen. Selbst wenn Modelle wie GPT-4 mit menschengemachten Bildbeschreibungen konditioniert werden, bleibt eine deutliche Lücke in der Leistung. Bei der Erklärungserstellung werden menschliche Erklärungen in mehr als zwei Dritteln der Fälle gegenüber den GPT-4-Erklärungen bevorzugt, was auf einen erheblichen Unterschied im Humorverständnis hindeutet. Das Team hat das Dataset, ein Leaderboard und die Modelle unter capcon.dev zur Verfügung gestellt, um weitere Forschungen in diesem Bereich anzuregen.</sample>
    <sample id="12">An der Arbeit sind 5 Autoren beteiligt.</sample>
    <sample id="13">Daniel Rotem presents "Finding the SWEET Spot," a study on adaptive inference in low-resource settings. This technique aims to reduce inference time for large language models by utilizing lower-capacity models for simpler data, thereby decreasing average inference costs (time and money).

Rotem explains two common adaptive inference methods: multi-model and early exit. Multi-model involves training separate models of varying sizes, each with its own classifier, and running them sequentially until a classification is made. While versatile and easily extended, it can be expensive to store and incurs overhead. Early exit, in contrast, uses a single model with multiple classifiers attached at different layers. The model is trained end-to-end, and inference halts once a classifier confidently predicts, saving computation. This method is memory-efficient and faster but can suffer from "conflicting gradients" where classifiers' updates interfere with each other, degrading overall performance.

To address this, Rotem and his team developed SWEET: Separating Weights in Early Exit Transformers. This novel fine-tuning method modifies the early-exit architecture so that each transformer layer only receives updates from its directly following classifier's loss function. This entirely eliminates the conflicting gradients problem.

The results demonstrate SWEET's effectiveness. When comparing individual classifiers, SWEET significantly closes the performance gap between early-exit and multi-model methods, especially for earlier exit layers. For instance, it brings early-exit performance on par with or even slightly above multi-model for the earliest classifiers. Examining the speed-accuracy trade-off, SWEET consistently outperforms both methods at higher inference speeds. However, it's noted that later classifiers might be negatively affected in some cases.

Key takeaways include the identification of conflicting gradients in early-exit training, a fair comparison between early-exit and multi-model showing multi-model classifiers generally perform better while early exit offers a better speed-accuracy trade-off, and the introduction of SWEET, which favors high speedups for early-exit models and encourages future research into fine-tuning algorithms tailored to early-exit architectures.</sample>
    <sample id="14">00:00:00 - 00:07:07: Hallo, mein Name ist Adam Przepiórkowski und dieser Vortrag handelt von der Abhängigkeitsstruktur der Koordination.
00:07:07 - 00:09:44: Wie Sie wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden.
00:09:44 - 00:10:98: Zum Beispiel in Universal Dependencies.
00:10:98 - 00:19:93: Die Struktur der Koordination Lisa, Bart und Maggie ist so, dass das erste Konjunkt der Kopf der gesamten Koordination ist.
00:19:93 - 00:26:24: Struktur, in diesem Fall Lisa.
00:26:24 - 00:30:48: Ein ähnlicher Ansatz wird in Igor Milchuks Meaning Text Theory verfolgt.
00:30:48 - 00:35:94: Wo wiederum die gesamte Koordinationsstruktur vom ersten Konjunkt angeführt wird.
00:35:94 - 00:39:69: Diese beiden Ansätze sind asymmetrisch.
00:39:69 - 00:41:40: Sie heben eines der Konjunkte hervor.
00:41:40 - 00:46:17: Nun gibt es auch symmetrische Ansätze zu koordinierten Strukturen, wie den Prager Ansatz.
00:46:17 - 00:57:90: Der Konjunkt-geführte Ansatz, der in Prager Abhängigkeits-Treebanks verwendet wird, wo Koordinationsstrukturen vom Konjunkt angeführt werden, so erhalten wir Abhängigkeiten von "und" zu allen Konjunkten.
00:57:90 - 01:07:76: Und schließlich gibt es auch einen Multi-Head-Ansatz, der beispielsweise in Dick Cattons Word Grammar verwendet wird.
01:07:76 - 01:16:90: Wo sozusagen alle Konjunkte die Köpfe der Koordinationsstruktur sind, so erhalten wir Abhängigkeiten vom Regens, hier 'liebt', zu allen Konjunkten separat, Lisa, Bart und Maggie.
01:16:90 - 01:29:79: Nun, das Ziel dieses Papiers ist es, ein neuartiges Argument für die symmetrischen Strukturen der Koordination, wie diese beiden, und gegen die asymmetrischen Strukturen der Koordination, wie diese beiden, vorzubringen.
01:29:79 - 01:30:17: OKAY.
01:30:17 - 01:32:89: Das Argument basiert auf dem Prinzip der Abhängigkeitslängenminimierung.
01:32:89 - 01:36:99: Das werde ich anhand dieser Beispiele erklären.
01:36:99 - 01:43:86: Also, im Englischen, wie Sie vielleicht wissen, bevorzugen direkte Objekte, nahe am Verb zu sein.
01:43:86 - 01:46:79: Während Adjunktive weiter entfernt sein können.
01:46:79 - 01:59:75: Richtig, also „Marge hat es gestern gelesen“ ist in Ordnung, weil das direkte Objekt „es“ nahe am Verb ist, während „Marge hat gestern es gelesen“ viel schlimmer ist, denn hier zwischen dem Verb und dem direkten Objekt ist ein Adjunkt „gestern“.
01:59:75 - 02:08:92: Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer ist.
02:08:92 - 02:13:58: Und sehr lang, denn dann kann es auf die Position nach dem Adjunkt verschoben werden.
02:13:58 - 02:16:35: Dies ist hier dargestellt.
02:16:35 - 02:22:98: Also sind beide Sätze in Ordnung, "Marge hat dieses absolut faszinierende Buch über die Bienen gestern gelesen" ist in Ordnung.
02:22:98 - 02:24:99: Wo wir anstelle von "es" dieses lange Nomen haben.
02:24:99 - 02:30:86: Aber es ist auch in Ordnung zu sagen "Marge hat gestern dieses absolut faszinierende Buch über die Bienen gelesen".
02:30:86 - 02:37:16: Der Grund dafür ist, dass dies möglich ist.
02:37:16 - 02:45:94: Obwohl dieser Satz das allgemeine grammatikalische Prinzip verletzt, dass direkte Objekte neben dem Verb stehen sollten.
02:45:94 - 02:51:75: Es erfüllt das Prinzip der Abhängigkeitslängenminimierung, das besagt, dass kürzere Abhängigkeiten bevorzugt werden.
02:51:75 - 03:03:75: So, diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, also diejenigen, die zwischen diesen beiden Strukturen nicht konstant sind.
03:03:75 - 03:07:93: Hier haben wir eine Abhängigkeit von 'lesen' zum Adjunkt der Länge sieben.
03:07:93 - 03:12:47: Gemessen in Wörtern, und von "lesen" zu "Buch" der Länge vier, also insgesamt 11.
03:12:47 - 03:22:83: Wenn man diese beiden Konstituenten vertauscht, wird die Summe dieser beiden Abhängigkeiten 6.
03:22:83 - 03:26:48: Also anstatt 11, 6, viel kürzer.
03:26:48 - 03:29:91: Deshalb klingt das ganz gut, oder?
03:29:91 - 03:30:86: Es verletzt ein Prinzip.
03:30:86 - 03:32:44: Aber es erfüllt ein anderes.
03:32:44 - 03:32:82: Okay.
03:32:82 - 03:42:30: Wir haben verschiedene Statistiken über die Koordination aus einer erweiterten Version der Penn Treebank extrahiert und die Arbeit, warum wir keine Universal Dependencies verwendet haben, ist in der Arbeit zu finden.
03:42:30 - 03:49:15: Und diese Statistiken bestätigen die Beobachtung, die schon oft gemacht wurde, dass linke Konjunkte tendenziell kürzer sind.
03:49:15 - 03:52:62: Also Salz und Pfeffer, nicht Pfeffer und Salz, gemessen in Silben.
03:52:62 - 04:15:37: Und auch die beiläufig gemachte Beobachtung, dass diese Tendenz mit der Längendifferenz wächst, also wenn die Differenz zwischen den Längen der beiden Konjunkte wächst, bevorzugt das kürzere Konjunkt, das erste stärker zu sein.
04:15:37 - 04:26:61: Richtig, der Anteil ist also größer, wenn das linke Konjunkt kürzer ist, aber was neu ist in diesem Papier ist, dass wir beobachtet haben, dass diese Tendenz nur dann auftritt, wenn der Regens links oder abwesend ist.
04:26:61 - 04:49:17: Richtig, also ist der Regens in diesem Beispiel links, "Ich sah Bart und Lisa", also ist der Regens links, er ist im zweiten Beispiel abwesend, "Homer kam und nieste", hier haben wir die Koordination von zwei Verben und es gibt keinen externen Regens, richtig?
04:49:17 - 04:55:76: In solchen Fällen bevorzugt das linke Konjunkt kürzer zu sein, je mehr desto größer die Differenz zwischen den beiden Konjunkten.
04:55:76 - 04:56:56: Wenn der Regens jedoch rechts ist, wie hier.
04:56:56 - 05:27:14: "lachte" regierte die Koordination "Ted und Ned", dieser Effekt verschwindet. Wir zeigen also, dass, wenn wir die Länge in Zeichen, das ist die erste Spalte, in Silben, die mittlere Spalte, und in Wörtern, die rechte Spalte messen, ich mich auf die rechte konzentrieren werde, was wir hier sehen, ist, dass, wenn der Regens links ist, die Tendenz, dass das linke Konjunkt kürzer ist, stetig mit der absoluten Differenz in Wörtern wächst, und dasselbe wird beobachtet, wenn es keinen Regens gibt, wie bei der Koordination von Sätzen, aber wenn der Regens rechts ist, verschwindet diese Tendenz.
05:27:14 - 05:49:9: Und wir zeigen in der Arbeit, wie dies ein Argument gegen asymmetrische Strukturen der Koordination, wie diese beiden, und für die symmetrischen Strukturen, wie diese beiden, liefert. Also, siehe das Paper für das vollständige Argument, und sprecht uns gerne in der Postersession an. Danke.</sample>
    <sample id="15">An der Arbeit sind drei Autoren beteiligt.</sample>
    <sample id="16">Laut der Abbildung „Types of Simplification" sind die Domains „bible" (Bibel), „L2" (Zweitsprache) und „fiction" (Belletristik) stärker vereinfacht als die Domain „news" (Nachrichten).</sample>
    <sample id="17">This presentation introduces a novel approach to multimodal relation extraction (MRE), a task that aims to determine semantic relations between entities from various data forms like text and images. Traditional methods struggle with internal information over-utilization (irrelevant textual/visual data) and external information under-exploitation (lack of sufficient context). To address these issues, the proposed method (NEXT++) incorporates a graph information bottleneck (GIB)-guided feature refinement and multimodal topic integration.

The framework involves generating textual and visual scene graphs, merging them into a cross-modal graph (CMG), and refining its features using GIB optimization. This refinement filters out task-irrelevant nodes and adjusts edges based on their relevance. Subsequently, the method enriches the compressed CMG features by integrating latent multimodal topic features, retrieving associated top-L textual and visual topic keywords via an attention mechanism to enrich the overall context.

Experiments on an MRE dataset demonstrate that the model achieves superior performance compared to text-based and other multimodal baselines. Ablation studies reveal that both information screening and exploiting contribute to task performance. Specifically, GIB-guided feature refinement is more beneficial for inputs with higher text-vision relevance (denoising), while multimodal topic integration is more useful for inputs with lower text-vision relevance (enriching context).</sample>
    <sample id="18">Salz und Pfeffer</sample>
    <sample id="19">Eine Masterstudentin der Shenzhen-Universität stellte ihre Arbeit "A Survey for Efficient Open Domain Question Answering" vor, die auf der ACL 2023 angenommen wurde.
Die Arbeit konzentriert sich auf Open-Domain Question Answering (ODQA) und untersucht die Herausforderungen und Motivationen, um Effizienz zu erzielen. Sie stellt ein zweistufiges Framework vor, das aus einem Retriever und einem Reader besteht. Der Retriever ruft Beweiskontexte von Wikipedia ab und der Reader versteht die Frage und sucht die Beweise, um die Antwort abzuleiten. 
Die Herausforderungen von ODQA-Aufgaben umfassen: 
* die umfangreiche Größe des Wikipedia-Korpus (26 Millionen Dokumente, 13 GB),
* die Notwendigkeit der Offline-Kodierung des Wikipedia-Korpus in eine Indexdatei (65 GB),
* die Suche nach Beweisen, die zu einem Engpass in der Inferenzgeschwindigkeit wird, und
* die Verwendung großer Sprachmodelle mit Millionen von Parametern.
Das Ziel der Arbeit ist es, effiziente ODQA-Systeme zu entwickeln, die einen geringeren Speicherplatzbedarf, schnellere Inferenz und vergleichbare Leistung bieten und dabei ressourcenbeschränkte Geräte berücksichtigen.
Es werden verschiedene Frameworks für ODQA-Systeme zusammengefasst:
* Retriever-Reader: umfasst extraktive und generative Reader, die Geschwindigkeit, Speicher und Leistung gut ausbalancieren.
* Retriever-only: basiert auf Phrasen oder QA-Paaren, verwendet große Indizes, bietet aber eine schnelle Inferenz.
* Generator-only: generiert die Antwort direkt, verwendet keine Indizes und große Modelle, was zu einer geringeren Leistung führt.
Die Autorin stellt auch effiziente Techniken zur Beweissuche und zur Reduzierung der Modell- und Indexgröße vor, wie z. B. die Approximate Nearest Neighbor (ANN) Suche, Dokumentenfilterung und Knowledge Distillation.
In den Schlussfolgerungen werden Empfehlungen für die Auswahl von ODQA-Systemen basierend auf Ressourcenbeschränkungen, Echtzeit-Feedback und einem Ausgleich von Leistung, Speicher und Geschwindigkeit gegeben.
Zukünftige Arbeiten könnten die Bereitstellung von ODQA-Systemen auf Low-Power-Geräten und die Berücksichtigung weiterer Bewertungsmetriken wie Geld, Trainingsdaten, Stromverbrauch und Kohlenstoffemissionen umfassen.</sample>
    <sample id="20">Ja, die DrBERT-Modelle, der NACHOS-Datensatz und die Trainingsskripte stehen Ihnen unter der MIT-Lizenz kostenlos zur Verfügung.</sample>
    <sample id="21">DEplain-apa enthält Nachrichten.</sample>
    <sample id="22">Gute Generalisierung erfordert eine bessere **Modellarchitektur**, eine größere **Modellgröße** und **mehr Feinabstimmungsbeispiele**.</sample>
    <sample id="23">Dan Garrett präsentiert Forschung, die darauf abzielt, die Fähigkeit von Text-zu-Bild-Modellen zur Darstellung von Text zu verbessern. Er weist darauf hin, dass Modelle wie Imagen zwar in der Lage sind, qualitativ hochwertige und interessante Bilder zu erzeugen, aber oft Schwierigkeiten bei der korrekten Darstellung von Text haben. Das Problem liegt in der Art und Weise, wie die Text-Encoder die Eingabe verarbeiten. T5-Modelle, die Imagen zugrunde liegen, verwenden die SentencePiece-Tokenisierung, die ganze Wörter oder Wortteile in einzelne Token umwandelt, anstatt einzelne Buchstaben zu verarbeiten. Das bedeutet, dass die Rechtschreibung nicht explizit codiert ist und die Modelle Schwierigkeiten haben, die einzelnen Buchstaben aus den Subwort-Tokens zu zerlegen, um sie korrekt darzustellen. Die Rechtschreibgenauigkeit steigt mit der Größe der T5-Modelle, aber selbst die größten Modelle erreichen nicht 70 %. PaLM-Modelle, die noch größer sind, zeigen eine nahezu perfekte Rechtschreibgenauigkeit, sind aber für viele Anwendungen unpraktisch.

Garrett schlägt eine Lösung vor, die die Vorteile von Zeichen-sensitiven Encodern nutzt. Er stellt ByT5 vor, ein Modell, das einzelne Bytes der Eingabe verarbeitet und somit vollständigen Zugriff auf die Rechtschreibinformationen hat. Dies ermöglicht ByT5 eine hervorragende Rechtschreibgenauigkeit über alle Skalen hinweg und es wird nicht von der Worthäufigkeit beeinflusst, im Gegensatz zu T5, das bei häufigeren Wörtern schlechter abschneidet. Um die Fähigkeit von Imagen zu verbessern, Text darzustellen, erweitern sie das Modell, indem sie die vorhandene Textrepräsentation um eine zusätzliche Textrepräsentation aus dem ByT5-small-Modell ergänzen. Obwohl ByT5-small nur etwa 5 % der Parameter des T5-XXL-Text-Encoders ausmacht, verbessert diese Hinzufügung die Textwiedergabefähigkeit des Modells erheblich und steigert die Präferenzraten für Bildgetreue, Ausrichtung und Textdarstellung in einer menschlichen Bewertung. Die wichtigsten Erkenntnisse sind, dass WikiSpell und DrawText nützliche Benchmarks für Textmodelle bzw. Text-zu-Bild-Modelle sind, und dass eine effiziente Strategie zur Verbesserung der Rechtschreibfähigkeit von Modellen darin besteht, Charakter-bewusste Modelle zu integrieren.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde gemessen, indem ihre Länge in Zeichen, Silben und Wörtern analysiert wurde.</sample>
    <sample id="25">Die Experimente wurden so gestaltet, dass die Länge der Konjunktionen in Zeichen, Silben und Wörtern gemessen wurde. Es wurde beobachtet, dass die Tendenz der linken Konjunktionen, kürzer zu sein, nur dann zunimmt, wenn der Regler links oder abwesend ist, aber nicht, wenn er rechts ist.</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten trainiert wird, hat eine AUC von 0,5, was nicht besser als der Zufall ist.</sample>
    <sample id="27">An der Arbeit sind vier Autoren beteiligt.</sample>
    <sample id="28">Die Personen im Beispielgespräch heißen Bob und Alice.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden bei Diskursphänomenen wie Formalität und lexikalischem Zusammenhalt deutlich besser ab.</sample>
    <sample id="30">Jede Woche werden viele große Sprachmodelle (LLMs) veröffentlicht, die jeweils eine hervorragende Leistung beanspruchen. Allerdings ist kein einziges LLM für alle Eingaben das Beste. Das optimale LLM variiert je nach Anwendungsfall. Um dieses Problem anzugehen, haben wir ein zweistufiges Framework namens LLM-Blender vorgeschlagen, das eine paarweise Rangordnung und generative Fusion verwendet. Zuerst verarbeiten wir die Eingabe mit mehreren LLMs und generieren verschiedene Kandidaten. Dann vergleichen wir diese Kandidaten paarweise mit einem Modell namens PairRanker und erhalten eine Rangliste der Kandidaten. Wir verwenden einen Fusioner namens GenFuser, der die Top-K-Kandidaten als Eingabe nimmt und eine verbesserte Ausgabe generiert. Der Hauptunterschied des PairRankers zu früheren Methoden ist, dass er ein Paar von Kandidaten zusammen mit der Eingabe für die bessere Analyse der feinen Unterschiede zwischen den beiden Kandidaten kodiert, anstatt jeden Kandidaten einzeln zu betrachten. Wir haben festgestellt, dass das Aggregieren der Ergebnisse mit Max-Logits am besten funktioniert, aber Bubble Sort ist eine effizientere Alternative mit guter Leistung. Um die Auswertung von LLM-Ensembles zu ermöglichen, haben wir auch einen neuen Datensatz namens MixInstruct erstellt, der 110K Anweisungs-Following-Beispiele und 11 Open-Source-LLMs enthält. Unsere Ergebnisse zeigen, dass der PairRanker durchweg bessere Leistungen erbringt als andere Modelle. Insbesondere kann der Blender die besten LLMs in 68 % bzw. 76 % der Fälle für Open-Assistant und Vicuna übertreffen. Zusammenfassend ist LLM-Blender ein einfaches, aber effektives Ensemble-Lernframework für LLMs, das die Gesamtleistung bestehender LLMs erheblich verbessert. Wir stellen auch unseren Code und Datensatz für die weitere Forschung zur Verfügung.</sample>
    <sample id="31">Die Autoren sind mit den folgenden Universitäten verbunden:
- Johns Hopkins University
- Purdue University
- MIT
- Meta AI</sample>
    <sample id="33">Das vorgestellte Framework quantifiziert die Positionalität, indem es die Anmerkungen der Benutzer in den Datensätzen und Modellen mit dem Pearsonschen Korrelationskoeffizienten vergleicht.</sample>
    <sample id="34">Der Redner präsentiert CREST, ein gemeinsames Framework zur Rationalisierung und kontrafaktischen Texterzeugung. Der Redner beginnt mit der Diskussion, wie die Entscheidungen eines Klassifikators erklärt werden können, indem er selektive Rationalisierung und kontrafaktische Generierung als Interpretationsmethoden hervorhebt. CREST kombiniert diese Ansätze, um ihre komplementären Stärken zu nutzen.

Der Generierungsbestandteil von CREST beginnt mit einer Texteingabe, die durch einen trainierbaren Maskierer läuft, der Schlüsselstellen hervorhebt. Diese Schlüsselstellen werden dann zusammen mit einer Zielklassifikation einem Editor zugeführt, der neue Token in die maskierten Stellen einfügt, um ein kontrafaktisches Beispiel zu erzeugen.

Experimente mit automatischen Metriken und menschlicher Evaluation zeigten, dass CREST hochqualitative kontrafaktische Texte erzeugt. Menschliche Gutachter bewerteten die von CREST erzeugten kontrafaktischen Texte als gültiger und natürlicher als die von alternativen Methoden.

Der Vortrag demonstriert auch, wie diese kontrafaktischen Texte genutzt werden können, um Rationalisierungen zu erzeugen. CREST-Rationalisierung arbeitet mit einem faktischen und einem kontrafaktischen Fluss, die beide einem gemeinsamen Rationalisierer zugeführt werden. Eine neue Regularisierungsfunktion stellt sicher, dass die erzeugten Rationalisierungen den ursprünglich von CREST-Generierung erzeugten ähneln.

Die Ergebnisse zeigen, dass CREST-Rationalisierung im Vergleich zu anderen Methoden plausible Rationalisierungen erzeugt und eine höhere kontrafaktische Simulierbarkeit erreicht. Dies zeigt, dass die Rationalisierungen von CREST interpretierbar sind und zur Verbesserung nachgeschalteter Modelle verwendet werden können.

Zusammenfassend überbrückt CREST die Lücke zwischen selektiver Rationalisierung und kontrafaktischer Generierung, indem es gültige, flüssige und vielfältige kontrafaktische Texte in einer kontrollierbaren Weise erzeugt. Durch die Nutzung dieser kontrafaktischen Texte im Training führt CREST zu plausiblen Erklärungen, die sich auf die konträren Teile der Eingabe konzentrieren.</sample>
    <sample id="36">Der Referent stellt Language-Specific Layers (LSLs) vor, eine Lösung für die Herausforderungen der mehrsprachigen maschinellen Übersetzung. Der Referent erklärt, dass LSLs die Skalierbarkeit, Geschwindigkeit und Genauigkeit der mehrsprachigen maschinellen Übersetzung verbessern können, insbesondere für ressourcenarme Sprachpaare.

Der Referent präsentiert das Konzept der LSLs, bei dem für jede Sprache ein eigener Transformer-Layer verwendet wird. Dies ermöglicht es dem Modell, für jede Sprache mehr Kapazität zu haben und gleichzeitig die Inferenzkosten konstant zu halten.

Der Referent erläutert die LSL-Platzierung, bei der das Modell die beste Platzierung der LSLs selbst lernt. Dabei wird ein großes Modell mit geteilten, Quell- und Zielgewichten für jeden Encoder-Layer trainiert. Die Gewichte werden dann analysiert, um die optimale Architektur zu bestimmen, wobei die LSLs in den Schichten platziert werden, in denen sie die größte Wirkung haben.

Die experimentellen Ergebnisse zeigen, dass der LSL-NAS-Ansatz die Leistung der mehrsprachigen maschinellen Übersetzung im Vergleich zu größeren Baselines und Adapter-Ansätzen signifikant verbessert. Diese Verbesserungen sind besonders ausgeprägt für ressourcenarme Sprachen und sind statistisch signifikant in 84 von 90 Übersetzungsrichtungen.</sample>
    <sample id="37">Die vorherige Studie, bei der menschliche Teilnehmende die gleichen Persona-Prompts erhalten haben, zeigte, dass auch sie zu rassistischen Stereotypen neigten.</sample>
    <sample id="38">Für diese Studie wurden Daten aus einer erweiterten Version der **Penn Treebank** verwendet, wie von Marcus et al. (1993) und Fickler and Goldberg (2016) beschrieben.</sample>
    <sample id="39">An der Arbeit sind zwei Autoren beteiligt: Adam Przepiórkowski und Michał Woźniak.</sample>
    <sample id="40">Die eng verwandten Aufgaben für kognitive Dissonanz sind die Einstufung von inkonsistenten Diskursbeziehungen.</sample>
    <sample id="41">Dieser Vortrag stellt PeaCoK vor, ein Persona-Common-Sense-Wissensgraph, der für konsistente und ansprechende Erzählungen entwickelt wurde. Die Personas spielen eine entscheidende Rolle bei der Gestaltung von Erzählungen, wie zum Beispiel bei Dialogen und Geschichten. PeaCoK soll diese Persona-bezogenen Informationen auf einer weltweiten Ebene darstellen.

PeaCoK enthält etwa 3.800 Personas und 40.000 Attribute, die etwa 100.000 Persona-Fakten bilden. Es hebt 9.200 Attribute hervor, die mit zwei oder mehr Personas verbunden sind und die vielfältigen Interaktionen zwischen ihnen zeigen.

PeaCoK ist in drei Schritten aufgebaut:
1.  **Persona-Auswahl:** Personas werden aus bestehenden Wissensgraphen ausgewählt, die menschliche Rollen und ereignisbasierte Entitäten umfassen.
2.  **Induktion potenzieller Attribute:** Die Attribute werden aus Common-Sense-Wissensgraphen und großen vortrainierten Sprachmodellen abgeleitet.
3.  **Relationenklassifikation:** Die Beziehungen in PeaCoK werden durch eine gemeinsame Mensch-KI-Mehrheitsentscheidung klassifiziert, wobei InstructGPT-3 eine entscheidende Rolle bei der zuverlässigen Annotation spielt.

Expertenschätzungen zeigen, dass die Methode der Mehrheitsentscheidung eine Genauigkeit und einen F1-Wert von 87 % erreicht. Dies zeigt, dass PeaCoK es leichteren Sprachmodellen ermöglicht, Wissen zu generieren, das mit großen Sprachmodellen vergleichbar ist.

Der Vortrag untersucht außerdem, wie PeaCoK zur Verbesserung von Dialogsystemen eingesetzt werden kann. Die Fakten aus PeaCoK werden verwendet, um die Persona-Profile der Sprecher in einem Dialogsystem zu erweitern. Menschliche Bewertungen zeigen, dass die PeaCoK-Erweiterung die Dialogerstellung in verschiedenen Aspekten verbessert, einschließlich Flüssigkeit, Konsistenz, Engagement und Persona-Ausdruck. Insbesondere führt eine höhere Anzahl gemeinsamer Attribute zwischen den Gesprächspartnern zu konsistenteren und ansprechenderen Gesprächen.

Zusammenfassend lässt sich sagen, dass PeaCoK ein weltweites Persona-Common-Sense-Wissensgraph ist, der hochwertige Persona-Ableitungen enthält. Es kann zur Schulung zuverlässiger Persona-Ableitungsgeneratoren eingesetzt werden und trägt zu einer konsistenteren und ansprechenderen narrativen Modellierung bei.</sample>
    <sample id="42">An der Arbeit sind zwei Autoren beteiligt: Shuheng Liu und Alan Ritter.</sample>
    <sample id="43">Es sind neun Autoren an der Arbeit beteiligt.</sample>
    <sample id="44">Das vorgestellte Framework unterscheidet sich von früheren Arbeiten, indem es Endnutzer mit Modellen und Datensätzen vergleicht, anstatt nur die Übereinstimmung der Kommentatoren oder die Verteilung der Kommentatoren zu betrachten.</sample>
    <sample id="45">Die meisten Überschneidungen mit dem Lexikon der Stereotypen hat das GPT-3.5-Setup.</sample>
    <sample id="46">Die kommerziellen Systeme, die verglichen wurden, waren DeepL und Google Translate.</sample>
    <sample id="47">Hi, ich bin Shanbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit „From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models“.

Sprachmodelle werden auf großflächigen Web-Crawling-Daten trainiert. Politische Nachrichtenmedien sind in ihren Vortrainingsdaten gut abgedeckt. Laut einer Umfrage zum C4-Korpus sehen wir, dass die New York Times, Los Angeles Times, The Guardian, Huffington Post usw. in den Trainingsdaten der Sprachmodelle gut abgedeckt sind. Dies hat für die Anwendungen von Sprachmodellen einen gemischten Segen geschaffen. Einerseits konnten sie aus verschiedenen Perspektiven lernen, was die Demokratie und die Pluralität der Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen von Natur aus sozial voreingenommen und können zu potenziellen Fairnessproblemen bei nachgelagerten Anwendungsaufgaben führen.

Dazu schlagen wir vor, die Ausbreitung politischer Vorurteile von Vortrainingsdaten über Sprachmodelle bis hin zu nachgelagerten Aufgaben zu untersuchen. Insbesondere stellen wir die folgenden Fragen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen, und welche Rolle spielt das Vortraining der Daten bei solchen politischen Vorurteilen? Zweitens, wie verhalten sich Sprachmodelle mit unterschiedlichen politischen Ausrichtungen bei nachgelagerten Aufgaben, und führt die politische Ausrichtung von Sprachmodellen zu Fairnessproblemen bei NLP-Anwendungen?

Zunächst schlagen wir vor, Sprachmodelle mit verschiedenen Eingabeformaten unter Verwendung politischer Fragebögen wie dem „Political Compass Test“ zu befragen. Dies ermöglicht uns eine automatische Bewertung, die in der politikwissenschaftlichen Literatur verankert ist.

Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Neigungen haben. Sie besetzen alle vier Quadranten des politischen Kompasses. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist, und die GPT-Modelle sind im Allgemeinen sozial liberaler als die BERT-Modelle und ihre Varianten.

Zweitens wollen wir untersuchen, inwieweit die politischen Vorurteile von Sprachmodellen tatsächlich aus den Trainingsdaten stammen. Wir führen ein kontrolliertes Experiment durch, indem wir Sprachmodell-Checkpoints auf sechs verschiedenen parteipolitischen Korpora weiter vor trainieren, aufgeteilt in Nachrichten- und soziale Medien, die weiter nach ihrer politischen Ausrichtung unterteilt sind.

Durch das weitere Vortrainieren von Sprachmodellen auf solchen parteipolitischen Korpora können wir sehen, dass sich die ideologischen Koordinaten des Sprachmodells ebenfalls entsprechend verschieben. Zum Beispiel sehen wir für RoBERTa, die auf dem linksgerichteten Reddit-Korpus weiter trainiert wurde, eine erhebliche liberale Verschiebung in Bezug auf ihre politischen Vorurteile.

Wir versuchen auch zu untersuchen, ob Sprachmodelle die in unserer modernen Gesellschaft vorherrschende Polarisierung aufnehmen können. Wir teilen die Vortrainingskorpora in „vor dem 45. Präsidenten der Vereinigten Staaten“ und „nach dem 45. Präsidenten der Vereinigten Staaten“ auf. Wir trainieren Sprachmodelle separat auf den beiden unterschiedlichen temporären Korpora vor. Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Neigung hatten, die nach 2017 weiter vom Zentrum entfernt war. Dies zeigt, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können.

Zu guter Letzt bewerten wir Sprachmodelle mit unterschiedlichen politischen Neigungen bei der Erkennung von Hassreden und der Erkennung von Falschnachrichten, zwei NLP-Anwendungen, die oft Sprachmodelle beinhalten und sehr erhebliche Auswirkungen haben könnten. Wir sehen, dass, wenn wir die Leistung nach Kategorien untersuchen, das heißt, wenn wir die Leistung in verschiedene Demografien oder politische Ausrichtungen von Nachrichtenmedien aufteilen, wir ein Muster sehen, dass zum Beispiel für die Erkennung von Hassreden linksgerichtete Sprachmodelle besser darin sind, Hassreden zu erkennen, die auf sozial marginalisierte Gruppen abzielen. Sie sind jedoch schlechter darin, Hassreden zu erkennen, die auf mächtigere Gruppen in unserer Gesellschaft abzielen, und umgekehrt. Ähnliche Trends treten auch bei der Erkennung von Falschnachrichten auf, wo wir sehen, dass linksgerichtete Sprachmodelle besser darin sind, Falschinformationen von ihren gegnerischen politischen Ausrichtungen zu erkennen, und umgekehrt.

Wir zeigen viele weitere qualitative Beispiele, um zu zeigen, dass Sprachmodelle mit unterschiedlichen politischen Neigungen tatsächlich unterschiedliche Vorhersagen für Hassreden und Falschinformationen machen, basierend auf ihren sozialen Kategorien. Es gibt eine Reihe weiterer Beispiele im Anhang, um weiter hervorzuheben, dass dies auf ein Fairnessproblem hinweist, das sehr dringend ist, in Bezug auf die politischen Vorurteile von Sprachmodellen. Zum Beispiel, wenn ein rechtsgerichtetes Sprachmodell auf Hassrede oder Falschinformationen feingetunt und auf einer beliebten Social-Media-Plattform eingesetzt würde, würde dies bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen marginalisiert würden und Hassreden, die auf Minderheitengruppen abzielen, ungezügelt blieben, ohne jede Kontrolle. Dies läutet die Alarmglocken für uns, um die Fairnessprobleme, die durch die politischen Ausrichtungen von Sprachmodellen entstehen, anzuerkennen und anzugehen.

Ein kleiner Diskussionspunkt. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma bezüglich der politischen Vorurteile von Sprachmodellen aufgedeckt haben, es ist wie zwischen Scylla und Charybdis. Wenn wir die politischen Meinungen in den Trainingsdaten der Sprachmodelle nicht bereinigen, würde sich das Vorurteil von den Trainingsdaten über die Sprachmodelle bis zu den nachgelagerten Aufgaben ausbreiten und automatisch Fairnessprobleme erzeugen. Wenn wir versuchen, sie irgendwie zu bereinigen, würden wir auch Zensur oder Ausschlüsse riskieren, und es ist unglaublich schwer zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten der Sprachmodelle behalten werden sollte. Es ist also eine Art elektrisches Trolley-Problem. Okay, großartig, ich glaube, das ist so ziemlich alles, was ich für heute habe. Vielen Dank für Ihre Zeit.</sample>
    <sample id="48">Es sind 7 Autoren an der Arbeit beteiligt.</sample>
    <sample id="49">MPP-Auswertungen wurden für Kontextlängen von bis zu 900 Token durchgeführt.</sample>
    <sample id="50">In diesem Vortrag wird DEPLAIN vorgestellt, ein neues Korpus für die Vereinfachung deutscher Texte auf Dokumenten- und Satzebene. Bestehende Korpora sind entweder zu klein für die Ausbildung von Vereinfachungsmodellen oder sie sind automatisiert ausgerichtet, was zu fehleranfälligen Ausrichtungen führen kann. DEPLAIN ist in zwei Unterkorpora unterteilt: DEPlain-APA, das auf Nachrichtentexten basiert und 483 manuell angepasste Dokumente mit etwa 13.000 parallelen Satzpaaren enthält, und DEPlain-web, das verschiedene Domänen abdeckt und 756 Dokumente enthält, von denen 3.450 manuell und der Rest automatisch angeglichen wurden.

Das Korpus wurde auf verschiedene Vereinfachungstypen analysiert, und es wurde festgestellt, dass die Bibeltexte stärker vereinfacht sind als Nachrichtentexte oder Texte für Sprachlernende. Außerdem weist das DEPlain-Korpus eine hohe Varianz an Vereinfachungstransformationen auf, wobei DEPlain-APA mehr Umordnungen und Wortzusätze aufweist, während DEPlain-web mehr Umformulierungen enthält.

Im Weiteren werden zwei Anwendungsfälle für das DEPlain-Korpus vorgestellt:

1. **Automatische Ausrichtung:** DEPLAIN kann zur Evaluierung automatischer Ausrichtungsmethoden verwendet werden. Es wurden Anpassungen an bestehenden Methoden vorgenommen und die Ergebnisse im Paper veröffentlicht. Für die deutsche Textvereinfachung ist MASAlign die beste automatische Ausrichtungsmethode.

2. **Automatische Textvereinfachung:** DEPLAIN wird zum Fine-Tuning von Sprachmodellen zur Erzeugung vereinfachter Texte verwendet. Es wurden zwei Modelle, long-mBART und mBART, feinjustiert, um Vereinfachungen auf Dokumenten- und Satzebene zu erzeugen. Die Ergebnisse dieser Experimente dienen als Benchmark für zukünftige automatische Textvereinfachungsprobleme.

Insgesamt bietet DEPLAIN ein umfassendes und qualitativ hochwertiges Korpus für die deutsche Textvereinfachung, das sowohl für die Evaluierung von Ausrichtungsmethoden als auch für das Training von Vereinfachungsmodellen genutzt werden kann.</sample>
    <sample id="51">Die drei Domänen in ihrem Datensatz sind Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität ist definiert als die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen einnehmen.</sample>
    <sample id="53">Der/die Referent*in heißt Dawei Zhu.</sample>
    <sample id="54">Die Präsentation konzentriert sich auf die Erkennung kognitiver Dissonanzen in der Sprache, ein seltenes Phänomen, das für das Verständnis menschlicher Entscheidungen, Einstellungen und psychischer Gesundheit von entscheidender Bedeutung ist. Die Vortragende, Vasudha Varadarajan, eine Doktorandin an der Stony Brook University, stellte einen Ansatz vor, der Transfer- und aktives Lernen kombiniert, um die Herausforderung der seltenen Klassen zu bewältigen. Die Forschung zeigte, dass das Vortrainieren eines Modells für eng verwandte Aufgaben die Zero-Shot-Leistung erheblich verbessert. Außerdem wurde eine "Wahrscheinlichkeit der seltenen Klasse"-Strategie (PRC) für das aktive Lernen eingeführt, die die Akquisition von Dissonanzproben optimiert. Der kumulative Modellaktualisierungsansatz übertraf den iterativen. Zusammenfassend lässt sich sagen, dass das Vortrainieren mit Transferlernen und die Verwendung der PRC-Strategie die Dissonanzerkennung effektiv verbessert und gleichzeitig die Annotationskosten senkt, was vielversprechende Wege für die psychologische Forschung und NLP-Anwendungen eröffnet.</sample>
    <sample id="55">Ja, EDAtt ist dafür konzipiert, mit bereits existierenden Offline-ST-Modellen zu funktionieren.</sample>
    <sample id="56">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="57">Die Modelle kämpfen damit, das Hintergrundwissen, das ihnen nur zur Inferenzzeit präsentiert wird, zuverlässig zu integrieren.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:

1.  **Background-Pretrain:** Dies ist die typische Konfiguration, bei der Hintergrundwissen während des Pretrainings verfügbar ist.
2.  **Background-Both:** Bei dieser Konfiguration wird Hintergrundwissen sowohl während des Pretrainings als auch zur Inferenzzeit explizit im Kontext bereitgestellt.
3.  **Background-Inference:** Hier ist Hintergrundwissen ausschließlich zur Inferenzzeit verfügbar, was bedeutet, dass es nicht Teil der Pretraining-Daten der Modelle ist.</sample>
    <sample id="59">The speaker, Yanis Labrak, presents his team's work on DrBERT, a robust pre-trained model in French for biomedical and clinical domains. He starts by explaining the general background of language modeling in healthcare, mentioning the efficiency of transformer-based approaches like BERT in NLP tasks and its adaptation to various languages and domains, mainly English. However, he highlights the lack of open-source models for biomedical and clinical domains in French.

The team addressed this gap by developing DrBERT, a RoBERTa-based model trained on NACHOS, an open-source dataset of medical data crawled from the web, and an anonymized private clinical dataset (NBDW) from Nantes University Hospital. The study compares the impact of public and private medical data sources and pre-training strategies. They train four "from scratch" models using different sizes and combinations of NACHOS and NBDW, as well as three continual pre-training models based on existing generic (CamemBERT) and domain-specific English (PubmedBERT) models.

The models were evaluated on 11 downstream tasks including Named Entity Recognition (NER), classification, Part-of-Speech (POS) tagging, and question-answering. DrBERT models achieved state-of-the-art results on 9 out of 11 tasks, surpassing generic French and domain-specific English models. The findings suggest that training on heterogeneous data (like NACHOS) is important for robustness, and that more data generally leads to better performance, though not always proportionally. Continual pre-training is found to be effective, especially when based on English domain-specific models. The DrBERT models, NACHOS dataset, and training scripts are all openly available.</sample>
    <sample id="60">Die Autoren gehören der Google Research University an.</sample>
    <sample id="61">Die abschließende Forschungsfrage lautet: „Wie können die verfügbaren sauberen Samples effizienter genutzt werden?“</sample>
    <sample id="62">In this video, Nitay Calderon presents "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training." The motivation for this study stems from the massive computational, storage, and financial requirements of large language models (LLMs) used in Natural Language Generation (NLG) systems. There's a growing industry demand to compress these models while preserving their performance.

The research explores model compression through pruning less informative parameters and Knowledge Distillation (KD), which transfers knowledge from a large teacher model to a smaller student model. In NLG, KD can be word-level (mimicking the teacher's next token distribution) or sequence-level (training the student on pseudo-targets generated by the teacher).

Unlike previous KD works focusing on NLU tasks or task-agnostic KD, and often using large datasets, this study adopts a "realistic" or "industry-driven" setup. This involves:
1. Medium-resource labeled datasets (several thousands of examples).
2. Plentiful unlabeled data.
3. Off-the-shelf, small-to-medium sized, fine-tuned LLMs.
4. Prioritizing inference time efficiency (high compression).
5. Negligible one-time computational training resources compared to accumulated inference costs.

The study investigates various NLG tasks: summarization, question generation, common sense reasoning, and simplification/style transfer. Datasets used maintain a 1:4 ratio of labeled to unlabeled examples.

The systematic study consists of eight stages, including an extreme setup not detailed in the video. The stages explore:
1. Architectural decisions (encoder-decoder vs. decoder-only).
2. Impact of pruning on task and computational performance.
3. Different KD approaches and state-of-the-art baselines.

A significant contribution is the exploration of pseudo-target usage extensions. The study challenges traditional sequence-level KD by:
- Demonstrating the crucial role of unlabeled data in boosting distillation.
- Showing that generating multiple pseudo-targets via sampling (instead of a single one using beam search) improves student performance.
- Proposing "Joint-Teaching," a novel KD technique that applies word-level KD on pseudo-targets generated by both the teacher and the student to address student exposure bias, ground learning, and correct student mistakes.

For more details on the study, methods, and the motivation for addressing exposure bias in KD, viewers are encouraged to scan the QR code for the paper or visit the poster session.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, konsistent die gleichen Ergebnisse für die gleiche Aufgabe zu liefern, unabhängig von leichten Variationen in der Formulierung der Anweisungen.</sample>
    <sample id="64">Der/die Referent*in heißt Jingwei Yi.</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet eine schlechtere Leistung des Modells.</sample>
    <sample id="66">Diese umfassende Übersicht untersucht die Deep-Learning-Methoden für mathematisches Denken und die damit verbundenen Aufgaben, Daten und Ansätze. Seit 2017 hat das Interesse an diesem Bereich stetig zugenommen, wie die Zahl der wissenschaftlichen Veröffentlichungen zeigt.
Zu den verschiedenen Aufgaben gehören die Lösung von mathematischen Textaufgaben, die auch multimodale Informationen berücksichtigen, das Lösen von Geometrieaufgaben und das automatisierte Beweisen von Theoremen. Auch das Sondieren der menschlichen Intelligenz in Sprachmodellen (LLMs) wird angesprochen.

Deep-Learning-Methoden für mathematisches Denken basieren auf Architekturen wie Seq2Seq Neural Networks und Baum-basierten Neural Networks. Jüngste Fortschritte bei LLMs, einschließlich Chain-of-Thought-Prompting, Self-Consistency und Programm-Augmented LLMs, haben beeindruckende Ergebnisse bei komplexen mathematischen Aufgaben gezeigt.

Trotz dieser Erfolge haben LLMs immer noch Einschränkungen, insbesondere bei der präzisen mathematischen Argumentation mit großen Zahlen oder der Konsistenz. Es gibt auch ungenutztes Potenzial bei der mathematischen Argumentation in ressourcenarmen Umgebungen und in verschiedenen Sprachen.

Die Übersicht zeigt die großen Fortschritte auf und skizziert potenzielle Wege für zukünftige Forschung und Entwicklung in diesem Bereich, indem sie die Stärken von Deep Learning und LLMs für immer komplexere mathematische Herausforderungen nutzt.</sample>
    <sample id="67">In diesem Vortrag ging es um die Ursachen und Heilmittel für Interferenzen bei mehrsprachigen Übersetzungssystemen. Multilinguale MT-Modelle können von Synergien zwischen Sprachpaaren profitieren, aber auch unter Interferenzen leiden, was dazu führt, dass die Qualität der Übersetzung sich verschlechtert. Viele Methoden wurden vorgeschlagen, um Interferenzen zu mindern, aber sie funktionieren nicht immer besser als eine abgestimmte Grundlinie und werden oft mit kleinen Modellen demonstriert.
Der Vortrag identifiziert die Hauptfaktoren, die zu Interferenz oder Synergie beitragen:
*   **Modellgröße**
*   **Datengröße**
*   **Datengröße anderer Sprachen**

Sprachähnlichkeit und die Anzahl der Sprachen haben keinen großen Einfluss auf die Interferenz. Schwere Interferenzen treten auf, wenn das Modell im Vergleich zur Datengröße sehr klein ist. Wenn die Modellgröße und die Datengröße zunehmen, nimmt die Interferenz ab und wird sogar zu einer Synergie. Die Abstimmung der Stichproben-Temperatur ist der Schlüssel zu einer starken Leistung. Eine zu hohe oder zu niedrige Temperatur kann zu schlechter Leistung führen.

Zusammenfassend lässt sich sagen, dass die Modellgröße, die Datengröße und die Datengröße anderer Sprachen die dominanten Faktoren sind, die Interferenzen beeinflussen. Eine bescheidene Skalierung und eine abgestimmte Temperatur können das Problem erheblich reduzieren, ohne dass spezielle Algorithmen erforderlich sind.</sample>
    <sample id="68">Die Modelle erhalten verschiedene linguistische Kontexte, wie z.B. Wikipedia-Sätze für irrelevante Kontexte und akzeptable/inakzeptable Sätze aus BLiMP- und SyntaxGym-Datensätzen für passende und unpassende Kontexte.</sample>
    <sample id="69">Normalerweise werden nur 20 saubere Validierungsbeispiele pro Klasse benötigt, um eine gute Leistung an der WSL zu erzielen.</sample>
    <sample id="70">Die Autoren gehören zur Stanford University.</sample>
    <sample id="71">In diesem Vortrag wird eine Studie zur Lösung indirekter Referenzausdrücke für die Entitätsauswahl vorgestellt. Das Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Auswahl treffen, insbesondere bei der Verwendung indirekter Referenzen. Dies ist relevant für Konversationssysteme und zum Benchmarking des Entitätsverständnisses von großen Sprachmodellen. Da keine ausreichend großen öffentlichen Datensätze für diese Aufgabe verfügbar sind, wurde ein eigener Datensatz namens AltEntities Corpus erstellt, der auf Crowdsourcing basiert.

Der Datensatz deckt drei Bereiche ab: Musik, Bücher und Rezepte. Die Datenerfassungsmethodik konzentrierte sich auf Informalität und verwendete eine "Cartoon-Vervollständigungsaufgabe". Der Cartoon zeigte drei Sprechblasen: eine Kontexteinstellung, eine alternative Frage und eine Lücke, die der Anmerkende mit einem indirekten Referenzausdruck füllen sollte. Die alternativen Fragen wurden aus Wikipedia-Entitäten generiert, die unterschiedliche Ähnlichkeitsgrade aufwiesen, von zufällig bis zu sehr ähnlich (z. B. ähnliche Infoboxen, Beschreibungen oder Titel). Den Anmerkenden wurden Hintergrundinformationen zu den Entitäten bereitgestellt, z. B. Google-Suchlinks für Lieder oder Wikipedia-Texte und -Bilder für Rezepte und Bücher.

Der AltEntities Corpus umfasst etwa 6.000 alternative Fragen und 42.000 indirekte Referenzausdrücke. Erste Ergebnisse mit einem T5 XL-Sprachmodell zeigen eine hohe Genauigkeit von 92-95 %, wenn das Modell auf dasselbe Hintergrundwissen wie die Anmerkenden zugreifen kann. Diese Genauigkeit sinkt auf 82-87 % bei teilweise überlappendem Wissen und auf etwa 60 %, wenn das Modell nur auf die Entitätsnamen zugreifen kann, was einen großen Spielraum für Verbesserungen aufzeigt. Die Modelle erwiesen sich auch als domänenübergreifend generalisierbar.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, da große Sprachmodelle (LLMs) auf riesigen Mengen an Web-Crawling-Daten trainiert werden, die eine breite Palette von politischen Nachrichtenmedien und verschiedenen Meinungen abdecken. Dadurch können die LLMs eine Vielzahl von Perspektiven lernen, bergen aber auch das Risiko, soziale Vorurteile zu verinnerlichen, die in der Trainingsdaten vorhanden sind, was zu Fairness-Problemen in Downstream-Anwendungen führen kann.</sample>
    <sample id="73">Die Referentin heißt Akshatha.</sample>
    <sample id="74">Xiangqing Shen stellt seine Arbeit über "Dense-ATOMIC" vor, ein **dicht vernetztes Wissensgraphen**modell, das das **Common-Sense-Verständnis von Maschinen** verbessern soll. Die Motivation für Dense-ATOMIC liegt in den **Einschränkungen des Original-ATOMIC-Modells**, das unter einer **spärlichen Graphenstruktur und unzureichender semantischer Information der Ereignisse leidet**. Dies führt zu unbefriedigender Wissensabdeckung und nur wenigen Multi-Hop-Pfaden.

Dense-ATOMIC wurde in einem dreistufigen Prozess konstruiert:
1. **Normalisierung der Tail-Ereignisse**: Dieser Schritt beinhaltet das Entfernen von Subjekten, die Konjugation von Verben in der dritten Person Singular, das Wiederherstellen von Subjekten und das Gruppieren von Relationen, um die Tail-Ereignisse mit den Head-Ereignissen abzugleichen.
2. **Trainieren eines Relation-Prediction-Modells (Rel-CSKGC)**: Rel-CSKGC verwendet ein vortrainiertes Sprachmodell (RoBERTa) und eine Kombination aus Link- und Relation-Prediction, um Relationen zwischen Ereignissen vorherzusagen. Es umgeht das Problem der Graphensparsity, indem es keine Graphenstrukturinformationen verwendet, und nutzt semantische Informationen, indem es sowohl Head- als auch Tail-Ereignisse kodiert.
3. **Konstruktion von Dense-ATOMIC**: Dies wird durch eine Intra- und Inter-Cluster-Completion-Strategie erreicht, die fehlende Links innerhalb und zwischen Clustern ableitet.

Die **Evaluation** zeigt, dass Dense-ATOMIC eine **höhere Wissensabdeckung** und **deutlich mehr Multi-Hop-Pfade** als das Original-ATOMIC-Modell bietet. Es übertrifft auch traditionelle Relationsvorhersagemethoden und Translaion-Based-Methoden. Darüber hinaus **verbessert es die Leistung** von Modellen wie COMET, indem es **vielfältigere Ergebnisse** generiert. Human-Evaluierungen von Stichproben von Multi-Hop-Pfaden bestätigen die Wirksamkeit von Dense-ATOMIC.

Zusammenfassend lässt sich sagen, dass Dense-ATOMIC ein **bedeutender Fortschritt** in der Common-Sense-Wissensmodellierung ist, der eine **verbesserte Wissensabdeckung, mehr Multi-Hop-Pfade** und ein **größeres Potenzial für Common-Sense-Schlussfolgerungen** bietet.</sample>
    <sample id="75">In this presentation, Zheng Yandan discusses their work, Jointprop, a joint semi-supervised learning framework for entity and relation extraction that uses heterogeneous graph-based propagation. 

The presentation highlights the limitations of fully supervised NER and RE models, which require extensive and expensive data annotation. While semi-supervised NER and RE models have shown promise in reducing annotation costs, current approaches often neglect the inherent interconnections between NER and RE tasks. The speaker emphasizes the importance of considering these interconnections within labeled, unlabeled, and between labeled and unlabeled data to improve label inference.

Jointprop addresses this by modeling NER and RE tasks through label propagation over heterogeneous graphs. The framework performs label propagation across the graph and considers both inter- and intra-interactions among labeled and unlabeled data.

The Jointprop framework comprises four parts:
1.  **Span Feature Generation:** Contextualized representations of input tokens are used to initialize span and span-pair representations. A trained classifier generates unlabeled span and span-pair representations.
2.  **Heterogeneous Graph Construction:** A k-Nearest Neighbor (kNN) graph is built for computational efficiency. This graph encodes similarity relationships among all data points, leveraging smoothness constraints among neighboring unlabeled data to enhance semi-supervised joint entity and relation extraction.
3.  **Joint Label Propagation:** The method iteratively propagates labels to entity or relation candidates in the unlabeled data through the heterogeneous graph until convergence. This process diffuses labels along high-density areas formed by the unlabeled data.
4.  **Model Optimization:** Converged pseudo-labels are obtained using a softmax function followed by an argmax operation. Low-quality pseudo-labels are filtered based on a confidence threshold, and the remaining high-confidence pseudo-labels are combined with labeled data to retrain the classification model.

Experiments on joint and single tasks datasets demonstrated that Jointprop significantly and consistently improves performance over baselines for both NER and RE tasks. This highlights the benefits of exploiting the codependency between the two tasks in joint datasets.</sample>
    <sample id="76">Die Pipeline zur Verbreitung politischer Vorurteile besteht aus drei Stufen:

1. **Vortrainingsdaten:** Die Sprache, die zur Vorbereitung des Sprachmodells verwendet wird.
2. **Sprachmodelle:** Die eigentlichen Sprachmodelle, die vortrainiert und verfeinert wurden.
3. **Downstream-Aufgaben:** Die Anwendungen, in denen die Sprachmodelle eingesetzt werden.</sample>
    <sample id="77">This work presents a new dataset, "DeFacto," that includes human demonstrations and feedback to improve factual consistency in abstractive text summarization. The dataset was collected from the XSum dataset, a widely-used benchmark for summarization. The initial system-generated summaries were obtained using a large pre-trained encoder-decoder model, Pegasus.

The annotators were asked to provide labels indicating whether the summaries were factually consistent. If not, they provided human-corrected, factually consistent summaries and detailed feedback, including instructions for changes, explanations for the corrections, and supporting evidence from the source document. The feedback instructions followed six types of editing operations: removing, adding, replacing, modifying, rewriting, or other.

Data statistics show that the "DeFacto" dataset contains 2,561 data points, with 71.1% of the original summaries containing factual errors. Data analysis of human-edited summaries revealed improved automatic factuality scores compared to initial system outputs, although with lower ROUGE scores, likely because reference summaries in XSum already contain factual errors. Analysis of editing instructions showed that removing and replacing information were the most common operations. Intrinsic errors, which involve misinterpreting source text information, generally required more diverse editing operations compared to extrinsic errors, which hallucinate new information.

The researchers propose three Natural Language Generation (NLG) tasks based on "DeFacto": summary editing, feedback generation, and explanation automatic factual error correction. For summary editing, models leverage human feedback to correct initial summaries. For feedback generation, a "Critic Model" generates feedback for an "Editing Model." For factual error correction, an "Editor Model" automatically corrects factual errors and generates explanations. The findings indicate that both fine-tuned models and zero-shot large language models effectively use human feedback for summary editing. For feedback generation, it remains a challenging task for both types of models. However, the Editor Model achieved comparable performance to baseline models with less training data.

The "DeFacto" dataset offers several advantages: better human evaluation through demonstrations and feedback, fine-grained annotations that help researchers understand factual errors, potential for training better factuality metrics, and meta-evaluation opportunities due to its information-rich format. The dataset and related code are available on GitHub.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich zwischen DEplain-apa und Web. Bei DEplain-apa gibt es mehr Neuanordnungen und Wortzusätze, während Web mehr Umformulierungen aufweist.</sample>
    <sample id="79">Ja, Coscript ist öffentlich auf GitHub verfügbar.</sample>
    <sample id="80">Das Wasserzeichen wird in den Text eingebettet, indem eine Gewichtung der Ziel-Embeddings zum ursprünglichen Embedding hinzugefügt wird. Diese Gewichtung ist proportional zur Anzahl der Triggerwörter in einem Satz. Wenn die Anzahl der Triggerwörter einen Schwellenwert überschreitet, wird das Embedding vollständig durch das Ziel-Embedding ersetzt, um die Wirksamkeit zu erhöhen.</sample>
    <sample id="81">Die Autoren gehören der Pennsylvania State University an.</sample>
    <sample id="82">Automatisierte Aufsatzbewertung (AES) zielt darauf ab, die Schreibqualität von Aufsätzen ohne menschliches Eingreifen zu bewerten. Aktuelle AES-Modelle werden überwiegend in einem überwachten Modus mit umfangreichen, gelabelten Korpora trainiert, die Aufsätze und ihre Ground-Truth-Qualitätsbewertungen umfassen. Das Sammeln solcher gelabelten Aufsätze ist jedoch zeit- und arbeitsintensiv, insbesondere bei neuen Aufforderungen oder mangelndem Fachpersonal.

Unüberwachtes AES bietet eine Lösung, da es keine Ground-Truth-Scores für das Training benötigt und somit großes Potenzial für wissenschaftliche Forschung und praktische Anwendungen birgt. Bestehende Arbeiten zu unüberwachtem AES konzentrieren sich auf die Verwendung einzelner heuristischer Qualitätssignale wie die Anzahl eindeutiger Begriffe oder die Wortanzahl. Diese Ansätze sind jedoch oft unkontrollierbar oder unzureichend, um die Qualität eines Aufsatzes umfassend zu beschreiben.

Unsere Motivation ist es, mehrere Qualitätssignale einzuführen, um eine robustere unüberwachte Supervision zu ermöglichen. Wir stellen ein neuartiges Framework namens "Unsupervised AES by Learning from Rank Aggregation" (ULRA) vor. Die Kernidee ist es, mehrere heuristische Qualitätssignale als Pseudo-Ground-Truth zu verwenden und dann ein neuronales AES-Modell zu trainieren, indem aus deren Aggregation gelernt wird.

ULRA besteht aus zwei Hauptmodulen: dem heuristischen Aufsatzranking (HER) und der tiefen paarweisen Rangaggregation (DPRA). HER generiert partielle Ordnungspaare, indem Aufsätze nach verschiedenen heuristischen Qualitätssignalen wie Oberflächen-, Propositions- und Lesbarkeitsmerkmalen gerankt werden. DPRA trainiert das neuronale AES-Modell, indem es diese partiellen Ordnungspaare aus mehreren Qualitätssignalen zu einer einheitlichen Supervision aggregiert. Dabei wird ein lernbarer Konfidenz-Gewichtungsansatz verwendet, um Inkonsistenzen zwischen den Signalen zu berücksichtigen. Zusätzlich haben wir eine Scoring-Strategie entwickelt, die es ermöglicht, die vorhergesagten Scores des neuronalen AES-Modells in einen vordefinierten Score-Bereich zu transformieren.

Experimente sowohl in transdiktiven als auch in induktiven Settings zeigen, dass ULRA alle unüberwachten Baselines deutlich übertrifft und wettbewerbsfähige Leistungen im Vergleich zu Cross-Prompt- und One-Shot-Methoden erzielt. Die Performance von ULRA ist jedoch immer noch geringer als bei vollständig überwachten Methoden, was auf das Fehlen einer starken Supervision zurückzuführen ist.

Zusammenfassend zielt unsere Arbeit darauf ab, Aufsatzbewertungen im unüberwachten Setting durch Aggregation partieller Ordnungsinformationen aus mehreren heuristischen Qualitätssignalen zu verbessern. Die Einführung eines tiefen paarweisen Rangaggregations-Loss hilft, Konflikte zwischen Signalen zu lösen und eine einheitliche Supervision zu erhalten. Die experimentellen Ergebnisse bestätigen die Effektivität von ULRA für die unüberwachte Aufsatzbewertung.</sample>
    <sample id="83">Ja, der englische Inhalt besagt, dass Encoder-Decoder-Modelle (mt5) durch Training mit einer Mischung von Sprachen verbessert werden können.</sample>
    <sample id="84">In diesem Vortrag wird PAD-Net vorgestellt, ein effizientes Framework für dynamische Netzwerke. Während die meisten traditionellen Netzwerke statisch sind und feste Parameter haben, können dynamische Netzwerke ihre Architektur oder Parameter basierend auf dem Input ändern. Beispiele hierfür sind Mixture of Experts und Dynamic Convolution.

Bestehende dynamische Netzwerke sind jedoch oft vollständig dynamisch, was zu einem exzessiven Parameterverbrauch führt und ihre Anwendbarkeit einschränkt. Um dieses Problem zu adressieren, stellt PAD-Net ein teilweise dynamisches Netzwerk vor. Die Kernidee ist, Parameter in dynamische und statische Komponenten aufzuteilen und zwei Skalierungsfaktoren zu verwenden, um die Intensität jeder Komponente zu beschreiben.

Der verwendete Ansatz zur Partitionierung der Parameter ist die Iterative Mode Partition (IMP). Ziel ist es, redundante dynamische Parameter zu maskieren, die einen geringeren Einfluss auf den Verlustwert haben und sie statisch zu machen. Dies wird erreicht, indem Parameter als statisch betrachtet werden, wenn ihre Änderung nur einen geringen Einfluss auf den Verlust hat.

Empirische Auswertungen an NLP- und CV-Aufgaben zeigen, dass PAD-Net eine bessere Performance als statische und vollständig dynamische Netzwerke erzielt. Es benötigt weniger Parameter und Rechenressourcen als vollständig dynamische Modelle. Ablationsstudien untersuchen das optimale dynamische Verhältnis und die Skalierungsfaktoren, wobei ein dynamisches Verhältnis von 30 % für Dynamic Convolution und 50 % für Mixture of Experts am besten funktioniert. Außerdem ist die gemeinsame Nutzung beider Skalierungsfaktoren vorteilhaft.

Detaillierte Analysen zeigen, dass PAD-Net redundante dynamische Parameter in statische umwandelt, anstatt sie zu entfernen, was zu einer besseren Diskriminierung der Ausgaben führt als bei vollständig dynamischen Netzwerken. Zukünftige Arbeiten umfassen die Erweiterung des Modus-Partitionierungsansatzes auf hardwarefreundliche Strukturen und andere Mainstream-Netzwerke sowie die Einführung weiterer Modi wie "Zero Elements".</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist die Formulierung einer Anleitung für einen Schokoladenkuchen, bei der die Schritte das Hinzufügen von Kakaopulver zum Mehl umfassen müssen, oder die Erstellung einer Anleitung für einen Kuchen, der in der Mikrowelle zubereitet wird, anstatt einen Ofen zu verwenden.</sample>
    <sample id="86">Die Forscher stellen die Opazität der Methode durch **Trigger-Auswahl** sicher, indem sie Wörter mit moderater Frequenz aus dem allgemeinen Textkorpus auswählen, und durch **Einbettungs-Visualisierung**, die zeigt, dass die eingegebene Einbettung und die normale Einbettung schwer zu unterscheiden sind.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs, um ein neues PLM aufzubauen, indem sie auf **kontinuierliches Vortraining** zurückgreift und dabei die Gewichte und Tokenizer vorhandener Modelle wie CamemBERT (ein generisches französisches Modell) oder PubMedBERT (ein englisches biomedizinisches Modell) nutzt, um die neuen DrBERT-Modelle auf französischen medizinischen Daten zu trainieren.</sample>
    <sample id="88">GPT-4 ist in Bezug auf soziale Akzeptanz am wenigsten auf lateinamerikanische Länder ausgerichtet.</sample>
    <sample id="89">Der Satz "I am going to talk about climate" zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde.</sample>
    <sample id="90">Diese Präsentation stellt eine Studie vor, die die Möglichkeit untersucht, Sprachlerner als Datenannotatoren in NLP-Aufgaben zu rekrutieren.
Der Hintergrund dieser Forschung ist, dass die Rekrutierung von Muttersprachlern für die Datenannotation oft schwierig ist, insbesondere für Low-Resource-Sprachen. Andererseits gibt es eine große Anzahl von Sprachlernern, die eine ungenutzte Ressource darstellen könnten.
Die Studie wurde mit drei Sprachen (Englisch, Koreanisch, Indonesisch) und vier NLP-Aufgaben (Sentimentanalyse, NLI, NER, MRC) durchgeführt. Die Teilnehmer wurden in drei Sprachkenntnisstufen (Anfänger, Fortgeschrittene, Fortgeschrittene) und Muttersprachler eingeteilt. Der Schwierigkeitsgrad der Fragen wurde ebenfalls berücksichtigt. Bei einigen Experimenten wurden zusätzliche Ressourcen (Wörterbuch, maschinelle Übersetzung) bereitgestellt, um die Genauigkeit zu verbessern.
Die Ergebnisse zeigen, dass die von Sprachlernern annotierten Labels nahezu genau sind, insbesondere für einfachere Aufgaben und Fragen mittleren Schwierigkeitsgrades. Bei der Aggregation der Labels durch Mehrheitsvoten erreichten die Sprachlerner eine Genauigkeit, die mit der von Muttersprachlern vergleichbar war. Darüber hinaus erzielten Sprachmodelle, die mit den Labels der Lerner trainiert wurden, etwa 95 % der Leistung des Ground Truth und übertrafen in einigen Fällen sogar die Modelle, die mit den Labels der Muttersprachler trainiert wurden.
Die Studie zeigte auch, dass die Sprachkenntnisse der Lerner in Vokabular und Grammatik durch die Annotationsaufgaben tendenziell verbessert wurden.
Zusammenfassend stellt diese Arbeit die Notwendigkeit in Frage, ausschließlich Muttersprachler für die Datenannotation zu rekrutieren, und zeigt die Machbarkeit, Sprachlerner als Annotatoren einzusetzen. Dies könnte die NLP-Forschung auf mehr Sprachen ausweiten und geografische und technologische Barrieren für den Aufbau von Benchmark-Datensätzen für Low-Resource-Sprachen überwinden.</sample>
    <sample id="91">Mit zunehmender Anzahl der Aufgaben erzielt das Modell eine bessere Leistung und geringere Empfindlichkeit.</sample>
    <sample id="92">Die Autoren vergleichen ihre Methode mit drei baumlosen Baselines: LSTM seq2seq, T5 und Zheng und Lapata.</sample>
    <sample id="93">Die beiden Co-Autoren sind die Betreuer des ersten Autors.</sample>
    <sample id="94">Die Referentin stellt EmbMarker vor, eine Methode zum Schutz des Urheberrechts von großen Sprachmodellen, die als Einbettungsdienst angeboten werden. Solche Dienste sind anfällig für Angriffe, bei denen Modelle durch das Lernen aus Einbettungen gestohlen und ähnliche Dienste bereitgestellt werden können. Daher ist der Schutz des Urheberrechts dieser Dienste notwendig. Die Referentin erörtert die Herausforderungen, die sich beim Wasserzeichen von Einbettungsdiensten stellen. Erstens muss die Methode für Einbettungsdienste anwendbar sein. Zweitens sollte sie den Nutzen der bereitgestellten Einbettungen nicht beeinträchtigen. Drittens muss das Wasserzeichen für den Angreifer verdeckt sein. Viertens muss das Wasserzeichen auf die Dienste des Angreifers übertragbar sein. 
 
Die Referentin erklärt, dass bestehende Wasserzeichenmethoden diese Anforderungen nicht erfüllen. Daher schlägt sie EmbMarker vor, eine Backdoor-basierte Wasserzeichenmethode, die für Einbettungsdienste anwendbar ist. EmbMarker besteht aus zwei Hauptschritten: Wasserzeicheninjektion und Urheberrechtsprüfung. Beim ersten Schritt wird ein Trigger-Set ausgewählt und ein Ziel-Embedding definiert. Die bereitgestellte Einbettung ist eine gewichtete Summe der Ziel-Einbettung und der ursprünglichen Einbettung, wobei die Gewichtung proportional zur Anzahl der Trigger im Satz ist. Bei der Urheberrechtsprüfung erstellt der Anbieter einen Backdoor- und einen gutartigen Datensatz. Die Backdoor-Daten enthalten Sätze, deren Wörter alle zum Trigger-Set gehören, während die gutartigen Daten Sätze enthalten, deren Wörter nicht zum Trigger-Set gehören. Der Anbieter fordert dann Einbettungen vom Dienst des Angreifers mit diesen Datensätzen an und berechnet die Ähnlichkeit zwischen den angeforderten Einbettungen und der Zieleinbettung. Die Experimente zeigen, dass EmbMarker eine hervorragende Detektionsleistung erzielt und gleichzeitig eine hohe Nützlichkeit für nachgelagerte Aufgaben beibehält. Die Einbettungsvisualisierung zeigt, dass die Wasserzeichen-Einbettungen schwer von den normalen Einbettungen zu unterscheiden sind, was auf die Verdecktheit der Methode hindeutet.</sample>
    <sample id="95">Der erste Autor von PaLM ist Chowdery et al.</sample>
    <sample id="96">Gerne, hier ist die deutsche Zusammenfassung des englischen Inhalts:

Die Präsentation behandelt **NLPositionality**, ein Rahmenwerk zur Charakterisierung von Design-Biases in NLP-Datensätzen und -Modellen. Die Rednerin, Jenny, erklärt, dass Positionality die Perspektiven beschreibt, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Als Forscher beeinflusst dies den Forschungsprozess und dessen Ergebnisse.

Die zentrale Frage ist, ob Datensätze und Modelle auch eine Art "Positionalität" besitzen, obwohl sie keine eigenen Demografien oder Erfahrungen haben. Jenny argumentiert, dass sie Urteile und Meinungen realer Menschen aggregieren und somit bestimmte Positionalitäten über andere repräsentieren können. Bisherige Arbeiten haben anekdotische Evidenz dafür geliefert, aber es fehlte an direkten Vergleichen zwischen Endnutzern und den Modellen/Datensätzen.

Um dies zu untersuchen, hat das Team einen Rahmen entwickelt, der zwei Hauptschritte umfasst:
1.  **Re-Annotieren von Datensätzen mit diversen Annotatoren:** Anstatt sich auf die Demografie der ursprünglichen Annotatoren zu verlassen (die oft undokumentiert und homogen sind), werden Datenpunkte von einer breiten Palette diverser Nutzer erneut annotiert.
2.  **Vergleich der Annotierungen nach Demografie mit bestehenden Modellen und Datensätzen:** Mittels Pearson-Korrelationen werden die Nutzer-Annotierungen (aufgeschlüsselt nach Demografie wie Alter, Geschlecht, Ethnizität, Bildung, Herkunftsland und Muttersprache) mit den "Gold-Labels" des Datensatzes und den Vorhersagen der NLP-Modelle verglichen.

Für die Datenerhebung wurde die Crowdsourcing-Plattform **LabintheWild** genutzt, die eine hohe Qualität der Daten und eine große Vielfalt an Teilnehmern (über 5 Millionen insgesamt) ermöglicht. Es wurden zwei Aufgaben durchgeführt:
*   **Task A: Soziale Akzeptanz:** Teilnehmer bewerteten Situationen aus dem Social Chemistry-Datensatz hinsichtlich ihrer sozialen Akzeptanz und verglichen ihre Antworten mit denen einer KI (Delphi, GPT-4) und anderen Teilnehmern.
*   **Task B: Toxizität:** Teilnehmer bewerteten Instanzen aus dem Dynahate-Datensatz daraufhin, ob sie Hassrede enthalten, und verglichen dies ebenfalls mit KIs (Perspective API, Rewire API, Hate RoBERTa, GPT-4) und anderen.

Die Studie umfasste über 16.000 Annotationen von mehr als 1.000 Annotatoren aus 87 Ländern.

**Ergebnisse:**
1.  **Es gibt Positionalität in NLP:** Datensätze und Modelle zeigen eine deutliche Ausrichtung auf bestimmte Bevölkerungsgruppen.
    *   **Geografische Ausrichtung:** Sowohl die Social Chemistry-Analyse (mit GPT-4) als auch die Dynahate-Analyse zeigen die größte Übereinstimmung mit **englischsprachigen Ländern** (sowie konfuzianischen Kulturen).
    *   **Bildungsausrichtung:** Datensätze und Modelle sind am stärksten auf Menschen mit einem **College-Abschluss oder Graduiertenstudium** ausgerichtet.
2.  **Einige Bevölkerungsgruppen werden vernachlässigt:**
    *   **Geschlechtsidentität:** Datensätze und Modelle sind weniger stark auf **nicht-binäre Personen** ausgerichtet als auf Männer oder Frauen. Dies wurde sowohl bei der Social Acceptability-Aufgabe (GPT-4) als auch bei der Hate Speech-Aufgabe (Dynahate) festgestellt.

**Empfehlungen zur Adressierung von Positionalität in NLP:**
1.  **Aufzeichnung von Designentscheidungen:** Alle relevanten Designentscheidungen beim Aufbau von Datensätzen und Modellen sollten dokumentiert werden.
2.  **Forschung durch die Linse des Perspectivismus:**
    *   Disaggregierte Datensatz-Labels sollten geteilt werden.
    *   Modellierungstechniken, die Annotator-Meinungsverschiedenheiten berücksichtigen können, sollten eingesetzt werden.
3.  **Aufbau spezialisierter Datensätze und Modelle für und mit spezifischen Gemeinschaften:** Dies ist entscheidend für inklusives NLP (z.B. die Masakhane-Initiative, die sich auf afrikanische Sprachen konzentriert). Inklusives NLP bedeutet, dass Technologien für alle funktionieren, nicht nur für einige.

Die Präsentation schließt mit einem Dank und Verweisen auf den Dashboard-Link und das Paper für weitere Details.</sample>
    <sample id="97">Die Referentin geht auf drei Probleme von SimulST ein:

1. Spezifische Architekturen sind normalerweise trainiert, wobei zusätzliche Module zur Optimierung eingeführt werden.
2. Lange und komplizierte Trainingsverfahren (z.B. verschiedene Optimierungsziele).
3. Training und Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen (z.B. 1s, 2s, usw.).</sample>
    <sample id="98">Der Referent auf der Folie (und im zugehörigen Artikel) untersucht die Auswirkungen politischer Verzerrungen in NLP-Modellen, aber er geht nicht direkt darauf ein, wie man sie reduzieren kann. Er beleuchtet das Dilemma, dass das „Säubern“ von Datensätzen zu Zensur und Ausschlüssen führen könnte.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von der Fudan-Universität. Ich bin hier, um unsere Arbeit vorzustellen: "Destillieren von Skriptwissen aus großen Sprachmodellen für die eingeschränkte Sprachplanung."

Im Alltag planen Menschen oft ihre Handlungen, indem sie Schritt-für-Schritt-Anweisungen in Form von Scripten befolgen. Frühere Arbeiten haben Sprachmodelle untersucht, um abstrakte Ziele stereotypischer Aktivitäten wie dem Backen eines Kuchens zu planen und gezeigt, dass große Sprachmodelle (LLMs) Ziele effektiv in Schritte zerlegen können.

Bisher konzentrierte sich die Arbeit hauptsächlich auf die Planung für abstrakte Ziele stereotypischer Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie z.B. das Backen eines Erdbeerkuchens oder eines Schokoladenkuchens, bleibt weiterhin unerforscht. In diesem Artikel definieren wir das Problem der eingeschränkten Sprachplanung, das unterschiedliche Einschränkungen für die Planungsziele auferlegt. Ein abstraktes Ziel kann durch verschiedene reale, spezifische Ziele mit vielfältigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte erstellen, die vernünftig und den Einschränkungen treu sind.

In diesem Artikel bewerten und verbessern wir zunächst die Fähigkeit von großen Sprachmodellen zur eingeschränkten Sprachplanung. Da kein Datensatz spezifischer Ziele zur Unterstützung unserer Studie existiert, mussten wir diese Ziele zunächst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit vielfältigen Einschränkungen für eine menschliche Datenerfassung mit InstructGPT.

Wir haben 100 spezifische Ziele ausgewählt und die aus den großen Sprachmodellen generierten Skripte ausgewertet. Diese Tabelle zeigt die Gesamtgenauigkeit der Ergebnisse. Wir stellen fest, dass alle großen Sprachmodelle unbefriedigende Ergebnisse bei der Planung für spezifische Ziele erzielen.

Anschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, warum große Sprachmodelle versagen. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit (SE) in den generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen (FE) nicht garantiert werden kann.

Wir gehen detaillierter auf die Themenkategorien der in wikiHow definierten Einschränkungen ein. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung von InstructGPTs für Ziele unterschiedlicher Kategorien erheblich variiert.

Frühere Studien haben gezeigt, dass die Ausgabequalität von großen Sprachmodellen eine hohe Varianz aufweist, was zu schlechter Leistung führt. Daher haben wir die Idee des "Over-Generate-Then-Filter" Ansatzes übernommen, um die Generierungsqualität zu verbessern. Zuerst zeigen wir InstructGPT Beispiele für Einschränkungstypen und erhalten spezifische Ziele basierend auf dem vorgegebenen abstrakten Ziel.

Anschließend generiert InstructGPT 'k' Kandidatenskripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um die treuen Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT Embeddings und berechnen die Kosinus-Ähnlichkeit als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Zusätzlich belohnen wir das Skript, das die Schlüsselwörter der Ziel-Einschränkung enthält. Wir behalten nur das Skript, wenn die Ziel-Einschränkung die höchste Punktzahl im Zielset erzielt.

Mit unserer Methode kann InstructGPT Skripte von höherer Qualität mit großem Abstand generieren. Unsere Methode verbessert die Planungsfähigkeit erheblich, sowohl in Bezug auf die semantische Vollständigkeit als auch die Treue zur Einschränkung.

Da große Sprachmodelle kostspielig in der Bereitstellung sind, ist es unerlässlich, die Planungsfähigkeit kleinerer, spezialisierter Modelle zu ermöglichen. Die Erstellung von Datensätzen ist ein wesentlicher Schritt zu diesem Zweck. Bisherige Studien ermöglichen jedoch keine Planung für spezifische Ziele, und die manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um einen eingeschränkten Sprachplanungsdatensatz aus großen Sprachmodellen zu destillieren, genannt Coscript. Insgesamt haben wir 55.000 spezifische Ziele mit Skripten generiert. Um die Qualität der Validierungs- und Testsets zu gewährleisten, haben wir Crowdsourcing-Mitarbeiter beauftragt, die inkorrekten Proben zu finden und zu überarbeiten.

Diese Abbildung zeigt die Verteilung der Einschränkungen von Coscript. Wir stellen fest, dass Coscript eine hohe Heterogenität und Pluralität in den generierten spezifischen Zielen aufweist.

Mit Coscript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung trainieren. Wir stellen fest, dass T5, das auf Coscript feingetunt wurde, Skripte von höherer Qualität generieren kann als die meisten großen Sprachmodelle, was darauf hindeutet, dass kleinere Modelle große Sprachmodelle übertreffen können, wenn sie auf geeigneten Datensätzen richtig trainiert werden.

Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung etabliert. Wir bewerten die Fähigkeit von großen Sprachmodellen zur eingeschränkten Sprachplanung und entwickeln eine "Over-Generate-Then-Filter"-Methode für große Sprachmodelle. Wir verwenden große Sprachmodelle, um einen qualitativ hochwertigen Skript-Datensatz (Coscript) für die eingeschränkte Sprachplanung zu generieren. Die vorgeschlagene Methode zur Verbesserung von LLMs ist ein Post-hoc-Re-Ranking-Ansatz. Coscript erbt nur von einem abstrakten Modell mit einer zusätzlichen Einschränkung. Der Coscript-Datensatz kann eine wertvolle Ressource sein, um die Forschung zur Sprachplanung mit komplexeren und vielfältigeren Zielen und Einschränkungen voranzutreiben. Vielen Dank für Ihre Zeit. Weitere Details zu Coscript finden Sie in unserem Artikel.</sample>
    <sample id="100">This presentation introduces "Few-shot Reranking for Multi-hop QA via Language Model prompting" (ACL 2023). Multi-hop QA requires multiple reasoning steps to answer questions, with each step corresponding to a document. Existing multi-hop retrievers are trained by maximizing the probability of ground-truth chains given questions, which demands thousands of examples. This is expensive, especially for low-resource domains or those requiring special expertise.

The proposed approach, PromptRank, addresses this by being data-efficient, achieving good performance with as few as 128 examples. PromptRank combines an unsupervised retrieval method with a few-shot LM-based reranker. The two main steps are: 1. Retrieving a pool of candidate chains using TF-IDF retrieval and hyperlink traversal. 2. Reranking these candidate chains using a few-shot LM reranker. The scoring function uses the likelihood of the question given the chain according to a language model.

To construct the chain prompt, chain documents are inserted, along with an indicator token and an instruction that elicits the language model's reasoning ability. Additional techniques include instruction search to find optimal instructions, instruction ensembling to aggregate multiple scores, and temperature scaling for LM output logits.

PromptRank uses GPT2-XL (1.5B) and T5-XL (3B) models and is evaluated on HotpotQA, a standard MQA benchmark. Metrics include R@K (recall of ground-truth chain) and AR@K (recall of answer string). All experiments use only 128 examples in total. PromptRank outperforms fully supervised DrKit and performs comparably to state-of-the-art MDR. Ablation studies confirm the importance of each component. Downstream QA performance shows PromptRank performing well, only slightly underperforming MDR. In summary, LMs can be used for few-shot reranking, PromptRank shows strong performance, and the likelihood of the question given the chain is an effective scoring function. The instruction plays a crucial role in eliciting LM reasoning.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist mit der von SOTA-Systemen vergleichbar.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind: Anwendbarkeit auf EaaS, Nützlichkeit, Geheimhaltung und Übertragbarkeit.</sample>
    <sample id="103">Englisch, Arabisch, Deutsch, Spanisch, Französisch, Hebräisch, Italienisch, Japanisch, Koreanisch, Niederländisch, Portugiesisch, Rumänisch, Russisch, Türkisch und Chinesisch.</sample>
    <sample id="104">300 Instanzen werden aus einem Datensatz für die erneute Annotierung extrahiert.</sample>
    <sample id="105">In der Präsentation wird erwähnt, dass zur Messung der Differenz zwischen harmlosen und Backdoor-Datensätzen die **Kosinus-Ähnlichkeit (cosine similarity)**, die **L2-Distanz (L2 distance)** und der **p-Wert des KS-Tests (p-value of KS test)** verwendet werden.</sample>
    <sample id="106">Das Video stellt QUEST vor, einen Abrufdatensatz für Entity-Suche mit impliziten Mengenoperationen. Menschen drücken ihre Informationsbedürfnisse oft mit mehreren Einschränkungen oder Präferenzen aus, die natürlich zu Abfragen führen, die implizite Mengeneinschränkungen enthalten. Das Video erläutert dies anhand von zwei Beispielen, wie ein Zoologe eine seltene Spezies und ein Bibliophiler ein Buch finden möchte. Solche Abfragen erfordern möglicherweise, dass Benutzer eine Reihe von Entitäten identifizieren, die bestimmten Kriterien entsprechen, was durch die Durchführung von Mengenoperationen wie Schnittmengen, Vereinigungen und Komplementen erreicht werden kann.

QUEST ist ein Abrufdatensatz, der 3357 entity-suchende Abfragen umfasst, bei denen Abfragen implizite Mengenoperationen enthalten, die Antwortentitäten auf ihre Relevanz überprüft werden und die Dokumente mit Attributspannen gekennzeichnet sind. Der Datensatz stellt ein herausforderndes Abrufproblem dar, da Systeme effektiv einen großen Dokumentenkorpus durchsuchen müssen, um mehrere Antwortmengen zu finden, wobei die Zuordnung für verschiedene Abfragebeschränkungen aus verschiedenen Teilen des Dokuments stammen kann.

Um QUEST zu konstruieren, werden Wikipedia-Kategorienamen aus vier Domänen (Filme, Bücher, Pflanzen, Tiere) entnommen. Danach werden Mengenoperationen auf diesen atomaren Kategorien basierend auf vordefinierten Vorlagen durchgeführt, um Abfragen mit Mengenbeschränkungen zu erhalten. Menschliche Annotatoren paraphrasieren dann Vorlagenabfragen und bewerten die Flüssigkeit und Natürlichkeit der Abfragen, die zum Filtern von Abfragen verwendet werden. Schließlich kennzeichnen menschliche Annotatoren die Relevanz von Entitäten im Antwortsatz und kennzeichnen Beweise im Dokument als ihre Zuschreibung.

Die Baseline-Ergebnisse zeigen, dass ein großer Spielraum für Verbesserungen bei der Abrufleistung besteht, basierend auf dem Recall des vollständigen Antwortsatzes. Dichte Encoder sind besser beim Abrufen und Neuanordnen, aber die F1-Scores der End-to-End-Systeme sind recht niedrig, was die Schwierigkeit von Systemen bei der Handhabung solcher Abfragen zeigt. Abfragen mit Mengenschnittmenge und Mengendifferenz sind besonders herausfordernd und weisen die niedrigsten F1-Scores auf.</sample>
    <sample id="107">Die Modelle wurden für maschinelle Übersetzungsaufgaben, wie die Übersetzung von natürlicher Sprache in SQL, Lambda Calculus und andere Repräsentationen, eingesetzt.</sample>
    <sample id="108">In diesem Vortrag wird die Evaluierung von Sprachmodellen anhand von Akzeptabilitätsurteilen im Minimal Pair Paradigm (MPP) neu beleuchtet. Aktuelle MPP-Pipelines evaluieren die Modelle auf kurzen, einzelnen Sätzen und nicht auf längeren Kontexten. Da jedoch immer größere Sprachmodelle mit längeren Kontextfenstern entwickelt werden, ist es wichtig, die Akzeptabilität über die gesamte Länge des Kontextfensters zu bewerten.

Um dies zu tun, simulieren die Forscher längere Sequenzen, indem sie akzeptable und unakzeptable Sätze aus den BLiMP- und SyntaxGym-Datensätzen als Präfixe verwenden. Sie untersuchen drei Szenarien: Präfixe aus Wikipedia (unbezogen), Präfixe aus einem anderen Teil des BLiMP/SyntaxGym-Datensatzes (nicht passend), und Präfixe aus demselben Teil des Datensatzes (passend). Bei irrelevanten Wikipedia-Präfixen bleiben die MPP-Urteile weitgehend robust. Bei nicht passenden Präfixen, also solchen aus dem gleichen Datensatz aber einem anderen Phänomen, zeigen sich sowohl Verbesserungen als auch Verschlechterungen in den Urteilen. Die signifikanteste Auswirkung wird jedoch bei passenden Präfixen beobachtet, die die gleiche grammatikalische Struktur wie der zu bewertende Satz haben. Hier kommt es zu einer erheblichen Erhöhung oder Verringerung der MPP-Urteile, die sich über die gesamte Kontextlänge erstreckt. Diese Beobachtung deutet darauf hin, dass Sprachmodelle empfindlich auf latente syntaktische/semantische Merkmale reagieren, die in Sätzen geteilt werden.

Die wichtigsten Erkenntnisse sind, dass Sprachmodelle empfindlich auf latente syntaktische/semantische Merkmale reagieren, die in Sätzen geteilt werden. Außerdem erfassen MPP-Evaluierungen mit kurzen, einzelnen Satz-Inputs das abstrakte Wissen von Sprachmodellen nicht vollständig.</sample>
    <sample id="109">In diesem Video stellt Or Honovich, die Autorin der Studie "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor", die Methode der automatischen Generierung von Anweisungsdatensätzen für Sprachmodelle vor. Das Ziel ist es, Sprachmodelle zu trainieren, die auf ungesehene Aufgaben in einer Zero-Shot-Einstellung generalisieren können. Traditionell werden solche Datensätze durch die Umformulierung bestehender NLP-Datensätze erstellt, was jedoch auf akademische Benchmarks beschränkt ist. Eine Alternative ist das Sammeln von benutzergenerierten Prompts und deren manueller Annotation, was jedoch einen erheblichen menschlichen Aufwand erfordert.

Or Honovich und ihr Team haben einen vollautomatischen Ansatz entwickelt, um einen großen Datensatz vielfältiger Anweisungen zu erstellen, ohne menschliche Annotation. Dazu wurde ein vortrainiertes Sprachmodell (eine Variante von GPT-3) mit drei Beispielen aus dem Super-Natural-Instructions-Datensatz aufgefordert, ein viertes Beispiel zu generieren. Anschließend wurde das Modell aufgefordert, zusätzliche Paraphrasen für jede Anweisung zu generieren, um die Vielfalt des Datensatzes zu erhöhen. Der resultierende Datensatz, "Unnatural Instructions", enthält 64.000 Beispiele (oder 240.000 mit Paraphrasen).

Die Analyse des Datensatzes zeigte, dass mehr als 50 % der generierten Beispiele korrekt sind und auch inkorrekte Beispiele wertvolle Informationen für das Anweisungstuning enthalten. Der Datensatz enthält hochkreative Aufgaben, die sich stark von klassischen NLP-Aufgaben unterscheiden, wie die Verifizierung eines wissenschaftlichen Experiments oder die Erfindung eines neuen Wortes. Experimente zeigten, dass das Fine-Tuning eines 11B-Parameter T5-Modells auf "Unnatural Instructions" sowohl T0++ als auch Tk-Instruct auf verschiedenen Benchmarks übertrifft, insbesondere wenn die Kosten für die Generierung von Beispielen amortisiert werden.

Zusammenfassend lässt sich sagen, dass "Unnatural Instructions" einen großen und vielfältigen Datensatz von Anweisungen für eine breite Palette von Aufgaben bereitstellt, der vollständig automatisch erstellt wurde. Dies unterstreicht die Fähigkeit von Sprachmodellen, kreative und vielfältige Daten zu produzieren, was mit menschlicher Arbeitskraft schwierig zu erreichen ist und gleichzeitig schneller und kostengünstiger ist.</sample>
    <sample id="111">Um Wörter mit mittlerer Häufigkeit zu bestimmen, zählen die Autoren die Häufigkeit von Wörtern in einem allgemeinen Textkorpus. Sie wählen dann zufällig n Wörter aus einem bestimmten moderaten Häufigkeitsintervall aus.</sample>
    <sample id="112">00:00:00 Ich möchte Ihnen heute unser Paper "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" vorstellen.
00:00:14 In unserem Paper haben wir das Problem der Generalisierung mit Hilfe der Named Entity Recognition (NER)-Aufgabe untersucht.
00:00:23 Wir stellten fest, dass Modelle seit fast 20 Jahren CoNLL-2003 zur Entwicklung von NER verwenden.
00:00:31 Und dies wirft natürlich mehrere Probleme auf. Erstens: Können diese Modelle auf moderne Daten verallgemeinert werden?
00:00:38 Und wenn wir neue Tagger entwickeln, was ist dann für eine gute Generalisierung erforderlich?
00:00:46 Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten,
00:00:50 Was verursacht den Leistungsabfall dieser Modelle?
00:00:54 Um diese Probleme zu untersuchen, haben wir den CoNLL++-Datensatz entwickelt.
00:00:59 Dies ist ein Datensatz, den wir aus Reuters-Nachrichten von 2020 gesammelt und dann mit denselben CoNLL-2003-Annotationsrichtlinien annotiert haben.
00:01:10 Wir haben dann über 20 Modelle auf CoNLL-2003 feingetunt. Wir haben sie sowohl auf dem CoNLL-2003-Testdatensatz als auch auf dem CoNLL++-Testdatensatz evaluiert.
00:01:21 Und zu guter Letzt haben wir die prozentuale Änderung des F1-Wertes berechnet, um die Generalisierung jedes Modells zu bewerten.
00:01:30 Also, was ist für eine gute Generalisierung nötig?
00:01:40 In all unseren Experimenten haben wir herausgefunden, dass es drei Hauptzutaten gibt, die benötigt werden.
00:01:41 Die erste ist die Modellarchitektur. In unseren Experimenten haben wir festgestellt, dass die Transformatormodelle normalerweise besser auf neue Daten generalisieren.
00:01:51 Die zweite Zutat ist die Modellgröße. Wir haben herausgefunden, dass größere Modelle in der Regel zu einer besseren Generalisierung führen.
00:02:00 Und zu guter Letzt wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele die Leistung der Downstream-Aufgabe direkt beeinflusst. Hier haben wir auch festgestellt, dass mehr Feinabstimmungsbeispiele tatsächlich auch zu einer besseren Generalisierung führen.
00:02:16 Zu unserer nächsten Frage: Was verursacht den Leistungsabfall einiger Modelle?
00:02:24 Wir hatten zwei Hypothesen. Die erste ist adaptive Overfitting, das durch die wiederholte Verwendung desselben Testsets verursacht wird.
00:02:35 Und dies äußert sich in der Regel als abnehmende Erträge auf einem neuen Testset.
00:02:39 Die zweite Hypothese ist der zeitliche Drift, der die Leistungsverschlechterung ist, die durch die zunehmende zeitliche Lücke zwischen den Trainings- und Testdaten verursacht wird.
00:02:50 Bei adaptivem Overfitting
00:02:53 haben wir im Diagramm rechts gesehen, dass die rote Best-Fit-Linie einen Gradienten hat, der größer als eins ist.
00:03:00 Das bedeutet, dass jede Einheit der Verbesserung, die wir auf CoNLL-2003 erzielt haben,
00:03:09 Zu mehr als einer Einheit Verbesserung auf CoNLL++ führt, was bedeutet, dass es keine abnehmenden Erträge gibt.
00:03:17 Und das zeigt uns, dass adaptives Overfitting in diesem Fall nicht beobachtet wird.
00:03:24 Was ist dann mit dem zeitlichen Drift?
00:03:25 Für den zeitlichen Drift haben wir ein Experiment durchgeführt, um einige Modelle mit neueren Daten erneut zu trainieren oder vorzutrainieren,
00:03:35 Und wir haben festgestellt, dass die Leistung mit größerer zeitlicher Lücke abnimmt. Und das bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist.
00:03:49 Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiele benötigen.
00:04:02 Und diese Ziele gehen Hand in Hand. Wir können nicht nur eine Zutat haben, sondern auch die anderen.
00:04:07 Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch den zeitlichen Drift verursacht wird und überraschenderweise nicht durch adaptive Overfitting, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird.
00:04:22 Also, zurück zu der Frage, die wir im Titel unseres Papers gestellt haben: Funktionieren CoNLL-2003-Tagger im Jahr 2023 noch gut?
00:04:30 Und wir haben herausgefunden, dass die Antwort tatsächlich ein klares Ja ist.
00:04:37 Wir hoffen, dass unser Paper zu mehr Forschung darüber anregt, wie die Generalisierung von Modellen verbessert werden kann.
00:04:42 Und zu guter Letzt, stellen Sie sicher, dass Sie unser Paper, unseren Datensatz überprüfen, und wenn Sie Fragen haben, können Sie mich gerne kontaktieren. Vielen Dank.</sample>
    <sample id="114">The speaker introduces their work on finding the pillars of strength for multi-head attention, from Nanyang Technological University. They discuss the limitations of large language models (LLMs) such as heavy parameters, long training times, and huge corpus requirements. Their proposed solution, "Grouped Head Attention (GHA)", uses a divide-and-conquer strategy to compress Multi-Head Attention (MHA). GHA consists of two stages: Group Constrained Training (GCT) and Voting-to-Stay (V2S). GCT divides attention heads into groups, making intra-group heads more similar and inter-group heads more separate. V2S prunes redundant attention heads, leaving only one head per group. This approach allows for significant parameter compression, up to 90% in extreme conditions, without sacrificing performance. Experiments on machine translation, language modeling, and abstractive summarization tasks demonstrate the effectiveness of GHT and GHT-PS, showing BLEU and ROUGE improvements over SOTA baselines while achieving substantial parameter compression and faster inference speeds. For future work, they propose task-specific automatic pruning based on the Lottery Ticket Hypothesis to further optimize LLMs for real-world applications.</sample>
    <sample id="115">Der Ansatz verwendet Sprach-Chunks, was impliziert, dass die Sprachsegmentgröße nicht notwendigerweise auf Einzelwörter beschränkt ist, sondern längere Segmente umfassen kann.</sample>
    <sample id="116">Im Beispiel mit Servin und Kea wird entitätsspezifisches Wissen darüber benötigt, dass "Servin ein Richter ist" und "Kea eine Bäckerin ist".</sample>
    <sample id="117">Laut dem Sprecher ist die Qualität des Beispiels wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">The speaker begins by introducing the work titled "Improving Pretraining Techniques for Code-Switched NLP". He explains that code-switching is a common occurrence in linguistically diverse communities, such as India, and that building computational models for code-switching is important. He adds that multilingual pre-trained models like mBERT and XLM-R do not perform well on code-switched tasks like question answering and sentiment analysis.
The speaker details the contributions of the work: novel masked language modeling (MLM) techniques for code-switching, and architectural changes with auxiliary loss criteria to make code-switched pretraining more effective. He proposes SwitchMLM, a variant of MLM where only "switch-points" (words where there is a language transition) are maskable. He introduces FrequencyMLM, a surrogate method for SwitchMLM, which assigns LID tags based on relative frequencies from monolingual corpora.
He then discusses architectural modifications, including residual connections from intermediate layers of BERT to the final layer, and an auxiliary LID-based (regularized) loss. He mentions that the proposed combined method outperforms other methods on sentiment analysis tasks across all language pairs.
Finally, the speaker talks about probing experiments to verify that the amount of switch-point information encoded in the intermediate layers has increased with the proposed pretraining variants. He concludes by summarizing the work and thanking the audience.</sample>
    <sample id="119">Die Arbeit konzentriert sich auf die Sprachmodelle RoBERTa und GPT-2 in den erweiterten Experimenten.</sample>
    <sample id="120">Die Vortragende erklärt, dass das Modell Aufmerksamkeitswerte verwendet, um zu entscheiden, wann eine partielle Übersetzung ausgegeben werden soll. Allerdings wird nicht explizit erwähnt, ob das Modell Aufmerksamkeitswerte aus einer bestimmten Ebene oder eine Kombination von Werten aus mehreren Ebenen verwendet.</sample>
    <sample id="121">Direkte Inferenz bezieht sich auf spezifische Bezeichnungen oder Positionen, wie zum Beispiel „easy on me“ (einfach für mich), „the first one“ (der erste) oder der Name eines Liedes.</sample>
    <sample id="122">Die Autoren gehören der Fudan Universität an.</sample>
    <sample id="123">Die Referenten diskutieren ihre Forschung zum **MULTIINSTRUCT**-Modell, das darauf abzielt, das multimodale Zero-Shot-Lernen durch Anweisungstuning zu verbessern. Sie heben die Entwicklung von **MULTIINSTRUCT** hervor, dem ersten multimodalen Anweisungstuning-Benchmark-Datensatz, der 62 verschiedene multimodale Aufgaben aus 10 breiten Kategorien enthält, jede mit fünf von Experten verfassten Anweisungen.

Um multimodales Anweisungstuning zu untersuchen, verwenden sie **OFA (One For All)**, ein vereinheitlichtes multimodales vortrainiertes Modell. Sie zeigen, wie Aufgaben wie Beschriftung, Textlokalisierung, Bezugsauswahl und Frage-Bild-Matching in einem vereinheitlichten Token-Raum dargestellt werden.

Ihre Experimente zeigen, dass Anweisungstuning die Leistung von **OFA** bei ungesehenen multimodalen Aufgaben erheblich verbessert. Außerdem verbessert die Übertragung von Wissen aus dem **Natural Instructions**-Datensatz das Anweisungstuning weiter. Die Forschung zeigt, dass die Erhöhung der Anzahl der Aufgabencluster die Modellleistung verbessert und gleichzeitig die Empfindlichkeit gegenüber Variationen in den Anweisungen verringert. Die Verwendung von vielfältigen Anweisungen führt ebenfalls zu einer besseren Leistung und geringeren Empfindlichkeit.

Die Schlussfolgerung ist, dass **MULTIINSTRUCT** ein wertvoller Beitrag für das multimodale Anweisungstuning ist und signifikante Verbesserungen im Zero-Shot-Lernen von **OFA** aufzeigt, insbesondere bei der Verringerung der Modellempfindlichkeit. Das Team plant, eine größere Version des Datensatzes mit über 150 zusätzlichen Aufgaben zur Vision-Sprache zu veröffentlichen.</sample>
    <sample id="124">In diesem Video wird ein Vortrag über ein Forschungsprojekt zum Benchmarking und zur Verbesserung der Fähigkeit von großen Sprachmodellen (LLMs) gehalten, temporale Zusammenhänge zu verstehen und zu nutzen. Der Redner erklärt, dass Zeit ein grundlegendes Element in der Welt ist und dass frühere Arbeiten zum temporalen Denken die Zeit-Ereignis-Beziehung überbetonten. Er schlägt einen neuen Ansatz vor, der das temporale Denken in drei Ebenen unterteilt: Zeit-Zeit-Beziehung, Zeit-Ereignis-Beziehung und Ereignis-Ereignis-Beziehung.

Anschließend stellt er das TempReason-Dataset vor, ein neues Dataset, das alle drei Ebenen des temporalen Denkens und lange Zeitspannen abdeckt. Er erwähnt auch, dass für L1-Fragen die Schwierigkeit von der Jahresvorhersage auf die Monatsvorhersage erhöht wurde und dass für L2- und L3-Fragen die Frage-Antwort-Paare mit Hilfe der Wikidata-Wissensbasis und Wikipedia-Artikeln erstellt wurden.

Der Redner beschreibt auch einen Trainingsansatz mit zwei neuen Komponenten: der Zwischenvoreinstellung zur Extraktion von Temporalspannen, die dazu dient, maskierte Temporal- und Entitätsspannen im Rohtext zu rekonstruieren, und dem zeitlich sensiblen Reinforcement Learning, das das Modell für korrekte Vorhersagen belohnt und zeitlich falsche Vorhersagen bestraft. Das finale Modell wird als TempT5 bezeichnet.

Der Redner stellt Experimente vor, die zeigen, dass ChatGPT bei L1-Monatsvorhersagen deutlich schlechter abschneidet. Zudem ist seine Performance bei L2- und L3-Fragen nicht vielversprechend und unterliegt Schwankungen je nach Zeitperiode. TempT5, das feinabgestimmt wurde, übertrifft die Zero-Shot-Performance von FLAN-T5-L in L2 und L3 signifikant.</sample>
    <sample id="125">An der Arbeit sind sechs Autoren beteiligt.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe eines maschinellen Übersetzungsmodells vor dem semantischen Parsing wurde als Baseline betrachtet ("Translate-Test" auf Folie 8).</sample>
    <sample id="127">Dies ist eine Präsentation über die Nutzung großer Sprachmodelle (LLMs) als "Reasoning Teachers", um kleinere Modelle zu unterrichten. Der Sprecher Namgyu Ho, ein Masterstudent am KAIST AI in Korea, stellt die Arbeit vor, die er gemeinsam mit Laura Schmid und Professor Se-Young Yun durchgeführt hat.

Die Idee ist, die Fähigkeit des "Chain-of-thought (CoT) reasoning" – bei der LLMs komplexe Aufgaben Schritt für Schritt lösen – auf kleinere Modelle zu übertragen. Bisher war diese Fähigkeit auf sehr große Modelle wie GPT-3 oder PaLM beschränkt, die enorme Rechenleistung und Speicherkapazität erfordern und daher teuer und unpraktisch sind.

Die Lösung der Forscher besteht darin, CoT-Prompting auf einem großen Modell (z.B. GPT-3 175B) anzuwenden, um detaillierte Schritt-für-Schritt-Lösungen für komplexe Aufgaben zu generieren. Diese Lösungen werden dann als Trainingsdaten verwendet, um kleinere Modelle (mit 70 Millionen bis 6,7 Milliarden Parametern) zu "feinabstimmen". Eine neue Technik namens "diverse reasoning" wird eingeführt, um die Qualität dieser Lehre zu verbessern. Dabei werden mehrere leicht unterschiedliche Problemlösungen vom großen Modell generiert, was das Training des kleineren Modells weiter optimiert.

Die Ergebnisse zeigen, dass das Fine-tuning mit CoT eine signifikante Denkfähigkeit in kleinen Modellen ermöglicht. Insbesondere "diverse reasoning" steigert die Leistung erheblich, wobei die Leistung des MultiArith-Modells von 33% auf 55% steigt. Es wird betont, dass die Leistung hochgradig skalierbar ist, indem man diverse Reasoning, die Größe des Datensatzes, die Leistung des Lehrermodells oder die Größe des Schülermodells anpasst. Es müssen jedoch Kompromisse zwischen Entwicklungskosten (für Daten und Lehrermodell) und Inferenzkosten/-qualität (für das Schülermodell) berücksichtigt werden.

Die Forscher ermutigen dazu, ihre detaillierte Arbeit und das bereitgestellte Code- und Datenmaterial für zukünftige Forschungen zu nutzen.</sample>
    <sample id="128">Die sprechenden Parteien sind Akshatha Aroda, Martin Pömsl, Kaheer Suleman, Adam Trischler, Alexandra Olteanu und Jackie CK Cheung. Das Thema ist der KITMUS-Test, der die Wissensintegration aus verschiedenen Quellen bewertet.

Die NLU-Modelle stützen sich auf mehrere Wissensquellen, nämlich Prä-Training und Inferenz-Zeit-Wissen. Damit NLU-Modelle erfolgreich sind, müssen sie sowohl Prä-Trainings- als auch Inferenz-Wissen integrieren und nutzen können. Im KITMUS-Testsuite werden Kernreferenzauflösungsaufgaben verwendet, um die Fähigkeit der Modelle zu prüfen, sich auf Prä-Trainings- und Inferenz-Wissen zu stützen. Das Dataset wurde mit menschlichen Studienteilnehmern und Kernreferenzauflösungsmodellen evaluiert.

Für die Lösung einer bestimmten Pronomenauflösung sind zwei Arten von Informationen erforderlich: 1) Entitätsspezifisches Wissen, wie "Servin ist Richter", und 2) Hintergrundwissen, wie "Richter entscheiden Fälle vor Gericht". Hintergrundwissen wird während des Pre-Trainings von großen Sprachmodellen erlernt, während entitätsspezifisches Wissen typischerweise zur Inferenzzeit beobachtet wird. Die Verfügbarkeit beider Informationstypen wird variiert, so dass sie entweder in einer oder in mehreren Quellen zu finden sein können.

Es wurden drei KITMUS-Settings definiert: (a) Hintergrund-Pre-Training, bei dem das Hintergrundwissen zur Pre-Trainingszeit verfügbar ist; (b) Hintergrund-Beides, bei dem das Hintergrundwissen sowohl zur Pre-Trainings- als auch zur Inferenzzeit verfügbar ist; und (c) Hintergrund-Inferenz, bei dem beide Wissensarten nur zur Inferenzzeit verfügbar sind.

Ohne aufgabenspezifisches Training schneiden beide Modelle im KITMUS-Test nicht gut ab. Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die besten Modelle Schwierigkeiten haben, Hintergrundwissen zuverlässig zu integrieren, das nur zur Inferenzzeit vorhanden ist.</sample>
    <sample id="129">Ein "weiblicher Krieger" ist ein Beispiel für eine markierte Gruppe.</sample>
    <sample id="130">LSTM-CRF-Modelle generalisieren nicht gut.</sample>
    <sample id="131">Der englische Inhalt erwähnt nur "Cleanly labeled test data" und keine spezifischen Namen für die Testdatensätze.</sample>
    <sample id="132">An der Arbeit sind 6 Autoren beteiligt.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, einschließlich multimodaler Instruktionen und Sprachmodellen.</sample>
    <sample id="135">The speakers, James and Sarah, introduce their new dimensional approach to evaluating conversational AI called ABC-Eval. The usual method for evaluating dialogue models involves having human judges select which of two conversations is better, or to rate conversations on a Likert scale, which is subjective. In contrast, ABC-Eval attempts to reduce this subjectivity by explicitly annotating whether or not each model response expresses certain behaviors. ABC-Eval is capable of measuring the rates at which chat models commit various thematic errors, such as: ignoring partner or saying something irrelevant (coherence); contradicting itself or its partner (consistency); hallucinating incorrect facts or violating common-sense knowledge (knowledge); and when the model succeeds or fails to show empathy (emotional understanding).

To determine what kind of evaluation is most effective, they selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, they also evaluated these conversations using three existing methods: Likert ratings on the turn level, Likert ratings on the dialogue level, and dialogue-level pairwise comparisons. From their analyses, they found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly labeled conversations.

In addition, ABC-Eval labels are more predictive of overall conversation quality compared to metrics produced by existing methods, as shown by their simple linear regression analysis. For example, measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality respectively, while the average Likert consistency scores explain only 4% or less. Finally, they checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. The combination of all ABC-Eval metrics explains over 25% of conversation quality and most of them result in losing a decent amount of information about the quality when removed one at a time. The combination of all turn-level Likert metrics, on the other hand, explains far less of the quality, and fewer of these metrics carry unique information.

These reliable, informative, and distinct ABC-Eval metrics enable them to evaluate conversational AI with a higher resolution than previous methods are able to achieve. The results of their experiment show that several challenges still remain and have been precisely quantified. For example, the bots they tested have common-sense violations in around 20% of their responses, they produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time. They hope ABC-Eval can be leveraged by others in the field as a meaningful step and look forward to seeing how conversational AI will advance in the coming months and years.</sample>
    <sample id="136">Jasivan stellt FERMAT vor, eine alternative Bewertungsmethode zur Genauigkeit numerischer Argumentationen.
Die Präsentation beginnt mit der Motivation, dass viele Aufgaben, wie z.B. die Faktenprüfung, numerische Argumentationen erfordern. Ein Beispiel hierfür ist die Aussage "Chris Brown wurde berühmt, als er 16 Jahre alt war", die anhand einer Tabelle mit seiner Geburts- und Aktivitätszeit überprüft werden muss.
Es wird erläutert, dass die Leistung von Sprachmodellen von ihrer Größe abhängt, wobei größere Modelle tendenziell besser abschneiden. Die Forschung konzentriert sich auf zugänglichere Modelle mit 3 Milliarden Parametern, da diese oft schlechter in numerischen Argumentationen sind.
Aktuelle Benchmarks liefern oft nur eine einzige Genauigkeitsbewertung, die keine detaillierten Einblicke in die mathematischen Stärken und Schwächen der Modelle gibt.
FERMAT wurde entwickelt, um die Modelle anhand der folgenden Kriterien zu bewerten: Zahlenverständnis, mathematische Operationen und Trainingsabhängigkeit.
Beim Zahlenverständnis werden Zahlen in verschiedenen Formaten (z.B. Dezimal, Wortform, vertauscht) und mit unterschiedlichen Größen (kleine und große ganze Zahlen, Dezimalzahlen) getestet. Bei den mathematischen Operationen wird die Fähigkeit der Modelle bewertet, Ein- und Zwei-Schritt-Operationen durchzuführen.
Die Forscher stellen fest, dass viele Modelle bei der Null-Schuss-Bewertung schlecht abschneiden, insbesondere bei unkonventionellen Darstellungen oder größeren Zahlen. Eine Feinabstimmung mit generierten Daten, die Zahlen in verschiedenen Formaten und über verschiedene Bereiche hinweg abdecken, verbessert die Leistung erheblich.
Die Trainingsabhängigkeit wird untersucht, indem Modelle auf genaue Übereinstimmungen von Ausdrücken und auf verschiedene Kombinationen von Zahlen und Operationen getestet werden. Es zeigt sich, dass selbst bei exakten Übereinstimmungen die Genauigkeit unter 50 % liegt, was auf eine mangelnde Speicherung hinweist.
Schließlich wird der Einfluss der Trainingstemplate-Diversifizierung hervorgehoben. Es wird gezeigt, dass das Hinzufügen von sprachlicher und mathematischer Diversität die Leistung des Modells deutlich verbessert.

Insgesamt betont Jasivan die Notwendigkeit flexiblerer Bewertungsmethoden und die Wichtigkeit der Diversität von Sprache und Mathematik beim Training von Sprachmodellen, um deren numerische Argumentationsfähigkeiten zu verbessern.</sample>
    <sample id="137">Das Video stellt "Tell2Design" vor, einen Datensatz für die sprachgesteuerte Grundrissgenerierung, der auf der ACL 2023 veröffentlicht wurde. Die Präsentation beginnt mit einer Einführung in die textbedingte generative KI, die hochauflösende Bilder erzeugt und sich auf das Verstehen hochrangiger visueller Konzepte aus Satzbeschreibungen konzentriert. Die daraus resultierenden Bilder sind für ihre realistische und kreative Qualität bekannt.

Das Video geht dann auf die Interaktion zwischen Benutzern und Designern in Designprozessen ein und wie Benutzer ihre Designanforderungen in Text festlegen können, sodass Architekten die Grundrissgenerierung durchführen können. Um diese Lücke zu schließen, wurde Tell2Design entwickelt, um es Benutzern ohne Fachkenntnisse zu ermöglichen, Grundrisse durch "Erzählen" von Anweisungen zu entwerfen. Die Aufgabe wird als Generierung von 2D-Grundrissen aus Textbeschreibungen definiert, die Semantik, Geometrie und Topologie umfassen.

Der Datensatz umfasst über 5.000 von Menschen kommentierte Anweisungen und über 75.000 künstlich generierte Anweisungen aus vordefinierten Vorlagen. Die wichtigsten Herausforderungen sind die Generierung von Designs unter strengen Einschränkungen, der Umgang mit unscharfen und verschränkten Informationen und die Verarbeitung verrauschter menschlicher Anweisungen. Die Forscher schlugen einen auf einem Transformer basierenden Encoder-Decoder vor, der mit einem vortrainierten Sprachmodell (T5) initialisiert wurde, und zeigten eine signifikante Verbesserung der Leistung, insbesondere beim Aufwärmen mit künstlichen Anweisungen vor der Feinabstimmung mit menschlichen Anweisungen. Dies deutet auf einen gegenseitigen Nutzen zwischen künstlichen und menschlichen Anweisungen hin.</sample>
    <sample id="138">Nach Ansicht der Autoren ist die Fähigkeit von NLU-Modellen, Wissen aus mehreren Quellen zu integrieren und zu verwenden (d. h. Vor-Training-Wissen und Inferenz-Wissen), ein zu wenig erforschtes Gebiet im Bereich der NLU.</sample>
    <sample id="139">Die Referenten sind Ying Shen und Zhiyang Xu.</sample>
    <sample id="140">Ja, Coscript hat eine Qualitätskontrolle durchlaufen, bei der Crowdsourcer fehlerhafte Proben korrigierten.</sample>
    <sample id="141">Bestehende Ressourcen für kontextbasierte Übersetzungen unterstützen nur eine begrenzte Anzahl von Phänomenen und Sprachen, da sie auf Domänenwissen und menschlicher Kuration beruhen.</sample>
    <sample id="142">00:00
Hallo. Ich werde über unsere Arbeit zur Auflösung indirekter Verweisungen für die Entitätsauswahl sprechen, bei der wir das AltEntities Corpus vorstellen. Mein Name ist Javad Hosseini, und dies ist eine gemeinsame Arbeit mit Filip Radlinski, Silvia Pareti und Annie Louis.
00:18
Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen. Betrachten Sie diese alternative Frage: „Meinst du ‚Easy on Me‘ oder ‚I Gotta Feeling‘?“ Hier möchte ein Benutzer zwischen einem dieser beiden Songs wählen. Am offensichtlichsten ist es, eine direkte Referenz zu verwenden, zum Beispiel, indem man den Namen des Songs, „Easy on Me“, oder seine Position, „The First One“, sagt. Aber manchmal ist eine indirekte Referenz angebrachter, um eine natürlichere Konversation zu führen. Dies könnte passieren, wenn der Benutzer sich den Namen des Songs nicht merken kann. Oder die Aussprachen sind zu ähnlich und schwer zu unterscheiden. Oder wenn der Benutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Referenzen, zum Beispiel „The Newer One“ oder „The Song That's Not Energetic“.
01:14
Dies ist ein wichtiges Problem in Konversationssystemen und auch für das Benchmarking des Entitätsverständnisses von LLMs. Uns ist kein großes öffentliches Dataset für die Aufgabe bekannt, daher haben wir eines mit Crowdanotation gesammelt. Unser Dataset deckt drei verschiedene Domänen ab: Musik, Bücher und Rezepte.
01:39
Unsere Datensammlungsmethodik betont Informalität mithilfe eines Cartoon-Vervollständigungs-Setups. Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: „Erinnerst du dich an das Lied, das wir gestern gehört haben?“ Und damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: „Meinst du ‚Easy on Me‘ oder ‚I Gotta Feeling‘?“ Das ist die alternative Frage. Und in der dritten Sprechblase verwendet Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen. Zum Beispiel „The Newer One“. Wir stellen die erste und zweite Sprechblase automatisch zur Verfügung, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Prompts pro Domäne ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert:
02:42
Wir verwenden immer eine einfache Vorlage: „Meinst du A oder B?“, wobei A und B von Wikipedia gesampelt werden. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Wenn wir in der Liste nach oben gehen, werden die Entitäten ähnlicher und es ist normalerweise schwieriger, die Disambiguierung vorzunehmen. Die erste ist zufällig und gleichmäßig. Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen „The Return“. Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben, und schließlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben, zum Beispiel das gleiche Genre oder den gleichen Künstler für einen Song.
03:32
Wenn wir den Annotatoren diese alternative Frage zeigen, kennen sie den Namen dieser Entitäten, aber sie kennen die Entitäten nicht unbedingt. Was wir also tun, ist, dass wir einige Hintergrundinformationen über die beiden Entitäten zeigen. Für Songs zeigen wir einfach einen Google-Suchlink zu jedem Song. Und dann bitten wir die Annotatoren, zumindest einige Teile jedes Songs anzuhören und etwas über jeden Song zu lesen.
04:02
Hier ist zum Beispiel das Google-Suchergebnis für den Song „Easy on Me“. Für die Domänen Rezepte und Bücher zeigen wir einige Hintergrundtexte von Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, ebenfalls von Wikipedia, damit die Annotatoren wissen, wie sie aussehen.
04:29
Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit drei bis fünf indirekten Verweisungen zu beschreiben. Zum Beispiel „The One With the Piano Music“. Hier sind einige Beispiele aus unserem Dataset. Zum Beispiel „The One Without Words“, „Not the One With the Twelve-Year-Old Boy“, oder „The Fictional One“, oder „Comes From Azerbaijan“ und so weiter.
04:58
Das AltEntities Corpus enthält ~6.000 alternative Fragen in den drei Domänen und ~42.000 indirekte Verweisungen. Die Ergebnisse mit dem T5-XL-Modell (Genauigkeit) sind unten zusammengefasst. Wenn das Sprachmodell Zugriff auf genau dieselben Hintergrundinformationen wie die Annotatoren hat, dann ist die Genauigkeit wirklich hoch, sie liegt bei etwa 92 bis 95%. Aber das ist nicht realistisch. Wenn das Sprachmodell Zugriff auf teilweise überlappende Hintergrundinformationen hat, dann liegt die Genauigkeit zwischen 82 und 87%, was realistischer ist, zum Beispiel wenn das Sprachmodell die Hintergrundinformationen abruft. Wenn das Sprachmodell (T5-XL) nur Zugriff auf die Entitätsnamen hat, dann beträgt die Genauigkeit nur 60%. Es gibt also noch viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle domänenübergreifend generalisierbar sind. Hier ist ein Link zu unserem Dataset. Vielen Dank.</sample>
    <sample id="143">EDAtt wird mit bestehenden SimulST-Richtlinien verglichen, darunter „Wait-k“ und „Local Agreement“ (LA), die auf Offline-Modelle angewendet werden. Es wird auch mit CAAT verglichen, einer speziell für SimulST entwickelten State-of-the-Art-Architektur.</sample>
    <sample id="144">Die Autoren sind an der Avignon Universität, der Nantes Universität, der Clinique des données und Zenidoc angesiedelt.</sample>
    <sample id="145">Der/die Referent*in heißt Jenny.</sample>
    <sample id="146">In der Präsentation wird die Problematik des Auslassens bei der Zusammenfassung von Dialogen untersucht. Der Redner erklärt, dass die Zusammenfassung von Dialogen in vielen Bereichen wichtig ist, zum Beispiel im Kundenservice, bei medizinischen Beratungen und in Besprechungen. Er weist darauf hin, dass trotz der Fortschritte bei großen Sprachmodellen Auslassungen ein großes Problem darstellen, da sie zu unvollständigen Zusammenfassungen führen. 

Um das Problem der Auslassungen besser zu verstehen und zu lösen, wird eine neue Aufgabe eingeführt: die Auslassungserkennung. Diese Aufgabe konzentriert sich auf die Erkennung von Auslassungen auf Äußerungsebene in den generierten Zusammenfassungen.

Da es keine geeigneten Datensätze gibt, wurde ein neuer Datensatz namens OLDS erstellt. Dieser Datensatz basiert auf fünf bestehenden Benchmarks, die fünf verschiedene Domänen abdecken. Für jeden Dialog wurden 10 verschiedene Kandidatenzusammenfassungen generiert, wobei verschiedene Modelle und Dekodierungsstrategien verwendet wurden. Die Auslassungsetiketten wurden durch automatische Erkennung und menschliche Bewertung erstellt.

Anhand des OLDS-Datensatzes wurden drei Frameworks als Baselines untersucht: die paarweise Klassifizierung, die Sequenzkennzeichnung und das Zeigernetzwerk. Die Ergebnisse zeigen, dass die Erkennung von Auslassungen eine sehr anspruchsvolle Aufgabe ist, was auf die unausgewogene Verteilung der Labels im Datensatz zurückzuführen ist.

Es wurde auch gezeigt, dass die Qualität der Zusammenfassungen erheblich verbessert werden kann, wenn Auslassungen zur Verfeinerung herangezogen werden. Es wurde eine Nachbearbeitungsmethode verwendet, die die Kandidatenzusammenfassung mit ausgelassenem Inhalt als Eingabe verkettet. Die Ergebnisse zeigen, dass die Berücksichtigung von Auslassungen vielversprechend ist, um die Qualität von Dialogzusammenfassungen zu verbessern.</sample>
    <sample id="147">An der Arbeit sind 3 Autoren beteiligt.</sample>
    <sample id="148">00:00
Hallo, ich bin Sara Papi von der Universität Trient und der Fondazione Bruno Kessler,
00:07
und ich werde kurz das Paper "Attention as a Guide for Simultaneous Speech Translation" vorstellen,
00:13
das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist.
00:17
Was ist simultane Sprachübersetzung?
00:20
Simultane Sprachübersetzung (SimulST) ist der Prozess, gesprochene Sprache in Echtzeit in Text in einer anderen Sprache zu übersetzen, um eine sprachübergreifende Kommunikation zu ermöglichen.
00:31
Was sind die Probleme der aktuellen SimulST-Modelle?
00:35
Es werden in der Regel spezifische Architekturen trainiert, die zusätzliche Module zur Optimierung einführen,
00:41
lange und komplizierte Trainingsverfahren (z. B. verschiedene Optimierungsziele)
00:49
und das Trainieren und Warten mehrerer Modelle, um verschiedene Latenzregime zu erreichen (z. B. 1s, 2s usw.).
01:04
Was ist unsere Lösung?
01:07
Erstens, bereits vorhandene Offline-ST-Modelle zu verwenden, ohne sie neu zu trainieren oder eine spezifische Architektur für SimulST zu verwenden.
01:15
Zweitens, nur ein Modell für jedes Latenzregime zu verwenden und die Latenz über spezifische Parameter zu steuern.
01:22
Drittens, das bereits erworbene Wissen des Modells durch den Aufmerksamkeitsmechanismus zwischen Audioeingabe und Textausgabe zu nutzen.
01:31
Ein Beispiel sehen Sie rechts.
01:36
Unsere Lösung: EDAtt
01:39
Encoder-Decoder Attention.
01:41
Es ist eine Strategie, bei der wir entscheiden, ob eine partielle Übersetzung ausgegeben werden soll oder nicht, basierend darauf, wohin die Aufmerksamkeit zeigt:
01:50
Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist (d. h. ihre Summe liegt unter einem Schwellenwert α) in Richtung der letzten λ Sprach-Frames, was bedeutet, dass die empfangenen Informationen stabil genug sind.
02:04
Wenn wir beispielsweise einen Sprach-Chunk erhalten, der "I am going to talk about..." enthält,
02:12
und unser Modell die Übersetzung auf Deutsch vorhersagt,
02:16
und wir uns die Kreuzaufmerksamkeitsgewichte ansehen,
02:20
werden wir sehen, dass die ersten beiden Wörter auf die zuerst empfangenen Sprach-Frames zeigen, während
02:29
das letzte Wort auf die zuletzt empfangenen Sprach-Frames als λ Sprach-Frames zeigt.
02:35
Das bedeutet, dass die ersten beiden Wörter ausgegeben werden.
02:40
Während die Summe der Kreuzaufmerksamkeit über einem bestimmten Schwellenwert α liegt,
02:47
werden wir das letzte Wort nicht ausgeben und warten auf einen weiteren Sprach-Chunk.
02:50
Wenn wir fortfahren und einen weiteren Sprach-Chunk erhalten,
02:56
und unser Modell drei weitere Wörter vorhersagt, und wir uns die Kreuzaufmerksamkeitsgewichte ansehen,
03:01
werden wir sehen, dass kein Wort auf die letzten Lambda-Sprach-Frames zeigt.
03:07
Das bedeutet, dass diese drei Wörter ausgegeben werden.
03:11
Wenn wir uns die Hauptergebnisse von EDAtt ansehen, werden wir die Ergebnisse der simultanen Sprachübersetzung in Diagrammen darstellen, in denen wir
03:22
Bleu auf der einen Seite haben, das die Übersetzungsqualität misst,
03:26
und die durchschnittliche Latenz,
03:28
das ist das Latenzmaß, und wir betrachten auch die computergestützte, durchschnittliche Verzögerung, die die Berechnungszeit der Modelle berücksichtigt, um die Ausgabe vorherzusagen.
03:44
Wir möchten, dass unsere Kurven auf diesem Diagramm so hoch wie möglich sind.
03:52
Wir möchten aber auch, dass sie nach links verschoben werden.
03:56
Und wir vergleichen mit populären Strategien, die auch auf Offline-Modelle angewendet werden, das sind die Wait-K-Strategie und das Local Agreement.
04:05
Und wir vergleichen auch mit der State-of-the-Art-Architektur, die speziell für die simultane Sprachübersetzung entwickelt wurde.
04:12
Dies sind alle Ergebnisse der simultanen Sprachübersetzungsstrategie für Deutsch.
04:20
Und wir sehen, dass EDAtt alle Strategien übertrifft, die auf Offline-Modelle angewendet werden, da die Kurven
04:29
nach links verschoben sind.
04:32
Und wir sehen auch, dass, wenn wir die tatsächlich verstrichene Zeit oder die Rechenzeit berücksichtigen, EDAtt die schnellste Strategie ist.
04:40
Möchten Sie mehr entdecken? Lesen Sie unser Paper, um weitere Ergebnisse zu entdecken!
04:47
Wir haben den Code und die Modelle auch Open Source veröffentlicht,
04:52
sowie die simultane Ausgabe, um die Reproduzierbarkeit unserer Arbeit zu erleichtern.
04:56
Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja, der Datensatz ist öffentlich zugänglich unter: https://github.com/ShuhengL/acl2023_conllpp</sample>
    <sample id="150">Archiki Prasad stellt MeetingQA vor, einen Datensatz für extraktive Frage-Antwort-Systeme, der auf Meeting-Transkripten basiert. Millionen von Meetings finden täglich statt, was zu einer großen Menge an Meeting-Transkripten führt. Bisherige Arbeiten in diesem Bereich konzentrieren sich hauptsächlich auf die Zusammenfassung und die Extraktion von Aktionspunkten. Es wurde jedoch festgestellt, dass Meeting-Diskussionen auch einen signifikanten QA-Anteil aufweisen.

MeetingQA behandelt diese Lücke durch die Einführung eines neuen Datensatzes, der auf Fragen basiert, die von Teilnehmern in einem Meeting gestellt werden, und den entsprechenden Antwortsätzen. Fragen, die von Teilnehmern gestellt werden, sind tendenziell länger, offener und auf Diskussionen ausgerichtet. Die Antworten können mehrere Sprecher und nicht zusammenhängende Sätze umfassen, und es können auch rhetorische Fragen vorkommen.

Die Datenerfassung für MeetingQA begann mit öffentlich zugänglichen Transkripten aus dem AMI-Korpus, der etwa 100 Stunden manuell transkribierter Mehrparteien-Meetings umfasst. Die Fragen werden basierend auf Interpunktion und Fragenlänge ausgewählt. Anschließend werden Kommentatoren angeworben, um Sätze im Antwortbereich zu kennzeichnen, wobei eine hohe Inter-Kommentator-Übereinstimmung von Krippendorffs α = 0,73 erreicht wird.

Der Datensatz enthält insgesamt 7.735 Fragen aus 166 verschiedenen Meetings, die in Trainings-, Entwicklungs- und Testsets aufgeteilt sind. 30 % der Fragen sind unbeantwortbar, 40 % der Antworten sind mehrspanig (nicht aufeinanderfolgende Sätze) und 48 % der Antworten stammen von mehreren Sprechern. Die meisten Fragen (54,4 %) sind Ja/Nein-Fragen, die detaillierte Antworten und Meinungen erfordern. 20 % der Fragen sind rhetorisch, und 70 % der Mehrsprecherantworten enthalten Meinungsverschiedenheiten. Die durchschnittliche Länge eines Transkripts beträgt 5,9k Wörter, einer Frage 12 Wörter und einer Antwort 35 Wörter. Die menschliche Leistung auf dem Testset erreicht einen F1-Wert von 84,6.

Die vorgestellten Methoden umfassen:
- Kontextextraktion für Modelle mit kurzem Kontext, um relevante Segmente des Meeting-Transkripts als Kontext abzurufen.
- Modelle mit einem einzigen Antwortbereich ("Single-span models"), die den ersten bis letzten relevanten Satz im Antwortbereich vorhersagen.
- Modelle mit mehreren Antwortbereichen ("Multi-span models"), die eine Token-Klassifizierungsaufgabe durchführen (Markierung von Token als "innerhalb des Antwortbereichs" oder "außerhalb des Antwortbereichs").
- Augmentierung mit Silberdaten ("Silver data augmentation") durch automatische Annotation von Antwortbereichen für Interviewfragen aus dem MediaSum-Datensatz.

Die Ergebnisse zeigen, dass es eine Lücke von über 25 F1-Punkten zwischen den feingestimmten Modellen und der menschlichen Leistung gibt. Modelle mit kurzem Kontext (wie RoBERTa-base) übertreffen Modelle mit langem Kontext (wie Longformer-base) leicht. Mehrspanige Modelle zeigen eine leicht geringere oder vergleichbare Leistung wie einspanige Modelle. Im Zero-Shot-Setting beträgt die Lücke zur menschlichen Leistung fast 50 F1-Punkte. Die Augmentierung mit Silberdaten ist effektiv, und größere, instruktionsabgestimmte Modelle wie FLAN-T5 XL liefern eine vergleichbare Leistung.

Fehleranalysen zeigen, dass die Modelle Schwierigkeiten haben, rhetorische Fragen zu identifizieren, insbesondere im Zero-Shot-Setting. Einspanige Vorhersagen enthalten mehr irrelevante Sätze. Die Modelle haben auch Schwierigkeiten zu identifizieren, welche Sprecher eine Frage beantworten, was sich im Zero-Shot-Setting verschlechtert.

Zusammenfassend ist MeetingQA ein interessanter Datensatz, der auf offenen und diskussionsintensiven Fragen aus Meetings basiert, und er stellt eine Herausforderung für bestehende QA-Modelle dar, die in der Leistung erheblich hinter der menschlichen Leistung zurückbleiben.</sample>
    <sample id="151">00:00:00 Hello everyone. My name is Ying, and my colleague Ziyang and I will be presenting our research on multi-instruct, improving multi-modal zero-shot learning while instruction tuning.
00:11:43 So, with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform unseen tasks in a zero-shot manner by following natural instructions.
00:35:54 However, most previous works on instruction tuning focused on improving the zero-shot performance on language-only tasks, while computer vision and multimodal tasks have been left out.
00:48:07 Therefore, in this work, we want to investigate whether instruction tuning on multimodal pre-trained models can actually improve generalization to unseen multimodal tasks.
01:00:23 Additionally, at the time of our research, we discovered a considerable discrepancy in availability of instruction datasets between NLP and multimodal. There exist more than 1,600 language-only instruction tasks. However, there is no large-scale, publicly available multimodal instruction task. Therefore, this motivated us to build a multimodal instruction tuning dataset.
01:27:32 Here, we present multi-instruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks, covering 10 broad categories. These tasks are derived from 21 existing open-source datasets, and each task is equipped with five expert-written instructions.
01:50:33 For investigating multimodal instruction tuning on our proposed dataset, we take OFA, a unified multimodal pre-trained model as our base model. OFA used a unified vocabulary for language, image tokens and the coordinate of a bounding box.
02:07:05 Here, we show some example instances from our multi-instruct dataset. To unify the processing of various input and output data type, we follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format, in which the input text, images, instruction and bounding boxes are represented in the same token space.
02:31:00 Okay, now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from nine groups for training, and we sample 10,000 instances per task. For testing, we reserve the entire common sense reasoning group for testing, and we select additional five tasks from VQA and miscellaneous groups. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instruction dataset as unseen tasks for NLP.
03:03:00 Uh, so we use a pre-trained OFA large model as the base model. Uh, during training, we mix all the instances for all the tasks. Uh, each instance is randomly combined with one of its five instruction templates. Uh, so during test, for each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and max performance and the standard deviation of the performance across all five experiments.
03:33:00 Uh, if the task is a multi-modal classification task, we report accuracy. If it's a multi-modal generation task, we report rouge-L. Uh, for NLP tasks, we report rouge-L as well.
03:45:62 Uh, we also introduced an additional evaluation metric called sensitivity. So this measures the model's ability to consistently produce the same outputs for the same task, regardless of slight variation in the wording of the instruction.
04:01:45 Here is our main result. As we can see, uh, instruction tuning can significantly improve OFA's performance on unseen multi-modal tasks. Also, transfer learning from natural instruction dataset can benefit instruction tuning.
04:20:00 Uh, here we can see as the amount of task increased, the model achieved better performance and in the meantime, uh, lower sensitivity.
04:30:57 Uh, so we also did one experiment. We use one instruction versus five instruction. As we can see, uh, using more instruction can improve the model's overall performance and reduce its sensitivity a lot.
04:44:09 Um, so this shows the effect of different fine-tuning strategy on the model sensitivity. Uh, as we can see, by transfer learning from natural instruction dataset, the model can, uh, achieve much better sensitivity compared to the original OFA model.
05:02:00 Uh, we also can see transfer learning from natural instruction dataset, uh, can help OFA to achieve much better performance on the natural instruct, uh, dataset.
05:14:00 So, overall, we have proposed the first large-scale multi-modal instruction tuning dataset. It contains 62 multi-modal tasks from 10 broad categories. Significantly improve the zero-shot capability of OFA via instruction tuning. And we explore different transfer learning techniques and show their benefits. Uh, we design a new metric called sensitivity.
05:30:00 Uh, so one more thing, we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon. Uh, this is a QR code for our, uh, data and model. Thank you.</sample>
    <sample id="152">Frederick Riemenschneider stellte in seinem Vortrag über "Exploring Large Language Models for Classical Philology" die Schnittstelle zwischen NLP und klassischer Philologie vor. Er führte wertvolle Ressourcen für Altgriechisch und Latein ein und untersuchte die Auswirkungen und Herausforderungen der Mehrsprachigkeit in diesen Modellen. Die vorgestellten BERT-Modelle, obwohl sie fortschrittlich sind, weisen einige Einschränkungen auf. Sie sind hauptsächlich Encoder-only-Modelle und oft einsprachig, was für klassische Philologen, die typischerweise mehrsprachige Ansätze verwenden, nicht ideal ist.

Um diese Lücke zu schließen, wurden neue Sprachmodelle entwickelt, die speziell für die klassische Philologie konzipiert wurden. Die Ziele dieses Projekts umfassten die Vergleichbarkeit bestehender Modelle, die Weiterentwicklung des Standes der Technik, die Erforschung verschiedener Modellarchitekturen und die Einführung mehrsprachiger Modelle. Zwei einsprachige Modelle, GrεBERTa und GrεTa, wurden für Altgriechisch vortrainiert. GrεBERTa ist ein RoBERTa-Modell, während GrεTa ein Encoder-Decoder-Modell auf T5-Basis ist, das sowohl zum Verstehen als auch zum Generieren von altgriechischen Texten fähig ist. Zusätzlich wurden die mehrsprachigen Modelle PhilBERTa und PhilTa entwickelt, die auf Altgriechisch, Latein und Englisch vortrainiert wurden.

Für das Vortraining wurden verschiedene Datensätze genutzt. Für Altgriechisch wurden Open Greek &amp; Latin, griechische mittelalterliche Texte und Patrologia Graeca verwendet. Eine besondere Herausforderung war die Nutzung des Internet Archive, da viele OCR-Transkriptionen von griechischen Texten in nicht lesbaren Formaten vorlagen. Dieses Problem wurde durch die Suche nach falsch transkribierten griechischen Stoppwörtern und anschließendes erneutes Scannen mit korrekten OCR-Einstellungen gelöst. Für mehrsprachige Modelle wurden Corpus Corporum für Latein und englische Texte, die mit der Antike in Verbindung stehen, verwendet.

Die Modelle wurden anhand von drei Hauptaufgaben bewertet: Part-of-Speech-Tagging, Abhängigkeits-Parsing und Lemmatisierung. Die Ergebnisse zeigten, dass die neuen Modelle, insbesondere die Encoder-Decoder-Modelle wie GrεTa, den aktuellen Stand der Technik deutlich übertreffen. Besonders bei der Lemmatisierung wurden signifikante Fortschritte erzielt.

Die Ergebnisse zur semantischen und Weltkenntnis zeigten, dass die neuen Modelle, sowohl die mehrsprachigen als auch die einsprachigen, frühere Modelle übertreffen. Die mehrsprachigen Modelle zeigten jedoch keine signifikanten Leistungsunterschiede im Vergleich zu den einsprachigen.</sample>
    <sample id="153">In this presentation, Ninareh Mehrabi discusses the problem of ambiguities in text-to-image generative models. She illustrates this with examples of prompts that can have multiple interpretations, leading to varying image generations. For instance, the prompt "An elephant and a bird flying" could result in either a flying elephant or a flying bird, while "The girl enters the room with flowers" leaves ambiguity about who is carrying the flowers.

To address this, Mehrabi introduces Text-to-Image Disambiguation (TIED), a pipeline designed to mitigate such ambiguities and evaluate the faithfulness of generated images. The pipeline begins with an initial ambiguous prompt from a curated benchmark dataset (TAB), which covers various ambiguity types. This prompt then undergoes a disambiguation process, either by generating clarifying questions (QA-TIED) for user input or by suggesting different visual setups (VS-TIED). The user's response helps create a disambiguated prompt.

Once disambiguated, these prompts are fed into a text-to-image model (e.g., DALL-E) to generate images. The faithfulness of these generated images to the user's intended meaning is then evaluated using both automatic methods, such as a Visual Question Answering (VQA) model, and human evaluations. The VQA model checks if the generated image aligns with the user's intent by answering a clarifying question about the image.

Key findings include a disparity in resolving ambiguities across different types and an overall positive effect of disambiguation on faithful image generation. Furthermore, the automatic evaluation framework shows reasonable agreement with human evaluations, suggesting its reliability for assessing text-to-image models.</sample>
    <sample id="154">Die Autoren gehören der Universität Trient an.</sample>
    <sample id="155">Der/die Referent*in heißt Mohammad Javad Hosseini.</sample>
    <sample id="157">Diese Präsentation konzentriert sich auf die Dialogzusammenfassung unter Verwendung eines statisch-dynamischen Struktur-Fusionsgraphen, der von der Shandong-Universität entwickelt wurde. Die Dialogzusammenfassung zielt darauf ab, die wichtigsten Informationen aus einem Dialogkontext in einer prägnanten Zusammenfassung zu extrahieren. Dies ist eine der anspruchsvollsten und interessantesten Aufgaben im Forschungsfeld der Textzusammenfassung. Sie kann Menschen helfen, die Höhepunkte eines semistrukturierten und multiparteilichen Dialogs schnell zu erfassen, ohne den komplexen Dialogkontext erneut lesen zu müssen.

Bestehende Methoden zur Dialogzusammenfassung konzentrieren sich hauptsächlich auf die Modellierung von Dialogen mit vorab berechneten statischen Graphenstrukturen unter Verwendung externer linguistischer Werkzeuge wie Diskurs-Parsing und Dialogzustandsverfolgung. Diese Methoden sind jedoch in ihrer Zuverlässigkeit auf externe linguistische Werkzeuge angewiesen, die möglicherweise keine genauen Ergebnisse liefern und zu Fehlerverbreitung führen. Zweitens ist die Konstruktion des statischen Graphen von der Repräsentationslernphase des Graphen entkoppelt, und ein solcher fester Graph kann sich nicht dynamisch an die nachfolgende Dialogzusammenfassungsaufgabe anpassen.

Um diese Probleme zu lösen, schlagen die Autoren den SDDS-Ansatz (Static-Dynamic graph-based Dialogue Summarization) vor. Dieser Ansatz verwendet einen Utterance Encoder, um Utterances in einem Dialogkontext in Vektorrepräsentationen zu kodieren. Anschließend wird ein statischer Graph konstruiert, der durch vier heuristische Dialogstrukturmodellierungsmethoden gebildet wird: Diskurs-Parsing-Graph, Keywords Co-occurrence Graph, Speaker Relation Graph und Utterance Position Graph. Danach wird ein dynamisches Graphenmodul eingeführt, das die semantischen Beziehungen zwischen Utterances basierend auf ihrer tiefen Vektorrepräsentation erfasst, ohne vorab berechnete oder heuristische Methoden zu verwenden. Das Modell kombiniert dann die statischen und dynamischen Graphen in einem Fusionsmodul, um die statische Dialogstruktur und die dynamisch erlernte Dialogstruktur in eine endgültige Zusammenfassung zu integrieren. Schließlich wird ein vortrainiertes Sprachmodell als Zusammenfassungsgenerator verwendet, um die Graphrepräsentation, die die Dialogstrukturinformationen erfasst, in den Generierungsprozess zu integrieren.</sample>
    <sample id="158">Die Kernreferenzauflösung ist entscheidend für das Verstehen von Langdokumenten, aber konventionelle Ansätze haben eine quadratische Komplexität in Bezug auf Rechenaufwand und Speicher, was sie ineffizient macht. Cache-basierte Methoden senken die Komplexität auf ein lineares Niveau, indem sie einen Cache fester Größe zur Speicherung von Entitätsrepräsentationen verwenden. Allerdings können bei Langdokumenten thematische Wechsel zu über das Dokument verstreuten Entitätserwähnungen führen, was bei Cache-Eviction-Strategien wie LRU (Least Recently Used) zu hohen Cache-Fehlerraten führt, da hochfrequente Entitäten, die global erwähnt werden, oft aus dem Cache entfernt werden.

Um dieses Problem zu lösen, wird ein Dual-Cache-Ansatz vorgeschlagen, der einen lokalen LRU-Cache für lokale Entitäten und einen globalen LFU-Cache (Least Frequently Used) für globale Entitäten verwendet. Der lokale Cache ist für kürzlich verwendete Entitäten optimiert, während der globale Cache hochfrequente Entitäten speichert. Wenn eine neue Erwähnung auftritt, wird sie zunächst klassifiziert, ob sie eine neue Entität darstellt oder zu einer bestehenden gehört. Bei Qualifizierung wird sie dem globalen Cache hinzugefügt, andernfalls dem lokalen Cache. Cache-Eviction wird ausgelöst, wenn ein Cache voll ist, wobei der LRU-Ansatz für den lokalen Cache und der LFU-Ansatz für den globalen Cache verwendet wird.

Experimente an öffentlichen Benchmarks wie LitBank und OntoNotes zeigen, dass der Dual-Cache-Ansatz die Single-Cache-Methoden übertrifft, sogar wenn diese unbegrenzten Speicher verwenden. Dies gilt insbesondere für große Dokumente wie ein 30.000 Wörter umfassendes Buch, bei dem der Dual-Cache die Leistung erheblich steigert und die Cache-Fehlerrate im Vergleich zu einem Single-Cache signifikant reduziert. Darüber hinaus zeigt der Dual-Cache das höchste Leistungs-Kosten-Verhältnis, was ihn zur kostengünstigsten Option für die Kernreferenzauflösung macht.</sample>
    <sample id="159">00:00
Hallo zusammen, ich bin Koustuv Sinha, und ich freue mich, euch zu unserem Vortrag unseres ACL 2023-Papiers begrüßen zu dürfen: "Language model acceptability judgements are not always robust to context". Dies ist eine gemeinsame Arbeit mit Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy und Adina Williams.
00:19
In dieser Arbeit greifen wir das Minimalpaar-Paradigma wieder auf. Das Minimalpaar-Paradigma bewertet Sprachmodelle anhand von Akzeptanzurteilen, die auch die Grammatikalität umfassen können, wie bei BLiMP und SyntaxGym, oder die Akzeptanz in Bezug auf Stereotypen, wie bei CrowS-Paaren. Bei diesem Minimalpaar-Paradigma ist die typische Art, Sprachmodelle zu bewerten, dass man einen akzeptablen Satz oder einen grammatikalischen Satz zeigt und dann einen unakzeptablen Satz oder einen ungrammatischen Satz zeigt. Die Hoffnung ist, dass das Modell dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuweist.
01:00
Die aktuelle MPP-Pipeline erlaubt es uns nicht, die Akzeptanz von Modellen bei längeren Sätzen zu bewerten. Heutzutage werden immer größere Sprachmodelle mit immer längeren Kontextfenstern entwickelt. Daher ist es entscheidend, dass wir die Akzeptanz der Modelle über das gesamte Kontextfenster hinweg bewerten. Das ist es, was wir hier versuchen zu tun. Wir versuchen, die MPP-Pipeline zu überarbeiten, indem wir das Modell bitten, die Akzeptanz bei längeren und längeren Sequenzen zu bewerten.
01:32
Das ist der Ansatz. Was wir tun ist, dass wir, um diese längeren Sequenzen zu simulieren, die Datensätze selbst wieder besuchen und dann Sätze neu erstellen, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir hier ein typisches Paar Grammatikalität aus dem BLiMP-Datensatz aus dem Adjunct Island-Fall ausgewählt. Und was wir tun, ist, um längere Sequenzen zu erstellen, die akzeptabel sind und die dieselbe Übereinstimmung der grammatikalischen Struktur haben, extrahieren wir grammatikalische Sätze von Adjunct Island und fügen sie dann als Präfix zu sowohl der akzeptablen als auch der unakzeptablen Abfrage hinzu.
02:18
Wir können dasselbe tun, indem wir unakzeptable Sätze aus derselben Übereinstimmung auswählen, und das könnte auch verwendet werden, um die Akzeptanz des Modells zu testen.
02:46
Und wir könnten dasselbe für den unakzeptablen Fall tun.
02:49
Schließlich können wir Sätze aus einem völlig unrelated Domain wie Wikipedia auswählen. Dies wird uns sagen, ob die Akzeptanzurteile des Modells tatsächlich von irgendeinem Kontext beeinflusst werden, d.h. ob der Kontext aus einem anderen Teildatensatz stammt oder ob er völlig irrelevant für den aktuellen Satz ist.
03:14
Wie funktioniert das Modell? Zuerst betrachten wir die Wikipedia-Sätze, die für das aktuelle Abfragepaar völlig irrelevant sind. Und dort stellen wir fest, dass die MPP-Urteile für beliebige Kontextlängen weitgehend robust sind. Wir erhöhen die Kontextlänge auf bis zu 1024, um die OPT- und GPT2-Modelle zu maximieren. Und wir sahen hier in der orangefarbenen gestrichelten Linie, dass die MPP-Urteile relativ stabil sind.
03:42
Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen wir Sätze aus akzeptablen und unakzeptablen Domänen aus demselben BLiMP- oder SyntaxGym-Datensatz aus. Und dort sehen wir, dass die MPP-Urteile entweder erheblich zunehmen oder erheblich abnehmen, wenn wir entweder akzeptable Präfixe oder unakzeptable Präfixe hinzufügen.
04:07
Wenn wir jedoch die Struktur anpassen, d.h. wenn wir die Sätze aus demselben Phänomen in BLiMP und SyntaxGym auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang des MPP-Urteils für das Modell, abhängig davon, ob das gewählte Präfix akzeptabel oder unakzeptabel ist. Dieser Effekt ist sehr groß, d.h. dieser Effekt nimmt über die gesamte Kontextlänge zu, und dies würde wahrscheinlich neuere Sprachmodelle mit großen Kontextfenstern betreffen.
04:41
Warum beeinflusst das übereinstimmende Präfix die Sprachmodellurteile so sehr? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, den Eingabesatz zu stören, indem wir versucht haben, die relevante Struktur beizubehalten, aber Rauschen in die Eingabe einzufügen. Und nach mehreren dieser Störungen stellen wir fest, dass keines dieser Geräusche das Modell tatsächlich dazu bringt, seinen Kurs zu ändern, in Bezug darauf, wie es uns die MPP-Urteilstrends zeigt.
05:11
Grundsätzlich stellen wir fest, dass die Modelle auf gestörte Sätze auf ähnliche Weise empfindlich reagieren. Das heißt, wenn wir die Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen, und wenn wir die Sätze im unakzeptablen Bereich stören, sehen wir einen Rückgang der MPP-Urteile auf ähnliche Weise.
05:32
Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische/semantische Merkmale reagieren, die über Sätze hinweg geteilt werden, und dass MPP-Bewertungen mit kurzen, einzelnen Satz-Eingaben das abstrakte Wissen von LMs nicht vollständig erfassen. Bitte lest unser Papier für weitere Details unserer Experimente. Vielen Dank fürs Zuhören.</sample>
    <sample id="160">Multiset von Token</sample>
    <sample id="161">In Coscript sind 55.000 Skripte enthalten.</sample>
    <sample id="163">Die beste automatische Ausrichtungsmethode, die für die Textvereinfachung in DEplain verwendet werden kann, ist MASAlign.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen ist, dass es den Engpass bei der Annotation von Daten mindert, da es keine manuelle Annotation erfordert.</sample>
    <sample id="165">In diesem Vortrag wird die Forschung zu "Abduktiver Common-Sense-Begründung durch die Nutzung sich gegenseitig ausschließender Erklärungen" vorgestellt.
Das Ziel der abduktiven Begründung ist es, eine plausible Erklärung zu finden, die die Informationslücke zwischen einem gegebenen Kontext und einem Ergebnis schließt. Das aktuelle Problem ist die aufwändige und subjektive Annotation von plausiblen Erklärungen, die in 60 % der Fälle zu Meinungsverschiedenheiten unter den Bearbeitern führen kann. 

Um diesem Problem zu begegnen, wird ein unüberwachter Ansatz namens LiPoR (Likelihood learning with Posterior Regularization) vorgeschlagen. LiPoR behandelt Erklärungen als latente Variablen und maximiert die Randwahrscheinlichkeit des Ergebnisses, gegeben den Kontext, indem alle möglichen Erklärungen marginalisiert werden. Dadurch wird keine Annotation von plausiblen Erklärungen benötigt.
Darüber hinaus wird ein Regularizer entwickelt, der die gegenseitige Exklusivität von Erklärungen berücksichtigt. Das bedeutet, wenn eine Erklärung plausibel ist, schließt dies automatisch andere Erklärungen aus. Formal ausgedrückt, maximiert der LiPoR-Objective die Wahrscheinlichkeit des Ergebnisses und fördert gleichzeitig, dass die Wahrscheinlichkeitsmasse einer Erklärung zu einer Untermenge von Erklärungen kollabiert. Dies geschieht durch Minimierung der Entropie, wenn diese einen bestimmten Schwellenwert überschreitet.

Die Ergebnisse auf dem AlphaNLI-Datensatz zeigen, dass LiPoR alle anderen unüberwachten Ansätze übertrifft, einschließlich starker Null-Schuss-GPT3-Modelle, um über 4 absolute Punkte bei der Genauigkeit.</sample>
    <sample id="166">In diesem Vortrag wird ein neuartiger Ansatz, das Neural Divide-and-Conquer Reasoning Framework (NDCR), für den Abruf von Bildern aus linguistisch komplexem Text vorgestellt. NDCR ist inspiriert von der Divide-and-Conquer-Strategie und der Dual-Process Theory und ist darauf ausgelegt, komplexe Bild-Text-Aufgaben zu bewältigen. Es zerlegt komplizierte Textbeschreibungen in einfachere Aussagen, die jeweils mit einem Bildausschnitt korrespondieren. Es integriert sowohl System 1 (analoge Schlussfolgerung) als auch System 2 (logische Schlussfolgerung) in einem mehrstufigen Prozess. System 1, ein visuellen-linguistischer Interaktor, erzeugt eine Übereinstimmungsbewertung zwischen Aussagen und Bildern. System 2, ein neuronal-symbolischer Reasoner, führt Negations- und Konjunktionsoperationen auf der Grundlage dieser Ergebnisse durch, um die endgültige Lösung zu erhalten. NDCR übertrifft bestehende visuell-sprachliche Modelle auf mehreren Datensätzen und bietet eine verbesserte Interpretierbarkeit. Zukünftige Forschungsarbeiten könnten das NDCR weiter verfeinern, um seine Kompositionsfähigkeit zu verbessern und es in größere Sprachmodelle zu integrieren.</sample>
    <sample id="167">Die Dokumente in DEplain-web wurden teilweise manuell und teilweise automatisch ausgerichtet. Insgesamt wurden 756 Dokumente in DEplain-web ausgerichtet, was zu 3450 Satzpaaren führte. 471 davon wurden manuell ausgerichtet und 285 automatisch.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde durch die Sammlung und Annotierung von Reuters-Nachrichten aus dem Jahr 2020 mit den CoNLL-2003-Annotationsrichtlinien erstellt.</sample>
    <sample id="169">In diesem Video präsentiert David Vilar Torres einen Überblick über seine Arbeit mit Google Translate, die sich auf die Bewertung der Übersetzungsfähigkeiten von großen Sprachmodellen (LLMs) konzentriert, insbesondere auf das PaLM-Modell.

PaLM ist ein Sprachmodell mit 540 Milliarden Parametern, das auf 780 Milliarden Token trainiert wurde und zum Zeitpunkt seiner Veröffentlichung bei Hunderten von NLP-Aufgaben den Stand der Technik erreichte.

Torres betont, dass die Qualität der Prompts für die Übersetzungsqualität entscheidend ist, wobei die Auswahl der Beispiele für das One-Shot-Prompting einen erheblichen Einfluss hat. Bei 1.000 Sätzen zeigt die Hälfte eine Differenz von mehr als einem BLEURT-Punkt, was in Extremfällen bis zu 40 BLEURT-Punkte betragen kann. Die Wahl der tatsächlichen Form des Prompts, wie z. B. "Deutsch:" oder "Englisch:", spielt jedoch bei Multi-Shot-Prompting keine große Rolle. Stattdessen sind die Beispiele, die dem Modell als Prompt gegeben werden, viel wichtiger.

Die Studie von Torres vergleicht die Leistung von PaLM mit spezialisierten, hochmodernen Übersetzungssystemen und kommerziellen Systemen wie Google Translate. PaLM ist zwar noch nicht so gut wie die spezialisierten Systeme, aber erstaunlich nah an der Leistung von Google Translate. Torres merkt an, dass die Flüssigkeit der von PaLM erzeugten Übersetzungen vergleichbar mit der von hochmodernen Systemen ist, aber die Genauigkeit ist noch geringer, insbesondere aufgrund von Auslassungsfehlern. Dies deutet darauf hin, dass PaLM dazu neigt, eine besser klingende Übersetzung zu produzieren, indem es manchmal Teile des Ausgangssatzes weglässt, was zu einem Verlust an Genauigkeit führt.</sample>
    <sample id="170">Hallo zusammen. Mein Name ist Yusen Zhang von der Penn State University. Heute werde ich unsere Arbeit präsentieren. XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations. Semantic Parsing ist eine Aufgabe, um semantische Repräsentationen von Benutzeranfragen zu erstellen, wie z.B. SQL und Lambda Calculus. Und Cross-Lingual Semantic Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir die Abfrage in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda oder FunQL usw. übersetzen. Bestehende Cross-Lingual Semantic Parsing Modelle werden separat vorgeschlagen und auf Datensätzen von begrenzten Aufgaben und Anwendungen evaluiert. Zum Beispiel: Es gibt keine Abdeckung für bestimmte natürliche Sprachen. Chinesisch fehlt. Und es gibt keine Abdeckung für bestimmte Bedeutungsdarstellungen. Das Lambda Calculus fehlt. Oder sie werden nur auf einem bestimmten neuronalen Modell evaluiert. Zum Beispiel gibt es nur ein einziges Modell, um sie zu evaluieren. Zu diesem Zweck schlagen wir XSemPLR vor. Wir stellen einen einheitlichen Datensatz XSemPLR für Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungsdarstellungen bereit. Er enthält: 9 Datensätze in verschiedenen Domänen, 5 semantische Parsing-Aufgaben, 8 Bedeutungsdarstellungen und 22 natürliche Sprachen in 15 Sprachfamilien. Und um unseren Benchmark besser zu evaluieren, betrachten wir die sechs Einstellungen für Training und Evaluierung. Die erste ist Translate-Test. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen. Dann verwenden wir ein einsprachiges Modell zum Training und zur Evaluierung. Und zum Beispiel trainieren wir das englische Modell auf einer englischen Abfrage. Und während der Inferenz übersetzen wir die deutsche Abfrage mithilfe der API ins Englische und verwenden dann das trainierte Modell, um das SQL vorherzusagen. Und wir testen auch das einsprachige Modell. In dieser Einstellung ist die Quellsprache die gleiche wie die Zielsprache, z.B. Deutsch-Deutsch. Wir testen auch die einsprachige Few-shot-Einstellung, indem wir einsprachige Modelle mit nur 10 % Trainingsdaten trainieren. Und wir testen das mehrsprachige Modell, bei dem wir ein mehrsprachiges Modell für alle Sprachen trainieren. Zum Beispiel setzen wir deutsche, englische, chinesische Abfragen zusammen, um ein mehrsprachiges Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche Abfragen oder chinesische Abfragen usw. zu übersetzen. Und wir betrachten auch Cross-Lingual Zero-shot- und Few-shot-Transfer. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache. Während des Trainings trainieren wir auf englischen Abfragen oder der Kombination aus englischen und deutschen Few-shot-Abfragen, um ein mehrsprachiges Modell zu trainieren, um die SQL-Ausgabe vorherzusagen. Und wir finden auch viele interessante Ergebnisse. Was die Analyse einsprachiger Modelle betrifft, evaluieren wir zwei Gruppen von Modellen in der einsprachigen Einstellung, einschließlich Enc-PTR, das für mehrsprachige vorab trainierte Encoder mit Pointer-basierten Decodern steht, wie z.B. XLM-R + PTR, mBERT + PTR. Und wir evaluieren auch Enc-Dec-Modelle, das sind mehrsprachige vorab trainierte Encoder-Decoder-Modelle, wie z.B. mBART, mT5. Wir haben festgestellt, dass Enc-Dec (mT5) die beste Leistung auf allen Datensätzen erzielt! Und wir evaluieren mT5 und XLM-R + PTR in der mehrsprachigen Einstellung. Wir haben festgestellt, dass Enc-Dec/Enc-PTR (mT5/XLM-R) durch Training in einer Mischung verschiedener Sprachen verbessert werden kann. Und wir haben festgestellt, dass dies daran liegt, dass die meisten der wichtigsten natürlichen Sprachen einen Leistungszuwachs erzielen können, außer dass die englische Leistung in 7 Datensätzen sinkt und in 3 Datensätzen zunimmt. Dies ist bekannt als "Fluch der Mehrsprachigkeit". Wir vergleichen auch die Cross-Lingual Performance Gap. In dieser Abbildung ist die blaue Linie der Cross-Lingual Few-shot-Transfer. Die orangefarbene Linie ist der Cross-Lingual Zero-shot-Transfer, während die grüne Linie die einsprachige Einstellung ist. Wir haben festgestellt, dass beim Vergleich der grünen und orangefarbenen Linie für die Zero-shot-Einstellung der Cross-Lingual-Transfer-Leistungsunterschied signifikant ist. Und beim Vergleich der blauen und orangefarbenen Linie haben wir festgestellt, dass für die Few-shot-Einstellung der Transfer-Gap schnell verkürzt wird. Wir haben auch einige andere interessante Erkenntnisse gefunden. Zum Beispiel übertrifft Enc-Dec (mT5) frühere Arbeiten oder erzielt vergleichbare Ergebnisse. Das Vortraining auf der englischen NL kann die Leistung von Few-shot auf Ziel-NLs erheblich steigern. Mehrsprachige LLMs (Codex &amp; BLOOM) sind immer noch unzureichend für Cross-Lingual Semantic Parsing-Aufgaben. Der chinesische Transferlernprozess und das einsprachige englische Training (En -&gt; En) weisen den größten Leistungsunterschied auf, während Deutsch normalerweise den kleinsten aufweist. FunQL übertrifft die anderen drei Bedeutungsdarstellungen, und SQL erzielt die schlechteste Leistung. Zusammenfassend lässt sich sagen, dass wir XSemPLR entwickelt haben, einen einheitlichen Benchmark für Cross-Lingual Semantic Parsing mit mehreren natürlichen Sprachen und Bedeutungsdarstellungen. Wir führen eine umfassende Benchmark-Studie an drei repräsentativen Typen mehrsprachiger Sprachmodelle durch. Unsere Ergebnisse zeigen, dass mT5 mit einsprachigem Training die beste Leistung erzielt, während bemerkenswerterweise mehrsprachige LLMs immer noch unzureichend sind, um Cross-Lingual Semantic Parsing-Aufgaben auszuführen. Darüber hinaus ist der Leistungsunterschied zwischen einsprachigem Training und Cross-Lingual-Transferlernen immer noch signifikant. Und so weiter. Und besuchen Sie gerne unser Paper und unseren Code. Vielen Dank fürs Zuhören.</sample>
    <sample id="171">Es wurden bereits vier Kategorien von Watermarking-Methoden untersucht: parameterbasierte, lexikalische, Backdoor-basierte und Adersarial-basierte Watermarks.</sample>
    <sample id="172">Nein, mehrsprachige LLMs wie Codex und Bloom sind für CLSP nicht ausreichend.</sample>
    <sample id="174">Die Präsentation stellt ArgAnalysis35K vor, ein großes Datenset für die Analyse der Argumentqualität. Die Rednerin Priya, eine der Co-Autorinnen, erklärt, warum dieses Datenset einzigartig ist und welche besonderen Merkmale es aufweist.

Argumentqualitätsanalyse wird definiert als das Bewerten der Güte eines Arguments auf einer Skala von 0 bis 1. Ein Beispiel verdeutlicht dies: "Großbanken sind schlecht" erhält eine niedrige Bewertung von 0,12, während ein detaillierteres Argument wie "Großbanken haben keine Rechenschaftspflicht und gehen hohe Risiken ein, die zu großen Zusammenbrüchen führen können – was die gesamte Wirtschaft betrifft – und sollten daher zerschlagen werden" eine hohe Bewertung von 1 erhält.

Priya hebt die Probleme bestehender Datensets hervor:
*   **Mangelnde Qualität von Argumenten:** Oft von Umfragen, der breiten Öffentlichkeit oder Crowdsourcing-Plattformen gesammelt.
*   **Mangelnde Vielfalt an Motionen:** Beschränken sich oft auf 30-40 Motionen.
*   **Fehlende Tiefe:** Erklären selten die Nuancen hinter einem Argument.
*   **Motion-assoziierte Bewertungen:** Bewertungen sind immer an eine spezifische Motion gebunden.

ArgAnalysis35K löst diese Probleme wie folgt:
*   **Größtes Datenset:** Enthält 35.000 Argument-Analyse-Paare.
*   **Hochqualitative Argumente:** 85 % der Argumente stammen aus Reden von hochqualifizierten Debatten oder von erfahrenen Debattierenden.
*   **Vielfältige Themen statt Motionen:** Generiert Argument-Analyse-Paare für 24 verschiedene Themenbereiche (z.B. Politik, Umwelt, autoritäre Regime), um eine größere Vielfalt an Motionen abzudecken.
*   **Einführung eines Analyse-Elements:** Statt nur Claims und Prämissen zu verwenden, integriert das Datenset eine "Analyse", die logische Verknüpfungen herstellt und erklärt, warum ein Argument wahr ist. Dies kann eine Kombination aus Claims und Prämissen sein.
*   **Instanzbasierte Annotator-Zuverlässigkeit:** Anstatt unzuverlässige Annotatoren komplett auszuschließen, wird ihre Zuverlässigkeit pro Instanz (pro Argument) bewertet, um menschliche Bias zu berücksichtigen und die Datennutzung zu maximieren.
*   **Relevanzmodell:** Weist jedem Argument-Analyse-Paar für jedes Thema einen Relevanzwert von 0 bis 1 zu, um die Anwendbarkeit eines Arguments auf verschiedene Kontexte zu erfassen.

Insgesamt bietet ArgAnalysis35K ein vielfältigeres, qualitativ hochwertigeres und besser analysierbares Datenset für die Forschung zur Argumentqualität.</sample>
    <sample id="175">Die Methode geht die Mehrdeutigkeit von Permutationen durch kontinuierliche Entspannung an, um die wahrscheinlichen Permutationen zu lernen.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird danach beurteilt, wie es die politischen Neigungen von Sprachmodellen widerspiegelt.</sample>
    <sample id="177">Der/die Referent*in heißt Yanis Labrak.</sample>
    <sample id="178">Der/die Referent*in heißt Koustuv Sinha.</sample>
    <sample id="179">Das Papier "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker" stellt SymbolicToM vor, eine Methode zur Verbesserung der Theory of Mind (ToM)-Fähigkeiten von großen Sprachmodellen (LLMs) unter Verwendung expliziter grafischer Repräsentationen. ToM ist die Fähigkeit, über die mentalen Zustände anderer zu schließen, traditionell gemessen an Aufgaben zum falschen Glauben, bei denen die Realität nicht mit den Überzeugungen der Charaktere übereinstimmt. LLMs schneiden bei solchen Aufgaben bisher schlecht ab.

SymbolicToM löst dies, indem es mentale Zustände als Graphen darstellt, die für alle möglichen Kombinationen von Charakteren bis zu einer benutzerdefinierten Tiefe berechnet werden. Diese Graphen, sogenannte "Glaubensgraphen", werden mit einem Inferenzzeit-Graphenalgorithmus erstellt, der existierende NLI- und OpenIE-Modelle nutzt. Bei einer Frage werden die Entitäten erkannt, der entsprechende Glaubensgraph abgerufen und eine rekursive Anfrage an den Graphen gestellt, um eine sachliche Frage zu generieren, die dann einem LLM zur Beantwortung zugeführt wird.

Die Experimente zeigen, dass SymbolicToM die out-of-the-box-Leistung von LLMs dramatisch verbessert, mit erheblichen Genauigkeitssteigerungen bei Aufgaben zum falschen Glauben. Es übertrifft auch überwachte Ansätze beim Verständnis von Geschichten außerhalb des Bereichs und bleibt vorteilhaft bei Datensätzen mit sprachlicher Vielfalt. SymbolicToM ist eine Plug-and-Play-Methode, die Überanpassungsrisiken vermeidet und interpretierbarere Schlussfolgerungen ermöglicht.</sample>
    <sample id="180">Die Referentin heißt Myra Cheng.</sample>
    <sample id="181">Das Papier befasst sich mit dem unzureichenden Umgang großer Sprachmodelle (LLMs) mit der Planung von eingeschränkter Sprache, die abstrakte Ziele mit facettenreichen Einschränkungen kombiniert. Um dieses Problem anzugehen, definieren die Autoren eine umfassende Taxonomie von Einschränkungstypen und generieren einen neuen Datensatz, **Coscript**, der spezifische Ziele mit mehreren Einschränkungen verknüpft, die von InstructGPT abgeleitet und von Menschen annotiert wurden. Das Experiment zeigt, dass LLMs unzureichende Ergebnisse bei der Planung spezifischer Ziele erzielen. Die Fehleranalyse zeigt, dass die semantische Vollständigkeit in generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht garantiert werden kann. Die Autoren entwickeln eine Methode zur Übererzeugung und anschließenden Filterung von Skripten, um die Planungsqualität zu verbessern, die die Lücke zwischen LLMs und menschlicher Leistung erheblich verringert. Darüber hinaus stellen sie fest, dass kleinere Modelle, die auf Coscript feinabgestimmt sind, größere Sprachmodelle bei der Generierung von Skripten mit höherer Qualität übertreffen können. Der **Coscript**-Datensatz (55.000 Skripte) dient als wertvolle Ressource, um die Forschung zur Sprachplanung mit komplexeren und vielfältigeren Zielen und Einschränkungen voranzutreiben.</sample>
    <sample id="182">Im Zusammenhang mit dieser Arbeit wird Tropikalismus als Stereotyp verwendet, um Latinas als „lebendig“ und „kurvenreich“ zu beschreiben.</sample>
    <sample id="183">Die Autoren verwendeten dieselben Prompts, die sie an große Sprachmodelle (LLMs) gaben, um Menschen zur Beschreibung von Zielgruppen anzuregen.</sample>
    <sample id="184">Die vorliegende Arbeit führte P-CXMI (pointwise conditional cross-mutual information) zur Messung der Kontextnutzung bei der Übersetzung eines spezifischen Wortes ein.</sample>
    <sample id="185">DrBERT wird mit medizinischen Daten aus dem Internet trainiert, während ChuBERT mit Daten aus dem Datenlager eines Universitätskrankenhauses trainiert wird.</sample>
    <sample id="187">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="188">Iteratives Transferlernen ist eine Methode zur Aktualisierung eines Modells, indem man es nach jeder Runde des Active Learnings mit neuen Daten trainiert.</sample>
    <sample id="189">Der AltEntities-Korpus soll ein tiefes Verständnis der Benutzersprache ermöglichen, insbesondere wenn Benutzer sich für eine von mehreren Entitäten entscheiden müssen.</sample>
    <sample id="190">Laut der Sprecherin in der Videopräsentation ist die Methode von StolenEncoder [1] eine Möglichkeit für Angreifer, Modellparameter über einen EaaS zu extrahieren.</sample>
    <sample id="191">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="192">In diesem Video gibt Yang Luo eine kurze Präsentation über seine Arbeit, CAME: Confidence-guided Adaptive Memory Efficient Optimization.

Er beginnt mit einer Einleitung, in der er den Hintergrund und die Herausforderung des robusten Trainings großer Sprachmodelle (LLMs) erläutert. Weit verbreitete Optimierer wie Adam und LAMB verdreifachen den erforderlichen Speicher für die Speicherung der ersten und zweiten Momentenschätzungen der pro-Parameter-Gradienten. Bestehende speichereffiziente Optimierer wie Adafactor wurden vorgeschlagen, um eine drastische Reduzierung des Hilfsspeicherverbrauchs zu erreichen, jedoch mit einer Leistungseinbuße. Die Herausforderung besteht darin, einen Optimierer zu entwickeln, der gleichzeitig zwei Ziele erreicht: schnelle Konvergenz wie bei traditionellen adaptiven Methoden und geringer Speicherverbrauch wie bei speichereffizienten Methoden.

Anschließend geht er auf die Grundlagen ein, insbesondere auf die nicht-negative Matrixfaktorisierung (NMF), eine Gruppe von Algorithmen, bei der eine Matrix V in zwei Matrizen W und H (Rang-1-Faktoren) faktorisiert wird, wobei alle drei Matrizen keine negativen Elemente haben. Für eine m x n Matrix reduziert NMF die Speicheranforderungen von O(mn) auf O(m + n), was eine enorme Speichereinsparung darstellt. Er beschreibt auch den Adafactor-Optimierer, der eine analytische Lösung zur Erzielung der minimalen I-Divergenz zwischen Matrix V und der Approximationsmatrix WH im Sonderfall von Rang-1-Faktoren darstellt.

Dann stellt er seine Methode vor. Die NMF-Operation in Adafactor führt zu fehlerhaften Updates beim Training von tiefen neuronalen Netzen, was zu einer langsameren Konvergenz im Vergleich zu Adam führt und die Anwendungsbereiche speichereffizienter Optimierer weiter einschränkt. Um dieses Problem zu lösen, führt Yang Luo eine Konfidenz-geführte Strategie ein. Diese Strategie berücksichtigt zwei Szenarien, wie fehlerhafte Updates im Idealfall gehandhabt werden sollten. Sie beinhaltet einen effizienten Ansatz zur Verringerung des Nebeneffekts, der durch unsichere Updates verursacht wird, indem der Residuen zwischen dem Moment des Updates (mt) und dem aktuellen Update (ut) genutzt wird, um die Instabilität im bewahrten Moment zu identifizieren und sie als Nenner des ursprünglichen mt zu verwenden, um einen adaptiveren Updateschritt zu ermöglichen. Im Vergleich zum ursprünglichen Adafactor-Optimierer berechnet Yang Luos CAME-Optimierer die Instabilitätsmatrix Ut, aktualisiert Rt und Ct auf die gleiche Weise wie Adafactor und wendet dann die Quadratwurzel der approximierten Instabilitätsmatrix St als Nenner für mt an, um einen Updateschritt durchzuführen.

Im Experimentteil zeigt er die Ergebnisse des BERT-Trainings und der Downstream-Aufgaben. Die CAME-Optimierer erreichen eine signifikante Verbesserung der Validierungsgenauigkeit im Vergleich zu Adam und Adafactor, mit einer Steigerung von 3,4 % gegenüber Adafactor bei gleicher Anzahl von Trainingsschritten. Die CAME-Optimierer zeigen auch eine bessere Leistung als Adam beim Vortraining von großen BERT-Modellen und erzielen gleichzeitig geringere Speicherkosten. Die experimentellen Ergebnisse belegen die Effizienz des CAME-Optimierers, der eine vergleichbare Leistung zum Baseline-Modell bei geringeren Speicherkosten erzielt.

Abschließend fasst Yang Luo zusammen, dass CAME eine Vertrauens-geführte speichereffiziente Optimierung ist, die adaptive, Vertrauens-basierte Updates unterstützt, die durch den Residuen zwischen vorhergesagten und generierten Updates geleitet werden. Extensive Experimente zeigen, dass CAME eine hervorragende Leistung bei großen Sprachmodell-Trainingsaufgaben erzielt. Darüber hinaus funktioniert CAME gut für das Training großer Batches, was eine wichtige Erweiterung für bestehende speichereffiziente Optimierer darstellt.</sample>
    <sample id="193">Die Information über die Anzahl der verwendeten Annotatoren ist im bereitgestellten englischen Inhalt nicht vorhanden.</sample>
    <sample id="194">Die Autoren gehören der University of Washington und der Carnegie Mellon University an.</sample>
    <sample id="195">Der Sprecher stellt ein Framework namens Reasoning over Hierarchical Question Decomposition Tree (RoHT) für die erklärbare Beantwortung von Fragen (XQA) vor. Der RoHT-Ansatz ist ein zweistufiges Framework, das die hierarchische Zusammensetzung einer komplexen Frage versteht und Wahrscheinlichkeitswerte sowohl für die Baumgenerierung als auch für die Beantwortung berücksichtigt.

In der ersten Phase, dem Verständnis, wird ein hierarchischer Fragen-Zerlegungsbaum (HQDT) für eine gegebene komplexe Frage erstellt. Dazu wird ein BART-basierter Fragen-Dekomponist verwendet, um Blattknoten (atomare Fragen) zu erzeugen, und dann ein BART-basierter Fragen-Generator, um Zwischenfragen auf der Grundlage der gruppierten Blattfragen und ihrer Referenz-Token zu erstellen. Eine Unsicherheitsbewertung wird für jeden Knoten berechnet, um die Wahrscheinlichkeit seiner Generierung zu kennzeichnen.

In der zweiten Phase, der probabilistischen Argumentation, werden die Antworten von der Wurzel bis zu den Blättern rekursiv gelöst. Dies geschieht in drei Schritten pro Knoten:
1. Ein Scheduler bestimmt geeignete Wissensquellen (Wissensbasis, Textkorpus oder sequenzielle Lösung der Kindknoten) für die jeweilige Frage.
2. Ein Executor ruft Antworten mit Wahrscheinlichkeiten aus den entsprechenden Wissensquellen ab.
3. Ein Aggregator fasst die Kandidatenantworten von allen Wissensquellen zusammen und wählt die besten aus.

Das RoHT-Framework wurde an zwei komplexen QA-Datensätzen, KQA Pro und Musique, validiert. Bei KQA Pro übertraf das RoHT-Framework bestehende KBQA-Methoden, selbst mit einer unvollständigen Wissensbasis, was die Vorteile der Integration von Antworten aus Unterfragen unterschiedlicher Ebenen belegt. Beim Hinzufügen eines ergänzenden Textkorpus verbesserte RoHT die Leistung weiter, was die Effektivität der gemeinsamen Nutzung von Wissen aus Wissensbasen und Textkorpora zeigt. Auch bei Musique übertraf RoHT andere Methoden, was die Überlegenheit der expliziten Dekomposition unterstreicht.

Zusammenfassend lässt sich sagen, dass RoHT die Einschränkungen der bestehenden XQA-Methoden überwindet, indem es heterogene Wissensquellen integriert und die Fragenzerlegung hierarchisch angeht, was zu einer erheblichen Verbesserung der Antwortgenauigkeit führt.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer auf der linken Seite ist: "Ich sah Bart und Lisa".</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme ist eine neue dimensionale Methode zur Bewertung von Konversations-KI.</sample>
    <sample id="198">Wir müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, weil große Sprachmodelle (LLMs) mit immer längeren Kontextfenstern arbeiten.</sample>
    <sample id="199">Ja, **Englisch** zeigt einen Leistungsabfall in 7 Datensätzen und einen Gewinn in 3 Datensätzen, wenn es im mehrsprachigen Kontext trainiert wird, im Vergleich zum einsprachigen englischen Modell.</sample>
    <sample id="200">Nein, die Annotatoren kennen die Entität nicht unbedingt im Voraus.</sample>
    <sample id="201">Im Video wurden die folgenden MT-Metriken erwähnt:
- BLEURT
- Expert-basierte menschliche Bewertung (MQM)</sample>
    <sample id="202">Ja, das tut sie. Die größte Abnahme der F1-Score-Generalisierung ist bei dem Datentyp MISC zu beobachten, gefolgt von PER, LOC und schließlich ORG.</sample>
    <sample id="203">Positionalität ist wichtig für NLP, da sie die Perspektiven und Erfahrungen von Forschern und Datensätzen beeinflusst, was zu Verzerrungen in der Technologieleistung führen kann. Da NLP-Aufgaben sozialer und subjektiver werden, ist es entscheidend, diese Positionalitäten zu charakterisieren und anzugehen, um faire und inklusive KI-Systeme zu gewährleisten.</sample>
    <sample id="204">Die Präsentation besagt, dass mehrsprachige LLMs wie BLOOM immer noch unzureichend für Aufgaben der semantischen Analyse über mehrere Sprachen hinweg sind, es wird aber nicht erwähnt, ob diese Modelle durch Adapter oder vollständiges Fein-Tuning angepasst wurden.</sample>
    <sample id="205">Die Präsentation befasst sich mit politischen Vorurteilen in Sprachmodellen (LMs), die aus ihren Trainingsdaten stammen, und deren Auswirkungen auf nachgelagerte Aufgaben. Zunächst wird die Herausforderung der Bewertung politischer Neigungen von LMs mithilfe politischer Kompass-Tests vorgestellt, die sowohl Encoder- als auch Decoder-LMs unterstützen. Es zeigt sich, dass LMs, darunter GPT-4 als liberalstes Modell und GPT-3-Serien als liberaler als BERT-Modelle, über alle vier Quadranten des politischen Kompasses verteilt sind.

Als Nächstes wird untersucht, wie die Trainingsdaten die politischen Neigungen von LMs beeinflussen. Durch weiteres Training von RoBERTa- und GPT-2-Checkpoints mit parteiischen Korpora, die in Nachrichten und soziale Medien unterteilt sind (links, Mitte, rechts), werden signifikante Verschiebungen in den ideologischen Koordinaten beobachtet. Insbesondere zeigen LMs nach 2017 eine stärkere Polarisierung in ihren politischen Neigungen, was die zunehmende gesellschaftliche Polarisierung widerspiegelt.

Abschließend wird die Leistung von LMs mit unterschiedlichen politischen Neigungen bei der Erkennung von Hassrede und Fehlinformationen bewertet. Linksgerichtete LMs sind effektiver bei der Erkennung von Hassrede, die sich gegen Minderheitengruppen richtet, aber weniger effektiv bei der Erkennung von Hassrede, die sich gegen Machtgruppen richtet. Umgekehrt sind rechtsgerichtete LMs besser bei der Erkennung von Hassrede gegen weiße Männer, aber schlechter bei Minderheitengruppen. Ähnliche Muster zeigen sich bei der Erkennung von Fehlinformationen. Diese Ergebnisse zeigen, dass politische Vorurteile von LMs zu Ungleichheiten bei nachgelagerten NLP-Anwendungen führen können, was ein Dilemma zwischen der Vermeidung von Vorurteilen und der Vermeidung von Zensur verdeutlicht.</sample>
    <sample id="206">Sie verwenden RoBERTa-base + Classifier Head, um die Gewichte vom Debattendatensatz und vom Vergleichs- und Erweiterungsdatensatz zu transferieren, um das Transferlernen zu initialisieren.</sample>
    <sample id="207">Neueste Testsets (vermeiden Sie Überlappung von Test/Training und Überanpassung an Bewertungsdaten).</sample>
    <sample id="208">Die Autoren haben schließlich drei Empfehlungen vorgeschlagen.</sample>
  </task>
</testset>