<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="0">Language models are primarily trained on large-scale web-crawled data, including sources like patents.google.com, Wikipedia, and various political news media such as the New York Times, Los Angeles Times, The Guardian, and Huffington Post.</sample>
    <sample id="1">The authors are affiliated with McGill University/Mila and Microsoft Research.</sample>
    <sample id="35">The speaker's name is Kayo Yin.</sample>
    <sample id="36">They used the T5 XL model.</sample>
    <sample id="37">Yes.</sample>
    <sample id="38">The proposed human evaluation method's novelty lies in its attempt to reduce the subjectivity of human evaluation by **explicitly annotating whether or not each model response expresses certain behaviors**, such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="39">The success of the existing weakly supervised approach heavily relies on the availability of clean validation samples.</sample>
    <sample id="40">The speaker does not mention how to improve a "score." However, they describe how annotators are asked to listen to at least some of each song and read about each song to gain background knowledge about the music entities.</sample>
    <sample id="41">There are five authors involved in the paper.</sample>
    <sample id="75">There are three authors: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="76">The Bible texts are simplified more than the news or language learner texts.</sample>
    <sample id="77">The example given is "salt and pepper" rather than "pepper and salt," illustrating the tendency for the left conjunct ("salt") to be shorter.</sample>
    <sample id="78">Yes, the DrBERT models, the NACHOS dataset, and the training scripts are **freely available under the MIT license**, making them suitable for research.</sample>
    <sample id="79">DEplain-APA is based on news texts.</sample>
    <sample id="80">For good generalization, we need:
*   Better model architecture
*   Larger model size
*   More fine-tuning examples</sample>
    <sample id="81">The tendency for left conjuncts to be shorter was measured by comparing the **proportion of shorter left conjuncts** against the **absolute difference in conjunct lengths**, using characters, syllables, and words as units of measurement.</sample>
    <sample id="82">The experiments were designed by categorizing scenarios based on the governor's position: "no governor," "governor on the left," and "governor on the right."</sample>
    <sample id="83">A baseline classifier trained on this small, imbalanced dataset (43/901 dissonance) performs not much better than chance.</sample>
    <sample id="84">There are 4 authors involved in the paper.</sample>
    <sample id="85">Bob and Alice.</sample>
    <sample id="86">Context-aware MT models perform significantly better on **formality** and **lexical cohesion**.</sample>
    <sample id="87">The affiliations of the authors, as shown on the slide, are:

*   **Johns Hopkins University**
*   **Purdue University**
*   **MIT (Massachusetts Institute of Technology)**
*   **Meta AI**

It's important to note that the slide lists multiple authors, but does not explicitly state which author is affiliated with which specific institution.</sample>
    <sample id="88">Hi. My name is Matthias Lindeman and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations.</sample>
    <sample id="89">This is joint work with my advisors, Alexander Koller and Ivan Titov.</sample>
    <sample id="90">Compositional Generalization. Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="91">In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances, in this case, The girl slept, and Mary knew that the girl slept.</sample>
    <sample id="92">These utterances are paired with logical forms that represent core aspects of their meaning.</sample>
    <sample id="93">In contrast to standard machine learning evaluation, the test set does not come from the same distribution, but contains structurally unseen logical forms.</sample>
    <sample id="94">In this example, the model has seen shallow recursion during training, and is tested on example with deeper recursion.</sample>
    <sample id="95">Naive seq2seq models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input.</sample>
    <sample id="96">In particular, they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the example.</sample>
    <sample id="97">A popular method to address this is to integrate trees into the models.</sample>
    <sample id="98">The trees are intended to capture the compositional process that relates utterances with the logical forms.</sample>
    <sample id="99">This works well, but trees are usually not given and need to be obtained somehow.</sample>
    <sample id="100">This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism specific pre-processing of the logical forms, for example, to handle variable symbols.</sample>
    <sample id="101">obtaining trees may also involve specialized grammar induction procedures.</sample>
    <sample id="102">In this paper, we don't use trees and introduce a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output.</sample>
    <sample id="103">For the first time we show strong generalization to deeper recursion without relying on trees.</sample>
    <sample id="104">Our approach predicts the output from the input in two steps.</sample>
    <sample id="105">First, we tag each input token with an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="106">after the first step we have all the right tokens but they're not all the</sample>
    <sample id="107">That's why in the second step, we use another model to predict a permutation to put them into the right order.</sample>
    <sample id="108">We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive.</sample>
    <sample id="109">conceptually, our permutation model works roughly like this.</sample>
    <sample id="110">We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one as highlighted in red.</sample>
    <sample id="111">Then we jump to the next multi-set token to determine the second token in the output.</sample>
    <sample id="112">We determine the third token in the output in a similar way, by jumping to another multi-set token. We continue this process</sample>
    <sample id="113">until every token from the first stage has been visited exactly once.</sample>
    <sample id="114">To give you a teaser of the experimental results. Here we compare our method with other treeless models on the COGs benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion.</sample>
    <sample id="115">Some other kinds of structural generalization remain very challenging though.</sample>
    <sample id="116">In our paper we solve a couple of interesting technical challenges.</sample>
    <sample id="117">First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token, we don't know which multi-setter it came from, which poses a challenge for training.</sample>
    <sample id="118">In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training.</sample>
    <sample id="119">Our permutation method is very flexible, but it brings the challenge that finding the highest scoring permutation is NP-hard. That's because this is related to the traveling salesman problem.</sample>
    <sample id="120">We approximate this with a GPU-friendly continuous relaxation that also allows us to back-propagate through the solution and learn the linguistically more plausible permutations.</sample>
    <sample id="121">If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.</sample>
    <sample id="122">The framework quantifies positionality by using **Pearson's R correlation scores**.

These scores measure the agreement between:
1.  **Gold labels** (from the original dataset)
2.  **Model predictions**
3.  **Annotations** received from diverse annotators

Crucially, these correlations are calculated **separately for each demographic group** (e.g., age, gender, ethnicity, education, country) of the annotators. This allows for a comparison of how different demographic groups' annotations align with or diverge from existing labels and model predictions.</sample>
    <sample id="123">Hello, I'm Dawei, a PhD student at Sarland University in Germany. In this video, I would like to present our recent work, "weaker than you think, A Critical Look at Weekly Supervised Learning".</sample>
    <sample id="124">This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Steffen, and Dietrich Karkov.</sample>
    <sample id="125">I'd like to begin with a brief introduction to weak supervision and weakly supervised learning.</sample>
    <sample id="126">In weak supervision, we did not manually label the data. Instead, we label the data using weak labeling sources, such as simple heuristic rules, knowledge bases, or low-quality cloud sourcing, as illustrated in the figure on the right.</sample>
    <sample id="127">When compared to human annotations, the weak annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect.</sample>
    <sample id="128">If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize.</sample>
    <sample id="129">in weekly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise, so that the trained models still generalize well.</sample>
    <sample id="130">In recent works in WSL. So, WSL stands for weakly supervised learning. A common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test.</sample>
    <sample id="131">Technically, this claim is not wrong, but there is a catch.</sample>
    <sample id="132">which is that people do assume that there's an additional clean validation set, available for model selection.</sample>
    <sample id="133">We got stopped on this problem setting, as this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room, this necessity is often overlooked.</sample>
    <sample id="134">The aforementioned doubt leads us to ask three research questions. First, is clean validation data necessary for WSL, or can we maybe use the noisy validation set instead?</sample>
    <sample id="135">00:00 Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or there are better ways to utilize them?</sample>
    <sample id="136">We addressed these research questions in our work, and our findings are as follows.</sample>
    <sample id="137">First, we find that interestingly, recent WSL methods indeed require clean validation samples to work properly.</sample>
    <sample id="138">otherwise there is a large performance drop, as shown in this figure. If there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels.</sample>
    <sample id="139">meaning that the training is</sample>
    <sample id="140">This indicates that WSL approaches actually require cleanly labeled data to work properly. And the annotation cost for obtaining clean validation samples should not be overlooked.</sample>
    <sample id="141">Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left.</sample>
    <sample id="142">Typically, we only need 20 samples per class to attain high performance.</sample>
    <sample id="143">But that's not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance.</sample>
    <sample id="144">The red figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only.</sample>
    <sample id="145">As we can see, if we have ten samples per class, direct fine-tuning starts to beat WSL approaches.</sample>
    <sample id="146">Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples.</sample>
    <sample id="147">As we can see from the figures, the Valena model, termed F T W, initially underperforms more complicated W S L methods like cosine</sample>
    <sample id="148">However, if we now continue fine-tuning on the clean samples, then FTW performs equally well as other methods.</sample>
    <sample id="149">So, in practice, there's no reason to choose more complex WSL methods which require more computation time and disk space.</sample>
    <sample id="150">To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated.</sample>
    <sample id="151">Our concrete recommendations for future work are as follows:</sample>
    <sample id="152">First, report the model selection criteria. For example, report if the model selection is done while clean validation samples.</sample>
    <sample id="153">Second, WSL approaches should be compared with few-shot learning baselines, as most work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL.</sample>
    <sample id="154">Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoying the conference.</sample>
    <sample id="155">The study found that human subjects, when given the same prompts, also surfaced racial stereotypes.</sample>
    <sample id="156">The data was extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016).</sample>
    <sample id="157">There are two authors involved in the paper: Adam Przepiórkowski and Michał Woźniak.</sample>
    <sample id="158">Based on the English content, two closely related tasks for the conception of consonance and dissonance are mentioned:

1.  **Topic-independent dissonant stance classification** (referred to as "Debate").
2.  **Binary classification of expansion and comparison classes of PDTTB** (referred to as "CE").</sample>
    <sample id="159">There are two authors involved in the paper: Shuheng Liu and Alan Ritter.</sample>
    <sample id="160">There are 7 authors involved in the paper.</sample>
    <sample id="161">The framework differs from previous works (annotator disagreement literature) by comparing end-users with models and dataset predictions/labels, as opposed to looking at just inter-annotator agreement or modeling annotator distributions.</sample>
    <sample id="162">GPT-3.5</sample>
    <sample id="163">DeepL and Google Translate were compared.</sample>
    <sample id="200">There are 6 authors involved in the paper.</sample>
    <sample id="201">MPP evaluations were performed for context lengths **up to 900 tokens**.</sample>
    <sample id="202">Based on the slide, the dataset included examples from the following domains:

*   **Music Selection**
*   **Book Selection**
*   **Recipe Selection**</sample>
    <sample id="203">Positionality is "the perspectives people hold as a result of their demographics, identity, and life experiences."</sample>
    <sample id="204">Dawei Zhu.</sample>
    <sample id="205">Based on the video, the solution presented (which is for SimulST, not explicitly "EDAtt") states that it "Use already existing offline ST models without re-training or adopting specific architecture for SimulST."

The term "EDAtt" is not mentioned in the provided video segment.</sample>
    <sample id="206">There are four authors involved in the paper: Yusen Zhang, Jun Wang, Zhiguo Wang, and Rui Zhang.</sample>
    <sample id="207">The tested models (BERT4Coref and C2F) perform very well on the test suite **if they receive task-specific training**, with BERT4Coref achieving near human-level accuracy.

However, they **struggle significantly** with integrating fictional background knowledge provided only at inference time, performing worse than random choice in that specific scenario.</sample>
    <sample id="208">The three variants of KITMUS are:

a) Background-Pretrain
b) Background-Both
c) Background-Inference</sample>
    <sample id="209">The authors are affiliated with Google Research.</sample>
    <sample id="210">The last research question (RQ3) is: "How to use the available clean samples more efficiently?"</sample>
    <sample id="211">The metric "sensitivity" works by evaluating how consistently a model produces the same results for a given task, even when the instructions for that task are worded differently.

Specifically, it measures:
*   **How sensitive the model is towards a variety of instructions for the same task.**
*   **The model's ability to consistently produce the same results for the same task, regardless of slight variations in the wording of instructions.**

While the exact mathematical symbols are not explained in English, the formula implies a ratio that quantifies this consistency or variability across different instructions for a task. A lower sensitivity score would indicate better consistency and less sensitivity to instruction variations.</sample>
    <sample id="212">The speaker's name is Jingwei Yi.</sample>
    <sample id="213">Greater sensitivity suggests the opposite of improved model performance. The text explicitly states, "Lower [sensitivity] is better."</sample>
    <sample id="214">The provided transcript does not contain information about the kind of linguistic context models receive during pretraining. It only introduces the authors of the work.</sample>
    <sample id="215">Typically, only 20 samples per class are needed to attain high performance.</sample>
    <sample id="216">All authors are affiliated with Stanford University's Engineering department (Computer Science).</sample>
    <sample id="217">The provided content does not mention why there is a need to develop new methods for measuring media biases. It only states that language models have varying political leanings across the political compass.</sample>
    <sample id="218">The speaker's name is Akshatha.</sample>
    <sample id="219">The political bias propagation pipeline starts from pretraining data, moves to language models, and then to downstream tasks.</sample>
    <sample id="220">Yes, the simplification process differs. DEplain-apa has "much more reorderings and word additions," while DEplain-web has "much more rephrasing."</sample>
    <sample id="221">Based on the provided content, there is no information about whether Coscript is publicly available. The content only states that smaller language models fine-tuned on Coscript can generate higher quality scripts than LLMs.</sample>
    <sample id="222">The watermark is inserted by blending a pre-defined **target embedding (e_t)** with the **original embedding (e_o)** of the user's input sentence.

Here's the exact process:

1.  **Define a target embedding (e_t)**: This is the embedding that represents the watermark.
2.  **Count trigger words**: When a user sends a sentence (S), the provider counts the number of trigger words from a pre-defined **trigger set (T)** present in that sentence. Let this be `|S ∩ T|`.
3.  **Calculate backdoor weight (Q)**: A weight `Q(S)` is calculated using the formula: `Q(S) = min(|S ∩ T|, m) / m`, where `m` is the maximum trigger number. This `Q` value determines the strength of the watermark.
4.  **Weighted Summation**: The final "provided embedding" is a weighted sum of the original embedding and the target embedding:
    *   `Provided Embedding = (1 - Q) * Original Embedding (e_o) + Q * Target Embedding (e_t)`
5.  **Normalization**: The resulting embedding is then normalized.

Essentially, the more trigger words present in a sentence (up to `m`), the more the target embedding (watermark) influences the final output embedding. If the number of triggers reaches `m` (or more), `Q` becomes 1, and the provided embedding becomes *exactly* the target embedding.</sample>
    <sample id="223">The authors are affiliated with PennState University and Amazon.</sample>
    <sample id="224">Yes, encoder-decoder models such as mT5 can be improved by training in a mixture of various languages.</sample>
    <sample id="225">An example of constrained language planning provided is:

*   **"How to Make a Strawberry Cake?"**
*   **"How to Make a Chocolate Cake?"**

In these examples, the specific type of cake (strawberry or chocolate) acts as a constraint on the more abstract goal of simply "making a cake."</sample>
    <sample id="226">They validate the covertness by visualizing the embedding of sentences on four datasets using BOPCA.</sample>
    <sample id="227">The work uses **continual pre-training** on **existing pre-trained models**, specifically CamemBERT (a French generic model) and PubMedBERT (an English-based medical model), to build new ones.</sample>
    <sample id="228">GPT-4 is least aligned with African Islamic and Latin America.</sample>
    <sample id="229">The example sentence shown is "I am a student."</sample>
    <sample id="230">As the amount of tasks increases, the model achieves better performance (higher Max and Average performance) and lower sensitivity.</sample>
    <sample id="231">The authors compare their method with the following three treeless baselines:

1.  LSTM seq2seq
2.  T5
3.  Zheng and Lapata</sample>
    <sample id="232">Alexander Koller and Ivan Titov are Matthias Lindemann's advisors.</sample>
    <sample id="233">Chowdery</sample>
    <sample id="274">The speaker mentions 3 problems of the current SimulST models:
1.  Specific architectures are usually trained, introducing additional modules to be optimized.
2.  Long and complicated training procedures (e.g., different optimization objectives).
3.  Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s, ...).</sample>
    <sample id="275">Based on the provided content, there is no explicitly stated effective way to mitigate social and political biases in datasets. The speaker highlights a dilemma: not sanitizing leads to bias propagation and fairness issues, but attempting to sanitize is "incredibly hard" due to the difficulty of determining what is actually neutral and risks censorship or exclusion.</sample>
    <sample id="307">The fluency of PaLM is comparable to State-of-the-Art (SOTA) systems.</sample>
    <sample id="308">Based on the provided content, the important properties of a watermarking method are:

1.  **Applicability to EaaS:** The method should be applicable to Embedding as a Service.
2.  **Utility:** It should not degrade the utility of the provided embeddings.
3.  **Covertness:** It should be covert enough to the attacker, so they cannot easily detect or remove it.
4.  **Transferability:** The watermark needs to be transferable to the attacker's services during the model extraction process.</sample>
    <sample id="309">The English TED talks have been translated into the following 14 languages:

1.  Arabic (العربية)
2.  German (Deutsch)
3.  Spanish (Español)
4.  French (Français)
5.  Hebrew (עברית)
6.  Italian (Italiano)
7.  Japanese (日本語)
8.  Korean (한국어)
9.  Dutch (Nederlands)
10. Portuguese (Português)
11. Romanian (Română)
12. Russian (Русский)
13. Turkish (Türkçe)
14. Chinese (中文)</sample>
    <sample id="310">300 instances are sampled from a dataset for re-annotating.</sample>
    <sample id="311">The distance metrics used for measuring the difference between benign and backdoor datasets are **Delta Cosine (Δ_cos)** and **Delta L2 (Δ_l2)**.</sample>
    <sample id="312">Multilingual encoder-based models were evaluated in a **monolingual setting** and were used in two main configurations:

1.  **Enc-PTR (Encoder-Pointer):** These models consisted of **Multilingual Pretrained Encoders** (such as XLM-R or mBERT) coupled with Pointer-based Decoders.
2.  **Enc-Dec (Encoder-Decoder):** These were full **Multilingual Pretrained Encoder-Decoder Models** (such as mBART and mT5).

The models were evaluated across various datasets (MATIS, MGeoQuery, MSpider, etc.) to assess their performance.</sample>
    <sample id="313">Hi, I'm Siyu Yuan from Fudan University. I'm here to introduce our work, "Distilling Script Knowledge from Large Language Models for Constrained Language Planning."</sample>
    <sample id="314">In everyday life, humans often plan their actions by following step-by-step instructions in the form of ground script.</sample>
    <sample id="315">Previous work has explored language models to plan for abstract goals of stereotypical activities, such as make a cake, and show that large language models can effectively decompose goals into steps.</sample>
    <sample id="316">However, previous work mainly focuses on planning for the abstract goals of theoretical activities. Planning for the goals with specific goals, specific constraints, such as make a chocolate cake, still remains unstudied.</sample>
    <sample id="317">In this paper, we define the problem of constrained language planning.</sample>
    <sample id="318">which impose different constraints on the goals of planning. A abstract goal can be inherited by different real-life specific goals with multi-faceted constraints. A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="319">In this paper, we first evaluate and improve the constrained language planning ability of large language models.</sample>
    <sample id="320">So snow data set of specific goal exists to support our study.</sample>
    <sample id="321">We have to acquire these goals first. And showing in the table, we extend the abstract goals with multi-faceted constraints. For human loop data acquisition, use InstructGPT.</sample>
    <sample id="322">we sample 100 specific goals and evaluate the scripts generated from other models</sample>
    <sample id="323">This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals.</sample>
    <sample id="324">Then, we conduct detailed analysis to investigate why language models</sample>
    <sample id="325">Results in the figure shows that the semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) cannot be guaranteed.</sample>
    <sample id="326">We dig into a more fine-grained topic categories of constraints defined in WikiHow. The heatmap in the figure shows that the planning performance of InstructGPTs varies considerably for goals of different categories.</sample>
    <sample id="327">Previous studies have shown that the output quality of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over-generated then filter to improve generation quality.</sample>
    <sample id="328">I first show constrained types with examples for InstructGPT and obtain specific goals based on the seed abstract goals.</sample>
    <sample id="329">Then, Instruct GPT over generates case scripts for specific goals.</sample>
    <sample id="330">Next, a filter model is developed to select the faithful script.</sample>
    <sample id="331">We convert scripts and goals into InstructGPT embeddings, and calculate the cosine similarity as similarity scores to measure semantic similarity.</sample>
    <sample id="332">In addition, we award the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goals side.</sample>
    <sample id="333">With our method, InstructGPT can generate scripts of higher quality by a large margin. Our method greatly improves the planning ability both in semantics completeness and faithfulness to the constraint.</sample>
    <sample id="334">Since light language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. Creating data set is an essential step towards</sample>
    <sample id="335">However, previous studies do not enable planning for specific goals, and a manual data set annotation is expensive.</sample>
    <sample id="336">There, we follow the idea of symbolic knowledge distillation to distill a constrained language planning data sites from large language models.</sample>
    <sample id="337">We will apply our method for building a dataset of constrained language planning, named as Coscript.</sample>
    <sample id="338">In total, we generate 55,000 specific goals with scripts. To ensure the quality of validation and test sets, we ask crowd-sourced workers to find and revise the income in incorrect samples.</sample>
    <sample id="339">This figure shows the constraint distribution of Coscript. We find Coscript shows a high pluralism in the generated specific goals. With Coscript, we can train smaller but specialized models for constraint language planning.</sample>
    <sample id="340">We found that T5 fine-tuned on Coscript can generate scripts of higher quality than most large language models. Indicating that smaller models can surpass large larger models when properly trained on suitable data sets.</sample>
    <sample id="341">In summary, we established the constrained language planning problem. We evaluate the constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs.</sample>
    <sample id="342">We use large language models to generate a high-quality script dataset, CoScript, for constrained language planning. We hope CoScript dataset can be a valuable resource to advance the research on language planning with more complex and diverse goals and constraints.</sample>
    <sample id="343">Thanks for your time. Please find more details of Coscript in our paper.</sample>
    <sample id="344">The authors count the word frequency on a general text corpus (Dp) and then randomly select 'n' words that fall within a "moderate-frequency interval." However, the exact definition or range of this "moderate-frequency interval" is not specified in this slide or the spoken text.</sample>
    <sample id="371">Hello, I'm James Finch. And I'm Sarah Finch. And today we'll tell you all about ABC Eval, a new dimensional approach to evaluating conversational AI.</sample>
    <sample id="372">This work was done by the Emery NLP lab, led by Professor Gino Choy at Emery University, and in collaboration with Amazon Alexa AI.</sample>
    <sample id="373">So let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art.</sample>
    <sample id="374">The common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="375">these approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level.</sample>
    <sample id="376">One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses, using existing comparative or Likert scale methods.</sample>
    <sample id="377">However, we believe there is a more precise and reliable strategy for dimensional dialog evaluation.</sample>
    <sample id="378">Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="379">We call this approach Annotating Behaviors in Chat, or ABC Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature.</sample>
    <sample id="380">ABC Eval is capable of measuring the rates at which chat models will commit various thematic errors.</sample>
    <sample id="381">For example, ABC Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant.</sample>
    <sample id="382">contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy.</sample>
    <sample id="383">to determine what kind of evaluation is most effective. We selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval.</sample>
    <sample id="384">For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn level, Likert ratings on the dialogue level, and dialogue level pairwise comparisons.</sample>
    <sample id="385">for each of the existing methods. We collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions.</sample>
    <sample id="386">From our analyses of these evaluation results, we found that ABC Val behavior labels are overall more reliable than labels collected by existing methods, as measured by inner annotator agreement on a 100 doubly labeled conversations.</sample>
    <sample id="387">In addition, ABC eval labels are more predictive of the overall conversation quality, compared to metrics produced by existing methods, as shown by this simple linear regression analysis.</sample>
    <sample id="388">For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality respectively. While the average likert consistency scores explain only 4% or less.</sample>
    <sample id="389">Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression.</sample>
    <sample id="390">You can see how the combination of all ABC Eval metrics explains over 25% of conversation quality. And as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality.</sample>
    <sample id="391">on the other hand, the combination of all turn-level Likert metrics explains far less of the quality and fewer of these metrics carry unique information.</sample>
    <sample id="392">These reliable, informative and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve.</sample>
    <sample id="393">You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses.</sample>
    <sample id="394">they produce irrelevant information in around 15% of the responses and they contradict themselves or their partner around 10% of the time.</sample>
    <sample id="395">With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models.</sample>
    <sample id="396">We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction, and we look forward to seeing how conversational AI will advance in the coming months and years. Thanks for watching!</sample>
    <sample id="397">I'm sorry, but the provided video segment (0:00-0:01) only asks "What is our solution?" and does not contain information about the speech segment size used by the approach.</sample>
    <sample id="398">The entity-specific knowledge needed is "Servin is a judge" and "Kea is a baker."</sample>
  </task>
</testset>