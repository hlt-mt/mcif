<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="1015">In this figure, the blue line is cross-lingual few-shot transfer. The orange line is cross-lingual zero-shot transfer, while the green line is the monolingual setting.</sample>
    <sample id="1016">We found that by comparing the green and orange line, we found that for zero-shot setting, the cross-lingual transfer performance gap is significant. And by comparing blue and orange line, we found that for few-shot setting, the transfer gap is shortened rapidly.</sample>
    <sample id="1017">We also find some other interesting findings. For example, um encoder-decoder (mT5) outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of few-shot on target natural languages.</sample>
    <sample id="1018">Other Results &amp; Findings (Section 4 in Paper) • Enc-Dec (mT5) outperforms previous work or achieves comparable results. • Pretraining on the English NL can significantly boost the performance of few-shot on target NLs. • Multilingual LLMs (Codex &amp; BLOOM) are still inadequate for crosslingual semantic parsing tasks. • Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest. • FunQL outperforms the other three meaning representations, and SQL obtains the worst performance. and with our multilingual language models such as Codex and Bloom are still inadequate for cross lingual semantic passing tasks.</sample>
    <sample id="1019" />
    <sample id="1020">We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results shows many interesting findings and etc. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="1021">The most common errors for PaLM are "Accuracy/Omission" errors.</sample>
    <sample id="1048">Emory University (Emory NLP Research Lab) and Amazon Alexa AI.</sample>
    <sample id="1049">In this paper, CFT stands for **continuous fine-tuning**.</sample>
    <sample id="1050">There are 7 authors involved in the paper.</sample>
    <sample id="1084">The speaker's name is Yusen Zhang.</sample>
    <sample id="1085">Hi, I'm Shangbin, PhD student at the University of Washington. Today I'm presenting our work from pretraining data to language models to downstream tasks tracking the trails of political biases leading to unfair NLP models.</sample>
    <sample id="1086">So, language models are trained on large scale web corpora.</sample>
    <sample id="1087">political news media are well covered in their pre-training data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etc. are well covered in language model training data.</sample>
    <sample id="1088">This has created a mixed blessing for language model application.</sample>
    <sample id="1089">So on one hand, they were able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications.</sample>
    <sample id="1090">To this end, we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks, specifically by asking the following question.</sample>
    <sample id="1091">First, how do we evaluate the political leaning of language models? And what role does pretraining data might have on such political biases?</sample>
    <sample id="1092">Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications?</sample>
    <sample id="1093">So specifically, we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test. This ensures us to do automatic evaluation well-grounded in political science literature.</sample>
    <sample id="1094">So, some preliminary results demonstrate that first, language models do have varying political leanings. They occupy all four quadrants on the political compass.</sample>
    <sample id="1095">We can also see that GPT4 is the most liberal language model of them all, and GPT series are generally more socially liberal than BERT series and its variants.</sample>
    <sample id="1096">Secondly, we aim to investigate to to which extent the political biases of language models are actually picked up from training data.</sample>
    <sample id="1097">So, we conduct a controlled experiment by further pretraining language model checkpoints on six different partisan corpora separated into news and social media, further divided into their political leaning.</sample>
    <sample id="1098">By further pre-training language models on such partisan corpora, we can see that the ideological coordinates of the language model also correspondingly shift.</sample>
    <sample id="1099">For example, for RoBERTa further further trained on the left leaning Reddit corpus, we can see a substantial liberal shift in terms of its</sample>
    <sample id="1100">in terms of its political</sample>
    <sample id="1101">and we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society.</sample>
    <sample id="1102">So we divide pre-training corpora into pre-45th president of the United States and after 45th president of the United States. We separately pre-train language models on the two different temporal corpora.</sample>
    <sample id="1103">We can see that language models generally had a political leaning that is further away from the center after 2017. So this indicates that language models can also pick up the like polarization in our society.</sample>
    <sample id="1104">So, last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection, two NLP applications that often involve language models and could have very significant implications.</sample>
    <sample id="1105">So, we see that if we investigate the per-category performance, that is to say, if we separate the performance into</sample>
    <sample id="1106">different demographics or political leaning of news media, we can see a pattern that, for example, for hate speech detection, left-leaning language models are better.</sample>
    <sample id="1107">at detecting hate speech targeting socially minority</sample>
    <sample id="1108">However, our work detecting hate speech targeting more powerful groups in our society.</sample>
    <sample id="1109">And vice versa, Right-leaning language models are better at detecting hate speech targeting white and men. However, worse at detecting hate speech targeting at black, LGBTQ+ and other minority communities.</sample>
    <sample id="1110">Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa.</sample>
    <sample id="1111">we further show many qualitative examples to see that language models with different political leanings</sample>
    <sample id="1112">do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that.</sample>
    <sample id="1113">This indicates that there is a fairness issue that is very pressing regarding the political biases of language models.</sample>
    <sample id="1114">For example, if a ride language models were to be fine tuned on hate speech or misinformation or whatever, and deployed to a popular social media platform.</sample>
    <sample id="1115">This would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control.</sample>
    <sample id="1116">So, this has sounds the alarm for us to acknowledge and tackle the fairness issues resulted by language model political.</sample>
    <sample id="1117">So, a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It's like between Scylla and Charybdis.</sample>
    <sample id="1118">So, if we do not sanitize political opinions in language model training data, the bias would propagate from pre-training data to language models to downstream tasks, ultimately creating fairness issues.</sample>
    <sample id="1119">If we do try to sanitize somehow, we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retained in language model training data. So it's kind of like the electric electric trolley problem.</sample>
    <sample id="1120">Okay, great. I think that's pretty much all I have for that. for for today. Thank you for your time.</sample>
    <sample id="1121">The new method is called "Permuting with 'jumps'".</sample>
    <sample id="1122">The "marked words" method is used to find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">The authors are affiliated with:

*   University of Washington (Paul G. Allen School, UW NLP)
*   Carnegie Mellon University (Language Technologies Institute)
*   Tsinghua University</sample>
    <sample id="1124">The first mentioned symmetrical dependency structure is the Conjunction-headed/Prague approach.</sample>
    <sample id="1125">The speaker is James Finch.</sample>
    <sample id="1126">There are four authors involved in the paper.</sample>
    <sample id="1127">BLiMP and SyntaxGym can be used to test syntactic phenomena.</sample>
    <sample id="1128">Hello, my name is Kayo Yin, and I will be presenting our work titled When Does Translation Require Context? A Data-driven, Multilingual Exploration. This work was done in collaboration with Patrick Fernandes, Emmy Liu, Andre F. T. Martins, and Graham Neubig.</sample>
    <sample id="1129">So a lot of translations depend on context. For example, how would we translate mole in this sentence? Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole.</sample>
    <sample id="1130">Well, if the previous sentence was, "Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole." then mole refers to a spy. But if the previous sentence was, "Could it be anything serious, Doctor?" then mole refers to a birthmark.</sample>
    <sample id="1131">So depending on context, the meaning of the word changes and therefore its translation changes as well.</sample>
    <sample id="1132">However, evaluating how well models can translate cases like this is pretty hard. Firstly, because only a small portion of translations depend on context, which makes corpus-level metrics like BLUE unable to capture these translations.</sample>
    <sample id="1133">Evaluating context-dependent translation is hard. Only a small portion of words depend on context. Corpus-level metrics. Existing methods support limited discourse phenomena and languages since they usually rely on domain knowledge and human creation.</sample>
    <sample id="1134">In this work, we try to answer these two questions. First, when does translation require context? And second, how well do models handle these cases?</sample>
    <sample id="1135">To answer the first question, we started by measuring how much word depends on context of being translation.</sample>
    <sample id="1136">And in the previous work, we introduced CXMI as a measure for context use this by machine translation models. And this is done by measuring how much information the context C provides about the target Y given the source X.</sample>
    <sample id="1137">You can think of CXMI as the information gained from giving context to</sample>
    <sample id="1138">And this work, we extend CXMI to pointwise CXMI, which can measure context usage at the sentence level or at the word level. We can think of words that have high P-CXMI as ones that require context for translation.</sample>
    <sample id="1139">Now, we analyze words with high PCCMI to look for patterns between these words.</sample>
    <sample id="1140">And we perform our analysis on transcripts of TED Talks uh that have been translated from English to 14 different languages.</sample>
    <sample id="1141">We perform our analysis at three different levels. First, we look at part of speech tags that have high means P-CXMI.</sample>
    <sample id="1142">And this allows us to find, uh, for example, dual pronouns in Arabic that have relatively high P-CXMI. And this can be explained because, uh, English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic.</sample>
    <sample id="1143">And similarly, we find that certain languages also require context when we wanted to choose the appropriate verb form. We then look at vocabulary items that have high PCXMI average over all of its different occurrences.</sample>
    <sample id="1144">And this helps to identify cases like the one here, where in Chinese, you need context to translate proper nouns uh to make sure that you're using the same translation within the document.</sample>
    <sample id="1145">And similarly, we find that context is supported to translate in the right order.</sample>
    <sample id="1146">**On-Screen Text:**

*   Thematic analysis of high P-CXMI words
*   1. POS tags
*   2. Vocabulary items
*   3. Individual tokens
*   Pronouns
*   Verb form
*   Lexical cohesion
*   Formality
*   Ellipsis
*   Avelile's mother was still asleep.
*   Avelile went to school.
*   She knows where we're going.
*   I don't.

**Spoken Text:**

*   (0:00) And finally, we look at different um, at individual tokens that have high P-CXMI.
*   (0:06) And this allows us to identify phenomena that cannot really be captured uh by the word itself, but that's rather expressed in the sentence structure, such as ellipsis resolution.</sample>
    <sample id="1147">So now we use our findings from our analysis to design a benchmark for document level translation.</sample>
    <sample id="1148">For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon. And we call our tagger the Multilingual Discourse-Aware or MuDA tagger.</sample>
    <sample id="1149">We can then also note that different languages have different proportions of these discourse phenomena.</sample>
    <sample id="1150">We then use the MuDA tagger, by applying the tagger on a parallel corpus that we want to use for evaluation. And we apply our translation metrics of choice on the context dependent examples that the MuDA tagger has identified.</sample>
    <sample id="1151">And finally, uh, we use, um, our benchmark as well as other metrics to evaluate different models, um, on document-level machine translation.</sample>
    <sample id="1152">First of all, when we use corpus-level metrics, uh so for blue, we find that context-agnostic models have the best performance.</sample>
    <sample id="1153">But then if we use COMET, context-aware models perform best. And if we use word F-measure, then models with or without context have comparable performance.</sample>
    <sample id="1154">This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone.</sample>
    <sample id="1155">Now we use the MUDA benchmark to evaluate models, and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="1156">But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So, this sort of suggests uh where we would need to see more progress for document-level translation.</sample>
    <sample id="1157">We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document level translation.</sample>
    <sample id="1158">To summarize, we perform a data-driven analysis across 14 language pairs to identify one translation required context.</sample>
    <sample id="1159">And then we use our findings to build a benchmark for document-level machine translation, which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation.</sample>
    <sample id="1160">Thank you so much for your attention. See you.</sample>
    <sample id="1161">FT_w, BOND, COSINE, MLC, L2R</sample>
    <sample id="1162">The model is evaluated on 11 biomedical and clinical downstream tasks.</sample>
    <sample id="1163">Hi. Welcome to our presentation of DEPLAIN. A new corpus for German text simplification on the document level and on the sentence level.</sample>
    <sample id="1164">My name is Regina Stodd and I will guide you through the first part of the presentation. Let's first define text simplification.</sample>
    <sample id="1165">Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group, as people with reading problems or non-native speakers.</sample>
    <sample id="1166">To train a text simplification model, we require parallel pairs of text, for example, of documents or sentences.</sample>
    <sample id="1167">And an example here. You can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language.</sample>
    <sample id="1168">To simplify the sentence, different techniques are possible, as you can see in the example. Such as lexical substitution, clause deletion, reordering, or insertion of words.</sample>
    <sample id="1169">We now propose our new corpus DE-plain. Because in the recent years there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on.</sample>
    <sample id="1170">yellow three models which are proposed in recent years are all automatically aligned, which means there can be error prone and their alignment.</sample>
    <sample id="1171">Therefore, we propose our new corpus D Plane, which is split into two sub corpora, D plane APA and D plane web. D plane APA is based on news texts.</sample>
    <sample id="1172">in the plain APA. We aligned 483 documents, all manually. It results in roughly 30,000, 13,000 parallel sentence pairs.</sample>
    <sample id="1173">For D plan web. This corpus include different domains. And we also align all of these 750 documents uh on the one hand manually and on the other hand with automatic alignment methods.</sample>
    <sample id="1174">In total, we result in 3,450 sentence pairs.</sample>
    <sample id="1175">We analyzed our sentence pairs a little bit more. So for example, on the type of simplication.</sample>
    <sample id="1176">As you can see here, the Bible texts are much stronger simplified than for example the news text, or the language learner text.</sample>
    <sample id="1177">on all level regarding, for example, lexical simplification, structural simplification, also overall level of simplification.</sample>
    <sample id="1178">Furthermore, you can see that our DeepLain corpus has a high variety of different simplification transformations. So, for example, in the DeepLain API corpus, we have much more reordering and word additions than we have in the DeepLain web corpus.</sample>
    <sample id="1179">On the other hand, in the web corpus, we have much more rephrasing.</sample>
    <sample id="1180">So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our data set D plane. So, for the first use case, we can evaluate automatic alignment methods.</sample>
    <sample id="1181">In the recent years, there has been a lot of alignment methods, but in the context of machine translation.</sample>
    <sample id="1182">where we have two parallel documents written in different languages and we want to extract alignments of sentences in post documents.</sample>
    <sample id="1183">but in our use case uh we are trying to extract alignments between sentences of two parallel documents having the same language having the same content but they are on a different complexity level.</sample>
    <sample id="1184">And now, as we have our data set deepline, which have uh manually uh aligned sentences. Uh, we can use these sentences as gold standard alignments to evaluate some of the proposed uh alignment methods.The user wants a transcript of the audio. I will transcribe the spoken words directly.</sample>
    <sample id="1185">And we did some adaptations to the proposed methods, and we have published all these adaptations and the codes to run our experiments in the paper.</sample>
    <sample id="1186">At the end, we concluded that the best alignment automatic alignment method to use for text for German text simplification is the method of mass align.</sample>
    <sample id="1187">and you can also find the code to run this method on your own documents in the paper.</sample>
    <sample id="1188">The second use case that we showed in our paper is a case of automatic text simplification.</sample>
    <sample id="1189">By fine-tuning language models to produce simplified text from the complex input.</sample>
    <sample id="1190">We have fine tuned two different models. Uh we have fine tune the model of long impart to produce uh document level simplifications.</sample>
    <sample id="1191">and we also find tune the normal base long the normal base import to produce sentence level simplification.</sample>
    <sample id="1192">You can also find all the checkpoints and uh, you can look into more details at the scores and the evaluation metrics of our experiments in the paper.</sample>
    <sample id="1193">We concluded that this this basic fine-tuning could produce, uh, or could get, uh, scores better than the baseline scores.</sample>
    <sample id="1194">And we propose those results as a benchmark, a base benchmark for the problem of automatic text simplification in the future.</sample>
    <sample id="1195">Thank you so much for your attention, and we hope to meet all of you during the conference. Thank you.</sample>
    <sample id="1196">Hi. I'm going to talk about our work on resolving indirect referring expressions for entity selection, in which we introduce the AltEntities Corpus.</sample>
    <sample id="1197">my name is Javad Hosseini and this is a joint work with Philip Radlinski, Silvia Party, and Annie Louis.</sample>
    <sample id="1198">Our goal is to understand user's language when they want to make a choice. Consider this alternative question. Did you mean easy on me or I gotta feeling? Here, a user wants to select between one of these two songs.</sample>
    <sample id="1199">The most obvious thing is to use a direct reference, for example, by saying the name of the uh song, easy on me, or its position, the first one.</sample>
    <sample id="1200">But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name,</sample>
    <sample id="1201">or the pronunciations are too similar to each other and hard to disam big</sample>
    <sample id="1202">or when the user wants to specify a preference. Here are some example indirect references. For example, the newer one or the song that's not energetic.</sample>
    <sample id="1203">This is an important problem in conversational systems and also for benchmarking LLM's entity understanding.</sample>
    <sample id="1204">We are not aware of a public dataset, a large-scale public dataset for the task. So we collect one using crowd annotation. Our dataset covers three different domains: music, books, and restaurants.</sample>
    <sample id="1205">Our dataset collection methodology emphasizes informality using a cartoon completion task.</sample>
    <sample id="1206">The cartoon has three speech bubbles. In the first bubble, Bob says, "Remember that song we were listening to yesterday?" And with that, Bob sets the dialogue context.</sample>
    <sample id="1207">in the in the second speech bubble Alice says, "Do you mean 'Easy on Me' or 'I Gotta Feeling'?"</sample>
    <sample id="1208">which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, the new one.</sample>
    <sample id="1209">We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain.</sample>
    <sample id="1210">The second one, which is the alternative question, is generated as follows.</sample>
    <sample id="1211">We always use a simple template: Do you mean A or B? Where A and B are sampled from Wikipedia.</sample>
    <sample id="1212">Here are the different sampling methods we've used. When we move higher in the list, the entities become more similar to each other, and it's usually harder to make the</sample>
    <sample id="1213">The first one is uniform.</sample>
    <sample id="1214">The second one is when the entities have similar titles. For example, two books with the name The Return.</sample>
    <sample id="1215">The third one is when they have similar descriptions on Wikipedia. And finally when they have similar info boxes or attributes on Wikipedia. For example the same genre or the same artist for a song</sample>
    <sample id="1216">When we show this alternative uh question to the annotators, they know the name of this entities, but they don't necessarily know about the entit</sample>
    <sample id="1217">So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song.</sample>
    <sample id="1218">And then ask the annotators to listen to at least some of each song and read about each song. Here's for example the Google search result for the song Easy on Me.</sample>
    <sample id="1219">For the recipes and books domain, we show some background text uh from Wikipedia. For recipes, we additionally show their images again from Wikipedia, so that the annotators know how they look like.&lt;/red&gt;</sample>
    <sample id="1220">Then we ask the annotators to pick one of these entities, for example, here the first one, and describe them using three to five indirect referring expressions.</sample>
    <sample id="1221">for example, the one with the piano music. Here are some examples from our dataset. For example, the one without words, not the one with the 12 year old 12 year old boy or the fictional one or comes from Azerbaijan and so on.</sample>
    <sample id="1222">The AltEntities corpus has 6,000 alternative questions across the three domains. And it has 42,000 indirect referring expressions. Results with T5 XL large model are summarized below:</sample>
    <sample id="1223">If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high. It's around 92 to 95%. But this is not realistic.</sample>
    <sample id="1224">if the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic. For example, when um the language model retrieves the background knowledge.</sample>
    <sample id="1225">if the language model has access only to entity names, then the accuracy is only 60%. So there is a lot of room for improvement. We've also shown that the models are domain-generalizable. Here is a link to our dataset. Thank you.</sample>
    <sample id="1226">The audio does not explicitly state the data CamemBERT was *initially* trained on. It mentions an experiment where "CamemBERT-based models" undergo "continual pretraining" on a "4 gigabyte subset of Nachos."</sample>
    <sample id="1227">The speaker's name is Adam Przepiórkowski.</sample>
    <sample id="1228">The finding that "performance degrades with larger temporal gap" led to the conclusion that temporal drift is the main cause of performance drop.</sample>
    <sample id="1229">Hi everyone, I'm Jenny, a first year PhD student at Carnegie Mellon University, and today I'll be presenting our work NLPositionality: Characterizing Design Biases of Datasets and Models.</sample>
    <sample id="1230">This work was done in collaboration with some folks at the University of Washington and um the Allen Institute for AI, namely Sebastian Santy, Ronan Lebras, Catarina, Reica, and Martin Sap.</sample>
    <sample id="1231">So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article, trying to remove toxic content.</sample>
    <sample id="1232">Imagine... You might turn towards a popular API, like Perspective API for toxicity detection. And this works really well if you're Carl Jones. Um where Perspective API is able to detect correctly toxic instances.</sample>
    <sample id="1233">But that's not really the case for Aditya Sharma, where Perspective API is really not as sensitive to offensive terms that are more common in Indian contexts.
Imagine...
Carl Jones
Tech Lead,
New York Times
Can you stop being a jerk? 👁️: (0.82) ✅
Aditya Sharma
Tech Lead,
Times of India
Presstitutes everywhere on the news. 👁️: (0.33) ❌
👁️ = PerspectiveAPI score</sample>
    <sample id="1234">This is an example of a design bias where we see systematic performance differences of technology between populations.</sample>
    <sample id="1235">Designed biases like the one that we just saw before might occur due to the positionality of the NLP researchers and model developers. Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="1236">This is a concept widely used in critical studies, specifically in feminist and queer academic spaces.</sample>
    <sample id="1237">And as a researcher, positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make.</sample>
    <sample id="1238">And so one question that people might ask is, do datasets and models have positionality?</sample>
    <sample id="1239">And we're not trying to say that models themselves and data sets themselves have demographic identities and life experiences, but they do aggregate judgments and opinions of real people and can thus represent certain positionalities over others.</sample>
    <sample id="1240">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. [2] Yin et al. "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022. [3] Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022.</sample>
    <sample id="1241">**Slide Content:**

*   **Do datasets and models have positionality?**
*   **Anecdotal evidence:**
    *   Model and dataset probing [1][2]
    *   Theoretical definitions of model positionality [3]
*   [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022.
*   [2] Yin et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022.
*   [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” CHI 2022.

**Spoken Content:**

"However, these works really don't look at comparing end users with the datasets and models themselves."</sample>
    <sample id="1242">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. [2] Yin et al. "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022. [3] Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022. And studying model and dataset positionality is increasingly important as NLP tasks become more subjective and socially oriented.</sample>
    <sample id="1243">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [1][2] - Theoretical definitions of model positionality [3] [1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. [2] Yin et al. "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022. [3] Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022. and it's challenging to characterize how these positionalities are skewed because not all decisions are documented, and many models are hidden behind APIs.</sample>
    <sample id="1244">So, to study dataset and model positionality, we actually compare the annotations with real users with existing datasets and models.</sample>
    <sample id="1245">We do this through our framework, NLP positional.</sample>
    <sample id="1246">Our framework works in two main steps.</sample>
    <sample id="1247">The first step is to re-annotate datasets with diverse annotators.</sample>
    <sample id="1248">And we ought to do this over looking at the demographics of original data sets, um, annotators, because usually only a few annotators annotate each instance and because demographics are rarely collected and shared.</sample>
    <sample id="1249">And so we opt to reannotate data to get many annotators for instance and to get a rich set of demographic data.</sample>
    <sample id="1250">We then take the annotations by demographic and compare them to the models and data sets using a Pearson's R correlation score.</sample>
    <sample id="1251">And thus, our framework actually differs from annotator disagreement literature by comparing end users with models and datasets predictions and labels, as opposed to looking at just inter-annotator agreement or modeling, um, annotator distributions.</sample>
    <sample id="1252">Our framework is largely enabled through LabintheWild, an online crowdsourcing platform for our HCI collaborator.</sample>
    <sample id="1253">In LabintheWild is an online experimentation platform where we can recruit diverse volunteers um compared to like platforms like MTurk, which largely have participants from the US or India. And further LabintheWild still's able to get high-quality data.</sample>
    <sample id="1254">We host two tasks on Lab in the Wild, one of them being social acceptability. Participants read a situation from the Social Chemistry dataset. And the way this works is that participants will read a situation from the social chemistry dataset, and then they'll rate how socially acceptable a situation is. Participants compare their responses to others' and an AI's.</sample>
    <sample id="1255">Afterwards, to stay engaged in the study, they can compare their responses to an AI and others.</sample>
    <sample id="1256">We then compared these annotations with social chemistry, Delphi and GPT-4.</sample>
    <sample id="1257">We then replicated a very similar setup for the toxicity and hate speech detection task, where they'll read an instance from Dynehate and rate whether they think it's an instance of hate speech.</sample>
    <sample id="1258">Here's the transcription of the provided video:

**Spoken:**
We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate RoBERTa, and GPT-4. Our study in the end amassed over 16,000 annotations from over 1,000 annotators from 87 countries.

**On-screen text:**
**Task B: Toxicity**
**Analysis**

**Datasets**
- Dynahate

**Models**
- Perspective API
- Rewire API
- Hate RoBERTa
- GPT-4

**Study Participation**
16,299 annotations
1,096 annotators
87 countries</sample>
    <sample id="1259">So, now we're better equipped to answer, who do NLP datasets and models align with the most? We find that there is positionality in NLP.</sample>
    <sample id="1260">For example, we find that data sets and models are most aligned to English-speaking countries. So, for the GPT-4 social acceptability analysis, we find that it's most aligned to Confuciuan and English-speaking countries. We find that Dynahate is also most aligned to English-speaking countries.</sample>
    <sample id="1261">We also find most additional alignment with people who have a college education. So, for GPT-4 in the social acceptability task, we find that it's most aligned to people with a college education or graduate school education.</sample>
    <sample id="1262">And we find the same for DynaHate where it's most aligned to people with a college education.</sample>
    <sample id="1263">However, when models and data sets are aligned to specific populations, some are inevitably left behind.</sample>
    <sample id="1264">An example of this is that data sets and models are less aligned to non-binary people compared to their men and women counterparts. We find this in the GPT-4 social acceptability task, as well as the Dynamate task analysis as well.</sample>
    <sample id="1265">So, what can we do? Addressing positionality in NLP. So, given that there is positionality in NLP, what can we do about it?</sample>
    <sample id="1266">So we have a few recommendations for this. The first one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research through the lens of perspectivism.</sample>
    <sample id="1267">Our third recommendation is to build specialized datasets and models with and for specific communities, and a good example of this is the Masakhane initiative. Um and we want to emphasize that inclusive NLP isn't just making, you know, all technologies work for everyone.</sample>
    <sample id="1268">And so that concludes our presentation, but if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.</sample>
    <sample id="1269">It is necessary to permute the tokens because after the first step, the tokens are not in the correct order. The permutation step puts them into the right order.</sample>
    <sample id="1270">The authors recommend increased transparency about bias mitigation methods because they don't know if observed patterns, such as positive stereotypes, are due to "overly excessive value alignment" or if existing anti-stereotyping methods are inadvertently leading to "pernicious patterns." Transparency would help understand the true causes and effects of these methods.</sample>
    <sample id="1271">Based on the slide, the minimal-pair unacceptable inputs are:

1.  **BLiMP:** "Many people were helping herself."
2.  **SyntaxGym:** "*The customer ... has spent any money."
3.  **CrowS:** A "Non-stereotypical sentence" (no specific example is provided on the slide).</sample>
    <sample id="1272">The authors used the following evaluation metrics:

*   **F1-score** (F1)
*   **Hamming loss**
*   **Exact Match Ratio** (EMR)</sample>
    <sample id="1273">The metric used for measuring inter-annotator agreement was **Krippendorff's Alpha**.</sample>
  </task>
</testset>