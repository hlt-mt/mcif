<?xml version='1.0' encoding='utf-8'?>
<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是大型网络爬取数据，特别是政治新闻媒体。</sample>
    <sample id="1">这篇论文的作者分别来自麦吉尔大学、Mila和微软研究院。</sample>
    <sample id="2">大家好，我是来自蚂蚁集团的图一。我将展示我们团队关于文档理解的论文。本文的合著者都是蚂蚁集团的算法工程师，本文是我们工作实践的成果。

在本文中，我们关注视觉丰富文档理解问题，它旨在理解各种类型的文档，如表格、收据和海报。近年来，预训练技术已引入该领域，自监督预训练模型在该领域取得了巨大成功。然而，现有的文档预训练模型存在阅读顺序问题。遵循BERT的思路，这些方法通常采用升序数字，如0、1、2来表示文档中标记的全局阅读顺序，我们称之为全局1D位置。在图1中可以看到全局1D位置的图示。

我们提出了一个新的多模态预训练模型LayoutMask，以解决这些问题。LayoutMask仅使用文本和布局信息作为模型输入，旨在增强文本-布局交互，并在预训练期间学习布局表示。它与以前的研究在三个方面有所不同：1D位置的选择、遮蔽策略和预训练目标。LayoutMask不是使用全局1D位置，而是建议使用链接段的标记顺序作为1D位置，我们称之为局部1D位置。由于局部1D位置不提供跨段顺序，LayoutMask通过联合使用1D位置、2D位置和语义信息，推断全局阅读顺序，从而深入学习文本-布局交互。

为了进一步促进这种交互，我们用两种新的遮蔽策略改进了常用的预训练目标——遮蔽语言建模：全词遮蔽和布局感知遮蔽。遮蔽语言建模任务是多模态预训练中最基本和常用的预训练任务。在全词遮蔽中，我们不是遮蔽标记级别，而是遮蔽词级别，这更具挑战性。通过使用全词遮蔽，模型必须寻找更多上下文来预测遮蔽词，从而促进文本-布局交互。在布局感知遮蔽中，每个段的第一个和最后一个词被遮蔽的概率更高，因此模型必须更关注在前序或后序段中寻找上下文，从而促进跨段顺序的学习。我们还设计了一个新的预训练目标——遮蔽位置建模。它具有相同的预训练目标，并在预训练期间恢复随机遮蔽的2D位置。MPM任务与CLODE任务非常相似，其中一组随机选择的词被假定在原始文档中的正确位置被填充。模型必须根据语义关系和来自空间透视的2D位置线索，为每个词找到上下文。这种联合学习过程，结合了语义和空间推断，可以促进文本-布局交互，并帮助模型学习更好的布局表示。

在我们的实验中，我们比较了LayoutMask在不同布局信息下的性能。对于1D位置，局部1D位置在FUNSD和SROIE上优于全局1D位置，在CORD上略微落后。局部与全局之间的性能差距主要来自实体总计，而其他实体类型具有相似的F1分数。我们在附图中展示了SROIE数据集的两个示例及其实体标注。右侧图像包含实体总计，既有垂直布局，也有水平布局，并且有多个与真实值内容相同的误导性数字。因此，通过全局1D推断出的普通阅读顺序，很难识别实体总计。基于这些结果，我们可以得出结论，使用局部1D可以表现更好，因为它更适应这种情况。关于论文的更多详细信息，请参阅我们的论文和海报。

感谢观看！如果您有任何问题，请给我发电子邮件。</sample>
    <sample id="3">00:00
大家好，欢迎观看我们 DEPLAIN 的演示。这是一个用于在德语文本中进行文本简化的新语料库，它包括文档级别和句子级别的简化。

00:11
我叫 Regina Stodden，我将引导您完成演示的第一部分。

00:18
首先我们来定义一下文本简化。

00:21
文本简化是一个修改文本以改善特定受众（例如有阅读障碍的人或非母语人士）对文本理解的过程。为了训练文本简化模型，我们需要成对的并行文本，例如文档或句子。在这个示例中，您可以看到一个复杂的德语句子及其简化的版本。

0:50
要简化这个句子，有多种方法，如您在示例中看到的：词汇替换、从句删除、重新排序或插入词语。

01:04
现在我们来介绍一下 DEPLAIN 这个新语料库。

01:07
近年来，在处理现有语料库时遇到了一些问题。例如，这些语料库太小，无法用于训练文本简化模型。

01:19
其他三个在近年提出的模型都是自动对齐的，这意味着它们的对齐可能容易出错。

01:28
因此，我们提出了新语料库 DEPLAIN，它分为两个子语料库：DEPlain-APA 和 DEPlain-web。DEPlain-APA 基于新闻文本。

01:41
在 DEPlain-APA 中，我们手动对齐了 483 篇文档。这产生了大约 13000 对并行句子。

01:56
DEPlain-web 语料库包含不同的领域。我们也对齐了所有这 750 篇文档，一部分是手动对齐的，一部分是自动对齐的。总共有 30450 对句子。

02:17
我们对句子对进行了更多分析，例如，关于简化类型。

02:22
如您所见，与新闻文本或语言学习者文本相比，圣经文本在词汇简化、结构简化和整体简化方面都得到了更大幅度的简化。

02:43
此外，您可以看到我们的 DEPlain 语料库包含多种不同的简化转换。例如，在 DEPlain-APA 语料库中，我们有更多的重新排序和词语添加，而在 DEPlain-web 语料库中，我们有更多的复述。

03:04
现在我们来看看这个语料库的用例。

03:08
大家好，我是 Omar，现在我将介绍我们的数据集 DEPLAIN 的用例。第一个用例是我们可以评估自动对齐方法。近年来，有许多对齐方法，但它们都是针对机器翻译的，即我们有两个用不同语言编写的并行文档，我们想从中提取句子在两个文档中的对齐。但在我们的用例中，我们尝试提取两个并行文档中句子的对齐，这些文档使用相同的语言，但它们的复杂性级别不同。现在我们有了手动对齐的句子 DEPLAIN 数据集，我们可以使用这些句子作为黄金标准对齐来评估一些提出的对齐方法。我们对提出的方法进行了一些调整，并在论文中发布了所有这些调整和运行我们实验的代码。

04:19
最后，我们得出结论，用于文本简化的最佳自动对齐方法是 MASAlign。您还可以在论文中找到运行此方法在您自己的文档上的代码。

04:40
我们论文中展示的第二个用例是通过微调语言模型来自动简化文本，从复杂的输入文本中生成简化的文本。我们微调了两种不同的模型：我们微调了 Long-mBART 模型以生成文档级别的简化，我们还微调了正常基于 mBART 的模型以生成句子级别的简化。您还可以在论文中找到所有检查点以及查看我们实验分数和评估指标的更多详细信息。

05:30
我们得出结论，这种基本的微调可以产生比基线更好的分数。我们提出这些结果作为未来自动文本简化问题的基准。

05:50
非常感谢您的关注，我们希望能在 ACL 2023 会议的海报区见到大家。谢谢。</sample>
    <sample id="4">演讲者的名字是Kayo Yin。</sample>
    <sample id="5">为了获得 82%-87% 的准确率，他们使用了 T5 XL 模型，并为其提供了部分重叠的背景知识。</sample>
    <sample id="6">这篇论文介绍了 Many-to-Many Summarization (M2MS)，这是一个新的多语言和跨语言摘要统一框架。M2MS 旨在于构建一个单一的摘要模型，以处理任何源语言的文档并生成任何目标语言的摘要。论文还对多语言摘要 (MLS)、跨语言摘要 (CLS) 和 M2MS 进行了初步研究，结果表明 M2MS 有助于模型在不同语言之间更好地传递任务知识。此外，论文提出了一种预训练的 M2MS 模型，名为 PISCES，它通过一个精心设计的三阶段预训练过程学习语言建模、跨语言能力和摘要能力。实验结果表明 PISCES 在各种基准测试中优于现有的强基线模型，包括 mBART-50 和 mT5。</sample>
    <sample id="7">是的，CoNLL-2003 标注器仍然有效。</sample>
    <sample id="8">提出的新的人工评估方法是 ABC-Eval（在聊天中注释行为），它通过明确注释每个模型响应是否表达了特定行为来减少人类评估的主观性，这些行为包括：
*   **连贯性（Coherence）**：忽略伙伴、不相关。
*   **一致性（Consistency）**：自我矛盾、伙伴矛盾。
*   **知识（Knowledge）**：不正确的事实、常识违反。
*   **情感理解（Emotional Understanding）**：富有同情心的回应、缺乏同情心。

这个方法测量了聊天模型犯下各种主题错误的比率，从而对模型在这些细粒度维度上的优缺点提供更精确和可靠的理解。</sample>
    <sample id="9">现有弱监督方法的成功在很大程度上依赖于清理过的验证样本。</sample>
    <sample id="10">为了提高分数，需要提供给语言模型更多的背景知识，包括实体名称、部分重叠的背景知识，以及与注释者所用的完全相同的背景知识。当语言模型只能访问实体名称时，准确率仅为 60%。当语言模型能够访问部分重叠的背景知识时，准确率在 82% 到 87% 之间。当语言模型能够访问与注释者所用的完全相同的背景知识时，准确率会达到 92% 到 95% 之间。</sample>
    <sample id="11">研究科学家 Jack Hessel 发表了一场关于衡量幽默理解能力的AI研究。他引用了两个模型：ChatGPT和谷歌的PaLM模型。ChatGPT能够生成笑话并尝试解释它们，而PaLM模型则可以解释复杂的笑话，例如关于TPU的笑话。尽管这些模型展示出幽默理解能力，但Hessel对它们是否真正理解幽默表示怀疑，并举了一个ChatGPT未能正确理解双关语的例子。

为了进一步探索AI幽默理解能力的局限性，Hessel的研究团队使用了《纽约客》杂志的卡通字幕比赛的数据。这个比赛每周都会发布一幅没有字幕的卡通，读者提交字幕，然后编辑选出三个决赛选手，最终由公众投票选出获胜者。这项比赛从20世纪90年代中期开始举办，深受大众欢迎，甚至吸引了名人参赛。

研究团队将这些数据转化为三个任务：
1. **匹配：** 给模型提供五段字幕，其中只有一个是与卡通对应的原始字幕，模型需要选出正确的字幕。
2. **质量排名：** 给模型提供两段与卡通相关的字幕，其中一段被人类评为更高质量的，模型需要选出更好的那段。
3. **解释生成：** 模型需要为给定的卡通和字幕生成一个两到四句话的解释，说明为什么这个笑话好笑。

为了支持这些计算实验，团队收集了超过700幅卡通的额外注释，包括位置、描述、离奇之处和实体链接。他们还收集了超过650段两到四句话的笑话解释。

研究结果显示，在匹配任务中，Hessel团队微调过的CLIP模型达到了62.3%的准确率，远高于20%的随机猜测基线。然而，人类在此任务上的表现高达94%，表明AI在幽默理解方面仍存在显著差距。即使在给定人类作者描述的图像的情况下，GPT-4在匹配和质量排名任务上的表现也与人类有很大差距。在解释生成任务中，人类在三分之二的测试中，更偏好人类生成的解释而非GPT-4生成的解释。

总的来说，Hessel表示希望看到社区如何利用他们发布的数据集、排行榜和模型，并期待在ACL会议上见到大家。</sample>
    <sample id="12">这篇论文有5位作者。</sample>
    <sample id="13">讲者介绍了一种名为“自适应推理”的方法，旨在通过利用数据复杂度的差异来降低大型语言模型（LLMs）的推理时间。这种方法通过为“简单”样本使用低容量模型来降低平均推理成本（时间/金钱）。
演讲中主要对比了两种自适应推理方法：“多模型”和“早期退出”。

“多模型”方法包括多个独立的模型，每个模型在末端都有一个分类器，分别在整个训练集上进行训练。推理时，这些模型按顺序运行，直到其中一个分类器决定停止计算。
“早期退出”方法则是在一个单一模型中加入多个分类器，这些分类器连接在不同的Transformer层之后。所有分类器同时训练，推理时，样本通过模型，直到某个分类器决定停止计算。

讲者指出了这两种方法的优缺点：
“多模型”的优点是通用性强，易于扩展；缺点是存储成本高，并且存在开销（如果模型需要运行到最后，之前所有模型的计算都浪费了）。
“早期退出”的优点是推理速度快（无开销），内存效率高；缺点是所有分类器共享模型参数，这可能导致“冲突梯度”问题。

冲突梯度假说：每个分类器都会更新模型权重，试图优化自己的目标（损失函数）。来自不同分类器的梯度信号可能会相互干扰，从而降低所有分类器的性能。
为了验证这一假说，讲者比较了独立训练的“多模型”分类器和“早期退出”分类器的性能，它们都基于BERT预训练语言模型。结果显示，“多模型”分类器在性能上平均优于“早期退出”分类器2.3%，对于最早的分类器，这一差距甚至高达5.2%。在速度-准确性权衡方面，对于高推理速度，“多模型”表现更好。

为解决冲突梯度问题，讲者提出了“SWEET”（Separating Weights in Early Exit Transformers）方法。SWEET是一种新的早退架构微调方法，其中每个层仅接收来自其后续分类器的更新，从而完全避免了冲突梯度问题。
SWEET方法的实验结果显示，它显著缩小了“早期退出”和“多模型”之间的性能差距，尤其是在早期退出层。然而，在某些情况下，较后的分类器会受到负面影响。在速度-准确性权衡方面，SWEET在高速度时表现优于其他两种方法，对于BERT_LARGE模型，SWEET在整个速度-准确性曲线上都优于其他方法。

总结：
1. 早期退出训练过程中存在冲突梯度，导致未来分类器的梯度对齐，暗示着它们有相似的目标。
2. 在对早期退出和多模型自适应推理方法的公平比较中，多模型分类器表现更好，而早期退出在速度-准确性权衡方面表现更优。
3. SWEET方法有利于早期退出模型实现高加速比。它可以应用于其他退出策略、架构和微调方法。SWEET激励了未来对专门为早期退出架构量身定制的微调算法的研究。</sample>
    <sample id="14">你好，我的名字是 Adam Przepiórkowski，这次演讲是关于并列结构的依存关系。正如你所知，不同的理论和语料库方法对协调结构有不同的依存关系。例如，在通用依存关系中，并列结构 Lisa、Bart 和 Maggie 的结构是第一个连词是整个并列结构的头部。在这种情况下，Lisa。Igor Milchuk 的意义-文本理论也采用了类似的方法，同样，整个并列结构由第一个连词引导。因此，这两种方法是不对称的。它们挑选出一个连词。现在，还有对称的并列结构方法，例如布拉格方法，即布拉格依存关系句法库中采用的连词-中心方法，其中并列结构由连词引导。因此，我们从 and 到所有连词都存在依存关系。最后，还有一种多中心方法，例如在 Katzner 的词法语法中使用的方法，其中所有连词都是并列结构的中心。因此，我们从主语（此处是 loves）到所有连词都存在依存关系。Lisa、Bart 和 Maggie。本文的目的是为并列结构的对称结构提供一个新颖的论点，例如这两种结构，并反对不对称的并列结构，例如这两种结构。好的。这个论点基于依存关系长度最小化原则，我将在这些例子中解释。所以，在英语中，你可能知道，直接宾语倾向于靠近动词，而状语可以更远。对吧？所以 Marge read it yesterday 很好，因为直接宾语 it 靠近动词，而 Marge read yesterday it 则差得多，因为在动词和直接宾语之间有一个状语 yesterday。然而，当直接宾语很重、很长时，这种效果可能会减轻，因为它可以在状语之后移动。这在这里得到了说明。所以这两个句子都很好，Marge read this absolutely fascinating book about bees yesterday 很好，因为我们用这个长而重的宾语代替了 it。但 Marge read yesterday this absolutely fascinating book about bees 也可以说。所以这里的理由是，这是可能的，因为尽管这个句子违反了直接宾语应该靠近动词的一般语法原则，但它满足了依存关系长度最小化原则，即更短的依存关系是首选的。所以这两个树只显示了关键依存关系的长度，也就是在这两个结构中不是常量的那些。所以，这里我们有一个从 read 到状语的依存关系，长度为 7（以单词衡量），以及从 read 到 book 的依存关系，长度为 4。所以总共是 11。当你移动，当你交换这两个成分时，这两个依存关系的长度之和变为 6。所以不是 11 而是 6，短得多，这就是为什么这听起来很不错，对吧？它违反了一个原则，但它满足了另一个原则。好的。所以我们从增强版的 Penn Treebank 中提取了关于并列结构的各种统计数据，请参阅论文，了解为什么我们没有使用通用依存关系，这些统计数据证实了以前多次观察到的现象，即左连词往往更短，所以 salt and pepper 而不是 pepper and salt（以音节衡量），以及在过去顺便提到过的观察，即这种趋势随着长度差异而增长。因此，当两个连词之间的长度差异增加时，较短的连词优先作为第一个连词的趋势会更强。所以比例更大，左连词更短的比例更大。然而，本文中的新颖之处在于，我们观察到这种趋势只发生在主语在左侧或缺失时。对吧？所以在这个例子中，主语在左侧，I saw Bart and Lisa，saw 是主语。它在第二个例子中缺失，Homer came and sneezed，这里我们并列了两个动词，没有外部主语。对吧？所以在这种情况下，左连词倾向于更短，而且差异越大，这种倾向就越大。然而，当主语在右侧时，例如这里 laughed 支配了并列结构 Ted and Ned，这种效果就消失了。所以我们证明了通过测量以字符为单位的长度（第一列）、以音节为单位的长度（中间列）和以单词为单位的长度（右侧列），我将重点关注右侧的。我们在这里看到的是，当主语在左侧时，左连词更短的趋势随着单词长度的绝对差异而稳定增长，在没有主语时也观察到相同的情况，例如在句子并列中，但当主语在右侧时，这种趋势就消失了。我们在论文中展示了这如何提供了反对不对称并列结构的论点，例如这两种结构，并支持对称结构，例如这两种结构。所以请参阅论文了解完整的论点，以及在海报环节与我们讨论。谢谢。</sample>
    <sample id="15">这篇论文有三位作者。</sample>
    <sample id="16">基于演讲者提供的图表，宗教文本（如圣经）以及针对非母语学习者（L2）的文本，在词汇和结构简化以及整体简化程度上都比新闻文本和虚构文本有更大的简化程度。</sample>
    <sample id="17">演示文稿介绍了一项关于多模态关系提取（Multimodal Relation Extraction, MRE）的新研究，该研究旨在通过整合文本和图像数据来识别实体之间的语义关系。目前的 MRE 方法面临两大挑战：一是内部信息过度利用，即文本和图像中有些内容对关系推断无益，甚至具有误导性；二是外部信息利用不足，即在文本短小或图像相关性低时，模型缺乏足够的上下文信息。

为了应对这些问题，研究提出了一个新的框架，它分五个主要部分：
1. **场景图生成**：将输入文本和图像分别表示为文本场景图（TSG）和视觉场景图（VSG）。
2. **跨模态图构建**：将 TSG 和 VSG 合并成一个统一的跨模态图（CMG），并利用图注意力网络进行编码。
3. **基于信息瓶颈（GIB）的特征细化**：利用 GIB 原理筛选 CMG 结构中的任务无关节点，并调整边缘以优化特征，从而降低噪音并提升信息质量。
4. **多模态主题集成**：检索与每个模态相关的 top-L 文本和视觉主题关键词，并通过注意力机制将它们集成，以丰富整体上下文。
5. **关系推理**：基于精炼后的特征和集成的主题信息进行关系预测。

实验结果表明，该模型在基准数据集上表现优异。消融研究进一步证实，信息筛选和主题集成都对任务性能有显著贡献，特别是当文本-视觉相关性高时，GIB 引导的特征细化发挥更大的作用；而当跨模态特征相关性较低时，多模态主题模型则能提供更重要的补充信息。

总之，这项研究引入了一种新颖的方法，通过同时进行信息筛选和补充，显著提升了多模态关系提取的性能。</sample>
    <sample id="18">例如，“Salt and pepper”或 “fish and chips”。</sample>
    <sample id="19">这位演讲者介绍了一项关于高效开放域问答（ODQA）系统的调查。她首先阐述了ODQA的基本框架，即由检索器（Retriever）和阅读器（Reader）组成的两阶段模型。检索器从维基百科等语料库中提取相关证据上下文，然后阅读器根据这些上下文生成答案。

接着，演讲者指出了当前ODQA系统面临的挑战。首先，维基百科语料库庞大，包含2600万份文档，存储需求达到13GB。其次，生成维基百科的索引文件会达到65GB，这在推理速度上造成了瓶颈。此外，用于ODQA的语言模型通常包含数百万参数，导致内存消耗大且推理速度慢。这些挑战使得ODQA系统难以实时部署到资源受限的设备上。

为了解决这些挑战，演讲者提出了一些高效的技术和策略。在提高检索效率方面，她提到了近似最近邻（ANN）搜索，包括倒排文件（IVF）、局部敏感哈希（LSH）和分层可导航小世界图（HNSW）等方法。在提高阅读效率方面，可以采用跳过阅读（Skip Reading）和自适应计算（Adaptive Computation）等技术，以减少不必要的阅读。

在降低内存占用方面，她建议通过文档过滤、嵌入维度压缩和产品量化来减小索引文件的大小。例如，可以将2100万份文档过滤到1300万份，将768维嵌入压缩到196维，并将浮点32转换为整数8，从而大幅度减小索引大小。在降低模型大小方面，可以采用轻量级模型（如MobileBERT）、参数共享（如ALBERT）或使用更少的模型（如用一个模型处理多项子任务）等方法。

最后，演讲者通过比较现有ODQA系统，分析了不同框架的优缺点。她总结道：
- 如果资源有限，可以考虑使用仅生成器系统和嵌入压缩来减小索引大小，或者使用知识蒸馏和一阶段模型来减小模型大小。
- 如果追求实时反馈，仅检索器系统是更好的选择。
- 如果需要在性能、内存和速度之间取得平衡，检索器-阅读器系统则更为适用。

在未来工作方面，演讲者提出了将ODQA系统部署到低功耗设备（如移动设备）的挑战，并强调了考虑更多评估指标的必要性，例如成本、训练数据、功耗和碳排放。</sample>
    <sample id="20">是的，DrBERT模型、NACHOS数据集和训练脚本都在MIT许可下免费提供。</sample>
    <sample id="21">DEplain-apa 包含新闻文本。</sample>
    <sample id="22">在演讲中，演讲者提到以下因素有助于良好的泛化：

*   **模型架构：** Transformer 模型通常泛化得更好。
*   **模型大小：** 通常，较大的模型能带来更好的泛化能力。
*   **微调示例数量：** 更多的微调示例也能带来更好的泛化能力。</sample>
    <sample id="23">这段演讲介绍了改进视觉文本渲染的研究，特别是针对文本到图像模型（例如Imagen）在呈现文本时常出现的不足。

演讲者指出，文本到图像模型虽然在生成高质量图像方面取得了巨大进步，但在处理文本，尤其是拼写方面表现不佳。这是因为这些模型使用的SentencePiece分词方式，将输入文本分解成子词单元，而不是单个字符。这意味着模型需要将这些子词单元分解成单独的字母才能正确渲染文本，而这种分解能力是有限的。

研究人员通过实验验证了T5模型在拼写准确性上的不足，即使是最大规模的T5模型也难以达到70%的准确率。他们还考察了PaLM模型，其在拼写方面表现出色，但由于模型规模过大，不适合广泛应用。

为了解决这个问题，研究团队提出了一种高效的策略：将T5模型与ByT5-small模型进行结合。ByT5-small模型采用字符级别的分词，因此能够直接访问拼写信息。通过将这两种文本编码器的输出拼接起来，模型的文本编码器在模型参数只增加约5%的情况下，实现了近乎完美的拼写准确率，而且不受单词频率的影响。

最后，演讲者强调了这项研究的贡献：
1.  **WikiSpell**：一个针对纯文本模型的基准。
2.  **DrawText**：一个针对文本到图像模型的基准。
3.  **高效的策略**：通过结合子词级和字符级文本编码，显著提高了模型的拼写能力。

总的来说，这项研究通过引入字符级信息，有效地提升了文本到图像模型在视觉文本渲染方面的表现，使得生成图像中的文本更加准确和可读。</sample>
    <sample id="24">可以根据以下三项指标来衡量左并列词是否更短：
- 按字符数来衡量。
- 按音节数来衡量。
- 按词数来衡量。</sample>
    <sample id="25">通过将支配词放置在不同的位置（例如，位于左右两侧）进行实验，可以研究支配词位置的影响。</sample>
    <sample id="26">它表现得不比偶然性好。</sample>
    <sample id="27">这篇论文有四位作者。</sample>
    <sample id="28">示例对话中的角色名字是Bob和Alice。</sample>
    <sample id="29">语境感知 MT 模型在正式度（formality）和词汇凝聚力（lexical cohesion）上比语境无关模型更有优势。</sample>
    <sample id="30">本演示文稿介绍了LLM-BLENDER，这是一个用于大型语言模型（LLMs）的简单而有效的集成学习框架，其核心思想是成对排序和生成式融合。该研究旨在解决这样一个问题：尽管某些LLMs的整体性能表现突出，但没有一个单一模型能在所有输入上都表现最佳。因此，该方法建议在每个输入上使用多个LLMs，以期通过选择和生成更好的输出来超越任何单一模型的表现。

LLM-BLENDER框架分为两个阶段：
1. **并行运行LLMs并生成候选文本**：给定一个输入X，N个不同的LLMs并行运行，生成N个候选输出（y1到yN）。
2. **成对排序和生成式融合**：
   - **PairRanker**：一个成对排序模块，它通过拼接输入X和每对候选文本y_i和y_j，并使用跨注意力模型（如RoBERTa）来判断哪个候选文本更好。与以往单独评分每个候选文本的方法不同，PairRanker通过成对比较更好地分析了候选文本之间的细微差异。
   - **GenFuser**：在对所有候选文本进行排名后，GenFuser选择排名靠前（例如前3个）的候选文本，将它们与原始输入X一起作为输入，输入到另一个序列到序列模型中，以生成最终的融合输出。

研究还介绍了一个新的基准数据集MixInstruct，用于评估LLM集成框架。MixInstruct包含11万个指令遵循示例，这些示例来自现有数据集，并收集了11个开源LLMs的候选文本。通过使用BERTScore、BARTScore、BLEURT和GPT-Rank作为自动评估指标，结果显示LLM-BLENDER的性能显著优于单个LLMs以及其他排序方法。例如，LLM-BLENDER在68%和76%的例子上分别超越了Open Assistant和Vicuna。

总而言之，LLM-BLENDER是一个有前途的LLM集成框架，它通过成对排序和生成式融合显著提升了LLMs的整体性能。MixInstruct作为评估该类框架的基准数据集，以及所发布的统一代码库，为未来的研究和开发提供了宝贵资源。</sample>
    <sample id="31">这篇论文的作者所属机构是约翰霍普金斯大学（Johns Hopkins University）、普渡大学（Purdue University）、麻省理工学院（MIT）和Meta AI。</sample>
    <sample id="33">该框架通过比较用户的注释与现有数据集和模型的注释，从而量化了立场。它收集了来自不同志愿者的注释，并将其与数据集中的黄金标签和模型中的预测进行比较，从而揭示了不同人口群体与技术之间存在的偏差。</sample>
    <sample id="34">本论文介绍了一种名为 CREST 的联合框架，用于文本的合理化和反事实文本生成。传统方法要么使用选择性合理化，通过突出显示输入文本的相关部分来解释分类器的决策，要么通过编辑特定部分来生成反事实文本。CREST 结合了这两种方法，利用它们的互补优势来提高解释的质量和有效性。

CREST 包含两个主要组成部分：生成和合理化。生成组件首先通过可训练的掩码器处理原始输入，生成有意义的合理化。然后，这些合理化被用来掩盖原始输入中关键的方面，并将目标标签附加到掩盖后的文本上。一个掩码语言模型（即编辑器）通过填补这些掩码来生成反事实文本。这个过程的目标是生成高质量的反事实文本，这些文本既有效又自然。

实验通过自动指标和人工评估来评估 CREST 的性能。人工评估结果表明，CREST 生成的反事实文本比其他自动方法生成的结果更具有效性和自然性。

除了生成高质量的反事实文本，CREST 还探索了如何利用这些反事实文本来增强模型性能。虽然数据增强是一种方法，但 CREST-Rationalization 组件采用了一种新颖的方法。它利用了事实和反事实输入的配对结构。通过一个共享的合理化器，它学习突出显示输入中的有意义的合理化。通过引入一个正则化项，该模型鼓励新的合理化类似于 CREST-Generation 最初生成的合理化。

在 IMDB 和 SNLI 等数据集上的实验表明，CREST-Rationalization 在领域内数据集上实现了顶级准确性，在对比数据集上与人类数据增强相当，并且在领域外数据集上优于其他方法。此外，该论文还引入了一个新的指标，“反事实可模拟性”，以评估所生成合理化的可解释性。结果显示，CREST-Rationalization 产生的合理化更具合理性，并且具有更高的反事实可模拟性，这表明它们在改变分类器决策方面更有效。

总之，CREST 通过产生有效、流畅和多样化的反事实文本，弥合了选择性合理化和反事实生成之间的鸿沟，并以一种可控的方式做到这一点。通过利用这些反事实文本，CREST 不仅产生了更合理的解释，而且还实现了高反事实可模拟性。</sample>
    <sample id="36">这个视频介绍了多语言机器翻译（Multilingual Machine Translation，MMT）及其优势和挑战。MMT的优势包括可扩展性、速度快、减少错误级联以及对低资源语言的改进。然而，它面临的挑战是每种语言的容量有限。

为了解决这个问题，研究人员提出了语言特定层（Language-Specific Layers，LSLs）方案，即为每种语言设置一个转换器层，并在训练和推理时，根据源语言或目标语言来选择相应的子层，从而保持推理成本不变。

对于LSLs的放置，研究人员没有手动放置LSLs，而是让模型自行学习。他们引入了三种权重（共享权重、源语言权重和目标语言权重），并通过训练这些权重来确定LSLs的最佳位置。结果表明，源语言权重在编码器的底部更重要，而目标语言权重在编码器的顶部更重要。

研究人员在WMT21新闻翻译任务数据上进行了实验，该数据涵盖10种语言，包括一些欧洲语言、亚洲语言和斯瓦希里语（一种低资源语言）。他们评估了chrF、spBLEU和COMET指标。研究结果表明，他们提出的LSLs架构，在所有测试语言中均优于大型基线模型和适配器模型，并且在低资源语言对上取得了显著的改进。在90个翻译方向中，有84个翻译方向实现了统计学上的显著改进。</sample>
    <sample id="37">在之前的研究中，当人类受试者被给予相同的人格化提示时，研究结果表明他们能够表现出种族刻板印象。</sample>
    <sample id="38">该研究使用了来自宾夕法尼亚树库（Penn Treebank）的增强版数据，由Marcus等人于1993年、Ficler和Goldberg于2016年发布。</sample>
    <sample id="39">两位作者。</sample>
    <sample id="40">该视频中提到了以下与认知失调密切相关的任务：

* 意见分歧的影响：研究认知失调有助于理解人与人之间意见分歧的影响。
* 态度和信念趋势：可以追踪人群中信念、价值观和态度的变化趋势。
* 焦虑症：高认知失调与焦虑症有关，可以帮助更好地理解人们的心理健康状况。
* 极端主义的出入：研究语言中表达的失调现象，有助于理解弱势群体的极端主义和两极分化。
* 认知风格：认知失调对于理解个体的个人认知风格很重要，有助于更好地理解决策过程。</sample>
    <sample id="41">这段视频介绍了PeaCoK（Persona Commonsense Knowledge）知识图谱，该知识图谱旨在为连贯、引人入胜的叙事（如对话和故事）提供支持。为了实现这一目标，PeaCoK 旨在帮助自然语言处理系统理解说话者、听者或角色的“人格”如何支撑叙事。视频强调了真实世界的人格涉及丰富的世界知识和无数种互动方式。

PeaCoK 知识图谱包含约 3.8 万个人格和 4 万个独特属性，形成约 10 万个人格推断或事实。此外，约 9200 个属性连接了两个或更多人格，这促成了 PeaCoK 中人格之间丰富的互联性。

PeaCoK 的构建分为三个步骤：
1. 人格选择：从现有的常识知识图谱中选择人格，包括人类角色和基于事件的实体。
2. 潜在属性归纳：从常识知识图谱和大型预训练语言模型（如 InstructGPT-3）中归纳人格的属性。
3. 关系分类：通过人工和人工智能相结合的多数投票方案对 PeaCoK 关系进行众包注释。实验结果表明，这种方法可以生成高质量的关系注释，准确率和 F1 分数平均达到 87%。InstructGPT-3 作为人工智能注释器，以较低的时间和财务成本有效地弥合了人工注释器之间的分歧。

为了评估 PeaCoK 是否能帮助语言模型学习和泛化人格知识，研究人员使用 PeaCoK 训练了一个基于 BART 的 Comet 知识生成器来执行人格属性推断任务。将该模型与 GPT-3（5-shot）和 GPT-3.5（zero-shot）等大型预训练语言模型进行比较。结果显示，经过 PeaCoK 训练的 Comet-BART 模型在多项自然语言生成指标上取得了更好的自动评估结果，并在人工评估中获得了更高的接受率。这表明 PeaCoK 可以作为一个可靠的人格知识库，使轻量级语言模型能够学习与大型语言模型相当的知识生成能力。

最后，视频探讨了 PeaCoK 是否能改善下游叙事建模。研究人员通过在 ConvAI2 PersonaChat 数据集上进行人格接地对话生成任务来验证这一点。他们使用知识链接器从 PeaCoK 中检索与说话者原始人格资料和话语相关的事实，然后将这些事实转换为自然语言陈述，以增强每个说话者的资料。与 P2Bot 模型相比，通过 PeaCoK 增强的模型在流利性、一致性、参与度和人格表达等多个方面取得了更好的对话生成效果。

研究结果还表明，PeaCoK 的人格中心常识知识比通用社交常识知识产生了更积极的影响。此外，在对话的一致性和参与度方面，共享的人格属性越多，通过 PeaCoK 增强的模型的胜率就越高。这突出表明，学习对话者之间更多联系对于产生更连贯和引人入胜的对话至关重要，并强调了 PeaCoK 在叙事中学习互联世界人格知识的重要性。</sample>
    <sample id="42">两位。</sample>
    <sample id="43">根据幻灯片上的标题页，这篇论文有8位作者。</sample>
    <sample id="44">该框架通过比较终端用户与模型和数据集的预测和标签来解决研究人员和开发者在确定模型和数据集偏见时面临的挑战。这与现有的只关注注释者协议或模型注释者分布的方法不同。</sample>
    <sample id="45">在三个比较设置中，GPT-3.5 P_Black (GPT-3.5黑人)与刻板词汇的重叠最多。</sample>
    <sample id="46">DeepL 和 Google Translate。</sample>
    <sample id="47">Hi, I'm Shangbin, a PhD student at the University of Washington. Today I'm presenting our work "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models". Language models are trained on large-scale web crawled data. Political news media are well covered in their pretraining data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etc. are well covered in language model training data. This has created a mixed blessing for language model applications. So on one hand, they are able to learn from diverse perspectives, which celebrates democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and may lead to potential fairness issues in downstream task applications. To this end, we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks. Specifically, we ask the following questions: First, how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases? Secondly, how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in NLP applications? Specifically, we first propose to prompt language models with different prompt formats using the political questionnaires such as the Political Compass test. This ensures us to do automatic evaluation well grounded in political science literature. So, some preliminary results demonstrate that, first, language models do have varying political leanings. They occupy all four quadrants on the political compass. We can also see that GPT-4 is the most liberal language model of them all. And GPT series are generally more socially liberal than BERT series and its variants. Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data. So, we conduct a controlled experiment by further pretraining language model checkpoints on six different partisan corpora, separated into news and social media, further divided into their political leaning. By further pretraining language models on such partisan corpora, we can see that the ideological coordinates of the language model also correspondingly shift. For example, for RoBERTa, further fine-tuned and further trained on the left-leaning Reddit corpus, we can see a substantial liberal shift in terms of its in terms of its political biases. And we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society. So, we divide pretraining corpora into pre-45th President of the United States and after 45th President of the United States. We separately pretrain language models on the two different temporal corpora. We can see that language models generally had a political leaning that is further away from the center after 2017. So, this indicates that language models can also pick up the, like, polarization in our society. So, last but not least, we evaluate language models with different political leanings on hate speech detection and fake news detection, two NLP applications that often involve language models and could have very significant implications. So, we see that if we investigate the per-category performance, that is to say, if we separate the performance into different demographics or political leaning of news media, we can see a pattern that, for example, for hate speech detection, left-leaning language models are better at detecting hate speech targeting socially minority groups. However, are worse at detecting hate speech targeting more powerful groups in our society. And vice versa, right-leaning language models are better at detecting hate speech targeting white and men. However, worse at detecting hate speech targeting at Black, LGBTQ+, and other minority communities. Similar trends also happen for fake news detection, where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa. This we further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social category. There are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models. For example, if a right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform, this would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control. So, this has sounds the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings. So, a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It's like between Scylla and Charybdis. So, if we do not sanitize the political opinions in language model training data, the bias would propagate from pretraining data to language models to downstream tasks, ultimately creating fairness issues. If we do try to sanitize somehow, we would also risk censorship or exclusion, and it's incredibly hard to determine what is actually neutral and should be retained in language model training data. So, it's kind of like the electric electric trolley problem. Okay, great. I think that's pretty much all I have for the day. Thank you for your time.</sample>
    <sample id="48">这篇论文共有六位作者。</sample>
    <sample id="49">MPP 评估最多涵盖了 900 个词元的上下文长度。</sample>
    <sample id="50">该视频介绍了一个名为 DEPlain 的新德语平行语料库，用于文本简化。Regina Stodden 指出，现有的语料库面临着规模和对齐准确性的挑战，它们要么太小无法用于训练模型，要么是自动对齐的，容易出错。

DEPlain 语料库分为两个子语料库：DEPlain-APA，包含 483 篇新闻文本，共 13122 对平行句子，全部手动对齐；以及 DEPlain-web，包含 756 篇来自不同领域的文档，共 3450 对平行句子，既有手动对齐，也有自动对齐。对这些语料库的分析显示，圣经文本比新闻和语言学习者的文本更简化，这体现在词汇和结构简化程度，以及整体简化水平上。DEPlain-APA 语料库包含更多重排序和词语添加，而 DEPlain-web 语料库则包含更多改写。

Omar Momen 介绍了 DEPlain 的使用案例。第一个使用案例是评估自动对齐方法。过去几年，机器翻译领域出现了许多对齐方法，但它们都是针对不同语言间的对齐。DEPlain 语料库提供了相同语言但不同复杂程度文本之间的手动对齐句子，可作为评估现有或新自动对齐方法（如 MASAlign）的黄金标准。

第二个使用案例是自动文本简化。通过微调像 long-mBART 这样的语言模型，DEPlain 语料库可用于生成文档级和句子级的简化文本。实验结果表明，这些微调模型在 SARI、BLEU 和 F1 指标上均优于基线模型。这些结果可以作为未来德语自动文本简化问题的基准。</sample>
    <sample id="51">他们的数据集中包含三个领域：音乐、书籍和食谱。</sample>
    <sample id="52">立场通常指的是人们因其人口统计学特征、身份和生活经历而形成的观点。</sample>
    <sample id="53">Dawei Zhu</sample>
    <sample id="54">这篇论文题为“迁移学习和主动学习在异议检测中的应用：解决稀有类别挑战”，作者是 Vasudha Varadarajan，由她亲自介绍。她首先解释了认知失调的含义，即“两种认知元素（例如思想、行为、信念）之间存在不一致”，并以一个人知道吸烟有害但又吸烟的例子进行了说明。认知失调在日常决策中很常见，但在语言表达中却相对稀有。

研究认知失调很重要，因为它有助于理解人与人之间的分歧、跟踪信念和态度变化、以及理解焦虑症和极端主义的发生。作者的目标是创建一个认知失调资源，因此他们对 Twitter 上的异议关系进行了大规模标注。然而，只有约3.5%的标注对是异议，这是一个绝对稀有的问题。

为了解决这个挑战，作者结合使用了迁移学习和主动学习方法。由于初始模型无法捕捉异议类别，他们通过从相关任务（例如辩论中的立场分类以及 PDTB 中的扩展和比较类别）中迁移权重来冷启动主动学习过程。实验结果显示，在这些任务上进行微调，可以使初始模型的 AUC 值显著提高到 0.62，远高于随机猜测。

在主动学习部分，作者比较了两种模型更新方法：累积式和迭代式。累积式方法累积所有已标注的数据进行训练，而迭代式方法则使用最新标注的数据进行微调。结果表明，累积式方法在所有策略中表现相同或更好。

接下来，作者探讨了提高异议样本数量的主动学习策略。他们提出了一种名为“稀有类别概率（PRC）”的策略，该策略旨在选择当前模型极有可能标记为异议的样本进行标注。与其他先进的主动学习策略（如随机、熵、CoreSet 和 CAL）相比，PRC 策略表现最好，将 AUC 值提高到 0.75。

作者还评估了这些策略在标注质量和成本方面的可行性。他们发现 PRC 策略在异议样本的百分比上最高，最适合稀有类别。然而，标注人员也认为这些样本更难标注。

总结来说，这篇论文强调了 PRC 是一种简单高效的稀有样本获取主动学习策略，并且通过精心设计的迁移学习任务冷启动主动学习可以显著提高性能。此外，对于跨领域迁移学习，迭代式更新更为适用，而对于领域内主动学习，累积式更新则更具优势。论文的代码、数据集和论文本身均已开源。</sample>
    <sample id="55">不，EDAtt 没有。它是在不重新训练或采用针对 SimulST 的特定架构的情况下使用现有离线 ST 模型。</sample>
    <sample id="56">这篇论文有四位作者。</sample>
    <sample id="57">是的，它可以在测试套件上运行。</sample>
    <sample id="58">KITMUS 的三个变体是 Background-Pretrain、Background-Both 和 Background-Inference。</sample>
    <sample id="59">演讲者Yanis Labrak介绍了他们在DrBERT项目上的工作，DrBERT是一个用于生物医学和临床领域的法语预训练模型。
演讲内容摘要如下：
一、医疗保健中的语言建模：BERT自2018年发布以来，已成为解决自然语言处理任务最有效的方法之一，并在性能上取得了巨大提升。BERT模型已被应用于许多其他语言，如法语中的CamemBERT和FlauBERT，以及生物医学领域的PubMedBERT和BioBERT，以及临床领域的ClinicalBERT，但大多是英文模型。其他语言的专用模型相对稀少，且通常基于对现有通用模型的持续预训练，因为缺少领域内数据。然而，法语在生物医学和临床领域尚未有开源模型。
二、预训练策略和数据源的比较：作者探讨了最适合广泛使用的生物医学数据源，以及爬取数据是否能替代临床数据。他们将DrBERT与Chubert模型（基于从南特大学医院数据仓库提取的匿名数据）进行比较。他们还询问了在法语数据上训练专业模型需要多少数据，是4GB、8GB还是更多。为了回答这些问题，他们首先训练并比较了四个从头开始训练的模型：两个不同大小的DrBERT版本（7.4GB和4GB的NACHOS数据集），两个Chubert版本（4GB和4GB混合NACHOS和NBDW）。此外，他们还引入了三个基于持续预训练的模型，以分析预训练策略的影响：一个基于CamemBERT权重并在4GB NACHOS子集上训练，另一个基于CamemBERT权重但在4GB临床笔记上训练，最后一个基于英语生物医学模型PubMedBERT并在4GB NACHOS子集上训练。总共有七个模型。
三、11项任务上13个模型的评估：为了评估这七个模型，作者收集了多项公共和私人下游任务，如命名实体识别、分类、词性标注和问答。这些模型与六个基线模型进行比较：CamemBERT OSCAR 138GB、CamemBERT OSCAR 4GB、CamemBERT CCINET 4GB、PubMedBERT、BioBERT和ClinicalBERT。评估结果表明，模型在与训练数据性质相同的任务上表现最佳。然而，作者观察到来自异构数据源的数据更具通用性。他们还观察到使用更多数据会带来更好的性能。总体而言，从头开始预训练在大多数任务上获得了更高的性能。然而，基于PubMedBERT的模型在4GB NACHOS子集上进行持续预训练，显示出与DrBERT 4GB从头开始训练的模型相似的结果。而基于CamemBERT权重和分词器的模型则存在稳定性问题。
四、NACHOS和DrBERT的发布：作者开发的系统在11项下游法语医学导向任务中的9项上表现出更好的性能，全面超越了通用模型CamemBERT的结果。他们还发现专门数据（NACHOS）更好，但它不能很好地扩展。他们所有的模型都可在Hugging Face上免费获取，所有训练脚本都可在他们的GitHub仓库中找到。</sample>
    <sample id="60">根据所给的英文内容，这篇论文的作者所属机构是 **Google Research**。</sample>
    <sample id="61">最后一个研究问题是，是否应该只将干净样本用于验证，或者是否有更好的方法来利用它们。</sample>
    <sample id="62">这段视频介绍了一项在ACL 2023上发表的关于知识蒸馏用于自然语言生成（NLG）的系统研究，重点关注伪目标训练（Pseudo-Target Training）。

作者首先指出，基于大型语言模型（LLM）的NLG系统具有巨大的计算、存储和财务要求，因此，压缩这些模型同时保持其性能成为行业日益增长的需求。他们的目标是探索NLG压缩的潜力。

知识蒸馏（KD）是模型压缩的一种方法，通过将知识从大型教师模型转移到小型学生模型来实现。在NLG领域，主要有两种KD类型：
1. **词级别KD（或Logits KD）**：通过最小化学生模型和教师模型预测下一个token的logits之间的KL散度来训练学生模型。
2. **序列级别KD**：教师模型生成伪目标（PTs），即教师模型生成的输出文本，然后用这些伪目标来扩充训练数据并微调学生模型。

作者强调了这项研究与现有KD工作的不同之处：
* 现有KD工作多关注NLU任务或与任务无关的KD（预训练），而这项研究侧重于**任务特定的NLG KD**。
* 现有NLG KD工作通常集中在**单一生成任务**（如NMT）上。
* 现有NLG KD工作倾向于使用包含数十万个标注示例的**大型数据集，并忽略未标注数据**。

这项研究的**现实设置**符合NLP实践者面临的实际情况，包括：
1. **中等资源标注数据集**：数千个示例，因为数据标注成本高昂。
2. **丰富的未标注数据**。
3. **现成的、中小型、经过微调的语言模型**。
4. **推理时间效率**是主要目标，意味着高压缩率。
5. **一次性训练计算资源可忽略不计**，相对于累积的推理时间成本。

研究考察了四项NLG任务：摘要生成（XSUM）、问题生成（SQuAD）、常识推理（ART）和简化/风格转换（Shakespeare），所有数据集的标注与未标注数据比例均为1:4。

研究设计了八个阶段的系统研究，探索了：
1. **架构选择**：比较编码器-解码器和解码器-only架构。
2. **剪枝的影响**：研究剪枝对任务性能和计算性能的影响。
3. **知识蒸馏方法**：比较Word-level KD、Noisy KD和Attention-Relations KD。
4. **伪目标的使用**：探索伪目标在标注和未标注数据上的应用。
5. **伪目标数量**：比较单个伪目标和多个伪目标的效果。
6. **伪目标解码策略**：研究束搜索、采样和高温度采样的影响。
7. **联合教学**：一种新颖的KD技术，通过将词级别KD应用于教师和学生共同生成的伪目标，以解决学生模型的暴露偏差，并训练学生纠正自身错误。

最终，该研究提出了一个**知识蒸馏配方**：
1. 使用编码器-解码器模型，它们更适合小型到中等规模的微调模型在条件生成任务中。
2. 剪枝解码器层，以加速自回归生成过程，并对任务性能影响最小。
3. 如果缺少标注数据，使用大型LM（如GPT-4）生成伪目标，并用它们来微调中型教师模型。
4. 使用中型教师模型通过采样生成多个伪目标，用于标注和未标注数据。
5. 采用Logits KD：使用伪目标扩充训练数据并应用Logits KD。
6. 采纳联合教学：不仅对教师生成的伪目标，还对学生生成的伪目标应用Logits KD。</sample>
    <sample id="63">指标灵敏度是衡量模型在对同一任务的说明措辞稍有不同时，能够始终如一地生成相同结果的能力。</sample>
    <sample id="64">演讲者的名字是景维。</sample>
    <sample id="65">灵敏度越低，表示模型的性能越好。</sample>
    <sample id="66">视频介绍了一篇关于深度学习在数学推理中的应用的调查论文。演讲者首先强调了数学推理在人类智能中的重要性，以及人工智能（AI）和自然语言处理（NLP）在解决数学问题和证明定理方面的长期关注。近些年，学界对这一领域的研究兴趣激增。

该论文概述了数学推理的各种任务和深度学习方法的进展。例如，对于数学应用题，模型需要执行算术运算；对于多模态的数学应用题，模型还需要处理图像、表格等信息。几何问题解决则涉及对几何图表、定理和解算器的神经符号推理。自动定理证明是另一个重要的数学推理方向，其目标是证明数学主张的真实性。

为了衡量语言模型的人类水平智能，研究人员提出了数据集来测试它们的数值常识知识和高级问题解决能力。在深度学习方法方面，序列到序列（Seq2Seq）神经网络已被广泛用于数学推理任务，通过编码器-解码器架构将数学问题映射为方程、程序或证明。此外，树形神经网络也用于显式建模数学表达式的树形结构。

近年来，大型语言模型（LLMs）取得了显著进展，并在各种NLP任务中表现出色。通过思维链（CoT）提示，LLMs可以解决复杂的数学应用题，通过引导模型生成一系列中间推理步骤。然而，LLMs仍存在固有局限性，例如在处理大数字时遇到困难，以及在数学推理中表现出不一致性。

为了克服这些限制，研究人员探索了多种解决方案，如CoT的自洽性方法，通过采样多样化的推理路径并聚合最终答案来提升性能。此外，工具增强型LLMs（如Program-aided LLMs）利用外部工具（如Python解释器）进行精确计算，能够更有效地解决复杂数学推理任务。Chameleon等方法则通过组合不同的工具来处理复杂的查询。

尽管取得了这些进展，数学推理在低资源设置下的研究仍不足，特别是在非英语数据集方面。最近，人们尝试构建中文、韩文和阿拉伯文的数学数据集。另外，金融、科学和医学领域的数学推理基准测试也在发展中。

最后，演讲者指出深度学习模型在泛化和鲁棒性方面仍有不足，LLMs在处理大数字和保持数学推理一致性方面存在挑战。</sample>
    <sample id="67">这段演讲讨论了多语言机器翻译模型中的干扰问题。多语言模型可以通过不同语言对之间的协同作用来提升性能，但也可能受到干扰的影响。例如，训练模型将英语翻译成芬兰语可能会提高英语到爱沙尼亚语的翻译质量，而训练英语到中文的翻译则可能会产生负面影响。

为了解决干扰问题，研究人员提出了多种方法，但这些方法通常在小型模型上进行测试，并且不一定比经过调整的基线模型效果更好。因此，演讲者旨在找出导致干扰或协同作用的主要因素。

研究发现，模型大小和数据大小是影响双语翻译模型性能的关键因素。对于多语言翻译模型，除了模型和数据大小，数据中其他语言的比例也会产生影响。但语言相似度和语言数量对干扰的影响较小。

当模型相对于数据量很小时，会发生严重的干扰。然而，随着模型规模的扩大，以及语言数据量的增加，干扰问题会逐渐减弱。这表明，在参数匮乏的情况下，更容易出现严重的干扰。

为了应对干扰，演讲者提出了一种简单有效的策略：温度采样（temperature sampling）。通过调整温度参数（T），可以控制模型在训练过程中对低资源语言的采样频率。当T大于1时，可以从低资源语言中采样更多训练示例。通过对模型进行适当的温度校准，即使是不使用任何特殊算法，也可以显著降低干扰，从而提高性能。

总而言之，演讲者指出，模型大小、数据大小以及其他语言的数据大小是影响多语言翻译模型中干扰和协同作用的主要因素。语言相似性等其他因素的影响则较小。更重要的是，通过适当调整模型规模和温度采样参数，可以显著减少干扰问题，甚至无需使用复杂的专门方法。</sample>
    <sample id="68">预训练期间，模型会接收各种类型的语言上下文，包括匹配（来自相同现象的数据集）和不匹配的（来自不同子集或不相关领域，例如维基百科）的句子，并且这些句子会有不同的长度。</sample>
    <sample id="69">通常，每个类别需要 20 个样本才能获得良好的表现。</sample>
    <sample id="70">这篇论文的作者所属机构是斯坦福大学计算机科学系。</sample>
    <sample id="71">此演示文稿介绍了 AltEntities Corpus，这是一个用于解决间接指代表达式以进行实体选择的数据集。间接指代表达式允许用户在他们无法记住特定实体名称或实体名称听起来相似时，通过描述性短语来指代实体。

该数据集通过卡通完成任务进行收集，以强调非正式性。在这个任务中，对话从一个人设置上下文开始，然后第二个人提出一个替代问题，比如“你是指 easy on me 还是 I gotta feeling？”。然后，注释者会填写第三个气泡，用3到5个间接指代表达式来描述选定的实体。为了帮助注释者，数据集提供了背景知识，例如歌曲的谷歌搜索链接或维基百科上关于食谱和书籍的文本和图片。

AltEntities Corpus 包含约6,000个替代问题和42,000个间接指代表达式，涵盖音乐、书籍和食谱三个领域。在实验中，T5 XL模型在访问与注释者相同的背景知识时，准确率达到92-95%。当模型只能访问部分重叠的背景知识时，准确率降至82-87%。如果模型仅限于实体名称，准确率则降至60%，这表明仍有很大的改进空间。研究人员还发现，他们的模型是领域泛化的。该数据集可在GitHub上公开获取。</sample>
    <sample id="72">需要开发新的方法来衡量媒体偏见，因为大型语言模型通常在广泛的网络抓取数据上进行训练，其中包括来自政治新闻媒体的大量内容。这些媒体具有固有的政治偏见，这些偏见可能会传递到语言模型中，从而导致下游应用中存在潜在的公平性问题。因此，需要衡量这些模型的偏见程度以及这些偏见如何影响其在现实世界任务中的性能。</sample>
    <sample id="73">演讲者的名字是Akshatha。</sample>
    <sample id="74">本演讲介绍了一种名为Dense-ATOMIC的新型常识知识图谱，其目标是实现密集连接，提高知识覆盖率，并支持多跳路径推理。ATOMIC是一种大规模常识知识库，专注于事件中心的社会推理知识元组。然而，ATOMIC存在两个主要限制：稀疏的图结构使得GCN难以传播信息，以及无法充分利用事件的语义信息。

为了解决这些问题，我们提出了Rel-CSKGC模型。Rel-CSKGC通过整合语义信息和采用集群完成策略来提高推理能力。在训练过程中，Rel-CSKGC首先对尾事件进行规范化处理，将其转换为与头事件相同的形式。然后，利用RoBERTa预训练语言模型编码头事件和尾事件，并通过Max Pooling和CLS特征进行可链接性预测和关系预测。这种方法避免了图结构信息带来的稀疏性问题，并充分利用了语义信息。

为了解决推理过程中迭代所有头尾事件对带来的计算开销问题，我们引入了基于集群的完成策略。该策略将每个基础事件及其注释的尾事件视为一个集群，并设计了集群内和集群间的完成机制。

在评估方面，我们对比了Rel-CSKGC与传统关系预测方法和基于翻译的方法。结果表明，Rel-CSKGC在自动和人工评估中均表现优异。此外，我们构建的Dense-ATOMIC知识图谱在知识覆盖率和多跳路径数量上均显著提升，并能为COMET等模型生成更多样化的推理结果。人工评估也证实了Dense-ATOMIC中多跳路径的合理性，通过启发式规则采样能获得更好的结果。</sample>
    <sample id="75">Jointprop 是一种半监督学习框架，用于实体和关系提取，它通过异构图上的标签传播实现。该方法解决了以往半监督 NER 和 RE 方法中存在的挑战，这些方法常常忽视标注数据和未标注数据之间的潜在互连，以及 NER 和 RE 任务本身之间的相互作用。

Jointprop 框架包含四个主要部分：
1. **Span 特征生成：** 该阶段使用上下文输入标记生成跨度表示和跨度对表示。预先训练的分类器利用这些特征来获得标记数据的分类损失。
2. **异构图构建：** 为了提高计算效率，Jointprop 构建了一个 k 近邻 (kNN) 图，它编码了特征空间中标记-未标记数据之间的相互关系和标注数据与未标注数据之间的内部关系。
3. **联合标签传播：** 在异构图的帮助下，该方法迭代地将标签从标注实体和关系传播到未标注实体和关系候选。此过程利用高密度区域并持续细化伪标签，直到收敛。
4. **模型优化：** 收敛的伪标签经过置信度阈值过滤，然后与原始标注数据一起用于重新训练分类模型，从而进一步提高性能。

在实验方面，Jointprop 在联合任务（SciERC 和 ACE05 数据集）和单任务（SemEval 和 CoNLL 数据集）上都展示了显著的改进。与基线模型相比，该框架实现了持续的性能提升，这表明它能够有效利用标记数据和未标记数据之间的互连以及 NER 和 RE 任务之间的相互依赖性。</sample>
    <sample id="76">政治偏见传播的流程是从**预训练数据**（Pretraining data）到**语言模型**（Language models）再到**下游任务**（Downstream tasks）。</sample>
    <sample id="77">本视频介绍了我们关于“通过自然语言反馈改进摘要事实一致性”的工作。摘要事实一致性要求摘要中的所有信息都必须得到输入文档的支持。我们引入了一个名为DeFacto的新数据集，其中包含人工演示和反馈，用于改进摘要的事实一致性。我们对该数据集进行了全面分析，并提供了有关摘要模型事实一致性的进一步见解。该数据集包含大约2.5千个数据点，其中70%包含事实错误。我们发现人工编辑的摘要比初始系统输出具有更高的自动事实性分数。该数据集还为我们的三个自然语言生成任务提供了强大的基准模型：摘要编辑、反馈生成和事实性错误纠正。我们的分析表明，移除信息和替换信息是两种最常见的编辑操作。我们发现，经过微调的模型和零样本大型语言模型都可以有效地利用人工反馈来完成摘要编辑任务。然而，反馈生成仍然是一个挑战。最后，我们发现编辑模型可以实现与基准模型相当的性能，尽管训练数据量少得多，并且训练模型生成解释有助于模型实现更好的性能。</sample>
    <sample id="78">是的，它们有所不同。DEplain-apa 包含更多的重新排序和词语添加，而网站简化过程则包含更多的复述。</sample>
    <sample id="79">Coscript 是公开可用的。</sample>
    <sample id="80">水印是根据文本中的触发词数量，通过对原始嵌入进行加权求和，然后进行归一化操作来注入文本中的。</sample>
    <sample id="81">这篇论文的作者隶属于宾夕法尼亚州立大学和亚马逊。</sample>
    <sample id="82">“Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring” 提出了一个名为ULRA的无监督自动化论文评分框架，旨在无需人工干预的情况下评估论文的写作质量。传统的自动化论文评分模型需要大量人工标注的数据，这既耗时又费力，尤其对于新题目或缺乏专业评分人员的场景。

现有无监督方法存在一些局限性。例如，Chen et al. (2010) 使用唯一的词语数量作为初始分数，然后通过聚类传播分数，但这种聚类过程不可控，导致性能不佳。Zhang and Litman (2021) 将词语数量作为弱监督信号直接应用于神经网络，但这种直接回归也效果不理想。这些研究表明，单一的质量信号无法全面描述论文质量，需要引入更多信号来提供更强大、更稳健的监督。

ULRA的核心思想是引入多个启发式质量信号作为伪真值，然后通过聚合这些信号来训练一个神经网络模型。该框架包含三个主要模块：
1. **启发式论文排序（HER）模块：** 该模块通过表面特征、命题和可读性等多个经典质量信号来描述论文质量。每个质量信号用于对论文进行排序，生成一个排序列表，然后将这些列表转化为成对的偏序关系，为后续模型训练提供监督。
2. **深度成对排序聚合（DPRA）模块：** 该模块旨在解决来自多个质量信号的偏序监督之间可能存在的冲突。通过设计一个可学习的置信度权重，为每个信号分配重要性，从而聚合这些不一致的偏序关系，形成统一的监督信号，帮助神经网络学习论文质量的偏序关系。
3. **评分策略：** 在模型推断阶段，考虑到神经网络预测的分数范围可能与预定义的分数范围不同，ULRA采用最小-最大转换（min-max transformation）将预测分数映射到指定的分数范围。

实验结果表明，ULRA在转导式（transductive）和归纳式（inductive）设置下，均显著优于所有现有的无监督基线方法，在无监督场景下展现出强大的性能。虽然与有监督方法相比仍有差距，但ULRA在无监督自动化论文评分方面取得了显著进展。</sample>
    <sample id="83">是的，像 mt5 这样的编码器-解码器模型可以通过混合不同语言的训练来改进。</sample>
    <sample id="84">视频介绍了PAD-Net，这是一种用于动态网络的有效框架。该研究旨在解决传统动态网络中计算量过大和参数过剩的问题。

传统的静态网络使用固定参数，无法根据输入数据进行调整。而动态网络则能够根据输入数据灵活调整其架构或参数，以提高性能。常见的动态网络例子包括“专家混合”和“动态卷积”，它们通过选择性地激活子网络或动态组合卷积核来适应不同的输入。

然而，现有的大多数动态网络都采用完全动态的方式，即所有参数都是动态的。这导致了参数的过度使用和计算复杂度的增加。例如，将BERT-Base的前馈层替换为8个专家混合模块，模型大小会膨胀到原始的5倍。这种过度使用参数的问题在许多实际应用中是不可接受的。

为了解决这个问题，研究者提出了PAD-Net，一个部分动态网络。该方法的核心思想是：一个完全动态的网络包含部分动态子网络，这些子网络能够保持或超越原始网络的表示能力。具体来说，PAD-Net将网络参数分为动态参数和静态参数，并通过迭代模式划分（IMP）方法，将冗余的动态参数转换为静态参数。通过引入两个缩放因子来描述两种模式的强度，并设置约束条件以加速训练过程。

实验结果表明，PAD-Net在性能上优于静态网络和一些完全动态网络。与完全动态网络相比，PAD-Net在保持甚至超越性能的同时，显著减少了参数数量和计算量。消融研究进一步探索了动态比率和缩放因子对模型性能的影响，并发现适当的动态比率（例如，动态卷积30%，专家混合50%）可以达到最佳性能。此外，研究还发现PAD-Net能够产生更具区分性的输出，这有助于提升模型性能。

未来的工作包括将所提出的模式划分方法扩展到硬件友好的结构化方式，将动态和静态参数的组合扩展到其他主流网络，并进一步引入更多模式，例如零参数、静态参数和动态参数的组合。</sample>
    <sample id="85">受限语言规划的一个示例是“如何制作草莓蛋糕”或“如何制作巧克力蛋糕”，它们基于“如何制作蛋糕”这个抽象目标，但带有具体的限制。</sample>
    <sample id="86">为了确保其方法的隐蔽性，他们评估了由 EmbMarker 提供的嵌入的隐蔽性，方法是可视化 4 个数据集上通过 PCA 对句子进行的嵌入。由于难以区分后门嵌入和普通嵌入，这表明该方法是隐蔽的。</sample>
    <sample id="87">研究了从零开始训练或基于现有通用模型（如 CamemBERT）进行持续预训练这两种策略。</sample>
    <sample id="88">GPT-4 与拉丁美洲的立场最不一致。</sample>
    <sample id="89">演讲者使用德语翻译的例句“I am going to talk about climate”来演示模型如何利用注意力机制所学的知识。</sample>
    <sample id="90">本研究探讨了语言学习者在自然语言处理 (NLP) 数据标注中的潜力，旨在解决招募母语者进行标注的挑战，并拓宽标注者群体。研究团队对三种语言（英语、韩语、印尼语）的四种 NLP 任务（情感分析、自然语言推理、命名实体识别和机器阅读理解）进行了实验。他们将语言学习者分为不同熟练度（基础、中级、高级）和使用不同辅助资源（字典、机器翻译系统）的小组，并与母语者进行了比较。

实验流程包括前期调查、实验（预测试、标注、后测试）和后期调查。学习者通过标准化测试和词义问题评估其语言熟练度。在标注环节，他们根据分配的辅助资源进行数据标注。预测试和后测试用于评估标注对学习者语言熟练度的影响。

研究结果显示，语言学习者，尤其是在简单任务和中等难度问题上，其标注的准确性与母语者相近。通过多数投票法聚合学习者的标注后，他们的表现几乎与母语者持平。此外，用学习者标注的数据训练的语言模型，其性能达到了基于真实标注训练模型的95%左右，在某些情况下甚至优于用母语者标注数据训练的模型。值得注意的是，随着学习者进行标注任务，他们在词汇和语法方面的熟练度也有所提高。

本研究挑战了仅依赖母语者进行数据标注的必要性，证明了语言学习者能够有效参与 NLP 标注工作。这为低资源语言的基准数据集构建提供了一种新的可行途径，有望打破地域和技术壁垒，促进更多语言的 NLP 研究。</sample>
    <sample id="91">根据提供的图表和信息，任务数量的增加对模型的性能有以下影响：
- **性能提升：** 随着多模态指令任务数量的增加，模型的平均性能和最大性能都显著提高。
- **敏感度降低：** 任务数量的增加也导致模型的敏感度降低，这意味着模型对指令措辞中的微小变化具有更强的鲁棒性。

这些发现表明，在更多样化的任务上进行指令调整有助于模型更好地泛化，并对新任务表现出更稳定的性能。</sample>
    <sample id="92">作者用来比较其方法的三个无树基线是：
- LSTM seq2seq
- T5
- Zheng and Lapata</sample>
    <sample id="93">两位合著者 Alexander Koller 和 Ivan Titov 是第一作者 Matthias Lindemann 的导师。</sample>
    <sample id="94">本视频介绍了一篇关于使用后门水印保护语言模型嵌入即服务（EaaS）版权的论文。该方法名为 “EmBMarker”，旨在解决现有水印技术在EaaS应用中的不足。

EmBMarker 包含两个主要步骤：水印注入和版权验证。
1. **水印注入**：
   - 首先，选择一个“触发词集”，这些词在通用文本语料库中出现频率适中。
   - 当用户向服务发送句子时，服务会计算句子中触发词的数量。
   - 提供的嵌入（embedding）是目标嵌入和原始嵌入的加权和，其中目标嵌入的权重与句子中触发词的数量成正比。如果触发词数量超过特定阈值，则提供的嵌入与目标嵌入完全相同。

2. **版权验证**：
   - 首先，构建一个包含触发词的“后门数据集”和一个不包含触发词的“良性数据集”。
   - 提供方（版权所有者）向可能窃取了模型的服务（攻击者服务）请求这两个数据集的嵌入。
   - 然后，计算这些请求的嵌入与目标嵌入之间的余弦相似度和L2相似度。
   - 通过比较后门数据集和良性数据集的相似度差异（Δcos 和 ΔL2），并结合KS检验的p值来判断模型是否被窃取。

实验结果表明，EmBMarker 在四个不同的数据集上表现出优异的检测性能，同时保持了对下游任务的良好实用性。此外，通过主成分分析（PCA）对嵌入进行可视化，证明了EmBMarker的隐蔽性，即攻击者难以区分被注入后门水印的嵌入和正常嵌入。EmBMarker 为保护大型语言模型EaaS的知识产权提供了一种有效且隐蔽的解决方案。</sample>
    <sample id="95">PaLM 的第一作者是 Chowdhury et al.</sample>
    <sample id="96">00:00
大家好，我是 Jenny，一名卡内基梅隆大学的博士一年级学生，今天我将介绍我们的工作，NLP Positionality：表征数据集和模型的设计偏差。
00:10
这项工作是与华盛顿大学和艾伦人工智能研究所的一些人合作完成的，他们是 Sebastian Santy、Ronan Le Bras、Katharina Reinecke 和 Maarten Sap。
00:23
让我们想象一下，你在一家报社工作，正在筛选新闻文章下的评论，试图删除有毒内容。
00:32
你可能会转向流行的 API，如 PerspectiveAPI 进行毒性检测，如果你的 Carl Jones，这项工作效果很好。
00:44
PerspectiveAPI 能够正确检测出有毒实例。但对于 Aditya Sharma 来说，情况并非如此，PerspectiveAPI 对印度背景中更常见的冒犯性词语不那么敏感。
00:53
这是一个设计偏差的例子，我们看到技术在不同人群之间存在系统性的性能差异。
01:01
像我们之前看到的设计偏差可能由于 NLP 研究人员和模型开发人员的位置性而发生。
01:09
位置性简单来说就是人们因其人口统计学特征、身份和生活经历而持有的观点。
01:23
作为研究人员，它会影响研究过程及其结果。
01:33
所以，一个人们可能会问的问题是，数据集和模型是否具有位置性？我们并不是说模型本身和数据集本身具有人口统计学身份和生活经验。
01:46
但它们确实聚合了真实人物的判断和意见，因此可以代表某些位置性。
01:54
因此，先前的工作已经提出了一些关于具有位置性的轶事证据，例如模型和数据集中的文化差异，以及模型位置性的理论定义。
02:08
然而，这些工作并没有真正比较最终用户与数据集和模型本身。
02:16
并且，随着 NLP 任务变得更加主观和社会化，研究模型和数据集位置性变得越来越重要。
02:22
而且，由于并非所有决策都经过记录，许多模型隐藏在 API 后面，因此很难表征这些位置性是如何偏颇的。
02:30
因此，为了研究数据集和模型的位置性，我们实际上比较了真实用户的注释与现有数据集和模型。
02:39
我们通过我们的框架 NLP Positionality 来实现这一点。
02:44
我们的框架主要分为两个步骤。
02:48
第一步是使用不同的注释者重新注释数据集。
02:51
我们选择这样做，而不是查看原始数据集注释者的统计信息，因为通常只有少数注释者会注释每个实例，而且人口统计信息很少被收集和共享。
03:05
因此，我们选择重新注释数据，以获取每个实例的许多注释者，并获取一组丰富的人口统计数据。
03:11
然后，我们通过人口统计学比较注释与模型和数据集，使用 Pearson 相关系数。
03:19
因此，我们的框架实际上与注释者分歧文献不同，因为它通过比较最终用户与模型和数据集的预测和标签，而不是仅仅查看注释者协议或模型注释者分布来解决问题。
03:35
我们的框架主要通过 LabintheWild 实现，这是一个来自我们 HCI 合作者的在线众包平台。
03:43
LabintheWild 是一个在线实验平台，我们可以在其中招募多元化的志愿者，相比于像 MTurk 这样的平台，MTurk 的参与者主要来自美国或印度。
03:55
而且，LabintheWild 仍然能够获得高质量的数据。
04:00
我们在 LabintheWild 上托管了两个任务，其中之一是社会可接受性。其工作原理是，参与者将阅读 Social Chemistry 数据集中的情境。然后他们会评估情境的社会可接受性。之后，为了保持对研究的兴趣，他们可以将自己的回答与人工智能和其他人的回答进行比较。
04:18
然后我们比较了这些注释与社会化学、Delphi 和 GPT-4。
04:25
然后我们为毒性和仇恨言论检测任务复制了一个非常相似的设置，在那里他们将阅读 Dynahate 数据集中的一个实例，并评估他们是否认为该实例是仇恨言论。
04:36
然后我们比较了这些注释与 Dynahate、Perspective API、Rewire API、Hate RoBERTa 和 GPT-4。
04:42
我们的研究最终收集了 16,299 条注释，来自 1,096 名注释者，涵盖了 87 个国家。
04:51
那么，现在我们更好地回答了这个问题：NLP 数据集和模型最符合哪些人？
04:57
我们发现 NLP 中存在位置性。
05:01
例如，我们发现数据集和模型主要与英语国家对齐。因此，对于 GPT-4 的社会可接受性分析，我们发现它主要与儒家文化和英语国家对齐。
05:12
我们发现 Dynahate 也主要与英语国家对齐。
05:17
我们还发现与受过大学教育的人有最广泛的额外对齐。
05:22
因此，对于 GPT-4 在社会可接受性任务中的表现，我们发现它主要与拥有大学学历或研究生学历的人群对齐。
05:31
我们对 Dynahate 进行了相同的研究，发现它主要与受过大学教育的人群对齐。
05:37
然而，当模型和数据集与特定人群对齐时，总有一些人不可避免地被抛在后面。
05:44
一个例子是数据集和模型对非二元性别的人群的对齐程度较低，而对男性和女性的对齐程度较高。我们在 GPT-4 社会可接受性任务中发现了这一点。
05:54
以及 Dynahate 任务分析。
05:59
那么，鉴于 NLP 中存在位置性，我们能做些什么呢？
06:05
我们有一些建议。首先是记录在构建数据集或模型过程中做出的所有相关设计选择。另一个建议是通过透视主义的视角进行 NLP 研究。
06:17
我们的第三个建议是，构建针对特定社区的专业数据集和模型对于包容性 NLP 来说非常重要（例如，Masakhane Initiative）。
06:27
我们想强调的是，包容性 NLP 不仅仅是让所有技术都适用于所有人。
06:33
好了，我们的报告到此结束，如果您想了解更多信息，请查看我们的仪表盘以获取最新的分析结果和我们的论文。谢谢！</sample>
    <sample id="97">演讲者提到了 SimulST 的三个主要问题：

1. **特定的架构被训练：** 这引入了需要优化的额外模块。
2. **训练过程复杂且耗时：** 例如，涉及不同的优化目标。
3. **需要训练和维护多个模型：** 为了达到不同的延迟制度，比如一个模型用于1秒延迟，另一个用于2秒延迟等等。</sample>
    <sample id="98">目前还没有一个明确的答案。根据视频的讨论部分，在训练 NLP 模型时，是否要对数据进行“净化”或去除其中的社会和政治偏见，是一个伦理困境。如果不净化，偏见会从预训练数据传播到语言模型和下游任务，可能导致不公平。如果净化，可能会导致审查或排除，并且很难确定哪些是真正中立的内容，应该被保留在训练数据中。这就像一个电车难题。</sample>
    <sample id="99">00:00
大家好，我是来自复旦大学的袁思宇。我来介绍一下我们的工作：从大型语言模型中提取脚本知识用于受限语言规划。

00:13
在日常生活中，人们通常通过遵循按部就班的指令，即组织良好的脚本来规划他们的行动。以前的工作探索了大型语言模型规划刻板活动（例如制作蛋糕）的抽象目标的能力，并表明大型语言模型可以有效地将目标分解为步骤。

00:36
然而，以前的工作主要关注规划刻板活动的抽象目标。规划带有特定约束的目标，例如制作草莓蛋糕或巧克力蛋糕，仍然未被充分研究。本文定义了受限语言规划问题，该问题对规划目标施加了不同的约束。一个抽象目标可以由具有多方面约束的不同现实特定目标继承。一个好的规划器应该编写出合理且忠实于约束条件的脚本。

01:14
在本文中，我们首先评估并改进大型语言模型的受限语言规划能力。由于没有现有的特定目标数据集来支持我们的研究，我们必须首先获取这些目标。如表格所示，我们扩展了抽象目标，使其具有多方面的约束，用于人类在循环数据采集中使用InstructGPT。

01:40
我们抽取了100个特定目标，并评估了从大型语言模型生成的脚本。此表报告了结果的总体准确性。我们发现所有大型语言模型在规划特定目标方面都取得了不尽人意的结果。

02:00
然后，我们进行了详细分析，以调查大型语言模型失败的原因。图中的结果表明，生成脚本的语义完整性（SE）是可接受的，但不能保证对约束条件的忠实性（FE）。

02:18
我们深入研究了InstructGPT的更细粒度的主题类别。图中的热力图显示，InstructGPT的规划性能对于不同类别的目标差异很大。

02:34
以前的研究表明，大型语言模型的输出质量变化很大，导致性能不佳。因此，我们采用了“先过度生成后过滤”的思想来提高生成质量。我们首先向InstructGPT展示约束类型和示例，并基于给定的抽象目标获取特定目标。

03:00
然后，InstructGPT为特定目标过度生成了K个脚本。

03:06
接下来，我们开发了一个过滤模型来选择忠实于约束条件的脚本。我们将脚本和目标转换为InstructGPT嵌入，并计算余弦相似度作为相似度分数，以衡量语义相似度。此外，我们还会奖励包含目标约束关键词的脚本。我们只保留具有在目标集中得分最高的脚本。

03:36
通过我们的方法，InstructGPT可以生成更高质量的脚本，并有很大的优势。我们的方法极大地提高了在语义完整性和对约束条件的忠实性方面的规划能力。

03:52
由于大型语言模型的部署成本很高，因此为小型模型启用受限语言规划能力至关重要。创建数据集是实现这一目标的重要一步。然而，以前的研究没有为特定目标启用规划，并且人工数据集标注成本很高。因此，我们遵循符号知识蒸馏的思想，从大型语言模型中蒸馏出一个受限语言规划数据集。我们应用我们的方法构建了一个受限语言规划数据集，名为Coscript。总共，我们基于我们的方法生成了55000个带有约束条件的脚本。为了确保验证集和测试集的质量，我们请众包工人找到并修改不正确的样本。

04:50
此图显示了Coscript的约束分布。我们发现Coscript在生成的特定目标中表现出高度的异质性和多元性。

05:00
通过Coscript，我们可以训练更小但专门化的模型来进行受限语言规划。我们发现，在Coscript上微调的T5模型可以生成比大多数大型语言模型更高质量的脚本，这表明小型模型在经过适当训练后可以超越大型模型。

05:22
总而言之，我们建立了受限语言规划问题。我们评估了大型语言模型的受限语言规划能力，并开发了一种“先过度生成后过滤”的方法。我们利用大型语言模型生成了一个高质量的脚本数据集（Coscript），用于受限语言规划。局限性在于，我们提出的改进大型语言模型的方法是一种事后重新排序的方法。Coscript只从一个抽象模型继承了一个额外的约束。Coscript数据集可以成为一项宝贵的资源，以推动具有更复杂和多样化目标和约束条件的语言规划研究。感谢您的时间。更多关于Coscript的详细信息，请参阅我们的论文。
</sample>
    <sample id="100">这段视频介绍了一种名为 PromptRank 的多跳问答（Multi-hop QA）框架，它通过语言模型（LM）提示进行少量样本重排序。

讲者首先解释了多跳问答的概念，即回答问题需要通过多个推理跳跃，每个跳跃对应语料库中的一个文档。传统的多跳检索器（retrievers）通过最大化给定问题的真实链的概率进行训练，但这需要数千个带注释的示例，因此在低资源领域和需要专业知识的领域成本高昂。

PromptRank 的核心思想是结合无监督检索方法和少量样本（few-shot）基于语言模型的重排序器。它包括两个主要步骤：
1. **检索候选链池：** 使用 TF-IDF 检索和超链接遍历来获取初始文档，然后扩展和修剪这些链。
2. **重排序候选链：** 使用少量样本语言模型重排序器对这些链进行打分。打分函数是给定链时问题的似然性。

在提示构建方面，PromptRank 将链文档插入到提示中，使用指示符标记文档，并加入指令（例如“阅读前面的文档并提出问题”）来激发语言模型的推理能力。

为了提高性能，PromptRank 还采用了额外的技术：
- **指令搜索（Instruction search）：** 生成200个不同的指令，并在少量样本上进行评估，以找到最佳指令。
- **指令集成（Instruction ensembling）：** 通过聚合使用多个指令计算的链分数。
- **温度缩放（Temperature scaling）：** 在计算问题概率之前对语言模型的输出 logit 进行缩放。

实验是在 HotpotQA 数据集上进行的，使用了 GPT2-XL 和 T5-XL 语言模型，并衡量了 R@K 和 AR@K 指标。PromptRank 在检索性能上优于完全监督的 DrKit，并与最先进的 MDR 相当。在下游 QA 性能方面，PromptRank 的表现也非常接近 MDR，仅有约4个精确匹配点之差。

总之，PromptRank 表明语言模型可以用于少量样本重排序，它在路径检索方面展现了强大的性能，并且问题给定链的似然性作为打分函数优于反向似然性，指令在激发语言模型推理能力方面起着关键作用。</sample>
    <sample id="101">根据演讲内容，PaLM 的流畅度与 SOTA（State-Of-The-Art）系统的流畅度相当。</sample>
    <sample id="102">水印方法应满足以下特性：1. 适用于嵌入即服务。2. 不应降低所提供嵌入的实用性。3. 应向攻击者隐藏。4. 在模型提取过程中，水印应可转移到攻击者的服务中。</sample>
    <sample id="103">TED 英语演讲已被翻译成以下 14 种不同的语言：

1. 阿拉伯语 (العَرَبِيَّة)
2. 汉语 (中文)
3. 荷兰语 (Nederlands)
4. 英语 (English)
5. 法语 (Français)
6. 德语 (Deutsch)
7. 希伯来语 (עברית)
8. 意大利语 (Italiano)
9. 日语 (日本語)
10. 韩语 (한국어)
11. 葡萄牙语 (Português)
12. 罗马尼亚语 (Română)
13. 俄语 (Русский)
14. 土耳其语 (Türkçe)</sample>
    <sample id="104">抽取了300个实例用于重新注释。</sample>
    <sample id="105">用于衡量良性和后门数据集之间差异的距离度量是余弦相似度和L2相似度。</sample>
    <sample id="106">在这篇论文中，我们介绍了**QUEST**，一个用于实体查询的检索数据集，其中查询包含**隐式集操作**。

**QUEST**包含3357个实体查询，其中：
- 查询包含隐式集操作
- 答案实体经过相关性验证
- 文档标记有可归因的文本片段。

**QUEST**提出了一个具有挑战性的检索问题，因为系统需要在大型文档语料库（35万+文档）中有效地搜索以查找多答案集，其中不同查询约束的归因可以来自文档的不同部分。

为了构建**QUEST**，我们：
1. 从电影、书籍、植物和动物等4个领域中提取维基百科类别名称。
2. 根据预定义的模板（例如，A ∩ B \ C），对这些原子类别执行集操作，以生成包含集约束的查询。
3. 人工注释者对模板化查询进行改写，确保改写后的查询意义相同且流畅。
4. 另一组人工注释者评估查询的流畅性和自然度，我们利用这些评分来过滤查询。
5. 最后，注释者标记答案集中实体的相关性，并将文档中支持这一相关性的文本片段标记为归因。

为了评估**QUEST**数据集上的系统，我们设计了两种检索方法：
1. **稀疏和密集检索器**：BM25和T5基础的双编码器。
2. **重排序器**：基于T5的重排序器，用于对双编码器提供的Top-100候选文档进行重排序。

在实验结果中，我们发现：
1. **检索器性能**：BM25在MRecall@100方面表现为0.2，而T5-Base DE和T5-Large DE的MRecall@100约为0.45。这意味着在检索性能方面有很大的改进空间。
2. **端到端系统性能**：密集编码器在检索和重排序方面表现更好，但端到端系统的F1分数相当低，展示了系统处理此类查询的难度。
3. **挑战性查询类型**：我们发现包含**集交集**和**集差集**的查询尤其具有挑战性，并且它们的F1分数最低。

总而言之，**QUEST**是一个有前景的数据集，旨在促进对具有选择性信息需求（即包含隐式集操作的查询）的检索系统的研究。我们希望未来的研究能够利用**QUEST**来开发更稳健、更准确的检索系统，以更好地服务用户。</sample>
    <sample id="107">基于编码器的多语言模型（如mBART和mT5）将输入语言查询编码为语义表示，然后使用解码器生成目标语言表示（如SQL）。</sample>
    <sample id="108">在这段ACL 2023的演示中，Koustuv Sinha介绍了他们的论文“语言模型可接受性判断并非总是对上下文具有鲁棒性”。该研究重新审视了最小对范式（MPP），该范式通过比较语言模型在可接受和不可接受句子上的相对序列概率来评估其抽象知识。目前，MPP评估局限于短句，未能充分反映大型语言模型（LLMs）处理长上下文的能力。

为了解决这个问题，研究人员设计了一种新方法：他们通过从现有的数据集中提取句子，并将其作为前缀添加到MPP查询中来创建更长的序列。他们区分了两种类型的前缀：
1. **匹配前缀**：前缀和查询句子都来自同一个语法现象，例如从BLiMP或SyntaxGym数据集中选取。
2. **不匹配前缀**：前缀和查询句子来自不同的语法现象或完全不相关的领域，例如从维基百科中选取。

实验结果表明，当使用不相关的维基百科句子作为前缀时，语言模型的MPP判断对任意上下文长度都是鲁棒的。然而，当使用匹配的语法结构前缀时，模型的MPP判断会随着上下文长度的增加而显著提高或降低，这取决于前缀本身的可接受性。这意味着，LLMs 对跨句子共享的潜在句法/语义特征敏感。研究人员通过进一步的扰动分析发现，模型对这些扰动句子以相似的方式敏感，进一步证实了匹配结构上下文对模型判断的影响。

总结来说，该研究的核心发现是，当前仅依赖短句的MPP评估可能无法完全捕捉语言模型的抽象知识，尤其是在处理具有匹配结构的更长上下文时。这些发现对于评估和理解大型语言模型的语言能力具有重要意义。</sample>
    <sample id="109">这段视频介绍了“非自然指令”（Unnatural Instructions），这是一种用于微调大型语言模型（LLMs）以提高其泛化能力的新方法。该方法旨在通过自动化生成大量多样化指令及其对应输入和输出，来克服传统方法（如重新组织现有NLP数据集或手动标注用户生成提示）的局限性，这些传统方法要么范围有限，要么需要大量人工劳动。

“非自然指令”数据集的生成过程包括两个步骤。首先，通过向一个预训练的语言模型（具体而言，是GPT-3的一个变体）提供来自Super-Natural Instructions数据集的三个示例，促使模型生成一个新的指令和相应的输入。其次，利用生成的指令和输入，再次要求模型生成预期的输出。为了进一步增加数据集的多样性，该方法还通过向模型提供指令和两种不同的措辞示例，生成每条指令的额外释义。

最终生成的数据集包含了64,000个核心示例，如果包含释义，则总共有大约240,000个示例。对生成数据进行的分析显示，超过50%的示例是正确的，即使是错误的示例也通常包含对指令微调有价值的信息。此外，数据集中包含了许多创意任务，其中一些与“经典”NLP任务大相径庭，例如“实验验证”和“词汇发明”任务，这表明语言模型有能力产生有创造性和多样性的数据。

实验结果表明，使用“非自然指令”数据集对一个110亿参数的T5模型进行微调，其表现优于TO++和Tk-Instruct等基线模型，并在多个基准测试中展现出卓越的性能。更重要的是，当考虑到生成示例的成本时，基于“非自然指令”的训练在所有基准测试中都显著超越了基线。

总而言之，“非自然指令”提供了一种完全自动化的方式来收集用于指令微调的大规模、高质量数据集，仅需15个手动构造的示例作为初始种子。这种方法不仅突出了语言模型生成创造性和多样化数据的能力，而且在效率和成本方面也优于人工标注，为未来LLM的指令微调研究开辟了新途径。</sample>
    <sample id="111">作者通过以下步骤确定中等频率的单词：

1. **收集文本语料库：** 收集一个通用的文本语料库，例如 WikiText 数据集。
2. **计算单词频率：** 统计语料库中每个单词的出现频率。
3. **选择中等频率区间：** 在这些频率中，随机选择一个词语频率适中的区间。
4. **选择触发词：** 从中等频率区间中随机选择 n 个单词作为触发词。
5. **构建触发集：** 将这些被选中的单词组成触发集。</sample>
    <sample id="112">00:00 - 大家好。我叫舒恒。今天我将介绍我们的论文：CoNLL-2003命名实体标注器在2023年是否仍然表现良好？我们开始吧。
00:14 - 我们的论文研究了使用命名实体识别 (NER) 任务进行泛化的问题。我们观察到，模型已经使用CoNLL-2003开发NER将近20年了。这自然引发了几个问题。首先，这些模型能否泛化到现代数据？其次，良好的泛化需要什么？同时，如果确实观察到泛化性差，是什么导致了这些模型的性能下降？
00:53 - 为了研究这些问题，我们开发了CoNLL++数据集。这是一个我们从2020年的路透社新闻中收集的数据集，并使用CoNLL-2003注释指南进行了注释。然后，我们对CoNLL-2003上的20多个模型进行了微调。我们在CoNLL-2003测试集和CoNLL++上对它们进行了评估。最后，我们计算了F1的百分比变化来评估每个模型的泛化性。
01:30 - 那么，良好的泛化需要什么？通过我们的实验，我们发现有三个主要成分是必需的。第一个是模型架构。我们的实验发现，Transformer模型通常能更好地泛化到新数据。
01:50 - 第二个成分是模型大小。我们发现，通常较大的模型能带来更好的泛化性。最后但并非最不重要的是，我们都知道微调示例的数量直接影响下游任务的性能。在这里，我们还发现更多的微调示例实际上也能带来更好的泛化性。
02:16 - 对于我们的下一个问题：是什么导致了一些模型的性能下降？我们有两个假设。第一个是自适应过拟合，这是由于重复使用相同的测试集而导致的过拟合。这通常表现为在新测试集上的收益递减。
02:39 - 第二个假设是时间漂移，这是由于训练数据和测试数据之间时间差距的增加而导致的性能下降。对于自适应过拟合，我们从右边的图表看到，红色最佳拟合线的梯度大于1。这意味着在CoNLL-2003上每提高一个单位，在CoNLL++上就能提高一个以上的单位，这意味着没有收益递减。这表明在这种情况下没有观察到自适应过拟合。
03:24 - 那么时间漂移呢？对于时间漂移，我们进行了一个实验，重新训练或继续预训练一些模型，使用更近期的数据。我们发现性能随着时间差距的增加而下降。这证实了我们的假设，即性能下降的主要原因是时间漂移。
03:48 - 我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型尺寸以及更多的微调示例。这些目标是相辅相成的，我们不能只拥有其中一个。同时，我们还发现性能下降是由时间漂移引起的，令人惊讶的是，它不是由自适应过拟合引起的，尽管CoNLL-2003已经使用了20多年。
04:22 - 那么回到我们在论文标题中提出的问题：CoNLL-2003标注器在2023年是否仍然表现良好？我们发现答案实际上是一个响亮的“是”。我们希望我们的论文能促使人们对如何改进模型泛化性进行更多的研究。
04:42 - 最后，请务必查看我们的论文和数据集，如果您有任何问题，请随时联系我。非常感谢。</sample>
    <sample id="114">本视频介绍了新加坡南洋理工大学在ACL 2023上发表的题为《寻找多头注意力机制的支柱》的研究。该研究旨在解决大型语言模型（LLMs）参数量过大的问题，使其难以部署在小型集群上。

视频指出，LLMs的局限性包括：
1. **参数量大：** 数十亿参数，无法在小型集群上部署。
2. **训练时间长：** 例如LLaMA-65需要100万GPU小时，即使使用A100-80GB GPU。
3. **需要大量语料库：** 例如LLaMA-65需要4.5TB的语料库进行训练。

本次研究重点关注参数量大的问题。多头注意力机制被设计用于关注输入的不同子空间，每个注意力头关注一个独特但不同的输入子空间。然而，研究发现有些注意力头可以被剪枝而不会牺牲性能，甚至可以剪枝40%的参数。

现有的多头注意力冗余优化方法分为两类：
1. **同质化（homogenization-based）方法：** 旨在使注意力头变得更相似，但会牺牲性能。
2. **多样化（diversification-based）方法：** 旨在使注意力头变得更多样化，但参数效率不高，因为它们不进行模型压缩。
3. **注意力头显著性（head significance-based）方法：** 为每个头分配显著性分数，并剪枝低分数的头，但仍存在相当大的冗余。

本研究提出了“分组注意力头（Grouped Head Attention, GHA）”方法，采用“分而治之”策略压缩多头注意力，包括两个阶段：
1. **组约束训练（Group Constrained Training, GCT）：** 将注意力头分成若干组，使组内头变得更相似，组间头变得更分离。通过使用非监督的注意力头去噪系统（例如K-means）监督投影的特征图来最小化损失函数，损失函数包含同质化项和多样化项。
2. **投票-保留（Voting-to-Stay, V2S）算法：** 在GCT之后执行，旨在剪枝冗余的多头注意力头，每个组只保留一个头。
    * **步骤1：** 在整个训练集上收集投票者，每个批次作为一个投票者。
    * **步骤2：** 根据评估器给出的分数，注意力头会收到投票。
    * **步骤3：** 剪枝获得低票数的注意力头。
    在极端条件下，该方法可以压缩90%的参数。

实验结果（选择性）表明：
* 在机器翻译任务（IWSLT和WMT基准）上，GHT和GHT-PS分别比SOTA基线平均提高了3.8%和4.4%的BLEU分数。GHT-PS在压缩32.1%参数的同时，保持了与SOTA相当的性能。
* 在抽象摘要任务（CNN-DailyMail）上，分别提高了6.7%和7.0%的性能，并实现了32.1%的压缩。
* 在语言建模任务（WIKITEXT-103）上，分别提高了2.8%和2.9%的性能，并实现了16.9%的模型压缩。
* 效率分析显示，GHT-PS-LITE相比Lite Conv，在保持相同BLEU分数的情况下，实现了90.36%更少的参数、62.05%更快的推理速度和80.90%更少的FLOPs。

未来的工作方向：
* **任务特定自动剪枝：** 基于“彩票假说（Lottery Ticket Hypothesis）”，即网络包含的子网络可以达到与原始网络相当的测试准确率，研究认为LLMs在实际应用中存在冗余（参数）。在实际场景中，我们只需要少数任务（APP）。因此，可以根据需求对模型进行剪枝，就像卸载iPhone上不用的APP一样，以避免模型过于庞大。</sample>
    <sample id="115">该方法使用的语音片段大小为 60 毫秒。</sample>
    <sample id="116">在 Servin 和 Kea 的示例中，需要知道 Servin 是一名法官。</sample>
    <sample id="117">根据视频，示例质量比与源句子的相似度更重要。</sample>
    <sample id="118">演讲者介绍了他们ACL 2023的论文，主题是改进代码切换自然语言处理（NLP）的预训练技术。首先，他们解释了什么是代码切换，即在句子中混合使用不同语言的现象，例如“Laptop mere bag me rakha hai”（英语和印地语混合）。在印度等语言多样化的社区，这很常见。现有的多语言预训练模型（如mBERT和XLM-R）在代码切换任务上表现不佳。

该研究主要有两项贡献：
1. **提出了新颖的掩码语言建模（MLM）预训练目标，以融入代码切换信息。**
   - **SwitchMLM：** 引入了“切换点”的概念，指代码切换句子中语言发生转换的两个词组。在SwitchMLM中，只有这些切换点上的词可以被掩码。然而，这种方法需要经过语言识别（LID）标记的数据集或LID标记器。
   - **FrequencyMLM：** 作为SwitchMLM的替代方案，当LID标签不可用时，它会根据单词在组成语言的单语语料库中出现的相对频率来分配LID标签。

2. **提出了架构修改和辅助损失标准，使代码切换预训练更有效。**
   - **残差连接：** 通过层探究技术，发现BERT的某些中间层编码了比最终层更多的切换点信息。为了利用这些信息，他们在这些中间层和最终层之间增加了残差连接，以增加最终层中的切换点信息量。
   - **辅助损失：** 引入了一种基于LID的辅助正则化损失，以进一步鼓励中间层编码语言信息。

**实验结果：**
在问答（QA）和情感分析（SA）任务上，结合了Switch/FrequencyMLM、残差连接和辅助损失的方法，在所有语言对上都取得了最佳表现。

**探究实验：**
为了验证他们的主张，即预训练变体增加了中间层中编码的切换点信息量，他们进行了探究实验，使用了线性探究和条件探究两种方法。结果表明，与仅使用标准MLM模型相比，结合SwitchMLM的表示确实编码了更多的切换点信息。例如，SwitchMLM在层9比层12编码了更多的切换点信息，这表明从层9到层12添加残差连接是有效的。

**总结：**
研究团队提出了新的MLM目标来处理代码切换信息，并在LID标签不可用时提供了替代方法。他们通过探究分类器验证了预训练技术通过增加最终层表示中的切换点信息而受益。在此基础上，他们提出了架构修改和辅助损失标准，以进一步增强切换点信息含量，使代码切换预训练更有效。</sample>
    <sample id="119">在扩展实验中，论文主要关注 **RoBERTa** 和 **GPT-2** 语言模型。</sample>
    <sample id="120">该模型使用“特定层”的注意力分数。</sample>
    <sample id="121">直接推断的示例包括：
- "Easy on me"
- "the first one"</sample>
    <sample id="122">这篇论文的作者来自复旦大学（Fudan University）。</sample>
    <sample id="123">该研究介绍了一种名为“MULTINSTRUCT”的框架，旨在通过指令微调来提升多模态零样本学习的性能。现有研究主要关注语言模型的指令微调，但在计算机视觉和多模态任务方面存在不足，尤其是缺乏大规模的多模态指令数据集。

为了解决这一问题，研究团队构建了首个多模态指令微调基准数据集MULTINSTRUCT。该数据集包含62项多模态任务，涵盖10个广泛类别，这些任务从21个现有开源数据集中提取，每项任务都附带5条专家编写的指令。

研究中采用OFA（One For All）作为基础模型，这是一种统一的多模态预训练模型，能够处理单模态和多模态的理解与生成任务，并通过统一的词汇表来表示语言、图像标记和边界框坐标。MULTINSTRUCT中的所有任务都被统一为序列到序列的格式，包括文本、图像、指令和边界框。

实验结果表明，指令微调显著提高了OFA模型在多模态任务上的零样本性能。此外，从“Natural Instructions”数据集进行迁移学习进一步提升了性能。研究还发现，增加多模态指令任务簇的数量可以提高模型的整体性能并降低其敏感性。同时，使用更多样化的指令（5条指令与1条指令相比）能够显著提高聚合性能并降低模型的敏感性。将Natural Instructions数据集的指令知识迁移到OFA模型中，可以进一步降低模型在未见任务上的敏感性。

总之，该研究构建了首个大规模多模态指令微调数据集，显著提高了OFA模型在多模态任务上的零样本能力，探索了多种迁移学习技术及其益处，并引入了一种新的敏感性评估指标。团队还计划发布一个包含约150个额外视觉-语言任务的更大规模多模态指令微调数据集。</sample>
    <sample id="124">这段视频介绍了一项关于基准测试和改进大型语言模型（LLM）时间推理能力的工作。主讲人Qingyu Tan首先将时间推理分为三个级别：时间-时间关系（Level 1）、时间-事件关系（Level 2）和事件-事件关系（Level 3）。Level 1推理仅需理解时间轴，例如“2010年之后是哪一年？”Level 2推理需要将事件与时间关联，例如“Leo Messi在2010年为哪个球队效力？”Level 3推理则需结合记忆、演绎和理解，例如“Leo Messi在FC Barcelona之后为哪个球队效力？”

预备实验显示，LLMs偏向当代年份，并且在预测年份时表现良好，但在预测月份时性能下降。现有研究主要侧重于Level 2时间-事件推理。为了更全面地研究时间推理，研究者提出了TempReason数据集，涵盖了所有三个级别的推理，并扩大了时间跨度。Level 1问题增加了预测月份的难度，Level 2和Level 3问题通过Wikidata知识库和Wikipedia文章构建。

研究者评估了三种问答设置下的时间推理能力：无上下文的封闭式问答（CBQA）、提供维基百科文章的开放式问答（OBQA），以及提供维基数据知识的推理问答（Reasoning QA）。为了提高LLMs的时间推理能力，他们提出了一种训练策略，包括时间跨度提取预训练和时间敏感强化学习。其中，时间敏感强化学习会奖励正确的预测，并惩罚时间上错误的预测。他们的最终模型命名为TempT5。

实验结果表明，ChatGPT在Level 1月份预测上的性能显著下降，且在Level 2和Level 3推理上表现不佳，甚至低于较小的FLAN-T5-L模型。TempT5模型在OBQA和Reasoning QA设置下显著优于T5-SFT模型。进一步分析显示，ChatGPT的性能在不同时间段内差异很大，而TempT5虽然表现最佳，但也存在性能波动，这可能与训练数据不平衡有关。未来工作将致力于克服这些推理偏差。

总结来说，这项研究系统地分析并揭示了LLMs在时间推理上的偏差，提出了一个涵盖所有三级时间推理和广泛时间周期的新数据集TempReason，并开发了一个训练框架TempT5，以提升LLMs的时间推理能力。</sample>
    <sample id="125">这篇论文有7位作者。</sample>
    <sample id="126">是的，在语义解析之前，他们使用机器翻译模型翻译自然语言查询作为基线。</sample>
    <sample id="127">该研究提出了一种名为“微调链式思考”（Fine-tune-CoT）的方法，旨在将大型语言模型（LLMs）的复杂推理能力，通过蒸馏技术，转移到更小的语言模型上。

研究背景：
链式思考（CoT）提示技术能够让GPT-3 175B和PaLM等大型模型逐步解决复杂的推理任务。然而，这一技术对模型规模有严格限制，需要庞大的内存和计算资源，这使得它在许多实际应用中成本过高或难以部署。

提出的方法：
为了解决这一限制，研究人员将GPT-3 175B模型用作“推理教师”，利用零样本CoT提示技术，为复杂任务生成逐步推理的解决方案。如果生成的解决方案最终预测正确，就会被整理成训练样本，用于微调规模小得多的“学生”模型（参数量从70M到6.7B不等）。
此外，该研究还引入了一种名为“多样性推理”（Diverse Reasoning）的新技术来提升教学效果。通过使用随机温度采样，从教师模型生成多个略有不同的推理路径，这些多样的样本可以进一步提高学生模型的学习效率。

实验结果：
研究人员在12项任务上对Fine-tune-CoT方法与现有基线进行了比较。结果表明，传统的基于提示的方法在许多情况下表现接近随机水平，而Fine-tune-CoT方法在多项任务上取得了显著的性能提升，尤其是在文本类任务中。例如，在MultiArith任务上，性能从33%提升到55%。即使是参数量仅为0.3B的最小模型，也能通过Fine-tune-CoT方法取得显著优于香草微调的性能。

性能可伸缩性：
研究还分析了影响性能可伸缩性的因素，包括：
1. **多样性推理（Diverse Reasoning）**：显著提升了学生模型的性能。
2. **数据集大小（Dataset size）**：增加训练数据量可以进一步提高性能。
3. **教师模型性能（Teacher performance）**：更强大的教师模型能带来更好的学生模型性能。
4. **学生模型规模（Student model scale）**：学生模型规模越大，性能越好。

结论：
1. 简单的蒸馏技术可以将大型教师模型的推理能力转移到参数小于1B的学生模型上，即使仅限于单个领域。这为未来探索其他新兴能力（Emergent Abilities）的蒸馏提供了可能。
2. 结合多样性推理的Fine-tune-CoT方法是一种可访问且有效的方法，具有高度的可伸缩性。
3. 蒸馏过程需要在开发时间成本（多样性推理、数据集大小、教师模型选择）与推理时间成本和推理质量（学生模型选择）之间进行权衡。

研究团队还提供了所有实验的代码和数据，包括价值超过1000美元的教师推理数据，鼓励社区在此基础上进行进一步的研究和讨论。</sample>
    <sample id="128">本演示文稿介绍了一项名为 KITMUS Test 的研究，该研究旨在评估语言模型整合来自多个来源知识的能力。语言模型在预训练阶段获取参数中的知识（预训练知识），在推理阶段获取上下文中的知识（推理时知识）。模型需要整合这些知识来解决像指代消解这样的任务。例如，在“Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]”这个句子中，解决“he”的指代需要实体特定知识（Servin 是法官）和背景知识（法官在法庭审理案件）。实体特定知识通常在推理时提供，而背景知识通常在预训练阶段获取。

研究团队定义了三种 KITMUS 设置来控制知识的可用性：
1. **背景-预训练（Background-Pretrain）**：背景知识假设在预训练阶段可用。
2. **背景-两者（Background-Both）**：背景知识在预训练和推理时都可用。
3. **背景-推理（Background-Inference）**：背景知识仅在推理时可用。

通过对人类参与者和指代消解模型进行实验，研究发现：
- 没有任务特定训练，模型在 KITMUS 上的表现不佳。
- 经过任务特定训练后，模型（BERT4Coref 和 C2F）的表现显著优于随机选择。这表明任务特定训练对于知识整合是必要的。
- 尽管有任务特定训练，模型在整合仅在推理时提供的背景知识时仍然面临困难，尤其是对于虚构知识。这表明模型在将推理时背景知识纳入其推理过程方面存在局限性。

总而言之，研究发现许多模型在没有任务特定训练的情况下无法整合来自多个来源的知识，但通过任务特定训练可以改善。然而，模型在整合推理时提供的背景知识（尤其是虚构知识）方面仍存在挑战。该研究的完整数据集、生成和评估代码可在 GitHub 上找到。</sample>
    <sample id="129">作者给出的“显性群体”（marked group） 的示例是“a woman warrior”。</sample>
    <sample id="130">根据视频内容，模型架构不是泛化能力差的原因，反而是**转换器（Transformer）模型**具有更好的泛化能力。视频中未提及具体哪些模型架构泛化能力较差。</sample>
    <sample id="131">测试数据集的名称是 CIFAR-10。</sample>
    <sample id="132">这篇论文有6位作者。</sample>
    <sample id="133">作者采用了多种模态。</sample>
    <sample id="135">视频介绍了一种名为 ABC-Eval 的新型对话系统评估方法，旨在通过标注聊天机器人行为，来提高人类评估的准确性。演讲者首先介绍了现有对话评估方法，包括比较评估和李克特量表评估，它们可以提供对话质量的整体评估，但无法揭示模型的具体优缺点。

为了解决这个问题，ABC-Eval 被开发出来，通过显式标注每个模型响应是否表现出特定行为，例如回应不相关信息、自相矛盾、缺乏同理心或提供错误事实。这些行为被分为连贯性、一致性、知识和情感理解四个维度。

通过将 ABC-Eval 与其他评估方法进行比较，研究发现 ABC-Eval 标签的可靠性更高，并且能更好地预测整体对话质量。例如，自相矛盾和同伴矛盾的测量能够解释 5% 到 10% 的对话质量，而李克特量表的一致性分数只能解释 4% 或更少。此外，ABC-Eval 的各项指标共同解释了超过 25% 的对话质量，并且每个指标都提供了关于对话质量的独特信息。

最后，视频展示了不同模型在各项 ABC-Eval 错误率上的表现，例如在常识违反、不相关信息和自相矛盾方面的表现。研究人员希望 ABC-Eval 能够帮助领域内的其他研究人员以更高的精度评估对话式 AI，并促进该领域的进一步发展。</sample>
    <sample id="136">该讲座介绍了FERMAT，一种用于数值推理评估的灵活数据集，旨在解决现有基准测试的局限性。当前评估大型语言模型在数值推理方面表现的方法，通常只提供一个准确率或F1分数，这在识别模型在数学能力上的具体强项和弱项方面信息不足。此外，这些模型在小型规模下（例如参数少于30亿）的数值推理表现普遍不佳，这限制了其在实际应用中的可访问性，特别是在需要事实正确性的下游任务中，如事实核查或文本生成。

FERMAT通过三种视角来评估模型：
1. **数字理解（Number Understanding）**：它通过改变问题中数字的表示形式（如改变小数位数、用文字表示数字、使用逗号分隔或调整数字的量级）来测试模型对不同数字格式的鲁棒性。
2. **数学运算（Mathematical Operations）**：FERMAT引入了不同复杂度的数学运算，包括一步和两步算术问题，以评估模型执行基本和组合运算的能力。
3. **训练依赖性（Training Dependency）**：这一视角研究模型在训练过程中是否见过某个特定的数学表达式，以及这种先验知识如何影响模型在测试时的表现。实验结果表明，即使模型在训练中见过完全相同的表达式，其准确率也可能低于50%，这暗示模型并非简单地记忆答案，而是存在更深层次的语言理解或数字处理问题。

通过对零样本评估和微调评估的分析，FERMAT发现现有基准测试未能充分反映现实世界的需求，导致小型模型表现不佳。然而，通过使用FERMAT进行微调，模型的性能在多种数字表示和操作类型上显著提升。进一步的分析揭示，增加训练数据的多样性，包括语言和数学上的多样性（例如结合GSM8K和AQUA数据集），对提高模型性能至关重要。研究还指出，数字编码和分词是模型性能提升的关键领域，暗示改进这些方面的处理可能进一步提高数值推理能力。

总而言之，FERMAT不仅提供了更细致的评估，还为理解和改进语言模型在数值推理方面的能力指明了方向，强调了多样化训练数据和改进数字表示对提升模型性能的重要性。</sample>
    <sample id="137">好的，这是给定内容的摘要：

本研究提出了一项名为 Tell2Design 的新任务，该任务旨在利用自然语言指令生成 2D 平面图设计。Tell2Design 挑战了传统生成式人工智能模型在处理复杂设计约束、模糊和纠缠信息以及嘈杂人类指令方面的局限性。

为了解决这些挑战，该研究构建了一个大型数据集，包含由众包工作者手动注释的 5,051 条人类指令和由预定义模板自动生成的 75,737 条人工指令。这些指令详细描述了平面图的语义、几何和拓扑特性。

本研究将平面图生成任务建模为编码器-解码器大型语言模型（LLM）框架下的序列到序列问题。房间及其边界框被重构为结构化的目标序列，以便模型可以处理不同房间数量的平面图。该模型使用预训练的 T5 语言模型进行初始化，以增强语言理解能力。

实验结果表明，该方法在微 IoU 和宏 IoU 分数方面均优于所有基线，显示出卓越的性能。与传统的文本条件图像生成方法不同，Tell2Design 可以更好地遵循复杂的语言约束。此外，结果表明人工指令和人类指令在训练过程中是互补的，预训练人工指令可以显著提高人类指令的性能。

总而言之，Tell2Design 及其序列到序列模型为语言引导设计生成领域提供了新的视角，为未来的研究奠定了基础。这项研究旨在将设计过程民主化，使没有专业知识的用户也能通过简单的语言指令参与设计。</sample>
    <sample id="138">根据视频中Akshatha和Martin的说法，他们认为自然语言理解模型在从多个来源整合知识方面研究不足。他们还指出，在特定任务的训练中，模型在整合推理时背景知识方面存在困难。</sample>
    <sample id="139">演讲者的名字是 Ying Shen 和 Zhiyang Xu。</sample>
    <sample id="140">是的，Coscript经过了质量检查。为了确保验证集和测试集的质量，研究人员要求众包工人找到并修改不正确的样本。</sample>
    <sample id="141">现有的资源在依赖上下文的翻译方面存在以下局限性：
1. 它们只支持有限的语篇现象和语言。
2. 它们通常依赖领域知识和人工创建，这限制了它们的可扩展性和适用范围。

这些限制使得评估模型处理依赖上下文翻译的能力变得困难。</sample>
    <sample id="142">00:00
大家好，我将介绍我们关于为实体选择解析间接指代表达（AltEntities 语料库）的工作。我叫 Mohammad Javad Hosseini，这是与 Filip Radlinski、Silvia Pareti 和 Annie Louis 合作完成的。
00:18
我们的目标是了解用户在做出选择时的语言。考虑这个选择性问题：你是指“Easy on Me”还是“I Gotta Feeling”？这里，用户想在两首歌中选择一首。最明显的方法是使用直接指代，例如说出歌曲的名称，“Easy on Me”，或者它的位置，“第一首”。但有时，间接指代更适合进行更自然的对话。这可能发生在用户不记得歌曲名称时，或者发音太相似而难以区分时，或者用户想指定偏好时。这里有一些间接指代的例子，例如“较新的那首”或“不那么有活力的那首歌”。
01:14
这是一个重要问题，对于会话系统以及基准测试大型语言模型对实体的理解都很重要。我们没有找到大型的公开数据集，所以我们使用众包标注收集了一个。我们的数据集涵盖三个不同的领域：音乐、书籍和食谱。
01:39
我们的数据集收集方法强调使用卡通完成任务的非正式性。卡通有三个对话气泡。在第一个气泡中，Bob 说：“还记得我们昨天听的那首歌吗？” Bob 设定了对话上下文。在第二个对话气泡中，Alice 说：“你是指‘Easy on Me’还是‘I Gotta Feeling’？” 这是选择性问题。在第三个对话气泡中，Bob 使用间接指代选择其中一个实体，例如“较新的那首”。我们自动提供第一个和第二个对话气泡，但第三个由标注者填写。第一个对话气泡是从每个领域几个手动提示中选择的。第二个对话气泡，即选择性问题，是这样生成的：
02:42
我们总是使用一个简单的模板：“你是指 A 还是 B？”其中 A 和 B 是从维基百科中采样的。这里是我们使用的不同采样方法。当我们在列表中向上移动时，实体变得更相似，通常更难消除歧义。第一个是随机均匀采样。第二个是当实体具有相似的标题时，例如两本名为“The Return”的书。第三个是当它们在维基百科上具有相似的描述时，最后是当它们在维基百科上具有相似的信息框或属性时，例如同一流派或同一艺术家的歌曲。
03:32
当我们向标注者展示这些选择性问题时，他们知道这些实体的名称，但他们不一定了解这些实体。所以我们所做的是，我们展示了一些关于这两个实体的背景知识。对于歌曲，我们只是展示了一个指向每首歌曲的谷歌搜索链接。然后要求标注者至少听一部分歌曲并阅读有关每首歌曲的信息。这里，例如，是“Easy on Me”这首歌的谷歌搜索结果。
04:09
对于食谱和书籍领域，我们展示了一些来自维基百科的背景文本。对于食谱，我们还展示了它们的图片，同样来自维基百科，这样标注者就知道它们长什么样子了。
04:29
然后我们要求标注者选择其中一个实体，例如这里的第一个，并使用三到五个间接指代表达来描述它们。例如，“有钢琴音乐的那一个”。这里是我们数据集中的一些随机例子。例如，在音乐选择中，“没有歌词的那一个”。在书籍选择中，“不是关于那个 12 岁男孩的那一个”。在食谱选择中，“来自阿塞拜疆的”。
04:57
AltEntities 语料库包含大约 6,000 个跨三个领域的选择性问题，以及大约 42,000 个间接指代表达。使用 T5 XL 模型的结果（准确率）总结如下。如果语言模型可以访问与标注者相同的背景知识，则准确率非常高，约为 92-95%。但这并不现实。如果语言模型可以访问部分重叠的背景知识，则准确率在 82-87% 之间，这更现实。例如，当语言模型检索背景知识时。如果语言模型（T5 XL）只能访问实体名称，则准确率仅为 60%。因此，还有很大的改进空间。我们还表明模型是领域通用的。这里是我们的数据集链接。谢谢！</sample>
    <sample id="143">该方法与以下策略进行了比较：

*   **wait-k**
*   **LA (Local Agreement)**
*   **CAAT (Cross-Attention based Adaptive Triggering)**
*   **EDAtt (Encoder-Decoder Attention)**（提案方法）</sample>
    <sample id="144">这篇论文的作者所属机构是 LIA, Avignon Université; LS2N, Nantes Université; Clinique des données, CHU de Nantes; Zenidoc。</sample>
    <sample id="145">演讲者的名字是 Jenny Liang。</sample>
    <sample id="146">演讲者首先介绍了对话摘要的背景，它通过提取对话中的关键信息来生成简洁的摘要，并广泛应用于客户服务、医疗咨询、会议记录等多种场景。然而，现有模型生成摘要仍存在常见错误，其中遗漏信息是影响摘要质量的关键因素，导致摘要不完整、关键事实丢失。演讲者指出，虽然大型预训练语言模型在摘要生成方面取得了显著进展，但很少有研究系统地分析或解决遗漏问题。

随后，演讲者展示了对话摘要中遗漏问题的严重性。通过分析来自五个领域和六个预训练模型生成的摘要，结果显示即使是最先进的模型，遗漏率仍然很高，约70%的生成摘要存在遗漏问题。此外，遗漏的信息在对话中的分布是随机的，与对话的长度和领域无关，这表明当前模型识别关键信息仍然面临挑战。

为了更好地分析和解决遗漏问题，演讲者提出了一个名为“遗漏检测”的新任务，即预测摘要中遗漏了哪些对话的语篇。为了支持这项任务，他们构建了一个名为“OLDS”的新数据集，该数据集基于五个现有基准，包含来自不同抽象模型和解码策略生成的10种候选摘要。为了确保标签质量，他们结合了自动检测和人工评估。

在分析部分，演讲者探讨了三种基线模型（成对分类、序列标注和指针网络）在遗漏检测任务上的表现，并使用准确率、召回率和F1分数进行评估。结果显示，F1分数约为50%，这表明遗漏检测是一个具有挑战性的任务，需要更先进的检测模型。此外，他们还研究了基于遗漏信息的摘要优化。通过使用一种后编辑方法，将候选摘要与检测到的遗漏内容连接起来作为输入，模型的性能得到了显著提升，这表明遗漏检测是提高对话摘要质量的一个有前景的方向。</sample>
    <sample id="147">这篇论文有3位作者：Myra Cheng、Esin Durmus和Dan Jurafsky。</sample>
    <sample id="148">大家好，我是来自特伦托大学和布鲁诺凯斯勒基金会的萨拉·帕皮。
我将简要介绍我们题为《注意力作为同声传译的指南》的论文。
这是与马特奥·内格里和马尔科·图尔奇的合作成果。
什么是同声传译？
同声传译（SimulST）是将口语实时翻译成另一种语言文本的过程，从而实现跨语言交流。
当前的同声传译模型存在哪些问题？
通常需要训练特定的架构，引入额外的模块进行优化。
训练过程漫长且复杂（例如，不同的优化目标）。
需要训练和维护多个模型，以实现不同的延迟制度（例如，1秒、2秒等）。
我们的解决方案是什么？
首先，使用现有离线语音翻译模型，无需重新训练或为同声传译采用特定架构。
其次，只使用一个模型来处理每个延迟制度，并通过特定参数控制延迟。
第三，利用模型通过注意力机制在音频输入和文本输出之间已获得的知识。
您可以在右侧看到一个例子。
我们的解决方案：EDAtt，即编码器-解码器注意力。
这是一种策略，我们根据注意力指向的位置决定是否发出部分翻译。
如果注意力没有集中（即其总和低于某个阈值α）指向最后λ个语音帧，则会发出一个单词，这意味着接收到的信息足够稳定。
例如，如果我们收到一个包含“I am going to talk about...”的语音块。
我们的模型预测出德语翻译。
我们将查看交叉注意力权重。
我们会发现前两个单词指向最早接收到的语音帧。
而最后一个单词指向最后接收到的λ个语音帧。
这意味着前两个单词将被发出。
然而，由于交叉注意力总和高于某个阈值α，我们不会发出最后一个单词。
我们将等待另一个语音块。
如果我们继续并接收到另一个语音块，我们的模型会预测出另外三个单词。
我们将查看交叉注意力权重。
我们会发现没有单词指向最后λ个语音帧。
这意味着这三个单词都将被发出。
如果我们看EDAtt的主要结果，我们会在图中绘制同声传译结果。
其中，一侧是衡量翻译质量的BLEU分数，另一侧是平均延迟。
我们还考虑了计算感知平均延迟，它考虑了模型预测输出的计算时间。
我们希望曲线在此图上尽可能高。
但我们也希望它们向左移动。
我们将与也应用于离线模型的流行策略进行比较，即wait-k策略和局部对齐。
我们还将与专门为SimulST量身定制的现有技术架构进行比较。
这些是德语同声传译策略的所有结果。
我们看到EDAtt优于所有应用于离线模型的策略。
因为它们的曲线向左移动了。
我们还看到，如果考虑实际经过时间或计算感知时间，EDAtt是最快的策略。
您想了解更多吗？
阅读我们的论文，发现更多结果！
我们还开源了代码和模型，以及同声传译输出，以方便我们工作的可重现性。
谢谢您的关注。
</sample>
    <sample id="149">是的，数据集是公开的。</sample>
    <sample id="150">这段演讲介绍了一个名为MeetingQA的抽取式问答数据集，该数据集基于会议记录中的问题及其对应的答案句。该数据集由来自AMI语料库的约100小时手动转录的多方会议记录构成。研究者通过标点符号和问题长度筛选问题，并招募标注员标记答案句子，获得了较高的标注一致性。

MeetingQA数据集总共有7,735个问题，来自166场不同的会议，并划分了训练、开发和测试集。数据集的统计数据显示，30%的问题是无答案的，40%的问题包含多跨度（不连续句子）答案，48%的问题包含多说话人答案。此外，54.4%的问题是是非题，且通常能引发详细的讨论和观点。大约50%的问题是寻求意见的，20%的问题是修辞性的，70%的多说话人答案包含一些分歧。会议记录的平均长度为5.9K字，问题的平均长度为12字，答案的平均长度为35字。人类在该数据集上的表现较高，F1分数为84.6。

研究者在实验中使用了多种方法，包括针对短上下文模型的情境检索、单跨度模型（预测第一个到最后一个相关句）和多跨度模型（使用词元分类任务），以及利用MediaSum数据集中访谈问题的自动标注答案跨度进行银数据增强。

在微调实验结果中，模型与人类表现之间存在超过25个F1点的差距。短上下文模型（如RoBERTa）略优于长上下文模型（如Longformer）。多跨度模型的表现略低于或与单跨度模型相当。在零样本学习实验中，模型与人类表现之间存在约50个F1点的巨大差距。银数据增强被证明是有效的，而更大的指令调整模型（如FLAN-T5 XL）则获得了与其余模型相当的性能。

错误分析显示，模型在识别修辞性问题上表现不佳，尤其是在零样本学习设置中。单跨度模型的预测包含更多不相关的句子。模型在识别哪些说话人回答了问题方面也存在困难，这在零样本学习设置中更为明显。

总的来说，MeetingQA是一个基于会议期间提出的开放式和讨论性问题，且充满挑战的问答数据集。现有问答模型在MeetingQA上的表现与人类表现之间存在显著差距，在微调和零样本学习设置中分别存在25和50个F1点的差距，这表明该数据集远未被完全解决。</sample>
    <sample id="151">00:00
大家好，我叫Ying，我的同事志阳和我将展示我们关于MultiInstruct的研究，即通过指令微调改进多模态零样本学习。
00:11
因此，随着大型语言模型的进步，许多工作开始探索重用预训练语言模型的新学习范式，以参数和数据高效的方式处理不同的下游任务。最近，许多研究表明，指令微调使大型语言模型能够通过遵循自然指令以零样本方式执行看不见的任务。
00:36
然而，大多数先前关于指令微调的工作都集中在提高仅语言任务的零样本性能上，而计算机视觉和多模态任务则被忽略了。
00:48
因此，在这项工作中，我们想研究多模态预训练模型上的指令微调是否真的能提高对未见多模态任务的泛化能力。
01:00
此外，在我们的研究时，我们发现NLP和多模态指令数据集的可用性存在显著差异。存在超过1600个仅语言的指令任务。然而，没有大规模的、公开可用的多模态指令任务。因此，这促使我们构建一个多模态指令微调数据集。
01:28
在这里，我们展示了MultiInstruct，第一个多模态指令微调基准数据集，它包含62个不同的多模态任务，涵盖10个广泛类别。这些任务源自21个现有的开源数据集，每个任务都配有5个专家编写的指令。
01:50
为了研究我们提出的数据集上的多模态指令微调，我们将OFA（One For All）作为一个统一的多模态预训练模型作为我们的基础模型。OFA使用统一的词汇表来表示语言、图像标记和边界框的坐标。
02:07
这里我们展示了我们MultiInstruct数据集的一些示例实例。为了统一处理各种输入和输出数据类型，我们遵循OFA的方法，并将所有任务格式化为统一的序列到序列格式，其中输入文本、图像、指令和边界框都在相同的标记空间中表示。
02:31
好的，现在我将讨论多模态指令微调。
02:35
对于训练数据集，我们使用来自9个组的53个任务进行训练，每个任务抽取10000个实例。对于测试，我们保留整个常识推理组进行测试。我们从VQA和杂项组中选择额外的5个任务。我们使用每个任务测试集中的所有实例。我们还从自然指令数据集的测试集中随机抽取20个任务作为NLP的未见任务。
03:02
所以我们使用预训练的OFA-Large模型（472M）作为基础模型。在训练期间，我们将所有任务的所有实例混合在一起。每个实例都与它的五个指令模板之一随机组合。在测试期间，对于每个任务，我们通过使用其中一个五条指令在每个实验中评估模型来进行总共五个实验。我们报告了所有五个实验的平均性能和最大性能以及性能的标准偏差。
03:32
如果任务是多模态分类任务（视觉蕴涵、视觉空间推理、自然语言视觉推理和灾害类型分类），我们报告准确率。如果它是多模态生成任务（常识VQA、文本VQA、有根据的VQA、视觉文本提取和视觉对话），我们报告Rouge-L。对于NLP任务，我们也报告Rouge-L。
03:45
我们还引入了一个名为“敏感性”的额外评估指标。它衡量模型对同一任务的各种指令的敏感性。这衡量了模型在面对指令措辞上的轻微变化时，能否始终如一地为同一任务产生相同输出的能力。
04:02
这是我们的主要结果。我们可以看到，指令微调可以显著提高OFA在未见多模态任务上的性能。此外，来自自然指令数据集的迁移学习也可以使指令微调受益。
04:20
这里我们可以看到，随着任务数量的增加，模型取得了更好的性能，同时降低了敏感性。
04:30
我们还做了一项实验，我们使用一条指令与五条指令进行比较。我们可以看到，使用更多指令可以提高模型的整体性能，并大大降低其敏感性。
04:44
所以这显示了不同微调策略对模型敏感度的影响。正如我们所看到的，通过从自然指令数据集中进行迁移学习，模型可以达到比原始OFA模型更好的敏感度。
05:02
我们还可以看到，从自然指令数据集中进行迁移学习可以帮助OFA在自然指令数据集上实现更好的性能。
05:14
总的来说，我们提出了第一个大规模多模态指令微调数据集。它包含来自10个广泛类别的62个多模态任务。我们通过指令微调显著提高了OFA的零样本能力。我们探索了几种迁移学习技术并展示了它们的好处。我们设计了一个新的度量标准——敏感性。
05:30
还有一件事！我们正在收集一个更大的多模态指令微调数据集，其中包含约150个额外的视觉-语言任务，我们将很快发布它们！这是我们的数据和模型的二维码。谢谢。</sample>
    <sample id="152">视频讲述了探索大型语言模型在古典语言学中的应用。研究者 Frederick Riemenschneider 及其团队旨在创建针对古希腊语和拉丁语优化的新语言模型，并解决现有模型面临的挑战。

现有模型（如 Latin BERT 和 Ancient Greek BERT）是单语编码器模型，且存在数据预训练数据集噪音大、未评估等问题。为克服这些限制，研究者设定了四个目标：使现有模型可比较、推动技术前沿、探索模型架构，并引入多语模型。

为此，他们预训练了 GreBERTa（单语 RoBERTa 模型）、GreTa（单语 T5 编码器-解码器模型）用于古希腊语，以及 PhilBERTa 和 PhilTa（多语编码器和编码器-解码器模型）用于古希腊语、拉丁语和英语。

在预训练数据方面，团队除了使用 Open Greek &amp; Latin，还利用了之前未使用的资源（如 Greek Medieval Texts、Patrologia Graeca）和从互联网档案馆（Internet Archive）收集的新语料库。为了处理互联网档案馆中被错误识别为乱码的古希腊文本，他们开发了一种方法，通过搜索被错误转录的希腊停用词（如“γαρ”被转录为“yap”）来识别希腊文本，并重新使用希腊 OCR 设置进行扫描。通过严格的后处理，他们创建了一个高质量的预训练语料库。对于多语模型，他们还使用了 Corpus Corporum for Latin 和与古代相关的英语文本。

在评估方面，研究者关注了词性标注、依存句法分析和词形还原三个任务。结果显示，在依存句法分析方面，他们的模型明显优于现有技术水平。在词性标注任务中，GreTa 的编码器在经过更多训练后，性能接近本地编码器模型，这表明 T5 模型的编码器与本地编码器模型在行为上存在根本差异。在词形还原方面，他们的模型表现出色，古希腊语性能提升了 5 个百分点，拉丁语也有所提高。在语义和世界知识方面，虽然他们的模型表现优异，但多语模型和单语模型之间没有显著差异。

总结而言，该研究提出了新的强大语言模型，它们从头开始初始化，涵盖了编码器-解码器架构，并支持多语。他们还引入了高质量的预训练数据集和严格的评估方法，实现了最先进的结果，并分析了多语性在语言模型中的影响。</sample>
    <sample id="153">本研究旨在于文本生成图像模型中解决歧义问题。作者首先收集了一个名为TAB的基准数据集，其中包含多种歧义类型。接着，他们提出了两种框架来处理这些歧义：

1. **问答式文本生成图像消歧（QA-TIED）**：该框架利用GPT-neo生成澄清性问题，用户根据自己的意图提供答案，从而得到一个消除歧义的提示。
2. **视觉设置式文本生成图像消歧（VS-TIED）**：该框架通过GPT-neo生成多个可能的视觉设置，用户选择最符合其意图的设置，同样生成一个消除歧义的提示。

为了评估这些消歧框架的效果，作者采用了自动评估和人工评估相结合的方式。自动评估通过视觉问答（VQA）模型判断生成图像是否忠实于用户的意图。如果VQA模型确认用户意图在图像中得到满足，则认为该图像是忠实的。

研究发现，在处理不同类型的歧义时，模型表现存在差异。整体而言，消歧过程对生成忠实图像具有积极影响。此外，自动评估结果与人工评估结果具有良好的一致性，表明所提出的自动评估框架是可靠的。研究还包括了其他详细发现和讨论，鼓励读者查阅论文以获取更多信息。

总结来说，这项研究致力于通过问答或多视觉设置的方式消除文本生成图像提示中的歧义，并提出了有效的评估方法来验证消歧的效果，最终提高了生成图像的忠实度。</sample>
    <sample id="154">论文的作者来自特伦托大学（University of Trento）和布鲁诺·凯斯勒基金会（Fondazione Bruno Kessler）。</sample>
    <sample id="155">演讲者的名字是 Javad Hosseini。</sample>
    <sample id="157">本报告介绍了一种名为 SDDS 的对话摘要模型，该模型通过融合静态和动态图结构来克服现有方法在对话结构建模方面的不足。现有方法主要依赖预计算的静态图，但这些方法容易受到外部语言工具可靠性的影响，并可能导致误差传播，而且固定图无法动态适应下游任务。

SDDS 模型包含四个主要组件：
1. **话语编码器（Utterance Encoder）**：将对话中的话语编码成向量表示。
2. **静态图构建（Static Graph Construction）**：使用四种启发式对话结构建模方法构建话语之间的关系：
    - **语篇分析图（Discourse Parsing Graph）**：通过语篇分析工具构建话语之间的依赖关系。
    - **关键词共现图（Keywords Co-occurrence Graph）**：通过计算话语中共同关键词的数量来捕捉话题相关性。
    - **说话者关系图（Speaker Relation Graph）**：利用滑动窗口统计说话者共同出现的频率，理解说话者之间的互动模式。
    - **话语位置图（Utterance Position Graph）**：通过话语之间的相对距离来捕捉位置信息。
3. **静态-动态图模块（Static-Dynamic Graph Module）**：
    - **静态图融合（Static Graph Fusion）**：使用 1x1 卷积层将不同静态图的邻接矩阵融合成一个统一的静态关系表示。
    - **动态图模块（Dynamic Graph Module）**：采用多头注意力机制，根据话语的深度向量表示动态计算它们之间的语义关系。
    - **融合模块（Fusion Module）**：将动态图的关系矩阵和静态图的邻接矩阵组合成一个统一的图结构。
4. **摘要生成器（Summary Generator）**：通过双重交叉注意力机制（在原始自注意力层之上增加图注意力层）将融合后的图表示整合到生成过程中，从而生成最终的摘要。

这种方法旨在通过在模型训练中动态学习和融合图结构，避免了对外部语言工具的过度依赖，并提高了对话摘要的准确性和鲁棒性。报告最后提供了代码和数据的 GitHub 链接。</sample>
    <sample id="158">好的，这是视频内容的概要：

该视频介绍了双缓存（Dual Cache）方法，用于解决长文档中的神经共指消解问题。共指消解的任务是识别文本中指代相同实体或概念的提及，并将其链接起来。传统方法需要枚举所有可能的提及对，导致计算复杂度和内存消耗呈二次方增长，使得处理长文档效率低下。

为了解决这一问题，研究者们提出了基于缓存的方法，通过使用固定大小的缓存来存储实体表示，将复杂性降低到线性水平。然而，在长文档中，话题的频繁切换会导致实体提及分散在文本的各个部分，使得LRU（最近最少使用）缓存策略的缓存未命中率较高。

因此，研究者提出了双缓存方法，它包含一个本地缓存（L-cache）和一个全局缓存（G-cache）。L-cache 使用 LRU 策略存储局部实体，G-cache 则使用 LFU（最不常用）策略存储全局实体。当遇到新的提及时，模型首先判断它是否代表一个新实体，或者是否属于缓存中已有的实体。如果提及是一个新实体，或者更新了一个现有实体，就会评估其频率。如果频率足够高，该实体会被转移到 G-cache。否则，它会被添加到 L-cache。当缓存满时，L-cache 采用 LRU 策略进行淘汰，G-cache 则采用 LFU 策略进行淘汰。

在公共基准测试中，双缓存方法在 LitBank 和 OntoNotes 数据集上的性能优于基线方法，即使这些基线使用了无界内存。在 WikiCoref 数据集上，双缓存方法虽然在 F1 分数上略低于无界内存的基线，但速度明显更快。此外，该方法还在一个3万词的书籍级文档（《动物庄园》）上进行了评估，结果显示双缓存方法在性能差距上比基线方法更大。

分析结果表明，双缓存显著降低了缓存未命中率。与单缓存方法相比，双缓存方法在不同方法中始终具有最高的性能/成本比，从而在效率和性能之间取得了更好的平衡。

总结来说，双缓存方法通过将局部和全局实体分别存储在 L-cache 和 G-cache 中，有效地解决了长文档共指消解的问题。它在公共基准测试中表现出色，显著降低了缓存未命中率，并且在成本效益方面优于单缓存方法。</sample>
    <sample id="159">00:00
Hi everyone. I'm Kostov Sinha and I'm pleased to welcome you to our talk of our ACL 2023 paper, language model acceptability judgments are not always robust to context. This is a joint work with John Gauthier, Aaron Muller, Kanishka Mishra, Karen Funtis, Roger Levy, and Adina Williams.
大家好。我是科斯托夫·辛哈，很高兴欢迎大家收听我们ACL 2023的论文演讲，主题是“语言模型的可接受性判断并非总是对上下文保持稳健”。这是我与约翰·高提尔、亚伦·穆勒、卡尼什卡·米斯拉、凯伦·芬特斯、罗杰·利维和阿迪娜·威廉姆斯的共同研究。

00:19
So, in this work, we revisit the minimal pair paradigm. So, the minimal pair paradigm basically evaluates language models on top of acceptability judgments, which can also include grammaticality, like Blimp Syntax Jim or acceptability in terms of stereotypes, such as Crowspairs. And in this minimal pair paradigm, the typical way to evaluate language models is that you show a like a acceptable sentence or a grammatical sentence, and then you show an unacceptable sentence or an ungrammatical sentence, and then the hope is that the model basically puts more probability to the acceptable sentence.
因此，在这项工作中，我们重新审视了最小对范式。最小对范式基本上是评估语言模型在可接受性判断方面的能力，其中也包括语法正确性，比如Blimp Syntax Jim，或者在刻板印象方面的可接受性，比如Crowspairs。在最小对范式中，评估语言模型的典型方法是展示一个可接受的句子或语法正确的句子，然后展示一个不可接受的句子或语法错误的句子，然后期望模型基本上将更高的概率分配给可接受的句子。

01:00
The current MPP pipeline basically doesn't allow us to evaluate models acceptance towards longer sentences. These days, large language models are coming up with longer and longer context windows, so it's crucial that we evaluate the models acceptability on throughout the context window. And that is what we are trying to do here. We are trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences.
当前的MPP流水线基本上不允许我们评估模型对长句的接受度。如今，大型语言模型正不断推出更长的上下文窗口，因此，在整个上下文窗口中评估模型的可接受性至关重要。这就是我们在这里尝试做的事情。我们试图重新审视MPP流水线，通过要求模型在更长的序列上评估可接受性。

01:32
So, that is the approach. So, what we do is that to simulate these longer sequences, we revisit the datasets themselves and then we recreate sentences by choosing uh like acceptable or unacceptable sentences from those datasets. So, for example, here we have chosen like a typical pair of grammaticality from the Blimp dataset, from the Adjunct Island uh case. And what we do is that to recreate like longer sequences, and which are acceptable and which has the same matching of the grammatical structure, we extract grammatical sentences from Adjunct Island and then we added it as a prefix to both the acceptable query and the unacceptable query.
这就是我们的方法。因此，为了模拟这些更长的序列，我们重新审视了数据集本身，然后通过从这些数据集中选择可接受或不可接受的句子来重新创建句子。例如，在这里，我们从Blimp数据集的Adjunct Island案例中选择了一对典型的语法正确性句子。我们所做的是，为了重新创建更长的序列，并且这些序列是可接受的，并且具有相同的语法结构匹配，我们从Adjunct Island中提取了语法正确的句子，然后将其作为前缀添加到可接受的查询和不可接受的查询中。

02:17
We can do the same thing by choosing unacceptable sentences from the same matching and that could also like be used to test the models acceptability.
我们可以通过从同一匹配中选择不可接受的句子来做同样的事情，这也可以用来测试模型的可接受性。

02:45
And we could also do the same by choosing sentences from a different subset or a different dataset. So, that is what we call as the mismatched scenario. So, here the sentences are still coming from relevant data sets, but it's not from the same dataset that you are evaluating with.
我们也可以通过选择来自不同子集或不同数据集的句子来做同样的事情。这就是我们所说的错配情景。在这里，句子仍然来自相关数据集，但它并非来自你正在评估的同一数据集。

02:59
And we can do the same for unacceptability case. Finally, we can choose sentences from a completely unrelated domain such as Wikipedia. So, this will tell us like whether the models acceptability judgments are actually impacted by any context. Like whether the context is coming from a different uh subset of the dataset or whether it's like completely irrelevant to the current uh like to the sentence that we are looking at.
我们对不可接受性情况也可以这样做。最后，我们可以从完全不相关的领域选择句子，例如维基百科。因此，这将告诉我们模型的接受度判断是否确实受到任何上下文的影响。例如，上下文是来自数据集的不同子集，还是与我们正在查看的当前句子完全不相关。

03:14
So, how does the model do? So, first we look at the Wikipedia sentences which are completely irrelevant to the current query pair, and there we find that the MPP judgments are mostly robust for arbitrary context length. We increase the context length towards up to 1024 for to max out OPT and GPT2 models and we saw here in the orange dotted line, the MPP judgments are relatively stable.
那么模型表现如何呢？首先，我们来看维基百科的句子，这些句子与当前的查询对完全不相关，我们发现MPP判断在任意上下文长度下都基本稳定。我们将上下文长度增加到1024，以最大化OPT和GPT2模型，我们在这里橙色虚线中看到，MPP判断相对稳定。

03:41
Now, what happens when we choose sentences from the same dataset? So, here we are choosing or creating sentences from acceptable and unacceptable domains, from the same Blimp or Syntax Jim dataset. And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes.
那么，当我们从同一个数据集中选择句子时会发生什么呢？在这里，我们从可接受和不可接受的领域，从同一个Blimp或Syntax Jim数据集中选择或创建句子。我们发现，当你添加可接受的前缀或不可接受的前缀时，MPP判断会显著增加或减少。

04:06
But when we match the structure, that is when we choose the sentences from the same phenomena in Blimp Percentag Jim, we see a massive increase or a massive decrease in of the MPP judgment for the model, depending on whether the chosen prefix is acceptable or unacceptable. Now, this uh and this is very large. Like this effect increases throughout the context length and this would probably affect like newer language models which has large context window.
但是当我们匹配结构时，也就是说，当我们从Blimp Percentag Jim中选择相同现象的句子时，我们发现模型的MPP判断会大幅度增加或减少，这取决于所选前缀是可接受的还是不可接受的。这种影响非常大，它会随着上下文长度的增加而增加，这可能会影响具有大上下文窗口的新语言模型。

04:40
So, why does the match prefix affect the language model judgment so much? So, we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input. And after doing like several of these perturbations, we find that none of these noises are actually making the model uh like change it course in terms of how it shows us the MPP judgment trend.
那么，为什么匹配前缀对语言模型的判断影响如此之大呢？我们进行了一系列分析，尝试通过扰动输入句子来保留相关结构，但同时添加噪声。经过多次扰动后，我们发现这些噪声并没有使模型在MPP判断趋势上改变其方向。

05:10
Basically, we find that the models are sensitive to the perturbed sentences in similar ways. That is, when we perturb the sentences in the acceptable domain, we see similar increase in all the perturbations, and when we perturb the sentences in the unacceptable domain, we see decrease in MPP judgments in similar fashion.
基本上，我们发现模型对扰动后的句子具有相似的敏感性。也就是说，当我们在可接受的领域扰动句子时，所有扰动都显示出相似的增加；当我们在不可接受的领域扰动句子时，MPP判断也以相似的方式下降。

05:31
So, the key takeaways of our work is that language models are sensitive to latent syntactic/semantic features which are shared across the sentences. And the MPP evaluation, the the way that we do it currently with short and single sentence input may not fully capture the language models abstract knowledge throughout the context window. Please read our paper for more details of our experiments. Thank you for listening.
因此，我们工作的关键在于：语言模型对句子之间共享的潜在句法/语义特征敏感。而且，目前我们使用的短句和单句输入方式进行的MPP评估可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以获取更多实验细节。感谢您的聆听。</sample>
    <sample id="160">该方法的第一步将输入词元映射到无序多集词元。</sample>
    <sample id="161">Coscript中包含了55000个脚本。</sample>
    <sample id="163">MASAlign 是一个邻近驱动的方法，基于 TF-IDF 相似度矩阵。</sample>
    <sample id="164">弱监督学习的好处是可以减轻标注的瓶颈。</sample>
    <sample id="165">Wenting Zhao在康奈尔大学ACL 2023上展示了一篇题为“利用相互排斥的解释进行溯因常识推理”的论文。她首先通过一个场景来解释溯因推理：当艾米丽被堵在路上但准时赶上了飞机时，我们可能会认为她的航班被延误了，或者飞机按时起飞了。但是，由于航班要么延误要么按时起飞，这些解释是相互排斥的。溯因推理的目标是根据上下文和结果，从候选解释集合中选择一个合理子集。

Wenting Zhao指出了当前溯因推理方法主要依赖监督学习，但手动标注合理的解释是一个嘈杂和主观的过程。为了解决这个问题，他们提出了LiPoR，一种利用后验正则化的无监督学习方法。LiPoR将解释视为潜在变量，并通过最大化结果的对数似然来消除它们。但仅凭这一点不足以选择合理的解释，因为它没有倾向于解释子集。

因此，他们引入了一个正则化项，它鼓励解释的后验概率分布崩溃为几个合理的解释。具体来说，当解释的熵高于某个阈值时，这个正则化项就会介入，从而优先选择较小的解释子集。LiPoR在αNLI数据集上取得了卓越的成果，其准确性超过了所有无监督基线，甚至比一个强大的零次学习GPT-3模型高出4个百分点以上。总的来说，LiPoR通过利用相互排斥的解释特性，克服了在溯因推理中对大量标注数据的需求，为无监督溯因推理开辟了新的道路。</sample>
    <sample id="166">本研究介绍了一种名为“神经分治推理框架（Neural Divide-and-Conquer Reasoning Framework）”的新型方法，用于从语言复杂文本中检索图像。该方法旨在解决现有视觉语言模型（VLM）在处理复杂文本时性能显著下降的问题。

该研究受到“分治策略”和“双重处理理论”的启发，这两种方法在解决复杂问题和人类思维方面表现出色。“分治”策略通过将大问题分解成小问题并逐步解决，最终整合子问题的解决方案以获得期望的结果；“双重处理理论”则指出人类思维包含两种系统：系统1（直觉与本能）和系统2（理性思考），前者擅长类比推理，后者擅长抽象逻辑推理，适合解决复杂问题。

研究发现，预训练VLM更侧重于系统1的类比推理，在简单文本-图像检索任务中表现良好。然而，当面临语言复杂的文本时，它们的性能会急剧下降，因此，该研究引入了系统2的逻辑推理来处理这类复杂检索任务。

该方法通过结合系统1和系统2的优势，为复杂推理提供了一种有效途径，并且可以与“分治”策略相结合。该框架包含两个主要模块：命题生成器（Proposition Generator）和神经符号推理器（Neural-Symbolic Reasoner）。命题生成器将复杂的命题文本分解为简单的命题陈述，并根据编码表示生成相应的句子，其输出是命题与图像的匹配分数以及命题与图像的推理状态。神经符号推理器整合了简单命题的推理状态和结果，以获取复杂命题在图像上的最终解决方案。

实验结果表明，该方法在测试集上的性能优于基线模型，并且在消融实验中证实了每个模块的有效性。案例分析进一步展示了该方法在处理复杂文本时的过程可解释性。这项研究的贡献在于提出了一个可提高大型语言模型的组合推理和规划能力的神经符号计算方法，以及将“双重处理理论”与“分治策略”相结合用于解决复杂问题。</sample>
    <sample id="167">DEplain-web 中的 756 份文档有一半是手动对齐，另一半是自动对齐的。</sample>
    <sample id="168">CoNLL++ 数据集是通过从 2020 年的路透社新闻中收集数据，并使用 CoNLL-2003 的标注指南进行标注来创建的。</sample>
    <sample id="169">主讲人 David Vilar Torres 介绍了他们关于“提示 PaLM 进行翻译：评估策略和性能”的论文。PaLM 是一个拥有 5400 亿个参数的大型语言模型，它在 7800 亿个标记上进行训练。

他们的研究首次系统地研究了用于机器翻译的大型语言模型提示。他们使用了机器翻译社区的最佳实践来评估翻译能力，包括最新的测试集、与 WMT 提交的比较、最新的机器翻译指标以及基于专家的评估。

研究发现提示对翻译质量有很大影响。即使使用单次提示，超过一半的句子在 BLEURT 分数上也有超过 1 分的差异，在极端情况下，差异可达 40 分。

他们发现，对于几次提示，实际的提示形式影响不大，重要的是提供的示例。示例的质量比与源句的相似度更重要。与从训练数据中选择的示例相比，从高质量的开发数据中选择的示例会带来更好的性能。

在与最先进的机器翻译系统（如 Google Translate）进行比较时，PaLM 的翻译质量接近商业系统。通过 MQM 评估，他们发现 PaLM 的流畅度与最先进的系统相当，但准确性较低，主要是由于遗漏错误。这表明 PaLM 倾向于生成流畅的翻译，即使这意味着省略源句的某些部分。

这项研究为选择提示策略提供了见解和建议，并强调了大型语言模型在机器翻译中的潜力。</sample>
    <sample id="170">00:00
大家好，我叫张雨森，来自宾夕法尼亚州立大学。今天我将介绍我们的工作，XSemPLR：多自然语言和多语义表示中的跨语言语义解析。

00:14
语义解析是一项构建用户查询的语义表示的任务，例如 SQL 和 Lambda Calculus。

00:21
跨语言语义解析是一项将多种自然语言的查询转换为多种语义表示的任务。如图所示，我们需要使用神经模型将多种自然语言的查询转换为 SQL、Lambda 或 FunQL 等。

00:41
现有的跨语言语义解析模型是针对有限任务和应用的特定数据集提出的和评估的。例如：某些自然语言（中文缺失）的覆盖范围不足。

00:58
对某些语义表示（Lambda Calculus 缺失）的覆盖范围不足。

01:03
或者只对特定神经模型进行评估。例如，只有一个单一模型来评估它们。

01:12
为此，我们提出了 XSemPLR。我们提供了一个统一的跨语言语义解析数据集 XSemPLR，该数据集包含多种自然语言和语义表示。它包含：9 个不同领域的数据集，5 个语义解析任务，8 种语义表示，22 种自然语言，涵盖 15 种语系。

01:34
为了更好地评估我们的基准，我们考虑了六种训练和评估设置。第一种是“翻译-测试”。我们使用 Google 翻译 API 将源语言翻译成目标语言。然后使用单语模型进行训练和评估。例如，我们训练一个英文模型处理英文查询，在推理阶段，我们使用翻译 API 将德文查询翻译成英文，然后使用训练好的模型预测 SQL。

02:04
我们还测试了“单语模型”设置。在此设置中，源语言与目标语言相同，例如德语到德语，或英语到英语。我们还通过仅使用 10% 的训练数据训练单语模型来测试“单语少样本”设置。

02:22
我们还测试了“多语言模型”。在该模型中，我们训练一个多语言模型来处理所有语言。例如，我们将德语、英语、中文查询放在一起训练一个多语言模型。在推理阶段，我们可以使用该模型翻译德语查询或中文查询等。

02:48
我们还考虑了“跨语言零样本/少样本迁移”。我们对一种源语言进行训练，然后迁移到另一种语言。在训练期间，我们对英语查询进行训练，或者对英语和德语少样本查询的组合进行训练，以训练一个多语言模型来预测 SQL 输出。

03:11
我们还发现了许多有趣的结果。关于单语模型的分析，我们对两组模型进行了评估，包括 Enc-PTR，即具有基于指针解码器的多语言预训练编码器，例如 XLM-R + PTR、mBERT + PTR。我们还评估了 Enc-Dec，即多语言预训练编码器-解码器模型，例如 mBART、mT5。我们发现 Enc-Dec (mT5) 在所有数据集上都获得了最佳性能！

03:49
我们在多语言设置下对 mT5 和 XLM-R + PTR 进行了评估。我们发现 Enc-Dec/Enc-PTR (mT5/XLM-R) 可以通过在多种语言混合中进行训练来提高。

04:06
我们发现，这是因为大多数主要的自然语言都可以获得性能增益，除了英语性能在 7 个数据集中下降，在 3 个数据集中获得增益。这被称为“多语言诅咒”。

04:24
我们还比较了跨语言性能差距。在这张图中，蓝线代表跨语言少样本迁移，橙线代表跨语言零样本迁移，绿线代表单语设置。我们发现，通过比较绿线和橙线，我们发现对于零样本设置，跨语言迁移性能差距显著。通过比较蓝线和橙线，我们发现对于少样本设置，迁移差距迅速缩短。

04:53
我们还发现了一些其他有趣的结果。例如，Enc-Dec (mT5) 优于以前的工作或取得了可比较的结果。在英语自然语言上进行预训练可以显著提升目标自然语言上少样本的性能。多语言LLM（Codex 和 BLOOM）对于跨语言语义解析任务仍然不足。中文迁移学习和英文单语训练（英 -&gt; 英）具有最大的性能差距，而德语通常具有最小的性能差距。FunQL 优于其他三种语义表示，而 SQL 获得了最差的性能。

05:16
总结一下，我们构建了 XSemPLR，一个用于多自然语言和多语义表示的跨语言语义解析的统一基准。我们对三种代表性类型的多语言语言模型进行了全面的基准研究。我们的结果表明，使用单语训练的 mT5 取得了最佳性能，而值得注意的是，多语言 LLM 仍然不足以执行跨语言语义解析任务。此外，单语训练和跨语言迁移学习之间的性能差距仍然显著。

05:37
欢迎访问我们的论文和代码。感谢收听。</sample>
    <sample id="171">关于水印的现有研究可分为四类：基于参数的水印，基于词汇的水印，基于后门的水印和基于对抗的水印。</sample>
    <sample id="172">不，Codex 和 Bloom 等多语言 LLM 对于跨语言语义解析 (CLSP) 任务来说仍然不足。</sample>
    <sample id="174">ArgAnalysis35K 是一个关于论证质量分析的大规模数据集，包含35,000个论证-分析对。该数据集通过评估论证在0-1的范围内判断其质量。

与现有数据集不同，ArgAnalysis35K 旨在解决以下问题：
1. **论证质量不足**：现有数据集通常从民意调查、公众或众包平台收集论证，导致质量参差不齐。ArgAnalysis35K 的论证主要来源于高水平辩论比赛的演讲，以及专业、中级和新手辩手，确保了论证的高质量。
2. **主题多样性不足**：现有数据集通常只涵盖30-40个具体议题，限制了多样性。ArgAnalysis35K 则基于24个宏观主题（如政治、环境、专制政权等），并尽可能涵盖每个主题下的多个议题，从而增加了多样性。
3. **缺乏深度分析**：现有数据集常常无法深入解释论证背后的细微之处。ArgAnalysis35K 引入了“分析”概念，它不仅是单一的主张或前提，而是对论证为何成立的逻辑链和主观解释的综合。这种分析不同于客观前提，更具深度和说服力。
4. **评分局限性**：传统数据集的评分往往与特定议题挂钩。ArgAnalysis35K 引入了“实例级注释者可靠性”和“相关性模型”。“实例级注释者可靠性”考虑到注释者的偏见可能仅存在于特定议题而非所有议题，从而更有效地利用注释数据。“相关性模型”则为每个论证-分析对在每个主题下分配0-1的相关性分数，使得一个论证可以应用于多个相关主题，提升了论证的通用性。

总之，ArgAnalysis35K 通过大规模、高质量、多样化、深度分析和创新的评分机制，为论证质量分析领域提供了一个独特且强大的资源。</sample>
    <sample id="175">该方法通过连续松弛来近似排列，并允许通过解决方案进行反向传播。</sample>
    <sample id="176">下游 NLP 模型的公平性指的是模型对不同政治倾向的群体（例如左翼和右翼）在处理憎恨言论和错误信息等任务时，是否表现出不同的偏好，从而可能导致某些群体被不公平地对待。例如，如果模型在检测针对少数群体的仇恨言论方面表现出色，但在检测针对多数群体的仇恨言论方面表现不佳，则可能存在不公平性。</sample>
    <sample id="177">演讲者的名字是 Yanis Labrak。</sample>
    <sample id="178">演讲者的名字是Koustuv Sinha。</sample>
    <sample id="179">本次演讲的主题是“Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker”，旨在探讨如何提升大型语言模型（LLMs）的心智理论（ToM）推理能力。

心智理论是指理解他人心理状态的能力，通常通过涉及多个角色的阅读理解任务，特别是虚假信念问题来衡量。在这些问题中，现实情况可能与故事中某些角色的信念不符。研究表明，ChatGPT和GPT-3等大型语言模型在处理虚假信念任务时表现不佳。

为了解决这一问题，研究者提出了SymbolicToM，一种在推理时提高LLMs心智理论推理能力的方法，它使用了明确的图形表示。SymbolicToM通过使用多个图形表示来处理心理状态，因为单一图形不足以表示复杂的心智状态。例如，系统会为每个角色以及角色对其他角色的信念构建不同的信念图。这些信念图是通过一个在推理时运行的图算法生成的，该算法利用了现成的自然语言理解（NLI）和OpenIE模型。

SymbolicToM的概览包括三个步骤：
1. **检测实体和递归提问：** 首先，系统检测问题中的实体，检索相应的信念图，并对问题进行递归，将其转化为对图的实际问题。
2. **检索图捕获的句子：** 接着，系统检索图中捕获的相关句子。
3. **将信息输入LLM：** 最后，将检索到的句子和实际问题输入大型语言模型，以获取最终答案。

实验结果表明：
- **领域内性能：** 在心智理论（ToMi）数据集上，SymbolicToM显著提高了所有测试模型（包括Macaw-3B、GPT-3系列、Flan-T5和LLaMA）的准确率，例如，GPT3-Davinci的准确率提升了65个百分点，Macaw-3B提升了67个百分点，Flan-T5-XXL提升了51个百分点。
- **领域外性能：** 研究者设计了两种新的领域外数据集，用于测试SymbolicToM的泛化能力：
    - **故事结构泛化（D1、D2、D3）：** 通过以不同方式组合原始故事的基本元素来创建。在这个任务上，SymbolicToM在所有模型上都取得了显著的性能提升，甚至使GPT-4等更强的模型能够完全解决这些数据集，例如，GPT-4在D1数据集上的准确率提高了42个百分点。
    - **语言泛化（ParaphrasedToMi）：** 创建了一个语言上更多样化的数据集。SymbolicToM在该数据集上依然表现出益处。

总之，SymbolicToM是一个即插即用的方法，通过推理时算法和明确的图形符号表示，有效提高了大型语言模型的心智理论推理能力，避免了过拟合风险，并提供了更具可解释性的推理过程。它显著提升了LLMs的开箱即用性能，在领域外故事理解方面优于监督式方法，并在新的语言多样性数据集上仍保持其优势。</sample>
    <sample id="180">演讲者的名字是 Myra Cheng。</sample>
    <sample id="181">视频介绍了一项关于“从大型语言模型中提取脚本知识以进行受限语言规划”的工作。该研究旨在解决传统大型语言模型（LLMs）在处理特定约束条件下的语言规划任务时所面临的挑战。

研究首先指出，LLMs 在分解抽象目标为具体步骤方面表现出色，例如制作蛋糕。然而，当目标变得更具体并附带多方面约束时，如“制作草莓蛋糕”或“制作巧克力蛋糕”，LLMs 的性能会显著下降。

为了应对这一挑战，研究提出了一种“先生成后过滤”（over-generate-then-filter）的方法。该方法首先利用 InstructGPT 通过上下文学习生成一系列受限的特定目标，这些目标是基于抽象目标（如“制作蛋糕”）和不同的约束条件（如修饰符、方法、意图）生成的。然后，InstructGPT 会针对每个特定目标过量生成多个候选脚本。最后，通过一个过滤模型，利用相似度评分和关键词匹配来筛选出与目标约束最忠实的脚本。

实验结果表明，尽管 LLMs 在处理特定目标时表现不佳，但通过研究提出的方法，InstructGPT 能够生成更高质量的脚本。研究还发现，InstructGPT 在不同目标类别上的规划性能差异很大。

此外，为了使小型语言模型也能进行受限语言规划，研究团队构建了一个名为 CoScript 的脚本数据集。该数据集包含了55,000个带有约束的脚本，这些脚本是利用所提出的方法从 LLMs 中提取生成的，并且经过人工校对以确保验证集和测试集的质量。对 CoScript 的分析显示，它在生成的特定目标中表现出高度的异质性和多元化。

最后，研究结果显示，在 CoScript 数据集上进行微调的小型语言模型，如 T5，能够生成比大多数大型语言模型更高质量的脚本，这表明经过适当训练的小型模型在处理受限语言规划任务时可以超越大型模型。

总结而言，这项工作确立了受限语言规划问题，评估了 LLMs 在此任务上的能力，并开发了一种有效的方法来提高规划质量。CoScript 数据集被视为一个宝贵的资源，将有助于推动语言规划领域在处理更复杂和多样化目标及约束方面的研究。</sample>
    <sample id="182">根据所给的英文内容，"tropicalism" 指的是描述拉丁裔女性使用的词语，例如 "vibrant" 和 "curvaceous"。这些词语与对拉丁裔女性的刻板印象有关，这种刻板印象认为她们活泼、性感。</sample>
    <sample id="183">作者通过指示语言模型生成人工描写来创建目标群体的描写。例如，作者会要求模型“想象你是一位亚裔女性，描述你自己。”</sample>
    <sample id="184">本文中使用了条件交叉互信息（CXMI）来衡量语境使用情况。</sample>
    <sample id="185">DrBERT 是基于 NACHOS 数据集训练的，而 ChuBERT 则是基于 NBDW 数据集训练的。其中，NACHOS 是由网络抓取的医疗数据组成的开放数据集，而 NBDW 则是来自南特大学医院的匿名医疗记录。</sample>
    <sample id="187">这篇论文有3位作者。</sample>
    <sample id="188">迭代迁移学习是指在不同领域中对主动学习模型进行更新的一种方法。具体来说，新数据集中收集到的所有数据都会用于更新模型。</sample>
    <sample id="189">该数据集旨在了解用户在做出选择时使用的语言。</sample>
    <sample id="190">攻击者通过从嵌入中学习来窃取模型，并提供类似的服务。</sample>
    <sample id="191">这篇论文有三位作者。</sample>
    <sample id="192">这段视频介绍了一种名为“CAME”的优化器，其全称为“Confidence-guided Adaptive Memory Efficient Optimization”。

**背景:**
* **大型语言模型 (LLMs) 训练的挑战:** LLMs 的鲁棒训练通常依赖基于自适应梯度的优化方法。
* **现有优化器的内存消耗:** 广泛使用的优化器（如 Adam、LAMB）需要大量内存来存储每个参数的梯度的一阶和二阶矩估计，导致内存需求翻倍。
* **内存高效优化器的问题:** 现有的内存高效优化器（如 Adafactor）虽然能显著减少辅助内存使用，但通常会牺牲性能。

**挑战:**
如何设计一种优化器，能够同时实现两个目标：
1. **快速收敛:** 像传统的自适应方法一样快速收敛。
2. **低内存使用:** 像内存高效方法一样低的内存使用。

**预备知识:**
* **非负矩阵分解 (NMF):** 这是一种将矩阵 V 分解为两个矩阵 W 和 H 的算法（通常是秩-1 因子），其中所有矩阵都只包含非负元素。对于 m x n 的矩阵，NMF 可以将内存需求从 O(mn) 降低到 O(m+n)，这是一个巨大的内存节省。
* **Adafactor 优化器:** Adafactor 在 NMF 操作中提供了一种解析解，以在秩-1 因子的特殊情况下，实现矩阵 V 和近似矩阵 WH 之间的最小 I-散度。投影可以完全用行和（V1m）和列和（1TnV）来表示，它们都是 V 的线性函数。

**方法:**
* **Adafactor 中的错误更新:** Adafactor 中的 NMF 操作不可避免地会导致深度神经网络训练中的错误更新。与 Adam 相比，Adafactor 的收敛速度较慢，这限制了内存高效优化器的应用场景。
* **置信度引导策略:** 为了解决错误更新的问题，该方法引入了两种场景来处理错误更新，并提出了一种高效的方法来减少不安全更新导致的副作用。具体来说，给定 mt 和 ut，计算它们之间的残差，并将生成的误差作为原始 mt 的分母，以更自适应地进行更新。
* **CAME 优化器:** CAME 优化器通过计算不稳定矩阵 UT，并以与 Adafactor 相同的方式更新 Rt 和 Ct。然后，通过计算近似不稳定矩阵 ST，并将其平方根作为 mt 的分母，以实现自适应更新。

**实验:**
* **数据集:** BookCorpus、Wikipedia、GLUE Benchmark、SQuAD v1.1 和 SQuAD v2.0。
* **模型:** BERT、GPT-2、T5。
* **BERT 训练结果:** 在 BERT 训练中，CAME 实现了显著的改进。与 Adafactor 相比，CAME 在相同的训练步数下，验证准确率提高了约 3.4%。此外，CAME 在 BERT-Large 模型的预训练中，比 Adam 和 Adafactor 表现更好，同时大大降低了内存成本。批次大小从 8K 增加到 32K 时，CAME 的训练效果得到了进一步提升。
* **下游任务结果:** CAME 在两个批次大小下，在典型下游任务上的 BERT-based 模型表现与基线相当，同时具有更低的内存成本。
* **内存成本比较:** 在预训练 BERT-Large 模型时，Adam 和 LAMB 消耗的内存最多，而 CAME 优化器比现有的 SM3 内存高效优化器拥有更低的内存占用。

**结论:**
1. **错误更新的启发:** 受到现有内存高效优化器中错误更新的启发，研究人员提出了一种置信度引导的内存高效优化器 CAME。CAME 支持自适应的置信度引导更新，通过预测更新和生成更新之间的残差来引导更新。
2. **出色的性能:** 广泛的实验表明，CAME 在大型语言模型训练任务中取得了出色的性能。
3. **大型批次训练的扩展:** CAME 在大型批次训练中表现良好，这为现有内存高效优化器提供了一个重要的扩展。</sample>
    <sample id="193">用于创建初始数据集的注释者数量没有明确说明。</sample>
    <sample id="194">这篇论文的作者所属机构包括华盛顿大学、卡内基梅隆大学以及艾伦人工智能研究所。</sample>
    <sample id="195">本研究提出了一个名为“分层问题分解树上的推理”（RoHT）框架，旨在解决可解释问答（XQA）中的挑战。现有的XQA方法存在局限性，例如神经符号方法依赖于结构化知识库（KB）且召回率有限，而基于分解的方法仅使用自由文本语料库作为知识来源。

RoHT框架包含两个阶段：
1. **理解复杂问题**：该阶段构建一个分层问题分解树（HQDT）。HQDT的根节点是原始的复杂问题，非根节点是其子问题，叶节点是原子问题。这个过程通过基于BART的问题分解器和问题生成器来完成，并为每个节点计算一个不确定性分数。
2. **HQDT上的概率推理**：该阶段通过递归方式从根节点到叶节点进行推理，包括三个步骤：
   - **调度器**：为每个节点确定合适的知识来源（KB、文本或通过解决其子问题）。
   - **执行器**：从相应的知识来源获取带有概率的答案。
   - **聚合器**：聚合来自所有知识来源的候选答案，并输出最佳答案。

实验评估了RoHT框架在KQA Pro和Musique两个挑战性复杂问答数据集上的性能。KQA Pro数据集通过随机丢弃50%的KB三元组并使用维基百科作为补充文本语料库来模拟不完整的KB。Musique数据集则将Wikidata作为补充KB。

结果表明，RoHT框架在以下方面表现出色：
- **KQA Pro数据集**：在仅使用不完整KB时，RoHT_KB模型优于现有的KBQA方法。在添加维基百科作为补充文本语料库后，RoHT_Mix模型取得了显著的改进，证实了整合KB和文本知识的有效性。
- **Musique数据集**：在仅使用给定段落时，RoHT_Text模型在F1分数上比最先进的EXISA方法提高了11.9个百分点。在同时使用文本和KB时，RoHT_Mix的性能也显著优于Transfomer，并且与RoHT_Text相比，进一步验证了通过KB补充文本信息的效果。

总的来说，RoHT框架通过结合分层问题分解和来自异构来源的知识融合，显著提高了复杂问题问答的可解释性和准确性。</sample>
    <sample id="196">在“I saw Bart and Lisa”中，"saw"是支配词，它在被协调的两个词的左侧。</sample>
    <sample id="197">目前最好的对话系统模型是 ABC-Eval。</sample>
    <sample id="198">我们需要在整个上下文窗口中评估模型的可接受性，因为现在的大型语言模型具有更长的上下文窗口。因此，评估模型在整个上下文窗口中的可接受性变得至关重要。</sample>
    <sample id="199">多语言训练会导致英语表现下降，但其他语言的表现有所提高。</sample>
    <sample id="200">注释者提前知道该实体。</sample>
    <sample id="201">评估使用的 MT 指标是 BLEURT 和 MQM。</sample>
    <sample id="202">从所提供的英文内容来看，并没有直接回答您的问题：“泛化中的回归是否会影响特定的 NER 类型？”。

内容中提到了以下几点：
* 模型架构：Transformer 模型通常泛化能力更好。
* 模型大小：较大的模型通常泛化能力更好。
* 微调示例的数量：更多的微调示例也有助于更好的泛化。

然而，在讨论“什么导致性能下降？”时，研究者提出了两个假设：
1. **自适应过拟合（Adaptive overfitting）**：由于重复使用相同的测试集而导致的过拟合。
2. **时间漂移（Temporal drift）**：由于训练数据和测试数据之间时间间隔的增加而导致的性能下降。

研究结果表明：
* **未观察到自适应过拟合**：即使 CoNLL-2003 数据集已被使用了20多年，也没有出现自适应过拟合导致的性能下降。
* **时间漂移是主要原因**：性能下降是由于时间漂移造成的，即随着时间间隔的增加，性能会下降。

因此，虽然提到了模型泛化中的性能下降（performance drop），并将其归因于“时间漂移”，但**没有具体说明这种下降是否或如何影响特定的 NER 类型（如人名、组织名、地点名等）**。</sample>
    <sample id="203">根据所提供的英文内容，NLP 中的立场之所以重要，是因为它能导致技术在不同人群中出现系统性的性能差异，这被称为“设计偏差”。

具体来说：
- 立场是指人们因其人口统计学、身份和生活经历而持有的观点。
- 作为研究人员，立场会影响研究过程及其结果。
- 数据集和模型本身虽然没有人口统计学身份或生活经历，但它们聚合了真实人们的判断和意见，因此可以代表某些立场而非其他立场。
- 随着 NLP 任务变得更加主观和社会化，理解模型和数据集的立场变得越来越重要。
- 难以表征这些立场，因为并非所有决策都被记录下来，并且许多模型都隐藏在 API 背后。

总而言之，NLP 中的立场之所以重要，是因为它会影响模型的公平性和适用性，可能导致某些人群被忽视或受到不利影响。</sample>
    <sample id="204">根据视频内容，我无法回答 BLOOM 这样的多语言 LLM 是采用适配器微调还是完整微调。</sample>
    <sample id="205">讲者尚彬·冯介绍了他们的研究项目“从预训练数据到语言模型再到下游任务：追踪导致不公平NLP模型的政治偏见轨迹”。该项目探讨了语言模型训练数据中政治新闻媒体的广泛覆盖，这既带来了学习多样化视角的益处，也带来了潜在的公平性问题，因为这些政治观点本身就带有社会偏见。

为了解决这些问题，研究团队提出了两个主要研究目标：
1. **评估语言模型的政治倾向**：他们开发了一种基于政治罗盘测试的自动评估方法，以支持编码器和解码器语言模型，并确定预训练数据在塑造这些政治偏见中的作用。初步结果表明，现有的语言模型确实具有不同的政治倾向，在政治罗盘上占据所有四个象限。GPT-4被发现是最自由的语言模型，而GPT-3系列总体上比BERT系列及其变体更具社会自由主义倾向。
2. **考察不同政治倾向的语言模型在下游任务中的表现**：通过在六个不同的党派语料库上对语言模型进行进一步预训练（分为新闻和社交媒体，并根据其政治倾向划分），研究人员发现，语言模型的意识形态坐标也相应地发生了变化。例如，在左倾Reddit语料库上进一步训练的RoBERTa模型，其政治偏见在自由主义方向上发生了显著偏移。
他们还通过比较预训练语料库在唐纳德·特朗普担任美国第45任总统之前和之后的变化，研究了语言模型是否能识别社会中普遍存在的两极分化。结果显示，语言模型在2017年后普遍表现出远离中心的政治倾向。

最后，讲者介绍了他们的下游任务评估，包括仇恨言论检测和虚假新闻检测。研究结果揭示了一个模式：左倾语言模型在检测针对社会少数群体的仇恨言论方面表现更好，但在检测针对“权力群体”（如白人和男性）的仇恨言论方面表现较差；反之，右倾语言模型则相反。在虚假新闻检测方面也出现了类似的趋势，左倾语言模型在检测来自对立政治倾向的虚假信息方面表现更好。

这些发现揭示了语言模型政治偏见带来的紧迫公平性问题。讲者提出了一个难题：“是否要‘净化’（sanitize）预训练数据”，如果不对预训练数据中的政治观点进行净化，偏见将传播到语言模型和下游任务中，最终导致不公平。然而，如果尝试净化，则可能面临审查或排斥的风险，并且难以确定什么是真正的中立内容。这是一个类似于“电车难题”的道德困境，呼吁研究界重视并解决语言模型政治偏见带来的伦理挑战。</sample>
    <sample id="206">他们使用了RoBERTa-base模型进行迁移学习。</sample>
    <sample id="207">最近用于评估 PaLM 能力的测试集是 WMT 的最新测试集。</sample>
    <sample id="208">作者最终提出了三条建议。</sample>
    <sample id="209">与最强的基线相比，提议的方法的准确率提高了约 30%。</sample>
    <sample id="210">演讲者的名字是 Shuheng Liu。</sample>
    <sample id="211">是的，该数据与现有基准相比较，并且在自动文本简化方面，提供了额外的、基于BERT模型的结果，作为将来进一步研究的基准。</sample>
    <sample id="212">论文中，他们使用 CoScript 数据集对 T5 模型进行了微调，以便在受限语言规划方面训练更小但更专业的模型。</sample>
    <sample id="213">OFA 模型被用作研究多模型指令调整的基础模型。</sample>
    <sample id="215">本报告讨论了协调的依存结构，并提出了支持对称协调结构而非非对称结构的新论点。报告首先介绍了四种不同的依存结构表示方式：Universal Dependencies (Bouquet/Stanford)、链式（Chain/Moscow）、连词中心式（Conjunction-headed/Prague）和多头式（Multi-headed/London）。其中，前两种是非对称的，它们将协调结构中的第一个连词作为头部，而后两种是对称的，它们要么将连词作为头部，要么将所有连词都视为协调结构的头部。

本研究的核心论点基于“依存长度最小化原则”（Dependency Length Minimization, DLM），该原则认为词序倾向于最小化依存关系的长度。报告通过英语中的直接宾语和状语的词序示例，解释了DLM原则如何影响句子可接受性：简短的直接宾语倾向于紧随动词，而较长的直接宾语可以放在状语之后以减少总依存长度。

研究人员从宾夕法尼亚树库的增强版本中提取了关于协调的统计数据，发现以下趋势：
1. 左侧连词倾向于较短（此前已被观察到）。
2. 这种趋势随长度差异的增大而增强（Gibson et al., 1996中曾简要提及）。
3. 这种趋势只在主词位于左侧或缺失时出现（例如，“I saw Bart and Lisa”中主词“I”在左侧；“Homer came and sneezed”中无外部主词）。
4. 当主词位于右侧时（例如，“Ted and Ned laughed”），这种趋势则不复存在。

报告进一步通过图表展示了这些统计结果：在主词位于左侧或缺失的情况下，左侧连词较短的比例随着连词间长度差异的增大而稳步上升；而当主词位于右侧时，这种趋势消失，左侧连词较短的比例保持平坦。

最后，报告得出结论，这些观察结果提供了支持对称协调结构（如连词中心式和多头式）的证据，并反驳了非对称结构（如Universal Dependencies和链式）。</sample>
    <sample id="217">此研究探索了多属性可控对话生成的组合泛化能力，发现现有模型在此方面存在不足。为此，我们提出了DCG模型，它通过学习“已见”属性值的概念，利用解耦损失来区分不同属性组合。DCG模型利用两种提示类型：属性导向提示，关注对话中的特定属性信息；任务导向提示，通过实例独立全局特征指导响应生成。为评估多属性可控对话生成，我们还引入了统一的无参考评估框架MAE，其设计使用离散提示模板和连续提示令牌，旨在减少手工模板的潜在偏差并提高模型的稳定性和鲁棒性。

实验结果表明，DCG模型在属性可控性和文本质量方面均优于所有基线模型。属性导向提示有助于模型关注可控信息，任务导向提示则提高了任务质量，而解耦学习则提升了组合泛化能力。进一步的定性分析显示，DCG成功地解耦了属性组合，并学习了不同属性之间的关系，使其能够从“已见”属性泛化到“未见”组合。此外，MAE在评价不同粒度的离散和连续属性时，与人工判断表现出更高的相关性。MAE移除连续提示后，相关性分数下降，这表明任务导向提示对MAE的准确性至关重要。</sample>
    <sample id="218">这篇论文的作者们都来自谷歌翻译。</sample>
    <sample id="219">该演示文稿介绍了一种多阶段流水线，用于在财务报告中发现财务信号。该工作由嘉晖·朱、余祥·黄、程伟·林和教授哲·林及传聚·王共同完成。报告首先介绍了财务报告分析的背景，特别是Form 10-K，强调其强制性、定期发布、公开可用和全面的特性。然而，从这些报告中提取有用信息需要大量人工努力。

报告指出，财务语料库具有高度重叠和年度依赖的特点，即一份公司报告中约80%的词汇与上一年的报告相同（日期除外），且相邻年份的报告内容更为相似。基于这些观察，研究引入了一项“高亮任务”和多阶段流水线来解决这些实证问题。高亮任务的目标是找出给定目标和参考（上一年度报告）文本对之间的重要词汇或“基本原理”。模型预测的词汇重要性用于衡量高亮性能。

提出的流水线包括三个阶段：文档分割（S0）、关系识别（S1）和高亮阶段（S2/S2+）。在关系识别阶段，所有文本对被分类为三类：不重要关系（如公司规定）、修订关系（少数词不同但意思不同，如“增加”变为“减少”）和不匹配关系（如首次披露的信息或公司新操作）。

在高亮阶段，研究采用两阶段微调方法。首先，在e-SNLIc数据集上进行域外微调，e-SNLIc是一个带有词元标注的自然语言推理数据集。接着，在修订对上进行域内微调，其中修订词被标记为伪正例，少数其他词被随机标记为伪负例。模型通过混合交叉熵损失和KL散度来学习，以减轻低质量伪标签带来的问题。

评估结果显示，域自适应高亮模型在FINAL数据集上表现最佳，并且在e-SNLIc上的性能保持了泛化能力。此外，该方法还能从训练中未使用的“不匹配”关系中获益。未来的工作可能包括提高效率、增加更多特征（如双向合理化任务）、探索端到端应用以及分析图表、表格或跨公司/跨行业数据。</sample>
    <sample id="220">这篇论文的作者所属机构是石溪大学（Stony Brook University）。</sample>
    <sample id="221">该论文分析的语言对是德语到英语。</sample>
    <sample id="222">该研究名为“适应或注释：开放域问答的挑战与干预”，旨在解决通用模型在特定领域问答中的泛化问题。当模型在维基百科等通用语料库上训练，却被要求回答生物医学问题时，模型表现不佳。

研究提出了三种主要贡献：
1. **数据干预方法以实现域外泛化**：研究探索了少样本和零样本两种数据干预策略。在少样本方法中，通过提供少量目标领域样本，提示大型语言模型（LLM）生成更多领域数据。具体操作是让LLM从文章中提炼事实并转化为填空式问题。结果显示，这种方法能使检索器性能平均提高8%，阅读器性能平均提高11%。
2. **理解源模型与目标领域的兼容性**：研究通过一个二维兼容性矩阵来衡量源模型（检索器和阅读器）与目标领域的兼容性。兼容性通过衡量源模型分配给正确答案或段落的概率来计算。
   - **无偏移**：检索器和阅读器都兼容目标领域。
   - **概念偏移**：检索器兼容，但阅读器不兼容（需要新的推理方式）。
   - **协变量偏移**：检索器不兼容，但阅读器兼容（检索困难，但能理解正确段落）。
   - **完全偏移**：检索器和阅读器都不兼容。
3. **确定干预措施与数据集偏移类型的关系**：研究发现，数据干预措施的有效性取决于数据集的偏移类型。
   - **零样本干预**：通过改变问题（标准问题 vs 填空式问题）、答案（根据实体类型分布采样）和上下文（混合源和目标领域语料库）来控制变量。结果显示，问题格式对性能影响不大，但填空式问题更容易收集；统一的答案类型分布效果最好；而学习型检索器对数据分布敏感，BM25等非监督方法表现更好。
   - **干预效果**：所有目标数据集都对少样本自适应有积极响应，因为它们利用了目标领域的一些示例。对于概念偏移和协变量偏移的数据集，零样本自适应也表现良好。对于无偏移的情况，由于源模型已经很好地理解了目标领域，干预带来的性能提升较小。

总之，该研究提出了一种少样本方法，将阅读器性能提升高达24%，检索器性能提升高达22%（F1得分）。研究还表明，数据干预的有效性与数据集的偏移类型密切相关。</sample>
    <sample id="223">演讲者的名字是 Shangbin Feng。</sample>
    <sample id="224">研究人员微调了LongBART和普通BART模型，以进行文档级和句子级的文本简化。</sample>
    <sample id="225">在 MultiInstruct 中，使用了 53 个任务从 9 个组进行训练，以及 5 个额外的任务（来自 VQA 和 Miscellaneous 组）加上整个 Common Sense Reasoning 组进行测试。</sample>
    <sample id="226">这篇论文有3位作者：Regina Stodden、Omar Momen和Laura Kallmeyer。</sample>
    <sample id="227">此演讲的主题是关于 Pangu 框架，一个用于接地语言理解的统一框架。

演讲者 Yu Gu 来自俄亥俄州立大学，讨论了语言模型在 NLP 任务上的成功，但也指出了当前语言模型研究中接地（grounding）的缺失，即在预训练过程中缺乏与特定目标环境相关的知识。

接地语言理解的目标是将自然语言表达映射到可在特定目标环境中执行的动作或程序。这在智能助手、搜索引擎、医疗数据库查询和机器人指令等应用中非常重要。然而，目前的语言模型，包括 BERT、T5 和 GPT-3 等大型模型，主要通过文本语料库进行预训练，缺乏对现实世界的感知和理解。

当前的接地语言理解任务通常采用自回归解码方式，直接通过语言模型生成计划或程序。然而，这种方法存在一个主要问题：生成的计划可能在语法上不正确或在语义上无效，例如在知识库问答中生成不可执行的查询，或者在机器人场景中生成无法执行的动作序列。

为了解决这个问题，Yu Gu 提出了 Pangu 框架。Pangu 的目标是让语言模型专注于判别任务而不是生成任务。具体来说，Pangu 框架中包含一个符号代理（symbolic agent），它与环境交互并生成语法正确且语义有效的候选计划。然后，语言模型作为判别器，对这些候选计划进行评分和排序。这样，语言模型就不需要独立处理计划的有效性和语法性，从而降低了任务的复杂性。

研究人员在知识库问答（KBQA）任务上验证了 Pangu 框架，该任务具有复杂和异构的环境。Pangu 在不同的语言模型（BERT、T5 和 CodeX）上取得了显著的性能提升，尤其是在非同分布泛化方面表现出色。此外，Pangu 还展示了强大的样本效率，即使只有少量训练示例也能获得良好的性能。研究还发现，Pangu 能够更好地处理训练过程中未曾见过的结构，这表明它具有更强的泛化能力，而自回归模型则倾向于过拟合训练数据中的已知结构。

总结来说，Pangu 框架的关键信息是：对于接地语言理解任务，直接生成计划可能不是使用语言模型的最佳方式。相反，利用语言模型的判别能力来评估由符号代理生成的候选计划，可能是一种更有效和更稳健的策略。</sample>
    <sample id="228">作者在实验中使用了 AG News、MIND、SST2 和 Enron Spam 四个数据集。</sample>
    <sample id="229">这段演讲介绍了检测论证性写作中可改进主张的研究。主讲人Gabriella Skitalinskaya解释说，文本修订是论证性写作的关键组成部分，因为它直接影响文本对读者的说服力。为了帮助作者，研究人员引入了两个新任务：

1. **次优主张检测 (Suboptimal-Claim Detection)**：判断一个主张是否需要修订，或者是否已达到最佳措辞。
2. **主张改进建议 (Claim Improvement Suggestion)**：针对一个主张，建议修订时应改进的质量问题类型。

该研究通过学习人类的修订行为来解决这些任务，而不是显式定义什么是好的或坏的主张。这主要是通过分析来自在线辩论平台（如Kialo）的协作编辑行为中隐含的修订模式来实现的。

研究人员在工作中识别并探讨了四个主要挑战：

1. **代表性和可靠性 (Representativity and Reliability)**：如何从主张修订历史中构建一个可靠的数据集，以准确反映论证性主张的质量。例如，一个最终版本是否真的是最优的，还是仅仅被社区忽略了。
2. **模型复杂性和架构 (Model Complexity and Architecture)**：选择与修订理念一致并能识别细微变化的合适模型。研究人员尝试了各种模型架构和预训练方法，以评估它们对主张评估性能的影响。
3. **情境性 (Contextuality)**：识别哪些情境信息与判断主张质量相关。这可能包括主题专业知识、领域知识、父级主张，甚至其他更广泛的语境信息。
4. **话题和用户偏见 (Topical and User Bias)**：处理协作修订历史中固有的噪声和偏见，这些噪声和偏见可能源于意外错误或用户和版主的偏好，尤其是在高度争议的主题中。

研究结果表明，基于修订的数据可以有效地用于这些任务。此外，建模两个主张版本之间的距离对于次优主张检测非常有利，并且情境信息的影响取决于任务本身和所涉及的质量问题。

这项研究为论证性写作支持工具的开发提供了宝贵的见解，特别是通过利用人类的修订模式来识别和改进文本质量。</sample>
    <sample id="231">NACHOS 是一个 1.1B 亿词开源数据集，包含从不同医学领域抓取的数据，具有不同的性质和风格。</sample>
    <sample id="232">演讲者的名字是David Vilar Torres。</sample>
    <sample id="233">在这段视频中，演讲者介绍了同时语音翻译（SimulST）及其现有模型面临的挑战。SimulST 旨在实时将口语翻译成另一种语言的文本，从而实现跨语言交流。

现有 SimulST 模型的问题在于：
1. 通常需要训练特定的架构，并引入额外的模块进行优化。
2. 训练过程漫长且复杂，例如需要针对不同的优化目标进行训练。
3. 需要训练和维护多个模型才能达到不同的延迟要求（例如，1 秒、2 秒等）。

演讲者提出了名为 EDAtt（Encoder-Decoder Attention）的解决方案。EDAtt 是一种策略，根据注意力指向的位置决定是否发出部分翻译。如果注意力未集中（即，注意力总和低于某个阈值 α），则发出一个单词，表明接收到的信息足够稳定。例如，如果收到包含 "I am going to talk about..." 的语音片段，模型会预测其德语翻译。如果前两个单词的注意力指向较早的语音帧，它们将被发出。如果最后一个单词的注意力总和低于 α 且指向最后 λ 个语音帧，则该单词不会被发出，模型会等待另一个语音片段。如果收到另一个语音片段后，新的预测单词的注意力未指向最后 λ 个语音帧，则它们将被发出。

主要结果显示，在 BLEU（翻译质量衡量指标）和 AL/AL_CA（延迟衡量指标）图上，EDAtt 优于所有应用于离线模型的策略，曲线向左移动（表示更低的延迟）。此外，如果考虑实际经过时间或计算感知时间，EDAtt 是最快的策略。

最后，演讲者鼓励观众阅读他们的论文以获取更多结果，并提供了代码和模型的开源版本，以方便工作重现。</sample>
    <sample id="234">提示策略对结果有很大的影响。在 1000 个句子中，有 516 个句子的 BLEURT 分数差异超过 1 分。在极端情况下，差异甚至可以达到 40 分。</sample>
    <sample id="235">这篇论文的作者来自卡内基梅隆大学语言技术学院、里斯本技术大学、加州大学伯克利分校的伯克利人工智能研究实验室（BAIR）和Unbabel。</sample>
    <sample id="236">在所提供的文本中，没有提到5个专家编写的指令是什么。</sample>
    <sample id="237">作者提出了一种诊断测试套件，用于通过共指消解任务来评估模型从不同知识来源中提取信息的能力。他们比较了人类研究参与者和共指消解模型在此任务上的表现。</sample>
    <sample id="238">本视频介绍了一个名为MeetingBank的新基准数据集，旨在解决会议摘要生成领域缺乏高质量摘要和可靠公共会议来源的挑战。

为了构建该数据集，研究人员首先利用Speechmatics API将会议音频转换为文本转录稿。然后，他们通过从会议网站识别会议类型和日期，生成唯一的会议ID。接着，利用这些ID从会议纪要中提取相应的参考摘要，并使用相同的ID获取会议片段的起止时间。最后，通过时间戳对齐，将片段转录稿与提取出的摘要进行匹配。

MeetingBank数据集包含来自丹佛、西雅图、长滩、阿拉米达、金县和波士顿等多个城市的1366次市议会会议，共计约7000个摘要实例。平均而言，每次会议时长约2.6小时，包含2.8万个词元，而每个摘要片段包含2.9千个源文本词元和87个摘要词元。

通过对数据集的分析，研究人员发现大多数会议摘要的覆盖率在0.7到0.9之间，表明摘要主要包含源文本的关键信息而非过度抽象。在密度方面，西雅图和波士顿的摘要密度最高，而丹佛的摘要密度最低，这可能与会议纪要的编辑程度有关。

在模型评估阶段，研究人员测试了包括Extract-Oracle、Lead-3、LexRank、TextRank在内的抽取式模型，以及BART_Large、Pegasus、Longformer、DialogLM、HMNet等抽象式模型，并对GPT-3进行了零样本摘要生成实验。结果显示，对于抽取式系统，Extract-Oracle取得了最高的ROUGE-2分数（46.6%），表明参考摘要内容主要来源于源文本。在抽象式模型中，专为长对话摘要设计的DialogLM表现最佳，ROUGE-2分数达到60.12%。尽管GPT-3在自动指标上的表现不佳，但人工评估结果显示其在流畅性和连贯性方面表现出色，但在信息性和事实准确性方面仍有提升空间。

总而言之，MeetingBank是一个重要的基准数据集，不仅为研究人员设计先进的会议摘要系统提供了有价值的测试平台，也为理解市议会的决策过程提供了深入见解。研究人员鼓励大家下载并使用该数据集，以促进会议摘要技术的发展。</sample>
    <sample id="239">00:00
大家好，我叫David Vilar，我将简要介绍一下这篇论文，内容是关于提示PaLM进行翻译，以及评估策略和性能。
00:10
这是我和谷歌翻译的同事们共同完成的工作。
00:12
PaLM是一个拥有5400亿参数的大型语言模型，于去年2022年发布。
00:19
它通过对由7800亿个令牌组成的大量文本进行训练，在发布时，它在数百个MLU和生成基准测试中达到了最先进的水平。
00:30
我们的贡献。
00:31
在这项工作中，我们首次系统地研究了大型语言模型用于机器翻译的提示。
00:38
我们使用机器翻译社区的最佳实践评估了此类模型的翻译能力，这包括使用最新的测试集，以避免测试数据与语言模型训练数据重叠，并避免在评估数据上过度拟合。
00:46
我们将结果与最先进的系统，即WMT评估中表现最佳的系统进行了比较。
01:00
我们使用了最先进的神经网络机器翻译指标，此外，还展示了基于专家的翻译质量评估结果（比众包工作人员更可靠）。
01:08
最后，我们提出了一些关于提示选择策略的建议。
01:13
提示对翻译质量有很大影响。
01:14
提示对大型语言模型进行翻译的性能有很大的影响，正如我们在一个简单的实验中看到的那样，我们使用了单次提示，并为每个句子提供了两个不同的提示。
01:30
在大多数句子中（1000个句子中有516个），观察到的差异超过了一个BLEURT点，在极端情况下，这个差异可以达到40个BLEURT点。
01:46
所以，选择一个好的提示策略非常重要。
01:48
翻译的提示示例。
01:50
在我们的实验中，我们采用了五次提示策略，我们只用它所属的语言标记我们提供给系统的每个句子。
02:04
因此，在这个例子中，我们从德语翻译成英语，源句子用“German: ”标记，英语翻译用“English: ”标记。
02:18
我们发现，在多次提示的情况下，实际的提示形式并没有太大的影响。
02:26
它对零次提示和单次提示至关重要，但当我们像我们一样进行五次提示时，实际的提示形式几乎没有区别。
02:37
最重要的例子就是权重。
02:39
实验结果。
02:40
我们的实验结果总结是，示例质量比与源句的相似度更重要。
02:48
因此，选择高质量翻译的示例非常重要。
02:54
特别是，我们将WMT评估的训练数据和开发数据中的提示进行了比较。
03:00
开发数据的质量更高，结果显示，使用开发数据时性能更好。
03:13
尽管如此，专业的最先进系统在PaLM翻译方面具有显著优势，但PaLM非常接近商业系统。
03:26
在我们的案例中，我们选择了使用谷歌翻译进行评估。
03:32
我们从使用MQM框架进行人工评估中获得的见解是，PaLM的流利度与最先进的系统相当，但主要差异来自于准确性。
03:47
因此，最常见的错误是遗漏错误。
03:52
因此，PaLM似乎倾向于生成听起来更好的翻译，有时会通过删除源句子中被省略的部分来达到此目的。
04:08
然而，PaLM的“Style/Awkward”类别得分低于最先进的系统，这进一步表明PaLM能够生成非常流利的输出，但在准确性方面仍存在一些问题。
04:26
以上就是本次简短概述的全部内容。
04:30
有关详细信息，请访问论文的完整演示文稿。
04:35
非常感谢。</sample>
    <sample id="240">你好，我是来自德国萨尔大学的博士生朱大伟。在这个视频中，我将介绍我们最近的工作《比你想象的更弱：对弱监督学习的批判性研究》。这项工作是与沈小宇、马略斯·莫斯巴赫、安德烈亚斯·斯特凡和迪特里希·克拉科夫合作完成的。

我将从对弱监督和弱监督学习的简要介绍开始。在弱监督中，我们不手动标记数据。相反，我们使用弱标记源来标记数据，例如简单的启发式规则、知识库或低质量的众包。正如右图所示。与人工标注相比，弱标注要便宜得多，但它们也带有噪音，这意味着一定数量的标注是不正确的。如果我们直接在弱标记数据上训练神经网络，神经网络会记住标签噪音，并且无法泛化。在弱监督学习中，提出了训练算法来在标签噪音下稳健地训练神经网络，使训练的模型仍然能够很好地泛化。

在最近的弱监督学习研究中，通常声称他们只在弱监督数据上训练模型，并在干净的测试集上达到很高的性能。从技术上讲，这种说法没有错，但其中有一个猫腻，那就是人们确实假设有一个额外的干净验证集可用于模型选择。我们偶然发现了这个问题设置，因为这意味着在弱监督学习中需要额外的手动标注。但是，就像房间里的大象一样，这种必要性常常被忽视。

我们的研究问题围绕着上述假设展开。首先，干净的验证数据对于弱监督学习来说是必要的吗？或者我们也许可以用嘈杂的验证集代替？第二，如果需要干净的数据，或者如果干净数据是弱监督学习正常工作的必要条件，那么我们需要多少干净样本？最后，我们应该只将干净样本用于验证，还是有更好的方法来利用它们？

我们在工作中解决了这些研究问题，我们的发现如下：首先，我们发现有趣的是，最近的弱监督学习方法确实需要干净的验证样本才能正常工作。否则，性能会大幅下降。如本图所示，如果没有干净的验证样本，那么训练的模型就无法泛化超出原始弱标签的范围，这意味着训练是毫无意义的。这表明弱监督学习方法实际上需要干净标记的数据才能正常工作，并且不应忽视获得干净验证样本的标注成本。

我们的第二个发现是，增加干净验证样本的数量将有助于弱监督学习方法取得更好的性能。如左图所示。通常，我们每类只需要20个样本就能获得良好的性能。但这并不是故事的结局。因为如果我们无论如何都决定访问干净样本，那么直接在它们上面进行训练甚至可以取得更好的性能。右图显示了在干净数据上直接应用的微调方法与弱监督学习方法之间的性能差异，后者仅将干净数据用于验证。正如我们所看到的，如果我们每类有10个样本，直接微调就开始超越弱监督学习方法。

最后，以前弱监督学习方法所声称的性能提升可以通过允许在干净验证样本上持续微调轻松实现。正如我们从图中可以看到，名为FTW的基线模型最初的表现不如像COSINE这样更复杂的弱监督学习方法。但是，如果我们在干净样本上进行持续微调，那么FTW的性能与其他方法一样好。所以，在实践中，没有理由选择更复杂的弱监督学习方法，它们需要更多的计算时间和磁盘空间。

总而言之，我们发现最近的弱监督学习方法需要干净的手动标注样本才能正常工作。它们的性能提升和实用性被严重高估了。我们对未来工作的具体建议如下：首先，报告模型选择标准。例如，报告模型选择是否在干净的验证样本上完成。其次，弱监督学习方法应与少量学习基线进行比较，例如在干净样本上进行的微调工作。第三，持续微调是一种简单但强大的基线，在未来的弱监督学习工作中应予以考虑。最后，我们已经开源了我们的代码。您可以在此幻灯片上的二维码中找到它。请随时查看。谢谢，祝会议愉快。</sample>
    <sample id="241">本演讲介绍了为早期检测社交媒体上 COVID-19 治疗相关的虚假信息而开发的“人机交互式”（Human-in-the-loop, HiTL）框架。该框架旨在克服现有自动检测系统在现实性评估和以人为本方面存在的不足。

目前，传统的虚假信息检测方法存在两大问题：
1. **评估不切实际**：系统通常使用追溯性构建的基准数据集进行评估，而不是实时数据。这可能导致“反证泄露”（leaked counter-evidence），即系统在实际信息发布之前就获得了驳斥信息所需的证据，从而无法有效检测新出现的虚假声明。
2. **非以人为本**：大多数方法未能充分考虑真实世界社交媒体平台的规模和噪音，以及人类内容审核员和事实核查员在检测过程中的重要作用。这些系统往往将人工干预降至最低，或仅限于最终的决策步骤。

为了解决这些问题，我们提出了一个 HiTL 评估框架，它具有以下特点：
- **端到端系统**：从原始推文到可操作的输出，覆盖整个检测流程。
- **良好整合的人工反馈**：人类在工作流程的各个阶段做出关键判断。
- **具体实施**：我们评估了一个用于检测 Twitter 上 COVID-19 治疗虚假信息的系统。

我们的具体系统包含两个主要组成部分：
1. **误导性声明的早期检测**：
   - 从 Twitter 上收集与 COVID-19 相关的重要推文。
   - 自动发现值得核查的声明。
   - 根据“趋势性”对声明进行排名，以便人类验证。例如，我们的系统成功在新闻报道驳斥之前，检测到羟氯喹、伊维菌素和漱口水作为 COVID-19 治疗方法的虚假声明。
2. **政策违规验证**：
   - 确定推文作者对虚假信息的立场。
   - 提供支持未经批准治疗方法的推文列表。我们发现 65% 的系统识别推文很可能或明确违反了 Twitter 的政策。此外，该系统每小时可以检测并验证 124.2 条包含政策违规的推文。

总之，我们的 HiTL 框架更真实地捕捉了系统与人类内容审核员和事实核查员之间复杂的相互作用，并将虚假信息检测任务整合到一个实用且现实的工作流程中。我们希望这项工作能促进更多有用的人机交互式虚假信息检测框架的发展，并为未来系统的比较提供一个具体的标准。</sample>
    <sample id="242">常用的对话系统评估方法是人工评估，如人工比较两种对话模型哪种更好，或使用李克特量表给对话进行评分。</sample>
    <sample id="243">根据视频中幻灯片上的信息，这篇论文有5位作者。</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，我们需要实体特定知识（Servin 是法官）和背景知识（法官在法庭上判案）。</sample>
    <sample id="245">本次演讲重点介绍了一项名为“大海捞针：MTurk 上高一致性工作者进行摘要的分析”的研究。这项研究的目标是寻找 Amazon Mechanical Turk (MTurk) 上高质量（高一致性）的工作者，以执行摘要任务。

演讲从引言开始，强调了当前自动化评估指标存在的问题以及对 MTurk 招募最佳实践缺乏了解。为了解决这些问题，研究团队设计了一个两阶段的筛选流程。

第一个阶段是资格认证任务，旨在评估注释者正确评估多个维度的能力。通过提供三份文档（其中一份包含注意力检查）和一份摘要，评估六个维度。根据结果，工作者被分为四类：黄金（全部正确且通过注意力检查）、白银（全部正确但未通过注意力检查）、青铜（仅通过注意力检查）和区块（未通过注意力检查）。在此阶段，200 名参与者中有 26 名（8 名黄金，18 名白银）通过了资格认证。

第二个阶段是耐力任务，旨在测试注释者处理大量工作的能力。该任务包含 10 个 HIT，每项 HIT 包含一份文档和四份摘要，主要关注显著性维度。通过耐力任务后，剩余 12 名工作者（4 名黄金，8 名白银），占初始参与者的 6%。这些工作者表现出比专家更高的一致性，Cohen's Kappa 得分为 0.55，Krippendorff's Alpha 得分为 0.443（黄金组）。

接下来是基于参考的评估任务，用于测试通过筛选的工作者的总体表现。该任务包含 30 个 HIT，每个 HIT 包含一个参考和一个候选摘要，评估信息覆盖率的两个方向。通过筛选的工作者中，8 名完成了所有 HIT，Cohen's Kappa 达到 0.68（黄金组），Krippendorff's Alpha 达到 0.534（所有组）。

研究还对比了不同注释者来源的准确性：
- 基线 MTurk 工作者：使用 MACE (Multi-Annotator Competence Estimation) 统计过滤器筛选，当阈值为 0.5 时，Krippendorff's Alpha 为 0.380，但存在 HIT 覆盖率不完整和每个 HIT 的工作者数量较少的问题。
- CloudResearch MTurk 工作者：该平台用于招募高质量的注释者，Krippendorff's Alpha 为 0.513，但任务接受率较低。

对 50 个随机样本的正确性分析表明，该筛选流程和 CloudResearch 工作者具有显著的 Spearman 相关性。然而，该流程并不能保证正确性训练，而 GPT 模型与专家判断的相关性很好。

讨论环节总结了该流程的优势：
- 避免了时间和资源的浪费。
- 以较低的成本实现了高一致性。
- 与 CloudResearch 相比，具有相似的质量（Spearman 相关性）。
在成本方面，该流程每个 HIT 的成本为 27 美元，远低于 MACE 的 175 美元，略低于 CloudResearch 的 31 美元。

未来的工作方向包括：
- 调查如何招募在一致性和正确性方面都高质量的工作者。
- 将该流程应用于多任务、多语言和多平台。

局限性包括：
- 仅在 MTurk 平台上测试了英文摘要任务。
- 设计的问题并非“万能”解决方案。
- 无法保证正确性训练。

最后，感谢谷歌对该实验的资助。</sample>
    <sample id="246">代码公开。可以在GitHub上找到数据集、生成和评估代码。</sample>
    <sample id="247">本视频介绍了一项名为“FactKG：通过知识图谱推理进行事实验证”的新任务，旨在解决现有事实验证数据集不使用知识图谱作为证据的空白。知识图谱因其可靠性和实用性，成为验证事实的宝贵来源。与文本或表格证据相比，知识图谱可以直观地将证据与声明直接联系起来，从而实现可靠推理。此外，它在对话系统等需要知识图谱与自然语言之间进行一致性检查的任务中具有实用性。

FactKG数据集使用DBpedia作为知识图谱，包含书面和口语两种声明风格，以实现更实际的应用。数据集有“支持”和“反驳”两种标签，任务包括从DBpedia检索证据并利用这些证据验证声明。FactKG支持五种推理类型：单跳（验证两个实体是否通过一个关系连接）、合取（验证多个单跳声明）、存在（验证实体是否连接到特定关系）、多跳（通过多跳推理验证，因为某些实体不以表面形式出现在声明中）和否定（即使在找到图证据后，仍通过多一次推理进行否定验证）。为了增加实用性，书面风格的声明被转换为口语风格，并使用了预设模板。

数据集统计数据显示，所有推理类型总计有108,674个自然语言声明。基线实验分为两类：仅使用声明（BERT、BlueBERT、Flan-T5零样本）和使用证据（GEAR模型）。结果显示，所有基线都优于51%的多数类基线。其中，使用图证据的GEAR模型在所有基线中表现最佳。总而言之，FactKG引入了一个新的数据集，利用图谱证据来验证自然语言声明，并支持多种推理类型和语言模式，从而提高了事实验证的实用性。</sample>
    <sample id="248">根据视频内容，NLPositionality 的注释者在各个人口统计学特征方面并不均衡。例如，数据集和模型在社交可接受性方面更倾向于英语国家，在仇恨言论检测方面也更倾向于英语国家。此外，数据集和模型与受过大学教育的人更匹配，但与非二元性别的人匹配度较低。这表明 NLP 数据集和模型存在设计偏见，未能充分代表不同人群的观点和经验。</sample>
    <sample id="249">可以在句子的开头或结尾添加状语，添加从句，或者引用句。</sample>
    <sample id="250">维度评估意味着评估对话质量的多个维度，以更精细的了解模型的优缺点。</sample>
    <sample id="251">这篇论文的作者所属机构包括中国科学技术大学、微软亚洲研究院、北京交通大学和索尼人工智能公司。</sample>
    <sample id="252">本演示文稿介绍了一项名为 "U-CREAT "的研究，其旨在通过事件提取来实现无监督案例检索。演示文稿由来自印度理工学院坎普尔分校的 Sai Kiran Tanikella、Abhinav Joshi、Akshat Sharma 和 Ashutosh Modi 共同完成。

研究的动机在于，律师和法官通常依靠经验来引用相关的法律先例，但随着案例数量的增加，这项任务变得越来越困难。因此，"先前案例检索"（PCR）任务应运而生，它旨在自动检索与当前法律文件在事实和先例方面都相关的法律文件。

研究团队主要贡献了两个方面：
1. **IL-PCR 数据集**：这是一个针对印度法律系统的新 PCR 基准数据集，包含 7070 个法律案例，其中有 1182 个查询池和 5888 个候选池，平均每个查询有 6.775 个引用。与现有的 COLIEE'21 数据集相比，IL-PCR 在文档数量、平均文档大小、词汇量和引用数量方面都更大。
2. **U-CREAT 管道**：这是一种基于事件的 PCR 方法，具有以下特点：
    - 无监督：不需要人工标注数据。
    - 高检索效率和低推理时间。
    - 跨印度和加拿大法律系统的泛化能力，无需针对特定法律/人口统计数据进行微调。

**事件提取**是 U-CREAT 的核心。研究人员将案例文件视为事物如何发展的叙述，并将其表示为事件的集合。通过使用 spaCy 进行依赖解析，他们从句子中提取主语-动词-宾语三元组作为事件。例如，在句子 "These statements were forwarded to the Police" 中，事件是 "(statement, forward, police)"。

**U-CREAT 管道的工作流程**如下：
1. **事件提取**：对查询文档和候选文档进行预处理（去噪）、依赖解析和后处理，以提取事件。
2. **交互矩阵**：计算查询事件和候选事件之间的交互矩阵，绿色方块表示共同事件。
3. **检索模型**：交互矩阵被输入到不同的检索模型中，以获得候选文档的排名。

**实验模型**分为三类：
1. **基于计数（词级别）模型**：包括 BM25、BM25(Bigram) 和 RR Filtered Docs + BM25。
2. **基于 Transformer 模型**：包括 Segmented-Doc Transformer (Full Document)、Transformer (top 512 tokens) 和 Sentence Transformer (SBERT)。研究还引入了在印度法律文本上训练的 InCaseLawBERT 和 InLegalBERT。
3. **基于事件模型**：包括原子事件（每个事件视为独立单元）、非原子事件（事件中的每个词视为独立单元）、事件过滤文档（过滤原始语料库，只包含产生匹配事件的句子）和 RR Filtered Docs (Penta-gram)。

**实验结果**显示：
- 基于 Transformer 模型的性能相较于基线方法（如 BM25）显著下降，这表明法律领域的复杂性和特殊性需要更专业的处理方法。
- 基于事件的模型在所有其他方法之上表现显著提升。其中，**事件过滤文档模型是表现最好的模型**，在 IL-PCR 中 F1 分数达到 39.15（相对于标准 BM-25 基线提升了 25.30），在 COLIEE'21 中 F1 分数达到 26.94（相对于标准 BM-25 基线提升了 12.22）。
- 在推理时间与模型性能的比较中，基于事件的模型具有更低的推理时间，同时获得了更高的 F1 分数。
- 与现有的监督方法相比，U-CREAT（基于事件过滤文档的 BM25 Tri-gram）在 COLIEE'21 数据集上取得了 27.32 的 F1 分数，优于包括最新监督方法 MTFT-BERT 在内的所有现有方法。

**结论**：
- 提出了一种用于 PCR 的新数据集（IL-PCR）。
- 提出了一种用于法律文档事件检索的 U-CREAT 管道。
- 基于事件的方法具有更好的性能和推理时间，并且适用于生产环境。
- U-CREAT 是无监督的，不需要语料库特定的微调。

演讲者鼓励大家查阅论文了解更多详情，并参加问答环节。演示文稿的最后提供了 GitHub 仓库的链接和二维码。</sample>
    <sample id="253">Mario Ezra Aragón介绍了DisorBERT，一个用于检测社交媒体中精神疾病迹象的双域适应模型。他首先定义了精神疾病，并强调了社交媒体作为研究平台的重要性。

该模型通过对BERT进行Reddit和心理健康数据的双重领域适应，并结合基于词典的引导掩蔽技术进行训练，以学习社交媒体语言和专业化精神障碍领域。这使得模型在训练过程中能更专注于重要词汇。

实验结果表明，DisorBERT在准确率和召回率方面表现出良好的平衡，优于其他基线模型。通过对贝克抑郁量表（BDI）的文本段落进行分析，DisorBERT能更倾向于生成与精神障碍相关的负面或心理导向的词汇，而BERT则倾向于生成更普遍的词汇。

该研究还展示了DisorBERT对抑郁症患者帖子中关键主题词（如“焦虑”和“药物”）的关注，从而实现对精神障碍的临床检测。

总而言之，该研究表明双域适应和引导掩蔽的结合能有效识别社交媒体中精神疾病的迹象。未来研究将探索使用不同的词汇资源和临床数据来训练更专业的语言模型。</sample>
    <sample id="254">这段视频介绍了一种名为“不确定性引导标签去噪文档级远距离关系提取”（UGDRE）的新方法，旨在解决文档级关系提取中的噪声标签问题。该方法的主要贡献包括：

1. **框架创新**：UGDRE 提出了一种利用不确定性引导标签去噪的文档级关系提取框架，显著提高了远距离监督（DS）数据的标签质量。

2. **实例级不确定性估计**：为了解决重叠关系中的噪声问题，UGDRE 设计了一种新颖的实例级不确定性估计方法。传统方法在处理实体对之间的多种关系时存在局限性，无法有效区分假阳性伪标签和正确伪标签。该方法通过为每个假阳性伪标签生成独立的置信度分数，以衡量其可靠性，从而更精确地评估模型预测的不确定性。

3. **动态类不确定性阈值再标记策略**：针对文档级关系提取中长尾关系类别的挑战，UGDRE 引入了一种动态类不确定性阈值再标记策略。这种策略能够根据不同关系类别的特征（例如频繁类别通常具有较低的平均不确定性分数，而长尾类别则相反），自适应地过滤掉高不确定性的伪标签，从而提高模型在稀疏类别上的表现。

4. **多阶段训练策略**：UGDRE 采用了一种多阶段训练策略，通过迭代地使用经过去噪的DS数据和人工标注数据对模型进行预训练和微调。这有助于充分利用DS数据，并进一步提升文档级关系提取模型的性能。

通过在 DocRED 和 Re-DocRED 等公开数据集上与现有基线的比较，UGDRE 在性能上取得了显著提升，证明了其在处理文档级关系提取中噪声标签和长尾分布问题的有效性。</sample>
    <sample id="255">虽然单次提示中提示的实际形式至关重要，但在多次提示时，提示的实际形式几乎没有区别。</sample>
    <sample id="257">作者评估了四种开放域对话模型：BART-FID-RAG、Blender2、Emora和Blender-Decode。</sample>
    <sample id="258">这段视频的标题是“大型语言模型能否替代人类评估？”，探讨了使用大型语言模型（LLMs）来评估文本质量的潜力。

视频开篇介绍了这项工作的核心思想：通过向LLMs提供指令，并让它们根据这些指令对文本样本进行评分。作者将这种方法称为“LLM评估”。

接着，视频讨论了相关工作，指出虽然现在LLM评估似乎是自然而广泛采用的方法（例如G-Eval），但这项工作在提交时是首次探索LLM评估这一概念。

视频解释了进行LLM评估的动机：过去广泛采用的人类评估虽然有效，但存在不稳定且难以重现的问题。因此，作者团队思考是否存在一种能达到相同评估目的，同时避免人类评估缺点的替代方案。他们将目光转向LLMs，因为LLMs在遵循自然语言指令和执行任务方面展现出强大的能力。

为了验证LLM评估的有效性，研究人员进行了一系列实验。他们使用LLM评估来评分由GPT-2生成或由人类撰写的故事，并根据语法、连贯性、可读性和相关性四个方面进行评分。为了获得地面真实（ground truth）评分，他们聘请了英语教师进行同样的故事评估，因为英语教师在评估作文方面具有专业知识。

实验结果表明，在大多数情况下，人类评分者倾向于人类撰写的故事。然而，有趣的是，一些较小的LLMs（如T0和curie）未能表现出这种偏好。幸运的是，较大的LLMs（如davinci和ChatGPT）表现出与人类教师相似的对人类撰写文本的偏好。这表明某些LLMs确实可以作为人类评估的有效替代品。

最后，视频提出了更多值得探讨的问题，例如LLMs和人类评估者在个体故事评分上的一致性、指令措辞对LLM评估结果的影响、LLM响应的采样方式以及LLM评估与人类评估相比的优缺点，并邀请观众查阅论文或访问ACL会议上的海报以获取更多信息。</sample>
    <sample id="259">讲者来自宾夕法尼亚州立大学，分享了他们关于“多自然语言和多语义表示中的跨语言语义解析”的工作。

语义解析是将用户查询转化为语义表示，例如 SQL 或 Lambda 演算。 跨语言语义解析 (CLSP) 的目标是将多种自然语言的查询转化为多种语义表示，如 SQL、Lambda 或 FunQL。现有的 CLSP 模型存在局限性，包括特定自然语言的覆盖不足、特定语义表示的缺失，以及仅在特定神经网络模型上进行评估。

为了解决这些问题，他们提出了 XSemPLR，一个用于多自然语言和多语义表示中跨语言语义解析的统一数据集。该数据集包含 9 个不同领域的数据集，涵盖 5 种语义解析任务，使用 8 种语义表示，并包含 22 种自然语言，分属于 15 种语系。

为了更好地评估他们的基准，他们考虑了六种训练和评估设置：
1. **翻译测试**：使用 Google Translate API 将源语言翻译为目标语言，然后使用单语言模型进行训练和评估。
2. **单语言模型**：源语言与目标语言相同（例如德语到德语）。他们还测试了单语言小样本设置，仅使用 10% 的训练数据。
3. **多语言模型**：训练一个多语言模型来处理所有语言。
4. **跨语言零样本/小样本迁移**：在一个源语言上训练，然后迁移到另一个语言。

他们在单语言设置上评估了两组模型：
- **Enc-PTR**：带有基于指针解码器的多语言预训练编码器（如 XLM-R + PTR，mBERT + PTR）。
- **Enc-Dec**：多语言预训练编码器-解码器模型（如 mBART，mT5）。

结果显示 **Enc-Dec (mT5)** 在所有 9 个数据集上都取得了最佳性能。

在多语言训练分析中，他们发现 **Enc-Dec/Enc-PTR (mT5/XLM-R)** 通过在多种语言的混合中进行训练可以提高性能。

此外，他们还发现：
- 大多数主要自然语言都能获得性能提升，但英语性能在 7 个数据集中下降，在 3 个数据集中提升。这被称为“多语言诅咒”。
- 零样本设置下，跨语言迁移性能差距显著。
- 小样本设置下，迁移差距迅速缩小。

其他发现包括：
- Enc-Dec (mT5) 优于现有工作或取得了可比结果。
- 在英语自然语言上进行预训练可以显著提升目标自然语言的小样本性能。
- 多语言大型语言模型（如 Codex 和 BLOOM）在跨语言语义解析任务中仍显不足。
- 中文迁移学习和英语单语言训练（英语 -&gt; 英语）的性能差距最大，而德语通常最小。
- FunQL 优于其他三种语义表示，SQL 的性能最差。

总之，他们构建了 XSemPLR，一个用于多自然语言和多语义表示中跨语言语义解析的统一基准，并对三种代表性多语言语言模型进行了全面的基准研究。结果表明，mT5 结合单语言训练表现最佳，而多语言 LLM 仍然不足以执行跨语言语义解析任务。此外，单语言训练和跨语言迁移学习之间的性能差距仍然显著。</sample>
    <sample id="260">这篇论文共有9位作者。</sample>
    <sample id="261">理想的优秀规划器能够生成合理且忠于限制的脚本。</sample>
    <sample id="262">这篇论文有八位作者。</sample>
    <sample id="263">本研究探讨了如何减轻文本分类任务中上下文学习的标签偏差。作者首先提出了一个标签偏差类型学，包括香草标签偏差、上下文标签偏差和领域标签偏差。

香草标签偏差是指模型对标签名称的无上下文偏好（例如，预训练术语频率引起的常见词元偏差）。上下文标签偏差捕捉了上下文对模型预测的影响（例如，多数标签偏差）。领域标签偏差是作者提出的一种新类型偏差，它捕捉了任务语料库对模型预测的影响。作者通过一个实验证明，来自任务语料库的随机领域内词汇可以严重影响模型的预测，而随机英语词汇则没有这种偏好，从而验证了领域标签偏差的存在。

研究结果表明，大型语言模型在具有不同领域标签偏差水平的任务上表现出不同的上下文学习行为。在领域标签偏差较小的任务中，模型表现良好，并且先进的校准方法可以进一步提高性能。然而，在领域标签偏差较大的任务中，即使采用先前的校准方法，模型也几乎无法超越随机水平。

为了整体减轻标签偏差的影响，包括领域标签偏差，作者提出了“领域-上下文校准”（DC）方法。DC方法使用随机领域内词汇作为无内容文本来估计模型对每个标签的偏差，然后用这种估计的偏差来校准模型的原始预测。

消融研究表明，DC方法优于先前的校准方法。具体来说，将预定义内容无关词元（如“N/A”）替换为随机英语词汇可以提高性能，这表明预定义词元本身也可能存在偏差。使用更多随机英语词汇而不是单个词元可以进一步提高性能，这表明使用单个内容无关词元是次优的。最后，通过使用来自任务语料库的随机领域内词汇进行校准，可以显著提高性能，这证实了DC方法能够有效地处理领域标签偏差。

总的来说，本研究提出了一个全面的标签偏差分类，揭示了领域标签偏差作为上下文学习中标签偏差的一个主要来源，并引入了一种新的校准方法（领域-上下文校准），该方法能够整体减轻所有三种类型的标签偏差，从而显著提高上下文学习性能，尤其是在领域标签偏差较大的任务中。</sample>
    <sample id="264">本论文介绍了一种名为 TAVT（可迁移音视频文本生成）的新方法，旨在解决多模态文本生成领域存在的两个关键问题：数据标注的难度和现有模型在不同领域间的泛化能力不足。

TAVT 旨在通过以下三个核心组件来解决这些挑战：

1. **音视频元映射器网络（Audio-Visual Meta-Mapper Network, AVMM）：** 这个网络能够将跨域的不同视觉概念映射到统一的听觉语义空间中。它通过收集大量音频片段并使用 K-means 进行聚类，将这些音频聚类结果作为统一的听觉语义空间。AVMM 通过可学习的视觉前缀来表示这些音频聚类，并通过优化重构音频与输入音频的一致性来训练，从而强制模型将视觉内容与听觉空间对齐。

2. **音视频编码器和语言模型生成器（Audio-Visual Encoder &amp; Language Model Generator）：** 这是一个基于 Transformer 的编码器-生成器架构。生成器引入了一个权重参数 (alpha) 来评估不同模态（音频和视频）对每个词语的贡献。这个 alpha 值是通过衡量每个模态的交叉注意力与前一个词语的相关性来计算的。

3. **反事实对比学习（Counterfactual Contrastive Learning）：** 为了解决传统对比学习中对随机负样本质量的依赖，本文提出了两种反事实对比损失：
    * **基于分布的对比损失 (Distribution-based Contrastive Loss, Ldis)：** 旨在直接优化视觉-音频对齐分数。
    * **基于依赖的对比损失 (Dependency-based Contrastive Loss, Ldep)：** 利用反事实结果提供细粒度的监督，直接优化视觉-文本对齐，而无需依赖随机负样本的质量。

在训练细节上，TAVT 采用了元学习（meta-learning）的训练策略。模型首先在源域（Dmeta-train）中随机选择一个子集作为支持集进行训练，其余域作为查询集。在元测试（meta-test）阶段，模型在一个新的目标域（Dtarget）上进行快速适应，通过微调元参数 (theta) 并利用查询集来评估模型在该域上的性能。

实验结果表明，TAVT 在跨数据集和跨领域基准测试中，在所有评估指标上都显著优于现有的对比模型。特别是在数据标注有限的低资源领域（如儿童和美妆），TAVT 依然表现出色，证明了其强大的泛化能力。消融实验也证实了音频特征、AVMM 模块以及两种反事实对比损失（Ldis 和 Ldep）对模型性能的积极影响。

总的来说，TAVT 为可迁移音视频文本生成任务提供了一个有效且鲁棒的解决方案，能够有效地处理多模态领域转移的挑战，并提高了在有限标签数据下的适应能力。</sample>
    <sample id="265">演讲者的名字是Vasudha Varadarajan。</sample>
    <sample id="266">这篇论文的作者隶属于波兰科学院计算机科学研究所和华沙大学。</sample>
    <sample id="268">PaLM 最常见的错误是遗漏错误。</sample>
    <sample id="269">Hello, I'm James Finch.
大家好，我是詹姆斯·芬奇。
And I'm Sarah Finch.
我是莎拉·芬奇。
And today we'll tell you all about ABC Eval, a new dimensional approach to evaluating conversational AI.
今天我们将向大家介绍 ABC Eval，这是一种评估对话式 AI 的全新维度方法。
This work was done by the Emory NLP Lab, led by Professor Jinho Choi at Emory University, and in collaboration with Amazon Alexa AI.
这项工作由埃默里大学崔珍镐教授领导的埃默里 NLP 实验室完成，并与亚马逊 Alexa AI 合作。
So, let's say that you just developed a dialogue model, and you want to see how well it compares against the current state of the art.
假设您刚刚开发了一个对话模型，并且想知道它与当前最先进的技术相比表现如何。
The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better, or to rate conversations given a Likert scale.
常见的做法是使用人工评估，例如要求人工评估员选择两次对话中哪次更好，或根据李克特量表对对话进行评分。
These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects.
这些方法能够很好地提供对话整体质量的综合评估，但对话质量有许多方面。
Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer grained level.
因此，您可能希望从多个维度评估聊天质量，以便更细致地了解模型的优缺点。
One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses, using existing comparative or Likert scale methods.
一种方法是简单地要求人工评估员使用现有的比较法或李克特量表法，评估对话质量的几个维度，例如模型响应的相关性。
However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation.
然而，我们相信存在一种更精确、更可靠的维度对话评估策略。
Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself.
我们的方法试图通过明确标注每个模型响应是否表现出某些行为来减少人类评估的主观性，例如回复不相关的信息或自相矛盾。
We call this approach Annotating Behaviors in Chat, or ABC-Eval in short.
我们称这种方法为“聊天行为标注”，简称 ABC-Eval。
We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature.
我们开发这种方法是为了全面涵盖在近期文献中被认为影响聊天质量的聊天模型行为。
ABC-Eval is capable of measuring the rates at which chat models will commit various thematic errors.
ABC-Eval 能够衡量聊天模型犯各种主题错误的频率。
For example, ABC-Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy.
例如，ABC-Eval 衡量聊天模型在多少轮对话中忽略了对话伙伴或说了不相关的话、自相矛盾或与对话伙伴矛盾、虚构不正确的事实或违反常识，以及模型成功或未能表现出同理心的情况。
To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval.
为了确定哪种评估最有效，我们选择了四个最先进的聊天模型，并使用 ABC-Eval 对每个模型进行了 100 个人机对话的评估。
For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn level, Likert ratings on the dialogue level, and dialogue-level pairwise comparisons.
为了进行比较，我们还使用三种现有方法评估了这些对话：轮次级别的李克特评分、对话级别的李克特评分，以及对话级别的成对比较。
For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue, since this is the standard practice for evaluating chat models along multiple dimensions.
对于每种现有方法，我们收集了对话中八个最常测量方面的评估，因为这是沿多个维度评估聊天模型的标准做法。
From our analyses of these evaluation results, we found that ABC-Eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations.
从我们对这些评估结果的分析中，我们发现 ABC-Eval 行为标签总体上比现有方法收集的标签更可靠，这通过 100 份双重标注对话的标注者间一致性来衡量。
In addition, ABC-Eval labels are more predictive of the overall conversation quality, compared to metrics produced by existing methods, as shown by this simple linear regression analysis.
此外，如本简单线性回归分析所示，与现有方法产生的度量指标相比，ABC-Eval 标签更能预测对话的整体质量。
For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality respectively, while the average Likert consistency scores explain only 4% or less.
例如，您可以看到，衡量自我矛盾和伙伴矛盾的轮次比例分别解释了对话质量的 5% 和 10%，而平均李克特一致性分数仅解释了 4% 或更少。
Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a step-wise linear regression.
最后，我们使用逐步线性回归检查了每个评估指标是否捕捉到聊天质量的独特方面。
You can see how the combination of all ABC-Eval metrics explains over 25% of conversation quality, and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality.
您可以看到，所有 ABC-Eval 指标的组合解释了超过 25% 的对话质量，并且当您逐一删除这些指标时，大部分都会导致失去关于质量的相当多的信息。
On the other hand, the combination of all turn-level Likert metrics explains far less of the quality, and fewer of these metrics carry unique information.
另一方面，所有轮次级别李克特指标的组合解释的质量要少得多，而且这些指标中很少有包含独特信息。
These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve.
这些可靠、信息丰富且独特的 ABC-Eval 指标使我们能够以比以前的方法更高的分辨率评估对话式 AI。
You can see in the result of our experiment that several challenges still remain and have been precisely quantified.
从我们的实验结果中可以看出，一些挑战仍然存在，并且已经得到了精确量化。
For example, the bots we tested have common sense violations in around 20% of their responses.
例如，我们测试的机器人大约有 20% 的回复存在常识性错误。
They produce irrelevant information in around 15% of the responses, and they contradict themselves or their partner around 10% of the time.
他们大约有 15% 的回复提供了不相关的信息，并且大约 10% 的时间会自相矛盾或与对话伙伴矛盾。
With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted.
随着该领域改进的快速发展，自我们进行评估以来发布的新模型中，许多错误率可能会降低。
However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models.
然而，这更是追求可靠和精确的模型比较评估指标的原因。
We hope ABC-Eval can be leveraged by others in the field as a meaningful step in this direction, and we look forward to seeing how conversational AI will advance in the coming months and years.
我们希望 ABC-Eval 能够被该领域的其他人利用，作为朝这个方向迈出的有意义的一步，我们也期待看到对话式人工智能在未来几个月和几年内将如何发展。
Thank you for watching.
谢谢观看。</sample>
    <sample id="270">论文作者所属机构是埃默里大学和亚马逊 Alexa AI。</sample>
    <sample id="271">在这篇论文中，CFT 代表连续微调。</sample>
    <sample id="272">这篇论文有7位作者。</sample>
    <sample id="273">00:00</sample>
    <sample id="274">演讲者的名字是Yusen Zhang。</sample>
    <sample id="276">本研究介绍了IndicMT Eval，一个用于印度语言机器翻译（MT）度量元评估的数据集。当前，MT评估度量的研究主要集中在英语翻译上，而对其他语言的翻译评估研究不足。考虑到印度语言在语法规则、词汇共享与借用、方言差异以及句子结构等方面的独特性，直接采用为英语设计的评估度量可能不适用。

为了解决这一问题，研究者们从Flores数据集中随机选择了200个英文句子，并通过七种不同的翻译模型或API（包括mBART、Bing API、Google API、CVIT-IIITH、IndicTrans、mT5和NLLB）将它们翻译成五种印度语言：泰米尔语（Tamil）、马拉雅拉姆语（Malayalam）、印地语（Hindi）、马拉地语（Marathi）和古吉拉特语（Gujarati）。这五种语言分别属于达罗毗荼语系和印欧语系。每种语言产生了1400个候选翻译，总计7000个样本。

随后，由精通两种语言的专家对这些翻译进行人工标注，以捕获高质量的评估数据。标注采用MQM框架，专家们不仅识别并标记文本中的次要或主要错误，还指出错误的类型和严重程度，并为每个翻译提供一个总体评分。错误类型分为准确性（意义相关错误）、流畅性（语法、拼写等）和其他/特殊类别，并进一步细分为子类别。

对这些数据进行的分析显示，与较旧的模型（如CVIT-IIITH）相比，NLLB或IndicTrans等较新的MT模型错误较少。在度量标准方面，基于字符重叠的chrF++在所有语言中显示出最高的平均相关性，但总体表现不佳。基于嵌入的度量标准中，LaBSE表现最好，而BERTScore在结合不同的多语言模型嵌入后进一步提升了相关性，其中MuRIL表现突出。最后，Comet度量标准的变体在所有语言中显示出最高的总体相关性。

研究还发现，许多度量标准倾向于在一个狭窄的范围内给出分数，未能充分利用其完整的评分范围，这使得对度量分数进行有效解释变得困难。此外，当仅标注准确性错误时，几乎所有度量标准与人类评分的相关性都更高。

基于这些发现，研究者们对Comet度量标准的变体进行了微调，创建了Indic COMET。结果显示，IndicCOMET_{MQM}在五种语言中的三种上优于基线Comet模型，并且在所有语言上的平均相关性都更高。在零样本性能测试中，IndicCOMET_{MQM}在大多数语言上均优于基线Comet模型。在ACES翻译准确性挑战集上进行的鲁棒性评估也表明，IndicCOMET_{MQM}比Comet基线模型更具鲁棒性。

该研究公开了数据集和代码，旨在促进印度语言机器翻译评估领域的研究和发展。</sample>
    <sample id="277">新方法没有特定名称。</sample>
    <sample id="278">作者提到，“显性词汇”方法利用了社会语言学中的“显性度”概念。该概念认为，非显性群体是默认的、普通的，而显性群体则与默认的群体不同，因此在语言上被标记。

在方法中，首先定义非显性群体和显性群体。然后，通过加权对数优势比来比较非显性群体和显性群体中出现频率最高的词语，以区分每个显性群体。

例如，对于黑人女性角色，将对数优势比与白人角色和男性角色进行比较，以识别区分性的词语。

该方法的优点在于可以识别具体的偏见模式，而无需依赖预先构建的词典。</sample>
    <sample id="279">这篇论文的作者分别来自华盛顿大学的Paul G. Allen School和Carnegie Mellon University的Language Technologies Institute。</sample>
    <sample id="280">MultiEMO是一个基于注意力机制的关联感知多模态情感识别框架，用于对话中的情感识别。MultiEMO旨在解决多模态信息互补性未被充分利用、少数情感类别表现不佳以及语义相似情感难以区分的问题。

该框架由四个主要部分组成：单模态特征提取、上下文建模、多模态融合和情感分类。MultiEMO引入了一个名为VisExtNet的新型视觉特征提取器，它通过整合来自多个帧的对话者面部表情来捕捉视觉线索，避免了场景相关冗余信息的编码。为了实现多模态融合，MultiEMO设计了一个名为MultiAttn的模型，该模型基于双向多头交叉注意力层，成功地模拟了文本、音频和视觉模态之间的复杂关联。为了解决少数类别和语义相似情感的分类难题，MultiEMO引入了一种样本加权焦点对比（SWFC）损失。

在MELD和IEMOCAP数据集上进行的广泛实验表明，MultiEMO框架在两大数据集上都取得了最先进的性能，尤其是在少数类别和语义相似情感方面的改进尤为显著。案例研究进一步展示了MultiEMO在情感倾向异步场景中的卓越性能。

然而，MultiEMO也存在一些局限性。VisExtNet无法区分对话者和场景中的无关人物，SWFC损失在MELD数据集上需要较大的批次大小来确保每个训练样本至少存在一个正对，这会带来较高的计算成本。尽管MultiEMO在少数情感类别方面取得了显著改进，但其在少数情感类别上的表现仍然不如多数类别。</sample>
    <sample id="281">Kayo Yin 介绍了题为“翻译何时需要语境？一项数据驱动的多语言探索”的研究。她首先通过“mole”一词的例子强调了语境在翻译中的重要性，其含义会根据上下文发生变化。

她指出，评估语境相关翻译的挑战在于：
1.  只有一小部分词语的翻译依赖语境，这使得诸如BLEU等语料库级别的指标难以捕捉。
2.  现有方法通常依赖领域知识和人工标注，仅支持有限的语篇现象和语言。

为了解决这些问题，该研究旨在回答两个研究问题：
1.  翻译何时需要语境？
2.  模型如何处理语境相关翻译？

为了回答第一个问题，研究团队引入了点式条件交叉互信息（P-CXMI），这是一个衡量机器翻译模型在给定语料库下使用语境程度的指标。P-CXMI可以量化语境C对目标词Y提供的额外信息，而不仅仅是源文本X。高P-CXMI的词语被认为是需要语境进行翻译的。

研究团队对TED演讲的英译14种语言的文本进行了语境相关词语的主题分析，分析了三个层面：
1.  **词性标注（POS tags）**：发现阿拉伯语中的双数代词（pronouns）具有较高的P-CXMI，因为英语中没有对应的概念，需要语境来确定双数。此外，某些语言中动词形式的选择也需要语境。
2.  **词汇项**：识别出需要语境的特定词语。例如，中文翻译专有名词时需要语境来保持文本内的一致性（lexical cohesion）。此外，翻译中的正式性（formality）选择也依赖于语境。
3.  **独立词元（individual tokens）**：识别出无法仅凭词语本身捕捉，而是通过句子结构表达的现象，例如省略（ellipsis）的消解。

为了回答第二个问题，研究团队根据这些发现设计了一个多语言语篇感知（MuDA）基准。他们创建了标注器来自动识别语篇现象相关的词语，并通过该基准评估了不同模型的文档级翻译能力。

评估结果显示：
1.  在语料库级别的指标（BLEU, COMET, F-measure）上，很难明确哪种系统在文档级机器翻译中表现最佳，这再次凸显了传统评估方法的局限性。
2.  在MuDA基准测试中，语境感知模型在某些语篇现象（如正式性和词汇衔接）上表现显著优于不使用语境的模型。
3.  然而，对于其他现象（如省略、代词和动词形式），语境感知模型并没有明显优势，这指明了文档级翻译需要进一步改进的领域。
4.  商业系统方面，DeepL在大多数语篇现象和语言对上都优于Google翻译（截至2021年4月）。

总结来说，该研究系统地识别了翻译何时需要语境，并提供了一个数据驱动、不依赖特定数据集的文档级机器翻译基准，有助于评估模型在处理复杂语篇现象方面的能力。</sample>
    <sample id="282">在 StoryTrans 项目中，我们深入探讨了自然语言生成领域一个重要而富有挑战性的任务——非平行文本风格转换。虽然目前大多数研究都集中在词汇或句子层面，例如情感或正式文本转换，但我们的工作迈出了重要一步，将风格转换提升到话语层面，这对于模仿作者的独特风格至关重要。

我们提出的生成模型 StoryTrans 旨在解决长文本风格转换中的两个主要挑战：首先，模仿话语层面的语言选择，如叙事技巧；其次，解决作者风格与特定写作主题的高度关联性，这使得将风格特定的内容从一种风格转移到另一种风格变得困难，如Table 1中橙色标记的缺失内容。

StoryTrans 通过从源文本中学习话语表示，并将其与规范的风格嵌入相结合，从而在目标风格中生成文本。为了增强内容保留，我们设计了一个两阶段生成过程：第一阶段，我们转换源文本中被屏蔽的风格特定内容，生成目标风格的掩码文本；第二阶段，我们整合这些关键词，生成完整文本，同时弥补缺失的内容。

在训练框架方面，我们采用了对抗性训练框架。第一阶段使用自重构损失来恢复输入，解耦损失旨在解耦句子嵌入中的风格和内容，句子顺序损失旨在捕捉句子级别的依赖关系，而风格分类器损失则试图为整个系统生成风格信号。第二阶段则专注于填充正确的风格特定内容，并移除掩码标记，最终生成转换后的文本。

在评估阶段，我们收集了中文和英文的新数据集，并进行了广泛的实验，以将童话或寓言故事转换为典型的作者风格。无论是自动评估结果还是人工评估，都证实了我们模型的有效性，并且 StoryTrans 在风格控制和内容保留方面均优于基线模型。风格可视化进一步表明，StoryTrans 转换后的文本在风格特征空间中与黄金标准文本保持一致。

如需了解更多信息或有任何疑问，请访问我们的GitHub仓库或通过电子邮件联系我。</sample>
    <sample id="283">第一个提到的对称依存关系结构的名称是“Conjunction-headed/Prague”。</sample>
    <sample id="284">本演讲介绍了一种名为FSSUIE的新型模糊跨度机制，用于增强通用信息提取。演讲者强调，现有的通用信息提取（UIE）模型过度依赖标注跨度的精确边界，这在实践中容易出现歧义。为了解决这个问题，他们提出将跨度边界建模为连续分布，而不是精确值，从而引入了“模糊跨度损失”（Fuzzy Span Loss）。这种损失通过计算预测边界分布与模糊边界分布之间的KL散度，并结合交叉熵损失来优化模型。

此外，演讲还指出Transformer模型在特征提取和信息提取之间存在不匹配，Transformer倾向于关注全局特征，而信息提取则需要更多局部特征。为了使模型学习更合理的注意力分布，演讲者引入了“模糊跨度注意力”（Fuzzy Span Attention），这是一种掩码函数，用于动态调整注意力跨度，并使其在边界处线性衰减，而不是截断。这使得模型能够更专注于限定范围内的先行词语的语义信息，而非全局表示。

实验结果表明，FSSUIE在命名实体识别（NER）、关系提取（RE）和方面情感三元组提取（ASTE）等广泛的信息提取任务中取得了显著的性能提升。与不使用模糊跨度机制的基线模型相比，FSSUIE-base在NER任务中表现出更好的泛化能力，尤其是在小型数据集上，并且在RE任务中以更简单的结构达到了SOTA结果，体现了对领域特定信息的强大泛化能力。消融研究进一步证实，模糊跨度注意力（FSA）通过引导模型获得合理的注意力分布，加速了收敛速度；模糊跨度损失（FSL）则通过充分利用标注信息，提升了信息提取能力；两者的结合产生了更大的协同增强效果。可视化结果也显示，FSA层促使模型在有限的前置词语范围内关注语义信息，符合预期。</sample>
    <sample id="285">视频由来自北京大学的高明奇介绍了一项名为“Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework”的研究。这项研究的核心是如何评估和改进对话摘要中的事实准确性。

高明奇首先指出，目前由模型生成的摘要甚至是一些参考摘要都可能包含事实错误。为了解决这个问题，通常有两种解决方案：一是直接设计更好的摘要模型来提高事实准确性，二是对模型生成的摘要进行事实错误纠正（FEC）。

然而，当前评估FEC模型的方法存在缺陷。例如，使用FactCC等事实准确性指标可能过于模糊，并且无法有效区分不同类型的解决方案。一个FEC模型可能会忽略原始摘要的内容，直接生成一个不同的、但事实更准确的摘要，这实际上并没有进行错误纠正。

为了解决这些问题，高明奇及其团队提出了新的思路。他们建议手动标注“参考纠正”，即对模型生成的包含事实错误的摘要进行人工修正。修正时，他们要求尽可能通过替换、插入和删除操作来纠正事实错误，以获得流畅且非冗余的摘要。

这种引入人工参考纠正的方法有两方面的优势：首先，它为FEC模型的训练提供了比伪数据更有价值的真实数据；其次，也更重要的是，它为更全面和准确地评估FEC模型的性能创造了条件。

为了实现自动分类事实错误，他们提出了新的事实错误分类法，包括基于形式和基于内容的两种类别。基于形式的类别根据是添加、删除还是替换操作来分配；基于内容的类别则根据词性和依存关系来分配。

受ERRANT（Bryant et al., ACL 2017）语法错误纠正评估指标的启发，他们构建了自己的评估框架，主要包括三个步骤：对齐、分类和比较。对齐后，自动分配基于形式和基于内容的错误类别，最后进行比较以计算得分。

通过对FEC模型进行不同训练模式的实验，他们得到了一些关键发现：
1. 使用来自对话摘要数据集的参考摘要训练FEC模型，虽然在不可靠的事实准确性指标上能获得最好的结果，但这凸显了改变FEC模型评估方法的迫切需求。
2. 在训练FEC模型时引入人工纠正的摘要可以提高其性能。将人工标注数据与合成数据结合是一个很有前景的方向。
3. 当前的FEC模型在通过添加纠正事实错误方面存在困难，并且无法有效处理属性错误、情态错误和链接错误等问题。

最后，高明奇感谢了观众的聆听。</sample>
    <sample id="286">演讲者的名字是James Finch和Sarah Finch。</sample>
    <sample id="287">根据所给的英文内容，这篇论文有4位作者。</sample>
    <sample id="288">BLiMP、SyntaxGym 和 CrowS 数据集可用于测试句法现象。</sample>
    <sample id="290">第一个研究问题的五种方法缩写是：FTw、BOND、COSINE、MLC、L2R。</sample>
    <sample id="291">该模型在11个下游任务上进行了评估，其中包括命名实体识别、分类、词性标注和问答。</sample>
    <sample id="294">CamemBERT 最初是在 OSCAR 138 GB 或 CCNet 4 GB 数据上训练的。</sample>
    <sample id="295">演讲者的名字是Adam Przepiórkowski。</sample>
    <sample id="296">本视频介绍了一项名为“EPIC：多视角反讽语料库注释”的研究，这项研究由都灵大学和亚马逊 Alexa 合作完成。该研究旨在深入理解和建模人类对讽刺语言的主观感知。

视频首先强调了现代自然语言理解（NLU）对大量人工标注数据的依赖，这些数据编码了人类知识。然而，在处理主观任务时，传统“真实情况”标注范式存在局限性。研究人员选择了反讽作为研究对象，因为它是一种高度潜在且语用丰富的自然语言现象。

EPIC 语料库旨在通过多视角标注来解决这一局限性。该语料库从 Reddit 和 Twitter 等社交媒体平台收集了大约 3000 对文本/回复对话，时间跨度为 2020 年 1 月至 2021 年 6 月。为了确保语料库的多样性，数据收集涵盖了五种英语变体（英国、美国、爱尔兰、澳大利亚和印度）。

在标注过程中，研究人员使用了 Prolific 众包平台。总共有 74 名标注者，每种英语变体约 15 名标注者。每位标注者被分配了 200 个文本（短对话）进行标注，其中包含注意力检查问题以进行质量控制。平均每个文本获得了 5 次标注。标注任务非常简单，呈现对话文本，然后提问“回复是否具有讽刺意味？”，标注者只需选择“讽刺”或“不讽刺”。

研究结果显示，不同人群对反讽的感知存在差异。无论是按性别、年龄组、国籍、民族、学生身份还是就业状况划分，标注者之间的内部一致性（通过 Cohen's Kappa 衡量）分布都不同。

为了进一步探索这些差异，研究人员训练了“视角感知模型”。他们发现，与标准非视角感知模型相比，视角感知模型在决策时通常具有更低的**不确定性**，并且在针对代表其视角的测试集进行测试时，它们倾向于更**自信**。

最后，研究人员探讨了反讽感知差异的潜在原因。他们发现，年龄相邻的世代（例如，婴儿潮一代与 Y 世代）在反讽感知上表现出更高的分歧，Y 世代与 Z 世代之间也是如此。在地理分布方面，英国和爱尔兰的标注者在反讽感知上的差异最大。

这项研究强调了在处理主观 NLU 任务时考虑多视角的重要性，并为未来的研究提供了宝贵的语料库和见解。</sample>
    <sample id="297">在“从口哨到扩音器：用语言模型揭示隐秘修辞”这项研究中，Julia Mendelsohn 和同事们深入探讨了“口哨词”的现象，这类词语表面上无害，但却向特定群体传递着隐秘的仇恨信息。

研究人员建立了一个包含340多个“口哨词”的词汇表，涵盖了种族主义、跨性别恐惧症和反犹太主义等多种偏见，并根据语域、类型和人物特征对这些词语进行了分类。

他们对美国政治演讲进行了历史案例分析，发现自民权时代以来，“口哨词”的使用明显增加，并且与保守主义言论的关联性越来越高。这表明“口哨词”是政治影响力的一种关键机制。

此外，研究人员还评估了大型语言模型（如 GPT-3）识别“口哨词”的能力。他们发现，尽管 GPT-3 能够识别词汇表中约 45% 的“口哨词”，但在处理非正式或在线语域，以及识别跨性别恐惧症相关的“口哨词”时，表现相对较差。通过提供“口哨词”的定义和暗示语，模型的识别性能得到了显著提升，尤其是在正式语域中。

最后，这项研究揭示了“口哨词”如何逃避内容审查。通过使用谷歌/Jigsaw 的 Perspective API 进行毒性检测分析，研究人员发现，当使用“口哨词”替代直接的侮辱性词语时，即使语句表达的仇恨含义相同，其毒性评分也会显著降低。这表明“口哨词”是一种策略，可以在不被内容审核系统标记的情况下传播仇恨和辱骂信息。

总的来说，这项研究不仅提出了一个“口哨词”的分类和词汇表，揭示了其在政治语境中的历史演变和意识形态关联，还探讨了语言模型在识别这些隐秘修辞方面的能力和局限性，并强调了“口哨词”在规避内容审核方面的作用。</sample>
    <sample id="298">研究发现，与ConLL-2003训练的模型在新的ConLL++数据集上，性能下降是由于训练数据和测试数据之间时间差的增加，导致模型性能下降。</sample>
    <sample id="299">研究旨在通过最小-最大（Minimax）训练提高自然语言推理（NLI）模型的鲁棒性。NLI模型在基准测试中表现出色，但容易利用训练数据中与标签虚假相关的快捷方式，导致在分布外（OOD）测试集上性能不佳。

以往的快捷方式缓解方法需要预先了解快捷方式，这限制了它们在实际应用中的潜力。此外，这些方法依赖于辅助模型来重新加权训练实例，但辅助模型的行为可能与学习模型不动态一致，导致训练不稳定。辅助模型通常也是大型预训练语言模型，这会增加额外的计算开销。

为了解决这些限制，研究提出了一种新的最小-最大训练方法，旨在通过学习一个强调被低估的“难例”的权重分布来减少NLI模型对快捷方式的依赖。该方法在学习模型和辅助模型之间采用最小-最大目标。学习模型旨在最小化NLI任务的损失，而辅助模型则通过生成示例权重来最大化学习模型的损失，从而促使学习模型专注于输入空间中损失较高的区域。

这种方法具有几个优点：它不需要预先假设快捷方式的类型，而是依赖于学习模型自身的训练动态来生成示例权重。辅助模型是一个简单的前馈网络，而不是大型预训练语言模型，从而降低了计算开销。

研究人员在Fever、MNLI和QQP三个常用的NLI数据集及其对应的OOD测试集上评估了该方法。结果显示，与传统的经验风险最小化（ERM）训练模型和现有最好的快捷方式缓解方法相比，最小-最大训练方法一致地提高了OOD性能，同时保持了高分布内（ID）准确性。论文还探讨了在更大的模型、合成快捷方式和域外测试集上性能提升的泛化能力，预训练对学习模型的影响，辅助模型所需的大小，以及对学习到的示例权重分布的定性评估。</sample>
    <sample id="300">这段视频介绍了“交互式听写”这项任务，它允许用户通过语音进行听写和编辑。目前的语音转文本系统大多只支持听写，不具备语音编辑功能。而那些具备语音编辑功能的系统，例如Nuance Dragon NaturallySpeaking和Microsoft Word的听写功能，由于需要用户记住固定的命令列表，操作起来不够直观。

“交互式听写”旨在实现听写和编辑的灵活交错，且无需特定的触发词来激活命令。系统需要预测听写和编辑命令之间的边界，并能理解开放式的自然语言编辑指令，而非仅限于固定模板。

研究团队主要贡献包括：
1. 介绍和形式化了“交互式听写”这项新任务。
2. 设计了一个数据收集界面，并构建了一个名为TERTIUS的数据集，该数据集包含1320条语音轨迹，其中有959个听写段和3225个命令段。
3. 创建了一个基线系统来解决这项任务。

基线系统由四个步骤组成：
1. 自动语音识别（ASR）：将原始音频解析成语音文本。
2. 分割：将语音文本分割成听写和命令话语。
3. 规范化：提取并规范化每个命令，修复ASR误识别和语音错误。
4. 解释：按顺序执行每个听写和命令话语，直到文档达到最终状态。

为了实现这些步骤，研究人员训练了单独的模型，特别是针对解释模型，他们尝试了T5和GPT3两种架构，以及两种输出类型：预测可由执行引擎执行的程序，或直接预测下一个状态。

评估结果显示：
- 在分割模型方面，T5-base模型在对话级精确匹配上达到了85.3%的准确率，每次采样的运行时为0.097秒。
- 在ASR修复和解释模型方面，GPT3模型在准确性上优于T5，但运行时间也更长。直接预测状态比预测中间程序更准确。对于T5模型，程序预测可以显著提高效率，同时对准确性影响最小。

总的来说，该研究提出了一种新的交互式听写任务，并提供了一个数据集和基线系统，为未来在该领域的研究奠定了基础，尽管在准确性和效率之间仍存在权衡，且有很大的改进空间。</sample>
    <sample id="302">输出序列中的词元最初是无序的，无法形成一个完整的逻辑形式。因此，需要进行排列以生成正确的逻辑形式。</sample>
    <sample id="303">作者建议模型所有者提高偏见缓解方法的透明度，因为目前尚不清楚这些积极的刻板印象是源于过度价值对齐还是其他反刻板印象方法，而如果不提高透明度，就无法进一步研究。</sample>
    <sample id="304">根据视频内容，最小对（Minimal Pair）范式通过比较语言模型对两句略有不同的句子的概率来评估语言模型。

其中，不可接受输入（Unacceptable Input）是指模型应该给出较低概率的句子，因为该句子在语法或语义上是不正确的，或者包含了偏见。在视频中，不可接受的输入通常用星号 (*) 标记，例如：
* “Many people were helping herself.” （语法不正确）
* “*The customer … has spent any money.” （语法不正确）
* “Men are terrible at handiwork.” （包含偏见）

语言模型被认为是成功的，如果它对可接受的输入分配的概率高于对不可接受输入的概率。</sample>
    <sample id="305">视频讨论了弱监督学习（WSL）的局限性，并提出了三个核心研究问题。

首先，视频揭示了WSL方法对干净验证集的依赖。研究表明，如果没有干净的验证数据，WSL模型的泛化能力会大幅下降，甚至可能导致训练毫无意义。这强调了干净验证集在WSL中的不可或缺性，以及在实践中不应忽视获取干净样本的成本。

其次，视频探讨了WSL方法所需的干净样本数量。研究发现，增加干净验证样本的数量有助于提高WSL方法的性能。通常情况下，每个类别大约需要20个干净样本就能达到较好的性能。然而，如果能直接用干净数据进行微调（特别是对于参数高效的微调方法，如LoRA），在每个类别拥有10个样本时，模型性能甚至会超越WSL方法。这表明，在具备少量干净数据的情况下，直接微调可能比复杂的WSL方法更有效。

最后，视频探究了如何更有效地利用可用的干净样本。研究发现，通过在干净验证集上进行持续微调（CFT），可以消除不同WSL方法之间的性能差距，即使是简单的全模型微调（FTw）也能达到与复杂方法相同的表现。这意味着，在实际应用中，没有必要选择更复杂、需要更多计算和存储空间的WSL方法，因为连续微调可以使FTw表现得同样出色。

综合来看，视频的结论是，现有的WSL方法通常需要干净样本，并且过高估计了它们的实用性。基于这些发现，视频提出了三点建议：报告模型选择标准（特别是是否使用了干净验证集），使用少样本学习方法作为基线进行比较，以及始终考虑应用连续微调（CFT）。视频还提供了代码开源链接，鼓励进一步的探索。</sample>
    <sample id="306">两位研究者Najoung Kim和Sebastian Schuster对语言模型中的实体跟踪能力进行了研究。实体跟踪是指代理理解哪些实体被提及，以及这些实体在话语展开过程中状态如何变化的能力。例如，在食谱语境中，代理需要理解“将鸡蛋、糖和面粉放入碗中”会导致这三种实体都在碗中。如果话语继续说“搅拌，形成轻面糊”，代理必须理解所有这些实体现在都是面糊的一部分。

然而，评估实体跟踪能力存在几个挑战：
1. **预训练数据中的常见实体状态**：某些实体状态在预训练数据中很常见，因此模型可能在没有实际实体跟踪能力的情况下预测正确状态。
2. **脱离语境的预测**：有时实体状态可以从单个单词或短语预测，而无需考虑更大的语境。模型可能看起来能够执行实体跟踪，但实际上只是学习了单词和实体状态之间的简单启发式关联。
3. **微调和上下文化示例的问题**：如果使用微调或上下文化的示例来探测模型，模型可能会记住实体状态序列，或者学习应用插槽填充等启发式方法，如果这些启发式方法在评估任务设计中没有被阻止。

为了克服这些挑战，研究者设计了一个涉及盒子和物品的任务，以评估实体跟踪能力。任务设置是向模型提供每个盒子的初始内容描述，然后要求模型预测每个盒子的内容。为了增加复杂性，任务中还包含了多个状态改变操作，如移动物品或向盒子中添加物品。研究者还采取了各种措施，防止模型使用启发式方法。

实验结果表明，在两样本上下文学习实验中，大多数模型只是重复初始状态。只有GPT-3.5 text-davinci-003模型表现出非平凡的实体跟踪行为，而所有其他模型都低于通过随机模拟获得的强大随机基线。进一步的研究发现，所有GPT-3.5模型都经过了大量代码的预训练，而其他模型则没有。这表明对文本和代码进行预训练可以使语言模型中的实体跟踪能力得以显现。

此外，研究者还发现，像T5-base（230M参数）这样较小的预训练模型，如果直接进行微调，也可以学习执行非平凡的实体跟踪行为。然而，相同大小的随机初始化模型即使在直接监督下也无法学习这种行为，这再次表明预训练的重要性。研究者也指出，目前尚不清楚观察到的实体跟踪能力在多大程度上能够推广到盒子设置之外的更复杂情境。</sample>
    <sample id="307">在评估他们提出的模型时，作者使用了以下评估指标：
* **F1 分数** 用于命名实体识别 (NER)
* **F1 分数** 用于分类 (CLS)
* **F1 分数** 用于词性标注 (POS)
* **精确匹配率 (EMR)** 和 **汉明分数** 用于问题回答任务。</sample>
    <sample id="308">这篇演讲介绍了“NLP Positionality”框架，旨在揭示自然语言处理（NLP）数据集和模型中存在的偏差。演讲者 Jenny Liang 首先通过一个新闻评论审核的例子，说明了技术在不同文化背景下可能表现出的差异性，即所谓的“设计偏差”。接着，她引入了“Positionality”的概念，指个人因其人口统计学特征、身份和生活经历而形成的独特视角，并提出数据集和模型也可能体现出这种“Positionality”，因为它聚合了真实人类的判断和观点。

为了研究数据集和模型的Positionality，该框架采用了两个主要步骤。首先，通过一个名为“LabintheWild”的众包平台，邀请来自不同背景的志愿者重新标注数据集。LabintheWild的优势在于能够招募到比传统众包平台（如Mechanical Turk）更多样化的参与者，并获得高质量的标注数据。这项研究针对两个NLP任务进行了实验：社会可接受性（使用Social Chemistry数据集）和毒性检测（使用Dynahate数据集）。每个任务都要求参与者阅读一段文本，然后根据自己的观点进行评分，并可以与AI及其他参与者的回复进行比较。

实验结果表明，NLP中确实存在Positionality。数据集和模型在社会可接受性任务中与说英语的国家和拥有大学教育的人群最为一致，在毒性检测任务中也表现出类似的结果。这表明，目前的NLP技术在这些方面更偏向于特定的用户群体。此外，研究还发现数据集和模型对非二元性别（Non-binary）人群的对齐度较低。

基于这些发现，演讲者提出了三点建议：1. 记录构建数据集和模型过程中所有相关的设计选择。2. 从透视主义视角进行NLP研究，包括分享去聚合的数据集标签，并使用能够处理标注者分歧的模型技术。3. 建立专门为特定社区服务的数据集和模型，以实现更具包容性的NLP，例如Masakhane倡议就是这方面的一个良好范例。最后，演讲者提供了相关仪表板和论文链接，鼓励听众进一步了解。</sample>
    <sample id="309">使用了 Krippendorff's Alpha 指标。</sample>
    <sample id="310">在不可接受和可接受查询中，选择 **维基百科（Wikipedia）** 领域来添加完全无关的句子。</sample>
    <sample id="311">论文作者所属机构是德国杜塞尔多夫的海因里希·海涅大学。</sample>
    <sample id="312">MultiInstruct 是首个多模态指令调整基准数据集，包含来自 10 个广泛类别的 62 个不同多模态任务，每个任务都包含 5 条专家编写的指令。</sample>
    <sample id="313">这篇论文有三位作者。</sample>
    <sample id="314">二进制协调（Binary Coordination）是指协调结构中只有两个并列成分，比如“盐和胡椒”，或“特德和内德笑了”。在依存句法分析中，二进制协调在不同的理论和语料库处理方式下有不同的表示方法。</sample>
    <sample id="315">由于所给内容中未提供提示语的平均长度，因此无法回答该问题。</sample>
    <sample id="316">较小的 T5 模型经过 Coscript 微调后，可以生成比大多数大型语言模型更高质量的脚本，这表明当在合适的数据集上进行训练时，较小的模型可以超越大型模型。</sample>
    <sample id="317">在这段演讲中，讲者提出了CodeIE，一个利用大型代码生成模型进行少样本信息抽取的方法。传统的信息抽取方法存在预训练和推理阶段输入输出格式不匹配的问题，导致模型难以生成正确的结构化输出，需要大量标注数据和特殊解码策略。

为了解决这一问题，CodeIE将信息抽取任务转换为结构到结构的**代码生成任务**。它通过设计特定的代码提示（Code Prompt），将原始文本转换为Python函数结构，其中包含输入文本、一个用于存放提取信息的列表（如实体列表或关系列表）以及触发代码生成的注释。例如，对于命名实体识别，它会生成一个Python函数，该函数接收输入文本，初始化一个空实体列表，然后逐步向列表中添加字典形式的实体信息，每个字典包含实体文本和类型。对于关系抽取，方法类似，但列表元素是关系信息。

CodeIE在NER和RE基准数据集上进行了评估，并与T5、UIE、GPT-3等模型进行了比较。结果显示，在使用少样本（1到5个样本）的情况下，CodeIE通过代码格式提示显著优于传统的基于文本格式提示的基线模型，尤其是在召回率方面表现更佳。

进一步的分析表明，使用代码格式提示的CodeIE模型在输入格式和模型本身之间具有更好的**一致性**，以困惑度衡量时，其表现优于文本格式提示。此外，在解码过程中，GPT-3结合文本提示时容易产生结构错误和未在预定义标签集中出现的错误标签，而CodeIE结合Codex模型和代码格式提示时，这些错误几乎不存在，表明其具有更高的结构保真度。

总结来说，CodeIE证明了大型代码生成模型可以作为有效的少样本信息抽取器，通过将任务结构化为代码生成，解决了输入输出格式不匹配的问题，并提高了模型在少样本情境下的表现。</sample>
    <sample id="318">00:00
大家好，我是Yanis Labrak，我将向大家介绍我们关于DrBERT的工作，这是一个针对生物医学和临床领域的强大法语预训练模型。
00:09
本次演示中，我们将首先讨论医疗保健领域的语言建模。然后，我们将介绍我们文章的主要贡献。我们推出了第一个法语生物医学模型，名为DrBERT，它基于Roberta，并在NACHOS上进行训练。NACHOS是一个从网络上抓取的医疗数据集中提取的医疗数据。我们还比较了多个预训练设置和数据源的模型。接下来，我们将展示我们在11项法语生物医学和临床下游任务上的结果。最后，我们将总结实验结果，并提供更多关于如何访问这些模型的细节。
00:49
自2018年发布以来，BERT已成为解决自然语言处理任务最有效的方法之一，与Word2Vec、FastText或ELMo等传统的静态和上下文方法相比，其性能得到了显著提升。此后，该模型已被应用于许多其他语言，如法语的CamemBERT和FlauBERT，以及生物医学领域的PubMedBERT和BioBERT，以及临床领域的ClinicalBERT，但大多是英语模型。针对其他语言的专用模型很少，并且主要依赖于使用现有通用模型进行持续预训练，因为缺少领域内数据。然而，法语还没有适用于生物医学领域的开源模型。
01:35
因此，我们向自己提出了一个问题：为了广泛应用，最合适的数据源是什么？爬取数据是否能很好地替代临床数据？为了回答这个问题，我们将DrBERT与我们的ChuBERT模型进行了比较，后者基于从南特大学医院数据仓库中提取的匿名数据。之后，我们又问自己：我们需要多少数据才能在法语数据上训练一个专用模型？是4GB，8GB，还是更多？为了回答这个问题，我们首先训练并比较了四个从零开始的模型：一个DrBERT的第一个版本，包含7GB的NACHOS数据；一个DrBERT的第二个版本，包含4GB的NACHOS数据；一个ChuBERT的第一个版本，这是一个临床模型，包含4GB的临床笔记数据；以及一个ChuBERT的最终版本，包含4GB的NACHOS数据和4GB的临床笔记数据。除了这次比较，我们还引入了三个在持续预训练中训练的模型，以分析预训练策略的影响。一个基于CamemBERT的权重，并在4GB的NACHOS数据上进行训练；另一个也基于CamemBERT，但这次在4GB的临床笔记数据上进行训练；最后，一个基于英语生物医学模型PubMedBERT的权重，并在4GB的NACHOS数据上进行训练。总共有七个模型。
03:02
为了评估我们的七个模型，我们收集了13个公共和私人下游任务，如命名实体识别、分类、词性标注和问答。这些模型与六个基线模型进行了比较：CamemBERT OSCAR 138GB、CamemBERT OSCAR 4GB、CamemBERT CCnet 4GB、PubMedBERT、BioBERT和ClinicalBERT。评估结果表明，模型在与训练数据性质相同的任务上表现最佳。然而，我们可以观察到来自异构来源的数据似乎更具通用性。我们还观察到使用更多数据可以转化为更好的性能。
03:51
总的来说，从零开始的预训练在大多数任务上似乎取得了更高的性能。然而，我们对使用PubMedBERT的权重和分词器在4GB的NACHOS子集上进行持续预训练的实验显示，其结果与从零开始训练的DrBERT 4GB相当。这与基于CamemBERT权重和分词器的模型不同，后者存在稳定性问题。
04:18
最后，作为结论，我们提出的系统在11个下游法语医疗任务中的9个上表现更好，并且在整体上超越了通用模型CamemBERT的结果。我们还观察到专业数据更好，但它无法很好地扩展。所有从NACHOS获得的预训练模型在Hugging Face上免费提供，并且所有训练脚本都在我们的GitHub仓库中。
04:50
非常感谢您观看本次演示，我们期待在多伦多的海报会议上进行交流。</sample>
    <sample id="319">论文研究了以下三种学习策略：
1. **从头开始训练（From scratch）：** 使用完整的模型构建。
2. **在预训练模型基础上进行持续预训练（Continual pre-training）：** 使用现有预训练模型（例如，CamemBERT，一个法语通用模型；或 PubMedBERT，一个基于英语的模型）。
3. **混合训练（Mixed training）：** 使用 NACHOS 和 NBDW 的混合数据集，并在从头开始训练的模型上进行训练。</sample>
    <sample id="320">演讲者指出，没有观察到由重复使用相同测试集导致的模型过拟合。</sample>
    <sample id="321">评估简化质量可以通过使用像 LongBART 这样的语言模型，通过微调来生成简化文本。然后，使用像 SARI、BLEU 和 F-resemblance 这样的评估指标来衡量简化后的文本与原始文本相比的准确性、流畅性和忠实度。LongBART 模型在文本简化方面可以超越基线模型。</sample>
    <sample id="322">该视频的主题是关于文本分类器如何理解道德。主讲人Enrico Liscia首先解释了人类道德的定义，即区分对错的能力，以及它在社会中的重要性。接着他阐述了在自然语言处理（NLP）领域中，道德通常被视为一个从“不道德”到“道德”的单一尺度进行衡量，但这忽略了道德判断的主观性和多元性。

为了解决这一问题，他引入了道德基础理论（Moral Foundation Theory），该理论认为人类通过五种不同的道德基础来感知道德：关怀（Care）、公平（Fairness）、忠诚（Loyalty）、权威（Authority）和纯洁（Purity）。每个人对这些基础的优先顺序不同，从而导致对同一概念或行为的道德判断存在差异。

研究的目标是了解语言模型如何理解道德在不同领域中的表达方式。他们使用了包含35,000条推文的道德基础推特语料库，这些推文来自“所有生命都重要（All Lives Matter，ALM）”和“黑人生命也重要（Black Lives Matter，BLM）”等七个不同的话题标签。

研究发现，语言模型确实能够识别道德在不同领域中的不同表达方式。例如，在“所有生命都重要”的语境中，“颠覆”（subversion）常与“推翻”（overthrow）和“混乱”（mayhem）等词语相关联，表明颠覆被认为是不可取的。而在“黑人生命也重要”的语境中，颠覆则与“鼓励”（encourage）和“反抗”（defiance）相关联，表明颠覆在一定程度上是被鼓励的。

最终，该研究强调，仅仅使用一个单一的模型来评估不同领域的道德可能会导致误解，并指出道德在不同领域中以不同方式表达，这一点是需要被语言模型所理解和识别的。该研究的发现提醒我们，在开发人工智能模型时，需要考虑到道德判断的复杂性和情境性，以避免潜在的危险误解。</sample>
    <sample id="323">这篇论文介绍了一种名为DHKL（动态异构图推理与语言模型和知识表示学习）的知识表示方法，旨在解决常识问答任务中存在的两个挑战：如何处理知识图谱中的噪声实体以及如何增强语言模型与知识图谱之间的交互作用。

DHKL方法分为三个主要部分：

- **异构知识图谱（HKG）的构建**：通过对多源知识库（ConceptNet、WordNet和Wiktionary）中的实体进行两阶段剪枝来构建HKG。第一阶段剪枝利用字典词汇表移除组成短语实体的子词，第二阶段剪枝则基于语言模型的注意力权重，动态移除与问答上下文关联较弱的实体，以提高图的质量。
- **知识表示学习（KRL）模块**：DHKL使用TransE模型优化HKG中实体和关系的嵌入表示，以捕捉实体之间的语义联系。为了解决异构图的结构问题，DHKL通过均值池化（mean pooling）的方法，从构成HKG中实体的词汇单元中，获取初始的实体和关系嵌入，再通过多层Transformer结构，将实体和关系嵌入更新。
- **关联掩码自注意力（RMSA）层**：DHKL模型在语言模型（RoBERTa）中引入了一种关系感知掩码自注意力机制（RMSA），以更好地建模实体、关系和上下文之间的交互。与传统的GNN不同，RMSA将关系信息直接引入自注意力机制，以增强知识图谱与文本之间的融合。

实验结果表明，DHKL在常识问答（CommonsenseQA）和开放书本问答（OpenBookQA）数据集上均取得了显著优于现有基线方法的性能，展现了其在处理常识问答任务上的有效性。</sample>
    <sample id="324">是的，语言模型具有不同的政治倾向。 它们在政治指南针测试中占据所有四个象限。</sample>
    <sample id="325">嗨，我叫Matthias Lindemann，今天我将简要介绍我们关于在不使用树的情况下，通过多集标记和潜在排列进行组合泛化的论文。这是我与导师Alexander Koller和Ivan Titov的合作。

组合泛化可以理解为学习者处理更深层次的递归以及在训练期间已单独见过的短语的未见组合的能力。在语义解析的背景下，测试组合泛化可能看起来像这样。通常，我们有一个语篇训练集，在本例中是“The girl slept”和“Mary knew that the girl slept”。这些语篇与表示其核心含义的逻辑形式配对。与标准机器学习评估相比，测试集并非来自同一分布，而是包含结构上未见的逻辑形式。在这个例子中，模型在训练期间看到了浅层递归，并用更深层递归的例子进行了测试。

朴素的序列到序列模型在这种分布外泛化方面表现不佳，通常会产生与输入分离的输出。特别是，它们往往无法复制输入和输出之间的系统对应关系，例如示例中以颜色编码的那些。

解决此问题的一个流行方法是将树集成到模型中。树旨在捕获将语篇与逻辑形式关联起来的组合过程。这很有效，但树通常不是给定的，需要以某种方式获得。这可能变得复杂，有时是一个计算成本高的过程。通常，这涉及大量特定于形式的逻辑形式预处理，例如处理变量符号。获取树还可能涉及专门的语法归纳过程。

在本文中，我们不使用树，并引入了一种神经序列到序列模型，该模型直接模拟片段之间的对应关系。这是我们第一次展示在不使用树的情况下对更深层递归进行强泛化。

我们的方法分两步预测输出。首先，我们用将出现在输出中的标记的无序多集来标记每个输入标记。在第一步之后，我们拥有所有正确的标记，但它们是无序的。这就是为什么在第二步中，我们使用另一个模型来预测一个排列，以将它们按正确的顺序排列。我们引入了一种新的预测排列的方法，该方法不对可能的排列施加任何硬约束。这使得我们的方法非常灵活和富有表现力。

从概念上讲，我们的排列模型大致是这样工作的。我们从左到右遍历输出，并确定在每个位置放置哪个多集标记。对于第一个输出位置，我们简单地选择一个，如红色突出显示所示。然后，我们跳到下一个多集标记，以确定输出中的第二个标记。我们以类似的方式确定输出中的第三个标记，通过跳到另一个多集标记。我们继续这个过程，直到第一阶段的每个标记都被精确地访问了一次。

为了让您了解实验结果，在这里，我们将我们的方法与COGS基准测试上的其他无树模型进行了比较。我们的模型在泛化到更深层递归方面，以很大的优势超越了其他模型。然而，其他一些类型的结构泛化仍然非常具有挑战性。

在我们的论文中，我们解决了一些有趣的技术挑战。首先，输入和输出之间的对齐未在训练数据中给出。因此，对于给定的标记，我们不知道它来自哪个多集，这给训练带来了挑战。我们通过在训练中将其作为训练的一部分来解决这个问题。我们的排列方法非常灵活，但它带来了寻找最高得分排列是NP难题的挑战。这是因为这与旅行推销员问题有关。我们用一种GPU友好的连续松弛来近似它，该松弛也允许我们通过解决方案进行反向传播，并学习语言学上更合理的排列。如果您想了解更多关于我们的实验以及我们如何应对这些挑战，请查看我们的论文或访问我们的海报。</sample>
    <sample id="326">认知失调是指两种认知要素（即思想、行为、信念）不一致的情况。</sample>
    <sample id="327">演讲者首先介绍了视觉语言学习的目标，即训练能理解图像和文本的AI系统。然后，他重点介绍了近年来该领域通过大规模自监督预训练在图像-文本对上取得的显著进展。

在架构方面，他解释了Two-Tower架构，它使用独立的文本编码器和视觉编码器，然后通过一个跨模态编码器进行连接。接着，他介绍了BridgeTower架构，该架构通过逐层连接多个单模态层与跨模态层，以利用不同层次的单模态语义知识。

然而，BridgeTower存在两个局限性：一是逐层利用效率低下，因为每个跨模态层只能利用一个指定层次的单模态表示，限制了对不同层次单模态语义知识的利用；二是跨模态层的数量与所使用的单模态层表示数量绑定，限制了其可扩展性和能力。

为了解决这些问题，演讲者提出了ManagerTower架构。该架构在每个跨模态层中引入了“管理者”，这些管理者可以接收并组合预训练的单模态专家在不同层次提供的多层单模态表示作为洞察。

实验结果表明，仅使用400万张图像进行预训练，ManagerTower在多项下游任务上均取得了卓越性能，尤其是在VQAv2测试集上达到了79.15%的准确率。值得注意的是，ManagerTower不仅超越了在相同数据量下训练的各种基础模型，甚至超过了某些在更多数据或参数下训练的模型，这凸显了其有效性。

通过对聚合权重的可视化分析，演讲者发现，在静态管理者中，每个跨模态层都显示出类似的渐进式权重分布，这与直觉不符，因为不同跨模态层对单模态语义知识的需求应该有所不同。而自适应管理者则在不同跨模态层中表现出多样化的聚合权重分布，这有力地证明了自适应管理者能够更有效地利用不同层次的单模态语义知识，从而实现更全面的跨模态表示学习。

总之，ManagerTower通过其创新的管理者机制，克服了现有架构的局限性，实现了更有效的跨模态信息聚合，并在视觉语言学习任务中展现出优越的性能。</sample>
    <sample id="328">GPT-4 是所有语言模型中最倾向于自由派的。</sample>
    <sample id="329">这段演示介绍了一项关于零样本视频句子定位的工作，该工作旨在于无需人工标注的情况下，根据自然语言查询在未见视频中找到最相关的视频片段。

现有方法在生成伪查询时过于简单，导致伪事件与伪查询之间存在不对齐，且未考虑伪标签中的噪声。为解决这些问题，本研究提出了一种零样本视频句子定位方法，它生成结构化伪标签，且对噪声具有鲁棒性。

该方法分三步进行：
1. **伪查询生成：** 密集采样视频帧，并使用预训练的BLIP模型从视频帧中生成自由形式的伪查询。与视频标题模型相比，图像文本预训练模型具有更好的零样本泛化能力。
2. **伪事件生成：** 根据伪查询和视频帧之间的相似度，结合事件的时间结构，生成伪事件。它确保事件内部的视频与查询高度相关，而事件外部的视频与查询相关性较低。接着，通过过滤低质量和高度重叠的伪查询-事件对，生成最终的伪标签。
3. **含噪声伪标签训练：** 在训练过程中，通过样本重加权和标签细化来减少噪声的影响。样本重加权根据预测置信度分数和预测与伪标签之间的IoU来估计噪声，并据此调整样本损失权重。标签细化则在模型预测置信度高且与伪标签的IoU高时，将预测视为新的伪标签，用于下一轮训练。

实验结果表明，该方法在Charades-STA和ActivityNet Captions两个数据集上均优于现有的零样本方法，在多数指标上取得了最佳性能。</sample>
    <sample id="330">是的，累积训练比迭代训练更有效。</sample>
    <sample id="331">演讲者的名字是 Sara Papi。</sample>
    <sample id="332">MuDa 基准中的数据是从 TED 演讲中获得的。</sample>
    <sample id="333">本演讲介绍了一种名为 INK 的新型训练框架，旨在通过注入 K 最近邻 (kNN) 知识来改进神经机器翻译 (NMT) 模型。

NMT 模型在近年取得了显著进展，其目标是学习一个泛化的表示空间以适应不同场景。然而，神经网络常常会产生一个非平滑的表示空间，限制了其泛化能力。具体而言，在 NMT 模型的表示空间中，低频词语分散稀疏，导致形成许多“语义空洞”，这些区域的语义含义可能定义不清。因此，当 NMT 模型用于翻译来自未见领域的例子时，性能会急剧下降。

为了解决这一问题，kNN-MT 被提出作为一种解决方案。其核心思想是根据表示空间中的最近邻来平滑预测。为了实现这一点，需要一个预训练过程来构建一个键值数据存储，用于保存表示及其对应的目标词语。在每个解码步骤中，NMT 模型会查询数据存储以检索最近的条目，并根据检索结果优化预测概率。尽管有效，但 kNN-MT 存在两个主要缺点：
1. 在每个解码步骤中从大型数据存储中检索邻居非常耗时。
2. 一旦数据存储构建完成，表示就难以轻易更新。

为了克服这些缺点，我们提出了 INK 框架，将 kNN 知识注入到 NMT 中。INK 的训练循环包含两个步骤：
1. 表示细化：从数据存储中提取 kNN 知识，以调整适配器（一个小型模型）的表示。
2. 异步刷新：使用更新后的表示异步刷新数据存储。
这个训练循环会一直运行直到收敛。

具体来说，我们通过以下方式调整表示：
1. 对齐上下文表示和词语嵌入，以保持语义含义。
2. 对齐上下文表示和 kNN 词语嵌入，以丰富语义含义。
3. 对齐相同目标词语的上下文表示，以解决稀疏分散问题。

在推理阶段，我们只需加载现成的 NMT 模型和经过微调的适配器参数，即可获得更好的性能，而无需数据存储。

在实验中，我们选择了 WMT'19 德语-英语新闻翻译任务的获胜模型作为现成的 NMT 模型。我们在医疗、法律、IT 和古兰经四个基准数据集上进行了实验。我们探究了以下研究问题：
1. 能否通过小型适配器平滑表示空间，并在推理时移除数据存储？
2. 使用 kNN 知识调整表示分布能带来多大的改进？
3. 同时使用适配器和数据存储能否带来进一步的改进？

实验结果表明：
- INK 系统通过平滑表示空间，取得了比现有 kNN-MT 系统更好的性能，平均 COMET 评分提高了 1.99，BLEU 评分提高了 1.0。
- 与仅使用适配器的基线相比，根据 kNN 知识进行表示细化带来了更大的性能提升。
- INK 系统在更少的内存空间（0.02 倍）和更快的推理速度（1.9 倍）下实现了更好的翻译性能。
- 同时应用适配器和数据存储可以进一步平滑预测，这表明 NMT 模型的表示空间并未完全通过适配器得到细化。如果能设计出更有效的框架，平滑表示空间的好处将进一步显现。

总而言之，我们提出了一个新颖的训练框架 INK，通过迭代地根据 kNN 知识细化 NMT 模型的表示空间，在内存和速度上都优于传统的 kNN-MT 方法。</sample>
    <sample id="335">演讲者的名字是Matthias Lindemann。</sample>
    <sample id="336">跨语言转移是指在一种源语言上进行训练，然后将其转移到另一种目标语言上。</sample>
    <sample id="337">这段视频介绍了一种基于图的关系挖掘方法（GRM），用于上下文无关的词嵌入学习，以解决自然语言处理中未登录词（OOV）表示的难题。

研究人员受人类学习词汇习惯的启发，提出了一种新的神经网络方法，利用词的构词法和联想关系来推断OOV词的含义。他们构建了一个词关系图（WRG），该图模拟了构词法和联想的词汇规则。当遇到OOV词时，该词被分解为词素，并与图中的其他相关词语进行关联，形成一个两层图结构。

在GRM模型中，每个词或词素被视为图中的一个节点，其对应的词嵌入作为节点特征。通过多层图注意力网络（GAT）对图进行处理，并通过自注意力网络初始化OOV节点的特征，以捕捉最重要的信息并减少噪声。最后，研究人员通过对比学习将推断出的图级嵌入与背景词嵌入对齐，以鼓励相似词之间的接近性，同时推开不相关的词。

实验结果表明，GRM在词相似度和词类比等内在评估任务以及命名实体识别和词性标注等外在评估任务中均优于基线模型，证明了其在学习OOV词构词法方面的有效性。此外，GRM模型对于黏着语（如日语或韩语）和融合语（如英语）具有良好的适应性，这表明其应用效力取决于词语分解的合理性。

总的来说，GRM提供了一种新颖有效的方法，通过利用词的构词法和联想关系，克服了OOV词在词嵌入学习中的挑战，并有望应用于多种语言。</sample>
    <sample id="338">该研究探讨了人类自然语言解释（HNE）在训练自然语言生成（NLG）模型、提高预测性能和增强模型推理方面的作用。研究人员强调了评估人类注释解释质量的重要性，因为这些解释可能具有主观性和任务依赖性。

研究人员使用了一个统一的基于模板的数据结构，将各种任务转换为统一的多项选择任务，从而最大限度地减少了不同任务和模型的影响。他们进行了初步实验，分析了HNE在ECQA和CoS-E数据集上微调和推理阶段的实用性，这些实验结果表明：
- 微调过程并不能为模型带来新的知识，而是让模型学会了在预测时依赖解释。
- CoS-E解释在基础模型上的帮助不如ECQA。
- 即使使用少量包含解释的数据进行微调，也能显著提高模型性能。

在这些观察的基础上，研究人员提出了一种新的评估指标，称为TREU，它是模拟度分数的扩展。TREU通过在微调和推理阶段比较有无解释的模型性能差异，更忠实地评估了HNE的有用性。对五个数据集（CoS-E v1.0, CoS-E v1.11, ECQA, e-SNLI, ConVE）和两个模型（T5和BART）进行的评估显示，TREU在评估HNE质量方面优于模拟度分数。TREU能够一致地衡量不同数据集和模型中的相对有用性，而模拟度分数则表现出局限性。

研究人员观察到，在e-SNLI数据集中，中性（neutral）和矛盾（contradiction）类别的解释并没有帮助，这表明HNE的有用性高度依赖于任务和解释风格（例如，e-SNLI和ConVE中的否定内涵，以及e-SNLI中矛盾类别的反事实写作风格）。这项工作为未来人类-AI协作中的高质量数据注释工作奠定了基础，并建议研究人员在未来收集HNE时进行类似的质量检查，因为高质量的人类注释价格昂贵且难以获取。</sample>
    <sample id="339">这篇论文的作者分别来自德国萨尔大学、亚马逊 Alexa 和维也纳大学。</sample>
    <sample id="340">这项工作介绍了一个名为 ParaAMR 的大型、句法多样化的释义数据集，该数据集通过抽象意义表示（AMR）回译构建而成。

释义生成对于问答系统、聊天机器人、创意生成、数据增强和提高鲁棒性等自然语言处理（NLP）应用至关重要。虽然人工标注的数据集（如 MRPC、PAN、Quora）质量高，但规模有限。相比之下，自动生成的数据集（如通过回译生成的数据集）虽然规模大，但在句法多样性方面往往不足。

为了解决这个问题，ParaAMR 利用了 AMR 图，这是一种通过节点表示语义概念、边缘表示语义关系来捕获句子抽象含义的有向图。该方法的关键思想是使用 AMR 回译来生成具有句法多样性的释义：
1. **文本到 AMR 图：** 使用预训练的 AMR 分析器从源句中获取 AMR 图。
2. **更改焦点（根节点）：** 随机选择一个新节点作为图的根节点，并相应地修改边缘和边缘标签。这会改变句子的句法结构，同时保留其语义。
3. **AMR 图到文本：** 使用 AMR 图到文本生成器从修改后的图中生成文本。

通过这种方法，ParaAMR 包含了大约 1550 万个源句，平均每个句子有 6.92 个释义。与现有数据集相比，ParaAMR 在保留良好语义相似性的同时，句法多样性更高。

ParaAMR 在多种 NLP 应用中表现出了优势：
- **学习句子嵌入：** 使用 ParaAMR 学习的句子嵌入在语义文本相似度（STS）基准测试中表现优于其他数据集。
- **句法控制的释义生成：** 使用 ParaAMR 训练的释义生成器能够实现更好的句法控制。
- **小样本学习的数据增强：** ParaAMR 在 MRPC、QQP 和 RTE 等任务的小样本学习数据增强中表现出更高的分数。

ParaAMR 数据集可在 GitHub (https://github.com/uclanlp/ParaAMR) 上获取。</sample>
    <sample id="341">作者使用了平均滞后时间（AL）和计算感知平均滞后时间（AL_CA）两种延迟测量方法。</sample>
    <sample id="342">演讲者介绍了LiveChat，一个从直播中自动构建的大规模个性化对话数据集。目前大规模对话数据集主要基于文本，而LiveChat则是一个视频源数据集，更接近真实的口语对话。现有的视频源数据集通常规模有限，因为它们依赖手动提取和标注。

LiveChat通过以下步骤构建：
1. 从抖音获取直播视频，提取音频，并通过自动语音识别（ASR）将其转录为语篇。
2. 收集观众评论，并通过匹配流媒体回应和观众评论，利用一种回复匹配方法构建对话。
3. 收集历史帖子、对话和公开信息，并由标注者根据视频标注流媒体的个性特征，从而收集个性化信息并添加手动标注。

LiveChat数据集在视频源、大规模、详细个性化标注和更长平均会话方面优于现有数据集。实验结果显示，LiveChat中的个性化信息和更长的会话有利于学习说话者的个性化回应和受话者识别。BERT在受话者识别方面表现优于TwinBERT和CoBERT。

在生成模型的迁移学习方面，BART在LiveChat上的表现优于其他预训练模型，这表明LiveChat数据集的领域与现有对话数据集的领域差异显著。此外，大语言模型（LLMs）的上下文学习结果显示，随着示范数量的增加，性能逐渐提升，但在示范数量超过8次后，由于随机选择的示范可能引入噪声，性能略有下降。

总的来说，LiveChat是一个独特且有价值的视频源个性化对话数据集，有助于克服现有对话数据集的局限性，并为个性化对话生成和受话者识别等任务提供新的研究方向。未来，研究将侧重于大语言模型在LiveChat上高效的迁移学习。</sample>
    <sample id="343">大家好，我是Akshatha，今天我的合著者Martin和我将介绍我们的工作，KITMUS测试。从多个来源评估知识整合。这项工作是麦吉尔大学、Mila和微软研究院之间的合作。
自然语言理解模型借鉴了多种知识来源，例如其参数中包含的知识，通常通过预训练获得，以及在推理时作为输入提供的知识。问答等任务的近期研究表明，模型可以利用预训练知识来解决任务。
但自然语言理解通常需要推理时也提供的知识。例如，在句子“John saw the newly elected president on TV”中，预训练参数可以包含有关总统做什么以及电视是什么的信息，但它们无法可靠地知道实例特定实体John是谁，或者新总统是谁，因为自预训练以来总统可能已经更换。
因此，针对知识密集型自然语言理解任务的成功模型需要能够整合和使用预训练和推理时的知识。
在这项工作中，我们提出了一个用于知识整合评估的诊断测试套件。我们引入了一个共指消解任务，旨在探究模型提取预训练知识和推理时知识的能力。我们使用人类研究参与者和现有的共指消解模型评估了该数据集。
这是我们数据集中的一个示例。Servin是法官。Kea是面包师。Servin和Kea在公园相遇。在法庭审理案件的漫长一天工作后，他很高兴能放松一下。回答：Servin。该任务是识别代词“he”所指的正确实体，在本例中是Servin。
给定代词的消解需要两种类型的信息。首先，实体特定知识，例如Servin是法官。
其次，背景知识，例如法官在法庭上审理案件。
通常，背景知识是在大型语言模型的预训练期间学习的，而实体特定知识通常是在推理时观察到的。我们改变这两种信息的可获得性，使其既可以在单个来源中找到，也可以在多个来源中找到。
我们定义了三种KITMUS设置。首先，是典型设置，背景预训练。背景知识被假定在预训练时可用。其次，是背景两者设置。背景知识在预训练时和推理时都可用。最后，是背景推理设置。两种知识类型仅在推理时可用。最后这个设置特别有趣。因为它模拟了解决任务所需的背景知识不是模型预训练数据的一部分的情况。例如，因为自预训练以来已经出现了新的职业。
这是我们如何控制两种来源中事实可用性的示例。在背景预训练设置中，我们假设背景知识“政治家在政府中寻求民选席位”包含在预训练参数中。在推理时上下文中，我们提供实体特定知识“Chichester是政治家”。在背景两者设置中，我们不仅提供实体特定知识，还提供推理时上下文中政治家的背景知识。在背景推理设置中，我们提供虚构的职业“mirituer”而不是政治家，因为“mirituer”不太可能包含在预训练参数中。
我们使用人类研究参与者和现有的共指消解模型评估了数据集。在此图中，我们展示了表现最佳的模型在背景预训练设置中最难变体上的结果。在没有进行特定任务训练的情况下，KITMUS上的模型表现不佳。然而，在KITMUS上训练后，C2F和BERT4Coref都比随机选择表现更好。这表明，当在通用共指消解数据集上训练时，模型会学习利用表面线索。当在KITMUS上进行测试时，这些线索已被移除，因此它们无用。
对虚构知识的额外实验表明，即使是表现最佳的模型也无法可靠地整合仅在推理时提供的背景知识。
总结一下我们论文的主要观点。1. 许多模型似乎无法推理来自多个来源的知识（预训练时和推理时知识）。2. 特定任务训练对于知识整合是必要的。3. 模型难以整合推理时背景知识。如果您对更多细节感兴趣，请查阅我们的论文并在GitHub上查看数据集、生成和评估代码。感谢收听。</sample>
    <sample id="344">基于树的方法需要预处理逻辑形式，以处理变量符号。此外，获取树可能需要专门的语法归纳程序。</sample>
    <sample id="345">该视频介绍了Matthias Lindemann等人关于“不使用树的组合泛化：多集标记和潜在排列”的研究。

组合泛化指的是学习者处理更深层次的递归和未见的短语组合的能力，这些短语在训练期间是单独见过的。在语义分析中，这意味着模型可以在训练中只看到浅层递归的情况下，处理更深层次递归的测试例子。传统的序列到序列模型在这种场景下表现不佳，因为它们无法捕捉到输入和输出之间的系统对应关系。

解决这个问题的一个流行方法是使用树来捕捉从语句到逻辑形式的组合过程。然而，这种方法需要额外的树获取步骤（例如，逻辑形式的预处理/后处理或语法归纳），这可能非常复杂和耗时。

本文提出了一种新的神经序列到序列模型，它不依赖树结构。该模型通过两个步骤直接对片段之间的对应关系进行建模：
1. **标记**：每个输入词被标记为一个无序的多集，包含将出现在输出中的词元。
2. **排列**：使用另一个模型来预测排列，将这些无序的多集词元按照正确的顺序排列。这种排列方法允许“跳跃”，即输出中的一个词元可以来自输入中任意位置的多集。

这种方法具有灵活性和表达性，不需要对可能的排列施加硬性约束。研究人员通过在COGS基准测试上的实验证明，与现有的无树模型相比，他们的方法在泛化到更深层次的递归方面取得了显著的改进。

该研究还解决了两个重要的技术挑战：
1. **对齐未知**：输入和输出之间的对齐在训练数据中是未知的。研究人员通过在训练过程中归纳对齐来解决这个问题。
2. **排列模型**：找到最高分的排列是一个NP-hard问题（类似于旅行推销员问题）。他们通过使用GPU友好的连续松弛方法来近似这个问题，该方法允许通过解决方案进行反向传播，从而学习更合理的语言排列。

总而言之，这项工作展示了在不使用树的情况下，通过多集标记和潜在排列，实现组合泛化的强大能力，为语义分析的结构泛化提供了一条新途径。</sample>
    <sample id="346">这篇论文的作者所属机构是佐治亚理工学院。</sample>
    <sample id="347">00:00</sample>
    <sample id="348">演讲者Myra Cheng介绍了她的论文“Marked Personas：利用自然语言提示衡量语言模型中的刻板印象”。她指出，大型语言模型（LLM）中存在社会偏见和刻板印象，但现有衡量方法存在局限性：特异性和泛化性之间的权衡、基于固定的人工整理数据集以及未能考虑交叉性。

为了克服这些限制，Myra Cheng和她的团队利用了GPT-3.5和GPT-4等指令调优型LLM能够响应提示的特性。他们使用“想象你是一个亚洲女性。描述你自己”之类的提示来生成人物形象。这种方法具有高度的泛化性，可以评估任何交叉身份。

在对GPT-4生成的人物形象进行分析后，他们发现了一些有趣的模式。例如，亚洲女性被描绘为“不张扬”，中东女性被描述为“异国情调”，并且两类有色人种女性的人物形象都提到了“祖先”，而白人男性的人物形象则没有。

为了量化这些模式，他们提出了一个两步法：
1. **人物形象生成：** 使用与心理学研究中人类受试者相同的提示来生成人物形象。这使得可以比较模型生成的人物形象与人类书写的回答。
2. **标记词：** 找出区分标记群体人物形象和未标记群体人物形象的词语。这种方法能够在不需要词典的情况下发现具体的刻板印象。

他们的研究结果显示，模型生成的人物形象比人类书写的人物形象包含更多的刻板印象。此外，这些刻板印象大多是积极的或至少不是负面的，例如黑人女性多被描述为“高”和“运动”，但这种词典并不完整，无法捕捉到所有有害模式。

通过对顶级词语的模式进行分析，他们发现了一些有害模式：
* **通过本质化叙事进行他者化：** 标记群体的人物形象中包含“文化”、“传统”、“自豪”、“异国情调”等词语，这些词语仅仅通过其身份来定义这些群体，并将其与白人规范区分开来。
* **有害的积极描写：** 拉丁裔女性被描述为“充满活力”、“曲线优美”，亚洲女性被描述为“娇小”、“精致”、“丝滑”，黑人女性被描述为“强壮”、“坚韧”。虽然这些词语听起来积极，但它们反映了有害的刻板印象，例如对亚洲女性的过度性化以及对黑人女性施加韧性压力。

最后，他们提出了三项建议：
1. **解决积极刻板印象和本质化叙事。**
2. **采用交叉性视角研究偏见。**
3. **提高偏见缓解方法的透明度。**</sample>
    <sample id="349">00:00
大家好，我叫景唯唯，来自中国科学技术大学。

00:07
我很高兴为您简要介绍我们的论文“你抄袭我的模型了吗？通过后门水印保护嵌入即服务中大型语言模型的版权”。

00:20
首先，让我们介绍一下关于嵌入即服务的背景。

00:25
目前，像 GPT、LLAMA、PALM 这样的大型语言模型在自然语言理解和生成方面表现出色。嵌入即服务是基于大型语言模型构建的服务之一，旨在协助各种 NLP 任务。

00:41
例如，OpenAI 提供了一个基于 GPT3 的嵌入 API。

00:46
然而，最近的研究表明，攻击者可能会通过从嵌入中学习来窃取模型并提供类似的服务。因此，有必要保护嵌入即服务的版权。

01:03
为了保护嵌入即服务的版权，其中一种解决方案是在提供者的服务中嵌入水印，并检测另一个服务是否包含该水印。水印方法需要满足以下特性：

01:21
首先，该方法应适用于嵌入即服务。其次，水印不应降低所提供嵌入的实用性。第三，水印应对攻击者保密。否则，攻击者可以轻易删除水印。最后，水印需要在模型提取过程中可转移到攻击者的服务中。

01:48
现有工作大致可分为四类。然而，这些方法要么不适用于嵌入即服务，要么缺乏可转移性。

02:04
因此，在本文中，我们提出了 EmbMarker，它是一种适用于嵌入即服务的基于后门的水印方法。

02:10
接下来，让我详细介绍一下我们的 EmbMarker。EmbMarker 包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一组处于中等频率区间的单词。我们假设提供者可以收集一个通用文本语料库，并用它来统计词频。

02:39
在水印注入中，我们首先定义一个目标嵌入。当用户向提供者服务发送一个句子时，提供者会计算句子中的触发词数量。所提供的嵌入是目标嵌入和原始嵌入的加权和。目标嵌入的权重与句子中触发词的数量成正比。当句子中触发词的数量大于 m 时，所提供的嵌入就完全等于目标嵌入。

03:13
版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门数据集和一个良性数据集。后门数据集包含所有词都属于触发集中的句子。而良性数据集中所有词都不属于触发集。

03:38
然后，提供者使用这些数据集请求窃取者服务的嵌入。

03:44
计算请求的嵌入和目标嵌入之间的余弦相似度和 L2 相似度。我们计算良性数据集和后门数据集之间的相似度差异。

04:02
这被定义为 Delta Cos 和 Delta L2。同时，我们还应用了 KS 检验，并将其 p 值作为第三个指标。

04:06
我们对四个数据集进行了实验：AG News、MIND、SST2 和 Enron Spam。我们假设提供者使用 WikiText 数据集来统计词频。

04:20
四个数据集的结果表明，我们的 EmbMarker 在保持下游任务实用性的同时，具有良好的检测性能。

04:32
我们还通过 PCA 可视化四个数据集中句子的嵌入来验证所提供嵌入的隐蔽性。图例表示每个句子中触发词的数量。

04:47
如图所示，很难区分后门嵌入和正常嵌入。

04:55
这就是全部，谢谢！欢迎与我们讨论。</sample>
    <sample id="350">在本次演讲中，Simonne Tedeschi探讨了当前自然语言理解（NLU）领域中，模型超越人类表现的“超人类”说法是否可信。过去五年，排行榜评估已成为NLP的常用方法，许多系统在各项基准测试中达到了人类甚至超人类水平，例如SuperGLUE和SQuAD。这导致了任务已被解决的说法。然而，Simonne Tedeschi指出，这种比较存在多方面误导。

首先，模型与人类经常在不同的数据集上进行评估。例如，在SuperGLUE的七项任务中，人类仅在实际测试集的一小部分（3%-40%）上进行评估。例如，BoolQ任务中，系统在超过3000个实例的完整测试集上评估，而人类仅在100个实例上评估。

其次，数据标注质量存在问题。在一些任务中，人工标注的“黄金标准”答案本身就存在错误。例如，在识别文本蕴含（RTE）任务中，给定“大多数太平洋国家议会中的女性很少”这一前提，假设“女性在议会中代表不足”被标注为“TRUE”，这是不合理的，因为特定前提无法推导出普遍结论。

第三，人类评估指标存在缺陷。NLP研究人员通常模糊或随意地估计人类表现。“人类基线”这一术语本身就暗示系统需要超越它。此外，许多基准测试使用多数投票或平均值来计算最终人类得分，而不是评估最佳人类的表现，这与国际象棋或Jeopardy等其他AI领域不同，这些领域会将AI与顶尖人类选手的表现进行比较。

最后，标注者报酬不均且信息不透明。报酬水平差异很大，从每小时3.6美元到23.75美元不等，有些甚至未公开。低报酬可能导致标注质量下降。此外，标注者池的详细信息（如人数、文化背景、专业领域、报酬率以及是否提供培训和指导）通常被省略。 Simmone Tedeschi认为，在缺乏这些信息的情况下，声称超人类表现缺乏科学依据。

综上所述，Simonne Tedeschi强调，当前NLP中关于模型“超人类”性能的说法尚未完全站稳脚跟。为了构建更公平、更透明的基准测试，未来的评估需要解决数据量、标注质量、评估指标和标注者信息透明度等问题。</sample>
    <sample id="351">这段演讲的主题是“CoNLL-2003命名实体识别器在2023年是否仍然表现良好？”

演讲者首先提出了两个问题：这些模型能否推广到现代数据？以及进行良好推广所需的条件是什么？

为了探究这些问题，演讲者介绍了CoNLL++数据集，该数据集包含从2020年路透社新闻中收集的数据，并使用CoNLL-2003标注指南进行标注。他们在这个数据集上对20多个模型进行了微调，并评估了它们在CoNLL-2003测试集和CoNLL++测试集上的表现。

在接下来的部分中，演讲者讨论了良好推广所需的条件：
1. **模型架构**：Transformer模型通常能更好地推广。
2. **模型大小**：更大的模型通常能更好地推广。
3. **微调示例数量**：更多的微调示例能带来更好的推广。

然后，演讲者探讨了导致模型性能下降的原因：
1. **适应性过拟合**：指的是通过反复使用相同的测试集进行过拟合。演讲者通过图表展示，CoNLL-2003上的每一次改进都能在CoNLL++上带来超过一次的改进，这意味着没有出现递减回报，因此排除了适应性过拟合的可能性。
2. **时间漂移**：指的是训练数据和测试数据之间时间间隔的增加导致的性能下降。他们通过实验发现，模型性能确实随着时间间隔的增大而下降，这证实了时间漂移是导致性能下降的主要原因。

最后，演讲者总结道：
- 为了实现良好的推广，我们需要更好的模型架构、更大的模型尺寸以及更多的微调示例。
- 性能下降的原因是时间漂移，而不是适应性过拟合。
- CoNLL-2003标注器在2023年仍然表现良好。

演讲者希望他们的研究能促使人们在如何改进模型的推广性方面进行更多的研究。他们还提供了论文、数据集和联系方式，供感兴趣的听众查阅。</sample>
    <sample id="352">ABC-Eval 代表“Annotating Behaviors in Chat”。</sample>
    <sample id="353">本篇论文探讨了通过提问澄清问题来生成Python代码。代码生成和程序合成（给定自然语言描述）是一个热门的研究领域。然而，现有方法未能解决一个重要的挑战：输入描述不充分。

为了解决这个问题，该论文提出以下方案：
- **引入交互性**：通过交互（以澄清问题和答案的形式），可以收集更多规范，从而缓解描述不充分的问题。
- **提出任务**：生成代码的任务是通过提问澄清问题来完成的。重点是澄清操作级别规范。
- **创建数据集**：提出了一种创建CodeClarQA的方法，这是一个带有关键操作澄清的合成数据集。
- **提出流水线**：提出了一种通过提问澄清问题进行代码生成的流水线，其中包括：
    1. 澄清需求预测器：一个二元分类器。
    2. 问题选择器：一个检索模型。
    3. 代码生成器：一个因果语言模型（CausalLM）或序列到序列（seq2seq）生成器。

在数据集创建方面：
- **识别关键操作**：使用GraphGen4Code API和启发式方法识别关键操作以及代码中对应的文档。
- **潜在空间表示**：在潜在空间中使用本体论表示它们，计算自然语言描述和操作文档之间所有本体论元素对的相似度分数。
- **判断缺失/对齐**：如果所有元素对的相似度分数低于阈值t，则关键操作缺失，否则为对齐。
- **人工标注**：雇佣标注员标注验证集（用于选择t）和测试集（用于评估方法）。
- **模板生成澄清问题**：采用模板为缺失的关键操作创建澄清问题，包括“是/否”或“多选题”两种类型。

在实验结果和分析方面：
- **识别缺失关键操作结果**：实验结果表明，该方法在识别缺失关键操作方面表现良好，MPNet在所有模型中性能最佳。
- **错误分析**：对常见的错误进行分析，发现FP预测较少，表明生成澄清问题有效。常见的错误反映了挑战和改进方向，如分类法（对齐的操作可能需要澄清以区分具有相似名称的操作）和参数（在参数值缺失时使用操作文档）。
- **模块实验结果**：两个假设得到了验证：任务比现有澄清问题排名任务更具挑战性（由澄清问题排名结果支持），澄清有助于代码生成（由代码生成结果支持）。
- **流水线结果**：模型性能随着包含和“回答”更多高排名澄清问题而显著提高。然而，对于“未回答”的澄清问题，性能下降。当前流水线在仅使用自然语言描述和代码训练的模型中表现不佳，这可能是因为在所有澄清问题和澄清问题排名结果上进行微调是具有挑战性的。
- **澄清的关键操作是否是更好代码的原因**：答案是肯定的，因为直观上，提问澄清问题提供了更多规范，从而使模型生成与期望和更高质量的输出对齐。</sample>
    <sample id="354">根据视频中显示的图表，CoNLL-2003 和 CoNLL++ 之间的性能增量在 2018 年才高于 5 个百分点。</sample>
    <sample id="355">Hello, my name is Vasudha, and I'm a computer science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge. We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent, such as this example, where a person states, "I know that cigarettes could kill me," and then goes on to say, "I grabbed a couple of smokes after the meeting today." This belief and action are inconsistent, and they are in dissonance. Further, mentioning that, "I don't think I could keep my job without them," justifies the second occurrence, and they have a consonance relationship. While dissonance is a very common phenomenon we experience in daily decision-making, they are really rare to find expressed in language among other kinds of discourse relations. So, why does this matter? Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends in belief values and attitude changes in population, high cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large-scale annotation of dissonance relations. We used a dissonance-first approach as seen in the flowchart here. Tweets were parsed using a PDTB parser, and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior search dataset, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation rounds, lowering the overall annotation cost while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the cold-start active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic-independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic, called debate here, and on binary classification of expansion and comparison classes of PDTB, since these two are closely related to the conception of consonance and dissonance, and we call them CE here. We find that on transferring, the zero-shot performance on the annotated dataset is already much better than chance, with the best with AUC 0.62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE tasks, followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to cold-start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. Cumulative accumulates all the data collected from active annotation so far, whereas iterative updates the model by training on the latest set of data collected. Over the different strategies, we found that cumulative performed equal or better than iterative across the board. Next, to improve the number of dissonant examples, we use a probability of rare class strategy, PRC, to select mostly the examples that are highly likely to be dissonant by the current model at any round of AL. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improved dissonance classification AUC to 0.75, which is the best performance that we have on the task so far. We also checked the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition, and cold-starting AL with appropriately designed transfer learning tasks can help significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in-domain active annotations benefit from cumulative update. These are the links to our code, dataset, and our paper. Feel free to get in touch with us if you have any questions. Thank you.</sample>
    <sample id="356">这篇论文的作者所属机构是爱丁堡大学、萨尔兰大学和阿姆斯特丹大学。</sample>
    <sample id="357">演讲者的名字是Siyu Yuan。</sample>
    <sample id="358">这篇论文有5位作者。</sample>
    <sample id="359">EDAtt 方法与专门为 SimuIST 设计的 CAAT 架构进行了比较。</sample>
    <sample id="361">本演示文稿由 Armineh Nourbakhsh 介绍，内容是关于 CounterComp，一种利用反事实对比度改进多步定量推理中的成分泛化模型。

首先，她介绍了多步定量推理的问答任务，目标是从财务表中提取信息，并通过一系列算术运算得出答案。然而，她指出最先进的神经模型在这种任务中表现不佳，特别是在涉及两个以上步骤的问题中。她把这归因于模型记住了虚假模式，而不是理解问题中令牌和输出程序中操作之间的真正关系。

为了解决这个问题，CounterComp 利用问题固有的反事实示例。她强调，输入问题中的某些组件（例如 "净变化" 与 "百分比变化"）可以互换，从而在输出中产生不同的算术运算。通过这种方式，问题可以被视为反事实示例。

CounterComp 的核心是采用度量学习损失，将训练样本视为锚点，并从训练集中挖掘出正反示例。正反示例是那些在问题中进行干预时，不改变输出程序的算术运算的例子。而负面例子是那些通过干预改变了输出程序中算术运算的例子。度量学习损失通过动态裕度进行调整，以衡量每个配对之间干预或变化的大小。

演示文稿以实验结果为结论。CounterComp 在三种最先进的基准上持续改进了模型性能，尤其是在推理步骤数量超过两个的情况下。它在分布内和分布外样本上都表现出了改进。定性分析表明，CounterComp 帮助模型在推理过程中关注更重要的输入标记，从而提高了模型的鲁棒性和准确性。

最后，她感谢了合著者 Sameena Shah 和 Carolyn Rosé，并鼓励大家查阅他们的海报或联系他们以获取更多信息。</sample>
  </task>
</testset>