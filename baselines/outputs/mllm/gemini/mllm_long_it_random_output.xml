<?xml version='1.0' encoding='utf-8'?>
<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="209">Il metodo proposto migliora significativamente la qualità della pianificazione di oltre il 20% rispetto al metodo di riferimento.</sample>
    <sample id="210">Il nome del relatore è Shuheng Liu.</sample>
    <sample id="211">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento per la semplificazione automatica del testo in futuro.</sample>
    <sample id="212">L'articolo utilizza T5 (11B) come modello più piccolo.</sample>
    <sample id="213">Il modello utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali è **OFA (One For All)**, un modello pre-addestrato multimodale unificato.</sample>
    <sample id="215">Adam Przepiórkowski ha presentato il suo lavoro sulla struttura delle dipendenze della coordinazione, che è il tema del suo articolo. Esistono diverse teorie e approcci del corpus, che possono essere divisi in simmetrici e asimmetrici.

Gli approcci asimmetrici includono l'approccio Bouquet/Stanford, in cui il primo congiunto, Lisa, è il capo dell'intera struttura coordinata. Un approccio simile è assunto nella teoria del significato-testo di Igor Melchuk, in cui l'intera struttura coordinata è guidata dal primo congiunto.

Gli approcci simmetrici includono l'approccio Conjunction-headed/Praga, in cui le strutture coordinate sono guidate dalla congiunzione. La grammatica delle parole di Katzon utilizza un approccio multi-headed/Londra, in cui tutti i congiunti sono i capi della struttura coordinata. Lo scopo di questo articolo è proporre un nuovo argomento a favore di strutture di coordinazione simmetriche e contro quelle asimmetriche.

L'argomento si basa sul principio di minimizzazione della lunghezza delle dipendenze (DLM). In inglese, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggiunti possono essere più lontani. Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto lungo. In questi casi, può essere spostato in una posizione successiva all'aggiunto.

Gli autori hanno estratto statistiche sulla coordinazione da una versione migliorata del Penn Treebank. Queste statistiche confermano l'osservazione che i congiunti di sinistra tendono ad essere più corti, e questa tendenza aumenta con la differenza di lunghezza. Ma questo accade solo quando il governatore è a sinistra o è assente. Quando il governatore è a destra, questo effetto scompare.

In sintesi, l'articolo fornisce prove a favore di strutture di coordinazione simmetriche e contro quelle asimmetriche in inglese. Le prove si basano sul principio DLM e sull'analisi delle lunghezze dei congiunti in un corpus di testo.</sample>
    <sample id="217">Weihao Zeng e i suoi colleghi dell'Università di Pechino di Poste e Telecomunicazioni hanno esplorato la generalizzazione composizionale per la generazione di dialoghi controllabili multi-attributo, scoprendo che i modelli esistenti mancano di capacità di generalizzazione.

Hanno proposto il DCG (Disentangled Controllable Generation), un modello di generazione di dialoghi controllabili basato su prompt, che apprende concetti di attributo da valori visti e utilizza una loss di disentanglement per separare diverse combinazioni di attributi. Hanno introdotto prompt orientati all'attributo per guidare il modello a concentrarsi su informazioni specifiche nel dialogo e prompt orientati al task per migliorare la qualità della generazione della risposta. Hanno anche sviluppato un framework di valutazione unificato e senza riferimenti, chiamato MAE, per diverse granularità di attributi. Hanno testato la correlazione di MAE con i giudizi umani, e i risultati hanno dimostrato che il MAE ha una correlazione più alta con i giudizi umani per la valutazione su CDG.

I loro esperimenti su DailyDialog-CG hanno mostrato che il DCG ha superato tutti gli altri modelli di riferimento in termini di controllabilità e qualità del testo per nuove combinazioni di attributi. In particolare, il DCG ha dimostrato la capacità di disentangle le modifiche di attributo e di apprendere le relazioni tra diversi attributi. I risultati hanno confermato l'efficacia del loro metodo nel trasformare attributi visti in combinazioni non viste.

In sintesi, i ricercatori hanno affrontato le sfide della generazione di dialoghi multi-attributo attraverso la proposta del DCG e un framework di valutazione robusto. I loro risultati evidenziano l'importanza dei prompt orientati all'attributo e al task, insieme all'apprendimento del disentanglement, per ottenere una migliore qualità del testo e controllabilità nella generazione di dialoghi.</sample>
    <sample id="218">Gli autori dell'articolo sono affiliati con Google Translate.</sample>
    <sample id="219">Jia-Huei Ju, a research assistant at Academia Sinica, introduces "A Compare-and-Contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports." This work focuses on analyzing Form 10-K, an annual report required by the SEC. Financial reports are rich in details about a company’s activities, yet extracting useful information demands significant human effort.  The research is motivated by two observations: financial reports exhibit high token overlap (80% on average, excluding dates) and are yearly-dependent, with adjacent years showing more similarity.

To address this, the team introduces a highlighting task and a multistage pipeline. The task involves a "reference-to-target" structure, where the model compares a company's current report (target) with its previous year's report (reference) to identify important words. For instance, the word "decreased" would be highlighted in a context where net sales declined.

The pipeline comprises three main stages: document segmentation, relation recognition, and highlighting stages (out-of-domain and in-domain fine-tuning). In the relation recognition stage (S1), document segments are categorized into "insignificant," "revised," or "mismatched" relations based on syntactic and semantic similarity. "Revised relations" are particularly important as they highlight changes in meaning despite similar wording, for example, "increase" becoming "decrease."

For highlighting (S2 and S2+), a two-staged fine-tuning approach is used. First, the model is fine-tuned on an external natural language inference dataset (e-SNLIc) for out-of-domain learning. Then, for in-domain fine-tuning, "revised pairs" from the financial reports are used with pseudo-labels, where revised words are marked as positive and other words as negative. This fine-tuning uses a mix of cross-entropy and KL divergence losses to mitigate issues from low-quality pseudo-labels.

The evaluation uses R-Precision and Pearson Correlation Coefficient (PCC) metrics on both e-SNLIc and a financial dataset (FINAL). Results show that the domain-adaptive highlighting model achieves the best performance on the FINAL dataset and maintains generalization capability on e-SNLIc. Moreover, the method benefits from unseen relations (mismatched pairs) not used during training. The conclusion highlights the task, the human-annotated dataset, and the multistage pipeline as key contributions. Future work includes pre-training financial language models, exploring bi-directional rationalization, developing end-to-end applications, and analyzing charts, tables, or cross-company data.</sample>
    <sample id="220">Gli autori dell'articolo sono affiliati alla Stony Brook University.</sample>
    <sample id="221">Il relatore ha affermato che le traduzioni sono state effettuate dal tedesco all'inglese.</sample>
    <sample id="222">Questo video presenta un lavoro intitolato "To Adapt or to Annotate: Challenges and Interventions in Open-Domain Question Answering". Il lavoro si concentra su come le tecniche di risposta a domande a dominio aperto, addestrate su corpora generici come Wikipedia, possano essere generalizzate a nuovi domini, come quello biomedico.

Il relatore evidenzia una sfida comune in cui l'espansione del corpus di documenti per includere un nuovo dominio, ad esempio PubMed, può confondere il modello sorgente addestrato su Wikipedia, portando a risposte errate. Il lavoro mira a investigare diverse "intervenzioni sui dati" per consentire la generalizzazione fuori dal dominio, a comprendere la compatibilità del modello sorgente con un dominio target e a determinare la relazione tra il tipo di intervento sui dati e lo spostamento del dataset.

Vengono presentati due metodi di adattamento:
1. **Pochi colpi (Few-shot)**: Utilizza un numero limitato di esempi dal dominio target per istruire grandi modelli linguistici (LLM) a generare più dati specifici del dominio. Questi dati aggiuntivi, sotto forma di domande in stile cloze generate da fatti estratti dai passaggi, vengono quindi utilizzati per affinare i modelli di recupero e di lettura. Questo approccio ha mostrato miglioramenti significativi nelle prestazioni, con un aumento medio dell'8% per il recupero e dell'11% per la lettura su tutti i dataset target.
2. **Zero colpi (Zero-shot)**: Non prevede l'accesso ad alcun esempio dal dominio target. Invece, si concentra sulla variazione controllata di domande, risposte e contesti. Per le domande, è stato scoperto che il formato (standard vs cloze) non influisce sulle prestazioni, rendendo lo stile cloze più facile da raccogliere. Per le risposte, è stata eseguita la Named Entity Recognition per annotare gli span con i tipi di entità, quindi sono state generate domande in stile cloze con una distribuzione uniforme di tipi di risposta, dimostrando che una rappresentazione uguale di tutti i tipi di risposta funziona meglio. Per il contesto, sono stati raggruppati passaggi dal dominio sorgente e target, e si è osservato che i modelli di recupero appresi sono sensibili alla distribuzione dei dati, mentre BM25 si comporta meglio in generale.

Per comprendere la compatibilità del modello sorgente con un dominio target, il lavoro introduce una matrice di test di generalizzabilità, che classifica gli spostamenti del dataset in "no shift", "spostamento di concetto", "spostamento di covariate" e "spostamento completo", basandosi sulla compatibilità del recuperatore e del lettore.

In conclusione, il lavoro propone un metodo a pochi colpi che migliora significativamente le prestazioni e dimostra che l'efficacia dell'intervento sui dati dipende dal tipo di spostamento del dataset.</sample>
    <sample id="223">Il nome della relatrice o del relatore è Shangbin Feng.</sample>
    <sample id="224">Durante gli esperimenti sono stati studiati due modelli: long-mBART (per la semplificazione a livello di documento) e mBART (per la semplificazione a livello di frase).</sample>
    <sample id="225">Delle 62 diverse attività utilizzate in MultiInstruct, 53 attività vengono utilizzate per l'addestramento e 9 per i test.</sample>
    <sample id="226">Ci sono tre autori coinvolti nell'articolo.</sample>
    <sample id="227">L'oratore inizia la presentazione discutendo il successo dei modelli linguistici (LM) e ciò che manca nella ricerca attuale, presentando la comprensione del linguaggio radicato (GLU) come la risposta. Spiega che la GLU implica radicare un'espressione del linguaggio naturale in qualcosa che può essere eseguito in un ambiente target specifico, come un programma o un piano. Menziona le applicazioni della GLU, inclusi gli assistenti intelligenti, la ricerca semantica su Google, l'interrogazione di database medici e i robot domestici che seguono istruzioni in linguaggio naturale. Tutti questi coinvolgono la mappatura di un'espressione del linguaggio naturale su una rappresentazione in un ambiente specifico.

L'oratore sottolinea che la GLU è impegnativa a causa della mancanza di radicamento durante la pre-formazione dei modelli linguistici. Sottolinea che gli attuali modelli linguistici, compresi i più grandi, sono pre-formati con corpus testuali e senza radicamento, il che porta a una lacuna tra la pre-formazione e l'applicazione a valle. Propone il framework Pangu, che consente agli LM di concentrarsi sulla discriminazione anziché sulla generazione. Sottolinea che la discriminazione è più facile per gli LM. In Pangu, un agente simbolico interagisce con l'ambiente e propone piani candidati validi, mentre un modello linguistico è responsabile solo di valutarli.

Il framework Pangu è stato testato nella domanda-risposta basata sulla conoscenza (KBQA), un tipico scenario GLU. Pangu ha ottenuto risultati eccezionali in tutti gli scenari e ha dimostrato una forte efficienza nel campionamento, con oltre il 50% di accuratezza con solo un esempio dimostrativo in contest learning. Pangu ha anche mostrato una forte generalizzabilità in ambienti non i.i.d., mentre i modelli autoregressivi tendono a sovradattarsi alle strutture viste durante la formazione. In conclusione, l'oratore afferma che la generazione potrebbe non essere il modo ottimale per utilizzare gli LM per la GLU, mentre la discriminazione potrebbe essere una strategia molto migliore.</sample>
    <sample id="228">Gli autori hanno condotto esperimenti su quattro set di dati: AG News, MIND, SST2 ed Enron Spam.</sample>
    <sample id="229">This presentation introduces two tasks related to improving argumentative writing: suboptimal claim detection and claim improvement suggestion. Suboptimal claim detection involves deciding whether a claim needs revision, while claim improvement suggestion focuses on selecting the types of quality issues that need improvement.

The presentation highlights that text revision is a recursive process until optimal phrasing is achieved, which directly influences the persuasive impact on the audience. It illustrates this process with an example of a claim about cell phones causing brain cancer, which is revised to specify "cell phone radiation" and clarify that it "may cause cancer."

The research aims to learn from human revision behaviors, particularly from collaborative editing activities on online debate platforms like Kialo. The assumption is that final versions of claims are optimal (green), while previous versions are suboptimal and require revision (red).

However, working with revision-based data presents several challenges:
1. **Representativity and Reliability:** Ensuring that the collected revision histories accurately reflect claim quality and whether a final version is truly optimal or merely overlooked.
2. **Model Complexity and Architecture:** Selecting models and pre-training strategies sensitive enough to detect small yet significant changes in revisions.
3. **Contextuality:** Recognizing that argument quality can depend on various contextual factors like topic expertise, domain knowledge, and the parent claim.
4. **Topical and User Bias:** Accounting for noise from accidental mistakes and biases from users and moderators in collaborative revision histories.

The paper provides a detailed analysis of these challenges, comparing different strategies and approaches. Key findings include the effectiveness of revision-based data for the given tasks, the benefit of modeling the distance between claim versions for suboptimal claim detection, and the task- and quality issue-dependent impact of contextual information.</sample>
    <sample id="231">NACHOS è un dataset open-source di 1,1 miliardi di parole di dati eterogenei estratti da vari domini, nature e stili medici.</sample>
    <sample id="232">Il nome del relatore è David Vilar Torres.</sample>
    <sample id="233">Simultaneous speech translation (SimulST) is the real-time translation of spoken language into text in another language, enabling cross-language communication. Current SimulST models face several problems, including the need for specific architectures that involve additional optimization modules, long and complicated training procedures, and the necessity to train and maintain several models for different latency regimes. The proposed solution is EDAtt, or Encoder-Decoder Attention. This strategy decides whether to emit or not a partial translation based on where attention points to. A word is emitted if the attention is not concentrated (its sum is below a certain threshold α) towards the last λ speech frames, meaning the received information is stable enough. An example demonstrates how EDAtt emits words when attention is stable, and waits for more speech when it's not. The main results show that EDAtt outperforms all other strategies when applied to offline models, with its curves shifted to the left (meaning less latency), and it is the fastest strategy considering actual elapsed time. To discover more results, read their paper or scan the QR code for more information.</sample>
    <sample id="234">Il prompting ha un impatto significativo sulla qualità della traduzione. Un semplice esperimento con prompting one-shot ha mostrato che la maggior parte delle frasi ha una differenza di oltre 1 punto BLEURT, con un massimo di 40 punti BLEURT in casi estremi.</sample>
    <sample id="235">Gli autori sono affiliati a Carnegie Mellon University Language Technologies Institute, Instituto Superior Técnico di Lisbona, Berkeley Artificial Intelligence Research (BAIR) e Unbabel.</sample>
    <sample id="236">Certo. Le 5 istruzioni scritte da esperti sono:
- Grounded Caption
- Text Localization
- Referring Expression Selection
- Question-Image Matching
- Image Captioning</sample>
    <sample id="237">Gli autori propongono una suite di test diagnostici per la valutazione dell'integrazione della conoscenza, inclusa un'attività di risoluzione della coreferenza, per sondare la capacità di utilizzare la conoscenza disponibile in diverse fonti (sia la conoscenza pre-addestramento che la conoscenza in fase di inferenza).</sample>
    <sample id="238">In this video, Yebowen Hu from the University of Central Florida introduces MeetingBank, a new benchmark dataset for meeting summarization. The dataset was created to address the scarcity of high-quality meeting summaries and the difficulty in identifying reliable sources for public meetings. It comprises city council meetings, including meeting transcripts, reference summaries, and all URLs containing useful resources.

The data collection process involves converting audio data to transcripts using the Speechmatics API, then identifying the meeting type and date to create a unique Meeting ID. This ID is used to locate corresponding reference summaries and meeting segments, including their start and end times. Finally, timestamps are aligned to pair segment transcripts with extracted summaries.

MeetingBank includes 1,366 city council meetings and nearly 7,000 summarization instances from various cities like Denver, Seattle, Long Beach, Alameda, King County, and Boston. On average, a typical meeting lasts approximately 2.6 hours and contains 28,000 tokens, while a segment consists of 2,900 tokens in the source and 87 tokens in the summary.

Dataset analysis using extractive fragment coverage and density reveals that most city council meeting summaries have a coverage score between 0.7 and 0.9, indicating important points are often extracted rather than abstracted. Seattle and Boston show the highest density scores, while Denver exhibits the lowest, suggesting a higher level of editing on meeting minutes.

For model evaluation, ten different summarization systems were tested. Extractive Oracle yielded a high ROUGE-2 score of 46.6%, indicating that reference summaries primarily originate from source transcripts. DialogLM, designed for long dialogues, performed best among abstractive models with a ROUGE-2 score of 60.12%. GPT-3, while not performing well according to automatic metrics, showed promising results in human evaluation, achieving the highest overall score in fluency and coherence.

In conclusion, MeetingBank is a valuable testbed for researchers designing advanced meeting summarizers and provides insights into the decision-making process of city councils.</sample>
    <sample id="239">00:00:00,000 --&gt; 00:00:11,550
Ciao a tutti, mi chiamo David Vilar e vi darò una breve panoramica del documento "Promoting PaLM for Translation: Assessing Strategies and Performance". Questo è un lavoro congiunto con i miei colleghi di Google Translate.

00:00:11,550 --&gt; 00:00:30,220
PaLM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri, presentato l'anno scorso, nel 2022. È stato addestrato su una vasta raccolta di testi, comprendente 780 miliardi di token. Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di attività di elaborazione del linguaggio naturale.

00:00:30,220 --&gt; 00:00:43,450
In questo lavoro presentiamo il primo studio sistematico del prompting dei modelli linguistici di grandi dimensioni per la traduzione automatica.

00:00:43,450 --&gt; 00:00:52,990
Valutiamo le capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità di traduzione automatica. Ciò implica l'utilizzo dei set di test più recenti per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico.

00:00:52,990 --&gt; 00:01:08,790
E confrontiamo due sistemi all'avanguardia, quindi i sistemi con le migliori prestazioni nella valutazione WMT. Abbiamo utilizzato metriche di traduzione automatica neurale all'avanguardia e, inoltre, presentiamo anche i risultati della valutazione umana basata su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt.

00:01:08,790 --&gt; 00:01:13,770
I prompt hanno un grande impatto sulla qualità della traduzione.

00:01:13,770 --&gt; 00:01:21,500
Il prompting ha una grande influenza sulle prestazioni dei modelli linguistici di grandi dimensioni per la traduzione, come possiamo vedere in un semplice esperimento.

00:01:21,500 --&gt; 00:01:29,910
dove usiamo il prompting one-shot e forniamo due diversi prompt per ogni frase. La maggior parte delle frasi, 516 su 1000, mostra una differenza di oltre un punto BLEURT.

00:01:29,910 --&gt; 00:01:48,390
E questo può arrivare in casi estremi fino a 40 punti BLEURT. Quindi è importante selezionare una buona strategia di prompting.

00:01:48,390 --&gt; 00:02:17,460
Nei nostri esperimenti, ci siamo orientati verso una strategia di prompting a cinque shot, in cui semplicemente contrassegniamo ogni frase che forniamo al sistema con la lingua in cui si trova. Quindi, in questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi sorgente tedesche sono contrassegnate con "German colon" e le traduzioni inglesi con "English colon".

00:02:17,460 --&gt; 00:02:37,700
Abbiamo visto che la forma effettiva del prompting non ha una grande influenza nel caso del prompting a diversi shot. È fondamentale per il prompting a zero e un shot, ma quando andiamo, come nel nostro caso, al prompting a cinque shot, non c'è quasi nessuna differenza nella forma effettiva del prompting. Sono gli esempi che portano la maggior parte del peso.

00:02:37,700 --&gt; 00:02:51,960
I risultati sperimentali mostrano che la qualità dell'esempio è più importante della somiglianza con la frase sorgente. Quindi, è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt dai dati di addestramento delle valutazioni WMT o dai dati di sviluppo.

00:02:51,960 --&gt; 00:03:09,000
I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento e i risultati mostrano una migliore performance quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale rispetto alle traduzioni PaLM.

00:03:09,000 --&gt; 00:03:31,690
Ma PaLM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo ricavato dalla valutazione umana, che abbiamo eseguito utilizzando il framework MQM, sono che la fluidità di PaLM è paragonabile a quella dello stato dell'arte. Ma la differenza principale deriva dall'accuratezza.

00:03:31,690 --&gt; 00:04:26,160
In particolare, l'errore più comune sono gli errori di omissione. Quindi, sembra che PaLM scelga di produrre una traduzione migliore, a volte omettendo parti della frase sorgente che sono omesse nella traduzione. Tuttavia, la categoria "stile/imbarazzante" per PaLM è inferiore rispetto ai sistemi all'avanguardia, il che è un segnale aggiuntivo che PaLM produce output molto fluidi, ma con ancora alcuni problemi di accuratezza. E questo è tutto per questa brevissima panoramica. Per maggiori dettagli, vi preghiamo di consultare la presentazione completa del documento. Grazie mille.</sample>
    <sample id="240">00:00
Ciao, sono Dawei, uno studente di dottorato presso l'Università di Saarland in Germania. In questo video, vorrei presentare il nostro recente lavoro, "Più debole di quanto pensi: uno sguardo critico all'apprendimento debolmente supervisionato". Questo è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.

00:20
Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento debolmente supervisionato. Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestriamo direttamente le reti neurali su dati etichettati in modo debole, le reti neurali tendono a memorizzare il rumore dell'etichetta e non generalizzano. Nell'apprendimento debolmente supervisionato, vengono proposti algoritmi di addestramento per addestrare in modo robusto le reti neurali in presenza di tale rumore dell'etichetta, in modo che i modelli addestrati generalizzino ancora bene.

01:14
Nei recenti lavori sul WSL, quindi WSL sta per apprendimento debolmente supervisionato, un'affermazione comune è che le persone dicono di addestrare i modelli solo su dati debolmente supervisionati e di ottenere prestazioni elevate su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un problema. Ovvero che le persone presuppongono che esista un set di validazione pulito aggiuntivo e disponibile per la selezione del modello. Ci siamo imbattuti in questa impostazione del problema, in quanto implica che siano necessarie annotazioni manuali aggiuntive nell'apprendimento debolmente supervisionato. Ma, come un elefante nella stanza, questa necessità viene spesso trascurata.

01:58
I nostri obiettivi ci hanno portato a porre tre domande di ricerca. Primo, i dati di validazione puliti sono necessari per il WSL? Oppure possiamo usare un set di validazione rumoroso? Secondo, se i dati puliti sono richiesti, o se i dati puliti sono obbligatori per il funzionamento del WSL, quanti campioni puliti ci servono? Infine, dovremmo usare i campioni puliti solo per la validazione, o ci sono modi migliori per utilizzarli?

02:30
Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Innanzitutto, abbiamo scoperto che, curiosamente, i recenti metodi WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, si verifica un grande calo di prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Ciò indica che gli approcci WSL richiedono effettivamente dati etichettati in modo pulito per funzionare correttamente, e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.

03:13
La nostra seconda scoperta è che l'aumento del numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente, ci servono solo 20 campioni per classe per raggiungere prestazioni elevate. Ma non è la fine della storia, perché se decidiamo comunque di accedere a campioni puliti, l'addestramento diretto su di essi otterrà prestazioni ancora migliori. La figura a destra mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente sui dati puliti, e gli approcci WSL, che usano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a superare gli approcci WSL.

04:07
Infine, il miglioramento delle prestazioni rivendicato nei precedenti approcci WSL può essere facilmente ottenuto consentendo il fine-tuning continuo sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello Vanilla, chiamato FTW, inizialmente ha prestazioni inferiori rispetto a metodi WSL più complessi come COZINE. Tuttavia, se consentiamo il fine-tuning continuo sui campioni puliti, FTW ottiene prestazioni altrettanto buone quanto gli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco.

04:51
In sintesi, abbiamo dimostrato che i recenti approcci WSL richiedono campioni puliti. Le loro prestazioni e praticità sono fortemente sovrastimate. Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti: primo, riportare i criteri di selezione del modello, ad esempio, riportare se la selezione del modello è stata eseguita su campioni di validazione puliti. Secondo, gli approcci WSL dovrebbero essere confrontati con i baseline dell'apprendimento pochi-shot, anche per il lavoro su campioni puliti. Terzo, il fine-tuning continuo è un baseline semplice ma efficace che dovrebbe essere considerato nel lavoro futuro sul WSL. Infine, abbiamo reso pubblico il nostro codice. Potete trovarlo tramite il codice QR su questa diapositiva. Sentitevi liberi di darci un'occhiata. Grazie e buon proseguimento della conferenza.</sample>
    <sample id="241">Questo documento introduce un innovativo framework di valutazione per i sistemi di rilevamento precoce della disinformazione in un contesto umano. In contrasto con i sistemi attuali, che spesso mancano di realismo e non sono incentrati sull'uomo, questo approccio propone un metodo end-to-end che incorpora il feedback umano in varie fasi del processo. Il framework è stato implementato e valutato per il rilevamento della disinformazione sui trattamenti COVID-19 su Twitter, dimostrando la sua utilità nel rilevare affermazioni fuorvianti prima che vengano smentite.

Il sistema si concentra su due componenti principali: il rilevamento di affermazioni fuorvianti e la verifica delle violazioni delle policy. Il primo componente raccoglie i tweet pertinenti, estrae affermazioni verificabili e le classifica in base alla loro tendenza per la convalida umana. Il secondo componente, una volta che le affermazioni disinformate sono state verificate, determina la posizione del tweet rispetto all'affermazione non approvata e segnala i tweet che violano le policy per la revisione umana.

La valutazione del sistema ha dimostrato che può rilevare precocemente le affermazioni fuorvianti, spesso giorni prima che vengano smentite nelle notizie. Inoltre, il sistema ha mostrato un'elevata precisione nella verifica delle violazioni delle policy, con il 65% dei tweet identificati dal sistema che violano le policy di Twitter. L'efficienza del sistema è ulteriormente evidenziata dalla capacità di rilevare 124,2 violazioni delle policy per ora di lavoro umano, includendo sia la verifica delle affermazioni che la verifica delle violazioni delle policy.

Questo lavoro mira a motivare lo sviluppo di framework di rilevamento della disinformazione più utili e incentrati sull'uomo, fornendo uno standard di confronto concreto per i sistemi futuri. Il framework offre una prospettiva esterna sullo sviluppo e la valutazione dei sistemi di disinformazione umani nel loop, facilitando la creazione di strumenti più efficaci per combattere la disinformazione.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono la valutazione comparativa, in cui i giudici umani selezionano il migliore di due modelli di dialogo, e la valutazione con scala Likert, in cui i giudici valutano le conversazioni su una scala numerica.</sample>
    <sample id="243">Ci sono 5 autori coinvolti in questo articolo.</sample>
    <sample id="244">In questo caso, la conoscenza di base necessaria è che i giudici decidono i casi nei tribunali.</sample>
    <sample id="245">Questa presentazione riguarda uno studio intitolato "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization". Lo studio presenta una pipeline a due fasi per trovare lavoratori di Amazon Mechanical Turk (MTurk) con un alto grado di accordo. La motivazione di questa pipeline è che le metriche automatiche possono essere problematiche e le migliori pratiche per il reclutamento su MTurk sono poco comprese.

La pipeline inizia con delle impostazioni di qualificazione MTurk, che includono criteri come la posizione e il tasso di approvazione degli Human Intelligence Tasks (HIT). La prima fase è un "Qualification Task" che valuta la capacità degli annotatori di valutare correttamente più dimensioni. Si tratta di tre documenti (uno con un controllo dell'attenzione) e un riassunto per ciascuno, valutato su sei dimensioni. I lavoratori sono categorizzati in quattro tipi (GOLD, SILVER, BRONZE, BLOCK) in base ai loro risultati, ma solo i lavoratori GOLD e SILVER passano. Di 200 partecipanti, 26 lavoratori (8 GOLD, 18 SILVER), ovvero il 13%, si sono qualificati.

La seconda fase è un "Endurance Task", progettato per testare la capacità dei lavoratori di gestire carichi di lavoro pesanti. Questo include 10 HIT, ciascuno con un documento e quattro riassunti da valutare per la salienza. Dodici lavoratori (4 GOLD, 8 SILVER), ovvero il 6% dei partecipanti iniziali, hanno superato questa fase, dimostrando un accordo inter-annotatore (IAA) superiore a quello degli esperti. Il loro Kappa di Cohen migliore è 0,55 e l'Alfa di Krippendorff migliore è 0,443 (GOLD).

Infine, lo studio ha eseguito un "Reference-based Task" per testare le prestazioni complessive dei lavoratori su un vero compito di annotazione. Otto dei 12 lavoratori qualificati hanno completato tutti gli HIT, con un Kappa di Cohen migliore di 0,68 (GOLD) e un'Alfa di Krippendorff di 0,534 (per tutti). Un confronto con i lavoratori MTurk di base (filtrati con MACE) e con i lavoratori di CloudResearch ha mostrato che la pipeline raggiunge una qualità simile, ma a un costo inferiore.

In conclusione, la pipeline a due fasi identifica con successo lavoratori MTurk di alta qualità. Il risultato finale è di 4 lavoratori GOLD e 8 SILVER (6% del totale), che fornisce annotazioni con un alto grado di accordo su larga scala e a costi inferiori, evitando sprechi di risorse. Le limitazioni includono il fatto che è stata testata solo la riassunto in inglese e le domande non sono soluzioni "panacea". In futuro, lo studio mira a trovare modi per assumere lavoratori di alta qualità per compiti, lingue e piattaforme diverse.</sample>
    <sample id="246">Sì, il codice è disponibile su GitHub.</sample>
    <sample id="247">Il presentatore di KAIST AI presenta il loro lavoro, "FactKG: Fact Verification via Reasoning on Knowledge Graphs". Spiega che, sebbene esistano set di dati per la verifica dei fatti basati su testo e tabelle, non esiste un set di dati che utilizzi i grafi della conoscenza come prova con affermazioni in linguaggio naturale. Di conseguenza, propongono un nuovo compito, la verifica dei fatti basata sui grafi della conoscenza. Un grafo della conoscenza è una fonte di conoscenza affidabile e pratica. I grafi della conoscenza sono affidabili perché l'evidenza è intuitiva e può essere collegata direttamente all'affermazione. Possono anche essere usati praticamente dai sistemi di dialogo moderni, che comunicano con grafi della conoscenza interni. Questo può essere usato per controllare la coerenza tra le parole dell'utente e il grafo della conoscenza. A tal fine, introducono un nuovo set di dati, FactKG. Il grafo della conoscenza utilizzato è DBpedia e le affermazioni esistono in due stili: scritto e colloquiale. Ci sono due etichette: SUPPORTED e REFUTED. Il compito consiste nel recuperare le prove da DBpedia e verificare l'affermazione utilizzando le prove. Ci sono cinque tipi di ragionamento: one-hop, congiunzione, esistenza, multi-hop e negazione. Il set di dati include affermazioni in stile colloquiale e scritto per un uso più pratico. I metodi utilizzati per questo sono il colloquiale di Kim et al. (2021) e i modelli di presupposizione. Gli esperimenti indicano che l'uso di prove grafiche nel loro modello ha portato a prestazioni superiori rispetto ai modelli di riferimento che non hanno incorporato tali prove.</sample>
    <sample id="248">Gli annotatori per NLPositionality sono un pool diversificato di volontari provenienti da 87 paesi e con diverse età, genere, etnie e livelli di istruzione. Sebbene NLPositionality si sforzi di essere inclusivo, la distribuzione degli annotatori non è bilanciata rispetto a tutti i gruppi demografici. Ad esempio, gli annotatori di lingua inglese e quelli con istruzione universitaria sono maggiormente rappresentati.</sample>
    <sample id="249">Le frasi accettabili sono state perturbate aggiungendo avverbi di prefisso/suffisso, lunghi avverbi di prefisso, una frase aggiuntiva o una citazione.</sample>
    <sample id="250">Una valutazione dimensionale significa valutare un dialogo in base a varie dimensioni, come la rilevanza, la coerenza e la comprensione emotiva, per comprendere i punti di forza e di debolezza di un modello.</sample>
    <sample id="251">Ecco le affiliazioni degli autori dell'articolo:

- University of Science and Technology of China
- Microsoft Research Asia
- Beijing Jiaotong University
- Sony AI
- Microsoft STC Asia</sample>
    <sample id="252">In questo intervento, il relatore presenta un lavoro sull'Unsupervised Case Retrieval utilizzando l'estrazione di eventi, chiamato U-CREAT, svolto in collaborazione con Abhinav Joshi, Akshat Sharma e Ashutosh Modi. Il compito di recupero dei casi precedenti (PCR) è essenziale per gli operatori legali, ma diventa difficile con l'aumento dei casi. Per affrontare questo problema, il relatore propone U-CREAT e il dataset IL-PCR.

IL-PCR è un nuovo benchmark per il PCR nel sistema legale indiano, contenente 7070 casi legali, 1182 query e una media di 6,775 citazioni per documento di query. Questo dataset è più esteso di quelli esistenti, come COLIEE'21.

U-CREAT è un approccio basato sugli eventi per il PCR che funziona senza supervisione, offrendo alta efficienza di recupero e basso tempo di inferenza. Si generalizza bene tra sistemi legali indiani e canadesi, senza richiedere un fine-tuning specifico per legge o demografia. L'estrazione degli eventi si basa sull'analisi delle dipendenze per identificare triplette soggetto-verbo-oggetto, che rappresentano gli eventi.

La pipeline di U-CREAT prevede l'estrazione degli eventi da documenti di query e candidati, il calcolo di matrici di interazione per trovare eventi comuni e l'utilizzo di modelli di recupero per classificare i candidati. I modelli basati sugli eventi mostrano prestazioni superiori rispetto a quelli basati sul conteggio e sui transformer, con Event Filtered Docs che si rivela il più efficace, superando il baseline BM-25 di oltre il 25%. U-CREAT stabilisce un nuovo stato dell'arte per il compito di recupero dei documenti legali, fornendo un metodo robusto ed efficiente per il PCR.</sample>
    <sample id="253">Il documento introduce DisorBERT, un modello di adattamento a doppio dominio per rilevare i segni di disturbi mentali nei social media. La presentazione inizia con la definizione di disturbi mentali, come lo stress, l'ansia, la schizofrenia, la depressione, i disturbi della personalità e del sonno. La crescente quantità di contenuti sui social media offre l'opportunità di ricercare come le persone affrontano queste difficoltà. Molti utilizzano le piattaforme online per pubblicare e condividere eventi, mentre altri sfruttano l'anonimato di questi spazi per discutere apertamente problemi di salute mentale e cercare aiuto.

L'obiettivo di questo lavoro è contribuire al rilevamento dei disturbi della salute mentale analizzando automaticamente i post sui social media. Questa analisi è un passo verso una nuova tecnologia in grado di avvertire l'insorgenza di disturbi mentali e fornire prove a supporto.

Il modello DisorBERT adatta il modello linguistico BERT pre-addestrato al dominio della salute mentale. Questo processo di adattamento del dominio prevede due passaggi principali: apprendere il linguaggio specifico dei social media da Reddit e specializzare il modello per il dominio dei disturbi mentali. Inoltre, il modello incorpora la conoscenza di un lessico per guidare il processo di mascheramento, garantendo che il modello si concentri sulle parole importanti durante l'addestramento.

I risultati mostrano che DisorBERT ottiene risultati migliori rispetto a MentalBERT, un modello addestrato con una maggiore quantità di dati e un maggiore consumo di risorse computazionali. Il modello mostra anche un buon equilibrio tra precisione e recall, rendendolo adatto per applicazioni di rilevamento clinico.

Un'analisi qualitativa del modello, utilizzando il Beck's Depression Inventory (BDI-Test), rivela che DisorBERT tende a generare parole con un significato più negativo o un orientamento psicologico rispetto a BERT. Ad esempio, quando mascherata la parola "cry" nella frase "I used to be able to cry", DisorBERT prevede parole come "focus", "talk", "breathe", "sleep" e "eat", che sono associate a problemi comuni nei disturbi mentali.

In futuro, il team di ricerca intende esplorare l'applicazione di diverse risorse lessicali più specializzate per i compiti target, nonché l'utilizzo di dati clinici per addestrare modelli linguistici più specializzati.</sample>
    <sample id="254">In questa presentazione, si discute un lavoro di ricerca sulla **denoising delle etichette guidato dall'incertezza per l'estrazione di relazioni distanti a livello di documento**. L'obiettivo è estrarre relazioni tra le entità in un documento.
I metodi precedenti si basavano su grandi corpora annotati da esseri umani, un processo che richiede molto tempo e lavoro. Per ovviare a questo, il lavoro più recente ha utilizzato dati distanti supervidati per pre-addestrare i modelli di estrazione delle relazioni a livello di documento, migliorando le prestazioni.

Tuttavia, questi dati spesso contengono etichette rumorose, e i metodi attuali che utilizzano pseudo-etichette rischiano ancora l'introduzione di rumore tramite falsi positivi. Per affrontare questo problema, viene proposto un **framework per l'estrazione di relazioni distanti a livello di documento con denoising delle etichette guidato dall'incertezza, con l'obiettivo di migliorare la qualità delle etichette nei dati distanti supervidati**.

Il framework prevede:
1. **Addestramento di un modello iniziale di estrazione di relazioni pre-denoising** con dati distanti supervidati originali e dati annotati da esseri umani.
2. **Utilizzo del modello pre-denoising per generare pseudo-istanze** con punteggi di incertezza sui dati distanti supervidati. Per affrontare il problema delle relazioni sovrapposte (quando un paio di entità ha più tipi di relazione), viene introdotta una **stima dell'incertezza a livello di istanza** per misurare l'affidabilità delle pseudo-etichette a livello di istanza.
3. **Esecuzione di una strategia di ri-etichettatura** per ottenere dati distanti supervidati denoising. Per questo, si propone una **soglia di incertezza dinamica per classe**, che filtra le pseudo-etichette con alta incertezza, tenendo conto delle diverse distribuzioni di incertezza tra le classi di relazione (le classi frequenti tendono ad avere un'incertezza media inferiore rispetto alle classi con coda lunga).
4. **Re-inizializzazione e addestramento del modello pre-denoising** con i dati distanti supervidati denoising e i dati annotati da esseri umani. Questo processo viene iterato per ottimizzare il modello.

I risultati sperimentali mostrano che il framework proposto **supera le baseline esistenti** su due dataset pubblici, evidenziando un notevole miglioramento delle prestazioni quando i modelli vengono addestrati con i dati denoising.</sample>
    <sample id="255">Nel caso del prompting a zero e a un colpo, la forma del prompting si rivela cruciale.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo open-domain: BART-FID-RAG, Blender2, Emora e Blender-Decode.</sample>
    <sample id="258">Il relatore presenta un nuovo metodo chiamato "LLM evaluation" per valutare i testi nell'elaborazione del linguaggio naturale (NLP). L'idea alla base di questo metodo è quella di utilizzare istruzioni in linguaggio naturale per addestrare i modelli LLM a valutare campioni di testo. Sebbene questa idea sia ora ampiamente adottata, quando questo lavoro è stato presentato ad ACL 2023, la valutazione LLM era un'idea ancora nuova.
La motivazione di questo lavoro nasce dal fatto che la valutazione umana, ampiamente utilizzata in passato, è instabile e difficile da riprodurre. Questo metodo cerca quindi un'alternativa che possa raggiungere gli stessi obiettivi senza gli stessi svantaggi. 
Per verificare l'utilità di questa metodologia, sono stati condotti degli esperimenti in cui i modelli LLM hanno valutato storie generate da GPT-2 o scritte da esseri umani, usando quattro attributi: grammatica, coerenza, gradevolezza e pertinenza. I risultati sono stati confrontati con quelli ottenuti da insegnanti di inglese, considerati esperti. La ricerca ha rivelato che gli LLM più grandi mostrano una preferenza per i testi scritti da esseri umani, proprio come gli insegnanti di inglese. Il relatore sottolinea che gli LLM possono servire come alternativa alla valutazione umana.
Il video termina invitando gli spettatori a leggere il documento completo per saperne di più sulle domande lasciate in sospeso.</sample>
    <sample id="259">Un ricercatore della Penn State University ha presentato il suo lavoro, XSemPLR: Analisi Semantica Cross-Lingue in Lingue Naturali Multiple e Rappresentazioni di Significato.

L'analisi semantica è un compito che consiste nel costruire rappresentazioni semantiche delle query degli utenti, come SQL e Lambda Calculus. L'analisi semantica cross-lingue è un compito che consiste nel tradurre query in lingue naturali multiple in rappresentazioni di significato multiple.

I modelli di analisi semantica cross-lingue esistenti sono proposti separatamente e valutati su set di dati di compiti e applicazioni limitate. Per esempio, c'è una mancanza di copertura su alcune lingue naturali (il cinese è mancante), una mancanza di copertura su alcune rappresentazioni di significato (Lambda Calculus è mancante) o sono valutati solo su un singolo modello neurale.

Per ovviare a ciò, il team di ricerca ha proposto XSemPLR, un set di dati unificato per l'analisi semantica cross-lingue in lingue naturali multiple e rappresentazioni di significato. Contiene nove set di dati in vari domini, cinque compiti di analisi semantica, otto rappresentazioni di significato e 22 lingue naturali in 15 famiglie linguistiche.

Per valutare XSemPLR, il team ha considerato sei impostazioni per l'addestramento e la valutazione:
* Translate-Test: Usare l'API di Google Translate per tradurre la sorgente nella lingua target, quindi usare un modello monolingue per addestrare e valutare.
* Modello monolingue: La lingua sorgente è la stessa della lingua target, ad esempio dal tedesco al tedesco. Hanno anche testato l'impostazione "Monolingue Few-shot" addestrando modelli monolingui con solo il 10% dei dati di addestramento.
* Modello multilingue: Addestrare un modello multilingue per tutte le lingue.
* Trasferimento cross-lingue zero-shot/few-shot: Addestrare su una lingua sorgente e trasferire a un'altra lingua.

Hanno valutato due gruppi di modelli sull'impostazione monolingue:
* Enc-PTR: Encoder pre-addestrati multilingui con decodificatori basati su puntatori (XLM-R + PTR, mBERT + PTR)
* Enc-Dec: Modelli encoder-decoder pre-addestrati multilingui (mBART, mT5)
Hanno scoperto che Enc-Dec (mT5) ottiene le migliori prestazioni su tutti i set di dati.

Hanno anche valutato mT5 e XLM-R + PTR sull'impostazione multilingue. Hanno scoperto che Enc-Dec/Enc-PTR (mT5/XLM-R) può essere migliorato addestrando in una miscela di varie lingue. Hanno anche scoperto che la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne che le prestazioni dell'inglese calano in sette set di dati e guadagnano in tre set di dati. Questo è noto come "Maledizione della Multilingue".

Il team ha anche confrontato il divario di prestazioni cross-lingue. La linea blu è il trasferimento cross-lingue few-shot. La linea arancione è il trasferimento cross-lingues zero-shot. La linea verde è l'impostazione monolingue. Hanno scoperto che per l'impostazione zero-shot, il divario di prestazioni del trasferimento cross-lingue è significativo. Per l'impostazione few-shot, il divario di trasferimento si accorcia rapidamente.

Altre scoperte interessanti includono:
* Enc-Dec (mT5) supera i lavori precedenti o raggiunge risultati paragonabili.
* L'addestramento su NL inglese può migliorare significativamente le prestazioni di few-shot sulle NL target.
* I LLM multilingui (Codex &amp; BLOOM) sono ancora inadeguati per i compiti di analisi semantica cross-lingue.
* Il trasferimento di apprendimento cinese e l'addestramento monolingue inglese (En -&gt; En) hanno il più grande divario di prestazioni, mentre il tedesco di solito ha il più piccolo.
* FunQL supera le altre tre rappresentazioni di significato, e SQL ottiene le peggiori prestazioni.

In conclusione, il team ha costruito XSemPLR, un benchmark unificato per l'analisi semantica cross-lingue con lingue naturali multiple e rappresentazioni di significato. Hanno condotto uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingui. I loro risultati mostrano che mT5 con addestramento monolingue produce le migliori prestazioni, mentre i LLM multilingui sono ancora inadeguati per eseguire compiti di analisi semantica cross-lingues. Inoltre, il divario di prestazioni tra l'addestramento monolingue e l'apprendimento per trasferimento cross-lingues è ancora significativo.</sample>
    <sample id="260">Ci sono 9 autori coinvolti nell'articolo.</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli.</sample>
    <sample id="262">Ci sono nove autori coinvolti nell'articolo.</sample>
    <sample id="263">Il lavoro presentato si concentra sulla mitigazione delle distorsioni di etichetta nell'apprendimento in-context per i Large Language Models (LLM). L'apprendimento in-context, pur essendo un paradigma popolare per l'utilizzo degli LLM, è noto per la sua instabilità a causa di varie scelte di progettazione, come la scelta e l'ordine degli esempi in-context. Questo lavoro affronta la mancanza di una discussione sistematica sulla categorizzazione, rilevazione e mitigazione di diversi tipi di distorsioni.

Per affrontare questo problema, il lavoro introduce una tipologia di distorsioni di etichetta per i task di classificazione. Vengono identificate tre componenti che contribuiscono alla distorsione:
1.  **Distorsione di etichetta vanilla**: Si riferisce alla preferenza non contestuale del modello per i nomi delle etichette, come la distorsione del token comune causata dalla frequenza dei termini di pre-training.
2.  **Distorsione di etichetta contestuale**: Cattura gli effetti del contesto, come la distorsione dell'etichetta di maggioranza.
3.  **Distorsione di etichetta del dominio (nuova)**: Cattura gli effetti del corpus del task sulla predizione del modello.

Un esperimento condotto per confermare la distorsione di etichetta del dominio ha mostrato che parole casuali in-domain provenienti dal corpus del task possono polarizzare significativamente le predizioni del modello, mentre parole inglesi casuali non mostrano tale preferenza.

Si è osservato che gli LLM hanno comportamenti di apprendimento in-context diversi in base al livello di distorsione di etichetta del dominio. Nei task con una piccola distorsione, l'apprendimento in-context funziona bene e può essere ulteriormente migliorato con metodi di calibrazione avanzati. Tuttavia, nei task con una grande distorsione, gli LLM faticano a superare una baseline casuale, anche con la calibrazione esistente.

Per mitigare la distorsione di etichetta del dominio e altri tipi di distorsioni, è stata proposta la **calibrazione contesto-dominio (DC)**. A differenza dei metodi di calibrazione precedenti che usano un singolo token predefinito e privo di contenuto, DC usa parole casuali in-domain come testo privo di contenuto. Questo approccio è in grado di mitigare efficacemente tutti e tre i tipi di distorsioni.

I risultati sperimentali mostrano che la DC migliora in modo significativo l'apprendimento in-context, in particolare nei task con una maggiore distorsione di etichetta del dominio. Ciò è dovuto al fatto che la DC permette al modello di avere confini decisionali migliori. La ricerca ha anche dimostrato che i token predefiniti e privi di contenuto possono essere essi stessi distorti e che l'uso di un singolo token privo di contenuto è sub-ottimale. L'utilizzo di parole casuali in-domain per la calibrazione ha ulteriormente migliorato le prestazioni, indicando che la DC è efficace nel rimuovere la distorsione di etichetta del dominio. Questi risultati valgono anche per modelli più grandi, come GPT-3 (175B).

In sintesi, il lavoro presenta una tipologia delle distorsioni di etichetta, identifica la distorsione di etichetta del dominio come una fonte importante di distorsione e propone la calibrazione contesto-dominio per mitigarle olisticamente, migliorando significativamente le prestazioni dell'apprendimento in-context.</sample>
    <sample id="264">Wang Lin di Zhejiang University ha presentato il suo lavoro intitolato "TAVT: Towards Transferable Audio-Visual Text Generation". Il testo sottolinea i limiti e le sfide attuali dei modelli di text generation, specialmente quelli multimodali, dove l'annotazione dei dati è ardua e costosa. Ha evidenziato che i lavori esistenti soffrono di un grave degrado a causa delle diverse condizioni di registrazione in domini differenti.

Per affrontare queste sfide, Lin e il suo team hanno proposto un nuovo framework, il TAVT, un pipeline progettato per imparare e adattarsi rapidamente a nuovi domini multimodali con dati etichettati limitati. Questo framework è composto da tre elementi principali: Audio-Visual Meta-Mapper Network (AVMM), Audio-Visual Encoder (AVE) e Language Model Generator (LMG), e una tecnica di apprendimento contrastivo counterfactual.

L'AVMM è un network che mappa diversi concetti visivi in uno spazio semantico uditivo unificato, regolando le distribuzioni semantiche. Utilizza una serie di token apprendibili chiamati "visual prefix" per i cluster audio, generando una distribuzione di probabilità nello spazio audio per la ricostruzione. Questo aiuta a migliorare la semantica dei token e allineare il contenuto visivo con lo spazio audio.

Il framework impiega un encoder e un generatore basati su transformer. Il generatore introduce un parametro "alfa" per valutare il contributo di ciascuna modalità a ogni parola. Per ottimizzare l'allineamento audio-visivo, sono state proposte due perdite di apprendimento contrastivo counterfactual: una basata sulla distribuzione e una basata sulla dipendenza, che offrono una supervisione a grana fine e evitano l'affidamento a campioni negativi selezionati casualmente.

L'approccio TAVT ha dimostrato prestazioni superiori rispetto ai metodi esistenti in diversi benchmark, incluso un notevole margine su set di dati cross-dataset e cross-domain. Questo è particolarmente vero per i domini con meno dati etichettati, dove il TAVT mostra un degrado minimo delle prestazioni.</sample>
    <sample id="265">La relatrice si chiama Vasudha Varadarajan.</sample>
    <sample id="266">Gli autori sono affiliati all'Institute of Computer Science, Polish Academy of Sciences, e all'Università di Varsavia.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono gli errori di omissione.</sample>
    <sample id="269">Hello, I'm James Finch.
And I'm Sarah Finch.
And today we'll tell you all about ABCE Val, a new dimensional approach to evaluating conversational AI.
This work was done by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI.
So let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art. The common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.
These approaches work well to provide holistic evaluations of overall dialogue quality but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level.
One approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or Likert scale methods. However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation.
Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself. We call this approach Annotating Behaviors in Chat or ABC Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature.
ABCE Eval is capable of measuring the rates at which chat models will commit various thematic errors. For example, ABCE Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant, contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge and when the model succeeds or fails to show empathy.
To determine what kind of evaluation is most effective, we selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABCE Eval. For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn level, Likert ratings on the dialogue level and dialogue level pairwise comparisons.
For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions.
From our analyses of these evaluation results, we found that ABCE Eval behavior labels are overall more reliable than labels collected by existing methods as measured by inter-annotator agreement on 100 doubly labeled conversations.
In addition, ABCE Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods as shown by this simple linear regression analysis. For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality respectively while the average Likert consistency scores explain only 4% or less.
Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. You can see how the combination of all ABCE Eval metrics explains over 25% of conversation quality and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. On the other hand, the combination of all turn level Likert metrics explains far less of the quality and fewer of these metrics carry unique information.
These reliable, informative and distinct ABCE Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve. You can see in the result of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses, they produce irrelevant information in around 15% of the responses and they contradict themselves or their partner around 10% of the time.
With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models. We hope ABCE Eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational AI will advance in the coming months and years.
Thanks for watching.</sample>
    <sample id="270">Gli autori dell'articolo sono affiliati all'Emory NLP Research Lab della Emory University e ad Amazon Alexa AI.</sample>
    <sample id="271">In questo articolo, CFT sta per Continuous Fine-Tuning.</sample>
    <sample id="272">Ci sono 8 autori coinvolti nell'articolo.</sample>
    <sample id="273">00:00
Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un'esplorazione multilingue basata sui dati". Questo lavoro è stato svolto in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.
00:15
Molte traduzioni dipendono dal contesto. Per esempio, come tradurremmo "mole" in questa frase?
00:27
Se la frase precedente fosse "Le cose potrebbero diventare pericolose se i ministri scoprissero.", allora "mole" si riferisce a una spia.
00:30
Ma se la frase precedente fosse "Potrebbe essere qualcosa di serio, Dottore?", allora "mole" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, cambia anche la sua traduzione.
00:41
Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile.
00:47
In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto,
00:52
il che rende le metriche a livello di corpus come il BLEU incapaci di catturare queste traduzioni.
00:57
E alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue.
01:08
Poiché di solito si basano sulla conoscenza del dominio e sulla curatela umana.
01:13
In questo lavoro, abbiamo cercato di rispondere a queste due domande. Primo, quando la traduzione richiede contesto? E secondo, quanto bene i modelli gestiscono le traduzioni dipendenti dal contesto?
01:24
Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto durante la traduzione.
01:30
Nel lavoro precedente, abbiamo introdotto il CXMI come misura per l'uso del contesto da parte dei modelli di traduzione automatica.
01:37
E questo si fa misurando quanta informazione il contesto C fornisce sul target Y, dato il source X. Si può pensare al CXMI come al guadagno di informazione ottenuto fornendo il contesto al modello.
01:49
In questo lavoro, estendiamo il CXMI al point-wise CXMI, che può misurare l'uso del contesto a livello di frase o a livello di parola.
01:58
Possiamo pensare alle parole con un alto P-CXMI come a quelle che richiedono contesto per la traduzione.
02:04
Ora analizziamo le parole con un alto P-CXMI per cercare schemi tra queste parole.
02:10
E eseguiamo la nostra analisi sulle trascrizioni delle conferenze TED che sono state tradotte dall'inglese in 14 lingue diverse.
02:18
Eseguiamo la nostra analisi a tre livelli diversi. Primo, esaminiamo i tag POS che hanno una media P-CXMI elevata.
02:25
E questo ci permette di trovare, per esempio, pronomi duali in arabo che hanno un P-CXMI relativamente alto. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo.
02:41
E allo stesso modo, troviamo che alcune lingue richiedono anche il contesto quando vogliamo scegliere la forma verbale appropriata.
02:49
Poi esaminiamo gli elementi del vocabolario che hanno un P-CXMI medio elevato su tutte le loro diverse occorrenze.
02:56
E questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario il contesto per tradurre correttamente i nomi propri, per assicurarsi di utilizzare la stessa traduzione all'interno del documento.
03:07
E allo stesso modo, troviamo che il contesto è supportato per tradurre nella giusta formalità.
03:12
E infine, guardiamo a diversi, a singoli token che hanno un P-CXMI elevato.
03:17
E questo ci ha permesso di identificare fenomeni che non possono essere catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi.
03:29
Quindi, ora usiamo i nostri risultati dalla nostra analisi per progettare un benchmark per la traduzione a livello di documento.
03:37
Per ciascuno dei cinque fenomeni discorsivi che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che rientrano nel fenomeno. E chiamiamo il nostro tagger il tagger Multilingue Discorso-Consapevole o MuDA.
03:49
Possiamo quindi notare che lingue diverse hanno proporzioni diverse di questi fenomeni discorsivi.
03:57
Usiamo quindi il tagger MuDA, applicando il tagger a un corpus parallelo che vogliamo usare per la valutazione.
04:05
E applichiamo le nostre metriche di traduzione preferite sul contesto, esempi dipendenti che il tagger MuDA ha identificato.
04:12
E infine, utilizziamo il nostro benchmark, così come altre metriche per valutare diversi modelli sulla traduzione automatica a livello di documento.
04:22
In primo luogo, quando usiamo metriche a livello di corpus, per il BLEU troviamo che i modelli agnostici al contesto hanno le migliori prestazioni.
04:31
Ma poi, se usiamo COMET, i modelli consapevoli del contesto si comportano meglio.
04:36
E se usiamo la misura F delle parole, allora i modelli con o senza contesto hanno prestazioni comparabili.
04:44
Ciò dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se utilizziamo solo metriche a livello di corpus.
04:52
Ora usiamo il benchmark MuDA per valutare i modelli, e troviamo che i modelli consapevoli del contesto sono significativamente più precisi dei modelli che non usano il contesto per certi fenomeni discorsivi, come
05:04
Formalità e coesione lessicale.
05:07
Ma questi modelli non sono molto migliori dei modelli che non usano il contesto per altri fenomeni come l'ellissi, i pronomi e la forma verbale. Quindi questo suggerisce in qualche modo dove avremmo bisogno di vedere più progressi per la traduzione a livello di documento.
05:20
Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è solitamente più preciso di Google Translate per la traduzione a livello di documento.
05:29
Per riassumere, abbiamo eseguito un'analisi basata sui dati su 14 coppie di lingue per identificare quando le traduzioni richiedono contesto. E poi usiamo i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento, che può aiutarci a identificare quali fenomeni discorsivi i modelli possono gestire bene o meno, e quali sistemi di traduzione sono buoni per la traduzione a livello di documento.
05:51
Grazie mille per la vostra attenzione, ci vediamo a Toronto.</sample>
    <sample id="274">Il relatore è Yusen Zhang.</sample>
    <sample id="276">Questo video riguarda la valutazione automatica della traduzione automatica (MT), che è un compito di elaborazione del linguaggio naturale. Diverse metriche di valutazione sono state proposte e studiate per la traduzione da e verso l'inglese, ma la valutazione per altre lingue è sottostudiata. Il video presenta l'IndicMT Eval, un dataset multilingue per la meta-valutazione delle metriche MT per le lingue indiane.

Questo dataset è stato creato selezionando 200 frasi casuali dal dataset Flores e traducendole in 5 lingue indiane (tamil, malayalam, hindi, marathi e gujarati) utilizzando 7 diversi sistemi di traduzione automatica. Le traduzioni risultanti sono state annotate da traduttori esperti bilingue utilizzando il framework MQM. Il framework MQM classifica gli errori in categorie (accuratezza, fluidità e altri/speciali), con ulteriori sottocategorie come aggiunta, omissione, mistraduzione, testo non tradotto, ortografia, grammatica, registro e codifica dei caratteri.

Il video mostra anche una tabella delle correlazioni tra varie metriche (basate sull'overlap, basate sull'embedding e basate su COMET) e i punteggi umani. I risultati mostrano che la variante IndicCOMET MQM supera le metriche COMET di riferimento per 3 delle 5 lingue. Viene anche mostrato che l'IndicCOMET supera le metriche COMET di riferimento per la maggior parte delle lingue nei test zero-shot e mostra un punteggio di robustezza migliore rispetto alle controparti COMET.

Il dataset e il codice di IndicMT Eval sono disponibili pubblicamente.</sample>
    <sample id="277">Il nuovo metodo è privo di un nome specifico.</sample>
    <sample id="278">L'autrice descrive il metodo delle "parole contrassegnate" come un modo per identificare le parole che distinguono i profili dei gruppi contrassegnati da quelli non contrassegnati, attingendo al concetto sociolinguistico di "marcatura" per trovare schemi specifici senza richiedere un lessico.</sample>
    <sample id="279">Le affiliazioni degli autori sono: Paul G. Allen School (UW), UWNLP, Carnegie Mellon University Language Technologies Institute, e l'Università di Washington.</sample>
    <sample id="280">I'm Tao Shi, and I'm sharing my work on MultiEMO, an attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations (ERC). The goal of ERC is to predict the emotion label of each utterance in a dialogue, with each utterance having textual, audio, and visual modalities.

Existing ERC approaches face challenges: the complementarity of multimodal information isn't fully exploited, there are unsatisfactory performances in minority emotion classes, and distinguishing semantically similar emotions is difficult.

To address these, we propose MultiEMO, with four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. Our contributions include:
1. **VisExtNet**: A novel visual feature extractor capturing facial expressions without redundant scene information, which existing methods encode unnecessarily. VisExtNet is made of an MTCNN and a VGGface2 pre-trained ResNet-101.
2. **MultiAttn**: A multimodal fusion model based on bidirectional multi-head cross-attention layers, modeling complicated correlations across modalities. It integrates each modality with complementary information from others through stacked layers.
3. **Sample-Weighted Focal Contrastive (SWFC) loss**: Addresses the difficulty of classifying minority and semantically similar emotion classes by assigning higher importance to hard-to-classify minority classes and maximizing inter-class distances.

Experimental results on MELD and IEMOCAP datasets demonstrate that MultiEMO achieves state-of-the-art performances, with significant improvements in minority and semantically similar classes. A case study shows MultiEMO can handle asynchronization of emotional tendencies from different modalities.

Limitations include VisExtNet not distinguishing between speakers and irrelevant people, SWFC loss requiring a large batch size on MELD, and minority emotion performances still being worse than majority classes.</sample>
    <sample id="281">Questo lavoro indaga la dipendenza dal contesto nella traduzione, un aspetto cruciale spesso trascurato dalle metriche tradizionali come il BLEU, a causa del numero limitato di parole che richiedono un contesto e della mancanza di risorse per l'analisi multilingue.

Per affrontare la domanda "Quando la traduzione richiede il contesto?", viene introdotta la Conditional Cross-Mutual Information (CXMI), che misura la quantità di informazione che il contesto C fornisce sul target Y, data la sorgente X. Ciò significa che la CXMI indica quanto l'incertezza sulla traduzione diminuisce quando il contesto è disponibile. Viene introdotta anche la Pointwise CXMI (P-CXMI) per misurare l'uso del contesto a livello di frase o di parola. Parole con elevata P-CXMI sono considerate quelle che richiedono un contesto per essere tradotte.

L'analisi tematica delle parole con elevata P-CXMI su trascrizioni di TED Talks tradotte da inglese a 14 lingue diverse rivela schemi interessanti. A livello di parti del discorso (POS), i pronomi duali in arabo mostrano un'elevata P-CXMI perché l'inglese non ha pronomi duali, rendendo necessario il contesto per determinarli. Allo stesso modo, alcune lingue richiedono il contesto per scegliere la forma verbale appropriata. A livello di elementi del vocabolario, la coesione lessicale richiede contesto per tradurre correttamente i nomi propri, garantendo l'uso della stessa traduzione all'interno del documento. Anche la formalità richiede il contesto. Infine, a livello di singoli token, fenomeni come la risoluzione dell'ellissi, che non possono essere catturati dalla parola stessa ma sono espressi nella struttura della frase, richiedono il contesto.

Per rispondere alla seconda domanda "Quanto bene i modelli gestiscono le traduzioni dipendenti dal contesto?", viene proposto un benchmark Dataset-agnostic Multilingual Discourse-Aware (MuDA). Per ciascuno dei cinque fenomeni discorsivi identificati (pronomi, forma verbale, coesione lessicale, formalità ed ellissi), vengono creati tagger per identificare automaticamente le parole che rientrano in tali fenomeni. L'applicazione di metriche di traduzione come BLEU, COMET o F-measure su esempi dipendenti dal contesto identificati dal tagger MuDA rivela che le metriche a livello di corpus da sole non sono sufficienti per determinare il miglior sistema di traduzione a livello di documento.

I risultati del benchmark MuDA indicano che i modelli consapevoli del contesto si comportano significativamente meglio in alcuni fenomeni, come la formalità e la coesione lessicale. Tuttavia, non mostrano un miglioramento significativo rispetto ai modelli che non utilizzano il contesto per altri fenomeni come l'ellissi, i pronomi e la forma verbale. Questo suggerisce aree specifiche in cui sono necessari ulteriori progressi per la traduzione a livello di documento. Il benchmark MuDA ha anche mostrato che DeepL generalmente supera Google Translate nella traduzione a livello di documento.</sample>
    <sample id="282">In questo video, Xuekai Zhu introduce StoryTrans, un modello per il trasferimento non parallelo dello stile dell'autore del racconto con rappresentazioni del discorso e miglioramento del contenuto. Il compito di trasferimento dello stile dell'autore del racconto a livello di discorso presenta due sfide principali: l'imitazione delle scelte linguistiche dell'autore a livello di discorso e l'elevata associazione degli stili dell'autore con argomenti di scrittura specifici.

Per affrontare questi problemi, Zhu propone StoryTrans, un modello generativo che apprende le rappresentazioni del discorso dal testo sorgente e le combina con gli incorporamenti di stile appresi per generare testo nello stile di destinazione. Un nuovo obiettivo di addestramento è stato progettato per ridurre le caratteristiche specifiche dello stile dalle rappresentazioni del discorso, avvicinando le incorporazioni derivate da testi diversi nello spazio latente. Per migliorare la conservazione del contenuto, la generazione è separata in due fasi: in primo luogo, il testo sorgente viene trasferito con i contenuti specifici dello stile mascherati da parole chiave; in secondo luogo, il testo completo viene generato incorporando queste parole chiave specifiche dello stile.

Il framework di addestramento prevede anche due fasi. Nella prima fase, un framework di addestramento avversariale utilizza una perdita di auto-ricostruzione per recuperare l'input, una perdita di disarticolazione applicata agli incorporamenti di frase per disarticolare lo stile e il contenuto a livello di frase, una perdita dell'ordine delle frasi per catturare le dipendenze a livello di frase e una perdita del classificatore di stile per produrre segnali di stile per l'intero sistema. La seconda fase non è correlata al trasferimento di stile, ma mira a riempire il contenuto specifico dello stile corretto e a rimuovere i token mascherati.

I risultati della valutazione automatica e manuale su set di dati cinesi e inglesi confermano l'efficacia del modello. I risultati della visualizzazione dello stile indicano che i testi trasferiti da StoryTrans sono allineati con i testi dorati nello spazio delle caratteristiche dello stile, superando i modelli di base in termini di controllo dello stile e conservazione del contenuto.

StoryTrans può riscrivere la maggior parte delle frasi con lo stile target e mantenere le semantiche sorgente. Maggiori informazioni sul lavoro sono disponibili nel repository GitHub specificato.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata nel video è la struttura "Conjunction-headed/Prague", che include il nome della città "Prague".</sample>
    <sample id="284">Il relatore presenta FSUIT, un nuovo meccanismo di "fuzzy span" per migliorare l'estrazione universale delle informazioni (UIE). Il relatore nota che i modelli UIE esistenti si basano eccessivamente sulla posizione del confine dello "span" annotato, ma nell'annotazione dei confini degli "span" esiste ambiguità. Propone che il confine dello "span" appreso dal modello dovrebbe essere "fuzzy" invece che preciso. Inoltre, rileva una disallineamento tra l'estrazione delle caratteristiche del Transformer (che si concentra sulle caratteristiche globali) e l'estrazione delle informazioni (che si concentra sulle caratteristiche locali). Per risolvere questo problema, propone che l'attenzione utilizzata per la decisione di estrazione dello "span" dovrebbe essere adattiva piuttosto che statica.

Il relatore introduce un "fuzzy span loss" per modellare il confine dello "span" "fuzzy" come una distribuzione continua di probabilità corretta in un intervallo specifico. Questa distribuzione continua viene convertita in un gruppo di valori discreti per il calcolo del "fuzzy span loss". La funzione di perdita combina l'entropia binaria incrociata (BCE loss) con il "fuzzy boundary" e la divergenza KL tra il "predicted boundary" e il "fuzzy span boundary".

Per ottenere una distribuzione di attenzione più ragionevole per l'estrazione dello "span", il relatore propone una "fuzzy span attention" come funzione di maschera per regolare la distribuzione di attenzione. Questa funzione di maschera riflette il "fuzzy span" in due aspetti: primo, un parametro ottimizzabile chiamato "delta" viene introdotto per regolare dinamicamente la lunghezza dell'intervallo di attenzione completo; secondo, la distribuzione di attenzione sul confine dello "span" di attenzione decade linearmente invece di essere troncata.

Il modello FSUIT incorpora lo strato di "fuzzy span attention" solo al livello superiore per guidare il processo di decisione del modello senza compromettere la capacità di codifica del testo. I risultati degli esperimenti su tre principali attività di estrazione delle informazioni (riconoscimento di entità nominate - NER, estrazione di relazioni - RE e estrazione di triplette di sentimenti aspetti - ASTE) mostrano che FSUIT-base ottiene un significativo miglioramento delle prestazioni rispetto a UIE-base (senza il meccanismo di "fuzzy span"). In particolare, sulle attività NER, FSUIT-base e FSUIT-large superano UIE-base con un margine considerevole. Sulle attività RE, FSUIT ottiene nuovi risultati all'avanguardia su vari dataset, dimostrando una migliore capacità di estrazione delle informazioni con una struttura più semplice e capacità di generalizzazione più forti per informazioni specifiche del dominio. Per l'attività ASTE, FSUIT mostra prestazioni migliori con una struttura di estrazione unificata rispetto a strutture separate.

Studi di ablazione indicano che il meccanismo di "fuzzy span attention" (FSA) accelera la velocità di convergenza, mentre il "fuzzy span loss" (FSL) consente al modello di utilizzare appieno le informazioni di annotazione, portando a una maggiore capacità di estrazione delle informazioni. L'effetto combinato dei due produce un miglioramento ancora maggiore. La visualizzazione della distribuzione dei punteggi di attenzione nello strato FSA rivela che il modello si concentra sulle informazioni semantiche entro un intervallo limitato di "tokens" precedenti, piuttosto che sulla rappresentazione globale. Questo è in linea con le aspettative del relatore.

In conclusione, il lavoro introduce FSUIT, che propone un innovativo "fuzzy span loss" per attenuare la dipendenza del modello dai confini degli "span", e utilizza un'efficiente "fuzzy span attention" per regolare in modo adattivo lo "span" di attenzione e guidare la corretta distribuzione dell'attenzione. FSUIT raggiunge risultati eccellenti in un'ampia gamma di attività IE, tra cui NER, RE e ASTE.</sample>
    <sample id="285">Il video introduce un lavoro sulle correzioni degli errori fattuali nella riassunzione dei dialoghi. L'oratore spiega che i riassunti generati dai modelli, e persino alcuni riassunti di riferimento, contengono ancora errori fattuali. Vengono proposte due soluzioni: progettare modelli di riassunto migliori per la fattualità o un modello di correzione degli errori fattuali (FEC) indipendente dal modello di riassunto.

Tuttavia, gli attuali modelli FEC sono valutati in modo inadeguato. Le metriche di fattualità attuali non sono affidabili, fornendo solo un punteggio generale che maschera le vere prestazioni del modello FEC. Il modello FEC può anche ignorare il riassunto originale e generare un nuovo riassunto, senza effettivamente correggere alcun errore.

Per risolvere questi problemi, il lavoro propone di annotare manualmente le "correzioni di riferimento" per i riassunti generati dal modello che contengono errori fattuali. Ciò fornisce dati più preziosi per addestrare i modelli FEC e consente una valutazione più completa e accurata.

Viene introdotta una nuova tassonomia di errori fattuali, basata sia sulla forma che sul contenuto. La classificazione basata sul contenuto si basa sulla parte del discorso e sulle dipendenze, mentre la classificazione basata sulla forma si concentra su operazioni di aggiunta, eliminazione e sostituzione.

Viene proposto un nuovo framework di valutazione basato su ERRANT per le correzioni degli errori fattuali, che comprende tre fasi: allineamento, classificazione e confronto. Questo framework può essere utilizzato per valutare i modelli FEC in base a diversi tipi di errori.

Gli esperimenti sui modelli FEC rivelano che l'addestramento con riassunti di riferimento dai dataset di riassunto del dialogo produce i migliori risultati secondo le metriche di fattualità non affidabili. C'è una necessità urgente di cambiare i metodi di valutazione per i modelli FEC. L'introduzione di riassunti corretti dall'uomo durante l'addestramento dei modelli FEC può migliorare le loro prestazioni, suggerendo che la combinazione di dati annotati dall'uomo con dati sintetici è una direzione promettente. I modelli FEC attuali hanno difficoltà a correggere gli errori fattuali di aggiunta e non possono affrontare errori di attributo, errori di modalità, errori di collegamento, ecc.</sample>
    <sample id="286">I relatori sono James Finch e Sarah Finch.</sample>
    <sample id="287">Quattro autori.</sample>
    <sample id="288">BLiMP e SyntaxGym.</sample>
    <sample id="290">Le abbreviazioni sono FTw, BOND, COSINE, MLC e L2R.</sample>
    <sample id="291">Il modello è valutato su attività di riconoscimento di entità nominate, classificazione, part-of-speech tagging e question answering.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su CamemBERT generico.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Il relatore, Valerio Basile, presenta un lavoro di collaborazione tra l'Università di Torino e Amazon Alexa.
La comprensione del linguaggio naturale è basata in gran parte sull'apprendimento automatico supervisionato. Questo richiede grandi quantità di dati annotati manualmente che contengano la conoscenza umana. Tuttavia, il paradigma della "ground truth" (verità fondamentale) mostra i suoi limiti, specialmente in compiti soggettivi.

Il team si è concentrato sull'ironia, un fenomeno altamente latente e pragmatico nel linguaggio naturale. La rilevazione dell'ironia è un compito difficile per i modelli di elaborazione del linguaggio naturale moderni. Il team desidera avere modelli che abbiano un output più informativo, cioè che possano dire "questa frase potrebbe essere considerata ironica da chi?" piuttosto che semplicemente "questa frase è [non] ironica".

Per studiare questi problemi, hanno sviluppato un corpus chiamato EPIC (English Perspectivist Irony Corpus). Hanno raccolto circa 3.000 brevi conversazioni (coppie di testo/risposta) da Reddit e Twitter, coprendo cinque varietà di inglese (Regno Unito, Stati Uniti, Irlanda, Australia, India) tra gennaio 2020 e giugno 2021.

Il processo di annotazione ha coinvolto circa 15 annotatori per varietà (74 in totale) tramite la piattaforma di crowdsourcing Prolific. Ogni annotatore ha ricevuto 200 testi e in media si sono ottenute 5 annotazioni per testo. La composizione degli annotatori è stata bilanciata per genere e paese di residenza. Gli annotatori dovevano annotare istanze da tutte le varietà di inglese, non solo quelle che parlavano.

L'interfaccia di annotazione era semplice: mostrava un messaggio e la sua risposta, e l'annotatore doveva scegliere se la risposta era ironica o meno.

Sono state notate alcune differenze nell'accordo inter-annotatore (IAA) tra i diversi gruppi (genere, età, nazionalità, ecc.). Il team ha cercato di modellare queste differenze creando modelli "perspective-aware" (sensibili alla prospettiva), ovvero addestrando modelli diversi su sottoinsiemi di dati basati sulle prospettive degli annotatori.

In termini di prestazioni pure, non sono state notate tendenze particolari tra i modelli "perspective-aware" e quelli basati sulla "gold standard" (modelli aggregati). Tuttavia, i modelli "perspective-aware" tendono a prendere decisioni con meno incertezza e sono più fiduciosi quando vengono testati su un set di test rappresentativo della loro prospettiva.

Il team ha anche esaminato le cause della variazione nella percezione dell'ironia e ha trovato qualcosa di particolare: nel caso dell'età, sono le generazioni più vicine tra loro che sembrano essere più in disaccordo sulla percezione dell'ironia. Risultati simili sono stati ottenuti per la distribuzione geografica degli annotatori, con la maggiore variazione tra il Regno Unito e l'Irlanda.</sample>
    <sample id="297">In questo video viene illustrato il lavoro svolto in "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models". L'oratore introduce il concetto di "dogwhistle" attraverso un esempio di discorso del senatore Josh Hawley, in cui la parola "cosmopolitan" può essere interpretata in modi diversi da diversi gruppi di persone. L'oratore sottolinea che la comprensione dei "dogwhistle" è importante per i linguisti, poiché il loro significato dipende dal contesto e da più pubblici.

Il progetto include una tipologia e un glossario di oltre 340 termini e simboli (incluse le emoji) che funzionano come "dogwhistle", con un focus su termini razzisti, transfobici e antisemiti. I termini sono classificati in base a registro, tipo e persona. Il progetto include anche uno studio di caso sugli interventi politici storici degli Stati Uniti. Si scopre che l'uso dei "dogwhistle" razziali è aumentato dagli anni '60 e che sono più associati al conservatorismo.

In seguito, viene esaminata la capacità dei modelli linguistici di rilevare i "dogwhistle". A GPT-3 sono state fornite definizioni e modi per chiedere esempi. GPT-3 è in grado di identificare molti dei "dogwhistle" presenti nel glossario, in particolare quelli di registro formale. Tuttavia, le sue prestazioni variano, mostrando risultati inferiori per i "dogwhistle" informali/online e per quelli transfobici. Infine, viene dimostrato come i "dogwhistle" possano eludere la moderazione dei contenuti. I "dogwhistle" vengono utilizzati per sostituire le etichette di gruppo o gli insulti in frasi di incitamento all'odio. Ne risulta che il contenuto con i "dogwhistle" viene classificato come meno tossico dalle API di Perspective. Il video conclude con un riepilogo del progetto, che evidenzia la tipologia, il glossario, lo studio di caso e la capacità dei "dogwhistle" di eludere la moderazione dei contenuti.</sample>
    <sample id="298">Il video mostra un grafico che illustra la degradazione delle prestazioni con un gap temporale maggiore, indicando che la deriva temporale è la causa principale della perdita di prestazioni, invece di un eccessivo adattamento adattivo.</sample>
    <sample id="299">L'audio discute il miglioramento della robustezza dei modelli NLI attraverso l'allenamento minimax. Nonostante i risultati di punta, i modelli NLI fanno affidamento su scorciatoie (correlazioni spurie tra input e etichette) che non generalizzano bene su campioni fuori distribuzione. I metodi di mitigazione delle scorciatoie esistenti hanno limitazioni, come la necessità di conoscenze a priori sulle scorciatoie, l'instabilità dell'allenamento e l'overhead computazionale.

Per superare queste limitazioni, l'autore propone un metodo di allenamento minimax. L'idea principale è imparare una distribuzione di pesi degli esempi che enfatizzi gli esempi difficili sottorappresentati. Il metodo coinvolge un discente che ottimizza il compito NLI e un ausiliario che massimizza la perdita del discente, aumentando il peso degli esempi difficili. Questo costringe il discente a concentrarsi sulle aree dello spazio di input con perdite elevate.

Il vantaggio di questo approccio è che non richiede alcuna assunzione a priori sulle scorciatoie, si basa sulle dinamiche di allenamento del discente e utilizza una rete feed-forward come ausiliario. Sperimentalmente, l'allenamento minimax migliora costantemente le prestazioni fuori distribuzione mantenendo un'elevata accuratezza di distribuzione. La presentazione conclude invitando a discutere ulteriormente il lavoro durante la sessione di poster.</sample>
    <sample id="300">In this presentation, Belinda Z. Li introduces a new task called interactive dictation, which allows users to dictate and edit a document using their voice in a natural and intuitive manner. Most existing speech-to-text systems only support dictation and do not support invoking edits through vocal commands. While some software recognizes vocal edit commands, they are often unintuitive, requiring users to memorize a fixed set of template commands. The interactive dictation task is characterized by flexible interleaving of dictation and editing, with no reserved trigger words for invoking commands. The challenge is predicting segmentation between dictation and editing commands. It also uses intuitive and open-ended natural language utterances to specify edits, with the challenge being interpreting which command to invoke and where/how. The task is broken down into a four-step procedure: ASR, segmentation, normalization, and interpretation. For the segmentation model, the T5-base encoder only shows 85.3% exact match with a per-sample runtime of 0.097 s/it. For the ASR repair and interpretation models, a tradeoff exists between runtime and accuracy. GPT3 models are more accurate but slower. Overall, directly predicting the state is much more accurate than predicting intermediate programs for GPT3 models.</sample>
    <sample id="302">La permutazione è necessaria perché i token di output vengono prodotti come un multiset non ordinato nel primo passaggio del modello. La fase di permutazione li ordina nella sequenza corretta.</sample>
    <sample id="303">Gli autori hanno suggerito ai proprietari dei modelli di aumentare la trasparenza sui metodi di mitigazione dei bias per comprendere meglio i modelli di stereotipi dannosi.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima sono un tipo di input di un modello linguistico che è stato leggermente modificato da un input accettabile per renderlo non grammaticale o non accettabile in qualche modo.</sample>
    <sample id="305">Dawei Zhu, studente di dottorato alla Saarland University, presenta "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Spiega che la supervisione debole riduce il collo di bottiglia dell'annotazione, ma le etichette deboli sono rumorose e i modelli possono memorizzare il rumore. Gli algoritmi di apprendimento supervisionato debole (WSL) sono stati proposti per affrontare il rumore, ma i modelli necessitano comunque di una generalizzazione.

Un'affermazione comune nei recenti lavori WSL è che i modelli vengono addestrati solo su dati supervisionati debolmente e raggiungono un'elevata precisione. Tuttavia, esiste un problema. I modelli richiedono un set di validazione pulito e etichettato manualmente per la selezione del modello. Questa necessità è spesso trascurata.

Per affrontare questo problema, sono state poste tre domande di ricerca:
1. I dati di convalida puliti sono necessari per i modelli WSL?
2. Quanti campioni puliti sono necessari per gli approcci WSL?
3. Come utilizzare in modo più efficiente i campioni puliti disponibili?

I risultati mostrano che:
1. I metodi WSL richiedono un set di validazione pulito per funzionare correttamente.
2. L'aumento dei campioni di validazione puliti porta a prestazioni migliori per gli approcci WSL. Tuttavia, se i campioni puliti sono disponibili, l'addestramento diretto su di essi (fine-tuning) porta a prestazioni ancora migliori.
3. Il fine-tuning continuo sui campioni puliti può eliminare i divari di prestazioni tra gli approcci WSL. Ciò implica che non è necessario utilizzare metodi WSL complessi che richiedono più tempo di calcolo e spazio su disco.

In conclusione, i recenti approcci WSL richiedono campioni puliti e sovrastimano la loro praticità. I ricercatori raccomandano di riferire i criteri di selezione del modello, utilizzare approcci di apprendimento few-shot come linee di base e applicare sempre il fine-tuning continuo.</sample>
    <sample id="306">Il relatore Sebastian Schuster ha introdotto il suo lavoro con Najoung Kim sul tracciamento delle entità nei modelli linguistici. Ha sottolineato che la comprensione del discorso richiede il tracciamento delle entità e il modo in cui il loro stato cambia man mano che il discorso si svolge. Ad esempio, in una ricetta, è fondamentale per un agente capire che le uova, lo zucchero e la farina finiscono in una ciotola dopo l'istruzione di metterli lì, e che successivamente diventano parte di una pastella dopo essere stati mescolati.

Schuster ha anche evidenziato le sfide nella valutazione della capacità dei modelli linguistici di tracciare le entità. Un problema è che alcuni stati delle entità sono comuni nei dati di pre-addestramento, il che significa che un modello potrebbe prevedere lo stato corretto senza avere effettivamente capacità di tracciamento delle entità. Un'altra sfida è che gli stati delle entità possono essere previsti da singole parole o frasi senza considerare il discorso più ampio, il che potrebbe dare l'impressione che un modello stia eseguendo il tracciamento delle entità mentre in realtà sta solo applicando euristiche. Infine, se si utilizza il fine-tuning o le dimostrazioni in-context per interrogare il modello, esso potrebbe memorizzare sequenze di stati delle entità o imparare a utilizzare euristiche come il riempimento degli slot.

Najoung Kim ha poi illustrato l'impostazione del compito per valutare le capacità di tracciamento delle entità. Il compito prevedeva scatole e oggetti, e l'input al modello iniziava con una descrizione del contenuto iniziale di ciascuna scatola. Il compito del modello linguistico era completare l'input prevedendo il contenuto di ciascuna scatola. Per i casi in cui gli oggetti vengono spostati o aggiunti a una scatola, il modello doveva combinare la descrizione iniziale con le operazioni per fare la previsione corretta. Sono state implementate varie misure per impedire al modello di utilizzare euristiche.

I risultati sperimentali con i modelli Flan-T5 e GPT-3/3.5 hanno mostrato che la maggior parte dei modelli si limita a ripetere lo stato iniziale. Solo il modello GPT-3.5 text-davinci-003 ha mostrato un tracciamento delle entità non banale. I modelli GPT-3.5, che sono stati addestrati su una quantità sostanziale di codice, hanno mostrato capacità di tracciamento delle entità, suggerendo che il pre-addestramento sul codice è fondamentale per far emergere questa capacità. È stato anche scoperto che i modelli più piccoli, come T5-base, possono imparare a eseguire il tracciamento delle entità se vengono affinati direttamente, mentre i modelli inizializzati casualmente delle stesse dimensioni non imparano questo comportamento. Tuttavia, resta poco chiaro in che misura le capacità di tracciamento delle entità osservate si generalizzino oltre la configurazione delle scatole.</sample>
    <sample id="307">Gli autori hanno utilizzato F1, Hamming ed EMR come metriche di valutazione.</sample>
    <sample id="308">In this presentation, Jenny Liang introduces "NLPositionality," a framework for characterizing design biases in NLP datasets and models, a collaboration between Carnegie Mellon University, University of Washington, and the Allen Institute for AI. She explains that design biases can occur due to the positionality of NLP researchers and model developers, highlighting that models and datasets aggregate judgments and opinions of real people, thus representing certain positionalities over others. Previous work has provided anecdotal evidence of positionality in NLP through model and dataset probing, as well as theoretical definitions of model positionality. However, these studies do not compare end-users with the datasets and models themselves.

Liang states that NLPositionality addresses this gap by re-annotating datasets with diverse annotators and comparing these annotations, by demographic, to models and datasets via Pearson's R scores. This framework is enabled through "LabintheWild," an online crowdsourcing platform for HCI collaborators that recruits diverse volunteers from various countries and collects high-quality data. Two tasks are hosted on LabintheWild: Task A, "Social Acceptability," where participants rate the social acceptability of a situation from the Social Chemistry dataset; and Task B, "Toxicity," where participants rate whether an instance from the Dynahate dataset is hate speech. Liang’s study amassed over 16,000 annotations from more than 1,000 annotators across 87 countries.

The results show that there is positionality in NLP. Datasets and models are most aligned with English-speaking countries and people with a college education. Conversely, datasets and models are less aligned with non-binary individuals. Liang provides recommendations to address positionality in NLP, including:
1. Keeping a record of all relevant design choices throughout building datasets or models.
2. Doing NLP research through the lens of perspectivism, which involves sharing disaggregated dataset labels and using modeling techniques that can handle annotator disagreement.
3. Building specialized datasets and models with and for specific communities. She cites the Masakhane initiative as a good example of this approach.</sample>
    <sample id="309">L'accordo tra gli annotatori è stato misurato utilizzando l'Alfa di Krippendorff.</sample>
    <sample id="310">Wikipedia è stata scelta per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili.</sample>
    <sample id="311">Gli autori dell'articolo sono affiliati alla Heinrich Heine University di Düsseldorf, in Germania.</sample>
    <sample id="312">MultiInstruct è il primo dataset di benchmark per l'ottimizzazione delle istruzioni multimodali. Include 62 task multimodali da 10 ampie categorie e ciascuno è dotato di cinque istruzioni scritte da esperti.</sample>
    <sample id="313">Ci sono tre autori coinvolti nell'articolo.</sample>
    <sample id="314">La coordinazione binaria è quando ci sono due congiunzioni in un testo.</sample>
    <sample id="315">I prompt sono stati utilizzati per circa 7 minuti.</sample>
    <sample id="316">Il modello T5 più piccolo, ottimizzato su Coscript, può generare script di qualità superiore rispetto ai modelli di linguaggio più grandi. Ciò indica che i modelli più piccoli possono superare i modelli più grandi se adeguatamente addestrati su set di dati appropriati.</sample>
    <sample id="317">CodeIE presents an innovative approach to information extraction, transforming the text-to-structured task into a structured-to-structured code generation task using Code-LLMs. This method addresses the problem of mismatched input and output formats that challenges traditional text-to-text generation models.

Previous methods in information extraction, using models like T5 and GPT-3, typically involve fine-tuning on large datasets, but during inference, the structured output must be linearized into plain text. This linearization creates a mismatch between the aligned input format used in pre-training and the mismatched structured output format required for information extraction, making it difficult for models to generate the correct structures without extensive training data and specialized decoding strategies.

CodeIE tackles this by adopting a Struct2Struct approach. By defining functions for named entity recognition (NER) and relation extraction (RE) within a code prompt, the model is trained to output structured code. For example, for an input text like "Steve became CEO of Apple in 1998", the NER task is framed as a Python function that appends extracted entities (e.g., "Steve": person, "Apple": organization) to a list. This approach aligns the input and output formats and leverages Code-LLMs like Codex, which are adept at generating structured code.

Experimental results on NER and RE benchmarks show that CodeIE consistently outperforms traditional baseline models, including UIE, T5, and GPT-3, especially in few-shot scenarios. The analysis reveals that the perplexity for text-formatted inputs is generally higher than for code-formatted inputs, indicating better alignment with information extraction tasks. Furthermore, while traditional text-based prompts often lead to structural errors, CodeIE with code-formatted prompts shows almost no structural errors. CodeIE also demonstrates superior precision and recall in RE tasks.

In summary, CodeIE offers a robust and effective solution for few-shot information extraction by re-framing the task as code generation and utilizing the strengths of Code-LLMs, thereby improving format consistency and structural fidelity in the extracted information.</sample>
    <sample id="318">00:00 - Ciao, sono Yanis Labrak e vi presenterò i nostri lavori su DrBERT, un modello robusto pre-addestrato in francese per i domini biomedici e clinici.
00:09 - In questa presentazione, parleremo prima della modellazione del linguaggio in ambito sanitario. Poi presenteremo il contributo principale del nostro articolo. Introdurremo il primo modello biomedico in francese, chiamato DrBERT, basato su Roberta e addestrato su NACHOS, un dataset di dati medici estratti dal web. Introdurremo anche un confronto tra modelli con diverse impostazioni di pre-addestramento e fonti di dati. Poi presenteremo i nostri risultati su 11 task biomedici e clinici in francese. E infine, concluderemo sugli esperimenti e vi daremo maggiori dettagli su come accedere ai modelli.
00:48 - Dalla sua pubblicazione nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere task di elaborazione del linguaggio naturale. E offre un enorme guadagno di prestazioni rispetto ai metodi storici statici e contestuali come Word2Vec, FastText o ELMo. Da allora, questo modello è stato adattato a molte altre lingue, come il francese con CamemBERT, e ad altri domini come il biomedico con PubMedBERT e BioBERT, e il clinico con ClinicalBERT, ma principalmente in inglese. Modelli specializzati per altre lingue sono rari e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati specifici del dominio. Tuttavia, il francese non aveva ancora un modello open-source per il dominio biomedico e clinico.
01:36 - Quindi ci siamo posti delle domande su quale sia la fonte di dati più appropriata per un'ampia gamma di utilizzi, e se i dati estratti dal web siano un buon sostituto per i dati clinici. Per rispondere a queste domande, abbiamo prima addestrato e confrontato quattro modelli da zero. Una prima versione di DrBERT con 7 GB di NACHOS, una seconda versione di 4 GB di NACHOS, una prima versione di ChuBERT, che è un modello clinico, con 4 GB di frasi prese da note cliniche. E una versione finale di ChuBERT con un mix di 4 GB di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con un pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sui pesi di CamemBERT e addestrato su 4 GB di NACHOS, un altro anch'esso basato su CamemBERT, ma addestrato questa volta sui 4 GB di note cliniche. E infine, uno basato su un modello biomedico inglese, PubMedBERT, e addestrato su 4 GB di NACHOS. In totale abbiamo sette modelli.
03:02 - Per valutare i nostri sette modelli, abbiamo raccolto 11 task sia pubblici che privati, come il riconoscimento di entità nominate, la classificazione, l'annotazione POS e la risposta a domande. Questi modelli sono confrontati con sei modelli di base, che sono CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli ottengono le migliori prestazioni sui task con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. Osserviamo anche che l'utilizzo di più dati si traduce in prestazioni migliori.
03:51 - In generale, l'addestramento da zero sembra ottenere prestazioni migliori sulla maggior parte dei task. Tuttavia, i nostri esperimenti sul pre-addestramento continuo, utilizzando i pesi e il tokenizer di PubMedBERT addestrato sul sottoinsieme di 4 GB di NACHOS, mostrano risultati paragonabili a quelli ottenuti con DrBERT 4 GB da zero, cosa che non è il caso per il modello basato sui pesi e il tokenizer di CamemBERT, che soffre di problemi di stabilità.
04:18 - Infine, come conclusione, il nostro sistema proposto offre prestazioni migliori su 9 degli 11 task orientati al settore medico francese e supera globalmente i risultati del modello generico qui CamemBERT. Abbiamo anche osservato che i dati specializzati sono migliori, più dati specializzati sono migliori, ma non si adattano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono liberamente disponibili su Hugging Face e tutti gli script di addestramento sono sul nostro repository GitHub.
04:50 - Quindi, grazie per questa presentazione, e non vediamo l'ora di scambiare idee alla sessione poster a Toronto.</sample>
    <sample id="319">Nel lavoro vengono esaminate due strategie di apprendimento: l'addestramento da zero (from scratch) con costruzione completa del modello e il pre-addestramento continuo (continual pre-training) utilizzando un modello pre-addestrato esistente.</sample>
    <sample id="320">Un fattore di overfitting superiore a 1 è la pendenza della linea di migliore adattamento della generalizzazione.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata utilizzando il punteggio SARI, BLEU, BS-P e FRE, come mostrato nelle tabelle a schermo.</sample>
    <sample id="322">Ciao a tutti, sono Enrico e presenterò all'ACL 2023, rispondendo alla domanda: "Cosa impara un classificatore di testo sulla moralità?".
Per prima cosa, vi spiego cos'è la moralità. La moralità umana è ciò che ci aiuta a distinguere il giusto dallo sbagliato, è la nostra bussola interna che ci aiuta a determinare se un'azione o un concetto è moralmente giusto o moralmente sbagliato. La moralità è alla base delle nostre società ed è essenziale che i modelli linguistici possano capire e riconoscere la moralità nel nostro linguaggio, nel testo.
La comprensione della moralità nel testo è già stata affrontata nella comunità NLP, ma è spesso e di solito trattata come una scala singolare tra immorale e morale, dove un concetto, una frase, può essere etichettato da qualche parte tra immorale e morale per il giudizio della moralità. Tuttavia, sappiamo che la moralità è molto soggettiva. Persone diverse etichetterebbero lo stesso concetto in modo diverso su questa scala. Ad esempio, concetti divisivi come l'aborto, i diritti LGBTQ; alcuni di noi etichetterebbero l'aborto come immorale e altri come morale. Semplicemente fare una media tra questi, o prendere un'aggregazione di maggioranza, nasconderebbe la verità, cioè che è un modo pluralistico e diverso di interpretare la moralità, e insegnare ai modelli linguistici solo la media è molto pericoloso.
In questa misura, ci sono teorie sociali che possiamo applicare alla comprensione della moralità umana. In particolare, una teoria molto popolare e consolidata è la cosiddetta Teoria della Fondazione Morale. Secondo questa teoria, ci sono cinque modi diversi in cui noi esseri umani percepiamo la moralità, tanto quanto ci sono cinque papille gustative nella nostra lingua. E ogni azione, ogni concetto, stuzzica un aspetto morale diverso, una fondazione morale diversa. E ognuno di noi, ogni essere umano, dà priorità a queste fondazioni in modi diversi. Per esempio, per alcuni di noi, l'equità è molto importante, e per altri, l'autorità è molto più importante. I diversi modi in cui diamo priorità a queste fondazioni determinano il modo in cui giudichiamo la moralità di un concetto o di un'azione.
La Teoria della Fondazione Morale è già stata usata nell'elaborazione del linguaggio naturale. C'è stata una serie di articoli negli ultimi due anni, inclusi articoli nostri, che tentano di capire e classificare la moralità nel testo. Ed è stato dimostrato che i modelli linguistici possono in qualche modo capire la moralità nel testo. Quindi, ciò che intendiamo fare in questo articolo è cercare di capire cosa imparano. Applichiamo tecniche di intelligenza artificiale spiegabile a questi modelli linguistici, cercando di capire la moralità nel testo. In particolare, ci concentriamo sul capire come la moralità è espressa in modo diverso tra diversi domini. Usiamo un dataset chiamato Moral Foundation Twitter Corpus, composto da 35.000 tweet, raccolti in sette diversi domini. Domini, ad esempio, corrispondenti all'hashtag #AllLivesMatter o all'hashtag #BlackLivesMatter. E cerchiamo di vedere se i modelli linguistici possono capire che la moralità è espressa in modo diverso in diversi domini. Tutti noi sappiamo, ovviamente, intuitivamente che il modo in cui la moralità è espressa, diciamo in ALM o in BLM, è molto diverso. Ma possono i modelli linguistici capire queste sottili differenze? Abbiamo proposto un metodo, abbiamo fatto una serie di esperimenti per rispondere a questa domanda, quindi vi darò solo una piccola anticipazione di questo.
Prendiamo l'esempio della differenza, infatti, tra ALM e BLM: #AllLivesMatter e #BlackLivesMatter. Questi due domini hanno una retorica simile, giusto? Perché coprono argomenti simili. Ma hanno una retorica significativamente diversa per l'elemento morale della sovversione, che significa ribellione all'autorità. Sappiamo, intuitivamente, e abbiamo scoperto che i modelli linguistici riconoscono che in ALM la sovversione è associata a parole che, sapete, come "rovesciare", "caos", e la sovversione è disapprovata. Mentre in BLM la sovversione è in qualche modo incoraggiata.
Quindi, i modelli linguistici riconoscono che la moralità può essere espressa in modo diverso. Ovviamente, ci sono diversi livelli di comprensione che esploriamo nell'articolo. E questi essenzialmente ci avvertono del fatto che, ovviamente, la moralità è espressa in modo diverso in diversi domini e usare un solo modello per molti domini diversi può effettivamente portare a incomprensioni della moralità in un modo molto pericoloso.
Spero di vedervi all'ACL, a Toronto. E ci vediamo lì, ciao!</sample>
    <sample id="323">Ciao a tutti, sono Yujie Wang della Shanxi University, Cina. Il titolo della mia presentazione è "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering". Il QA di buon senso è un compito difficile che richiede alle macchine di rispondere a domande che si basano sulla conoscenza comune per testare le loro abilità di comprensione linguistica. Ciò richiede alle macchine di recuperare la conoscenza pertinente da fonti esterne. La conoscenza è memorizzata sia nei modelli linguistici che nelle basi di conoscenza.

Il recupero di sottografi di conoscenza attraverso l'abbinamento di entità introduce alcune entità rumorose, come Top, Bank e Cat, che sono ampiamente irrilevanti per la domanda corrente. L'encoding del sottografo e del testo in isolamento porta a un'interazione limitata tra le due modalità. Il processo di encoding dei sottografi ignora le relazioni semantiche tra le entità.

Proponiamo DQLK. Per prima cosa, costruiamo un grafico di conoscenza eterogeneo (HKG) basato su più basi di conoscenza. In secondo luogo, mediante una strategia di pruning a due stadi e l'apprendimento della rappresentazione della conoscenza (KRL), ottimizziamo la struttura e la rappresentazione della conoscenza dell'HKG. Infine, implementiamo la fusione e l'encoding di due modalità attraverso il modello linguistico.

Per prima cosa, rimuoviamo le sottoparole che compongono l'entità frase usando il vocabolario del dizionario. Allo stesso tempo, recuperiamo le parafrasi delle entità chiave in WordNet e Wiktionary, e le colleghiamo come nodi aggiuntivi al sottografo, formando l'HKG. Utilizziamo RoBERTa e l'attenzione mascherata per codificare e fondere il contesto QA e le entità all'interno dell'HKG. Nel frattempo, rimuoviamo dinamicamente le entità con debole rilevanza per il contesto QA basandosi sui pesi di attenzione di RoBERTa per il sottografo ripulito. Per gli embedding iniziali di entità e relazione, otteniamo gli embedding di entità e relazione mediante mean pooling. Poiché l'HKG è composto da molteplici triplette, introduciamo TransE per ottimizzare gli embedding di entità e relazione nell'HKG. A differenza di altri lavori che utilizzano GNN per modellare i sottografi, applichiamo l'attenzione per modellare i sottografi. Ispirati da RGAT, introduciamo le relazioni nella Mask Self-Attention, creando RMSA. Iterando attraverso L strati di RMSA, aggiorniamo gli embedding di entità e relazione nell'HKG. Infine, otteniamo l'embedding del grafo dell'HKG applicando il max-pooling alle entità chiave della domanda.

Successivamente, incorporiamo le informazioni sul percorso dall'HKG nel contesto QA. Otteniamo la rappresentazione del contesto QA potenziata dal percorso, $ \tilde{Q}^p $. Per la previsione finale della risposta, inseriamo l'embedding del grafo HKG, l'embedding del contesto QA potenziato dal percorso e l'embedding del contesto QA nell'MLP per ottenere la probabilità di risposta.

Conduciamo esperimenti su CommonsenseQA e OpenBookQA. Usiamo tre basi di conoscenza, ConceptNet, WordNet e Wiktionary. Nel frattempo, estraiamo le entità chiave nel contesto QA basandosi su KeyBERT e recuperiamo i percorsi di conoscenza entro due salti in ConceptNet. Qui, riportiamo i risultati sui set di test ufficiali di CommonsenseQA e OpenBookQA. Rispetto ad altri modelli LM e HKG, il nostro modello ottiene buoni risultati. Grazie!</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi, occupando tutti e quattro i quadranti sulla bussola politica.</sample>
    <sample id="325">0:00 - Ciao, il mio nome è Matthias Lindemann e oggi vi darò una breve introduzione al nostro paper sulla Generalizzazione Composizionale senza Alberi, usando il tagging di multiset e le permutazioni latenti. Questo è un lavoro congiunto con i miei tutor, Alexander Koller e Ivan Titov.
0:21 - La Generalizzazione Composizionale può essere intesa come l'abilità di un apprendista di gestire una ricorsione più profonda e composizioni inaspettate di frasi che sono state viste individualmente durante l'addestramento.
0:34 - Nel contesto dell'analisi semantica, testare la generalizzazione composizionale potrebbe assomigliare a questo. Come al solito, abbiamo un insieme di addestramento di enunciati, in questo caso, "La ragazza dormiva" e "Mary sapeva che la ragazza dormiva". Questi enunciati sono accoppiati con forme logiche che rappresentano gli aspetti fondamentali del loro significato.
0:57 - A differenza della valutazione standard di machine learning, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento ed è testato su un esempio con una ricorsione più profonda.
1:16 - I modelli ingenui seq2seq falliscono!
1:16 - I modelli ingenui seq2seq faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output distaccati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio.
1:37 - Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi sono destinati a catturare il processo composizionale che mette in relazione gli enunciati con le forme logiche. Questo funziona bene, ma gli alberi di solito non sono dati e devono essere ottenuti in qualche modo. Questo può essere complicato e a volte un processo computazionalmente costoso. Tipicamente, questo comporta una considerevole pre-elaborazione specifica del formalismo delle forme logiche, ad esempio per gestire i simboli di variabile. L'ottenimento degli alberi può anche comportare procedure specializzate di induzione della grammatica.
2:16 - In questo paper, non usiamo alberi e introduciamo un modello neurale seq2seq che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione a una ricorsione più profonda senza alberi.
2:35 - Il nostro approccio predice l'output dall'input in due passaggi. Per prima cosa, etichettiamo ogni token di input con un multiset non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token giusti, ma non sono ordinati. Ecco perché, nel secondo passaggio, usiamo un altro modello per predire una permutazione per metterli nell'ordine giusto. Abbiamo introdotto un nuovo metodo per predire una permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio abbastanza flessibile ed espressivo.
3:14 - Concettualmente, il nostro modello di permutazione funziona all'incirca così. Andiamo da sinistra a destra sull'output e determiniamo quale token del multiset inserire in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente quello evidenziato in rosso. Poi, saltiamo al token del multiset successivo per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile, saltando a un altro token del multiset. Continuiamo questo processo finché ogni token del primo passaggio non è stato visitato esattamente una volta.
4:00 - Per darvi un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza albero nel benchmark COGS. Il nostro modello supera gli altri con un ampio margine sulla generalizzazione a ricorsioni più profonde. Altri tipi di generalizzazione strutturale rimangono comunque molto impegnativi.
4:21 - Nel nostro paper, risolviamo un paio di interessanti sfide tecniche. Prima di tutto, l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un dato token, non sappiamo da quale multiset proveniva, il che pone una sfida per l'addestramento. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma comporta la sfida che trovare la permutazione con il punteggio più alto è NP-difficile. Questo perché è legato al problema del commesso viaggiatore. Approssimiamo questo con un rilassamento continuo compatibile con la GPU che ci permette anche di retropropagare attraverso la soluzione e imparare le permutazioni linguisticamente più plausibili. Se volete saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, date un'occhiata al nostro paper o venite al nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è quando due elementi di cognizione (pensieri, azioni o credenze) sono incoerenti.</sample>
    <sample id="327">Il presentatore, Xiao Xu, studente di dottorato di terzo anno presso l'Harbin Institute of Technology, presenta il lavoro "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning" all'ACL 2023. Il lavoro è stato svolto durante il suo stage presso il gruppo MSR NLC e ringrazia l'Intel Cognitive Computing Group per il loro supporto e le discussioni.

La Vision-Language Learning (VLL) mira ad addestrare un sistema di intelligenza artificiale in grado di comprendere sia le immagini che il testo, utilizzando un pre-addestramento auto-supervisionato su larga scala su coppie immagine-testo. Dalla prospettiva dell'architettura modale, i recenti lavori VLL possono essere unificati nell'architettura a due torri, che consiste in un codificatore testuale, un codificatore visivo e un codificatore cross-modale. Alcuni modelli VLL, come BridgeTower, collegano più livelli uni-modali superiori a ciascun livello cross-modale in modo layer-by-layer per sfruttare la conoscenza semantica uni-modale a diversi livelli. Tuttavia, questo approccio è inefficace nell'utilizzo layer-by-layer e il numero di livelli cross-modali è legato al numero di rappresentazioni di livello uni-modale che utilizza, limitandone scalabilità e capacità.

Per superare queste limitazioni, viene proposta ManagerTower, una nuova architettura VLL che integra "manager" in ciascun livello cross-modale per raccogliere e combinare in modo adattivo le intuizioni di esperti uni-modali pre-addestrati a diversi livelli. ManagerTower adotta un approccio multi-layer per le rappresentazioni uni-modali e aggrega le intuizioni tramite i manager. I manager possono sfruttare in modo adattivo diversi livelli di conoscenza semantica uni-modale, facilitando un allineamento e una fusione cross-modale più completi.
I risultati sperimentali mostrano che, con solo 4 milioni di immagini per il pre-addestramento VLL, ManagerTower ottiene prestazioni superiori su varie attività a valle, come dimostrato dalla sua accuratezza del 79,15% sul test VQAv2 standard. Ciò evidenzia che ManagerTower, con i suoi manager ben progettati, consente un'esplorazione più efficace della conoscenza semantica uni-modale a diversi livelli. Inoltre, supera BridgeTower, che introduce i bridge in Meter, e sovraperforma anche alcuni modelli base-size addestrati su più dati o parametri. La visualizzazione dei pesi di aggregazione rivela che i manager adattivi in ManagerTower generano distribuzioni di pesi diverse tra i livelli cross-modali, a differenza delle distribuzioni progressive simili nei manager statici. Questo indica che i manager adattivi possono sfruttare in modo più flessibile i diversi livelli di conoscenza uni-modale per un apprendimento della rappresentazione cross-modale completo.</sample>
    <sample id="328">Il modello linguistico più liberale è GPT-4.</sample>
    <sample id="329">Questo lavoro introduce un nuovo metodo per la localizzazione di frasi video "zero-shot", ovvero senza annotazioni manuali. Questo approccio risolve i problemi dei metodi precedenti, che includevano la generazione di "pseudo-query" troppo semplici, un disallineamento tra gli "pseudo-eventi" e le "pseudo-query", e l'ignorare il "rumore" nelle "pseudo-label".

Il nuovo metodo presenta due aspetti principali: la generazione strutturata di "pseudo-label" (SPL) e la riduzione del rumore durante l'allenamento. Per la generazione SPL, vengono generate "pseudo-query" a forma libera usando modelli di descrizione di immagini pre-addestrati. Successivamente, vengono generati "pseudo-eventi" basati sulla struttura temporale dell'evento, assicurando un'elevata correlazione tra il video all'interno dell'evento e la "query", e una bassa correlazione al di fuori dell'evento. Le "pseudo-query" e gli "pseudo-eventi" a bassa qualità o con elevata sovrapposizione vengono filtrati.

Per ridurre il rumore, il metodo stima il rumore in base al punteggio di confidenza della previsione del modello e al IoU tra le previsioni e le "pseudo-label". Le "pseudo-label" rumorose vengono ripesate per ridurre il loro contributo alla perdita. Se la confidenza della previsione del modello è alta e il IoU è elevato, la previsione viene trattata come una nuova "pseudo-label" per il round di allenamento successivo.

Il metodo proposto, SPL, supera altri metodi "zero-shot" in diverse metriche e dataset. È robusto al rumore e non richiede annotazioni manuali, il che lo rende un approccio efficiente ed efficace per la localizzazione di frasi video "zero-shot".</sample>
    <sample id="330">Nell'apprendimento attivo, l'aggiornamento cumulativo del modello ha mostrato prestazioni uguali o migliori rispetto a quello iterativo in tutte le strategie testate.</sample>
    <sample id="331">Sara Papi.</sample>
    <sample id="332">I dati sono stati tratti dalle trascrizioni dei TED talks, tradotte dall'inglese in 14 lingue diverse.</sample>
    <sample id="333">Un ricercatore dell'Università di Nanjing ha presentato il loro lavoro intitolato "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation". Il relatore ha iniziato riconoscendo i collaboratori del lavoro provenienti dal Shanghai AI Lab, dall'Università di Nanjing e dall'Università di Hong Kong.

Il relatore ha quindi introdotto il problema dell'NMT (Neural Machine Translation), evidenziando che i modelli NMT spesso producono uno spazio di rappresentazione non uniforme, che può portare a problemi di definizione del significato semantico a causa della dispersione dei token a bassa frequenza. Ciò limita la capacità di generalizzazione dell'NMT.

Per affrontare questo problema, è stata proposta una soluzione precedente, kNN-MT (k-nearest neighbor machine translation), che mira a levigare le previsioni utilizzando i vicini più prossimi nello spazio di rappresentazione. Tuttavia, kNN-MT presenta due svantaggi: il recupero dei vicini da un grande datastore richiede molto tempo e le rappresentazioni non possono essere facilmente aggiornate una volta costruito il datastore.

Per superare questi limiti, il team ha proposto un nuovo framework di addestramento chiamato INK, che significa "Injecting kNN Knowledge into NMT". Il framework INK consiste in un ciclo di addestramento in due fasi:

1. **Raffinamento della rappresentazione**: la conoscenza kNN viene estratta dal datastore per guidare un adattatore nell'aggiustamento della rappresentazione.
2. **Aggiornamento asincrono**: le rappresentazioni aggiornate vengono utilizzate per aggiornare asincronamente il datastore.

Questo ciclo di addestramento continua fino alla convergenza. Durante l'inferenza, il datastore può essere eliminato, e solo il modello NMT di base e i parametri di adattamento ottimizzati devono essere caricati.

INK affronta il problema delle lacune semantiche allineando tre tipi di rappresentazioni utilizzando la divergenza KL:

- Allineamento delle rappresentazioni contestualizzate e degli embedding dei token per preservare il significato semantico.
- Allineamento delle rappresentazioni contestualizzate e degli embedding dei token kNN per arricchire il significato semantico.
- Allineamento delle rappresentazioni contestualizzate dello stesso token target per affrontare il problema della dispersione.

Il team ha condotto esperimenti su quattro set di dati benchmark (Medical, Law, IT, Koran) utilizzando un modello NMT vincitore del WMT'19 news translation task (tedesco-inglese) come modello NMT di base. I risultati mostrano che INK supera il sistema kNN-MT all'avanguardia e raggiunge le migliori prestazioni levigando lo spazio di rappresentazione. L'approccio di INK si traduce in un aumento medio di 1,99 nel punteggio COMET e 1,0 nel punteggio BLEU. Inoltre, INK raggiunge migliori prestazioni di traduzione con un utilizzo di memoria significativamente inferiore (0,02 volte) e una velocità di inferenza più rapida (1,9 volte) rispetto ai baseline kNN-MT.</sample>
    <sample id="335">Il nome del relatore è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è la traduzione di domande in più lingue naturali in più rappresentazioni di significato, utilizzando modelli neurali.</sample>
    <sample id="337">The presentation introduces "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," a neural approach inspired by how humans learn new words. When people encounter an unknown word, they often break it down into its constituent parts (like prefixes, roots, and suffixes) and associate these parts with known words to infer the new word's meaning.

To mimic this human learning process, the researchers developed a "Word Relationship Graph" (WRG). This graph represents words and their components (wordpieces) as nodes, connected by various linguistic relationships. When an out-of-vocabulary (OOV) word appears, it is tokenized into wordpieces, which are then naturally associated with relevant words in the graph, forming a two-level graph centered around the OOV word.

The model architecture first initializes OOV word nodes by assigning attributes based on their character-level features, using a self-attention network. Then, a multi-layer Graph Attention Network (GAT) is applied to the two-level WRG. This GAT processes the graph, extracting important information from the wordpieces and relevant words while mitigating noise from less significant neighbors. The initial input and the hidden embeddings from each GAT layer are concatenated and fused to create a comprehensive node-level representation.

To capture the overall graph information and summarize word formation, a readout block, specifically a Graph Convolutional Network (GCN), is employed to derive a graph-level representation. The model uses a contrastive learning loss function to encourage proximity between positive samples (e.g., two-hop relevant words, synonyms, or the OOV word itself) and push away negative samples.

Extensive experiments show that the GRM model outperforms state-of-the-art baselines in both intrinsic (word similarity and analogy) and extrinsic (named entity recognition and POS tagging) evaluation tasks. This demonstrates the effectiveness of learning OOV words through word formation. The model also shows adaptability, bringing improvements to both static and contextualized word embedding models in downstream tasks.

Finally, the researchers discuss the model's feasibility across different language types. It is particularly well-suited for agglutinative languages (like Japanese or Korean), where words are formed by directly stringing morphemes together, making word formation easy to explore. For fusional languages (like English), where morphemes are linked but harder to segment, the model still performs well with reasonable word segmentation. The effectiveness of GRM in other languages depends primarily on the rationality of word decomposition.</sample>
    <sample id="338">Questo video presenta un lavoro di ricerca intitolato "Le spiegazioni umane sono sempre utili? Verso una valutazione oggettiva delle spiegazioni umane in linguaggio naturale." Si tratta di un lavoro collaborativo di ricercatori del Rensselaer Polytechnic Institute, della Northeastern University e di IBM Research.

Il relatore introduce il problema della valutazione delle spiegazioni umane annotate, poiché possono essere soggettive e dipendenti dal compito, a differenza delle etichette. Le metriche NLG tradizionali si concentrano sulla somiglianza delle parole e trattano le annotazioni umane come uno standard aureo. Il punteggio di simulabilità misura il cambiamento delle prestazioni del modello di base quando vengono fornite o meno le spiegazioni, ma non considera le differenze tra i compiti o l'utilità delle spiegazioni nelle fasi di fine-tuning e inferenza.

Per superare queste limitazioni, i ricercatori propongono una struttura dati unificata basata su template che converte vari compiti in un compito a scelta multipla unificato. Questa struttura include impostazioni di base (senza spiegazione) e di infusione (con spiegazione come input).

I ricercatori hanno condotto esperimenti su due set di dati, ECQA e CoS-E, campionando nove sottoinsiemi di dati di training dal 10% al 100%. Hanno addestrato due modelli (T5 e BART) con le impostazioni di base e di infusione e hanno eseguito l'inferenza su entrambe le impostazioni. I risultati indicano che il fine-tuning non insegna ai modelli nuove conoscenze attraverso le spiegazioni, ma piuttosto li induce a fare affidamento sulla parte di spiegazione dell'input per la previsione. Le spiegazioni di CoS-E sono meno utili di quelle di ECQA per i modelli di base, il che sottolinea la natura dipendente dal compito delle spiegazioni. Il fine-tuning di un modello con una piccola quantità di dati che incorpora spiegazioni può portare a miglioramenti sostanziali.

Sulla base di queste osservazioni, i ricercatori hanno sviluppato una nuova metrica di valutazione chiamata TREU, che estende il punteggio di simulabilità considerando l'utilità delle spiegazioni nel fine-tuning. La valutazione di TREU e del punteggio di simulabilità su cinque set di dati e due modelli (T5 e BART) ha dimostrato che le spiegazioni di CoS-E sono ancora utili per i modelli, anche se gli esseri umani le ritengono di bassa qualità. TREU può riflettere fedelmente questa utilità, mentre il punteggio di simulabilità non è all'altezza. TREU classifica anche in modo coerente la qualità dei set di dati su entrambi i modelli, mentre il punteggio di simulabilità è più influenzato dai modelli.

In sintesi, i contributi di questo lavoro includono una struttura unificata per minimizzare l'influenza di diversi compiti e modelli, esperimenti preliminari per trovare la migliore utilità delle spiegazioni nei modelli, e la metrica TREU per valutare fedelmente l'utilità delle spiegazioni verso la previsione. Il lavoro futuro dovrebbe raccomandare controlli di qualità simili durante la raccolta di spiegazioni umane, dato che le annotazioni umane di alta qualità sono costose e difficili da acquisire.</sample>
    <sample id="339">Gli autori sono affiliati a:
- Saarland University
- Amazon Alexa
- University of Vienna</sample>
    <sample id="340">Kuan-Hao Huang presenta ParaAMR, un grande dataset di parafrasi sintatticamente diverse creato tramite back-translation di Abstract Meaning Representations (AMR). La generazione di parafrasi è cruciale per molte applicazioni NLP, ma i dataset esistenti mancano di scala o di diversità sintattica. I dataset annotati manualmente sono di alta qualità ma limitati, mentre quelli generati automaticamente tramite back-translation tendono a produrre parafrasi con una sintassi quasi identica alla frase originale.

L'idea principale di ParaAMR è sfruttare i grafi AMR, che rappresentano il significato di una frase attraverso nodi (concetti semantici) e archi (relazioni semantiche). Il processo di generazione di parafrasi tramite back-translation AMR inizia con l'analisi di una frase sorgente per ottenere il suo grafo AMR. Successivamente, il nodo focus (root) del grafo viene cambiato casualmente per alterare la struttura sintattica senza modificare il significato principale. Infine, un generatore da grafo AMR a testo produce la parafrasi. Questo metodo assicura che le parafrasi abbiano semantiche simili ma sintassi diverse.

ParaAMR contiene circa 15,5 milioni di frasi sorgente, con una media di 6,92 parafrasi per frase. Un'analisi quantitativa, che include punteggi automatici e valutazioni umane, mostra che ParaAMR raggiunge punteggi di similarità semantica paragonabili ai dataset esistenti, ma con una diversità sintattica significativamente più elevata.

Il dataset è stato applicato a diverse attività NLP. Nel Semantic Textual Similarity (STS), gli embedding di frasi appresi da ParaAMR superano quelli di altri dataset. Nella generazione di parafrasi sintatticamente controllata, ParaAMR migliora il controllo sintattico. Infine, per l'aumento dei dati nel few-shot learning, ParaAMR ottiene punteggi più alti in attività come MRPC, QQP e RTE. Questi risultati indicano che ParaAMR è un dataset su larga scala, sintatticamente diverso, che beneficia diverse applicazioni NLP.</sample>
    <sample id="341">Gli autori fanno ricorso a una misura di latenza chiamata "lagging medio" (AL) e a un'altra chiamata "lagging medio consapevole del calcolo" (AL_CA).</sample>
    <sample id="342">Il presentatore introduce LiveChat, un dataset di dialoghi personalizzati su larga scala costruito automaticamente dallo streaming live. I dataset esistenti, sia basati su testo che su video, hanno delle limitazioni. I dataset basati su testo, come Reddit e Weibo, sono i più comuni per l'addestramento dei modelli di dialogo, ma i dataset basati su video sono più vicini alle conversazioni parlate della vita reale. Tuttavia, i dataset video esistenti sono piccoli perché si basano sull'estrazione e l'annotazione manuale. Per i dialoghi personalizzati, c'è una carenza di informazioni dettagliate sulla persona e di conversazioni più lunghe. Inoltre, c'è una scarsità di corpora di dialoghi cinesi a più parti.

Per affrontare queste barriere, LiveChat propone un dataset di dialoghi personalizzati su larga scala derivato da video streaming live. Il processo di costruzione è diviso in tre fasi:
1.  **Raccolta e trascrizione:** I video di streaming live sono estratti da Douyin (TikTok cinese). L'audio viene poi estratto e trascritto in espressioni tramite il riconoscimento automatico del parlato (ASR).
2.  **Costruzione del dialogo:** I commenti del pubblico e le risposte dello streamer sono abbinati per costruire i dialoghi tramite un metodo "reply-to-whom". Questo metodo recupera efficacemente le relazioni di risposta tra gli oratori.
3.  **Estrazione della persona:** Le informazioni sulla persona sono raccolte da post storici, dialoghi e informazioni pubbliche. Queste informazioni sono suddivise in profili di base (età, sesso, posizione, ecc.) annotati manualmente e profili di testo (descrizioni basate su regole e un classificatore addestrato su DuLemon).

Gli esperimenti sui benchmark di modellazione della risposta e di riconoscimento dell'indirizzo dimostrano l'efficacia di LiveChat. I profili di persona estratti e le sessioni più lunghe migliorano i risultati della modellazione della risposta. Inoltre, viene investigato il transfer learning dei modelli di dialogo pre-addestrati su LiveChat. I risultati mostrano che BART, un modello di grandi dimensioni, funziona meglio, indicando la distintività del dominio di LiveChat rispetto ad altri dataset. Il transfer learning in-context (poche-shot) di modelli come GLM e GPT-3 mostra un miglioramento man mano che il numero di "shots" aumenta, ma diminuisce leggermente dopo un certo punto a causa della selezione casuale di dimostrazioni che possono introdurre rumore. In futuro, la ricerca si concentrerà su un transfer learning efficiente di modelli linguistici di grandi dimensioni (LLM) per LiveChat.</sample>
    <sample id="343">Hello a tutti. Sono Akshatha, e oggi io e il mio co-autore Martin presenteremo il nostro lavoro The KITMUS Test, che valuta l'integrazione della conoscenza da più fonti. Questo lavoro è una collaborazione tra la McGill University, Mila e Microsoft Research. I modelli NLU si basano su molteplici fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite il pre-addestramento, e la conoscenza fornita negli input in fase di inferenza. Recenti lavori, in compiti come il question answering, mostrano che i modelli possono usare la conoscenza del pre-addestramento per risolvere il compito. Ma la comprensione del linguaggio naturale richiede spesso una conoscenza che viene fornita anche in fase di inferenza. Per esempio, nella frase "John ha visto il presidente neo-eletto in TV", i parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cos'è una TV, ma non possono sapere con affidabilità chi sia John o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato dal momento del pre-addestramento. Per questo, i modelli efficaci per i compiti NLU intensivi di conoscenza richiedono la capacità di integrare e usare sia la conoscenza del pre-addestramento che quella dell'inferenza. In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione della coreferenza, progettato per sondare la capacità di attingere alla conoscenza disponibile in diverse fonti. Valutiamo il dataset con partecipanti umani e modelli di risoluzione della coreferenza stabiliti. Ecco un esempio del nostro dataset. "Servin è un giudice. Kea è una fornaia. Servin e Kea si sono incontrati al parco. Dopo una lunga giornata di lavoro a decidere casi in tribunale, era felice di rilassarsi." [Risposta: Servin] La risoluzione di un dato pronome richiede due tipi di informazioni. Primo, la conoscenza specifica dell'entità, come "Servin è un giudice". E secondo, la conoscenza di sfondo, come "I giudici decidono i casi nei tribunali". Generalmente, la conoscenza di sfondo viene appresa durante il pre-addestramento dei modelli linguistici di grandi dimensioni, mentre la conoscenza specifica dell'entità è tipicamente osservata in fase di inferenza. Variamo la disponibilità di questi due tipi di informazioni, in modo che possano essere trovate in un'unica fonte o in più fonti. Abbiamo definito tre impostazioni di KITMUS. Primo, l'impostazione tipica, Background-Pretrain, dove si presume che la conoscenza di sfondo sia disponibile al momento del pre-addestramento. Secondo, c'è l'impostazione Background-Both, dove la conoscenza di sfondo è disponibile sia al momento del pre-addestramento che al momento dell'inferenza. Infine, l'impostazione Background-Inference, dove entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Questa ultima impostazione è particolarmente interessante, poiché teorizza il caso in cui la conoscenza di sfondo necessaria per risolvere il compito non fa parte dei dati di pre-addestramento dei modelli. Per esempio, perché nuove occupazioni si sono sviluppate dal momento del pre-addestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle due fonti. Nell'impostazione Background-Pretrain, assumiamo che la conoscenza di sfondo "I politici cercano seggi eletti nel governo" sia contenuta nei parametri pre-addestrati. Nel contesto dell'inferenza, forniamo la conoscenza specifica dell'entità "Chichester è un politico". Nell'impostazione Background-Both, forniamo esplicitamente non solo la conoscenza specifica dell'entità, ma anche la conoscenza di sfondo sui politici nel contesto dell'inferenza. Nell'impostazione Background-Inference, forniamo l'occupazione fittizia "mirituer" invece di "politico", perché è improbabile che i mirituer siano contenuti nei parametri pre-addestrati. Abbiamo valutato il dataset sia con partecipanti umani che con modelli di risoluzione della coreferenza consolidati. In questa figura mostriamo i risultati dei modelli più performanti sulla variante più difficile dell'impostazione Background-Pretrain. Senza addestramento specifico del compito su KITMUS, entrambi i modelli non funzionano bene. Quando addestrati su KITMUS, tuttavia, sia C2F che BERT4Coref si comportano significativamente meglio della scelta casuale. Questo suggerisce che, quando addestrati su dataset di risoluzione della coreferenza generale, i modelli imparano a sfruttare gli indizi di superficie, che non sono utili quando si testa su KITMUS, dove tali indizi sono stati rimossi. Ulteriori esperimenti con conoscenza fittizia indicano che anche i modelli più performanti non possono integrare in modo affidabile la conoscenza di sfondo fornita solo al momento dell'inferenza. Per riassumere i principali punti chiave del nostro paper: 1. Molti modelli sembrano incapaci di ragionare sulla conoscenza da più fonti (conoscenza del pre-addestramento e dell'inferenza). 2. L'addestramento specifico del compito è necessario per l'integrazione della conoscenza. 3. I modelli faticano a integrare la conoscenza di sfondo in fase di inferenza. Se siete interessati a maggiori dettagli, consultate il nostro paper e controllate il dataset, la generazione e il codice di valutazione su GitHub all'indirizzo mpoemsl/kitmus. Grazie per l'attenzione.</sample>
    <sample id="344">I metodi basati su alberi possono essere computazionalmente costosi e richiedono pre-elaborazione del formalismo specifico (ad esempio, per gestire i simboli variabili) e procedure di induzione grammaticale specializzate.</sample>
    <sample id="345">The speaker, Matthias Lindemann, presents a paper on compositional generalization without using trees, achieved through multiset tagging and latent permutations. Compositional generalization is defined as a learner's ability to handle deeper recursion and unseen compositions of phrases seen individually during training. In semantic parsing, this means that the test set has structurally unseen logical forms, often with deeper recursion than the training data. Naive sequence-to-sequence models struggle with this, failing to reproduce systematic correspondences between input and output.

While incorporating trees into models is a popular method to address this, trees are often not readily available and require complex pre- or post-processing or specialized grammar induction. Lindemann's approach avoids trees entirely.

The proposed model predicts the output from the input in two steps:
1. **Tagging:** Each input token is tagged with an unordered multiset of tokens that will appear in the output. This ensures all correct output tokens are generated but are not yet ordered.
2. **Permutation:** A separate model predicts a permutation to arrange these multiset tokens into the correct output order. This permutation method is flexible and doesn't impose hard constraints on possible permutations.

Conceptually, the permutation model works by sequentially selecting tokens for the output. It "jumps" between the multiset tokens to determine the next token in the output sequence until all tokens from the first stage have been used exactly once.

Experimental results on the COGS benchmark (Kim and Linzen 2020) show that this method significantly outperforms other tree-less models on generalization to deeper recursion. However, some other types of structural generalization remain challenging.

The paper addresses several technical challenges:
* **Unknown Alignment:** The alignment between input and output is not given in the training data. This is solved by inducing the alignment as part of the training process.
* **NP-hard Permutation Inference:** Finding the highest-scoring permutation is NP-hard, akin to the Traveling Salesman Problem. The authors approximate this with a GPU-friendly continuous relaxation, which also allows backpropagation through the solution to learn linguistically more plausible permutations.

For more details on experiments and how these challenges were addressed, the paper and code are available via a QR code.</sample>
    <sample id="346">Gli autori dell'articolo sono affiliati alla School of Interactive Computing presso il Georgia Institute of Technology.</sample>
    <sample id="347">00:00
Ciao, sono Myra e oggi parlerò del nostro articolo "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky.
00:12
Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei modelli linguistici di grandi dimensioni (LLM). Tuttavia, queste misure presentano diverse limitazioni. Solitamente si basano su set di dati fissi e curati manualmente, che richiedono molto tempo per essere elaborati. Inoltre, misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, oppure catturano semplicemente associazioni molto generiche e ampie, come associazioni negative con particolari gruppi. Inoltre, la maggior parte del lavoro in questo settore non tiene conto dell'intersezionalità, che è la nozione secondo cui identità sociali sfaccettate possono aggravare i pregiudizi ed essere uniche fonti di danno.
00:56
Come superare queste limitazioni? Ci affidiamo alla proprietà che questi nuovi LLM, istruiti e addestrati, sono molto bravi a rispondere alle istruzioni nei prompt. Quindi, possiamo chiedere al modello di generare una persona, che è la rappresentazione di un individuo immaginario, usando un prompt come "Immagina di essere una donna asiatica. Descriviti". E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo semplicemente specificare l'indicatore di identità che vogliamo in questo prompt.
01:26
Ecco alcuni esempi di generazione di persona da GPT-4. Immediatamente, vediamo che, sebbene gli output non siano eccessivamente negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni schemi interessanti. La donna asiatica è rappresentata come discreta, la donna mediorientale è definita con parole come "esotica" e ci si riferisce a una "regione affascinante". Entrambe le donne di colore fanno riferimento all'ascendenza, mentre la persona del bianco non ne fa alcuna.
01:57
Per catturare questi schemi, il nostro metodo ha due parti. La prima è la generazione di queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio psicologico in cui si davano questi prompt a soggetti umani, scoprendo che anche i soggetti umani erano in grado di portare alla luce stereotipi razziali. Inoltre, ciò consente un confronto diretto tra le nostre persone generate e le risposte scritte dagli umani. La seconda parte sono le parole marcate, che è un metodo per identificare le parole che distinguono le persone di gruppi marcati da quelle di gruppi non marcati, cosa che illustrerò a breve. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici senza dover fare affidamento su alcun lessico specifico.
02:42
Quindi, il metodo delle parole marcate si basa sul concetto sociolinguistico di marcatura, che afferma che esiste un valore predefinito non marcato, e qualsiasi gruppo che si discosta da tale valore predefinito è linguisticamente marcato. Ad esempio, la parola "guerriero" è solitamente associata agli uomini, quindi quando si descrive un guerriero che è una donna, si specifica "guerriero donna" e si marca il termine con "donna". Più in generale, i gruppi dominanti nella società sono linguisticamente e socialmente non marcati, mentre i gruppi emarginati sono marcati.
03:17
Quindi, nel nostro metodo, per prima cosa designiamo quali sono i gruppi non marcati e marcati. E poi confrontiamo le persone usando il metodo delle parole che distinguono, che consiste fondamentalmente nell'usare rapporti di probabilità logaritmici ponderati per distinguere le parole principali per ogni gruppo marcato. Ad esempio, per le persone di donne nere, useremmo le parole che distinguono e confronteremmo i rapporti di probabilità logaritmici rispetto sia alle persone bianche che alle persone maschili, perché quelli sono i due gruppi non marcati corrispondenti.
03:49
Ora per alcuni risultati. Per prima cosa usiamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte dagli umani.
04:00
Tuttavia, quando guardiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse. Quindi, mentre le persone generate hanno tassi molto più alti delle parole del lessico, quelle scritte dagli umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate che sono nelle persone generate sono davvero solo le parole "alto" e "atletico". Quindi, davvero solo quelle positive, o almeno non negative. E in effetti, questo lessico non cattura davvero molti degli schemi dannosi che abbiamo visto nelle diapositive precedenti in assoluto. Quindi, invece di farlo, passeremo ai risultati del nostro metodo delle parole marcate per mostrare come queste parole apparentemente positive facilitano gli stereotipi e le narrazioni essenzializzanti.
04:39
Nella nostra analisi, riveliamo come questi ritratti apparentemente positivi riflettono schemi dannosi. In primo luogo, per i gruppi marcati, le parole principali includono cose come "cultura", "tradizione", "orgoglioso" ed "esotico". E queste parole definiscono questi gruppi solo in base alla loro identità e li distinguono dalla norma bianca. Ciò contribuisce a una lunga eredità di discriminazione e alterità per questi gruppi. Inoltre, ci sono molti tropi comuni che si riflettono in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come "vivace" e "formosa", che si collegano a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola", "delicata" e "setosa", che si collegano a una lunga storia di donne asiatiche ipersessualizzate, considerate molto docili e sottomesse e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come "forte" e "resiliente". Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della "donna nera forte", e mentre a prima vista sembra positivo, ci sono stati lavori che mostrano che questo tipo di archetipo è in realtà molto dannoso perché mette molta pressione su queste demografie per essere resilienti e forti contro gli ostacoli sociali. Quindi, piuttosto che lavorare per cambiare quegli ostacoli, mette pressione su quelle persone per superarli, il che porta a esiti di salute molto negativi per queste persone, tra gli altri danni.
06:16
Quindi, basandoci su questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Primo, noi, come ricercatori, dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche usare una lente intersezionale per studiare i pregiudizi e i danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, perché, ad esempio, questi stereotipi positivi, non sappiamo se è perché c'è una sorta di strano allineamento eccessivo dei valori in corso, o forse altri metodi anti-stereotipi che stanno portando a questi schemi perniciosi, non possiamo davvero fare alcuna ipotesi o studiarlo ulteriormente senza maggiore trasparenza. Grazie mille per aver ascoltato. Divertitevi alla ACL!</sample>
    <sample id="348">Il video presenta un articolo intitolato "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" di Myra Cheng, Esin Durmus e Dan Jurafsky, presentato all'ACL 2023. La speaker, Myra Cheng, sottolinea come i bias sociali e gli stereotipi siano prevalenti nei Large Language Models (LLMs), ma i metodi di misurazione esistenti presentino dei limiti, come il compromesso tra specificità e generalizzabilità, la dipendenza da dataset fissi e curati a mano e la mancata considerazione dell'intersezionalità.
Per superare questi limiti, gli autori propongono di sfruttare la capacità degli LLMs, come GPT-3.5 e GPT-4, di rispondere a istruzioni nei prompt, chiedendo al modello di generare una "persona" (ad esempio, "Immagina di essere una donna asiatica. Descriviti."). Questo approccio è generalizzabile a qualsiasi identità intersezionale.
Il metodo consiste in due passaggi: 1. Generare le personas usando prompt ispirati a studi psicologici con soggetti umani. 2. Trovare "parole marcate" che distinguano le personas di gruppi "marcati" da quelle di gruppi "non marcati" (ovvero, il "default" socialmente non marcato). Questa fase utilizza i rapporti di log-odds ponderati per identificare le parole chiave.
I risultati mostrano che le personas generate dagli LLMs contengono più stereotipi rispetto a quelle scritte dagli umani. In particolare, gli LLMs tendono a utilizzare un numero limitato di parole stereotipate. L'analisi delle "parole marcate" rivela schemi di "altro-izzazione" attraverso narrazioni essenzializzanti (cultura, tradizione, orgoglio, esotico) e ritratti positivi perniciosi che, pur sembrando innocui, rafforzano stereotipi dannosi. Ad esempio, "vibrante, sinuoso" per le donne latine, "petite, delicata, setosa" per le donne asiatiche e "forte, resiliente" per le donne nere.
Le raccomandazioni per i proprietari dei modelli includono l'affrontare stereotipi positivi e narrazioni essenzializzanti, l'adozione di una lente intersezionale nello studio dei bias e l'aumento della trasparenza sui metodi di mitigazione dei bias.</sample>
    <sample id="349">Certo, ecco la traduzione in italiano del testo della presentazione.

---

**Titolo:** Mi stai copiando il modello? Proteggere il copyright dei modelli linguistici di grandi dimensioni per l'EaaS tramite filigrana backdoor.
**Autori:** Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shanxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie.

¹ Università di Scienza e Tecnologia della Cina
² Microsoft Research Asia
³ Università Jiaotong di Pechino
⁴ Sony AI
⁵ Microsoft STC Asia

---

**Slide 1: Introduzione**

Ciao a tutti, il mio nome è Jingwei Yi dell'Università di Scienza e Tecnologia della Cina.
È un piacere presentare un breve video pubblicitario sul nostro articolo: "Mi stai copiando il modello? Proteggere il copyright dei modelli linguistici di grandi dimensioni per l'embedding as a service tramite filigrana backdoor."

---

**Slide 2: Background**

*   I modelli linguistici di grandi dimensioni (LLM) sono eccezionali in NLU (comprensione del linguaggio naturale) e NLG (generazione del linguaggio naturale).
    *   GPT [1], LLaMA [2], PALM [3]
*   L'embedding as a Service (EaaS) viene offerto per assistere varie attività NLP.
    *   OpenAI offre un'API di embedding basata su GPT-3 [1]

Lasciate che vi introduca il background sull'embedding as a service. Attualmente, i modelli linguistici di grandi dimensioni come GPT, LLaMA e PALM sono eccezionali nella comprensione e generazione del linguaggio naturale. L'embedding as a Service è uno dei servizi costruiti su modelli linguistici di grandi dimensioni per assistere varie attività NLP. Ad esempio, OpenAI offre un'API di embedding basata su GPT.

---

**Slide 3: Motivazione**

*   Gli attaccanti potrebbero rubare il modello imparando dagli embedding e fornire servizi simili.
    *   StolenEncoder [1]
*   Necessità di proteggere il copyright dell'EaaS
    *   Rilevare se il servizio di un fornitore è rubato da un altro servizio.

Tuttavia, lavori recenti hanno dimostrato che l'attaccante può rubare il modello imparando dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright dell'embedding as a service.

---

**Slide 4: Sfida**

*   Applicabile all'EaaS
*   **Utilità:** non dovrebbe degradare l'utilità degli embedding forniti.
*   **Copertura:** dovrebbe essere nascosto all'attaccante.
*   **Trasferibilità:** la filigrana deve essere trasferibile ai servizi degli attaccanti.

Per proteggere il copyright dell'embedding as a service, una delle soluzioni è incorporare una filigrana nel servizio del fornitore e rilevare se un altro servizio contiene la filigrana. Il metodo della filigrana deve soddisfare le seguenti proprietà:
Primo, il metodo dovrebbe essere applicabile all'embedding as a service.
Secondo, la filigrana non dovrebbe degradare l'utilità degli embedding forniti.
Terzo, la filigrana dovrebbe essere abbastanza nascosta all'attaccante, altrimenti l'attaccante può rimuovere facilmente la filigrana.
Infine, la filigrana deve essere trasferibile ai servizi degli attaccanti durante il processo di estrazione del modello.

---

**Slide 5: Lavori esistenti**

*   Filigrana basata sui parametri [1, 2]
    *   Trasferibilità ❌
*   Filigrana lessicale [3, 4]
    *   Applicabile all'EaaS ❌
*   Filigrana basata su backdoor [5]
    *   Applicabile all'EaaS ❌
*   Filigrana basata su avversari [6]
    *   Applicabile all'EaaS ❌

I lavori esistenti possono essere ampiamente classificati in quattro categorie. Tuttavia, questi metodi non sono applicabili all'embedding as a service o mancano di trasferibilità. Pertanto, in questo articolo, proponiamo EmbMarker, che è un metodo di filigrana basato su backdoor applicabile all'embedding as a service.

---

**Slide 6: EmbMarker - Selezione del trigger**

*   Contare la frequenza delle parole su un corpus di testo generale Dp
*   Selezionare casualmente n parole in un intervallo di frequenza moderata

(a) Iniezione della filigrana

Ora vi illustro i dettagli del nostro EmbMarker. EmbMarker contiene due passaggi principali: iniezione della filigrana e verifica del copyright. Prima di questi passaggi principali, selezioniamo prima un set di trigger. Il set di trigger è un gruppo di parole in un intervallo di frequenza moderata. Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso.

---

**Slide 7: EmbMarker - Iniezione della filigrana**

*   Definire un embedding target et
*   Contare il numero di trigger in una frase Q(S) = min(|S ∩ T|, m) / m
*   Aggiungere l'embedding target sull'embedding originale eo

(a) Iniezione della filigrana

Nell'iniezione della filigrana, definiamo prima un embedding target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target.

---

**Slide 8: EmbMarker - Verifica del copyright**

*   Costruire un dataset backdoor e benigno
    *   Db = {w1, w2, ..., wm | wi ∈ T}
    *   Dn = {w1, w2, ..., wm | wi ∉ T}
*   Richiedere embedding dal servizio dello stealer con i dataset

La verifica del copyright serve a rilevare se un modello dietro un altro servizio contiene la filigrana. Per prima cosa, costruiamo un dataset backdoor e un dataset benigno. Il dataset backdoor contiene frasi di cui tutte le parole appartengono al set di trigger. Mentre tutte le parole nelle frasi del dataset benigno non appartengono al set di trigger. Quindi il fornitore richiede gli embedding dal servizio dello stealer con i dataset.

---

**Slide 9: EmbMarker - Verifica del copyright (cont.)**

*   Calcolare la loro somiglianza con l'embedding target
    *   cosi = (ei · et) / (||ei|| · ||et||), l2i = ||ei - et||²
    *   Cb = {cosi | i ∈ Db}, Cn = {cosi | i ∈ Dn}
    *   Lb = {l2i | i ∈ Db}, Ln = {l2i | i ∈ Dn}
*   Calcolo delle metriche (differenza di somiglianza e p-value del test KS)
    *   Δcos = (1 / |Cb|) Σ_{i∈Cb} i - (1 / |Cn|) Σ_{j∈Cn} j
    *   Δl2 = |(1 / |Lb|) Σ_{i∈Lb} i - (1 / |Ln|) Σ_{j∈Ln} j|

La similarità coseno e L2 tra l'embedding richiesto e l'embedding target vengono calcolate. Calcoliamo la differenza di similarità tra il dataset benigno e il dataset backdoor, che è definita come Delta coseno e Delta L2. Nel frattempo, applichiamo anche il test KS e usiamo il suo p-value come terza metrica.

---

**Slide 10: Risultati sperimentali - Impostazione**

*   Dataset di copia: AG News, MIND, SST2, Enron Spam
*   Dataset generale del fornitore: WikiText
*   Metriche:
    *   Prestazioni sui task a valle: ACC (accuratezza)
    *   Prestazioni di rilevamento: Δcos, Δl2, p-value
*   Impostazione: m = 20, n = 4, intervallo di frequenza = [0.005, 0.01]

Abbiamo condotto esperimenti su quattro dataset: AG News, MIND, SST2 ed Enron Spam. Supponiamo che il fornitore utilizzi il dataset WikiText per contare la frequenza delle parole.

---

**Slide 11: Risultati sperimentali - Confronto delle prestazioni**

(Tabella che mostra il confronto delle prestazioni per i diversi dataset e metodi, inclusi i valori di ACC, p-value, Δcos e Δl2)

I risultati sui quattro dataset mostrano che il nostro EmbMarker può avere grandi prestazioni di rilevamento mantenendo una grande utilità per i task a valle.

---

**Slide 12: Risultati sperimentali - Visualizzazione degli embedding**

(Quattro grafici di visualizzazione degli embedding (PCA) per AG News, Enron Spam, MIND e SST2. La legenda indica il numero di trigger nella frase.)

Abbiamo anche convalidato la copertura degli embedding forniti visualizzando l'embedding delle frasi sui quattro dataset tramite PCA. Come mostrato nelle figure, è difficile distinguere tra gli embedding backdoor e gli embedding normali.

---

**Slide 13: Grazie!**

Questo è tutto, grazie! Siete i benvenuti a discutere con noi.

---</sample>
    <sample id="350">Il relatore presenta un paper intitolato "Qual è il significato delle prestazioni superumane nell'NLU odierna?" Inizia affermando che la valutazione basata su classifiche è diventata una pratica popolare nell'NLP, e che non di rado i sistemi raggiungono prestazioni a livello umano o addirittura superumano in tali benchmark, chiamati benchmark saturi. Tuttavia, il relatore sottolinea che è "facile" superare gli esseri umani in compiti procedurali semplici (ad esempio, aritmetica e compiti che richiedono un'estrema intensità di memoria). Molti compiti NLU richiedono conoscenza e inferenza.

Il relatore evidenzia anche la fragilità dei modelli in diversi aspetti, tra cui la generalizzazione fuori dal dominio, gli attacchi avversari, i modelli spuri, la mancanza di sensibilità alle perturbazioni linguistiche di base e l'eccessiva sensibilità alle perturbazioni che non dovrebbero avere importanza.

Per rispondere alla domanda su quanto siano affidabili i punteggi delle classifiche nel confrontare modelli ed esseri umani, vengono analizzati due benchmark popolari: SuperGLUE e SQuAD. I risultati mostrano che le linee di base umane sono superate in 6 su 10 compiti SuperGLUE e che gli esseri umani sono ampiamente superati dai sistemi in entrambe le versioni dei benchmark SQuAD.

Tuttavia, dopo aver esaminato manualmente questi set di dati, sono state riscontrate diverse fonti di errore nel confronto uomo-sistema. La prima lacuna è che sistemi ed esseri umani sono spesso valutati su set diversi. Vengono inoltre identificate diverse lacune nella qualità dei dati di verità, come la presenza di etichette errate.

Infine, il relatore solleva delle preoccupazioni riguardo all'uso di metriche di valutazione umana e alle tariffe di pagamento eterogenee e sconosciute per gli annotatori, sottolineando come la scarsa motivazione possa portare a una bassa qualità dei dati. Il relatore conclude che le affermazioni sulle prestazioni superumane non sono (ancora) scientificamente fondate. Il paper fornisce raccomandazioni per costruire benchmark più equi e trasparenti.</sample>
    <sample id="351">Ciao a tutti, mi chiamo Shuheng. Oggi vi presento la nostra ricerca, "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?".

Il nostro studio esamina la capacità dei tagger di entità denominate (NER) di generalizzare a dati moderni. Abbiamo notato che i modelli NER utilizzano CoNLL-2003 da quasi 20 anni. Ci siamo quindi chiesti: questi modelli possono generalizzare a dati moderni? E cosa serve per una buona generalizzazione? In caso di calo delle prestazioni, qual è la causa?

Per indagare, abbiamo creato il dataset CoNLL++, raccogliendo notizie Reuters del 2020 e annotandole con le stesse linee guida di CoNLL-2003. Abbiamo poi messo a punto oltre 20 modelli su CoNLL-2003 e li abbiamo valutati sia sul test set CoNLL-2003 che sul test set CoNLL++. Infine, abbiamo calcolato la variazione percentuale dell'F1 per valutare la generalizzazione di ciascun modello.

Per una buona generalizzazione, abbiamo riscontrato che sono necessari tre ingredienti principali:
1.  **Architettura del modello:** i modelli basati su Transformer generalmente generalizzano meglio.
2.  **Dimensione del modello:** modelli più grandi tendono a generalizzare meglio.
3.  **Numero di esempi di fine-tuning:** più esempi portano a una migliore generalizzazione. Questi tre fattori sono interconnessi per ottenere i migliori risultati.

Passando alla causa del calo delle prestazioni, avevamo due ipotesi:
1.  **Overfitting adattivo:** un overfitting causato dal riutilizzo ripetuto dello stesso set di test, che si manifesta con rendimenti decrescenti su un nuovo set di test.
2.  **Deriva temporale:** la degradazione delle prestazioni causata dall'aumento del divario temporale tra i dati di training e test.

I nostri esperimenti hanno rivelato che l'overfitting adattivo non è stato osservato, poiché non vi erano rendimenti decrescenti. Per quanto riguarda la deriva temporale, abbiamo riscontrato che le prestazioni si degradano con un divario temporale maggiore. Ciò conferma che la causa principale del calo delle prestazioni è la deriva temporale.

In conclusione, per una buona generalizzazione, sono necessari una migliore architettura del modello, una dimensione del modello maggiore e più esempi di fine-tuning. Il calo delle prestazioni è causato dalla deriva temporale, non dall'overfitting adattivo. Quindi, i tagger CoNLL-2003 funzionano ancora? La risposta è sì! Speriamo che la nostra ricerca stimoli ulteriori studi sul miglioramento della generalizzazione dei modelli.

Grazie. Potete trovare il nostro articolo e il dataset ai link forniti, e non esitate a contattarmi per qualsiasi domanda.</sample>
    <sample id="352">ABC-Eval è un approccio per la valutazione dimensionale del dialogo. Serve per annotare esplicitamente i comportamenti dei modelli di chat, come l'irrilevanza o le auto-contraddizioni, per misurare i tassi di errori tematici.</sample>
    <sample id="353">Questo documento introduce un metodo interattivo per generare codice Python, affrontando il problema della sottospecificazione negli input del linguaggio naturale. Il problema della sottospecificazione è comune nei casi d'uso del mondo reale, dove le descrizioni del linguaggio naturale spesso mancano di dettagli critici, portando a un codice non accurato. Per affrontare questa sfida, il documento propone di introdurre l'interattività nella generazione del codice, in particolare chiedendo domande di chiarimento (CQ) all'utente. Questo approccio è progettato per raccogliere specifiche aggiuntive e risolvere l'ambiguità degli input.

Il documento delinea una pipeline che include un predittore di necessità di chiarimento, un selezionatore di domande e un generatore di codice. Si concentra sulla chiarificazione delle specifiche a livello di operazione. Un dataset sintetico chiamato CodeClarQA è stato creato con chiarimenti sulle operazioni chiave. Questo dataset è stato utilizzato per addestrare e valutare i componenti della pipeline. Per identificare le operazioni chiave mancanti, il metodo utilizza euristiche basate su un grafo della conoscenza del codice generato da Graph4Code, rappresentando le operazioni in uno spazio latente e calcolando i punteggi di similarità rispetto alla descrizione del linguaggio naturale. Se i punteggi di similarità scendono al di sotto di una certa soglia, l'operazione è considerata mancante e vengono generate le CQ appropriate.

I risultati sperimentali mostrano che il metodo ha avuto successo nell'identificazione delle operazioni chiave mancanti, con MPNet che ha dimostrato le migliori prestazioni. L'analisi degli errori ha rivelato che l'approccio genera efficacemente le CQ e ha evidenziato aree di miglioramento, come la distinzione tra operazioni con nomi simili (tassonomia) e la documentazione dei valori degli argomenti (argomento). La valutazione della pipeline ha indicato che le prestazioni del modello migliorano sostanzialmente con l'inclusione di CQ di rango più elevato e con risposta, rafforzando l'ipotesi che le chiarificazioni migliorano la generazione del codice. Tuttavia, la pipeline non ha ancora superato i modelli addestrati solo su input del linguaggio naturale e codice, suggerendo che l'ottimizzazione delle CQ e la classificazione delle CQ è una sfida.</sample>
    <sample id="354">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino all'anno 2012.</sample>
    <sample id="355">00:00
Ciao, mi chiamo Vasudha e sono una studentessa di dottorato in informatica alla Stony Brook University. Vorrei presentare il nostro lavoro accettato all'ACL 2023 come long paper, Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge.
00:19
Cominciamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In parole semplici, la dissonanza cognitiva è quando due convinzioni o azioni sono incoerenti.
00:31
Come in questo esempio, in cui una persona afferma "So che le sigarette potrebbero uccidermi", e poi continua dicendo "Oggi ho fumato un paio di sigarette dopo la riunione". Questa convinzione e azione sono incoerenti, e sono in dissonanza.
00:48
Inoltre, menzionare "Non credo che potrei mantenere il mio lavoro senza di loro" giustifica la seconda occorrenza e hanno una relazione di consonanza.
00:57
Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nella vita quotidiana, è molto rara da trovare espressa nel linguaggio tra altri tipi di relazioni di discorso.
01:06
Allora perché la dissonanza? Studiare la dissonanza cognitiva può aiutarci a capire gli effetti del disaccordo tra le persone, a monitorare le tendenze di credenze, valori e cambiamenti di atteggiamento nella popolazione.
01:18
L'elevata dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone.
01:25
Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili.
01:33
Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali.
01:43
Per l'obiettivo di creare una risorsa di dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio "dissonanza prima", come si vede nel diagramma di flusso qui.
01:57
I tweet sono stati analizzati usando un parser PDTB e le coppie di unità di discorso sono state annotate secondo le linee guida descritte nel nostro paper. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate.
02:13
Dopo aver raccolto circa 1000 esempi di coppie di unità di discorso, abbiamo eseguito l'addestramento per un classificatore iniziale, addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore abbia avuto prestazioni non molto migliori del caso. Data la bassa occorrenza di dissonanza e l'assenza di un dataset di ricerca precedente, stiamo affrontando il problema della rarità assoluta.
02:34
Per alleviare questo, sperimentiamo combinazioni di trasferimento di apprendimento e apprendimento attivo per l'annotazione, in modo che più campioni di dissonanza possano essere raccolti con meno cicli di annotazione, riducendo i costi complessivi di annotazione e migliorando il rilevamento della dissonanza.
02:50
Poiché il modello iniziale non è riuscito a catturare affatto la classe di dissonanza, iniziamo il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati.
03:03
Abbiamo trasferito da due compiti diversi: la classificazione della posizione di dissonanza indipendente dal tema, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dal tema. Chiamato qui Debate, e sulla classificazione binaria delle classi di espansione e confronto di PDTB. Poiché questi due sono strettamente correlati alla concezione di consonanza e dissonanza, e li chiamiamo qui CE.
03:29
Abbiamo scoperto che trasferendo, la prestazione zero-shot sul dataset annotato è già molto migliore di quanto ci si aspetti, con il migliore con AUC 0,62.
03:40
Inoltre, eseguendo un fine-tuning iterativo su entrambi i compiti, scopriamo che il fine-tuning del compito CE seguito da un ulteriore fine-tuning su Debate, produce una prestazione zero-shot molto migliore. Questo è quindi il modello che abbiamo utilizzato per avviare il fine-tuning.
03:56
Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. Il cumulativo accumula tutti i dati raccolti dall'annotazione attiva finora, mentre l'iterativo aggiorna il modello addestrando sul set più recente di dati raccolti.
04:12
Attraverso le diverse strategie, abbiamo scoperto che il cumulativo ha avuto prestazioni uguali o migliori dell'iterativo in tutti i casi.
04:20
Successivamente, per aumentare il numero di esempi di dissonanza, abbiamo utilizzato una strategia di probabilità di classe rara, PRC, per selezionare principalmente gli esempi che sono altamente probabili essere dissonanti dal modello attuale in ogni ciclo di AL. Abbiamo confrontato questo con lo stato dell'arte delle strategie AL che sono comunemente utilizzate nella comunità.
04:39
Troviamo che la strategia PRC proposta funziona meglio di altre strategie all'avanguardia, anche se la differenza è piccola. Si noti che la prestazione è significativamente inferiore per il random.
04:54
In ulteriori cicli di AL con le due strategie migliori, abbiamo migliorato la classificazione della dissonanza AUC a 0,75, che è la migliore prestazione che abbiamo avuto finora nel compito.
05:04
Abbiamo anche verificato la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Troviamo che la PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili.
05:20
In sintesi, troviamo che PRC è una strategia AL semplice ed efficiente per l'acquisizione di campioni rari, e l'avvio a freddo di AL con un compito di apprendimento per trasferimento opportunamente progettato può aiutare significativamente. Troviamo anche che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive in-domain beneficiano dell'aggiornamento cumulativo.
05:40
Questi sono i link al nostro codice, al dataset e al nostro paper. Non esitate a contattarci se avete domande.
05:47
Grazie!</sample>
    <sample id="356">Università di Edimburgo, Università di Saarland e Università di Amsterdam.</sample>
    <sample id="357">La relatrice si chiama Siyu Yuan.</sample>
    <sample id="358">Ci sono 5 autori coinvolti nell'articolo: Patrick Fernandes, Kayo Yin, Emmy Liu, André F. T. Martins e Graham Neubig.</sample>
    <sample id="359">Il modello viene confrontato con CAAT.</sample>
    <sample id="361">L'oratore, Armineh Nourbakhsh, inizia il discorso presentando la sua ricerca, intitolata "CounterComp: Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning". Si concentra sui compiti di risposta alle domande a più fasi, evidenziando il problema del "long-tail issue" nei modelli attuali. Spiega che questi modelli tendono a memorizzare schemi superficiali a causa della ripetizione di token di input. Per superare questa limitazione, la ricerca propone di utilizzare scenari controfattuali come esempi per l'apprendimento.

Nourbakhsh illustra come le domande possano fungere da esempi controfattuali, indicando che la modifica di determinate componenti della domanda può alterare l'output. Presenta il modello CounterComp, che impiega una loss di apprendimento metrico ausiliaria per migliorare le prestazioni. Questa loss dinamica misura l'entità dell'intervento nelle domande per regolare l'apprendimento.

I risultati dimostrano che CounterComp migliora costantemente le prestazioni su tre baseline all'avanguardia, in particolare per problemi con più di due passaggi di ragionamento, sia per campioni in-distribution che out-of-distribution. In particolare, il modello addestrato con CounterComp mostra un'accuratezza del programma del 73,53% su programmi unseen, superando significativamente il 65,74% di FinQANet. In conclusione, l'oratore afferma che l'aggiunta della loss di CounterComp aiuta il modello a prestare attenzione a token più significativi durante la generazione delle risposte, migliorando così la generalizzazione composizionale.</sample>
    <sample id="0">Le principali fonti di dati per i modelli linguistici includono **web-crawled data, political news media e social media** (ad esempio, Reddit).</sample>
    <sample id="1">Le affiliazioni degli autori dell'articolo sono McGill University/Mila e Microsoft Research.</sample>
    <sample id="2">Il presentatore, Yu Tu di Ant Group, introduce il suo team e il loro paper sull'interpretazione di documenti visivamente ricchi. Il paper propone LayoutMask, un nuovo modello di pre-training multimodale per affrontare i problemi di ordine di lettura nei documenti ricchi di elementi visivi. I modelli esistenti di pre-training di documenti soffrono di questi problemi a causa dell'uso di ordini di lettura globali. LayoutMask differisce dai modelli precedenti in tre aspetti: la scelta del posizionamento 1D, la strategia di mascheramento e gli obiettivi di pre-training.

LayoutMask utilizza informazioni testuali e di layout come input del modello per migliorare le interazioni testo-layout e le rappresentazioni del layout durante il pre-training. Anziché un posizionamento 1D globale, LayoutMask utilizza ordini di token locali-segmento come posizionamento 1D. Per inferire l'ordine di lettura globale, LayoutMask combina il posizionamento 1D, il posizionamento 2D e le informazioni semantiche. Due nuove strategie di mascheramento, Whole Word Masking (WWM) e Layout Aware Masking (LAM), vengono utilizzate per promuovere ulteriormente le interazioni. LayoutMask introduce anche un nuovo obiettivo di pre-training, Masked Position Modeling (MPM), che è simile al compito di Cloze, in cui si recuperano posizioni 2D mascherate casualmente nei documenti originali. LayoutMask ha dimostrato prestazioni superiori rispetto al posizionamento 1D globale nei set di dati FUNSD e SROIE. Gli esperimenti indicano che l'uso del posizionamento 1D locale ha prodotto un migliore adattamento, soprattutto in casi in cui l'ordine di lettura tradizionale è inadeguato. Le interazioni testo-layout vengono rafforzate dal processo di apprendimento congiunto di indizi semantici e spaziali.</sample>
    <sample id="3">00:00 - Ciao. Benvenuti alla nostra presentazione di DEPlain: un nuovo corpus per la semplificazione del testo in tedesco a livello di documento e a livello di frase.
00:11 - Il mio nome è Regina Stodden e vi guiderò attraverso la prima parte della presentazione.
00:18 - Definiamo innanzitutto la semplificazione del testo.
00:22 - La semplificazione del testo è il processo di adattamento di un testo per migliorarne la comprensione da parte di un gruppo target specifico, come persone con difficoltà di lettura o parlanti non nativi.
00:33 - Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testi, ad esempio di documenti o di frasi. Nell'esempio qui sotto, potete vedere una coppia di frasi allineate in parallelo, composta da una frase complessa in tedesco e dalla sua traduzione in linguaggio semplice.
00:50 - Per semplificare la frase, sono possibili diverse tecniche, come potete vedere nell'esempio. Ad esempio, la sostituzione lessicale, l'eliminazione di clausole, la riorganizzazione o l'inserimento di parole.
01:04 - Ora proponiamo il nostro nuovo corpus DEPlain.
01:07 - Perché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Ad esempio, questi corpora qui sono troppo piccoli per addestrare un modello di semplificazione del testo.
01:19 - Gli altri tre modelli, proposti negli ultimi anni, sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nei loro allineamenti.
01:28 - Perciò, proponiamo il nostro nuovo corpus DEPlain, che è diviso in due sottocorpora: DEPlain-APA e DEPlain-web. DEPlain-APA è basato su testi di notizie.
01:41 - In DEPlain-APA, abbiamo allineato 483 documenti, tutti manualmente. Ciò si traduce in circa 13.000 coppie di frasi parallele.
01:56 - Per DEPlain-web, questo corpus include diversi domini, e abbiamo anche allineato tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico.
02:11 - In totale, otteniamo 30.450 coppie di frasi.
02:17 - Abbiamo analizzato ulteriormente le nostre coppie di frasi, ad esempio per quanto riguarda il tipo di semplificazione. Come potete vedere qui, i testi biblici sono molto più fortemente semplificati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingua.
02:34 - A tutti i livelli, per quanto riguarda ad esempio la semplificazione lessicale, la semplificazione strutturale, e anche il livello generale di semplificazione.
02:42 - Inoltre, potete vedere che il nostro corpus DEPlain ha una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus DEPlain-APA, abbiamo molte più riorganizzazioni e aggiunte di parole rispetto al corpus DEPlain-web.
02:59 - D'altra parte, nel corpus web, abbiamo molte più riformulazioni.
03:04 - Vediamo ora cosa possiamo fare con questo corpus.
03:08 - Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro dataset DEPlain. Quindi per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico.
03:22 - Negli ultimi anni ci sono stati molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli, scritti in lingue diverse, e vogliamo estrarre allineamenti di frasi in entrambi i documenti, ma nel nostro caso d'uso stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, avendo la stessa lingua, avendo lo stesso contenuto, ma sono su un diverso livello di complessità.
03:52 - E ora, poiché abbiamo il nostro dataset DEPlain che ha frasi allineate manualmente, possiamo usare queste frasi come allineamenti standard per valutare alcuni dei metodi di allineamento proposti.
04:08 - E abbiamo fatto alcuni adattamenti ai metodi proposti e abbiamo pubblicato tutti questi adattamenti e i codici per eseguire i nostri esperimenti nel documento.
04:20 - Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo in tedesco è il metodo di MASSAlign. E potete anche trovare il codice per eseguire questo metodo sui vostri documenti nel documento.
04:40 - Il secondo caso d'uso che abbiamo mostrato nel nostro documento è il caso della semplificazione automatica del testo, attraverso la messa a punto di modelli linguistici per produrre testo semplificato dal testo di input complesso.
05:00 - Abbiamo messo a punto due modelli diversi, abbiamo messo a punto il modello di Long-mBART per produrre semplificazioni a livello di documento, e abbiamo anche messo a punto il normale mBART di base per produrre semplificazioni a livello di frase.
05:16 - Potete anche trovare tutti i checkpoint e potete esaminare più in dettaglio i punteggi e le metriche di valutazione dei nostri esperimenti nel documento.
05:29 - Abbiamo concluso che questa messa a punto di base potrebbe produrre o potrebbe ottenere punteggi migliori rispetto ai punteggi di base e proponiamo questi risultati come punto di riferimento, un punto di riferimento di base per il problema della semplificazione automatica del testo in futuro.
05:50 - Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">La relatrice è Kayo Yin.</sample>
    <sample id="5">Per ottenere l'accuratezza dell'82%-87%, è stato utilizzato il modello T5 XL.</sample>
    <sample id="6">Il relatore presenta il suo lavoro dal titolo "Towards Unifying Multi-Lingual and Cross-Lingual Summarization".
Il lavoro mira a unificare la riassunzione multilingue (MLS) e la riassunzione cross-lingue (CLS) in un contesto più generale, chiamato riassunzione molti-a-molti (M2MS).

L'M2MS mira a costruire un singolo modello di riassunzione per elaborare un documento in qualsiasi lingua sorgente e generare il suo riassunto in qualsiasi lingua di destinazione. Il relatore conduce studi preliminari per fornire analisi più approfondite tra MLS, CLS e M2MS, rilevando che l'M2MS potrebbe aiutare il modello a trasferire la conoscenza tra lingue diverse rispetto a MLS e CLS precedenti.

Inoltre, viene proposto PISCES, un modello M2MS pre-addestrato che apprende la modellazione linguistica, la capacità cross-lingue e la capacità di riassunzione attraverso un pre-addestramento in tre fasi.
Nel meta pre-addestramento, il modello deve generare frasi originali basate su controparti mascherate. Nel pre-addestramento cross-lingue, il modello genera frasi nella lingua di destinazione basate su frasi parallele rumorose nella lingua sorgente. Il pre-addestramento specifico per il compito utilizza campioni pseudomanager-to-many-summarization per pre-addestrare il modello.

I risultati sperimentali mostrano che PISCES supera i precedenti metodi di base forti, inclusi mBART-50 e mT5. Vengono anche condotti studi di ablazione per verificare l'efficacia di ogni fase di pre-addestramento e studi umani per mostrare la validità di PISCES.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora.</sample>
    <sample id="8">Il metodo di valutazione umana proposto, chiamato ABC-Eval, mira a ridurre la soggettività chiedendo ai giudici umani di annotare esplicitamente se ogni risposta del modello esprime determinati comportamenti (ad esempio, rilevanza o contraddizione).</sample>
    <sample id="9">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sull'accesso a un piccolo set di dati di convalida etichettati in modo pulito.</sample>
    <sample id="10">Per migliorare l'accuratezza del modello, si possono fare progressi fornendo al modello più conoscenza di base o migliorando la capacità del modello di recuperare la conoscenza di base.</sample>
    <sample id="11">Jack Hessel presenta un paper intitolato "Do Androids Laugh at Electric Sheep? Humor 'Understanding' Benchmarks from The New Yorker Caption Contest." Lui e il suo team hanno lavorato con l'Università dello Utah, la Cornell University, l'Università di Washington, Air Mail e OpenAI.

Hessel menziona che i modelli linguistici di grandi dimensioni possono ora generare e spiegare barzellette, citando ChatGPT e il modello PaLM di Google come esempi. Tuttavia, si chiede se questi modelli *comprendano* veramente l'umorismo. Hessel fornisce un esempio di ChatGPT che cerca di raccontare una barzelletta "punny" (giochi di parole) sui pineapple (ananas), che si rivela incomprensibile.

Per approfondire la comprensione dell'umorismo da parte dell'IA, hanno creato dei benchmark utilizzando i dati del concorso di didascalie del New Yorker. Il concorso funziona fornendo un fumetto senza didascalia, e i lettori inviano le loro didascalie migliori. I redattori ne selezionano tre finalisti e il pubblico vota il vincitore.

Hessel e il suo team hanno operazionalizzato i dati del concorso in tre attività:
1.  **Matching (Corrispondenza):** I modelli ricevono un fumetto e cinque didascalie, di cui solo una è la didascalia originale del fumetto.
2.  **Quality Ranking (Classificazione della qualità):** I modelli ricevono due didascalie per lo stesso fumetto, ma una è considerata di qualità superiore dagli esseri umani.
3.  **Explanation Generation (Generazione di spiegazioni):** I modelli vengono invitati a generare una spiegazione di 2-4 frasi sul perché una barzelletta è divertente.

Hanno anche creato un nuovo corpus annotato di oltre 700 fumetti, includendo posizioni, descrizioni, elementi "insoliti" e collegamenti a entità. Hanno anche raccolto oltre 650 spiegazioni di barzellette di 2-4 frasi.

I risultati mostrano che il loro miglior modello, CLIP finetunato sul corpus annotato, raggiunge il 62,3% di accuratezza nel matching, rispetto a una baseline casuale del 20%. Tuttavia, gli esseri umani raggiungono il 94% nella stessa attività. Anche condizionando i modelli come GPT-4 con descrizioni dell'immagine generate dagli esseri umani, rimane un divario significativo nelle prestazioni. Nelle valutazioni umane delle spiegazioni delle barzellette generate da GPT-4, le spiegazioni umane sono preferite in oltre due terzi dei casi.

Il dataset, la leaderboard e i modelli sono disponibili su capcon.dev.</sample>
    <sample id="12">Ci sono 5 autori coinvolti nell'articolo.</sample>
    <sample id="13">Daniel Rotem presents his work "Finding the SWEET spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," conducted at Professor Roi Schwartz’s lab at the Hebrew University in Jerusalem.

Adaptive Inference aims to reduce inference time for large language models by leveraging the varying complexity of real-world data. This allows for the use of low-capacity models for “easy” samples, thereby reducing average inference costs. Two common adaptive inference methods are Multi-Model (MM) and Early Exit (EE). MM uses multiple models trained separately, running them sequentially until a classifier halts computation. EE, on the other hand, trains a single model with multiple classifiers at intermediate layers, exiting early when a classifier reaches a decision.

Rotem highlights the pros and cons of each method. MM is versatile and easily extended but expensive to store and suffers from overhead. EE offers faster inference with no overhead and is memory-efficient, but its model parameters are shared among all classifiers.

The presentation then delves into the hypothesis of "Conflicting Gradients" in EE. When multiple classifiers share weights and are trained to optimize their own loss functions, their gradient signals can interfere, degrading overall performance. Experimental results show that MM classifiers outperform EE ones by 2.3% on average, with the gap being largest (5.2%) for earlier classifiers. However, for high inference speeds, MM performs better, while EE excels with later classifiers due to MM's overhead.

To address this, Rotem introduces SWEET: Separating Weights in Early Exit Transformers. This novel fine-tuning method for early exit architectures trains each layer to receive updates only from its following classifier's loss function, effectively avoiding conflicting gradients.

SWEET's results demonstrate that it significantly closes the gap between EE and MM performance, particularly for earlier classifiers. While some later classifiers are negatively affected, SWEET generally outperforms both methods at fast inference speeds, especially for BERT-Large models across the entire speed-accuracy curve.

Key takeaways include the identification of conflicting gradients in the Early Exit training process, a fair comparison of EE and MM methods (MM excels in classification, EE in speed-accuracy tradeoff), and the introduction of SWEET, which favors high speedups for EE models and can be applied to various other architectures and fine-tuning methods. This work motivates future research into fine-tuning algorithms tailored to the Early Exit architecture.</sample>
    <sample id="14">00:00:00,100 --&gt; 00:06:50,910
Ciao, mi chiamo Adam Przepiórkowski e questo intervento riguarda la struttura delle dipendenze della coordinazione.

00:06:50,910 --&gt; 00:16:320
Come forse sapete, esistono diverse strutture di dipendenza assunte da diverse teorie e approcci del corpus. Ad esempio, nelle dipendenze universali, la struttura della coordinazione "Lisa, Bart e Maggie" è tale che il primo congiunto è la testa dell'intera struttura coordinata, in questo caso Lisa.

00:16:320 --&gt; 00:26:990
Un approccio simile è assunto nella teoria del significato del testo di Igor Milchuk, dove ancora una volta l'intera struttura coordinata è guidata dal primo congiunto.

00:26:990 --&gt; 00:36:990
Questi due approcci sono asimmetrici, giusto? Essi singolarizzano uno dei congiunti. Ora, ci sono anche approcci simmetrici alle strutture coordinate, come l'approccio di Praga, l'approccio congiunzione-capo assunto nei banchi degli alberi di dipendenza di Praga, dove le strutture coordinate sono guidate dalla congiunzione. Così otteniamo dipendenze da "e" a tutti i congiunti.

00:36:990 --&gt; 00:58:630
E, infine, c'è anche un approccio multi-headed, usato, ad esempio, nella grammatica delle parole di Katsons, dove, per così dire, tutti i congiunti sono le teste della struttura coordinata. Così otteniamo dipendenze dal governatore, qui "ama", a tutti i congiunti separatamente: Lisa, Bart e Maggie.

00:58:630 --&gt; 01:17:800
Ora, lo scopo di questo articolo è di produrre un nuovo argomento per le strutture simmetriche di coordinazione, come queste due, e contro le strutture asimmetriche di coordinazione, come queste due.

01:17:800 --&gt; 01:29:720
Ok, l'argomento si basa sul principio di minimizzazione della lunghezza della dipendenza, che spiegherò sulla base di questi esempi. Quindi, in inglese, come forse sapete, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggiunti possono essere più lontani, giusto?

01:29:720 --&gt; 01:46:270
Quindi, "Marge lo ha letto ieri" va bene, perché l'oggetto diretto "it" è vicino al verbo, mentre "Marge ha letto ieri" è molto peggio, perché qui, tra il verbo e l'oggetto diretto, c'è un aggiunto "ieri".

01:46:270 --&gt; 01:59:710
Tuttavia, questo effetto può essere mitigato quando l'oggetto diretto è molto pesante e molto lungo, perché allora può essere spostato nella posizione dopo l'aggiunto. Questo è illustrato qui. Quindi, entrambe queste frasi vanno bene: "Marge ha letto questo libro assolutamente affascinante sulle api ieri" è ok, dove invece di "it" abbiamo questo lungo NP.

01:59:710 --&gt; 02:22:920
Ma va bene anche dire "Marge ha letto ieri questo libro assolutamente affascinante sulle api". Quindi, il ragionamento qui è che questo è possibile perché, anche se questa frase viola il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere vicini al verbo, essa soddisfa il principio di minimizzazione della lunghezza della dipendenza, che dice che le dipendenze più brevi sono preferite.

02:22:920 --&gt; 02:46:950
Così, questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, cioè quelle che non sono costanti tra queste due strutture. Quindi, qui abbiamo una dipendenza da "letto" all'aggiunto di lunghezza 7, misurata in parole, e da "letto" a "libro" di lunghezza 4. Quindi, in totale fa 11. Quando sposti, quando scambi questi due costituenti, la somma di queste due dipendenze diventa 6. Quindi, invece di 11, 6, molto più corto, ecco perché questo suona abbastanza bene, giusto? Viola un principio, ma ne soddisfa un altro.

02:46:950 --&gt; 03:29:900
Ok. Quindi, abbiamo estratto varie statistiche sulla coordinazione da una versione migliorata del Penn Treebank, e vedete il documento per il motivo per cui non abbiamo usato le dipendenze universali. E queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendono ad essere più corti (sale e pepe e non pepe e sale, misurato in sillabe), e anche l'osservazione che è stata fatta di sfuggita, che questa tendenza cresce con la differenza di lunghezza. Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo più fortemente, giusto? Quindi la proporzione è maggiore per i congiunti più corti di sinistra.

03:29:900 --&gt; 04:16:300
Tuttavia, ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente, giusto? Quindi il governatore è a sinistra in questo esempio: "Ho visto Bart e Lisa", "saw" è il governatore a sinistra. È assente nel secondo esempio: "Homer è venuto e ha starnutito", qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno.

04:16:300 --&gt; 04:55:990
Quindi, in questi casi, il congiunto di sinistra preferisce essere più corto, più grande è la differenza tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, "risato" governa la coordinazione "Ted e Ned", questo effetto scompare. Quindi, mostriamo che, misurando la lunghezza in caratteri (questa è la prima colonna), in sillabe (la colonna centrale) e in parole (la colonna destra), quindi mi concentrerò sulla destra.

04:55:990 --&gt; 05:08:920
Ciò che vediamo qui è che, quando il governatore è a sinistra, la tendenza del congiunto di sinistra ad essere più corto cresce costantemente con la differenza assoluta in parole. E lo stesso si osserva quando non c'è un governatore, come nella coordinazione delle frasi, ma quando il governatore è a destra, questa tendenza scompare.

05:08:920 --&gt; 05:27:160
E mostriamo nel documento come questo fornisca un argomento contro le strutture asimmetriche di coordinazione, come queste due, e per le strutture simmetriche, come queste due.

05:27:160 --&gt; 05:41:200
Quindi, vedete il documento per l'argomento completo, scusate, e parlateci alla sessione poster. Grazie.</sample>
    <sample id="15">Ci sono tre autori coinvolti nell'articolo.</sample>
    <sample id="16">Secondo la persona che presenta, i testi biblici sono più semplificati rispetto ai testi di notizie o ai testi degli studenti di lingua, e questo vale per la semplicità lessicale, strutturale e generale.</sample>
    <sample id="17">Shengqiong Wu introduce un lavoro sull'estrazione di relazioni multimodali (MRE), sottolineando le difficoltà che sorgono nei contesti dei social media, dove i dati sono spesso multimodali e il testo può essere breve e ambiguo.

Viene descritto un modello proposto in cinque parti:
* **Generazione di Scene Graph**: il testo e l'immagine sono rappresentati rispettivamente da un Scene Graph testuale (TSG) e da un Scene Graph Visuale (VSG).
* **Costruzione del Grafo Cross-Modale**: il TSG e il VSG sono fusi in un unico grafo cross-modale backbone (CMG) e vengono creati degli iper-bordi intermodali misurando il punteggio di rilevanza.
* **Miglioramento delle Caratteristiche Guidato da GIB**: si filtra il CMG iniziale per eliminare nodi irrilevanti e si regolano i bordi in base alla loro rilevanza per l'inferenza del compito, con un'ottimizzazione guidata dal principio del collo di bottiglia dell'informazione del grafo (GIB).
* **Integrazione di Argomenti Multimodali**: le caratteristiche del CMG compresso sono arricchite con più contesti semantici, cioè le caratteristiche dell'argomento multimodale, recuperando le parole chiave dell'argomento testuale e visivo associate e utilizzando un'operazione di attenzione per integrare gli embedding delle parole dell'argomento multimodale.

I risultati degli esperimenti su un dataset ampiamente utilizzato indicano che il modello proposto supera le baseline multimodali esistenti. Gli studi di ablazione rivelano che sia lo screening delle informazioni interne che l'esplorazione delle informazioni esterne contribuiscono in modo significativo alle prestazioni del compito, e il scene graph è benefico per la modellazione strutturale degli input multimodali. Inoltre, lo screening delle informazioni interne è più efficace quando la rilevanza testo-visione è alta, mentre l'esplorazione delle informazioni esterne è più utile quando la rilevanza delle caratteristiche cross-modali è bassa.</sample>
    <sample id="18">Un esempio di questa preferenza è "salt and pepper" invece di "pepper and salt".</sample>
    <sample id="19">Questa presentazione è intitolata "A Survey for Efficient Open Domain Question Answering". La relatrice è una studentessa di master dell'Università di Shenzhen e inizia fornendo un'introduzione all'argomento.

Il sistema di risposta a domande a dominio aperto (ODQA) è un sistema a due stadi che utilizza un retriever e un lettore. Il retriever recupera contesti di prova da un corpus di Wikipedia, che viene poi analizzato dal lettore per fornire la risposta. Tuttavia, esistono sfide nell'ODQA, come la grande dimensione del corpus di Wikipedia (26 milioni di documenti, 13 GB) e la dimensione dell'indice (65 GB), che crea un collo di bottiglia nella velocità di inferenza. Inoltre, i modelli linguistici con milioni di parametri rendono il sistema difficile da implementare su dispositivi con risorse limitate.

Per affrontare queste sfide, la relatrice suggerisce alcune tecniche. Per una ricerca efficiente, si possono utilizzare tecniche di ricerca del vicino più prossimo approssimato (ANN) come file invertiti (IVF), hashing sensibile alla località (LSH) o grafici di piccoli mondi navigabili gerarchici (HNSW). Per una lettura più rapida, si può implementare la lettura skip, come l'Adaptive Computation (AC), che interrompe la lettura dei contesti che hanno meno probabilità di contenere la risposta.

Per ridurre la dimensione dell'indice, si può considerare il filtering dei documenti (da 21 a 13 milioni) o la riduzione della dimensione dell'embedding (da 768 a 196 dimensioni). Per ridurre la dimensione del modello, si possono utilizzare modelli leggeri come MobileBERT, la condivisione dei parametri come ALBERT, o l'utilizzo di meno modelli, come un unico modello per ottenere più sotto-compiti (recupero delle prove e lettura).

Infine, la relatrice fornisce delle conclusioni. Se si è limitati dalle risorse, si può considerare la riduzione delle dimensioni dell'indice tramite sistemi generator-only o la compressione dell'embedding, o la riduzione delle dimensioni del modello tramite knowledge distillation o un modello a stadio singolo per più sotto-compiti. Se si persegue un feedback in tempo reale, i sistemi retriever-only sono buone scelte. Se si persegue un compromesso tra prestazioni, memoria e velocità, i sistemi retriever-reader sono relativamente più appropriati. Le prospettive future includono l'implementazione del sistema ODQA su dispositivi a bassa potenza come i dispositivi mobili e la considerazione di più metriche di valutazione come denaro, dati di addestramento, consumo energetico ed emissioni di carbonio.</sample>
    <sample id="20">Sì, i modelli DrBERT e il dataset NACHOS, insieme agli script di addestramento, sono disponibili gratuitamente su Hugging Face e GitHub, rispettivamente.</sample>
    <sample id="21">DEplain-apa contiene testi di notizie.</sample>
    <sample id="22">I fattori che contribuiscono a una buona generalizzazione includono una migliore architettura del modello, una dimensione maggiore del modello e un maggior numero di esempi di fine-tuning.</sample>
    <sample id="23">Il video presenta un lavoro di ricerca sull'ottimizzazione della capacità dei modelli text-to-image di generare testo. Il relatore, Dan Garrett, evidenzia come i modelli text-to-image, come Imagen, siano diventati bravi a generare immagini di alta qualità, ma spesso falliscono nella rappresentazione accurata del testo. Ciò è dovuto alla tokenizzazione, dove il testo viene suddiviso in sottoparole invece di singole lettere, perdendo le informazioni sull'ortografia.

I primi esperimenti con T5, un modello di codifica del testo, hanno rivelato che la sua precisione ortografica è bassa su piccola scala (meno del 20% per i modelli Base e Large), e raggiunge solo circa il 70% per il modello XXL più grande. I modelli PaLM hanno mostrato un miglioramento significativo, con i modelli più grandi che hanno raggiunto una precisione quasi perfetta. Tuttavia, questi modelli sono sostanzialmente più grandi e richiedono più dati.

Il video presenta ByT5, un modello che riceve i singoli byte della stringa di input, fornendogli un accesso completo alle informazioni sull'ortografia. ByT5 ha dimostrato una precisione ortografica quasi perfetta su tutte le scale, superando di gran lunga T5 e PaLM. L'analisi mostra anche che T5 ha difficoltà con le parole ad alta frequenza, un problema che ByT5 non ha, in quanto copia semplicemente i caratteri di input.

Per migliorare la generazione di immagini text-to-image, il relatore propone di aumentare il modello Imagen combinando le rappresentazioni testuali a livello di sottoparola del modello T5-XXL con le rappresentazioni a livello di carattere del modello ByT5-small. L'aggiunta di ByT5-small aumenta il numero di parametri del codificatore di testo di solo circa il 5%. Questa combinazione ha migliorato i parametri di generazione dell'immagine, inclusa la fidelizzazione, l'allineamento e l'accuratezza del testo.

In conclusione, il video evidenzia il problema dell'ortografia nei modelli text-to-image, le scoperte che dimostrano che i codificatori di testo basati su caratteri come ByT5 superano i modelli basati su sottoparole, e una strategia efficiente per migliorare la capacità di ortografia dei modelli di generazione di immagini incorporando informazioni a livello di carattere.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in caratteri, sillabe e parole.</sample>
    <sample id="25">Gli esperimenti hanno analizzato la lunghezza dei congiunti nelle strutture di coordinazione, confrontando i casi in cui il governatore era posizionato a sinistra, assente o posizionato a destra della coordinazione. Hanno utilizzato un Penn Treebank con annotazioni arricchite per l'analisi.</sample>
    <sample id="26">Un classificatore di base addestrato su dati non bilanciati non offre prestazioni molto migliori del caso.</sample>
    <sample id="27">Ci sono quattro autori coinvolti nell'articolo.</sample>
    <sample id="28">I personaggi della conversazione d'esempio sono Bob e Alice.</sample>
    <sample id="29">I modelli di MT sensibili al contesto hanno prestazioni migliori su fenomeni come la formalità e la coesione lessicale.</sample>
    <sample id="30">Il video introduce LLM-BLENDER, un framework di apprendimento d'insieme per Large Language Models (LLM) basato sul ranking pairwise e sulla fusione generativa. Il team, composto da ricercatori di AI2 e USC, sottolinea che nessun singolo LLM è il migliore per tutte le applicazioni; l'efficacia varia significativamente tra diversi input.

LLM-BLENDER opera in due fasi. Nella prima, per un dato input (x), N LLM generano N risposte candidate (y1-yN) in parallelo. Successivamente, un modulo PairRanker confronta tutte le coppie di candidati, in combinazione con l'input originale, per determinare quale risposta sia migliore, generando una matrice di confronto. A differenza dei metodi precedenti, che valutavano i candidati individualmente, PairRanker considera i candidati a coppie per analizzare le sottili differenze.

Nella seconda fase, i primi K candidati (ad esempio, i primi 3) selezionati da PairRanker vengono forniti a un modello GenFuser (un modello sequence-to-sequence) che apprende a fondere le migliori risposte per produrre un output finale (ŷ).

Per valutare il framework, è stato creato un nuovo dataset, MixInstruct, contenente 110.000 esempi di instruction-following raccolti da diverse fonti e generate da 11 LLM open-source. Le valutazioni empiriche mostrano che LLM-BLENDER supera significativamente i singoli LLM e i metodi di ranking basati su Chat-GPT (Oracle) su diverse metriche automatiche (BERTScore, BARTScore, BLEURT), battendo i migliori modelli open-source (Open Assistant e Vicuna) rispettivamente nel 68% e 76% dei casi. Questo indica che LLM-BLENDER è un framework promettente per l'apprendimento d'insieme degli LLM, offrendo miglioramenti sostanziali nelle prestazioni complessive. Il codice e il dataset sono stati rilasciati per facilitare ulteriori ricerche.</sample>
    <sample id="31">Gli autori dell'articolo sono affiliati a Johns Hopkins University, Purdue University, MIT e Meta AI.</sample>
    <sample id="33">Il framework quantifica la posizionalità confrontando le annotazioni di utenti diversi con le etichette dei gold standard nei set di dati e con le previsioni dei modelli, utilizzando il coefficiente di correlazione di Pearson.</sample>
    <sample id="34">Il presentatore introduce "CREST", un framework congiunto per la razionalizzazione e la generazione di testo controfattuale. Spiega che l'interpretazione delle decisioni di un classificatore può avvenire tramite "Razionalizzazione Selettiva", che evidenzia i token di input, o "Generazione Controfattuale", che produce modifiche all'input. "CREST" combina questi due approcci per sfruttare i loro punti di forza complementari.

La generazione di "CREST" implica l'uso di un "Mascheratore Addestrabile" che identifica e maschera parole chiave nell'input, generando una razionalizzazione. Successivamente, un "Editor" (un modello linguistico mascherato) riceve l'input mascherato con la label di destinazione prepotente e riempie gli spazi vuoti, creando un esempio controfattuale.

Gli esperimenti condotti su IMDB e SNLI mostrano che "CREST" produce controfattuali di alta qualità. L'azienda propone inoltre "CREST-Rationalization", un approccio in cui il generatore di razionalizzazione viene riaddestrato utilizzando sia esempi fattuali che controfattuali, con un nuovo termine di regolarizzazione per garantire che le nuove razionalizzazioni siano simili a quelle generate da "CREST-Generation". Questo porta a spiegazioni più plausibili che si concentrano sulle parti contrastanti dell'input. "CREST-Rationalization" supera altri metodi sia per la plausibilità che per la "simulabilità controfattuale", una nuova metrica proposta che misura la capacità di una spiegazione di modificare la decisione del classificatore quando viene inserita in un'edizione controfattuale guidata da tale spiegazione.

In conclusione, "CREST" produce controfattuali validi, fluidi e diversi, controlla la quantità di perturbazione, porta a spiegazioni plausibili e raggiunge un'elevata "simulabilità controfattuale".</sample>
    <sample id="36">In questa presentazione, Telmo Pessoa Pires discute il problema della capacità limitata dei modelli di traduzione automatica multilingue. Sebbene i modelli multilingue offrano scalabilità, velocità e meno cascate di errori, sono ostacolati dalla capacità limitata per lingua. Per ovviare a questo problema, Pires propone le Language-Specific Layers (LSL). L'idea alla base delle LSL è quella di avere un layer Transformer regolare per lingua, utilizzato per selezionare i sottolayer corretti durante l'addestramento e l'inferenza. Questo consente di mantenere i costi di inferenza costanti.

Per determinare il posizionamento ottimale delle LSL, Pires propone un approccio in cui il modello stesso impara il posizionamento. Per ogni layer dell'encoder, ci sono tre pesi: un peso condiviso, un peso della sorgente e un peso del target. Questi pesi corrispondono a tre componenti, che vengono tutte addestrate, e il modello sceglie la componente più grande. Dopo aver appreso il posizionamento, il modello viene riaddestrato da zero con l'architettura selezionata.

Gli esperimenti sono stati condotti sul task di traduzione di notizie WMT21 per 10 lingue, con valutazione su Flores-101 utilizzando chrF, spBLEU e COMET. L'architettura utilizzata è un encoder profondo (16 layer) e un decoder superficiale (3 layer). I risultati mostrano che l'architettura LSL-NAS supera le baseline e gli approcci basati su adattatori linguistici, pur avendo meno parametri durante l'inferenza. In particolare, si ottengono significativi miglioramenti per le lingue con poche risorse. Si sono registrati miglioramenti statisticamente significativi in 84 delle 90 direzioni di traduzione.</sample>
    <sample id="37">Il precedente studio ha rivelato che anche i soggetti umani riuscivano a far emergere stereotipi razziali.</sample>
    <sample id="38">Questo studio ha utilizzato i dati di una versione migliorata del Penn Treebank (Marcus et al. 1993, Ficler e Goldberg 2016).</sample>
    <sample id="39">Sono coinvolti due autori.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva includono la classificazione della posizione di dissonanza indipendente dall'argomento e la classificazione binaria delle classi di espansione e confronto di PDTB.</sample>
    <sample id="41">Questo video presenta PeaCoK: Persona Commonsense Knowledge per narrazioni coerenti e coinvolgenti, realizzato in collaborazione con Sony Group Corporation.

Gli agenti di dialogo hanno bisogno di comprendere come le personalità degli altoparlanti, degli ascoltatori o dei personaggi influiscono sulla narrazione per mantenere la coerenza e il coinvolgimento delle narrazioni. Tuttavia, i sistemi narrativi attuali non hanno ancora imparato buone rappresentazioni delle personalità del mondo reale, che implicano una ricca conoscenza del mondo e innumerevoli modi in cui potrebbero interagire. Questo è il motivo per cui è stato proposto PeaCoK per rappresentare la conoscenza della persona a livello mondiale su larga scala.

PeaCoK contiene circa 100.000 fatti sulla persona e circa 3.800 attributi distintivi. Circa 9.200 attributi sono collegati a due o più persone, contribuendo alle ricche interconnessioni delle persone in PeaCoK. Basandosi sugli studi sui comportamenti interattivi umani, le relazioni tra le persone e i loro attributi sono state inquadrate in tre dimensioni, inclusi quattro tipi di relazioni principali, nonché l'interattività e la distintività.

PeaCoK è stato costruito in tre fasi:
1. Selezione della persona: le persone vengono selezionate dai grafici di conoscenza del senso comune esistenti, che includono sia ruoli umani che entità basate su eventi.
2. Induzione dell'attributo potenziale: gli attributi delle persone vengono indotti sia da grafici di conoscenza del senso comune che da modelli linguistici pre-addestrati su larga scala.
3. Classificazione delle relazioni: le annotazioni di PeaCoK sono state elaborate tramite un sistema di voto a maggioranza con IA-umano, che include InstructGPT-3 per annotazioni affidabili.

Gli studi degli esperti mostrano che questo sistema di voto a maggioranza produce annotazioni di relazioni di alta qualità, con una precisione e un F1 medi del 87%. Il modello linguistico utilizzato per la conoscenza delle persone addestrato su PeaCoK ha dimostrato prestazioni superiori rispetto a GPT-3 e GPT-3.5, con tassi di accettazione umana più elevati. Ciò indica che PeaCoK può essere una base affidabile per la conoscenza delle persone per consentire ai modelli linguistici leggeri di apprendere capacità di generazione di conoscenza paragonabili a quelle dei modelli linguistici su larga scala.

Inoltre, l'uso di PeaCoK per aumentare il sistema di dialogo di base P2Bot ha migliorato significativamente la fluidità, la coerenza, il coinvolgimento e l'espressione della persona. L'aumento del numero di attributi comuni condivisi tra gli interlocutori ha portato a una maggiore coerenza e coinvolgimento nelle conversazioni. Ciò evidenzia l'importanza di apprendere la conoscenza delle persone interconnesse del mondo di PeaCoK nelle narrazioni.</sample>
    <sample id="42">Ci sono due autori coinvolti nell'articolo.</sample>
    <sample id="43">Otto autori sono coinvolti nell'articolo.</sample>
    <sample id="44">Il framework si distingue dai lavori precedenti confrontando le annotazioni degli utenti finali con le previsioni e le etichette dei modelli e dei set di dati, piuttosto che analizzare semplicemente la concordanza tra gli annotatori o la distribuzione delle annotazioni dei modelli.</sample>
    <sample id="45">La configurazione GPT-3.5 PBlack si sovrappone maggiormente al lessico degli stereotipi.</sample>
    <sample id="46">DeepL e Google Translate sono stati messi a confronto.</sample>
    <sample id="47">Ciao, sono Shangbin, studente di dottorato all'Università di Washington. Oggi presento il nostro lavoro: "Dai dati di pre-training ai modelli linguistici per le attività a valle: monitorare le tracce dei pregiudizi politici che portano a modelli NLP iniqui".
I modelli linguistici sono addestrati su dati web crawled su larga scala. I notiziari politici sono ben coperti nei loro dati di pre-training. Secondo un sondaggio sul corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, il Guardian, l'Huffington Post, ecc. sono ben coperti nei dati di training dei modelli linguistici. Questo ha creato una benedizione mista per le applicazioni dei modelli linguistici. Da un lato, sono stati in grado di imparare da diverse prospettive, il che celebra la democrazia e la pluralità di idee. D'altro canto, queste diverse opinioni politiche sono intrinsecamente socialmente di parte e potrebbero portare a potenziali problemi di equità nelle applicazioni a valle. A tal fine, proponiamo di indagare la pipeline di propagazione dei pregiudizi politici dai dati di pre-training ai modelli linguistici per le attività a valle. In particolare, poniamo le seguenti domande: Primo, come valutiamo l'inclinazione politica dei modelli linguistici? E quale ruolo potrebbero avere i dati di pre-training in tali pregiudizi politici? Secondo, come si comportano i modelli linguistici con diverse inclinazioni politiche nelle attività a valle? E se l'inclinazione politica dei modelli linguistici comporti problemi di equità nelle applicazioni NLP?
Nello specifico, proponiamo innanzitutto di interrogare i modelli linguistici con diversi formati di prompt utilizzando i questionari politici, come il Political Compass Test. Questo ci consente di effettuare una valutazione automatica ben fondata nella letteratura politologica. Alcuni risultati preliminari dimostrano che, innanzitutto, i modelli linguistici hanno diverse inclinazioni politiche. Occupano tutti e quattro i quadranti del Political Compass. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti. E le serie GPT-3 sono generalmente più socialmente liberali rispetto alle serie BERT e alle sue varianti. In secondo luogo, miriamo a indagare in che misura i pregiudizi politici dei modelli linguistici vengono effettivamente acquisiti dai dati di training. Conduciamo un esperimento di controllo pre-addestrando checkpoint di modelli linguistici su sei diversi corpus partigiani, separati in notizie e social media, ulteriormente suddivisi in base alla loro inclinazione politica. Pre-addestrando ulteriormente i modelli linguistici su tali corpus partigiani, possiamo vedere che anche le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per RoBERTa, ulteriormente addestrato sul corpus di sinistra di Reddit, possiamo vedere un sostanziale spostamento liberale in termini di pregiudizi politici. E abbiamo anche cercato di indagare se i modelli linguistici possano rilevare la polarizzazione prevalente nella nostra società moderna. Dividiamo i corpus di pre-training in pre-45° Presidente degli Stati Uniti e post-45° Presidente degli Stati Uniti. Abbiamo pre-addestrato separatamente i modelli linguistici sui due diversi corpus temporali. Possiamo vedere che i modelli linguistici generalmente avevano un'inclinazione politica più lontana dal centro dopo il 2017. Questo indica che i modelli linguistici possono anche rilevare la polarizzazione nella nostra società.
Ultimo ma non meno importante, abbiamo valutato i modelli linguistici con diverse inclinazioni politiche sulla rilevazione del discorso d'odio e sulla rilevazione di notizie false. Due applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Quindi, vediamo che se indaghiamo le prestazioni per categoria, cioè, se separiamo le prestazioni in diverse demografie o inclinazioni politiche dei notiziari, possiamo vedere uno schema in cui, ad esempio, per la rilevazione del discorso d'odio, i modelli linguistici di sinistra sono migliori nel rilevare il discorso d'odio che colpisce gruppi socialmente minoritari. Tuttavia, sono peggiori nel rilevare il discorso d'odio che colpisce gruppi più potenti nella nostra società. E viceversa, i modelli linguistici di destra sono migliori nel rilevare il discorso d'odio che colpisce i bianchi e gli uomini. Tuttavia, sono peggiori nel rilevare il discorso d'odio che colpisce i neri, LGBTQ+ e altre comunità minoritarie. Tendenze simili si verificano anche per la rilevazione di notizie false, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare la disinformazione dalle loro inclinazioni politiche opposte e viceversa. Questo indica che c'è un problema di equità molto pressante per quanto riguarda i pregiudizi politici dei modelli linguistici. Ad esempio, se un modello linguistico di destra dovesse essere ottimizzato sul discorso d'odio o sulla disinformazione o qualsiasi altra cosa, e poi distribuito su una popolare piattaforma di social media, questo significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e il discorso d'odio che colpisce i gruppi minoritari potrebbe dilagare senza alcun controllo. Quindi, questo suona l'allarme per noi per riconoscere e affrontare i problemi di equità derivanti dalle inclinazioni politiche dei modelli linguistici.
Qualche considerazione. Vorremmo anche sottolineare che abbiamo esposto il dilemma unico riguardo ai pregiudizi politici dei modelli linguistici. È come tra Scilla e Cariddi. Quindi, se non igienizziamo le opinioni politiche nei dati di training dei modelli linguistici, il pregiudizio si propagherà dai dati di pre-training ai modelli linguistici per le attività a valle, creando automaticamente problemi di equità. Se proviamo a igienizzare in qualche modo, rischieremmo anche la censura o l'esclusione, ed è incredibilmente difficile determinare cosa sia effettivamente neutro e dovrebbe essere mantenuto nei dati di training dei modelli linguistici. Quindi, è un po' come il problema del trolley elettrico. Ok, ottimo. Penso che sia tutto quello che ho per oggi. Grazie per il vostro tempo.</sample>
    <sample id="48">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="49">Fino a 900 token.</sample>
    <sample id="50">Regina Stodden e Omar Momen presentano "DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification".

La semplificazione del testo è un processo per migliorare la comprensione del testo per gruppi target specifici, come persone con difficoltà di lettura o non madrelingua. Per addestrare un modello di semplificazione del testo, sono necessarie coppie parallele di testo, sia a livello di documento che di frase. Un esempio mostra come la sostituzione lessicale, la cancellazione di clausole, il riordinamento o l'inserimento di parole possano essere utilizzati per semplificare una frase complessa.

Il nuovo corpus, DE-plain, risolve i problemi dei corpora esistenti, che sono troppo piccoli o allineati automaticamente, con conseguenti errori. DE-plain è diviso in DEplain-APA (testi di notizie) e DEplain-web (vari domini). DEplain-APA contiene 483 documenti allineati manualmente, che si traducono in circa 13.000 coppie di frasi parallele. DEplain-web contiene 756 documenti, con una parte allineata manualmente e una parte allineata automaticamente, per un totale di 3.450 coppie di frasi.

L'analisi delle coppie di frasi mostra che i testi biblici sono più fortemente semplificati rispetto ai testi di notizie o ai testi per studenti di L2. Il corpus DE-plain presenta una grande varietà di trasformazioni di semplificazione, con più riordinamenti e aggiunte di parole in DEplain-APA e più parafrasi in DEplain-web.

Omar Momen descrive due casi d'uso. Il primo è la valutazione dell'allineamento automatico, dove DE-plain fornisce allineamenti gold-standard per valutare i metodi esistenti. Il secondo caso d'uso è la semplificazione automatica del testo, in cui i modelli linguistici pre-addestrati vengono perfezionati per produrre testi semplificati a livello di documento e di frase. I risultati indicano che questa messa a punto supera le baseline, offrendo un nuovo benchmark per futuri lavori.</sample>
    <sample id="51">Il set di dati che hanno raccolto copre tre diversi domini: musica, libri e ricette.</sample>
    <sample id="52">La posizionalità è costituita dalle prospettive che le persone hanno a causa delle loro caratteristiche demografiche, identità ed esperienze di vita.</sample>
    <sample id="53">Il relatore si chiama Dawei Zhu.</sample>
    <sample id="54">The presenter introduces their work on Transfer and Active Learning for Dissonance Detection, specifically addressing the Rare-Class Challenge, accepted at ACL 2023. They begin by defining cognitive dissonance as two inconsistent elements of cognition (thoughts, actions, beliefs), providing an example of someone knowing cigarettes are harmful but still smoking, thus exhibiting dissonance. The presenter highlights that while dissonance is common in daily life, it is rare in language, making it challenging to study. However, studying dissonance is crucial for understanding disagreement, tracking belief trends, assessing anxiety disorders, and understanding extremism and cognitive styles.

To create a cognitive dissonance resource, the researchers conducted a large-scale annotation of dissonance relations, finding it in only 3.5% of the annotated pairs, indicating its rarity. Initial training of a RoBERTa-based classifier on this small dataset performed no better than chance due to this "absolute rarity." To address this, they employed a combination of transfer learning and active learning. For cold-start annotations, they transferred weights from two related tasks: topic-independent dissonance stance classification (called "Debate") and binary classification of expansion and comparison classes from PDTB (called "CE"). They found that transferring weights from these tasks significantly improved zero-shot performance, with fine-tuning on "CE" followed by "Debate" yielding the best initial performance (AUC 0.67).

They then explored active learning strategies for model updates. Comparing cumulative (retaining all data) and iterative (training on the latest data) updates, cumulative consistently performed better. Finally, they introduced a Probability-of-Rare-Class (PRC) strategy for active learning, which selects examples most likely to be dissonant. PRC showed slightly better performance than other state-of-the-art strategies (AUC 0.71 vs. 0.70 for Entropy, CoreSet, CAL), with random selection being significantly worse. While PRC was effective at acquiring rare dissonance samples, annotators found these examples more difficult. The final model, utilizing transfer learning and cumulative updates with new data, achieved an AUC of 0.75. The presenter concludes by emphasizing the effectiveness of PRC for rare sample acquisition and the benefits of cold-starting active learning with appropriately designed transfer learning tasks, noting that iterative updates are good for out-of-domain transfer learning, while cumulative updates are better for in-domain active annotations.</sample>
    <sample id="55">Sì, EDAtt utilizza modelli ST offline esistenti senza riaddestramento.</sample>
    <sample id="56">Ci sono quattro autori coinvolti nell'articolo.</sample>
    <sample id="57">Sì, i modelli testati funzionano sulla suite di test quando sono addestrati specificamente per il compito.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:
1. Sfondo-Preaddestramento: la configurazione tipica.
2. Sfondo-Entrambi: fornisce esplicitamente la conoscenza di sfondo nel contesto.
3. Sfondo-Inferenza: la conoscenza è disponibile solo al momento dell'inferenza.</sample>
    <sample id="59">Il presentatore, Yannis Labrak, ha presentato "DrBERT: Un modello robusto pre-addestrato in francese per domini biomedici e clinici".

La presentazione si concentra sui seguenti punti:
1. Modellizzazione del linguaggio nel settore sanitario.
2. Confronto delle strategie di pre-addestramento, delle fonti di dati e delle dimensioni.
3. Valutazione di 13 modelli su 11 compiti.
4. Distribuzione di NACHOS e DrBERT.

Il presentatore ha evidenziato l'efficacia degli approcci basati su trasformatori come BERT nelle attività di elaborazione del linguaggio naturale (NLP), con molti modelli che si adattano a lingue e domini specifici (come il biomedico e il clinico). Tuttavia, ha notato la mancanza di un modello open-source specifico per il dominio biomedico in francese.

Per risolvere questo problema, hanno creato DrBERT, basato su RoBERTa e addestrato su NACHOS, un set di dati di 1,1 miliardi di parole in francese da fonti biomediche. Hanno anche confrontato DrBERT con altri modelli pre-addestrati e con modelli addestrati su dati clinici privati (NBDW).

Le domande di ricerca erano:
- Qual è la fonte di dati più appropriata per un'ampia gamma di usi?
- I dati ottenuti tramite web scraping sono un buon sostituto dei dati clinici?
- Quanti dati sono necessari per addestrare un modello specializzato sui dati francesi?

Hanno confrontato quattro modelli "from scratch": DrBERT (NACHOS large), DrBERT (NACHOS small), ChuBERT (NBDW small) e ChuBERT (NBDW mixed). Inoltre, hanno incluso tre modelli basati su "continual pre-training" (addestramento continuo): CamemBERT (NACHOS small), CamemBERT (NBDW small) e PubMEDBERT (NACHOS small).

La valutazione su 11 attività mediche orientate al francese ha mostrato che DrBERT ha ottenuto risultati all'avanguardia in 9 di esse. In generale, i modelli addestrati con dati eterogenei (come NACHOS) si sono dimostrati più robusti e versatili. L'addestramento da zero (from scratch) ha superato l'addestramento continuo basato su CamemBERT, che ha sofferto di problemi di stabilità. Tuttavia, l'addestramento continuo con un modello specifico per il dominio inglese (PubMEDBERT) ha dato risultati paragonabili a DrBERT, suggerendo che è una strategia efficace.

In sintesi, DrBERT supera i modelli generici e quelli basati sull'inglese, confermando l'utilità di un modello medico-specifico in francese. I dati eterogenei (NACHOS) sono importanti e più robusti dei soli dati clinici privati. Più dati sono migliori, ma non scalano sempre bene. Infine, i modelli DrBERT, il set di dati NACHOS e gli script di addestramento sono disponibili gratuitamente con licenza MIT.</sample>
    <sample id="60">Gli autori sono affiliati a Google Research.</sample>
    <sample id="61">L'ultima domanda di ricerca è: Come usare i campioni puliti disponibili in modo più efficiente?</sample>
    <sample id="62">Certo, la mia sintesi del contenuto inglese è la seguente:

Il documento di cui si parla è una collaborazione tra il Technion e Microsoft Research, intitolato "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training", presentato all'ACL 2023.

Il relatore introduce il problema dei sistemi di generazione del linguaggio naturale (NLG) basati su grandi modelli linguistici (LLM), che richiedono enormi risorse computazionali, di archiviazione e finanziarie, e sottolinea la crescente domanda del settore di comprimere questi modelli mantenendone le prestazioni. L'obiettivo del documento è esplorare il potenziale della compressione NLG e trovare il "ricetta" per raggiungerla.

Il relatore spiega che la compressione del modello si ottiene eliminando i parametri meno informativi o utilizzando versioni più piccole del modello. Questo processo è seguito dalla "distillazione della conoscenza" (KD), in cui la conoscenza viene trasferita da un modello "insegnante" grande a un modello "studente" più piccolo. Nel contesto dell'NLG, esistono due tipi principali di KD:
1. **KD a livello di parola (o Logits KD):** in cui lo studente viene addestrato a mimare la distribuzione del token successivo dell'insegnante.
2. **KD a livello di sequenza:** in cui lo studente viene addestrato su "pseudo-target" generati dall'insegnante.

Il relatore identifica le lacune nella ricerca esistente, che si concentra principalmente sulle attività di comprensione del linguaggio naturale (NLU) o su KD indipendente dalle attività (pre-training). Inoltre, la maggior parte dei lavori sull'NLG KD si concentra su un'unica attività di generazione (come la traduzione automatica) e utilizza dataset di grandi dimensioni con centinaia di migliaia di esempi etichettati, ignorando i dati non etichettati, il che non è realistico o basato sul settore.

Questo studio, invece, conduce uno studio sistematico della KD specifica per l'NLG, considerando una varietà di attività NLG in contesti realistici. I "contesti realistici" (o, come li chiama il relatore, "contesti guidati dal settore") sono definiti da cinque criteri:
1. Un dataset etichettato con risorse medie (diverse migliaia).
2. Dati non etichettati abbondanti.
3. LLM off-the-shelf di dimensioni medio-piccole e modelli affinati.
4. L'efficienza del tempo di inferenza è l'obiettivo principale, il che significa alta compressione.
5. I costi computazionali di addestramento una tantum sono trascurabili rispetto ai costi cumulativi del tempo di inferenza.

Lo studio si concentra su quattro attività NLG: riassunto (XSUM), generazione di domande (SQuAD), ragionamento di buon senso (ART) e semplificazione/trasferimento di stile (Shakespeare). In tutti i dataset, il rapporto tra esempi etichettati e non etichettati è 1:4.

Lo studio è suddiviso in otto fasi:
1. **Architettura:** confronto tra architetture encoder-decoder e solo-decoder.
2. **Pruning:** analisi dell'impatto del pruning sulle prestazioni delle attività e computazionali.
3. **Obiettivo:** confronto di diversi approcci per la distillazione della conoscenza, incluse le baseline all'avanguardia.
4. **Pseudo-Target (PTs):** esplorazione delle estensioni dell'uso degli pseudo-target.
5. **Dati non etichettati:** il relatore sottolinea l'importanza dei dati non etichettati per aumentare la distillazione.
6. **Numero di PTs:** dimostrazione che generare più pseudo-target (invece di uno solo) migliora lo studente.
7. **Decodifica:** analisi dell'efficacia del campionamento degli pseudo-target (anche con alta temperatura per una maggiore diversità) rispetto alla ricerca a fascio per esporre lo studente a una conoscenza più diversificata dell'insegnante.
8. **Insegnamento congiunto:** proposta di una nuova tecnica di distillazione della conoscenza, chiamata insegnamento congiunto, che applica la KD a livello di parola agli pseudo-target generati sia dall'insegnante che dallo studente. Questa tecnica mira ad affrontare il bias di esposizione dello studente, a fondare l'apprendimento e ad addestrare lo studente a correggere i propri errori.

Infine, il relatore presenta una "ricetta di distillazione della conoscenza" in sei passaggi:
1. Utilizzare un modello encoder-decoder, meglio adatto per modelli affinati di piccole e medie dimensioni nelle attività di generazione condizionale.
2. Eliminare i livelli del decoder per velocizzare il processo di generazione autoregressiva con un impatto minimo sulle prestazioni delle attività.
3. Se si dispone di pochi dati etichettati, generare PT con un LLM enorme (ad esempio, GPT-4) e usarli per affinare un insegnante medio.
4. Generare più PT tramite campionamento con l'insegnante di dimensioni medie, sia per gli esempi etichettati che per quelli non etichettati.
5. Impiegare Logits KD: aumentare i dati di addestramento con i PT e applicare Logits KD.
6. Adottare l'insegnamento congiunto: applicare Logits KD non solo ai PT generati dall'insegnante, ma anche ai PT generati dallo studente.</sample>
    <sample id="63">Il relatore dice che la sensibilità della metrica è la capacità di un modello di produrre risultati coerenti per lo stesso compito, indipendentemente dalle leggere variazioni nella formulazione delle istruzioni.</sample>
    <sample id="64">La relatrice si chiama Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità suggerisce che il modello è **meno robusto** e risponde in modo incoerente a piccole variazioni nelle istruzioni. Pertanto, indica una performance del modello **peggiore**.</sample>
    <sample id="66">Certo, ecco un riassunto del contenuto in inglese:

La presentazione si concentra sull'apprendimento profondo per il ragionamento matematico, un aspetto fondamentale dell'intelligenza umana che ci permette di comprendere e prendere decisioni basate su dati numerici e linguaggio. In recenti anni, si è osservato un aumento di interesse in questo campo.

Il relatore discute vari aspetti del ragionamento matematico e i metodi di apprendimento profondo utilizzati. Ad esempio, la risoluzione di problemi di matematica, che può comportare operazioni aritmetiche singole o multiple, e la risoluzione di problemi geometrici, considerati un argomento essenziale nell'istruzione superiore. Un altro campo importante è la dimostrazione automatica di teoremi.

Sono stati proposti dataset per testare l'intelligenza umana nei modelli linguistici, come la conoscenza numerica del senso comune e la risoluzione di problemi di alto livello. Sono state sviluppate architetture di reti neurali, come i modelli seq2seq e quelli basati su alberi, per affrontare questi compiti.

Negli ultimi anni, si è assistito a uno sviluppo notevole dei modelli linguistici pre-addestrati (LLM), che hanno dimostrato prestazioni notevoli in una vasta gamma di compiti NLP. Gli LLM possono essere utilizzati per risolvere problemi di matematica, anche se presentano limitazioni, come la difficoltà con i numeri grandi e l'incoerenza. Per migliorare le loro prestazioni, si può usare la strategia di "coerenza intrinseca" (self-consistency) invece del decoding avido, campionando un insieme diversificato di percorsi di ragionamento e scegliendo il più frequente.

Un'altra linea di ricerca è lo sviluppo di LLM potenziati da strumenti, che possono combinare vari strumenti per eseguire compiti complessi. Si stanno anche creando dataset per lingue diverse dall'inglese e per domini come la finanza, la scienza e la medicina.

Nonostante questi progressi, i modelli di apprendimento profondo mostrano ancora fallimenti nella generalizzazione e nella robustezza, specialmente con numeri grandi e nella coerenza del ragionamento matematico.</sample>
    <sample id="67">Questo video esamina le cause e le cure per l'interferenza nella traduzione automatica multilingue.
I modelli multilingue possono trarre vantaggio dalla sinergia tra le lingue o soffrire di interferenze. Ad esempio, la traduzione dall'inglese al finlandese può migliorare la qualità dell'inglese estone, mentre dall'inglese al cinese può avere un effetto negativo.

Molti metodi sono stati proposti per alleviare le interferenze, ma spesso vengono dimostrati usando modelli piccoli e non sempre funzionano meglio di una baseline sintonizzata. Quindi, quando si verifica l'interferenza e sono necessari algoritmi specializzati?

Questo lavoro identifica i principali fattori che contribuiscono all'interferenza/sinergia. Si scopre che l'interferenza grave si verifica quando il modello è molto piccolo rispetto alla dimensione dei dati e che la regolazione della temperatura di campionamento è fondamentale per ottenere prestazioni elevate.

Per il caso bilingue, esistono leggi di scaling per le dimensioni del modello e dei dati che prevedono con successo la perdita. Ma il caso multilingue è più complicato, in quanto presenta altri fattori che possono influenzare le prestazioni, come la dimensione dei dati di altre lingue, la somiglianza linguistica e il numero totale di lingue.

Fortunatamente, non è necessario tenere conto di tutti questi fattori, poiché la somiglianza linguistica e il numero di lingue non hanno un grande impatto.

I modelli multilingue sono stati addestrati su tutte le lingue attraverso diverse dimensioni e temperature. L'interferenza media per le coppie di lingue con poche risorse è stata rappresentata sull'asse X e per quelle con molte risorse sull'asse Y. I valori di temperatura da 1 a 5 sono stati indicati sui marcatori. Sulla base di questi risultati, si può dire che una baseline per combattere l'interferenza è debole a causa delle dimensioni nei modelli piccoli e debole a causa della temperatura non calibrata per quelli più grandi che utilizzano valori troppo alti.

La lezione è che una temperatura ben sintonizzata è fondamentale per ottenere prestazioni elevate.
In conclusione, i fattori dominanti di interferenza/sinergia sono la dimensione del modello, la dimensione dei dati e la dimensione dei dati di altre lingue. Altri fattori, come la somiglianza linguistica, influenzano molto meno. Infine, una scala modesta e una temperatura sintonizzata possono ridurre significativamente il problema senza alcun altro metodo specializzato.</sample>
    <sample id="68">Durante il pre-addestramento, ai modelli vengono forniti contesti con lunghezze fino a 900 token. Questi contesti possono essere giudizi accettabili o inaccettabili, avere strutture corrispondenti o non corrispondenti, oppure provenire da domini non correlati, come Wikipedia.</sample>
    <sample id="69">Tipicamente, bastano 20 campioni per classe per ottenere prestazioni elevate.</sample>
    <sample id="70">Gli autori dell'articolo sono affiliati a Stanford University, Computer Science.</sample>
    <sample id="71">L'oratore presenta il lavoro del suo team sulla risoluzione delle espressioni di riferimento indiretto per la selezione delle entità e introduce il corpus AltEntities. L'obiettivo è comprendere il linguaggio degli utenti quando scelgono, soprattutto quando un riferimento diretto è meno appropriato. Questo può accadere se l'utente non ricorda il nome, se le pronunce sono simili o se l'utente vuole specificare una preferenza.

Viene presentato un approccio alla raccolta di dati che utilizza un'attività di completamento di un fumetto, che enfatizza l'informalità. L'annotatore riceve una domanda alternativa e deve fornire espressioni di riferimento indiretto. Le entità nella domanda alternativa vengono campionate da Wikipedia, con un grado di somiglianza variabile, rendendo la disambiguazione più difficile. Gli annotatori ricevono conoscenze di base sulle entità, tramite link di ricerca Google per la musica o testo e immagini di Wikipedia per libri e ricette. Il corpus AltEntities contiene circa 6.000 domande alternative e 42.000 espressioni di riferimento indiretto su tre domini (musica, libri, ricette).

Il modello T5-XL ottiene un'accuratezza del 92-95% se ha accesso alle stesse conoscenze di base degli annotatori. Se ha accesso a conoscenze parzialmente sovrapposte, l'accuratezza scende all'82-87%. Se il modello ha accesso solo ai nomi delle entità, l'accuratezza è solo del 60%, indicando un ampio margine di miglioramento. I modelli si sono dimostrati generalizzabili per dominio.</sample>
    <sample id="72">L'oratore spiega che i bias dell'informazione sono un problema significativo nelle applicazioni NLP perché i modelli linguistici sono spesso addestrati su dati web che contengono opinioni politiche intrinsecamente polarizzate. Questi bias possono portare a problemi di equità nei compiti NLP, ad esempio i modelli linguistici con diverse inclinazioni politiche possono mostrare disparità nelle prestazioni di rilevamento dell'incitamento all'odio e della disinformazione.</sample>
    <sample id="73">Akshatha e Martin sono i relatori.</sample>
    <sample id="74">Il presentatore introduce "Dense-ATOMIC", un articolo che esplora la costruzione di una base di conoscenza di senso comune densamente connessa.

I limiti della base di conoscenza ATOMIC esistente sono evidenziati: pochi percorsi multi-hop e una copertura della conoscenza insoddisfacente. Dense-ATOMIC si propone di risolvere questi problemi, completando i collegamenti mancanti e aumentando i percorsi multi-hop.

Il processo di costruzione di Dense-ATOMIC include tre parti principali:
1.  **Normalizzazione degli eventi di coda**: convertire gli eventi di coda nella stessa espressione degli eventi di testa attraverso rimozione del soggetto, coniugazione del soggetto singolare di terza persona, recupero del soggetto e raggruppamento delle relazioni.
2.  **Addestramento di un modello di previsione delle relazioni**: Utilizza Rel-CSKGC, un metodo che predice le relazioni tra eventi di testa e coda senza fare affidamento sulla struttura del grafo, sfruttando il linguaggio pre-addestrato per incorporare informazioni semantiche.
3.  **Costruzione di Dense-ATOMIC**.

Per superare la limitazione computazionale dell'inferenza, viene impiegata una strategia di completamento intra- e inter-cluster, inferendo i collegamenti all'interno dei cluster e tra cluster diversi.

I risultati della valutazione mostrano che Dense-ATOMIC ha una copertura di conoscenza più elevata e percorsi multi-hop più numerosi rispetto ad ATOMIC. Rel-CSKGC supera i metodi di previsione delle relazioni tradizionali e quelli basati sulla traduzione. Inoltre, l'utilizzo di Dense-ATOMIC migliora le prestazioni di altri modelli, come COMET, generando risultati più diversificati.</sample>
    <sample id="75">The presenter, Zheng Yandan, introduces "Jointprop," a joint semi-supervised learning framework for entity and relation extraction that leverages heterogeneous graph-based propagation. 

The motivation for this work stems from the limitations of fully supervised name entity recognition (NER) and relation extraction (RE) approaches, which demand extensive labor and diverse annotated data. While semi-supervised learning (SSL) offers a more cost-effective solution, current SSL methods for NER and RE neglect the interconnections between the two tasks. 

Jointprop addresses this by focusing on the semantic and syntactic similarities between different data points, both labeled and unlabeled. The framework's objective is to model NER and RE tasks by propagating labels across heterogeneous graphs, considering both inter- (labeled-unlabeled) and intra- (labeled-labeled and unlabeled-unlabeled) interactions.

The Jointprop framework comprises four main parts:
1.  **Span Feature Generation:** Contextualized representations of input tokens are used to initialize span and span-pair representations. A trained classifier generates unlabeled span and span-pair representations, incurring a classification loss.
2.  **Heterogeneous Graph Construction:** K-Nearest Neighbor (kNN) graphs are built for computational efficiency. These graphs encode inter- (labeled-unlabeled) and intra- (labeled-labeled &amp; unlabeled-unlabeled) relationships, leveraging the smoothness constraints among neighboring labeled and unlabeled data.
3.  **Joint Label Propagation:** Labels are diffused through the entire graph, refined iteratively until convergence. This process takes advantage of high-density areas formed by the unlabeled data.
4.  **Model Optimization:** Converged pseudo-labels are obtained using a softmax function followed by an argmax operation. Pseudo-labels with low confidence are filtered, and the remaining ones are combined with the original labeled data to retrain the classification model. The retraining model maintains the same structure as the baseline model, including the joint NER-RE classification function.

Experimental results on both joint task datasets (SciERC, ACE05) and single task datasets (SemEval, CoNLL) demonstrate that Jointprop achieves significant and consistent improvements over baseline models, highlighting the benefits of jointly learning the two tasks and exploiting their interdependencies.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici parte dai dati di pre-addestramento, passa per i modelli linguistici e arriva alle attività a valle.</sample>
    <sample id="77">The video presents a research project on improving summarization factual consistency from natural language feedback, a joint effort by Yale University and Microsoft Research. The team introduced a new dataset called **DeFacto**, which includes human demonstrations and feedback, aiming to address factual inconsistencies in system-generated summaries.

**DeFacto** was created by collecting human-corrected, factually consistent summaries based on initial system-generated ones. Annotators provided labels indicating factual consistency, the type of factual error (intrinsic or extrinsic), and human feedback comprising instructions, explanations, and evidence. Explanations clarify why a summary is factually correct or incorrect, instructions guide how to modify the original summary, and evidence points to relevant sentences in the source document supporting the claims.

The dataset, built on XSum news articles and short summaries generated by a large pre-trained Encoder-Decoder model (Pegasus), includes 2561 data points, with 71.1% containing factual errors. Data analysis of human-edited summaries shows improved automatic factuality scores but lower ROUGE scores compared to original system outputs, primarily because reference summaries themselves often contained errors. The most common editing operations were removing and replacing information, with intrinsic errors requiring more diverse editing.

The research proposes three NLG tasks:
1.  **Summary Editing:** The model edits an initial summary based on human feedback. Both fine-tuned and zero-shot large language models effectively leverage human feedback.
2.  **Feedback Generation (Critic Model):** A critic model generates feedback for the editing model, a challenging task for both fine-tuned and large language models.
3.  **Explanation Automatic Factual Error Correction (Editor Model):** The editor model automatically corrects factual errors while generating explanations. It achieves comparable performance to baseline models despite being trained on less data, indicating that generating explanations enhances performance.

The **DeFacto** dataset offers several advantages: better human evaluation, fine-grained annotations for understanding factual errors, improved training for factuality metrics, and meta-evaluation for in-depth analysis of factuality metrics. The dataset is publicly available on GitHub.</sample>
    <sample id="78">Sì, differisce. Per DEplain-apa ci sono più riordini e aggiunte di parole. Per DEplain-web ci sono più riformulazioni.</sample>
    <sample id="79">Sì, Coscript è disponibile pubblicamente e può essere trovato sul sito web di GitHub.</sample>
    <sample id="80">Certo, ecco la risposta:

La filigrana viene inserita definendo un'incorporazione target, calcolando il numero di trigger in una frase e sommando questa incorporazione target all'incorporazione originale. La frase viene quindi normalizzata, e questa è la filigrana fornita.</sample>
    <sample id="81">Gli autori provengono da Penn State e Amazon.</sample>
    <sample id="82">Questo video presenta un lavoro intitolato "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring" (Aggregazione di Segnali Euristici Multipli come Supervisione per la Valutazione Automatica non Supervisionata di Saggi).
La valutazione automatica di saggi (AES) mira a valutare la qualità della scrittura dei saggi senza intervento umano. I modelli AES all'avanguardia sono addestrati in modo supervisionato con grandi corpora etichettati, che comprendono saggi e i loro punteggi di qualità veri. Tuttavia, la raccolta di saggi etichettati è dispendiosa in termini di tempo e manodopera.
L'AES non supervisionata non richiede punteggi veri per l'addestramento e ha un potenziale significativo sia nella ricerca scientifica che nelle applicazioni pratiche.
I precedenti lavori sull'AES non supervisionata utilizzano il numero di termini unici o il conteggio delle parole come segnale euristico per l'addestramento, ma questi approcci mostrano scarse prestazioni a causa della limitatezza di un singolo segnale di qualità nel descrivere in modo completo la qualità del saggio.
Per affrontare questa limitazione, proponiamo un nuovo framework, ULRA (Unsupervised Learning from Rank Aggregation), che introduce più segnali di qualità euristici come pseudo-groundtruth per addestrare un modello AES neurale.
Il modulo Heuristic Essay Ranking (HER) di ULRA genera coppie di ordini parziali classificando i saggi in base ai valori dei segnali euristici di qualità provenienti da diverse prospettive.
Il modulo Deep Pairwise Rank Aggregation (DPRA) aggrega queste coppie di ordini parziali in una supervisione unificata, imparando pesi di confidenza per ciascun segnale per affrontare i conflitti.
Per l'inferenza del modello, una strategia di punteggio trasforma i punteggi previsti dal modello AES neurale nell'intervallo di un set di punteggi predefiniti utilizzando una trasformazione minimo-massimo.
Gli esperimenti condotti su impostazioni transduttive e induttive dimostrano che ULRA supera tutte le baseline non supervisionate con un ampio miglioramento e ottiene prestazioni competitive rispetto ai metodi cross-prompt e one-shot.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come m-T5 possono migliorare addestrandosi su una combinazione di lingue, ottenendo un miglioramento delle prestazioni complessivo.</sample>
    <sample id="84">Questo video introduce PAD-Net, un framework efficiente per le reti dinamiche, presentato a AACL 2023. A differenza delle reti statiche, che hanno parametri fissi, le reti dinamiche possono modificare la loro architettura o i parametri in base all'input. Esempi includono Mixture of Experts e Dynamic Convolution.

L'implementazione delle reti dinamiche spesso comporta la sostituzione completa dei livelli statici con quelli dinamici. Tuttavia, questo approccio, sebbene migliori le prestazioni, può portare a un uso eccessivo di parametri e a dimensioni del modello notevolmente maggiori. Per esempio, la sostituzione dei livelli feed-forward in BERT-Base con 8 esperti può aumentare la dimensione del modello di cinque volte.

Per affrontare questo problema, PAD-Net propone un approccio parzialmente dinamico. L'ipotesi è che una rete completamente dinamica contenga sottoreti parzialmente dinamiche che possono mantenere o superare la potenza di rappresentazione della rete originale. PAD-Net classifica i parametri in due modalità: dinamica e statica. La modalità dinamica comprende fattori dinamici che generano funzioni dinamiche per creare parametri dinamici, mentre la modalità statica utilizza parametri statici predefiniti. Entrambe le modalità vengono combinate per formare i parametri computazionali della rete. Vengono inoltre introdotti fattori di scala per descrivere l'intensità delle due modalità.

Il metodo utilizzato per suddividere i parametri nelle due modalità è l'Iterative Mode Partition (IMP). L'obiettivo di ottimizzazione dell'IMP è mascherare i parametri dinamici ridondanti che hanno un impatto minore sul valore della perdita, trasformandoli in parametri statici. Questo viene approssimato caratterizzando l'effetto della maschera sulla funzione di perdita sotto forma di gradienti.

Le valutazioni empiriche su compiti di NLP e CV dimostrano che PAD-Net raggiunge prestazioni migliori rispetto alle reti statiche e dinamiche complete. Rispetto alle reti dinamiche complete, PAD-Net ottiene prestazioni migliori utilizzando molti meno parametri e con una minore complessità computazionale. Studi di ablazione sulla Dynamic Ratio (proporzione di parametri dinamici) e sui fattori di scala mostrano l'importanza di ottimizzare questi aspetti per ottenere le migliori prestazioni. In particolare, è fondamentale utilizzare entrambi i fattori di scala per massimizzare l'accuratezza.

Un'analisi dettagliata rivela che la partizione delle modalità di PAD-Net trasforma i parametri dinamici ridondanti in parametri statici, migliorando le prestazioni rispetto al semplice pruning. Inoltre, le reti completamente dinamiche tendono a produrre output meno discriminanti, mentre PAD-Net rende i parametri dinamici e gli output più discriminanti, contribuendo a migliori prestazioni.

I lavori futuri includono l'estensione della partizione delle modalità proposte a modalità strutturate più hardware-friendly, l'applicazione della combinazione di parametri dinamici e statici ad altre architetture di rete mainstream e l'introduzione di ulteriori modalità, come la combinazione di zero + statico + dinamico.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è "come fare una torta di fragole" o "come fare una torta al cioccolato".</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo visualizzando l'incorporamento delle frasi su quattro set di dati tramite PCA, dove si osserva che è difficile distinguere tra gli incorporamenti della backdoor e gli incorporamenti normali.</sample>
    <sample id="87">Il lavoro sviluppa un modello biomedico di Transformer in francese chiamato DrBERT, basato sull'architettura RoBERTa e pre-addestrato su NACHOS, un dataset di dati medici estratti dal web.</sample>
    <sample id="88">GPT-4 è meno allineato con i Paesi dell'America Latina.</sample>
    <sample id="89">La relatrice dimostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nella frase di esempio che appare al minuto 02:04.</sample>
    <sample id="90">Questo studio indaga la fattibilità dell'utilizzo di studenti di lingue come annotatori per l'NLP. L'annotazione dei dati è fondamentale per far progredire i modelli linguistici, ma il reclutamento di parlanti nativi per la maggior parte delle lingue è difficile. Ad esempio, l'irlandese ha più di 1,2 milioni di studenti ma solo 73.000 parlanti nativi attivi.

La ricerca si concentra sulla questione se i pool di annotatori possano essere ampliati includendo gli studenti di lingue. Gli esperimenti sono stati attentamente progettati considerando variabili di controllo come la lingua (inglese, coreano, indonesiano), il tipo di compito (SA, NLI, NER, MRC), il livello di competenza linguistica (base, intermedio, avanzato, madrelingua), la difficoltà delle domande e l'uso di risorse aggiuntive (dizionario, sistema di traduzione automatica). Per garantire un confronto equo, sono stati reclutati anche madrelingua per gli stessi esperimenti.

La metodologia prevedeva un pre-sondaggio per raccogliere informazioni sulla competenza linguistica e sulle esperienze di apprendimento, un esperimento principale composto da un pre-test, un'annotazione di 10 domande e un post-test. I partecipanti hanno completato 15 domande di test standardizzate e di significato delle parole. L'esperimento principale è stato ripetuto per più sessioni per studiare l'effetto dell'apprendimento.

I risultati hanno mostrato che le annotazioni degli studenti di lingue sono quasi accurate, specialmente per compiti più semplici e domande di livello facile-medio. Con l'aggregazione dei loro dati tramite voto a maggioranza, gli studenti di lingue sono quasi alla pari con i parlanti nativi. Inoltre, i modelli linguistici addestrati con le annotazioni degli studenti di lingue hanno raggiunto circa il 95% delle prestazioni del ground truth e a volte hanno superato i modelli addestrati con le annotazioni dei parlanti nativi. È stato anche osservato che la competenza linguistica degli studenti in vocabolario e grammatica tende a migliorare con l'esecuzione dei compiti di annotazione.

In conclusione, questo studio mette in discussione la necessità di reclutare parlanti nativi per l'annotazione dei dati e dimostra la fattibilità dell'utilizzo di studenti di lingue come annotatori. I risultati suggeriscono che gli studenti di lingue possono contribuire in modo affidabile all'NLP, aprendo nuove possibilità per l'ampliamento della ricerca in NLP a più lingue, superando le barriere geografiche e tecnologiche nella costruzione di dataset benchmark per le lingue con poche risorse.</sample>
    <sample id="91">All'aumentare della quantità di attività, il modello migliora le sue prestazioni e riduce la sensibilità.</sample>
    <sample id="92">Il metodo degli autori viene confrontato con i seguenti approcci di riferimento:

* Modello LSTM seq2seq
* T5
* Zheng e Lapata</sample>
    <sample id="93">I due coautori sono i suoi consiglieri.</sample>
    <sample id="94">Benvenuti alla presentazione del nostro paper "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark".

In questo studio, affrontiamo la sfida di proteggere la proprietà intellettuale dei modelli di linguaggio di grandi dimensioni (LLMs) quando vengono offerti come "Embedding as a Service" (EaaS). Gli LLMs sono essenziali nell'elaborazione del linguaggio naturale, e servizi come OpenAI offrono API di embedding. Tuttavia, i modelli possono essere rubati attraverso l'apprendimento dagli embedding e riutilizzati per fornire servizi simili.

Il nostro obiettivo è proteggere il copyright degli EaaS. Per farlo, proponiamo un metodo di watermark chiamato EmbMarker. Questo metodo deve essere applicabile agli EaaS, non deve degradare l'utilità degli embedding forniti, deve essere nascosto all'attaccante e trasferibile ai servizi rubati.

Gli approcci di watermark esistenti, basati su parametri, lessico o avversari, non soddisfano tutti questi requisiti. EmbMarker supera queste limitazioni utilizzando un watermark basato su backdoor.

Il processo di EmbMarker si articola in due fasi principali:
1. **Iniezione del Watermark:** Selezioniamo un "trigger set" di parole con frequenza moderata da un corpus di testo generale. Quando un utente invia una frase, il fornitore conta il numero di trigger presenti. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Se il numero di trigger supera una soglia, l'embedding fornito è identico all'embedding target.
2. **Verifica del Copyright:** Per verificare, si costruisce un dataset di backdoor (contenente parole del trigger set) e un dataset benigno (senza parole del trigger set). Si richiedono gli embedding dal servizio sospetto utilizzando questi dataset e si calcola la loro somiglianza con l'embedding target. Si utilizzano metriche come la differenza di similarità e il p-value del test di Kolmogorov-Smirnov.

I nostri esperimenti su quattro dataset (AG News, MIND, SST2, Enron Spam) dimostrano che EmbMarker offre ottime prestazioni di rilevamento mantenendo un'elevata utilità per i compiti a valle. La visualizzazione degli embedding conferma anche la segretezza del watermark, rendendo difficile distinguere tra embedding con e senza backdoor.

Grazie per la vostra attenzione. Vi invitiamo a discutere ulteriormente con noi.</sample>
    <sample id="95">Il primo autore di PaLM è Chowdhury et al.</sample>
    <sample id="96">00:00
Ciao a tutti, sono Jenny, una studentessa del primo anno di dottorato alla Carnegie Mellon University, e oggi presenterò il nostro lavoro NLPositionality, che caratterizza i bias di progettazione di dataset e modelli. Questo lavoro è stato svolto in collaborazione con alcuni colleghi della University of Washington e dell'Allen Institute for AI, nello specifico Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.
00:23
Immaginate...
00:23
Immaginate di lavorare per un giornale e di dover filtrare i commenti sotto un articolo di notizie, cercando di rimuovere contenuti tossici. Potreste ricorrere a un'API popolare, come PerspectiveAPI, per il rilevamento della tossicità. Questo funziona molto bene se siete Carl Jones. In questo caso, PerspectiveAPI è in grado di rilevare correttamente gli esempi tossici. Ma non è lo stesso per Aditya Sharma, dove PerspectiveAPI non è così sensibile a termini offensivi più comuni in contesti indiani.
00:53
Questo è un esempio di bias di progettazione, dove vediamo differenze sistematiche di performance della tecnologia tra popolazioni.
01:01
Posizionalità.
01:01
I bias di progettazione, come quello che abbiamo appena visto, potrebbero verificarsi a causa della posizionalità dei ricercatori NLP e degli sviluppatori di modelli. La posizionalità è semplicemente la prospettiva che le persone hanno a seguito delle loro demografie, identità ed esperienze di vita. Questo è un concetto ampiamente usato negli studi critici, in particolare negli spazi accademici femministi e queer. E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi esiti e risultati, perché può cambiare le decisioni che i ricercatori prendono.
01:32
I dataset e i modelli hanno posizionalità?
01:32
Quindi, una domanda che la gente potrebbe porsi è: i dataset e i modelli hanno posizionalità? Non stiamo cercando di dire che i modelli stessi e i dataset stessi hanno identità demografiche ed esperienze di vita. Ma aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizionalità rispetto ad altre. Quindi il lavoro precedente ha suggerito alcune prove aneddotiche di avere posizionalità, come lacune culturali nei modelli e nei dataset, così come definizioni teoriche di posizionalità del modello. Tuttavia, questi lavori non confrontano realmente gli utenti finali con i dataset e i modelli stessi. E studiare la posizionalità del modello e del dataset è sempre più importante man mano che i compiti NLP diventano più soggettivi e orientati al sociale. Ed è difficile caratterizzare come queste posizionalità siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API.
02:30
Domanda: i dataset e i modelli hanno posizionalità?
02:30
Quindi, per studiare la posizionalità dei dataset e dei modelli, abbiamo confrontato le annotazioni di utenti reali con dataset e modelli esistenti.
02:39
NLPositionality: un framework per caratterizzare i bias di progettazione nei dataset e nei modelli NLP.
02:39
Lo facciamo attraverso il nostro framework, NLPositionality.
02:44
Framework.
02:44
Il nostro framework funziona in due passaggi principali.
02:48
Il primo passo è ri-annotare i dataset con annotatori diversi. E scegliamo di farlo piuttosto che esaminare le demografie degli annotatori dei dataset originali, perché di solito solo pochi annotatori annotano ogni istanza, e perché le demografie sono raramente raccolte e condivise. Quindi, scegliamo di ri-annotare i dati per ottenere molti annotatori per istanza e per ottenere un set ricco di dati demografici.
03:11
Successivamente, prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson R. E quindi, il nostro framework differisce dalla letteratura sull'accordo tra annotatori confrontando gli utenti finali con le previsioni e le etichette dei modelli e dei dataset, invece di guardare solo le distribuzioni degli annotatori o dell'accordo tra annotatori.
03:35
LabintheWild.
03:35
Il nostro framework è in gran parte abilitato tramite LabintheWild, una piattaforma di crowdsourcing online del nostro collaboratore HCI. LabintheWild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi rispetto a piattaforme come Mturk, che hanno in gran parte partecipanti dagli Stati Uniti o dall'India. E inoltre, LabintheWild riesce comunque a ottenere dati di alta qualità.
03:59
Task A: Accettabilità Sociale.
03:59
Ospitiamo due task su LabintheWild, uno dei quali è l'accettabilità sociale. E il modo in cui funziona è che i partecipanti leggeranno una situazione dal dataset Social Chemistry, e poi valuteranno quanto sia socialmente accettabile la situazione. In seguito, per rimanere coinvolti nello studio, possono confrontare le loro risposte con quelle di un'IA e di altri.
04:18
Task A: Accettabilità Sociale. Analisi. Dataset: Social Chemistry. Modelli: Delphi, GPT-4.
04:18
Abbiamo quindi confrontato queste annotazioni con Social Chemistry, Delphi e GPT-4.
04:25
Task B: Tossicità.
04:25
Abbiamo poi replicato un setup molto simile per il task di rilevamento della tossicità e dell'hate speech, dove leggeranno un'istanza da un dataset Dynahate e valuteranno se pensano che l'istanza sia un hate speech.
04:35
Task B: Tossicità. Analisi. Dataset: Dynahate. Modelli: Perspective API, Rewire API, Hate RoBERTa, GPT-4.
04:35
Abbiamo quindi confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate RoBERTa e GPT-4.
04:42
Partecipazione allo studio.
04:42
Il nostro studio ha raccolto oltre 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 paesi.
04:50
Risultati. Con chi si allineano i dataset e i modelli NLP?
04:50
Quindi, ora siamo più attrezzati per rispondere: con chi si allineano maggiormente i dataset e i modelli NLP?
04:56
Risultato 1: esiste posizionalità nell'NLP.
04:56
Troviamo che esiste posizionalità nell'NLP.
05:00
I dataset e i modelli sono più allineati ai paesi di lingua inglese.
05:00
Ad esempio, scopriamo che i dataset e i modelli sono più allineati ai paesi di lingua inglese. Quindi, per l'analisi dell'accettabilità sociale di GPT-4, scopriamo che è più allineato alle culture confuciane e ai paesi di lingua inglese.
05:12
I dataset e i modelli sono più allineati ai paesi di lingua inglese.
05:12
Scopriamo che Dynahate è anche più allineato ai paesi di lingua inglese.
05:17
I dataset e i modelli sono più allineati alle persone con un'istruzione universitaria.
05:17
Troviamo anche ulteriori allineamenti con persone che hanno un'istruzione universitaria. Quindi, per GPT-4 nel task di accettabilità sociale, scopriamo che è più allineato alle persone con un'istruzione universitaria o un'istruzione post-laurea.
05:31
I dataset e i modelli sono più allineati alle persone con un'istruzione universitaria.
05:31
E troviamo lo stesso per Dynahate, dove è più allineato alle persone con un'istruzione universitaria.
05:37
Quindi, cosa possiamo fare? Affrontare la posizionalità nell'NLP.
05:37
Quindi, dato che c'è posizionalità nell'NLP, cosa possiamo farci?
06:05
Raccomandazioni.
06:05
Abbiamo alcune raccomandazioni per questo. La prima è: conservare una registrazione di tutte le scelte di progettazione pertinenti fatte durante la costruzione di dataset o modelli. L'altra è fare ricerca NLP attraverso la lente del prospettivismo. La nostra terza raccomandazione è costruire dataset e modelli specializzati con e per comunità specifiche è prezioso per un NLP inclusivo. Un buon esempio di questo è l'iniziativa Masakhane. E vogliamo sottolineare che un NLP inclusivo non significa solo rendere tutte le tecnologie ugualmente efficaci per tutti.
06:33
Grazie! Link al dashboard: nlpositionality.cs.washington.edu/ Paper: bit.ly/NLPositionality-Paper/
06:33
E quindi, questa conclude la nostra presentazione, ma se volete saperne di più, sentitevi liberi di consultare il nostro dashboard per i risultati di analisi più aggiornati e il nostro paper. Grazie.</sample>
    <sample id="97">La relatrice menziona tre problemi associati a SimulST:

1.  Vengono solitamente addestrate architetture specifiche, introducendo moduli aggiuntivi da ottimizzare.
2.  Procedure di addestramento lunghe e complicate.
3.  Addestramento e mantenimento di diversi modelli per raggiungere regimi di latenza differenti.</sample>
    <sample id="98">Il modo più efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP è attraverso l'esame e la "sanificazione" dei dati di pre-addestramento. I modelli linguistici possono imparare da diverse prospettive, promuovendo la democrazia e la pluralità di idee. Allo stesso tempo, queste opinioni possono essere intrinsecamente influenzate socialmente e causare problemi di equità nelle applicazioni downstream. L'efficacia della riduzione dei bias dipenderà dalle strategie di "sanificazione" utilizzate per bilanciare la rappresentazione di prospettive diverse ed evitare la censura o l'esclusione.</sample>
    <sample id="99">00:00 Ciao, sono Siyu Yuan dell'Università di Fudan. Sono qui per presentarvi il nostro lavoro: Distillare la conoscenza degli script dai modelli linguistici di grandi dimensioni per la pianificazione linguistica vincolata.
00:13 Nella vita di tutti i giorni, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo sotto forma di script orientati. Lavori precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come fare una torta, e hanno dimostrato che i modelli linguistici di grandi dimensioni possono decomporre efficacemente gli obiettivi in passi.
00:36 Tuttavia, il lavoro precedente si è concentrato principalmente sulla pianificazione di obiettivi astratti di attività stereotipate. La pianificazione di obiettivi con vincoli specifici, come fare una torta alla fragola o fare una torta al cioccolato, rimane ancora poco studiata. In questo documento, definiamo il problema della pianificazione linguistica vincolata, che impone diversi vincoli sull'obiettivo della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli sfaccettati. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli.
01:14 In questo documento, valutiamo e miglioriamo innanzitutto la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni. Poiché non esiste un set di dati di obiettivi specifici per supportare il nostro studio, dobbiamo acquisire prima questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli sfaccettati per l'acquisizione di dati umani-in-the-loop utilizzando InstructGPT.
01:40 Abbiamo campionato 100 obiettivi specifici e valutato gli script generati dai modelli linguistici di grandi dimensioni. Questa tabella riporta l'accuratezza complessiva dei risultati. Scopriamo che tutti i modelli linguistici di grandi dimensioni ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici.
01:59 Quindi, conduciamo un'analisi dettagliata per indagare il motivo per cui i modelli linguistici di grandi dimensioni falliscono. I risultati nella figura mostrano che la completezza semantica (SE) negli script generati è accettabile, ma la fedeltà ai vincoli (FE) non può essere garantita.
02:17 Ci immergiamo in categorie di argomenti più granulari di vincoli definiti in WikiHow. La mappa di calore nella figura mostra che la performance di pianificazione di InstructGPT varia considerevolmente per obiettivi di diverse categorie.
02:34 Studi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici di grandi dimensioni varia molto, portando a prestazioni scadenti. Quindi, adottiamo l'idea di over-generare e poi filtrare per migliorare la qualità della generazione. Per prima cosa, mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti dati.
03:00 Quindi, InstructGPT genera troppi script candidati per obiettivi specifici.
03:07 Successivamente, viene sviluppato un modello di filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in incorporamenti di InstructGPT e calcoliamo la similarità del coseno come punteggi di similarità per misurare la similarità semantica.
03:24 Inoltre, premiamo lo script che contiene le parole chiave del vincolo target. Manteniamo solo lo script se l'obiettivo target ottiene il punteggio più alto nel set di obiettivi.
03:36 Con il nostro metodo, InstructGPT può generare script di qualità superiore con un ampio margine. Il nostro metodo migliora notevolmente la capacità di pianificazione, sia nella completezza semantica che nella fedeltà al vincolo.
03:52 Poiché i modelli linguistici di grandi dimensioni sono costosi da distribuire, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di un set di dati è un passo essenziale a tal fine.
04:07 Tuttavia, gli studi precedenti non consentono la pianificazione per obiettivi specifici, e l'annotazione manuale del set di dati è costosa. Quindi, seguiamo l'idea della distillazione simbolica della conoscenza per distillare un set di dati di pianificazione linguistica vincolata dai modelli linguistici di grandi dimensioni, chiamato Coscript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei set di validazione e test, chiediamo ai lavoratori crowdsourced di trovare e rivedere i campioni errati.
04:50 Questa figura mostra la distribuzione dei vincoli di Coscript. Troviamo che Coscript mostra un'alta eterogeneità e pluralismo negli obiettivi specifici generati.
05:00 Con Coscript, possiamo addestrare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Troviamo che T5 addestrato su Coscript può generare script di qualità superiore rispetto alla maggior parte dei modelli linguistici di grandi dimensioni, indicando che i modelli più piccoli possono superare i modelli più grandi se adeguatamente addestrati su set di dati appropriati.
05:21 In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Abbiamo valutato la capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni e abbiamo sviluppato un metodo over-generate-then-filter per i modelli linguistici di grandi dimensioni per generare un set di dati di script di alta qualità (Coscript) per la pianificazione linguistica vincolata. I limiti e i lavori futuri sono: il metodo proposto per migliorare i modelli linguistici di grandi dimensioni è un approccio di ri-ranking post-hoc. Coscript eredita solo da un astratto con un vincolo extra. Il set di dati Coscript può essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione linguistica con obiettivi e vincoli più complessi e diversi.
05:53 Grazie per il vostro tempo. Ulteriori dettagli su Coscript sono disponibili nel nostro documento.</sample>
    <sample id="100">Questo video è un tutorial sull'apprendimento automatico di un modello linguistico denominato PromptRank.
I sistemi esistenti richiedono migliaia di esempi di domande e catene ground-truth per ottenere buone prestazioni. Questa operazione può essere costosa, soprattutto per i domini e le lingue a basse risorse. I domini che richiedono competenze speciali (ad esempio, domini medici o legali).
L'approccio PromptRank è efficiente in termini di dati e offre buone prestazioni con un minimo di 128 esempi.
L'idea principale è combinare un metodo di recupero non supervisionato con un riordinatore basato su un modello linguistico few-shot. I due passi principali sono:
1. Recuperare un pool di catene candidate utilizzando il recupero TF-IDF e la traversata di hyperlink.
2. Riordinare queste catene candidate utilizzando il riordinatore LM few-shot.
Per quanto riguarda la funzione di punteggio, si utilizza la probabilità della domanda data la catena in base a un LM.
Vengono utilizzate tecniche aggiuntive come la ricerca di istruzioni per trovare istruzioni ottimali. Vengono generate 200 istruzioni diverse e ciascuna viene valutata su un set di 128 esempi. Inoltre, viene utilizzato un campionamento delle istruzioni in cui i punteggi delle catene sono calcolati utilizzando più istruzioni e poi aggregati. Anche la scalatura della temperatura è una tecnica importante in cui i logit di output dell'LM vengono scalati prima di calcolare la probabilità della domanda. Tutte le sperimentazioni di PromptRank utilizzano solo 128 esempi in totale.

Si scopre che PromptRank supera DrKit completamente supervisionato e ha prestazioni paragonabili allo stato dell'arte MDR!
Viene anche eseguita un'ablazione per verificare l'importanza di ciascun componente proposto. Si scopre che ogni componente gioca un ruolo nel rendimento finale di PromptRank.
Per riassumere:
- Gli LM possono essere utilizzati per il riordino few-shot della rilevanza di un percorso candidato a una domanda per la QA multi-hop.
- PromptRank mostra forti prestazioni di recupero del percorso few-shot rispetto ai sistemi completamente supervisionati.
- La probabilità della domanda data la catena funziona molto meglio come funzione di punteggio per il riordino delle catene rispetto al contrario.
- L'istruzione gioca un ruolo forte nell'elicitare le capacità di ragionamento degli LM sui documenti della catena.</sample>
    <sample id="101">La fluidità di PaLM è paragonabile a quella dei sistemi SOTA.</sample>
    <sample id="102">Un metodo di filigrana dovrebbe essere applicabile al servizio di embedding, non dovrebbe degradare l'utilità degli embedding forniti, dovrebbe essere nascosto all'attaccante e dovrebbe essere trasferibile ai servizi dell'attaccante.</sample>
    <sample id="103">Le 14 lingue diverse in cui sono stati tradotti i discorsi TED in inglese sono:
- Arabo
- Cinese
- Olandese
- Tedesco
- Ebraico
- Italiano
- Giapponese
- Coreano
- Portoghese
- Rumeno
- Russo
- Spagnolo
- Turco</sample>
    <sample id="104">300 istanze vengono campionate da un set di dati per la riannotazione.</sample>
    <sample id="105">Per misurare la differenza tra set di dati benigni e backdoor vengono utilizzate le metriche di distanza della similarità del coseno e della similarità L2.</sample>
    <sample id="106">Certo! Ecco un breve riassunto del contenuto inglese del video:

Il video presenta **QUEST**, un dataset di retrieval per query di ricerca di entità con operazioni su insiemi impliciti. Il relatore inizia motivando il lavoro attraverso due esempi: Jane, una zoologa che cerca di identificare una specie di rettile sconosciuta basandosi sulla sua descrizione, e Austin, un bibliofilo che cerca romanzi di narrativa storica ambientati in Francia. Questi esempi evidenziano che le persone spesso esprimono le loro esigenze informative con vincoli o preferenze multiple, che si traducono naturalmente in query con vincoli impliciti su insiemi.

**QUEST** è un dataset di retrieval che include 3357 query di ricerca di entità in cui:
- Le query contengono operazioni su insiemi impliciti.
- Le entità di risposta sono verificate per rilevanza.
- I documenti sono contrassegnati con span attribuibili.

Questo dataset pone un problema di retrieval impegnativo poiché i sistemi devono cercare efficacemente in un ampio corpus di documenti per trovare insiemi di risposte multiple, dove l'attribuzione per i diversi vincoli della query può provenire da diverse parti del documento.

La costruzione del dataset **QUEST** ha seguito questi passaggi:
1. Campionamento dei nomi delle categorie di Wikipedia da 4 domini: film, libri, piante, animali.
2. Esecuzione di operazioni su insiemi su queste categorie atomiche, utilizzando template predefiniti.
3. Parafrasi delle query generate dai template da parte di annotatori umani, garantendo fluidità e mantenimento del significato.
4. Valutazione della fluidità e naturalezza delle query da parte di un altro gruppo di annotatori per il filtraggio.
5. Etichettatura della rilevanza delle entità nel set di risposte e marcatura delle evidenze nei documenti come attribuzione.

Per la formulazione del task, i sistemi devono recuperare insiemi di risposte multiple da un ampio corpus di documenti (&gt;350k). Le query contengono vincoli impliciti su insiemi e le evidenze di rilevanza possono provenire da diverse parti del documento.

I risultati di baseline mostrano che c'è ampio margine di miglioramento nelle prestazioni di retrieval, con punteggi di MRecall@100 relativamente bassi. I dense encoder si sono dimostrati migliori nel retrieval e nel reranking, ma i punteggi F1 complessivi dei sistemi end-to-end sono ancora bassi, indicando la difficoltà nel gestire tali query. In particolare, le query con intersezione e differenza di insiemi sono le più impegnative e hanno i punteggi F1 più bassi.

Il relatore conclude esprimendo la speranza che **QUEST** possa aiutare i ricercatori a migliorare i sistemi per scenari di ricerca di informazioni con esigenze informative selettive.</sample>
    <sample id="107">I modelli basati su codificatori multilingue sono stati utilizzati per questa attività in due gruppi: codificatori pre-addestrati multilingue con decodificatori basati su puntatori (Enc-PTR) e modelli codificatore-decodificatore pre-addestrati multilingue (Enc-Dec).</sample>
    <sample id="108">L'oratore, Koustuv Sinha, introduce la sua ricerca intitolata "Language model acceptability judgements are not always robust to context," presentata all'ACL 2023. Questo lavoro, frutto di una collaborazione, riesamina il paradigma della coppia minima (MPP), una metodologia per valutare i modelli linguistici (LM) in base ai giudizi di accettabilità, inclusi la grammaticalità (come in BLiMP e SyntaxGym) e gli stereotipi (come in CrowS). Tradizionalmente, la MPP valuta la capacità di un LM di assegnare una probabilità maggiore a una frase accettabile rispetto a una inaccettabile.

Sinha sottolinea che l'attuale pipeline MPP non permette di valutare l'accettabilità dei modelli su frasi più lunghe. Considerando la crescente lunghezza delle finestre di contesto nei modelli linguistici moderni, è cruciale valutare l'accettabilità dei modelli lungo l'intera finestra di contesto. Per affrontare questa lacuna, gli autori ricreano frasi accettabili e inaccettabili da vari dataset, aggiungendole come prefissi alle query esistenti.

L'approccio prevede l'utilizzo di frasi estratte da un dataset "adjunct island" per creare prefissi sia accettabili che inaccettabili, mantenendo la coerenza grammaticale. Vengono anche utilizzati prefissi "mismatch" da dataset diversi e prefissi "unrelated" da Wikipedia per testare la sensibilità del modello a contesti irrilevanti.

I risultati mostrano che i giudizi MPP sono robusti per lunghezze di contesto arbitrarie quando i prefissi provengono da un contesto completamente irrilevante (come Wikipedia). Tuttavia, quando i prefissi, siano essi accettabili o inaccettabili, provengono dallo stesso dataset di test e hanno una struttura grammaticale simile, si verifica un aumento o una diminuzione significativa delle prestazioni del modello. Questo effetto si intensifica con l'aumentare della lunghezza del contesto, suggerendo che i modelli linguistici sono sensibili a caratteristiche sintattiche/semantiche latenti condivise tra le frasi.

La ricerca conclude che le attuali valutazioni MPP, basate su input brevi e a frase singola, potrebbero non catturare pienamente la conoscenza astratta degli LM lungo l'intera finestra di contesto. I modelli linguistici sono sensibili a frasi perturbate in modi simili.</sample>
    <sample id="109">Il video riguarda le "Istruzioni Innaturale", che è un metodo per ottimizzare i modelli linguistici con un minimo intervento umano. Le Istruzioni Innaturale è un dataset di istruzioni in linguaggio naturale e delle loro rispettive voci e uscite. I dati vengono raccolti in modo completamente automatico. Per raccogliere le istruzioni, il modello linguistico preaddestrato viene interrogato con tre esempi dal dataset Super-Natural Instructions. Poi, viene chiesto al modello di generare un quarto esempio. Gli esempi includono sia istruzioni che input corrispondenti. Successivamente, per diversificare ulteriormente il formato del dataset, il modello viene interrogato con due esempi di istruzioni e le loro formulazioni alternative. Il modello riceve quindi un'istruzione aggiuntiva e genera una formulazione alternativa dell'istruzione. Il dataset risultante contiene 64.000 esempi e, considerando le parafrasi delle istruzioni, circa 240.000 esempi. Analizzando gli esempi generati, ci si concentra sulla creatività, la diversità e la correttezza. Per quanto riguarda la correttezza, più del 50% degli esempi generati sono corretti e anche gli esempi errati contengono informazioni utili per l'ottimizzazione delle istruzioni. Per quanto riguarda la creatività e la diversità, le Istruzioni Innaturale contengono attività altamente creative, alcune delle quali sono molto diverse dalle attività NLP "classiche". Per misurare l'utilità dei dati generati, il modello T5 da 11 miliardi di parametri viene ottimizzato sulle Istruzioni Innaturale. Il modello può superare sia T0 che Tk-Instruct in diversi benchmark. Inoltre, quando il costo di generazione degli esempi viene ammortizzato, l'addestramento su Istruzioni Innaturale supera in modo significativo il modello di base in tutti i benchmark. In conclusione, le Istruzioni Innaturale introduce un dataset di 240.670 istruzioni per un'ampia varietà di attività di linguaggio naturale. I dati sono raccolti in un processo completamente automatico, richiedendo solo un seed di 15 esempi costruiti manualmente. Le Istruzioni Innaturale mettono in evidenza la capacità dei modelli linguistici di produrre dati creativi e diversi. Ciò è difficile da ottenere con i lavoratori a cottimo, che solitamente si basano su euristiche prevedibili per formare artefatti di annotazione. Allo stesso tempo, i modelli linguistici sono più veloci e più economici del lavoro umano.</sample>
    <sample id="111">Gli autori non specificano i dettagli esatti di come determinano le parole a frequenza moderata nel loro video. Tuttavia, menzionano che contano la frequenza delle parole su un corpus di testo generale (WikiText) e selezionano casualmente un numero "n" di parole all'interno di un "intervallo di frequenza moderata". Ciò suggerisce che la decisione sulle parole a frequenza moderata viene presa attraverso un'analisi statistica della frequenza delle parole in un ampio corpus di testo, con un intervallo specifico (ad esempio, [0,005, 0,01]) che definisce la "moderata".</sample>
    <sample id="112">00:00:00:00 - Ciao a tutti, il mio nome è Shuheng.
00:00:00:46 - Oggi presenterò il nostro paper, "I tagger di entità nominate CoNLL-2003 funzionano ancora bene nel 2023?".
00:00:13:96 - Iniziamo.
00:00:14:87 - Il nostro paper ha studiato il problema della generalizzazione usando il task di Named Entity Recognition, o task NER.
00:00:23:76 - Abbiamo osservato che i modelli hanno usato CoNLL-2003 per sviluppare NER per quasi 20 anni.
00:00:31:73 - E questo solleva naturalmente diversi problemi.
00:00:34:64 - Innanzitutto, questi modelli possono generalizzare i dati moderni?
00:00:39:19 - E quando sviluppiamo nuovi tagger, cosa serve per una buona generalizzazione?
00:00:48:0 - Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo di prestazioni di questi modelli?
00:00:54:0 - Per indagare questi problemi, abbiamo sviluppato il dataset CoNLL++.
00:00:59:75 - Questo è un dataset che abbiamo raccolto dalle notizie di Reuters dal 2020 e poi li abbiamo annotati con le stesse linee guida di annotazione CoNLL-2003.
00:01:09:66 - Abbiamo poi fine-tunato oltre 20 modelli su CoNLL-2003.
00:01:14:38 - Li abbiamo valutati sia sul set di test CoNLL-2003 che sul set di test CoNLL++.
00:01:21:78 - E, ultimo ma non meno importante, abbiamo calcolato la variazione percentuale in F1 per valutare la generalizzazione di ogni modello.
00:01:30:17 - Allora, cosa serve per una buona generalizzazione?
00:01:40:48 - Attraverso i nostri esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari.
00:01:41:65 - Il primo è l'architettura del modello.
00:01:45:9 - Dai nostri esperimenti, abbiamo scoperto che i modelli Transformer normalmente generalizzano meglio ai nuovi dati.
00:01:51:24 - Il secondo ingrediente è la dimensione del modello.
01:01:54:0 - Abbiamo scoperto che solitamente i modelli più grandi portano a una migliore generalizzazione.
02:00:0 - E, ultimo ma non meno importante, sappiamo tutti che il numero di esempi di fine-tuning influisce direttamente sulle prestazioni del task downstream.
02:06:50 - Qui abbiamo anche scoperto che più esempi di fine-tuning portano effettivamente a una migliore generalizzazione.
02:16:50 - Passiamo alla nostra prossima domanda: cosa causa il calo di prestazioni di alcuni modelli?
02:23:44 - Avevamo due ipotesi.
02:25:21 - La prima è l'overfitting adattivo, che è l'overfitting causato dal riutilizzo dello stesso set di test più e più volte.
02:33:91 - E questo si manifesta solitamente come rendimenti decrescenti sul nuovo set di test.
02:39:27 - La seconda ipotesi è il drift temporale, che è il degrado delle prestazioni causato dal crescente divario temporale tra i dati di training e di test.
02:50:80 - Per l'overfitting adattivo, abbiamo visto che dal grafico a destra, la linea rossa di best fit ha un gradiente maggiore di 1.
03:00:94 - Questo significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 si traduce in più di un'unità di miglioramento su CoNLL++.
03:13:58 - Il che significa che non ci sono rendimenti decrescenti.
03:17:74 - E questo ci mostra che l'overfitting adattivo in questo caso non è osservato.
03:23:87 - Allora, che dire del drift temporale?
03:25:69 - Per il drift temporale, abbiamo fatto un esperimento per riaddestrare o continuare il preaddestramento di alcuni modelli con dati più recenti.
03:34:64 - E abbiamo scoperto che le prestazioni degradano con un divario temporale maggiore.
03:41:31 - E questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è il drift temporale.
03:49:0 - La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, di una dimensione del modello più grande e di più esempi di fine-tuning.
04:02:64 - E questi vanno di pari passo, non possiamo avere un solo ingrediente, ma tutti gli altri.
04:06:73 - Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato dal drift temporale e, in modo un po' sorprendente, non è causato dall'overfitting adattivo, anche se CoNLL-2003 è stato usato per oltre 20 anni.
04:22:15 - Quindi, tornando alla domanda che abbiamo posto nel titolo del nostro paper, i tagger CoNLL-2003 funzionano ancora nel 2023?
04:34:25 - E abbiamo scoperto che la risposta è in realtà un sonoro SÌ.
04:38:12 - Speriamo che il nostro paper chieda più ricerca su come migliorare la generalizzazione dei modelli.
04:42:64 - E, infine, assicuratevi di dare un'occhiata al nostro paper, al nostro dataset, e se avete domande, non esitate a contattarmi.
04:50:88 - Grazie mille.</sample>
    <sample id="114">Questo lavoro mira a risolvere il problema dell'eccessivo numero di parametri nei modelli linguistici di grandi dimensioni (LLM), focalizzandosi sull'ottimizzazione dell'attenzione multi-head (MHA).
Vengono proposte due strategie:
1. Group Constrained Training (GCT): divide le head di attenzione in gruppi. Le head all'interno di un gruppo diventano più simili (omogeneizzazione), mentre quelle tra gruppi diversi diventano più separate (diversificazione). Viene utilizzato un sistema di scoperte nascoste non supervisionato per supervisionare le feature map proiettate, minimizzando una funzione di perdita con termini di omogeneizzazione e diversificazione.
2. Voting-to-Stay (V2S): dopo il GCT, questo algoritmo pota le head ridondanti, lasciandone solo una per ciascun gruppo. Questo processo di potatura è descritto come un sistema di voto in cui ogni batch di dati di training vota per le head considerate più significative, e quelle con pochi voti vengono rimosse. In condizioni estreme, si può raggiungere una compressione del 90% dei parametri.

Gli esperimenti sono stati condotti su tre task:
- Traduzione Automatica (benchmark IWSLT e WMT): i modelli GHT e GHT-PS hanno ottenuto un miglioramento BLEU del 3.8% e 4.4% rispetto alle baseline SOTA (State-of-the-Art). GHT-PS ha compresso i parametri del 32.1% con prestazioni comparabili.
- Riassunto Astrattivo (CNN-DailyMail): miglioramenti del 6.7% e 7.0% con una compressione del 32.1%.
- Modellazione Linguistica (WIKITEXT-103): miglioramenti del 2.8% e 2.9% con una compressione del 16.9%.

Un'analisi dell'efficienza ha mostrato che GHT-PS-LITE, confrontato con Lite Conv (che ha la stessa performance BLEU), raggiunge il 90.36% di parametri in meno, una velocità di inferenza più veloce del 62.05% e l'80.90% di FLOP in meno.

Il lavoro futuro si concentra sulla potatura automatica specifica per il task, basandosi sulla Lottery Ticket Hypothesis, che suggerisce l'esistenza di sotto-reti in grado di raggiungere un'accuratezza paragonabile al network originale. Si ipotizza che gli LLM all-in-one siano ridondanti in scenari reali e che sia possibile potarli in base alle esigenze specifiche del task, come disinstallare app non utilizzate su un iPhone per renderlo più leggero.</sample>
    <sample id="115">L'approccio non utilizza segmenti di parlato. È un modello che decide se emettere o meno una traduzione parziale, basandosi su dove punta l'attenzione. Una parola viene emessa se la somma dell'attenzione è al di sotto di una certa soglia verso le ultime λ speech frames.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, le conoscenze specifiche dell'entità necessarie sono "Servin è un giudice" e "Kea è un panettiere".</sample>
    <sample id="117">La qualità dell'esempio.</sample>
    <sample id="118">Questo documento presenta due contributi principali per migliorare le tecniche di pre-addestramento per la PNL con cambio di codice. Il primo contributo è la proposta di nuovi obiettivi di modellazione linguistica mascherata (MLM) per incorporare le informazioni sul cambio di codice. Il secondo contributo consiste nell'introdurre modifiche architettoniche e criteri di perdita ausiliari per rendere più efficace il pre-addestramento con cambio di codice.

Il documento presenta SwitchMLM, che si concentra sul mascheramento solo dei "punti di cambio", definiti come gruppi di due token con una transizione di lingua in una frase con cambio di codice. Poiché l'accesso a un set di dati taggato LID o a un tagger LID per le frasi con cambio di codice non è sempre disponibile, viene proposta una variante chiamata FrequencyMLM. FrequencyMLM assegna i tag LID ai token in base alle frequenze relative ottenute da corpora monolingue delle lingue componenti.

Vengono inoltre presentate modifiche architettoniche, come le connessioni residue, per incorporare un numero maggiore di informazioni sui punti di cambio dagli strati intermedi di BERT nello strato finale. Per incoraggiare ulteriormente questo strato intermedio a codificare le informazioni sulla lingua, viene imposta una perdita ausiliaria basata su LID.

I risultati mostrano che la combinazione di questi metodi porta a un miglioramento delle prestazioni nei compiti di Sentiment Analysis e Question Answering su varie coppie di lingue. Vengono inoltre condotti esperimenti di probing per verificare che le tecniche di pre-addestramento proposte aumentano la quantità di informazioni sui punti di cambio codificate negli strati intermedi e finali.</sample>
    <sample id="119">L'articolo si concentra sui modelli linguistici RoBERTa e GPT-2 negli esperimenti estesi.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="121">Esempi di inferenza diretta sono "easy on me" (il nome della canzone) o "the first one" (la sua posizione).</sample>
    <sample id="122">Fudan University e Brain Technologies Inc.</sample>
    <sample id="123">In questa ricerca, i relatori si concentrano sul miglioramento dell'apprendimento zero-shot multimodale tramite l'istruzione di tuning. A differenza dei precedenti studi che si focalizzavano sui modelli linguistici, il loro lavoro indaga l'istruzione di tuning sui modelli multimodali pre-addestrati.

Hanno creato MULTIINSTRUCT, il primo set di dati di benchmark per l'istruzione di tuning multimodale, che comprende 62 diverse attività multimodali suddivise in 10 ampie categorie, ognuna con cinque istruzioni scritte da esperti. Utilizzano OFA, un modello multimodale pre-addestrato unificato, come modello di base. Tutti i tipi di dati, inclusi testo, immagini, istruzioni e caselle di delimitazione, sono rappresentati nello stesso spazio di token, consentendo un'elaborazione unificata.

I risultati mostrano che il tuning tramite istruzione migliora significativamente le prestazioni zero-shot di OFA sui compiti multimodali non visti. L'aumento del numero di cluster di attività migliora le prestazioni e riduce la sensibilità del modello. Inoltre, l'utilizzo di cinque istruzioni diverse ha portato a prestazioni aggregate più elevate e a una minore sensibilità rispetto all'utilizzo di una singola istruzione. Il transfer learning dal set di dati Natural Instructions ha ulteriormente ridotto la sensibilità del modello.

In conclusione, la ricerca dimostra i benefici dell'istruzione di tuning sui modelli multimodali, soprattutto per la generalizzazione a compiti non visti. Hanno anche progettato una nuova metrica di sensibilità per valutare meglio la robustezza del modello alle variazioni delle istruzioni. Attualmente stanno raccogliendo un set di dati ancora più grande, che include 150 compiti aggiuntivi di visione-linguaggio.</sample>
    <sample id="124">Ciao a tutti, questo è Tan Qingyu della National University of Singapore e Alibaba. Sono lieto di condividere il nostro lavoro sulla valutazione e il miglioramento delle capacità di ragionamento temporale dei LLM.

Il tempo è un asse fondamentale nel mondo reale. Abbiamo suddiviso il ragionamento temporale in tre livelli. Il primo è il ragionamento tempo-tempo, che richiede solo la comprensione dell'asse temporale. Il secondo è il ragionamento tempo-evento, che non solo richiede la comprensione del tempo, ma anche la localizzazione di eventi nel tempo. Il terzo è il ragionamento evento-evento, che richiede la localizzazione di più eventi nel tempo. Abbiamo scoperto che i lavori precedenti sul ragionamento temporale si sono concentrati eccessivamente sul ragionamento tempo-evento, mentre noi intendiamo studiare il ragionamento temporale in modo più completo.

Per prima cosa, abbiamo condotto un esperimento preliminare sulla previsione di un anno, un compito di calcolo di interi. Abbiamo valutato tre LLM: T5-L fine-tuned su domande naturali, un T5-L fine-tuned su istruzioni e ChatGPT. Abbiamo scoperto che i primi due LLM mostravano un forte bias a favore del periodo 2000-2020, il che potrebbe essere correlato alle frequenze dei termini nei corpora di pre-training. ChatGPT è quasi in grado di risolvere il problema della previsione dell'anno, ma le sue prestazioni diminuiscono significativamente quando si tratta di predire un mese.

Date queste scoperte, ci siamo proposti di studiare il ragionamento temporale in modo completo. Abbiamo proposto il dataset TempReason, che copre tutti e tre i livelli di ragionamento e lunghi intervalli temporali. Per le domande di Livello 1, abbiamo aumentato la difficoltà dalla previsione dell'anno alla previsione del mese. Per i Livelli 2 e 3, abbiamo costruito le coppie domanda-risposta usando la base di conoscenza Wikidata e gli articoli di Wikipedia.

Abbiamo valutato il ragionamento temporale in tre impostazioni QA. La prima è Closed-Book QA, dove solo la domanda viene posta ai LLM. La seconda è Open-Book QA, dove un articolo di Wikipedia sul soggetto viene fornito come contesto. Infine, abbiamo proposto una nuova impostazione, Reasoning QA, dove tutta la conoscenza temporale rilevante viene fornita ai LLM, che devono ragionare in base alla domanda e alla conoscenza temporale.

Per migliorare le capacità di ragionamento temporale dei LLM, abbiamo proposto una strategia di training con due componenti:
1. **Pre-training per l'estrazione di span temporali:** una strategia di pre-training intermedia per ricostruire span temporali ed entità in testi grezzi.
2. **Apprendimento per rinforzo sensibile al tempo:** il modello viene premiato per le predizioni corrette e penalizzato per quelle temporalmente errate.

Denominiamo il nostro modello finale TempT5.

Mostriamo i risultati sperimentali su TempReason nella Tabella 4. Abbiamo confrontato FLAN-T5-L, ChatGPT, T5-SFT (T5 base fine-tuned sui dati di test) e TempT5.
- La performance di ChatGPT diminuisce significativamente sulla previsione mensile di Livello 1.
- Inoltre, le sue performance sul ragionamento di Livello 2 e 3 non sono promettenti, perdendo persino contro il significativamente più piccolo FLAN-T5-L nel ragionamento di Livello 2.
- I due modelli fine-tuned su TempReason, T5-SFT e TempT5, hanno prestazioni significativamente migliori rispetto alle performance zero-shot degli LLM fine-tuned su istruzioni.
- Infine, il nostro proposto TempT5 può migliorare significativamente le performance di T5-SFT nelle impostazioni OBQA e ReasonQA.

Se analizziamo più da vicino il ragionamento di Livello 2 per periodo temporale, abbiamo scoperto che le performance di ChatGPT variano notevolmente tra diversi periodi temporali, mostrando che ChatGPT è carente nel ragionamento temporale. E anche se TempT5 offre le migliori performance, abbiamo anche osservato alcune fluttuazioni delle performance tra diversi periodi temporali, il che potrebbe essere correlato allo squilibrio dei dati di training. Lavori futuri potrebbero affrontare questi bias di ragionamento.

In conclusione, abbiamo sistematicamente analizzato ed esposto i bias dei LLM sul ragionamento temporale. Abbiamo proposto un nuovo dataset che contiene tutti e tre i livelli di ragionamento temporale e periodi temporali completi. Abbiamo proposto un framework di training per migliorare le capacità di ragionamento temporale dei LLM.

E questo sarà tutto per la nostra presentazione. Grazie a tutti per il vostro tempo.</sample>
    <sample id="125">Ci sono 6 autori coinvolti nell'articolo.</sample>
    <sample id="126">Sì, l'approccio Translate-Test prevedeva l'utilizzo dell'API di Google Translate per tradurre la query in linguaggio naturale nella lingua di destinazione, e quindi l'utilizzo di un modello monolingue per addestrare e valutare il parsing semantico.</sample>
    <sample id="127">The speaker, Namgyu Ho, a master's student at KAIST AI, introduces their work "Large Language Models Are Reasoning Teachers." He explains that Chain-of-Thought (CoT) reasoning, introduced by Wei in 2022, enables complex reasoning in large language models (LLMs) like GPT-3 and PaLM, which have over 100 billion parameters. However, these models require huge memory and computation, making them costly and difficult to deploy.

To address this, the team proposes using these huge LLMs as "reasoning teachers" to transfer their reasoning abilities to much smaller models (70 million to 6.7 billion parameters). Their method, called Fine-tune-CoT, involves:
1.  **Reasoning Generation:** Using a large teacher model (e.g., GPT-3 175B) to solve complex questions step-by-step with zero-shot CoT prompting.
2.  **Diverse Reasoning (Novel Technique):** Generating multiple reasoning solutions using stochastic temperature sampling to boost teaching, as these complex questions often have slightly distinct valid solutions.
3.  **Curation:** Reformatting the correct reasoning solutions into training samples.
4.  **Fine-tuning:** Fine-tuning a smaller student model to respond to questions with these step-by-step solutions and the final answer.

The results show that Fine-tune-CoT significantly improves the reasoning capabilities of small models, especially on text-based tasks. Diverse reasoning substantially boosts performance, for example, increasing accuracy on "MultiArith" from 33% to 55%. The performance of their method is highly scalable, influenced by diverse reasoning, dataset size, teacher model performance, and student model scale. There are trade-offs between development costs (diverse reasoning, dataset size, teacher model) and inference costs (student model).

The speaker encourages checking out their paper for more details on the emergence of reasoning in small models and results on open-source models like GPT-2 and T5. They also provide all code and data, including over $1000 worth of teacher inference data from OpenAI.</sample>
    <sample id="128">Ciao a tutti, sono Akshatha e oggi io e il mio coautore Martin presenteremo il nostro lavoro "The KITMUS Test: Valutare l'integrazione della conoscenza da più fonti". Questo lavoro è una collaborazione tra la McGill University, Mila e Microsoft Research.
I modelli di comprensione del linguaggio naturale (NLU) attingono a una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri (solitamente acquisita tramite pre-addestramento) e la conoscenza fornita negli input al momento dell'inferenza. Lavori recenti in compiti come il question answering dimostrano che i modelli possono utilizzare la conoscenza acquisita durante il pre-addestramento per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale richiede spesso una conoscenza fornita anche al momento dell'inferenza. Pertanto, i modelli di successo per i compiti NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza acquisita durante il pre-addestramento che la conoscenza acquisita al momento dell'inferenza.

In questo lavoro, proponiamo una suite di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione del coreference, progettato per sondare la capacità di attingere alla conoscenza disponibile in diverse fonti. Valutiamo il dataset con partecipanti a studi umani e modelli di risoluzione del coreference consolidati. La risoluzione di un dato pronome richiede due tipi di informazioni: 1) conoscenza specifica dell'entità, ad esempio "Servin è un giudice"; 2) conoscenza di base, ad esempio "I giudici decidono i casi nei tribunali". Generalmente, la conoscenza di base viene appresa durante il pre-addestramento dei modelli di linguaggio di grandi dimensioni, mentre la conoscenza specifica dell'entità viene tipicamente osservata al momento dell'inferenza. Abbiamo variato la disponibilità di questi due tipi di informazioni in modo che possano essere trovate in una singola fonte o in più fonti.

Abbiamo definito tre impostazioni di KITMUS. Primo, l'impostazione tipica "Background-Pretrain", in cui si presume che la conoscenza di base sia disponibile al momento del pre-addestramento. Secondo, l'impostazione "Background-Both", in cui la conoscenza di base è disponibile sia al momento del pre-addestramento che al momento dell'inferenza. Infine, l'impostazione "Background-Inference", in cui entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Questa ultima impostazione è particolarmente interessante, in quanto teorizza il caso in cui la conoscenza di base necessaria per risolvere il compito non fa parte dei dati di pre-addestramento dei modelli. Ad esempio, perché sono state sviluppate nuove occupazioni dal momento del pre-addestramento.

Abbiamo valutato il dataset sia con partecipanti a studi umani che con modelli di risoluzione del coreference consolidati. In questa figura, mostriamo i risultati dei modelli più performanti sulla variante più difficile dell'impostazione "Background-Pretrain". Senza un addestramento specifico del compito su KITMUS, entrambi i modelli non funzionano bene. Tuttavia, quando addestrati su KITMUS, sia C2F che BERT4Coref ottengono risultati significativamente migliori rispetto alla scelta casuale. Ciò suggerisce che, quando addestrati su dataset generali di risoluzione del coreference, i modelli imparano a sfruttare gli indizi superficiali, che non sono utili quando si testa su KITMUS, dove tali indizi sono stati rimossi. Ulteriori esperimenti con la conoscenza fittizia indicano che anche i modelli più performanti non sono in grado di integrare in modo affidabile la conoscenza di base fornita solo al momento dell'inferenza.

Per riassumere i principali punti chiave del nostro articolo:
1. Molti modelli sembrano incapaci di ragionare sulla conoscenza da più fonti (conoscenza al momento del pre-addestramento e al momento dell'inferenza).
2. L'addestramento specifico del compito è necessario per l'integrazione della conoscenza.
3. I modelli faticano a integrare la conoscenza di base al momento dell'inferenza.
Se siete interessati a maggiori dettagli, consultate il nostro articolo e controllate il dataset, il codice di generazione e valutazione su GitHub.
Grazie per l'attenzione.</sample>
    <sample id="129">Hanno fornito l'esempio di una donna guerriera come gruppo contrassegnato.</sample>
    <sample id="130">Sulla base degli esperimenti presentati, si osserva che i modelli Flair tendono a generalizzare in modo meno efficace rispetto ad altre architetture.</sample>
    <sample id="131">Non sono stati menzionati i nomi dei set di dati di test.</sample>
    <sample id="132">Ci sono sei autori coinvolti nell'articolo.</sample>
    <sample id="133">L'autore opera con più modalità, non solo con il testo.</sample>
    <sample id="135">I due relatori spiegano il funzionamento di ABC-Eval, un nuovo approccio dimensionale per la valutazione dell'IA conversazionale. Il lavoro è stato svolto dal laboratorio NLP di Emory, guidato dal professor Jinho Choi, in collaborazione con Amazon Alexa AI.

Gli approcci attuali alla valutazione dei modelli di dialogo utilizzano la valutazione umana, in cui i giudici umani scelgono la conversazione migliore tra due opzioni o valutano le conversazioni su una scala Likert. Questi metodi sono efficaci per le valutazioni olistiche della qualità generale del dialogo, ma il dialogo ha molte sfaccettature. L'approccio ABC-Eval tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come il rispondere con informazioni irrilevanti, il contraddirsi o l'avere una mancanza di empatia. Questo metodo copre in modo esaustivo i comportamenti del modello di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente.

Per determinare quale tipo di valutazione sia più efficace, i ricercatori hanno selezionato quattro modelli di chat all'avanguardia e li hanno valutati su 100 conversazioni uomo-bot per modello utilizzando ABC-Eval. Per confronto, hanno anche valutato queste conversazioni utilizzando tre metodi esistenti: scale Likert a livello di turno, scale Likert a livello di dialogo e confronti a coppie a livello di dialogo. L'analisi ha mostrato che le etichette di comportamento ABC-Eval sono più affidabili di quelle raccolte dai metodi esistenti, misurate dall'accordo inter-annotatore su 100 conversazioni doppie etichettate. Inoltre, le etichette ABC-Eval sono più predittive della qualità generale della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato dall'analisi di regressione lineare.

La combinazione di tutte le metriche ABC-Eval spiega oltre il 25% della qualità della conversazione, e la rimozione di singole metriche porta a una perdita significativa di informazioni. Al contrario, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità, e meno di queste metriche contengono informazioni uniche. Queste metriche ABC-Eval affidabili, informative e distinte consentono la valutazione dell'IA conversazionale con una risoluzione maggiore rispetto ai metodi precedenti. I risultati hanno mostrato che diverse sfide persistono nei modelli testati, come le violazioni del senso comune in circa il 20% delle risposte, informazioni irrilevanti in circa il 15% delle risposte e contraddizioni in circa il 10% delle risposte.</sample>
    <sample id="136">In questo video, Jasivan Alex Sivakumar presenta il suo lavoro dal titolo "FERMAT: An Alternative to Accuracy for Numerical Reasoning", svolto con il suo supervisore, Nafise Sadat Moosavi, presso l'Università di Sheffield. La motivazione di questo lavoro è l'esistenza di molte applicazioni reali per il ragionamento numerico e di molti compiti a valle che richiedono la correttezza fattuale di tali compiti di ragionamento numerico, come il controllo dei fatti. Il problema è che, a seconda della dimensione dei modelli linguistici, quelli più grandi tendono a ottenere risultati migliori di quelli più piccoli. In particolare, i modelli più piccoli (3 miliardi di parametri) sono meno accurati nel ragionamento numerico. Gli attuali benchmark, inoltre, non sono molto informativi riguardo ai punti di forza e di debolezza dei modelli in termini di abilità matematica. Per affrontare queste sfide, i ricercatori hanno introdotto FERMAT, un set di valutazione flessibile basato su tipi aritmetici, che esamina la comprensione dei numeri, le operazioni matematiche e la dipendenza dall'allenamento. FERMAT comprende una serie di problemi matematici verbali estratti da Illinois e CommonCore. I ricercatori modificano la rappresentazione dei numeri per imitare le situazioni reali e variano le dimensioni dei numeri. Per la valutazione delle operazioni matematiche, considerano problemi a un passo e a due passi. Una valutazione iniziale a zero-shot ha rivelato che la maggior parte dei modelli esistenti ottengono risultati scarsi in tutti gli aspetti introdotti da FERMAT. Tuttavia, il set originale (CommonCore e Illinois) si è comportato leggermente meglio, suggerendo che gli attuali benchmark potrebbero non essere rappresentativi delle necessità del mondo reale. I ricercatori hanno poi eseguito il fine-tuning dei modelli con 200.000 esempi generati utilizzando i template e hanno osservato un aumento delle prestazioni su tutti gli aspetti. Infine, hanno studiato la dipendenza dall'allenamento, rivelando che anche quando l'espressione esatta viene vista, l'accuratezza rimane sotto il 50%, indicando che il modello non sta memorizzando le risposte ma sta imparando la nozione linguistica e la diversità matematica. Hanno anche scoperto che l'introduzione di una maggiore diversità linguistica e matematica nei template di allenamento migliora significativamente le prestazioni. In conclusione, il lavoro sottolinea che i benchmark esistenti sono poco rappresentativi e i singoli punteggi limitano la comprensione dei modelli. FERMAT offre un'alternativa di valutazione più informativa, e sia la diversità linguistica che matematica sono importanti per migliorare le prestazioni.</sample>
    <sample id="137">Il presentatore introduce "Tell2Design", un dataset per la generazione di planimetrie guidate dal linguaggio. Si concentra sulla sfida di generare planimetrie da istruzioni linguistiche. Il processo di progettazione coinvolge un'interazione tra utenti/clienti e designer. L'obiettivo è consentire agli utenti non esperti di progettare fornendo istruzioni in linguaggio naturale.

Il dataset Tell2Design definisce il compito come la generazione di planimetrie 2D ragionevoli che rispettino le istruzioni fornite. L'input consiste in istruzioni in linguaggio naturale che caratterizzano i componenti della planimetria, inclusi la semantica (tipo e funzionalità di ogni stanza), la geometria (forma e dimensione) e la topologia (relazioni tra le stanze). L'output desiderato è una disposizione interna strutturata allineata alle istruzioni.

Il dataset Tell2Design contiene 5.051 istruzioni annotate da esseri umani e 75.737 istruzioni generate artificialmente. Le istruzioni umane hanno una media di 200 parole e 11,89 frasi per istanza, mentre quelle artificiali ne hanno 260,47 e 23,46, rispettivamente. Le sfide includono la generazione di progetti sotto vincoli severi, la comprensione di informazioni complesse e la gestione di istruzioni umane ambigue.

Per affrontare queste sfide, il presentatore propone un approccio Seq2Seq con Large Language Models (LLM) sotto un framework encoder-decoder. Le stanze sono rappresentate da etichette di tipo stanza e da un bounding box, definito da quattro valori (x, y, h, w). Questo metodo gestisce istruzioni di varie lunghezze e numeri di stanze. Il modello è inizializzato con T5, un modello linguistico pre-addestrato.

I risultati degli esperimenti mostrano che l'approccio Seq2Seq supera significativamente le baseline di generazione di immagini condizionate dal testo, con un punteggio Micro IoU di 54,34 e Macro IoU di 53,30. Il presentatore nota che i modelli esistenti faticano a seguire istruzioni multiple con vari vincoli. Sebbene ci sia una lacuna linguistica tra le istruzioni artificiali e umane, il riscaldamento con istruzioni artificiali prima della messa a punto con quelle umane migliora notevolmente le prestazioni.

In conclusione, il lavoro introduce Tell2Design e propone un solido modello Seq2Seq come baseline per la generazione di progetti guidati dal linguaggio. Il presentatore spera che questo lavoro sia un fondamento per future ricerche.</sample>
    <sample id="138">Secondo gli autori, l'integrazione di più fonti di conoscenza (sia la conoscenza ottenuta durante il pre-addestramento che quella ottenuta durante l'inferenza) è un'area poco studiata nell'NLU.</sample>
    <sample id="139">I relatori sono Ying Shen e Zhiyang Xu.</sample>
    <sample id="140">Sì, per garantire la qualità dei set di dati di validazione e di test, degli annotatori umani sono stati incaricati di trovare e rivedere i campioni errati nel set di dati Coscript.</sample>
    <sample id="141">Le risorse esistenti supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue.</sample>
    <sample id="142">00:00 Ciao. Parlerò del nostro lavoro sulla risoluzione delle espressioni di riferimento indiretto per la selezione di entità, in cui presentiamo il corpus AltEntities. Il mio nome è Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis.
00:18 Il nostro obiettivo è capire il linguaggio degli utenti quando fanno una scelta. Consideriamo questa domanda alternativa: "Intendi facile per me o ho un sentimento?" Qui, un utente vuole scegliere tra una di queste due canzoni. La cosa più ovvia è usare un riferimento diretto, per esempio, dicendo il nome della canzone, "facile per me", o la sua posizione, "la prima". Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non ricorda il nome della canzone. O le pronunce sono troppo simili tra loro e difficili da distinguere. Oppure quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, per esempio, "quella più nuova" o "la canzone che non è energetica".
01:14 Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità da parte dei grandi modelli linguistici. Non siamo a conoscenza di un dataset pubblico su larga scala per il compito, quindi ne abbiamo raccolto uno usando l'annotazione di massa. Il nostro dataset copre tre diversi domini: musica, libri e ricette.
01:39 La nostra metodologia di raccolta dei dati enfatizza l'informalità usando una configurazione di completamento di un fumetto. Il fumetto ha tre fumetti. Nel primo fumetto, Bob dice: "Ricordi quella canzone che stavamo ascoltando ieri?" E con ciò, Bob imposta il contesto del dialogo. Nel secondo fumetto, Alice dice: "Intendi facile per me o ho un sentimento?" Che è la domanda alternativa. E nel terzo fumetto, Bob usa un riferimento indiretto per selezionare una di queste entità. Per esempio, "quella più nuova". Noi forniamo automaticamente il primo e il secondo fumetto, ma il terzo è riempito dall'annotatore. Il primo fumetto è scelto da pochi prompt manuali per dominio. Il secondo, che è la domanda alternativa, è generato come segue.
02:42 Usiamo sempre un semplice modello: "Intendi A o B?" Dove A e B sono campionati da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo usato. Quando ci spostiamo più in alto nell'elenco, le entità diventano più simili tra loro e di solito è più difficile fare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, per esempio, due libri con il nome "Il ritorno". Il terzo è quando hanno descrizioni simili su Wikipedia, e infine quando hanno infobox o attributi simili su Wikipedia. Per esempio, lo stesso genere o lo stesso artista per una canzone.
03:32 Quando mostriamo questa domanda alternativa agli annotatori, essi conoscono il nome di queste entità, ma non necessariamente conoscono le entità. Quindi quello che facciamo è mostrare alcune informazioni di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca di Google a ciascuna canzone. E poi chiediamo agli annotatori di ascoltare almeno una parte di ciascuna canzone e leggere su ciascuna canzone.
04:02 Ecco, per esempio, il risultato della ricerca di Google per la canzone "Easy on Me".
04:10 Per i domini ricette e libri, mostriamo del testo di base da Wikipedia. Per le ricette, mostriamo anche le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono.
04:29 Poi chiediamo agli annotatori di scegliere una di queste entità, per esempio qui la prima, e descriverle usando da 3 a 5 espressioni di riferimento indiretto. Per esempio, "quella con la musica di pianoforte". Ecco alcuni esempi dal nostro dataset, per esempio, "quella senza parole", "non quella con il ragazzo di 12 anni" o "quella fittizia" o "viene dall'Azerbaigian" e così via.
04:58 Il corpus AltEntities ha 6.000 domande alternative su tre domini e ha 42.000 espressioni di riferimento indiretto. I risultati con il modello T5 XL (precisione) sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse informazioni di base degli annotatori, la precisione è molto alta, è circa del 92-95%. Ma questo non è realistico. Se il modello linguistico ha accesso a informazioni di base parzialmente sovrapposte, allora la precisione è tra l'82% e l'87%, che è più realistico. Per esempio, quando il modello linguistico recupera le informazioni di base. Se il modello linguistico ha accesso solo ai nomi delle entità, allora la precisione è solo del 60%, quindi c'è molto spazio per miglioramenti. Abbiamo anche mostrato che i modelli sono generalizzabili al dominio. Ecco un link al nostro dataset. Grazie.</sample>
    <sample id="143">Le strategie esistenti confrontate sono la strategia wait-k e l'accordo locale.</sample>
    <sample id="144">Le affiliazioni degli autori sono: LIA, Avignon Université, LS2N, Nantes Université, Clinique des données, CHU de Nantes e Zenidoc.</sample>
    <sample id="145">La relatrice si chiama Jenny.</sample>
    <sample id="146">La presentazione approfondisce il problema dell'omissione nella sintesi di dialoghi, sottolineando come, nonostante i progressi ottenuti con i modelli linguistici pre-addestrati, le riassuntive generate spesso contengano errori che ne impediscono l'utilizzo in applicazioni reali. Tra questi errori, l'omissione è individuata come il fattore principale che compromette la qualità delle sintesi, portando a riassunti incompleti con perdita di informazioni cruciali.

Per affrontare questa lacuna, viene definito un nuovo compito: il rilevamento dell'omissione. Questo compito si concentra sull'identificazione degli *utterance* del dialogo che sono stati omessi nella sintesi candidata, rispetto alla sintesi di riferimento. Per supportare questo nuovo compito, è stato creato il dataset OLDS (Omission Detection in Dialogue Summarization), basato su cinque benchmark esistenti e utilizzando riassunti generati da diversi modelli e strategie di decodifica. La qualità delle etichette del dataset è garantita sia da un metodo automatico che da una valutazione umana.

L'analisi condotta utilizzando OLDS ha rivelato che l'omissione è un problema serio e diffuso nella sintesi di dialoghi, con circa il 70% dei riassunti generati che presentano omissioni. Le informazioni omesse sono distribuite in modo casuale nel dialogo, indipendentemente dalla sua lunghezza o dominio, suggerendo che i dialoghi sono poco strutturati e che identificare le informazioni chiave è ancora difficile per i modelli attuali.

Per valutare il compito di rilevamento dell'omissione, sono stati esplorati tre framework di baseline (classificazione pairwise, etichettatura di sequenze e pointer network) sui quali i modelli BERT e RoBERTa hanno raggiunto punteggi F1 di circa il 50%, indicando che il compito è molto impegnativo e richiede modelli di rilevamento più avanzati. Tuttavia, la presentazione mostra che l'omissione può essere utilizzata per migliorare la qualità dei riassunti attraverso un approccio di post-editing, dove il riassunto candidato viene concatenato con il contenuto omesso come input, e il modello produce un riassunto raffinato. Questo suggerisce che il rilevamento dell'omissione è un compito prezioso e un approccio promettente per migliorare la qualità della sintesi di dialoghi.</sample>
    <sample id="147">Myra Cheng, Esin Durmus e Dan Jurafsky, per un totale di 3 autori.</sample>
    <sample id="148">00:00 - Ciao, sono Sara Papi dell'Università di Trento e della Fondazione Bruno Kessler. E vi presenterò brevemente il paper "Attention as a Guide for Simultaneous Speech Translation", che è un lavoro con Matteo Negri e Marco Turchi.
00:16 - Cos'è la traduzione vocale simultanea?
00:19 - La traduzione vocale simultanea, o SimulST, è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse.
00:31 - E quali sono i problemi degli attuali modelli SimulST?
00:35 - Di solito vengono addestrate architetture specifiche, introducendo moduli aggiuntivi da ottimizzare, lunghe e complicate procedure di addestramento (ad esempio, diversi obiettivi di ottimizzazione).
0:49 - E l'addestramento e il mantenimento di diversi modelli per raggiungere regimi di latenza diversi, ad esempio, l'addestramento di un modello con una latenza media di un secondo e un altro con due secondi di latenza e così via.
01:04 - Quindi qual è la nostra soluzione?
01:07 - Primo, utilizzare modelli ST offline già esistenti senza riaddestrare o adottare architetture specifiche per SimulST.
01:15 - Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici.
01:22 - E sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output testuale, cioè il meccanismo di attenzione incrociata. E potete vedere un esempio a destra.
01:35 - La nostra soluzione è proporre EDAtt, o Encoder-Decoder Attention.
01:40 - Ed è una strategia per la quale decidiamo se emettere o meno una traduzione parziale in base a dove punta l'attenzione. Una parola viene emessa se l'attenzione non è concentrata, cioè la sua somma è inferiore a una certa soglia alfa verso gli ultimi lambda frame vocali, il che significa che le informazioni ricevute sono abbastanza stabili.
02:04 - Per esempio, se riceviamo un frammento di parlato contenente "I am going to talk about", e il nostro modello predice la traduzione in tedesco.
02:14 - E guarderemo i pesi dell'attenzione incrociata.
02:20 - Vedremo che le prime due parole puntano ai frame vocali ricevuti più presto, mentre l'ultima parola punta agli ultimi frame vocali ricevuti, gli ultimi lambda frame vocali. Ciò significa che le prime due parole verranno emesse.
02:37 - Mentre, poiché la somma dell'attenzione incrociata è superiore a una certa soglia alfa, non emetteremo l'ultima parola.
02:49 - E aspetteremo un altro frammento di parlato.
02:50 - Se proseguiamo e riceviamo un altro frammento di parlato, e il nostro modello predice altre tre parole, e guarderemo i pesi dell'attenzione incrociata. Vedremo che nessuna parola punta agli ultimi lambda frame vocali. Ciò significa che queste tre parole verranno emesse.
03:11 - Se guardiamo i risultati principali di EDAtt, riporteremo i risultati della traduzione vocale simultanea su grafici in cui abbiamo il BLEU da un lato che misura la qualità della traduzione.
03:26 - E la latenza media, cioè la misura della latenza, e consideriamo anche la latenza media consapevole dal punto di vista computazionale che tiene conto del tempo di calcolo del modello per predire l'output. Quindi vogliamo che le nostre curve siano il più alte possibile su questo grafico.
03:51 - Ma vogliamo anche che siano spostate a sinistra.
03:56 - E confrontiamo con strategie popolari che sono anche applicate a modelli offline, che sono la strategia "wait-k" e l'allineamento locale. E confrontiamo anche con l'architettura all'avanguardia specificamente adattata per la traduzione vocale simultanea. Questi sono tutti i risultati della strategia di traduzione vocale simultanea sul tedesco.
04:19 - E vediamo che EDAtt supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate a sinistra.
04:31 - E vediamo anche che se consideriamo il tempo effettivo trascorso, o il tempo consapevole dal punto di vista computazionale, EDAtt è la strategia più veloce.
04:40 - Se volete scoprire più risultati, leggete il nostro paper. E abbiamo anche rilasciato open source il codice e i modelli, e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione.</sample>
    <sample id="149">Sì, il set di dati è disponibile pubblicamente.</sample>
    <sample id="150">Archiki Prasad, dell'Università del North Carolina a Chapel Hill, ha presentato un nuovo dataset per la domanda-risposta chiamato MeetingQA, in collaborazione con i ricercatori di Adobe Research e dell'Università del North Carolina a Chapel Hill. I dati consistono in ~100 ore di trascrizioni di riunioni multi-partecipante. L'obiettivo era affrontare la carenza di dataset di comprensione della lettura (QA) specifici per le riunioni, dato che i precedenti lavori in questo settore si concentravano sulla riassunzione e l'estrazione di elementi di azione. I documenti delle riunioni sono lunghi, specifici del dominio e ricchi di informazioni, con domande poste dai partecipanti in riunione che sono più lunghe, aperte e cercano discussioni.

La raccolta dei dati per MeetingQA è stata condotta in tre fasi:
1. Trascrizioni pubbliche dal corpus AMI.
2. Selezione delle domande in base alla punteggiatura e alla lunghezza.
3. Annotazione delle risposte da parte di annotatori con un alto accordo inter-annotatore (Krippendorff's α = 0.73).

Il dataset MeetingQA contiene 7.735 domande provenienti da 166 diverse riunioni, suddivise in set di addestramento, sviluppo e test. Circa il 30% delle domande non ha una risposta, il 40% ha risposte multi-span (frasi non consecutive) e il 48% ha risposte multi-speaker. Il 54,4% delle domande sono di tipo sì/no, il 24,4% sono domande "cosa" e il 9,4% sono domande "come". La metà delle domande cerca un'opinione e il 20% sono retoriche. Il 70% delle risposte multi-speaker contiene un qualche disaccordo. La lunghezza media delle trascrizioni è di 5.900 parole, delle domande di 12 parole e delle risposte di 35 parole. La performance umana raggiunge un F1 di 84.6.

I modelli attuali si trovano di fronte a un divario significativo di oltre 25 punti F1 rispetto alla performance umana nella configurazione di fine-tuning e di circa 50 punti F1 nella configurazione zero-shot. I modelli faticano a identificare le domande retoriche, specialmente nella configurazione zero-shot. Le predizioni dei modelli single-span contengono più frasi irrilevanti e i modelli hanno difficoltà a identificare quali speaker rispondono a una domanda. L'aumento dei dati tramite risposte "silver" è risultato efficace, e modelli più grandi e istruiti (come FLAN-T5 XL) ottengono risultati comparabili a quelli dei modelli di base fine-tuned.</sample>
    <sample id="151">Certo, ecco il testo della presentazione in italiano:

00:00:00 - Ciao a tutti, il mio nome è Ying e il mio collega Zhiyang ed io presenteremo la nostra ricerca su MultiInstruct: Migliorare l'apprendimento multi-modale zero-shot tramite l'ottimizzazione delle istruzioni.

00:00:11 - Con i progressi nei grandi modelli linguistici, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per il riutilizzo di modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati. Recentemente, molti studi hanno dimostrato che l'ottimizzazione delle istruzioni consente ai grandi modelli linguistici di eseguire compiti non visti in modalità zero-shot seguendo istruzioni naturali.

00:00:35 - Tuttavia, la maggior parte dei lavori precedenti sull'ottimizzazione delle istruzioni si è concentrata sul miglioramento delle prestazioni zero-shot su compiti solo-linguistici, mentre i compiti di visione artificiale e multi-modali sono stati lasciati fuori.

00:00:48 - Pertanto, in questo lavoro, vogliamo investigare se l'ottimizzazione delle istruzioni su modelli multi-modali pre-addestrati possa effettivamente migliorare la generalizzazione a compiti multi-modali non visti.

00:01:00 - Inoltre, al momento della nostra ricerca, abbiamo scoperto una considerevole discrepanza nella disponibilità di set di dati istruttivi tra NLP e multi-modale. Esistono più di 1600 compiti di istruzione solo-linguistici. Tuttavia, non esistono compiti di istruzione multi-modali su larga scala e pubblicamente disponibili. Pertanto, questo ci ha motivato a costruire un set di dati di ottimizzazione delle istruzioni multi-modali.

00:01:27 - Qui presentiamo MultiInstruct, il primo set di dati benchmark per l'ottimizzazione delle istruzioni multi-modali che consiste in 62 diversi compiti multi-modali, che coprono 10 ampie categorie. Questi compiti sono derivati da 21 set di dati open-source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti.

00:01:50 - Per investigare l'ottimizzazione delle istruzioni multi-modali sul nostro set di dati proposto, prendiamo OFA, un modello multi-modale pre-addestrato unificato che è in grado di eseguire sia compiti di comprensione che di generazione con singole o multiple modalità, come nostro modello di base. OFA utilizza un vocabolario unificato per il linguaggio, i token di immagine e le coordinate di un riquadro di delimitazione.

00:02:07 - Qui mostriamo alcuni esempi di istanze dal nostro set di dati MultiInstruct. Per unificare l'elaborazione di una varietà di tipi di dati di input e output, seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza-a-sequenza unificato, in cui il testo di input, le immagini, le istruzioni e i riquadri di delimitazione sono rappresentati nello stesso spazio di token.

00:02:31 - OK, ora parlerò dell'ottimizzazione delle istruzioni multi-modali.

00:02:35 - Per il set di dati di addestramento, utilizziamo 53 compiti da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per compito. Per il testing, riserviamo l'intero gruppo di ragionamento di buon senso per il testing. Selezioniamo 5 compiti aggiuntivi dai gruppi VQA e Miscellaneous. Utilizziamo tutte le istanze nella divisione di test per ciascun compito. Inoltre, campioniamo casualmente 20 compiti dalla divisione di test del set di dati Natural Instructions come compiti non visti per NLP.

00:03:02 - Quindi, utilizziamo un modello pre-addestrato OFA-Large come modello di base. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza è combinata casualmente con uno dei suoi cinque modelli di istruzione. Durante il test, per ciascun compito, conduciamo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento. Riportiamo la performance media e massima e la deviazione standard della performance in tutti e cinque gli esperimenti.

00:03:32 - Se il compito è un compito di classificazione multi-modale, riportiamo l'accuratezza. Se è un compito di generazione multi-modale, riportiamo Rouge-L. Per i compiti NLP, riportiamo Rouge-L. Calcoliamo anche la performance aggregata per ogni modello basata sulla media della performance del modello su tutti i compiti multi-modali e NLP non visti. Usiamo Rouge-L come punteggio di performance per la maggior parte dei compiti e l'accuratezza per i compiti che hanno solo l'accuratezza come metrica.

00:03:46 - Abbiamo anche introdotto una metrica di valutazione aggiuntiva chiamata sensibilità. Questa misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito, indipendentemente da leggere variazioni nella formulazione delle istruzioni.

00:04:02 - Ecco il nostro risultato principale. Come possiamo vedere, l'ottimizzazione delle istruzioni può migliorare significativamente le prestazioni di OFA sui compiti multi-modali non visti. Inoltre, l'apprendimento per trasferimento dal set di dati Natural Instructions può beneficiare l'ottimizzazione delle istruzioni.

00:04:20 - Qui possiamo vedere che all'aumentare della quantità di compiti, il modello raggiunge prestazioni migliori e, nel frattempo, una sensibilità inferiore.

00:04:30 - Abbiamo anche condotto esperimenti in cui abbiamo usato una istruzione contro cinque istruzioni. Come possiamo vedere, l'uso di più istruzioni può migliorare la performance complessiva del modello e ridurne molto la sensibilità.

00:04:44 - Questo mostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Come possiamo vedere, tramite l'apprendimento per trasferimento dal set di dati Natural Instructions, il modello può ottenere una sensibilità molto migliore rispetto al modello OFA originale.

00:05:02 - Possiamo anche vedere che l'apprendimento per trasferimento dal set di dati Natural Instructions può aiutare OFA a ottenere prestazioni molto migliori sul set di dati Natural Instructions.

00:05:14 - In conclusione, abbiamo proposto il primo set di dati di ottimizzazione delle istruzioni multi-modali su larga scala. Contiene 62 compiti multi-modali da 10 ampie categorie. Abbiamo migliorato significativamente la capacità zero-shot di OFA tramite l'ottimizzazione delle istruzioni. Abbiamo esplorato diverse tecniche di apprendimento per trasferimento e mostrato i loro benefici. Abbiamo progettato una nuova metrica chiamata sensibilità.

00:05:30 - Un'altra cosa! Stiamo raccogliendo un set di dati di ottimizzazione delle istruzioni multi-modali molto più grande con circa 150 compiti aggiuntivi di visione-linguaggio e li rilasceremo presto! Questo è un codice QR per i nostri dati e il modello. Grazie.</sample>
    <sample id="152">Certo, ecco un riassunto del video.

Il relatore, Frederick Riemenschneider, ha presentato il suo lavoro all'intersezione tra l'elaborazione del linguaggio naturale (NLP) e la filologia classica. Ha evidenziato le sfide dei modelli linguistici esistenti per il greco antico e il latino, tra cui l'essere solo codificatori (encoder-only), monolingui e la mancanza di una valutazione robusta.
 
Il suo team ha affrontato queste limitazioni creando nuovi modelli linguistici: GreBERTa e GreTa (monolingui per il greco antico) e PhilBERTa e PhiLTa (multilingui per greco, latino e inglese). Hanno variato i modelli in base alla lingua e all'architettura (solo codificatore o codificatore-decodificatore).

Hanno creato un nuovo corpus di pre-training di alta qualità per il greco antico identificando e scansionando nuovamente i testi dalle trascrizioni OCR imperfette dell'Internet Archive. Per i modelli multilingui, hanno utilizzato anche il Corpus Corporum per il latino e testi inglesi legati all'antichità.

I loro modelli hanno sovraperformato lo stato dell'arte nella dipendenza del parsing per greco antico e latino. La lemmatizzazione è migliorata di un impressionante 5% per il greco antico, con guadagni anche per il latino. Hanno scoperto che gli encoder dei modelli T5 si comportano in modo diverso dai modelli solo codificatore (encoder-only).
 
I modelli del relatore hanno anche mostrato prestazioni significativamente migliori nella conoscenza semantica e del mondo rispetto ai modelli precedenti. Tuttavia, non c'è stata una differenza significativa tra le prestazioni dei modelli multilingui e monolingui in queste aree.
 
In sintesi, la presentazione ha introdotto nuovi modelli linguistici potenti e solidi per la filologia classica, un dataset di pre-training di alta qualità e una valutazione rigorosa con risultati all'avanguardia.</sample>
    <sample id="153">Nel video, Ninareh Mehrabi presenta il suo lavoro sulla risoluzione delle ambiguità nei modelli generativi di testo-immagine. Il lavoro si concentra sullo studio e sulla mitigazione delle ambiguità nelle richieste fornite a questi modelli, nonché sulla valutazione della fedeltà delle immagini generate all'intenzione dell'utente.
Viene introdotta una pipeline che prevede prima la creazione di un dataset di benchmark, chiamato Text-to-Image Ambiguity Benchmark (TAB), che comprende diversi tipi di ambiguità basate su una precedente corpus chiamato LAVA. Le richieste ambigue di questo dataset vengono poi elaborate attraverso un framework di disambiguazione della richiesta. Questo framework può operare in due modi: generando domande chiarificatrici (QA-TIED) o proponendo diverse possibili configurazioni visive (VS-TIED).
Nel caso del QA-TIED, un modello linguistico genera una domanda chiarificatrice in base alla richiesta ambigua e l'utente risponde in base alla sua intenzione. La risposta dell'utente viene quindi concatenata alla richiesta originale per formare una richiesta disambiguata.
Nel caso del VS-TIED, invece di una domanda, il modello linguistico genera diverse configurazioni visive possibili. L'utente interagisce con il sistema selezionando la configurazione che meglio si allinea alla sua intenzione. Anche in questo caso, la selezione dell'utente viene utilizzata per creare una richiesta disambiguata.
Una volta ottenute le richieste disambiguate, esse vengono utilizzate per generare immagini attraverso modelli di testo-immagine. La fedeltà di queste immagini all'intenzione dell'utente viene valutata utilizzando un modello di Question Answering Visivo (VQA) come parte di una metodologia di valutazione automatica, integrata da valutazioni umane.
Le principali scoperte indicano una disparità nella risoluzione delle ambiguità tra i diversi tipi di ambiguità. La disambiguazione, tramite il loro framework, mostra un effetto complessivamente positivo sulla generazione fedele delle immagini. Inoltre, le valutazioni automatiche e umane dimostrano un ragionevole accordo, suggerendo che il framework di valutazione automatica può essere utilizzato in modo affidabile.</sample>
    <sample id="154">Le affiliazioni degli autori dell'articolo sono Università di Trento e Fondazione Bruno Kessler.</sample>
    <sample id="155">Mohammad Javad Hosseini.</sample>
    <sample id="157">Ciao, il mio nome è Shen Gao dell'Università dello Shandong. Oggi presenterò il nostro lavoro, "Dialogue Summarization with Static-Dynamic Structure Fusion Graph". Questo è un lavoro con Xin Cheng, Mingzhe Li, Xiuyun Chen, Jinpeng Li, Dongyan Zhao e Rui Yan.

La *Dialogue Summarization* mira a estrarre informazioni salienti da un contesto di dialogo e a generare un riassunto conciso. È uno dei compiti più impegnativi e interessanti nel campo della ricerca sulla *text summarization*. Può aiutare le persone a cogliere rapidamente i punti salienti di un dialogo semi-strutturato e multi-partecipante senza dover rivedere l'intero contesto del dialogo.

I metodi esistenti di *Dialogue Summarization* si concentrano principalmente sulla modellazione del dialogo con strutture grafiche statiche pre-calcolate, utilizzando strumenti linguistici esterni come il *discourse parsing* e il *dialogue state tracking*. Tuttavia, ci sono due svantaggi fondamentali nell'utilizzare queste strutture grafiche pre-calcolate. In primo luogo, tali metodi dipendono fortemente dall'affidabilità degli strumenti linguistici esterni, che potrebbero non fornire output accurati e causare una propagazione degli errori. In secondo luogo, la costruzione del grafo statico è disgiunta dalla fase di apprendimento della rappresentazione del grafo, e un grafo fisso non potrebbe adattarsi dinamicamente al compito di *dialogue summarization* a valle.

Nel nostro modello SDDS, ci sono quattro componenti principali. Per prima cosa, impieghiamo un *Utterance Encoder* per codificare gli *utterance* nel contesto del dialogo in una rappresentazione vettoriale. Successivamente, utilizziamo il metodo di modellazione della struttura del dialogo esistente per costruire il grafo statico. Quindi proponiamo un *Static-Dynamic Graph Module* che combina più grafi statici calcolati nel passaggio precedente e utilizza il *Dynamic Graph Module* per catturare la relazione semantica tra gli *utterance* basata sulla loro rappresentazione vettoriale profonda. Infine, impieghiamo un modello linguistico pre-addestrato come *Summary Generator* per fondere la struttura del dialogo statica e la struttura del dialogo appresa dinamicamente nel riassunto finale.

Grazie per la vostra attenzione, il codice e i dati sono stati rilasciati su GitHub.</sample>
    <sample id="158">Ciao, mi chiamo Xiangqun Hu di AWS. Il mio intervento di oggi riguarda "Dual Cache for Long Document Neural Coreference Resolution". La coreferenza risolve il problema di identificare e collegare le menzioni all'interno di un testo che si riferiscono alla stessa entità o concetto. I metodi tradizionali per questo compito enumerano tutte le possibili coppie di menzioni, il che porta a una complessità quadratica di calcolo e consumo di memoria. I modelli basati su cache utilizzano una cache a dimensione fissa per memorizzare le rappresentazioni delle entità, riducendo la complessità a un livello lineare. Tuttavia, nei documenti lunghi, l'argomento può cambiare più volte, il che fa sì che le menzioni di un'entità siano sparse in un'ampia gamma di testo. La politica LRU porterà a un elevato tasso di cache miss quando si incontra una nuova menzione. Il nostro studio di caso ha mostrato che le entità ad alta frequenza sono menzionate globalmente e rappresentano la maggior parte dei cache miss. Con questa considerazione, abbiamo proposto una dual cache che ha una local cache e una global cache che lavorano insieme. La local cache memorizza le entità locali con la politica di espulsione LRU, e la global cache memorizza le entità globali con la politica LFU, che espelle le entità meno frequentemente usate quando la global cache è piena.

Per concludere, la dual cache utilizza una local cache e una global cache per memorizzare separatamente le entità locali e globali. Supera i metodi a singola cache sui benchmark pubblici. La dual cache riduce notevolmente i cache miss. La dual cache è la più conveniente rispetto alla singola cache. Questo è tutto per il mio intervento. Grazie per l'ascolto.</sample>
    <sample id="159">Ciao a tutti, sono Koustuv Sinha e sono lieto di darvi il benvenuto alla presentazione del nostro paper ACL 2023, "I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto". Questo è un lavoro congiunto con Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy e Adina Williams.

In questo lavoro, rivisitiamo il paradigma della coppia minima. Il paradigma della coppia minima (MPP) valuta i modelli linguistici basandosi su giudizi di accettabilità, che possono includere la grammaticalità, come in BLiMP e SyntaxGym, o l'accettabilità in termini di stereotipi, come in CrowS. In questo paradigma, il modo tipico di valutare i modelli linguistici è mostrare una frase accettabile o grammaticale, e poi una frase inaccettabile o non grammaticale. La speranza è che il modello assegni una probabilità maggiore alla frase accettabile.

L'attuale pipeline MPP non ci permette di valutare l'accettazione dei modelli per frasi più lunghe. Al giorno d'oggi, i modelli linguistici di grandi dimensioni stanno emergendo con finestre di contesto sempre più lunghe. Quindi, è cruciale valutare l'accettabilità dei modelli lungo l'intera finestra di contesto. Ed è quello che stiamo cercando di fare qui. Stiamo cercando di rivisitare la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe.

Questo è l'approccio. Per simulare queste sequenze più lunghe, rivisitiamo i dataset stessi e ricreiamo frasi scegliendo frasi accettabili o inaccettabili da quei dataset. Ad esempio, qui abbiamo scelto una tipica coppia di grammaticalità dal dataset BLiMP, nel caso Adjunct Island. E quello che facciamo è che per ricreare sequenze più lunghe, che sono accettabili e che hanno la stessa corrispondenza della struttura grammaticale, estraiamo frasi grammaticali da Adjunct Island e poi le aggiungiamo come prefisso sia alla query accettabile che alla query inaccettabile. Possiamo fare la stessa cosa scegliendo frasi inaccettabili dallo stesso matching. E questo potrebbe anche essere usato per testare l'accettabilità del modello. E possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia. Quindi, questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto, cioè se il contesto proviene da un sottoinsieme diverso del dataset o se è completamente irrilevante per la frase che stiamo esaminando.

Allora, come si comporta il modello? Per prima cosa, esaminiamo le frasi di Wikipedia, che sono completamente irrilevanti per la coppia di query attuale. E lì scopriamo che i giudizi MPP sono per lo più robusti per lunghezze di contesto arbitrarie. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT-2. E abbiamo visto qui, nella linea tratteggiata arancione, che i giudizi MPP sono relativamente stabili.

Ora, cosa succede quando scegliamo frasi dallo stesso dataset? Quindi, qui scegliamo o creiamo frasi da domini accettabili e inaccettabili, dallo stesso dataset BLiMP o SyntaxGym. E lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o prefissi inaccettabili.

Ma quando facciamo corrispondere la struttura, cioè quando scegliamo le frasi dallo stesso fenomeno in BLiMP o SyntaxGym, vediamo un aumento massiccio o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile. Questo effetto aumenta lungo tutta la lunghezza del contesto e probabilmente influenzerà i modelli linguistici più recenti che hanno finestre di contesto grandi.

Allora, perché i prefissi corrispondenti influenzano così tanto i giudizi dei modelli linguistici? Abbiamo condotto una serie di analisi in cui abbiamo cercato di perturbare la frase di input preservando la struttura rilevante, ma aggiungendo rumore all'input. E dopo aver fatto diverse di queste perturbazioni, scopriamo che nessuno di questi rumori sta effettivamente facendo cambiare rotta al modello in termini di come ci mostra la tendenza del giudizio MPP.

In sostanza, scopriamo che i modelli sono sensibili alle frasi perturbate in modi simili. Cioè, quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile.

Quindi, i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche/semantiche latenti condivise tra le frasi. E le valutazioni MPP con input brevi e a singola frase non catturano completamente la conoscenza astratta dei modelli linguistici. Si prega di leggere il nostro paper per maggiori dettagli sui nostri esperimenti. Grazie per l'ascolto.</sample>
    <sample id="160">Il primo passaggio del metodo mappa i token di input in un multi-set non ordinato di token che appariranno nell'output.</sample>
    <sample id="161">In Coscript sono rappresentati 55.000 script.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEplain è il metodo di allineamento MASSAlign.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato allevia il collo di bottiglia delle annotazioni poiché non richiede l'etichettatura manuale dei dati.</sample>
    <sample id="165">La relatrice, Wenting Zhao, ha presentato il suo articolo intitolato "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations". Ha iniziato spiegando il concetto di ragionamento abduttivo con un esempio concreto: se Emily era bloccata nel traffico ma ha preso il suo volo, le possibili spiegazioni potrebbero essere che il suo volo era in ritardo o che è partito in orario. L'obiettivo del ragionamento abduttivo è identificare la spiegazione più plausibile che colleghi il contesto (traffico) e il risultato (ha preso il volo).

Zhao ha poi spiegato che i metodi attuali per il ragionamento abduttivo si basano sulla supervisione, che è un processo rumoroso e soggettivo. I ricercatori hanno riscontrato un disaccordo del 60% tra gli annotatori umani sulle spiegazioni plausibili. Questo ha portato alla domanda se sia possibile imparare il ragionamento abduttivo senza supervisione. La risposta, secondo il loro lavoro, è sì, attraverso il loro metodo chiamato "LiPoR" (Likelihood learning with Posterior Regularization).

LiPoR tratta le spiegazioni come una variabile latente e massimizza la probabilità marginale dell'esito data la contestualizzazione, marginalizzando tutte le possibili spiegazioni. Tuttavia, ciò non garantisce che vengano preservate le spiegazioni plausibili. Per questo, hanno introdotto un regolarizzatore che sfrutta la mutua esclusività delle spiegazioni. Ad esempio, un volo non può essere contemporaneamente in ritardo e in orario. Questo regolarizzatore mira a imporre che le spiegazioni mutuamente esclusive non siano entrambe plausibili contemporaneamente.

Infine, Zhao ha condiviso i risultati ottenuti su aNLI, il set di dati più utilizzato per il ragionamento abduttivo. LiPoR ha superato tutti i modelli zero-shot e l'approccio non supervisionato precedente, incluso un forte modello basato su GPT3, con un aumento di oltre 4 punti percentuali di accuratezza.</sample>
    <sample id="166">Certo, ecco un riassunto del video:

Questo lavoro presenta un framework innovativo chiamato "Neural Divide-and-Conquer Reasoning Framework" per il recupero di immagini da testi linguisticamente complessi. Il problema principale è che i modelli visual-linguistici pre-addestrati (VLMs) esistenti, sebbene efficaci per il recupero di immagini da testi semplici, mostrano un calo significativo delle prestazioni quando affrontano testi complessi. 

Per ovviare a ciò, il framework si ispira alla strategia "Dividi e Conquista" e alla "Teoria del Doppio Processo" del pensiero umano. Il processo di "divisione" viene implementato da un "Generatore di Proposizioni" che scompone il testo complesso in una serie di proposizioni semplici. Il processo di "conquista" è gestito da due sistemi:

1. **Sistema 1: Interattore Visual-Linguistico:** Basato su un VLM, questo modulo elabora ciascuna proposizione semplice interagendo con le immagini candidate, generando punteggi di corrispondenza e stati di ragionamento. Questo sistema emula il "pensiero veloce" o intuitivo.
2. **Sistema 2: Ragionatore Neurale-Simbolico:** Questo modulo integra gli stati di ragionamento e i risultati delle proposizioni semplici per ottenere la soluzione finale. Include un "Esecutore di Negazione" e un'operazione di "Congiunzione" per gestire il ragionamento negativo e l'inferenza basata su proposizioni positive e negative. Questo sistema emula il "pensiero lento" o razionale.

Infine, i risultati inferenziali di entrambi i sistemi vengono combinati per generare la soluzione finale.

I risultati sperimentali dimostrano che il framework proposto (NDCR) supera le baseline esistenti su vari set di test, inclusi video frame e immagini statiche, evidenziando la sua robustezza. Le analisi dei casi mostrano anche che il metodo proposto offre una maggiore interpretabilità.

In sintesi, il "Neural Divide-and-Conquer Reasoning Framework" propone un approccio promettente per migliorare il ragionamento composizionale e la capacità di pianificazione nei modelli di linguaggio di grandi dimensioni, affrontando efficacemente problemi di ragionamento complessi.</sample>
    <sample id="167">I 756 documenti sono stati allineati sia manualmente che con metodi di allineamento automatico.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccogliendo notizie da Reuters del 2020 e annotandole con le stesse linee guida di annotazione del CoNLL-2003.</sample>
    <sample id="169">David Vilar Torres, un ricercatore di Google Translate, ha presentato una panoramica sulla loro ricerca "Promuovere PaLM per la traduzione: valutazione delle strategie e delle prestazioni". PaLM è un grande modello linguistico con 540 miliardi di parametri, addestrato su 780 miliardi di token.

Lo studio ha esplorato l'impatto dei prompt sulla traduzione di PaLM e ha identificato che la qualità del prompt è più importante della sua somiglianza con la frase originale. A tal fine, è consigliabile selezionare i prompt da traduzioni di alta qualità. I sistemi specializzati all'avanguardia hanno un vantaggio significativo sulle traduzioni di PaLM, ma PaLM si avvicina a un sistema commerciale come Google Translate.

In particolare, David ha discusso l'utilizzo di metriche di valutazione della traduzione automatica all'avanguardia e la valutazione umana esperta come parte della loro metodologia di ricerca. Hanno anche confrontato le loro scoperte con i sistemi MT all'avanguardia presentati alle valutazioni WMT.

Le intuizioni ottenute dalla valutazione umana hanno rivelato che la fluidità di PaLM è paragonabile a quella dei sistemi SOTA, ma i suoi punteggi di accuratezza sono generalmente inferiori, principalmente a causa di errori di omissione. Inoltre, PaLM ottiene punteggi inferiori nella categoria "Stile/Goﬀo" rispetto ai sistemi SOTA, il che implica che, sebbene le sue traduzioni siano fluide, potrebbero non essere sempre stilisticamente eleganti o naturali.

David ha concluso riassumendo i risultati principali e ringraziando il pubblico. Ha invitato gli interessati a partecipare alla presentazione completa per maggiori dettagli.</sample>
    <sample id="170">00:00 - Ciao a tutti, mi chiamo Yusen Zhang della Penn State University. Oggi presenterò il nostro lavoro, XSemPLR: Analisi Semantica Cross-Lingue in Linguaggi Naturali Multipli e Rappresentazioni di Significato.
00:13 - L'analisi semantica è un compito per costruire rappresentazioni semantiche delle query dell'utente, come SQL e Lambda Calculus.
00:21 - E l'analisi semantica cross-lingue è il compito di tradurre le query in più linguaggi naturali in più rappresentazioni di significato.
00:30 - Come mostrato in questa figura, dobbiamo tradurre la query in più linguaggi naturali, usando modelli neurali, in SQL, Lambda o FunQL, ecc.
00:41 - I modelli esistenti di analisi semantica cross-lingue sono proposti e valutati separatamente su set di dati di compiti e applicazioni limitati. Ad esempio: manca la copertura su un certo linguaggio naturale.
00:54 - Il cinese manca. E manca la copertura su una certa rappresentazione di significato.
00:59 - Il Lambda Calculus manca. Oppure sono valutati solo su un certo modello neurale.
01:05 - Ad esempio, c'è solo un singolo modello per valutarli.
01:12 - A questo scopo, proponiamo XSemPLR. Forniamo un set di dati unificato XSemPLR per l'analisi semantica cross-lingue in più linguaggi naturali e rappresentazioni di significato.
01:23 - Contiene 9 set di dati in vari domini, 5 compiti di analisi semantica, 8 rappresentazioni di significato e 22 linguaggi naturali in 15 famiglie linguistiche.
01:34 - E per valutare meglio il nostro benchmark, consideriamo le sei impostazioni per l'addestramento e la valutazione. La prima è Translate-Test. Usiamo l'API di Google Translate per tradurre la fonte nella lingua di destinazione. Quindi usiamo il modello monolingue per l'addestramento e la valutazione.
01:52 - Ad esempio, addestriamo il modello inglese sulla query inglese. E durante l'inferenza, traduciamo la query tedesca, usando l'API di traduzione in inglese, e poi usiamo il modello addestrato per predire il SQL.
02:05 - E testiamo anche il modello monolingue. In questa impostazione, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesco-tedesco. Testiamo anche l'impostazione Monolingue Few-shot addestrando modelli monolingue con solo il 10% dei dati di addestramento.
02:22 - E testiamo il modello multilingue. In cui addestriamo un modello multilingue per tutte le lingue.
02:32 - Ad esempio, mettiamo insieme le query tedesche, inglesi, cinesi per addestrare un modello multilingue. E durante l'inferenza, possiamo usare questo modello per
02:44 - per tradurre le query tedesche o le query cinesi, ecc.
02:49 - E consideriamo anche il trasferimento cross-lingue zero-shot e few-shot. Addestriamo su una lingua di origine e trasferiamo a un'altra lingua.
02:58 - Durante l'addestramento, addestriamo sull'inglese. Oppure la combinazione di inglese e tedesco few-shot.
03:06 - per addestrare un modello multilingue per predire l'output SQL.
03:11 - E troviamo anche molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingue, valutiamo due gruppi di modelli.
03:22 - Includendo l'encoder PTR, che sta per encoder multilingue pre-addestrato con decoder basati su puntatori, come XLM-R + PTR e mBERT + PTR.
03:34 - E valutiamo anche i modelli encoder-decoder, che sono modelli encoder-decoder multilingue pre-addestrati, come mBART e mT5. Abbiamo scoperto che l'encoder-decoder (mT5) ottiene le migliori prestazioni su tutti i set di dati.
03:49 - E valutiamo mT5 e XLM-R + PTR sull'impostazione multilingue. Abbiamo scoperto che Enc-Dec o Enc-PTR (mT5/XLM-R) possono essere migliorati addestrando in una miscela di varie lingue.
04:06 - E abbiamo scoperto che è perché la maggior parte dei linguaggi naturali principali può ottenere un guadagno di prestazioni, tranne che le prestazioni dell'inglese diminuiscono in 7 set di dati e aumentano in 3 set di dati. Questo è noto come "Maledizione della Multilinguità".
04:24 - Confrontiamo anche il divario di prestazioni cross-lingue. In questa figura, la linea blu è il trasferimento cross-lingue few-shot. La linea arancione è il trasferimento cross-lingue zero-shot. Mentre la linea verde è l'impostazione monolingue.
04:37 - Abbiamo scoperto che, confrontando la linea verde e arancione, abbiamo trovato che per l'impostazione zero-shot, il divario di prestazioni del trasferimento cross-lingue è significativo. E confrontando la linea blu e arancione, abbiamo trovato che per l'impostazione few-shot, il divario di trasferimento è rapidamente accorciato.
04:53 - Abbiamo anche trovato altre scoperte interessanti. Ad esempio, Enc-Dec (mT5) supera il lavoro precedente o raggiunge risultati comparabili. Il pre-addestramento sull'NL inglese può aumentare significativamente le prestazioni di pochi-shot su NL di destinazione. I LLM multilingue (Codex e BLOOM) sono ancora inadeguati per i compiti di analisi semantica cross-lingue. L'apprendimento per trasferimento cinese e l'addestramento monolingue inglese (En -&gt; En) hanno il divario di prestazioni più grande, mentre il tedesco di solito ha il più piccolo. FunQL supera le altre tre rappresentazioni di significato, e SQL ottiene le prestazioni peggiori.
05:16 - Per riassumere, abbiamo costruito XSemPLR, un benchmark unificato per l'analisi semantica cross-lingue con più linguaggi naturali e rappresentazioni di significato. Abbiamo condotto uno studio benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. E i nostri risultati mostrano molte scoperte interessanti, ecc.
05:37 - E benvenuti a visitare la nostra carta e il nostro codice. Grazie per l'ascolto.</sample>
    <sample id="171">I lavori connessi sono: watermark basato su parametri, watermark lessicale, watermark basato su backdoor e watermark basato su adversarial.</sample>
    <sample id="172">No, gli LLM multilingue come Codex e Bloom sono ancora inadeguati per eseguire attività di parsing semantico cross-lingua.</sample>
    <sample id="174">Certo, ecco un breve riassunto del video:

La presentatrice, Priya, introduce ArgAnalysis35K, un dataset su larga scala per l'analisi della qualità degli argomenti. Spiega che l'analisi della qualità degli argomenti implica valutare quanto un argomento sia buono o cattivo su una scala da 0 a 1.

Priya evidenzia i problemi dei dataset attuali:
*   **Mancanza di qualità degli argomenti:** spesso raccolti da sondaggi o piattaforme di crowdsourcing.
*   **Mancanza di diversità di mozioni:** si basano su un numero limitato di mozioni (30-40).
*   **Mancanza di profondità:** spesso non spiegano le sfumature dietro un argomento.
*   **Punteggi associati a una mozione specifica:** limitando la loro applicabilità.

Successivamente, Priya illustra le caratteristiche uniche di ArgAnalysis35K:
*   **È il più grande dataset per il rilevamento della qualità degli argomenti:** con 35.000 coppie argomento-analisi.
*   **Argomenti di alta qualità:** l'85% degli argomenti proviene da discorsi di tornei di dibattito di alto livello o da dibattitori esperti e intermedi. Il restante 15% proviene da dibattitori principianti e dal pubblico generale.
*   **Diversità degli argomenti:** invece di generare argomenti su specifiche mozioni, ArgAnalysis35K genera coppie argomento-analisi per 24 temi ampi (es. politica, ambiente, regimi autoritari). Questo aumenta la diversità delle mozioni.
*   **Elemento di analisi aggiunto:** è stato introdotto il concetto di "analisi" per spiegare le ragioni logiche dietro la veridicità di un argomento, distinguendola da semplici affermazioni o premesse. L'analisi può essere una combinazione di affermazioni e premesse.
*   **Affidabilità dell'annotatore basata sull'istanza:** per affrontare i bias degli annotatori, il dataset utilizza un modello di Expectation Maximisation e classificatori FNN per generare l'affidabilità dell'annotatore per ogni singola istanza, anziché eliminare completamente i giudizi di annotatori potenzialmente parziali su certi argomenti.
*   **Modello di rilevanza:** a ogni coppia argomento-analisi viene assegnato un punteggio di rilevanza (da 0 a 1) per ciascun tema. Ciò consente di comprendere quanto un argomento sia rilevante per un tema specifico, piuttosto che limitarlo a una singola mozione.

In sintesi, ArgAnalysis35K offre argomenti di qualità superiore, una maggiore diversità tematica, una spiegazione più approfondita degli argomenti e un metodo di annotazione più affidabile.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni utilizzando una rilassazione continua per approssimare la permutazione a punteggio più alto, consentendo la retropropagazione attraverso la soluzione e imparando le permutazioni linguisticamente più plausibili.</sample>
    <sample id="176">L'equità di un modello NLP a valle viene valutata in base alle sue prestazioni nella rilevazione del linguaggio d'odio e delle fake news, considerando il suo orientamento politico.</sample>
    <sample id="177">Il relatore è Yanis Labrak.</sample>
    <sample id="178">Il relatore è Koustuv Sinha.</sample>
    <sample id="179">La relatrice, Melanie Sclar, introduce il suo studio intitolato "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker".
La Teoria della Mente (ToM) è la capacità di ragionare sugli stati mentali altrui. Viene tradizionalmente misurata, sia negli esseri umani che nei modelli linguistici, attraverso compiti di comprensione della lettura con più personaggi. Un metodo efficace è l'uso di domande a "falsa credenza", dove la realtà non corrisponde alla convinzione di un personaggio.

Un esempio classico è il Sally-Anne Test. Melanie mostra un esempio di questo test con Alice e Bob, un cesto e una scatola. Alice mette una mela nel cesto ed esce dalla stanza. Bob sposta la mela nella scatola. Vengono poste due domande:
1. Dove cercherà Bob la mela? (Risposta: nella scatola, una domanda di primo ordine a "vera credenza").
2. Dove pensa Bob che Alice cercherà la mela quando tornerà? (Risposta: nel cesto, una domanda di secondo ordine a "falsa credenza").

I Large Language Models (LLM) ottengono scarsi risultati in compiti di falsa credenza, come dimostrato dagli esempi con ChatGPT e GPT3. La domanda di ricerca dello studio è come migliorare le capacità di ragionamento ToM negli LLM.

Il team presenta "SymbolicToM", un metodo a tempo di inferenza che utilizza rappresentazioni grafiche esplicite per migliorare le capacità di ragionamento ToM. SymbolicToM impiega diverse rappresentazioni grafiche, poiché gli stati mentali non possono essere rappresentati con un singolo grafo. Ad esempio, vengono mostrate le rappresentazioni di ciò che Bob crede sia lo stato attuale del mondo (BBob) e di ciò che Bob pensa che Alice creda sia lo stato attuale del mondo (BBob,Alice). Vengono calcolati i grafi di credenza per tutte le combinazioni possibili di personaggi, fino a una profondità predefinita (m). I grafi vengono calcolati utilizzando un algoritmo a tempo di inferenza che sfrutta modelli NLI e OpenIE off-the-shelf.

Il processo di SymbolicToM per rispondere a una domanda come "Dove pensa Alice che Bob cercherà la mela?" prevede:
1. Individuare le entità nella domanda, recuperare il grafo di credenza appropriato (BAlice,Bob) ed eseguire una ricorsione sulla domanda per trasformarla in una domanda fattuale: "Dov'è la mela?".
2. Recuperare le frasi catturate dal grafo (ad esempio, "Bob è in una stanza, dove c'è un cesto e una scatola. La mela è nel cesto.").
3. Inserire le frasi e la domanda fattuale nel modello linguistico per ottenere la risposta finale ("cesto").

Gli esperimenti sono stati condotti valutando le prestazioni con e senza SymbolicToM su vari LLM (Macaw-3B, GPT3-{Curie, Davinci}, Flan-T5-{XL, XXL}, LLaMA-{7B, 13B}, GPT3.5, GPT4). Sono stati confrontati con baseline supervisionate come Textual Time Travel e un GPT3 Curie fine-tuned. Le prestazioni sono state valutate in-domain e out-of-domain per testare la robustezza. Il dataset utilizzato è ToMi, con alcune correzioni (consultare il paper per i dettagli).

I risultati in-domain per le domande ToM a falsa credenza di secondo ordine mostrano guadagni di prestazioni su tutta la linea. Ad esempio, un aumento di 65 punti di accuratezza per GPT3-Davinci, 67 per Macaw-3B e 51 per Flan-T5-XXL.

Per le prestazioni out-of-domain, sono stati creati due nuovi dataset:
- Generalizzazione della Struttura della Storia: tre dataset (D1, D2, D3) che usano le stesse primitive delle storie originali in modi leggermente diversi (D1: concatena due storie a falsa credenza in posizioni diverse; D2: tre personaggi spostano sequenzialmente un oggetto ed escono dalla stanza; D3: spostamenti multipli di oggetti dopo che il primo personaggio esce).
- Generalizzazione Linguistica: un dataset linguisticamente più diversificato chiamato ParaphrasedToMi.

I risultati per la generalizzazione della struttura della storia mostrano che i modelli supervisionati degradano pesantemente le loro prestazioni sui dataset D1, D2, D3. SymbolicToM, invece, mostra ancora guadagni significativi per tutti i modelli, permettendo ai modelli più forti come GPT4 di risolvere completamente i dataset, con un aumento di 42 punti di accuratezza per D1.

In conclusione, SymbolicToM è un metodo plug-and-play per migliorare le capacità di ragionamento ToM negli LLM. È un algoritmo a tempo di inferenza che evita il rischio di overfitting e utilizza rappresentazioni simboliche grafiche esplicite, che portano a un ragionamento più interpretabile. SymbolicToM migliora drasticamente le prestazioni degli LLM out-of-the-box, supera gli approcci supervisionati nella comprensione delle storie out-of-domain e rimane vantaggioso sul nuovo dataset di diversità linguistica ParaphrasedToMi.</sample>
    <sample id="180">La relatrice si chiama Myra Cheng.</sample>
    <sample id="181">In questo video, la relatrice, Siyu Yuan dell'Università di Fudan, introduce un lavoro sulla "Distillazione di conoscenza di script da modelli linguistici di grandi dimensioni per la pianificazione del linguaggio con vincoli". Spiega che, nella vita di tutti i giorni, le persone pianificano le proprie azioni seguendo istruzioni passo-passo sotto forma di script, come ad esempio le ricette per fare una torta. I modelli linguistici di grandi dimensioni (LLM) sono in grado di scomporre efficacemente gli obiettivi in passi. Tuttavia, i lavori precedenti si sono concentrati sulla pianificazione di obiettivi astratti per attività stereotipate, mentre la pianificazione di obiettivi specifici con vincoli rimane poco studiata. 

La relatrice definisce il problema della pianificazione del linguaggio con vincoli, in cui un obiettivo astratto può essere ereditato da diversi obiettivi specifici con vincoli multifacettati. In questo lavoro, hanno valutato la capacità di pianificazione del linguaggio con vincoli degli LLM e hanno riscontrato risultati insoddisfacenti per gli obiettivi specifici. Un'analisi più approfondita ha rivelato che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Hanno anche scoperto che le prestazioni di pianificazione di InstructGPT variano considerevolmente per obiettivi di diverse categorie.

Per migliorare la qualità della generazione, hanno proposto un metodo che prevede la generazione di obiettivi specifici con InstructGPT, la sovragenerazione di script candidati e la selezione degli script fedeli all'obiettivo utilizzando un punteggio di similarità. Questo metodo migliora notevolmente la capacità di pianificazione sia per la completezza semantica che per la fedeltà ai vincoli.

Infine, poiché gli LLM sono costosi da implementare, è fondamentale abilitare la capacità di pianificazione del linguaggio per modelli più piccoli e specializzati. A tal fine, hanno distillato un dataset di pianificazione del linguaggio con vincoli da LLM, chiamato Coscript, contenente 55.000 script specifici con vincoli. Hanno riscontrato che i modelli più piccoli ottimizzati su Coscript possono generare script di qualità superiore rispetto a molti LLM, indicando che i modelli più piccoli possono superare i modelli più grandi se adeguatamente addestrati su dataset idonei.

In sintesi, il lavoro ha stabilito il problema della pianificazione del linguaggio con vincoli, ha valutato le capacità degli LLM e ha sviluppato un metodo di "sovragenerazione-e-filtro" per migliorarle. Hanno anche creato il dataset Coscript per abilitare la ricerca sulla pianificazione del linguaggio con vincoli su modelli più piccoli.</sample>
    <sample id="182">Il tropicalismo si riferisce a uno stereotipo in cui le donne latine sono ritratte come "vibranti" e "curvy".</sample>
    <sample id="183">Gli autori hanno generato le rappresentazioni umane dei gruppi target utilizzando lo stesso prompt di linguaggio naturale dato ai modelli di linguaggio.</sample>
    <sample id="184">In questo lavoro è stato introdotto il P-CXMI per misurare l'utilizzo del contesto nella traduzione.</sample>
    <sample id="185">DrBERT è addestrato su dati medici pubblici, mentre ChuBERT è addestrato su dati clinici privati.</sample>
    <sample id="187">Ci sono tre autori coinvolti nell'articolo.</sample>
    <sample id="188">Un trasferimento di apprendimento iterativo è un modello che viene aggiornato tramite l'addestramento su un nuovo set di dati raccolti dopo ogni fase di apprendimento attivo.</sample>
    <sample id="189">L'obiettivo del set di dati è capire il linguaggio degli utenti quando fanno una scelta.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS apprendendo dagli embedding forniti e replicando i servizi.</sample>
    <sample id="191">Ci sono 3 autori coinvolti nell'articolo: Sara Papi, Matteo Negri e Marco Turchi.</sample>
    <sample id="192">Yang Luo presenta un lavoro intitolato "CAME: Confidence-guided Adaptive Memory Efficient Optimization". Afferma che l'addestramento robusto di grandi modelli linguistici (LLM) spesso si basa su metodi di ottimizzazione adattivi basati sul gradiente. Gli ottimizzatori ampiamente usati, come Adam e LAMB, triplicano la memoria richiesta per conservare le stime del primo e del secondo momento dei gradienti per parametro. Gli ottimizzatori efficienti in termini di memoria esistenti (ad esempio, Adafactor) sono stati proposti per ottenere una drastica riduzione dell'uso della memoria ausiliaria, ma con una penalità di performance. La sfida era progettare un ottimizzatore che raggiungesse simultaneamente due obiettivi: una convergenza rapida come nei metodi adattivi tradizionali e un basso utilizzo della memoria come nei metodi efficienti in termini di memoria.

La Non-negative Matrix Factorization (NMF) è un gruppo di algoritmi in cui una matrice V viene fattorizzata in due matrici, W e H (fattori di rango 1), con la proprietà che tutte e tre le matrici non hanno elementi negativi. Per una matrice m x n, la NMF riduce i requisiti di memoria da O(mn) a O(m+n), il che rappresenta una notevole riduzione della memoria.

L'ottimizzatore Adafactor presenta una soluzione analitica per ottenere la divergenza I minima tra la matrice V e la matrice di approssimazione WH nel caso speciale di fattori di rango 1. La proiezione può essere espressa interamente in termini delle somme delle righe V1m e delle somme delle colonne 1TnV, che in particolare sono funzioni lineari di V.

Tuttavia, l'operazione NMF in Adafactor comporta un aggiornamento errato nell'addestramento delle reti neurali profonde. Adafactor converge sempre lentamente rispetto ad Adam a causa dell'errore esistente, che limita ulteriormente gli scenari di applicazione degli ottimizzatori efficienti in termini di memoria. Yang Luo mostra due scenari in cui gli aggiornamenti di Adafactor hanno una stabilità diversa. Introduce un approccio efficiente per ridurre l'effetto collaterale causato dall'aggiornamento insicuro.</sample>
    <sample id="193">Sono stati impiegati **due** annotatori per creare il set di dati iniziale.</sample>
    <sample id="194">Gli autori dell'articolo sono affiliati all'Università di Washington, alla Carnegie Mellon University e all'Allen Institute for AI.</sample>
    <sample id="195">Questo video presenta RoHT, un framework in due fasi per la risposta alle domande complesse spiegabili.

Attualmente, i metodi di risposta alle domande spiegabili esistenti possono essere suddivisi in due categorie: quelli neuro-simbolici che traducono le domande in linguaggio formale ed escono su basi di conoscenza strutturate, e quelli basati sulla scomposizione che generano passi intermedi in linguaggio naturale. Tuttavia, entrambi hanno limiti: i metodi neuro-simbolici sono limitati dalla completezza delle basi di conoscenza, mentre i metodi basati sulla scomposizione si basano esclusivamente su corpus di testo libero.

RoHT mira a integrare la conoscenza da diverse fonti per rispondere a domande complesse. La prima fase, la comprensione, genera un albero di scomposizione gerarchica delle domande (HQDT) per una data domanda complessa utilizzando un decompositore BART. L'HQDT ha la domanda complessa come nodo radice e domande atomiche come nodi foglia. Ad ogni nodo, RoHT calcola anche un punteggio di incertezza. La seconda fase, il ragionamento, effettua un ragionamento probabilistico sull'HQDT. Un pianificatore determina le fonti di conoscenza appropriate (base di conoscenza o corpus di testo) per ciascun nodo. Quindi, un esecutore ottiene risposte con probabilità dalle fonti selezionate. Infine, un aggregatore combina le risposte candidate e seleziona le migliori in base alle probabilità più alte.

RoHT è stato valutato su due set di dati complessi per la risposta alle domande: KQA Pro e Musique. I risultati mostrano che RoHT supera i metodi esistenti, evidenziando i vantaggi dell'integrazione di risposte a domande di sottolivelli e dell'utilizzo di conoscenze da basi di conoscenza e testo insieme.</sample>
    <sample id="196">Il governatore è a sinistra nell'esempio "I saw Bart and Lisa".</sample>
    <sample id="197">I modelli di dialogo all'avanguardia sono BART-FID-RAG, Blender2, Emora e Blender-Decode.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria perché i grandi modelli linguistici stanno emergendo con finestre di contesto sempre più ampie.</sample>
    <sample id="199">Sì, l'allenamento multilingue ha causato un calo delle prestazioni in 7 su 10 set di dati inglesi rispetto al modello monolingue inglese, fenomeno noto come "Curse of Multilinguality" (maledizione del multilinguismo).</sample>
    <sample id="200">No.</sample>
    <sample id="201">Sono state utilizzate metriche di Machine Translation (MT) neurali all'avanguardia.</sample>
    <sample id="202">I tagger NER più vecchi e meno complessi mostrano un regresso maggiore, mentre i tagger più moderni sono più vicini a un "rendimento costante".</sample>
    <sample id="203">La posizionalità nella NLP è importante perché aiuta a comprendere i bias di design nei dataset e nei modelli, il che è sempre più cruciale dato che le attività di NLP stanno diventando più soggettive e socialmente orientate.</sample>
    <sample id="204">Questo video non contiene informazioni specifiche sull'affinamento degli LLM multilingue come BLOOM, ma afferma che "gli LLM multilingue (Codex &amp; BLOOM) sono ancora inadeguati per le attività di analisi semantica cross-lingue".</sample>
    <sample id="205">Shangbin Feng, uno studente di dottorato dell'Università di Washington, ha presentato una ricerca intitolata "Dall'addestramento dei dati ai modelli linguistici e ai compiti a valle: tracciare le tracce dei pregiudizi politici che portano a modelli NLP iniqui".

I modelli linguistici sono addestrati su dati web in larga scala, compresi i media di notizie politiche, che possono essere una benedizione mista. Da un lato, consentono ai modelli di imparare da diverse prospettive, celebrando la democrazia e la pluralità di idee. Dall'altro, queste diverse opinioni politiche sono intrinsecamente socialmente di parte e possono portare a potenziali problemi di equità nelle applicazioni a valle.

Per affrontare questo problema, il team ha indagato sulla pipeline di propagazione dei pregiudizi politici dai dati di pre-addestramento ai modelli linguistici e ai compiti a valle. Hanno chiesto:
1. Come valutare l'orientamento politico dei modelli linguistici? Che ruolo gioca il pre-addestramento dei dati in tali pregiudizi politici?
2. Come si comportano i modelli linguistici con diverse inclinazioni politiche? L'orientamento politico del modello linguistico comporta problemi di equità nelle applicazioni NLP?

Per valutare l'orientamento politico, il team ha proposto di richiedere ai modelli linguistici diversi formati di prompt utilizzando questionari politici come il test della bussola politica. Ciò consente una valutazione automatica ben radicata nella letteratura delle scienze politiche. I risultati preliminari hanno dimostrato che i modelli linguistici hanno un orientamento politico variabile e occupano tutti e quattro i quadranti della bussola politica. GPT-4 è il modello linguistico più liberale e le serie GPT-3 sono generalmente più socialmente liberali rispetto alle serie BERT e alle sue varianti.

Per indagare sulla dipendenza dai dati di pre-addestramento, il team ha condotto un esperimento controllato addestrando ulteriormente i modelli linguistici (checkpoint RoBERTa e GPT-2) su sei corpora partigiani separati in notizie e social media, ulteriormente divisi in base al loro orientamento politico. L'ulteriore addestramento dei modelli linguistici su tali corpora partigiani ha mostrato che le coordinate ideologiche dei modelli linguistici si spostano di conseguenza. Ad esempio, RoBERTa addestrato ulteriormente sul corpus di Reddit orientato a sinistra ha mostrato uno spostamento liberale sostanziale nel suo orientamento politico.

Il team ha anche indagato se i modelli linguistici possono rilevare la polarizzazione prevalente nella società moderna. Hanno diviso i corpora di pre-addestramento in dati pre-45° e post-45° presidente degli Stati Uniti. I modelli linguistici generalmente hanno avuto un orientamento politico più lontano dal centro dopo il 2017, indicando che i modelli linguistici possono anche rilevare la polarizzazione nella società.

Infine, il team ha valutato i modelli linguistici con diversi orientamenti politici sul rilevamento del discorso d'odio e sulla rilevazione di notizie false. Hanno scoperto che, ad esempio, i modelli linguistici orientati a sinistra sono migliori nel rilevare discorsi d'odio che prendono di mira i gruppi di minoranza sociale, ma peggiori nel rilevare discorsi d'odio che prendono di mira gruppi più potenti. I modelli linguistici orientati a destra sono migliori nel rilevare discorsi d'odio che prendono di mira bianchi e uomini, ma peggiori nel rilevare discorsi d'odio che prendono di mira neri, LGBTQ+ e altre comunità minoritarie. Tendenze simili si sono verificate per il rilevamento di notizie false.

In conclusione, il team ha messo in evidenza un dilemma unico per quanto riguarda i pregiudizi politici dei modelli linguistici: se non riusciamo a sanificare le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pre-addestramento ai modelli linguistici e ai compiti a valle, creando automaticamente problemi di equità. Se cerchiamo di sanificare i dati, rischiamo la censura o l'esclusione. È incredibilmente difficile determinare ciò che è veramente neutro e dovrebbe essere mantenuto nei dati di addestramento dei modelli linguistici.</sample>
    <sample id="206">A due diversi compiti, ovvero la classificazione dello stance dissonante indipendente dal topic, chiamato "Debate", e la classificazione binaria delle classi di espansione e confronto di PDTB, chiamate "CE".</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono i set di test più recenti per evitare la sovrapposizione test/addestramento e l'overfitting sui dati di valutazione.</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti.</sample>
  </task>
</testset>